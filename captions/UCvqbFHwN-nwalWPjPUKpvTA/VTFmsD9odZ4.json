[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "okay um hello everyone uh thanks for joining um i know it's friday so i appreciate you are here",
    "start": "320",
    "end": "6640"
  },
  {
    "text": "um my name is wojtek taczynski i work in google i'm",
    "start": "6640",
    "end": "12559"
  },
  {
    "text": "the tl of sixth scalability and together with martial gimba who is one of the chairs of six",
    "start": "12559",
    "end": "18880"
  },
  {
    "text": "scalability we are going to to present like the sixth scalability update today",
    "start": "18880",
    "end": "24400"
  },
  {
    "text": "um okay so let's start so uh what we are actually doing what the",
    "start": "24400",
    "end": "30400"
  },
  {
    "start": "28000",
    "end": "338000"
  },
  {
    "text": "scalability is actually doing so there are basically five main aspects of things we are doing",
    "start": "30400",
    "end": "38000"
  },
  {
    "text": "which kind of correspond to sub projects but uh um in terms of like six scalability that",
    "start": "38000",
    "end": "43760"
  },
  {
    "text": "doesn't really own any non-test related code like sub projects this um",
    "start": "43760",
    "end": "48879"
  },
  {
    "text": "sub projects are somewhat somewhat fluid here so um",
    "start": "48879",
    "end": "55680"
  },
  {
    "text": "so yeah so the first thing that we are doing is actually defining what does scalability really mean for kubernetes",
    "start": "55680",
    "end": "62320"
  },
  {
    "text": "um it it both means like defining",
    "start": "62320",
    "end": "67600"
  },
  {
    "text": "defining what does it actually mean but also like where we are actually heading like the",
    "start": "67600",
    "end": "73520"
  },
  {
    "text": "core principle for any scalability related efforts is to um focus on actual real life",
    "start": "73520",
    "end": "80720"
  },
  {
    "text": "scenarios and real life needs like optimization for the sake of optimization doesn't really make sense",
    "start": "80720",
    "end": "86000"
  },
  {
    "text": "because it's usually with some exceptions but usually it's like complicates the system so we should be",
    "start": "86000",
    "end": "92079"
  },
  {
    "text": "only focusing on things that are um really needed for someone um",
    "start": "92079",
    "end": "97439"
  },
  {
    "text": "the next thing that is like strictly related to that is like once we have those goals like we need to",
    "start": "97439",
    "end": "103439"
  },
  {
    "text": "actually execute towards those so um so actually",
    "start": "103439",
    "end": "109200"
  },
  {
    "text": "driving and ensuring that like the actual improvements that are needed to to to",
    "start": "109200",
    "end": "114720"
  },
  {
    "text": "get to those goals are are um are something that we are working on um this doesn't really mean in many",
    "start": "114720",
    "end": "120960"
  },
  {
    "text": "cases that we are doing the improvements um if if those are fitting into",
    "start": "120960",
    "end": "126240"
  },
  {
    "text": "um individual six like i don't know signature or sig api and missionary or whatever we are trying to",
    "start": "126240",
    "end": "133440"
  },
  {
    "text": "um to work with those six and ensure that those will be doing that but we are often um also contributing to those",
    "start": "133440",
    "end": "140239"
  },
  {
    "text": "themselves and for any crosstic improvements then there were a bunch of",
    "start": "140239",
    "end": "146000"
  },
  {
    "text": "those like over the past years um we are we are usually coordinating those across those six too",
    "start": "146000",
    "end": "152160"
  },
  {
    "text": "um the next thing is",
    "start": "152160",
    "end": "157120"
  },
  {
    "text": "ensuring or checking where we actually are like we know where we would like to be based on the first item",
    "start": "158319",
    "end": "164000"
  },
  {
    "text": "but um we don't really often know where exactly we are now um so monitoring and measuring where",
    "start": "164000",
    "end": "171440"
  },
  {
    "text": "exactly we are is is pretty critical to to to understand how far we are from our goals and is",
    "start": "171440",
    "end": "178080"
  },
  {
    "text": "there anything is there really any work that we we have to do in this area um",
    "start": "178080",
    "end": "183599"
  },
  {
    "text": "the next item which is strictly related to that is is ensuring that we are not regressing",
    "start": "183599",
    "end": "190080"
  },
  {
    "text": "so once we reach certain level of scalability we need to ensure that",
    "start": "190080",
    "end": "196239"
  },
  {
    "text": "that we actually stay at that level it's it's pretty pretty easy with scalability to regress",
    "start": "196239",
    "end": "201840"
  },
  {
    "text": "um because like every every new feature is or every new change in the system is",
    "start": "201840",
    "end": "208159"
  },
  {
    "text": "often um if not well thought through it it's often possible to easily regress the",
    "start": "208159",
    "end": "213599"
  },
  {
    "text": "system and finally we need to ensure that scalability is is",
    "start": "213599",
    "end": "220319"
  },
  {
    "text": "not the job for like a very small group of of people like sitting in the corner but it's actually",
    "start": "220319",
    "end": "226239"
  },
  {
    "text": "a job for everyone like similarly to reliability or security or whatever like when you are designing your",
    "start": "226239",
    "end": "232959"
  },
  {
    "text": "designing and implementing your your feature you need to be thinking about like scalability on your own like we",
    "start": "232959",
    "end": "239360"
  },
  {
    "text": "like it can't be we can't ensure scalability um if the overall community won't be",
    "start": "239360",
    "end": "246159"
  },
  {
    "text": "thinking about that so we actually made a very very long journey and very successful",
    "start": "246159",
    "end": "251599"
  },
  {
    "text": "journey i would say since the since um the early beginning of kubernetes when there were pretty much like",
    "start": "251599",
    "end": "258400"
  },
  {
    "text": "two or three of us like thinking about scalability now um scalability is actually an inherent",
    "start": "258400",
    "end": "264000"
  },
  {
    "text": "part for pretty much every single cap which is like i i i hope that you all know what cap is this it's like our",
    "start": "264000",
    "end": "271120"
  },
  {
    "text": "design dock more or less um people are thinking about scalability but um",
    "start": "271120",
    "end": "276160"
  },
  {
    "text": "not everyone knows exactly how to do it they know they should be thinking about it so this this item is actually about",
    "start": "276160",
    "end": "282800"
  },
  {
    "text": "ensuring that when they reach out to us like we have like best practices we",
    "start": "282800",
    "end": "288000"
  },
  {
    "text": "um we can recommend what they should be doing in that situation and so on and so on um",
    "start": "288000",
    "end": "294639"
  },
  {
    "text": "okay um one more thing probably worth mentioning is that like we you shouldn't be confusing it or it's often confused",
    "start": "294639",
    "end": "301120"
  },
  {
    "text": "like stick out six scalability is often confused or mixed with sick out of scaling like auto scanning is about",
    "start": "301120",
    "end": "307680"
  },
  {
    "text": "scaling either your size of the cluster like add by adding nodes or removing nodes or",
    "start": "307680",
    "end": "313680"
  },
  {
    "text": "um scaling your number of pods or or pots in general either horizontally",
    "start": "313680",
    "end": "319120"
  },
  {
    "text": "or vertically um scalability is about like the amount of stuff that you that you",
    "start": "319120",
    "end": "325520"
  },
  {
    "text": "can do or amount of of for example like how big your cluster is or how many",
    "start": "325520",
    "end": "330720"
  },
  {
    "text": "um how big services you can handle in a single cluster and so on and so on um",
    "start": "330720",
    "end": "336479"
  },
  {
    "text": "okay so um what is really scalability in terms of",
    "start": "336479",
    "end": "341840"
  },
  {
    "start": "338000",
    "end": "797000"
  },
  {
    "text": "kubernetes so as i mentioned like you should all we should always be focused on on actual",
    "start": "341840",
    "end": "347199"
  },
  {
    "text": "user needs so when you ask users if they want scalability or they if they want this",
    "start": "347199",
    "end": "352960"
  },
  {
    "text": "cluster to skate the answer is obvious like they are they're all saying yes of course but if you know if you ask them",
    "start": "352960",
    "end": "358319"
  },
  {
    "text": "like what does it really mean um the answer is often like i don't know",
    "start": "358319",
    "end": "364400"
  },
  {
    "text": "or it's even more it's often also i don't care because they don't want to like understand all the details of the system",
    "start": "364400",
    "end": "371840"
  },
  {
    "text": "um they want us to handle that so we need to define it our on our own so",
    "start": "371840",
    "end": "378400"
  },
  {
    "text": "like the first approximation that we did like in early 2015 um is we approximated scalability with",
    "start": "378400",
    "end": "385680"
  },
  {
    "text": "like size of the cluster so number of nodes basically so when i first started looking into that like in",
    "start": "385680",
    "end": "392639"
  },
  {
    "text": "february 2015 or something like that like the 25 note cluster was basically blowing up",
    "start": "392639",
    "end": "398160"
  },
  {
    "text": "um and over time we are basically improving that like um starting from like support 400 notes in",
    "start": "398160",
    "end": "405600"
  },
  {
    "text": "1.0 um reaching like 5 000 notes um in the steps like in in 1.6 release",
    "start": "405600",
    "end": "412639"
  },
  {
    "text": "and we didn't really go higher than that since then um",
    "start": "412639",
    "end": "418880"
  },
  {
    "text": "so now you are now you are probably thinking like if we did anything like in the last like five years and yes we did",
    "start": "418880",
    "end": "425199"
  },
  {
    "text": "um we did quite a lot because like um scalability is like actually much more",
    "start": "425199",
    "end": "430800"
  },
  {
    "text": "than than like the size of the cluster or number of nodes um basically",
    "start": "430800",
    "end": "437039"
  },
  {
    "text": "scalability is like a multi-dimensional um multi-dimensional or scalability in",
    "start": "437039",
    "end": "443840"
  },
  {
    "text": "kubernetes it's a multi-dimensional problem where like things like number of services number of pods number of",
    "start": "443840",
    "end": "451360"
  },
  {
    "text": "end points in a service number of persistent volumes and so on like the all they are actually matter for",
    "start": "451360",
    "end": "457039"
  },
  {
    "text": "scalability um so we introduced this concept of like scalability envelope",
    "start": "457039",
    "end": "463280"
  },
  {
    "text": "which is basically saying that if your cluster within a single with within each",
    "start": "463280",
    "end": "470160"
  },
  {
    "text": "of those dimensions actually fitting into that scalability envelope that means that like",
    "start": "470160",
    "end": "476000"
  },
  {
    "text": "your cluster has like scales or your cluster will be happy basically um so",
    "start": "476000",
    "end": "483120"
  },
  {
    "text": "what does it really mean that the like your cluster skates or is it happy so we are building or we are defining that",
    "start": "483120",
    "end": "489919"
  },
  {
    "text": "like based on like two main concepts um [Music] it's a license is a low so sli is like",
    "start": "489919",
    "end": "496960"
  },
  {
    "text": "silver service level indicator as the lowest service level objective you can conceptually think about them as like",
    "start": "496960",
    "end": "504240"
  },
  {
    "text": "sli being a metric and the slo being like a metric and with threshold as it's",
    "start": "504240",
    "end": "509599"
  },
  {
    "text": "actually much more subtle but like conceptually it's it's basically um it's basically that so",
    "start": "509599",
    "end": "517360"
  },
  {
    "text": "we are saying that like the cluster is happy if all those scalability slows are basically satisfied",
    "start": "518159",
    "end": "524320"
  },
  {
    "text": "so more or less like all the metrics are within that the threshold that we that we defined so",
    "start": "524320",
    "end": "530480"
  },
  {
    "text": "um we don't have much time and we don't have like um",
    "start": "530480",
    "end": "536640"
  },
  {
    "text": "enough time to talk like in very details about like slis and slows so um if you are interested i had a talk",
    "start": "536640",
    "end": "543360"
  },
  {
    "text": "purely in that topic purely about the topic in like barcelona you've gone like so you can probably find find that in in",
    "start": "543360",
    "end": "550720"
  },
  {
    "text": "youtube if you want so just very briefly like we have like six main slos that we are currently",
    "start": "550720",
    "end": "557040"
  },
  {
    "text": "measuring um slice and slos they have like sub varia some of them",
    "start": "557040",
    "end": "562399"
  },
  {
    "text": "have some sub variants but like um um conceptually we have six of them like the first two are with us like since the",
    "start": "562399",
    "end": "570000"
  },
  {
    "text": "um very beginning um the the the latter four um were added over time",
    "start": "570000",
    "end": "577839"
  },
  {
    "text": "so as you can see like we don't have like a super large coverage",
    "start": "578160",
    "end": "583519"
  },
  {
    "text": "um there are different pieces of system that aren't really covered like i mean some of them are covered implicitly like",
    "start": "583519",
    "end": "590640"
  },
  {
    "text": "for example like scheduling latency um it's actually part of pot startup",
    "start": "590640",
    "end": "596640"
  },
  {
    "text": "time or pot startup latency so um there is there's more cover to like",
    "start": "596640",
    "end": "603519"
  },
  {
    "text": "um implicitly than expo then then then from what looks it's from the one",
    "start": "603519",
    "end": "609120"
  },
  {
    "text": "from from how it looks like from the first glance but um we are still we still have a bunch of",
    "start": "609120",
    "end": "614720"
  },
  {
    "text": "like holsters so it's one of the things that we want to invest for quite some time",
    "start": "614720",
    "end": "620480"
  },
  {
    "text": "and we we never like have time for that so um if you are interested like it's it's",
    "start": "620480",
    "end": "625839"
  },
  {
    "text": "definitely one of their ideas that we would um we would benefit from your help um",
    "start": "625839",
    "end": "632640"
  },
  {
    "text": "so let's take maybe one one example and look into a little bit more detail um i'm not going to to to read it and i i",
    "start": "632640",
    "end": "640880"
  },
  {
    "text": "don't even expect you to read it like the the main point is that um even though like the the conceptually the sli",
    "start": "640880",
    "end": "648160"
  },
  {
    "text": "should be um are we are trying to make them very simple and that like high level so basically",
    "start": "648160",
    "end": "654640"
  },
  {
    "text": "for mutating api calls we can say that like the um what we want to achieve is um that like",
    "start": "654640",
    "end": "662000"
  },
  {
    "text": "the mutate like the 99 percent of mutating api calls is finishing one in one second",
    "start": "662000",
    "end": "667519"
  },
  {
    "text": "and this is more or less what is written in the slide but we need to be super precise in the slos to ensure that the way we",
    "start": "667519",
    "end": "674800"
  },
  {
    "text": "understand it and the way we measure it is like exactly how you as a user also understand it's",
    "start": "674800",
    "end": "681120"
  },
  {
    "text": "it's like we had a bunch of cases or i've heard about other cases also from outside kubernetes where",
    "start": "681120",
    "end": "687600"
  },
  {
    "text": "um the the fact that like the the ones who defined the slo",
    "start": "687600",
    "end": "694000"
  },
  {
    "text": "um understood it differently than the actual users was was causing um a lot of",
    "start": "694000",
    "end": "700320"
  },
  {
    "text": "friction so we need to be super precise here um",
    "start": "700320",
    "end": "705519"
  },
  {
    "text": "oops yeah okay so um basically",
    "start": "705519",
    "end": "711600"
  },
  {
    "text": "once once we have the like once we defined like all those slos users know what to expect where",
    "start": "711600",
    "end": "719360"
  },
  {
    "text": "what to expect from the system or how the system will be behaving if they are within the scalability envelope but",
    "start": "719360",
    "end": "725920"
  },
  {
    "text": "they also like or maybe primarily in many cases they want to know like what the scalability and below or how",
    "start": "725920",
    "end": "732560"
  },
  {
    "text": "how large it is or how how far they can go in in the dimension um they are actually interested in so",
    "start": "732560",
    "end": "739680"
  },
  {
    "text": "um computing is precisely it's like super hard or maybe even impossible",
    "start": "739680",
    "end": "746079"
  },
  {
    "text": "so we are not trying even to do that we are fortunately able to like approximate it",
    "start": "746079",
    "end": "752480"
  },
  {
    "text": "pretty well um by by",
    "start": "752480",
    "end": "757760"
  },
  {
    "text": "by providing like a certain thresholds in many dimensions in all of the dimensions um it it's if you if you take",
    "start": "757760",
    "end": "765600"
  },
  {
    "text": "a combination of those um it's",
    "start": "765600",
    "end": "770639"
  },
  {
    "text": "it's not actually the whole whole envelope it's possible to go much farther in in certain dimensions if you",
    "start": "770639",
    "end": "777040"
  },
  {
    "text": "go lower in other dimensions but we want to provide something that is",
    "start": "777040",
    "end": "783120"
  },
  {
    "text": "that is easily consumable for everyone and for the most sophisticated kind of most sophisticated users we can work for",
    "start": "783120",
    "end": "790000"
  },
  {
    "text": "them and explain them better now like if they can actually go farther with that",
    "start": "790000",
    "end": "795040"
  },
  {
    "text": "um okay and with that i'm i'm passing the microphone to marcel we'll be talking",
    "start": "795040",
    "end": "801519"
  },
  {
    "start": "797000",
    "end": "827000"
  },
  {
    "text": "about other stuff thank you so basically right now we know what scalability means",
    "start": "801519",
    "end": "807200"
  },
  {
    "text": "what kind of sls do we care about but the question is like how do we measure it how we ensure that this as these slos",
    "start": "807200",
    "end": "814880"
  },
  {
    "text": "are actually satisfied in kubernetes so we will go through scalability testing infrastructure what kind of tools we use",
    "start": "814880",
    "end": "822399"
  },
  {
    "text": "and basically how we ensure that kubernetes is scalable so",
    "start": "822399",
    "end": "827760"
  },
  {
    "start": "827000",
    "end": "949000"
  },
  {
    "text": "our main tool that we use every day is cluster loader two you can think of it as bring your own",
    "start": "827760",
    "end": "834399"
  },
  {
    "text": "cloud bring your own layout basically kind of similar to like regular deployments that",
    "start": "834399",
    "end": "840240"
  },
  {
    "text": "you that you do for kubernetes and you can think of it as",
    "start": "840240",
    "end": "845360"
  },
  {
    "text": "you specify kind of states in which you want to have your cluster in",
    "start": "845360",
    "end": "850959"
  },
  {
    "text": "for example let's say that you want to make sure that you can run 200 000 pods",
    "start": "850959",
    "end": "856000"
  },
  {
    "text": "in your cluster then what you do is you specify this state as for example you",
    "start": "856000",
    "end": "861279"
  },
  {
    "text": "create 1000 deployments with 200 pods each",
    "start": "861279",
    "end": "866560"
  },
  {
    "text": "and then you can specify even more states like okay once i have those 200",
    "start": "866560",
    "end": "871920"
  },
  {
    "text": "000 thoughts let's say i want to delete half of them and when you have those states what you can",
    "start": "871920",
    "end": "878880"
  },
  {
    "text": "do is you can specify how you transition between those states because obviously",
    "start": "878880",
    "end": "884320"
  },
  {
    "text": "if you create 1000 deployments with 200 000 pods in total then",
    "start": "884320",
    "end": "890079"
  },
  {
    "text": "most likely something will break but what you can do is basically say okay i want to create one deployment per",
    "start": "890079",
    "end": "897120"
  },
  {
    "text": "10 seconds or maybe you want to specify that um you want to create all those",
    "start": "897120",
    "end": "904000"
  },
  {
    "text": "1000 deployments within 10 minutes or one hour so you can specify all of those",
    "start": "904000",
    "end": "909199"
  },
  {
    "text": "transitions between states and then on top of that once you do it",
    "start": "909199",
    "end": "914639"
  },
  {
    "text": "what you can do is measure measure those slows that we mentioned so you specify that for",
    "start": "914639",
    "end": "920639"
  },
  {
    "text": "example you care about api latency or pod startup latency you care that the",
    "start": "920639",
    "end": "926399"
  },
  {
    "text": "pod startup latency will be within 10 seconds and so on and except for that we have a bunch of",
    "start": "926399",
    "end": "932320"
  },
  {
    "text": "other extra features that allow us to monitor observe and test it basically you can find all of",
    "start": "932320",
    "end": "939120"
  },
  {
    "text": "that on our documentation for cluster loader too and if you ever want to",
    "start": "939120",
    "end": "944720"
  },
  {
    "text": "scale test kubernetes i strongly recommend to use this tool",
    "start": "944720",
    "end": "949759"
  },
  {
    "start": "949000",
    "end": "992000"
  },
  {
    "text": "but of course as you may imagine running cluster node is pretty expensive",
    "start": "950240",
    "end": "956480"
  },
  {
    "text": "and in open source what we do is actually we run 5 000 nodes every day to test for scalability regressions",
    "start": "956480",
    "end": "964399"
  },
  {
    "text": "but this doesn't give us like really good observability because let's imagine that you know there is one day and there",
    "start": "964399",
    "end": "970720"
  },
  {
    "text": "are 50 prs merge into kubernetes and we see some regression this would be quite hard to kind of like",
    "start": "970720",
    "end": "977120"
  },
  {
    "text": "see which kind of pr's are actually breaking the kubernetes but fortunately",
    "start": "977120",
    "end": "982240"
  },
  {
    "text": "we have cubemark and cubemark is simulation of the cluster instead of running 5000 nodes what you can do is",
    "start": "982240",
    "end": "989920"
  },
  {
    "text": "just run like 80 nodes and until on those nodes you can",
    "start": "989920",
    "end": "995199"
  },
  {
    "start": "992000",
    "end": "1082000"
  },
  {
    "text": "schedule parts for example what we call whole nodes and hollow nodes is",
    "start": "995199",
    "end": "1001040"
  },
  {
    "text": "something that simulates regular node but it doesn't actually run the pods so",
    "start": "1001040",
    "end": "1008160"
  },
  {
    "text": "you can think of it as okay regular kublet just takes the pod and it's running the container but whole node",
    "start": "1008160",
    "end": "1014880"
  },
  {
    "text": "just reports back to api server i'm running the container but it's not actually running the container",
    "start": "1014880",
    "end": "1020480"
  },
  {
    "text": "so whole node consists of actually three parts one is holocubat",
    "start": "1020480",
    "end": "1026160"
  },
  {
    "text": "and we also have hollow cube proxy because kubrox is actually",
    "start": "1026160",
    "end": "1031600"
  },
  {
    "text": "one of the parts of kubernetes that is putting quite a lot of pressure on the api server and kubernetes master",
    "start": "1031600",
    "end": "1038480"
  },
  {
    "text": "and except for that we have also hollow node problem detector so with this kind of setup what you can",
    "start": "1038480",
    "end": "1045678"
  },
  {
    "text": "do is with 600 600 cpus you can test or",
    "start": "1045679",
    "end": "1051120"
  },
  {
    "text": "simulate 5000 node cluster um yeah but then you might ask okay so",
    "start": "1051120",
    "end": "1057360"
  },
  {
    "text": "how how do you actually run those 5000 hollow nodes it's it's not easy right so what we do is",
    "start": "1057360",
    "end": "1064640"
  },
  {
    "text": "we actually we actually create second cluster so we have one kubernetes cluster that is responsible for",
    "start": "1064640",
    "end": "1071679"
  },
  {
    "text": "uh for just running those hollow nodes and then those whole nodes connect to the actual kubernetes master that we are",
    "start": "1071679",
    "end": "1078559"
  },
  {
    "text": "scale testing with cluster loader two yeah except for that what we do is here",
    "start": "1078559",
    "end": "1085600"
  },
  {
    "start": "1082000",
    "end": "1139000"
  },
  {
    "text": "you can see our great tool perf dash as you can see it's quite outdated but it's like one of the most useful tools that",
    "start": "1085600",
    "end": "1091679"
  },
  {
    "text": "we have uh in six callability and here you can see example that okay",
    "start": "1091679",
    "end": "1097200"
  },
  {
    "text": "this is 5000 um ci test that we constantly run for for kubernetes and we measure here pod",
    "start": "1097200",
    "end": "1105200"
  },
  {
    "text": "startup latency and you can see like 99th percentile 50 percentile and so on and",
    "start": "1105200",
    "end": "1112960"
  },
  {
    "text": "based on that uh we can detect some possible regressions because slo says that okay",
    "start": "1112960",
    "end": "1119520"
  },
  {
    "text": "the startup latency 99 percentile needs to be below 5 seconds but um we are still unhappy when you",
    "start": "1119520",
    "end": "1126559"
  },
  {
    "text": "know the 99 percentile was three seconds and then it bumped to four seconds for example so we we see it in the perth",
    "start": "1126559",
    "end": "1133840"
  },
  {
    "text": "dash and based on that we can still debug it and see it",
    "start": "1133840",
    "end": "1139600"
  },
  {
    "start": "1139000",
    "end": "1165000"
  },
  {
    "text": "um then of course we have graphana we are collecting like bunch of metrics i",
    "start": "1139919",
    "end": "1145200"
  },
  {
    "text": "actually recommend to to check it out and those dashboards are in our our",
    "start": "1145200",
    "end": "1150960"
  },
  {
    "text": "repository and you can use it for your own cluster if you want and there is like bunch of like really",
    "start": "1150960",
    "end": "1157039"
  },
  {
    "text": "cool graphs that allow you to to check like api server latency but scheduling and and all of that stuff",
    "start": "1157039",
    "end": "1165840"
  },
  {
    "start": "1165000",
    "end": "1198000"
  },
  {
    "text": "then we also have profiling so when you are running cluster loader two what happens",
    "start": "1166320",
    "end": "1171360"
  },
  {
    "text": "is that uh it's not only like gathering those uh prometheus metrics but also it's",
    "start": "1171360",
    "end": "1178799"
  },
  {
    "text": "getting the profiling from the api server for example and based on this profiling uh what we",
    "start": "1178799",
    "end": "1184960"
  },
  {
    "text": "usually do is we go through it and see what kind of parts of api server is actually",
    "start": "1184960",
    "end": "1190720"
  },
  {
    "text": "consuming most of the cpu or memory and based on that we can plan and iterate on",
    "start": "1190720",
    "end": "1196080"
  },
  {
    "text": "improvements to api server so now okay i talked a little bit about",
    "start": "1196080",
    "end": "1202480"
  },
  {
    "start": "1198000",
    "end": "1324000"
  },
  {
    "text": "our infrastructure what kind of tools we use for for scalability testing and now",
    "start": "1202480",
    "end": "1207919"
  },
  {
    "text": "we can go back to scalability test what kind of tests we actually do run and",
    "start": "1207919",
    "end": "1214559"
  },
  {
    "text": "what you can see in our test grid basically so we have periodic tests ci tests and we",
    "start": "1214559",
    "end": "1221840"
  },
  {
    "text": "split them into two categories one is release blocking and one is non-release blocking so release blocking tests",
    "start": "1221840",
    "end": "1229200"
  },
  {
    "text": "are the ones that you know there is new release of kubernetes and we see that",
    "start": "1229200",
    "end": "1234960"
  },
  {
    "text": "okay the scalability doesn't look great then we say okay we just need to stop the release of kubernetes and debug it",
    "start": "1234960",
    "end": "1241760"
  },
  {
    "text": "what happened basically and we have performance tests with 100 nodes performance tests with 5000 nodes",
    "start": "1241760",
    "end": "1249120"
  },
  {
    "text": "and correctness tests that make sure that regular features in kubernetes work at",
    "start": "1249120",
    "end": "1254640"
  },
  {
    "text": "scale as well except for that we have non-release blocking which are kind of like more",
    "start": "1254640",
    "end": "1260000"
  },
  {
    "text": "informative for us so cubemark is one of these examples that i talked about",
    "start": "1260000",
    "end": "1265679"
  },
  {
    "text": "but except for that we have storage gohank benchmarking different type of but one of my favorites is actually",
    "start": "1265679",
    "end": "1272480"
  },
  {
    "text": "going so with going we actually saw like multiple times that",
    "start": "1272480",
    "end": "1278559"
  },
  {
    "text": "you know going changed the compiler changed and it totally broke kubernetes so we have one dedicated test just for",
    "start": "1278559",
    "end": "1286159"
  },
  {
    "text": "just for testing compiler of the going which is basically running fixed version",
    "start": "1286159",
    "end": "1291440"
  },
  {
    "text": "of kubernetes and but we are just changing the calling compiler",
    "start": "1291440",
    "end": "1298000"
  },
  {
    "text": "and if you are a contributor then probably you saw our pre-submit we have uh 100 notes per submit test",
    "start": "1298000",
    "end": "1305679"
  },
  {
    "text": "that it's running for each pr that um someone is trying to merge to kubernetes master",
    "start": "1305679",
    "end": "1312000"
  },
  {
    "text": "and this is like early warning that something might be broken basically",
    "start": "1312000",
    "end": "1318480"
  },
  {
    "text": "and it protects basically kubernetes from like super super big regressions",
    "start": "1318480",
    "end": "1324640"
  },
  {
    "start": "1324000",
    "end": "1358000"
  },
  {
    "text": "so yeah now let's get to protecting scalability of kubernetes so",
    "start": "1325280",
    "end": "1330320"
  },
  {
    "text": "this is our that's great you can check it out it's uh it's in uh like you know like",
    "start": "1330320",
    "end": "1335600"
  },
  {
    "text": "basically the test screen that all the tests are there and you can find like six scalability test grid",
    "start": "1335600",
    "end": "1341840"
  },
  {
    "text": "um and we test also release branches old ones and",
    "start": "1341840",
    "end": "1347520"
  },
  {
    "text": "to make sure that you know if there are some cherry peaks to 122 123 then they",
    "start": "1347520",
    "end": "1352880"
  },
  {
    "text": "are also not breaking old versions of kubernetes",
    "start": "1352880",
    "end": "1358399"
  },
  {
    "start": "1358000",
    "end": "1409000"
  },
  {
    "text": "as whitehead mentioned before scalability is very sensitive and",
    "start": "1359520",
    "end": "1364880"
  },
  {
    "text": "what happens is that there are so many things that can break scalability of kubernetes",
    "start": "1364880",
    "end": "1369919"
  },
  {
    "text": "to name a few we saw as i mentioned before going that compiler changes",
    "start": "1369919",
    "end": "1375600"
  },
  {
    "text": "can break but also operating system controllers and",
    "start": "1375600",
    "end": "1380880"
  },
  {
    "text": "api machinery meaning like api server schedule scheduler at cd and cubot",
    "start": "1380880",
    "end": "1386480"
  },
  {
    "text": "basically everywhere so what we do is we try to",
    "start": "1386480",
    "end": "1391679"
  },
  {
    "text": "try out those issues debug them and once we know that okay the fault is because of",
    "start": "1391679",
    "end": "1396960"
  },
  {
    "text": "the change in scheduler or going what we do is we reach out to those six and try",
    "start": "1396960",
    "end": "1402080"
  },
  {
    "text": "to to help them fix those issues and sometimes we just fix them by",
    "start": "1402080",
    "end": "1407360"
  },
  {
    "text": "ourselves um so i will give you a few examples of",
    "start": "1407360",
    "end": "1412720"
  },
  {
    "start": "1409000",
    "end": "1485000"
  },
  {
    "text": "like really cool regressions that we recently debugged so one was spot startup latency and the idea was that",
    "start": "1412720",
    "end": "1420559"
  },
  {
    "text": "okay since like 120 we've been heavily investing in",
    "start": "1420559",
    "end": "1425679"
  },
  {
    "text": "priority and fairness and along the way there was one regression that significantly increased the pot startup",
    "start": "1425679",
    "end": "1432240"
  },
  {
    "text": "latency so we were the ones that were debugging and the root cause was actually quite simple so",
    "start": "1432240",
    "end": "1437760"
  },
  {
    "text": "um we started supporting watches in priority and fairness and the number of routines that one",
    "start": "1437760",
    "end": "1444880"
  },
  {
    "text": "watch was requiring was actually doubled so instead of like having one thousand",
    "start": "1444880",
    "end": "1450080"
  },
  {
    "text": "one hundred fifty thousand of routines we had three hundred thousand guardians and at this point scalability just",
    "start": "1450080",
    "end": "1456799"
  },
  {
    "text": "degraded for for pot startup latency there's also like api called latency",
    "start": "1456799",
    "end": "1462720"
  },
  {
    "text": "regression which was also connected to priority and fairness this actually was",
    "start": "1462720",
    "end": "1469120"
  },
  {
    "text": "this actually basically increased the api call latency for all the possible resources",
    "start": "1470559",
    "end": "1476159"
  },
  {
    "text": "and so we were also debugging it and fixing there is pretty cool debugging uh",
    "start": "1476159",
    "end": "1482080"
  },
  {
    "text": "in this issue that you can you can find on github so except for protecting from",
    "start": "1482080",
    "end": "1487760"
  },
  {
    "text": "scalability regressions we are also driving scalability improvements and recently",
    "start": "1487760",
    "end": "1493440"
  },
  {
    "text": "we were helping with the migration of going to 118 and then we also",
    "start": "1493440",
    "end": "1500400"
  },
  {
    "text": "help with implementing efficient watch resumption or immutable secrets like secrets are actually one of those things",
    "start": "1500400",
    "end": "1507440"
  },
  {
    "text": "in cluster that can put quite a lot of pressure on api servers so if you are deploying um your workloads and",
    "start": "1507440",
    "end": "1515679"
  },
  {
    "text": "um and let's say that you are using secrets then maybe you should consider using immutable secrets to just",
    "start": "1515679",
    "end": "1521360"
  },
  {
    "text": "make your kubernetes api reliable",
    "start": "1521360",
    "end": "1526799"
  },
  {
    "text": "more reliable and except for that we also work heavily on priority and fairness which further",
    "start": "1526799",
    "end": "1532559"
  },
  {
    "text": "increases the reliability of kubernetes",
    "start": "1532559",
    "end": "1537840"
  },
  {
    "start": "1537000",
    "end": "1585000"
  },
  {
    "text": "so if you want to get involved there are bunch of links you can attend",
    "start": "1538400",
    "end": "1543919"
  },
  {
    "text": "our public meetings they are on thursday 17 30.",
    "start": "1543919",
    "end": "1549360"
  },
  {
    "text": "and yeah join our slack channel if you have any questions or mailing lists you can you can always",
    "start": "1549360",
    "end": "1555760"
  },
  {
    "text": "reach out to us and we are happy to help if you are developing new features then we can help you with reviewing it or if",
    "start": "1555760",
    "end": "1563039"
  },
  {
    "text": "you have any issues then we can try to help you with debugging those issues",
    "start": "1563039",
    "end": "1568240"
  },
  {
    "text": "and if you want to get involved there are some issues that are marked as help wanted and basically getting started and",
    "start": "1568240",
    "end": "1576000"
  },
  {
    "text": "so if you are interested in our tooling then you can help us with developing tooling if you are interested in",
    "start": "1576000",
    "end": "1581360"
  },
  {
    "text": "regressions you can help us with debugging regressions so thank you and now it's time for q a",
    "start": "1581360",
    "end": "1589550"
  },
  {
    "start": "1585000",
    "end": "1595000"
  },
  {
    "text": "[Applause]",
    "start": "1589550",
    "end": "1597690"
  },
  {
    "text": "so nowadays there are sometimes more crts and contrasts in a cluster than than workload or anything",
    "start": "1607440",
    "end": "1613919"
  },
  {
    "text": "else so how do you what do you think about this and how do you consider it",
    "start": "1613919",
    "end": "1619440"
  },
  {
    "text": "yeah so um i think there are different aspects of that like well well-designed controllers",
    "start": "1619440",
    "end": "1625600"
  },
  {
    "text": "with cr like based on crds are not really a problem um i mean crds are a little bit more",
    "start": "1625600",
    "end": "1632000"
  },
  {
    "text": "expensive than like built-in apis because because of not using protobufs for",
    "start": "1632000",
    "end": "1638000"
  },
  {
    "text": "example and using json and so on but like that's not a core of the problem like we",
    "start": "1638000",
    "end": "1643200"
  },
  {
    "text": "it's fine like the biggest problems that we see are the like",
    "start": "1643200",
    "end": "1649039"
  },
  {
    "text": "node but note oriented controllers so that like the demo sets more or less that are like watching",
    "start": "1649039",
    "end": "1655200"
  },
  {
    "text": "or even worse listing in some cases like a bunch of state from many nodes so so",
    "start": "1655200",
    "end": "1660240"
  },
  {
    "text": "that is really what is causing the problem so i think that our main problem is not the fact that we",
    "start": "1660240",
    "end": "1666320"
  },
  {
    "text": "have crts and controllers it's the fact that like um still there are people that are",
    "start": "1666320",
    "end": "1673679"
  },
  {
    "text": "designing the controllers in like in an inefficient way i would say",
    "start": "1673679",
    "end": "1679120"
  },
  {
    "text": "um i think we we would like to get to like improve like the efficiency of protobufs in general like the the",
    "start": "1679120",
    "end": "1685840"
  },
  {
    "text": "discussions about supporting protobufs for crds were happening since",
    "start": "1685840",
    "end": "1691039"
  },
  {
    "text": "i don't know four years or something it's just like big thing with not",
    "start": "1691039",
    "end": "1697200"
  },
  {
    "text": "high enough priority at this point okay thanks",
    "start": "1697200",
    "end": "1703679"
  },
  {
    "start": "1704000",
    "end": "2004000"
  },
  {
    "text": "hi uh my question is um where are uh the trade-offs or focuses",
    "start": "1708640",
    "end": "1716720"
  },
  {
    "text": "in terms of improving uh scalability um",
    "start": "1716720",
    "end": "1722159"
  },
  {
    "text": "or or where are the red lines that you would not like to cross uh",
    "start": "1722159",
    "end": "1727679"
  },
  {
    "text": "more concretely for example have you considered rewriting some of the components",
    "start": "1727679",
    "end": "1734559"
  },
  {
    "text": "in another language or or switching out a very important important component",
    "start": "1734559",
    "end": "1740480"
  },
  {
    "text": "or deprecating a feature that impacts scalability a lot but",
    "start": "1740480",
    "end": "1746000"
  },
  {
    "text": "a lot of people rely on that yes so um good question so um",
    "start": "1746000",
    "end": "1753279"
  },
  {
    "text": "i think the first example that that you that you made for example rewriting components something that we really",
    "start": "1753279",
    "end": "1759440"
  },
  {
    "text": "would like to avoid i don't think we want to do that i mean especially in a different language i think we want to",
    "start": "1759440",
    "end": "1764720"
  },
  {
    "text": "like consistency um it's fairly important like for the project as a whole like for",
    "start": "1764720",
    "end": "1771840"
  },
  {
    "text": "us as a community and so on like we we don't want um to",
    "start": "1771840",
    "end": "1777760"
  },
  {
    "text": "diverge too much like between components so i think it's it's it's probably one of those lines that we don't want to",
    "start": "1777760",
    "end": "1783440"
  },
  {
    "text": "cross um redesigning individual components",
    "start": "1783440",
    "end": "1789600"
  },
  {
    "text": "i would treat it as a last resort i but i wouldn't exclude it like for example networking",
    "start": "1791679",
    "end": "1798720"
  },
  {
    "text": "sig network is actually considering or they are they are thinking and there",
    "start": "1798720",
    "end": "1803919"
  },
  {
    "text": "there's even kept that it's like um not yet approved but like active on like how to redesign cube proxy to make",
    "start": "1803919",
    "end": "1810799"
  },
  {
    "text": "it a little bit more efficient so like those kind of things is something that we um definitely consider but like it",
    "start": "1810799",
    "end": "1818399"
  },
  {
    "text": "should go into like individual six like the um the requirements should be coming",
    "start": "1818399",
    "end": "1823840"
  },
  {
    "text": "from us but like um it shouldn't be us doing the work it should be that sick driving the work um to ride that",
    "start": "1823840",
    "end": "1830640"
  },
  {
    "text": "controller and uh and us may like like in coordination with us and like us helping",
    "start": "1830640",
    "end": "1837919"
  },
  {
    "text": "with that but like driven by them more or less i would say um",
    "start": "1837919",
    "end": "1843440"
  },
  {
    "text": "sorry i forgot the last part of your question i think there was something more at the end the last part was about deprecating",
    "start": "1843440",
    "end": "1851200"
  },
  {
    "text": "features oh yeah tom i think",
    "start": "1851200",
    "end": "1855519"
  },
  {
    "text": "the goal is to actually uncover the regressions uh or uncover the",
    "start": "1856399",
    "end": "1861760"
  },
  {
    "text": "unscalable things like as early as possible so like in general like purely",
    "start": "1861760",
    "end": "1866799"
  },
  {
    "text": "from scalability like wearing my scalability hat like yes i would like there are some things",
    "start": "1866799",
    "end": "1871840"
  },
  {
    "text": "that i would like to deprecate and i would like to get rid um wearing more my like production readiness hud and like",
    "start": "1871840",
    "end": "1878480"
  },
  {
    "text": "like all those like higher level things um we really don't want to do that like we",
    "start": "1878480",
    "end": "1883840"
  },
  {
    "text": "really don't want to break users so um what we are trying to do is we are",
    "start": "1883840",
    "end": "1889919"
  },
  {
    "text": "trying to introduce a different way of doing stuff",
    "start": "1889919",
    "end": "1895279"
  },
  {
    "text": "or the more efficient way of doing stuff for example like um podante affinity is one of those",
    "start": "1895279",
    "end": "1901200"
  },
  {
    "text": "features like of one of those like scheduler features that doesn't really scale well and it's causing a lot of troubles for",
    "start": "1901200",
    "end": "1907279"
  },
  {
    "text": "scheduler we introduced like the feature that is called how it's called",
    "start": "1907279",
    "end": "1913200"
  },
  {
    "text": "bot topology spread if i remember correctly that is kind of doing a very similar job or like for majority of",
    "start": "1913200",
    "end": "1919360"
  },
  {
    "text": "cases um so we are trying to steer people towards like more scalable",
    "start": "1919360",
    "end": "1926320"
  },
  {
    "text": "regressions by not by disallowing them to run to use the old",
    "start": "1926320",
    "end": "1932799"
  },
  {
    "text": "ones but rather about like giving them a carrot to to use those new ones or you are a",
    "start": "1932799",
    "end": "1938159"
  },
  {
    "text": "better one or whatever you call them there there",
    "start": "1938159",
    "end": "1943360"
  },
  {
    "text": "are exceptions i think we recently deprecated like the selfling field",
    "start": "1943360",
    "end": "1949519"
  },
  {
    "text": "um that was part of like like every object used to have like a self wing",
    "start": "1949519",
    "end": "1955200"
  },
  {
    "text": "that was basically like a self link right um",
    "start": "1955200",
    "end": "1960640"
  },
  {
    "text": "and we we actually deprecated it it's no longer set anymore um but",
    "start": "1960720",
    "end": "1966799"
  },
  {
    "text": "that was done based on like significant research and and the outcome that no one",
    "start": "1966799",
    "end": "1972480"
  },
  {
    "text": "is really using it for anything useful that and of like all the cases where that we are aware and many different",
    "start": "1972480",
    "end": "1978960"
  },
  {
    "text": "repos can easily be replaced with already existing stuff and that are not based on",
    "start": "1978960",
    "end": "1985200"
  },
  {
    "text": "stealthing so um that's the only example that i remember",
    "start": "1985200",
    "end": "1990640"
  },
  {
    "text": "in the last like two three years where we did that um and we generally would really like to",
    "start": "1990640",
    "end": "1997039"
  },
  {
    "text": "avoid it as a project",
    "start": "1997039",
    "end": "2000320"
  },
  {
    "text": "okay oh sorry sorry i i have two questions um firstly",
    "start": "2003200",
    "end": "2008960"
  },
  {
    "start": "2004000",
    "end": "2094000"
  },
  {
    "text": "i think uh way at the start you mentioned you have some documented",
    "start": "2008960",
    "end": "2014080"
  },
  {
    "text": "design principles for other maintainers so",
    "start": "2014080",
    "end": "2019760"
  },
  {
    "text": "uh or maybe i misunderstood so if if as a uh as a component maintainer in in",
    "start": "2019760",
    "end": "2027440"
  },
  {
    "text": "kubernetes i i want to go and read the the you know the cliff notes uh what",
    "start": "2027440",
    "end": "2033919"
  },
  {
    "text": "what should i be thinking about um when making design changes to my component do you do",
    "start": "2033919",
    "end": "2041039"
  },
  {
    "text": "you have a general principles we are document it a little bit like",
    "start": "2041039",
    "end": "2046880"
  },
  {
    "text": "it's there are some pieces some small pieces here and there written but i think",
    "start": "2046880",
    "end": "2052158"
  },
  {
    "text": "um one of the things which is we are missing like this go to page that you can go and like read those are seven",
    "start": "2052159",
    "end": "2059200"
  },
  {
    "text": "things that you should first think about it and if you are interested then in more detail let's shot but um",
    "start": "2059200",
    "end": "2066240"
  },
  {
    "text": "we are actually missing that really yeah but also like if you are uh",
    "start": "2066240",
    "end": "2071520"
  },
  {
    "text": "developing uh some component then i would recommend like just adding it to",
    "start": "2071520",
    "end": "2076638"
  },
  {
    "text": "to our load test so it will be just covered by our regular ci tests which can also be",
    "start": "2076639",
    "end": "2082560"
  },
  {
    "text": "helpful for you if it's possible of course because it's not not always possible i maintain",
    "start": "2082560",
    "end": "2089440"
  },
  {
    "text": "cloud components so um yeah normally not but um [Music]",
    "start": "2089440",
    "end": "2096079"
  },
  {
    "start": "2094000",
    "end": "2264000"
  },
  {
    "text": "okay um so my other one was was kind of related to the previous question",
    "start": "2096079",
    "end": "2101520"
  },
  {
    "text": "so i was kind of interested if you could you described",
    "start": "2101520",
    "end": "2107119"
  },
  {
    "text": "diagnosing a pod latency issue and the root cause was i think you said",
    "start": "2107119",
    "end": "2115119"
  },
  {
    "text": "a change had doubled the number of go routines yeah yeah yeah so um",
    "start": "2115119",
    "end": "2120320"
  },
  {
    "text": "presumably somebody had a reason for doubling the number of go routines this was just like you know programming back",
    "start": "2120320",
    "end": "2126400"
  },
  {
    "text": "basically that was introduced okay you know and uh still like the slo was fine right so we didn't see it at first but",
    "start": "2126400",
    "end": "2133200"
  },
  {
    "text": "using the perv dash where we can see like the changes over the time we were able to spot that you know 99th",
    "start": "2133200",
    "end": "2139520"
  },
  {
    "text": "percentile of pots turtle latency increase from like three seconds to four seconds and based on that we were able",
    "start": "2139520",
    "end": "2145520"
  },
  {
    "text": "to kind of like you know see that okay it was within this time that the start of latency increased then we went",
    "start": "2145520",
    "end": "2152320"
  },
  {
    "text": "through a few pr's and we picked this one that it's most likely this one and",
    "start": "2152320",
    "end": "2158400"
  },
  {
    "text": "then we work with wojtek to fix it yes maybe to tweak the answer i wouldn't",
    "start": "2158400",
    "end": "2163520"
  },
  {
    "text": "because i'm the one responsible for the progression",
    "start": "2163520",
    "end": "2167680"
  },
  {
    "text": "i wouldn't say it was a buck it was basically a thing like how to like when you are like implementing a",
    "start": "2168720",
    "end": "2174880"
  },
  {
    "text": "feature or designing a feature you want to get to like to production as soon as possible so um",
    "start": "2174880",
    "end": "2183680"
  },
  {
    "text": "there are like there are some shortcuts that you are sometimes kind of like taking and it it was like kind of like",
    "start": "2183680",
    "end": "2189760"
  },
  {
    "text": "unaware shortcut that like we like the tests were passing so we we didn't really pay that attention",
    "start": "2189760",
    "end": "2196880"
  },
  {
    "text": "um but but yeah it's like it was basically um",
    "start": "2196880",
    "end": "2202000"
  },
  {
    "text": "don't over optimize if it's not really not really needed it appeared that it's actually needed so so we like fixed that",
    "start": "2202000",
    "end": "2209280"
  },
  {
    "text": "but yeah okay so yeah you didn't you didn't have",
    "start": "2209280",
    "end": "2214960"
  },
  {
    "text": "to ask somebody yourself in this case to uh to remove a feature um",
    "start": "2214960",
    "end": "2220079"
  },
  {
    "text": "uh or or redesign something yeah unfortunately it was relatively",
    "start": "2220079",
    "end": "2226240"
  },
  {
    "text": "small change so it's it wasn't like really like conceptually like",
    "start": "2226240",
    "end": "2231839"
  },
  {
    "text": "we are paying like we are trying to pay as much attention as possible to the design itself and once the design looks",
    "start": "2231839",
    "end": "2239599"
  },
  {
    "text": "scalable we should be pretty convinced that like the implementation is possible to do in",
    "start": "2239599",
    "end": "2245040"
  },
  {
    "text": "scalable way and like even if the first attempt is not maybe scalable then you",
    "start": "2245040",
    "end": "2250240"
  },
  {
    "text": "you can re-implement it somehow or pieces of that um and um keep the same semantics which is",
    "start": "2250240",
    "end": "2258240"
  },
  {
    "text": "which is the car right off of it",
    "start": "2258240",
    "end": "2262400"
  },
  {
    "text": "um hey so you mentioned about the 5k nodes number",
    "start": "2263280",
    "end": "2269920"
  },
  {
    "text": "and basically the edge scenarios are pushing that number quite to the edge and i'm thinking about",
    "start": "2269920",
    "end": "2277520"
  },
  {
    "text": "what what is your approach about how to tackling the increasement of eventually nodes that's",
    "start": "2277520",
    "end": "2284880"
  },
  {
    "text": "running in higher latency environments",
    "start": "2284880",
    "end": "2291040"
  },
  {
    "text": "um i think we've never really focused on like the high latency environments to be honest",
    "start": "2291040",
    "end": "2298400"
  },
  {
    "text": "um in terms of like increasing the number of nodes like uh gke which like occurs like google",
    "start": "2298400",
    "end": "2305440"
  },
  {
    "text": "managed kubernetes we already support like 15 000 nodes so it's and it's built on top of kubernetes we need to do a",
    "start": "2305440",
    "end": "2312000"
  },
  {
    "text": "bunch of like other things around to make make that really work and that was like a lot of work but",
    "start": "2312000",
    "end": "2318480"
  },
  {
    "text": "um the building blocks from kubernetes in open source kubernetes are there so um",
    "start": "2318480",
    "end": "2325520"
  },
  {
    "text": "it's not that it's impossible to get there um if we didn't see enough use cases like or",
    "start": "2325520",
    "end": "2333040"
  },
  {
    "text": "people in in open source community asking for more um so we didn't we",
    "start": "2333040",
    "end": "2338079"
  },
  {
    "text": "didn't try to do that in open source because there was like a bunch of other things to do in open source to to make",
    "start": "2338079",
    "end": "2344560"
  },
  {
    "text": "that work but um but it's certainly possible if if we will start hearing those requests um",
    "start": "2344560",
    "end": "2351680"
  },
  {
    "text": "but yeah as i mentioned that we've never really were looking into like um edge cases or like the high latency or",
    "start": "2351680",
    "end": "2358480"
  },
  {
    "text": "isolated things or things like that so um there is some work happening in that",
    "start": "2358480",
    "end": "2364640"
  },
  {
    "text": "area not in the context of scalability but i'm not super familiar with that to",
    "start": "2364640",
    "end": "2369680"
  },
  {
    "text": "be honest okay thank you",
    "start": "2369680",
    "end": "2374000"
  },
  {
    "text": "okay any any last questions oh there is your head",
    "start": "2378320",
    "end": "2383838"
  },
  {
    "start": "2384000",
    "end": "2561000"
  },
  {
    "text": "yeah um what do you think the biggest constraints on scalability are going forward with kubernetes like if you wanted to do 50 000 nodes what are the",
    "start": "2384400",
    "end": "2391200"
  },
  {
    "text": "biggest challenges you think you'd face i think there are",
    "start": "2391200",
    "end": "2397040"
  },
  {
    "text": "many different steps many different things depending on like where exactly you would like to to go so i can imagine",
    "start": "2397040",
    "end": "2403200"
  },
  {
    "text": "like going to 50 000 notes with not much effort if you for example",
    "start": "2403200",
    "end": "2410160"
  },
  {
    "text": "run just like simple batch pot per node workloads without using any sophisticated and working storage or",
    "start": "2410160",
    "end": "2416880"
  },
  {
    "text": "anything like that that shouldn't be that hard um especially if like a low term cluster",
    "start": "2416880",
    "end": "2423359"
  },
  {
    "text": "and so on so it really depends like um",
    "start": "2423359",
    "end": "2428160"
  },
  {
    "text": "what are the what are the dimensions that you would like to you would like to",
    "start": "2429040",
    "end": "2434160"
  },
  {
    "text": "to stress more like usually networking stack is something that is like most stressing",
    "start": "2434160",
    "end": "2440880"
  },
  {
    "text": "the control plane because there are um most of other things are basically",
    "start": "2440880",
    "end": "2447520"
  },
  {
    "text": "um are basically okay maybe taking a step back so",
    "start": "2447520",
    "end": "2454640"
  },
  {
    "text": "we should split the the problems on like control plane components which which there are like very few of them",
    "start": "2454640",
    "end": "2461440"
  },
  {
    "text": "or i mean very few per cluster not very few components but very few per class like",
    "start": "2461440",
    "end": "2466560"
  },
  {
    "text": "very few instances of a single component per cluster and those that are running on nodes um",
    "start": "2466560",
    "end": "2472560"
  },
  {
    "text": "and those that are running on nodes are usually like contributing like the highest number of like load on the",
    "start": "2472560",
    "end": "2478960"
  },
  {
    "text": "control plane um and from those like the networking ones are usually the most",
    "start": "2478960",
    "end": "2485040"
  },
  {
    "text": "um the most stressing so so this is this is something that we definitely need to",
    "start": "2485040",
    "end": "2491520"
  },
  {
    "text": "um we need to come up with something if we really need to go go visibly higher than that um in terms",
    "start": "2491520",
    "end": "2499040"
  },
  {
    "text": "of individual components it's usually boils down to um",
    "start": "2499040",
    "end": "2505280"
  },
  {
    "text": "to throughput or things like that like on the individual components which",
    "start": "2505440",
    "end": "2511680"
  },
  {
    "text": "which is kind of solvable at the per component level so i would say",
    "start": "2511920",
    "end": "2518560"
  },
  {
    "text": "being able to horizontally scale server is something that is that is fairly critical and that we may want to",
    "start": "2518560",
    "end": "2525760"
  },
  {
    "text": "invest a little bit into that more um the storage layers of the lcd itself",
    "start": "2525760",
    "end": "2530800"
  },
  {
    "text": "is is potential bottlenecks and then like",
    "start": "2530800",
    "end": "2535839"
  },
  {
    "text": "the components themselves will or can drive like improvements if we",
    "start": "2535839",
    "end": "2541440"
  },
  {
    "text": "really need to go higher which is in dimension related to that component which is usually will be throughput or",
    "start": "2541440",
    "end": "2549040"
  },
  {
    "text": "the time to process all all the objects of a given type or something",
    "start": "2549040",
    "end": "2555599"
  },
  {
    "text": "okay thank you very much for coming",
    "start": "2558960",
    "end": "2563240"
  }
]