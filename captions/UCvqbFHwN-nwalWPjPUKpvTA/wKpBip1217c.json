[
  {
    "text": "uh good morning everybody thanks for showing up today this is uh computational fluid dynamics kubernetes",
    "start": "120",
    "end": "5920"
  },
  {
    "text": "uh Cube flow open philam um before we get started I just wanted to thank two folks uh I don't think either of them",
    "start": "5920",
    "end": "11400"
  },
  {
    "text": "are in the room today Eduardo orango and Aldo go kodor um they were super helpful",
    "start": "11400",
    "end": "17800"
  },
  {
    "text": "when I was going through this sort of process of figuring all this stuff out um Eduardo's actually not at Red Hat",
    "start": "17800",
    "end": "23640"
  },
  {
    "text": "anymore he moved over to Nvidia so um either way still helpful or thankful for his help um",
    "start": "23640",
    "end": "30920"
  },
  {
    "text": "this is not a super serious presentation it's Friday we've been here all week it's you know all tired um it's going to",
    "start": "30920",
    "end": "36239"
  },
  {
    "text": "be fun I promise I hope uh if you want serious there's a blog post that I wrote that details all the Gory GitHub repos",
    "start": "36239",
    "end": "43160"
  },
  {
    "text": "and all this other stuff this presentation's on the sked website you don't have to worry we can go back to this whatever um so if you don't get a",
    "start": "43160",
    "end": "49559"
  },
  {
    "text": "picture of this it's fine um and like I said this is going to be fun full bad jokes terrible memes all that kind of",
    "start": "49559",
    "end": "55680"
  },
  {
    "text": "stuff so I'm getting at least one round of applause so we're good okay here we go this presentation's built like building",
    "start": "55680",
    "end": "62039"
  },
  {
    "text": "blocks right we're just going to try to put things together get deeper deeper deeper as we go to explain how I",
    "start": "62039",
    "end": "67119"
  },
  {
    "text": "actually was able to achieve this because in reality it was mostly just kind of an experiment um before we get",
    "start": "67119",
    "end": "72479"
  },
  {
    "text": "started just a couple questions if you're kubernetes user raise your hand real quick everybody okay that's not",
    "start": "72479",
    "end": "77720"
  },
  {
    "text": "totally a surprise who's doing HPC not necessarily on kubernetes just anything that would qualifies as HPC only a few",
    "start": "77720",
    "end": "84280"
  },
  {
    "text": "anybody doing MPI stuff about the same number and then lastly open foam",
    "start": "84280",
    "end": "89320"
  },
  {
    "text": "existing one one okay cool uh how did we get here so me uh I've been a red hat",
    "start": "89320",
    "end": "94479"
  },
  {
    "text": "for 14 and 1 12 years I'm not a software engineer I'm not a kubernetes cube flow open foam developer um I'm not an",
    "start": "94479",
    "end": "101360"
  },
  {
    "text": "aerodynamicist but I do have a race car and it does have a wing on it so uh I",
    "start": "101360",
    "end": "107439"
  },
  {
    "text": "ended up getting this email from an account team out of APAC and they were like oh this automaker is doing open",
    "start": "107439",
    "end": "112960"
  },
  {
    "text": "foam in a container they want to do it in VMS and I was like can we do this on",
    "start": "112960",
    "end": "118000"
  },
  {
    "text": "kubernetes and I remembered I have these friends who do aerodynamic stuff and they offered me some of their um uh",
    "start": "118000",
    "end": "124479"
  },
  {
    "text": "models to play with and I was like cool okay let's let's let's do an experiment right let me see if I can run their",
    "start": "124479",
    "end": "131120"
  },
  {
    "text": "fluid dynamics Wing job in kubernetes environment so review open foam and MPI",
    "start": "131120",
    "end": "137879"
  },
  {
    "text": "what is MPI it does parallel or distributed computing so we have lots of things that we do then lots of little",
    "start": "137879",
    "end": "144400"
  },
  {
    "text": "things in parallel open foam is for computational fluid dynamics air is a fluid water as a fluid it can analyze",
    "start": "144400",
    "end": "152440"
  },
  {
    "text": "how it flows around objects so we have this big job we want to break it into little jobs and that's where MPI come",
    "start": "152440",
    "end": "159640"
  },
  {
    "text": "well but not really right MPI doesn't do the breaking up it just does the processing open foam is a tool well it's",
    "start": "159640",
    "end": "166840"
  },
  {
    "text": "actually lots of tools to help do fluid Dynamic stuff water is a fluid so we take our",
    "start": "166840",
    "end": "174280"
  },
  {
    "text": "big fluid dynamics thing and we break it up into little pieces with open foam and then we use MPI to do all the things uh",
    "start": "174280",
    "end": "181800"
  },
  {
    "text": "those things are actually really like individual servers for most folks they're used to HBC and clusters and Etc",
    "start": "181800",
    "end": "187680"
  },
  {
    "text": "um and it does this over SSH right so MPI causes finds all these systems with SSH goes into them does the",
    "start": "187680",
    "end": "194400"
  },
  {
    "text": "things we need some kind of shared storage because all the little pieces need to find the work that they have to",
    "start": "194400",
    "end": "200280"
  },
  {
    "text": "do so to speak right so we got to figure out figure that out as well okay it's not really that simple",
    "start": "200280",
    "end": "207799"
  },
  {
    "text": "but for the sake of Simplicity we're going to draw this picture open foam NPI lots of stuff rwx cool clusters and",
    "start": "207799",
    "end": "215560"
  },
  {
    "text": "kubernetes so how many of you have something that looks like this right you got a rack of servers all stuff okay",
    "start": "215560",
    "end": "221040"
  },
  {
    "text": "cool so 2014 this cool kubernetes thing happened and then people started trying",
    "start": "221040",
    "end": "226159"
  },
  {
    "text": "to do this right we want to run all the workloads in kubernetes so this is a",
    "start": "226159",
    "end": "231280"
  },
  {
    "text": "workload can we run it in kubernetes let's see what we can do starting to draw parallels right so open foam MPI",
    "start": "231280",
    "end": "238439"
  },
  {
    "text": "fluid dynamics job we've got kubernetes kubernetes runs pods if you remember the previous picture we actually have a",
    "start": "238439",
    "end": "244680"
  },
  {
    "text": "bunch of little things that we're doing fluid dynamics that's a lot of",
    "start": "244680",
    "end": "250239"
  },
  {
    "text": "PODS we have SSH we have rwx storage we have kubernetes we have rwx storage",
    "start": "250239",
    "end": "256720"
  },
  {
    "text": "we'll figure out the rest um I'm not really sure but we we'll get there right",
    "start": "256720",
    "end": "262560"
  },
  {
    "text": "so let's go deeper Cube flow how many of you know what Cube flow is ever heard of it oh",
    "start": "262560",
    "end": "267639"
  },
  {
    "text": "wow lots of folks Okay cool so Cube flow project dedicated to making deployments of machine learn well we're not doing",
    "start": "267639",
    "end": "273400"
  },
  {
    "text": "machine learning we're doing fluid dynamics right but it has this operator",
    "start": "273400",
    "end": "278759"
  },
  {
    "text": "called the MPI operator now it's says it's for all reduced distributed training it's just NPI like we're doing",
    "start": "278759",
    "end": "285199"
  },
  {
    "text": "an NPI thing let's do an NPI thing re one the thing so let's go deeper uh operators",
    "start": "285199",
    "end": "292800"
  },
  {
    "text": "how many of you familiar kubernetes operators most everybody in the room okay operators are this it's too complicated so operators are just a pod",
    "start": "292800",
    "end": "300000"
  },
  {
    "text": "right there is a controller pod that looks at instances of a custom resource",
    "start": "300000",
    "end": "305840"
  },
  {
    "text": "custom resource definition extends the kubernetes API and then it does stuff in",
    "start": "305840",
    "end": "311400"
  },
  {
    "text": "this case we're talking about the MPI operator the stuff that it does is MPI stuff so we've got MPI operator we have",
    "start": "311400",
    "end": "318919"
  },
  {
    "text": "this MPI job I'm running a wing cfd job it's going to happen in a lot of PODS",
    "start": "318919",
    "end": "325440"
  },
  {
    "text": "okay I I don't know so lots of pods kubernetes rwx all the",
    "start": "325440",
    "end": "331240"
  },
  {
    "text": "things I used open shift that happens to be the red hat kubernetes we have a storage product uh well we moving to IBM",
    "start": "331240",
    "end": "338360"
  },
  {
    "text": "but we have a storage product open data Foundation that gives me rwx and then we had to do stuff in the Middle with Cube",
    "start": "338360",
    "end": "344000"
  },
  {
    "text": "flow so let's go deeper again Wing cfd MPI operator when you use the MPI",
    "start": "344000",
    "end": "350240"
  },
  {
    "text": "operator and you instantiate an MPI job object it fires up a launcher pod that",
    "start": "350240",
    "end": "356199"
  },
  {
    "text": "launcher pod then runs MPI into all all the job pods that do the work they talk",
    "start": "356199",
    "end": "363520"
  },
  {
    "text": "to storage if you have storage defined um and all this happens on top of your kubernetes environment okay still does",
    "start": "363520",
    "end": "370919"
  },
  {
    "text": "this over SSH though it's like that's gross and I had to do some nasty tweaks that are in the blog post to make it",
    "start": "370919",
    "end": "376039"
  },
  {
    "text": "work on open shift but whatever we got it working it's fine okay here we go so",
    "start": "376039",
    "end": "381080"
  },
  {
    "text": "what is the open foam process so uh the first step is this meshing process you take a 3D object you break it up into",
    "start": "381080",
    "end": "387520"
  },
  {
    "text": "like a meshy thing okay cool then you decompose this into lots of small jobs",
    "start": "387520",
    "end": "393080"
  },
  {
    "text": "right that was the thing we talked about early on uh then you do the problem solving",
    "start": "393080",
    "end": "398280"
  },
  {
    "text": "and figure out what the fluid job is and then you put it all back together relatively trivial so I'm",
    "start": "398280",
    "end": "406440"
  },
  {
    "text": "really good at this and fortunately the uh open foam project provides a bunch of",
    "start": "406440",
    "end": "413759"
  },
  {
    "text": "examples for how to do stuff and I also had these guys to give me things to do I actually helped Morland build their open",
    "start": "413759",
    "end": "421800"
  },
  {
    "text": "foam muunu cluster like they in Atlanta I'm in Atlanta I went over there and had lunch one day it was fun um okay",
    "start": "421800",
    "end": "429000"
  },
  {
    "text": "so Dam break is the example job that's provided by the open foam project it's a",
    "start": "429000",
    "end": "435120"
  },
  {
    "text": "very simple like water flowing over a thing uh let's try to run that job and see if we can make it work so what do we",
    "start": "435120",
    "end": "441479"
  },
  {
    "text": "actually have to put in here right this is where the the rubber finally meets the road uh trigger warning there's",
    "start": "441479",
    "end": "446560"
  },
  {
    "text": "actually code it's really is not code it's barely code so this is the the dam break",
    "start": "446560",
    "end": "452879"
  },
  {
    "text": "example so you you do some stuff you break it up you run the parallel job that's the meat and then you put it back",
    "start": "452879",
    "end": "458960"
  },
  {
    "text": "together so how do we get this how do I get that script into this launcher pod",
    "start": "458960",
    "end": "464520"
  },
  {
    "text": "right well fortunately I actually know a thing at least maybe about kubernetes we have this thing in kubernetes called a",
    "start": "464520",
    "end": "469960"
  },
  {
    "text": "config map it makes it easy to inject files into the containers well that's just a script why can't I just inject",
    "start": "469960",
    "end": "477319"
  },
  {
    "text": "that as a file into the container and then make the MPI job run the whole script as the entry point to the work so",
    "start": "477319",
    "end": "486360"
  },
  {
    "text": "that's what I did I put it in a config map that I defined against the dam break job that went into the launcher pod that",
    "start": "486360",
    "end": "492319"
  },
  {
    "text": "launched the wing cfd pods that talked to the storage on the container platform okay cool and then yes it did work this",
    "start": "492319",
    "end": "498680"
  },
  {
    "text": "is what it looks like it was doing things the first one is the dam break job the second one is motorbike which is",
    "start": "498680",
    "end": "504199"
  },
  {
    "text": "another example within this is the open shift user interface that just shows like hey they're using CPUs pods stuff",
    "start": "504199",
    "end": "510960"
  },
  {
    "text": "that's the log output from open foam for the one person who's using open foam they've seen that a lot they spend a lot",
    "start": "510960",
    "end": "516440"
  },
  {
    "text": "of time looking at that text this is the Morin job um this is not really a",
    "start": "516440",
    "end": "521680"
  },
  {
    "text": "picture that I generated it's one they sent me but that's what I did I ran this job and it worked well sort of um",
    "start": "521680",
    "end": "529040"
  },
  {
    "text": "remember this picture I I don't have one of these I got this I wish that was me that's not my",
    "start": "529040",
    "end": "536160"
  },
  {
    "text": "laptop it's not my background it's that'd be cool place to work every day I guess or just really humid and M mosquitoes",
    "start": "536160",
    "end": "543079"
  },
  {
    "text": "anyway so I didn't have one of these I did have one of these I really I had",
    "start": "543079",
    "end": "548160"
  },
  {
    "text": "that right so uh fortunately I didn't spend a lot but I did have this so again I just kind",
    "start": "548160",
    "end": "555360"
  },
  {
    "text": "of was messing around uh I used m5a forx large which are not compute optimized",
    "start": "555360",
    "end": "562079"
  },
  {
    "text": "instances uh I was like all right well let me do two CPUs for pods I'll just do a boatload of PODS because more is",
    "start": "562079",
    "end": "568480"
  },
  {
    "text": "always better right um 512 CPUs it took 5 hours and Morland was like man that's really",
    "start": "568480",
    "end": "574959"
  },
  {
    "text": "slow and so the reality is you have to know what you're doing right tuning tuning tuning tuning is important um I'm",
    "start": "574959",
    "end": "581800"
  },
  {
    "text": "not really sure you know I I could have used the compute optimize instance type I was probably uh I probably should use",
    "start": "581800",
    "end": "588720"
  },
  {
    "text": "bigger pods that are using like the whole server instead of like lots of tiny ones um maybe I was overs slicing",
    "start": "588720",
    "end": "596200"
  },
  {
    "text": "the job you know when you when you have the surface and break it up into lots of little pieces you end up with overlap",
    "start": "596200",
    "end": "602320"
  },
  {
    "text": "and then you're basically just reprocessing the same things over and over that get edited out at the end when",
    "start": "602320",
    "end": "607920"
  },
  {
    "text": "you put it all back together or just know what you're doing right actually be an Aeron anist and know what any of",
    "start": "607920",
    "end": "613920"
  },
  {
    "text": "these things mean um where can we go from here right so PMX is a new thing",
    "start": "613920",
    "end": "622399"
  },
  {
    "text": "around open MPI that gets rid of SSH there's like an actual server Damon kind",
    "start": "622399",
    "end": "627640"
  },
  {
    "text": "of deal um it can run on kuber so it would be great if we didn't have to legitimately like run an SSH Damon in",
    "start": "627640",
    "end": "633959"
  },
  {
    "text": "all the pods and then like SSH via MPI from the launcher into this is just like",
    "start": "633959",
    "end": "639560"
  },
  {
    "text": "gross um of course more tuning um Nvidia is working on doing um GPU enablement",
    "start": "639560",
    "end": "647079"
  },
  {
    "text": "for this particular project for open foam so that you can use um accelerated uh processing via",
    "start": "647079",
    "end": "653760"
  },
  {
    "text": "gpus um but ultimately like what what was the point of this whole exercise right it really was an experiment can",
    "start": "653760",
    "end": "660040"
  },
  {
    "text": "can I even do this and so the reality is that I wasn't trying to test if AWS was a good place to do this because the",
    "start": "660040",
    "end": "665639"
  },
  {
    "text": "reality is AWS really isn't a good place to do HPC your data is probably not there so moving all your data into there",
    "start": "665639",
    "end": "672519"
  },
  {
    "text": "that's going to be ludicrously expensive um and it's just expensive to do your",
    "start": "672519",
    "end": "677680"
  },
  {
    "text": "instance types might not be available if you need a boatload of them like there's all kinds of problems with this so most of you if you're doing HBC you probably",
    "start": "677680",
    "end": "684600"
  },
  {
    "text": "have clusters on site that big picture of all this like you can do this right it actually did work we were able to run",
    "start": "684600",
    "end": "691160"
  },
  {
    "text": "open foam and and more generally you know whatever MPI type job you have we",
    "start": "691160",
    "end": "696639"
  },
  {
    "text": "were able to do it um one of the important things to think about though is that for those of you that are raised",
    "start": "696639",
    "end": "702399"
  },
  {
    "text": "your hand that you are doing HPC you know kubernetes doesn't think like most",
    "start": "702399",
    "end": "707440"
  },
  {
    "text": "a HPC schedulers it is auler I would like you to run my job it will do it it",
    "start": "707440",
    "end": "713560"
  },
  {
    "text": "has no concept of priority preemption queuing like that kind of stuff like traditional slurmy whatever your HBC",
    "start": "713560",
    "end": "720760"
  },
  {
    "text": "tool of choice is it's not doing that today but there's a whole Army of extensions Anders and other things that",
    "start": "720760",
    "end": "726480"
  },
  {
    "text": "are coming about that are doing those things red hat is investigating how we can do some of them we've actually got",
    "start": "726480",
    "end": "731880"
  },
  {
    "text": "the product manager for that in the room right now um but ultimately the the the",
    "start": "731880",
    "end": "737880"
  },
  {
    "text": "purpose of the job the purpose of the experiment was like can we do this yes we could okay great um that's it that",
    "start": "737880",
    "end": "744519"
  },
  {
    "text": "was really fast wow that was like barely 12 minutes",
    "start": "744519",
    "end": "750199"
  },
  {
    "text": "um so yeah H happy to answer questions I realize again this was supposed to just be kind of a fun happy one um but yeah",
    "start": "750440",
    "end": "759800"
  },
  {
    "text": "that's all I got for",
    "start": "759800",
    "end": "762519"
  },
  {
    "text": "you no questions oh okay we got a few hold on we got a",
    "start": "765399",
    "end": "773320"
  },
  {
    "text": "microphone uh great talk um so where where are you going to go from here now like is is this project done was this",
    "start": "777120",
    "end": "783040"
  },
  {
    "text": "this kind of like yeah so um it's good thanks for asking that um I I'm talking",
    "start": "783040",
    "end": "788440"
  },
  {
    "text": "to the Nvidia folks about it they're they're interested in figuring out how we can do that um I've I've done Nvidia",
    "start": "788440",
    "end": "793600"
  },
  {
    "text": "GPU things on AWS before on open shift and kubernetes We Know It Works um so it's just a matter of how do we do that",
    "start": "793600",
    "end": "800199"
  },
  {
    "text": "with open foam the other thing is um uh where's the script so like these",
    "start": "800199",
    "end": "808639"
  },
  {
    "text": "steps it I was thinking about it yesterday actually it doesn't really make sense to do all these steps in the launcher",
    "start": "808639",
    "end": "815040"
  },
  {
    "text": "pod maybe would make sense to like do the pre-processing and and decomposition",
    "start": "815040",
    "end": "820160"
  },
  {
    "text": "in a tecton pipeline and then take that and then attach that as a storage object into the launcher podt and then just run",
    "start": "820160",
    "end": "827120"
  },
  {
    "text": "the MPI thing and then when that job finishes do another tecton pipeline to run reconstruct like does that make",
    "start": "827120",
    "end": "833560"
  },
  {
    "text": "sense I don't know maybe um it would probably work but like how much effort would it be to automate it would I need",
    "start": "833560",
    "end": "840199"
  },
  {
    "text": "to write a tekon task to to launch the MPI job and",
    "start": "840199",
    "end": "846880"
  },
  {
    "text": "then like another so I don't know it's it's an experiment that I'd be interested in doing um but that that's",
    "start": "846880",
    "end": "852040"
  },
  {
    "text": "kind of the like where where would it go from here and then um trying to figure",
    "start": "852040",
    "end": "858240"
  },
  {
    "text": "out some performan stuff uh where were we yeah here so rejiggering some of this",
    "start": "858240",
    "end": "865839"
  },
  {
    "text": "make it look more like Morin's environment um and see if I can you know get close to the performance or whatever",
    "start": "865839",
    "end": "871880"
  },
  {
    "text": "we we do have some clusters and and Labs at red hat that I could run some of these things in um and then just figure",
    "start": "871880",
    "end": "878160"
  },
  {
    "text": "out you know like is there more storage optimization that I need to do or those kinds of things so that that's kind of where where does",
    "start": "878160",
    "end": "885320"
  },
  {
    "text": "it go you know again it was really it was a thsh experiment like can we run these types of jobs um grav like I",
    "start": "885320",
    "end": "892079"
  },
  {
    "text": "mentioned is the PM the handling a lot of the HPC stuff um just doing more HPC things um like this not necessar neily",
    "start": "892079",
    "end": "899320"
  },
  {
    "text": "more open phone but just other HBC things and see how do they work and just lots of blogs and other things it's a",
    "start": "899320",
    "end": "904839"
  },
  {
    "text": "long",
    "start": "904839",
    "end": "907079"
  },
  {
    "text": "answer I sure go ahead yeah I I I noticed under",
    "start": "913680",
    "end": "919560"
  },
  {
    "text": "the MPI run command you didn't have the allow run as root flag just curious like",
    "start": "919560",
    "end": "926279"
  },
  {
    "text": "uh did you have any issues with setting it up to like run as user instead of run",
    "start": "926279",
    "end": "932399"
  },
  {
    "text": "as yeah so where was it privilege so many images lots of",
    "start": "932399",
    "end": "940600"
  },
  {
    "text": "memes nope that wasn't it it was I don't remember anyway we we'll say here um so",
    "start": "940600",
    "end": "946880"
  },
  {
    "text": "the the most of the challenges I had were because of SSH and because of the",
    "start": "946880",
    "end": "952680"
  },
  {
    "text": "open foam container and so the open foam container expects to run as a very specific user and trying to run SSH is",
    "start": "952680",
    "end": "959000"
  },
  {
    "text": "non-root is like a monstrous pain in the butt um and so for the sake of just like",
    "start": "959000",
    "end": "964240"
  },
  {
    "text": "get it done I I I just kind of blew the permissions wide open and tried to do it however I could um Red Hat has done a",
    "start": "964240",
    "end": "971800"
  },
  {
    "text": "lot of work around randomizing user IDs and other security enhancements in in",
    "start": "971800",
    "end": "977160"
  },
  {
    "text": "open shift to um to reduce the blast",
    "start": "977160",
    "end": "982800"
  },
  {
    "text": "radius of like misbehave workloads so to speak uh and be more secure and so we have some weird tricks that we we've",
    "start": "982800",
    "end": "989079"
  },
  {
    "text": "done in our Builder images to like do things to Etsy password as the container",
    "start": "989079",
    "end": "994440"
  },
  {
    "text": "comes up so that things think they're running as the right user anyway I just I didn't want to spend the time to go through it so tldr and your question is",
    "start": "994440",
    "end": "1001360"
  },
  {
    "text": "um yeah I just blew the permissions wide open um to make it run but you you you",
    "start": "1001360",
    "end": "1006399"
  },
  {
    "text": "could probably spend a little bit more time to make it run not like so so",
    "start": "1006399",
    "end": "1012759"
  },
  {
    "text": "badly you wanted to see one of the I think it's one this one",
    "start": "1012759",
    "end": "1019480"
  },
  {
    "text": "yep oh I did thank you you're you're welcome no no question about the slide just want to",
    "start": "1019480",
    "end": "1025558"
  },
  {
    "text": "see it yeah okay",
    "start": "1025559",
    "end": "1028959"
  },
  {
    "text": "cool yes so were you or red hat doing more on",
    "start": "1033000",
    "end": "1040160"
  },
  {
    "text": "kubernetes and HPC or was this just kind of a one-off experiment for me it was a one-off",
    "start": "1040160",
    "end": "1046720"
  },
  {
    "text": "experiment because of that email that came in like you know they wanted to do it in vmss and I said well you probably",
    "start": "1046720",
    "end": "1051840"
  },
  {
    "text": "don't need to do it in VMS you could probably just do it directly with with kubernetes um you know redhead is not",
    "start": "1051840",
    "end": "1058799"
  },
  {
    "text": "going to suddenly become an open foam company right like that wasn't the purpose of this experiment at all it was really mostly like what what does it",
    "start": "1058799",
    "end": "1066200"
  },
  {
    "text": "look like to use the NPI operator redhead has invested in the cube flow project Community for other reasons",
    "start": "1066200",
    "end": "1071640"
  },
  {
    "text": "around AIML um so there happened to be people that I could talk to easily to to",
    "start": "1071640",
    "end": "1076720"
  },
  {
    "text": "to fuss around with this thing um but yeah in terms of like any kind of strategic Road mppy thing like it's it's",
    "start": "1076720",
    "end": "1083360"
  },
  {
    "text": "it's not this specifically it's more generally like we are exploring how to do HBC in a kubernetes world and what we",
    "start": "1083360",
    "end": "1090799"
  },
  {
    "text": "could maybe productize um you know or just what we can support who we can partner with you know and so on and so",
    "start": "1090799",
    "end": "1096840"
  },
  {
    "text": "forth um of course we're doing a lot of great work with IBM in this Arena but there's a lot of other companies and projects that are out there that that",
    "start": "1096840",
    "end": "1102799"
  },
  {
    "text": "sort of play in this space as well okay and if I was going to do a followup uh we",
    "start": "1102799",
    "end": "1108960"
  },
  {
    "text": "got time okay um when you were running an AWS were you using their MPI drivers no",
    "start": "1108960",
    "end": "1116360"
  },
  {
    "text": "so o Open foam and open MPI and Cube flow don't require any kind of special drivery bits um it was really just I did",
    "start": "1116360",
    "end": "1124360"
  },
  {
    "text": "a vanilla open shift installation on um on AWS you probably could have also used",
    "start": "1124360",
    "end": "1129679"
  },
  {
    "text": "eks on AWS um I used the um open data Foundation because I needed rwx uh you",
    "start": "1129679",
    "end": "1136919"
  },
  {
    "text": "probably could have used EFS you know you just need some Central uh rwx storage that everybody can read and",
    "start": "1136919",
    "end": "1142960"
  },
  {
    "text": "write to and the only reason I needed rwx was because the launcher did the",
    "start": "1142960",
    "end": "1148919"
  },
  {
    "text": "initial writing and decomposition of the job and then wrote that out to the shared storage and then all the workers",
    "start": "1148919",
    "end": "1155760"
  },
  {
    "text": "needed to access it and then write their stuff back so like if I had done it in a pipeline thing I might have actually",
    "start": "1155760",
    "end": "1161960"
  },
  {
    "text": "been able to get away with some other way to do it but I again I just didn't explore that deeply um but yeah there",
    "start": "1161960",
    "end": "1169120"
  },
  {
    "text": "was there was nothing special about the way I was using AWS or what I was doing on AWS if I had had a large cluster of",
    "start": "1169120",
    "end": "1176440"
  },
  {
    "text": "Hardware like at my disposal internally easily um I mean we have a lab like my group I just didn't feel like figuring",
    "start": "1176440",
    "end": "1183400"
  },
  {
    "text": "out how to to use it uh this was easier we have a system I can order a cluster it's",
    "start": "1183400",
    "end": "1189159"
  },
  {
    "text": "like well it's this guy anyway yes sir uh",
    "start": "1189159",
    "end": "1195640"
  },
  {
    "text": "question for context so what is Morin's workflow right now then if not using something like this what are they doing",
    "start": "1195640",
    "end": "1200919"
  },
  {
    "text": "uh he he has a rack of Old Dell one Rus I think he's got like six",
    "start": "1200919",
    "end": "1208960"
  },
  {
    "text": "or seven of them or whatever um that he was using and and it was mostly like I wasn't trying to get them to go",
    "start": "1208960",
    "end": "1215960"
  },
  {
    "text": "kubernetes or anything like that it was more like well I know somebody that uses this tool so I can at least poke them",
    "start": "1215960",
    "end": "1221400"
  },
  {
    "text": "for you know if I run into run into a jam um but yeah their their existing",
    "start": "1221400",
    "end": "1226799"
  },
  {
    "text": "workload today is just 100% bare metal um no no Cube no containers it's just",
    "start": "1226799",
    "end": "1232240"
  },
  {
    "text": "like a buntu installed directly on the on the servers and then um you know open foam installed in the",
    "start": "1232240",
    "end": "1239960"
  },
  {
    "text": "OS well no he I mean it's it's it's using MPI so so he's got a head server",
    "start": "1242480",
    "end": "1248320"
  },
  {
    "text": "uh I mean it looks a lot like that in a way like there's a head server where that he runs",
    "start": "1248320",
    "end": "1255080"
  },
  {
    "text": "um the the pre-processing stuff and then that's done on a NFS share the head",
    "start": "1255080",
    "end": "1260440"
  },
  {
    "text": "server is also the NFS server and then they do NPI run which then goes out to the other more powerful servers um and",
    "start": "1260440",
    "end": "1267679"
  },
  {
    "text": "then they are pulling from the NFS share and doing the actual processing and then the reconstruct happens at the end and",
    "start": "1267679",
    "end": "1274240"
  },
  {
    "text": "then it spits out the cool",
    "start": "1274240",
    "end": "1277320"
  },
  {
    "text": "image microphone disappear",
    "start": "1282000",
    "end": "1286360"
  },
  {
    "text": "hi great talk um I I was curious with regard to and I'm not super familiar",
    "start": "1292559",
    "end": "1297720"
  },
  {
    "text": "with open form uh do you have to how do you handle the scheduling of all",
    "start": "1297720",
    "end": "1305799"
  },
  {
    "text": "the yeah so all the all the parts on on the on the right hand side do all of",
    "start": "1305960",
    "end": "1311159"
  },
  {
    "text": "these need to be scheduled in do at like one go do essentially what I'm trying to",
    "start": "1311159",
    "end": "1316720"
  },
  {
    "text": "say is is this a gangster scheduling problem that uh we are trying to solve",
    "start": "1316720",
    "end": "1322240"
  },
  {
    "text": "here yeah so I'm going to partially answer your question CU I don't fully know so so take what I'm saying with a",
    "start": "1322240",
    "end": "1327400"
  },
  {
    "text": "grain of salt the NPI operator in the actual job definition and I don't have",
    "start": "1327400",
    "end": "1333000"
  },
  {
    "text": "the the full job definition you you write a podspec effectively for the launcher and a podspec effectively for",
    "start": "1333000",
    "end": "1339039"
  },
  {
    "text": "the workers and you tell it how many workers you want when you run the job or",
    "start": "1339039",
    "end": "1344200"
  },
  {
    "text": "or sorry when you instantiate the job instance mhm uh the the CR the operator",
    "start": "1344200",
    "end": "1350520"
  },
  {
    "text": "creates the launcher pod and creates all of the um worker pods and it it pretty much expects that they all can run at",
    "start": "1350520",
    "end": "1356799"
  },
  {
    "text": "that time and I think if it can't run all of the workers it just sits there",
    "start": "1356799",
    "end": "1361880"
  },
  {
    "text": "and waits so it doesn't like run one of them ahead of time and wait for it to finish and then oh now I can fit another",
    "start": "1361880",
    "end": "1367039"
  },
  {
    "text": "one and run that on the back so I guess it is a gang operation like it wants it all to happen at once I don't know if",
    "start": "1367039",
    "end": "1372360"
  },
  {
    "text": "that's a limitation on the NPI operator like if if there's a way to tell it like it's okay to run something ahead of time",
    "start": "1372360",
    "end": "1379760"
  },
  {
    "text": "I I just never looked into it any deeper and when you ran the onia cloud provider AWS in this case uh did you have to like",
    "start": "1379760",
    "end": "1386760"
  },
  {
    "text": "prescale the compute to be able to handle all those parts and then scale it down so I I what I wanted to do was I",
    "start": "1386760",
    "end": "1393559"
  },
  {
    "text": "actually wanted to do this as a cluster autoscaling job and so the way open shift is built and and you can do this",
    "start": "1393559",
    "end": "1399799"
  },
  {
    "text": "with with most of the kubernetes um there's a way to make open shift Auto scale itself based on workload demand",
    "start": "1399799",
    "end": "1405360"
  },
  {
    "text": "and so what I was hoping to do was have like nowhere near enough actual nodes and then run this job and then when the",
    "start": "1405360",
    "end": "1412880"
  },
  {
    "text": "cluster autoscaler detected that there were pods waiting it would start jacking up the number of nodes to actually run",
    "start": "1412880",
    "end": "1419799"
  },
  {
    "text": "enough stuff but I just I didn't I didn't get there um so you wouldn't you wouldn't necessarily have to have that",
    "start": "1419799",
    "end": "1426640"
  },
  {
    "text": "ahead of time the only caveat I guess is I don't know what the timeout is if any",
    "start": "1426640",
    "end": "1432640"
  },
  {
    "text": "on the MPI job operator and so if it took like a really long time to get enough nodes to then schedule the pods I",
    "start": "1432640",
    "end": "1439320"
  },
  {
    "text": "don't know if the job would have already failed because it was like oh I I didn't get to do it never mind I give up like",
    "start": "1439320",
    "end": "1444799"
  },
  {
    "text": "I'm not sure what the timeout kind of deal is yeah that makes sense the my next question is on uh I think you",
    "start": "1444799",
    "end": "1451720"
  },
  {
    "text": "mentioned that multiple parts were running on like one bare metal instance 1 VM is that uh accurate did you see",
    "start": "1451720",
    "end": "1460080"
  },
  {
    "text": "resource contention when multiple ports were running on the same VM yeah",
    "start": "1460080",
    "end": "1466240"
  },
  {
    "text": "um no I didn't see resource attention because the Pod when you write the",
    "start": "1466240",
    "end": "1471399"
  },
  {
    "text": "podspec you specify like a li a request and a limit to do the best sorry",
    "start": "1471399",
    "end": "1477919"
  },
  {
    "text": "guaranteed quality of service tier so for those of you who've ever played with the quality of service tiers in kubernetes like you've got requests and",
    "start": "1477919",
    "end": "1483799"
  },
  {
    "text": "you've got limits if they're the same it says like make sure I absolutely have that much to run or don't run me um and",
    "start": "1483799",
    "end": "1490679"
  },
  {
    "text": "then underneath the covers you know the Linux kernel and cgroups and the scheduler and all that other stuff make sure that there's no contention if you",
    "start": "1490679",
    "end": "1497039"
  },
  {
    "text": "wrote the spec for the badly and didn't have a",
    "start": "1497039",
    "end": "1503399"
  },
  {
    "text": "um a request and a limit to put you in the quality uh the guaranteed Quality Service tier and you ended up in either",
    "start": "1503399",
    "end": "1508919"
  },
  {
    "text": "best effort or like uh whatever the other one is like you could totally end up with resource contention where it just would throw two pods on the thing",
    "start": "1508919",
    "end": "1515240"
  },
  {
    "text": "and they're both trying to use all the CPU and they both one of them starves the other one out so like you definitely want to put limits and requests to make",
    "start": "1515240",
    "end": "1522000"
  },
  {
    "text": "sure you're in the guaranteed qos tier um otherwise you you'll end up with",
    "start": "1522000",
    "end": "1527039"
  },
  {
    "text": "contention yeah that's like normal that that's not specific to the HBC stuff that's like generic kubernetes stuff",
    "start": "1527039",
    "end": "1535640"
  },
  {
    "text": "y just wanted to uh confirm so when it came to the cube flow it was uh",
    "start": "1543480",
    "end": "1550559"
  },
  {
    "text": "providing your MPI operator so then all the uh communication from the launcher",
    "start": "1550559",
    "end": "1556720"
  },
  {
    "text": "pod to the compute pods uh was just being solely managed through whatever",
    "start": "1556720",
    "end": "1563039"
  },
  {
    "text": "Cube flow provided you correct uh at that point it's all uh where's the SSH",
    "start": "1563039",
    "end": "1570559"
  },
  {
    "text": "picture so it's it's all still SSH between the um the launcher and the",
    "start": "1571520",
    "end": "1576679"
  },
  {
    "text": "workers so the operator ensures that the launcher pod is created and the worker pods are created once all of the pods",
    "start": "1576679",
    "end": "1582799"
  },
  {
    "text": "are up and running the jaw or the script",
    "start": "1582799",
    "end": "1587960"
  },
  {
    "text": "will run inside the launcher mhm and that MPI run command MPI itself will",
    "start": "1587960",
    "end": "1594080"
  },
  {
    "text": "then look at the the host file to find well they it'll look at the NPI host file to find all the targets but those",
    "start": "1594080",
    "end": "1600320"
  },
  {
    "text": "targets are all just pod names and so then literally like NPI fires off a bunch of SSH connections into all the",
    "start": "1600320",
    "end": "1607200"
  },
  {
    "text": "workers and then executes whatever what was",
    "start": "1607200",
    "end": "1613240"
  },
  {
    "text": "it yeah so that MPI run MP4 Intero so everything after the four is what gets",
    "start": "1613240",
    "end": "1619960"
  },
  {
    "text": "run um effectively on each of the workers okay um so so the all the cube",
    "start": "1619960",
    "end": "1627840"
  },
  {
    "text": "flow all the NPI operator part of cube flow is doing is watching for those um",
    "start": "1627840",
    "end": "1635679"
  },
  {
    "text": "that's not bright enough it just watches for these and then creates the pods and then executes the command it's not",
    "start": "1635679",
    "end": "1641760"
  },
  {
    "text": "managing any of the like communication stuff that all happens effectively over the kubernetes service layer well over",
    "start": "1641760",
    "end": "1648760"
  },
  {
    "text": "the kubernetes software defined Network I don't think it's actually even using Services because it's talking directly to each pod by host name I think that's",
    "start": "1648760",
    "end": "1655399"
  },
  {
    "text": "where I was curious where the obfuscation was happening or the the um",
    "start": "1655399",
    "end": "1660720"
  },
  {
    "text": "yeah of of the traffic it sounds like you know C Cube flow is providing you the MPI so if you have questions",
    "start": "1660720",
    "end": "1668080"
  },
  {
    "text": "regarding MPI you got to look at Cube flow's source code and then regarding uh",
    "start": "1668080",
    "end": "1674039"
  },
  {
    "text": "uh like in the the communication outside of that SSH or or otherwise um you're",
    "start": "1674039",
    "end": "1679960"
  },
  {
    "text": "looking more back to kubernetes itself uh yeah sort of I think Mo again most of",
    "start": "1679960",
    "end": "1687840"
  },
  {
    "text": "the communication is coming from what what is happening in the NPI run commands",
    "start": "1687840",
    "end": "1694320"
  },
  {
    "text": "from a like Network or storage communication perspective the the SSH",
    "start": "1694320",
    "end": "1700320"
  },
  {
    "text": "connections from the launcher to the workers is just over the local sdn",
    "start": "1700320",
    "end": "1705399"
  },
  {
    "text": "whatever Network you've configured for your kubernetes it doesn't have to be a software to find network some people who do it on bare metal they don't use ndn",
    "start": "1705399",
    "end": "1711679"
  },
  {
    "text": "um and then of course you've got the storage communication which in this case is is network communication um between",
    "start": "1711679",
    "end": "1718200"
  },
  {
    "text": "the workers and the the storage that contains the little job bits and pieces",
    "start": "1718200",
    "end": "1724360"
  },
  {
    "text": "um from a tuning perspective like openr is pretty good about the storage e bit",
    "start": "1724360",
    "end": "1730399"
  },
  {
    "text": "because basically all the workers like they they load it up into memory once and then they do their thing and then",
    "start": "1730399",
    "end": "1735880"
  },
  {
    "text": "they spit it out they're not constantly talking to the story but if you have a different type of NPI workload that is constantly read writing",
    "start": "1735880",
    "end": "1742080"
  },
  {
    "text": "to storage then of course that storage network is going to be important to ensure that it's got you know high",
    "start": "1742080",
    "end": "1747480"
  },
  {
    "text": "performance um uh characteristics or that the discs have high performance or both and then I haven't had a chance to",
    "start": "1747480",
    "end": "1753919"
  },
  {
    "text": "look at your blog post which I look forward to but um when it came to open foam my ticket you probably just pulled",
    "start": "1753919",
    "end": "1761360"
  },
  {
    "text": "uh like a Docker file from somewhere or or you pulled the container yeah I I",
    "start": "1761360",
    "end": "1766559"
  },
  {
    "text": "think I had to rebuild their image to make like a oh I had to rebuild their image to change the SSH configuration to",
    "start": "1766559",
    "end": "1773760"
  },
  {
    "text": "to make some of that stuff work because the the the open pH they provide a",
    "start": "1773760",
    "end": "1780679"
  },
  {
    "text": "container for you to do stuff in the open Pham project but the container that they provide is like super hard lock to",
    "start": "1780679",
    "end": "1786760"
  },
  {
    "text": "one specific uid and like behaves weirdly if you don't use that and so I had to like mess with the bash RC to",
    "start": "1786760",
    "end": "1795000"
  },
  {
    "text": "make it load the right pathy stuff to find all the tools and everything it's a pretty minimal change but basically yes",
    "start": "1795000",
    "end": "1801080"
  },
  {
    "text": "I'm just using their their container image and and all anything I had to do from a container image perspective it's",
    "start": "1801080",
    "end": "1806720"
  },
  {
    "text": "all linked in the blog post and it's in a repo I think there's a couple repos that have the different bits and pieces",
    "start": "1806720",
    "end": "1812080"
  },
  {
    "text": "okay thank",
    "start": "1812080",
    "end": "1814480"
  },
  {
    "text": "you I I have not I know we have customers that do do some of those more",
    "start": "1823840",
    "end": "1829000"
  },
  {
    "text": "advanced network storage things um I don't even know that I have access to infin somebody I Red Hat somewhere in a",
    "start": "1829000",
    "end": "1835640"
  },
  {
    "text": "lab has has infin ban that I could try um but the reality is that like that's kind of invisible to the environment it",
    "start": "1835640",
    "end": "1843320"
  },
  {
    "text": "it would be more of a like the the beauty of kubernetes and to",
    "start": "1843320",
    "end": "1849039"
  },
  {
    "text": "a certain extent storage and and the kernel and all these other things is like that stuff shouldn't kind of matter",
    "start": "1849039",
    "end": "1854480"
  },
  {
    "text": "like maybe I would have to define a PVC that exists as a as a um as an endpoint on that infin band",
    "start": "1854480",
    "end": "1860559"
  },
  {
    "text": "storage right I don't know is infin Ban like ice Gish or is it more of a fiber",
    "start": "1860559",
    "end": "1866080"
  },
  {
    "text": "Channell where none of the above all of the above anyway tldr HC",
    "start": "1866080",
    "end": "1875398"
  },
  {
    "text": "got it did you hear any of that sort of so so you may have to",
    "start": "1890080",
    "end": "1895799"
  },
  {
    "text": "attach infin band devices directly into the pods which is which is something that that you can do um there there's",
    "start": "1895799",
    "end": "1901360"
  },
  {
    "text": "ways to attach devices into pods um it would probably make the the wording of",
    "start": "1901360",
    "end": "1907639"
  },
  {
    "text": "the um the job and like the Pod definitions",
    "start": "1907639",
    "end": "1912840"
  },
  {
    "text": "for the launcher and the other things it would make that kind of start to get hairy because you got to like be adding storage everything I had to do that",
    "start": "1912840",
    "end": "1919120"
  },
  {
    "text": "anyway to make these connect to the rwx but if you also had to add devices to the pods like it would it would just get",
    "start": "1919120",
    "end": "1927518"
  },
  {
    "text": "trickier yeah and I don't know if open Foams command like tools speak",
    "start": "1930720",
    "end": "1937320"
  },
  {
    "text": "INF finan",
    "start": "1937320",
    "end": "1940600"
  },
  {
    "text": "yeah all kubernetes is doing is is effectively if you think about the the path of how pods get launched",
    "start": "1948039",
    "end": "1954880"
  },
  {
    "text": "like kubernetes tells a cubet on a node to run a a pod and then under the covers",
    "start": "1954880",
    "end": "1961559"
  },
  {
    "text": "cryo and runc and all these other things are being instructed on how to bring up that container and in that process that",
    "start": "1961559",
    "end": "1967720"
  },
  {
    "text": "includes like you need to attach this stuff from the host into this container",
    "start": "1967720",
    "end": "1972760"
  },
  {
    "text": "at this point so in the case of an infiniband device you know it would like if you think about a podman or a Docker",
    "start": "1972760",
    "end": "1979120"
  },
  {
    "text": "run command it would be like you know attach slev whatever into the Container",
    "start": "1979120",
    "end": "1984679"
  },
  {
    "text": "atdev whatever um so there there's no there's no knowledge required of",
    "start": "1984679",
    "end": "1990159"
  },
  {
    "text": "kubernetes of infiniband unless you're talking about like some CSI infin ban",
    "start": "1990159",
    "end": "1995240"
  },
  {
    "text": "driver thing but that would be baked into the CSI driver for infin ban so anyway so back to the original question",
    "start": "1995240",
    "end": "2000960"
  },
  {
    "text": "I did not try infiniband um I might be able to find some somewhere uh but I don't know that that open phone would",
    "start": "2000960",
    "end": "2007320"
  },
  {
    "text": "know what to do with it even even if I did it does okay well then there you",
    "start": "2007320",
    "end": "2016120"
  },
  {
    "text": "go got it cool so you alluded to a couple race",
    "start": "2017519",
    "end": "2023000"
  },
  {
    "text": "conditions in the MPI bootstrapping process uh can you speak a little bit about those and whether you address",
    "start": "2023000",
    "end": "2030200"
  },
  {
    "text": "those no so okay so you you you said it was 512",
    "start": "2030880",
    "end": "2037240"
  },
  {
    "text": "ranks is that that correct um what did I",
    "start": "2037240",
    "end": "2042600"
  },
  {
    "text": "do uh so I did 256 pods with two CPUs per pod so that ended up being a total",
    "start": "2043600",
    "end": "2050200"
  },
  {
    "text": "of 512 CPUs and I think it ended up being I don't know how many that ends up",
    "start": "2050200",
    "end": "2055800"
  },
  {
    "text": "being per node I'll make up a number it was like but presumably it was one rank per CPU one what one rank per CPU one",
    "start": "2055800",
    "end": "2063720"
  },
  {
    "text": "rank MPI rank oh yeah yeah yeah yeah yeah the I mean the MPI r run command was pretty vanilla um and the host file",
    "start": "2063720",
    "end": "2071118"
  },
  {
    "text": "that gets generated is also super vanilla um yeah so there's no there's no",
    "start": "2071119",
    "end": "2076960"
  },
  {
    "text": "fancy MPI stuff going on the re the reason I asked is that we ran into bootstrapping Pro uh problems with about",
    "start": "2076960",
    "end": "2083440"
  },
  {
    "text": "80 ranks or so so we'd see race conditions where whereby the workers would report ready um would sorry the",
    "start": "2083440",
    "end": "2090599"
  },
  {
    "text": "workers the MPI launcher would start before all the workers were ready sometimes which would c a crash in the",
    "start": "2090599",
    "end": "2096320"
  },
  {
    "text": "bootstrap and then the other thing that would happen sometimes is the workers would start and would report ready",
    "start": "2096320",
    "end": "2102359"
  },
  {
    "text": "before SSH was ready which then would cause the MPI bootstrap to fail again ah",
    "start": "2102359",
    "end": "2107440"
  },
  {
    "text": "okay yeah yeah yeah I so I definitely did see this problem um especially when you're dealing with large numbers of",
    "start": "2107440",
    "end": "2113119"
  },
  {
    "text": "PODS the launcher pod will fail like a bunch of times because nothing else is",
    "start": "2113119",
    "end": "2118839"
  },
  {
    "text": "ready yet so MPI comes up and it tries to do it thing and it's like I couldn't find half of these machines I don't know",
    "start": "2118839",
    "end": "2124400"
  },
  {
    "text": "what to do and it crash loops and then kind of backs off so I don't know that",
    "start": "2124400",
    "end": "2130200"
  },
  {
    "text": "they could probably do things to make it smarter one number two I'm not sure if you can",
    "start": "2130200",
    "end": "2136640"
  },
  {
    "text": "Define liveness and Readiness probes in the template spec for the for the job I",
    "start": "2136640",
    "end": "2142640"
  },
  {
    "text": "mean it should support the full podspec but the challenge is like I you'd have",
    "start": "2142640",
    "end": "2148240"
  },
  {
    "text": "to write a really nasty livess Readiness probe for the worker to like SSH into itself you can you can do a post dark",
    "start": "2148240",
    "end": "2155200"
  },
  {
    "text": "hook where you just like check to make sure the port Port 22 is open for example yeah I mean you could do you",
    "start": "2155200",
    "end": "2160520"
  },
  {
    "text": "could do things it's just I never I never got to that point fortunately the um the pre-processing",
    "start": "2160520",
    "end": "2167480"
  },
  {
    "text": "and this is why doing it as a tecton pipeline could be beneficial because all the pre-processing steps would get done",
    "start": "2167480",
    "end": "2173359"
  },
  {
    "text": "and then the only thing that's failing is the NPI run if you look at this like blog mesh set Fields",
    "start": "2173359",
    "end": "2179040"
  },
  {
    "text": "decompose they're happy to run multiple times while it fails because ultimately",
    "start": "2179040",
    "end": "2184839"
  },
  {
    "text": "it does the first three things then runs NPI run and that's when it fails so if you have other steps that come before",
    "start": "2184839",
    "end": "2190680"
  },
  {
    "text": "the actual MPI that are not I'll use the word idempotent which is probably the wrong word to use here but we we'll go",
    "start": "2190680",
    "end": "2196720"
  },
  {
    "text": "with it for now like if you have things that would blow up if they got run twice OB this is going to be bad Bad News",
    "start": "2196720",
    "end": "2202520"
  },
  {
    "text": "Bears right okay we're getting the we're getting the hook thanks very much uh if you have other questions I'm happy to",
    "start": "2202520",
    "end": "2208240"
  },
  {
    "text": "answer them outside uh again the presentation uploaded to the portal um",
    "start": "2208240",
    "end": "2214160"
  },
  {
    "text": "you can find me pretty much on lots of places on the internet I appreciate it have a great tra home travel home safe",
    "start": "2214160",
    "end": "2222640"
  }
]