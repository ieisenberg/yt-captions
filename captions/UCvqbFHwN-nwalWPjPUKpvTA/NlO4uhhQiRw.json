[
  {
    "text": "hello hello everybody well thank you for coming and",
    "start": "199",
    "end": "7120"
  },
  {
    "text": "my name is Antonio and work at Google I'm kubernetes contributor and JY",
    "start": "7120",
    "end": "12960"
  },
  {
    "text": "networking developer I'm going to present with Git about modern law",
    "start": "12960",
    "end": "18279"
  },
  {
    "text": "balancing improving application resource availability and",
    "start": "18279",
    "end": "23640"
  },
  {
    "text": "performance hi everybody I'm Garrett uh I'm here today uh with Antonio thank you",
    "start": "25039",
    "end": "30519"
  },
  {
    "text": "all for for having us and glad to see a good turnout here uh I was joking I said",
    "start": "30519",
    "end": "35760"
  },
  {
    "text": "maybe no one would show up so that was sort of a little fear that I had but uh hopefully today we'll we'll have a good Deep dive I play a network uh engineer",
    "start": "35760",
    "end": "42640"
  },
  {
    "text": "on TV uh work in support engineering at at Google um yeah so why don't we we can",
    "start": "42640",
    "end": "48480"
  },
  {
    "text": "get started okay the the way that we are going to organize the talk and I'm going to do a brief summary of touching",
    "start": "48480",
    "end": "56359"
  },
  {
    "text": "several to topics about what is lo balancing what problems load balancing soles and G is going to do a more deep",
    "start": "56359",
    "end": "63120"
  },
  {
    "text": "dive into the actual problem of load balancing and H cases so the first",
    "start": "63120",
    "end": "69560"
  },
  {
    "text": "question is why do you need a load balancer right or why why do when should I need a Lo",
    "start": "69560",
    "end": "76840"
  },
  {
    "text": "balancer if you have just an application that is running and you don't care about availability or",
    "start": "76840",
    "end": "83520"
  },
  {
    "text": "performance or you don't care about service Discovery for sure you don't need a load balancer",
    "start": "83520",
    "end": "91159"
  },
  {
    "text": "but if this is your situation that you",
    "start": "91159",
    "end": "96600"
  },
  {
    "text": "care about these things L balance is going to be your best friend in this in",
    "start": "96600",
    "end": "102560"
  },
  {
    "text": "this in this journey so before going into the L balancer topic let's do a",
    "start": "102560",
    "end": "109719"
  },
  {
    "text": "first stoping what is the networking stack how the application work right so",
    "start": "109719",
    "end": "115200"
  },
  {
    "text": "you can see in this diagram you can see two host the host has processor right",
    "start": "115200",
    "end": "120240"
  },
  {
    "text": "the processes run and they want to communicate between each other so they",
    "start": "120240",
    "end": "125479"
  },
  {
    "text": "send a packet open a socket the packet goes through an application layer this all are abstraction right then the",
    "start": "125479",
    "end": "132840"
  },
  {
    "text": "application layer assembles the the TCP packet the TCP packet goes to an IP IP",
    "start": "132840",
    "end": "139120"
  },
  {
    "text": "packet and then it creates an packet that goes through the network right and",
    "start": "139120",
    "end": "146040"
  },
  {
    "text": "magically this packet happens to appear in the the networking stack of the",
    "start": "146040",
    "end": "151319"
  },
  {
    "text": "second host and do the reverse path it start going up on the application ledes",
    "start": "151319",
    "end": "157000"
  },
  {
    "text": "and finally go to the other process so what we we have is how how the hell",
    "start": "157000",
    "end": "166040"
  },
  {
    "text": "get the packet to the other Pro right because there is some cloud with some",
    "start": "166040",
    "end": "171120"
  },
  {
    "text": "devices that are able to get this packets and forward to the right place",
    "start": "171120",
    "end": "176519"
  },
  {
    "text": "right and one of these virtual devices is a network Lo balancer uh a network Lo",
    "start": "176519",
    "end": "182400"
  },
  {
    "text": "balancer what it simply does is just it gets the pcket and magically is able to",
    "start": "182400",
    "end": "189440"
  },
  {
    "text": "forward the packet to another destination right and depending on the layer with this load balance is working",
    "start": "189440",
    "end": "196840"
  },
  {
    "text": "is able to do let's say different things right if we have a l balancer at ler two level",
    "start": "196840",
    "end": "205640"
  },
  {
    "text": "the only thing that we can do is to send packets to other HS but the only information that we have is the the the",
    "start": "205640",
    "end": "214000"
  },
  {
    "text": "in Ethernet the Mac add right so typically this is you have a kuet cluster you can use metal B with the L2",
    "start": "214000",
    "end": "222360"
  },
  {
    "text": "mode or if you are used to the more conventional routing you can use brrp to",
    "start": "222360",
    "end": "230480"
  },
  {
    "text": "implement active passive Gateway right but the the the problem of this mode is",
    "start": "230480",
    "end": "235760"
  },
  {
    "text": "that it only works in a in a local broadcast domain so it's very limited",
    "start": "235760",
    "end": "241319"
  },
  {
    "text": "for what we want right if we go one layer ahead we can see that in the",
    "start": "241319",
    "end": "246439"
  },
  {
    "text": "network layer that layer that is typically the IP layer we can do load balancing right this is commonly",
    "start": "246439",
    "end": "253000"
  },
  {
    "text": "implemented with h routing any cast so typically on routers you are able to",
    "start": "253000",
    "end": "259160"
  },
  {
    "text": "forward to one or to other host but you still don't have the granularity that you want because what we want when we",
    "start": "259160",
    "end": "266440"
  },
  {
    "text": "run applications is to forward at the ler",
    "start": "266440",
    "end": "271960"
  },
  {
    "text": "4 the ler four that usually is TCP or UDP because you think that when you open",
    "start": "271960",
    "end": "277960"
  },
  {
    "text": "an application you open a socket right and the socket is releasing in in a in a",
    "start": "277960",
    "end": "283840"
  },
  {
    "text": "IP and a port so this is the this is the I would say the the most",
    "start": "283840",
    "end": "291440"
  },
  {
    "text": "common Lo balancer implemented and if you go to kubernetes you can see that the service abstraction is basically",
    "start": "291440",
    "end": "299280"
  },
  {
    "text": "that is basically attracting a a TCP oh sorry a layer four load balance right",
    "start": "299280",
    "end": "307160"
  },
  {
    "text": "you define a virtual IP and you define the ports and the protocol that you want",
    "start": "307160",
    "end": "312479"
  },
  {
    "text": "to forward the good thing with with virtual",
    "start": "312479",
    "end": "317600"
  },
  {
    "text": "network devices and not balances is that you can change it right with the cluster IP we solve the problem of a PO",
    "start": "317600",
    "end": "323720"
  },
  {
    "text": "communicating with other pods right you arra the the pods you use a label selectors and you send the the traffic",
    "start": "323720",
    "end": "330960"
  },
  {
    "text": "to the virtual IP the cluster IP and it magically appears in one another backend",
    "start": "330960",
    "end": "337759"
  },
  {
    "text": "with the problem is that we need to send traffic from",
    "start": "337759",
    "end": "343000"
  },
  {
    "text": "outside the cluster to the cluster then the common abstraction is to use a",
    "start": "343000",
    "end": "348319"
  },
  {
    "text": "service of TI Lo balancer this create a a chaining of load balancer so there is",
    "start": "348319",
    "end": "354759"
  },
  {
    "text": "an external Lo balancer that is able to forward traffic to the cluster where where the service is able to forward",
    "start": "354759",
    "end": "361319"
  },
  {
    "text": "traffic to the back end ports oh sorry so if we go to the last",
    "start": "361319",
    "end": "367919"
  },
  {
    "text": "layer we can see that there is an application L at the application layer",
    "start": "367919",
    "end": "373280"
  },
  {
    "text": "there is a protocol so when you are talking we are talking about ler 4 or TCP we are talking about the streams of",
    "start": "373280",
    "end": "380479"
  },
  {
    "text": "data that go into IP packets but when we are talking at the application we already have this stream right and the",
    "start": "380479",
    "end": "387520"
  },
  {
    "text": "protocol has a high a high level so this load balance is requires to reassemble",
    "start": "387520",
    "end": "392840"
  },
  {
    "text": "all this data pass the protocol and based on the protocol forward to one place or to the other this in kubernetes",
    "start": "392840",
    "end": "400560"
  },
  {
    "text": "can be mapped if only in HTTP to the Ingress object right you have you define",
    "start": "400560",
    "end": "406319"
  },
  {
    "text": "a URL and Ingress object is able to forward to one service that is other",
    "start": "406319",
    "end": "412840"
  },
  {
    "text": "forload balance the the problem is that this uh",
    "start": "412840",
    "end": "421000"
  },
  {
    "text": "abstraction has limitations you attended different talks uh during this week you",
    "start": "421000",
    "end": "426720"
  },
  {
    "text": "could see that the kubernetes community is pushing towards the Gateway API why",
    "start": "426720",
    "end": "432599"
  },
  {
    "text": "because those abstractions are getting short or have limitations or have",
    "start": "432599",
    "end": "439280"
  },
  {
    "text": "problems so gway API is coming to solve all this problem of declaring this type",
    "start": "439280",
    "end": "445039"
  },
  {
    "text": "of load balancer not ler seven only ler 4 to",
    "start": "445039",
    "end": "450960"
  },
  {
    "text": "so now that that we review this brief lesson of networking",
    "start": "450960",
    "end": "456599"
  },
  {
    "text": "and lo balancing is what are the practical applications of the Lo bances",
    "start": "456599",
    "end": "462039"
  },
  {
    "text": "so the the the one when I started say if you have you want High availability how",
    "start": "462039",
    "end": "468360"
  },
  {
    "text": "do I solve the problem with Lo balances so imagine that you have your application and you have your client",
    "start": "468360",
    "end": "473919"
  },
  {
    "text": "right so you are start sending traffic and suddenly the application dies the network is not able to forward it means",
    "start": "473919",
    "end": "481479"
  },
  {
    "text": "that all the clients are going to lose the connection so your application is no",
    "start": "481479",
    "end": "486840"
  },
  {
    "text": "longer available for this user you can use a loot balancer with head checks to",
    "start": "486840",
    "end": "492759"
  },
  {
    "text": "pull the application and say oh when this application is is dead I just don't",
    "start": "492759",
    "end": "498080"
  },
  {
    "text": "forward traffic to it I forward to the right this is really easy to demonstrate",
    "start": "498080",
    "end": "506680"
  },
  {
    "text": "theoretically but G will will show all all the problems that are behind this",
    "start": "506680",
    "end": "511760"
  },
  {
    "text": "later so one practical application of of this set up of using H checks and",
    "start": "511760",
    "end": "519039"
  },
  {
    "text": "detecting the data application is to implement rolling upd because you have",
    "start": "519039",
    "end": "524120"
  },
  {
    "text": "application with with your version one and then you want to say Okay I want to",
    "start": "524120",
    "end": "529200"
  },
  {
    "text": "roll out my new version the version two so once you have your application you",
    "start": "529200",
    "end": "535720"
  },
  {
    "text": "tell your Lo balancer to to get that that that backet into rotation but you",
    "start": "535720",
    "end": "541120"
  },
  {
    "text": "still don't start to forward traffic to it so when you stop the first version of",
    "start": "541120",
    "end": "547959"
  },
  {
    "text": "the application the Lo balancer will detect that is no longer available and",
    "start": "547959",
    "end": "553000"
  },
  {
    "text": "will make the application B too available as you can see for the client this is going to be transparent and you",
    "start": "553000",
    "end": "559079"
  },
  {
    "text": "are able to achieve rolling updates with zero disruption other application for for",
    "start": "559079",
    "end": "567640"
  },
  {
    "text": "load balances are using training is to implement High availability Regional High availability if you are in a data",
    "start": "567640",
    "end": "575519"
  },
  {
    "text": "imagine that you have requirements your application need to run in a in a in a",
    "start": "575519",
    "end": "581640"
  },
  {
    "text": "region and be available in different count so one of the typical setups is to set up a different cluster in different",
    "start": "581640",
    "end": "588320"
  },
  {
    "text": "zones or just different noes in different zones and have a loot balancer in front of them that is regional so if",
    "start": "588320",
    "end": "595880"
  },
  {
    "text": "one of the data centers goes down it is able to detect it and forward the traffic to the other data center but as",
    "start": "595880",
    "end": "602800"
  },
  {
    "text": "we said before this can be Chained and you can keep growing so instead of uh",
    "start": "602800",
    "end": "608279"
  },
  {
    "text": "your failure domain to be a continent you can see oh I have an application",
    "start": "608279",
    "end": "613600"
  },
  {
    "text": "that has to be work available so you keep changing load balance right and",
    "start": "613600",
    "end": "618839"
  },
  {
    "text": "that allows you to have full availability that are very simple cases",
    "start": "618839",
    "end": "626839"
  },
  {
    "text": "of H high high ability with problems but when we started we talk too about",
    "start": "626839",
    "end": "632079"
  },
  {
    "text": "performance problems so the performance problems can be because you know that the application is running a host and",
    "start": "632079",
    "end": "638600"
  },
  {
    "text": "it's going to be bounded by the CPU and the memory of this host so the more clients that you get the more low the",
    "start": "638600",
    "end": "645920"
  },
  {
    "text": "more CPU and memory that this application need to consume but these resources are not Infinity right so it",
    "start": "645920",
    "end": "652800"
  },
  {
    "text": "can be that the application cannot keep up with the load so some of the clients",
    "start": "652800",
    "end": "658040"
  },
  {
    "text": "are not going to be a able to have answers so your application is not going",
    "start": "658040",
    "end": "663839"
  },
  {
    "text": "to be available for these clients their performance is going to be the greatest so the solution the common solution is",
    "start": "663839",
    "end": "670839"
  },
  {
    "text": "to scale up right one common solution is but the problem is that if you are in kubernetes this there are efforts to",
    "start": "670839",
    "end": "679200"
  },
  {
    "text": "solve this problem so the PS can be scal up and add more resources but this has a",
    "start": "679200",
    "end": "684600"
  },
  {
    "text": "lot of sze effects and and uh is complicated to achieve so the most common solution is to scale up what you",
    "start": "684600",
    "end": "692160"
  },
  {
    "text": "do is you put a load balancer in front your application and then you can create",
    "start": "692160",
    "end": "699279"
  },
  {
    "text": "more copies of your application the load balancer is going to be to Distributing",
    "start": "699279",
    "end": "704880"
  },
  {
    "text": "the Lo between the different application this can be useful because it's just not",
    "start": "704880",
    "end": "710120"
  },
  {
    "text": "only to have better performance it's that also can save cost because you can",
    "start": "710120",
    "end": "716920"
  },
  {
    "text": "autoscale dynamically the number number of pens and with that GAR you are going to",
    "start": "716920",
    "end": "725600"
  },
  {
    "text": "explain the internals of all the low balancing program sure thank you so I'm",
    "start": "725600",
    "end": "730959"
  },
  {
    "text": "going to dig in a little bit uh looking peeling back a layer of magic uh but",
    "start": "730959",
    "end": "736040"
  },
  {
    "text": "first I want to want to kind of separate load balancers into into a couple of different categories uh I find this",
    "start": "736040",
    "end": "742079"
  },
  {
    "text": "helpful to conceptualize first category is a pass through load balancer or something like a pass through Network",
    "start": "742079",
    "end": "747760"
  },
  {
    "text": "load balancer and the second category is a proxy load balancer which can include things like application load balancing",
    "start": "747760",
    "end": "753839"
  },
  {
    "text": "when I say pass through I mean a load balancer that can process any uh OSI",
    "start": "753839",
    "end": "759000"
  },
  {
    "text": "layer three layer four protocol so TCP UDP and and other friends uh I also mean",
    "start": "759000",
    "end": "764279"
  },
  {
    "text": "something uh that acts as a router so it does not terminate a connection so if we're thinking about something like a",
    "start": "764279",
    "end": "770279"
  },
  {
    "text": "TCP connection there's not two there's just one at routes uh rep uh request packets arrive",
    "start": "770279",
    "end": "776959"
  },
  {
    "text": "on the network interface of the the backend so the node and they arrive",
    "start": "776959",
    "end": "782440"
  },
  {
    "text": "bearing the destination IP address of the VIP so this is a true uh true pass through load balancer there's no there's",
    "start": "782440",
    "end": "788480"
  },
  {
    "text": "no datat uh and then of course the node will perform datat uh to the Pod IP",
    "start": "788480",
    "end": "794399"
  },
  {
    "text": "address and we'll get into a little bit more detail about about that path the Pod will reply and then the node will",
    "start": "794399",
    "end": "800480"
  },
  {
    "text": "perform snat and change the source from the pods IP back to the load balancer VIP and so we have something called",
    "start": "800480",
    "end": "807160"
  },
  {
    "text": "direct server return and this is nice for the pass through load balancer because in our opinion this works really",
    "start": "807160",
    "end": "813120"
  },
  {
    "text": "well for services of type load balancer it's not the only way to do Services of type load balancer uh as an example in",
    "start": "813120",
    "end": "819920"
  },
  {
    "text": "gke kubernetes this is this is how we do it for a proxy load balancer we're talking about something that's generally",
    "start": "819920",
    "end": "826320"
  },
  {
    "text": "always TCP based uh there's two TCP connections you can think of the first TCP connection between the client and",
    "start": "826320",
    "end": "833399"
  },
  {
    "text": "the proxy or the proxy software being pods running in the cluster or or something outside the cluster and then",
    "start": "833399",
    "end": "839440"
  },
  {
    "text": "the second TCP connection between those proxies and the serving pods and this is",
    "start": "839440",
    "end": "844759"
  },
  {
    "text": "typically done at like an application layer something that's layer four or above so we could say like we use the",
    "start": "844759",
    "end": "851160"
  },
  {
    "text": "terms proxy Network load balancer when we mean something like a TCP or SSL",
    "start": "851160",
    "end": "856240"
  },
  {
    "text": "proxy and we use application load balancer when we're talking about htvp and friends and in an ideal",
    "start": "856240",
    "end": "862040"
  },
  {
    "text": "implementation the load balancer will be able to establish that second TCP connection uh between the proxy software",
    "start": "862040",
    "end": "868800"
  },
  {
    "text": "and the pods directly so the pods use IP addresses that are routable on the network so we call that container native",
    "start": "868800",
    "end": "875000"
  },
  {
    "text": "or container native load balancing the Pod of course would reply to the proxy and again in an ideal uh implementation",
    "start": "875000",
    "end": "882519"
  },
  {
    "text": "the Pod IPS are routable in the network the proxy receives the pods response",
    "start": "882519",
    "end": "887639"
  },
  {
    "text": "packet and the proxy copies the data from it into uh its own packet to send back to the client and these are these",
    "start": "887639",
    "end": "894680"
  },
  {
    "text": "can be used for services of type load balancer uh but they're really kind of perfect for for Gateway and Ingress as",
    "start": "894680",
    "end": "900480"
  },
  {
    "text": "as Antonio said I'm going to focus a little bit on the life of a packet for a",
    "start": "900480",
    "end": "905880"
  },
  {
    "text": "pass through load balancer uh and just sort of work through this at a high level and then and then dig in so if we",
    "start": "905880",
    "end": "912120"
  },
  {
    "text": "start I'm using documentation uh IP address ranges here by the way so uh this could apply to either an internal",
    "start": "912120",
    "end": "919000"
  },
  {
    "text": "or an external load balancer if you want to imagine the sources and destinations in in either way uh but we have a",
    "start": "919000",
    "end": "924600"
  },
  {
    "text": "request packet and in this case the source is the client the destination is the IP address of the load balancer or",
    "start": "924600",
    "end": "930040"
  },
  {
    "text": "the VIP and this packet is transmitted across the network the routing uh capability of the the pass through load",
    "start": "930040",
    "end": "936399"
  },
  {
    "text": "balancer will'll deliver that to one of one of the back ends we'll look at that how that works in just a moment and once",
    "start": "936399",
    "end": "942839"
  },
  {
    "text": "it's delivered to the back end in this case the node the node will process it performing destination Network address",
    "start": "942839",
    "end": "949120"
  },
  {
    "text": "translation rewriting the destination replacing the load balancer VIP with the IP address of a pod in this example I've",
    "start": "949120",
    "end": "955880"
  },
  {
    "text": "chosen the IP address of a pod on the same node that received the packet but that's not always the case uh this would",
    "start": "955880",
    "end": "961680"
  },
  {
    "text": "be with external traffic policy local pod would process the packet of course the response packet will flip the source",
    "start": "961680",
    "end": "968240"
  },
  {
    "text": "and destinations and after processing uh the node will perform Source Network",
    "start": "968240",
    "end": "973399"
  },
  {
    "text": "address translation uh changing the source from the Pod IP to the load balancer uh VIP uh IP address or",
    "start": "973399",
    "end": "979440"
  },
  {
    "text": "forwarding rule IP and then of course that response packet is sent on the network uh back to the client so if we",
    "start": "979440",
    "end": "985680"
  },
  {
    "text": "sort of put all of this together we sort of have a picture of that looks kind of like that the request",
    "start": "985680",
    "end": "992079"
  },
  {
    "text": "packet uh destination KN processing the packet and then Source n uh and this is",
    "start": "992079",
    "end": "997920"
  },
  {
    "text": "this is again I want to emphasize this is a little bit different from a proxy load balancer the load balancer is not",
    "start": "997920",
    "end": "1003240"
  },
  {
    "text": "delivering packets to destinations uh that match like a node Port we're we're delivering it to the network interface",
    "start": "1003240",
    "end": "1009880"
  },
  {
    "text": "of of the uh of the node but we're preserving the destination IP address uh",
    "start": "1009880",
    "end": "1016000"
  },
  {
    "text": "uh being the forwarding rule of the load balancer and I like to call this type of load balancer or this type of load",
    "start": "1016000",
    "end": "1021839"
  },
  {
    "text": "balancing as like load balancer inclusive because there's there's this term for like container native if we're talking about proxies where we where the",
    "start": "1021839",
    "end": "1028678"
  },
  {
    "text": "proxy able to communicate directly with a pod IP but in this case I I think that term works works out pretty well and",
    "start": "1028679",
    "end": "1035000"
  },
  {
    "text": "what I've shown you so far is an example that uses external traffic policy local",
    "start": "1035000",
    "end": "1041038"
  },
  {
    "text": "um there's two choices and and I presume everybody's familiar with this local and cluster and this helps the load balancer",
    "start": "1041039",
    "end": "1046520"
  },
  {
    "text": "decide which nodes will receive these load balanced packets let's talk a little bit more",
    "start": "1046520",
    "end": "1053559"
  },
  {
    "text": "about external traffic policy and how we group these nodes now a little little bit of information up front I can't give",
    "start": "1053559",
    "end": "1060039"
  },
  {
    "text": "you all possible examples I only have a few minutes here so most of my examples are going to uh focus on external",
    "start": "1060039",
    "end": "1065480"
  },
  {
    "text": "traffic policy local but one method is we group The nodes using instance groups",
    "start": "1065480",
    "end": "1071520"
  },
  {
    "text": "and we uh those instance groups are comprised of all the nodes uh from all",
    "start": "1071520",
    "end": "1077799"
  },
  {
    "text": "node pools of the cluster and we decide which nodes will receive the load Balan packets by using the external traffic",
    "start": "1077799",
    "end": "1084440"
  },
  {
    "text": "policy and load balancer health checks so for example if we have three nodes in",
    "start": "1084440",
    "end": "1090280"
  },
  {
    "text": "a cluster and we use external traffic policy cluster we would expect for all",
    "start": "1090280",
    "end": "1096000"
  },
  {
    "text": "three of them to pass what we call the load balance or health check whether or not the node actually contains a serving",
    "start": "1096000",
    "end": "1102840"
  },
  {
    "text": "pod and if we use external traffic policy local we would expect for only",
    "start": "1102840",
    "end": "1107880"
  },
  {
    "text": "those no to pass the load balcer health check if the node contains at least one",
    "start": "1107880",
    "end": "1113080"
  },
  {
    "text": "pod that has passed its Readiness probe if defined and is also not in a",
    "start": "1113080",
    "end": "1118960"
  },
  {
    "text": "terminating state which will be important in just a moment make a little smaller version of this so we can look at this little quick",
    "start": "1118960",
    "end": "1126039"
  },
  {
    "text": "detour here when I talk about load balance or health checks these are packets that are sent from the load balancing infrastructure to assess",
    "start": "1126039",
    "end": "1133320"
  },
  {
    "text": "whether or not a back end is healthy these are not the same thing as a kubernetes 's Readiness or liveness",
    "start": "1133320",
    "end": "1140200"
  },
  {
    "text": "probe and they're also responded to so so the entity that receives these health",
    "start": "1140200",
    "end": "1145880"
  },
  {
    "text": "check packets and the entity that replies to them would be something like Cube proxy or its material equivalent",
    "start": "1145880",
    "end": "1152200"
  },
  {
    "text": "like psyllium agent and uh that software will respond to them and it does so based on uh whether or not the there is",
    "start": "1152200",
    "end": "1161679"
  },
  {
    "text": "at least one pod that is not terminating and passing its Readiness probe so think",
    "start": "1161679",
    "end": "1166880"
  },
  {
    "text": "about it like this for external traffic policy cluster we always pass the load mouns or health check no matter what for",
    "start": "1166880",
    "end": "1172679"
  },
  {
    "text": "external traffic policy local we pass it only if there is at least one pod that",
    "start": "1172679",
    "end": "1178120"
  },
  {
    "text": "is passing its Readiness probe and that's not terminating because from the perspective of the load balancer The",
    "start": "1178120",
    "end": "1183440"
  },
  {
    "text": "Entity that it deals with our nodes uh or network interfaces for nodes there's",
    "start": "1183440",
    "end": "1188480"
  },
  {
    "text": "another way that that we group or that we can group uh nodes that that have serving pods and that's to use Network",
    "start": "1188480",
    "end": "1195360"
  },
  {
    "text": "endpoint groups and I'm only going to show the example with exter external traffic policy local but in this case",
    "start": "1195360",
    "end": "1200960"
  },
  {
    "text": "the same sort of things apply with one exception instead of grouping all the nodes into instance groups what we do is",
    "start": "1200960",
    "end": "1208200"
  },
  {
    "text": "we place only the nodes that have at least one serving pod that is not",
    "start": "1208200",
    "end": "1214640"
  },
  {
    "text": "terminating and that has passed its ringness probe so we kind of get one of these conditions for free uh and of",
    "start": "1214640",
    "end": "1221919"
  },
  {
    "text": "course if there's a Readiness probe but that still has to that still has to pass that doesn't change if you're curious",
    "start": "1221919",
    "end": "1227159"
  },
  {
    "text": "about how can group them using external traffic policy cluster using network endpoint groups you can visit that URL",
    "start": "1227159",
    "end": "1234200"
  },
  {
    "text": "there but I need to sort of move along so if we think about uh using external traffic policy local we want to define a",
    "start": "1234200",
    "end": "1240960"
  },
  {
    "text": "meaningful Readiness probe so that the load bouncer health check uh will",
    "start": "1240960",
    "end": "1246000"
  },
  {
    "text": "actually be IND indicative of whether or not there is a serving pod that that's ready to serve and to think about this",
    "start": "1246000",
    "end": "1253120"
  },
  {
    "text": "we need to think about a couple of timelines here so the load balancers health check will pass or F fail after",
    "start": "1253120",
    "end": "1259480"
  },
  {
    "text": "the Readiness probe passes or fails I'm assuming that there is a radius probe defined and so if we think about from",
    "start": "1259480",
    "end": "1265159"
  },
  {
    "text": "the perspective of a pod when the Pod starts up you can define an initial delay seconds and you can also Define a",
    "start": "1265159",
    "end": "1271440"
  },
  {
    "text": "period seconds and a timeout seconds for the Rous probe and this repeats however",
    "start": "1271440",
    "end": "1276720"
  },
  {
    "text": "many times you define it and you specify a success threshold here we have an example three one other thing I'll",
    "start": "1276720",
    "end": "1283240"
  },
  {
    "text": "mention is that although I don't think the kubernetes API actually enforces this",
    "start": "1283240",
    "end": "1288679"
  },
  {
    "text": "uh for the purposes of of this discussion and for sanity assume that timeout seconds is less than I I should",
    "start": "1288679",
    "end": "1294559"
  },
  {
    "text": "probably have said less than or equal to the period seconds uh just just to sort of keep this simple so if you think",
    "start": "1294559",
    "end": "1300120"
  },
  {
    "text": "about when will the Readiness probe pass well it might pass after if we say the",
    "start": "1300120",
    "end": "1305760"
  },
  {
    "text": "success threshold is three it might pass after two periods and then a few seconds",
    "start": "1305760",
    "end": "1311120"
  },
  {
    "text": "into the third period or it might pass at the end of",
    "start": "1311120",
    "end": "1316440"
  },
  {
    "text": "the timeout seconds with in that third period but a good way to estimate this is if you just say it's whatever the",
    "start": "1316440",
    "end": "1323120"
  },
  {
    "text": "initial delay seconds happens to be plus the product of the success threshold and",
    "start": "1323120",
    "end": "1329200"
  },
  {
    "text": "and the period that way you get sort of an upper bound and that's helpful because there's another timeline that",
    "start": "1329200",
    "end": "1335000"
  },
  {
    "text": "follows this when the load balancer health check passes and that one starts uh at the end of the first timeline and",
    "start": "1335000",
    "end": "1341240"
  },
  {
    "text": "the load balancer has a health check which is defined in very similar way we don't call it period seconds we call it",
    "start": "1341240",
    "end": "1346279"
  },
  {
    "text": "interval and we call it timeout uh but same thing can happen you can define a healthy threshold and in this",
    "start": "1346279",
    "end": "1353039"
  },
  {
    "text": "case we've got two and so Al together the time from starting the Pod to the",
    "start": "1353039",
    "end": "1358240"
  },
  {
    "text": "time when the load balance or health check passes again for external traffic policy local that would be the time when",
    "start": "1358240",
    "end": "1363559"
  },
  {
    "text": "it would accept new connections would be the initial delay seconds plus the product of the success threshold and the",
    "start": "1363559",
    "end": "1370559"
  },
  {
    "text": "period seconds for the Readiness probe and also the product of the interval and",
    "start": "1370559",
    "end": "1375600"
  },
  {
    "text": "the healthy threshold for the load balancers health check and so with that we can have a picture",
    "start": "1375600",
    "end": "1382120"
  },
  {
    "text": "of what an active back end happens to be for a load balancer from the load bouncers perspective the nodes that pass",
    "start": "1382120",
    "end": "1389559"
  },
  {
    "text": "the load bouncer health check Are All That Matters To Be A to be an active back end and the node grouping method",
    "start": "1389559",
    "end": "1395360"
  },
  {
    "text": "and external traffic policy work together so if we go back to life of a packet for just a minute and we think",
    "start": "1395360",
    "end": "1401120"
  },
  {
    "text": "external traffic policy local we think about a cluster that has three nodes two nodes have serving pods none of the pods",
    "start": "1401120",
    "end": "1407320"
  },
  {
    "text": "are terminating and all of the pods pass the Readiness check so we're kind of in a steady state",
    "start": "1407320",
    "end": "1412840"
  },
  {
    "text": "so we have two nodes with serving pods will pass the load balancer health checks that's what we expect and uh that",
    "start": "1412840",
    "end": "1419919"
  },
  {
    "text": "means that those two nodes are the load balancers active back ends so the picture looks kind of like this here's",
    "start": "1419919",
    "end": "1425600"
  },
  {
    "text": "our request packet and that goes to the load balancer and now we're going to zoom in a little bit on the load balancer logic so inside the load",
    "start": "1425600",
    "end": "1431760"
  },
  {
    "text": "balancer you can think of of there's sort of two two engines two two hash tables that that that are used one is if",
    "start": "1431760",
    "end": "1439400"
  },
  {
    "text": "we have a brand new connection there's no connection tracking table entry created so we have to pick a backend and",
    "start": "1439400",
    "end": "1447400"
  },
  {
    "text": "this is what I like to call the backend selection hash method we refer to it as session Affinity so we need to pick a",
    "start": "1447400",
    "end": "1453600"
  },
  {
    "text": "backend we're going to use session Affinity if you're using a service of type load balancer in gke kubernetes",
    "start": "1453600",
    "end": "1459559"
  },
  {
    "text": "this will be a five Tuple Tuple hash to pick the back end and so you can think of that hash as a big number and we're",
    "start": "1459559",
    "end": "1467159"
  },
  {
    "text": "going to divide it by two because we've got two load balancer healthy nodes and we'll take the remainder so that gives us a slot and the slot would correspond",
    "start": "1467159",
    "end": "1474600"
  },
  {
    "text": "to a network interface of a node and so we might pick say the network interface of node a and then we can populate the",
    "start": "1474600",
    "end": "1481360"
  },
  {
    "text": "connection tracking table with a five Tuple hash of the source IP address Source Port IP protocol by number",
    "start": "1481360",
    "end": "1488559"
  },
  {
    "text": "destination IP address which is the load balcer VIP and destination port and then of course the packet is delivered to to",
    "start": "1488559",
    "end": "1494559"
  },
  {
    "text": "the node where the node will do destination that to send it to to the Pod the Pod will reply and then the node",
    "start": "1494559",
    "end": "1500799"
  },
  {
    "text": "will produce uh will perform sourceap and send it right back and you notice",
    "start": "1500799",
    "end": "1506200"
  },
  {
    "text": "when I sent it back I skipped the load balancer this is direct server return so this is why we let the why we let the",
    "start": "1506200",
    "end": "1511799"
  },
  {
    "text": "node do do the n and why it accepts the packet with the destination match and load balancer VIP now the next time a",
    "start": "1511799",
    "end": "1517640"
  },
  {
    "text": "packet comes in for this connection we've already got a connection tracking table ENT so we don't have to play the",
    "start": "1517640",
    "end": "1523480"
  },
  {
    "text": "backend selection hash method we know where the packet needs to go and the",
    "start": "1523480",
    "end": "1528679"
  },
  {
    "text": "load balancer will deliver it there and of course that's what we get just as before now the connection tracking table",
    "start": "1528679",
    "end": "1535000"
  },
  {
    "text": "entry is an interesting thing to focus on because it's good it lasts as long as",
    "start": "1535000",
    "end": "1540480"
  },
  {
    "text": "there are packets flowing for the connection every routing load balancer has a concept of an idle connection and",
    "start": "1540480",
    "end": "1547520"
  },
  {
    "text": "how long the connection tracking table will last for example if you create a",
    "start": "1547520",
    "end": "1552760"
  },
  {
    "text": "service of type load balancer and use the internal annotation in Google Cloud by default that idle time is 10 minutes",
    "start": "1552760",
    "end": "1558520"
  },
  {
    "text": "but you could manually set it to 16 hours if you create an external service of typ load balancer in in Google Cloud",
    "start": "1558520",
    "end": "1565360"
  },
  {
    "text": "uh that uh connection tracking table lifetime is one minute so this is where",
    "start": "1565360",
    "end": "1570480"
  },
  {
    "text": "some interesting things happen idle connections so when a connection",
    "start": "1570480",
    "end": "1575559"
  },
  {
    "text": "tracking table entry is removed due to the connection being idle so let's",
    "start": "1575559",
    "end": "1580640"
  },
  {
    "text": "assume that's the case the next packet that's processed for that connection is processed as if it's a first packet",
    "start": "1580640",
    "end": "1587039"
  },
  {
    "text": "doesn't matter if there's a sin flag set or not but the next packet of course is not part of a new connection it's part",
    "start": "1587039",
    "end": "1593080"
  },
  {
    "text": "of the previous one and so naturally the question that comes up quite a bit uh",
    "start": "1593080",
    "end": "1598120"
  },
  {
    "text": "from a supportability perspective is when are idle connections problematic and this is something I want to dig into",
    "start": "1598120",
    "end": "1603399"
  },
  {
    "text": "a little bit because this is this is some some places where we've we've been able to provide some advice to people so",
    "start": "1603399",
    "end": "1609320"
  },
  {
    "text": "the easiest place to start is when they're not problematic they're not problematic as long as the number of",
    "start": "1609320",
    "end": "1615200"
  },
  {
    "text": "active backends are constant and so if you imagine kind of walk through the life of a packet I did this in text form",
    "start": "1615200",
    "end": "1621679"
  },
  {
    "text": "because drawing it was hard but if we have a new TCP connection load balancer picks the back end you can kind of",
    "start": "1621679",
    "end": "1627440"
  },
  {
    "text": "imagine my previous animation we're going to use session Affinity to pick the back end we'll create a connection tracking table entry and the connection",
    "start": "1627440",
    "end": "1634559"
  },
  {
    "text": "tracking table entry maps that hash for the for the packet characteristics to the nodes network interface and let's",
    "start": "1634559",
    "end": "1641039"
  },
  {
    "text": "say the packets delivered to the to the to the node and the connection becomes idle for longer than the connection",
    "start": "1641039",
    "end": "1646840"
  },
  {
    "text": "tracking table can tolerate so the connection tracking table entry is evicted but the client and the serving",
    "start": "1646840",
    "end": "1653600"
  },
  {
    "text": "pod on the Node still think the connection is alive so that the left and right parties think the connection is still active the middle party the load",
    "start": "1653600",
    "end": "1660559"
  },
  {
    "text": "balancer has cleared the connection tracking table the next packet is sent from a client so what the load balancer",
    "start": "1660559",
    "end": "1667000"
  },
  {
    "text": "will do is it will pick a backend using the session Affinity AKA backend selection hash algorithm as if this were",
    "start": "1667000",
    "end": "1673840"
  },
  {
    "text": "a new connection again for from the perspective of this hashing algorithm it doesn't matter if it's actually a new",
    "start": "1673840",
    "end": "1679279"
  },
  {
    "text": "connection or whether a sin flag is set good thing here is that the number of active back ends is constant so we pick",
    "start": "1679279",
    "end": "1686720"
  },
  {
    "text": "the same back end and we build an identical connection tracking table entry so not a problem the new",
    "start": "1686720",
    "end": "1693519"
  },
  {
    "text": "connection tracking table entry will route the packets to the same node that the previous uh connection tracking",
    "start": "1693519",
    "end": "1699960"
  },
  {
    "text": "table entry did because their identical entries now the fun part when the idle",
    "start": "1699960",
    "end": "1706480"
  },
  {
    "text": "connections are problema when the number of backends changes and a connection tracking table entry is",
    "start": "1706480",
    "end": "1712640"
  },
  {
    "text": "removed so we'll go through the first Parts new TCP connection pick a back end",
    "start": "1712640",
    "end": "1717760"
  },
  {
    "text": "connection tracking table created okay we route packets to the node connection becomes idle and the connection tracking",
    "start": "1717760",
    "end": "1723799"
  },
  {
    "text": "table entry is evicted now the client and the serving pods still think the",
    "start": "1723799",
    "end": "1729320"
  },
  {
    "text": "connection is active so an next packet is sent but if the number of active",
    "start": "1729320",
    "end": "1734360"
  },
  {
    "text": "backends nodes are is different has changed then the load balancer will pick",
    "start": "1734360",
    "end": "1740360"
  },
  {
    "text": "a back end using a session Affinity hash again and there's a good chance it will pick a different uh different node so a",
    "start": "1740360",
    "end": "1746960"
  },
  {
    "text": "new connection tracking table entry is created but this one's different and let's say that it picks a different node",
    "start": "1746960",
    "end": "1753519"
  },
  {
    "text": "and the first packet that that node receives will be let's assume TCP uh",
    "start": "1753519",
    "end": "1758559"
  },
  {
    "text": "will be a packet that does not carry the sin flag so that's delivered to the NIT of of a new node a new node will be",
    "start": "1758559",
    "end": "1765519"
  },
  {
    "text": "rather nonplused by this so the kernel of the node will issue a TCP reset this is correct Behavior and the part that I",
    "start": "1765519",
    "end": "1772360"
  },
  {
    "text": "think surprises a lot of people because I'm always asked is where do we find this in logs and the answer is you don't",
    "start": "1772360",
    "end": "1777600"
  },
  {
    "text": "it happens at the kernel level so there's nothing in application logs and again the idea is to tell the client to",
    "start": "1777600",
    "end": "1783480"
  },
  {
    "text": "to reestablish a connection so idle connections are problematic when the number of active",
    "start": "1783480",
    "end": "1788760"
  },
  {
    "text": "backends changes and there's uh no connection tracking table entry present so I've kind of come up with a grid of",
    "start": "1788760",
    "end": "1794760"
  },
  {
    "text": "advice that we've used kind of internally in Google uh if the total number of nodes in your cluster varies",
    "start": "1794760",
    "end": "1800840"
  },
  {
    "text": "but the number of nodes with serving pods is constant then the goal is to",
    "start": "1800840",
    "end": "1805960"
  },
  {
    "text": "keep the active backend count stable and you should use external traffic policy local because that's the part that's",
    "start": "1805960",
    "end": "1812600"
  },
  {
    "text": "stable only the nodes that are passing the the load balancers health check if the total nodes is constant but the",
    "start": "1812600",
    "end": "1819279"
  },
  {
    "text": "number of serving pods is variable same goal but we're going to use external traffic policy cluster I suppose I could",
    "start": "1819279",
    "end": "1825039"
  },
  {
    "text": "have put a fourth column here if they're both con but we'll say that's a strict subset of the left column there so again",
    "start": "1825039",
    "end": "1831720"
  },
  {
    "text": "here we're we're we're using external traffic policy and we're going to try to keep the connection uh uh or we're going",
    "start": "1831720",
    "end": "1838039"
  },
  {
    "text": "to try to have a situation where if the connection goes idle a new connection tracking table is entry entry is created",
    "start": "1838039",
    "end": "1844440"
  },
  {
    "text": "and it's identical to the first but this is the real fun one when both are variable and this is unavoidable",
    "start": "1844440",
    "end": "1850399"
  },
  {
    "text": "sometimes so the goal here is don't let the connection go idle and for that you",
    "start": "1850399",
    "end": "1855799"
  },
  {
    "text": "can use either external traffic policy local but you want to use TCP keep Alives and I would say if in doubt use",
    "start": "1855799",
    "end": "1863000"
  },
  {
    "text": "TCP keep Alives and if we think about TCP keep Alives just a little bit uh",
    "start": "1863000",
    "end": "1868440"
  },
  {
    "text": "this is something that is also I I find is a little bit confused when I say A",
    "start": "1868440",
    "end": "1874559"
  },
  {
    "text": "TCP keep alive I don't mean an HTTP keep alive that's a TCP Idol when I'm talking",
    "start": "1874559",
    "end": "1879799"
  },
  {
    "text": "about a TCP keep alive you can follow the link there it's a situation where the client in the server will",
    "start": "1879799",
    "end": "1885159"
  },
  {
    "text": "periodically send a zero payload package to to pump the connection tracking table and keep it active so to do this",
    "start": "1885159",
    "end": "1892159"
  },
  {
    "text": "application code has to open a socket with the keep alive option set and then you can configure two keep alive",
    "start": "1892159",
    "end": "1898480"
  },
  {
    "text": "parameters either using uh kernel defaults or at the time that the that the socket is created and each of these",
    "start": "1898480",
    "end": "1905399"
  },
  {
    "text": "things have a different meaning the first one uh defines the amount of time from the last packet to when the first",
    "start": "1905399",
    "end": "1910760"
  },
  {
    "text": "keep alive packet is sent and the second defines the interval thereafter and of course there's also a parameter for",
    "start": "1910760",
    "end": "1917360"
  },
  {
    "text": "controlling how many keep lives are sent and there's a couple different ways to do this so you can see here's here's how",
    "start": "1917360",
    "end": "1922559"
  },
  {
    "text": "to set kernel defaults if CIS cuddle is available in your container you can use that uh or you can write uh to to one of",
    "start": "1922559",
    "end": "1929399"
  },
  {
    "text": "those paths in the in the product file system uh and and this is namespace so you you can do this uh from within a",
    "start": "1929399",
    "end": "1935399"
  },
  {
    "text": "within a container uh here's an example and and there's also an example from uh",
    "start": "1935399",
    "end": "1941279"
  },
  {
    "text": "I decided to pick sort of one of the more complex examples uh big shout out to our friends at F5 here for this",
    "start": "1941279",
    "end": "1946600"
  },
  {
    "text": "example it's on their support site uh this is ISO and Envoy as an example um",
    "start": "1946600",
    "end": "1952120"
  },
  {
    "text": "you can configure this using an Envoy filter uh and I would not have guessed how to do this myself because it's a",
    "start": "1952120",
    "end": "1958919"
  },
  {
    "text": "little bit a little bit complicated but here's the option that sets uh keep alive here's the option that sets the",
    "start": "1958919",
    "end": "1965440"
  },
  {
    "text": "time to the first keep alive packet and here is the the interval so you can do this in the application code or you can",
    "start": "1965440",
    "end": "1971200"
  },
  {
    "text": "do this uh at the kernel layer if your application code uh opens up a socket",
    "start": "1971200",
    "end": "1976279"
  },
  {
    "text": "and doesn't specify custom parameters and finally if we want to talk about other situations where things",
    "start": "1976279",
    "end": "1983799"
  },
  {
    "text": "might change like if you're rolling out increasing or decreasing the number of serving pods uh during during a roll out",
    "start": "1983799",
    "end": "1990960"
  },
  {
    "text": "so let's just assume that total nodes uh is variable and the number of of pods",
    "start": "1990960",
    "end": "1996320"
  },
  {
    "text": "with serving nodes is also variable and we already covered we don't want to let the connection go idle so our second",
    "start": "1996320",
    "end": "2002000"
  },
  {
    "text": "goal is to allow existing connections to close naturally uh because in time you",
    "start": "2002000",
    "end": "2007279"
  },
  {
    "text": "do a roll out you're going to terminate pods so let's think about terminating pods so we want the load balancer health check to fail quickly in order to repel",
    "start": "2007279",
    "end": "2013799"
  },
  {
    "text": "new connections and we need the serving pod to keep processing packets uh for a",
    "start": "2013799",
    "end": "2019200"
  },
  {
    "text": "duration that we find reasonable to meet your needs for existing connections uh even after the load balancer health",
    "start": "2019200",
    "end": "2025360"
  },
  {
    "text": "check has failed and I'd like to break goal two up into two pieces we want to fail the load balancer health check",
    "start": "2025360",
    "end": "2031559"
  },
  {
    "text": "quickly and again remember how the load bouncer health check varies based on external traffic policy and we also want",
    "start": "2031559",
    "end": "2038240"
  },
  {
    "text": "to keep uh processing packets for a reasonable amount of time here is a a",
    "start": "2038240",
    "end": "2043880"
  },
  {
    "text": "sort of three trains leave the station when when we're terminating a pod and these three trains are these three",
    "start": "2043880",
    "end": "2050240"
  },
  {
    "text": "timelines happen uh in in in series with each other so or sorry in parallel with",
    "start": "2050240",
    "end": "2055760"
  },
  {
    "text": "each other so not in series uh we have the amount of time that it takes for the pre-stop uh execution to finish before",
    "start": "2055760",
    "end": "2062679"
  },
  {
    "text": "Sig term we've got termination grace period seconds and then of course we've got the load balance interval and unhealthy count and so",
    "start": "2062679",
    "end": "2070158"
  },
  {
    "text": "anytime while the load balancer health check is still passing but the Pod is terminating there's a nice kubernetes",
    "start": "2070159",
    "end": "2077280"
  },
  {
    "text": "enhancement called kep 1669 proxy terminating end points which does the right thing it tries to Route packets to",
    "start": "2077280",
    "end": "2083599"
  },
  {
    "text": "a terminating pod if as a last resort and then after the load bounce or health check has failed you can sort of",
    "start": "2083599",
    "end": "2091240"
  },
  {
    "text": "manipulate the the pre-stop execution time and termination grace period seconds some software will will stop",
    "start": "2091240",
    "end": "2097440"
  },
  {
    "text": "processing packets at the Sig term others at the Sig kill so just a quick",
    "start": "2097440",
    "end": "2102520"
  },
  {
    "text": "summary here if your software stops at Sig term uh modify your pre-stop",
    "start": "2102520",
    "end": "2107760"
  },
  {
    "text": "execution time and make termination grace period seconds sufficiently long uh otherwise just termination grace",
    "start": "2107760",
    "end": "2113400"
  },
  {
    "text": "period seconds and you want to kind of adhere to this so those that's the relationship but the load balancer",
    "start": "2113400",
    "end": "2119520"
  },
  {
    "text": "health check time to unhealthy is sort of your lower bound so if you know the load bounce your health check will fail after you know five let's say 5 Seconds",
    "start": "2119520",
    "end": "2127200"
  },
  {
    "text": "uh intervals two of them that's about 10 seconds so you know your pre-stop needs to be at least 10 seconds potentially",
    "start": "2127200",
    "end": "2133359"
  },
  {
    "text": "longer however you decide that needs to be to bleed off existing connections and then uh termination grace period seconds",
    "start": "2133359",
    "end": "2140520"
  },
  {
    "text": "uh is your upper bound and I think I've got this right on the nose as far as time so I hope uh hope you all enjoyed",
    "start": "2140520",
    "end": "2147760"
  },
  {
    "text": "this thank you very much",
    "start": "2147760",
    "end": "2151040"
  },
  {
    "text": "yeah",
    "start": "2153760",
    "end": "2156760"
  }
]