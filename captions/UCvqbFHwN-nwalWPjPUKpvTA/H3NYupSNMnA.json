[
  {
    "text": "hey everybody we are live um I'll start I'll start on time in a minute I know I",
    "start": "120",
    "end": "6399"
  },
  {
    "text": "want to give the extra minute for the people to find the room but like I was saying thanks everyone",
    "start": "6399",
    "end": "14160"
  },
  {
    "text": "for finding this meeting room now that they shut down the gates from the exibit hall um it's an adventure getting",
    "start": "14160",
    "end": "23279"
  },
  {
    "text": "here so here we go we're starting um welcome to my talk my name is Jess",
    "start": "23279",
    "end": "29039"
  },
  {
    "text": "beleta uh actually a um Technical Community Advocate uh at is surveillant",
    "start": "29039",
    "end": "35320"
  },
  {
    "text": "now Cisco um and I'm here to talk to you I'm going to read the title the key",
    "start": "35320",
    "end": "40360"
  },
  {
    "text": "value of SD over custom resources scalability for",
    "start": "40360",
    "end": "45480"
  },
  {
    "text": "[Music] now so why am I talking about scalability I am not a practitioner I do",
    "start": "45480",
    "end": "52600"
  },
  {
    "text": "not run kubernetes clusters I'm not a maintainer for any of the projects I'm going to talk about I am associated with",
    "start": "52600",
    "end": "58680"
  },
  {
    "text": "celium um um as a technical Advocate and so that",
    "start": "58680",
    "end": "63840"
  },
  {
    "text": "means I am concerned about making sure endusers and the project are aligning right and I talking about this because",
    "start": "63840",
    "end": "71479"
  },
  {
    "text": "there is an opportunity for practitioners people with technical expertise running clusters to help all",
    "start": "71479",
    "end": "78439"
  },
  {
    "text": "the cncf projects tackle the issue of scalability uh by providing testing and",
    "start": "78439",
    "end": "83960"
  },
  {
    "text": "I'm going to use celium as an example of what I'm talking about um this is a celium uh State scalability story I'm",
    "start": "83960",
    "end": "92200"
  },
  {
    "text": "not talking about evf or the superpower that it is to provide performance inside your nose I'm talking about how you um",
    "start": "92200",
    "end": "100799"
  },
  {
    "text": "how the state of celium State impacts scalability um inside a cluster and",
    "start": "100799",
    "end": "108759"
  },
  {
    "text": "across multiple clusters so this is mostly for the people on watching the video and not for",
    "start": "108759",
    "end": "115520"
  },
  {
    "text": "everyone here uh this is the cast of characters these are the things I'll be mentioning um that impact uh the",
    "start": "115520",
    "end": "122000"
  },
  {
    "text": "scalability story for selum and probably much of it uh will impact other",
    "start": "122000",
    "end": "127360"
  },
  {
    "text": "projects um so celium is an ebpf powered U connectivity observability and",
    "start": "127360",
    "end": "132560"
  },
  {
    "text": "security platform uh for kubernetes clusters uh like I said I'm not going to go into the details of evf but evf is uh",
    "start": "132560",
    "end": "140400"
  },
  {
    "text": "uh basically makes your kernel the L kernel programmable and selum takes",
    "start": "140400",
    "end": "145959"
  },
  {
    "text": "advantage of that to do enhanced um networking performance uh inside your kubernetes nodes right so from that",
    "start": "145959",
    "end": "152720"
  },
  {
    "text": "you're able to get U transparent encryption Advanced load balancing transparent observability um all of that",
    "start": "152720",
    "end": "158959"
  },
  {
    "text": "stuff falls out of the fact that we are using EF underneath and building capabilities on top of it if you're not",
    "start": "158959",
    "end": "165000"
  },
  {
    "text": "familiar with celium this is not a sales pitch uh but please check out the project if you're not familiar with it but I'm talking about it here because",
    "start": "165000",
    "end": "172239"
  },
  {
    "text": "that's the project I'm most familiar with and I'm going to talk about some of the scalability issues uh associated",
    "start": "172239",
    "end": "177680"
  },
  {
    "text": "with uh custom crd or custom resource use by this",
    "start": "177680",
    "end": "182920"
  },
  {
    "text": "project so the other project that's the key player in this story is the actual kubernetes API server right it we all",
    "start": "182920",
    "end": "190840"
  },
  {
    "text": "interact with this when we are maintaining or customizing our kubernetes clusters uh it provides a",
    "start": "190840",
    "end": "197319"
  },
  {
    "text": "suite of features uh one of those is the um the custom resource definitions by",
    "start": "197319",
    "end": "203599"
  },
  {
    "text": "which other components can extend the API to include new resources that they can then uh oper operators uh can manage",
    "start": "203599",
    "end": "211439"
  },
  {
    "text": "those resources and do exciting new things inside of your cluster right so",
    "start": "211439",
    "end": "216920"
  },
  {
    "text": "you know ultimately uh all KU resources are exposed via this API and backed by",
    "start": "216920",
    "end": "222400"
  },
  {
    "text": "Keys uh in an ETD Val key value",
    "start": "222400",
    "end": "227319"
  },
  {
    "text": "store which brings me to ETD this is the Beating Heart of kinetes and I have an",
    "start": "228080",
    "end": "233480"
  },
  {
    "text": "opinion it uh statement at the bottom I think ETD is sort of an unsung hero inside the project landscape",
    "start": "233480",
    "end": "240480"
  },
  {
    "text": "almost it's probably the most downloaded cncf project because almost all almost",
    "start": "240480",
    "end": "246079"
  },
  {
    "text": "all kubernetes clusters are going to end up using this and it is how we",
    "start": "246079",
    "end": "252400"
  },
  {
    "text": "collectively keep up with State inside the cluster so that um you get a every",
    "start": "252400",
    "end": "258639"
  },
  {
    "text": "node gets a consistent view of the resources um and that means um you uh",
    "start": "258639",
    "end": "266080"
  },
  {
    "text": "should be able to rely on it when you're doing crds and it turns out out we can't",
    "start": "266080",
    "end": "272199"
  },
  {
    "text": "at the scales of some of our users are using uh kubernetes and cyan manage",
    "start": "272199",
    "end": "277840"
  },
  {
    "text": "kubernetes nodes um clusters kubernetes um custom resource",
    "start": "277840",
    "end": "285160"
  },
  {
    "text": "definitions uh this is um the thing that uh other components can add to the",
    "start": "285160",
    "end": "291680"
  },
  {
    "text": "kubernetes API server so you can access all the greatness that is baked in",
    "start": "291680",
    "end": "296840"
  },
  {
    "text": "community IP server so I just want to mention it this is this is cium and other projects will extend the API uh so",
    "start": "296840",
    "end": "303280"
  },
  {
    "text": "that you can use the exact same mechanisms that you would do for core objects like nodes or Services selum has",
    "start": "303280",
    "end": "309759"
  },
  {
    "text": "a whole Fleet of of uh crds that we um um add to the API to get things done",
    "start": "309759",
    "end": "318800"
  },
  {
    "text": "most of that is just config management but a couple of those things are sort of critical uh to how the celum cni",
    "start": "318800",
    "end": "326400"
  },
  {
    "text": "operates so like I said this is a story act one scalability in a single celium",
    "start": "326400",
    "end": "333360"
  },
  {
    "text": "managed cluster so the state of celum State you know what the celum keep as state so",
    "start": "333360",
    "end": "340360"
  },
  {
    "text": "there's no local state which is what you need to hold on to if you restart the celium agent so you can continue for the",
    "start": "340360",
    "end": "347479"
  },
  {
    "text": "evf programs can continue to do their work on a node and they can actually route um packets from one node outside",
    "start": "347479",
    "end": "354440"
  },
  {
    "text": "of the world that is uh doesn't have to be kept globally it's uh it's sort of of",
    "start": "354440",
    "end": "359759"
  },
  {
    "text": "a solve problem uh configuration State the bulk of what state uh celum needs is",
    "start": "359759",
    "end": "365720"
  },
  {
    "text": "sort of um policy right human managed uh state things that you will edit to",
    "start": "365720",
    "end": "371400"
  },
  {
    "text": "configure how celum operates um celium Network policy egress policy things",
    "start": "371400",
    "end": "377720"
  },
  {
    "text": "like that these These are held in crds uh and it works just fine it's not going",
    "start": "377720",
    "end": "382960"
  },
  {
    "text": "to be a scalability issue that we have to deal with cluster mesh state which I will talk that's a I'm setting up the",
    "start": "382960",
    "end": "389840"
  },
  {
    "text": "the loaded gun to talk to in the second act right so cluster mesh state is uh information that that selum will",
    "start": "389840",
    "end": "396039"
  },
  {
    "text": "propagate across clusters when you're using celium cluster mesh I'll talk about that in the second act but right",
    "start": "396039",
    "end": "401440"
  },
  {
    "text": "now I want to talk about cni State the information that celium has to propagate",
    "start": "401440",
    "end": "406919"
  },
  {
    "text": "across nodes to get basic cni connectivity so that pods uh across your",
    "start": "406919",
    "end": "412840"
  },
  {
    "text": "network or across your cluster can talk to each other and you can enforce U Network",
    "start": "412840",
    "end": "418520"
  },
  {
    "text": "policy it's actually a pretty thin amount of State uh celium has a concept",
    "start": "418520",
    "end": "424080"
  },
  {
    "text": "called identities which is how psyllium it's sort of a core concept it's how cium is able to efficiently apply uh",
    "start": "424080",
    "end": "430639"
  },
  {
    "text": "Network policy so that um multiple pods with the same labels anywhere in your",
    "start": "430639",
    "end": "436720"
  },
  {
    "text": "cluster get the same identity and then Network policy is apply to that identity so multiple pods can have can share",
    "start": "436720",
    "end": "444000"
  },
  {
    "text": "identity but each of these pods have a unique psyllium endpoint um resource",
    "start": "444000",
    "end": "450120"
  },
  {
    "text": "associated with them um and that's so endpoints are unique identities are shared but all of this has to be uh",
    "start": "450120",
    "end": "457479"
  },
  {
    "text": "consistent across your cluster and all the nodes so that um the different celum",
    "start": "457479",
    "end": "462840"
  },
  {
    "text": "agents opera on each node uh agree on how to handle uh traffic flow inside the",
    "start": "462840",
    "end": "469680"
  },
  {
    "text": "cluster but this is critical state it has to propagate for connectivity to",
    "start": "469680",
    "end": "476639"
  },
  {
    "text": "work so when scaling um celium manage clusters up there are two modes",
    "start": "477960",
    "end": "484400"
  },
  {
    "text": "basically of of installing psyllium U such that you can address scale right uh",
    "start": "484400",
    "end": "490199"
  },
  {
    "text": "the default mode is crd mode where everything those two entities I should point out these these two these two",
    "start": "490199",
    "end": "496479"
  },
  {
    "text": "resources are just handled like everything else in their crd you're going to use the kapi server and that's",
    "start": "496479",
    "end": "502639"
  },
  {
    "text": "the default right now um out of the boxes is what you're going to get but it turns out that if you want to go up to",
    "start": "502639",
    "end": "509680"
  },
  {
    "text": "10,000 nodes and thank you um large language model uh workload uh",
    "start": "509680",
    "end": "517680"
  },
  {
    "text": "users uh the the external SCD uh KV store mode is there to get us past 5,000",
    "start": "517680",
    "end": "524000"
  },
  {
    "text": "nodes and up to 10,000 modes reliably and so this is uh so why do we why do we",
    "start": "524000",
    "end": "529760"
  },
  {
    "text": "need that right um You Like I said this is this is a critical state that has to propagate and if you hit a problem with",
    "start": "529760",
    "end": "536760"
  },
  {
    "text": "the K API server you have some sort of backup up there some sort of pressure it can significantly uh uh impact your",
    "start": "536760",
    "end": "544240"
  },
  {
    "text": "connectivity here so this is uh this is a way around uh bottlenecks associated",
    "start": "544240",
    "end": "549480"
  },
  {
    "text": "with the K API server once you're operating at this scale uh just a historic note this was",
    "start": "549480",
    "end": "556240"
  },
  {
    "text": "actually the first mode that was implemented because psyllium is older than crds are right I think psyllium",
    "start": "556240",
    "end": "562760"
  },
  {
    "text": "psyllium uh 0.9 came out rough through the same time that kubernetes officially",
    "start": "562760",
    "end": "567839"
  },
  {
    "text": "released crds as a as a core component uh in 2017 uh in the summer of",
    "start": "567839",
    "end": "575440"
  },
  {
    "text": "so so should you be relying on kubernetes trds",
    "start": "575440",
    "end": "581360"
  },
  {
    "text": "absolutely you should not be doing it this way the it should absolutely be",
    "start": "581360",
    "end": "587519"
  },
  {
    "text": "such that you can rely on the kuat's API server to do this um it's interesting",
    "start": "587519",
    "end": "593200"
  },
  {
    "text": "though the documentation actually when they talk about whether you should actually build custom resources and or",
    "start": "593200",
    "end": "599680"
  },
  {
    "text": "or build your own API with their own Primitives scalability is not one of the things they have you consider like no",
    "start": "599680",
    "end": "605519"
  },
  {
    "text": "one anticipated this being like a real problem it's just in the real world it",
    "start": "605519",
    "end": "611160"
  },
  {
    "text": "is a problem that celium users are facing and that's why the etz mode is still there um so you know is using ETD",
    "start": "611160",
    "end": "620480"
  },
  {
    "text": "directly worth it um General answer uh it's more flexible but it comes at",
    "start": "620480",
    "end": "626800"
  },
  {
    "text": "operational complexity right because there's a lot of there's a lot of um time and investment in the kapi server",
    "start": "626800",
    "end": "634720"
  },
  {
    "text": "to manage the STD that backs it right there that it caches in front for",
    "start": "634720",
    "end": "640000"
  },
  {
    "text": "caching reads there actually does some it actually does some uh life cycle management for the STD store itself in",
    "start": "640000",
    "end": "645079"
  },
  {
    "text": "terms of compaction and uh making it easier for you to do upgrades or the SD",
    "start": "645079",
    "end": "650320"
  },
  {
    "text": "server that kpi server is backed by you give all of that up when you uh need to",
    "start": "650320",
    "end": "658839"
  },
  {
    "text": "use uh this ATD mode in selum and the only reason why you do it is because",
    "start": "658839",
    "end": "663959"
  },
  {
    "text": "you're avoiding that performance bottle NEX so that you don't lose connectivity",
    "start": "663959",
    "end": "669920"
  },
  {
    "text": "like I said this is very thin it's actually not very big stage but it's critical",
    "start": "669920",
    "end": "675800"
  },
  {
    "text": "right so what are those uh what are those challenges or those bottlenecks that have",
    "start": "675800",
    "end": "681519"
  },
  {
    "text": "historically historically impacted uh the the kubernetes API server well one I",
    "start": "681519",
    "end": "687760"
  },
  {
    "text": "think this has been known for a very long time now now you know uh kubernetes events can be a fire hose in very active",
    "start": "687760",
    "end": "693639"
  },
  {
    "text": "clusters and actually the the a mitigation was added to tune this in API",
    "start": "693639",
    "end": "698760"
  },
  {
    "text": "server so that you can actually specify different SD stores for different parts",
    "start": "698760",
    "end": "704120"
  },
  {
    "text": "of your resources um in the API um and that's actually uh been in there I",
    "start": "704120",
    "end": "710600"
  },
  {
    "text": "forget exactly what released been in there for a while now um the issue I'm going to talk about this one a lot uh",
    "start": "710600",
    "end": "716760"
  },
  {
    "text": "this is sort of the theme actually is that um even though kubernetes Watchers exist and ETD Watchers exist uh Watchers",
    "start": "716760",
    "end": "723440"
  },
  {
    "text": "aren't free and Watcher load associated with a high churn and I will leave that",
    "start": "723440",
    "end": "729600"
  },
  {
    "text": "undefined by now right now uh High churn in your cluster uh can create uh load",
    "start": "729600",
    "end": "737600"
  },
  {
    "text": "not on just the API server but on ETD underneath and in very interesting ways",
    "start": "737600",
    "end": "743760"
  },
  {
    "text": "M well we'll talk about that uh we we we'll talk act two right there Act too",
    "start": "743760",
    "end": "749839"
  },
  {
    "text": "so so for example uh Q proxy ran into this with uh in with end points as you",
    "start": "749839",
    "end": "755839"
  },
  {
    "text": "as you churn pods uh they found that that Q proxy couldn't deal with that",
    "start": "755839",
    "end": "760959"
  },
  {
    "text": "because the object being end up being very large um because you had you had a huge list of endpoints and that created",
    "start": "760959",
    "end": "767959"
  },
  {
    "text": "a bottleneck right uh simply because the objects were quite large they solved that uh by introducing endpoint slic",
    "start": "767959",
    "end": "773399"
  },
  {
    "text": "that's solved it just may not be rolled out in production everywhere but it's a mitigation that exists similarly",
    "start": "773399",
    "end": "779800"
  },
  {
    "text": "cyan its inpoint concept is different but it had the same problem right and that problem even shows up when we're",
    "start": "779800",
    "end": "786920"
  },
  {
    "text": "using external STD it's just that it's not um as critical because you're able",
    "start": "786920",
    "end": "793079"
  },
  {
    "text": "to tune that SD cluster separately from the kapi so um the third one is crds are",
    "start": "793079",
    "end": "800399"
  },
  {
    "text": "not equal citizens with core objects yet in the API server because the core",
    "start": "800399",
    "end": "805880"
  },
  {
    "text": "objects are actually encoded as binary when they're stored in the ATD um key Value Store where crds are",
    "start": "805880",
    "end": "812240"
  },
  {
    "text": "encoded as Json and that actually has a significant uh performance burn it it",
    "start": "812240",
    "end": "818279"
  },
  {
    "text": "may not be enough right now to impact Psy's need to use thatd for users but it",
    "start": "818279",
    "end": "825279"
  },
  {
    "text": "is there and so if you're doing a lot of uh rewrites out of ETD with Ki server",
    "start": "825279",
    "end": "831519"
  },
  {
    "text": "with with the custom resources you will eventually see this I don't know if it's a pacing onum yet but it exists but more",
    "start": "831519",
    "end": "838079"
  },
  {
    "text": "but there's a kept there people are aware of it there's actually a binary data format that they're trying to uh",
    "start": "838079",
    "end": "843800"
  },
  {
    "text": "make the standard moving forward so this exists and so these are bottlenecks uh in the Ki server and they should be all",
    "start": "843800",
    "end": "851639"
  },
  {
    "text": "solved or in the process of being solved in discussion um you know back toyan cni",
    "start": "851639",
    "end": "858839"
  },
  {
    "text": "State you know it churns quickly because every time we touch a pod every time we create a pod we're changing the cni",
    "start": "858839",
    "end": "865240"
  },
  {
    "text": "state uh it's not expected to be human generated just part of the system so",
    "start": "865240",
    "end": "871000"
  },
  {
    "text": "it's actually a pretty good candidate uh to do a dedicated ETD um key value watch",
    "start": "871000",
    "end": "876839"
  },
  {
    "text": "here instead of going through the K uh the API server because we don't need a lot of the features of the API server uh",
    "start": "876839",
    "end": "883079"
  },
  {
    "text": "which are really meant to help you manage as a human like we don't need validation for this stuff for example because celium can just do it we don't",
    "start": "883079",
    "end": "891279"
  },
  {
    "text": "expect a human to ever touch this and you know typo anything so like I said this is more of",
    "start": "891279",
    "end": "898040"
  },
  {
    "text": "a recap talk there was a a great talk in Paris that goes into specifically The Watcher load issues um I highly",
    "start": "898040",
    "end": "905720"
  },
  {
    "text": "encourage you to look at this and and wrap your mind around this uh because it really gets into the question of what Hy",
    "start": "905720",
    "end": "912279"
  },
  {
    "text": "churn is which is contextual but it also talks about the details of um what the",
    "start": "912279",
    "end": "918120"
  },
  {
    "text": "Watcher load problem looks like um and it's a repeating pattern",
    "start": "918120",
    "end": "923680"
  },
  {
    "text": "uh so you know wrap up act one you know etdm mode works but it's not ideal right",
    "start": "923680",
    "end": "931199"
  },
  {
    "text": "we don't want to have to have users do this because it it adds complexity to the celum itself we would rather just",
    "start": "931199",
    "end": "937800"
  },
  {
    "text": "have to rely on the API server um and then uh we don't want to ask cluster",
    "start": "937800",
    "end": "943240"
  },
  {
    "text": "administrators to have a need a higher level of expertise in SD management",
    "start": "943240",
    "end": "949040"
  },
  {
    "text": "right because again there's been investments in kubernetes API server to to uh lower that burden on",
    "start": "949040",
    "end": "956360"
  },
  {
    "text": "people um you know DS are probably the future for most celum users um there's a",
    "start": "956360",
    "end": "963560"
  },
  {
    "text": "desire from both the celum side and you know kubernetes API server side for for",
    "start": "963560",
    "end": "969160"
  },
  {
    "text": "this to be performant right we don't want to have to do this um uh I will say this is a personal opinion cautionary",
    "start": "969160",
    "end": "975440"
  },
  {
    "text": "note you know this Watcher uh based uh data access um pattern could be",
    "start": "975440",
    "end": "984920"
  },
  {
    "text": "everywhere in your cluster if you have if you have customized uh customized components that are using crds and",
    "start": "984920",
    "end": "991759"
  },
  {
    "text": "you're watching them this could still be burning you in a way that psyllium could never know about the API server team",
    "start": "991759",
    "end": "998079"
  },
  {
    "text": "could never know about right because it comes down to a data access uh pattern issue where if you are doing too many",
    "start": "998079",
    "end": "1004959"
  },
  {
    "text": "Watchers uh then you create a a bottleneck for everybody right uh potentially so um I'm expecting that",
    "start": "1004959",
    "end": "1012800"
  },
  {
    "text": "there are some users out there who are pushing the scalable limits right now even the 10,000 nodes probably have",
    "start": "1012800",
    "end": "1017959"
  },
  {
    "text": "other components that that are going to have similar problems which is why when I'm talking about it here it's not I",
    "start": "1017959",
    "end": "1024400"
  },
  {
    "text": "don't think it's just a cyan problem I think it's scalability is a problem for across the landscape and um and as we",
    "start": "1024400",
    "end": "1033400"
  },
  {
    "text": "have uh users pushing scale we're going to see this pop up other",
    "start": "1033400",
    "end": "1038918"
  },
  {
    "text": "places so act two so that was a single cluster and that was more about kapi",
    "start": "1038919",
    "end": "1044880"
  },
  {
    "text": "let's talk about psyllium across uh multiple clusters so so celium has a",
    "start": "1044880",
    "end": "1050200"
  },
  {
    "text": "capability called cluster mesh where you basically are able to run uh celium managed clusters and connect them",
    "start": "1050200",
    "end": "1056720"
  },
  {
    "text": "together in such a way that you're able to Define things like uh Global Services where you have pods in one cluster being",
    "start": "1056720",
    "end": "1063840"
  },
  {
    "text": "able to communicate with backends or for the same service on another cluster it gives you some really interesting use cases like failover or Regional Regional",
    "start": "1063840",
    "end": "1071360"
  },
  {
    "text": "services and a global uh configuration I won't like I said not a sales pitch is",
    "start": "1071360",
    "end": "1076400"
  },
  {
    "text": "sort of what's interesting is like this is a new type of scale right we're scaling past the bounds of a single cluster and we're but we're adding",
    "start": "1076400",
    "end": "1083360"
  },
  {
    "text": "connectivity at the multiple cluster level everything in this cluster mesh can talk to each other potentially",
    "start": "1083360",
    "end": "1089400"
  },
  {
    "text": "encrypted tunnels between everybody um all made available by celium and an evf",
    "start": "1089400",
    "end": "1095080"
  },
  {
    "text": "the magic of vpf um what's interesting is uh the way it's designed is is we're",
    "start": "1095080",
    "end": "1102640"
  },
  {
    "text": "actually bypassing K server a little bit we're using like an embedded ETD proxy",
    "start": "1102640",
    "end": "1107919"
  },
  {
    "text": "here right uh to do that cross cross cluster State we're actually taking a",
    "start": "1107919",
    "end": "1114480"
  },
  {
    "text": "little bit of information from each cluster replicating into thisd and then",
    "start": "1114480",
    "end": "1119679"
  },
  {
    "text": "having readers across clusters uh watching these these different SD stores right so every cluster has a dedicated",
    "start": "1119679",
    "end": "1126440"
  },
  {
    "text": "ETD store and every node every celium agent is watching every cluster right so",
    "start": "1126440",
    "end": "1134400"
  },
  {
    "text": "agents on all nodes watch everything so so assuming you have clusters with 200",
    "start": "1134400",
    "end": "1139799"
  },
  {
    "text": "nodes each you're talking about 1,00 Watchers for each ETD so if you have so",
    "start": "1139799",
    "end": "1145799"
  },
  {
    "text": "that's there are 10 clusters there there's 10d um uh servers running that",
    "start": "1145799",
    "end": "1151400"
  },
  {
    "text": "maintain in and so each one of those has 1,00 Watchers for you know for all the nodes",
    "start": "1151400",
    "end": "1157039"
  },
  {
    "text": "across um so you know why not expose the kapi server well this is cross-cluster I",
    "start": "1157039",
    "end": "1164120"
  },
  {
    "text": "I think this is a I think partly it's a design issue you don't want to do that you don't want to you don't want to expose your Ki server across clusters",
    "start": "1164120",
    "end": "1171120"
  },
  {
    "text": "like that uh but when this is really designed uh it ends up being prophetic",
    "start": "1171120",
    "end": "1176360"
  },
  {
    "text": "uh because it actually prevents uh a scalability issue that's come up and has been mitigated with a new beta feature",
    "start": "1176360",
    "end": "1183080"
  },
  {
    "text": "but I'll get to that right now so just to give you some sense of scale",
    "start": "1183080",
    "end": "1190080"
  },
  {
    "text": "of the scalability issue that could pop up here cluster mesh allows 255 clusters",
    "start": "1190080",
    "end": "1195240"
  },
  {
    "text": "to talk to each other with 5,000 I should say 10,000 I just said that we can 10,000 right so you know that's over",
    "start": "1195240",
    "end": "1201640"
  },
  {
    "text": "a million nodes that's 38 million pods that's over a million Watchers per",
    "start": "1201640",
    "end": "1208120"
  },
  {
    "text": "ETD instance that's too much that's going to blow up right if someone",
    "start": "1208120",
    "end": "1213480"
  },
  {
    "text": "actually attempt to do that it would blow up in fact it blows up way below that but uh you know what is the actual",
    "start": "1213480",
    "end": "1220919"
  },
  {
    "text": "achievable limits here good question because it really comes down to this",
    "start": "1220919",
    "end": "1226559"
  },
  {
    "text": "churn term which I have not defined because churn is contextual and in the case of cluster mesh it's not pod churn",
    "start": "1226559",
    "end": "1232799"
  },
  {
    "text": "but it's actually node churn if you bring up too many nodes too fast you end up causing a really big problem for",
    "start": "1232799",
    "end": "1238640"
  },
  {
    "text": "these etds uh servers and the load becomes too much and CPU saturates and then everything sort of stops uh like I",
    "start": "1238640",
    "end": "1246480"
  },
  {
    "text": "said this is becomes I should say everything this is across clusters everything inside a single cluster is still running but you're but these cool",
    "start": "1246480",
    "end": "1253480"
  },
  {
    "text": "global scale services that cluster mesh allows to be defined stop working as you",
    "start": "1253480",
    "end": "1258679"
  },
  {
    "text": "expect because the state is not consistent across all the Clusters again it's a matter of scaling the state not",
    "start": "1258679",
    "end": "1264960"
  },
  {
    "text": "not the actual functionality of on kernel itself right",
    "start": "1264960",
    "end": "1271840"
  },
  {
    "text": "so what end up happening is we had to redesign that data access or I should I",
    "start": "1271840",
    "end": "1277240"
  },
  {
    "text": "should say my co-workers or my the maintainers of cium uh the the very",
    "start": "1277240",
    "end": "1282279"
  },
  {
    "text": "expansive Wii right uh xyum uh basically had to redesign uh this feature to",
    "start": "1282279",
    "end": "1288600"
  },
  {
    "text": "mentioned uh to reduce the amount of Watchers so instead of instead of having every node talk to every cluster what",
    "start": "1288600",
    "end": "1296200"
  },
  {
    "text": "happens is now clusters uh talk to each other and then nodes just talk to the",
    "start": "1296200",
    "end": "1303799"
  },
  {
    "text": "ETD inside of its own cluster uh so what happens is 255 clusters theoretical Max",
    "start": "1303799",
    "end": "1311120"
  },
  {
    "text": "goes down from 1 million Watchers per Servo to just 5K Watchers or basically a",
    "start": "1311120",
    "end": "1317159"
  },
  {
    "text": "little bit more than the number of in each cluster right so so that is absolutely possible now right uh that is",
    "start": "1317159",
    "end": "1325120"
  },
  {
    "text": "a significant benefit the the trade-off there is that it takes a little longer for for state to populate because you",
    "start": "1325120",
    "end": "1331159"
  },
  {
    "text": "now have to have um each of the operators um basically talking to each",
    "start": "1331159",
    "end": "1338400"
  },
  {
    "text": "of the oper other operators essentially the operators uh but nothing Falls over you're not saturating any of those",
    "start": "1338400",
    "end": "1344159"
  },
  {
    "text": "STDs now it just takes you've traded off a little bit of initial propagation lat",
    "start": "1344159",
    "end": "1349200"
  },
  {
    "text": "to reli Rel reliability but again this is uh this is the same design pattern or",
    "start": "1349200",
    "end": "1354480"
  },
  {
    "text": "the same data access pattern that ultimately uh impacted Q proxy and celum",
    "start": "1354480",
    "end": "1359880"
  },
  {
    "text": "on a single cluster where there were too many Watchers um doing too much and um",
    "start": "1359880",
    "end": "1367000"
  },
  {
    "text": "overwhelmed the ability to to respond to that but in this case there's no kapi server this is just STD by itself so",
    "start": "1367000",
    "end": "1373559"
  },
  {
    "text": "it's the same dat data access pattern even though we've we've cut the the",
    "start": "1373559",
    "end": "1379120"
  },
  {
    "text": "kubernetes API are out the loop so so STD doesn't necessarily solve the problem but a redesign of the data",
    "start": "1379120",
    "end": "1384640"
  },
  {
    "text": "access does and this I think is what I really want to stress is like this could be happening everywhere right where",
    "start": "1384640",
    "end": "1391240"
  },
  {
    "text": "we're using Watchers inside the landscape and it's just a matter of what scale you hit this and you might say",
    "start": "1391240",
    "end": "1398720"
  },
  {
    "text": "yeah we'll never hit that scale uh I don't I don't think people anticipated the scale we're operating at today 5",
    "start": "1398720",
    "end": "1404720"
  },
  {
    "text": "years ago when some of this stuff was designed so it may be happening and so users may be scratching their heads at",
    "start": "1404720",
    "end": "1409960"
  },
  {
    "text": "it because it's it it's contextually dependent on what churn causes a problem and I",
    "start": "1409960",
    "end": "1416720"
  },
  {
    "text": "can't tell you what that churn will be like you you that's projects can't tell us what that churn will be sort of users",
    "start": "1416720",
    "end": "1422640"
  },
  {
    "text": "have to hit it first and when we're all scratching our heads and then we have to figure out how to simulate these ridiculously large",
    "start": "1422640",
    "end": "1429799"
  },
  {
    "text": "workloads so that brings me to the epilogue like why am I up here like you",
    "start": "1429799",
    "end": "1435039"
  },
  {
    "text": "know scalability testing I think is important but it's also so super hard",
    "start": "1435039",
    "end": "1440200"
  },
  {
    "text": "and it's also it's also something project maintainers don't have the resour we",
    "start": "1440200",
    "end": "1446000"
  },
  {
    "text": "can't just replicate these workloads in testing right you can't do it but you",
    "start": "1446000",
    "end": "1451799"
  },
  {
    "text": "can maybe simulate them you can maybe maybe simulate them right like a million",
    "start": "1451799",
    "end": "1459840"
  },
  {
    "text": "nodes right like or or when mil when when you have to when 30 nodes are going",
    "start": "1460039",
    "end": "1465840"
  },
  {
    "text": "up and down in 10 minutes across a global mult cluster landscape like we can't test that in the in the same way",
    "start": "1465840",
    "end": "1472559"
  },
  {
    "text": "that users experience it but maybe we can simulate it right um and like I said",
    "start": "1472559",
    "end": "1477600"
  },
  {
    "text": "I want to point out to this other talk and this was from a year ago Chicago where the person who was working on KV",
    "start": "1477600",
    "end": "1483600"
  },
  {
    "text": "store mes did a very good job explaining how to sim how they were able to reproduce this user experience uh by",
    "start": "1483600",
    "end": "1491080"
  },
  {
    "text": "doing a simulated workload at scale or a simulated workload that created the the",
    "start": "1491080",
    "end": "1496760"
  },
  {
    "text": "necessary churn uh to introduce the problem so that it could be addressed in",
    "start": "1496760",
    "end": "1502120"
  },
  {
    "text": "a redesign right uh so it's a matter of being able to consistently see these",
    "start": "1502120",
    "end": "1507440"
  },
  {
    "text": "problems in a testing environment so that it can be addressed uh by the maintainers no mean nobody wants these",
    "start": "1507440",
    "end": "1513399"
  },
  {
    "text": "problems to exist but man they're sometimes real hard to catch so and so I",
    "start": "1513399",
    "end": "1519840"
  },
  {
    "text": "say celium needs more and better scalability test but I think all projects probably could use help here",
    "start": "1519840",
    "end": "1525000"
  },
  {
    "text": "this is a great opportunity for technical contrib rors who don't necessarily understand the code base but",
    "start": "1525000",
    "end": "1531880"
  },
  {
    "text": "understand their workloads can actually help here like uh some of the scalability testing may be as simple as",
    "start": "1531880",
    "end": "1538399"
  },
  {
    "text": "some fun bash grips right uh you don't have to be for us you don't have to be an ebpf expert it's is not an EF issue",
    "start": "1538399",
    "end": "1544640"
  },
  {
    "text": "this is really uh uh about adding scalability testing to catch uh critical",
    "start": "1544640",
    "end": "1552320"
  },
  {
    "text": "churn in your environment that are happening now so so that's my parting thoughts like",
    "start": "1552320",
    "end": "1559240"
  },
  {
    "text": "you know scalability is a journey you know it's hard to anticipate where you're going to hit the next scaling problem I don't think anybody who are",
    "start": "1559240",
    "end": "1565880"
  },
  {
    "text": "designing these uh data access patterns expected this right like I know fory I'm",
    "start": "1565880",
    "end": "1571200"
  },
  {
    "text": "like very very interested in the UPF to solve some performance issues inside the node I don't think anyone anticipated",
    "start": "1571200",
    "end": "1578480"
  },
  {
    "text": "this being the next problem to be experienced right um you know churn uh",
    "start": "1578480",
    "end": "1585360"
  },
  {
    "text": "is critical but it's also difficult to Define so it's diffult to test for uh because it's contextual to what an",
    "start": "1585360",
    "end": "1591880"
  },
  {
    "text": "individual project is doing uh in our case you know it ended up being in one",
    "start": "1591880",
    "end": "1598320"
  },
  {
    "text": "situation it was pod churn in one situation it was no churn I don't pod churn I think we all anticip think it's",
    "start": "1598320",
    "end": "1603880"
  },
  {
    "text": "a you know we expect right but node churn where you where you have to think oh what happens if a hundred nodes come",
    "start": "1603880",
    "end": "1610440"
  },
  {
    "text": "online within the next minute I don't think anyone thought about that uh",
    "start": "1610440",
    "end": "1615520"
  },
  {
    "text": "across a multicluster it's uh it's a difficult thing so like I said there's",
    "start": "1615520",
    "end": "1620720"
  },
  {
    "text": "an opportunity here to help from the users uh because uh you know your",
    "start": "1620720",
    "end": "1625919"
  },
  {
    "text": "workloads and it's a little bit of effort here uh to add some scalability testing for every project and I don't so",
    "start": "1625919",
    "end": "1633200"
  },
  {
    "text": "if you're using if you're using cium talk to me if you're using anything else talk to your maintainers for the",
    "start": "1633200",
    "end": "1639520"
  },
  {
    "text": "projects you think are critical for your workload uh and see what they're thinking about and maybe you can you can",
    "start": "1639520",
    "end": "1645760"
  },
  {
    "text": "add some expertise and contribute and then maybe you can win an award like Adobe did for contributions yesterday at",
    "start": "1645760",
    "end": "1651320"
  },
  {
    "text": "the uh in the keynote that'd be really cool so uh I think that's it for",
    "start": "1651320",
    "end": "1657200"
  },
  {
    "text": "me um thank you for coming to my talk on the end of the last day of a cubec con I",
    "start": "1657200",
    "end": "1662679"
  },
  {
    "text": "really appreciate it uh I am three minutes ahead of time but we have like seven minutes for questions so if there",
    "start": "1662679",
    "end": "1668679"
  },
  {
    "text": "are questions I can maybe try to answer them otherwise thank you very",
    "start": "1668679",
    "end": "1675640"
  },
  {
    "text": "much there it is he's from five minutes in he was ready to ask a",
    "start": "1676679",
    "end": "1683759"
  },
  {
    "text": "question read writes size of the keys like what's the like you mentioned churn",
    "start": "1686000",
    "end": "1691760"
  },
  {
    "text": "but churn is of how much data did you write or how much right did you did",
    "start": "1691760",
    "end": "1697320"
  },
  {
    "text": "right so I will repeat that because I'm not sure if that mic was hot the question was what was the ultimately the",
    "start": "1697320",
    "end": "1703960"
  },
  {
    "text": "limiting uh consideration is it read writes or is the size of the data",
    "start": "1703960",
    "end": "1709000"
  },
  {
    "text": "what is churn good questions it depends uh my understanding and and I'm I am I",
    "start": "1709000",
    "end": "1715720"
  },
  {
    "text": "am not uh maintainer so I may get this wrong but this is my understanding right now for Q proxy you know in the uh in",
    "start": "1715720",
    "end": "1722519"
  },
  {
    "text": "the itself when they introduced based on my understanding when they introduced endpoint slices ended up being the size",
    "start": "1722519",
    "end": "1729519"
  },
  {
    "text": "of the objects as you added an endpoint uh it ended up being uh you just had a",
    "start": "1729519",
    "end": "1734720"
  },
  {
    "text": "huge array of endpoints for some of these services and moving the updating the service became a problem because the",
    "start": "1734720",
    "end": "1740039"
  },
  {
    "text": "size of each object was big so they add added the slices concept and it helped reduce that for celium it was the amount",
    "start": "1740039",
    "end": "1748440"
  },
  {
    "text": "of read writes I believe uh because the size of the objects I think aren't that",
    "start": "1748440",
    "end": "1754320"
  },
  {
    "text": "of a big of a deal but but that's my that's my understanding right now but um",
    "start": "1754320",
    "end": "1759840"
  },
  {
    "text": "so it can depend and and for uh cluster mesh specifically it was it was the",
    "start": "1759840",
    "end": "1766159"
  },
  {
    "text": "overall amounts of of read reads um",
    "start": "1766159",
    "end": "1771200"
  },
  {
    "text": "because as you bring up 50 nodes you're just asking for things to do a lot of work so like I said like that's and it's",
    "start": "1771200",
    "end": "1778200"
  },
  {
    "text": "different scales like for for the endpoint stuff it was poing right inside of a single cluster for the cluster mesh",
    "start": "1778200",
    "end": "1785240"
  },
  {
    "text": "it was node churn across the entire Global span of the of your of your",
    "start": "1785240",
    "end": "1790960"
  },
  {
    "text": "entire infrastructure uh which is a different different turn for different",
    "start": "1790960",
    "end": "1796720"
  },
  {
    "text": "problems so I can only speak to how works because it's you know because it's uh deeply related to cni but I imagine",
    "start": "1796720",
    "end": "1803960"
  },
  {
    "text": "other projects have other issues where churn uh will show up and cause some",
    "start": "1803960",
    "end": "1809200"
  },
  {
    "text": "sort of performance issue does that answer the question sure than thanks uh oh uh have you looked at",
    "start": "1809200",
    "end": "1819000"
  },
  {
    "text": "um other data stores other than NCD have we looked at other data storage",
    "start": "1819000",
    "end": "1824559"
  },
  {
    "text": "other than thatd I have not uh I wanted to to uh avoid that discussion because",
    "start": "1824559",
    "end": "1829760"
  },
  {
    "text": "it just complicates things a little bit uh I think I think scalability testing",
    "start": "1829760",
    "end": "1834919"
  },
  {
    "text": "will always depend on the technology you use but at some point you will have a",
    "start": "1834919",
    "end": "1840039"
  },
  {
    "text": "problem in a place that you're not well not expecting it right so",
    "start": "1840039",
    "end": "1845679"
  },
  {
    "text": "um so this isn't so much about picking uh going SD isn't good enough I actually",
    "start": "1845679",
    "end": "1852960"
  },
  {
    "text": "think ETD is good enough it was just a matter of the the act the data access patterns we can mess that up with",
    "start": "1852960",
    "end": "1858799"
  },
  {
    "text": "anything right there's a way there's a way to use any data store inefficiently right so it's a matter of catching those",
    "start": "1858799",
    "end": "1866240"
  },
  {
    "text": "patterns in the system yeah I I mean there are other data stores that are",
    "start": "1866240",
    "end": "1871880"
  },
  {
    "text": "have different like consistency guarantees but are more scalable yeah in fact like I I sort of mentioned that I",
    "start": "1871880",
    "end": "1877240"
  },
  {
    "text": "sort of as is like most kubernetes clusters are using SD but I know there are people who are working to to sort of",
    "start": "1877240",
    "end": "1884799"
  },
  {
    "text": "make this a specification so that you don't have to use ATD but but I don't this isn't a technology technology I was",
    "start": "1884799",
    "end": "1891120"
  },
  {
    "text": "more about the patterns of of you know leading to scal this this just happens",
    "start": "1891120",
    "end": "1896760"
  },
  {
    "text": "to be the ones that we're hitting right now in celium right because we just didn't anticipate data access patterns being a",
    "start": "1896760",
    "end": "1904760"
  },
  {
    "text": "problem right because we were focused on ebpf solving some scalability problems in the node so than but I get where",
    "start": "1904760",
    "end": "1913159"
  },
  {
    "text": "you're coming from yeah I get where you're coming from it's just not not this talk I think there was a talk earlier about",
    "start": "1913159",
    "end": "1918519"
  },
  {
    "text": "uh maybe replacing ETD with something else I know I know it's out there so oh",
    "start": "1918519",
    "end": "1924000"
  },
  {
    "text": "there's one later okay yeah 420 there's a uh progress SQL yeah yeah and I'm",
    "start": "1924000",
    "end": "1929279"
  },
  {
    "text": "aware of that I'm but that's you know I I didn't want that to be the focus of the talk I'm not trying to beat up any",
    "start": "1929279",
    "end": "1935159"
  },
  {
    "text": "project this is just about patterns where I think users can help any project",
    "start": "1935159",
    "end": "1940720"
  },
  {
    "text": "uh uh address this right uh if we have tools to kind of",
    "start": "1940720",
    "end": "1945880"
  },
  {
    "text": "play with the scalability of uh I think I wrote one that generated a ton of uhum identity operators is Sig",
    "start": "1945880",
    "end": "1951840"
  },
  {
    "text": "scalability and the slack the right place to come to discuss that or yeah absolutely in fact I mean it it",
    "start": "1951840",
    "end": "1958200"
  },
  {
    "text": "absolutely is um and like I said if if you looked at if you look at Drew's um",
    "start": "1958200",
    "end": "1964240"
  },
  {
    "text": "uh talk from last year right he did some excellent work uh to do a mockup tool",
    "start": "1964240",
    "end": "1970039"
  },
  {
    "text": "too so but yeah and like please please help us in fact like I said every",
    "start": "1970039",
    "end": "1976559"
  },
  {
    "text": "project probably wants users who are doing this some inhouse for your own testing to help contribute so we can get",
    "start": "1976559",
    "end": "1983799"
  },
  {
    "text": "a framework maybe a framework across the landscape where we can started doing this in an organized",
    "start": "1983799",
    "end": "1991080"
  },
  {
    "text": "way hi uh so you mentioned that um you I mean Shard the HD so that you are able",
    "start": "1991600",
    "end": "1998600"
  },
  {
    "text": "to uh send the events to a different HD using the overides oh I I mentioned that",
    "start": "1998600",
    "end": "2005159"
  },
  {
    "text": "as a as a historical mitigation for sort of bottlenecks like you know the API server maintainers you know as these",
    "start": "2005159",
    "end": "2011639"
  },
  {
    "text": "things come up they do mitigate them and I think that's one of the oldest mitigations uh in terms of the design is",
    "start": "2011639",
    "end": "2017440"
  },
  {
    "text": "being able to uh instead of using a single LD you can you can take part of your resource uh set and moving it on it",
    "start": "2017440",
    "end": "2026200"
  },
  {
    "text": "started with just events but then it it actually got more generalized so you can tune it to groups um but you know it's",
    "start": "2026200",
    "end": "2033720"
  },
  {
    "text": "that's the first that was the first mitigation I'm aware of that's true I we do that as well in the gner project uh",
    "start": "2033720",
    "end": "2040600"
  },
  {
    "text": "but what we realized later on is that we can do this only for Native Kus resources but for CS right so is that a",
    "start": "2040600",
    "end": "2047000"
  },
  {
    "text": "pain point for you as well uh well I well I think we avoided that by just doing our own FTD uh but do you see",
    "start": "2047000",
    "end": "2054358"
  },
  {
    "text": "value in being able to do that for CDs natively if let's say CDs are now stored",
    "start": "2054359",
    "end": "2059398"
  },
  {
    "text": "as binary yeah I think I think everyone wins if we if we can make a uh the kuber",
    "start": "2059399",
    "end": "2067000"
  },
  {
    "text": "API server perform it right the first thing is once we start having crds binary encoded that should help that",
    "start": "2067000",
    "end": "2072960"
  },
  {
    "text": "should make it possible or easier and easier right but I agree like if you could take that further the more the",
    "start": "2072960",
    "end": "2078240"
  },
  {
    "text": "more you bake it in into the API server the less of these it's not a hack it's just",
    "start": "2078240",
    "end": "2083878"
  },
  {
    "text": "workarounds um and then we all benefit like the more the more we can drive this API server though and then we we don't",
    "start": "2083879",
    "end": "2090040"
  },
  {
    "text": "have to have individualized Solutions so thank you yep I think that's I got 34",
    "start": "2090040",
    "end": "2095800"
  },
  {
    "text": "seconds hopefully that's the last question thank you everybody for coming to my talk",
    "start": "2095800",
    "end": "2102540"
  },
  {
    "text": "[Applause]",
    "start": "2102540",
    "end": "2106820"
  }
]