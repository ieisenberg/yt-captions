[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "hello thanks for joining us today my name is Ravi Laurie I work on open",
    "start": "30",
    "end": "6240"
  },
  {
    "text": "shift automation and tooling I'm Sebastien drug also work on tooling mostly and go and I also focus on",
    "start": "6240",
    "end": "13019"
  },
  {
    "text": "container runtime performance we are going to talk about automated Cuban at a scalability testing so who are we we are",
    "start": "13019",
    "end": "20820"
  },
  {
    "text": "part of performance and scalability team Metroid had working primarily on open shift the wider performance and",
    "start": "20820",
    "end": "26730"
  },
  {
    "text": "scalability team who works on the entire Red Hat's products portfolio I'm sure",
    "start": "26730",
    "end": "32189"
  },
  {
    "start": "30000",
    "end": "76000"
  },
  {
    "text": "everyone over here might have played around with communities are open shift which is an enterprise version of Cuban",
    "start": "32189",
    "end": "37440"
  },
  {
    "text": "IDs at some point of time quick show of hands how many of you are running kubernetes our open shipped in",
    "start": "37440",
    "end": "43350"
  },
  {
    "text": "production that's a lot of people when running or bishops in production one",
    "start": "43350",
    "end": "49620"
  },
  {
    "text": "might have these questions in their mind does OpenShift support running applications at scale if yes then what",
    "start": "49620",
    "end": "55500"
  },
  {
    "text": "are the cluster limits how can we tune across so to get maximum performance what are the challenges one might race",
    "start": "55500",
    "end": "61320"
  },
  {
    "text": "when managing and running a large and dense cluster we had similar questions in our mind over the next few slides we",
    "start": "61320",
    "end": "68909"
  },
  {
    "text": "are going to talk about how we build and used our automation pipeline and tooling to come up with the answers to these",
    "start": "68909",
    "end": "74850"
  },
  {
    "text": "questions infrastructure is one of the key things we need to test any product",
    "start": "74850",
    "end": "80430"
  },
  {
    "start": "76000",
    "end": "102000"
  },
  {
    "text": "at scale with this mode if we build scale lab it's an initiative to operate",
    "start": "80430",
    "end": "86070"
  },
  {
    "text": "when hats products in their upper limits the scale that consists of three fifty plus nodes about eight thousand plus",
    "start": "86070",
    "end": "92009"
  },
  {
    "text": "course which is a lot of resources and we keep adding more and more physical nodes to this lab every year the",
    "start": "92009",
    "end": "98670"
  },
  {
    "text": "resources are all shared and are available on demand let's talk a bit about our cluster availability we",
    "start": "98670",
    "end": "105750"
  },
  {
    "start": "102000",
    "end": "133000"
  },
  {
    "text": "install 2,000 node cluster once every openshift release and runway Nia's performance and scale that's on top of",
    "start": "105750",
    "end": "111509"
  },
  {
    "text": "it we maintain a 250 node cluster all the time to resolve customer issues to",
    "start": "111509",
    "end": "118860"
  },
  {
    "text": "design new tests for features added upstream and for R&D work we refresh",
    "start": "118860",
    "end": "124409"
  },
  {
    "text": "this 250 notes cluster with the latest and greatest OpenShift which once every sprint our Sprint's are usually three",
    "start": "124409",
    "end": "130649"
  },
  {
    "text": "weeks long by the way we recently completed a 2,000 or",
    "start": "130649",
    "end": "135980"
  },
  {
    "text": "large-scale testing for openshift 3.11 running kubernetes mode not 11 I'll use",
    "start": "135980",
    "end": "141170"
  },
  {
    "text": "this run as a reference to talk about how our automation tool or automation pipeline and tooling works this is how",
    "start": "141170",
    "end": "148219"
  },
  {
    "start": "147000",
    "end": "174000"
  },
  {
    "text": "our automation pipeline looks like the pipeline runs in stages with each stage being an individual jenkees job so all",
    "start": "148219",
    "end": "155359"
  },
  {
    "text": "the blocks over here are separate jenkees jobs and the pipeline controller is responsible for time them up together",
    "start": "155359",
    "end": "161920"
  },
  {
    "text": "the pipeline is flexible enough to allow us to run a particular job instead of",
    "start": "161920",
    "end": "167659"
  },
  {
    "text": "running all the stages by default I'll talk about what each job doesn't detail however the coming slides the first job",
    "start": "167659",
    "end": "175549"
  },
  {
    "start": "174000",
    "end": "208000"
  },
  {
    "text": "in the pipeline is image provisioner this job watches for new OpenShift code bits and builds a.m. eyes and cue cows",
    "start": "175549",
    "end": "182480"
  },
  {
    "text": "with various packages configurations and container images baked into it by doing",
    "start": "182480",
    "end": "188389"
  },
  {
    "text": "this it reduces the in the time taken by the Installer to build out an operative cluster as the Installer need not fetch",
    "start": "188389",
    "end": "195530"
  },
  {
    "text": "any packages or container images over the network during the install time this job has suffered to build images based",
    "start": "195530",
    "end": "201949"
  },
  {
    "text": "on both well and atomic once the image provision is done building an image it",
    "start": "201949",
    "end": "207889"
  },
  {
    "text": "calls the image validator job this job validates the image built by the image",
    "start": "207889",
    "end": "213079"
  },
  {
    "start": "208000",
    "end": "221000"
  },
  {
    "text": "provisioner by installing an all-in-one node OpenShift cluster so at this point we can be sure that the image is good",
    "start": "213079",
    "end": "220840"
  },
  {
    "text": "the next components of pipeline controller and watcher given a properties file which is where we define",
    "start": "220840",
    "end": "227120"
  },
  {
    "start": "221000",
    "end": "270000"
  },
  {
    "text": "the parameters needed to build all the jobs in the pipeline pipeline controller reads it or HTTP and then calls the",
    "start": "227120",
    "end": "234139"
  },
  {
    "text": "respective jobs the pipeline can run multiple jobs in parallel and supports running the jobs on both public and",
    "start": "234139",
    "end": "240709"
  },
  {
    "text": "private clouds we store all the job templates in a public github depository",
    "start": "240709",
    "end": "247189"
  },
  {
    "text": "it's all open source and it's in Djamel by the way so it's easy to edit and let's say we modify a particular job",
    "start": "247189",
    "end": "254629"
  },
  {
    "text": "template or add a new job template to the github repo this is where the",
    "start": "254629",
    "end": "259639"
  },
  {
    "text": "watcher component comes into the picture the watcher component watches for these changes",
    "start": "259639",
    "end": "265080"
  },
  {
    "text": "and creates or updates the jobs and Jenkins now that we have the image and",
    "start": "265080",
    "end": "272729"
  },
  {
    "start": "270000",
    "end": "316000"
  },
  {
    "text": "properties files ready we kickoff the pipeline double OpenStack so why did we choose OpenStack OpenStack provides the",
    "start": "272729",
    "end": "280050"
  },
  {
    "text": "virtualization platform to scale out the OPF cluster to a higher node count so by",
    "start": "280050",
    "end": "285300"
  },
  {
    "text": "using virtual machines we can make efficient use of the lab resources and also can run large scale tests on a",
    "start": "285300",
    "end": "291900"
  },
  {
    "text": "smaller hardware footprint we use Tripler to deploy OpenStack trip flow",
    "start": "291900",
    "end": "297930"
  },
  {
    "text": "basically brings up the OpenStack under cloud which manages the hardware and",
    "start": "297930",
    "end": "303090"
  },
  {
    "text": "installs of the stack over cloud workload is where we host all our open",
    "start": "303090",
    "end": "308520"
  },
  {
    "text": "ships virtual machines Chapleau also installs say accept cluster which manages the storage for all these",
    "start": "308520",
    "end": "314280"
  },
  {
    "text": "OpenStack virtual machines this slide has details about the hardware that we",
    "start": "314280",
    "end": "320039"
  },
  {
    "start": "316000",
    "end": "336000"
  },
  {
    "text": "use for our mm not scale test run we got 84 machines allocated from Skylab",
    "start": "320039",
    "end": "325860"
  },
  {
    "text": "and we converted that to a 2012 open shift cluster impressive right we use",
    "start": "325860",
    "end": "331830"
  },
  {
    "text": "the same flavors as as an AWS the next",
    "start": "331830",
    "end": "337830"
  },
  {
    "start": "336000",
    "end": "415000"
  },
  {
    "text": "stage in the pipeline once the OpenStack cluster is up and running is openshift",
    "start": "337830",
    "end": "342979"
  },
  {
    "text": "everyone uses their own architecture to install and set up openshift",
    "start": "342979",
    "end": "348300"
  },
  {
    "text": "this is how we do it there's a gem nerve or Bastion host from where we control",
    "start": "348300",
    "end": "353460"
  },
  {
    "text": "the install manage the cluster and run wages performance of skilled us there's",
    "start": "353460",
    "end": "358529"
  },
  {
    "text": "a load balancer which balances the request the masters there's a DNS which manages a cluster dns this job installs",
    "start": "358529",
    "end": "366389"
  },
  {
    "text": "the bare minimum operative cluster with three masters it CDs three infra nodes",
    "start": "366389",
    "end": "371490"
  },
  {
    "text": "three storage nodes two app nodes of worker nodes I'll explain the reason for just",
    "start": "371490",
    "end": "378240"
  },
  {
    "text": "installing 11 node cluster in the coming slides most of we call this 11 node",
    "start": "378240",
    "end": "384599"
  },
  {
    "text": "cluster as core cluster most of the nodes in the Cole cluster uses nvme pass-through and that's because at CDs",
    "start": "384599",
    "end": "391650"
  },
  {
    "text": "running on the master nodes elasticsearch running on the interest Infinite's rahat OpenShift container storage",
    "start": "391650",
    "end": "397469"
  },
  {
    "text": "running on the storage notes needs high throughput and low latency for better performance we",
    "start": "397469",
    "end": "404010"
  },
  {
    "text": "also make sure none of the core notes land on the same hypervisor for the cluster to be true hey CheY and also for",
    "start": "404010",
    "end": "410130"
  },
  {
    "text": "the performance and scale test results to be valid the next job in the pipeline",
    "start": "410130",
    "end": "417450"
  },
  {
    "start": "415000",
    "end": "513000"
  },
  {
    "text": "is to set up tooling we built a bunch of open-source tools and cluster loader is",
    "start": "417450",
    "end": "423480"
  },
  {
    "text": "one of the most extensively used tool by us given a yama definition cross the",
    "start": "423480",
    "end": "429390"
  },
  {
    "text": "loader read set and loads of the cluster with various objects this is a sample cursor loader config I will explain what",
    "start": "429390",
    "end": "436050"
  },
  {
    "text": "this config does this this config creates a cluster project 0 name space",
    "start": "436050",
    "end": "442050"
  },
  {
    "text": "and if the name space already exists on the cluster and deletes it and recreates it again or else we can set if exist to",
    "start": "442050",
    "end": "449760"
  },
  {
    "text": "reuse to reuse the same name space and it loads up that name space with thousand passports the step size is set",
    "start": "449760",
    "end": "457410"
  },
  {
    "text": "to 50 and the pause is set to 60 in the tuning sets which means that the cluster loader will create 50 parts in the first",
    "start": "457410",
    "end": "464010"
  },
  {
    "text": "iteration waits for the parts to be up and running and then creates the second",
    "start": "464010",
    "end": "469350"
  },
  {
    "text": "part second set of 50 parts and goes on till like it reaches a thousand part count this is just a simple example of",
    "start": "469350",
    "end": "476550"
  },
  {
    "text": "what clustering can do the things that cluster loader can do are unlimited we can use the cluster loader to load up",
    "start": "476550",
    "end": "482430"
  },
  {
    "text": "the cluster with bunch of open shift templates to create thousands of namespaces over thousands of parts",
    "start": "482430",
    "end": "488670"
  },
  {
    "text": "secret routes etc etc we've also",
    "start": "488670",
    "end": "493680"
  },
  {
    "text": "embedded cross the loader in the atomic help it should test package so those of you that have open shipped already",
    "start": "493680",
    "end": "498840"
  },
  {
    "text": "installed you may or may not have this package installed but yum install it any one step away from loading up your own",
    "start": "498840",
    "end": "505050"
  },
  {
    "text": "cluster with whatever images or objects you like it's also available in upstream kubernetes in case anyone wants to check",
    "start": "505050",
    "end": "511110"
  },
  {
    "text": "it out the next tool that we use extensively is",
    "start": "511110",
    "end": "516450"
  },
  {
    "start": "513000",
    "end": "592000"
  },
  {
    "text": "P bench it's a benchmarking and performance analysis framework pH has",
    "start": "516450",
    "end": "521760"
  },
  {
    "text": "three subsystems P bench Asian which runs on all the OPF nodes on which we",
    "start": "521760",
    "end": "526890"
  },
  {
    "text": "want to capture the performance data P bench asian provides way stools likes our highest add bit stat",
    "start": "526890",
    "end": "534440"
  },
  {
    "text": "and flexible enough to allow us to run our own tool the next P bench agent also",
    "start": "534440",
    "end": "541400"
  },
  {
    "text": "provides benchmark scripts like file new poof and user benchmark so using AP",
    "start": "541400",
    "end": "546920"
  },
  {
    "text": "bench user benchmark we can literally run any script and expect the agent running on the open chef nodes to",
    "start": "546920",
    "end": "552830"
  },
  {
    "text": "collect the tool data the next sub component is the event server once the",
    "start": "552830",
    "end": "559970"
  },
  {
    "text": "agent is run dunning the bender once the it's done running the benchmark it tears up the results ships it out to the",
    "start": "559970",
    "end": "566240"
  },
  {
    "text": "server where the results get unpacked archived and indexed the last subsystem",
    "start": "566240",
    "end": "572210"
  },
  {
    "text": "is the B bench web server it's responsible for visualizing the data collected from the OpenShift nodes I'll",
    "start": "572210",
    "end": "579470"
  },
  {
    "text": "give a simple example to show how pH works let's just enable bit stat tool on",
    "start": "579470",
    "end": "584810"
  },
  {
    "text": "the open ship compute nodes and use pH user benchmark to run the cursor loader config that we saw before at the end of",
    "start": "584810",
    "end": "594200"
  },
  {
    "start": "592000",
    "end": "656000"
  },
  {
    "text": "the run we can see the memory usage of cubelet cryo darker or any other process",
    "start": "594200",
    "end": "600290"
  },
  {
    "text": "running on the opposite nerd when the cluster loaded is loading of the cluster powerful right the next stages in the",
    "start": "600290",
    "end": "610190"
  },
  {
    "text": "pipeline are conformance tests and scale up this job runs cuban√≠a days end-to-end",
    "start": "610190",
    "end": "615470"
  },
  {
    "text": "test and once the conformance is green which scales up the cluster to a desired node count the reason for running the",
    "start": "615470",
    "end": "622700"
  },
  {
    "text": "conformance before scaling up the cluster to 2000 is that let's say we find out that the cluster is bad after",
    "start": "622700",
    "end": "630500"
  },
  {
    "text": "after scaling it up then we'll have to tear it down and build it from scratch again which takes a lot of time so in",
    "start": "630500",
    "end": "636380"
  },
  {
    "text": "order to avoid that we build a core cluster run conformance on top of it and once the cluster looks sane we scale it",
    "start": "636380",
    "end": "643070"
  },
  {
    "text": "up to 2000 the openshift installer is ansible based so we usually set the",
    "start": "643070",
    "end": "649940"
  },
  {
    "text": "number of folks to 50 for the tasks to run in parallel and speed up the scale of time the last stage in the pipeline",
    "start": "649940",
    "end": "658360"
  },
  {
    "start": "656000",
    "end": "687000"
  },
  {
    "text": "is performance and scale test we design various tests to look at the performance",
    "start": "658360",
    "end": "664370"
  },
  {
    "text": "of plain cubelet networking HF proxy and storage we also have bunch of tests in",
    "start": "664370",
    "end": "672050"
  },
  {
    "text": "the pipeline to validate and push the cluster limits we repeat the same",
    "start": "672050",
    "end": "678980"
  },
  {
    "text": "pipeline run with the latest and greatest open shift bits once every sprint to take a look at the performance",
    "start": "678980",
    "end": "684080"
  },
  {
    "text": "regressions the motive behind building the",
    "start": "684080",
    "end": "689420"
  },
  {
    "start": "687000",
    "end": "725000"
  },
  {
    "text": "automation pipeline tooling and the scale lab is not just for our team we",
    "start": "689420",
    "end": "695030"
  },
  {
    "text": "are actively unloading other teams to enable them to test their workload set scale this way they need not have huge",
    "start": "695030",
    "end": "701750"
  },
  {
    "text": "infrastructure they need not install and maintain Alaska coaster with the latest and greatest bits they can reuse the",
    "start": "701750",
    "end": "708860"
  },
  {
    "text": "tooling and automation that we build instead of maintaining their own adding a new workload to the pipeline is as",
    "start": "708860",
    "end": "715130"
  },
  {
    "text": "simple as creating a PR with the right job template and the watcher component will pick it up rhiannon Jenkins job and",
    "start": "715130",
    "end": "722240"
  },
  {
    "text": "adds it to the pipeline Sebastian will take it from here thanks Ravi",
    "start": "722240",
    "end": "730990"
  },
  {
    "start": "725000",
    "end": "740000"
  },
  {
    "text": "so we have our cluster running and then we need to run the tests and let's take",
    "start": "730990",
    "end": "736880"
  },
  {
    "text": "a look at what we found some challenges that we face so this is just a quick",
    "start": "736880",
    "end": "742100"
  },
  {
    "start": "740000",
    "end": "774000"
  },
  {
    "text": "list of some tests that we'll be covering we have a good sized team so we have",
    "start": "742100",
    "end": "748760"
  },
  {
    "text": "specialists in storage networking that take their respective components and make sure that openshift is actually",
    "start": "748760",
    "end": "755380"
  },
  {
    "text": "compliant and optimized in each of these different test fields and since we need to cover all the bases not just the",
    "start": "755380",
    "end": "761600"
  },
  {
    "text": "control plane because the decisions that are made on behalf of Red Hat for the choices in HTTP for the choices in",
    "start": "761600",
    "end": "768740"
  },
  {
    "text": "storage we had we got to make a lot more tests than just regular control plane tests the first test I guess you could",
    "start": "768740",
    "end": "776180"
  },
  {
    "start": "774000",
    "end": "800000"
  },
  {
    "text": "say is really just the scale-up so while you might not think it's a test per se we do need to measure how long it",
    "start": "776180",
    "end": "782090"
  },
  {
    "text": "takes to scale up as well as each step to make sure that it's optimized cached",
    "start": "782090",
    "end": "787760"
  },
  {
    "text": "and not duplicated if obviously if you're scaling to a small node count this all times not that relevant but",
    "start": "787760",
    "end": "794000"
  },
  {
    "text": "when you have 2,000 nodes take if things go wrong several days so",
    "start": "794000",
    "end": "801270"
  },
  {
    "start": "800000",
    "end": "831000"
  },
  {
    "text": "the node vertical is a most basic cubelet density focused test and what it does is create the max pause across the",
    "start": "801270",
    "end": "807180"
  },
  {
    "text": "compute nodes and here you can see the cubelet as well as the container runtime system resources used by both of them",
    "start": "807180",
    "end": "814020"
  },
  {
    "text": "across the versions 3.9 310 and 311 and this is one of the things that we optimize and strive to maintain is no",
    "start": "814020",
    "end": "821790"
  },
  {
    "text": "performance regressions across versions so it's the same test adding bunch of features but you can see things don't",
    "start": "821790",
    "end": "828870"
  },
  {
    "text": "get worse at least Master vertical is the follow-up more advanced version of",
    "start": "828870",
    "end": "835560"
  },
  {
    "start": "831000",
    "end": "888000"
  },
  {
    "text": "node vertical it's a more advanced cluster this density test that doesn't just create pause pods it's not just a",
    "start": "835560",
    "end": "841650"
  },
  {
    "text": "theoretical best-case scenario but actually includes builds secrets routes",
    "start": "841650",
    "end": "846660"
  },
  {
    "text": "and all sorts of openshift objects outside of pods the goal of the massive vertical test is to have a more fully",
    "start": "846660",
    "end": "852779"
  },
  {
    "text": "loaded environment that is more representative of real clusters so here",
    "start": "852779",
    "end": "859650"
  },
  {
    "text": "we have the control plane again the API server the controller as well as at CD with a exponentially increasing project",
    "start": "859650",
    "end": "868440"
  },
  {
    "text": "account and the projects contain the same amount of objects so as you can see as we move to double the project count quadruple the project count the control",
    "start": "868440",
    "end": "876330"
  },
  {
    "text": "plane scales much better than linearly so once again scalability is something that we pay close attention to as we",
    "start": "876330",
    "end": "882600"
  },
  {
    "text": "want to make sure that you can run as many projects as humanly possible with your objects in them the first decision",
    "start": "882600",
    "end": "889320"
  },
  {
    "start": "888000",
    "end": "927000"
  },
  {
    "text": "that we make I guess for you guys in openshift is the choice of an ingress HTTP router what we use in openshift 311",
    "start": "889320",
    "end": "897690"
  },
  {
    "text": "is H a proxy 1.8 so the major feature that we have in a sea of proxy 1.8 is",
    "start": "897690",
    "end": "904230"
  },
  {
    "text": "the router threading and you can see the difference between openshift 310 and 311 in the different versions of HD a proxy",
    "start": "904230",
    "end": "910680"
  },
  {
    "text": "as we add in more concurrent connections the throughput of the a tree proxy router scales quite nicely of course",
    "start": "910680",
    "end": "918120"
  },
  {
    "text": "this is also brought to you by another open sure open source tool called MB then we've created specifically to be",
    "start": "918120",
    "end": "923550"
  },
  {
    "text": "able to saturate these links networking is actually a relatively",
    "start": "923550",
    "end": "929160"
  },
  {
    "start": "927000",
    "end": "989000"
  },
  {
    "text": "complicated topic with openness shift on OpenStack because each of these products",
    "start": "929160",
    "end": "934890"
  },
  {
    "text": "carries their own Sdn so this thing introduces a problem that we would like to refer to a chaise otherwise known as double encapsulation needless to say",
    "start": "934890",
    "end": "942959"
  },
  {
    "text": "that is not great for a footprint performance we use among other tools we",
    "start": "942959",
    "end": "947970"
  },
  {
    "text": "use you perf to benchmark a node to node service the service and pada pada throughput and latency for both the TCP",
    "start": "947970",
    "end": "954149"
  },
  {
    "text": "and ATP connections the way we are working to get around the double",
    "start": "954149",
    "end": "959700"
  },
  {
    "text": "encapsulation is a new product in OpenStack called courier which leverages the network abstraction of OpenStack",
    "start": "959700",
    "end": "965010"
  },
  {
    "text": "Neutron to isolate the networks of the kubernetes cluster in this example we",
    "start": "965010",
    "end": "970470"
  },
  {
    "text": "have the default iptables hybrid Neutron firewall driver compared to the newer I",
    "start": "970470",
    "end": "976230"
  },
  {
    "text": "guess open vSwitch firewall driver but small node sizes open V switch produces",
    "start": "976230",
    "end": "981540"
  },
  {
    "text": "better TCP throughput performance but quickly falls off when the cluster gets larger as for storage our storage",
    "start": "981540",
    "end": "991680"
  },
  {
    "start": "989000",
    "end": "1019000"
  },
  {
    "text": "solution on openshift is cluster so otherwise known as Red Hat open shift",
    "start": "991680",
    "end": "996750"
  },
  {
    "text": "container storage form lay container no dip native storage in case you're not keeping able the names essentially we",
    "start": "996750",
    "end": "1003170"
  },
  {
    "text": "run a pretty decent-sized Postgres or MongoDB install on the cluster and then",
    "start": "1003170",
    "end": "1009290"
  },
  {
    "text": "load it up with PG bench specifically and the improvements at 311",
    "start": "1009290",
    "end": "1014540"
  },
  {
    "text": "the Gluster block performance has increased substantially keeping with the",
    "start": "1014540",
    "end": "1020510"
  },
  {
    "start": "1019000",
    "end": "1075000"
  },
  {
    "text": "theme default installation of open shift does come with Prometheus pre-installed the previous stack is cube state metrics",
    "start": "1020510",
    "end": "1028280"
  },
  {
    "text": "node exporter and at very least alerting so two topics that we cover with",
    "start": "1028280",
    "end": "1034010"
  },
  {
    "text": "Prometheus is the research and development of guidance as to capacity planning for Prometheus we want to",
    "start": "1034010",
    "end": "1040220"
  },
  {
    "text": "enable customers to collect large scale metrics or alternatively maintain 15",
    "start": "1040220",
    "end": "1045980"
  },
  {
    "text": "days of retention so the recommendation here is to have at least one terabyte of",
    "start": "1045980",
    "end": "1051400"
  },
  {
    "text": "general-purpose SSD storage we also applied synthetics large-scale workloads",
    "start": "1051400",
    "end": "1056809"
  },
  {
    "text": "that are based on graph on queries but applied synthetically just to make sure that",
    "start": "1056809",
    "end": "1062390"
  },
  {
    "text": "Prometheus can scale and the next version of openshift not 311 will have",
    "start": "1062390",
    "end": "1067820"
  },
  {
    "text": "Prometheus two and a half which does have some substantial both memory improvements as well as scalability",
    "start": "1067820",
    "end": "1073250"
  },
  {
    "text": "improvements so configuring there's not really another test but it's another tool that we've written one of the",
    "start": "1073250",
    "end": "1081680"
  },
  {
    "start": "1075000",
    "end": "1122000"
  },
  {
    "text": "challenges that we're always striving to improve is the reality of the load test right we started like I said back with",
    "start": "1081680",
    "end": "1088730"
  },
  {
    "text": "node vertical and then master vertical and now we have the config mirror and what this enables us to do is",
    "start": "1088730",
    "end": "1094370"
  },
  {
    "text": "essentially try to extract all the objects that we can from large-scale clusters like open shift online and take",
    "start": "1094370",
    "end": "1101360"
  },
  {
    "text": "out the user data and then transfer them onto our scale lab or other large piece of hardware this enables us to debug",
    "start": "1101360",
    "end": "1109040"
  },
  {
    "text": "performance issues that customers see is well on the field because we don't often have access to their environments and we",
    "start": "1109040",
    "end": "1115790"
  },
  {
    "text": "certainly can't reproduce all of their configurations so this is one of our neighbor tools that enable us to do so",
    "start": "1115790",
    "end": "1122230"
  },
  {
    "start": "1122000",
    "end": "1149000"
  },
  {
    "text": "so as with upstream we've been closely tracking the cluster limits and each of",
    "start": "1122230",
    "end": "1128000"
  },
  {
    "text": "these fields pause per node pause for core namespaces we have tests specifically designed most of them using",
    "start": "1128000",
    "end": "1133940"
  },
  {
    "text": "cluster loader has this very trivially easily to generate whatever you want namespaces or the pods or any object and",
    "start": "1133940",
    "end": "1139700"
  },
  {
    "text": "load it up to hit these but essentially every release at very minimum we track and verify in the cluster limits so",
    "start": "1139700",
    "end": "1149360"
  },
  {
    "start": "1149000",
    "end": "1181000"
  },
  {
    "text": "higher limits recently open shift we started diverging a bit from upstream as our pod per namespace the third seeing",
    "start": "1149360",
    "end": "1156650"
  },
  {
    "text": "some issues for kubernetes start seeing some issues with pods per namespace the API server started slowing down bogging",
    "start": "1156650",
    "end": "1163190"
  },
  {
    "text": "down and we didn't see those same issues so given everything we do eventually or",
    "start": "1163190",
    "end": "1168380"
  },
  {
    "text": "we try to make everything land upstream some things that make it but we're working with sig's scalability as I'm a",
    "start": "1168380",
    "end": "1174170"
  },
  {
    "text": "member of them to try to figure out why they're why we have this difference right now",
    "start": "1174170",
    "end": "1181690"
  },
  {
    "start": "1181000",
    "end": "1258000"
  },
  {
    "text": "is it possible to run to a 500 pods per node so we currently support 250 pounds",
    "start": "1183380",
    "end": "1188480"
  },
  {
    "text": "per node theoretically it is now possible to run 500 pounds per node if your application does nothing so yes of",
    "start": "1188480",
    "end": "1198200"
  },
  {
    "text": "course getting there there were some challenges even with DNS mask it didn't like it especially if you have a small",
    "start": "1198200",
    "end": "1204890"
  },
  {
    "text": "cluster you have like 1 2 or 3 worker nodes 500 pods per node sure 2000 pods or 2 thousand nodes later",
    "start": "1204890",
    "end": "1210500"
  },
  {
    "text": "that's not gonna work it out so it is possible to run 500 pods per node but",
    "start": "1210500",
    "end": "1215960"
  },
  {
    "text": "the issue becomes when you have real applications real containers running on a node it gets a lot more complicated",
    "start": "1215960",
    "end": "1223330"
  },
  {
    "text": "first start is the first thing that happens is the container runtime seems to be the weakness at this point",
    "start": "1223330",
    "end": "1229180"
  },
  {
    "text": "preventing higher node counts as the plug latency will keep increasing as you have more applicative real applications",
    "start": "1229180",
    "end": "1236030"
  },
  {
    "text": "not just the control plane applications the plague latency which is a good indicator of health for the container",
    "start": "1236030",
    "end": "1241910"
  },
  {
    "text": "runtime will keep increasing over time and eventually the nodes will become not ready and things just escalate from",
    "start": "1241910",
    "end": "1248870"
  },
  {
    "text": "there the scheduler will say we just lost a node we can reschedule 500 pods or whatever it might be and there goes",
    "start": "1248870",
    "end": "1255530"
  },
  {
    "text": "your cluster tuning so we'd like to take a layered approach to tuning at address",
    "start": "1255530",
    "end": "1261260"
  },
  {
    "start": "1258000",
    "end": "1311000"
  },
  {
    "text": "performance issues as best as we can we try to optimize for performance at the lowest level which is the code level we",
    "start": "1261260",
    "end": "1267290"
  },
  {
    "text": "work very closely with engineering teams at it right had obviously and all of our work does eventually for the most part",
    "start": "1267290",
    "end": "1274820"
  },
  {
    "text": "github stream next we try to take tuning on so if you're running ro you",
    "start": "1274820",
    "end": "1281570"
  },
  {
    "text": "definitely out of the box get the tune D daemon essentially what this is is a daemon that runs on your system and",
    "start": "1281570",
    "end": "1287480"
  },
  {
    "text": "automatically applies our best practices our best tuning knobs to your nodes",
    "start": "1287480",
    "end": "1292820"
  },
  {
    "text": "based on whatever profile you select so if you have a storage node you can select for throughput or i/o not both",
    "start": "1292820",
    "end": "1299180"
  },
  {
    "text": "like someone was asking for earlier at the booth if you have an open shift",
    "start": "1299180",
    "end": "1304220"
  },
  {
    "text": "master node it's all tuned out of the box so you just need to select it it's set at boot and you don't worry about it",
    "start": "1304220",
    "end": "1310990"
  },
  {
    "text": "of course you can take this further if you have special needs for example the API server has",
    "start": "1310990",
    "end": "1316770"
  },
  {
    "start": "1311000",
    "end": "1339000"
  },
  {
    "text": "PS and burst rates out of the box we set this in as same defaults for any size",
    "start": "1316770",
    "end": "1323070"
  },
  {
    "text": "cluster but if you have these monster iron rigs with 96 cores and four",
    "start": "1323070",
    "end": "1330660"
  },
  {
    "text": "different sockets then we can do a little better if you up the QPS and burst rates the scalability does",
    "start": "1330660",
    "end": "1336300"
  },
  {
    "text": "drastically increase but at some cost so every major release we update our",
    "start": "1336300",
    "end": "1342210"
  },
  {
    "start": "1339000",
    "end": "1374000"
  },
  {
    "text": "scaling and performance guide and this guide covers all the topics that we discussed everything from the",
    "start": "1342210",
    "end": "1347880"
  },
  {
    "text": "installation to nibbles the API server overrides that I just discussed the cubelet configs to increase the density",
    "start": "1347880",
    "end": "1354690"
  },
  {
    "text": "the networking and routing optimization as well as newer features that we're working at working with and adding to",
    "start": "1354690",
    "end": "1361890"
  },
  {
    "text": "kubernetes to support running your actual workload and tuning your workload to make sure that you get the best",
    "start": "1361890",
    "end": "1367500"
  },
  {
    "text": "performance for your applications such as the CPU manager huge pages and now GP is so like everything we do our code is",
    "start": "1367500",
    "end": "1378180"
  },
  {
    "text": "all open source and upstream good bad or ugly it's here so if you want to take a",
    "start": "1378180",
    "end": "1383400"
  },
  {
    "text": "look run it kick the tires a bit or fork it and contribute it's up to you guys",
    "start": "1383400",
    "end": "1388610"
  },
  {
    "text": "what's nice already we are planning to test operators at scale which is the",
    "start": "1388610",
    "end": "1394800"
  },
  {
    "start": "1390000",
    "end": "1428000"
  },
  {
    "text": "buzzword for this conference by the way and we are working on onboarding more and more teams into a pipeline to take",
    "start": "1394800",
    "end": "1401580"
  },
  {
    "text": "advantage of the automation infrastructure including that we build and we will continue to push towards",
    "start": "1401580",
    "end": "1407160"
  },
  {
    "text": "higher cluster limits and we are also working on adding support to the pipeline to run in various public clouds",
    "start": "1407160",
    "end": "1414840"
  },
  {
    "text": "it does have support children these Kael tha's in any cloud since you solve containerized its install phase that you",
    "start": "1414840",
    "end": "1421920"
  },
  {
    "text": "are working on thank you next",
    "start": "1421920",
    "end": "1428060"
  },
  {
    "start": "1428000",
    "end": "1914000"
  },
  {
    "text": "so before we jump on to the Q&A session",
    "start": "1428060",
    "end": "1435720"
  },
  {
    "text": "I just have one last thing to say we are hiring the famous last words please feel",
    "start": "1435720",
    "end": "1441780"
  },
  {
    "text": "free to reach out to us to know more about the interesting things we are",
    "start": "1441780",
    "end": "1447330"
  },
  {
    "text": "working at Red Hat any questions",
    "start": "1447330",
    "end": "1451970"
  },
  {
    "text": "[Music]",
    "start": "1458250",
    "end": "1461369"
  },
  {
    "text": "yep yep",
    "start": "1464550",
    "end": "1468500"
  },
  {
    "text": "no it's on the lab results are all shared like it's in the lab is like huge like we just get like some notes",
    "start": "1471060",
    "end": "1477640"
  },
  {
    "text": "allocated once every open ship release we take them install up and start open stack on top of it and then run our",
    "start": "1477640",
    "end": "1483610"
  },
  {
    "text": "pipeline and then we just give away the machines once we are done we use the",
    "start": "1483610",
    "end": "1493840"
  },
  {
    "text": "same VM sizes like same machines with like same memory resources CPUs and",
    "start": "1493840",
    "end": "1499000"
  },
  {
    "text": "everything all the time so that like we can get a run to run comparison between the previous runs you mean on the OPF",
    "start": "1499000",
    "end": "1508240"
  },
  {
    "text": "cluster yeah we manually tune the Cubist and buster's for a cluster so one one time we want them when we are running a",
    "start": "1508240",
    "end": "1516070"
  },
  {
    "text": "test which created ten thousand namespaces with like lot of objects secrets routes and all that like the API",
    "start": "1516070",
    "end": "1523540"
  },
  {
    "text": "server got overloaded and then we had to like tune the QPS and bus rates like",
    "start": "1523540",
    "end": "1529000"
  },
  {
    "text": "double double them up quadrille them and to like get the cluster back to run and state so this didn't happen on the 250",
    "start": "1529000",
    "end": "1536020"
  },
  {
    "text": "node cluster which we usually maintain every sprint and that's because there are just 250 nodes hitting API server",
    "start": "1536020",
    "end": "1541780"
  },
  {
    "text": "instead of two thousand nodes so depending on the node size like we do we do some tunings on the cluster yeah",
    "start": "1541780",
    "end": "1553620"
  },
  {
    "text": "[Music]",
    "start": "1565960",
    "end": "1569008"
  },
  {
    "text": "yes we have a number of different test rate so in different stages of reality we do try to make this as real as",
    "start": "1575299",
    "end": "1582270"
  },
  {
    "text": "possible and we do have I guess like whatever database specific tests and all sorts of different tests that would have",
    "start": "1582270",
    "end": "1588270"
  },
  {
    "text": "state as well sorry good idea",
    "start": "1588270",
    "end": "1593539"
  },
  {
    "text": "yeah so it's that's more of an upstream",
    "start": "1599830",
    "end": "1606140"
  },
  {
    "text": "test and it doesn't work with all the open shift objects so right now we're",
    "start": "1606140",
    "end": "1611840"
  },
  {
    "text": "moving in upstream as well I believe we're moving away from cue mark we're rewriting the upstream cluster loader to",
    "start": "1611840",
    "end": "1620330"
  },
  {
    "text": "add the additional features also I'm pretty sure cube mark is the hollow pod",
    "start": "1620330",
    "end": "1626270"
  },
  {
    "text": "one it doesn't run on real like pods and nodes it just creates like virtual ones so all of these tests are on real",
    "start": "1626270",
    "end": "1632210"
  },
  {
    "text": "Hardware using real nodes and all that so it we're really trying to strive for reality and that's where we come with",
    "start": "1632210",
    "end": "1638000"
  },
  {
    "text": "above the tunings that Robby mentioned is just actual issues that we encounter",
    "start": "1638000",
    "end": "1643270"
  },
  {
    "text": "yeah",
    "start": "1643420",
    "end": "1646420"
  },
  {
    "text": "yeah so since that's not GA we can't discuss that at the point but we might",
    "start": "1655440",
    "end": "1662909"
  },
  {
    "text": "definitely maybe yeah but we'll have any Muslim yeah",
    "start": "1662909",
    "end": "1672840"
  },
  {
    "text": "so I think that started with the we have a large performance of scalability group",
    "start": "1687560",
    "end": "1693410"
  },
  {
    "text": "that doesn't just work on openshift specifically it works across all relics so we needed something that was",
    "start": "1693410",
    "end": "1699290"
  },
  {
    "text": "pluggable and modular to work with all the basic tools because I'm sure you guys are familiar with that chart of",
    "start": "1699290",
    "end": "1705320"
  },
  {
    "text": "Linux performance measurement tools so we needed something that could capture anything and everything and could be",
    "start": "1705320",
    "end": "1711620"
  },
  {
    "text": "easily extensible and so that's why we wrote P bench was just we didn't want to",
    "start": "1711620",
    "end": "1717860"
  },
  {
    "text": "have some sort of like tailor-made stuff it's pretty easy to just add a new tool to P bench and capture it and record it",
    "start": "1717860",
    "end": "1725210"
  },
  {
    "text": "and compare it so really just came down to extensibility and standard tooling",
    "start": "1725210",
    "end": "1731200"
  },
  {
    "text": "the room yeah it's quite flexible like we can literally like create a script",
    "start": "1731200",
    "end": "1736910"
  },
  {
    "text": "right now which collects OC get nodes and then it's it and we can add it to P",
    "start": "1736910",
    "end": "1742910"
  },
  {
    "text": "bench and then it will collect OC get nodes every three seconds or ten seconds depending on the interval while your",
    "start": "1742910",
    "end": "1748100"
  },
  {
    "text": "test is running so we can accelerate and stop like depending on the other tools",
    "start": "1748100",
    "end": "1754090"
  },
  {
    "text": "Stram it's up",
    "start": "1759879",
    "end": "1763079"
  },
  {
    "text": "[Music]",
    "start": "1777300",
    "end": "1780390"
  },
  {
    "text": "yeah of course of course yeah so sixgill ability just trying to sorry going",
    "start": "1797530",
    "end": "1808110"
  },
  {
    "text": "I would say we don't have plans but that's something that I do care more",
    "start": "1836369",
    "end": "1842009"
  },
  {
    "text": "about so we should talk about why that is maybe offline or whatever not on video",
    "start": "1842009",
    "end": "1849019"
  },
  {
    "text": "yeah we do report thanks to six scalability so recently we ran a test to",
    "start": "1849019",
    "end": "1857190"
  },
  {
    "text": "see if we can increase the pods per namespace more than like it's three",
    "start": "1857190",
    "end": "1863789"
  },
  {
    "text": "thousand right now so upstream lower two limits so it without like why not create a test and then try to see like if we",
    "start": "1863789",
    "end": "1869999"
  },
  {
    "text": "can increase the number of parts per namespace so we found out that it can be increased so we reported the results back to six scalability so our plan is",
    "start": "1869999",
    "end": "1879570"
  },
  {
    "text": "basically to get the change upstream first and then basically get that into o P shift instead of just modifying it in",
    "start": "1879570",
    "end": "1886169"
  },
  {
    "text": "the efficient level yeah [Music]",
    "start": "1886169",
    "end": "1892779"
  },
  {
    "text": "[Applause] yeah we haven't changed any offer limits so we are trying to get it merged",
    "start": "1893320",
    "end": "1899570"
  },
  {
    "text": "upstream and then get it down to a publisher all right thank you very much",
    "start": "1899570",
    "end": "1912730"
  }
]