[
  {
    "text": "here are some here are a couple of light-hearted jokes about observability why did the metric go to",
    "start": "680",
    "end": "7040"
  },
  {
    "text": "therapy because it had too many issues with its",
    "start": "7040",
    "end": "11480"
  },
  {
    "text": "counterparts you're not booing me you're booing the AI and it will remember",
    "start": "13120",
    "end": "19760"
  },
  {
    "text": "this why was the log file so popular at parties because it always had the latest",
    "start": "19760",
    "end": "25800"
  },
  {
    "text": "gossip down to the Last Detail remember humor can be quite subjective so what's",
    "start": "25800",
    "end": "30840"
  },
  {
    "text": "funny for one person may not be for another especially in fields of specialized as",
    "start": "30840",
    "end": "37520"
  },
  {
    "text": "observability how are we on that mic working on",
    "start": "37680",
    "end": "43399"
  },
  {
    "text": "it let's let's go some more the AI will remember that",
    "start": "44480",
    "end": "53038"
  },
  {
    "text": "too there we",
    "start": "58920",
    "end": "64359"
  },
  {
    "text": "go you know what it's not answering so I think it heard you I think you heard I think you heard chat gpd's feelings okay",
    "start": "65920",
    "end": "73080"
  },
  {
    "text": "we got we got one more all right this is this one's pretty good how do you comfort a JavaScript bug",
    "start": "73080",
    "end": "81400"
  },
  {
    "text": "you console it why did the time series database",
    "start": "81400",
    "end": "88439"
  },
  {
    "text": "break up with the alert manager it was tired of its constant overreactions to a little bit of",
    "start": "88439",
    "end": "95280"
  },
  {
    "text": "Jitter finally why did the developer go broke because they used up all their",
    "start": "96640",
    "end": "103640"
  },
  {
    "text": "cash again you're you're hurting GPT Force feelings here",
    "start": "105040",
    "end": "110719"
  },
  {
    "text": "everyone all right I think we already testing testing",
    "start": "110719",
    "end": "116039"
  },
  {
    "text": "Can You Hear Me yep all right take it away so all right I'm going to rescue from",
    "start": "116039",
    "end": "121840"
  },
  {
    "text": "that um okay uh so this is going to be kind of",
    "start": "123600",
    "end": "129800"
  },
  {
    "text": "like let's take you real quick through sampling 101 and then I want to talk",
    "start": "129800",
    "end": "135000"
  },
  {
    "text": "about some Advanced sampling techniques so so to start we just kind of need to level set a little bit about sampling",
    "start": "135000",
    "end": "140840"
  },
  {
    "text": "what it means that kind of stuff so imagine you have a big bag of marbles and you want to know how many marbles",
    "start": "140840",
    "end": "146959"
  },
  {
    "text": "are in it so you're going to pick a few of them weigh them multiply divide you",
    "start": "146959",
    "end": "153280"
  },
  {
    "text": "know and then you weigh the bag you weigh the handful you have you count the handful you have and then you can calculate the ratio and get the weight",
    "start": "153280",
    "end": "159560"
  },
  {
    "text": "of the bag now you want to know how many black ones are in that bag well you pick",
    "start": "159560",
    "end": "164720"
  },
  {
    "text": "a random sample and you count the fraction of those that you sampled um compared to how many you",
    "start": "164720",
    "end": "171959"
  },
  {
    "text": "believe are in the whole bag and then multiply by that number and you will get a reasonable approximation and it kind",
    "start": "171959",
    "end": "179120"
  },
  {
    "text": "of depends on the the size of your sample right if you have a bag of 10,000 Marbles and you pull 10 out you're only",
    "start": "179120",
    "end": "185080"
  },
  {
    "text": "going to get to within the nearest thousand or so but if you take more of them out in your sample you take as many",
    "start": "185080",
    "end": "191599"
  },
  {
    "text": "as you can and you get you know you get closer to the right answer um one thing I want to address",
    "start": "191599",
    "end": "199080"
  },
  {
    "text": "when I talk about sampling is to be slightly pedantic about it to sample means to choose a sample so you keep the",
    "start": "199080",
    "end": "208280"
  },
  {
    "text": "sample people often say well I'm sampling out these things or I'm sampling heavily meaning I take many",
    "start": "208280",
    "end": "215159"
  },
  {
    "text": "fewer of them and I try to really avoid those terms um because it just it's confusing people you know when you start",
    "start": "215159",
    "end": "222080"
  },
  {
    "text": "using those words people don't know that that's what you're you're you know which side you're going on and you always end",
    "start": "222080",
    "end": "228200"
  },
  {
    "text": "up getting confusing so I really prefer to use the words keep and drop um when",
    "start": "228200",
    "end": "233280"
  },
  {
    "text": "you need to be clear and if I mess up in this talk it's because I'm human I do the same as everybody else but the other",
    "start": "233280",
    "end": "239200"
  },
  {
    "text": "piece of it is sampling rate sampling probability and to some extent sampling",
    "start": "239200",
    "end": "244920"
  },
  {
    "text": "threshold are all different ways of talking about the same thing you have a",
    "start": "244920",
    "end": "250040"
  },
  {
    "text": "fraction of of items and you're estimating how you you're you're taking",
    "start": "250040",
    "end": "255360"
  },
  {
    "text": "a subset of them what is that subset so if you're choosing one in 10 your sampling rate is 10 your sampling",
    "start": "255360",
    "end": "262639"
  },
  {
    "text": "probability is 10% or 01 and um you know you're sampling",
    "start": "262639",
    "end": "269759"
  },
  {
    "text": "threshold is either you know 90 or 10 depending on how you do it",
    "start": "269759",
    "end": "275720"
  },
  {
    "text": "but those those kinds of numbers uh will all come up when we when we talk about",
    "start": "275720",
    "end": "280960"
  },
  {
    "text": "this stuff in general um I suppose I should tell you who I am um I work for",
    "start": "280960",
    "end": "286720"
  },
  {
    "text": "honeycomb uh staff engineer and I'm the lead on the project we call Refinery",
    "start": "286720",
    "end": "291960"
  },
  {
    "text": "which is Honeycombs sampling proxy and what I'm going to be talking about today is a bunch of the things that um we can",
    "start": "291960",
    "end": "299240"
  },
  {
    "text": "do in the honeycomb sampling proxy um for a really Advanced kind of sampling",
    "start": "299240",
    "end": "305039"
  },
  {
    "text": "but we're going to start with some Basics so you're managing a network of",
    "start": "305039",
    "end": "311160"
  },
  {
    "text": "microservices and you can imagine that there are a lot of them and a request comes in from a customer and that thing",
    "start": "311160",
    "end": "317560"
  },
  {
    "text": "talks to your web service which talks to your login service which talks to your database and all these things go flying",
    "start": "317560",
    "end": "323160"
  },
  {
    "text": "around the world and you're getting all of these Services sending we're going to column spans uh",
    "start": "323160",
    "end": "331319"
  },
  {
    "text": "log messages however you want to do it but basically they're recording what's happening and passing that",
    "start": "331319",
    "end": "336400"
  },
  {
    "text": "on um so you can assemble all of those requests together and make a trace and",
    "start": "336400",
    "end": "343639"
  },
  {
    "text": "say when this happened and all these other things happened and this is actually what honeycomb does for a living um and so this I'm just going to",
    "start": "343639",
    "end": "352000"
  },
  {
    "text": "uh show you this more live um so this is a trace in honeycomb so you can see the",
    "start": "352000",
    "end": "359520"
  },
  {
    "text": "request came in to the checkout service and you can see that there are different lengths the the width of a bar is how",
    "start": "359520",
    "end": "367039"
  },
  {
    "text": "much time it took and you get this stacked trace of here's here's some",
    "start": "367039",
    "end": "372240"
  },
  {
    "text": "calls here's some processing that happens and then here's a longer processing call and then when it's done some more processing happens with",
    "start": "372240",
    "end": "378520"
  },
  {
    "text": "shorter traces and so this is a one user action has caused uh what's",
    "start": "378520",
    "end": "385680"
  },
  {
    "text": "the number 53 spans um so so now you're sending all",
    "start": "385680",
    "end": "391160"
  },
  {
    "text": "those spans to somebody like honeycomb",
    "start": "391160",
    "end": "396319"
  },
  {
    "text": "and that is fine it's cool you get a lot of need information but now multiply",
    "start": "396319",
    "end": "403160"
  },
  {
    "text": "that times thousands of users per day and um you know internet scale uh and",
    "start": "403160",
    "end": "410800"
  },
  {
    "text": "you can quickly be talking millions of spans and now how do you deal with that because if you're paying by the span you",
    "start": "410800",
    "end": "417680"
  },
  {
    "text": "know at some point you you need to control that so the way to handle it is",
    "start": "417680",
    "end": "423360"
  },
  {
    "text": "by sampling it so how do we sample the first and most obvious",
    "start": "423360",
    "end": "430400"
  },
  {
    "text": "mechanism and it's available in all the libraries including all the hotel libraries is to head sample in other",
    "start": "430400",
    "end": "437599"
  },
  {
    "text": "words at the time you create a span you roll a die and you say you know let's",
    "start": "437599",
    "end": "443639"
  },
  {
    "text": "say it's a six-sided die if it's a one we're going to keep that span if it's two through six we're just going to drop",
    "start": "443639",
    "end": "451199"
  },
  {
    "text": "so you do that and you're taking one6 the amount of data escapes your system",
    "start": "451199",
    "end": "456919"
  },
  {
    "text": "and gets into your back end but now when you get to the back end in",
    "start": "456919",
    "end": "462800"
  },
  {
    "text": "order to get your data back you need to multiply by six so you need to know that you were doing a sampling by",
    "start": "462800",
    "end": "470720"
  },
  {
    "text": "six so if you do that at the span level and you just do that randomly for every",
    "start": "470720",
    "end": "477280"
  },
  {
    "text": "span um it works if your data is first randomly distributed and you sample enough and",
    "start": "477280",
    "end": "483520"
  },
  {
    "text": "the spans are all independent of each other but that's actually not true right we caused those spans and they're all",
    "start": "483520",
    "end": "488960"
  },
  {
    "text": "dependent on each other and so when you have a trace like we were showing a",
    "start": "488960",
    "end": "494159"
  },
  {
    "text": "minute ago you have all those services and if you sample them randomly the",
    "start": "494159",
    "end": "500319"
  },
  {
    "text": "probability that you will have sampled an entire Trace is effectively zero um",
    "start": "500319",
    "end": "505759"
  },
  {
    "text": "you know the more the more spans you have like with 10 spans at a sampling rate of 1 in six it's like 60 1 in 60",
    "start": "505759",
    "end": "511840"
  },
  {
    "text": "million traces will be intact you know by the time you get up to 50 you're talking you know number of atoms in the",
    "start": "511840",
    "end": "518279"
  },
  {
    "text": "universe so you need to figure out a way to sample whole traces now you can still",
    "start": "518279",
    "end": "523880"
  },
  {
    "text": "do that with head sampling you can use the trace ID which is passed around and shared by all the",
    "start": "523880",
    "end": "530839"
  },
  {
    "text": "items you can use the trace ID or some some piece of element you share among the whole Trace to um make a pseudo",
    "start": "530839",
    "end": "538600"
  },
  {
    "text": "random decision ision and make the same decision for everybody so the one and six is based on whether the trace ID",
    "start": "538600",
    "end": "545160"
  },
  {
    "text": "starts with you know five um or the one in 16 or whatever it might be so you can",
    "start": "545160",
    "end": "551440"
  },
  {
    "text": "get or drop drop entire traces and so then the traces that arrive are intact a",
    "start": "551440",
    "end": "557480"
  },
  {
    "text": "span is is chosen or dropped based on whether it's part of a trace so yay we",
    "start": "557480",
    "end": "563720"
  },
  {
    "text": "got now we have intact traces we can do our Telemetry we can multiply everything by six and we're great",
    "start": "563720",
    "end": "569760"
  },
  {
    "text": "right not right so",
    "start": "569760",
    "end": "575720"
  },
  {
    "text": "um when you have low probability events particularly when the low probability",
    "start": "575720",
    "end": "581440"
  },
  {
    "text": "events are lower probability than your sampling rate then what it means is that",
    "start": "581440",
    "end": "586959"
  },
  {
    "text": "you don't get those events um you know so if you",
    "start": "586959",
    "end": "593000"
  },
  {
    "text": "have you know a thousand users generating a million spans and your error rate is 59",
    "start": "593000",
    "end": "599640"
  },
  {
    "text": "that means that like only 10 of those users had errors but if you're sampling at one in 100 that means 99 out of a 100",
    "start": "599640",
    "end": "607519"
  },
  {
    "text": "errors aren't getting sampled and like on any given day if you have a million users in a day you're not even getting any of",
    "start": "607519",
    "end": "613480"
  },
  {
    "text": "them so I just want to show you what that looks like let's go over here for a",
    "start": "613480",
    "end": "622000"
  },
  {
    "text": "sec and so this I sent a thousand traces",
    "start": "622000",
    "end": "629839"
  },
  {
    "text": "I sent 1% of them as errors and what we see here is um what I",
    "start": "629839",
    "end": "637920"
  },
  {
    "text": "did is I ran this through the refinery and dry run mode and it basically sent all the traces but it marked some of",
    "start": "637920",
    "end": "643600"
  },
  {
    "text": "them as having been sampled and some of them is not and so what we have is um",
    "start": "643600",
    "end": "649399"
  },
  {
    "text": "the ones we kept uh these are the ones that were so we we kept this collection of of",
    "start": "649399",
    "end": "658000"
  },
  {
    "text": "traces we dropped this collection of traces um we kept two errors out of that",
    "start": "658000",
    "end": "665200"
  },
  {
    "text": "thousand but they were close to 10 in fact there were nine errors total these were the ones we",
    "start": "665200",
    "end": "671440"
  },
  {
    "text": "dropped so good we got we sampled some",
    "start": "671440",
    "end": "677320"
  },
  {
    "text": "errors but the problem is that the errors are the part we actually want to",
    "start": "677320",
    "end": "683480"
  },
  {
    "text": "see um tol stoy put it pretty well you know",
    "start": "683480",
    "end": "689120"
  },
  {
    "text": "all happy families are alike but all unhappy families are different in their own interesting ways um and the book was",
    "start": "689120",
    "end": "698160"
  },
  {
    "text": "about the unhappy family because that's the interesting part so we should be telling the interesting stories and",
    "start": "698160",
    "end": "703639"
  },
  {
    "text": "that's what we need to be able to do we need to be able to sample and keep the stuff that matters you know cuz all of",
    "start": "703639",
    "end": "709959"
  },
  {
    "text": "those 200s who cares all your health checks hey long as as long as they came",
    "start": "709959",
    "end": "715680"
  },
  {
    "text": "back healthy you're happy so now we want want to do tail sampling and for anybody who doesn't",
    "start": "715680",
    "end": "722639"
  },
  {
    "text": "know it honeycomb is big on dogs and we name all our services after dogs and so",
    "start": "722639",
    "end": "728519"
  },
  {
    "text": "you have to have a dog and a honeycomb talk um all right so we're going to send all",
    "start": "728519",
    "end": "734000"
  },
  {
    "text": "our Telemetry to one proxy or to a proxy system that is doing tail sampling and",
    "start": "734000",
    "end": "741440"
  },
  {
    "text": "what we do is we aggregate all of the spans in the trace in one place we look",
    "start": "741440",
    "end": "746480"
  },
  {
    "text": "at them in context and then we can figure out what's interesting and make Intelligent Decisions about what to keep",
    "start": "746480",
    "end": "752440"
  },
  {
    "text": "and what to drop based on um you know how interesting they are based on how",
    "start": "752440",
    "end": "758399"
  },
  {
    "text": "we've decided what interesting means so one way we do this and Refinery has this",
    "start": "758399",
    "end": "764240"
  },
  {
    "text": "feature is called Rule based sampling and you can literally write down a set of rules and this is a fairly common",
    "start": "764240",
    "end": "769760"
  },
  {
    "text": "kind of thing if you're talking like an internet service if any span has an error you keep it if there's an error",
    "start": "769760",
    "end": "774880"
  },
  {
    "text": "you know status code over 500 keep it if any status code is in the 400 range well that's a user error but maybe they're",
    "start": "774880",
    "end": "781040"
  },
  {
    "text": "interesting so like sample them more frequently um health checks you know",
    "start": "781040",
    "end": "790240"
  },
  {
    "text": "once in a while you probably want a couple of them in your data set just so you know they're happening but mostly",
    "start": "790240",
    "end": "795560"
  },
  {
    "text": "you can drop them and a lot of people do actually just drop them completely we don't care if health checks ever leave our system we don't want to pay for them",
    "start": "795560",
    "end": "801720"
  },
  {
    "text": "and then everything else maybe you pick a rate of 100 whatever whatever your rate is that your volume that you can manage with the with this content of",
    "start": "801720",
    "end": "809079"
  },
  {
    "text": "your data you know the bigger you are the higher that number is and it works works pretty well but it",
    "start": "809079",
    "end": "816800"
  },
  {
    "text": "doesn't work if things are changing rapidly it doesn't deal well with surprises it doesn't deal well with that",
    "start": "816800",
    "end": "824040"
  },
  {
    "text": "customer who suddenly decides that they're going to send you you know a million spans a minute um and",
    "start": "824040",
    "end": "831480"
  },
  {
    "text": "so you need to do something um one of the things actually",
    "start": "831480",
    "end": "838360"
  },
  {
    "text": "I should just mentioned is this you have to attach attach the sample rate to the trace",
    "start": "838360",
    "end": "846279"
  },
  {
    "text": "because now if you're sampling at different rates you need to know how to compensate for that in the back end so",
    "start": "846279",
    "end": "853120"
  },
  {
    "text": "that you get reasonable data when you look at the combination of things because otherwise your errors are going to look a lot more frequent if you're",
    "start": "853120",
    "end": "859279"
  },
  {
    "text": "sampling you you're keeping every error but only keeping one in 100 of the other thing it's going to look like errors are half your",
    "start": "859279",
    "end": "865360"
  },
  {
    "text": "data or a lot more than that so",
    "start": "865360",
    "end": "870680"
  },
  {
    "text": "now we get to the real meat here what if your data isn't predictable what if it's",
    "start": "870880",
    "end": "875920"
  },
  {
    "text": "too complex to write general rules or it's changing too often or you bursty",
    "start": "875920",
    "end": "882800"
  },
  {
    "text": "traffic or in a lot of cases the people who are running the sampling system",
    "start": "882800",
    "end": "888199"
  },
  {
    "text": "aren't the people who are generating the data in the first place and so you don't know what some engineer you know on the",
    "start": "888199",
    "end": "894440"
  },
  {
    "text": "other side of the world is turning on and and may like over overwhelm your",
    "start": "894440",
    "end": "899959"
  },
  {
    "text": "Telemetry so this is where Dynamic sampling comes in so this is where we let the sampler",
    "start": "899959",
    "end": "906959"
  },
  {
    "text": "make the decision as to what we're going to keep and so we take a set of key fields",
    "start": "906959",
    "end": "914320"
  },
  {
    "text": "we we think about what matters to us and a classic example if you're talking",
    "start": "914320",
    "end": "919480"
  },
  {
    "text": "again about HTTP is well let's take a look at the URL let's take a look at the",
    "start": "919480",
    "end": "925000"
  },
  {
    "text": "maybe the verb take a look at the error code response from the HTTP uh result",
    "start": "925000",
    "end": "931880"
  },
  {
    "text": "and maybe a you know if there are any other error Flags or something we're trying to check and you attach that to your traces and then you can use that",
    "start": "931880",
    "end": "939319"
  },
  {
    "text": "combination of fields to determine unique key for everybody and then you evaluate sample",
    "start": "939319",
    "end": "946199"
  },
  {
    "text": "rates based on that key so we do this mathematically what we do is we take we",
    "start": "946199",
    "end": "953399"
  },
  {
    "text": "take these keys and we collect and we watch how many happen and",
    "start": "953399",
    "end": "959360"
  },
  {
    "text": "excuse me did I get a slide out of order yeah no I'm good um we take these keys and we we we",
    "start": "959360",
    "end": "969120"
  },
  {
    "text": "calculate which things are most common which things are least common and we make sure we keep all of the least common ones but the most common ones we",
    "start": "969120",
    "end": "976519"
  },
  {
    "text": "can fairly heavily reduce the the amount of samples we keep so so then you end up",
    "start": "976519",
    "end": "982920"
  },
  {
    "text": "with okay well you know 200s to my login service you know one in 100 those but",
    "start": "982920",
    "end": "989680"
  },
  {
    "text": "obviously 500s from some other service over here that's crashing we want to see everything from",
    "start": "989680",
    "end": "995519"
  },
  {
    "text": "those so I want to go back to my other thing here I'm going to show",
    "start": "995519",
    "end": "1002560"
  },
  {
    "text": "you kind of a sample so what I did is I sent batches of a thousand traces and",
    "start": "1002560",
    "end": "1009480"
  },
  {
    "text": "just simulated things this one has URL Paths of",
    "start": "1009480",
    "end": "1015880"
  },
  {
    "text": "like um just the the first part of the path has only had a cardinality of two",
    "start": "1015880",
    "end": "1021440"
  },
  {
    "text": "and the second part of the path only had a cardinality of four and then I have some errors in status as other fields",
    "start": "1021440",
    "end": "1026880"
  },
  {
    "text": "and I'm using that as my error key so now I'm showing you a graph which shows for this thousand traces I sent what the",
    "start": "1026880",
    "end": "1034280"
  },
  {
    "text": "distribution of the various Fields was and you see that in the compensated",
    "start": "1034280",
    "end": "1039319"
  },
  {
    "text": "thing it says okay I got 102 192 of the ones drawer clear with a 200 status code",
    "start": "1039319",
    "end": "1046438"
  },
  {
    "text": "later on we'd see drawer clear is probably down here as a you know here's a 201 status code there were 22 of those",
    "start": "1046439",
    "end": "1053880"
  },
  {
    "text": "um and now I can look at this and I can say remove the compensation don't do the",
    "start": "1053880",
    "end": "1060840"
  },
  {
    "text": "math um and in this case we have drawer clear there were actually",
    "start": "1060840",
    "end": "1067559"
  },
  {
    "text": "only 28 of those that made it to my back end but because we knew that the sample",
    "start": "1067559",
    "end": "1075360"
  },
  {
    "text": "rate on those was um whatever number we used um I'd have to look it up but but",
    "start": "1075360",
    "end": "1082039"
  },
  {
    "text": "anyway uh there were 192 of them so I guess said seven um",
    "start": "1082039",
    "end": "1087600"
  },
  {
    "text": "so the sample rate of the of this particular value is different from the",
    "start": "1087600",
    "end": "1093480"
  },
  {
    "text": "sample rate from some other less common value so if I go back here for a second you'll",
    "start": "1093480",
    "end": "1098799"
  },
  {
    "text": "see we have these different counts and notice it goes all the way down to the ones every sample was kept every unique",
    "start": "1098799",
    "end": "1106280"
  },
  {
    "text": "combination of keys was kept but the ones that were popular were much more we",
    "start": "1106280",
    "end": "1111840"
  },
  {
    "text": "kept many fewer of them than we kept of the ones that were rare um and the way this works basically",
    "start": "1111840",
    "end": "1118640"
  },
  {
    "text": "is we just we track it for 30 seconds or some number you have control over that",
    "start": "1118640",
    "end": "1124080"
  },
  {
    "text": "you record how many you have in the various buckets and then you use those buckets to deal with the next 30 seconds",
    "start": "1124080",
    "end": "1131000"
  },
  {
    "text": "and you meanwhile record those and then you use those buckets to deal with the next 30 seconds so it adapts over time",
    "start": "1131000",
    "end": "1136679"
  },
  {
    "text": "and actually we have a bunch of different Samplers that have different adaptation models depending on what your",
    "start": "1136679",
    "end": "1142039"
  },
  {
    "text": "particular needs are and I'll show you that in a minute um one of the things I want to note is this key strategy",
    "start": "1142039",
    "end": "1149480"
  },
  {
    "text": "actually fails if you have too many keys so here I did a sample s i",
    "start": "1149480",
    "end": "1159600"
  },
  {
    "text": "sampled um I I said the data I sent had a cardinality where the URL cardinality",
    "start": "1159600",
    "end": "1165919"
  },
  {
    "text": "was over 100 or was a 100 and then the combin then you have the additional combinations of what were the error",
    "start": "1165919",
    "end": "1171799"
  },
  {
    "text": "codes and what were the status codes so that we literally have you know probably",
    "start": "1171799",
    "end": "1177080"
  },
  {
    "text": "500 different um states of key in this",
    "start": "1177080",
    "end": "1182880"
  },
  {
    "text": "batch of a thousand traces so that's way too much combination to actually get good data out of it and to draw the",
    "start": "1182880",
    "end": "1190039"
  },
  {
    "text": "thing and so the thing I want to show you is so for the first set of data we",
    "start": "1190039",
    "end": "1195760"
  },
  {
    "text": "have this graph which shows the average sample rate here in the",
    "start": "1195760",
    "end": "1200799"
  },
  {
    "text": "bottom graph um the first 30 seconds the sampler is just going okay tell me what",
    "start": "1200799",
    "end": "1207360"
  },
  {
    "text": "you want I'm just going to do six because you said it was six and then it figures out what the average sample rate",
    "start": "1207360",
    "end": "1212600"
  },
  {
    "text": "that it can achieve and it's achieving a sample rate of about four because of the low cardinality is of the of the key",
    "start": "1212600",
    "end": "1218760"
  },
  {
    "text": "field we've chosen this one here the cardinality of the key field was much",
    "start": "1218760",
    "end": "1224280"
  },
  {
    "text": "higher and notice we can't get the sample rate up because basic we're sampling every key at somewhere between",
    "start": "1224280",
    "end": "1230280"
  },
  {
    "text": "one and two because there just too many keys to do this thing mathematically you",
    "start": "1230280",
    "end": "1235880"
  },
  {
    "text": "know uh sanely and so the the the sampler is trying as hard as it can but",
    "start": "1235880",
    "end": "1241840"
  },
  {
    "text": "it just can't get the sample rate up there at this volume now this may be a fine strategy if we had millions of keys",
    "start": "1241840",
    "end": "1250320"
  },
  {
    "text": "um and we're s you know sampling at these rates that would be okay but but because there's so much difference",
    "start": "1250320",
    "end": "1257280"
  },
  {
    "text": "between all the data none of the keys can be like pumped up into a range that that actually reduces your sample rate",
    "start": "1257280",
    "end": "1263039"
  },
  {
    "text": "in a meaningful way so let's",
    "start": "1263039",
    "end": "1268840"
  },
  {
    "text": "just go back to here sorry all right so just",
    "start": "1268840",
    "end": "1277919"
  },
  {
    "text": "generally what we doing with Dynamic tail sampling is you know we're catching the most interesting data types the more",
    "start": "1277919",
    "end": "1285080"
  },
  {
    "text": "common events are dropped more frequently the less common events are kept more frequently and are anything",
    "start": "1285080",
    "end": "1291559"
  },
  {
    "text": "new anything uncommon is guaranteed to show up but if you're cardinality is too",
    "start": "1291559",
    "end": "1296919"
  },
  {
    "text": "high it's a problem um as I mentioned before we have a bunch of different sampler types in",
    "start": "1296919",
    "end": "1303000"
  },
  {
    "text": "Refinery that do Dynamic and they basically combine current data with past",
    "start": "1303000",
    "end": "1309000"
  },
  {
    "text": "data or don't in different ways so we have an exponential moving average a",
    "start": "1309000",
    "end": "1314120"
  },
  {
    "text": "couple of Samplers that do that we have a couple of Samplers that are designed to limit three throughput where you can just say I would like my numbers to work",
    "start": "1314120",
    "end": "1320880"
  },
  {
    "text": "out to you know 10,000 spans per minute or whatever number you want um and and",
    "start": "1320880",
    "end": "1327360"
  },
  {
    "text": "so the throughput will they will optimize for throughput rather than sample rate some of these other ones will be more in Sample rate and then we",
    "start": "1327360",
    "end": "1333840"
  },
  {
    "text": "have a windowed throughput sampler which was actually donated by a uh third party",
    "start": "1333840",
    "end": "1339279"
  },
  {
    "text": "um that um does a different form of moving average it basically keeps a",
    "start": "1339279",
    "end": "1344679"
  },
  {
    "text": "record of the last like I don't know sever several rounds and and so you it it",
    "start": "1344679",
    "end": "1353880"
  },
  {
    "text": "adapts better in certain circumstances it was generated by one large",
    "start": "1353880",
    "end": "1359320"
  },
  {
    "text": "customer um so since we're here talking a lot about otel today um The otel",
    "start": "1359320",
    "end": "1366320"
  },
  {
    "text": "Collector has a tail sampling processor um but in today's world it",
    "start": "1366320",
    "end": "1372640"
  },
  {
    "text": "can't add sample rate to the output it you can use the um thing Tyler was just",
    "start": "1372640",
    "end": "1379679"
  },
  {
    "text": "talking about you can use a transform processor to add a sample rate but then it has to be a constant sample rate",
    "start": "1379679",
    "end": "1384919"
  },
  {
    "text": "because they don't talk to each other so we can't do this Dynamic sampling Concept in The otel Collector",
    "start": "1384919",
    "end": "1392240"
  },
  {
    "text": "today um now good news is I'm on the sampling Sig and we have a spec revision",
    "start": "1392240",
    "end": "1398400"
  },
  {
    "text": "or an Otep that's out right now which is going to allow us to attach a sampling",
    "start": "1398400",
    "end": "1404000"
  },
  {
    "text": "threshold which as I said is equivalent to sample rate to traces such that it can propagate through your system and so",
    "start": "1404000",
    "end": "1411039"
  },
  {
    "text": "that that solves half the problem and the other half of the problem is that the collector the tail sampling",
    "start": "1411039",
    "end": "1416520"
  },
  {
    "text": "processor composes Samplers by making a series of individual binary decisions and there",
    "start": "1416520",
    "end": "1423279"
  },
  {
    "text": "really not a good way that I've found so far to figure out how to attach the correct sample rate based on having made",
    "start": "1423279",
    "end": "1430559"
  },
  {
    "text": "a sequential series of binary decisions so I want to try and think about that as",
    "start": "1430559",
    "end": "1435720"
  },
  {
    "text": "to how we can get this into the tail sampling process as we go forward but it's going to need some serious",
    "start": "1435720",
    "end": "1441520"
  },
  {
    "text": "rework so um if you're going to give feedback on The Talk I'd love it if you shoot that thing and pass that on um and",
    "start": "1441520",
    "end": "1450919"
  },
  {
    "text": "if you uh get a chance come by the honeycomb booth and and check out the stuff we're",
    "start": "1450919",
    "end": "1456520"
  },
  {
    "text": "doing we're up in in22 uh [Applause]",
    "start": "1456520",
    "end": "1466799"
  },
  {
    "text": "questions",
    "start": "1466799",
    "end": "1469799"
  },
  {
    "text": "uh um the question is how are the keys selected the keys are controlled by configuration so you decide as the user",
    "start": "1481159",
    "end": "1488080"
  },
  {
    "text": "which fields are meaningful to you as keys and then for any given Trace the",
    "start": "1488080",
    "end": "1494640"
  },
  {
    "text": "assorted combination of all of the values of those fields for all of the spans and the trays is",
    "start": "1494640",
    "end": "1500320"
  },
  {
    "text": "assembled to make the",
    "start": "1500320",
    "end": "1503440"
  },
  {
    "text": "key yes yes there is so so they are per",
    "start": "1510960",
    "end": "1516520"
  },
  {
    "text": "trace and they are um the there is a",
    "start": "1516520",
    "end": "1521799"
  },
  {
    "text": "semantic assumption that when you have a given field name that that field means",
    "start": "1521799",
    "end": "1527200"
  },
  {
    "text": "the same thing on all the elements the trace so if you have a status field for example and it's not actually a status",
    "start": "1527200",
    "end": "1532480"
  },
  {
    "text": "code in some cases you might want to do something like in your Telemetry to copy that to a a value That's",
    "start": "1532480",
    "end": "1539880"
  },
  {
    "text": "Unique",
    "start": "1540080",
    "end": "1543080"
  },
  {
    "text": "yeah I'm sorry would you try anomaly",
    "start": "1553080",
    "end": "1560120"
  },
  {
    "text": "detection okay integrating random sampling with anomaly detection is the question and generally in that",
    "start": "1565360",
    "end": "1572120"
  },
  {
    "text": "circumstance probably what we would do is say you should probably use the rule based sampler and write rules that",
    "start": "1572120",
    "end": "1579320"
  },
  {
    "text": "Define the anomalies in particular that you're looking for we're not doing AI automatic detection of anomalies based",
    "start": "1579320",
    "end": "1584880"
  },
  {
    "text": "on the shape of your data overall or anything like that we're just you you you could specify a rule that says these are the things I care about",
    "start": "1584880",
    "end": "1592360"
  },
  {
    "text": "and then and then basically set the samples on those to zero somebody here at the mic hey uh great talk thanks uh I",
    "start": "1592360",
    "end": "1599240"
  },
  {
    "text": "want to ask so you described a lot of the head tast head based sampling ta",
    "start": "1599240",
    "end": "1604720"
  },
  {
    "text": "sampling Dynamic sampling this all happens in the agent on my infrastructure typically in",
    "start": "1604720",
    "end": "1612799"
  },
  {
    "text": "head-based sampling um either it happens at the point of Telemetry where you're adding",
    "start": "1612799",
    "end": "1618640"
  },
  {
    "text": "Telemetry the the library you're using within your application you can actually set a sample rate there or you may have",
    "start": "1618640",
    "end": "1626000"
  },
  {
    "text": "you may have an egress agent within within a pod or something like that that is like a local collector and you can",
    "start": "1626000",
    "end": "1631760"
  },
  {
    "text": "set a sample rate on that I mean there I say head and tail but really sampling can happen in a variety of places along",
    "start": "1631760",
    "end": "1638440"
  },
  {
    "text": "the pipeline um so my question is I think maybe I have like functions as a service maybe in a local maybe in a",
    "start": "1638440",
    "end": "1644960"
  },
  {
    "text": "cloud maybe a couple quads maybe local environment and I don't want to set up infrastructure",
    "start": "1644960",
    "end": "1650159"
  },
  {
    "text": "for The Collector to for the agent to sample is it possible that I just send everything to hyamp and do the sampling",
    "start": "1650159",
    "end": "1657080"
  },
  {
    "text": "uh configure the sampling in in your Cloud uh not today okay thank you no it's it's been",
    "start": "1657080",
    "end": "1664080"
  },
  {
    "text": "discussed from time to time but it's it's not something we're doing today right so i' have to like set up an infrastructure where I put up your agent",
    "start": "1664080",
    "end": "1670640"
  },
  {
    "text": "or the hotel and right you're e you're either running a collector and doing it there or you're running a Refinery as as",
    "start": "1670640",
    "end": "1677799"
  },
  {
    "text": "an endpoint within your within your services and then um refiner is passing that data onto H okay okay thanks very",
    "start": "1677799",
    "end": "1684399"
  },
  {
    "text": "much you're welcome uh great talk by the way um",
    "start": "1684399",
    "end": "1692159"
  },
  {
    "text": "quick question on you mentioned the rule based sampling and then the dynamic sampling in some cases don't you need",
    "start": "1692159",
    "end": "1697720"
  },
  {
    "text": "the rule-based sampling to kind of augment some of the dynamic stuff yes because I mean then you know if you're",
    "start": "1697720",
    "end": "1703840"
  },
  {
    "text": "yeah if you're doing you know you don't want to collect all those successful samples because not going to give you anything useful right the question is do",
    "start": "1703840",
    "end": "1710080"
  },
  {
    "text": "you sometimes need rule based sampling anyway even with Dynamic sampling and yes and in fact what we often do is you",
    "start": "1710080",
    "end": "1716279"
  },
  {
    "text": "write a rule to capture those particular circumstances you want to make sure you don't miss or particular rules we say",
    "start": "1716279",
    "end": "1723159"
  },
  {
    "text": "look this service over here is too chatty and I really need to you know cut it down but but then what we have is the",
    "start": "1723159",
    "end": "1730960"
  },
  {
    "text": "idea that within those rules even you can say and then fall back to a dynamic sampler once you you know if if the rule",
    "start": "1730960",
    "end": "1737919"
  },
  {
    "text": "doesn't match these conditions now use a dynamic sampler to do that so so it can become your fallback to other things",
    "start": "1737919",
    "end": "1745600"
  },
  {
    "text": "okay is is it possible to do that in in your platform like in terms of mesh the",
    "start": "1745600",
    "end": "1750799"
  },
  {
    "text": "two together yes yes that that works today",
    "start": "1750799",
    "end": "1758600"
  },
  {
    "text": "yes yes",
    "start": "1762960",
    "end": "1769320"
  },
  {
    "text": "yeah we don't adapt we don't have an Adaptive model for adapting rate in that model it is it is basically a counts",
    "start": "1783640",
    "end": "1791480"
  },
  {
    "text": "counts kind of thing so you can you can do that sort of detection in the back end once it happens you can cause um you",
    "start": "1791480",
    "end": "1800720"
  },
  {
    "text": "can send alerts and things like that but you would have to then modify configuration and deal with it I think I'm getting the queue so no oh wait one",
    "start": "1800720",
    "end": "1808120"
  },
  {
    "text": "more okay yeah hi thanks for the talk can you clarify which parts of what you talked about are open source and self",
    "start": "1808120",
    "end": "1814080"
  },
  {
    "text": "hostable and which are honeycomb specific um all of this the refinery is in fact fully open source along with our",
    "start": "1814080",
    "end": "1820559"
  },
  {
    "text": "Dynamic sampling Library dine sampler go which is also open source um and then um",
    "start": "1820559",
    "end": "1826120"
  },
  {
    "text": "as I said uh you know Tyler who works with me and and I we both work and our whole team works on the um on The",
    "start": "1826120",
    "end": "1833039"
  },
  {
    "text": "Collector and we're sort of chewing through how to move this stuff forward so that we can get it into the collector",
    "start": "1833039",
    "end": "1838559"
  },
  {
    "text": "entirely yes that'd be great it's great stuff thanks you're welcome thank you very",
    "start": "1838559",
    "end": "1845000"
  },
  {
    "text": "much",
    "start": "1845279",
    "end": "1848279"
  }
]