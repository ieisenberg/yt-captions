[
  {
    "text": "uh I'm nikil uh Rana I'm from Google cloud and I have my friend join Ed uh so",
    "start": "880",
    "end": "6799"
  },
  {
    "text": "we been we're going to be talking about uh lm's Edge deployment with the wasam and uh web",
    "start": "6799",
    "end": "13879"
  },
  {
    "text": "GPU um so just a disclaimer this session I think uh was supposed to be part of",
    "start": "13879",
    "end": "21039"
  },
  {
    "text": "Aid Dev I think we submitted in the wrong category so apologies for",
    "start": "21039",
    "end": "26960"
  },
  {
    "text": "that um so to start off I think if you haven't been reminded about this million",
    "start": "26960",
    "end": "33520"
  },
  {
    "text": "times uh yeah this is where we are with respect to large language",
    "start": "33520",
    "end": "39160"
  },
  {
    "text": "models uh so yeah I mean notable mention to our little bird there the OG of uh",
    "start": "39160",
    "end": "48480"
  },
  {
    "text": "language models uh so while sort of picking up a",
    "start": "48480",
    "end": "55680"
  },
  {
    "text": "model uh for your applications there are certain key uh",
    "start": "55680",
    "end": "61239"
  },
  {
    "text": "considerations um like why do you need like a large language model for your",
    "start": "61239",
    "end": "66920"
  },
  {
    "text": "application uh and basically which sort of pre-trained model is something that",
    "start": "66920",
    "end": "72680"
  },
  {
    "text": "aligns with your goals uh of of your uh application and your",
    "start": "72680",
    "end": "78000"
  },
  {
    "text": "customers and then uh accuracy requirements uh you do multiple",
    "start": "78000",
    "end": "83159"
  },
  {
    "text": "evaluations and benchmarks uh to arrive the arrive at the right model that you would sort of use in your uh you know uh",
    "start": "83159",
    "end": "91159"
  },
  {
    "text": "Downstream application and then uh also something that uh you would take care is basically",
    "start": "91159",
    "end": "97920"
  },
  {
    "text": "the financial investment this is both in terms of deploying deploying these models or in terms of using this models",
    "start": "97920",
    "end": "104000"
  },
  {
    "text": "VI an API uh how much tokens are you paying uh based on your user base uh and",
    "start": "104000",
    "end": "110399"
  },
  {
    "text": "based on the provider and then uh purpose and Licensing obviously if you're using like an open weight model",
    "start": "110399",
    "end": "115960"
  },
  {
    "text": "or an open source model you have to sort of take care of the proprietary uh licenses uh to you know use in your",
    "start": "115960",
    "end": "124799"
  },
  {
    "text": "applications so U how how are llm sort of deployed U uh today um so so you",
    "start": "124799",
    "end": "132200"
  },
  {
    "text": "basically have this um you know very high level architecture uh you have your CPU node pool which is basically taking",
    "start": "132200",
    "end": "139200"
  },
  {
    "text": "care of all your you know application Level of uh you know uh requests and",
    "start": "139200",
    "end": "144720"
  },
  {
    "text": "then you have the TPU slice nodes which is basically taking care of the model part so you're getting your model from",
    "start": "144720",
    "end": "149959"
  },
  {
    "text": "um probably some sort of a storage and basically you're loading it into the model server and basically then exposing",
    "start": "149959",
    "end": "156000"
  },
  {
    "text": "it uh you know via the CPU node pool which is basically your uh API Gateway",
    "start": "156000",
    "end": "161159"
  },
  {
    "text": "and then you have you know all your authentication and load balancing and all part of of the same",
    "start": "161159",
    "end": "168480"
  },
  {
    "text": "engine uh this uh is a slightly higher uh level now when you talk about",
    "start": "168480",
    "end": "175280"
  },
  {
    "text": "deploying using such deployed models in your applications for example rag uh a",
    "start": "175280",
    "end": "181120"
  },
  {
    "text": "very famous architecture framework so in this case basically you have your data",
    "start": "181120",
    "end": "186519"
  },
  {
    "text": "system subsystems and your uh serving subsystems uh wherein basically you would have your llm inference tack uh at",
    "start": "186519",
    "end": "194080"
  },
  {
    "text": "the bottom and basically you know you would sort of build all the front end uh you know uh part of of the application",
    "start": "194080",
    "end": "202319"
  },
  {
    "text": "uh and basically you would also have a vector database which is basically for all your semantic search and you know",
    "start": "202319",
    "end": "207440"
  },
  {
    "text": "storing your embeddings and all so uh now uh in terms of uh limitations",
    "start": "207440",
    "end": "215280"
  },
  {
    "text": "now uh obviously large language models they are pretty large so there are a lot of limitation in terms of using these",
    "start": "215280",
    "end": "221560"
  },
  {
    "text": "models uh in a customer facing application uh first and foremost is",
    "start": "221560",
    "end": "227360"
  },
  {
    "text": "latency um you know obviously these are cloud-based models and you know uh it sort of uh you know get impacted in in",
    "start": "227360",
    "end": "234680"
  },
  {
    "text": "your real-time applications If you're sort of building let's say a search engine and if it takes you know 10",
    "start": "234680",
    "end": "240360"
  },
  {
    "text": "seconds to sort of load the search results then it's bad bad user",
    "start": "240360",
    "end": "245760"
  },
  {
    "text": "experience uh cost obviously as I mentioned in the previous slides significant cost in terms of both",
    "start": "245760",
    "end": "251720"
  },
  {
    "text": "deployment as well as using them uh VI an API privacy concerns uh I think this uh",
    "start": "251720",
    "end": "260400"
  },
  {
    "text": "I mean we have seen lot of uh news articles recently and as well as in the past of uh you know security breaches",
    "start": "260400",
    "end": "268120"
  },
  {
    "text": "and you know prompt hacking and all all the sort of things where you know customer data user data has been sort of",
    "start": "268120",
    "end": "273840"
  },
  {
    "text": "uh you know taken into the model and basically used for training U newer",
    "start": "273840",
    "end": "280960"
  },
  {
    "text": "models and uh also there's a dependency on internet connectivity because",
    "start": "281199",
    "end": "286360"
  },
  {
    "text": "everything you're using Y an API uh so you have you need to have an internet connectivity so let's say if you are uh",
    "start": "286360",
    "end": "293960"
  },
  {
    "text": "in a in a flight or basically you have you are in in an area where you have uh",
    "start": "293960",
    "end": "299080"
  },
  {
    "text": "low or no internet connectivity these models are difficult to access and then scalability challenges",
    "start": "299080",
    "end": "305479"
  },
  {
    "text": "are obviously there as I mentioned like you know if if your customer base sort of starts using your application at a",
    "start": "305479",
    "end": "312120"
  },
  {
    "text": "higher skill then probably you know U you know your latency would sort of increase and there might be scalability",
    "start": "312120",
    "end": "318160"
  },
  {
    "text": "issues in in the long run so what brings us to the next part",
    "start": "318160",
    "end": "324400"
  },
  {
    "text": "is basically you have small language models uh now what are small language models",
    "start": "324400",
    "end": "330440"
  },
  {
    "text": "obviously the name suggest they're small uh now the small uh refers to the uh to",
    "start": "330440",
    "end": "336240"
  },
  {
    "text": "the number of parameters that the model has uh as compared to the large language models uh as well as the data that it's",
    "start": "336240",
    "end": "343039"
  },
  {
    "text": "trained on so they can have like a few 100 million parameters to a a bill a few",
    "start": "343039",
    "end": "349400"
  },
  {
    "text": "billion parameters and then uh like this is just to summarize If U llms are like",
    "start": "349400",
    "end": "356880"
  },
  {
    "text": "Wikipedia slm is like your pocket dictionary uh it's portable efficient and it's very",
    "start": "356880",
    "end": "363479"
  },
  {
    "text": "specialized uh this is just to I'm not sure if this is visible on the side but",
    "start": "363479",
    "end": "368680"
  },
  {
    "text": "um this is just to sort of uh uh sort of showcase uh the landscape",
    "start": "368680",
    "end": "374800"
  },
  {
    "text": "of uh the current landscape of uh small language models that we have so on the y axis you have the number of parameters",
    "start": "374800",
    "end": "381639"
  },
  {
    "text": "in billion and on the bottom you have your uh release date so um like the",
    "start": "381639",
    "end": "387319"
  },
  {
    "text": "latest models that you have uh from meta and Google and uh Microsoft so uh there is a lot of",
    "start": "387319",
    "end": "394400"
  },
  {
    "text": "evolution coming U you know in in this space and a lot of models are being launched um every every",
    "start": "394400",
    "end": "401199"
  },
  {
    "text": "month uh now in talking in terms of um the advantages so U obviously as I",
    "start": "401199",
    "end": "407599"
  },
  {
    "text": "mentioned U slms give you that uh tailored uh efficiency and precision for",
    "start": "407599",
    "end": "412759"
  },
  {
    "text": "your task uh for your workloads uh as compared to uh a general purpose llm uh",
    "start": "412759",
    "end": "420080"
  },
  {
    "text": "speed obviously because they are small so it will take less time to sort of process a request and sort of serve it",
    "start": "420080",
    "end": "425319"
  },
  {
    "text": "to to to let's say your customers and cost obviously you know uh it's it's a",
    "start": "425319",
    "end": "431000"
  },
  {
    "text": "smaller model you can sort of probably fit it in one GPU or you know M or just a bunch of gpus and you can sort of",
    "start": "431000",
    "end": "437440"
  },
  {
    "text": "easily Deploy on your own in your Cloud instance and sort of start uh you know",
    "start": "437440",
    "end": "442720"
  },
  {
    "text": "uh uh using it or serving it uh so just giving one one example um",
    "start": "442720",
    "end": "450840"
  },
  {
    "text": "of of an slm um so this is this is Jemma um I'm from Google so I have to talk",
    "start": "450840",
    "end": "457280"
  },
  {
    "text": "about JMA sorry uh so JMA is is a family of",
    "start": "457280",
    "end": "463080"
  },
  {
    "text": "lightweight um open models um it's built from the same technology as uh Google",
    "start": "463080",
    "end": "468599"
  },
  {
    "text": "Gemini and uh the flavors that Gemma offers is basically uh the these five uh",
    "start": "468599",
    "end": "475479"
  },
  {
    "text": "so you have your base Gemma model which you can use for your text generation task you have a lot of variants in that",
    "start": "475479",
    "end": "482159"
  },
  {
    "text": "2 billion model 2 billion instruction T 7 billion 9 billion 27 billion Etc then",
    "start": "482159",
    "end": "488240"
  },
  {
    "text": "for code generation you have code Gemma recurrent Gemma is something that uses",
    "start": "488240",
    "end": "493520"
  },
  {
    "text": "RNN U as as a hybrid architecture and it sort of gives you faster inference as",
    "start": "493520",
    "end": "498560"
  },
  {
    "text": "compared to U you know a normal um Jemma model then you have a vision open model",
    "start": "498560",
    "end": "504319"
  },
  {
    "text": "also uh which is Palama it's 3 billion it's a smaller model so you can use for image Q&A or you know captioning and all",
    "start": "504319",
    "end": "511319"
  },
  {
    "text": "and then you have uh task specific model like Shield Gemma which is specific for you know safety purposes so you can use",
    "start": "511319",
    "end": "518120"
  },
  {
    "text": "this as part of your workload if you want to sort of filter out uh inappropriate responses Etc uh in in",
    "start": "518120",
    "end": "525959"
  },
  {
    "text": "your in your entire llm workflow so yes um we are excited to use",
    "start": "525959",
    "end": "534560"
  },
  {
    "text": "slms uh but um you know how can slm function with with fewer",
    "start": "534560",
    "end": "541800"
  },
  {
    "text": "parameters um so obviously uh it comes down to to the training methods uh be",
    "start": "541800",
    "end": "548000"
  },
  {
    "text": "transfer learning or distillation or fine tuning that is basically used to sort of make these models more task",
    "start": "548000",
    "end": "554720"
  },
  {
    "text": "specific uh domain specific adaptation that is where fine-tuning sort of comes into picture and techniques like Laura Q",
    "start": "554720",
    "end": "560880"
  },
  {
    "text": "Laura or Dora something like that is sort of used to make sure that uh for your uh you know domain uh the language",
    "start": "560880",
    "end": "567839"
  },
  {
    "text": "understands the nuances and the style adaptation and then Effectiveness obviously it depends on the entire uh",
    "start": "567839",
    "end": "574399"
  },
  {
    "text": "pre pre-training and fine-tuning U and and this is just to highlight uh one",
    "start": "574399",
    "end": "579440"
  },
  {
    "text": "example of let's say you have a query and you have an intelligent uh llm query",
    "start": "579440",
    "end": "584760"
  },
  {
    "text": "router and then you can have smaller U slms uh which probably can fit in into",
    "start": "584760",
    "end": "591160"
  },
  {
    "text": "your your INF infrastructure and then basically based on you you know uh the",
    "start": "591160",
    "end": "596240"
  },
  {
    "text": "the query you can sort of use a specific LM so for example um you have an slm to just summarize a",
    "start": "596240",
    "end": "603120"
  },
  {
    "text": "conversation you don't need a large language model there so you can have a very uh you know U domain specific slm",
    "start": "603120",
    "end": "610240"
  },
  {
    "text": "fine-tuned on your data on your style of uh you know responses that you want and",
    "start": "610240",
    "end": "615360"
  },
  {
    "text": "then it sort of does the job at lower cost and lower compute uh this is the last slide from",
    "start": "615360",
    "end": "621480"
  },
  {
    "text": "my perspective uh before I hand over so just you know whatever I spoke about",
    "start": "621480",
    "end": "626560"
  },
  {
    "text": "just putting some uh high level you know uh stages of how U an slm is trained and",
    "start": "626560",
    "end": "632720"
  },
  {
    "text": "then sort of uh fine-tuned for for a specific U you know task so uh now that",
    "start": "632720",
    "end": "640320"
  },
  {
    "text": "we've seen about you know what are large language models and you know the evolution of slms now join will sort of",
    "start": "640320",
    "end": "647079"
  },
  {
    "text": "talk about running these models on device",
    "start": "647079",
    "end": "654440"
  },
  {
    "text": "hello hello uh yeah thanks n uh",
    "start": "654440",
    "end": "660000"
  },
  {
    "text": "I think so far what we discussed was what are slms how to use it the benefits you get right and yesterday if some of",
    "start": "660000",
    "end": "665880"
  },
  {
    "text": "you in our talk we talked about how deploying this model in the cloud is a pain because of the GPU availability uh",
    "start": "665880",
    "end": "673000"
  },
  {
    "text": "you know maintaining the large infrastructure which required to deploy this large models right",
    "start": "673000",
    "end": "679839"
  },
  {
    "text": "and yeah and the second part is uh uh internet connectiv or access to",
    "start": "679839",
    "end": "686639"
  },
  {
    "text": "these models are a challenge I think I'll start with a demo first before I go to uh you know uh an use case first",
    "start": "686639",
    "end": "694079"
  },
  {
    "text": "right I think I flew down from Bangalore here I opened chat. and well it's not",
    "start": "694079",
    "end": "699279"
  },
  {
    "text": "accessible to me right I cannot access uh these uh tools for me to work right luckily I had uh this deployed for me in",
    "start": "699279",
    "end": "708160"
  },
  {
    "text": "my laptop and it's the same right if I can I can just do this right so if I just turn down",
    "start": "708160",
    "end": "714480"
  },
  {
    "text": "my and hope demo CS are with me today",
    "start": "714480",
    "end": "719560"
  },
  {
    "text": "uh yeah so you see I'm giving this a random code saying okay this write this in Python and and also explain to me",
    "start": "719560",
    "end": "726320"
  },
  {
    "text": "right and uh this is all the model is running locally for me to now uh give me",
    "start": "726320",
    "end": "732120"
  },
  {
    "text": "the response imagine a situation where you know you need to access to customers or your actual users who don't have",
    "start": "732120",
    "end": "737399"
  },
  {
    "text": "internet right who are in a very uh remote location where they cannot connect to your Cloud deployments to run",
    "start": "737399",
    "end": "742959"
  },
  {
    "text": "this s slms to get the responses you can still build products for them at their site right uh if imagine use cases like",
    "start": "742959",
    "end": "750680"
  },
  {
    "text": "disaster management um you know things like Farm helping the farmers or or",
    "start": "750680",
    "end": "755800"
  },
  {
    "text": "helping those remote locations where you don't have connectivity as well as you don't have the compu availability right",
    "start": "755800",
    "end": "761440"
  },
  {
    "text": "uh and again U from from an company perspective you're also saving a lot of cost you're offloading your",
    "start": "761440",
    "end": "767320"
  },
  {
    "text": "infrastructure to the user itself you using use their infra their phone their laptop their machines to run your models",
    "start": "767320",
    "end": "772880"
  },
  {
    "text": "and then uh you know build your product around that right uh other thing it's",
    "start": "772880",
    "end": "778320"
  },
  {
    "text": "not just text we can also do is if I just load this whisper model right I could say write a code in Python to add",
    "start": "778320",
    "end": "784720"
  },
  {
    "text": "two numbers yeah and then it's everything a voice model the the uh the llm which is",
    "start": "784720",
    "end": "791880"
  },
  {
    "text": "powering the the inference and everything is running locally on the device itself right on the machine",
    "start": "791880",
    "end": "797680"
  },
  {
    "text": "itself uh yeah so now let's go ahead and see how this is done right how you're able to do itally these are three steps",
    "start": "797680",
    "end": "804240"
  },
  {
    "text": "right uh you take an llm from or an slm from your research team or men from the",
    "start": "804240",
    "end": "809600"
  },
  {
    "text": "cloud from which are released and then next thing is optimize right so this is very important step right how do you now",
    "start": "809600",
    "end": "815399"
  },
  {
    "text": "optimize that memory make it the llm to make it small so that it can now run in",
    "start": "815399",
    "end": "820560"
  },
  {
    "text": "this Ed devices which is where converting an llm to an slm is happening right but you can also optimize further",
    "start": "820560",
    "end": "826600"
  },
  {
    "text": "the slm to a size where now you can run in your laptops in your phones right and then finally you go ahead and",
    "start": "826600",
    "end": "832720"
  },
  {
    "text": "deploy uh so these are three three or four steps right essentially three steps right you start with uh you take a pre",
    "start": "832720",
    "end": "839120"
  },
  {
    "text": "slm authored in pyto tensorflow any of your languages go ahead and now do on device optimization right uh you would",
    "start": "839120",
    "end": "845720"
  },
  {
    "text": "do things like quantization graph pruning and a lot of these I will not go into details of that potentially you are",
    "start": "845720",
    "end": "852120"
  },
  {
    "text": "pruning shrinking the model to now use lesser memory now use lesser compute and",
    "start": "852120",
    "end": "858199"
  },
  {
    "text": "also give you the sponses faster right uh now once you have that what you do is Now package the model into a binary",
    "start": "858199",
    "end": "865639"
  },
  {
    "text": "which can now be deployed uh in your EDD devices right so in this case uh we'll use Wasa web",
    "start": "865639",
    "end": "872839"
  },
  {
    "text": "assembly to now help us to deploy this to end customers and users so that it's like it's like Docker for the cloud",
    "start": "872839",
    "end": "879199"
  },
  {
    "text": "right so sorry Docker for the web right so I can write it once and run it anywhere I want uh so the the secret",
    "start": "879199",
    "end": "887160"
  },
  {
    "text": "source is wasm along with uh web assembl media pipe so media pipe is an open",
    "start": "887160",
    "end": "892880"
  },
  {
    "text": "source project Cloud uh Cloud platform project for you to now deploy uh any of",
    "start": "892880",
    "end": "898120"
  },
  {
    "text": "your ml based uh workload right beat mobile web desktop embedded applications uh most of us might have",
    "start": "898120",
    "end": "905079"
  },
  {
    "text": "used uh Google meets right uh the background uh the feature that changes",
    "start": "905079",
    "end": "910399"
  },
  {
    "text": "your background that uses this technology right so that uses this to remove background on your device before",
    "start": "910399",
    "end": "916279"
  },
  {
    "text": "it ships sends your video Feit to your the other person on the other side right and it provides a very low code no code",
    "start": "916279",
    "end": "922680"
  },
  {
    "text": "way to build deploy this packages uh essentially this right and it's not only",
    "start": "922680",
    "end": "927920"
  },
  {
    "text": "llm or you could do things like Bo estimation U voice classification speech",
    "start": "927920",
    "end": "933519"
  },
  {
    "text": "to text text to speech and all of these scenarios all with media pipe right uh while medip was there they were not able",
    "start": "933519",
    "end": "940160"
  },
  {
    "text": "to run um u llm slms in a way uh till they integrated something called as wasn",
    "start": "940160",
    "end": "947600"
  },
  {
    "text": "right uh so wasam was already there but wnn was an extension from wasam which now allows them to uh you know run this",
    "start": "947600",
    "end": "955560"
  },
  {
    "text": "ml inference right in existing uh uh model in Frameworks which also increases",
    "start": "955560",
    "end": "960920"
  },
  {
    "text": "the performance right for things like uh I think they use xn and pack and other things to esate the performance you",
    "start": "960920",
    "end": "967000"
  },
  {
    "text": "could run these models in SDS gpus tpus fpgs or which were written in various",
    "start": "967000",
    "end": "974079"
  },
  {
    "text": "Frameworks right as flow on exop and whatnot so it looks something like this right so you",
    "start": "974079",
    "end": "979759"
  },
  {
    "text": "have your W on top which now has the ml backends to uh infer what is written in",
    "start": "979759",
    "end": "987319"
  },
  {
    "text": "the model how to run it and then at the end it has it can you can have CPUs gpus tpus whichever is available to now",
    "start": "987319",
    "end": "993839"
  },
  {
    "text": "access them and run your workloads there uh yeah so this is a process right",
    "start": "993839",
    "end": "999279"
  },
  {
    "text": "so you start with an um model example mobile net you now take that uh and",
    "start": "999279",
    "end": "1006199"
  },
  {
    "text": "convert them into a mobile net or wasm file mobile net wasm container which now allows you to run this container",
    "start": "1006199",
    "end": "1012560"
  },
  {
    "text": "anywhere in the world right uh and this container essentially you can ship your native code right be rust python C+",
    "start": "1012560",
    "end": "1018920"
  },
  {
    "text": "Plus+ which can now run in that uh browser environment and then uh in the",
    "start": "1018920",
    "end": "1024720"
  },
  {
    "text": "in the deployment side essentially your web page just loads this wasm file which can now talk to your web GPU runtime and",
    "start": "1024720",
    "end": "1031438"
  },
  {
    "text": "run the ml applications on your machine uh yeah so if you a simpler",
    "start": "1031439",
    "end": "1038640"
  },
  {
    "text": "architecture is this that you have a model which is in concept js on next time media pip which now might have a GS",
    "start": "1038640",
    "end": "1044880"
  },
  {
    "text": "component for you to now inference or talk through that model right and then the wasm container which has a model",
    "start": "1044880",
    "end": "1050400"
  },
  {
    "text": "itself and then internally it talks to if a GPU is available well in good for example Max they have GPU CPU combined",
    "start": "1050400",
    "end": "1057080"
  },
  {
    "text": "right so it talks to that to escalate your performance and give you back if it if it doesn't have a GPU it will default",
    "start": "1057080",
    "end": "1062440"
  },
  {
    "text": "your CPU to still give you the performance or still give [Music] you uh yeah a typical scenario right",
    "start": "1062440",
    "end": "1068720"
  },
  {
    "text": "let's say when you are building something right this is how it looks right so uh essentially you will collect the",
    "start": "1068720",
    "end": "1074200"
  },
  {
    "text": "user data go ahead do a model training uh then um uh Builder model maybe make",
    "start": "1074200",
    "end": "1080679"
  },
  {
    "text": "it available via Firebase or any other web distribution platform for you to now do over the deployments right on your",
    "start": "1080679",
    "end": "1087200"
  },
  {
    "text": "mobile you'll have this on device container which runs uh wasm and then",
    "start": "1087200",
    "end": "1092520"
  },
  {
    "text": "this just checks if a model is available from the cloud downloads it make it available locally and then whenever user",
    "start": "1092520",
    "end": "1098400"
  },
  {
    "text": "gives the request it is running everything locally to give the response out uh yeah so first let's say you have",
    "start": "1098400",
    "end": "1105159"
  },
  {
    "text": "a model right uh you have a model uh first the is if you have a native model",
    "start": "1105159",
    "end": "1110240"
  },
  {
    "text": "right for example if a model which is just train you can directly just call converted conversion config to now",
    "start": "1110240",
    "end": "1116320"
  },
  {
    "text": "convert that model in a format in a bin format which can now run um in your uh",
    "start": "1116320",
    "end": "1124520"
  },
  {
    "text": "like media can Now understand and and run that right if you let's say you",
    "start": "1124520",
    "end": "1130440"
  },
  {
    "text": "don't use the base model but you are fine-tuning it right you might be using L right so what you just do have to do",
    "start": "1130440",
    "end": "1135520"
  },
  {
    "text": "is have to say Okay this is my L config this is my L checkpoint uh this is a TF light file that I want",
    "start": "1135520",
    "end": "1141240"
  },
  {
    "text": "to adapt it to now it can add everything together and give you a be file which can be now used to uh run the",
    "start": "1141240",
    "end": "1148960"
  },
  {
    "text": "model uh and then once your model is available you just do bundler do bundle config to now uh bundle that model into",
    "start": "1148960",
    "end": "1155799"
  },
  {
    "text": "a wasum container sorry a model file which can now the like w container can",
    "start": "1155799",
    "end": "1161559"
  },
  {
    "text": "use to run that model uh wherever you want right uh and and this is how your",
    "start": "1161559",
    "end": "1167000"
  },
  {
    "text": "pipeline looks like right so you would use some something like as a wasm and media pipe together to allow you to run",
    "start": "1167000",
    "end": "1172320"
  },
  {
    "text": "this on top of that the ml inference itself is accelerated by imp right and slmc could use gamma and any other",
    "start": "1172320",
    "end": "1178799"
  },
  {
    "text": "models uh if we look at here right something like this so this is all",
    "start": "1178799",
    "end": "1185799"
  },
  {
    "text": "uh the code you have if you're if you're using something out of the box that's all you have to do right you just have to create a JS file uh there you would",
    "start": "1185799",
    "end": "1193039"
  },
  {
    "text": "say okay this is the media pipe task container I want to use maybe if it's not visible",
    "start": "1193039",
    "end": "1200639"
  },
  {
    "text": "yeah so you would just add this right you'll just add uh task container to say",
    "start": "1201799",
    "end": "1207320"
  },
  {
    "text": "okay I'm going to use uh this container then what you do is",
    "start": "1207320",
    "end": "1213600"
  },
  {
    "text": "uh yeah you now uh from options from the file set you are loading that me model",
    "start": "1213919",
    "end": "1219520"
  },
  {
    "text": "into the memory right and then you same parameters right next tokens stop care temperatures and whatever uh then",
    "start": "1219520",
    "end": "1226440"
  },
  {
    "text": "basically what you do is you now started inferring right whenever you come you just uh call this to now generate more",
    "start": "1226440",
    "end": "1234480"
  },
  {
    "text": "tokens for you and as the tokens are being generated you are uh displaying",
    "start": "1234480",
    "end": "1239720"
  },
  {
    "text": "them on the user right a simple web app just with this and then I have an HTML to just take the input and uh show the",
    "start": "1239720",
    "end": "1246280"
  },
  {
    "text": "output to the user right uh uh simple application of this right",
    "start": "1246280",
    "end": "1253320"
  },
  {
    "text": "so you see as I loading uh this is downloading the bin file right it has downloaded the bin file",
    "start": "1253320",
    "end": "1259120"
  },
  {
    "text": "on the browser itself and then it is running everything there right and I could ask",
    "start": "1259120",
    "end": "1265240"
  },
  {
    "text": "like so if you see the speed right it since everything's rning locally you are",
    "start": "1279760",
    "end": "1285400"
  },
  {
    "text": "you essentially don't see any latency any performance issues right it depends on the compute that you have yes the",
    "start": "1285400",
    "end": "1291760"
  },
  {
    "text": "performance might vary if you have a high Machine versus a phone versus it it varies but again the the performance",
    "start": "1291760",
    "end": "1299080"
  },
  {
    "text": "across uh is still better than connecting to cloud and getting the response back right",
    "start": "1299080",
    "end": "1306159"
  },
  {
    "text": "uh and and this is what right so there are two things that we are tracking here right we're not tracking things like Mees uh and and latency but two things",
    "start": "1306159",
    "end": "1313720"
  },
  {
    "text": "we're tracking is prefill performance and decode performance uh prefill performance basically whenever I give an",
    "start": "1313720",
    "end": "1319000"
  },
  {
    "text": "input how much time the model takes to process that and process that right and if you see here it is under 600 like 600",
    "start": "1319000",
    "end": "1327520"
  },
  {
    "text": "tokens per second right is the speed at which it can work and at the same time",
    "start": "1327520",
    "end": "1333200"
  },
  {
    "text": "decode performance is okay I have given a prompt how much time the model takes to Now give me response right it is",
    "start": "1333200",
    "end": "1338679"
  },
  {
    "text": "around as average is around 25 tokens per second is and that's that's more",
    "start": "1338679",
    "end": "1343960"
  },
  {
    "text": "than human readable speed that I cannot read 25 seconds in 25 tokens in this uh",
    "start": "1343960",
    "end": "1349640"
  },
  {
    "text": "second all together right uh yeah and the second is now okay you have deployed",
    "start": "1349640",
    "end": "1355840"
  },
  {
    "text": "uh you you explore deploying it on our web devices what if I want to deploy it in Edge devices right let's say in fpgs",
    "start": "1355840",
    "end": "1363279"
  },
  {
    "text": "or your Rasberry PS or things like this right so which is where we have Wasa meas R time right and it's the same",
    "start": "1363279",
    "end": "1369200"
  },
  {
    "text": "right you just download the container and this is just three steps for you to now be able to download this and run it",
    "start": "1369200",
    "end": "1375279"
  },
  {
    "text": "right I think uh second state has a booth outside where are demoing this and looking at how you can run this right so",
    "start": "1375279",
    "end": "1381440"
  },
  {
    "text": "all you do is uh same you just download the llm chat VSM container and then you pass it the",
    "start": "1381440",
    "end": "1388480"
  },
  {
    "text": "model file that metm instruct ggf so GF is a format which allows you to now run",
    "start": "1388480",
    "end": "1393520"
  },
  {
    "text": "this and then you just run right uh you you start preloading it and then you can",
    "start": "1393520",
    "end": "1398720"
  },
  {
    "text": "just talk to it give your inputs and it will start giving output so imagine and user terminals which are very low",
    "start": "1398720",
    "end": "1403919"
  },
  {
    "text": "compute smaller uh things like Jets and Nano boards things like raspberry pip or even I think day before yesterday Google",
    "start": "1403919",
    "end": "1411240"
  },
  {
    "text": "has launched for you to allow to run it in a serverless fashion right you could run it in Cloud run where you don't essentially need to provision a server",
    "start": "1411240",
    "end": "1418320"
  },
  {
    "text": "to be able to run these right uh I think that's all uh we had U you know",
    "start": "1418320",
    "end": "1424919"
  },
  {
    "text": "essentially what you are doing is slms are making this model smaller uh and",
    "start": "1424919",
    "end": "1430120"
  },
  {
    "text": "these small models can beat uh these models in those specific tasks right and I think Technologies like media p was",
    "start": "1430120",
    "end": "1436840"
  },
  {
    "text": "some Edge was and time allows you to increase ad option reach those end users",
    "start": "1436840",
    "end": "1442120"
  },
  {
    "text": "reach those users who don't have those compute U or even even Network availability to even use these",
    "start": "1442120",
    "end": "1447760"
  },
  {
    "text": "Technologies uh yeah I think uh that's all that's all for us there's some links for you to go ahead and understand this",
    "start": "1447760",
    "end": "1453320"
  },
  {
    "text": "in detail uh thank you thanks everyone any",
    "start": "1453320",
    "end": "1458080"
  },
  {
    "text": "[Applause] questions yeah please thanks uh this was this great",
    "start": "1460170",
    "end": "1466919"
  },
  {
    "text": "presentation uh just a quick uh thing the the link at the end doesn't work",
    "start": "1466919",
    "end": "1472120"
  },
  {
    "text": "okay I I'll update yeah so I love the slides so I would love a copy of those and second of all um our organization we",
    "start": "1472120",
    "end": "1478559"
  },
  {
    "text": "don't use stuff like GitHub co-pilot because of the proprietary nature of our",
    "start": "1478559",
    "end": "1484799"
  },
  {
    "text": "code we don't want GitHub to have access to it um would Edge deployment of llms",
    "start": "1484799",
    "end": "1493360"
  },
  {
    "text": "for instance the the Cod readed the code llm be useful for a situation like this",
    "start": "1493360",
    "end": "1499919"
  },
  {
    "text": "and do you recomend exactly right so exactly the this one that you see right the demo that I showed you right this is exactly for that right for me to able to",
    "start": "1499919",
    "end": "1507399"
  },
  {
    "text": "have access to and copilot still not sending anything outside right I'm sending anything outs second is um I",
    "start": "1507399",
    "end": "1513399"
  },
  {
    "text": "don't have that link available but we can chat offline and give you link one friend of mine works at plug P so he",
    "start": "1513399",
    "end": "1518799"
  },
  {
    "text": "created a uh vs code plugin so they takes your code trains a model in your",
    "start": "1518799",
    "end": "1525039"
  },
  {
    "text": "machine and then makes it available for you as a vs code plugin right same co-pilot experience but it's on your",
    "start": "1525039",
    "end": "1531080"
  },
  {
    "text": "data on locally and everything right right and so it's trained on your own yeah on your data on your machine so nothing is going out that's great yeah's",
    "start": "1531080",
    "end": "1537720"
  },
  {
    "text": "chat offline you could build those Technologies again let's say like how it helps me is here I cannot access open or",
    "start": "1537720",
    "end": "1544799"
  },
  {
    "text": "J right because of geography restrictions but I still have an slm in the same experience that I can talk to",
    "start": "1544799",
    "end": "1550520"
  },
  {
    "text": "and get my work done understands and has been trained on your code yeah that's great yeah let's chat about that after this that's great",
    "start": "1550520",
    "end": "1558880"
  },
  {
    "text": "I would like to ask did you guys have the experience on using the par dramma in the rep AI I see the jamera is is",
    "start": "1562640",
    "end": "1571440"
  },
  {
    "text": "working but is is how about the multim model so to be able to deploy yeah yeah",
    "start": "1571440",
    "end": "1577600"
  },
  {
    "text": "so SE wasm allows you to run not only poly but things like uh uh you know",
    "start": "1577600",
    "end": "1582840"
  },
  {
    "text": "Delhi distill version of Delhi right for you to generate images in in the scenarios and how about the size of the",
    "start": "1582840",
    "end": "1589640"
  },
  {
    "text": "model that you guy tell as the as I know the the the size of the model for for",
    "start": "1589640",
    "end": "1595039"
  },
  {
    "text": "example par drama is is quite large and I'm not sure about is how much memory",
    "start": "1595039",
    "end": "1600760"
  },
  {
    "text": "all the data for the for the browsers to be able to run a a soal not very small L",
    "start": "1600760",
    "end": "1608200"
  },
  {
    "text": "yeah so uh what we have tested so far right is uh at least if you have in a phone around 4 GB of memory right some",
    "start": "1608200",
    "end": "1615919"
  },
  {
    "text": "of these models like 53 uh you know gamma 2 right gamma models run yeah",
    "start": "1615919",
    "end": "1621640"
  },
  {
    "text": "right uh there are some Hardware Conant let's say if I'm using on browser right I cannot low I think have more than 500",
    "start": "1621640",
    "end": "1629799"
  },
  {
    "text": "MB of cach right I cannot save that large model in Cache so where I have to maybe save it in uh the the dis of the",
    "start": "1629799",
    "end": "1636600"
  },
  {
    "text": "machine to Now read it from there right there are some um ways for like tweaks you have to do to get it done right",
    "start": "1636600",
    "end": "1642520"
  },
  {
    "text": "second is if you're doing building an IOS app right iOS natively doesn't allows any model more than size of 2GB",
    "start": "1642520",
    "end": "1649640"
  },
  {
    "text": "right so it has to be under 2GB so GMA is a very good candidate there because the size once you quantize it once you",
    "start": "1649640",
    "end": "1655720"
  },
  {
    "text": "make it available it is just under 2GB okay for you the model size but this it's just the compiler we the size of",
    "start": "1655720",
    "end": "1662760"
  },
  {
    "text": "the model to to be able to run so so the size is not exactly from haen phrase",
    "start": "1662760",
    "end": "1669519"
  },
  {
    "text": "they are they are for Python and and I think it's small so this step right",
    "start": "1669519",
    "end": "1675039"
  },
  {
    "text": "conver yeah this step right so this step also allows you to shrink the size so you would do uh things like you see uh",
    "start": "1675039",
    "end": "1680840"
  },
  {
    "text": "this checkpoint formatting right so you can say Okay I want an in8 model right in of float 16 I want an in8 so that",
    "start": "1680840",
    "end": "1686559"
  },
  {
    "text": "conversion makes this model smaller yes there will be some performance loss right but you'll have to overcome",
    "start": "1686559",
    "end": "1692039"
  },
  {
    "text": "basically you'll have 5 to 10% of performance loss because you are doing a low Precision compute but you can do the",
    "start": "1692039",
    "end": "1698320"
  },
  {
    "text": "same thing getting done and also about the loading time is the model gets largest is yeah yeah it it depends on",
    "start": "1698320",
    "end": "1705120"
  },
  {
    "text": "the internet right if the model the file is large uh you'll still need to download the container in the browser",
    "start": "1705120",
    "end": "1710559"
  },
  {
    "text": "right so that is a bit High time but suppose to able to cat it in the browser as for my use case maybe the different",
    "start": "1710559",
    "end": "1718559"
  },
  {
    "text": "different website may may want to share the same model for example oh lo the",
    "start": "1718559",
    "end": "1723799"
  },
  {
    "text": "parras is there and then they go to different website and then we use the the model um but as I know at this",
    "start": "1723799",
    "end": "1731440"
  },
  {
    "text": "moment the cash is something like browser cach yeah so browser cache uh let say Chrome or Firefox they don't",
    "start": "1731440",
    "end": "1737559"
  },
  {
    "text": "allow more more than 500 MB of cash okay so you have to write in your local storage and then read it from there okay",
    "start": "1737559",
    "end": "1744559"
  },
  {
    "text": "okay thank you but but this is about the cashing isue is the model is getting larger is able to beuse for different",
    "start": "1744559",
    "end": "1751480"
  },
  {
    "text": "website uh it will be great as the first time maybe maybe even build in in the",
    "start": "1751480",
    "end": "1756760"
  },
  {
    "text": "Chrome later we can install the PLU in and then download some model from extension and then keep me excit yeah so",
    "start": "1756760",
    "end": "1763399"
  },
  {
    "text": "uh so there is uh there is something called a Gemini Nano that is uh now",
    "start": "1763399",
    "end": "1769600"
  },
  {
    "text": "being released as part of chrome as as a inbuilt chrome U you know feature uh",
    "start": "1769600",
    "end": "1775720"
  },
  {
    "text": "that will sort of enable you to sort of use this without uh loading the model but that is like very specific models",
    "start": "1775720",
    "end": "1781159"
  },
  {
    "text": "that I mean you can't it's basically Google propriety models that you will use as part of chrome as as a product",
    "start": "1781159",
    "end": "1788080"
  },
  {
    "text": "here you can use sort of any open model which you can sort of fit in your uh",
    "start": "1788080",
    "end": "1793240"
  },
  {
    "text": "Hardware or infrastructure and also uh there are other uh Vision models uh",
    "start": "1793240",
    "end": "1798480"
  },
  {
    "text": "previous to Palama like you can go to tensorflow HUB and sort of look at mobile net and all smaller models",
    "start": "1798480",
    "end": "1804840"
  },
  {
    "text": "depends on your uh task specifically let's say you want to sort of do just",
    "start": "1804840",
    "end": "1809880"
  },
  {
    "text": "image captioning um you know you can sort of use those models as well they they smaller in size not as big as pen",
    "start": "1809880",
    "end": "1816440"
  },
  {
    "text": "okay okay thank",
    "start": "1816440",
    "end": "1819080"
  },
  {
    "text": "you any last questions I think we have four minutes",
    "start": "1823480",
    "end": "1829120"
  },
  {
    "text": "sure thank you [Applause]",
    "start": "1851960",
    "end": "1857470"
  }
]