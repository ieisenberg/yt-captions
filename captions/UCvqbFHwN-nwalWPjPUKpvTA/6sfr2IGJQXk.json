[
  {
    "start": "0",
    "end": "243000"
  },
  {
    "text": "welcome",
    "start": "2720",
    "end": "3600"
  },
  {
    "text": "this session is how we're dealing with",
    "start": "3600",
    "end": "5359"
  },
  {
    "text": "metrics at scale on gitlab.com",
    "start": "5359",
    "end": "9440"
  },
  {
    "text": "my name is andrew nudigate and i'm an",
    "start": "11759",
    "end": "13599"
  },
  {
    "text": "engineer at gitlab where i work in the",
    "start": "13599",
    "end": "15280"
  },
  {
    "text": "infrastructure team",
    "start": "15280",
    "end": "16240"
  },
  {
    "text": "and i help work help build gitlab.com",
    "start": "16240",
    "end": "19520"
  },
  {
    "text": "this talk is about how we've scaled our",
    "start": "19520",
    "end": "21199"
  },
  {
    "text": "monitoring to support a site that has",
    "start": "21199",
    "end": "23359"
  },
  {
    "text": "over the past few years grown rapidly in",
    "start": "23359",
    "end": "25519"
  },
  {
    "text": "size and complexity",
    "start": "25519",
    "end": "27599"
  },
  {
    "text": "to illustrate that growth here are some",
    "start": "27599",
    "end": "29439"
  },
  {
    "text": "figures to show how things have changed",
    "start": "29439",
    "end": "31359"
  },
  {
    "text": "since i joined",
    "start": "31359",
    "end": "34160"
  },
  {
    "text": "back in 2017 we'd only recently adopted",
    "start": "34800",
    "end": "37600"
  },
  {
    "text": "prometheus and were migrating off influx",
    "start": "37600",
    "end": "39680"
  },
  {
    "text": "db",
    "start": "39680",
    "end": "40559"
  },
  {
    "text": "we had a single prometheus server six",
    "start": "40559",
    "end": "42640"
  },
  {
    "text": "infrastructure engineers a handful of",
    "start": "42640",
    "end": "44559"
  },
  {
    "text": "alerting rules",
    "start": "44559",
    "end": "45680"
  },
  {
    "text": "and recording rules we only had 21",
    "start": "45680",
    "end": "48000"
  },
  {
    "text": "dashboards and we were processing about",
    "start": "48000",
    "end": "49920"
  },
  {
    "text": "100 000 samples per second",
    "start": "49920",
    "end": "53680"
  },
  {
    "text": "roll forward 400 years and we now run",
    "start": "53680",
    "end": "56879"
  },
  {
    "text": "thanos federated cluster deployed into",
    "start": "56879",
    "end": "59120"
  },
  {
    "text": "kubernetes using tanka",
    "start": "59120",
    "end": "60800"
  },
  {
    "text": "the infrastructure department is around",
    "start": "60800",
    "end": "62719"
  },
  {
    "text": "40 people so",
    "start": "62719",
    "end": "63840"
  },
  {
    "text": "six times bigger we have over 2 600",
    "start": "63840",
    "end": "67439"
  },
  {
    "text": "recording rules 400 grafana dashboards",
    "start": "67439",
    "end": "70159"
  },
  {
    "text": "and we're ingesting about 2.8 million",
    "start": "70159",
    "end": "72080"
  },
  {
    "text": "samples per second",
    "start": "72080",
    "end": "75040"
  },
  {
    "text": "so it's important to state this what",
    "start": "75920",
    "end": "77600"
  },
  {
    "text": "worked for us then worked fine for us at",
    "start": "77600",
    "end": "79439"
  },
  {
    "text": "that scale",
    "start": "79439",
    "end": "80159"
  },
  {
    "text": "it was the right solution at the time",
    "start": "80159",
    "end": "82159"
  },
  {
    "text": "but that approach wouldn't work for us",
    "start": "82159",
    "end": "83759"
  },
  {
    "text": "now",
    "start": "83759",
    "end": "84240"
  },
  {
    "text": "and this talk is about some of the tools",
    "start": "84240",
    "end": "86000"
  },
  {
    "text": "and techniques that we've used to go",
    "start": "86000",
    "end": "87600"
  },
  {
    "text": "from that scale to where we are today",
    "start": "87600",
    "end": "91438"
  },
  {
    "text": "so what prompted our efforts to improve",
    "start": "93200",
    "end": "95119"
  },
  {
    "text": "our alerting",
    "start": "95119",
    "end": "96720"
  },
  {
    "text": "we were seeing numerous problems that",
    "start": "96720",
    "end": "98479"
  },
  {
    "text": "indicated that our approach to",
    "start": "98479",
    "end": "99759"
  },
  {
    "text": "monitoring was no longer working for us",
    "start": "99759",
    "end": "102479"
  },
  {
    "text": "one of these problems was low precision",
    "start": "102479",
    "end": "104479"
  },
  {
    "text": "alerting",
    "start": "104479",
    "end": "106560"
  },
  {
    "text": "by this we mean that the proportion of",
    "start": "106560",
    "end": "108320"
  },
  {
    "text": "alerts that was actionable",
    "start": "108320",
    "end": "109759"
  },
  {
    "text": "was low and we're seeing a high number",
    "start": "109759",
    "end": "111680"
  },
  {
    "text": "of false positives",
    "start": "111680",
    "end": "113680"
  },
  {
    "text": "at any time many of the alerting rules",
    "start": "113680",
    "end": "116640"
  },
  {
    "text": "inadvertently generated low quality",
    "start": "116640",
    "end": "118799"
  },
  {
    "text": "unactionable flappy alerts very often",
    "start": "118799",
    "end": "121600"
  },
  {
    "text": "the engineer on call would determine",
    "start": "121600",
    "end": "123119"
  },
  {
    "text": "that users were not being impacted",
    "start": "123119",
    "end": "124880"
  },
  {
    "text": "that everything seemed okay and they",
    "start": "124880",
    "end": "126719"
  },
  {
    "text": "would acknowledge the alert",
    "start": "126719",
    "end": "128000"
  },
  {
    "text": "and effectively ignore it",
    "start": "128000",
    "end": "131200"
  },
  {
    "text": "not only was the precision of our alerts",
    "start": "131920",
    "end": "133680"
  },
  {
    "text": "very poor but so was the recall",
    "start": "133680",
    "end": "135920"
  },
  {
    "text": "recall refers to the proportion of user",
    "start": "135920",
    "end": "138000"
  },
  {
    "text": "impacting events that are detected by",
    "start": "138000",
    "end": "139840"
  },
  {
    "text": "the alerting system",
    "start": "139840",
    "end": "141280"
  },
  {
    "text": "this means that instead of finding out",
    "start": "141280",
    "end": "142959"
  },
  {
    "text": "about incidents through an alert we",
    "start": "142959",
    "end": "144640"
  },
  {
    "text": "would sometimes be",
    "start": "144640",
    "end": "145599"
  },
  {
    "text": "made aware of the incident by people",
    "start": "145599",
    "end": "148000"
  },
  {
    "text": "rather than the software that we'd built",
    "start": "148000",
    "end": "150000"
  },
  {
    "text": "to detect these incidents in other cases",
    "start": "150000",
    "end": "152720"
  },
  {
    "text": "the alert would fire",
    "start": "152720",
    "end": "154080"
  },
  {
    "text": "but too late and we already knew that",
    "start": "154080",
    "end": "156080"
  },
  {
    "text": "there was a problem and now it was just",
    "start": "156080",
    "end": "157840"
  },
  {
    "text": "extra noise while we were trying to",
    "start": "157840",
    "end": "159360"
  },
  {
    "text": "solve the issue",
    "start": "159360",
    "end": "162080"
  },
  {
    "text": "yet another problem we found was that",
    "start": "162720",
    "end": "164560"
  },
  {
    "text": "the dashboards were very often broken",
    "start": "164560",
    "end": "166879"
  },
  {
    "text": "and not working as we expected since our",
    "start": "166879",
    "end": "169120"
  },
  {
    "text": "dashboards were not",
    "start": "169120",
    "end": "170000"
  },
  {
    "text": "managed alongside our other metrics",
    "start": "170000",
    "end": "171920"
  },
  {
    "text": "there was no way of validating that they",
    "start": "171920",
    "end": "173680"
  },
  {
    "text": "were still working",
    "start": "173680",
    "end": "174560"
  },
  {
    "text": "until we took a look at them and this",
    "start": "174560",
    "end": "176640"
  },
  {
    "text": "often happened during an incident",
    "start": "176640",
    "end": "178239"
  },
  {
    "text": "so now instead of having one problem we",
    "start": "178239",
    "end": "180400"
  },
  {
    "text": "had two and that we had to fix the",
    "start": "180400",
    "end": "182000"
  },
  {
    "text": "dashboard",
    "start": "182000",
    "end": "182640"
  },
  {
    "text": "before we could fix the problem",
    "start": "182640",
    "end": "185840"
  },
  {
    "text": "one of the things that we began to",
    "start": "187120",
    "end": "188640"
  },
  {
    "text": "realize was that having three distinct",
    "start": "188640",
    "end": "190480"
  },
  {
    "text": "configurations for our metric stack was",
    "start": "190480",
    "end": "192640"
  },
  {
    "text": "part of the problem",
    "start": "192640",
    "end": "193760"
  },
  {
    "text": "the source of metrics was independent",
    "start": "193760",
    "end": "195599"
  },
  {
    "text": "from alerting and recording rules",
    "start": "195599",
    "end": "197360"
  },
  {
    "text": "our alerting and recording rules were",
    "start": "197360",
    "end": "199440"
  },
  {
    "text": "managed independently from our",
    "start": "199440",
    "end": "200720"
  },
  {
    "text": "dashboards",
    "start": "200720",
    "end": "201760"
  },
  {
    "text": "and our dashboards were installed in git",
    "start": "201760",
    "end": "203840"
  },
  {
    "text": "and they didn't have any form of change",
    "start": "203840",
    "end": "205280"
  },
  {
    "text": "control",
    "start": "205280",
    "end": "206000"
  },
  {
    "text": "and they weren't validated",
    "start": "206000",
    "end": "213040"
  },
  {
    "text": "with this in mind we set out to improve",
    "start": "213040",
    "end": "214959"
  },
  {
    "text": "our stack with these goals",
    "start": "214959",
    "end": "217519"
  },
  {
    "text": "one to develop a common monitoring",
    "start": "217519",
    "end": "219599"
  },
  {
    "text": "strategy across",
    "start": "219599",
    "end": "220560"
  },
  {
    "text": "all of our services based on a set of",
    "start": "220560",
    "end": "222319"
  },
  {
    "text": "key metrics and service level indicators",
    "start": "222319",
    "end": "225599"
  },
  {
    "text": "two use those metrics to improve the",
    "start": "225599",
    "end": "228000"
  },
  {
    "text": "precision recall and detection time",
    "start": "228000",
    "end": "230400"
  },
  {
    "text": "of our alerts and three unify our",
    "start": "230400",
    "end": "233200"
  },
  {
    "text": "metrics slo",
    "start": "233200",
    "end": "234400"
  },
  {
    "text": "loading configuration recording rules",
    "start": "234400",
    "end": "236319"
  },
  {
    "text": "dashboards everything into a single",
    "start": "236319",
    "end": "238480"
  },
  {
    "text": "source",
    "start": "238480",
    "end": "238959"
  },
  {
    "text": "to avoid inconsistencies between the",
    "start": "238959",
    "end": "240720"
  },
  {
    "text": "definitions",
    "start": "240720",
    "end": "243439"
  },
  {
    "start": "243000",
    "end": "367000"
  },
  {
    "text": "let's look at how we tackle the first",
    "start": "243599",
    "end": "245120"
  },
  {
    "text": "goal of building a set of key metrics",
    "start": "245120",
    "end": "246640"
  },
  {
    "text": "for our application",
    "start": "246640",
    "end": "249280"
  },
  {
    "text": "we based our metrics on google's four",
    "start": "249280",
    "end": "251280"
  },
  {
    "text": "golden signals but made some changes to",
    "start": "251280",
    "end": "253200"
  },
  {
    "text": "better fit our requirements",
    "start": "253200",
    "end": "255040"
  },
  {
    "text": "for latency we measure aptx as a ratio",
    "start": "255040",
    "end": "258160"
  },
  {
    "text": "rather than a percentile duration",
    "start": "258160",
    "end": "259919"
  },
  {
    "text": "measurements in seconds",
    "start": "259919",
    "end": "261759"
  },
  {
    "text": "requests and errors are measured at a",
    "start": "261759",
    "end": "263759"
  },
  {
    "text": "per second rate and saturation is",
    "start": "263759",
    "end": "265680"
  },
  {
    "text": "measured as a percent",
    "start": "265680",
    "end": "267120"
  },
  {
    "text": "lower being better",
    "start": "267120",
    "end": "269840"
  },
  {
    "text": "saturation is a pretty big topic on its",
    "start": "271040",
    "end": "273040"
  },
  {
    "text": "own so i'm not going to go into detail",
    "start": "273040",
    "end": "274800"
  },
  {
    "text": "in this today",
    "start": "274800",
    "end": "275840"
  },
  {
    "text": "if you're interested in finding out more",
    "start": "275840",
    "end": "277759"
  },
  {
    "text": "here's a plug for a talk i did on a",
    "start": "277759",
    "end": "279919"
  },
  {
    "text": "saturation on saturation monitoring on",
    "start": "279919",
    "end": "281680"
  },
  {
    "text": "gitlab.com",
    "start": "281680",
    "end": "282800"
  },
  {
    "text": "i've included a link to the slides",
    "start": "282800",
    "end": "286638"
  },
  {
    "text": "with our key metrics decided on the next",
    "start": "288080",
    "end": "290479"
  },
  {
    "text": "step was to break the application down",
    "start": "290479",
    "end": "292560"
  },
  {
    "text": "first into a set of services and then",
    "start": "292560",
    "end": "294479"
  },
  {
    "text": "break each service down into a set of",
    "start": "294479",
    "end": "296320"
  },
  {
    "text": "components",
    "start": "296320",
    "end": "297520"
  },
  {
    "text": "so for example we modeled web git and",
    "start": "297520",
    "end": "300000"
  },
  {
    "text": "api services",
    "start": "300000",
    "end": "301199"
  },
  {
    "text": "and then broke these down further into",
    "start": "301199",
    "end": "303199"
  },
  {
    "text": "one or more components",
    "start": "303199",
    "end": "304960"
  },
  {
    "text": "for the git servers for example we have",
    "start": "304960",
    "end": "306960"
  },
  {
    "text": "ssh and https components",
    "start": "306960",
    "end": "309600"
  },
  {
    "text": "each component has three key metrics app",
    "start": "309600",
    "end": "312240"
  },
  {
    "text": "decks",
    "start": "312240",
    "end": "312800"
  },
  {
    "text": "errors and requests so for some",
    "start": "312800",
    "end": "315199"
  },
  {
    "text": "components",
    "start": "315199",
    "end": "316479"
  },
  {
    "text": "or sorry part of me for some components",
    "start": "316479",
    "end": "318720"
  },
  {
    "text": "it's not always possible to measure",
    "start": "318720",
    "end": "320000"
  },
  {
    "text": "latency directly",
    "start": "320000",
    "end": "321120"
  },
  {
    "text": "so aptx is optional in those cases",
    "start": "321120",
    "end": "325840"
  },
  {
    "text": "from our three key metrics we're able to",
    "start": "326800",
    "end": "328720"
  },
  {
    "text": "derive two service level indicators or",
    "start": "328720",
    "end": "331120"
  },
  {
    "text": "slis",
    "start": "331120",
    "end": "332240"
  },
  {
    "text": "an sli is normally expressed as a",
    "start": "332240",
    "end": "334000"
  },
  {
    "text": "percentage of requests that are bad",
    "start": "334000",
    "end": "336639"
  },
  {
    "text": "an aptx is really the inverse of that",
    "start": "336639",
    "end": "338639"
  },
  {
    "text": "it's a percentage of requests that are",
    "start": "338639",
    "end": "340000"
  },
  {
    "text": "have a satisfactory latency or good",
    "start": "340000",
    "end": "342880"
  },
  {
    "text": "because our organization was already",
    "start": "342880",
    "end": "344560"
  },
  {
    "text": "using the concept of aptx we decided it",
    "start": "344560",
    "end": "346560"
  },
  {
    "text": "would be better to adapt our monitoring",
    "start": "346560",
    "end": "348400"
  },
  {
    "text": "system to the organization",
    "start": "348400",
    "end": "350240"
  },
  {
    "text": "rather than the other way around",
    "start": "350240",
    "end": "352000"
  },
  {
    "text": "therefore our aptx sli",
    "start": "352000",
    "end": "353840"
  },
  {
    "text": "is an inverted sli with 100 being the",
    "start": "353840",
    "end": "356960"
  },
  {
    "text": "best service level",
    "start": "356960",
    "end": "358800"
  },
  {
    "text": "for errors we use a conventional sli",
    "start": "358800",
    "end": "360960"
  },
  {
    "text": "definition with zero percent",
    "start": "360960",
    "end": "362720"
  },
  {
    "text": "being no errors and the best service",
    "start": "362720",
    "end": "364840"
  },
  {
    "text": "level",
    "start": "364840",
    "end": "367840"
  },
  {
    "start": "367000",
    "end": "500000"
  },
  {
    "text": "once we had our approach to monitoring",
    "start": "368240",
    "end": "369840"
  },
  {
    "text": "our key metrics in players it was time",
    "start": "369840",
    "end": "371440"
  },
  {
    "text": "to start thinking about our second goal",
    "start": "371440",
    "end": "373440"
  },
  {
    "text": "to improve the quality of our alerting",
    "start": "373440",
    "end": "377440"
  },
  {
    "text": "as i mentioned for each component we",
    "start": "379120",
    "end": "381360"
  },
  {
    "text": "derived two slis",
    "start": "381360",
    "end": "382960"
  },
  {
    "text": "app decks and errors for each of these",
    "start": "382960",
    "end": "385680"
  },
  {
    "text": "we set a service level objective",
    "start": "385680",
    "end": "388000"
  },
  {
    "text": "and trigger and triggered an alert if an",
    "start": "388000",
    "end": "390000"
  },
  {
    "text": "sli is violating its slo target",
    "start": "390000",
    "end": "392479"
  },
  {
    "text": "if our aptx is below sl is below the slo",
    "start": "392479",
    "end": "395360"
  },
  {
    "text": "threshold or our error ratio is above",
    "start": "395360",
    "end": "397360"
  },
  {
    "text": "slo we trigger",
    "start": "397360",
    "end": "398720"
  },
  {
    "text": "an alert for some services we also",
    "start": "398720",
    "end": "401600"
  },
  {
    "text": "trigger anomaly alerts for high request",
    "start": "401600",
    "end": "403919"
  },
  {
    "text": "rate anomalies",
    "start": "403919",
    "end": "409360"
  },
  {
    "text": "the original approach we took to alert",
    "start": "409360",
    "end": "411280"
  },
  {
    "text": "was any violations over a five minute",
    "start": "411280",
    "end": "413599"
  },
  {
    "text": "period",
    "start": "413599",
    "end": "415520"
  },
  {
    "text": "for example if you have a thousand",
    "start": "415520",
    "end": "417039"
  },
  {
    "text": "requests in a five minute period and two",
    "start": "417039",
    "end": "418880"
  },
  {
    "text": "of those requests results in an error",
    "start": "418880",
    "end": "420960"
  },
  {
    "text": "two in a thousand gives you zero point",
    "start": "420960",
    "end": "422720"
  },
  {
    "text": "two percent error rate",
    "start": "422720",
    "end": "424240"
  },
  {
    "text": "and if you have a ninety nine point nine",
    "start": "424240",
    "end": "425759"
  },
  {
    "text": "percent slo this zero point two percent",
    "start": "425759",
    "end": "428479"
  },
  {
    "text": "exceeds the zero point one percent",
    "start": "428479",
    "end": "430080"
  },
  {
    "text": "threshold",
    "start": "430080",
    "end": "430960"
  },
  {
    "text": "causing the alert to fire unfortunately",
    "start": "430960",
    "end": "433840"
  },
  {
    "text": "this is a very naive approach and it has",
    "start": "433840",
    "end": "436479"
  },
  {
    "text": "very poor precision",
    "start": "436479",
    "end": "437759"
  },
  {
    "text": "in that it generates a huge number of",
    "start": "437759",
    "end": "440319"
  },
  {
    "text": "false positives",
    "start": "440319",
    "end": "443120"
  },
  {
    "text": "taken to the extreme the alert could",
    "start": "443120",
    "end": "445199"
  },
  {
    "text": "fire hundreds of times a day",
    "start": "445199",
    "end": "446720"
  },
  {
    "text": "yet the slo the sli could still achieve",
    "start": "446720",
    "end": "449440"
  },
  {
    "text": "its",
    "start": "449440",
    "end": "449759"
  },
  {
    "text": "slo in fact our new alerting was no",
    "start": "449759",
    "end": "452960"
  },
  {
    "text": "better than the old alerts that we were",
    "start": "452960",
    "end": "454400"
  },
  {
    "text": "trying to improve on",
    "start": "454400",
    "end": "457199"
  },
  {
    "text": "we went back to the drawing board and",
    "start": "457759",
    "end": "459440"
  },
  {
    "text": "looked for better alternatives",
    "start": "459440",
    "end": "461280"
  },
  {
    "text": "we settled on using multi-window",
    "start": "461280",
    "end": "463199"
  },
  {
    "text": "multi-burn rate alerts instead",
    "start": "463199",
    "end": "466000"
  },
  {
    "text": "i'm not going to go into the details in",
    "start": "466000",
    "end": "467759"
  },
  {
    "text": "this talk but if you're interested in",
    "start": "467759",
    "end": "469520"
  },
  {
    "text": "knowing more google have published an",
    "start": "469520",
    "end": "471120"
  },
  {
    "text": "excellent guide in their sre workbook i",
    "start": "471120",
    "end": "473680"
  },
  {
    "text": "have included a link",
    "start": "473680",
    "end": "474800"
  },
  {
    "text": "on the slide this approach has provided",
    "start": "474800",
    "end": "477440"
  },
  {
    "text": "us with high precision load detection",
    "start": "477440",
    "end": "479280"
  },
  {
    "text": "time and good recall on our alerts",
    "start": "479280",
    "end": "481280"
  },
  {
    "text": "the problem with this approach is the",
    "start": "481280",
    "end": "483039"
  },
  {
    "text": "amount of complexity it brings",
    "start": "483039",
    "end": "485280"
  },
  {
    "text": "for each component that we monitor we",
    "start": "485280",
    "end": "487280"
  },
  {
    "text": "need 12 recording rules to be correctly",
    "start": "487280",
    "end": "489520"
  },
  {
    "text": "configured",
    "start": "489520",
    "end": "491520"
  },
  {
    "text": "with dozens of components you really",
    "start": "491520",
    "end": "493199"
  },
  {
    "text": "need a configuration tool to help with",
    "start": "493199",
    "end": "494960"
  },
  {
    "text": "this",
    "start": "494960",
    "end": "495440"
  },
  {
    "text": "as doing it manually would be very",
    "start": "495440",
    "end": "497199"
  },
  {
    "text": "painful",
    "start": "497199",
    "end": "499599"
  },
  {
    "start": "500000",
    "end": "845000"
  },
  {
    "text": "so before we could roll out our slo",
    "start": "501360",
    "end": "503440"
  },
  {
    "text": "alerting with multi-window multi-burn",
    "start": "503440",
    "end": "505440"
  },
  {
    "text": "rate alerts",
    "start": "505440",
    "end": "506560"
  },
  {
    "text": "we needed to investigate better tooling",
    "start": "506560",
    "end": "509280"
  },
  {
    "text": "to deal with all the repetitive",
    "start": "509280",
    "end": "510479"
  },
  {
    "text": "configuration that was required",
    "start": "510479",
    "end": "513440"
  },
  {
    "text": "for each service we may have several",
    "start": "513440",
    "end": "515360"
  },
  {
    "text": "dozen similar but slightly different",
    "start": "515360",
    "end": "516959"
  },
  {
    "text": "recording rules",
    "start": "516959",
    "end": "518240"
  },
  {
    "text": "in our dashboards we might have other",
    "start": "518240",
    "end": "520159"
  },
  {
    "text": "queries that are also similar but use",
    "start": "520159",
    "end": "522080"
  },
  {
    "text": "different aggregations",
    "start": "522080",
    "end": "523599"
  },
  {
    "text": "changing these area these queries was an",
    "start": "523599",
    "end": "525920"
  },
  {
    "text": "error-prone",
    "start": "525920",
    "end": "526800"
  },
  {
    "text": "error-prone process so we started",
    "start": "526800",
    "end": "529279"
  },
  {
    "text": "thinking about",
    "start": "529279",
    "end": "530160"
  },
  {
    "text": "what tools we could use to make this",
    "start": "530160",
    "end": "532000"
  },
  {
    "text": "process easier",
    "start": "532000",
    "end": "535839"
  },
  {
    "text": "the idea we had was to describe all of",
    "start": "537360",
    "end": "539440"
  },
  {
    "text": "our metrics in what we call the metrics",
    "start": "539440",
    "end": "541200"
  },
  {
    "text": "catalog",
    "start": "541200",
    "end": "542080"
  },
  {
    "text": "this is an abstract configuration it's",
    "start": "542080",
    "end": "544240"
  },
  {
    "text": "written in json and is designed to be",
    "start": "544240",
    "end": "546240"
  },
  {
    "text": "user-friendly",
    "start": "546240",
    "end": "547200"
  },
  {
    "text": "validatable and with as little",
    "start": "547200",
    "end": "549040"
  },
  {
    "text": "repetition as possible",
    "start": "549040",
    "end": "550959"
  },
  {
    "text": "the configuration is stored in git",
    "start": "550959",
    "end": "553120"
  },
  {
    "text": "change is managed through merge requests",
    "start": "553120",
    "end": "555279"
  },
  {
    "text": "on commit we use ci to validate the",
    "start": "555279",
    "end": "557279"
  },
  {
    "text": "config generate new prometheus recording",
    "start": "557279",
    "end": "559600"
  },
  {
    "text": "rules",
    "start": "559600",
    "end": "560160"
  },
  {
    "text": "alert configurations and grifana",
    "start": "560160",
    "end": "561920"
  },
  {
    "text": "dashboards amongst other resources",
    "start": "561920",
    "end": "565680"
  },
  {
    "text": "this is what a typical entry in the",
    "start": "566480",
    "end": "568240"
  },
  {
    "text": "catalog looks like",
    "start": "568240",
    "end": "569680"
  },
  {
    "text": "this definition is from the web service",
    "start": "569680",
    "end": "571839"
  },
  {
    "text": "and shows one component from that",
    "start": "571839",
    "end": "573440"
  },
  {
    "text": "service called",
    "start": "573440",
    "end": "574160"
  },
  {
    "text": "workhorse we define our slos as well as",
    "start": "574160",
    "end": "577440"
  },
  {
    "text": "our aptx",
    "start": "577440",
    "end": "579040"
  },
  {
    "text": "request rates and error rates that will",
    "start": "579040",
    "end": "580959"
  },
  {
    "text": "be used to generate the slis",
    "start": "580959",
    "end": "583839"
  },
  {
    "text": "these definitions are then used to",
    "start": "583839",
    "end": "585440"
  },
  {
    "text": "generate prometheus expressions",
    "start": "585440",
    "end": "587120"
  },
  {
    "text": "in our prometheus recording rule",
    "start": "587120",
    "end": "588959"
  },
  {
    "text": "configuration as well as dashboards and",
    "start": "588959",
    "end": "591360"
  },
  {
    "text": "everything else",
    "start": "591360",
    "end": "594000"
  },
  {
    "text": "as you can see this generates lots of",
    "start": "596080",
    "end": "598160"
  },
  {
    "text": "very similar but slightly different",
    "start": "598160",
    "end": "599839"
  },
  {
    "text": "prometheus configuration",
    "start": "599839",
    "end": "602320"
  },
  {
    "text": "depending on the burn rates that you're",
    "start": "602320",
    "end": "604079"
  },
  {
    "text": "evaluating",
    "start": "604079",
    "end": "606880"
  },
  {
    "text": "the last part of our goal was to",
    "start": "610560",
    "end": "612320"
  },
  {
    "text": "generate our dashboards 2.",
    "start": "612320",
    "end": "614720"
  },
  {
    "text": "now as it happens the grafina team have",
    "start": "614720",
    "end": "616640"
  },
  {
    "text": "built a jsonnet library called graphonet",
    "start": "616640",
    "end": "619200"
  },
  {
    "text": "we could use it to automatically",
    "start": "619200",
    "end": "620560"
  },
  {
    "text": "generate sacrifice dashboards from the",
    "start": "620560",
    "end": "622640"
  },
  {
    "text": "metrics catalog",
    "start": "622640",
    "end": "623920"
  },
  {
    "text": "this is a typical example of one of our",
    "start": "623920",
    "end": "625920"
  },
  {
    "text": "generated dashboards",
    "start": "625920",
    "end": "628000"
  },
  {
    "text": "this is from our web service dashboard",
    "start": "628000",
    "end": "630800"
  },
  {
    "text": "and what i really like about these",
    "start": "630800",
    "end": "632160"
  },
  {
    "text": "dashboards is the consistency",
    "start": "632160",
    "end": "634079"
  },
  {
    "text": "we have dozens of different services and",
    "start": "634079",
    "end": "635920"
  },
  {
    "text": "for each service the dashboard layout",
    "start": "635920",
    "end": "637760"
  },
  {
    "text": "the color scheme the data presented is",
    "start": "637760",
    "end": "639760"
  },
  {
    "text": "consistent",
    "start": "639760",
    "end": "640880"
  },
  {
    "text": "the top row of each dashboard provides",
    "start": "640880",
    "end": "642640"
  },
  {
    "text": "an aggregation of all the slis within",
    "start": "642640",
    "end": "644800"
  },
  {
    "text": "that service",
    "start": "644800",
    "end": "645760"
  },
  {
    "text": "and this is followed by a row for each",
    "start": "645760",
    "end": "647440"
  },
  {
    "text": "component of the service",
    "start": "647440",
    "end": "648959"
  },
  {
    "text": "with charts with key metrics and",
    "start": "648959",
    "end": "650560"
  },
  {
    "text": "collapse rows containing even more",
    "start": "650560",
    "end": "654079"
  },
  {
    "text": "detail",
    "start": "658839",
    "end": "661600"
  },
  {
    "text": "once we had our slo monitoring in place",
    "start": "661600",
    "end": "663519"
  },
  {
    "text": "the next challenge we faced was making",
    "start": "663519",
    "end": "665200"
  },
  {
    "text": "slo alerts easier for operators and",
    "start": "665200",
    "end": "667120"
  },
  {
    "text": "engineers to understand",
    "start": "667120",
    "end": "669600"
  },
  {
    "text": "and in particular reducing the time to",
    "start": "669600",
    "end": "671760"
  },
  {
    "text": "diagnosis on our slo alerts",
    "start": "671760",
    "end": "674320"
  },
  {
    "text": "one of the big differences between the",
    "start": "674320",
    "end": "675839"
  },
  {
    "text": "old way that we alerted with causal",
    "start": "675839",
    "end": "677600"
  },
  {
    "text": "alerts and our slo loads is that when an",
    "start": "677600",
    "end": "679760"
  },
  {
    "text": "slo alert fires",
    "start": "679760",
    "end": "681200"
  },
  {
    "text": "it's not always immediately apparent",
    "start": "681200",
    "end": "682880"
  },
  {
    "text": "what the problem is",
    "start": "682880",
    "end": "684800"
  },
  {
    "text": "it's up to the operator to understand",
    "start": "684800",
    "end": "686480"
  },
  {
    "text": "the sli then investigate the problem by",
    "start": "686480",
    "end": "689040"
  },
  {
    "text": "digging through metrics logs and other",
    "start": "689040",
    "end": "690800"
  },
  {
    "text": "signals",
    "start": "690800",
    "end": "691600"
  },
  {
    "text": "until the cause becomes apparent so",
    "start": "691600",
    "end": "694640"
  },
  {
    "text": "our goal here is to give the operator",
    "start": "694640",
    "end": "696800"
  },
  {
    "text": "the tools to navigate from an slo",
    "start": "696800",
    "end": "698720"
  },
  {
    "text": "violation signal",
    "start": "698720",
    "end": "700480"
  },
  {
    "text": "back up to the back up the stack to the",
    "start": "700480",
    "end": "702720"
  },
  {
    "text": "cause of the problem",
    "start": "702720",
    "end": "705600"
  },
  {
    "text": "here's an example of to illustrate why",
    "start": "706800",
    "end": "709360"
  },
  {
    "text": "our existing tooling was insufficient",
    "start": "709360",
    "end": "711040"
  },
  {
    "text": "for our needs",
    "start": "711040",
    "end": "712079"
  },
  {
    "text": "alert manager has a feature that",
    "start": "712079",
    "end": "713680"
  },
  {
    "text": "provides a link to the prometheus ui",
    "start": "713680",
    "end": "716399"
  },
  {
    "text": "pre-populated with the query that caused",
    "start": "716399",
    "end": "718720"
  },
  {
    "text": "the alert to fire",
    "start": "718720",
    "end": "720240"
  },
  {
    "text": "it's called generator url before we",
    "start": "720240",
    "end": "722720"
  },
  {
    "text": "moved over to slo violation alerts we",
    "start": "722720",
    "end": "724880"
  },
  {
    "text": "relied pretty heavily on this feature",
    "start": "724880",
    "end": "727360"
  },
  {
    "text": "each alert would include a link to the",
    "start": "727360",
    "end": "729120"
  },
  {
    "text": "expression that caused it",
    "start": "729120",
    "end": "730800"
  },
  {
    "text": "in the prometheus ui we would manipulate",
    "start": "730800",
    "end": "733120"
  },
  {
    "text": "the expression by adding labels changing",
    "start": "733120",
    "end": "735279"
  },
  {
    "text": "selectors",
    "start": "735279",
    "end": "736480"
  },
  {
    "text": "or changing aggregations until we could",
    "start": "736480",
    "end": "738720"
  },
  {
    "text": "spot the problem",
    "start": "738720",
    "end": "740320"
  },
  {
    "text": "what we found with slo alerts is that",
    "start": "740320",
    "end": "742480"
  },
  {
    "text": "this approach doesn't work very well",
    "start": "742480",
    "end": "744560"
  },
  {
    "text": "the problem is that the recording rules",
    "start": "744560",
    "end": "746320"
  },
  {
    "text": "that we used in the expression are",
    "start": "746320",
    "end": "747839"
  },
  {
    "text": "highly aggregated",
    "start": "747839",
    "end": "749120"
  },
  {
    "text": "and it's likely that the labels which",
    "start": "749120",
    "end": "750959"
  },
  {
    "text": "may have been useful in an investigation",
    "start": "750959",
    "end": "753200"
  },
  {
    "text": "have been removed unfortunately there's",
    "start": "753200",
    "end": "755920"
  },
  {
    "text": "no quick way to navigate back to the",
    "start": "755920",
    "end": "757920"
  },
  {
    "text": "source expression from a recording rule",
    "start": "757920",
    "end": "760560"
  },
  {
    "text": "arriving at a chart of an slr burn rate",
    "start": "760560",
    "end": "762959"
  },
  {
    "text": "expression like this",
    "start": "762959",
    "end": "764320"
  },
  {
    "text": "often led to more confusion for",
    "start": "764320",
    "end": "766079"
  },
  {
    "text": "engineers instead of clarity",
    "start": "766079",
    "end": "768240"
  },
  {
    "text": "we needed to create a better initial",
    "start": "768240",
    "end": "770000"
  },
  {
    "text": "experience for the operator",
    "start": "770000",
    "end": "771600"
  },
  {
    "text": "following an alert",
    "start": "771600",
    "end": "774480"
  },
  {
    "text": "the way we addressed this was to take",
    "start": "776000",
    "end": "778000"
  },
  {
    "text": "advantage of the metadata present in the",
    "start": "778000",
    "end": "780000"
  },
  {
    "text": "metric in the metrics catalog",
    "start": "780000",
    "end": "782240"
  },
  {
    "text": "since we're generating the slo alerts",
    "start": "782240",
    "end": "784160"
  },
  {
    "text": "and the dashboard from the same source",
    "start": "784160",
    "end": "785920"
  },
  {
    "text": "we can include deep links from the",
    "start": "785920",
    "end": "787519"
  },
  {
    "text": "appropriate",
    "start": "787519",
    "end": "788480"
  },
  {
    "text": "to the appropriate grafana panel and",
    "start": "788480",
    "end": "790880"
  },
  {
    "text": "these can be embedded directly in the",
    "start": "790880",
    "end": "792800"
  },
  {
    "text": "generated alert definition",
    "start": "792800",
    "end": "795360"
  },
  {
    "text": "by by navigating to the dashboard from",
    "start": "795360",
    "end": "797680"
  },
  {
    "text": "the alerts the operators immediately",
    "start": "797680",
    "end": "799600"
  },
  {
    "text": "provided with context around the alert",
    "start": "799600",
    "end": "801440"
  },
  {
    "text": "signal",
    "start": "801440",
    "end": "802639"
  },
  {
    "text": "the thresholds the status of other slis",
    "start": "802639",
    "end": "804880"
  },
  {
    "text": "and the same service",
    "start": "804880",
    "end": "806079"
  },
  {
    "text": "and links for onwards investigation",
    "start": "806079",
    "end": "809760"
  },
  {
    "text": "in our generated dashboards for each",
    "start": "810000",
    "end": "811600"
  },
  {
    "text": "component we include this set of links",
    "start": "811600",
    "end": "813839"
  },
  {
    "text": "to other observability tools that we use",
    "start": "813839",
    "end": "816079"
  },
  {
    "text": "to assist in deeper investigation into",
    "start": "816079",
    "end": "818000"
  },
  {
    "text": "problems for that component",
    "start": "818000",
    "end": "820639"
  },
  {
    "text": "like everything else these links are",
    "start": "820639",
    "end": "822079"
  },
  {
    "text": "described in the metrics catalog",
    "start": "822079",
    "end": "823760"
  },
  {
    "text": "they include links into stackdrivers",
    "start": "823760",
    "end": "825839"
  },
  {
    "text": "sentry kibana searches and",
    "start": "825839",
    "end": "827279"
  },
  {
    "text": "visualizations",
    "start": "827279",
    "end": "828639"
  },
  {
    "text": "amongst other things they're presented",
    "start": "828639",
    "end": "830560"
  },
  {
    "text": "directly alongside each component",
    "start": "830560",
    "end": "832399"
  },
  {
    "text": "in the grafana dashboard so when",
    "start": "832399",
    "end": "834079"
  },
  {
    "text": "arriving at a dashboard from an alert",
    "start": "834079",
    "end": "836480"
  },
  {
    "text": "we get an easy experience for the",
    "start": "836480",
    "end": "838240"
  },
  {
    "text": "on-call engineer to continue their",
    "start": "838240",
    "end": "839920"
  },
  {
    "text": "investigation",
    "start": "839920",
    "end": "840959"
  },
  {
    "text": "in through our other observability tools",
    "start": "840959",
    "end": "844880"
  },
  {
    "text": "the final part of this talk describes",
    "start": "845680",
    "end": "847360"
  },
  {
    "text": "the challenges we've experienced scaling",
    "start": "847360",
    "end": "849040"
  },
  {
    "text": "iso monitoring from a single prometheus",
    "start": "849040",
    "end": "851120"
  },
  {
    "text": "server",
    "start": "851120",
    "end": "851839"
  },
  {
    "text": "up to the thanos federated cluster that",
    "start": "851839",
    "end": "854079"
  },
  {
    "text": "we use today",
    "start": "854079",
    "end": "857279"
  },
  {
    "start": "856000",
    "end": "885000"
  },
  {
    "text": "let's start off by describing the",
    "start": "857279",
    "end": "858800"
  },
  {
    "text": "simplest approach to slo monitoring and",
    "start": "858800",
    "end": "860880"
  },
  {
    "text": "that is by using a single prometheus",
    "start": "860880",
    "end": "862800"
  },
  {
    "text": "instance for monitoring the entire",
    "start": "862800",
    "end": "864240"
  },
  {
    "text": "application",
    "start": "864240",
    "end": "866160"
  },
  {
    "text": "with this approach all work to collect",
    "start": "866160",
    "end": "868160"
  },
  {
    "text": "metrics aggregate",
    "start": "868160",
    "end": "869760"
  },
  {
    "text": "them into slis and evaluate those slis",
    "start": "869760",
    "end": "872480"
  },
  {
    "text": "against service level objectives",
    "start": "872480",
    "end": "874320"
  },
  {
    "text": "is done in a single prometheus server",
    "start": "874320",
    "end": "876959"
  },
  {
    "text": "this approach is very straightforward",
    "start": "876959",
    "end": "878720"
  },
  {
    "text": "and easy to operate",
    "start": "878720",
    "end": "879760"
  },
  {
    "text": "but it's limited in how far we are able",
    "start": "879760",
    "end": "881519"
  },
  {
    "text": "to vertically scale a single prometheus",
    "start": "881519",
    "end": "883519"
  },
  {
    "text": "instance",
    "start": "883519",
    "end": "885839"
  },
  {
    "start": "885000",
    "end": "936000"
  },
  {
    "text": "once we hit that scaling limit the next",
    "start": "886560",
    "end": "889199"
  },
  {
    "text": "logical step",
    "start": "889199",
    "end": "890000"
  },
  {
    "text": "is to break the monitoring down into",
    "start": "890000",
    "end": "891519"
  },
  {
    "text": "multiple silo prometheus instances",
    "start": "891519",
    "end": "893760"
  },
  {
    "text": "with this approach the data for each sli",
    "start": "893760",
    "end": "896240"
  },
  {
    "text": "is fully processed within a single",
    "start": "896240",
    "end": "897680"
  },
  {
    "text": "prometheus instance",
    "start": "897680",
    "end": "899040"
  },
  {
    "text": "so it can continue to collect aggregate",
    "start": "899040",
    "end": "901120"
  },
  {
    "text": "and evaluate our slis in a similar",
    "start": "901120",
    "end": "903040"
  },
  {
    "text": "manner to before",
    "start": "903040",
    "end": "904320"
  },
  {
    "text": "except that each prometheus only",
    "start": "904320",
    "end": "906000"
  },
  {
    "text": "contains a distinct subset of their slis",
    "start": "906000",
    "end": "909279"
  },
  {
    "text": "in grafana we use multiple data sources",
    "start": "909279",
    "end": "911600"
  },
  {
    "text": "to visualize data across different",
    "start": "911600",
    "end": "913279"
  },
  {
    "text": "sources",
    "start": "913279",
    "end": "915199"
  },
  {
    "text": "the advantage of this approach is that",
    "start": "915199",
    "end": "916800"
  },
  {
    "text": "it remains fairly simple",
    "start": "916800",
    "end": "918560"
  },
  {
    "text": "to both deploy and understand while",
    "start": "918560",
    "end": "920639"
  },
  {
    "text": "allowing us to scale prometheus",
    "start": "920639",
    "end": "922320"
  },
  {
    "text": "horizontally",
    "start": "922320",
    "end": "923760"
  },
  {
    "text": "one limitation to this approach though",
    "start": "923760",
    "end": "925760"
  },
  {
    "text": "is that all the metrics required to",
    "start": "925760",
    "end": "927279"
  },
  {
    "text": "evaluate an sli",
    "start": "927279",
    "end": "928880"
  },
  {
    "text": "must be contained within a single",
    "start": "928880",
    "end": "930399"
  },
  {
    "text": "prometheus instance and unfortunately",
    "start": "930399",
    "end": "932720"
  },
  {
    "text": "this requirement became problematic for",
    "start": "932720",
    "end": "934639"
  },
  {
    "text": "us",
    "start": "934639",
    "end": "936800"
  },
  {
    "start": "936000",
    "end": "1000000"
  },
  {
    "text": "this happened when our kubernetes",
    "start": "937920",
    "end": "939440"
  },
  {
    "text": "migration project kicked off",
    "start": "939440",
    "end": "941199"
  },
  {
    "text": "as workloads migrated to kubernetes some",
    "start": "941199",
    "end": "943600"
  },
  {
    "text": "sli's were split between prometheus",
    "start": "943600",
    "end": "945519"
  },
  {
    "text": "instances",
    "start": "945519",
    "end": "946880"
  },
  {
    "text": "used for vms and new prometheus",
    "start": "946880",
    "end": "948800"
  },
  {
    "text": "instances contained within our",
    "start": "948800",
    "end": "950560"
  },
  {
    "text": "kubernetes cluster this was made worse",
    "start": "950560",
    "end": "952959"
  },
  {
    "text": "by the fact that we decided to employ",
    "start": "952959",
    "end": "954800"
  },
  {
    "text": "three zonal kubernetes clusters",
    "start": "954800",
    "end": "956639"
  },
  {
    "text": "each with their own prometheus incidents",
    "start": "956639",
    "end": "958959"
  },
  {
    "text": "so instead of metrics being collected in",
    "start": "958959",
    "end": "961040"
  },
  {
    "text": "a single prometheus instance",
    "start": "961040",
    "end": "962800"
  },
  {
    "text": "some of our slis were now being split",
    "start": "962800",
    "end": "964720"
  },
  {
    "text": "between up to four different instances",
    "start": "964720",
    "end": "967120"
  },
  {
    "text": "the problem with this is that there may",
    "start": "967120",
    "end": "968880"
  },
  {
    "text": "be local slo violations but when",
    "start": "968880",
    "end": "971040"
  },
  {
    "text": "aggregated across the entire application",
    "start": "971040",
    "end": "973360"
  },
  {
    "text": "the service level objective is not being",
    "start": "973360",
    "end": "975199"
  },
  {
    "text": "violated this led to a series of low",
    "start": "975199",
    "end": "977839"
  },
  {
    "text": "precision alerts which we nicknamed",
    "start": "977839",
    "end": "979600"
  },
  {
    "text": "split brain alerts because they were",
    "start": "979600",
    "end": "982000"
  },
  {
    "text": "only applicable to a single prometheus",
    "start": "982000",
    "end": "983759"
  },
  {
    "text": "instance not the entire cluster",
    "start": "983759",
    "end": "986399"
  },
  {
    "text": "a second problem with having sli split",
    "start": "986399",
    "end": "988320"
  },
  {
    "text": "between multiple prometheus instances",
    "start": "988320",
    "end": "990480"
  },
  {
    "text": "is that it becomes difficult to get a",
    "start": "990480",
    "end": "991920"
  },
  {
    "text": "global view of an sli",
    "start": "991920",
    "end": "994000"
  },
  {
    "text": "since we need to combine data from",
    "start": "994000",
    "end": "995680"
  },
  {
    "text": "multiple sources in our visualizations",
    "start": "995680",
    "end": "999920"
  },
  {
    "start": "1000000",
    "end": "1070000"
  },
  {
    "text": "the solution we used to address this",
    "start": "1002160",
    "end": "1003920"
  },
  {
    "text": "problem was to deploy thanos",
    "start": "1003920",
    "end": "1005839"
  },
  {
    "text": "thanos is a cncf incubating project i'm",
    "start": "1005839",
    "end": "1008320"
  },
  {
    "text": "sure many of you know of it thanos",
    "start": "1008320",
    "end": "1010240"
  },
  {
    "text": "provides single view",
    "start": "1010240",
    "end": "1011440"
  },
  {
    "text": "across multiple prometheus instances",
    "start": "1011440",
    "end": "1014639"
  },
  {
    "text": "it also has a component called thanos",
    "start": "1014639",
    "end": "1016480"
  },
  {
    "text": "rule which can be used for evaluating",
    "start": "1016480",
    "end": "1018560"
  },
  {
    "text": "recording rules against the single view",
    "start": "1018560",
    "end": "1020959"
  },
  {
    "text": "this provides us with a mechanism to",
    "start": "1020959",
    "end": "1022880"
  },
  {
    "text": "aggregate across multiple prometheus",
    "start": "1022880",
    "end": "1024880"
  },
  {
    "text": "instances",
    "start": "1024880",
    "end": "1026319"
  },
  {
    "text": "thanos rule will also evaluate alerts",
    "start": "1026319",
    "end": "1028558"
  },
  {
    "text": "using the same approach as prometheus",
    "start": "1028559",
    "end": "1030400"
  },
  {
    "text": "except that it evaluates using the",
    "start": "1030400",
    "end": "1032400"
  },
  {
    "text": "single global view once again",
    "start": "1032400",
    "end": "1034880"
  },
  {
    "text": "to use thanos rule we broke our slo",
    "start": "1034880",
    "end": "1037360"
  },
  {
    "text": "recording rules into two parts",
    "start": "1037360",
    "end": "1039760"
  },
  {
    "text": "most of the metrics processing remains",
    "start": "1039760",
    "end": "1041760"
  },
  {
    "text": "in prometheus",
    "start": "1041760",
    "end": "1042798"
  },
  {
    "text": "here we convert potentially higher",
    "start": "1042799",
    "end": "1044480"
  },
  {
    "text": "cardinality application metrics",
    "start": "1044480",
    "end": "1046880"
  },
  {
    "text": "into low cardinality key metric",
    "start": "1046880",
    "end": "1049440"
  },
  {
    "text": "constituents",
    "start": "1049440",
    "end": "1050720"
  },
  {
    "text": "then in thanos rule we sum the key",
    "start": "1050720",
    "end": "1052799"
  },
  {
    "text": "metric constituents across",
    "start": "1052799",
    "end": "1054160"
  },
  {
    "text": "all instances before calculating global",
    "start": "1054160",
    "end": "1056240"
  },
  {
    "text": "aptx and error rate slis",
    "start": "1056240",
    "end": "1058559"
  },
  {
    "text": "these are evaluated against slos in",
    "start": "1058559",
    "end": "1060960"
  },
  {
    "text": "thanos ruler to provide alerting",
    "start": "1060960",
    "end": "1063200"
  },
  {
    "text": "on globally aggregated values doing away",
    "start": "1063200",
    "end": "1065600"
  },
  {
    "text": "with the problem of the split brain",
    "start": "1065600",
    "end": "1066960"
  },
  {
    "text": "alerts",
    "start": "1066960",
    "end": "1069360"
  },
  {
    "start": "1070000",
    "end": "1140000"
  },
  {
    "text": "this example configuration shows how we",
    "start": "1072640",
    "end": "1074559"
  },
  {
    "text": "aggregate multiple prometheus metrics in",
    "start": "1074559",
    "end": "1076559"
  },
  {
    "text": "a thanos rule",
    "start": "1076559",
    "end": "1077600"
  },
  {
    "text": "using recording rules for each of our",
    "start": "1077600",
    "end": "1079760"
  },
  {
    "text": "key metrics we aggregate",
    "start": "1079760",
    "end": "1081280"
  },
  {
    "text": "all our prometheus metrics whilst being",
    "start": "1081280",
    "end": "1083679"
  },
  {
    "text": "careful to exclude any previously",
    "start": "1083679",
    "end": "1085280"
  },
  {
    "text": "evaluated thanos metrics",
    "start": "1085280",
    "end": "1088799"
  },
  {
    "text": "the first recording rule aggregates the",
    "start": "1088799",
    "end": "1090400"
  },
  {
    "text": "error rate sli the second shows",
    "start": "1090400",
    "end": "1092559"
  },
  {
    "text": "operation rate and the third recording",
    "start": "1092559",
    "end": "1094880"
  },
  {
    "text": "rule uses the two previous values to",
    "start": "1094880",
    "end": "1096720"
  },
  {
    "text": "create a global error ratio sli",
    "start": "1096720",
    "end": "1100480"
  },
  {
    "text": "note that we use monitor equals global",
    "start": "1100480",
    "end": "1102720"
  },
  {
    "text": "as a thanos selector to control whether",
    "start": "1102720",
    "end": "1104799"
  },
  {
    "text": "to include or exclude globally",
    "start": "1104799",
    "end": "1106640"
  },
  {
    "text": "aggregated metrics in these expressions",
    "start": "1106640",
    "end": "1109840"
  },
  {
    "text": "another important point is that the",
    "start": "1109840",
    "end": "1111679"
  },
  {
    "text": "partial response strategy",
    "start": "1111679",
    "end": "1113360"
  },
  {
    "text": "is set to warn instead of the default",
    "start": "1113360",
    "end": "1115360"
  },
  {
    "text": "which is abort",
    "start": "1115360",
    "end": "1116960"
  },
  {
    "text": "the reason for this is that when partial",
    "start": "1116960",
    "end": "1119840"
  },
  {
    "text": "response is set to warn",
    "start": "1119840",
    "end": "1121120"
  },
  {
    "text": "if a single prometheus store is",
    "start": "1121120",
    "end": "1122559"
  },
  {
    "text": "unavailable",
    "start": "1122559",
    "end": "1124400"
  },
  {
    "text": "the aggregation won't fail instead the",
    "start": "1124400",
    "end": "1127039"
  },
  {
    "text": "metrics from that prometheus instance",
    "start": "1127039",
    "end": "1128799"
  },
  {
    "text": "will temporarily not be included in the",
    "start": "1128799",
    "end": "1130559"
  },
  {
    "text": "aggregation",
    "start": "1130559",
    "end": "1131679"
  },
  {
    "text": "but this is a better trade-off than",
    "start": "1131679",
    "end": "1133200"
  },
  {
    "text": "losing all the metrics",
    "start": "1133200",
    "end": "1134960"
  },
  {
    "text": "we work around this by monitoring for",
    "start": "1134960",
    "end": "1136720"
  },
  {
    "text": "partial response warnings in our",
    "start": "1136720",
    "end": "1138320"
  },
  {
    "text": "monitoring stack",
    "start": "1138320",
    "end": "1141200"
  },
  {
    "start": "1140000",
    "end": "1179000"
  },
  {
    "text": "in conclusion here are some of the ways",
    "start": "1142480",
    "end": "1144160"
  },
  {
    "text": "we've learned to deal with metrics at",
    "start": "1144160",
    "end": "1145760"
  },
  {
    "text": "scale",
    "start": "1145760",
    "end": "1147039"
  },
  {
    "text": "firstly we define key metrics for each",
    "start": "1147039",
    "end": "1149120"
  },
  {
    "text": "service component",
    "start": "1149120",
    "end": "1150720"
  },
  {
    "text": "we manage complexity and repetition by",
    "start": "1150720",
    "end": "1153039"
  },
  {
    "text": "using an abstract definition in the",
    "start": "1153039",
    "end": "1154640"
  },
  {
    "text": "metrics catalog",
    "start": "1154640",
    "end": "1155600"
  },
  {
    "text": "as our single source of truth we've",
    "start": "1155600",
    "end": "1157840"
  },
  {
    "text": "migrated to multi-window multi-burn rate",
    "start": "1157840",
    "end": "1160080"
  },
  {
    "text": "slo alerts for improved alerting",
    "start": "1160080",
    "end": "1162720"
  },
  {
    "text": "we generate our dashboards to ensure",
    "start": "1162720",
    "end": "1164480"
  },
  {
    "text": "that they're kept up to date and",
    "start": "1164480",
    "end": "1165679"
  },
  {
    "text": "validated",
    "start": "1165679",
    "end": "1167360"
  },
  {
    "text": "we focus on improving on-call engineers",
    "start": "1167360",
    "end": "1169760"
  },
  {
    "text": "experience because slr alerts",
    "start": "1169760",
    "end": "1171520"
  },
  {
    "text": "are not always intuitive and finally we",
    "start": "1171520",
    "end": "1174400"
  },
  {
    "text": "federated our service level monitoring",
    "start": "1174400",
    "end": "1176160"
  },
  {
    "text": "using thanos and thanos rule",
    "start": "1176160",
    "end": "1179840"
  },
  {
    "start": "1179000",
    "end": "1191000"
  },
  {
    "text": "one last point if you're interested in",
    "start": "1180720",
    "end": "1182320"
  },
  {
    "text": "learning more i highly recommend reading",
    "start": "1182320",
    "end": "1184559"
  },
  {
    "text": "these fantastic resources on monitoring",
    "start": "1184559",
    "end": "1186720"
  },
  {
    "text": "in general",
    "start": "1186720",
    "end": "1187600"
  },
  {
    "text": "and slo monitoring in particular",
    "start": "1187600",
    "end": "1192320"
  },
  {
    "text": "finally all the code for our metrics",
    "start": "1192320",
    "end": "1194880"
  },
  {
    "text": "catalog is available on gitlab.com in",
    "start": "1194880",
    "end": "1196880"
  },
  {
    "text": "our runbooks project",
    "start": "1196880",
    "end": "1198080"
  },
  {
    "text": "i've included a link here thank you very",
    "start": "1198080",
    "end": "1202120"
  },
  {
    "text": "much",
    "start": "1202120",
    "end": "1205120"
  }
]