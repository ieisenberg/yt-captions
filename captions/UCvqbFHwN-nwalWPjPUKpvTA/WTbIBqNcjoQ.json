[
  {
    "text": "thank you all for joining us today like Chris said my name is Greg Taylor and I'm a manager on our infrastructure team",
    "start": "30",
    "end": "6960"
  },
  {
    "text": "and I'm here to share follow up on last last year's Q con talk where we laid out our plans for kubernetes for 2019 during",
    "start": "6960",
    "end": "15089"
  },
  {
    "text": "this session you'll hear a candid retrospective for how it all worked out and if you didn't see last year's talk",
    "start": "15089",
    "end": "20340"
  },
  {
    "text": "not Tory we'll get you all caught up as we go but first a little bit about reddit reddit is the home for discussion",
    "start": "20340",
    "end": "27720"
  },
  {
    "text": "on the web and also a few memes cats and dogs I'm all sorts of other interesting",
    "start": "27720",
    "end": "34230"
  },
  {
    "text": "things if you have an interest or a hobby or need for support camaraderie or encouragement you'll find a home in at",
    "start": "34230",
    "end": "40770"
  },
  {
    "text": "least one of our hundred thousand communities and you'll be relieved to hear that we don't need to know your",
    "start": "40770",
    "end": "46079"
  },
  {
    "text": "real name what you look like where you went to school where you from or here you know on reddit these things don't",
    "start": "46079",
    "end": "51930"
  },
  {
    "text": "have to matter the focus is on user driven communities and the sharing and",
    "start": "51930",
    "end": "57180"
  },
  {
    "text": "the discussion that happens within them as an example of one of these communities",
    "start": "57180",
    "end": "62280"
  },
  {
    "text": "I will shamelessly plug our kubernetes on our kubernetes or 30,000 kubernetes",
    "start": "62280",
    "end": "67979"
  },
  {
    "text": "users administrators developers gather here to ask questions share victories and to keep up-to-date on what's",
    "start": "67979",
    "end": "74549"
  },
  {
    "text": "happening around the community if you haven't seen it yet check it out something that is unique about reddit is",
    "start": "74549",
    "end": "80850"
  },
  {
    "text": "the scale at which we operate our user base of over 330 million makes us one of",
    "start": "80850",
    "end": "86400"
  },
  {
    "text": "the most busiest sites in the world we got to this point with what has been a very small team up until the last few",
    "start": "86400",
    "end": "92490"
  },
  {
    "text": "years around 250 engineers are responsible for the fifth most trafficked site in the US",
    "start": "92490",
    "end": "98640"
  },
  {
    "text": "and as small as that is relative to our peers even that 250 feels like a sizable",
    "start": "98640",
    "end": "104700"
  },
  {
    "text": "army of Engineers compared to where we were even just a few years back and you",
    "start": "104700",
    "end": "109710"
  },
  {
    "text": "may find yourself saying that this is all interesting Greg but what does this have to do with kubernetes and to that I would say join me in my totally safe",
    "start": "109710",
    "end": "116729"
  },
  {
    "text": "osha-approved time machine if you'd like to find out let's go back way back to a",
    "start": "116729",
    "end": "122399"
  },
  {
    "text": "time where you could count all of Reddit engineering on your fingers and toes yes",
    "start": "122399",
    "end": "128190"
  },
  {
    "text": "the distant past of 2016 not even four years ago this was a much simpler time for us",
    "start": "128190",
    "end": "133650"
  },
  {
    "text": "had around 20 engineers at the company a single monolithic code base and a much slower pace of product development our",
    "start": "133650",
    "end": "140310"
  },
  {
    "text": "engineering organization functioned in a traditional further could over to walk to the ops team arrangement and the",
    "start": "140310",
    "end": "147030"
  },
  {
    "text": "brave and cunning protagonists in the story our infrastructure team numbered around six in total the beginning of",
    "start": "147030",
    "end": "153359"
  },
  {
    "text": "2016 it was around this time that we could fit all of reddit's engineers in a",
    "start": "153359",
    "end": "158430"
  },
  {
    "text": "single VW camper but soon reddit aspirations grew and with it the",
    "start": "158430",
    "end": "165030"
  },
  {
    "text": "engineering organization in early 2016 we were around 20 engineers by 2017 we",
    "start": "165030",
    "end": "171090"
  },
  {
    "text": "grew to around 80 in 2018 we are over 150 and today we number over 250 our VW",
    "start": "171090",
    "end": "178680"
  },
  {
    "text": "camper is a bit more cramped today but all of this rapid expansion wasn't without growing pains with new engineers",
    "start": "178680",
    "end": "185549"
  },
  {
    "text": "came new engineering teams these new teams struggled to jointly develop our aging moneth so we steel it ourselves",
    "start": "185549",
    "end": "191489"
  },
  {
    "text": "and embarked on a journey to microservices paradise anybody heard the",
    "start": "191489",
    "end": "196859"
  },
  {
    "text": "story before or something like that we pulled logic and core functionality",
    "start": "196859",
    "end": "202949"
  },
  {
    "text": "out of our monolith and into distinct services as more code and logic were liberated the sky filled with the course",
    "start": "202949",
    "end": "209579"
  },
  {
    "text": "of microservices the beautiful sounds of a binary-based opera filled the air",
    "start": "209579",
    "end": "214609"
  },
  {
    "text": "magnificent but that's not exactly what happened as the new services came up our",
    "start": "214609",
    "end": "221810"
  },
  {
    "text": "throw coat over the wall to ops model became more apparently and compatible with our new direction we struggled to",
    "start": "221810",
    "end": "228269"
  },
  {
    "text": "enable service owners to safely and reasonably take on more responsibilities without expecting them all to moonlight",
    "start": "228269",
    "end": "233940"
  },
  {
    "text": "as infrastructure engineers service owners couldn't handle their own instant response to bug issues do DB migrations",
    "start": "233940",
    "end": "240959"
  },
  {
    "text": "and many other things without our help as a consequence we ended up on the hook for most of these things as you might",
    "start": "240959",
    "end": "248040"
  },
  {
    "text": "imagine this wasn't great for us or the service owners so we went back to the drawing board to rethink our",
    "start": "248040",
    "end": "253829"
  },
  {
    "text": "approach we sought to shift our role in the orga way from an Operations focus and mores towards in an empowerment and",
    "start": "253829",
    "end": "261299"
  },
  {
    "text": "enable MIT focus our challenge was to package up our knowledge our best practices and",
    "start": "261299",
    "end": "266660"
  },
  {
    "text": "such that other teams could use it without having to become infrastructure engineers our end goal is to empower any",
    "start": "266660",
    "end": "272570"
  },
  {
    "text": "engineer at reddit to develop deploy and operate a service so we shifted our mentality from providing a pile of tools",
    "start": "272570",
    "end": "279410"
  },
  {
    "text": "and good wishes to packaging our various offerings up into a more accessible product we chose kubernetes as our",
    "start": "279410",
    "end": "286580"
  },
  {
    "text": "foundation then began assembling tools processes and documentation into an ecosystem that we now call infrared I",
    "start": "286580",
    "end": "292610"
  },
  {
    "text": "won't have time to go into detail about infrared today but check out last year's presentation if you'd like to learn more",
    "start": "292610",
    "end": "297680"
  },
  {
    "text": "about our internal infrastructure product fast forwarding to early 2018 we",
    "start": "297680",
    "end": "303320"
  },
  {
    "text": "worked with a brave and patient set of a doctors to prototype the offering",
    "start": "303320",
    "end": "308680"
  },
  {
    "text": "services on infrared enter production at non-trivial scale in mid 2018 it was at",
    "start": "308680",
    "end": "314480"
  },
  {
    "text": "this point that kubernetes began serving external user facing traffic after about",
    "start": "314480",
    "end": "320300"
  },
  {
    "text": "a year of exploding things and also iterating we launched infrared to org-wide general availability in March",
    "start": "320300",
    "end": "326540"
  },
  {
    "text": "of this year at that point infrared and kubernetes became the default environment for services at reddit this",
    "start": "326540",
    "end": "334640"
  },
  {
    "text": "is where I'll start our retrospective of the last year of kubernetes at reddit our first topic is the org-wide",
    "start": "334640",
    "end": "341030"
  },
  {
    "text": "onboarding has started our launch in March as our early adopters got more",
    "start": "341030",
    "end": "346490"
  },
  {
    "text": "comfortable with kubernetes they began to realize a degree of self-sufficiency they were able to handle their own",
    "start": "346490",
    "end": "351980"
  },
  {
    "text": "incident responses diagnosis issues and generally handle day-to-day operations with minimal infrastructure team",
    "start": "351980",
    "end": "357350"
  },
  {
    "text": "involvement other teams around the organization began to notice this and the hype train was soon shooting along",
    "start": "357350",
    "end": "363680"
  },
  {
    "text": "at full steam leading up to our March leading up to our launch in March we",
    "start": "363680",
    "end": "369230"
  },
  {
    "text": "ended up with a long backlog of teams wanting to get their services on infrared unfortunately for us the",
    "start": "369230",
    "end": "375080"
  },
  {
    "text": "service launch process was manual and also still taking shape further complicating things are small and 14",
    "start": "375080",
    "end": "381410"
  },
  {
    "text": "needed to assist with launches while also having enough scheduling daylight to iterate and automate on the offering",
    "start": "381410",
    "end": "386900"
  },
  {
    "text": "itself so we needed a plan to avoid sinking all of our times into launches",
    "start": "386900",
    "end": "391970"
  },
  {
    "text": "we set up a schedule of launch windows that were two weeks long we aim to use at least half of the two",
    "start": "391970",
    "end": "397340"
  },
  {
    "text": "weeks for hands-on onboarding and training with new service owners as we ran through each launch we had",
    "start": "397340",
    "end": "402980"
  },
  {
    "text": "improve the documentation refine our processes and build automation for ourselves and the service owners the",
    "start": "402980",
    "end": "408710"
  },
  {
    "text": "thinking was that supplemented with training our hands on approach to launches would help us reach in an",
    "start": "408710",
    "end": "414020"
  },
  {
    "text": "organizational critical mass at that point we'd have an internal community that would at least be partially self",
    "start": "414020",
    "end": "420190"
  },
  {
    "text": "self-supporting and when it came time to launch and start the org-wide onboarding",
    "start": "420190",
    "end": "426170"
  },
  {
    "text": "process the plan ended up mostly working well after about six months of launching",
    "start": "426170",
    "end": "431330"
  },
  {
    "text": "a service every two weeks most engineering teams in the organization had at least one service running on",
    "start": "431330",
    "end": "436700"
  },
  {
    "text": "kubernetes supplementing the support that the in fátima offered service owners were quick to answer one",
    "start": "436700",
    "end": "442880"
  },
  {
    "text": "another's questions and assist where possible having a larger more supportive community has been key in our quest to",
    "start": "442880",
    "end": "449390"
  },
  {
    "text": "remove ourselves as a central hot spot the other thing that went pretty well was our top-line goal of empowering",
    "start": "449390",
    "end": "455330"
  },
  {
    "text": "service owners to control their software development lifecycle as the internal community grew and infra-red mature we",
    "start": "455330",
    "end": "461510"
  },
  {
    "text": "saw service owners doing things themselves that would previously required in for help examples can range",
    "start": "461510",
    "end": "466550"
  },
  {
    "text": "from DB migrations diagnosing performance issues and most exciting of all Incident Response in fact especially",
    "start": "466550",
    "end": "474710"
  },
  {
    "text": "motivated services even began self-driving their service launches despite our manual process and as well",
    "start": "474710",
    "end": "481970"
  },
  {
    "text": "as things worked out the first few months following the launch were especially challenging most engineers that we were on boarding had no prior",
    "start": "481970",
    "end": "488420"
  },
  {
    "text": "experience to kubernetes until we could reach a critical mass of early adopters most support had to go through the infra",
    "start": "488420",
    "end": "495200"
  },
  {
    "text": "team combined with the work on infrared itself this is a lot for us to juggle the other side of this coin is that we",
    "start": "495200",
    "end": "502640"
  },
  {
    "text": "didn't have a great plan for how to support new service owners after their first launch we were too small a team to",
    "start": "502640",
    "end": "508730"
  },
  {
    "text": "appoint a dedicated rep for each service owner do is a long launch backlog we needed to be wrapping these up and",
    "start": "508730",
    "end": "515060"
  },
  {
    "text": "moving on to the next launch as quickly as possible this left our new service owners in a state where they had greatly expanded",
    "start": "515060",
    "end": "521570"
  },
  {
    "text": "superpowers that they didn't quite know how to use as far as what we do",
    "start": "521570",
    "end": "526820"
  },
  {
    "text": "different than if we if we had a six second chance we would have started the build-out of our site reliability engineering organization much earlier in",
    "start": "526820",
    "end": "533840"
  },
  {
    "text": "the process preferably before launch a well-placed SRA services that steadying presence",
    "start": "533840",
    "end": "538910"
  },
  {
    "text": "that we were missing on our service owner teams and I strongly feel that the right person in this role can level up",
    "start": "538910",
    "end": "545270"
  },
  {
    "text": "the service ownership skills of the entire division that they are embedded in as far as where we are today while we",
    "start": "545270",
    "end": "552530"
  },
  {
    "text": "are still building organizational competency just about every non mobile oriented team has at least one",
    "start": "552530",
    "end": "558200"
  },
  {
    "text": "production service on kubernetes in fact our most prolific service owner divisions have have a have an embedded",
    "start": "558200",
    "end": "564830"
  },
  {
    "text": "partner in there and our SOE program enabled by kubernetes and infrared service owners are able to handle most",
    "start": "564830",
    "end": "571190"
  },
  {
    "text": "of their day-to-day operations on their own and though there is much work to be done on automating our launch process",
    "start": "571190",
    "end": "576410"
  },
  {
    "text": "and improving Docs and tooling and training before we move on I wanted to",
    "start": "576410",
    "end": "581840"
  },
  {
    "text": "share a visual visualization of our adoption this is a graph of the total",
    "start": "581840",
    "end": "587270"
  },
  {
    "text": "count of Reddit authored services running on kubernetes over time this might be too small to see in the back",
    "start": "587270",
    "end": "592970"
  },
  {
    "text": "but you can see the jump in March of 2019 that's the second arrow pointing down sergyei launch when we launched",
    "start": "592970",
    "end": "599470"
  },
  {
    "text": "also note that the overall curve is pretty steady this is due to our two",
    "start": "599470",
    "end": "605630"
  },
  {
    "text": "week launch cadence today we have about 45 reddit authored services on kubernetes in production we also have",
    "start": "605630",
    "end": "611780"
  },
  {
    "text": "about 20 services in the queue to be migrated moving on our next topic is",
    "start": "611780",
    "end": "616910"
  },
  {
    "text": "cluster management at this point we're consistency consistently launching at least one service every two weeks",
    "start": "616910",
    "end": "623030"
  },
  {
    "text": "between the addition of new services and the growth of the existing services we found ourselves with larger and more",
    "start": "623030",
    "end": "628790"
  },
  {
    "text": "numerous clusters and truth be told we were still gaining operational experience with kubernetes in a high",
    "start": "628790",
    "end": "634610"
  },
  {
    "text": "traffic setting there were times where we had missteps while tuning scaling or performing maintenance on our clusters",
    "start": "634610",
    "end": "640490"
  },
  {
    "text": "another fun fact is that our adoption of kubernetes predated Amazon's elastic kubernetes service so we had a quite a",
    "start": "640490",
    "end": "647870"
  },
  {
    "text": "bit of self learning to do I hope that you will all enjoy this graphic I take",
    "start": "647870",
    "end": "653360"
  },
  {
    "text": "my art very seriously at the time we employed multi a Z clusters for all of",
    "start": "653360",
    "end": "659960"
  },
  {
    "text": "our environments and this mostly worked well until we blew something up or were a victim of infrastructure gremlins",
    "start": "659960",
    "end": "665890"
  },
  {
    "text": "upgrades were more scary and heist as we're config updates we had all of",
    "start": "665890",
    "end": "671420"
  },
  {
    "text": "our eggs and a few large kubernetes shaped baskets but we had a plan this is",
    "start": "671420",
    "end": "678200"
  },
  {
    "text": "very creative right we decided to split our largest and most crucial multi easy clusters up into multiple single AZ",
    "start": "678200",
    "end": "685100"
  },
  {
    "text": "clusters each of these single AZ clusters would run the same workload in the example that you're seeing we refer",
    "start": "685100",
    "end": "691970"
  },
  {
    "text": "to these three identical clusters as a cluster group each cluster and the services within would be a",
    "start": "691970",
    "end": "697520"
  },
  {
    "text": "self-contained as possible this means that we try to minimize traffic leaving the cluster and by extension the AZ that",
    "start": "697520",
    "end": "704900"
  },
  {
    "text": "the cluster is in in order to avoid saddling our service owners with this additional complexity we'd enlist",
    "start": "704900",
    "end": "710360"
  },
  {
    "text": "spinnaker for its multi cluster deploy support and finally we'd manage the additional clusters with terraform and",
    "start": "710360",
    "end": "716090"
  },
  {
    "text": "helm file which together will reduce the burden of keeping a cluster group in sync to summarize the last year our",
    "start": "716090",
    "end": "723710"
  },
  {
    "text": "multi cluster has paid off it has prevented several outages often without",
    "start": "723710",
    "end": "729280"
  },
  {
    "text": "significant user visible impact we now have additional peace of mind when doing cluster upgrades config changes or",
    "start": "729280",
    "end": "736280"
  },
  {
    "text": "responding to incidents and knowing that we have the additional capacity to fall back on has been a great reducer of",
    "start": "736280",
    "end": "742010"
  },
  {
    "text": "stress if something does go wrong with the cluster our CDN and load balancer layer take the unhealthy cluster out of",
    "start": "742010",
    "end": "748070"
  },
  {
    "text": "rotation and the rest of the cluster group quickly scales up to handle the extra traffic this is especially helpful",
    "start": "748070",
    "end": "754070"
  },
  {
    "text": "when Amazon temporary temporarily misplaced is in AZ which tends to happen",
    "start": "754070",
    "end": "759770"
  },
  {
    "text": "about once every year year and a half so that as well the other nice thing has",
    "start": "759770",
    "end": "764900"
  },
  {
    "text": "been some cost and latency savings from not being as sloppy about going across easy boundaries with network traffic",
    "start": "764900",
    "end": "770930"
  },
  {
    "text": "with the single AZ cluster model it's more difficult to unintentionally send traffic to other AZ's today the most",
    "start": "770930",
    "end": "778550"
  },
  {
    "text": "common case where we do reach across a Z's is for databases and caches this big",
    "start": "778550",
    "end": "787760"
  },
  {
    "text": "change in our cluster management strategy hasn't been without some challenges as you might expect more",
    "start": "787760",
    "end": "792830"
  },
  {
    "text": "clusters does mean more administrative overhead it took us a while to figure out the best way to manage these nearly",
    "start": "792830",
    "end": "798110"
  },
  {
    "text": "identical clusters and after some experimentation we settled on splitting are interesting but we settled",
    "start": "798110",
    "end": "806000"
  },
  {
    "text": "on templating our terraform for our cluster groups pass in the availability zones that a cluster group should cover",
    "start": "806000",
    "end": "812029"
  },
  {
    "text": "and the code generator rates out the corresponding tariffs one while this",
    "start": "812029",
    "end": "817730"
  },
  {
    "text": "simplifies the process of provisioning new clusters we do now have to maintain more clusters to minimize the impact of",
    "start": "817730",
    "end": "824690"
  },
  {
    "text": "the additional overhead we tend to only use the cluster group pattern in environments that are mission-critical",
    "start": "824690",
    "end": "830120"
  },
  {
    "text": "everything else still stays with the old Mustard multi cluster model if we were",
    "start": "830120",
    "end": "835700"
  },
  {
    "text": "if we were able to start over with the benefit of hindsight we'd probably shift our more sensitive environments to the multi to the cluster",
    "start": "835700",
    "end": "842510"
  },
  {
    "text": "group pattern sooner doing so would have save some pain made us less worried about making changes to our clusters and",
    "start": "842510",
    "end": "848960"
  },
  {
    "text": "what have also kept our individual cluster sizes smaller as we gained experience with operating kubernetes",
    "start": "848960",
    "end": "854420"
  },
  {
    "text": "atlatl at a higher scale today most of our central environments do use the",
    "start": "854420",
    "end": "860120"
  },
  {
    "text": "cluster group model for environments that aren't critical again we continue to stamp those multi easy clusters and",
    "start": "860120",
    "end": "865790"
  },
  {
    "text": "in total we have about 19 clusters and are adding at least a few every quarter yeah while we have ended up with more",
    "start": "865790",
    "end": "872780"
  },
  {
    "text": "clusters spinnaker has done an admirable job in making this mostly transparent to the service owners during deploys moving",
    "start": "872780",
    "end": "881540"
  },
  {
    "text": "out to our next topic cluster policy continuing our story we've now launched",
    "start": "881540",
    "end": "887030"
  },
  {
    "text": "a bunch of new services and stood up close to 20 clusters service owners are managing theirs their own services and",
    "start": "887030",
    "end": "893030"
  },
  {
    "text": "doing all kinds of exciting things just",
    "start": "893030",
    "end": "898790"
  },
  {
    "text": "a second here we go though sometimes perhaps maybe a little",
    "start": "898790",
    "end": "904460"
  },
  {
    "text": "too exciting with the full power that the kubernetes",
    "start": "904460",
    "end": "910709"
  },
  {
    "text": "api offers it's easy to do things that you might later regret so skip this up at night for a little",
    "start": "910709",
    "end": "918149"
  },
  {
    "text": "while but alas we had a plan some of the mistakes that are made in production can",
    "start": "918149",
    "end": "924149"
  },
  {
    "text": "be as innocuous is missing a cost allocation tag does something is dangerous is accidentally exposing a",
    "start": "924149",
    "end": "929940"
  },
  {
    "text": "service to the entire world I don't know has anybody has anybody done here anybody anybody done that here",
    "start": "929940",
    "end": "935339"
  },
  {
    "text": "yeah it happens doesn't it that's fun and you know our back provides some safety but it really it's it's limited",
    "start": "935339",
    "end": "942660"
  },
  {
    "text": "in scope and it's not the general purpose policy engine that we were looking for and controlling for some of",
    "start": "942660",
    "end": "947730"
  },
  {
    "text": "these factors fortunately open policy agent already existed and looked to be",
    "start": "947730",
    "end": "952769"
  },
  {
    "text": "just the thing that we needed this may be again this may be a little bit difficult to see but here is an example",
    "start": "952769",
    "end": "960230"
  },
  {
    "text": "policy written in Rago the domain-specific language that opa uses to define policies if you've ever heard",
    "start": "960230",
    "end": "966899"
  },
  {
    "text": "horror stories about somebody accidentally exposing an eternal service to the world OPA has got your back this particular",
    "start": "966899",
    "end": "973709"
  },
  {
    "text": "example prevents the creation of kubernetes services of type load balancer that haven't been whitelisted",
    "start": "973709",
    "end": "979670"
  },
  {
    "text": "OPA comes with a library of other example policies that cover a good deal of ground and it's also easy to offer",
    "start": "979670",
    "end": "985620"
  },
  {
    "text": "your own fast 40 today OPA has done well and enforcing a small and growing set of",
    "start": "985620",
    "end": "991620"
  },
  {
    "text": "protective policies for us if you've ever wanted to restrict actions that involve specific attributes on",
    "start": "991620",
    "end": "997260"
  },
  {
    "text": "kubernetes objects OPA is likely to be of interest to you by combining our back",
    "start": "997260",
    "end": "1002390"
  },
  {
    "text": "and OPA we're able to empower our service owners while also having sensible guardrails in place as far as",
    "start": "1002390",
    "end": "1009200"
  },
  {
    "text": "what didn't go well in our experience with OPA the only gotcha has been with kubernetes versions before 1.14 story",
    "start": "1009200",
    "end": "1017390"
  },
  {
    "text": "time we had a kubernetes 1.10 development cluster that saw a great deal of pod churn during peak hours the",
    "start": "1017390",
    "end": "1025069"
  },
  {
    "text": "churn put quite a bit of stress on the control plane after passing some arbitrary load threshold request to the",
    "start": "1025069",
    "end": "1030949"
  },
  {
    "text": "kubernetes api server began slowing down and piling up the logs showed that the",
    "start": "1030949",
    "end": "1036110"
  },
  {
    "text": "api service request to the OPA admissions hook were timing out and the end result was a pile up of requests",
    "start": "1036110",
    "end": "1042230"
  },
  {
    "text": "kubernetes api server the API server could no longer service most in down requests from control plane components",
    "start": "1042230",
    "end": "1048110"
  },
  {
    "text": "or the workload the cluster scheduling and cleanup of pod just reached to a halt removing the open missions controller",
    "start": "1048110",
    "end": "1055220"
  },
  {
    "text": "hook caused things to clear up immediately the good news is that starting with kubernetes one at 14 you",
    "start": "1055220",
    "end": "1061580"
  },
  {
    "text": "can now set configurable admission hook admissions hook timeouts that are much lower than the previous 30 second",
    "start": "1061580",
    "end": "1068299"
  },
  {
    "text": "default it's now I think it's 10 seconds in 1.14 and beyond also the OPA folks",
    "start": "1068299",
    "end": "1074510"
  },
  {
    "text": "have worked to optimize the agent to good effect we have not seen this failure mode in our 1.14 clusters with",
    "start": "1074510",
    "end": "1080390"
  },
  {
    "text": "appropriate timeout set with newer versions of the OPA agent as far as what",
    "start": "1080390",
    "end": "1085490"
  },
  {
    "text": "we do differently we've had an overall positive experience with OPA so far our death spiral issue was largely due to a",
    "start": "1085490",
    "end": "1091429"
  },
  {
    "text": "specific cluster control plane being under sized with OPA only exacerbating a",
    "start": "1091429",
    "end": "1096679"
  },
  {
    "text": "situation that was already not great if we could go back and do one thing differently we probably should have",
    "start": "1096679",
    "end": "1102320"
  },
  {
    "text": "checked out how the agent would have fared on a highly stressed control plane this would have ultimately led us to",
    "start": "1102320",
    "end": "1107570"
  },
  {
    "text": "assign a higher priority for our kubernetes 1.14 upgrade so that we could get those configurable admissions hook",
    "start": "1107570",
    "end": "1114110"
  },
  {
    "text": "timeouts today OPA is running on every one of our 19 clusters and we have policies in place",
    "start": "1114110",
    "end": "1120890"
  },
  {
    "text": "for most of the things that keep us up at night the other thing that's been really handy has been being able to set",
    "start": "1120890",
    "end": "1127070"
  },
  {
    "text": "new policies to something called monitor mode to see when they would have fired in enforce mode this is especially",
    "start": "1127070",
    "end": "1133700"
  },
  {
    "text": "useful when you're on the lead-up to bringing incorporating a new policy or even in ad-hoc experiments maybe you",
    "start": "1133700",
    "end": "1140330"
  },
  {
    "text": "want to check when what cases am I using the latest tag without actually taking any action really handy and now for",
    "start": "1140330",
    "end": "1149570"
  },
  {
    "text": "everyone's favorite topic this is why you are all here right yeah makan when",
    "start": "1149570",
    "end": "1155419"
  },
  {
    "text": "it comes to llamo our story is not unique in late 2016 we were just beginning to experiment with kubernetes",
    "start": "1155419",
    "end": "1161540"
  },
  {
    "text": "we started our journey using static manifests and get wrapped in some bash scripts to push things to our cluster",
    "start": "1161540",
    "end": "1167919"
  },
  {
    "text": "anybody doing that anybody done that be honest I know you're out there yeah",
    "start": "1167919",
    "end": "1172990"
  },
  {
    "text": "lots of they're good at some point we decided we needed to do some light templating does anybody know where this",
    "start": "1172990",
    "end": "1179320"
  },
  {
    "text": "is going so he started doing some some obscene things was said and bash and re",
    "start": "1179320",
    "end": "1185080"
  },
  {
    "text": "mo scripts how about this you know what done the old said in amyl trick okay there's a few hands yeah very",
    "start": "1185080",
    "end": "1193120"
  },
  {
    "text": "very flippant like I guess alright but early 2017 we had had about enough",
    "start": "1193120",
    "end": "1200140"
  },
  {
    "text": "of the bash and said trick and began thinking about how he'd like our Manifest story to look longer term in the spirit of not asking our service",
    "start": "1200140",
    "end": "1206740"
  },
  {
    "text": "owners to become kubernetes experts it was important that we provide a simple and minimal and interface as possible we",
    "start": "1206740",
    "end": "1212920"
  },
  {
    "text": "also didn't want to write abstractions on top of ammo without having first gained a thorough understanding of our",
    "start": "1212920",
    "end": "1218020"
  },
  {
    "text": "eventual production usage cases after evaluating options we were attracted to helm for its growing uptake around the",
    "start": "1218020",
    "end": "1224380"
  },
  {
    "text": "community we also felt like we could present just the values override files to our service owners instead of the",
    "start": "1224380",
    "end": "1230200"
  },
  {
    "text": "whole helm chart this could serve is a reasonable user interface for the early goings so further reduced oil we to",
    "start": "1230200",
    "end": "1236530"
  },
  {
    "text": "write tooling to generate comp charts for reddit services that use base plate our internal service framework when it",
    "start": "1236530",
    "end": "1242800"
  },
  {
    "text": "came time to deploy a service spinnaker would pull the helm chart the value overrides render the amyl and deploy it",
    "start": "1242800",
    "end": "1249040"
  },
  {
    "text": "to the cluster without using tiller the good news is that this arrangement has",
    "start": "1249040",
    "end": "1254320"
  },
  {
    "text": "served us very well as a starting point our hypothesis that we could mostly limit service owner contact to just the",
    "start": "1254320",
    "end": "1260830"
  },
  {
    "text": "values override has mostly proven proven true with a few exceptions related to how we structure configs we made the",
    "start": "1260830",
    "end": "1268000"
  },
  {
    "text": "decision to auto generate charts for each service instead of having them all usually central mega chart and we did",
    "start": "1268000",
    "end": "1273760"
  },
  {
    "text": "this to avoid jumping into the deep end of template ID amyl and to reduce widespread churn and breakage as we",
    "start": "1273760",
    "end": "1279220"
  },
  {
    "text": "evolved our patterns the other pleasant surprise has been how well spinnaker and",
    "start": "1279220",
    "end": "1284260"
  },
  {
    "text": "helm have worked together spinnaker is v2 kubernetes provider doesn't care much about where the am yam well that it's",
    "start": "1284260",
    "end": "1290320"
  },
  {
    "text": "deploying comes from which makes helm easy to drop in spinnaker essentially runs the helm template sub command then",
    "start": "1290320",
    "end": "1296500"
  },
  {
    "text": "pipes that through its deploy service we currently run about a hundred production deploys a day through spinnaker using",
    "start": "1296500",
    "end": "1302350"
  },
  {
    "text": "this parent and now for the less rosy stuff that's we're really here for right while our",
    "start": "1302350",
    "end": "1308410"
  },
  {
    "text": "helm chart generator approach worked well in the beginning we're to the point where some of the trade offs began to",
    "start": "1308410",
    "end": "1313450"
  },
  {
    "text": "become more apparent more problematic we now have a few dozen helmet charts that are mostly the same save for a few",
    "start": "1313450",
    "end": "1319270"
  },
  {
    "text": "tweaks and it's been tough keeping all of our charts in sync as our patterns evolve and solidify our eventual goal",
    "start": "1319270",
    "end": "1325870"
  },
  {
    "text": "was to flesh out the helmet our generator to the point where we didn't need to customize the output but we",
    "start": "1325870",
    "end": "1331300"
  },
  {
    "text": "didn't quite make it there we stopped short of that goal in favor of a different approach more on that shortly",
    "start": "1331300",
    "end": "1336910"
  },
  {
    "text": "please stay tuned as far as what we do differently it's easy for me to begin",
    "start": "1336910",
    "end": "1342310"
  },
  {
    "text": "using the benefit of hindsight to give you a like a specific prescriptive answer but honestly I think we've",
    "start": "1342310",
    "end": "1348940"
  },
  {
    "text": "benefitted from going from static Hamill template add amble said in-home and establishing our patterns and in best",
    "start": "1348940",
    "end": "1356290"
  },
  {
    "text": "practices along the way we used an existing community standard in helm instead of reinventing something on our",
    "start": "1356290",
    "end": "1361870"
  },
  {
    "text": "own before we knew what we were doing however our service charts had many possible configurations and optional",
    "start": "1361870",
    "end": "1368500"
  },
  {
    "text": "pieces rather than continue to develop and maintain large complex service charts we could have benefitted from",
    "start": "1368500",
    "end": "1375130"
  },
  {
    "text": "making our transition to a programmatic amo generator sooner and that is exactly",
    "start": "1375130",
    "end": "1381490"
  },
  {
    "text": "what we are in the process of doing instead of a central and single reddit service home chart we wrote a star lark",
    "start": "1381490",
    "end": "1388660"
  },
  {
    "text": "powered domain-specific language for deploying baseplate services at reddit the dialect is a narrow subset of Python",
    "start": "1388660",
    "end": "1396010"
  },
  {
    "text": "which is convenient giving our orgs Python inclinations it is opinionated and only exposes parameters for the",
    "start": "1396010",
    "end": "1402400"
  },
  {
    "text": "things that tend to differ between reddit services though deeper customization is when if deeper",
    "start": "1402400",
    "end": "1408040"
  },
  {
    "text": "customization is needed the DSL offers an escape hatch with which you can provide your own resources another",
    "start": "1408040",
    "end": "1414730"
  },
  {
    "text": "important note is that we do not attempt to conceal kubernetes mechanics and terminology from the developers our",
    "start": "1414730",
    "end": "1420490"
  },
  {
    "text": "philosophy has been to compliment but not conceal kubernetes to summarize to the full flow we passed in a few inputs",
    "start": "1420490",
    "end": "1427300"
  },
  {
    "text": "to the resource generator which then generates the corresponding animal for our service and since the DSL maps",
    "start": "1427300",
    "end": "1433870"
  },
  {
    "text": "directly to the kubernetes go api types under the hood we get a solid type checking and validation story",
    "start": "1433870",
    "end": "1439780"
  },
  {
    "text": "while authoring the starlike files which we did not get with with helm and by shedding deeply templated llamo for the",
    "start": "1439780",
    "end": "1446920"
  },
  {
    "text": "specific case of Reddit authoring services we hope to simplify the development and maintenance of this",
    "start": "1446920",
    "end": "1452290"
  },
  {
    "text": "critical piece of our stack as far as where we are today we have now battle we",
    "start": "1452290",
    "end": "1458410"
  },
  {
    "text": "now have a battle-tested library of patterns and best practices we encounter far fewer firsts first times these days",
    "start": "1458410",
    "end": "1464530"
  },
  {
    "text": "and baseplate our internal service framework allows us to make some assumptions about the shape of our",
    "start": "1464530",
    "end": "1469540"
  },
  {
    "text": "services with our codified best practices in hand or writing a Starlog powered resource generator to replace",
    "start": "1469540",
    "end": "1475870"
  },
  {
    "text": "home charts just for reddit authored services however we wanted the resource",
    "start": "1475870",
    "end": "1480910"
  },
  {
    "text": "generator fit as narrow a use case as possible this means that helm is still used for everything but reddit authored services",
    "start": "1480910",
    "end": "1488040"
  },
  {
    "text": "now with everybody's favorite topic covered let's move on to everyone's",
    "start": "1488040",
    "end": "1493870"
  },
  {
    "text": "second favorite topic it's not service meshes local development environments",
    "start": "1493870",
    "end": "1499120"
  },
  {
    "text": "with kubernetes and gratuitous quantities of micro services all right",
    "start": "1499120",
    "end": "1504340"
  },
  {
    "text": "now be honest with me is anyone here fortunate enough to be super happy with your kubernetes base local dev",
    "start": "1504340",
    "end": "1510820"
  },
  {
    "text": "environment raise your hands if this describes you okay that is a that's a",
    "start": "1510820",
    "end": "1517240"
  },
  {
    "text": "good answer I am not really some maybe like one hand why should I go out and high-five you wherever you were I just",
    "start": "1517240",
    "end": "1522700"
  },
  {
    "text": "saw you yeah probably but you know I'm not here to judge but I'll be honest and",
    "start": "1522700",
    "end": "1531550"
  },
  {
    "text": "I'll say you know we haven't quite made it to that promised land yet it read it in fact we've we've lift a little bit we",
    "start": "1531550",
    "end": "1537520"
  },
  {
    "text": "went into 2019 with a plan we have plan for everything we were admittedly not sure how well it will",
    "start": "1537520",
    "end": "1543310"
  },
  {
    "text": "work out but we had a plan nonetheless a few of our main requirements were the dev environment should resemble",
    "start": "1543310",
    "end": "1549010"
  },
  {
    "text": "production where possible and we wanted to reuse the same home charts throughout the software development lifecycle we",
    "start": "1549010",
    "end": "1555010"
  },
  {
    "text": "also needed to be able to develop multiple macro services in parallel multiple related micro services in",
    "start": "1555010",
    "end": "1561190"
  },
  {
    "text": "parallel and of course all of this was useless without some kind of service a story for making sure service",
    "start": "1561190",
    "end": "1567220"
  },
  {
    "text": "dependencies were accounted for but we were a little bit sketchy on on what that would look like going into 20",
    "start": "1567220",
    "end": "1572310"
  },
  {
    "text": "team the initial plan was to start with scaffold and local mini cube maybe some",
    "start": "1572310",
    "end": "1577860"
  },
  {
    "text": "of you have heard the story before our testing showed that this would work well in simple cases and perhaps it'd be a",
    "start": "1577860",
    "end": "1584190"
  },
  {
    "text": "good foundation to start just just to start it and to start building on as we got further into our experimentation",
    "start": "1584190",
    "end": "1590310"
  },
  {
    "text": "with scaffold a mini cube we found the experience to be very pleasant when all of the pieces functioned at the same",
    "start": "1590310",
    "end": "1596070"
  },
  {
    "text": "time mini cube is an easy way to get a local cluster going without much fuss and the scaffold build loop is fast and",
    "start": "1596070",
    "end": "1603570"
  },
  {
    "text": "solid however early adopters ran into all kinds of breakage and flakiness",
    "start": "1603570",
    "end": "1608610"
  },
  {
    "text": "driving up our support load in frustrating the service owners mini cube seemed to break in new and exciting ways",
    "start": "1608610",
    "end": "1614310"
  },
  {
    "text": "with each release and scaffold had some periodic issues as well yeah when things went wrong the users",
    "start": "1614310",
    "end": "1620160"
  },
  {
    "text": "were frequently unable to figure things out without blowing away configs in cluster state that was that was a bit of",
    "start": "1620160",
    "end": "1625650"
  },
  {
    "text": "a bummer we also ran into a common class of issues associated with multi service",
    "start": "1625650",
    "end": "1630870"
  },
  {
    "text": "development how do we stand up a services full dependency chain how do I fit multiple resource heavy services on",
    "start": "1630870",
    "end": "1637440"
  },
  {
    "text": "my machine scaffold didn't have much of an opinion on how to approach multi service development so most of this",
    "start": "1637440",
    "end": "1644310"
  },
  {
    "text": "exercise was what was left up to us and as we attempted to build tooling around mini Kuban scaffold we quickly arrived",
    "start": "1644310",
    "end": "1650640"
  },
  {
    "text": "at the realization that we had probably lift with this overall approach this is",
    "start": "1650640",
    "end": "1656160"
  },
  {
    "text": "the one case where if I could you know if I could go back and do things differently I would ask for a",
    "start": "1656160",
    "end": "1661920"
  },
  {
    "text": "dramatically different course instead of attempting to build a local development environment today I'd focus us on",
    "start": "1661920",
    "end": "1668490"
  },
  {
    "text": "building against remote clusters instead and there are trade-offs associated with developing against a remote cluster but",
    "start": "1668490",
    "end": "1676020"
  },
  {
    "text": "we found more positives than negatives for our our own users in their usage cases the user doesn't need to hassle",
    "start": "1676020",
    "end": "1682290"
  },
  {
    "text": "with heavy local vm's anymore we don't launch laptops into the ceiling due to all of their cooling fans kicking in at",
    "start": "1682290",
    "end": "1688500"
  },
  {
    "text": "once and we can centralized common service dependencies to reduce complexity and cost all of this can",
    "start": "1688500",
    "end": "1695160"
  },
  {
    "text": "happen in an environment that looks very much like production as far as where we are today we halted work on our mini",
    "start": "1695160",
    "end": "1702300"
  },
  {
    "text": "Cube on a minute cube setup and pivoted to a remote first approach we ended up swapping",
    "start": "1702300",
    "end": "1708750"
  },
  {
    "text": "scaffold out for tilt which ended up being a better dev experience for us as we discussed earlier we're retiring our",
    "start": "1708750",
    "end": "1715110"
  },
  {
    "text": "helm charts in favor of a star lark powered resource generator and tilt is able to invoke the resource generator",
    "start": "1715110",
    "end": "1721470"
  },
  {
    "text": "and directly consumes the amyl that it produces for multi service deployment development till it will project the",
    "start": "1721470",
    "end": "1728490"
  },
  {
    "text": "state of your local service code to a remote cluster and for the sake of simplicity and cost control we will be",
    "start": "1728490",
    "end": "1735090"
  },
  {
    "text": "able to auto deploy the master branches for all cores core services into the remote dev cluster when tilts and your",
    "start": "1735090",
    "end": "1741510"
  },
  {
    "text": "service up that'll already have all of its dependencies in cluster without having to stand up the whole stack for each developer if you want to swap one",
    "start": "1741510",
    "end": "1748350"
  },
  {
    "text": "of their dependencies out to another branch this is possible this will be possible with a quick config update and the result of all this is that we end up",
    "start": "1748350",
    "end": "1755549"
  },
  {
    "text": "with a code development loop that feels similar to true local development future iterations of our dev environment may",
    "start": "1755549",
    "end": "1761309"
  },
  {
    "text": "see a more hybrid local and remote flow where services can be selectively delegated to the remote cluster instead",
    "start": "1761309",
    "end": "1767190"
  },
  {
    "text": "of being fully remote or we may go heavier towards mocking out dependencies it's honestly difficult to predict at",
    "start": "1767190",
    "end": "1773010"
  },
  {
    "text": "this point and thus concludes our look back on the last year of kübra of our",
    "start": "1773010",
    "end": "1778230"
  },
  {
    "text": "kubernetes adoption at reddit well there were certainly some struggles I characterize the last 12 months as very",
    "start": "1778230",
    "end": "1783900"
  },
  {
    "text": "successful service owners are treaded are now empowered to own their services development lifecycle and while there",
    "start": "1783900",
    "end": "1789630"
  },
  {
    "text": "are still some things that require assistance from the infrastructure team those group grow fewer with each passing month but there is still much to do",
    "start": "1789630",
    "end": "1796919"
  },
  {
    "text": "let's take a look at some of the big-ticket items on our list for the next year and beyond first up is our",
    "start": "1796919",
    "end": "1803100"
  },
  {
    "text": "launch process we now have a very well-defined repeatable launch process but there are still lots of manual steps",
    "start": "1803100",
    "end": "1809190"
  },
  {
    "text": "and some in fátima assistance is still required we'd like to make service launches as automated and self-serve as",
    "start": "1809190",
    "end": "1815040"
  },
  {
    "text": "possible and while I ran out of time to cover the story of our sto adoption I'll summarize this by saying we're a thrift",
    "start": "1815040",
    "end": "1822600"
  },
  {
    "text": "shop trying to add thrift support to a gr PC centric mesh that is a wonderful",
    "start": "1822600",
    "end": "1827820"
  },
  {
    "text": "time if there are other any if there are any other thrift users using orgs out in",
    "start": "1827820",
    "end": "1833610"
  },
  {
    "text": "the audience please come see us we would love to collaborate next up to continue to work on our dev",
    "start": "1833610",
    "end": "1838890"
  },
  {
    "text": "environment is a top priority for us we've also got to start optimizing more for cost and density",
    "start": "1838890",
    "end": "1844710"
  },
  {
    "text": "these have been lesser concerns for us in the early goings but we feel that kubernetes has the potential to crunch",
    "start": "1844710",
    "end": "1850379"
  },
  {
    "text": "our fleet down quite a bit and finally I mentioned our budding essary organization earlier on it's a crucial",
    "start": "1850379",
    "end": "1856950"
  },
  {
    "text": "player in our quest to scale infrared and kubernetes out to a rapidly expanding engineering organization last",
    "start": "1856950",
    "end": "1864330"
  },
  {
    "text": "of all here's some information about me if you'd like to stay in touch though I am the lead I'm by far the least",
    "start": "1864330",
    "end": "1869820"
  },
  {
    "text": "interesting part of all of this everything covered today was pondered designed and implemented by a great",
    "start": "1869820",
    "end": "1875759"
  },
  {
    "text": "collection of infrastructure platform and security engineers who many of whom are in the tenants my reddit colleagues",
    "start": "1875759",
    "end": "1883470"
  },
  {
    "text": "please stand for a moment and let's embarrass you once again yes they're over here look right here [Applause]",
    "start": "1883470",
    "end": "1892580"
  },
  {
    "text": "and here's one right here - and of course if you'd like to join the team we",
    "start": "1892580",
    "end": "1898799"
  },
  {
    "text": "are hiring across all sorts of different functions check out our listings at reddit.com slash jobs I am going to go",
    "start": "1898799",
    "end": "1906119"
  },
  {
    "text": "on ahead and step down our booth is on the other side of this hall we've got about 30 minutes before the the the",
    "start": "1906119",
    "end": "1912119"
  },
  {
    "text": "exhibit hall closes please come and let's talk I will handle questions over there rather than here and I thank you",
    "start": "1912119",
    "end": "1919049"
  },
  {
    "text": "very much for coming in uh following along with us today [Applause]",
    "start": "1919049",
    "end": "1930049"
  }
]