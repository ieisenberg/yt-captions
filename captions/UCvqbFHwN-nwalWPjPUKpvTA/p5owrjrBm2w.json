[
  {
    "text": "so yeah hi I'm Jeremy and I'm a Staff engineer with the oero platform team hi",
    "start": "280",
    "end": "6200"
  },
  {
    "text": "my name is kah I'm a ppal engineer in offal platform team uh thank you for",
    "start": "6200",
    "end": "12160"
  },
  {
    "text": "joining with us today uh we are excited to share how we manage and upgrade hundreds of kubernetes Custer daily",
    "start": "12160",
    "end": "19359"
  },
  {
    "text": "without any service Interruption okay so let's go through",
    "start": "19359",
    "end": "25320"
  },
  {
    "text": "the agenda first so we'll talk about the context and our challenges then we will",
    "start": "25320",
    "end": "30400"
  },
  {
    "text": "briefly introduce our platform and then we will spend a large trunk of time to talk about our Solutions finally we will",
    "start": "30400",
    "end": "37559"
  },
  {
    "text": "talk about some outcome and the result last but not least we will have a Q&A",
    "start": "37559",
    "end": "42640"
  },
  {
    "text": "[Music] section so um our offal platform has two",
    "start": "42640",
    "end": "50559"
  },
  {
    "text": "offering we have a public car offering which is a um multi-tenant multi",
    "start": "50559",
    "end": "56000"
  },
  {
    "text": "subscriber and it support multiple customer per environment also we have a private car offering",
    "start": "56000",
    "end": "63000"
  },
  {
    "text": "which has uh which is dedicated for One customer per environment and we have hundreds of them across the RO on top of",
    "start": "63000",
    "end": "70159"
  },
  {
    "text": "that for the pirate car it also W on different car provider currently it w on",
    "start": "70159",
    "end": "76320"
  },
  {
    "text": "ad and assure as well so um with a big picture of like",
    "start": "76320",
    "end": "83680"
  },
  {
    "text": "how many Custer that we spend globally right so uh let me summarize like how we",
    "start": "83680",
    "end": "88960"
  },
  {
    "text": "like what's the management challenge that we Face daily so first as I mentioned from the",
    "start": "88960",
    "end": "95079"
  },
  {
    "text": "previous slides we have hundreds of environment and we are globally distributed and also we have hundreds of",
    "start": "95079",
    "end": "102680"
  },
  {
    "text": "deployment that happen daily and they happen like 24 hour 7 and each customer environment are they",
    "start": "102680",
    "end": "109640"
  },
  {
    "text": "different like they have different infrastructure size like they have different kubernetes size different DB",
    "start": "109640",
    "end": "114799"
  },
  {
    "text": "size and they run on different application stack that serve different purpose and they are multi as",
    "start": "114799",
    "end": "121320"
  },
  {
    "text": "well and also really to like make sure that our infrastructure version always up to dat so let's say right eks release",
    "start": "121320",
    "end": "128160"
  },
  {
    "text": "a new kubernetes version we need to upgrade yway without any down time finally we need to have a constant",
    "start": "128160",
    "end": "136160"
  },
  {
    "text": "security update that means that our OS level and our application stack level",
    "start": "136160",
    "end": "141720"
  },
  {
    "text": "security patch need to up toate y",
    "start": "141720",
    "end": "145800"
  },
  {
    "text": "away so our pass it to Jeremy to introduce platform",
    "start": "146760",
    "end": "152280"
  },
  {
    "text": "thanks so before we get into more technical details about our deploy strategies we wanted to quickly go over",
    "start": "152280",
    "end": "158400"
  },
  {
    "text": "our platform uh just to give you some important context about what we",
    "start": "158400",
    "end": "164720"
  },
  {
    "text": "built so our platform runs all zero for all our customers it unifies many",
    "start": "164720",
    "end": "170720"
  },
  {
    "text": "different customer offerings into a modern automated and scalable platform",
    "start": "170720",
    "end": "176000"
  },
  {
    "text": "that is built for the future our goals were to increase uh business agility developer",
    "start": "176000",
    "end": "182879"
  },
  {
    "text": "productivity scalability reliability and security by using uh industry leading",
    "start": "182879",
    "end": "188640"
  },
  {
    "text": "tools and fully automated operations so like we mentioned our",
    "start": "188640",
    "end": "193799"
  },
  {
    "text": "platform is multicloud So currently we support a Azure it is a container",
    "start": "193799",
    "end": "199000"
  },
  {
    "text": "orchestrated all our services runs inside kubernetes clusters it's stateless so all the",
    "start": "199000",
    "end": "206959"
  },
  {
    "text": "components are scalable and replaceable by design including all kubernetes",
    "start": "206959",
    "end": "213239"
  },
  {
    "text": "clusters it is immutable so it only runs trusted code and",
    "start": "213239",
    "end": "219000"
  },
  {
    "text": "configuration it is fully automated so the provisioning upgrades and decommissioning are all done by",
    "start": "219000",
    "end": "225720"
  },
  {
    "text": "machines we also slightly rely on GitHub so our GitHub releases are used as a source of",
    "start": "225720",
    "end": "232239"
  },
  {
    "text": "Truth our platform is not only used by our customers we also use it internally",
    "start": "232239",
    "end": "237280"
  },
  {
    "text": "for many of our tools and services",
    "start": "237280",
    "end": "241720"
  },
  {
    "text": "so that's a very high level view of the platform so on one side we have our",
    "start": "243120",
    "end": "249040"
  },
  {
    "text": "control plane which contains many different grpc services and controllers",
    "start": "249040",
    "end": "255360"
  },
  {
    "text": "as well as an Argo workflows and Argo CD instances on the other side we have our",
    "start": "255360",
    "end": "261639"
  },
  {
    "text": "customer environments in various regions and clouds so to manage this our control",
    "start": "261639",
    "end": "269000"
  },
  {
    "text": "plane stores many entities in a database they represent everything we deploy from",
    "start": "269000",
    "end": "274400"
  },
  {
    "text": "kubernetes cluster configuration and secret values to actual uh",
    "start": "274400",
    "end": "280280"
  },
  {
    "text": "deployments so in order to better manage those entities we also maintain our own terraform provider with it we are able",
    "start": "280280",
    "end": "287639"
  },
  {
    "text": "to do infra as code for all the objects that are managed by the",
    "start": "287639",
    "end": "292880"
  },
  {
    "text": "platform and we also maintain our own set of customized plugins this is to",
    "start": "292880",
    "end": "298560"
  },
  {
    "text": "allow our service owner an easy way to generate communities manifest that will follow all of our",
    "start": "298560",
    "end": "305520"
  },
  {
    "text": "best practices so at first we used a",
    "start": "305520",
    "end": "312520"
  },
  {
    "text": "traditional rolling deployment on the first sitation of the platform it meant we could have two versions saving",
    "start": "312520",
    "end": "319919"
  },
  {
    "text": "traffic at the same time so in case of an issue with the new version it can actually reduce our",
    "start": "319919",
    "end": "326400"
  },
  {
    "text": "capacity and also rolling back is not trivial because we need to ensure that both",
    "start": "326400",
    "end": "332479"
  },
  {
    "text": "service version are compatible with whatever infra version is at the time it was also time consuming and had",
    "start": "332479",
    "end": "340560"
  },
  {
    "text": "to be done by an operator manually with Cube control and at last with rolling deploy",
    "start": "340560",
    "end": "347160"
  },
  {
    "text": "as many of you know it's really difficult to do no downtime kubernetes version upgrades as well as node base",
    "start": "347160",
    "end": "354639"
  },
  {
    "text": "image or just changing any of the configuration of your node pools",
    "start": "354639",
    "end": "360280"
  },
  {
    "text": "and also uh rolling deploy means that any secret rotation has to work on both",
    "start": "360280",
    "end": "367680"
  },
  {
    "text": "clusters on both versions of the",
    "start": "367680",
    "end": "371880"
  },
  {
    "text": "infra thanks jery for the platform introduction so um with the boing",
    "start": "375080",
    "end": "380720"
  },
  {
    "text": "deployment right we know that some of the deployment requirement canot be fulfill because Li we always serve two",
    "start": "380720",
    "end": "386080"
  },
  {
    "text": "application at the same time so let's recap Li what we need to have for the our department requirement first we to",
    "start": "386080",
    "end": "391680"
  },
  {
    "text": "have low down time during upgrade and also we to have fast roll back during the",
    "start": "391680",
    "end": "396759"
  },
  {
    "text": "incidents also we have a important features called opto automated size change that means for our customer",
    "start": "396759",
    "end": "403840"
  },
  {
    "text": "environment they can up scale and down scale D upgrade without any down time",
    "start": "403840",
    "end": "409599"
  },
  {
    "text": "and also I like we have a seamless infrastructure version upgrade as well that also lead to support low down",
    "start": "409599",
    "end": "415039"
  },
  {
    "text": "time so with a low out of box solution offo platform team a solution called Web",
    "start": "415039",
    "end": "420639"
  },
  {
    "text": "back deployment so what is a web back deployment so basically we create a",
    "start": "420639",
    "end": "427919"
  },
  {
    "text": "brand new kubernetes cluster for every deploy that we occurs in each customer",
    "start": "427919",
    "end": "433160"
  },
  {
    "text": "devourment I mean environment sorry so as you can see from the diagram right on",
    "start": "433160",
    "end": "438199"
  },
  {
    "text": "the left side let's say like initially we only have a old cust rning which is",
    "start": "438199",
    "end": "443599"
  },
  {
    "text": "the black Custer in there and is serving traffic so let's say a deployment occurs",
    "start": "443599",
    "end": "449280"
  },
  {
    "text": "our control ping will create a brand new kubernetes Custer with the related infrastructure which is the red Custer",
    "start": "449280",
    "end": "455520"
  },
  {
    "text": "on the left side then our control ping will launch couple worko that will install the new application stack on the",
    "start": "455520",
    "end": "462400"
  },
  {
    "text": "new Custer and it will want the system test across um against to the new Custer",
    "start": "462400",
    "end": "467840"
  },
  {
    "text": "so if the Custer is running up and he healthy and then the system test pass that me is update the DNS Swit by pting",
    "start": "467840",
    "end": "475680"
  },
  {
    "text": "the traffic to the new Custer so after that we will delete the old Custer along",
    "start": "475680",
    "end": "481039"
  },
  {
    "text": "with the old application stack and then decare the deployment as verified so it require as I said it",
    "start": "481039",
    "end": "489120"
  },
  {
    "text": "require a lot of Step but actually our reack deployment is 100% automated with low down time so um in order to achieve",
    "start": "489120",
    "end": "497800"
  },
  {
    "text": "that uh there are several area that we to focus on doing the implementation so",
    "start": "497800",
    "end": "503319"
  },
  {
    "text": "first we have a set of well fought control ping data model that drive the",
    "start": "503319",
    "end": "508360"
  },
  {
    "text": "whole deployment life cycle also we have a very consistent release Pipeline and definition and on",
    "start": "508360",
    "end": "515680"
  },
  {
    "text": "top of release there's low impact on the secret and confi update across",
    "start": "515680",
    "end": "521479"
  },
  {
    "text": "releases lastly we also have aine control workl orchestration that handle like different incident",
    "start": "521479",
    "end": "528880"
  },
  {
    "text": "scenario so um let's focus on the control data model definition",
    "start": "529240",
    "end": "534920"
  },
  {
    "text": "first so basically we abstract everything such as the customer environment",
    "start": "534920",
    "end": "540399"
  },
  {
    "text": "kubernetes Custer um config and sequence that the microsurface need as a data model objects that the control pain can",
    "start": "540399",
    "end": "548560"
  },
  {
    "text": "understand by the control pain I don't means like what joury introduced like those microsurface that drive the",
    "start": "548560",
    "end": "554480"
  },
  {
    "text": "deployment logic right it also include the tform pugin that we Implement so",
    "start": "554480",
    "end": "559959"
  },
  {
    "text": "basically the tform pugin can consume the object and you can create a correlated um infrastructure or update",
    "start": "559959",
    "end": "566959"
  },
  {
    "text": "or delete it based on the object state and our homec customized plin can also",
    "start": "566959",
    "end": "574040"
  },
  {
    "text": "consume the um uh data model object and render the appropriate manifest to",
    "start": "574040",
    "end": "581079"
  },
  {
    "text": "install our application application correctly so finally uh our we also map",
    "start": "581079",
    "end": "588440"
  },
  {
    "text": "the kubernetes object to the application uh to AG C application such that like for example if we create one uh",
    "start": "588440",
    "end": "594800"
  },
  {
    "text": "kubernetes Custer we will also like create one agoc C application to install the stack on top of",
    "start": "594800",
    "end": "601920"
  },
  {
    "text": "it so um regarding to the release peline we have a very consistent release",
    "start": "601920",
    "end": "608519"
  },
  {
    "text": "pipeline so um our release pipeline is called release channel so a release channel is basically um uh a pipline",
    "start": "608519",
    "end": "616360"
  },
  {
    "text": "that the custom the customer environment can fall into and then like different release Channel um use different",
    "start": "616360",
    "end": "622959"
  },
  {
    "text": "release and our deployment must Target to like at uh One release and our",
    "start": "622959",
    "end": "628519"
  },
  {
    "text": "release is based Bas on something called release manifest so what is a release manifest is basically a compete",
    "start": "628519",
    "end": "635760"
  },
  {
    "text": "description of the following item such as the microservice infrastructure",
    "start": "635760",
    "end": "641040"
  },
  {
    "text": "change um kubernetes manifest change and customized changes so let's say if one",
    "start": "641040",
    "end": "646959"
  },
  {
    "text": "of the item above item change our control P will create One release and this release can deploy to the customer",
    "start": "646959",
    "end": "653120"
  },
  {
    "text": "environment so with the concept of the release Channel release manifest it",
    "start": "653120",
    "end": "658240"
  },
  {
    "text": "gives us a very consistent me of what get deployed to different customer environment and then we can Backtrack on",
    "start": "658240",
    "end": "664560"
  },
  {
    "text": "live what's wrong with each environment by looking into like each release what C chain something like",
    "start": "664560",
    "end": "670760"
  },
  {
    "text": "that thanks so in the previous slid we mentioned a few times uh configs and",
    "start": "670760",
    "end": "675920"
  },
  {
    "text": "secret values and so we handle them a bit differently as well so for at first",
    "start": "675920",
    "end": "682120"
  },
  {
    "text": "what are they used for so typically the microservices will require configuration or secret to function properly",
    "start": "682120",
    "end": "689880"
  },
  {
    "text": "the configs and secrets are generated by terraform during the infra provisioning phase they are generated either by our",
    "start": "689880",
    "end": "696360"
  },
  {
    "text": "own terraform Plugin or by any of the third party providers we use so they",
    "start": "696360",
    "end": "701680"
  },
  {
    "text": "will be based on uh infra information for example a database endpoint three",
    "start": "701680",
    "end": "706839"
  },
  {
    "text": "bucket name but sometimes they are also generated directly by our services uh to",
    "start": "706839",
    "end": "712040"
  },
  {
    "text": "share secret values between services for for instance so what happens if a secret or",
    "start": "712040",
    "end": "718440"
  },
  {
    "text": "a config value changes uh and works only on one specific version of infra then uh we",
    "start": "718440",
    "end": "725680"
  },
  {
    "text": "need to make sure that this will not cause any issue if we need to roll back the cluster and so to solve this problem",
    "start": "725680",
    "end": "733000"
  },
  {
    "text": "we have uh the concept of config snapshot and secret so basically a",
    "start": "733000",
    "end": "738279"
  },
  {
    "text": "config snapshot or secret is um a specific value for a specific customer",
    "start": "738279",
    "end": "745079"
  },
  {
    "text": "for a specific release version and so since each cluster will tie to a specific release then we know",
    "start": "745079",
    "end": "752800"
  },
  {
    "text": "that during every deployment we always have that snapshot to fall back to and",
    "start": "752800",
    "end": "757920"
  },
  {
    "text": "this is like a very crucial step in our release",
    "start": "757920",
    "end": "763639"
  },
  {
    "text": "automation thanks Jeremy so let's Deep dive into our deployment uh rle so um we",
    "start": "764199",
    "end": "771000"
  },
  {
    "text": "have a very fine control light deployment rle to support like different incident scenario but first let's focus",
    "start": "771000",
    "end": "776760"
  },
  {
    "text": "on our happy path that doesn't have any deployment fil so let's say a release is create",
    "start": "776760",
    "end": "783680"
  },
  {
    "text": "from the release peline and then our deployment schedule in the control pain detect a new release deployment",
    "start": "783680",
    "end": "789519"
  },
  {
    "text": "scheduler will create a new deploy to the controller so controller will submit",
    "start": "789519",
    "end": "797279"
  },
  {
    "text": "couple AR CD Aro workflow and the work first workflow will be potioning",
    "start": "797279",
    "end": "802360"
  },
  {
    "text": "workflow so the responsibility of this workfall is to poition the new",
    "start": "802360",
    "end": "808279"
  },
  {
    "text": "kubernetes custom along with all the related infrastructure and also it will create all the necessary config and",
    "start": "808279",
    "end": "815079"
  },
  {
    "text": "secret that the microsurface need then when the INF positioning is",
    "start": "815079",
    "end": "820279"
  },
  {
    "text": "done right this refle will also ask AR AG CD to send the new application stack",
    "start": "820279",
    "end": "826800"
  },
  {
    "text": "against the new Custer and then this workflow we also launch a system test to test again this",
    "start": "826800",
    "end": "833079"
  },
  {
    "text": "Custer and then if everything is fine and happy then control pain will launch",
    "start": "833079",
    "end": "838560"
  },
  {
    "text": "a PO promoting workfall so for this workfall you do a couple things the",
    "start": "838560",
    "end": "843600"
  },
  {
    "text": "first thing it will do a DNS switch and it will switch the traffic by pointing to the new Custer and also make sure",
    "start": "843600",
    "end": "850120"
  },
  {
    "text": "that like the old Custer doesn't have any pending active connection before we move",
    "start": "850120",
    "end": "855880"
  },
  {
    "text": "forward so once we Lo that light the old Custer is has nothing to do with the",
    "start": "855880",
    "end": "861720"
  },
  {
    "text": "release then the control pain will submit a cleanup workflow so what this workflow does is that you just do the",
    "start": "861720",
    "end": "868199"
  },
  {
    "text": "cleanup like the old Custer KY of the a CD application stack and then we will decare the deployment as verifi and",
    "start": "868199",
    "end": "875020"
  },
  {
    "text": "[Music] finish so we also need to support a r back use case so let's say deployments",
    "start": "875020",
    "end": "881959"
  },
  {
    "text": "happen we upgrad in customer environment and then something happened right and then I incidence occurs s is Page and",
    "start": "881959",
    "end": "890720"
  },
  {
    "text": "then I S see that what happened and then I will just start the roll back",
    "start": "890720",
    "end": "896040"
  },
  {
    "text": "operation at that time control P will submit W back workflow the first workflow will be a w back traffic",
    "start": "896040",
    "end": "902839"
  },
  {
    "text": "workflow so what does this workflow does is basically just Pawn the traffic back to the old Custer and the turn around",
    "start": "902839",
    "end": "909120"
  },
  {
    "text": "time is less than 10 minutes and the main idea of this workow is to mitigate",
    "start": "909120",
    "end": "914160"
  },
  {
    "text": "any down time on the customer size so that we can like extend our investigation so once we know that the",
    "start": "914160",
    "end": "921480"
  },
  {
    "text": "traffic pointing to the old Custer then the then we can start investigating like",
    "start": "921480",
    "end": "926680"
  },
  {
    "text": "what's wrong with the little Custer and see like if can fix it so let's say if you cannot fix it and then we can decare",
    "start": "926680",
    "end": "932800"
  },
  {
    "text": "is a full failure SLE can resume the full full roll back and control pain",
    "start": "932800",
    "end": "938279"
  },
  {
    "text": "will submit a roll back kup workflow this workflow will remove any fail",
    "start": "938279",
    "end": "944600"
  },
  {
    "text": "infrastructure including the fail kuber cluster and also the FD application and we will declare this upgrade is",
    "start": "944600",
    "end": "952399"
  },
  {
    "text": "fail also we to handle a false alarm use case so let's say right like s is Page",
    "start": "952399",
    "end": "960639"
  },
  {
    "text": "and then s submit work for and then control ping switch to traffic back to",
    "start": "960639",
    "end": "968000"
  },
  {
    "text": "the USTA by running the B traffic work for at that point inant responser step",
    "start": "968000",
    "end": "975560"
  },
  {
    "text": "in and look into the luster and found that oh like is a force alarm it's okay to upgrade what what we what he would do",
    "start": "975560",
    "end": "983199"
  },
  {
    "text": "is that you tell SLE that like oh you can go ahead and retry the promotion and",
    "start": "983199",
    "end": "988240"
  },
  {
    "text": "then s submit a retri promotion operation to a controller controller will submit a retri promotion I mean the",
    "start": "988240",
    "end": "995319"
  },
  {
    "text": "promotion workflow this workflow is just same as the happy path that you will Pawn the customer Pawn the traffic back",
    "start": "995319",
    "end": "1000680"
  },
  {
    "text": "to the real Custer so from this diagram right I haven't I I didn't mention that like after the promoting workflow you",
    "start": "1000680",
    "end": "1007360"
  },
  {
    "text": "also want the kup workflow and it will delete the the uster and decare the upgrade is",
    "start": "1007360",
    "end": "1013600"
  },
  {
    "text": "finished thank you uh so yeah that was the high level view of the red black",
    "start": "1014800",
    "end": "1020440"
  },
  {
    "text": "deploy strategy so once we had a good first version and we started using it we",
    "start": "1020440",
    "end": "1025839"
  },
  {
    "text": "had some additional considerations to take into account so first of all we",
    "start": "1025839",
    "end": "1030918"
  },
  {
    "text": "need to ensure we have enough capacity after the DNS switch we also need to ensure we have no",
    "start": "1030919",
    "end": "1036880"
  },
  {
    "text": "data loss when we switch from the old to the new cluster we need to take into account",
    "start": "1036880",
    "end": "1042360"
  },
  {
    "text": "some specific edge cases such as singl tone services and since the Clusters are",
    "start": "1042360",
    "end": "1048919"
  },
  {
    "text": "feral we need to have a new requirement that all components in the cluster needs to be",
    "start": "1048919",
    "end": "1054760"
  },
  {
    "text": "stateless also this allowed us to build some extra features on top of red black such as the traffic segmentation to",
    "start": "1054760",
    "end": "1061480"
  },
  {
    "text": "slowly roll out releases on our public Cloud environment cluster overlapping period",
    "start": "1061480",
    "end": "1067600"
  },
  {
    "text": "which is used in some Edge case scenarios where we need extra reliability and at last the no downtime",
    "start": "1067600",
    "end": "1073840"
  },
  {
    "text": "secret rotation so for the capacity issue uh so",
    "start": "1073840",
    "end": "1080480"
  },
  {
    "text": "the the main problem is that when we do the DNS switch the black cluster to the old cluster is serving traffic up until",
    "start": "1080480",
    "end": "1087640"
  },
  {
    "text": "the point we start the DNS switch process and since we use horizontal pod",
    "start": "1087640",
    "end": "1093000"
  },
  {
    "text": "Autos scaling and cluster Autos scaling it means that up until that point we cannot be sure what actual node size and",
    "start": "1093000",
    "end": "1099880"
  },
  {
    "text": "replica counts are in the cluster so what we do is right before the DNS switch we will copy over uh the",
    "start": "1099880",
    "end": "1107559"
  },
  {
    "text": "replica account from the old cluster to the new cluster so we will use the old cluster replicas as the new cluster",
    "start": "1107559",
    "end": "1114080"
  },
  {
    "text": "current and minimum replicas once the deployment is finished we will reset the HPA to the default",
    "start": "1114080",
    "end": "1121640"
  },
  {
    "text": "values and from now on we just let it handle a scale as usual we apply the",
    "start": "1121640",
    "end": "1127280"
  },
  {
    "text": "same logic on the Node sizing where we copy the node desired and minan count",
    "start": "1127280",
    "end": "1132760"
  },
  {
    "text": "from the old to the new cluster so for the data loss uh so we",
    "start": "1132760",
    "end": "1139919"
  },
  {
    "text": "have a few Asing tasks that are running in the cluster such as mq and we also want to make sure before",
    "start": "1139919",
    "end": "1146960"
  },
  {
    "text": "we switch to the new cluster that everything is ready to go in there so for that we mainly use polling and so we",
    "start": "1146960",
    "end": "1154559"
  },
  {
    "text": "emit data do Matrix um Matrix to data do in our control plane and so what we do",
    "start": "1154559",
    "end": "1161159"
  },
  {
    "text": "is that we have a basically two steps in our workflow to pull a cluster and check",
    "start": "1161159",
    "end": "1167600"
  },
  {
    "text": "some matrics so before we do the DNS switch we will pull the new cluster and",
    "start": "1167600",
    "end": "1173400"
  },
  {
    "text": "make sure all the HTTP endpoints are ready to accept connections and before we start the",
    "start": "1173400",
    "end": "1179760"
  },
  {
    "text": "cleanup workflow we will PLL for matrics such as um inflight messages and",
    "start": "1179760",
    "end": "1187440"
  },
  {
    "text": "connection counts on some other endpoints and so what we do is that we wait until those metric which the",
    "start": "1187440",
    "end": "1193400"
  },
  {
    "text": "numbers we want and when they do we just move on to the next step",
    "start": "1193400",
    "end": "1199919"
  },
  {
    "text": "uh so single Services is mainly an issue due to some Legacy services so once upon a time we had services that had",
    "start": "1201120",
    "end": "1209200"
  },
  {
    "text": "exclusive exclusive access requirement to the to the to",
    "start": "1209200",
    "end": "1215000"
  },
  {
    "text": "database uh so it means that those Services cannot run in two clusters at the same time even if the cluster is not",
    "start": "1215000",
    "end": "1222000"
  },
  {
    "text": "serving actually any traffic so to fix that we provide our service owners with a specific",
    "start": "1222000",
    "end": "1228840"
  },
  {
    "text": "customized annotation they can use so like we mentioned earlier the service owners will use our customized plugin",
    "start": "1228840",
    "end": "1235440"
  },
  {
    "text": "which exports many customiz generators that they can use to generate ni kubernetes manifest and so one such",
    "start": "1235440",
    "end": "1242159"
  },
  {
    "text": "annotation is a single tone service annotation so they will indicate that",
    "start": "1242159",
    "end": "1247280"
  },
  {
    "text": "their service needs to run exclusively in one cluster and what we do is during the automation uh after the Aro CD sync",
    "start": "1247280",
    "end": "1255039"
  },
  {
    "text": "and before the Aro CD sync we will check for all the singl ton services and those services are scheduled on the new",
    "start": "1255039",
    "end": "1261679"
  },
  {
    "text": "cluster at first with a zero replica and once the syn is done we will copy over",
    "start": "1261679",
    "end": "1267240"
  },
  {
    "text": "the replica count from the old to the new cluster we will scale it down on the old cluster and we will scale it back up",
    "start": "1267240",
    "end": "1273000"
  },
  {
    "text": "on the new cluster thanks Jeremy so um Canary",
    "start": "1273000",
    "end": "1281640"
  },
  {
    "text": "traffic routing so as I mentioned before right during the department fly cycle we promote the traffic ass point of the",
    "start": "1281640",
    "end": "1288600"
  },
  {
    "text": "promoting Rec for so um basically when we switch the DNS it's not only one",
    "start": "1288600",
    "end": "1293919"
  },
  {
    "text": "record but it include like a lot of record and each record include a lot of tenants so um control P has an option to",
    "start": "1293919",
    "end": "1302400"
  },
  {
    "text": "slow down speed up or do a bu DNS update during the promotion so why we to do it",
    "start": "1302400",
    "end": "1309080"
  },
  {
    "text": "because we need to like reduce the B radius like on some sensitive deployment so let's say like release a very",
    "start": "1309080",
    "end": "1315720"
  },
  {
    "text": "sensitive release on the on new Custer and want to co the it you want to slow down the DNS um traffic segmentation and",
    "start": "1315720",
    "end": "1322679"
  },
  {
    "text": "try to see if everything works fine if something happen right then the bus radius is reduced and then we can do a",
    "start": "1322679",
    "end": "1328480"
  },
  {
    "text": "bu update to P back the traffic wway to the uster similarly we have a custom",
    "start": "1328480",
    "end": "1336000"
  },
  {
    "text": "overlapping feature so imagine like we finish the deployment the uster is gone",
    "start": "1336000",
    "end": "1343240"
  },
  {
    "text": "and then inent start why after deployment is gone so the action item is that I to patch or make a new release",
    "start": "1343240",
    "end": "1350120"
  },
  {
    "text": "and do another web back deployment which takes a lot of time so this option allow",
    "start": "1350120",
    "end": "1356159"
  },
  {
    "text": "the both customer coexist at the same time even up to several weeks so that",
    "start": "1356159",
    "end": "1362520"
  },
  {
    "text": "means that like if like um something happen right then we can move back to traffic wway without being doing a new",
    "start": "1362520",
    "end": "1369760"
  },
  {
    "text": "deployment but then like this feature pay a price because like with two Comm",
    "start": "1369760",
    "end": "1374960"
  },
  {
    "text": "cust Waring um is expensive in aut the car provider so we only do it in like",
    "start": "1374960",
    "end": "1380200"
  },
  {
    "text": "some special environment such as all our public car",
    "start": "1380200",
    "end": "1385159"
  },
  {
    "text": "environment so as far as observability I like I mention we use data dog and we",
    "start": "1385919",
    "end": "1391080"
  },
  {
    "text": "have instrumented our platform to emit metrix on many different events for example our deploy managers will emit",
    "start": "1391080",
    "end": "1397200"
  },
  {
    "text": "metrics when we have a deploy success or a deploy failure in addition to this we will also Leverage The metrics that are",
    "start": "1397200",
    "end": "1403880"
  },
  {
    "text": "emitted by Argo workflows and Aro CD so we combine all those metric into dashboards for the SES and operators to",
    "start": "1403880",
    "end": "1411640"
  },
  {
    "text": "consume uh these dashboards can be used to troubleshoot deployment issues and we also have alerting set up on some",
    "start": "1411640",
    "end": "1418480"
  },
  {
    "text": "metrics so for example we have dashboards for step uh space deployment steps duration uh reliability of",
    "start": "1418480",
    "end": "1425720"
  },
  {
    "text": "specific steps uh and deployment success and failures and for incidents we have a",
    "start": "1425720",
    "end": "1434000"
  },
  {
    "text": "good example would be that we have a a trigger incident when many deployment failures event occur at the same time",
    "start": "1434000",
    "end": "1440919"
  },
  {
    "text": "which is usually indicative that there is an issue with one of our third party",
    "start": "1440919",
    "end": "1446080"
  },
  {
    "text": "providers so these metrics are not only used for troubl shooting purpose we will also use them to improve our platform so",
    "start": "1446080",
    "end": "1452880"
  },
  {
    "text": "we will use them to improve our scaling we will use them to improve our deployment speed",
    "start": "1452880",
    "end": "1458679"
  },
  {
    "text": "Etc so now that we want over the deploy strategy we can share some of the",
    "start": "1460279",
    "end": "1465399"
  },
  {
    "text": "outcomes of that work so first of all we improved our security",
    "start": "1465399",
    "end": "1471440"
  },
  {
    "text": "posture which is very important for us so we have no friction when we need to",
    "start": "1471440",
    "end": "1478080"
  },
  {
    "text": "update uh the kubernetes version or the OS base image operating system version",
    "start": "1478080",
    "end": "1485360"
  },
  {
    "text": "as well as any of the internal cluster components it also gives us a really",
    "start": "1485360",
    "end": "1490440"
  },
  {
    "text": "nice safety net so when we do the red back deployment we always have a cluster",
    "start": "1490440",
    "end": "1495679"
  },
  {
    "text": "to fall back to and we always wait until the last point before we switch cluster traffic to the new cluster so for",
    "start": "1495679",
    "end": "1502279"
  },
  {
    "text": "example when we had the big outage this summer uh on Asia environments in our",
    "start": "1502279",
    "end": "1508080"
  },
  {
    "text": "case we just have a bunch of failure deployment failures but we had actually no impact on our live",
    "start": "1508080",
    "end": "1515320"
  },
  {
    "text": "traffic this also means yeah so with red back deploys we",
    "start": "1515320",
    "end": "1522000"
  },
  {
    "text": "are able to scale up and down our clusters from note types and counts to",
    "start": "1522000",
    "end": "1527559"
  },
  {
    "text": "service replica and so we have specific size defined for specific",
    "start": "1527559",
    "end": "1532720"
  },
  {
    "text": "customers having a low friction way to scale on not size means we are able to adopt a very wide range of cluster sizes",
    "start": "1532720",
    "end": "1540679"
  },
  {
    "text": "so we go all the way from two small um two small nodes spot instances to 250",
    "start": "1540679",
    "end": "1546919"
  },
  {
    "text": "nodes on our larger clusters the safety of the deploy strategy means we can iterate quickly on",
    "start": "1546919",
    "end": "1553799"
  },
  {
    "text": "our releases so we have an average 100 release per day and when there is an issue on the new",
    "start": "1553799",
    "end": "1560559"
  },
  {
    "text": "cluster during a deployment we have no incentive to fix it fast we can take our",
    "start": "1560559",
    "end": "1565880"
  },
  {
    "text": "time to properly troubled to this issue and in some scenarios we actually keep the fail cluster around for a long",
    "start": "1565880",
    "end": "1572600"
  },
  {
    "text": "period of time just to give us time to properly troubleshoot even like raise tickets with our Cloud",
    "start": "1572600",
    "end": "1580520"
  },
  {
    "text": "providers so um we Implement repack deployment and also we have going",
    "start": "1582760",
    "end": "1588520"
  },
  {
    "text": "deployment since day one right then the question come is that like do we need to use both the answer is yes but by",
    "start": "1588520",
    "end": "1595880"
  },
  {
    "text": "default we use rep deployment for all our customer environment um we still use roing",
    "start": "1595880",
    "end": "1601039"
  },
  {
    "text": "deployment in Ser situation such as the time sensitive H FES any minor microsurface patches or any like testing",
    "start": "1601039",
    "end": "1608840"
  },
  {
    "text": "environment for developer to do their microsurface testing so overall like um",
    "start": "1608840",
    "end": "1614760"
  },
  {
    "text": "the rule of farm is that like if the deployment doesn't involve any downtown we just use voting especially for local",
    "start": "1614760",
    "end": "1621880"
  },
  {
    "text": "environment otherwise we just use R back for our all our like customer",
    "start": "1621880",
    "end": "1628240"
  },
  {
    "text": "environment okay any",
    "start": "1628320",
    "end": "1632080"
  },
  {
    "text": "[Applause] question I think you uh partially",
    "start": "1637010",
    "end": "1643799"
  },
  {
    "text": "answered it in that last slide but I'm curious how the hundreds of deploys a",
    "start": "1643799",
    "end": "1649840"
  },
  {
    "text": "day lines up with the 2hour time to healthy and the multiple",
    "start": "1649840",
    "end": "1656039"
  },
  {
    "text": "overlapping clusters at a time so 100 deploys across all customer so one",
    "start": "1656039",
    "end": "1662840"
  },
  {
    "text": "customer do one deploy like at least once a week so we have like hundreds of",
    "start": "1662840",
    "end": "1668120"
  },
  {
    "text": "like environment right so like if you average it out like 100 of customer you",
    "start": "1668120",
    "end": "1674399"
  },
  {
    "text": "do like upgrade daily and then as I mention right we have the release channnel the release channel is a schedu",
    "start": "1674399",
    "end": "1681919"
  },
  {
    "text": "controll right which customer environment do like on Monday Tuesday something like that so add up is like",
    "start": "1681919",
    "end": "1687200"
  },
  {
    "text": "100 per 100 like per per day something like that so does it answer a question yeah it does thank you very",
    "start": "1687200",
    "end": "1693000"
  },
  {
    "text": "much yeah I was uh curious about the the stateless thing I mean you are in",
    "start": "1693000",
    "end": "1698640"
  },
  {
    "text": "control of all the application running on on the Clusters because you cannot really know if they have volumes for",
    "start": "1698640",
    "end": "1705159"
  },
  {
    "text": "example I can't hear you sorry I canar ah sorry so if the downstream cluster so the the",
    "start": "1705159",
    "end": "1711320"
  },
  {
    "text": "one that you are upgrading with this red black um approach have volumes this will",
    "start": "1711320",
    "end": "1716840"
  },
  {
    "text": "not work right you answer sorry I I didn't get the I",
    "start": "1716840",
    "end": "1722279"
  },
  {
    "text": "couldn't hear yeah yeah so the the fact that if you have a",
    "start": "1722279",
    "end": "1727880"
  },
  {
    "text": "Precision volumes in the cluster how do you manage that okay so yeah as Jeremy",
    "start": "1727880",
    "end": "1733080"
  },
  {
    "text": "mentioned our application is stess so like the only state for set is a tier",
    "start": "1733080",
    "end": "1738960"
  },
  {
    "text": "three application that one that consume the rapid mq which is also local but all the tier one t surface they all status",
    "start": "1738960",
    "end": "1745840"
  },
  {
    "text": "the requirement is that like they to be status so that like both customers and standing so for all the Legacy",
    "start": "1745840",
    "end": "1751080"
  },
  {
    "text": "application that that was Implement before R was involved they used they to",
    "start": "1751080",
    "end": "1756919"
  },
  {
    "text": "use the singon surface that means that they to scale down and then scale up across the Custer something like that so",
    "start": "1756919",
    "end": "1762679"
  },
  {
    "text": "yeah yeah but the requirement is that like like everything need to be St so yeah yeah okay thank you",
    "start": "1762679",
    "end": "1770039"
  },
  {
    "text": "um for your uh new deployments for onto the new clusters how do you deal with pod disruption budgets uh for certain",
    "start": "1771279",
    "end": "1777960"
  },
  {
    "text": "applications uh that come that that come with the our customized plugin so our",
    "start": "1777960",
    "end": "1783279"
  },
  {
    "text": "customiz plugin basically we call it application X all the micros servers",
    "start": "1783279",
    "end": "1788600"
  },
  {
    "text": "need to follow that blue pin to use that and then our customized plugin we",
    "start": "1788600",
    "end": "1794919"
  },
  {
    "text": "estimate like U the we have the pdb at minimum PVB that like um need to have",
    "start": "1794919",
    "end": "1801440"
  },
  {
    "text": "for different type of service something like that so they cannot like randomly craft a manifest and deoy to it we don't",
    "start": "1801440",
    "end": "1807320"
  },
  {
    "text": "allow it like we to have a something called uh we call application X service and then I we need to go for our",
    "start": "1807320",
    "end": "1813240"
  },
  {
    "text": "customiz ping to render it so yeah that's that's that's what that's what I call it like we do a fine control on",
    "start": "1813240",
    "end": "1819360"
  },
  {
    "text": "everything so that we have low down time so M so so you basically control how",
    "start": "1819360",
    "end": "1824600"
  },
  {
    "text": "apps are completely like there's very little room for the app developers to do kind",
    "start": "1824600",
    "end": "1830559"
  },
  {
    "text": "of what they want on this platform is that correct yeah like like um like to",
    "start": "1830559",
    "end": "1835640"
  },
  {
    "text": "see like what surface day one right and then like we to we to like consult with them that like if your surface is T1",
    "start": "1835640",
    "end": "1843039"
  },
  {
    "text": "then we to allocate more resources for that and we to alate more pdb for that otherwise if it's t free it it allow",
    "start": "1843039",
    "end": "1851039"
  },
  {
    "text": "then we can allow like you have low pdb but yeah but it depends on Surface by surface so okay thank you",
    "start": "1851039",
    "end": "1859679"
  },
  {
    "text": "so uh if I'm following your in your overall like",
    "start": "1860679",
    "end": "1866760"
  },
  {
    "text": "process um my question is around how your",
    "start": "1866760",
    "end": "1872320"
  },
  {
    "text": "terraform interacts with sequencing of the work so you're bringing up",
    "start": "1872320",
    "end": "1878279"
  },
  {
    "text": "kubernetes nodes they have to have privileges to get started you probably don't want them running with the same",
    "start": "1878279",
    "end": "1883880"
  },
  {
    "text": "privileges how do you basically protect your",
    "start": "1883880",
    "end": "1889519"
  },
  {
    "text": "uh your kubernetes stuff from or your overall infrastructure from a like things that",
    "start": "1889519",
    "end": "1898679"
  },
  {
    "text": "need to be done sequentially in terraform um I don't quite quite follow like uh what you mean by the by the",
    "start": "1898679",
    "end": "1906240"
  },
  {
    "text": "sequence and uh um what's the first word like the sequence so uh the idea is like",
    "start": "1906240",
    "end": "1913639"
  },
  {
    "text": "when you bring your cluster up it has to have extra permissions to get started and initialize and then you don't want",
    "start": "1913639",
    "end": "1920639"
  },
  {
    "text": "it necessarily running with those same rot level permissions after the fact how",
    "start": "1920639",
    "end": "1926679"
  },
  {
    "text": "do you deal with that with your infrastructure as code yeah so when when we generate when we create a kubernetes",
    "start": "1926679",
    "end": "1933679"
  },
  {
    "text": "Custer we have used the for example eks used the AWS off and that one will will",
    "start": "1933679",
    "end": "1940200"
  },
  {
    "text": "generate template and then install in the eks um custom so that one is",
    "start": "1940200",
    "end": "1945240"
  },
  {
    "text": "basically a blue pin that I determine I um what type what kind of level that they can assess for example our platform",
    "start": "1945240",
    "end": "1952000"
  },
  {
    "text": "team has admin assess for the eks Custer but for the developer they don't have it but the magic only happen um is in the",
    "start": "1952000",
    "end": "1960360"
  },
  {
    "text": "during the telephone phase we consume the data model and then we look at the state and generate the um ads off Tay",
    "start": "1960360",
    "end": "1967960"
  },
  {
    "text": "and then install to it then we can control the assess something like that okay thank you yeah",
    "start": "1967960",
    "end": "1973440"
  },
  {
    "text": "mhm are you concerned with the capacity limitations of cloud and environments",
    "start": "1973440",
    "end": "1978720"
  },
  {
    "text": "when sorry are you concerned with capacity in Cloud environments by doing a red black deployment especially for",
    "start": "1978720",
    "end": "1985200"
  },
  {
    "text": "some of your larger customers I could imagine you might have hundreds of nodes and sudden you need to duplicate the",
    "start": "1985200",
    "end": "1990760"
  },
  {
    "text": "number of nodes given your hpa's minimums are going up are you concerned or how do you mitigate the risk of a",
    "start": "1990760",
    "end": "1997320"
  },
  {
    "text": "cloud provider running out of capacity with this model youan do you mean I CH your your own answer do do you mean like",
    "start": "1997320",
    "end": "2004200"
  },
  {
    "text": "quas in what whatever uh nodes are available yeah ec2s if c6s run out and then you're",
    "start": "2004200",
    "end": "2011600"
  },
  {
    "text": "on different particular different types of Hardware are you concerned about or how do you mitigate those kind of",
    "start": "2011600",
    "end": "2017080"
  },
  {
    "text": "capacity constraints with this model so so sometimes we do have trenant issues",
    "start": "2017080",
    "end": "2023600"
  },
  {
    "text": "with a specific knob type not being not being available and in this case we will have just a deployment failure that we",
    "start": "2023600",
    "end": "2029519"
  },
  {
    "text": "can roll back and try again next week or next day yeah and and also like um in",
    "start": "2029519",
    "end": "2035919"
  },
  {
    "text": "terraform we can provide list of instant type that can support right like if",
    "start": "2035919",
    "end": "2041200"
  },
  {
    "text": "something is not available we use the back up and also there's an interesting um topic though like we recently used",
    "start": "2041200",
    "end": "2047080"
  },
  {
    "text": "the armm upgrade everything armm before we right we need to talk to a to see if",
    "start": "2047080",
    "end": "2052158"
  },
  {
    "text": "that region have enough armm capacity in order to roll out that region so we do",
    "start": "2052159",
    "end": "2057320"
  },
  {
    "text": "really fine control on like like for example like this customer is on us is one to then really to make sure that",
    "start": "2057320",
    "end": "2064560"
  },
  {
    "text": "they have enough armm instant then we can enable arm something like that so yeah yeah but we always have a backup in",
    "start": "2064560",
    "end": "2070000"
  },
  {
    "text": "the telephone to saying that armm not aaable use like AMD like for large for",
    "start": "2070000",
    "end": "2076358"
  },
  {
    "text": "that low group something like that so yeah perfect thank you I I had two questions that was my",
    "start": "2076359",
    "end": "2083079"
  },
  {
    "text": "first one so yeah shortcut so my second question is still on the same on the",
    "start": "2083079",
    "end": "2088480"
  },
  {
    "text": "same topic uh you said you do hundreds of deployments per day so do you do you",
    "start": "2088480",
    "end": "2094440"
  },
  {
    "text": "create some sort of a limits on how many deployments any given application can do",
    "start": "2094440",
    "end": "2100400"
  },
  {
    "text": "per day or uh do you create cues how do you manage that so um we have something",
    "start": "2100400",
    "end": "2105960"
  },
  {
    "text": "called deploy window like the deploy window is to um to restrain like for for",
    "start": "2105960",
    "end": "2112200"
  },
  {
    "text": "several hour right we can only deploy like 70 environment something like that",
    "start": "2112200",
    "end": "2117320"
  },
  {
    "text": "and also our controller has a uh Power TI Loop saying that like um uh we can",
    "start": "2117320",
    "end": "2122839"
  },
  {
    "text": "only deploy 30 of them like and then if we finish all 30 of them then we can do next 30 something like that the reason",
    "start": "2122839",
    "end": "2128920"
  },
  {
    "text": "is that like uh we use Aro worko and Aro CD and we don't want to stretch Aro CD",
    "start": "2128920",
    "end": "2134200"
  },
  {
    "text": "so we want to like and you can see like we have 2 million kubernetes resoures right if you stretch a city a city will",
    "start": "2134200",
    "end": "2140240"
  },
  {
    "text": "crash so you want to like like do a Power TI Loop and make sure that all three 30 of them install St and then go",
    "start": "2140240",
    "end": "2146520"
  },
  {
    "text": "through the next 30 something like that so that means when in your first slide when you said you had a controller a",
    "start": "2146520",
    "end": "2152280"
  },
  {
    "text": "controller cluster you only have one controller cluster for all of that yeah that's Brave thanks",
    "start": "2152280",
    "end": "2158200"
  },
  {
    "text": "yeah yeah yeah we we we yeah we are very good programmer and it optim optimize everything so",
    "start": "2158200",
    "end": "2165318"
  },
  {
    "text": "yeah uh how do you handle switching over like background workers and Kafka",
    "start": "2165920",
    "end": "2171640"
  },
  {
    "text": "consumers and non essentially non HTP workloads I I can't hear how how do you",
    "start": "2171640",
    "end": "2178480"
  },
  {
    "text": "handle switching like the switch over between the red and black clusters for non HTTP resour uh workloads like Kafka",
    "start": "2178480",
    "end": "2186240"
  },
  {
    "text": "consumers Etc so the we only switch the edge right the edge is always",
    "start": "2186240",
    "end": "2192400"
  },
  {
    "text": "HTTP so we we don't have like GPC do we don't have drpc like we don't have it",
    "start": "2192400",
    "end": "2198119"
  },
  {
    "text": "like just yeah I but I mean like background workers that don't get like",
    "start": "2198119",
    "end": "2203280"
  },
  {
    "text": "incoming requests that are like pulling from a queue or consuming from Kafka um sing yeah Singleton right like",
    "start": "2203280",
    "end": "2211079"
  },
  {
    "text": "we have a Singleton surface that like for any tier free surface like we have Singleton to make sure that like the",
    "start": "2211079",
    "end": "2217599"
  },
  {
    "text": "only the old Custer is one thing or the little Custer is one thing something like that so okay yeah M thank you yeah",
    "start": "2217599",
    "end": "2222960"
  },
  {
    "text": "yeah yeah uh you mentioned your release manifest includes uh versions of",
    "start": "2222960",
    "end": "2228640"
  },
  {
    "text": "microservices but you might also do rolling deployments of microservices how do you make sure the deployments kind of",
    "start": "2228640",
    "end": "2235240"
  },
  {
    "text": "stay in sync and and use the are always deploying the latest version of every service eyes that oh can you repeat it",
    "start": "2235240",
    "end": "2243839"
  },
  {
    "text": "yeah sorry um so if if you release manifest has has uh specific versions of",
    "start": "2243839",
    "end": "2249560"
  },
  {
    "text": "each microservice but you might also do a rolling deployment of a microservice how do you make sure that whatever is",
    "start": "2249560",
    "end": "2256200"
  },
  {
    "text": "released is always the latest version uh so we have a release part L",
    "start": "2256200",
    "end": "2261839"
  },
  {
    "text": "called CSP and then the CSU will just like scan through all the micros Services rep and then grab the new",
    "start": "2261839",
    "end": "2267560"
  },
  {
    "text": "version and we will update the release manifest Yeah so basically the release manifest is a monoo that describe all",
    "start": "2267560",
    "end": "2274280"
  },
  {
    "text": "the micros servers that on to different rle and then we have a automation call CSP and then it will scan like if",
    "start": "2274280",
    "end": "2281160"
  },
  {
    "text": "surface micros surface one make a new release then the robot right geub we",
    "start": "2281160",
    "end": "2286880"
  },
  {
    "text": "have a robot to talk to geub and update the release man and track it in automatically so so so yeah so this",
    "start": "2286880",
    "end": "2293599"
  },
  {
    "text": "process we make sure that light yeah we always have up to dat um release version so mhm so will that always create a new",
    "start": "2293599",
    "end": "2301119"
  },
  {
    "text": "release manifest uh when you start a rolling deployment or at the end of a rolling deployment I don't get what you",
    "start": "2301119",
    "end": "2307200"
  },
  {
    "text": "mean uh is that about uh GitHub so we don't really like we don't",
    "start": "2307200",
    "end": "2313720"
  },
  {
    "text": "really update our environments when we get new commits in git so we follow",
    "start": "2313720",
    "end": "2319280"
  },
  {
    "text": "GitHub in the sense that we will use a GitHub relas tag as a version that we",
    "start": "2319280",
    "end": "2325040"
  },
  {
    "text": "deploy in our clusters but we will not uh redeploy a version if a new service",
    "start": "2325040",
    "end": "2330400"
  },
  {
    "text": "is updated and so some customers who are more Rovers",
    "start": "2330400",
    "end": "2335920"
  },
  {
    "text": "will want an update uh that that is that has soaked for three weeks across other environment and for like our public",
    "start": "2335920",
    "end": "2342920"
  },
  {
    "text": "Cloud environment we will update it multiple times a day as new service versions comes up okay thank",
    "start": "2342920",
    "end": "2351078"
  },
  {
    "text": "you uh yeah hi thanks for the talk uh two questions so you talked about black",
    "start": "2353119",
    "end": "2359920"
  },
  {
    "text": "uh black red deployment is it different from uh more used like blue green",
    "start": "2359920",
    "end": "2368440"
  },
  {
    "text": "is it the same uh what what the like uh is it same as blue green deployment or I",
    "start": "2368440",
    "end": "2375359"
  },
  {
    "text": "never heard of that like you talked about red black uh uh depends like how",
    "start": "2375359",
    "end": "2381640"
  },
  {
    "text": "you describe what is it's just a name guess we call BL green as rolling it's just a name right yeah we call BL green",
    "start": "2381640",
    "end": "2388200"
  },
  {
    "text": "as rolling but some people call BL green as red that's what I thought it's same pretty much yeah yeah second question is",
    "start": "2388200",
    "end": "2395000"
  },
  {
    "text": "I think more detailed uh what I understand you are using the same process for even cluster upgrades like",
    "start": "2395000",
    "end": "2401359"
  },
  {
    "text": "you're going from 130 to 131 you will use the same process right yeah okay",
    "start": "2401359",
    "end": "2406640"
  },
  {
    "text": "exactly right yeah and uh since it's a like big change could be application",
    "start": "2406640",
    "end": "2412800"
  },
  {
    "text": "could be the cluster do you test uh following the same process in your like",
    "start": "2412800",
    "end": "2418359"
  },
  {
    "text": "uat environment or you follow some different process we do the same process like yeah whenever upgrade we just like",
    "start": "2418359",
    "end": "2424800"
  },
  {
    "text": "do the red back one see some test make make sure the real CER is working then we cut over yeah it's always the same we",
    "start": "2424800",
    "end": "2431880"
  },
  {
    "text": "have low special in the test environment you use the same process yeah yeah yeah yeah mhm black red okay thanks",
    "start": "2431880",
    "end": "2440400"
  },
  {
    "text": "yep yeah so my question was um what do you do for DNS like for an Engish",
    "start": "2442599",
    "end": "2449359"
  },
  {
    "text": "resource on the cluster like how do you handle those the DNS on what sorry like",
    "start": "2449359",
    "end": "2455480"
  },
  {
    "text": "if I have an Ingress resour K8 how do you migrate it over to this other cluster the inrease resource on the old",
    "start": "2455480",
    "end": "2462440"
  },
  {
    "text": "cluster how do I migrate to the new cluster is that what you mean right yeah if you're duplicating the whole cluster",
    "start": "2462440",
    "end": "2467960"
  },
  {
    "text": "and I have an iners resource like what do you do to actually so they are two different NLB so the old Custer spin up a Ingress",
    "start": "2467960",
    "end": "2477240"
  },
  {
    "text": "with one NLB and then the little Custer spin n Ingress if NR NLB so the DNS is",
    "start": "2477240",
    "end": "2483000"
  },
  {
    "text": "pointing to the NLB it's not pointing to the Ingress so like you don't use external DNS or anything like that like",
    "start": "2483000",
    "end": "2488480"
  },
  {
    "text": "like what do you do for the actual like records in DNS yeah yeah the records in DNS yeah M so those are manual you do",
    "start": "2488480",
    "end": "2495440"
  },
  {
    "text": "you manually create those like no no it's like automation like you just use API to so so the inest controller in",
    "start": "2495440",
    "end": "2503960"
  },
  {
    "text": "kubernetes I mean the external DNS can program the the the record right to the",
    "start": "2503960",
    "end": "2509599"
  },
  {
    "text": "DNS uh provider but then after that we can have we have the API to update that",
    "start": "2509599",
    "end": "2516119"
  },
  {
    "text": "to update that record to pointing to the to the um uh to the little Custer yeah there are a couple layer like I can't",
    "start": "2516119",
    "end": "2522800"
  },
  {
    "text": "tell you that like what layer it is but yeah but the the top layer that touch the edge we we manage it we don't use",
    "start": "2522800",
    "end": "2529200"
  },
  {
    "text": "the external and manage but but below that right like we have two exactly same",
    "start": "2529200",
    "end": "2534800"
  },
  {
    "text": "kuber along with the NLB that pointing to the edge so yeah mhm okay it's like",
    "start": "2534800",
    "end": "2540280"
  },
  {
    "text": "it's like abstracted out that you you don't actually like you don't actually handle the records and Stu like it' be",
    "start": "2540280",
    "end": "2546400"
  },
  {
    "text": "higher up kind of thing yeah mhm okay yeah mhm",
    "start": "2546400",
    "end": "2551440"
  },
  {
    "text": "yeah okay thank you very much thank",
    "start": "2553079",
    "end": "2559000"
  }
]