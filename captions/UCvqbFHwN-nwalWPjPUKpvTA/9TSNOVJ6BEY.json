[
  {
    "text": "hi everyone my name is Rachel I am from cash app uh and together with my",
    "start": "80",
    "end": "5359"
  },
  {
    "text": "colleagues we own operate and maintain our kubernetes Fleet for cash app services today I'm going to be talking",
    "start": "5359",
    "end": "11840"
  },
  {
    "text": "to you about some work we've done over the past year to delve into the multicluster ecosystem with our",
    "start": "11840",
    "end": "17199"
  },
  {
    "text": "kubernetes setup um so just to set the ground a little before multicluster we had about",
    "start": "17199",
    "end": "22600"
  },
  {
    "text": "six clusters that we were operating um three of these being actual services so we have a prod development and staging",
    "start": "22600",
    "end": "29679"
  },
  {
    "text": "cluster and three maintenance utility clusters uh we had about 5,000 nodes we had 500",
    "start": "29679",
    "end": "36440"
  },
  {
    "text": "services on these clusters and um a pretty high number of cores RAM and dis",
    "start": "36440",
    "end": "42800"
  },
  {
    "text": "usage after multicluster we can see that same number of services double number of",
    "start": "42800",
    "end": "48879"
  },
  {
    "text": "clusters but relatively comparable cores RAM and dis usage and we've actually",
    "start": "48879",
    "end": "54160"
  },
  {
    "text": "seen a decrease in node usage thanks to some work we've done around our Carpenter setup",
    "start": "54160",
    "end": "60440"
  },
  {
    "text": "so why would you want to use multicluster when you're you when you're running kuiz uh clusters um the big part",
    "start": "60440",
    "end": "67880"
  },
  {
    "text": "is removing a single point of failure and improving platform resiliency the case study that we have is when you're",
    "start": "67880",
    "end": "74400"
  },
  {
    "text": "doing an in place upgrade or just a cluster fails and you only have one cluster tied to your environment you",
    "start": "74400",
    "end": "79680"
  },
  {
    "text": "lose that environment until you spin up another cluster or restore your cluster back to normal um so part of this is",
    "start": "79680",
    "end": "86600"
  },
  {
    "text": "building into multicluster spreading out multiple clusters per environment so that we can fail over more easily",
    "start": "86600",
    "end": "92960"
  },
  {
    "text": "whenever we need it um and so part of this extension into what we're looking",
    "start": "92960",
    "end": "98000"
  },
  {
    "text": "in 2025 and Beyond is looking into new operational opportunities to simplify adding more clusters to our environment",
    "start": "98000",
    "end": "104320"
  },
  {
    "text": "and making it easier for us to spin them up tear them down and migrate Services back and forth between them uh the last",
    "start": "104320",
    "end": "110399"
  },
  {
    "text": "part is reducing per cluster resource usage uh part of R case study from last year is when we upgraded up uh our",
    "start": "110399",
    "end": "117320"
  },
  {
    "text": "kubernetes version we saw that there were just too many API server calls because we have too many services",
    "start": "117320",
    "end": "123079"
  },
  {
    "text": "running the same cluster fully scaled up and the API server couldn't keep up with it until we over provisioned",
    "start": "123079",
    "end": "129200"
  },
  {
    "text": "it however even if you want to move to multicluster there are some challenges along the way uh first thing you have to",
    "start": "129200",
    "end": "135319"
  },
  {
    "text": "consider is resources for services things like Secrets Etc config maps how do you keep them synced across clusters",
    "start": "135319",
    "end": "142000"
  },
  {
    "text": "in the same state to make sure you're serving traffic reliably how do you make sure that as you're moving services",
    "start": "142000",
    "end": "147360"
  },
  {
    "text": "between clusters that you are making sure that you're doing in an incremental fashion that lets you roll back easily",
    "start": "147360",
    "end": "152840"
  },
  {
    "text": "send 1% of traffic at a time and not disrupt any customer flows",
    "start": "152840",
    "end": "157879"
  },
  {
    "text": "Downstream the next part is how do you figure out how to operate double the number of clusters or more without",
    "start": "157879",
    "end": "163560"
  },
  {
    "text": "doubling your efforts as a compute team or a team that's owning your platform and so the final part that I want to",
    "start": "163560",
    "end": "168920"
  },
  {
    "text": "touch on here and challenges part is making this transparent to service owners without requiring them to learn",
    "start": "168920",
    "end": "174239"
  },
  {
    "text": "about kubernetes one thing we have at Cash app is a uh UI that service owners",
    "start": "174239",
    "end": "179840"
  },
  {
    "text": "can reach out to when they want to deploy their service see their deploy history uh update Auto scaling resource",
    "start": "179840",
    "end": "185400"
  },
  {
    "text": "config Etc and one thing we did when we were moving to multicluster was we upgraded this view to now show a pod",
    "start": "185400",
    "end": "191000"
  },
  {
    "text": "skew across clusters so if that you you as a service owner received an alert that oh one of my pods is unhealthy you",
    "start": "191000",
    "end": "197080"
  },
  {
    "text": "can narrow down which po it was which cluster it was when you open up your support ticket to receive assistance on",
    "start": "197080",
    "end": "202400"
  },
  {
    "text": "the issue and so when you're moving the multicluster there's a lot of different ways to do it and it really depends on",
    "start": "202400",
    "end": "208879"
  },
  {
    "text": "your organization for which which way it works best for you the most straightforward approach might be to",
    "start": "208879",
    "end": "213920"
  },
  {
    "text": "raise for tiered clusters where you segment your services based on tiers whether you have your highly critical tier one services that can go down at",
    "start": "213920",
    "end": "220400"
  },
  {
    "text": "all or disrupts customers usage you have your tier three services which aren't",
    "start": "220400",
    "end": "225959"
  },
  {
    "text": "super critical they could go down for a day and be recovered and there's probably some ssq back backing them that",
    "start": "225959",
    "end": "231319"
  },
  {
    "text": "will restore their states and then you have the majority of services in the middle um that can maybe go down for an",
    "start": "231319",
    "end": "237200"
  },
  {
    "text": "hour or more and they won't impact things too too much the issue with this though is that if you have cross cluster",
    "start": "237200",
    "end": "243840"
  },
  {
    "text": "dependencies now if a highly critical service depends on a tier 2 service and you lose your tier 2 cluster you've just",
    "start": "243840",
    "end": "250239"
  },
  {
    "text": "you've just coupled all your clusters together again and created your same problem so the next thing we looked into",
    "start": "250239",
    "end": "256040"
  },
  {
    "text": "is a clusters where we are an AWS shop at Cash app and so you're looking into",
    "start": "256040",
    "end": "262600"
  },
  {
    "text": "per region per availability Zone spin up a cluster and have replicas of your services in those clusters serving",
    "start": "262600",
    "end": "269120"
  },
  {
    "text": "traffic out of them this is great this is something we want to move towards in the future but for getting started with multicluster this",
    "start": "269120",
    "end": "275000"
  },
  {
    "text": "is a big lift for every region doing at least three or more clusters that's just a lot of work to spin up and provision",
    "start": "275000",
    "end": "280840"
  },
  {
    "text": "those and migrate across them so what we settled on for this past year was clone",
    "start": "280840",
    "end": "286000"
  },
  {
    "text": "clusters where we take a one toone copy of our cluster State provision a new cluster with that state and migrate",
    "start": "286000",
    "end": "291759"
  },
  {
    "text": "Services over to them and the goal with this is one so that if we had if we can have two clusters running at 100% at any",
    "start": "291759",
    "end": "298880"
  },
  {
    "text": "point along the migration path if things fail we have a very easy way to go back to our original state without disrupting",
    "start": "298880",
    "end": "304639"
  },
  {
    "text": "the business and the other part is that this gets us into the process of thinking of how do we provision more",
    "start": "304639",
    "end": "310000"
  },
  {
    "text": "clusters how do we think about the processes that go into that the components that go into that and how do we make it easier for us as a platform",
    "start": "310000",
    "end": "316639"
  },
  {
    "text": "team to continue operating at the scale that we do and so this is a brief overview of",
    "start": "316639",
    "end": "322800"
  },
  {
    "text": "like the core Cloud Technologies we leverage at Cash app um and because a lot of the way we provision is through",
    "start": "322800",
    "end": "328960"
  },
  {
    "text": "things like Teri form wrapped in heror Grunt and we're using Atlantis to manage those um and so we run on eks WE manage",
    "start": "328960",
    "end": "336759"
  },
  {
    "text": "our node groups with Carpenter and we do HPA with Kus so we get custom metrics",
    "start": "336759",
    "end": "342199"
  },
  {
    "text": "and all of these are wrapped into IAC components and modules that we can solidify as a process in a runbook so",
    "start": "342199",
    "end": "348240"
  },
  {
    "text": "any platform engineer can pick up this process provision a new cluster and start migrating Services into it which",
    "start": "348240",
    "end": "354039"
  },
  {
    "text": "is really good when you're working on a large team and you want things to be reproducible um the next part that's",
    "start": "354039",
    "end": "359919"
  },
  {
    "text": "really important when you're starting this process out that we learned was having good coordination like spreadsheets and so this involves taking",
    "start": "359919",
    "end": "366599"
  },
  {
    "text": "a snapshot in time of all the services that you operate in your clusters and sort of breaking them up tracking which",
    "start": "366599",
    "end": "372639"
  },
  {
    "text": "tiers they are tracking which weird components some services staff that others don't that you need to be aware",
    "start": "372639",
    "end": "378840"
  },
  {
    "text": "of when you're migrating them and reaching out to service teams and having their like slack Channel or however you",
    "start": "378840",
    "end": "384280"
  },
  {
    "text": "communicate in that sheet so if something does come up or if you need to communicate that hey I'm going to migrate services one two and three on",
    "start": "384280",
    "end": "390400"
  },
  {
    "text": "this day you can reach out to those service owner teams and make sure you've sort of acquired the lock on that service for the day so that they're not",
    "start": "390400",
    "end": "396520"
  },
  {
    "text": "updating things in the background and disrupting things um the last key component that I'll be catching on more",
    "start": "396520",
    "end": "402319"
  },
  {
    "text": "is defining a cluster State uh what we really want to think about here is the life cycle of a cluster we're not just",
    "start": "402319",
    "end": "407680"
  },
  {
    "text": "introducing new clusters but we want to also PVE a path towards the cycling of clusters and the sort of distribution of",
    "start": "407680",
    "end": "413759"
  },
  {
    "text": "clusters for different workloads in the future and so we've defined this in four main states the init state is when you",
    "start": "413759",
    "end": "420520"
  },
  {
    "text": "have just first set up all of your clusters components um whether that is like your eks module whether that is a",
    "start": "420520",
    "end": "426440"
  },
  {
    "text": "few initial Argo CD components things like that the Gateway state is when you're thinking about per service",
    "start": "426440",
    "end": "432639"
  },
  {
    "text": "ingresses when you've started to prepare your cluster to be ready to start migrating Services into it so this may",
    "start": "432639",
    "end": "438759"
  },
  {
    "text": "involve things like backfilling owner and deployer roles Secrets config Maps",
    "start": "438759",
    "end": "443960"
  },
  {
    "text": "things at the cluster level that are getting prepped for per service needs and then we have this partial State and",
    "start": "443960",
    "end": "449120"
  },
  {
    "text": "this is where our clusters tend to live for most of the time that we're in a migration uh this partial State defines",
    "start": "449120",
    "end": "454720"
  },
  {
    "text": "it so that we have some clusters some services in the cluster that are ready and healthy and receiving traffic as we",
    "start": "454720",
    "end": "460639"
  },
  {
    "text": "slowly iterate through our um wealth of services uh but we're not at 100% yet",
    "start": "460639",
    "end": "466000"
  },
  {
    "text": "and so once we finish our migration whenever we repeat this flow per environment per cluster Etc then we move",
    "start": "466000",
    "end": "471280"
  },
  {
    "text": "to a steady state we Market as ready and so now when you're building out any",
    "start": "471280",
    "end": "476479"
  },
  {
    "text": "tooling that needs to help Services service owners provision new services update their services they can read",
    "start": "476479",
    "end": "482960"
  },
  {
    "text": "against the state and they can make sure that these changes are propagated correctly to the right clusters um",
    "start": "482960",
    "end": "489360"
  },
  {
    "text": "instead of getting ahead of yourself if say you're in a partial State and we service X hasn't been migrated yet we",
    "start": "489360",
    "end": "494400"
  },
  {
    "text": "want to make sure that changes to that don't suddenly cause it to become active in a cluster where not all the resources for it are present",
    "start": "494400",
    "end": "501520"
  },
  {
    "text": "yet and so after you've sort of figured out the process you've thought through your architecture you've thought through",
    "start": "501520",
    "end": "507120"
  },
  {
    "text": "how you provision new clusters now you need get your migration tooling right um and so some of the things that worked",
    "start": "507120",
    "end": "513039"
  },
  {
    "text": "relable for us was batch scripts uh at Cash app we have an internal tool that",
    "start": "513039",
    "end": "519279"
  },
  {
    "text": "just like the UI has a CLI offering that does a lot of the same things um and so",
    "start": "519279",
    "end": "524399"
  },
  {
    "text": "for us it was really straightforward for us to write some go and Bash scripts to wrap that up and make sure that we could",
    "start": "524399",
    "end": "529640"
  },
  {
    "text": "do this efficiently for a number of services instead of going through one by one on a suite of 500 of them",
    "start": "529640",
    "end": "536480"
  },
  {
    "text": "um and the next part like I just mentioned with the these cluster States we want to track these in a global",
    "start": "536480",
    "end": "542360"
  },
  {
    "text": "cluster topology file and so we've Define this sort of yaml object uh that we syn in S3 that can be read by all of",
    "start": "542360",
    "end": "548720"
  },
  {
    "text": "our other tooling that tell us at any given point what are all the Clusters that live in a single environment and",
    "start": "548720",
    "end": "554320"
  },
  {
    "text": "within those clusters in the environment um how are they like like what is their current state are they steady are they",
    "start": "554320",
    "end": "561000"
  },
  {
    "text": "partial how much traffic should they be receiving compared to other clusters in the environment um and it's just really",
    "start": "561000",
    "end": "566600"
  },
  {
    "text": "nice way to track that state the next part is our original clusters were set up to",
    "start": "566600",
    "end": "572240"
  },
  {
    "text": "have namespace scoped resources and so you had service a and service A's config Maps Secrets all of that other stuff",
    "start": "572240",
    "end": "579839"
  },
  {
    "text": "would be in the same namespace and so this made it really easy when we were wrapping q q commands uh because you could sort of just pass in a list of",
    "start": "579839",
    "end": "586519"
  },
  {
    "text": "service names and know that that would also correspond to the name space you wanted when you were running cctl get",
    "start": "586519",
    "end": "591600"
  },
  {
    "text": "this resource and apply it to another cluster the other part is updating your observability tools we use data dog a",
    "start": "591600",
    "end": "598200"
  },
  {
    "text": "lot but a lot of this was making sure that any like saved views we had for deployment Health traffic Health Etc",
    "start": "598200",
    "end": "604560"
  },
  {
    "text": "were updated ahead of time to account for these new clusters so that as we're in the migration process we're not",
    "start": "604560",
    "end": "609720"
  },
  {
    "text": "having to consider that we can just turn on new views and make sure that as new services enter the new cluster they're",
    "start": "609720",
    "end": "615640"
  },
  {
    "text": "coming up healthily we see any error logs metrics Etc and the final part uh",
    "start": "615640",
    "end": "620839"
  },
  {
    "text": "which sort of tackles the challenge of how do you keep secrets in sync across the closer was for us to move away from",
    "start": "620839",
    "end": "626600"
  },
  {
    "text": "our old AWS CSI Secrets driver into external Secrets operator which was",
    "start": "626600",
    "end": "631720"
  },
  {
    "text": "really nice for us it made things really simple to have secrets moved out of just being directly volume mounted by the",
    "start": "631720",
    "end": "637320"
  },
  {
    "text": "operator and just having everything being in uh namespace scoped in Secrets manager for us to do a grip over",
    "start": "637320",
    "end": "643880"
  },
  {
    "text": "everything and then also have all of our tooling write to and update and read from a single",
    "start": "643880",
    "end": "650279"
  },
  {
    "text": "store and so like I mentioned earlier we do sto stuff uh we want to be able to move our services over incrementally we",
    "start": "650279",
    "end": "658079"
  },
  {
    "text": "want to be able to take critical service a turn it to 1% on a new cluster and make sure that any errors or any",
    "start": "658079",
    "end": "665160"
  },
  {
    "text": "requests that are coming in are in the exact sort of expected pattern as we would see in the Legacy cluster to make",
    "start": "665160",
    "end": "670800"
  },
  {
    "text": "sure we're not introducing any new regressions as we move Services over um and so part of this is so really the",
    "start": "670800",
    "end": "677560"
  },
  {
    "text": "magic of the virtual service where we can take all of our clusters both east and west and represent them as hosts and",
    "start": "677560",
    "end": "684240"
  },
  {
    "text": "routes in our virtual service resource on a per service basis and so basically",
    "start": "684240",
    "end": "689839"
  },
  {
    "text": "this is just tracking to make sure that for a given service it should receive 100% of the traffic but as you're going",
    "start": "689839",
    "end": "695120"
  },
  {
    "text": "through this flow you can distribute that 100% through these different routes and that's giving you your cross cluster",
    "start": "695120",
    "end": "701440"
  },
  {
    "text": "load balancing if you were so at the end of the state um you'll see that like for",
    "start": "701440",
    "end": "706839"
  },
  {
    "text": "all the services that are coming into that local cluster this is taken from one of the new clusters um that that is",
    "start": "706839",
    "end": "714800"
  },
  {
    "text": "now receiving 100% of traffic that's coming in from an A a caller higher up in the stack and so this really",
    "start": "714800",
    "end": "721200"
  },
  {
    "text": "simplifies how we move traffic back and forth and along with that ISO is really great for Telemetry uh this dashboard at",
    "start": "721200",
    "end": "727800"
  },
  {
    "text": "the top is just a fraction of the metrics we get from ISO and how we're able to track the health of a service in",
    "start": "727800",
    "end": "734040"
  },
  {
    "text": "in the traffic sense as we move them over of seeing all the inbound errors all of the outbound errors and thinking",
    "start": "734040",
    "end": "739639"
  },
  {
    "text": "through questions that we might give from a service owner as we're Midway through the migration of oh I see that",
    "start": "739639",
    "end": "744880"
  },
  {
    "text": "my service has been moved but why is it not a 50/50% split we can see that in the inbound traffic that there are just",
    "start": "744880",
    "end": "750920"
  },
  {
    "text": "some cers further Upstream that only exist in the ly cluster still because we're still migrating so you won't see",
    "start": "750920",
    "end": "756519"
  },
  {
    "text": "that 50/50% split until we finish moving everything and the full call Path down can stay within either the new cluster",
    "start": "756519",
    "end": "763480"
  },
  {
    "text": "or the old cluster and a little bit more breakdown on our tiers um I think this is just a",
    "start": "763480",
    "end": "770880"
  },
  {
    "text": "really cool way to set yourself up for Success when you're thinking about any large scale migrations you want to do as",
    "start": "770880",
    "end": "775959"
  },
  {
    "text": "an organization is really segmenting services in the tiers and not telling",
    "start": "775959",
    "end": "781519"
  },
  {
    "text": "people what tiers they should be but giving them a guideline so that they can self- elect what tier their services",
    "start": "781519",
    "end": "786760"
  },
  {
    "text": "belong into based on some set criteria so some of the big key uh factors in this for us are the impacts of sevs on",
    "start": "786760",
    "end": "794240"
  },
  {
    "text": "the business uh how badly does it crash everything else if service a goes down",
    "start": "794240",
    "end": "799279"
  },
  {
    "text": "and the numbers of of nines of availability that a service must adhere to the serice use case um so as you have",
    "start": "799279",
    "end": "807000"
  },
  {
    "text": "Ser service owners do this you'll tend to see a pretty even bell curve desription bell curve distribution where",
    "start": "807000",
    "end": "813320"
  },
  {
    "text": "your highly critical services are going to be like yes absolutely we are tier zero Services you have services that are",
    "start": "813320",
    "end": "818560"
  },
  {
    "text": "like hey we could go down for a week we don't really care we're background workers jobs Etc we can go into tier",
    "start": "818560",
    "end": "823720"
  },
  {
    "text": "three and then your defaults are going to be a lot of tier twos and tier ones as people who have pled like nebulas in",
    "start": "823720",
    "end": "829000"
  },
  {
    "text": "that space and could use some further distinction but also this gives you a lot of padding in your migration work as",
    "start": "829000",
    "end": "834800"
  },
  {
    "text": "well because you can start with your tier three services that you know aren't going to be critical aren't going to be breaking any anything if a migration for",
    "start": "834800",
    "end": "840639"
  },
  {
    "text": "a single service goes wrong and you can use them as your as your test bed in each environment to move them over the line and this guides you all the way up",
    "start": "840639",
    "end": "847720"
  },
  {
    "text": "so that by the time you're touching these tier zero services that we know are highly critical you've refined your pattern you know the process you know",
    "start": "847720",
    "end": "854120"
  },
  {
    "text": "the success path and so now we get into the actual migration and on the right here I have",
    "start": "854120",
    "end": "861000"
  },
  {
    "text": "this diagram that really illustrates the life cycle of a cluster at Cash app now where we're going to provision a new",
    "start": "861000",
    "end": "866680"
  },
  {
    "text": "cluster we're going to backfill all those resources we're going to make sure that we can scale the deployment first",
    "start": "866680",
    "end": "872040"
  },
  {
    "text": "no traffic just make sure it's healthy all of the resources it needs to run are present we're not getting in event",
    "start": "872040",
    "end": "877079"
  },
  {
    "text": "errors and then we actually think about the traffic part and so part of this to make this part easy both at the service",
    "start": "877079",
    "end": "883839"
  },
  {
    "text": "scaling level and also the traffic level is thinking about guard rails how can we automate our clusters to do the work for",
    "start": "883839",
    "end": "889920"
  },
  {
    "text": "us so that we don't have to do the heavy lifting on every service three things that we found really helpful on this",
    "start": "889920",
    "end": "895360"
  },
  {
    "text": "front are ketta and ketta is just an ex really enhanced version of HPA where we",
    "start": "895360",
    "end": "901440"
  },
  {
    "text": "can go in and just based off an annotation on the namespace object say that when you back fill a service for",
    "start": "901440",
    "end": "906880"
  },
  {
    "text": "the first time set its replicas to zero until we manually say that we want to scale that individual's service up and",
    "start": "906880",
    "end": "912440"
  },
  {
    "text": "test to see if it's ready or not the next part is Cooper healthy Cooper healthy allows you to define checks",
    "start": "912440",
    "end": "918000"
  },
  {
    "text": "within your uh cluster and within your infrastructure to Define an expected State and so one thing we used this here",
    "start": "918000",
    "end": "924759"
  },
  {
    "text": "for was the traffic portion of our migration where we had a cuber healthy job continually pinging all the services",
    "start": "924759",
    "end": "931480"
  },
  {
    "text": "in the cluster and so we could go into Data dog and sort of evaluate that state whether it's healthy or not to know if",
    "start": "931480",
    "end": "937360"
  },
  {
    "text": "it's ready to start receiving real traffic and we're ready to start shifting that over and the last one's cerno um cerno is just a policy engine",
    "start": "937360",
    "end": "945360"
  },
  {
    "text": "similar to gatekeeper where you define sort of guard rails in your system based on policies and they prohibit actions",
    "start": "945360",
    "end": "952560"
  },
  {
    "text": "being taken in the cluster that you don't want people to take um and then of course we test them with chainsaw in the",
    "start": "952560",
    "end": "957880"
  },
  {
    "text": "back and then the big part around this zero downtime and making sure that we are true to that",
    "start": "957880",
    "end": "963920"
  },
  {
    "text": "is making sure that we are set up for Success on our observability side and so this is making sure you have notebooks",
    "start": "963920",
    "end": "969440"
  },
  {
    "text": "dashboards Etc that track deployment errors route Health request rates and any other tertiary error SES error",
    "start": "969440",
    "end": "977199"
  },
  {
    "text": "metrics that you would want to be associated with any of those to make sure that when your migrating servic is over in batches you have a full snapshot",
    "start": "977199",
    "end": "983399"
  },
  {
    "text": "view of that entire batch of services that they moved over healthfully that services that traffic is being sent in",
    "start": "983399",
    "end": "989040"
  },
  {
    "text": "and you're not just depending on Cube logs to be the only source of Truth here and so even though we addressed a",
    "start": "989040",
    "end": "996000"
  },
  {
    "text": "lot of the challenges we saw from the get-go there were definitely still paint points in our journey uh the first one",
    "start": "996000",
    "end": "1002079"
  },
  {
    "text": "being backfill inconsistencies um our backfill tooling works pretty great for about 90% of the time but there are are",
    "start": "1002079",
    "end": "1008360"
  },
  {
    "text": "occasional instances where you might have a secret placeholder that doesn't get moved or an AWS secret that doesn't",
    "start": "1008360",
    "end": "1013800"
  },
  {
    "text": "get replicated correctly so it's really just understanding what to do in those situations documenting them the process",
    "start": "1013800",
    "end": "1019519"
  },
  {
    "text": "so that as different people pick up this migration work they can go touch on a run book touch on an FAQ and know which",
    "start": "1019519",
    "end": "1025558"
  },
  {
    "text": "secondary script we have in place to manually fix that um and then the next",
    "start": "1025559",
    "end": "1031000"
  },
  {
    "text": "part is just making sure you're keeping track of the cluster scope and there's really tool schools the thought that I've had throughout this process where",
    "start": "1031000",
    "end": "1037760"
  },
  {
    "text": "services will continue to be created after you've started your migration people will continue to modify their",
    "start": "1037760",
    "end": "1043360"
  },
  {
    "text": "services people continue to provision new ones and even though you created a snapshot at the start of your migration",
    "start": "1043360",
    "end": "1049039"
  },
  {
    "text": "by the time you've ended there might be another 75 plus 75 services that you had no idea existed until that point in time",
    "start": "1049039",
    "end": "1055440"
  },
  {
    "text": "and so you could take two schools of thought one being as new Services come up we're going to have something that",
    "start": "1055440",
    "end": "1061120"
  },
  {
    "text": "updates our snapshot and continues to increase the scope and drag out this migration but at least we're getting everything along the way or just do a",
    "start": "1061120",
    "end": "1068280"
  },
  {
    "text": "check-in once you're finished once you've completed the initial scope of your migration and then take count of any new services that come up and just",
    "start": "1068280",
    "end": "1074919"
  },
  {
    "text": "add them to the bottom and just know that you're going to do some cleanup work to get those over the line as well well and then make sure because",
    "start": "1074919",
    "end": "1080640"
  },
  {
    "text": "basically when we think about cluster State like I said earlier once we've reached that steady state at that point",
    "start": "1080640",
    "end": "1086799"
  },
  {
    "text": "any new created Services go fully full fully to both clusters evenly and so you're worried about that middle state",
    "start": "1086799",
    "end": "1092960"
  },
  {
    "text": "where any new services that were created during the partial state that didn't get that treatment and then finally the",
    "start": "1092960",
    "end": "1098440"
  },
  {
    "text": "weird services not every service is perfect not every service migrates correctly or easily uh and so part of",
    "start": "1098440",
    "end": "1105400"
  },
  {
    "text": "our workflow was just getting through the easy services and if a service gave us an issue that was going to take us",
    "start": "1105400",
    "end": "1110640"
  },
  {
    "text": "longer than like half an hour to an hour to debug mark it on the spreadsheet track it for later and work with the",
    "start": "1110640",
    "end": "1116440"
  },
  {
    "text": "service owners more closely to make sure we're rectifying things without breaking them money by the way um so what I",
    "start": "1116440",
    "end": "1124880"
  },
  {
    "text": "mentioned earlier is that to keep this fallback State easy for us to move Services back and forth between clusters",
    "start": "1124880",
    "end": "1130799"
  },
  {
    "text": "if something goes wrong um is that we kept both clusters in this setup scaled",
    "start": "1130799",
    "end": "1136120"
  },
  {
    "text": "to 100 until we reach the end of the migration uh uh this made it really easy for us if there was ever an incident or",
    "start": "1136120",
    "end": "1141200"
  },
  {
    "text": "an outage we could just update the virtual service to point back to the old cluster and we would have been fine within minutes um and so one thing we",
    "start": "1141200",
    "end": "1149960"
  },
  {
    "text": "one really cool thing we found out was that afterwards after we completed the migration work we were working",
    "start": "1149960",
    "end": "1155039"
  },
  {
    "text": "through the scale down of basically being like if all these services in both clusters are now receiving 50% of the",
    "start": "1155039",
    "end": "1161760"
  },
  {
    "text": "traffic they did before they probably only need around 50% of the replicas that they did in one cluster to run as",
    "start": "1161760",
    "end": "1168320"
  },
  {
    "text": "expected and so that's where we see the thing on the first slide where same number of services um same number of",
    "start": "1168320",
    "end": "1176480"
  },
  {
    "text": "double number of clusters less nodes and comparable resource usage so we didn't",
    "start": "1176480",
    "end": "1181960"
  },
  {
    "text": "actually see a huge jump up in cost after finishing the full migration um",
    "start": "1181960",
    "end": "1187000"
  },
  {
    "text": "and so these are just things that you have to consider it work really well for us but depending on your business needs depending on your organization needs if",
    "start": "1187000",
    "end": "1192760"
  },
  {
    "text": "that is something that is worth it to you to be like yep we're going to keep it scaled at 100 until we finish and then we're going to do some cleanup work",
    "start": "1192760",
    "end": "1198840"
  },
  {
    "text": "versus cleaning up along the way and so part of that scaled on process was making another batch tool",
    "start": "1198840",
    "end": "1205400"
  },
  {
    "text": "using some more go um and it's really going in and T tackling two cases not",
    "start": "1205400",
    "end": "1211039"
  },
  {
    "text": "all services leverage autoscaling for whatever purposes maybe the autoscaler doesn't bring up new pods as it needs to",
    "start": "1211039",
    "end": "1217400"
  },
  {
    "text": "to serve the needs of the service or something else and so what we did was we had the first fetch that auto scaling",
    "start": "1217400",
    "end": "1224400"
  },
  {
    "text": "type whether it was Keta whether it was a rare Legacy HPA configuration or",
    "start": "1224400",
    "end": "1229640"
  },
  {
    "text": "whether it did didn't have any auto scaling whatsoever and treat them accordingly uh a couple of the other things that we did was we operate in",
    "start": "1229640",
    "end": "1237080"
  },
  {
    "text": "approximately three availability zones we provision our node groups into those availability zones and so for services",
    "start": "1237080",
    "end": "1242880"
  },
  {
    "text": "that maybe only had three replica three three pods before we wanted to make sure we set that as the floor for both",
    "start": "1242880",
    "end": "1248880"
  },
  {
    "text": "clusters so that if any availability Zone experience an outage a new node group in the in a nearby availability",
    "start": "1248880",
    "end": "1255640"
  },
  {
    "text": "Zone could pick up that workload and continue scaling the service for forward",
    "start": "1255640",
    "end": "1260679"
  },
  {
    "text": "um and then other things that I mentioned in the open source Community my team has done a lot of really great",
    "start": "1260679",
    "end": "1266080"
  },
  {
    "text": "work around upgrading our Carpenter usage and getting our node compaction set up so we can see that big decrease",
    "start": "1266080",
    "end": "1271159"
  },
  {
    "text": "in nodes and also we are starting to explore leveraging vpa for some of our",
    "start": "1271159",
    "end": "1276559"
  },
  {
    "text": "heavier Services um more around like the kafa space where you have a big burst of messages and you scaled down and you",
    "start": "1276559",
    "end": "1282679"
  },
  {
    "text": "don't necessarily want to do that for all of the topics that your team may manage and so we've seen a lot of",
    "start": "1282679",
    "end": "1287799"
  },
  {
    "text": "success there uh in terms of just cost and ease of usage with implementing",
    "start": "1287799",
    "end": "1292880"
  },
  {
    "text": "vpa um so now I want to take a step back we've gone through the migration process it all worked it was Zero downtime it",
    "start": "1292880",
    "end": "1298279"
  },
  {
    "text": "was great uh and I want to talk about more about how we as platform Engineers might view this process a little",
    "start": "1298279",
    "end": "1304200"
  },
  {
    "text": "differently from product engineers and how we need to think about the process to sort of Shield them and give them",
    "start": "1304200",
    "end": "1309320"
  },
  {
    "text": "their happy path while we proceed on ours and so the sort of big distinguishers here is for platform",
    "start": "1309320",
    "end": "1314720"
  },
  {
    "text": "Engineers when we think about going to multicluster when we think about doing a large scale migration about this worried about things like cost spend uh worried",
    "start": "1314720",
    "end": "1321480"
  },
  {
    "text": "about things like how can we scale up our platform but also still keep it maintainable for the same number of team",
    "start": "1321480",
    "end": "1327520"
  },
  {
    "text": "members that we have right now and the same level of effort we're putting in t clusters today and how can we take a",
    "start": "1327520",
    "end": "1333120"
  },
  {
    "text": "look at anything that deviates from that and look into automations simplifications of process to make it",
    "start": "1333120",
    "end": "1338559"
  },
  {
    "text": "easier on ourselves as we continue to grow and scale product Engineers on the other side they don't want to hear about",
    "start": "1338559",
    "end": "1343720"
  },
  {
    "text": "kuber nights most of the time they want an abstraction layer that is invisible to them they want to be they want to",
    "start": "1343720",
    "end": "1348919"
  },
  {
    "text": "have the ability to focus on delivering uh changes and updates to business logic features Etc and they want a really",
    "start": "1348919",
    "end": "1354760"
  },
  {
    "text": "simple deploy path and so for many uh service owners at Cash app they knew we were doing migration work we",
    "start": "1354760",
    "end": "1361080"
  },
  {
    "text": "communicated with them we were going to make a change for their service but at the end of the day all they really see is that cool now I have the same number",
    "start": "1361080",
    "end": "1367919"
  },
  {
    "text": "of PODS but in my deploy UI I just see that they're spread across two different clusters now and that's",
    "start": "1367919",
    "end": "1374159"
  },
  {
    "text": "fun and so this is how we sort of kept that product story simple it really just communicate one time at the beginning",
    "start": "1374159",
    "end": "1380799"
  },
  {
    "text": "set up a slack Channel set up an FAQ doc give service owners the space to ask their questions and get their confusion",
    "start": "1380799",
    "end": "1386960"
  },
  {
    "text": "out of the way to make the rest of the process really simple and keep them in like don't keep them in the dark because",
    "start": "1386960",
    "end": "1392120"
  },
  {
    "text": "if an alert goes off and an incident goes off and it was because you were moving something you didn't tell them they will not be happy about that um and",
    "start": "1392120",
    "end": "1399480"
  },
  {
    "text": "the power batch tooling the reason that we were able to execute on this with just a handful of Engineers is because",
    "start": "1399480",
    "end": "1405799"
  },
  {
    "text": "we were able to write these batch rappers around our existing a tooling and abstraction tooling to make this a",
    "start": "1405799",
    "end": "1411559"
  },
  {
    "text": "really seamless process uh like I just said FAQ and project Channel and the last one goes without saying but don't",
    "start": "1411559",
    "end": "1418640"
  },
  {
    "text": "cause an incident do things safely do things slowly if you're uncertain about something um like say traffic shifting",
    "start": "1418640",
    "end": "1425320"
  },
  {
    "text": "if you don't want to just YOLO it and go to 100% start off at 1% watch the metrics for 20 minutes or so make sure",
    "start": "1425320",
    "end": "1432240"
  },
  {
    "text": "that this is stable before ramping up the rest of the services load and improving the platform story",
    "start": "1432240",
    "end": "1438080"
  },
  {
    "text": "this is where where I'm going to talk about a lot of the future facing things that we want to do in 2025 and Beyond at Cash app to help make this easier for us",
    "start": "1438080",
    "end": "1446039"
  },
  {
    "text": "uh the first one first couple being actually infr pipelines and infra backup uh infr pipelines being if we're going",
    "start": "1446039",
    "end": "1452080"
  },
  {
    "text": "to be provisioning a lot of new clusters especially as we move towards a clusters or towards workload specific clusters we",
    "start": "1452080",
    "end": "1458400"
  },
  {
    "text": "need to make sure there's a way to do environment promotion of ISC resources and that there is a reproducible path of",
    "start": "1458400",
    "end": "1465159"
  },
  {
    "text": "saying that hey if we want to upgrade this component in one clust it should probably be propagated across",
    "start": "1465159",
    "end": "1470640"
  },
  {
    "text": "all the Clusters in some automated fashion and we're looking in the pipeline solutions to help us with this",
    "start": "1470640",
    "end": "1475760"
  },
  {
    "text": "the other is infr backup uh this has been the big year of giops um when we have several members of our team",
    "start": "1475760",
    "end": "1481399"
  },
  {
    "text": "contributing heavily to getting G UPS off the ground at Cash app and making sure that we have a way if needed to",
    "start": "1481399",
    "end": "1486720"
  },
  {
    "text": "fully restore our ecosystem uh using Argo app manifest Etc from a get repo",
    "start": "1486720",
    "end": "1492720"
  },
  {
    "text": "it's been really great um the next part is just kind of ties in the pipelines but time to new clust CL time un cluster",
    "start": "1492720",
    "end": "1499640"
  },
  {
    "text": "right now in the span of days what if we got it into the span of hours um that would be just really amazing for our",
    "start": "1499640",
    "end": "1506520"
  },
  {
    "text": "reliability story and our operational story uh get Ops like I just mentioned and the prevalence of game days um I",
    "start": "1506520",
    "end": "1513279"
  },
  {
    "text": "have a whole slide dedicated to this but this is really just how do you as a platform team secure some dedicated time",
    "start": "1513279",
    "end": "1519679"
  },
  {
    "text": "to go crash your system and know that when you crash your system you know how it's going to recover or what gaps you",
    "start": "1519679",
    "end": "1525200"
  },
  {
    "text": "have that you need to automate and build out to make sure that it recover safely and so we're going to dive into",
    "start": "1525200",
    "end": "1531840"
  },
  {
    "text": "cluster operations a little bit so a lot of this is around automating and streamlining that cluster 0 to one",
    "start": "1531840",
    "end": "1537320"
  },
  {
    "text": "making sure that we know our process today but we also know the parts where it lags a little bit parts that we could",
    "start": "1537320",
    "end": "1543559"
  },
  {
    "text": "maybe compact together parts that we could streamline and so one thing I've been looking into with one of my colleagues is exploring crossplane",
    "start": "1543559",
    "end": "1550039"
  },
  {
    "text": "compositions as a way to create multistage uh cluster provisioning steps so that your you as a platform engineer",
    "start": "1550039",
    "end": "1556480"
  },
  {
    "text": "aren't following a run book with 15 steps and manually doing each one of them submitting your PRS getting your approvals but you can have this",
    "start": "1556480",
    "end": "1562760"
  },
  {
    "text": "composition take care of that for you simplify the number of reviews you need to get this over the line and we're",
    "start": "1562760",
    "end": "1568520"
  },
  {
    "text": "hopeful that this is going to bring some improvements um the next part is some",
    "start": "1568520",
    "end": "1573840"
  },
  {
    "text": "bat cluster tasks and so this is things like um making sure your service gets deployed to all the Clusters making sure",
    "start": "1573840",
    "end": "1580720"
  },
  {
    "text": "that if you're doing an upgrade to a component those get propagated out without you having to sort of manually",
    "start": "1580720",
    "end": "1585760"
  },
  {
    "text": "sit by it as soon as you've uh affirmed and asserted that this is healthfully",
    "start": "1585760",
    "end": "1590880"
  },
  {
    "text": "it's working in one of your other clusters in your blue Greening sort of um and so in on the tooling side we're",
    "start": "1590880",
    "end": "1595960"
  },
  {
    "text": "looking into extending our CLI capabilities to be more multicluster we're looking into improving our update",
    "start": "1595960",
    "end": "1602720"
  },
  {
    "text": "Cadence process and we're looking into how we can sync additional resources like we do with ESO for Secrets um in",
    "start": "1602720",
    "end": "1609960"
  },
  {
    "text": "terms of practice really heavy on the Argo CD get up side and then really looking forward to exploring I pipelines",
    "start": "1609960",
    "end": "1616520"
  },
  {
    "text": "more in that same van to help us move faster um in the last part like I mentioned with guard rails before we",
    "start": "1616520",
    "end": "1622760"
  },
  {
    "text": "want to continue expanding in the space uh we do a lot of stuff with CNO and Chainsaw now we we anticipate this",
    "start": "1622760",
    "end": "1629080"
  },
  {
    "text": "growth this usage to grow uh over the next year and then also on the Cooper healthy side as we get more people up",
    "start": "1629080",
    "end": "1636120"
  },
  {
    "text": "and running and understanding what this tooling can offer them both on our Computing but maybe other things like observability traffic cscd that they are",
    "start": "1636120",
    "end": "1642919"
  },
  {
    "text": "implementing more checks so that at some point 90% of your cluster like you have",
    "start": "1642919",
    "end": "1648279"
  },
  {
    "text": "the expected state of what a new cluster should look like and you can fall back on these policies fall back on these checks to assert that that's what you",
    "start": "1648279",
    "end": "1655240"
  },
  {
    "text": "are tracking against and then for game days the big tool that we've been using",
    "start": "1655240",
    "end": "1660279"
  },
  {
    "text": "is AWS fault injection simulator or FIS um very similar Concepts to something",
    "start": "1660279",
    "end": "1665440"
  },
  {
    "text": "like chaos monkey where it gives you the ability to Target outages in your infrastructure um or your services and",
    "start": "1665440",
    "end": "1673240"
  },
  {
    "text": "so we just run regular game days I think we've run like two or three now at this point uh but it's just coord efforts",
    "start": "1673240",
    "end": "1678880"
  },
  {
    "text": "where support Engineers from all different abstractions of our platform get together we run the FIS tool for",
    "start": "1678880",
    "end": "1685320"
  },
  {
    "text": "maybe 5 10 minutes to just crash something and we observe we wait for things to come back up we identify",
    "start": "1685320",
    "end": "1692039"
  },
  {
    "text": "bottle gaps where oh this database didn't come back up what do we need to do to make sure that next time we run this game day that does come up",
    "start": "1692039",
    "end": "1697799"
  },
  {
    "text": "correctly um and we really want to extend this to cover additional things",
    "start": "1697799",
    "end": "1702880"
  },
  {
    "text": "um how can we track to make sure that all of our deployer role components come up how do we make make sure that any",
    "start": "1702880",
    "end": "1709640"
  },
  {
    "text": "services that come up that if we want to do the get Ops res restoration path that they are actually serving traffic as",
    "start": "1709640",
    "end": "1716120"
  },
  {
    "text": "soon as they spin up and Argo and they sink correctly um and it's just really looking into what is the manual effort",
    "start": "1716120",
    "end": "1722600"
  },
  {
    "text": "we have to do to run these game days versus what can we automate how can we get this down to a few button presses to",
    "start": "1722600",
    "end": "1727760"
  },
  {
    "text": "get this whole process into end and be more confident as a platform team that we are serving our users at the",
    "start": "1727760",
    "end": "1734440"
  },
  {
    "text": "company and that's all I had so I'm really happy for a Q&A if anyone has any questions uh this is",
    "start": "1734440",
    "end": "1740440"
  },
  {
    "text": "my dog Hara thank [Applause]",
    "start": "1740440",
    "end": "1751279"
  },
  {
    "text": "you thanks for the presentation uh can you brings some clarity on how you switch Network because you specify you",
    "start": "1751279",
    "end": "1758159"
  },
  {
    "text": "have cluster a and cluster B you have virtual service in cluster a you put a",
    "start": "1758159",
    "end": "1763399"
  },
  {
    "text": "weights and start shifting traffic gradually to Cluster B but eventually you want to destroy cluster a or not or",
    "start": "1763399",
    "end": "1770559"
  },
  {
    "text": "viral server still exit there an entry point is still in one cluster and you're just making a mesh uh sorry two meshes",
    "start": "1770559",
    "end": "1776159"
  },
  {
    "text": "connected correct so each service will have its own Ingress Gateway in each cluster and so when we do that virtual",
    "start": "1776159",
    "end": "1781799"
  },
  {
    "text": "service update um we're thinking in like a times two capacity for two clusters where if we set it to be at 50% in one",
    "start": "1781799",
    "end": "1789960"
  },
  {
    "text": "cluster that's 100% share for the new cluster and 100% for the old cluster but you're rather like that's 100% in",
    "start": "1789960",
    "end": "1796279"
  },
  {
    "text": "cluster calls and you split that out 5050 so our Legacy services do still exist in this context and they're just",
    "start": "1796279",
    "end": "1802760"
  },
  {
    "text": "sharing that load if that answers your question which means that network still goes to Cluster a first and and as entry",
    "start": "1802760",
    "end": "1809640"
  },
  {
    "text": "point before shifting the traffic so we split it up higher up and they get load balanced across oh you have a global",
    "start": "1809640",
    "end": "1815440"
  },
  {
    "text": "load balance there on top of them yep thanks for clarifying of course thank",
    "start": "1815440",
    "end": "1820879"
  },
  {
    "text": "you hey thanks for your presentation um I'm curious about how you handle multi-tenancy and then multi",
    "start": "1821120",
    "end": "1826360"
  },
  {
    "text": "environments for your different tenants in this setup you have so we I don't",
    "start": "1826360",
    "end": "1831840"
  },
  {
    "text": "believe I've explored into that space too much uh what we have right now is for our three primary environments that",
    "start": "1831840",
    "end": "1837399"
  },
  {
    "text": "Services deploy into they're separate AWS accounts that are isolated from each other and so we have this set up so that",
    "start": "1837399",
    "end": "1844679"
  },
  {
    "text": "these two clusters now are both deploy targets within the same AWS account for say like staging or production and the",
    "start": "1844679",
    "end": "1850519"
  },
  {
    "text": "services go deploy out and that's how we get our segmentation but that is something we'll probably be looking into",
    "start": "1850519",
    "end": "1856519"
  },
  {
    "text": "in the next year yeah thanks for the um presentation my",
    "start": "1856519",
    "end": "1863039"
  },
  {
    "text": "question was around kind of the actual setup of your clusters I saw saw like",
    "start": "1863039",
    "end": "1868279"
  },
  {
    "text": "you had a lot of bash script so there's you don't actually have anything like in terraform or infrastructure as code for",
    "start": "1868279",
    "end": "1874440"
  },
  {
    "text": "your cluster or are you more like just running CLI that actually do all these things it's a bit of both the for the",
    "start": "1874440",
    "end": "1881120"
  },
  {
    "text": "actual cluster setup it's primarily terraform modules wrapped in like a terround wrapper um and so you'll have",
    "start": "1881120",
    "end": "1887960"
  },
  {
    "text": "your like eks model uh deploy onto Argo as an application you'll set up like",
    "start": "1887960",
    "end": "1893399"
  },
  {
    "text": "your Argo CD uh UI and everything and your certificates um but the majority of",
    "start": "1893399",
    "end": "1899080"
  },
  {
    "text": "our cluster setup is IAC it's the actual migration part where we start wrapping these uh copl scripts okay yeah thank",
    "start": "1899080",
    "end": "1908399"
  },
  {
    "text": "you and if there is oh yes uh if uh while you are migrating",
    "start": "1908480",
    "end": "1915799"
  },
  {
    "text": "from cluster a to Cluster B and and uh let's say one of the services not acting",
    "start": "1915799",
    "end": "1922200"
  },
  {
    "text": "upright during the migration on the targeted cluster however that service is using some underlying Downstream",
    "start": "1922200",
    "end": "1929039"
  },
  {
    "text": "Services as well so first question would be that do you first migrate the service based on the dependency tree or if not",
    "start": "1929039",
    "end": "1936159"
  },
  {
    "text": "if the each and every service are independent in the case of microservices how do you manage that so we actually uh",
    "start": "1936159",
    "end": "1943840"
  },
  {
    "text": "broke this up a little bit we we separated the traffic from the deployment part so so all of these",
    "start": "1943840",
    "end": "1949360"
  },
  {
    "text": "traffic resources are still present in the new cluster when we do our back fill but they're pointing back at the Legacy",
    "start": "1949360",
    "end": "1955480"
  },
  {
    "text": "cluster so even if you take a random service and you deploy it out in your new clust you scale it up you start",
    "start": "1955480",
    "end": "1960880"
  },
  {
    "text": "sending traffic to it the dependencies because those virtual servic configure they're still reaching back into the oldak cluster you do incur a tiny bit of",
    "start": "1960880",
    "end": "1968279"
  },
  {
    "text": "latency like one millisecond bump from the cross cluster jump um but that's just something we accepted as we moved",
    "start": "1968279",
    "end": "1973519"
  },
  {
    "text": "throughout this process and just make sure we time Bo the actual migration to get through it as efficiently as possible um but yeah like in dependency wise the",
    "start": "1973519",
    "end": "1981519"
  },
  {
    "text": "only real thing we saw was uh like things like rolls um or secrets that other like worker jobs in a higher tier",
    "start": "1981519",
    "end": "1988840"
  },
  {
    "text": "depending on but we would see those within the um logs of the service and",
    "start": "1988840",
    "end": "1994080"
  },
  {
    "text": "event errors as we scaled it up and notice that oh hey we actually do need to go do a backfill on that service to make sure at least the resources are",
    "start": "1994080",
    "end": "2000320"
  },
  {
    "text": "there even if we haven't scaled it up yet okay so are there any validation at what point you decide that okay I'm",
    "start": "2000320",
    "end": "2006720"
  },
  {
    "text": "ready to move on to a new cluster because until it is not completely",
    "start": "2006720",
    "end": "2012080"
  },
  {
    "text": "validated uh at one point you'll be running a double the uh Computer Resources until it is fully complete",
    "start": "2012080",
    "end": "2019440"
  },
  {
    "text": "correct I I I didn't hear the last part of that uh I meant to say that if let's say the service you are migrating and if",
    "start": "2019440",
    "end": "2025519"
  },
  {
    "text": "there is an issue with that at one point you have to run cluster a and cluster B at the same time in this kind of model",
    "start": "2025519",
    "end": "2032440"
  },
  {
    "text": "that means double theour we fall back on our observability a lot whether it's monitors and alarms to alert us if",
    "start": "2032440",
    "end": "2037799"
  },
  {
    "text": "things go wrong um this wasn't something that we faced during this migration but that would have caught us if it did come",
    "start": "2037799",
    "end": "2043320"
  },
  {
    "text": "up okay thank you thank you any other",
    "start": "2043320",
    "end": "2050520"
  },
  {
    "text": "questions cool well thank you all for coming I'm going to hang out here a bit if anyone wants to come chat uh but",
    "start": "2050879",
    "end": "2056280"
  },
  {
    "text": "thank you [Applause]",
    "start": "2056280",
    "end": "2061868"
  }
]