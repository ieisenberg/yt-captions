[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "all right so this session is kubernetes",
    "start": "30",
    "end": "5220"
  },
  {
    "text": "networking at scale and I am Bowie I",
    "start": "5220",
    "end": "10550"
  },
  {
    "text": "work at Google and I'm l'homme I work at data dog in the team responsible for",
    "start": "10550",
    "end": "16680"
  },
  {
    "text": "operating tremendous cluster and we run pretty big register with an whole",
    "start": "16680",
    "end": "22220"
  },
  {
    "text": "different kind of issues with networking and this is one of the feedback we give",
    "start": "22220",
    "end": "27260"
  },
  {
    "text": "ok so in general we're looking at to",
    "start": "27260",
    "end": "32488"
  },
  {
    "start": "28000",
    "end": "28000"
  },
  {
    "text": "give a concrete data point for what we're gonna talk about in terms of scaling challenges so we're talking about thousands of nodes dozens of",
    "start": "32489",
    "end": "39780"
  },
  {
    "text": "clusters and each cluster having you know tens of thousands of pods and then",
    "start": "39780",
    "end": "45270"
  },
  {
    "text": "thousands of services in terms of the application that we're looking at we're looking at you know applications that",
    "start": "45270",
    "end": "51690"
  },
  {
    "text": "process say trillions of RPS and gigabytes of data per second in terms of",
    "start": "51690",
    "end": "60120"
  },
  {
    "text": "topology we're talking not just of single cluster or communication within a single cluster but communication across",
    "start": "60120",
    "end": "66960"
  },
  {
    "text": "clusters and from VMs and clusters mixing them and finally in terms of",
    "start": "66960",
    "end": "72000"
  },
  {
    "text": "latency we would like the lowest latency possible for these workloads now in",
    "start": "72000",
    "end": "78330"
  },
  {
    "text": "terms of the high-level looking at scaling challenges we need to consider a couple of things first is in the data",
    "start": "78330",
    "end": "85320"
  },
  {
    "text": "plane you really want to remove as many inefficiencies as possible right ku",
    "start": "85320",
    "end": "90960"
  },
  {
    "text": "Bernays offers a whole bunch of abstractions but we really want to reduce the cost of those abstractions in",
    "start": "90960",
    "end": "96060"
  },
  {
    "text": "the control plane when you're scaling up your cluster you will hit these large end issues where n is number of nose",
    "start": "96060",
    "end": "103530"
  },
  {
    "text": "number of resources in addition to the large n issues you will also want to",
    "start": "103530",
    "end": "108600"
  },
  {
    "text": "simplify your architecture simplifying your architecture really means reducing dependencies and making it easier to",
    "start": "108600",
    "end": "115470"
  },
  {
    "text": "debug and in general we have found that the really the strategy to do this is to",
    "start": "115470",
    "end": "120540"
  },
  {
    "text": "sign of enable more native integration with the infrastructure that you're running on now let's see what this means",
    "start": "120540",
    "end": "128369"
  },
  {
    "text": "in practice so we'll talk about pod networking we'll talk about service load balancing",
    "start": "128369",
    "end": "133370"
  },
  {
    "text": "we'll talk about l7 load-balancing which is ingress and then finally we'll discuss DNS so first let's talk about",
    "start": "133370",
    "end": "139340"
  },
  {
    "text": "pod networking but networking is the way",
    "start": "139340",
    "end": "144409"
  },
  {
    "text": "to give interfaces and IP addresses to two pods there are many different ways",
    "start": "144409",
    "end": "150829"
  },
  {
    "start": "146000",
    "end": "146000"
  },
  {
    "text": "to do it the most simple one that many people have tried when they get started is to use static routes so the way it",
    "start": "150829",
    "end": "157760"
  },
  {
    "text": "works is you give every node on your cluster a cider and then you use the",
    "start": "157760",
    "end": "162769"
  },
  {
    "text": "cloud provider routing table and you add static routes for each Siler give pointing to to the nodes this is very",
    "start": "162769",
    "end": "169670"
  },
  {
    "text": "simple works fine however there are limits in terms of number of static",
    "start": "169670",
    "end": "174950"
  },
  {
    "text": "routes for instance by default on AWS you can't have more than 50 static routes which means you can have more",
    "start": "174950",
    "end": "180230"
  },
  {
    "text": "than 50 nodes and also and we'll dive into this later it's not very efficient in term of",
    "start": "180230",
    "end": "186200"
  },
  {
    "text": "address space because if you give a slash 24 to all the nodes if you have 10",
    "start": "186200",
    "end": "191359"
  },
  {
    "text": "to 20 minutes per node it's not gonna be very efficient and you're going to consume a lot of address space and it's gonna be tricky to manage another option",
    "start": "191359",
    "end": "199040"
  },
  {
    "text": "is to use overlays so this is I think where most people are at today by",
    "start": "199040",
    "end": "205340"
  },
  {
    "text": "overlays I mean either calico or flannel for instance and the way it worked in",
    "start": "205340",
    "end": "210470"
  },
  {
    "text": "that case you also allocate a cider for each node but instead of relying on the",
    "start": "210470",
    "end": "216470"
  },
  {
    "text": "cloud for value routing to get traffic per traffic from one node to another you actually create another way which means",
    "start": "216470",
    "end": "223280"
  },
  {
    "text": "all these ciders replicated two nodes are actually virtual sizes which means",
    "start": "223280",
    "end": "228470"
  },
  {
    "text": "the underlying network doesn't know anything about it and what the way it works is you actually do tunneling using",
    "start": "228470",
    "end": "235639"
  },
  {
    "text": "either the X LAN or IP IP this works fine and it's very efficient because it",
    "start": "235639",
    "end": "241040"
  },
  {
    "text": "means it can whirring is going to work regardless of the underlying infrastructure which makes it very easy",
    "start": "241040",
    "end": "246829"
  },
  {
    "text": "to set up and that's why it's very popular to do it this way however there's quite another head in",
    "start": "246829",
    "end": "252620"
  },
  {
    "text": "internals because you need to create packets and you need to add the additional header or for them and also",
    "start": "252620",
    "end": "259250"
  },
  {
    "text": "distributing the route is sometimes a bit tricky so here is a typical example",
    "start": "259250",
    "end": "265310"
  },
  {
    "start": "263000",
    "end": "263000"
  },
  {
    "text": "of how that would work your pod will get an interface so a VF interface connected to a bridge and then",
    "start": "265310",
    "end": "271850"
  },
  {
    "text": "you would have a victor an interface connected to this bridge that's actually getting traffic out of the node so in",
    "start": "271850",
    "end": "277130"
  },
  {
    "text": "this example here I'm getting traffic from one bird with IP 191 since I go one",
    "start": "277130",
    "end": "282320"
  },
  {
    "text": "two one nine two six eight oh why okay and on the wire between the two",
    "start": "282320",
    "end": "287750"
  },
  {
    "text": "instances young traffic between the node IP so the one in ten shows up in ten and",
    "start": "287750",
    "end": "293810"
  },
  {
    "text": "the original layer two so sent by bus buzz is actually encapsulated in that fixed an error and as you can see that",
    "start": "293810",
    "end": "301130"
  },
  {
    "text": "like there's a curse because you have all this additional information in the packet and also creating the vehicle and",
    "start": "301130",
    "end": "306680"
  },
  {
    "text": "frame is consuming in terms of channel CPU usage another issue is control plane",
    "start": "306680",
    "end": "314509"
  },
  {
    "start": "312000",
    "end": "312000"
  },
  {
    "text": "I mean having overlays is fine when you're you know number of node is not that big but at the certain scale it",
    "start": "314509",
    "end": "320479"
  },
  {
    "text": "starts to be tricky because as you can see in this example it's a typical deployment of an overlay you have an",
    "start": "320479",
    "end": "326000"
  },
  {
    "text": "agent on the node that is going to configure routes and channel in for instance vehicle in that case and this",
    "start": "326000",
    "end": "331970"
  },
  {
    "text": "agent needs to get information on the cluster so it's going to connect to the API server and quite often it's going",
    "start": "331970",
    "end": "337490"
  },
  {
    "text": "also to need a data store to store all the information it needs it's quite a van HDD and as your number of node grows",
    "start": "337490",
    "end": "344449"
  },
  {
    "text": "the load all these agents are going to cause on data stalls and API server is going to be non negligible and can",
    "start": "344449",
    "end": "351020"
  },
  {
    "text": "become an issue so in terms of best",
    "start": "351020",
    "end": "356659"
  },
  {
    "start": "354000",
    "end": "354000"
  },
  {
    "text": "practices what we think where everybody is going to use native pod routing which means avoid avoid overlays in this case",
    "start": "356659",
    "end": "364789"
  },
  {
    "text": "we want to have to open to a gradable IP on the network and so no overlay it's",
    "start": "364789",
    "end": "370250"
  },
  {
    "text": "much easier to debug it's simpler and also it allows for flat network which means communication between two clusters",
    "start": "370250",
    "end": "376370"
  },
  {
    "text": "put in different clusters and even VM outside of your cluster to your pods is going to work because everything is",
    "start": "376370",
    "end": "382159"
  },
  {
    "text": "radical another optimization that we're seeing more and more and we're relying heavily on it a data dog is removed",
    "start": "382159",
    "end": "388820"
  },
  {
    "text": "bridging like typical installation is the one I showed you before where all pods are connecting to a bridge and then",
    "start": "388820",
    "end": "394039"
  },
  {
    "text": "routed outside the host but when you think about it you actually don't need this bridge quite often you just can",
    "start": "394039",
    "end": "399740"
  },
  {
    "text": "rely on the house itself and this is what what we do most of the time another optimization that is",
    "start": "399740",
    "end": "406210"
  },
  {
    "text": "that we see quite often is IP VLAN where it's been faster than having routing so",
    "start": "406210",
    "end": "415280"
  },
  {
    "text": "you remember the slide from before you can see that this one is much simpler because in that case all all parts of",
    "start": "415280",
    "end": "422360"
  },
  {
    "text": "radical IPS and there are in different space than the IPS of the nodes but everything just work and it's much more",
    "start": "422360",
    "end": "427730"
  },
  {
    "text": "efficient in terms of so this is the",
    "start": "427730",
    "end": "433790"
  },
  {
    "text": "principle and now we're going to talk about how this is known in practice so we're gonna very quickly talk about on",
    "start": "433790",
    "end": "438980"
  },
  {
    "text": "premise on premise sometimes you're very lucky because you can actually control the network and you can establish bgp",
    "start": "438980",
    "end": "444710"
  },
  {
    "text": "connection between your nodes and the network which means the network is going to know about the Sider allocated to",
    "start": "444710",
    "end": "450110"
  },
  {
    "text": "each node is going to be transparent it's just going to be another side or in",
    "start": "450110",
    "end": "455630"
  },
  {
    "text": "cloud provider it's slightly different so let's first talk about how it's done at Google okay so in the google",
    "start": "455630",
    "end": "462470"
  },
  {
    "start": "460000",
    "end": "460000"
  },
  {
    "text": "kubernetes engine or if you run kubernetes directly on tcp it used to be in the old way that we create these",
    "start": "462470",
    "end": "467750"
  },
  {
    "text": "static routes now as we discussed before static routes the static route functionality is actually fairly complex",
    "start": "467750",
    "end": "473240"
  },
  {
    "text": "and you can easily run into quota issues because it's somewhat resource intensive to sort of implement all the different",
    "start": "473240",
    "end": "479030"
  },
  {
    "text": "features there what we have added is a VPC native mode so in VPC native you",
    "start": "479030",
    "end": "485780"
  },
  {
    "text": "have a pre allocated range for the pods and from this range the cloud actually allocates the IPS as alias ranges to the",
    "start": "485780",
    "end": "493550"
  },
  {
    "text": "VMS and the cloud actually manages this IP allocation for the cluster now this",
    "start": "493550",
    "end": "499400"
  },
  {
    "text": "is much more semantic and efficient for the underlying SDM and in fact you are",
    "start": "499400",
    "end": "504470"
  },
  {
    "text": "telling the Sdn about the pods so the Sdn kind of knows that there is this range allocated of the pods and the",
    "start": "504470",
    "end": "510830"
  },
  {
    "text": "stn also knows about this allocation to the pods itself so let's see how this works so in classic kubernetes you have",
    "start": "510830",
    "end": "518719"
  },
  {
    "text": "a range that's devoted to the VM IPS you have this pod IP allocation and then you",
    "start": "518719",
    "end": "524390"
  },
  {
    "text": "have the service IPS on the left and then with the V PC native mode the",
    "start": "524390",
    "end": "530519"
  },
  {
    "text": "the cloud will allocate these slash 24s to the various nodes that come up in",
    "start": "530519",
    "end": "535870"
  },
  {
    "text": "your cluster and in fact because we have delegated IP allocation to the cloud we",
    "start": "535870",
    "end": "542110"
  },
  {
    "text": "can do interesting things like have the cloud control the size of this",
    "start": "542110",
    "end": "547269"
  },
  {
    "text": "allocation to various nodes and actually this becomes a scalability issue as well that kubernetes I'm sure everyone has",
    "start": "547269",
    "end": "554050"
  },
  {
    "text": "noticed when you create a cluster with the default settings it's actually quite greedy in terms of eating up your IP",
    "start": "554050",
    "end": "559930"
  },
  {
    "text": "addresses for instance with the default settings 1k nodes with a slash 24 well",
    "start": "559930",
    "end": "565269"
  },
  {
    "text": "that's a thousand times 256 which is actually 256 IPs on your network and we",
    "start": "565269",
    "end": "571240"
  },
  {
    "text": "find that a lot of people when they try to stitch their networks together actually run out of space so with the",
    "start": "571240",
    "end": "576790"
  },
  {
    "text": "cloud mediated IP allocation we can tell it oh hey I don't need that many IPS this is a node that's gonna run a very",
    "start": "576790",
    "end": "583209"
  },
  {
    "text": "specialized workload we can give it a fewer I P so you can kind of right size it for your node footprint and in",
    "start": "583209",
    "end": "590860"
  },
  {
    "text": "general we hope that sort of as future work is that we get away from these contiguous ranges and basically go to",
    "start": "590860",
    "end": "597870"
  },
  {
    "text": "discontinuous ranges that can be merged together so we can kind of have clusters that fit into your network footprint",
    "start": "597870",
    "end": "606870"
  },
  {
    "text": "another interesting thing about VPC native sort of cloud Sdn integrated plot",
    "start": "606870",
    "end": "614199"
  },
  {
    "text": "routing is that there's an opportunity to give the Sdn full visibility of your traffic so when you create a DPC native",
    "start": "614199",
    "end": "622660"
  },
  {
    "text": "cluster you can when you see that when a pod talks to another pod over the Sdn",
    "start": "622660",
    "end": "629019"
  },
  {
    "text": "from node to node the VPC is able to see the flow but when the pod talks pod to pod actually this flow is not visible to",
    "start": "629019",
    "end": "635649"
  },
  {
    "text": "the Sdn well you can play some tricks with the PDP to basically pin that traffic out to the Sdn and what this",
    "start": "635649",
    "end": "642430"
  },
  {
    "text": "means is that all of your pod traffic whether it's between nodes or within the node will be visible by the Sdn and kind",
    "start": "642430",
    "end": "648730"
  },
  {
    "text": "of use the same infrastructure for example you can use the same kind of tools on your Sdn to observe all the",
    "start": "648730",
    "end": "654069"
  },
  {
    "text": "flows in your cluster and so this was",
    "start": "654069",
    "end": "659860"
  },
  {
    "start": "658000",
    "end": "658000"
  },
  {
    "text": "for DCP an address it's slightly different because they no equivalent feature in terms of at ple",
    "start": "659860",
    "end": "664959"
  },
  {
    "text": "assurance you cannot associate an additional range to your notes but there are still some CNI plugin that allow you",
    "start": "664959",
    "end": "671050"
  },
  {
    "text": "to give to give radabaugh IP stupids but the first one that's pretty well-known because it's the one provided",
    "start": "671050",
    "end": "677230"
  },
  {
    "text": "by AWS and use any key s is the e key sen i plug in the way this plugin works is you basically attach additional units",
    "start": "677230",
    "end": "684910"
  },
  {
    "text": "to nodes and additional IPS to this yennai and this IP are gonna be the pod IP the way it works in terms of workflow",
    "start": "684910",
    "end": "691269"
  },
  {
    "text": "is when the cubit in runtime are going to create the interface and me to four feet apart the first thing they're going",
    "start": "691269",
    "end": "698170"
  },
  {
    "text": "to do is the shehnai plugin is going to call a demon that is responsible for allocating yen eyes and providing",
    "start": "698170",
    "end": "703329"
  },
  {
    "text": "eyepiece and then this runtime is going to create the interface associate this",
    "start": "703329",
    "end": "708820"
  },
  {
    "text": "IP with it and do some clever writing on the host to make sure that traffic from IP one on part one is actually going to",
    "start": "708820",
    "end": "715360"
  },
  {
    "text": "throw in the face Ethier h1 in this is raid zero an alternative plug-in is a",
    "start": "715360",
    "end": "723250"
  },
  {
    "start": "721000",
    "end": "721000"
  },
  {
    "text": "plug-in developed by lift which is very similar in terms of how it's designed because it's also attaching additionally",
    "start": "723250",
    "end": "729880"
  },
  {
    "text": "an eyes an additional IPS it's a bit more complicated because they did some optimization so the first difference is",
    "start": "729880",
    "end": "735850"
  },
  {
    "text": "each team unless so you actually have a binary called I Pam something and this",
    "start": "735850",
    "end": "740890"
  },
  {
    "text": "spanner is responsible for manipulating eni and allocating IPs and the main traffic for the pod is going to use an",
    "start": "740890",
    "end": "747310"
  },
  {
    "text": "IP VLAN interface the goal for this one is to be much more efficient than during just routing because I believe you'll",
    "start": "747310",
    "end": "752920"
  },
  {
    "text": "and you don't have to cross the same part of the kernel so it's gonna be faster the disk was pretty fine but",
    "start": "752920",
    "end": "758500"
  },
  {
    "text": "sometimes also need to reach to reach and IP addresses on the node such as the",
    "start": "758500",
    "end": "763570"
  },
  {
    "text": "cubit resistance in that case you need an additional interface and that's why there is this plug-in they're called PTP",
    "start": "763570",
    "end": "769899"
  },
  {
    "text": "which is going to create an interface to the house so this works pretty fine we've been using it for more than a year",
    "start": "769899",
    "end": "775569"
  },
  {
    "text": "now and we've been very happy with it but honestly it's a bit complicated to debug sometimes at the conclusion on",
    "start": "775569",
    "end": "784600"
  },
  {
    "text": "this and this part native patrolling is very efficient would pretty sure that",
    "start": "784600",
    "end": "789910"
  },
  {
    "text": "everybody is going to go this way in the future you don't pay the cost of the overlay CNI plug in a still a bit young but",
    "start": "789910",
    "end": "796680"
  },
  {
    "text": "they're getting there I mean we've seen a lot of progress in the last year and as I were saying before it's allows for",
    "start": "796680",
    "end": "802620"
  },
  {
    "text": "traffic between clusters and traffic with vm so it's a lot it's a lot easier and finally it allows ingress traffic to",
    "start": "802620",
    "end": "808410"
  },
  {
    "text": "be managed much more efficiently and we discuss this later ok so let's talk about service routing so when we talk",
    "start": "808410",
    "end": "816900"
  },
  {
    "start": "815000",
    "end": "815000"
  },
  {
    "text": "about service routing we instantly get into the land of queue proxy to kind of give a sketch of how queue proxy works",
    "start": "816900",
    "end": "823140"
  },
  {
    "text": "on every node there's going to be a queue proxy instance it's going to watch the API server associated with this",
    "start": "823140",
    "end": "828750"
  },
  {
    "text": "whole service mechanism is a set of resources which are controlled by the",
    "start": "828750",
    "end": "834210"
  },
  {
    "text": "endpoints control and the service controller and the endpoints and service controller basically are responsible for",
    "start": "834210",
    "end": "839880"
  },
  {
    "text": "looking at the service resources assembling all the relevant endpoints and writing them into the set CD that",
    "start": "839880",
    "end": "845670"
  },
  {
    "text": "then can be consumed by the queue proxy now the PUE proxy implements a",
    "start": "845670",
    "end": "850770"
  },
  {
    "text": "client-side load balancer on every node right so here you have the client and it's sending traffic and there's this",
    "start": "850770",
    "end": "857220"
  },
  {
    "text": "proxy err box in most typical implementations with IP tables that's actually just a Linux kernel and then",
    "start": "857220",
    "end": "863280"
  },
  {
    "text": "it's going to load balanced traffic to the various pods now the original",
    "start": "863280",
    "end": "869280"
  },
  {
    "start": "868000",
    "end": "868000"
  },
  {
    "text": "implementation of queue proxy was in user space this is old history most people use the IP tables based queue",
    "start": "869280",
    "end": "876660"
  },
  {
    "text": "proxy it's default since 1.2 and it's way faster than the user space but we'll",
    "start": "876660",
    "end": "883320"
  },
  {
    "text": "see that they have some problems and then finally there's an IPS implementation that when GaN 1.11 and it",
    "start": "883320",
    "end": "890220"
  },
  {
    "text": "uses a different kernel mechanism but it still relies on IP tables for some things it's faster than IP tables scales",
    "start": "890220",
    "end": "895980"
  },
  {
    "text": "better and Lauren will talk about their experience with ipbs so in terms of IP",
    "start": "895980",
    "end": "902790"
  },
  {
    "start": "902000",
    "end": "902000"
  },
  {
    "text": "tables what is it doing so it has to make a load balancing decision on the client side and to keep the traffic",
    "start": "902790",
    "end": "909930"
  },
  {
    "text": "flowing it has to handle two cases it has to take the outgoing traffic find a",
    "start": "909930",
    "end": "914970"
  },
  {
    "text": "back-end rewrite the back end the service IP to the back end and then also",
    "start": "914970",
    "end": "920190"
  },
  {
    "text": "handle getting the traffic back in the reverse path and the way this works I won't go into detail",
    "start": "920190",
    "end": "926279"
  },
  {
    "start": "924000",
    "end": "924000"
  },
  {
    "text": "but it's a lot of chains of iptables right and generally speaking iptables is the rules are basically a bunch of if",
    "start": "926279",
    "end": "932519"
  },
  {
    "text": "statements chained together in a linear list so if you know you first have to select a back end and then you have to",
    "start": "932519",
    "end": "939149"
  },
  {
    "text": "go to a service and then you have to traverse all these tables and generally speaking it's a long chain and",
    "start": "939149",
    "end": "945420"
  },
  {
    "text": "especially when you have a lot of services and a lot of back ends these chains grow linearly with that so what",
    "start": "945420",
    "end": "953610"
  },
  {
    "start": "952000",
    "end": "952000"
  },
  {
    "text": "are the challenges well a couple and I kind of hinted on it in the previous slide the IB tables kernel interface",
    "start": "953610",
    "end": "960209"
  },
  {
    "text": "requires a sink of all the rules of one shot this is just how that kernel interface works you have a longer sink",
    "start": "960209",
    "end": "965910"
  },
  {
    "text": "time now this used to be a big problem but some kernel fixes went in and it has gone down to seconds but it's still you",
    "start": "965910",
    "end": "972779"
  },
  {
    "text": "have to pay that cost and there's significant memory usage because you have to generate this list of rules it",
    "start": "972779",
    "end": "978059"
  },
  {
    "text": "has to even if you're changing one thing generate the whole list which may be in a large cluster over 100 megabytes sync",
    "start": "978059",
    "end": "984660"
  },
  {
    "text": "it and then you continue the second thing is IP tables uses contract which",
    "start": "984660",
    "end": "990120"
  },
  {
    "text": "leads to a lot of right sizing issue right how much memory do you have to reserve for contract and we have seen",
    "start": "990120",
    "end": "996089"
  },
  {
    "text": "this especially challenging when you have DNS traffic which consumes a lot of contract entries finally just it's like",
    "start": "996089",
    "end": "1004850"
  },
  {
    "text": "a future proofing though this is basically whatever you see in Cooper Miley's in terms of load balancing is basically everything you could do with",
    "start": "1004850",
    "end": "1011120"
  },
  {
    "text": "the IP tables it's very hard to conceive of adding additional functionality when",
    "start": "1011120",
    "end": "1018949"
  },
  {
    "start": "1018000",
    "end": "1018000"
  },
  {
    "text": "we started our Ajani in' in cuban ideas we knew okay so we're going to be big in terms of number of nodes number of",
    "start": "1018949",
    "end": "1024918"
  },
  {
    "text": "endpoints and we knew we were gonna have issue with IP tables so from the get-go we decided to try a PDF because we we",
    "start": "1024919",
    "end": "1031339"
  },
  {
    "text": "assumed it was going to be a lot faster and a lot more efficient in terms of design the logical design is the same as",
    "start": "1031339",
    "end": "1036530"
  },
  {
    "text": "the same one as we used before and in terms of mapping in the umber of components accumulated service is mapped",
    "start": "1036530",
    "end": "1042380"
  },
  {
    "text": "to a virtual server in a previous and but as a real real server which is a back-end in a virtual server so as we",
    "start": "1042380",
    "end": "1051049"
  },
  {
    "text": "were saying before the main advantage here is that updates are atomic so your only chance I mean if you delete a pod",
    "start": "1051049",
    "end": "1057169"
  },
  {
    "text": "you own two days the backend and in a virtual server so it's much more efficient in",
    "start": "1057169",
    "end": "1063360"
  },
  {
    "start": "1063000",
    "end": "1063000"
  },
  {
    "text": "terms of how it looks this is the output of the IPS ADM command which is giving you information of what's configuring a",
    "start": "1063360",
    "end": "1069570"
  },
  {
    "text": "PPS and you can see that this is a service IP for an HTTP service and you",
    "start": "1069570",
    "end": "1076680"
  },
  {
    "text": "can see two backends using port 5000 this example ipbs has been working",
    "start": "1076680",
    "end": "1084660"
  },
  {
    "start": "1082000",
    "end": "1082000"
  },
  {
    "text": "pretty great for us I mean we've been using it for more than a year as I was saying but there was one chain that was",
    "start": "1084660",
    "end": "1089880"
  },
  {
    "text": "very important and that made things a bit tricky over the last few months I PPS used to",
    "start": "1089880",
    "end": "1095580"
  },
  {
    "text": "not support graceful termination at all which meant as soon as you delete a pod or and big a pod becomes not ready it's",
    "start": "1095580",
    "end": "1101820"
  },
  {
    "text": "remove of the backend which means no traffic is going to flow to this pod anymore which when you have an existing connection open you would want some time",
    "start": "1101820",
    "end": "1108990"
  },
  {
    "text": "for example file HTTP you would want for the end of the communication to go on which is a lot better so graceful",
    "start": "1108990",
    "end": "1114750"
  },
  {
    "text": "termination was introduced in 112 and back party to 111 and the first the",
    "start": "1114750",
    "end": "1119760"
  },
  {
    "text": "first version of it add a few bugs and but we're getting there it's a lot better now so if you have any issues with that PVS",
    "start": "1119760",
    "end": "1126510"
  },
  {
    "text": "please let us know up and issues and queue proxy will go fix it I mean we're really committed to fixing it one thing",
    "start": "1126510",
    "end": "1133110"
  },
  {
    "text": "that's important with that PVS is we have double contracting IP vs as its own connection tracking system and you also",
    "start": "1133110",
    "end": "1139740"
  },
  {
    "text": "rely on the contracting Bakula and it's a bit tricky because then I mean I'm Bobby mentioned issues with contract",
    "start": "1139740",
    "end": "1146220"
  },
  {
    "text": "sizing and timeout before so now we have to do it twice based on IP vs and in the in the normal",
    "start": "1146220",
    "end": "1152280"
  },
  {
    "text": "contract and the default time at for PBS are not great the figure there are four",
    "start": "1152280",
    "end": "1157470"
  },
  {
    "text": "TCP TCP fee which is when a connection in tab is dead terminating and the 300",
    "start": "1157470",
    "end": "1162540"
  },
  {
    "text": "so five minutes is UDP as you can imagine like keeping a connection the contract of five minutes when it's a DNS",
    "start": "1162540",
    "end": "1167940"
  },
  {
    "text": "query is very bad idea we also had a few issues with shinai because no CNI",
    "start": "1167940",
    "end": "1173640"
  },
  {
    "text": "plugins are tested with IP tables and routing with a PPS is slightly different so we noticed some movie interaction so",
    "start": "1173640",
    "end": "1179820"
  },
  {
    "text": "if you want write EVs make sure that you test everything with cyanide",
    "start": "1179820",
    "end": "1186710"
  },
  {
    "start": "1186000",
    "end": "1186000"
  },
  {
    "text": "so haproxy is one of the implementation of clans eyelid balancing there are several others and queuebrowser is one",
    "start": "1187780",
    "end": "1195430"
  },
  {
    "text": "of them so the brother is doing a lot of things if you if you know it it also allows you to control routing",
    "start": "1195430",
    "end": "1201160"
  },
  {
    "text": "between junior birds and it comes also with service load balance using IPPs and it's a different limitation another",
    "start": "1201160",
    "end": "1208000"
  },
  {
    "text": "option would be to use CDM which is using EBP F to do client-side load balancing and in addition to and as many",
    "start": "1208000",
    "end": "1213820"
  },
  {
    "text": "other features in addition to that okay so just to wrap up on the service load",
    "start": "1213820",
    "end": "1220150"
  },
  {
    "start": "1217000",
    "end": "1217000"
  },
  {
    "text": "balancing there's actually a couple of other exciting things going on so first is implementation of topology",
    "start": "1220150",
    "end": "1226930"
  },
  {
    "text": "aware service routing so what this means is like you can express things for example low load balance first to my",
    "start": "1226930",
    "end": "1234310"
  },
  {
    "text": "node and if that's not available then spread it out across the cluster and that's the link to the cap please take a",
    "start": "1234310",
    "end": "1240010"
  },
  {
    "text": "look at it it's pretty much implementable but we really would like feedback the second is I'm sure if you run in a",
    "start": "1240010",
    "end": "1247810"
  },
  {
    "text": "big cluster and run big services you'll notice that eventually your endpoints object will become super massive and no",
    "start": "1247810",
    "end": "1253900"
  },
  {
    "text": "longer fit in your at CD table so this project endpoint slice is a project to",
    "start": "1253900",
    "end": "1259810"
  },
  {
    "text": "take that endpoints object and shard it among several endpoint slices to get around this scalability issue so please",
    "start": "1259810",
    "end": "1267520"
  },
  {
    "text": "take a look at that cap as well that is an ongoing design the third topic we're",
    "start": "1267520",
    "end": "1274510"
  },
  {
    "text": "going to talk about is ingress traffic the first way to get traffic inside a cluster is to use a load balancer type",
    "start": "1274510",
    "end": "1281320"
  },
  {
    "start": "1277000",
    "end": "1277000"
  },
  {
    "text": "service in that case this is how it works so you create a load service of type load balancer and the service",
    "start": "1281320",
    "end": "1287710"
  },
  {
    "text": "controller in masters is going to pro to provision a load balancer on your cloud provider and this web browser is going",
    "start": "1287710",
    "end": "1294040"
  },
  {
    "text": "to send traffic to node balls and the typical flow of traffic is traffic is going to the load balancer",
    "start": "1294040",
    "end": "1299950"
  },
  {
    "text": "hitting the notebook on a node and then your proxy is going to take care of advancing the traffic to the actual pods",
    "start": "1299950",
    "end": "1305380"
  },
  {
    "text": "having the application it seems fine when you see it this way but the first thing is it's not very efficient because",
    "start": "1305380",
    "end": "1311290"
  },
  {
    "text": "it's very unlikely and large cluster that you're gonna hit a node where the service is actually located so you're",
    "start": "1311290",
    "end": "1317050"
  },
  {
    "text": "going to first hit a node and then be righted again to a new where the police action is is another",
    "start": "1317050",
    "end": "1322970"
  },
  {
    "text": "issue in that if you have large cluster with very different workloads for instance a Kafka or Cassandra cluster",
    "start": "1322970",
    "end": "1329240"
  },
  {
    "text": "you wouldn't want this node to get a part of web traffic for instance because it's rena deficient and you would want",
    "start": "1329240",
    "end": "1335030"
  },
  {
    "text": "another traffic to impact traffic of very critical data store an option is to",
    "start": "1335030",
    "end": "1341840"
  },
  {
    "start": "1340000",
    "end": "1340000"
  },
  {
    "text": "use external traffic policy local in which case q proxy is only going to send",
    "start": "1341840",
    "end": "1347330"
  },
  {
    "text": "traffic to local pods and it's going to fail the health checks if there's no pod on the service so this is much better",
    "start": "1347330",
    "end": "1353750"
  },
  {
    "text": "than before because you don't have to do the additional hub however you still have a few issues with the first one",
    "start": "1353750",
    "end": "1358940"
  },
  {
    "text": "that you have if you have hundreds or thousands of nodes all these nodes need to be registered with load balancer and",
    "start": "1358940",
    "end": "1365540"
  },
  {
    "text": "there are limits in terms of the number of nodes you can register with load balancer to give you an idea on AWS this",
    "start": "1365540",
    "end": "1371300"
  },
  {
    "text": "limit is 1000 there's no way you can have more than 1,000 instances attached to an ELB another one in an issue with",
    "start": "1371300",
    "end": "1378200"
  },
  {
    "text": "routing in the past is that health check are still going to be sent to all the pods which means this is going to be a",
    "start": "1378200",
    "end": "1383210"
  },
  {
    "text": "lot of traffic see already we were talking about load balancer service ol4",
    "start": "1383210",
    "end": "1390410"
  },
  {
    "text": "ingress another way to do to get traffic inside cluster is to use l7 ingresses so",
    "start": "1390410",
    "end": "1396410"
  },
  {
    "text": "the ingress object in kubernetes and it's actually exactly the same implementation when you when you look",
    "start": "1396410",
    "end": "1401870"
  },
  {
    "text": "and look about it the only difference in this slide is that this object are not",
    "start": "1401870",
    "end": "1407330"
  },
  {
    "text": "controlled by the service controller on the master but on the English controller that it's exactly the same flow and",
    "start": "1407330",
    "end": "1412550"
  },
  {
    "text": "exactly the same issues another option would be to use nginx as a proxy or H a",
    "start": "1412550",
    "end": "1420350"
  },
  {
    "start": "1415000",
    "end": "1415000"
  },
  {
    "text": "proxy for instance and in that case the right part of the study is much more efficient because this these pods are",
    "start": "1420350",
    "end": "1426650"
  },
  {
    "text": "going to be configured directly to road traffic to relevant pods so the traffic between the proxy and the prod is going to be very",
    "start": "1426650",
    "end": "1432290"
  },
  {
    "text": "efficient but as you can see on the left part of the slide you still need to get traffic to these proxies and so you need",
    "start": "1432290",
    "end": "1438830"
  },
  {
    "text": "usually a lot as a service in front with all the issues we mentioned before for",
    "start": "1438830",
    "end": "1444890"
  },
  {
    "start": "1444000",
    "end": "1444000"
  },
  {
    "text": "this reason we really believe that the future is Chris Lu native running in in this case blood bank server traffic",
    "start": "1444890",
    "end": "1451130"
  },
  {
    "text": "directly to Paddy peas and this is of course only possible if you put out native IPS in the V PC which is what we",
    "start": "1451130",
    "end": "1457789"
  },
  {
    "text": "were showing before there are several implementation of this design on AWS for",
    "start": "1457789",
    "end": "1463580"
  },
  {
    "start": "1461000",
    "end": "1461000"
  },
  {
    "text": "instance you have a controller called the alb ingress controller and which can be configured to route traffic directly",
    "start": "1463580",
    "end": "1469309"
  },
  {
    "text": "to pods instead of instances so it's much more efficient the way it works is it's creating a lb and the target groups",
    "start": "1469309",
    "end": "1475610"
  },
  {
    "text": "directly directly targeting targeting IPs and GCP load balancers can be",
    "start": "1475610",
    "end": "1480830"
  },
  {
    "text": "configured to use the network endpoint groups which is doing going to do exactly the same and write traffic directly to the pods proxy also do it as",
    "start": "1480830",
    "end": "1488659"
  },
  {
    "text": "I was saying before but of course you still need to put something in front and it's probably going to be either based",
    "start": "1488659",
    "end": "1493700"
  },
  {
    "text": "on network endpoint groups or alb in that example okay so now you notice that",
    "start": "1493700",
    "end": "1502070"
  },
  {
    "start": "1497000",
    "end": "1497000"
  },
  {
    "text": "it's those cases cover some of the workloads surrounding elf seven in",
    "start": "1502070",
    "end": "1508130"
  },
  {
    "text": "ingress but you know what about l4 and ELB so currently there doesn't seem to",
    "start": "1508130",
    "end": "1514850"
  },
  {
    "text": "be a solution there and it's also limited to HTTP traffic now we want to",
    "start": "1514850",
    "end": "1520190"
  },
  {
    "text": "consider UDP or TCP traffic and so this is kind of a work in progress to get",
    "start": "1520190",
    "end": "1525710"
  },
  {
    "text": "container native load balancing for all the different types of load balancing that we need finally let's talk about",
    "start": "1525710",
    "end": "1532520"
  },
  {
    "text": "DNS challenges so this is a lot of people run into this so kind of this is",
    "start": "1532520",
    "end": "1539929"
  },
  {
    "start": "1539000",
    "end": "1539000"
  },
  {
    "text": "a summary of the DEA's challenges that people have seen so you have an application let's say it's in nodejs or",
    "start": "1539929",
    "end": "1545330"
  },
  {
    "text": "ruby and it does is get post by name well hopefully the recursive variant it",
    "start": "1545330",
    "end": "1553970"
  },
  {
    "text": "has a search path which means that the number of queries actually gets amplified quite a bit and we hit the",
    "start": "1553970",
    "end": "1559880"
  },
  {
    "text": "contract table because keep DNS is going to be a service so we may run out of contract entries now also DNS is a UDP",
    "start": "1559880",
    "end": "1567110"
  },
  {
    "text": "protocol and who knows where that UDP packet went so we might have lost it going to the query and finally because",
    "start": "1567110",
    "end": "1574190"
  },
  {
    "text": "of this search path amplification you may not have enough instances to soak up all this load so there's a whole",
    "start": "1574190",
    "end": "1582410"
  },
  {
    "text": "our problems associated with the volume of DNS queries that we see in kubernetes",
    "start": "1582410",
    "end": "1588670"
  },
  {
    "text": "so one of the solutions to this problem is to actually drop a no local cache",
    "start": "1588670",
    "end": "1595090"
  },
  {
    "start": "1589000",
    "end": "1589000"
  },
  {
    "text": "into your cluster so the no local cache runs on every single node and it can",
    "start": "1595090",
    "end": "1600230"
  },
  {
    "text": "first of all as a cache so it can absorb some of the queries right away the second thing that it does is it",
    "start": "1600230",
    "end": "1605360"
  },
  {
    "text": "avoids contract it actually inserts special rules into your IP tables chains",
    "start": "1605360",
    "end": "1610990"
  },
  {
    "text": "to basically skip contract and go directly to the no local cache and finally it upgrades all your connections",
    "start": "1610990",
    "end": "1617510"
  },
  {
    "text": "instead of sending UDP it sends TCP which is a lot more reliable and then finally hopefully it has reduced load",
    "start": "1617510",
    "end": "1623480"
  },
  {
    "text": "anyways so then you don't have these scalability problems with the number of instances another interesting thing and",
    "start": "1623480",
    "end": "1630560"
  },
  {
    "start": "1629000",
    "end": "1629000"
  },
  {
    "text": "this is a ongoing cap is that why don't we take care of that search path issue",
    "start": "1630560",
    "end": "1636200"
  },
  {
    "text": "so no local cache is a smart agent we can put some logic in there and it can",
    "start": "1636200",
    "end": "1641630"
  },
  {
    "text": "actually coordinate with the upstream cube DNS infrastructure what if we have the node local cache insert that search",
    "start": "1641630",
    "end": "1648020"
  },
  {
    "text": "path somehow into the DNS query perhaps as a a DNS field and then actually the",
    "start": "1648020",
    "end": "1654710"
  },
  {
    "text": "rural search path mechanism could be resolved upstream inside the server and you won't have to send all those packets",
    "start": "1654710",
    "end": "1660410"
  },
  {
    "text": "over the wire now I'm just going to",
    "start": "1660410",
    "end": "1666680"
  },
  {
    "start": "1664000",
    "end": "1664000"
  },
  {
    "text": "describe you the the way we do DNS and data Doug and it's a lot inspired by the node local cache that Bob mentioned just",
    "start": "1666680",
    "end": "1672470"
  },
  {
    "text": "before so this is the typical way we do DNS a data log and a very standard one",
    "start": "1672470",
    "end": "1678830"
  },
  {
    "text": "so this is a default configuration the qubit is going to inject search domains the end option which governs when",
    "start": "1678830",
    "end": "1686690"
  },
  {
    "text": "there's going to be an expansion of the of the searches and so this works fine",
    "start": "1686690",
    "end": "1692090"
  },
  {
    "text": "but the when I put wants to do in DNS query it's going to contact the DNS",
    "start": "1692090",
    "end": "1697940"
  },
  {
    "text": "service and it's going to use that PPS and it's going to go to the main system node are all the nodes running your Co",
    "start": "1697940",
    "end": "1704090"
  },
  {
    "text": "DNS buds and then the query is going to come from there and if the Cordilleras",
    "start": "1704090",
    "end": "1709310"
  },
  {
    "text": "doesn't know about this service is going to fold it upstream as we were saying",
    "start": "1709310",
    "end": "1714500"
  },
  {
    "text": "before this this led to this leads to a few issue and we have we had quite a few outages for this design so for application that",
    "start": "1714500",
    "end": "1721279"
  },
  {
    "text": "do a lot of queries and I'm talking about title of query per seconds the design is pretty inefficient so we",
    "start": "1721279",
    "end": "1728089"
  },
  {
    "text": "introduced an alternative that's opted in so people can just add an annotation on their part and we have a mutating web",
    "start": "1728089",
    "end": "1734029"
  },
  {
    "text": "hook that's going to modify the configuration the DNS configuration of the pod and you can see here the",
    "start": "1734029",
    "end": "1740029"
  },
  {
    "text": "configuration of the pod so we do two important things the first one is we have a single search domain which is",
    "start": "1740029",
    "end": "1745909"
  },
  {
    "text": "going to be SVC that cluster dot local which means apt even in the single in the same name space application we'll",
    "start": "1745909",
    "end": "1752210"
  },
  {
    "text": "need to put service dot name space they can't use just service so that's the reason it's obtained it's because it's",
    "start": "1752210",
    "end": "1757429"
  },
  {
    "text": "breaking some of the kubernetes contracts but it's much more efficient because most of the time it's going to be in single query in addition to that",
    "start": "1757429",
    "end": "1764210"
  },
  {
    "text": "because we are very opinionated in the way we do search domains we can go back to n dot equal to which means if you do",
    "start": "1764210",
    "end": "1770450"
  },
  {
    "text": "a standard query like wwe.com this has two dots which means has not gonna be",
    "start": "1770450",
    "end": "1775460"
  },
  {
    "text": "expansion and traffic can be it has not gonna be it's just going to be asked could do read google.com and not ww that",
    "start": "1775460",
    "end": "1782690"
  },
  {
    "text": "google.com that service after local which is typical expansion we modify",
    "start": "1782690",
    "end": "1788899"
  },
  {
    "text": "also the resolver in point to a node local DNS cache okay so it's a very small coordinates instance running",
    "start": "1788899",
    "end": "1794899"
  },
  {
    "text": "locally and this instance that's to think it does caching so if you hit the",
    "start": "1794899",
    "end": "1800570"
  },
  {
    "text": "same name a lot you can a bit better table caching is gonna be a lot more efficient and in addition it writes",
    "start": "1800570",
    "end": "1806419"
  },
  {
    "text": "traffic that is not resolvable by coordinates natively in the in the cluster directly to the upstream and locally so",
    "start": "1806419",
    "end": "1813409"
  },
  {
    "text": "if the query doesn't hang with cluster dot local it's directly routed to the BBC resolver in our case so it's much",
    "start": "1813409",
    "end": "1819889"
  },
  {
    "text": "more efficient and and we've had quite quite good results with it and the underworld will probably gonna switch",
    "start": "1819889",
    "end": "1826039"
  },
  {
    "text": "from opt-in behavior for this behavior to this behavior by default with opt-out for application that counts work with it",
    "start": "1826039",
    "end": "1834249"
  },
  {
    "text": "okay so in conclusion",
    "start": "1834820",
    "end": "1838749"
  },
  {
    "start": "1837000",
    "end": "1837000"
  },
  {
    "text": "really a common theme is you know removing the cost of all the",
    "start": "1840040",
    "end": "1845049"
  },
  {
    "text": "abstractions right native integrations with the infrastructure is key so a lot of these things that we described it's",
    "start": "1845049",
    "end": "1850660"
  },
  {
    "text": "like how do we get things to route directly how do we remove overlays and so forth one thing to observe as many",
    "start": "1850660",
    "end": "1857200"
  },
  {
    "text": "caps in flight to improve scalability and then the third thing is actually there are many interesting future",
    "start": "1857200",
    "end": "1863200"
  },
  {
    "text": "technologies that haven't been explored yet in terms of the implementation so for example EBP F maybe some integration",
    "start": "1863200",
    "end": "1869080"
  },
  {
    "text": "with the service mesh and then in general I don't know if you attended any of the scalability talks as well but the",
    "start": "1869080",
    "end": "1875410"
  },
  {
    "text": "idea and the where the community wants to go is to just make sure everything scales by default out of the box I think",
    "start": "1875410",
    "end": "1881280"
  },
  {
    "text": "one of the scalability talks talked a lot about SL O's it's like how do we get",
    "start": "1881280",
    "end": "1887110"
  },
  {
    "text": "the implementation to such a state that we can kind of give you an SLO that you can rely on even though your cluster is",
    "start": "1887110",
    "end": "1893169"
  },
  {
    "text": "$5000 $10000 with that you end up our talk",
    "start": "1893169",
    "end": "1899669"
  },
  {
    "text": "and by the way thanks for staying because this is the last talk on the last day and if you want I think we have",
    "start": "1906210",
    "end": "1912779"
  },
  {
    "text": "a few minutes for questions and we're gonna stay around anyway for a bit longer",
    "start": "1912779",
    "end": "1917990"
  },
  {
    "text": "and what with the native approach do you find like very rapid scaling and like",
    "start": "1926240",
    "end": "1931590"
  },
  {
    "text": "large rapid scaling up and down under the cluster and a lot of movement departs to sort of cause additional",
    "start": "1931590",
    "end": "1936720"
  },
  {
    "text": "problems so are you like a bit AWS specific ok so yes I mean we don't have",
    "start": "1936720",
    "end": "1945570"
  },
  {
    "text": "the issue that much because we don't increase the cluster size very fast but yes a way of boosting work it they",
    "start": "1945570",
    "end": "1952380"
  },
  {
    "text": "locally try and get an address and so you can get rate limited by the other escapee is so yeah I think this can be",
    "start": "1952380",
    "end": "1959429"
  },
  {
    "text": "improved in both the AWS and the lift plugin but I it hasn't been a problem for us but we know we've talked to",
    "start": "1959429",
    "end": "1965580"
  },
  {
    "text": "people that have had the issue too and I think one of the way to solve it is to actually pre locate ip's and/or a bunch",
    "start": "1965580",
    "end": "1972299"
  },
  {
    "text": "of IPS to avoid all the instances doing the same thing at the same time I can answer this question for the network",
    "start": "1972299",
    "end": "1978539"
  },
  {
    "text": "temporal groups actually we had interesting a technical challenge in terms of trying to batch things together",
    "start": "1978539",
    "end": "1985559"
  },
  {
    "text": "so they can go fast the code is actually open source if you want to check it out",
    "start": "1985559",
    "end": "1991130"
  },
  {
    "text": "because I do link any other questions yes Chris as mentioned part of the",
    "start": "1991130",
    "end": "2001549"
  },
  {
    "text": "iptables problem is like the N squared ever-present networking problem and a linear search and IP tables how's this",
    "start": "2001549",
    "end": "2007669"
  },
  {
    "text": "helped by moving that into the load balancer or or the neg implementation is",
    "start": "2007669",
    "end": "2014690"
  },
  {
    "text": "just a different data structure that does it require the linear search or you're just scale it out a different way",
    "start": "2014690",
    "end": "2023200"
  },
  {
    "text": "well you don't need that linear search because it knows the direct back end so",
    "start": "2023230",
    "end": "2028549"
  },
  {
    "text": "it can kind of I mean it can run up smarter data structure than just a linear list that's one answer I think",
    "start": "2028549",
    "end": "2037279"
  },
  {
    "text": "that's like a big answer actually because IP tables you you can use IP sets but that's not currently the",
    "start": "2037279",
    "end": "2043460"
  },
  {
    "text": "implementation if you had IP sets actually IP tables certain aspects of it don't aren't linear anymore but that",
    "start": "2043460",
    "end": "2049908"
  },
  {
    "text": "just doesn't exist right now",
    "start": "2049909",
    "end": "2053020"
  },
  {
    "text": "it depends although we'd have to look to see which parts that would be relevant",
    "start": "2054980",
    "end": "2060179"
  },
  {
    "text": "for because I don't know if you can do that with the service like the service",
    "start": "2060179",
    "end": "2065940"
  },
  {
    "text": "lookup for IP to could clearly be in a hash table but with IP tables you have to put in a list and I think that there",
    "start": "2065940",
    "end": "2074310"
  },
  {
    "text": "are there have been a few discussion how how to shop the a table chain but I think it's I mean it's been discussed",
    "start": "2074310",
    "end": "2080850"
  },
  {
    "text": "but it's I don't think that's any implementation ready so regarding the",
    "start": "2080850",
    "end": "2089398"
  },
  {
    "text": "same IP tables issue during the presentation there was a it was",
    "start": "2089399",
    "end": "2095850"
  },
  {
    "text": "mentioned that for ITP tables you need to write a lot of long rules to route",
    "start": "2095850",
    "end": "2101730"
  },
  {
    "text": "from to throughout the network between two ports and have you thought like",
    "start": "2101730",
    "end": "2108090"
  },
  {
    "text": "using this service for doing that for storing all the rules and how have you",
    "start": "2108090",
    "end": "2115110"
  },
  {
    "text": "tried to use open the switch for that oh",
    "start": "2115110",
    "end": "2121820"
  },
  {
    "text": "I see that's an alternative implementation that I guess we haven't covered it could completely replace IP",
    "start": "2121820",
    "end": "2129300"
  },
  {
    "text": "tables so in some sense you're talking about a separate implementation what was your",
    "start": "2129300",
    "end": "2134580"
  },
  {
    "text": "first question",
    "start": "2134580",
    "end": "2137090"
  },
  {
    "text": "I see the the issue that was illustrated here so the question is have you thought",
    "start": "2142410",
    "end": "2148080"
  },
  {
    "text": "about using a separate service for storing the rules the issue that was brought up was more that the kernel",
    "start": "2148080",
    "end": "2153180"
  },
  {
    "text": "interface to change IP tables is actually all just one shot like you have to generate the whole thing and put it",
    "start": "2153180",
    "end": "2159030"
  },
  {
    "text": "in even though your change might be very small so that turned out to be very",
    "start": "2159030",
    "end": "2164420"
  },
  {
    "text": "inefficient because if you have a lot of endpoints a lot of services you kind of have to regenerate that thing over and",
    "start": "2164420",
    "end": "2170460"
  },
  {
    "text": "over again if you cashed it you would still have the problem that you'd have to shove it through to the kernel repeatedly even though you still have",
    "start": "2170460",
    "end": "2176850"
  },
  {
    "text": "the thing generated any other questions",
    "start": "2176850",
    "end": "2182870"
  },
  {
    "text": "in the on-prem scenario it's not always an option to be able to use the Royal architecture IP addresses and overlays",
    "start": "2183710",
    "end": "2192180"
  },
  {
    "text": "it seems to be the only solution unless going for another address space right and ipv6 so given what you know about",
    "start": "2192180",
    "end": "2199310"
  },
  {
    "text": "communities at scale is ipv6 an option to get those things I cannot answer your",
    "start": "2199310",
    "end": "2206130"
  },
  {
    "text": "direct question but luckily the dual stack kept is in a pretty good shape so hopefully just work on ipv6 soon and for",
    "start": "2206130",
    "end": "2217740"
  },
  {
    "text": "the unprimed squishin most of the people I know but I'm really not a specialist what they do is they do native routing",
    "start": "2217740",
    "end": "2224160"
  },
  {
    "text": "they have BGP so either a browser or calico running on the node and all this",
    "start": "2224160",
    "end": "2229860"
  },
  {
    "text": "during peering I mean two older BGP routers and announcing route to the",
    "start": "2229860",
    "end": "2234930"
  },
  {
    "text": "network in which case you don't in another light if you can't do that because in many companies you can't actually do that then you have to use an",
    "start": "2234930",
    "end": "2241500"
  },
  {
    "text": "array but yeah if you can like try a negotiated with the network team to make it happen it's going to be a lot more",
    "start": "2241500",
    "end": "2248070"
  },
  {
    "text": "efficient okay thank you for a great talk a hell questions the question just",
    "start": "2248070",
    "end": "2254190"
  },
  {
    "text": "in case did you try playing with any tables this new fancy firewall subsystem",
    "start": "2254190",
    "end": "2260730"
  },
  {
    "text": "in yes so someone mentioned nf tables and it definitely solves some of the IP tables problems I would love to see an",
    "start": "2260730",
    "end": "2268140"
  },
  {
    "text": "implementation so if anyone is interested I'm pretty sure that they would be interested in",
    "start": "2268140",
    "end": "2275390"
  },
  {
    "text": "one of the biggest issue in the cube proxy contracts today is pretty",
    "start": "2275390",
    "end": "2280410"
  },
  {
    "text": "complicated as we've seen on the I mean I've worked a little Mia PBS implementation and the contract",
    "start": "2280410",
    "end": "2286200"
  },
  {
    "text": "established by the de facto default which is IP labels is very complicated and getting this right in a new",
    "start": "2286200",
    "end": "2292650"
  },
  {
    "text": "implementation may be tricky it's gonna be a lot of work so I mean if someone started I'm sure the community would",
    "start": "2292650",
    "end": "2298230"
  },
  {
    "text": "love to try and use it but to be honest nf table is not a load balancing solution either so it's gonna handle",
    "start": "2298230",
    "end": "2304200"
  },
  {
    "text": "please it's gonna be better than IP tables but it's not going to be a client-side load balancer the future is",
    "start": "2304200",
    "end": "2309720"
  },
  {
    "text": "probably ebps based for for this for this entire thing so you're using IP vs for a year",
    "start": "2309720",
    "end": "2316799"
  },
  {
    "text": "right yes and there any reason not to use IP vs now I mean it's not the default solution so what what are the",
    "start": "2316799",
    "end": "2323460"
  },
  {
    "text": "kind of drawbacks this quest for termination was a major blocker I would say what no it's apparently solved",
    "start": "2323460",
    "end": "2329819"
  },
  {
    "text": "I'd say today for most use cases IPS is stable there's a few issues up an",
    "start": "2329819",
    "end": "2335220"
  },
  {
    "text": "uptrend but nothing in terms of features so there's one where this communication",
    "start": "2335220",
    "end": "2340529"
  },
  {
    "text": "behavior is slightly different in IP vs in IP tables for instance you can do localhost note ports this doesn't work",
    "start": "2340529",
    "end": "2348109"
  },
  {
    "text": "all that most of the teamwork the only remaining issues are related to graceful",
    "start": "2348109",
    "end": "2354539"
  },
  {
    "text": "termination and we decided two weeks ago to remove gradual termination for UDP",
    "start": "2354539",
    "end": "2359609"
  },
  {
    "text": "which is going to solve most of the ongoing issues and since the IP tables implementation is not supporting restful",
    "start": "2359609",
    "end": "2366450"
  },
  {
    "text": "termination for UDP either is going to be the same behavior so we had a few people that wanted graceful termination",
    "start": "2366450",
    "end": "2371579"
  },
  {
    "text": "for UDP for quick for instance but it's very hard to get right so we decided to",
    "start": "2371579",
    "end": "2377010"
  },
  {
    "text": "go exactly as IP tables and remove that automation for UDP which games it's going to solve most of the remaining",
    "start": "2377010",
    "end": "2383069"
  },
  {
    "text": "issues and once again if you try it and have an issue create an issue and on the",
    "start": "2383069",
    "end": "2388230"
  },
  {
    "text": "repo and we can look into it yeah exactly that so I PVS I know some people",
    "start": "2388230",
    "end": "2393539"
  },
  {
    "text": "had some issues because it puts the IP address on an interface and that ended up capturing traffic",
    "start": "2393539",
    "end": "2399619"
  },
  {
    "text": "given how they're seen I set up so really it's like you have to it hasn't been battle tested so it just",
    "start": "2399619",
    "end": "2405509"
  },
  {
    "text": "needs wider usage and then you can get the feedback what's that",
    "start": "2405509",
    "end": "2412729"
  },
  {
    "text": "well it's battle tested for a vertical slice nobody yes I mean most of the new issues",
    "start": "2412729",
    "end": "2419549"
  },
  {
    "text": "are related to edge cases that we don't know about for instance we had a lot of discussion with people from metal lb",
    "start": "2419549",
    "end": "2425989"
  },
  {
    "text": "which does pretty clever saying that no one in the IPV Aseema tried before and",
    "start": "2425989",
    "end": "2431160"
  },
  {
    "text": "so we had to solve this for instance so it's like specific use cases specific CNI implementations so I'm sure we're",
    "start": "2431160",
    "end": "2438180"
  },
  {
    "text": "gonna get it fixed but we need more users to do to be sure that we're covering everything one more question",
    "start": "2438180",
    "end": "2449390"
  },
  {
    "text": "so you mentioned on enemy Nestor to seen hi plugins official AWS one which also",
    "start": "2449779",
    "end": "2455729"
  },
  {
    "text": "is used by you case and you are using the lift one yes do you have any elaborate comparison what are the pros",
    "start": "2455729",
    "end": "2462809"
  },
  {
    "text": "and cons or no way to fuentes and why's it obvious creating a new one and so we",
    "start": "2462809",
    "end": "2470009"
  },
  {
    "text": "started with the abrasion i plug in because it seems to be the most natural thing to do the first implementation was",
    "start": "2470009",
    "end": "2477029"
  },
  {
    "text": "very young when we started it had quite a few bug and we had to thick them and",
    "start": "2477029",
    "end": "2482789"
  },
  {
    "text": "the interaction with the team developing the plugin was bit difficult at the time",
    "start": "2482789",
    "end": "2488369"
  },
  {
    "text": "so we decided to try lifted the lid implementation and it turned out this implementation was pretty solid and it",
    "start": "2488369",
    "end": "2495779"
  },
  {
    "text": "was very easy to get things patched upstream so that's why we went this way in addition in terms of design its",
    "start": "2495779",
    "end": "2502109"
  },
  {
    "text": "demoness which we find a lot better because it's simpler to just chain genell binaries on the host and and",
    "start": "2502109",
    "end": "2509789"
  },
  {
    "text": "honestly I mean we've been running the live Chennai plug-in for more than a year now without any problem oh we fixed",
    "start": "2509789",
    "end": "2520019"
  },
  {
    "text": "a few things along the way but okay thank you all for coming",
    "start": "2520019",
    "end": "2526530"
  },
  {
    "text": "[Applause]",
    "start": "2526530",
    "end": "2532900"
  },
  {
    "text": "[Music]",
    "start": "2532900",
    "end": "2536650"
  }
]