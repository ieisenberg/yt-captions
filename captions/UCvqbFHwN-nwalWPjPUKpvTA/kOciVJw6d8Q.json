[
  {
    "text": "uh hi and Welcome to our talk about patterns of multic claster in kubernetes it's awesome to see so many people we",
    "start": "320",
    "end": "5920"
  },
  {
    "text": "weren't necessarily expecting a packed room uh but it's really cool I'm Dan mcken I'm a product manager at mongodb",
    "start": "5920",
    "end": "12719"
  },
  {
    "text": "and I'm George harez I'm an engineering director at mongodb together we lead uh mongodb's effort to support kubernetes",
    "start": "12719",
    "end": "21000"
  },
  {
    "text": "specifically through kubernetes operators um and you probably have spotted that there's a third person in",
    "start": "21000",
    "end": "27560"
  },
  {
    "text": "the presentation but only two people here uh um our colleague Mira he's contributed a lot of information and",
    "start": "27560",
    "end": "33719"
  },
  {
    "text": "insight to this presentation um he couldn't make it today so this is our",
    "start": "33719",
    "end": "38879"
  },
  {
    "text": "way of um of crediting his contributions both in our multicluster journey as a",
    "start": "38879",
    "end": "45600"
  },
  {
    "text": "product and as a presentation um he's also following closely the the",
    "start": "45600",
    "end": "51239"
  },
  {
    "text": "multicluster Sig um and there's obviously a lot of a lot of overlap of the topics discussed there and what",
    "start": "51239",
    "end": "57440"
  },
  {
    "text": "we're going to present here but um we're going to try and keep this through our perspective and try to um to tie Theory",
    "start": "57440",
    "end": "64878"
  },
  {
    "text": "with kind of our decisions on on multicluster yeah we're not claiming to be absolute experts and if you are a",
    "start": "64879",
    "end": "70960"
  },
  {
    "text": "member of the SE please don't raise your hand we don't want to be even more nervous uh so mongodb's interest in",
    "start": "70960",
    "end": "77520"
  },
  {
    "text": "multicluster kubernetes started a few years ago um before either of us actually joined the company um it came",
    "start": "77520",
    "end": "84439"
  },
  {
    "text": "about because we had customers wanting to run mongodb within kubernetes across multiple clusters",
    "start": "84439",
    "end": "90439"
  },
  {
    "text": "uh this was something we already supported with mongodb itself on kind of VMS or or bare metal uh but we didn't",
    "start": "90439",
    "end": "96240"
  },
  {
    "text": "yet support it on kubernetes and although a few did kind of go ahead and build it themselves the time and the",
    "start": "96240",
    "end": "102159"
  },
  {
    "text": "complexity to do so made us realize we needed to do it formally and properly um",
    "start": "102159",
    "end": "107280"
  },
  {
    "text": "so that's kind of where it came about so um we've come we've come a long way",
    "start": "107280",
    "end": "112600"
  },
  {
    "text": "since then and and we've invested a lot of time and effort we've reviewed uh We've Revisited a lot of our decisions",
    "start": "112600",
    "end": "119600"
  },
  {
    "text": "so so in this presentations we're going to try to kind of squeeze in all of the",
    "start": "119600",
    "end": "124640"
  },
  {
    "text": "all of this experience in in this half an hour so we're going to cover a few",
    "start": "124640",
    "end": "130520"
  },
  {
    "text": "things today and to try to keep it interest interesting we're going to keep on alterating between Theory and and our",
    "start": "130520",
    "end": "137760"
  },
  {
    "text": "decisions and what we are doing so we're going to we're going to present some Concepts and",
    "start": "137760",
    "end": "143400"
  },
  {
    "text": "considerations um before showing you how we chose to implement them so for that we're going to look at our own",
    "start": "143400",
    "end": "150400"
  },
  {
    "text": "multicluster implementation in the um kubernetes operator for mongodb",
    "start": "150400",
    "end": "155480"
  },
  {
    "text": "enterprise we wanted to hold our hands up at this point and admit that while it's not all open source at the moment",
    "start": "155480",
    "end": "162599"
  },
  {
    "text": "we are uh we are heading toward that direction as you can see it's quite a",
    "start": "162599",
    "end": "168200"
  },
  {
    "text": "packed agenda and in all honesty each of these topics could easily have been a 35 minute talk by themselves so we're going",
    "start": "168200",
    "end": "174200"
  },
  {
    "text": "to do our best to give you an overview and kind of explain our experiences our perspective with some time for Q&A at",
    "start": "174200",
    "end": "179280"
  },
  {
    "text": "the end uh we're going to start with the motivations and benefit for multicluster talk a little bit about the",
    "start": "179280",
    "end": "184640"
  },
  {
    "text": "architectures um before kind of digging into the network Centric pattern which we'll explain in a minute we'll talk",
    "start": "184640",
    "end": "190599"
  },
  {
    "text": "about each of the three key challenges that kind of revolve around uh implementing multicluster cluster",
    "start": "190599",
    "end": "196680"
  },
  {
    "text": "inventory workload distribution across clusters and networking uh and then",
    "start": "196680",
    "end": "202360"
  },
  {
    "text": "we'll cover the considerations for multicluster operators and controllers before finishing off with our own",
    "start": "202360",
    "end": "208120"
  },
  {
    "text": "thoughts on this topic so multicluster offers a number of",
    "start": "208120",
    "end": "215360"
  },
  {
    "text": "benefits improveed performance and reduce latency by hosting Services closer to the end users companies can",
    "start": "215360",
    "end": "222159"
  },
  {
    "text": "significantly improve the application performance and the user experience by reducing",
    "start": "222159",
    "end": "228360"
  },
  {
    "text": "latency compliance and regulation some regions have strict data sovereignty",
    "start": "228360",
    "end": "233560"
  },
  {
    "text": "rules that that dictate where data must be stored uh some even require that data has to be stored in a certain way way",
    "start": "233560",
    "end": "240079"
  },
  {
    "text": "such as different clouds to ensure that the data is never lost and finally and",
    "start": "240079",
    "end": "245120"
  },
  {
    "text": "probably the biggest and most obvious reason for most companies is the high availability disaster recovery and data",
    "start": "245120",
    "end": "251959"
  },
  {
    "text": "redundancy hosting both the services and the data across location ensures that failure at one has minimal",
    "start": "251959",
    "end": "260600"
  },
  {
    "text": "impact so um we wanted to dig a little bit deeper in terms of mongodb itself to",
    "start": "260600",
    "end": "267160"
  },
  {
    "text": "illustrate how this actually works in practice and and why uh we decided",
    "start": "267160",
    "end": "272479"
  },
  {
    "text": "multicluster made sense so mongodb offers two topologies that work well with multicluster uh replica sets that",
    "start": "272479",
    "end": "279360"
  },
  {
    "text": "offer data redundancy and high availability by maintaining multiple lances of the same",
    "start": "279360",
    "end": "287160"
  },
  {
    "text": "data and um sharded clusters that split the data into shards and each Shard is",
    "start": "288000",
    "end": "293240"
  },
  {
    "text": "effectively a replica set this ensures that each Shard has",
    "start": "293240",
    "end": "299600"
  },
  {
    "text": "redundancy and that the data can be split um for performance or geographical distribution so both replica sets and",
    "start": "299600",
    "end": "307160"
  },
  {
    "text": "sharded clusters are supported by mongodb Enterprise uh operator and with",
    "start": "307160",
    "end": "312240"
  },
  {
    "text": "replica sets directly supported at the moment and for multic clusters and sharded clusters coming soon so that",
    "start": "312240",
    "end": "319560"
  },
  {
    "text": "gives geographic distribution of data across different kubernetes clusters that can be in different availability",
    "start": "319560",
    "end": "325720"
  },
  {
    "text": "zone different data centers or even different Cloud providers and once stting is supported that gives the",
    "start": "325720",
    "end": "331520"
  },
  {
    "text": "ability to segment data and and localize data in a specific kubernetes cluster we",
    "start": "331520",
    "end": "337240"
  },
  {
    "text": "were able to get a lot of a leg up by the fact that mongodb already has these capabilities we just had to make it work",
    "start": "337240",
    "end": "343240"
  },
  {
    "text": "within kubernetes and when it came to multicluster there's two major architectural models um though each has",
    "start": "343240",
    "end": "350759"
  },
  {
    "text": "a lot of there's a lot in each about different ways of doing things so on the",
    "start": "350759",
    "end": "355800"
  },
  {
    "text": "left we have the kubernetes Centric model kind of more often known as cluster Federation uh it's all about",
    "start": "355800",
    "end": "362160"
  },
  {
    "text": "establishing some sort of shared control plane where the goal is to enable developers and operators uh to treat the",
    "start": "362160",
    "end": "368680"
  },
  {
    "text": "multiple clusters as one when deploying managing workloads uh to do it you have a federation control plane and at a high",
    "start": "368680",
    "end": "375160"
  },
  {
    "text": "level that's that's kind of two main components there's the API server to provide a single unified API uh point",
    "start": "375160",
    "end": "381800"
  },
  {
    "text": "for users to interact with the Federation and manage workloads and a controller manager that actually manages",
    "start": "381800",
    "end": "388319"
  },
  {
    "text": "the life cycle of f ated resources now on on the other side we we",
    "start": "388319",
    "end": "393919"
  },
  {
    "text": "have what we call the network Cent Centric model the concept there is Distributing uh workloads across",
    "start": "393919",
    "end": "400599"
  },
  {
    "text": "multiple distinct clusters um Each of which has its own control plane now",
    "start": "400599",
    "end": "407400"
  },
  {
    "text": "kubernetes operators deploying and managing workloads on the Clusters have to interact with with each cluster",
    "start": "407400",
    "end": "414039"
  },
  {
    "text": "independently uh using each of the API servers in the different clusters but",
    "start": "414039",
    "end": "419080"
  },
  {
    "text": "the underlying clusters are managed individually by the administrator so we",
    "start": "419080",
    "end": "424360"
  },
  {
    "text": "we don't try to to provide the view of one single cluster so while neither is",
    "start": "424360",
    "end": "429520"
  },
  {
    "text": "simple of the two models when it comes to actually deploying and managing workloads Federation uh simplifies",
    "start": "429520",
    "end": "435720"
  },
  {
    "text": "things by allowing you to treat everything as one cluster and and the",
    "start": "435720",
    "end": "440919"
  },
  {
    "text": "multicluster uh tooling helps you uh helps you kind of put all the complexity",
    "start": "440919",
    "end": "447840"
  },
  {
    "text": "under H the hood on the other other hand distinct clusters can offer more resiliency um as one cluster failing has",
    "start": "447840",
    "end": "454879"
  },
  {
    "text": "no direct impact in in the rest of your infrastructure now in in our case we're",
    "start": "454879",
    "end": "461759"
  },
  {
    "text": "focused on Distributing workloads across distinct uh kubernetes cluster to support users who've not yet deployed",
    "start": "461759",
    "end": "468840"
  },
  {
    "text": "Federated Solutions Federation Solutions and who want the increased resiliency",
    "start": "468840",
    "end": "474039"
  },
  {
    "text": "for their data of totally separate clusters so the network Centric model is",
    "start": "474039",
    "end": "479080"
  },
  {
    "text": "the primary focus of of the next slides as",
    "start": "479080",
    "end": "483720"
  },
  {
    "text": "well so the network set network Centric or workload distribution comes with its",
    "start": "484479",
    "end": "491000"
  },
  {
    "text": "sets of patents and and Concepts so the Hub cluster pattern",
    "start": "491000",
    "end": "497680"
  },
  {
    "text": "refers to a model where you have one Central kubernetes cluster typically known as The Hub managing one or more",
    "start": "497680",
    "end": "503720"
  },
  {
    "text": "addition additional member or spoke clusters we're mostly going to call them member clusters through the rest of this",
    "start": "503720",
    "end": "508800"
  },
  {
    "text": "talk in this setup something in the hub cluster usually a kubernetes operator",
    "start": "508800",
    "end": "514880"
  },
  {
    "text": "and controllers is responsible for centralizing the management tasks and acting as the multicluster control plane",
    "start": "514880",
    "end": "521159"
  },
  {
    "text": "for workloads running across the member clusters this includes deploying applications managing the workloads",
    "start": "521159",
    "end": "527560"
  },
  {
    "text": "setting Global configuration potentially Distributing shared Secrets controlling access integrating uh into and",
    "start": "527560",
    "end": "534720"
  },
  {
    "text": "leveraging cicd pipelines centralized monitoring and logging and more more",
    "start": "534720",
    "end": "540440"
  },
  {
    "text": "potentially applications could be deployed across the member clusters from the h cluster taking into account",
    "start": "540440",
    "end": "546480"
  },
  {
    "text": "factors like their workload requirements the need for Geographic proximity to users optimizing the use of resources",
    "start": "546480",
    "end": "553760"
  },
  {
    "text": "their High availability needs Etc there's different schools of thought about whether workload resources should",
    "start": "553760",
    "end": "559920"
  },
  {
    "text": "be run on the central cluster or not but from the customers that we have already deploying mongodb which uses the Hub",
    "start": "559920",
    "end": "566519"
  },
  {
    "text": "cluster pattern for multicluster we've not seen anybody that isn't deploying their workloads on the central cluster",
    "start": "566519",
    "end": "572760"
  },
  {
    "text": "as well so another model can be described",
    "start": "572760",
    "end": "578279"
  },
  {
    "text": "as the replicated architecture uh replicated application architecture um",
    "start": "578279",
    "end": "583839"
  },
  {
    "text": "in this case an entire workload uh or it can be even an entire stack is replicated to another cluster um and the",
    "start": "583839",
    "end": "591839"
  },
  {
    "text": "whole cluster itself in might cases might even be cloned to another location to another cluster depending on whether",
    "start": "591839",
    "end": "598839"
  },
  {
    "text": "the database uh backing the application is also replicated centralized or",
    "start": "598839",
    "end": "604079"
  },
  {
    "text": "somehow distributed it's possible that this architecture could still serve to provide some of the benefits like",
    "start": "604079",
    "end": "610760"
  },
  {
    "text": "performance increase reduce latency uh load balancing and so on now if the",
    "start": "610760",
    "end": "616560"
  },
  {
    "text": "secondary cluster is not actively serving traffic it becomes more of an active passive uh setup which we might",
    "start": "616560",
    "end": "623760"
  },
  {
    "text": "be familiar with um and this is a less cost effective option though",
    "start": "623760",
    "end": "630959"
  },
  {
    "text": "and lastly multicloud which is arguably less of a pattern on its own and more of an implementation choice that can apply",
    "start": "630959",
    "end": "637600"
  },
  {
    "text": "to either of the other two here you can see the Hub Cloud cluster pattern is implemented over three clouds two",
    "start": "637600",
    "end": "644639"
  },
  {
    "text": "private clouds and in this case Amazon eks just as an example third Cloud it's something we're seeing from a",
    "start": "644639",
    "end": "650800"
  },
  {
    "text": "few companies with highest requirements for resilience it also seems that a lot of customers are using this kind of high",
    "start": "650800",
    "end": "659040"
  },
  {
    "text": "hybrid on pram uh and public Cloud approach uh as a tentative step towards trusting the public clouds with their",
    "start": "659040",
    "end": "665560"
  },
  {
    "text": "most critical workloads the biggest challenge with multic cloud is ensuring that your choice of tools and mechanisms are",
    "start": "665560",
    "end": "672079"
  },
  {
    "text": "supported across the different environments this makes it uh incredibly important to choose tools and mechanisms",
    "start": "672079",
    "end": "678200"
  },
  {
    "text": "that are compatible with base kubernetes because that's the consistent element across different kubernetes based",
    "start": "678200",
    "end": "684959"
  },
  {
    "text": "distributions this does limit how much you can use the value add capabilities of of different clouds whether private",
    "start": "684959",
    "end": "690519"
  },
  {
    "text": "or public um that they typically layer on top of their their own kubernetes",
    "start": "690519",
    "end": "696320"
  },
  {
    "text": "offerings in in the case of the mongod DB operator we've chosen to go with a hub cluster pattern the the first",
    "start": "696320",
    "end": "703040"
  },
  {
    "text": "pattern you saw so the central PL pattern has has a few different capabilities it first off it hosts the",
    "start": "703040",
    "end": "709920"
  },
  {
    "text": "kubernetes operator um and acts as the control plane for multicluster",
    "start": "709920",
    "end": "715880"
  },
  {
    "text": "deployments it hosts the mongodb multicluster custom resource um in which",
    "start": "715880",
    "end": "720959"
  },
  {
    "text": "the operation the operator defines a deployment and optionally it can host",
    "start": "720959",
    "end": "726720"
  },
  {
    "text": "the mongodb management server that we call ops manager um and as as we",
    "start": "726720",
    "end": "732360"
  },
  {
    "text": "mentioned before it can also host members of the mongodb DB uh deployments",
    "start": "732360",
    "end": "737680"
  },
  {
    "text": "it can host a replica out of a replica set member clusters the only thing that they do is that they uh host mongodb",
    "start": "737680",
    "end": "744839"
  },
  {
    "text": "replica sets um and while the data plane is already resilience to a cluster",
    "start": "744839",
    "end": "750040"
  },
  {
    "text": "failure we're currently working on making it easier to recover both our operator and our management server ops",
    "start": "750040",
    "end": "756079"
  },
  {
    "text": "manager to another cluster so until we can offer a highly available operator",
    "start": "756079",
    "end": "761880"
  },
  {
    "text": "this will at least make Disaster Recovery quicker and",
    "start": "761880",
    "end": "767279"
  },
  {
    "text": "easier now we're moving on the three key challenges that we need to be solved to successfully implement multicluster the",
    "start": "767320",
    "end": "774959"
  },
  {
    "text": "first of those is cluster inventory so cluster inventory is is kind of what it sounds like it's all",
    "start": "774959",
    "end": "780920"
  },
  {
    "text": "about keeping track of the Clusters included in a multicluster environment um but this covers more than more than",
    "start": "780920",
    "end": "787320"
  },
  {
    "text": "just listing them there's cluster credentials which arguably are a bare minimum",
    "start": "787320",
    "end": "793360"
  },
  {
    "text": "uh you need these in order to be able to access the Clusters and to manage workloads across them uh and between the",
    "start": "793360",
    "end": "799920"
  },
  {
    "text": "kind of cluster addresses and the credentials you can actually deploy and managed workloads but another arguably uh pretty",
    "start": "799920",
    "end": "806920"
  },
  {
    "text": "firm requirement is cluster health checks that 's really no point in trying to deploy to a cluster that's",
    "start": "806920",
    "end": "812800"
  },
  {
    "text": "missing many solutions ours included provide an inbuilt mechanism to monitor clusters and redistribute workloads to",
    "start": "812800",
    "end": "819519"
  },
  {
    "text": "healthy clusters if a single member cluster goes down and lastly cluster",
    "start": "819519",
    "end": "825040"
  },
  {
    "text": "resource tracking uh this is less of a hard requirement but very much a nice to have uh and it's something most",
    "start": "825040",
    "end": "833120"
  },
  {
    "text": "multicluster efforts seem to be aiming for it's very similar to how kubernetes monitors nodes within a single cluster",
    "start": "833120",
    "end": "839560"
  },
  {
    "text": "assesses their resources and works out which you can reschedule or schedule uh pods to uh to ensure that they have",
    "start": "839560",
    "end": "846240"
  },
  {
    "text": "sufficient resources in the case of M cluster that's more complicated is you're not just talking about the nodes",
    "start": "846240",
    "end": "851680"
  },
  {
    "text": "in the cluster you're talking about multiple clusters it's also valuable to be able",
    "start": "851680",
    "end": "856880"
  },
  {
    "text": "to configure rules that actually govern and dictate uh when and where things should be moved uh to give users the",
    "start": "856880",
    "end": "863440"
  },
  {
    "text": "kind of the control over over where things are running and it's useful to support affinity and anti-affinity to",
    "start": "863440",
    "end": "869040"
  },
  {
    "text": "ensure that workloads are running where they needed",
    "start": "869040",
    "end": "873720"
  },
  {
    "text": "to now in going back to the to our use case for the cluster inventory we have a",
    "start": "874160",
    "end": "880399"
  },
  {
    "text": "relatively simple approach uh that we've built in house firstly we require the",
    "start": "880399",
    "end": "885800"
  },
  {
    "text": "creation of a number of arback resources on each of the member clusters uh this creates the service account used to",
    "start": "885800",
    "end": "892079"
  },
  {
    "text": "manage resources via the uh cluster API uh via the each cluster's API server",
    "start": "892079",
    "end": "899680"
  },
  {
    "text": "um next we need a cube config secret for the member clusters um you can see an",
    "start": "899680",
    "end": "906120"
  },
  {
    "text": "example of that on the right so the cube context needs to contain the address of the cluster and",
    "start": "906120",
    "end": "913240"
  },
  {
    "text": "the certificate created with the service account and at the bottom you see the",
    "start": "913240",
    "end": "918440"
  },
  {
    "text": "user to use including the token created for the service account now while we see some companies",
    "start": "918440",
    "end": "925519"
  },
  {
    "text": "automating this uh we also provide a cube uh Cube cutle plugin that does it all for you by creating the name spaces",
    "start": "925519",
    "end": "932920"
  },
  {
    "text": "automating the application um of vrb on the Clusters and creating the cube",
    "start": "932920",
    "end": "938040"
  },
  {
    "text": "context on the central cluster The Hub cluster it's worth mentioning here that",
    "start": "938040",
    "end": "943120"
  },
  {
    "text": "our automation uses the Legacy Long Live API token tokens uh for the service",
    "start": "943120",
    "end": "948639"
  },
  {
    "text": "accounts um and in the future we'd like to switch to using the token request API to refresh credentials used to access",
    "start": "948639",
    "end": "955399"
  },
  {
    "text": "the member clusters now now we're on the second key challenge uh that someone needs to solve",
    "start": "955399",
    "end": "962720"
  },
  {
    "text": "to successfully Implement multicluster which is workload distribution so that relates to how",
    "start": "962720",
    "end": "968639"
  },
  {
    "text": "workloads are actually distributed across the clusters for example in this simple diagram we've got a hub cluster pattern",
    "start": "968639",
    "end": "974800"
  },
  {
    "text": "across three clusters uh shown vertically each of the three workloads shown horizontally uh has a different",
    "start": "974800",
    "end": "981839"
  },
  {
    "text": "distribution across the three clusters and there's a number of considerations that can factor into how you distribute",
    "start": "981839",
    "end": "986920"
  },
  {
    "text": "a workload uh the requ level of redundancy the required Geographic spread either for resilience or to",
    "start": "986920",
    "end": "992800"
  },
  {
    "text": "locate workloads near to users or other systems and the resources available on each of the clusters for example",
    "start": "992800",
    "end": "1000079"
  },
  {
    "text": "workload one in blue may be serving users in each of the three locations where the Clusters are running and it",
    "start": "1000079",
    "end": "1005680"
  },
  {
    "text": "may also need three replicas to provide the level of performance that's needed workload two in the yellow May primarily",
    "start": "1005680",
    "end": "1011639"
  },
  {
    "text": "serve uses in the location of the central cluster but want the resilience of running copies in each of the other two clusters and finally workload 3 in",
    "start": "1011639",
    "end": "1019160"
  },
  {
    "text": "the orange may have arguably the simplest requirements primarily providing its service in the central cluster with resilience only needed on",
    "start": "1019160",
    "end": "1025319"
  },
  {
    "text": "one of the others uh so it's it's it's also possible that there may be Affinity or anti Affinity considerations in play",
    "start": "1025319",
    "end": "1032678"
  },
  {
    "text": "to ensure that certain workflows remain collocated or distributed from each",
    "start": "1032679",
    "end": "1037880"
  },
  {
    "text": "other going back to uh to the mongod mongodb operator um we've chosen a",
    "start": "1037880",
    "end": "1044640"
  },
  {
    "text": "declarative approach uh for work workload distribution across multiple",
    "start": "1044640",
    "end": "1050039"
  },
  {
    "text": "clusters so you can see that in the custom resource we include uh the uh the",
    "start": "1050039",
    "end": "1055760"
  },
  {
    "text": "cluster spec list parameter users can set how many members of the replica Set",
    "start": "1055760",
    "end": "1063080"
  },
  {
    "text": "uh they want to uh they want on each of the of the member clusters and then we",
    "start": "1063080",
    "end": "1068559"
  },
  {
    "text": "deploy the appropriate State full sets on each of those clusters now in case of a member cluster",
    "start": "1068559",
    "end": "1075880"
  },
  {
    "text": "failing the operator is able to automatically read distribute those workload those stateful sets on the",
    "start": "1075880",
    "end": "1082400"
  },
  {
    "text": "available clusters um and this this is currently enabled in the operator level so this is",
    "start": "1082400",
    "end": "1089360"
  },
  {
    "text": "not shown here using the automated redistribution seems like it kind of",
    "start": "1089360",
    "end": "1095520"
  },
  {
    "text": "undermines GTH Up's workflows which are usually used but the changes are stored as annotations and they persist while",
    "start": "1095520",
    "end": "1103280"
  },
  {
    "text": "the administrator is able to restore the Lost cluster uh they can then manually reconfigure",
    "start": "1103280",
    "end": "1109159"
  },
  {
    "text": "uh their distribution so multicluster is is",
    "start": "1109159",
    "end": "1116039"
  },
  {
    "text": "pretty impossible without the networking needed to connect the Clusters so there's actually two subtopics within",
    "start": "1116039",
    "end": "1121679"
  },
  {
    "text": "this uh larger topic there's intercluster connectivity connecting the Clusters and their workloads together",
    "start": "1121679",
    "end": "1127799"
  },
  {
    "text": "but there's also egress and Ingress to or from within the Clusters they have pretty similar set of",
    "start": "1127799",
    "end": "1134919"
  },
  {
    "text": "considerations there's Network topology it's critical to consider how connectivity is already or can be",
    "start": "1134919",
    "end": "1140679"
  },
  {
    "text": "established uh between the Clusters and also to external services and users",
    "start": "1140679",
    "end": "1145880"
  },
  {
    "text": "there's security uh what kind of security what level of security do you require between the users and the",
    "start": "1145880",
    "end": "1150919"
  },
  {
    "text": "workloads or between the workloads uh and is it going over private networks of the open internet changes things",
    "start": "1150919",
    "end": "1156440"
  },
  {
    "text": "considerably performance how can you maximize the performance of the connectivity is it sufficient for the",
    "start": "1156440",
    "end": "1161559"
  },
  {
    "text": "workloads your running multicluster and finally res reliability and resilience",
    "start": "1161559",
    "end": "1166720"
  },
  {
    "text": "how will you ensure that the connectivity is reliable and fault tolerant so looking into intercluster",
    "start": "1166720",
    "end": "1174559"
  },
  {
    "text": "networking there's there's a number of different options first shown in the top",
    "start": "1174559",
    "end": "1179799"
  },
  {
    "text": "left we've got the VPN option creating a secure tunnel between the member clusters on the top right we have VPC",
    "start": "1179799",
    "end": "1186960"
  },
  {
    "text": "peering uh which creates a private network connection between different vpcs where our uh kubernetes clusters",
    "start": "1186960",
    "end": "1193360"
  },
  {
    "text": "live in on the bottom left we have load balancers uh and they expose traffic",
    "start": "1193360",
    "end": "1200159"
  },
  {
    "text": "Services externally and they connect through the internet and lastly we have the service mess option service mesh is",
    "start": "1200159",
    "end": "1207760"
  },
  {
    "text": "the most popular popular option amongst our users um they can provide routing",
    "start": "1207760",
    "end": "1213799"
  },
  {
    "text": "load balancing and security for communication between the Clusters and even for services and users that are",
    "start": "1213799",
    "end": "1219919"
  },
  {
    "text": "external to the Clusters so we found that it's rarely the case that mongodb is the first workload spanning multiple",
    "start": "1219919",
    "end": "1227080"
  },
  {
    "text": "clusters for our us users and most of the users already have a service M like",
    "start": "1227080",
    "end": "1232720"
  },
  {
    "text": "is steo linkerd or something in place um and there are also cloud provider",
    "start": "1232720",
    "end": "1237840"
  },
  {
    "text": "specific service messes like mesh app for AWS antho service mes for Google and",
    "start": "1237840",
    "end": "1243360"
  },
  {
    "text": "so on moving on to Ingress and egress which",
    "start": "1243360",
    "end": "1248559"
  },
  {
    "text": "we're going to cover quickly there are a few different options uh again top left we've got low balances they expose the",
    "start": "1248559",
    "end": "1254720"
  },
  {
    "text": "services uh on a public IP providing Ingress and egress uh for multicluster Ingress typically",
    "start": "1254720",
    "end": "1261120"
  },
  {
    "text": "you need a global load balancer that actually distributes and balances the traffic between the different clusters",
    "start": "1261120",
    "end": "1266440"
  },
  {
    "text": "on the top right we've got service measures again uh again the most popular choice for our own users uh which enable",
    "start": "1266440",
    "end": "1272919"
  },
  {
    "text": "intercluster connectivity and provide routing load balancing in security for external access bottom left we've got",
    "start": "1272919",
    "end": "1279320"
  },
  {
    "text": "Network policies uh that can control the traffic uh to the Clusters and between the Clusters but again they need",
    "start": "1279320",
    "end": "1285400"
  },
  {
    "text": "something to distribute the traffic between the Clusters and finally bottom right we have the Ingress controllers uh",
    "start": "1285400",
    "end": "1292919"
  },
  {
    "text": "great for managing Ingress traffic but again they need something to distribute the traffic between the different",
    "start": "1292919",
    "end": "1298320"
  },
  {
    "text": "clusters when you're running multicluster now in the case of our operator we",
    "start": "1298320",
    "end": "1303799"
  },
  {
    "text": "wanted to allow users to choose how they want to handle networking um we've based",
    "start": "1303799",
    "end": "1309799"
  },
  {
    "text": "our own quick start on the use of service mesh and specifically steo which",
    "start": "1309799",
    "end": "1315000"
  },
  {
    "text": "we can use we use to do our own testing but ultimately users just need to meet",
    "start": "1315000",
    "end": "1320200"
  },
  {
    "text": "the minimum networking requirements which is fqdn resolution between the workload pods across the different",
    "start": "1320200",
    "end": "1327080"
  },
  {
    "text": "clusters we also have guidance for using load balancers but so far we've seen",
    "start": "1327080",
    "end": "1332600"
  },
  {
    "text": "most companies choose to use uh service Mees for network connectivity if they",
    "start": "1332600",
    "end": "1337679"
  },
  {
    "text": "don't as we just saw now there's a number of options that they they can use but it's ultimately up to them to ensure",
    "start": "1337679",
    "end": "1343400"
  },
  {
    "text": "that the connectivity between clusters is there and working uh to support the",
    "start": "1343400",
    "end": "1349760"
  },
  {
    "text": "deployments now we're going to move on to multicluster controllers and operators this could be a huge talk in",
    "start": "1349760",
    "end": "1355760"
  },
  {
    "text": "and of itself and it's actually one we're thinking about doing perhaps in Paris next year uh for the next cubec",
    "start": "1355760",
    "end": "1360880"
  },
  {
    "text": "con and uh CNF so there are a number of considerations when designing",
    "start": "1360880",
    "end": "1367120"
  },
  {
    "text": "controllers and operators um consistency is a is a massive one you need to maintain a consistent State across all",
    "start": "1367120",
    "end": "1372799"
  },
  {
    "text": "of the Clusters that can be challenging uh if different clusters are running different flavors or versions of",
    "start": "1372799",
    "end": "1378760"
  },
  {
    "text": "kubernetes uh and may even have different hardware and software configurations again this makes",
    "start": "1378760",
    "end": "1384200"
  },
  {
    "text": "compatibility critical resilience uh mainly to Cluster failure this ideally covers both the",
    "start": "1384200",
    "end": "1390919"
  },
  {
    "text": "workload and your multicluster control plane whatever you're using and scalability to support a large number of",
    "start": "1390919",
    "end": "1397240"
  },
  {
    "text": "clusters and workloads and finally a big one security to protect both the control plane and the workloads being",
    "start": "1397240",
    "end": "1405159"
  },
  {
    "text": "run now in terms of functionality there are a few key areas that unsurprisingly",
    "start": "1405159",
    "end": "1411080"
  },
  {
    "text": "align with the topics that we just covered um first off cluster inventory and workload distribution ideally it",
    "start": "1411080",
    "end": "1418000"
  },
  {
    "text": "should take into account the available resources on each cluster and what",
    "start": "1418000",
    "end": "1423760"
  },
  {
    "text": "performance uh requirements the workloads have failure",
    "start": "1423760",
    "end": "1429159"
  },
  {
    "text": "handling ideally the workloads remain healthy even with a cluster down but a",
    "start": "1429159",
    "end": "1434200"
  },
  {
    "text": "minimum uh at a minimum a user should be able to reconfigure as needed preferably",
    "start": "1434200",
    "end": "1440120"
  },
  {
    "text": "this would be automated um so able to detect failures and take corrective action such as rescheduling workloads uh",
    "start": "1440120",
    "end": "1447080"
  },
  {
    "text": "to remaining healthy clusters monitoring and observability um",
    "start": "1447080",
    "end": "1452440"
  },
  {
    "text": "administrators need to be able to monitor both multicluster control plane and its managed workloads um this means",
    "start": "1452440",
    "end": "1459480"
  },
  {
    "text": "that your solution should emit metrics and logs that can be used to track their performance and health ideally via",
    "start": "1459480",
    "end": "1466200"
  },
  {
    "text": "external and common Ed systems like promethus alert management manager and",
    "start": "1466200",
    "end": "1471279"
  },
  {
    "text": "so on now we're taking a slightly a slight",
    "start": "1471279",
    "end": "1476799"
  },
  {
    "text": "departure from the pattern of covering a bit of theory and then looking at what we did at mongodb instead we're going to",
    "start": "1476799",
    "end": "1483840"
  },
  {
    "text": "take a quick look at a couple of prominent options that already exist for implementing uh deploying multicluster",
    "start": "1483840",
    "end": "1491799"
  },
  {
    "text": "so these Solutions are agnostic of any particular vendor's offering which means",
    "start": "1491799",
    "end": "1497080"
  },
  {
    "text": "they don't take into into account the the logic of a service like Mong DB for example but they do allow you to run",
    "start": "1497080",
    "end": "1502640"
  },
  {
    "text": "existing workloads across multicluster environments kada and open cluster",
    "start": "1502640",
    "end": "1508000"
  },
  {
    "text": "management are two of the more prominent cfcm projects that we're aware of I don't doubt there are probably others",
    "start": "1508000",
    "end": "1513799"
  },
  {
    "text": "but um these are kind of two that we're aware of and we picked to kind of talk about and they've been a source of inspiration for us for both existing and",
    "start": "1513799",
    "end": "1520399"
  },
  {
    "text": "future iterations they're both great solutions for implementing multicluster distribution in a unified manner without",
    "start": "1520399",
    "end": "1527760"
  },
  {
    "text": "relying on custom controllers for each piece of software that you're trying to deploy uh like I said some some tools",
    "start": "1527760",
    "end": "1533240"
  },
  {
    "text": "like mongodb have this kind of clust multicluster functionality built in and it has the advantage of catering to our",
    "start": "1533240",
    "end": "1538720"
  },
  {
    "text": "own logic and requirements but obviously the cost of not necessarily being or not being not at all being able to support",
    "start": "1538720",
    "end": "1544840"
  },
  {
    "text": "other services running multicluster we're not going to go into massive detail here since both of these do a lot",
    "start": "1544840",
    "end": "1552120"
  },
  {
    "text": "uh but there is a lot of overlap such as considering cluster availability and resources automating or manual failover",
    "start": "1552120",
    "end": "1559559"
  },
  {
    "text": "of workloads between clusters and cross-cluster service Discovery they also both offer solutions for",
    "start": "1559559",
    "end": "1565640"
  },
  {
    "text": "cross-cluster networking which as we mentioned we don't we're relatively unopinionated about that though each",
    "start": "1565640",
    "end": "1571360"
  },
  {
    "text": "does do it in a slightly different way one one slightly subjective",
    "start": "1571360",
    "end": "1576720"
  },
  {
    "text": "difference between the two is that kada is a bit more abstracted um while oper",
    "start": "1576720",
    "end": "1581799"
  },
  {
    "text": "open cluster management offers more control through uh prescriptive configurations",
    "start": "1581799",
    "end": "1588520"
  },
  {
    "text": "they can both run with an agent on each of the spoke or member clusters which pulls the config from the center cluster",
    "start": "1588520",
    "end": "1594679"
  },
  {
    "text": "and applies it but kada can also run uh on an agentless mode by pushing configuration to the member clusters uh",
    "start": "1594679",
    "end": "1602480"
  },
  {
    "text": "again through the respective API server of each cluster um and one last",
    "start": "1602480",
    "end": "1607679"
  },
  {
    "text": "difference that we can see being useful to some users is that kada supports",
    "start": "1607679",
    "end": "1613360"
  },
  {
    "text": "retrofit fitting a single cluster workload to multicluster",
    "start": "1613360",
    "end": "1619278"
  },
  {
    "text": "so having to Tak a couple of uh look at a couple of great existing Solutions and also talked about the considerations for",
    "start": "1620960",
    "end": "1626720"
  },
  {
    "text": "building your own multicluster offering or just choosing one off the shelf we wanted to reflect on some of our",
    "start": "1626720",
    "end": "1632559"
  },
  {
    "text": "decisions at mongodb by Sherry some of the changes we'd like to potentially have made in the past but at least make",
    "start": "1632559",
    "end": "1638480"
  },
  {
    "text": "going forwards and perhaps some of these might help you on your own Journey identifying what you need and what you need to",
    "start": "1638480",
    "end": "1645360"
  },
  {
    "text": "do so as you saw today we we offer a relatively prescriptive approach to",
    "start": "1645360",
    "end": "1651039"
  },
  {
    "text": "workload distribution uh we mentioned the declarative approach um so you set",
    "start": "1651039",
    "end": "1656520"
  },
  {
    "text": "how many instances of a replica you want on each of the member clusters this works but we would love to",
    "start": "1656520",
    "end": "1663360"
  },
  {
    "text": "abstract it a bit more uh it is a bit counterintuitive if you think of kubernetes Primitives and and um the",
    "start": "1663360",
    "end": "1670679"
  },
  {
    "text": "philosophy behind it so today we compute an index of each of which instance of",
    "start": "1670679",
    "end": "1675840"
  },
  {
    "text": "the database lives on which cluster ERS um users can see the index in annotations but can't edit it even",
    "start": "1675840",
    "end": "1683880"
  },
  {
    "text": "though if you would like to um but what they really want is to decide where the",
    "start": "1683880",
    "end": "1689240"
  },
  {
    "text": "primary instance of their database lives since that's the only one that they can write on and that's that's the important",
    "start": "1689240",
    "end": "1695840"
  },
  {
    "text": "part um they don't really need to edit the index they just need to prioritize the list of clusters and have the",
    "start": "1695840",
    "end": "1702000"
  },
  {
    "text": "operator manage things so that they the primary ends up in the right place um",
    "start": "1702000",
    "end": "1707600"
  },
  {
    "text": "that's that's one of the things that we we hope to offer in the future for us resource awareness is also",
    "start": "1707600",
    "end": "1714679"
  },
  {
    "text": "another uh existing piece although we check the availability of the Clusters in order to potentially reallocate",
    "start": "1714679",
    "end": "1721159"
  },
  {
    "text": "workloads in need and also to deploy them in the first place we don't currently check their available resources essentially for us this pushes",
    "start": "1721159",
    "end": "1728440"
  },
  {
    "text": "the resource allocation problem onto the users which is again something we'd like to make easier for them uh in the future",
    "start": "1728440",
    "end": "1735600"
  },
  {
    "text": "as it's especially something you don't want to have to consider in a Dr scenario on a similar note we we do",
    "start": "1735600",
    "end": "1742120"
  },
  {
    "text": "offer automated failover like we said um but it's not highly customizable it's also on or off at the operator level U",
    "start": "1742120",
    "end": "1748880"
  },
  {
    "text": "and while it's valuable we'd like to make it on a per workload basis and iterate to give more rules and more control over when a workload should be",
    "start": "1748880",
    "end": "1756320"
  },
  {
    "text": "redistributed and even potentially how it should be",
    "start": "1756320",
    "end": "1761120"
  },
  {
    "text": "redistributed and lastly uh would' like to make the operator highly available um",
    "start": "1762600",
    "end": "1768799"
  },
  {
    "text": "have it spun multiple clusters um it's easier to restore it on another cluster",
    "start": "1768799",
    "end": "1774320"
  },
  {
    "text": "than it would be for our database or management server but it still is a single point of failure for for this",
    "start": "1774320",
    "end": "1781640"
  },
  {
    "text": "setup um having the operator also run on other clusters could also have an added benefit um we would be able to offer a",
    "start": "1781640",
    "end": "1790000"
  },
  {
    "text": "pull model uh as we described before for that similar that that kada does um this",
    "start": "1790000",
    "end": "1796000"
  },
  {
    "text": "would allow secondary operators on each member cluster to intermittently pull configurations from the center cluster",
    "start": "1796000",
    "end": "1802640"
  },
  {
    "text": "and apply it locally uh instead of the current model which well the the model",
    "start": "1802640",
    "end": "1808240"
  },
  {
    "text": "of having of pushing from the central cluster to the to the member clusters um this would be really good",
    "start": "1808240",
    "end": "1815440"
  },
  {
    "text": "for scaling uh and supporting multiple kubernetes clusters but also allows for",
    "start": "1815440",
    "end": "1821320"
  },
  {
    "text": "secondary operations to member clusters to make changes in the event of losing the central cluster",
    "start": "1821320",
    "end": "1827760"
  },
  {
    "text": "um electing a new primary um or pulling uh putting a database in read only mode",
    "start": "1827760",
    "end": "1834840"
  },
  {
    "text": "on a single cluster cut off uh and so on so we're very close to the end which",
    "start": "1834840",
    "end": "1840840"
  },
  {
    "text": "also means you're all quite close to getting lunch which I'm excited about before we wrap up with some Q&A we",
    "start": "1840840",
    "end": "1846279"
  },
  {
    "text": "wanted to share some final thoughts on what we see being the future of multicluster uh some of this may may all",
    "start": "1846279",
    "end": "1851559"
  },
  {
    "text": "be more already be more in progress than we're aware of um but this is kind of what we imagine coming up",
    "start": "1851559",
    "end": "1858960"
  },
  {
    "text": "the first and most significant we thing we expect is that it becomes significantly easier to run multicluster",
    "start": "1858960",
    "end": "1864080"
  },
  {
    "text": "uh systems uh likely through more powerful and user friendly multicluster management tools there's always a drive",
    "start": "1864080",
    "end": "1871760"
  },
  {
    "text": "to simplify across kind of our ecosystem Community uh and ultimate and Abstract",
    "start": "1871760",
    "end": "1877480"
  },
  {
    "text": "the complexity further and further away uh to make it much easier to manage uh and we've already outlined a",
    "start": "1877480",
    "end": "1884360"
  },
  {
    "text": "couple of existing projects that are making great progress here we don't think that it'll stop at simplifying for the cluster admins either you also need",
    "start": "1884360",
    "end": "1890320"
  },
  {
    "text": "to config the consider the developers who want a simplify abstracted way to actually deploy across multiple clusters",
    "start": "1890320",
    "end": "1896320"
  },
  {
    "text": "without knowing all of the finer details about how it necessarily works at a deeper level most application developers",
    "start": "1896320",
    "end": "1903000"
  },
  {
    "text": "don't have the time or the inclination to become kubernetes experts the next is almost a cliche now",
    "start": "1903000",
    "end": "1911240"
  },
  {
    "text": "um increased use of AI and ml uh it's it's already sted uh there's a lot of booths in the Expo hold that you can go",
    "start": "1911240",
    "end": "1917559"
  },
  {
    "text": "and see that are already working on this and integrating this into what they're doing but we're expecting this to be used in multicluster management um and",
    "start": "1917559",
    "end": "1924760"
  },
  {
    "text": "arguably kubernetes use in general and this can free up the administrators to work on more strategic",
    "start": "1924760",
    "end": "1932399"
  },
  {
    "text": "tasks we're also expecting better Network support for multicluster kubernetes um this is one where there's",
    "start": "1932960",
    "end": "1940080"
  },
  {
    "text": "already a lot of work happening um there are a lot of interesting cncf projects some of some of them are Cloud agnostic",
    "start": "1940080",
    "end": "1947919"
  },
  {
    "text": "uh commercial offerings and of course this is this is something that cloud providers already have many",
    "start": "1947919",
    "end": "1953760"
  },
  {
    "text": "solutions and finally the bullet that's been there all along um better support",
    "start": "1953760",
    "end": "1959399"
  },
  {
    "text": "for hybrid and multicloud deployments um as we mentioned we're seeing some of our users spanning across more than one",
    "start": "1959399",
    "end": "1966399"
  },
  {
    "text": "Cloud providers some even using it as a stepping stone uh towards Cloud adoption",
    "start": "1966399",
    "end": "1972159"
  },
  {
    "text": "some on Prem some on cloud um and multicluster kubernetes management tools",
    "start": "1972159",
    "end": "1977360"
  },
  {
    "text": "need to provide better support for for these type of hybrid uh and multicloud",
    "start": "1977360",
    "end": "1983840"
  },
  {
    "text": "deployments uh and that's it uh this QR code or or link that go to the same place on the right we'll help you get to",
    "start": "1984080",
    "end": "1990720"
  },
  {
    "text": "various related reading materials from docs about our own operator if you're interested in that uh to info about uh",
    "start": "1990720",
    "end": "1997320"
  },
  {
    "text": "the multi Sig multicluster and there's a load of information on their site it's really interesting reading if you enjoy",
    "start": "1997320",
    "end": "2003159"
  },
  {
    "text": "this then you'll probably enjoy that as well uh to more about kada and open CL management because we talked about them",
    "start": "2003159",
    "end": "2008760"
  },
  {
    "text": "and we're fans um but now I think we do have a little bit of time for questions if you do have any please make your way",
    "start": "2008760",
    "end": "2015200"
  },
  {
    "text": "to the microphone as this is being recorded and that way people will be able to hear your questions as well as our",
    "start": "2015200",
    "end": "2022039"
  },
  {
    "text": "answers hi thank you so I believe mongod DV has two modes of operation all the",
    "start": "2024679",
    "end": "2030360"
  },
  {
    "text": "rights go to the primary and then the reads can come from secondary sorry can you get a little bit closer to the microphone so yeah so mango DV has two",
    "start": "2030360",
    "end": "2037840"
  },
  {
    "text": "modes of operation strong strongly consistent and more like eventually consistent where rids can come",
    "start": "2037840",
    "end": "2044799"
  },
  {
    "text": "from stale secondaries right yeah you can multi cluster setups what's the",
    "start": "2044799",
    "end": "2050398"
  },
  {
    "text": "typical deployment do your customers run them in a strongly consistent manner or or in a eventually consistent Manner and",
    "start": "2050399",
    "end": "2058800"
  },
  {
    "text": "um how's that specifi by your users it varies on the use case but through our",
    "start": "2058800",
    "end": "2064800"
  },
  {
    "text": "operator whether it's multicluster or not they can configure how many nodes need to be written to before it's",
    "start": "2064800",
    "end": "2070158"
  },
  {
    "text": "confirmed as written to um so they effectively they have control over it on a per application basis and it does vary",
    "start": "2070159",
    "end": "2076760"
  },
  {
    "text": "based on the use case as to whether they need the speed or the guaranteed",
    "start": "2076760",
    "end": "2082679"
  },
  {
    "text": "consistency the reason I asked is that how do customers specify as strongly consistent or visually consistent to the",
    "start": "2082679",
    "end": "2090398"
  },
  {
    "text": "Federated Services Rec abstraction is there a way for them to tune them or yeah the same way across both",
    "start": "2090399",
    "end": "2097400"
  },
  {
    "text": "I think the terminology is called right concern and uh I think the default is that it has to be written to the majority of members of a replica set but",
    "start": "2097400",
    "end": "2104880"
  },
  {
    "text": "they can alter that they could say that the right concern is one in which case as soon as it's written to the primary they'd get an acknowledgement that it's",
    "start": "2104880",
    "end": "2110680"
  },
  {
    "text": "written they could say that it's all in which case they get maximum consistency and it's guaranteed to be written before",
    "start": "2110680",
    "end": "2116560"
  },
  {
    "text": "the application gets a response saying that it's been confirmed written thank",
    "start": "2116560",
    "end": "2123319"
  },
  {
    "text": "you um you did speak about the improved um resilience with the multi cluster",
    "start": "2123800",
    "end": "2129280"
  },
  {
    "text": "design I just want to get an idea of how you UND see maintenance moves across multiple clusters sorry I I missed that",
    "start": "2129280",
    "end": "2135480"
  },
  {
    "text": "how you handle like maintenance um like maintenance modes across multiple clusters cluster upgrades you know post",
    "start": "2135480",
    "end": "2141400"
  },
  {
    "text": "them you know rep placement Etc like that kind of thing um and uh because if",
    "start": "2141400",
    "end": "2147960"
  },
  {
    "text": "you do have multi clusters one goes down you're doing something on it um how that process how youcrate that M to be yeah",
    "start": "2147960",
    "end": "2156319"
  },
  {
    "text": "did you cut that no sorry we we're not hearing you that great okay the summary",
    "start": "2156319",
    "end": "2162000"
  },
  {
    "text": "is how do you handle maintenance modes AC cross the multicluster design that you are",
    "start": "2162000",
    "end": "2167319"
  },
  {
    "text": "Str did you say maintenance mod yes maintenance yeah like Windows rather maintenance",
    "start": "2167319",
    "end": "2173400"
  },
  {
    "text": "windows in in what sense do you mean maintenance modes no maintenance Windows is the is",
    "start": "2173400",
    "end": "2179839"
  },
  {
    "text": "the question like oh M Window all right okay I see",
    "start": "2179839",
    "end": "2185040"
  },
  {
    "text": "um uh well that's that's not something we've necessarily got a great solution",
    "start": "2185079",
    "end": "2191040"
  },
  {
    "text": "to that we've built into kubernetes mongod DB already takes into account that so uh our upgrades for example",
    "start": "2191040",
    "end": "2197760"
  },
  {
    "text": "though are done in a rolling fashion even across multicluster so if you needed to upgrade the operator will",
    "start": "2197760",
    "end": "2203160"
  },
  {
    "text": "actually handle that and a do it in a rolling fashion so mongodb kind of already has support in for that uh most",
    "start": "2203160",
    "end": "2209839"
  },
  {
    "text": "other impactful changes are done in the same sort of rolling fashion as well so that kind of mostly covers most of the",
    "start": "2209839",
    "end": "2216119"
  },
  {
    "text": "maintenance this required and that's that's kind of covered by the part we said that you use",
    "start": "2216119",
    "end": "2222160"
  },
  {
    "text": "a declarative configuration to Define what number of replica sets lives in each kubernetes cluster so if for any",
    "start": "2222160",
    "end": "2228760"
  },
  {
    "text": "reason one cluster is brought down for maintenance upgrades whatever then these are going to be rescheduled to the",
    "start": "2228760",
    "end": "2234760"
  },
  {
    "text": "remaining clusters okay okay and uh and the control play manages that automatically",
    "start": "2234760",
    "end": "2241000"
  },
  {
    "text": "yeah the operator manages that yeah yeah perfect uh I have a question so uh so",
    "start": "2241000",
    "end": "2249800"
  },
  {
    "text": "you mentioned uh like when a cluster fails the operator or the multicluster operator basically transparently moves",
    "start": "2249800",
    "end": "2256240"
  },
  {
    "text": "the the that those set of replicas to the other clusters um how what what do",
    "start": "2256240",
    "end": "2261480"
  },
  {
    "text": "you do about storage or is it in the same a and you try to attach the same volumes or like how how does storage",
    "start": "2261480",
    "end": "2267640"
  },
  {
    "text": "like U basically how does the the new replicas coming in the new cluster uh get seated with the right uh storage",
    "start": "2267640",
    "end": "2274839"
  },
  {
    "text": "well that that's something that happens not because of the operator but because of so essentially because it's a",
    "start": "2274839",
    "end": "2281640"
  },
  {
    "text": "it's as we said at the beginning we want different kubernetes clusters to be kind of isolated and not managed in the same",
    "start": "2281640",
    "end": "2288200"
  },
  {
    "text": "way so we're talking about different PVS different PVCs so they're not uh it's not it's not easy to uh so essentially",
    "start": "2288200",
    "end": "2295160"
  },
  {
    "text": "what happens is that you spin up a new replica set and takes care of",
    "start": "2295160",
    "end": "2300319"
  },
  {
    "text": "replicating the data there okay so you spin up like something with empty basically empty volum yeah as if you",
    "start": "2300319",
    "end": "2307160"
  },
  {
    "text": "would add a replica set into your deployment and then it takes care of it's similar as if you scaled and you",
    "start": "2307160",
    "end": "2313480"
  },
  {
    "text": "added new members of the replica set they would just synchronize the data so uh then do you have to do some sort of",
    "start": "2313480",
    "end": "2319680"
  },
  {
    "text": "uh uh orchestration options in the control plane to make sure that these new workloads that are showing up are",
    "start": "2319680",
    "end": "2325920"
  },
  {
    "text": "showing up as uh like new replicas not the existing ones so like is it because they have a different DNS name or like",
    "start": "2325920",
    "end": "2332839"
  },
  {
    "text": "like what's the process uh that's that's that's automated so they're identif IFI as effectively new members of the",
    "start": "2332839",
    "end": "2339040"
  },
  {
    "text": "replica set okay okay that makes sense and another sort of similar question to the last question um so like when you're",
    "start": "2339040",
    "end": "2345839"
  },
  {
    "text": "running in a single cluster mode for example each stateful set can you know you can set things like part disruption",
    "start": "2345839",
    "end": "2350880"
  },
  {
    "text": "budgets and to control uh life cycle management operations but like when you're multicluster there is no such",
    "start": "2350880",
    "end": "2356560"
  },
  {
    "text": "abstraction available right so how do you manage s of like uh if a cloud provider is taking down parts of a node",
    "start": "2356560",
    "end": "2362720"
  },
  {
    "text": "or parts of a cluster like how is disruptions of managed across multiple",
    "start": "2362720",
    "end": "2368000"
  },
  {
    "text": "clusters so at the moment we haven't gone uh that far in that sense in addressing that there something we're",
    "start": "2368000",
    "end": "2373160"
  },
  {
    "text": "very conscious of and pod disruption as you say um it's one of the iterations",
    "start": "2373160",
    "end": "2378800"
  },
  {
    "text": "but if we listed all of the changes we'd like to make we'd have been here all day um but yeah thank you",
    "start": "2378800",
    "end": "2386319"
  },
  {
    "text": "thanks U forgiving if this maybe this is more carada question but I don't know K",
    "start": "2387240",
    "end": "2392720"
  },
  {
    "text": "so but um how do you handle um for on one of the workload clusters things",
    "start": "2392720",
    "end": "2400640"
  },
  {
    "text": "like that I was curious about you know because you have the C config and I'm assuming you're using a c config at have",
    "start": "2400640",
    "end": "2407560"
  },
  {
    "text": "a series of those at a hub cluster and I wonder how do you handle the situations like for instance the search rotated on",
    "start": "2407560",
    "end": "2415160"
  },
  {
    "text": "one of the other clusters that basically invalidates your um at the moment we're using uh",
    "start": "2415160",
    "end": "2423240"
  },
  {
    "text": "kind of static uh API tokens to actually manage the workloads on the member clusters as we mentioned we'd like to",
    "start": "2423240",
    "end": "2430319"
  },
  {
    "text": "we'd like to switch to the uh the kind of the more automated model with the automated rotation and so on um so at",
    "start": "2430319",
    "end": "2437599"
  },
  {
    "text": "the moment we're we're kind of avoiding it by having static um if someone were to rotate those on the member clusters",
    "start": "2437599",
    "end": "2443160"
  },
  {
    "text": "effectively we lose access to to manage the member clusters so that's kind of one of the reasons why we want to kind",
    "start": "2443160",
    "end": "2449280"
  },
  {
    "text": "of move forward and iterate on that so standard token means that that is the",
    "start": "2449280",
    "end": "2455319"
  },
  {
    "text": "authenticated uh are you how are you um uh",
    "start": "2455319",
    "end": "2460520"
  },
  {
    "text": "authenticating the remote KU server are you doing insecure you mean how does the Hub",
    "start": "2460520",
    "end": "2467680"
  },
  {
    "text": "cluster call each of the API servers yeah that's two parts the there's authentication token and then there's",
    "start": "2467680",
    "end": "2474040"
  },
  {
    "text": "the sech to identify server right so those either of those two could change for various",
    "start": "2474040",
    "end": "2480240"
  },
  {
    "text": "reasons if if I understand correctly uh so the first step of setting it up is we",
    "start": "2480240",
    "end": "2485920"
  },
  {
    "text": "set are back in all of the member clusters and we create the roles to give",
    "start": "2485920",
    "end": "2491000"
  },
  {
    "text": "access to the member cluster API and then we issue the tokens and those are",
    "start": "2491000",
    "end": "2496760"
  },
  {
    "text": "all stored uh in Cube secrets in the hub cluster and those are used to make the API calls okay how do you do your tation",
    "start": "2496760",
    "end": "2504760"
  },
  {
    "text": "with those guys um I don't remember that not sure my",
    "start": "2504760",
    "end": "2512160"
  },
  {
    "text": "head yeah but this is this is one of the advantag of moving to a pool model if we could have the operator BHA then we",
    "start": "2512160",
    "end": "2518599"
  },
  {
    "text": "effectively have an agent running in each of the member clusters and a a lot of those considerations get easier okay",
    "start": "2518599",
    "end": "2524960"
  },
  {
    "text": "yeah that's I was thinking we similar problem",
    "start": "2524960",
    "end": "2530359"
  },
  {
    "text": "yeah a point of clarification when you say you have Hub clusters are you using",
    "start": "2531359",
    "end": "2537599"
  },
  {
    "text": "client clusters are you deploying your own or you deploying like a cluster inside of a cluster well essentially",
    "start": "2537599",
    "end": "2545160"
  },
  {
    "text": "it's just a cube cluster where you just install the operator and then that becomes your Hub cluster for the mongodb",
    "start": "2545160",
    "end": "2551480"
  },
  {
    "text": "deployments there there's no other official designation other than the fact the operators running that and then the",
    "start": "2551480",
    "end": "2556520"
  },
  {
    "text": "other question is how do you handle logging and tracing in terms of is that all being sent back to the hub cluster",
    "start": "2556520",
    "end": "2563720"
  },
  {
    "text": "or do you have like caching on the individual clusters uh so that's that's another",
    "start": "2563720",
    "end": "2570200"
  },
  {
    "text": "thing on our on our to-do list eventually um at the moment you'd have to go on to any of those hub clust to",
    "start": "2570200",
    "end": "2576599"
  },
  {
    "text": "pull the relevant logs for for our workloads running there um that that's",
    "start": "2576599",
    "end": "2581680"
  },
  {
    "text": "why we propose you use something external like prus because then you can add an extra layer like add Thanos or",
    "start": "2581680",
    "end": "2587559"
  },
  {
    "text": "something else yeah and then you can have a unified view of of what's happening in the different clusters",
    "start": "2587559",
    "end": "2592599"
  },
  {
    "text": "great thank you thank you all right so full disclosure I'm an",
    "start": "2592599",
    "end": "2599000"
  },
  {
    "text": "opener management maintainer so okay you're one of the scary people for us",
    "start": "2599000",
    "end": "2605079"
  },
  {
    "text": "anyway but I was wondering if you had plans or had thought about integrating with um a solution like open cluster",
    "start": "2605079",
    "end": "2612200"
  },
  {
    "text": "management or or Cara so when I was doing research and I",
    "start": "2612200",
    "end": "2617480"
  },
  {
    "text": "was talking to in particular mercher about this um I kind of asked like these",
    "start": "2617480",
    "end": "2622920"
  },
  {
    "text": "the these these tools are Serv so solving so many of the problems that we want to solve why why wouldn't we now",
    "start": "2622920",
    "end": "2629160"
  },
  {
    "text": "they're gaining maturity why wouldn't we move over and stop worrying about building own and we kind of had this",
    "start": "2629160",
    "end": "2634200"
  },
  {
    "text": "long and in-depth conversation about the fact that there's so much bespoke logic to mongodb that we want to take into",
    "start": "2634200",
    "end": "2641000"
  },
  {
    "text": "account building that those consider on top of that would probably be as expensive as us just continuing our",
    "start": "2641000",
    "end": "2646599"
  },
  {
    "text": "journey with our own solution that already takes into account lots of that logic so and it's also a matter of of reducing",
    "start": "2646599",
    "end": "2654839"
  },
  {
    "text": "the the number of tools that our customers need in order to use it uh we",
    "start": "2654839",
    "end": "2660760"
  },
  {
    "text": "we have seen cases where uh people don't even want to use hell more and like the",
    "start": "2660760",
    "end": "2666280"
  },
  {
    "text": "bare minimum so we try to use only the bare minimum so we make it easier for everyone yeah I mean in general we're",
    "start": "2666280",
    "end": "2673240"
  },
  {
    "text": "very unopinionated about third party tools because if we recommend anything let alone uh require them to be running",
    "start": "2673240",
    "end": "2679720"
  },
  {
    "text": "anything then they expect us to know loads about that and support them on that and we honestly we we we don't have",
    "start": "2679720",
    "end": "2686240"
  },
  {
    "text": "the time and the expertise to be able to do that sort of thing so yeah that's fair thank you",
    "start": "2686240",
    "end": "2693559"
  },
  {
    "text": "thanks hey thank you uh I a question about running that operator in in the",
    "start": "2693559",
    "end": "2698880"
  },
  {
    "text": "hub cluster yeah uh how did you set up watches across different clusters",
    "start": "2698880",
    "end": "2706200"
  },
  {
    "text": "running a single copy of operator did you use some kind of framework or all your own you have any issues with like",
    "start": "2706200",
    "end": "2713280"
  },
  {
    "text": "latency Etc watching you know remote clusters uh so we're using uh I I can't",
    "start": "2713280",
    "end": "2720839"
  },
  {
    "text": "don't quote me on the terminology but we're using a a health endpoint on the member clusters to basically check their",
    "start": "2720839",
    "end": "2726559"
  },
  {
    "text": "availability we'd like to go deeper and check their resource availability as well um in terms of latency no we",
    "start": "2726559",
    "end": "2732520"
  },
  {
    "text": "haven't experienced any latency problems either in the kind of control plan of our multic claster setup or in the",
    "start": "2732520",
    "end": "2737599"
  },
  {
    "text": "workloads but we can't take credit for the workload aspect that's Mong be's inherent functionality but there's no",
    "start": "2737599",
    "end": "2745119"
  },
  {
    "text": "Mong be operator running on on the sport clusters no they're not the the operator",
    "start": "2745119",
    "end": "2750480"
  },
  {
    "text": "on the on the central cluster is literally just using the API server on each of the member clusters to manage",
    "start": "2750480",
    "end": "2755520"
  },
  {
    "text": "the state stateful sets that it's running there that although there are individual stateful sets across all the Clusters it effectively amounts to one",
    "start": "2755520",
    "end": "2762280"
  },
  {
    "text": "replica set and it's watching right yeah for changes across multiple clust yeah",
    "start": "2762280",
    "end": "2769079"
  },
  {
    "text": "okay thanks well because we deploy the state for set the within that kubernetes cluster a lot is managed automatically",
    "start": "2769079",
    "end": "2775359"
  },
  {
    "text": "using the the the inherent stateful set functionality and so on so um like if",
    "start": "2775359",
    "end": "2780920"
  },
  {
    "text": "the Pod is lost it will be replaced and so on and we we get a lot of that for free because of B is brilliant and",
    "start": "2780920",
    "end": "2787960"
  },
  {
    "text": "somebody deletes a state set there you you need to know about that from an",
    "start": "2787960",
    "end": "2793480"
  },
  {
    "text": "operator yes yeah so we're we're still reconciling periodically to make sure",
    "start": "2793480",
    "end": "2798760"
  },
  {
    "text": "that what's running on the member clusters is correct okay thanks",
    "start": "2798760",
    "end": "2805960"
  },
  {
    "text": "thanks so regarding to The Rolling upgrade for the cluster sorry I don't",
    "start": "2806359",
    "end": "2813280"
  },
  {
    "text": "know if we're positioned badly to the microphone but we've been struggling to hear sorry so regarding to The Rolling",
    "start": "2813280",
    "end": "2818440"
  },
  {
    "text": "upgrade to the multicluster so do you provide the availability to like test if",
    "start": "2818440",
    "end": "2824960"
  },
  {
    "text": "the new feature is running well like on a single cluster well at like um the",
    "start": "2824960",
    "end": "2831839"
  },
  {
    "text": "deployment is finished by the first cluster so like is that uh like",
    "start": "2831839",
    "end": "2838359"
  },
  {
    "text": "seamlessly so that there is no chance to test on like um like the separate",
    "start": "2838359",
    "end": "2844960"
  },
  {
    "text": "cluster to see if the feature is deployed or not so if if I understand correctly if",
    "start": "2844960",
    "end": "2852000"
  },
  {
    "text": "we if we have a roll back mechanism if we do a rolling upgrade on multicluster [Music]",
    "start": "2852000",
    "end": "2857359"
  },
  {
    "text": "M do you have the answer I know we have visibility of it and we expose it in the the status of",
    "start": "2857359",
    "end": "2864559"
  },
  {
    "text": "the custom resource that you deploy in the central cluster I don't know if that answers the",
    "start": "2864559",
    "end": "2870480"
  },
  {
    "text": "question um so it will be exposed from the central cluster that like um like",
    "start": "2870480",
    "end": "2877640"
  },
  {
    "text": "the first uh some of the feature is deployed to like maybe one or several like part of those multic cluster",
    "start": "2877640",
    "end": "2885359"
  },
  {
    "text": "right so like uh maybe I can have a denies to only just a separate cluster",
    "start": "2885359",
    "end": "2893040"
  },
  {
    "text": "so that I can test if the feature is already deployed there or like if some Arrow like comes up so that I can uh",
    "start": "2893040",
    "end": "2900440"
  },
  {
    "text": "roll back to deployment or something so you you you upgrade only one member and",
    "start": "2900440",
    "end": "2907440"
  },
  {
    "text": "if it goes okay then upgrade the others yeah actually that's what we are currently doing now so I'm wondering if",
    "start": "2907440",
    "end": "2913680"
  },
  {
    "text": "that is implemented in your system so maybe it's automatically uh process so",
    "start": "2913680",
    "end": "2919800"
  },
  {
    "text": "you roll up rolling update to all the multiple uh clusters so I'm wondering if",
    "start": "2919800",
    "end": "2925119"
  },
  {
    "text": "there is a point that I can check if that's running well uh in the like deployed cluster with this new feature",
    "start": "2925119",
    "end": "2932960"
  },
  {
    "text": "so for the actual kind of functional feat features of mongodb itself we don't have that at the operator level um",
    "start": "2932960",
    "end": "2940480"
  },
  {
    "text": "there's a lot of interaction with the management server which provides the automation conf configures all that um",
    "start": "2940480",
    "end": "2945839"
  },
  {
    "text": "at a kind of upgrade level for instance though if we if we upgrade a given",
    "start": "2945839",
    "end": "2951160"
  },
  {
    "text": "member of a replica set we wait till it comes up and reports healthy before we then move on to any of the others okay",
    "start": "2951160",
    "end": "2957200"
  },
  {
    "text": "so thank you if if it failed and got stuck we wouldn't we wouldn't proceed okay I see",
    "start": "2957200",
    "end": "2964520"
  },
  {
    "text": "yeah cool I think that's all the question thank you everyone for",
    "start": "2964520",
    "end": "2970640"
  },
  {
    "text": "coming",
    "start": "2970680",
    "end": "2973680"
  }
]