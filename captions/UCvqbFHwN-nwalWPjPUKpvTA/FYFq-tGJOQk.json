[
  {
    "text": "so thank you for coming to our talk about uh building up attitude on kubernetes or the immersion cells",
    "start": "0",
    "end": "6359"
  },
  {
    "text": "pattern data so first there won't be any line of yaml in this presentation so you",
    "start": "6359",
    "end": "11639"
  },
  {
    "text": "can get out now if that's what you came for and also we're going to talk about what we are putting in our communities",
    "start": "11639",
    "end": "17580"
  },
  {
    "text": "what workload we are running so maybe not the best one in in uh kubecon so I'm",
    "start": "17580",
    "end": "24840"
  },
  {
    "text": "Alex Trek I work at Dailymotion as a senior devops engineer and and I'm Sarah",
    "start": "24840",
    "end": "31260"
  },
  {
    "text": "I'm devops architect I at the emotion",
    "start": "31260",
    "end": "36500"
  },
  {
    "text": "we are the devops team and we are building kubernetes platform over uh",
    "start": "37340",
    "end": "43500"
  },
  {
    "text": "different Cloud providers and on-premise we are also helping developers to uh",
    "start": "43500",
    "end": "52920"
  },
  {
    "text": "to set in predictions all the workloads that means we use to deploy the chcds",
    "start": "52920",
    "end": "59160"
  },
  {
    "text": "and to enforce best practices for them and we are we are also using we are also",
    "start": "59160",
    "end": "67320"
  },
  {
    "text": "doing all the Encore stuff we are companies that build video",
    "start": "67320",
    "end": "74040"
  },
  {
    "text": "platform you should not know us we have like four billion views per month and we",
    "start": "74040",
    "end": "81299"
  },
  {
    "text": "have two main projects so the dailymotion.com service that is our main platform and uh",
    "start": "81299",
    "end": "89880"
  },
  {
    "text": "a network of partners with an advertising platforms for other customers and we serve all the metrics",
    "start": "89880",
    "end": "97920"
  },
  {
    "text": "and all the partners datas through to it and we will talk about that in this presentation",
    "start": "97920",
    "end": "104000"
  },
  {
    "text": "so yes I introduced you uh what Apache do it is but first uh does anyone here",
    "start": "104000",
    "end": "110220"
  },
  {
    "text": "run any Druid quick show of hands please really I'm sorry great so basically what do it",
    "start": "110220",
    "end": "120600"
  },
  {
    "text": "is a it's a database you use it to for analytics capacities it's all up you",
    "start": "120600",
    "end": "127740"
  },
  {
    "text": "want to use it for time series and that's a lot of benefits like time series and and the column it's a",
    "start": "127740",
    "end": "135300"
  },
  {
    "text": "columnar database also so plenty of exciting capacities",
    "start": "135300",
    "end": "140700"
  },
  {
    "text": "um the way here we have a presentation of the architecture it's pretty complex",
    "start": "140700",
    "end": "146040"
  },
  {
    "text": "there are different ways to run it basically let's take a top-down approach",
    "start": "146040",
    "end": "151560"
  },
  {
    "text": "you have zero query node at the top here which are with what we will interact",
    "start": "151560",
    "end": "157739"
  },
  {
    "text": "basically they handle all the routing all the queries that are made to the historicals that has a nodes that will",
    "start": "157739",
    "end": "165300"
  },
  {
    "text": "serve the data the indexer have the room the mirror",
    "start": "165300",
    "end": "170400"
  },
  {
    "text": "walls of the historical is in that they ingest the data into your clusters to be served by the historicals uh underneath",
    "start": "170400",
    "end": "177300"
  },
  {
    "text": "we have a layer of deep storage we are running on well S3 protocol at our CSP",
    "start": "177300",
    "end": "183019"
  },
  {
    "text": "and on the side you have as a control plan which is a actually fairly complex",
    "start": "183019",
    "end": "188940"
  },
  {
    "text": "because here we can see Apache zookeeper metadata storage actually it's a",
    "start": "188940",
    "end": "194099"
  },
  {
    "text": "database we are running some MySQL and you have some other Druid nodes so I",
    "start": "194099",
    "end": "199319"
  },
  {
    "text": "didn't know all the Persistence of the data also balancing so that all your historicals are in synchronization",
    "start": "199319",
    "end": "207659"
  },
  {
    "text": "um no I'll explain to you our query works with Druid so basically we have a",
    "start": "207659",
    "end": "213060"
  },
  {
    "text": "scatter gather approach so when a request is made to the broker it will",
    "start": "213060",
    "end": "218220"
  },
  {
    "text": "scatter this query to all the historicals and that will each executes a partial",
    "start": "218220",
    "end": "223799"
  },
  {
    "text": "query and then send back to the data to be as a result of the partial results to be aggregated as a gather bot at the",
    "start": "223799",
    "end": "231720"
  },
  {
    "text": "broker [Music] it has uh it's felt nice because because",
    "start": "231720",
    "end": "237840"
  },
  {
    "text": "actually uh if you have good patterns you can have all the ram intensive stuff",
    "start": "237840",
    "end": "242879"
  },
  {
    "text": "in your historicals and just ends up doing reconciliation almost on your",
    "start": "242879",
    "end": "248220"
  },
  {
    "text": "Brokers which is very efficient since all your historicals are built with a lot of ram",
    "start": "248220",
    "end": "254400"
  },
  {
    "text": "to be able to serve that are very efficiently um thank you",
    "start": "254400",
    "end": "260220"
  },
  {
    "text": "um so why use Druid um basically I've listed you the pros but release a big and the big thing here",
    "start": "260220",
    "end": "267660"
  },
  {
    "text": "is the ability to do a lab on time series it's rather unique can do that",
    "start": "267660",
    "end": "273540"
  },
  {
    "text": "with other stuff but not as well and also you can do it in uh with columnar",
    "start": "273540",
    "end": "279419"
  },
  {
    "text": "data so it's well it's a standard in terms of olap so this way you can just",
    "start": "279419",
    "end": "285479"
  },
  {
    "text": "add in other other columns as much as you want and never have to think about your schema or what you're using as long",
    "start": "285479",
    "end": "292800"
  },
  {
    "text": "as you aggregate the data it's easy to scale because actually you can scale",
    "start": "292800",
    "end": "299160"
  },
  {
    "text": "horizontally all your nodes you can increase the number of nodes you can just increase the storage if you if such",
    "start": "299160",
    "end": "305940"
  },
  {
    "text": "as what you need so very powerful on this and yeah also documentation is great uh",
    "start": "305940",
    "end": "312660"
  },
  {
    "text": "which ties into a bigger a big cons here it's that uh the configuration is pretty",
    "start": "312660",
    "end": "318660"
  },
  {
    "text": "complex once you have your setup up and running up and running you will need to do quite a bit of tuning with your",
    "start": "318660",
    "end": "324240"
  },
  {
    "text": "values you need to adapt it to all your use cases to really make the best out of",
    "start": "324240",
    "end": "330479"
  },
  {
    "text": "what you have otherwise you can just swarm or run at the problem it will eventually work but it's not exactly why",
    "start": "330479",
    "end": "336900"
  },
  {
    "text": "we are here for and uh yes as I mentioned the complex is the setup is",
    "start": "336900",
    "end": "342720"
  },
  {
    "text": "pretty complex we have a lot of moving parts and our do",
    "start": "342720",
    "end": "348720"
  },
  {
    "text": "it run like five different nodes the different type of nodes so that can be a",
    "start": "348720",
    "end": "353940"
  },
  {
    "text": "bit of a so when you're doing a rollout you need to make sure that everything is in the right order those kind of things",
    "start": "353940",
    "end": "360120"
  },
  {
    "text": "and finally you need to need it because that's something that's pretty unique",
    "start": "360120",
    "end": "366419"
  },
  {
    "text": "and that's only adapted for a bunch of use case use cases that are fairly spread mostly I'm thinking about",
    "start": "366419",
    "end": "373199"
  },
  {
    "text": "statistics here uh it's very powerful for that especially since we are using a",
    "start": "373199",
    "end": "378419"
  },
  {
    "text": "lot of approximate counts to to to save on group buys this kind of",
    "start": "378419",
    "end": "384840"
  },
  {
    "text": "queries which makes it very fast um yeah no I'm gonna briefly present to",
    "start": "384840",
    "end": "392039"
  },
  {
    "text": "you as a drilled UI I'm not sure we we can see it quite right we have some color problems uh but basically this is",
    "start": "392039",
    "end": "399660"
  },
  {
    "text": "a standard SQL SQL query you can interact with do it by using a native",
    "start": "399660",
    "end": "406380"
  },
  {
    "text": "root coil language which looks like which is a bunch of Json where you list all the operations you need and in which",
    "start": "406380",
    "end": "413039"
  },
  {
    "text": "order or you can use a SQL and we have a translation engine here it works fairly",
    "start": "413039",
    "end": "420660"
  },
  {
    "text": "well especially with the latest version but you always run the risk to to have",
    "start": "420660",
    "end": "427020"
  },
  {
    "text": "inefficient queries because we are on a fairly specific data patterns so",
    "start": "427020",
    "end": "432539"
  },
  {
    "text": "specific queries run best here and uh yeah also I didn't mention uh you",
    "start": "432539",
    "end": "438180"
  },
  {
    "text": "have two kinds of uh well you have three kinds of fields into it you have your",
    "start": "438180",
    "end": "443520"
  },
  {
    "text": "timestamp that will be your primary aggregation means that is unique and I'll show you live",
    "start": "443520",
    "end": "450240"
  },
  {
    "text": "Dimensions which are basically strings and can even be erased not something we are using I think it's dangerous to be",
    "start": "450240",
    "end": "458280"
  },
  {
    "text": "honest and uh you always I I don't have any here but you have floats for all your aggregation for all the stats you",
    "start": "458280",
    "end": "465180"
  },
  {
    "text": "want to be you want to be serving that will be your your metrics also I wanted to mention so",
    "start": "465180",
    "end": "473340"
  },
  {
    "text": "yeah let's talk about ingestion uh we usually ingest data hourly and daily uh",
    "start": "473340",
    "end": "481380"
  },
  {
    "text": "basically the data is reconciled at the end of the day it's standard business practice for",
    "start": "481380",
    "end": "487460"
  },
  {
    "text": "advertising data so some things are just done about everywhere and we have",
    "start": "487460",
    "end": "494039"
  },
  {
    "text": "usually the refreshness of a bit less than two hours like once an event",
    "start": "494039",
    "end": "500280"
  },
  {
    "text": "happens two hour afterwards it is counted in our Druid cluster we use",
    "start": "500280",
    "end": "505919"
  },
  {
    "text": "Apache airflow for that so it's an orchestrator I'm sure a bunch of you are familiar with the way with it it runs",
    "start": "505919",
    "end": "513539"
  },
  {
    "text": "quite well the only problem we have is when we want to run very heavy tasks so when we are backfilling an entire table",
    "start": "513539",
    "end": "520380"
  },
  {
    "text": "it's called data source into it but it's a table basically you can take some days",
    "start": "520380",
    "end": "526680"
  },
  {
    "text": "it can be very cumbersome it can kill the cluster basically so what we are doing is we are popping",
    "start": "526680",
    "end": "533540"
  },
  {
    "text": "Adobe clusters we use it as a SAS and it runs well it's",
    "start": "533540",
    "end": "538800"
  },
  {
    "text": "not my favorite part of it to to be honest I I would advise against doing it",
    "start": "538800",
    "end": "544140"
  },
  {
    "text": "you can find other patterns especially with latest version of the but well Legacy is legacy",
    "start": "544140",
    "end": "551360"
  },
  {
    "text": "so now we are going to talk about how we set up Hardware down communities just to know are you running a stateful",
    "start": "551519",
    "end": "557700"
  },
  {
    "text": "workloads on your on your kubernetes clusters okay we got some okay in production",
    "start": "557700",
    "end": "564060"
  },
  {
    "text": "yes okay so that's nice uh just a brief uh recap",
    "start": "564060",
    "end": "569399"
  },
  {
    "text": "uh for some people that don't know operators the goal of the operators is",
    "start": "569399",
    "end": "574800"
  },
  {
    "text": "to have a decorative States and to have all the logic on the operator it will",
    "start": "574800",
    "end": "580140"
  },
  {
    "text": "reconcile the state and apply the resources each time you change the state it will reconcile apply to change and",
    "start": "580140",
    "end": "587700"
  },
  {
    "text": "Report the status that's uh how it works almost all the case",
    "start": "587700",
    "end": "594019"
  },
  {
    "text": "we have some use case at the emotion and we're earning a lot of operators these are the main use case with a cool so we",
    "start": "594019",
    "end": "601500"
  },
  {
    "text": "have data management database management application monitoring application deployment monitoring and",
    "start": "601500",
    "end": "607320"
  },
  {
    "text": "logging machine learning and University management we use uh with grid some of those like",
    "start": "607320",
    "end": "615000"
  },
  {
    "text": "set module to provide certificates and Ingress and on ingresses and we are",
    "start": "615000",
    "end": "622320"
  },
  {
    "text": "using others operators search Arts blogging operator to manage options dynamically",
    "start": "622320",
    "end": "630240"
  },
  {
    "text": "for doing operator it's a single adjuster just a point uh recently we",
    "start": "631380",
    "end": "636480"
  },
  {
    "text": "Fork the Ripple with the maintainers it's no maintain on this GitHub repository is not on the drilled",
    "start": "636480",
    "end": "642779"
  },
  {
    "text": "operator the old one for Apache reason they don't have any",
    "start": "642779",
    "end": "649140"
  },
  {
    "text": "maintenance currently working on the product but all those people do",
    "start": "649140",
    "end": "656100"
  },
  {
    "text": "um it's containing one on and only one crd for the moment that is with clusters and",
    "start": "656100",
    "end": "662339"
  },
  {
    "text": "the crd is deploying the Clusters with several type of nuts that we have",
    "start": "662339",
    "end": "669360"
  },
  {
    "text": "presented and so the routers mineral measures coordinators historical brokers",
    "start": "669360",
    "end": "676200"
  },
  {
    "text": "basically it's by default all stateful sets but it's also you can change the",
    "start": "676200",
    "end": "682320"
  },
  {
    "text": "kind depending of the type of nuts so for example in our UK in our use case we",
    "start": "682320",
    "end": "688620"
  },
  {
    "text": "have only historicals that are stateful um and that it was your operator",
    "start": "688620",
    "end": "696660"
  },
  {
    "text": "the goal of these operators is to do these features so rolling deploy that",
    "start": "696660",
    "end": "702720"
  },
  {
    "text": "means when you made the changement modification of the configuration on any",
    "start": "702720",
    "end": "709740"
  },
  {
    "text": "grid objects on any parts of crd it will redeploy the cluster on the good way",
    "start": "709740",
    "end": "716459"
  },
  {
    "text": "that's mean in and order we will talk a bit later about it but with the",
    "start": "716459",
    "end": "721860"
  },
  {
    "text": "historical first and the other nodes Parts by parts",
    "start": "721860",
    "end": "727440"
  },
  {
    "text": "then it provides Auto scaling and HPA on all the components also volume expansion",
    "start": "727440",
    "end": "733800"
  },
  {
    "text": "that's useful for historicals without any description we can we didn't use did but we can",
    "start": "733800",
    "end": "743040"
  },
  {
    "text": "manage our funds PVC and delete the receipts of the the tool settings are",
    "start": "743040",
    "end": "749220"
  },
  {
    "text": "living and the features that we really want to use is steering management that",
    "start": "749220",
    "end": "755820"
  },
  {
    "text": "means you can create several historical nodes and serve different type of data",
    "start": "755820",
    "end": "761880"
  },
  {
    "text": "in this historical that's really interesting and the operator is doing it",
    "start": "761880",
    "end": "767399"
  },
  {
    "text": "pretty well Yeah Yeah so basically what happened is",
    "start": "767399",
    "end": "773160"
  },
  {
    "text": "uh we had a legacy cluster uh by Druid um I try it started as a POC uh it ended",
    "start": "773160",
    "end": "781620"
  },
  {
    "text": "up in production but it was still kind of a POC so also it mostly it was",
    "start": "781620",
    "end": "787860"
  },
  {
    "text": "working uh it was kind of reliable I'd say but the main issue was that we",
    "start": "787860",
    "end": "793320"
  },
  {
    "text": "didn't knew what we wanted to do with it so basically we provided this to the",
    "start": "793320",
    "end": "799320"
  },
  {
    "text": "data engineers and they had fun with it so we ended up scaling a lot in terms of",
    "start": "799320",
    "end": "805459"
  },
  {
    "text": "storage we didn't we didn't think we would have that much stuff like this and",
    "start": "805459",
    "end": "811019"
  },
  {
    "text": "so after about two years we ended up with an old version because we had never",
    "start": "811019",
    "end": "816720"
  },
  {
    "text": "made the F4 to update and a bench of need that had changed so it was time to",
    "start": "816720",
    "end": "822240"
  },
  {
    "text": "do a refactorization a re-architecture even um we got rid of tidyb I really liked",
    "start": "822240",
    "end": "829320"
  },
  {
    "text": "idb it's a good product but the problem is we used it on our cluster we run on a",
    "start": "829320",
    "end": "834540"
  },
  {
    "text": "CSP as a database in cluster and it didn't really suit our need because it's good",
    "start": "834540",
    "end": "841800"
  },
  {
    "text": "for performances stuff like this but what we needed was Simplicity basically all you need is a MySQL to do all",
    "start": "841800",
    "end": "848940"
  },
  {
    "text": "project rest to do to do metadata data so in the end we ended up getting it but",
    "start": "848940",
    "end": "855240"
  },
  {
    "text": "we are pretty happy about the the experience we earned on it and we intend",
    "start": "855240",
    "end": "860579"
  },
  {
    "text": "to reuse it on other other projects and also yes we had a lot of pain points mostly roll out like it took 28 hours we",
    "start": "860579",
    "end": "869700"
  },
  {
    "text": "couldn't do any operations on our node pools it was very complicated so it's",
    "start": "869700",
    "end": "875160"
  },
  {
    "text": "the main goal was to change the the this fact we were using local SSD which is",
    "start": "875160",
    "end": "881220"
  },
  {
    "text": "good for the price uh very good performance for the price but it wasn't",
    "start": "881220",
    "end": "886380"
  },
  {
    "text": "satisfying enough in the end we calculated that we saved a lot of money by switching to PVC which are twice as",
    "start": "886380",
    "end": "893399"
  },
  {
    "text": "expensive but just the time as engine doing time saved was massive",
    "start": "893399",
    "end": "898760"
  },
  {
    "text": "and so how we did that is that we used githubs through Flag City",
    "start": "898760",
    "end": "904860"
  },
  {
    "text": "which is something we use uh very widely basically we have moved about two-thirds",
    "start": "904860",
    "end": "911459"
  },
  {
    "text": "of our projects to flux CD and all new projects are on it obviously and we",
    "start": "911459",
    "end": "916680"
  },
  {
    "text": "intend to be done by the end of the year I'd say um so yes that was what we why we needed",
    "start": "916680",
    "end": "923760"
  },
  {
    "text": "to to change uh all of that also's deployment was manual uh it was really a",
    "start": "923760",
    "end": "930120"
  },
  {
    "text": "bunch of legacies that was left to rot in a corner and so we needed to take",
    "start": "930120",
    "end": "935399"
  },
  {
    "text": "back the ownership uh of our Stacks that's why we did this Mission and basically it's a point to us to build uh",
    "start": "935399",
    "end": "942480"
  },
  {
    "text": "brand new direct clusters that was in face with our needs and as that enforce back best practice and also that would",
    "start": "942480",
    "end": "948959"
  },
  {
    "text": "cost less because we were using big machines with uh with 128 gigabytes of",
    "start": "948959",
    "end": "955380"
  },
  {
    "text": "RAM and well money to save yes and we we are going to talk about",
    "start": "955380",
    "end": "962579"
  },
  {
    "text": "this uh this is the no the current state of our root crosses so we have the",
    "start": "962579",
    "end": "969360"
  },
  {
    "text": "standard note tool with all the main parts so the coordinates those the",
    "start": "969360",
    "end": "974579"
  },
  {
    "text": "routers Brokers and uh the external parts of the studio I would like to",
    "start": "974579",
    "end": "980639"
  },
  {
    "text": "remind that zookeeper and man cardi are not deployed with the operator that I'm sure that are deployed separately",
    "start": "980639",
    "end": "988519"
  },
  {
    "text": "is not managed by the operator it just dip the to the operator just deploying",
    "start": "988519",
    "end": "994320"
  },
  {
    "text": "Ingress uh so we have this on a separate nade pool we have a node pool dedicated",
    "start": "994320",
    "end": "999600"
  },
  {
    "text": "to the exposition of the service with an Ingress and the grass controller and go",
    "start": "999600",
    "end": "1006560"
  },
  {
    "text": "Application that serves the API and uh all the statistics and that's queries",
    "start": "1006560",
    "end": "1012680"
  },
  {
    "text": "that read and we have then the stateful nodes so the historicals there are a",
    "start": "1012680",
    "end": "1018920"
  },
  {
    "text": "pretty huge uh in memory utilization and on storage we have like uh if I remember",
    "start": "1018920",
    "end": "1025640"
  },
  {
    "text": "correctly it was material went there are more than one octave iPods of PVCs and",
    "start": "1025640",
    "end": "1032660"
  },
  {
    "text": "we have a atg no we have 50 gigabytes of memory",
    "start": "1032660",
    "end": "1039140"
  },
  {
    "text": "storage spots these workloads are very memory",
    "start": "1039140",
    "end": "1044298"
  },
  {
    "text": "intensive and that's a point of to it and that's why we do that we have a part",
    "start": "1044299",
    "end": "1050120"
  },
  {
    "text": "of our 3D historicals to reduce costs that are in spot instance and the",
    "start": "1050120",
    "end": "1056000"
  },
  {
    "text": "because the the Druid cluster when it kill a historical uh just uh does not",
    "start": "1056000",
    "end": "1063380"
  },
  {
    "text": "return the queries that are running and it's retrying on other nodes and we have a verification Factor at two so we can",
    "start": "1063380",
    "end": "1072559"
  },
  {
    "text": "backboard the query on all the nodes automatically and that save us a lot of",
    "start": "1072559",
    "end": "1077780"
  },
  {
    "text": "money that's almost everything for these spots",
    "start": "1077780",
    "end": "1083720"
  },
  {
    "text": "yes so as I said we wanted to save money wanted to improve performances so what",
    "start": "1083720",
    "end": "1089120"
  },
  {
    "text": "we needed was benchmarking what we did was obviously we made a tester a set",
    "start": "1089120",
    "end": "1095360"
  },
  {
    "text": "test of requests so you have the number of seconds to run the world test here as",
    "start": "1095360",
    "end": "1100520"
  },
  {
    "text": "a smaller is better and obviously the yellow one I guess it's yellow here is",
    "start": "1100520",
    "end": "1106520"
  },
  {
    "text": "uh the old one and what we did was we created uh two sets for Benchmark",
    "start": "1106520",
    "end": "1112940"
  },
  {
    "text": "basically one test to test all the limit the edge cases uh basically we're",
    "start": "1112940",
    "end": "1119480"
  },
  {
    "text": "covering all this data in the cluster trying to break it it allowed us to test resiliency as well as uh performances in",
    "start": "1119480",
    "end": "1127520"
  },
  {
    "text": "this context which is not really conducive of real real usage and also",
    "start": "1127520",
    "end": "1134179"
  },
  {
    "text": "using real real requests taken for from production so we ended up with the with",
    "start": "1134179",
    "end": "1139400"
  },
  {
    "text": "this test uh we have more fine-grained results but those are the as a summary",
    "start": "1139400",
    "end": "1145760"
  },
  {
    "text": "um we tested a bunch of configuration uh one interesting stuff that we that we",
    "start": "1145760",
    "end": "1151280"
  },
  {
    "text": "did was we mimicked what we had so basically we had local ssds and instead of replacing four localizes these with",
    "start": "1151280",
    "end": "1157580"
  },
  {
    "text": "one PPC we replaced each time uh one a local CSG with one PVC to ensure good",
    "start": "1157580",
    "end": "1164419"
  },
  {
    "text": "performances and we we saw very very important differences when using more PVCs and it cost exactly the same so it",
    "start": "1164419",
    "end": "1172400"
  },
  {
    "text": "was really almost a hack pretty happy about that also we tried uh remote",
    "start": "1172400",
    "end": "1178280"
  },
  {
    "text": "caching so we used to have caching now do it but caching is very good for do it because as I mentioned earlier you have",
    "start": "1178280",
    "end": "1185059"
  },
  {
    "text": "also States for your queries so you can have query results partial query results",
    "start": "1185059",
    "end": "1190640"
  },
  {
    "text": "or even data caching with segments on the historicals they use the spare Ram to do that so the more caching you have",
    "start": "1190640",
    "end": "1198200"
  },
  {
    "text": "the more efficiency you can have especially when you are serving traffic for statistics with often redundancy",
    "start": "1198200",
    "end": "1204919"
  },
  {
    "text": "between your request everybody wants to see the data from the last seven days stuff like this on Monday so yes and in",
    "start": "1204919",
    "end": "1213740"
  },
  {
    "text": "the end remote caching was very interesting in terms of performances it was very easy to set up with memcache d",
    "start": "1213740",
    "end": "1220400"
  },
  {
    "text": "and M chart as Cyril mentioned I really advise on adding caching when you can",
    "start": "1220400",
    "end": "1225799"
  },
  {
    "text": "trying to do it really I think it's uh it's neat",
    "start": "1225799",
    "end": "1231620"
  },
  {
    "text": "and yeah and so on no I'll talk about the migration process so this was the",
    "start": "1231620",
    "end": "1237200"
  },
  {
    "text": "biggest headache I'd say with the configuration because what we needed was",
    "start": "1237200",
    "end": "1242299"
  },
  {
    "text": "um node on time so we need to do a double run of our clusters the problem is we add to have",
    "start": "1242299",
    "end": "1249200"
  },
  {
    "text": "concurrency between operators who which were now the same version so we had to",
    "start": "1249200",
    "end": "1255320"
  },
  {
    "text": "cheat a bit and what we did we started by killing the old operators",
    "start": "1255320",
    "end": "1260480"
  },
  {
    "text": "uh when that was done we steal our crg we still had our crg in place with none",
    "start": "1260480",
    "end": "1266179"
  },
  {
    "text": "of the control of the operator but it was fine especially with someone doing an operation on it and afterwards we set",
    "start": "1266179",
    "end": "1273980"
  },
  {
    "text": "up the new operator all that with flax CD so everything was ready to roll out it was really really easier with this uh",
    "start": "1273980",
    "end": "1282679"
  },
  {
    "text": "to to perform this operation then comes the event took your spot so we scaled",
    "start": "1282679",
    "end": "1288500"
  },
  {
    "text": "down some nodes the ones that write information the database so that we were able to do a database migration because",
    "start": "1288500",
    "end": "1295280"
  },
  {
    "text": "we wanted at the same time to change what we used so we scaled down the nodes that right to the DBS that interact with",
    "start": "1295280",
    "end": "1301940"
  },
  {
    "text": "it we killed all the injection tasks so that was our only downtime we had a bit",
    "start": "1301940",
    "end": "1307460"
  },
  {
    "text": "of less freshness on our data which is for our use cases perfectly fine and",
    "start": "1307460",
    "end": "1314120"
  },
  {
    "text": "then we set up all the all we had with the new cluster ended up with uh yes and",
    "start": "1314120",
    "end": "1321860"
  },
  {
    "text": "and uh and so we migrated the database and in the end we just switched to the new cluster and we were able to kill the",
    "start": "1321860",
    "end": "1328460"
  },
  {
    "text": "old one after after two days so so all operation uh took about one hour to",
    "start": "1328460",
    "end": "1335659"
  },
  {
    "text": "migrate about 13 terabytes of data which is nice to realness and uh we didn't",
    "start": "1335659",
    "end": "1343880"
  },
  {
    "text": "have any downtime and the most time consuming was actually dumping the database and restoring it",
    "start": "1343880",
    "end": "1351320"
  },
  {
    "text": "um I guess we could have made Improvement on that but we we were happy about several results of this and now",
    "start": "1351320",
    "end": "1357860"
  },
  {
    "text": "we'll talk about monitoring and Exposition yep so for our use case we",
    "start": "1357860",
    "end": "1362900"
  },
  {
    "text": "are using data dug but there is a lot of plugins so you can see all of them there",
    "start": "1362900",
    "end": "1368919"
  },
  {
    "text": "the open Matrix Primitives and strategy uh plugins are available and we have uh",
    "start": "1368919",
    "end": "1377419"
  },
  {
    "text": "two ingresses so that we deploy one for the console that we showed before",
    "start": "1377419",
    "end": "1382700"
  },
  {
    "text": "and one for the apis that is protected with mtls and we use set manager to",
    "start": "1382700",
    "end": "1388340"
  },
  {
    "text": "prove using it also certificates on all case that's all we did uh just uh a",
    "start": "1388340",
    "end": "1395780"
  },
  {
    "text": "point we also use a data pen to monitorize all the all the traces and",
    "start": "1395780",
    "end": "1403220"
  },
  {
    "text": "that's useful sometimes on go API uh we are going to talk how we get",
    "start": "1403220",
    "end": "1409039"
  },
  {
    "text": "upstairs get a grid sorry uh that's mean how we deploy original cluster so as",
    "start": "1409039",
    "end": "1416480"
  },
  {
    "text": "Alex said we are using a fixed CD in our case that's",
    "start": "1416480",
    "end": "1421940"
  },
  {
    "text": "working with two controllers mainly the customized controllers and the M controllers and with different kind of",
    "start": "1421940",
    "end": "1428419"
  },
  {
    "text": "sources so in our case we have GitHub sources and um and animal repositories and we",
    "start": "1428419",
    "end": "1436280"
  },
  {
    "text": "deployed several customization to uh to Inner clusters and we are doing",
    "start": "1436280",
    "end": "1443659"
  },
  {
    "text": "some sharing too that's mean we have first customization that this that is",
    "start": "1443659",
    "end": "1449059"
  },
  {
    "text": "deploying all the CLD stuff and all the operators another ones that deploy all",
    "start": "1449059",
    "end": "1454400"
  },
  {
    "text": "the business names ways one with the memcache D and the skipper",
    "start": "1454400",
    "end": "1459440"
  },
  {
    "text": "and finalists that we trust us that's allows us to stop the to suspend the",
    "start": "1459440",
    "end": "1465200"
  },
  {
    "text": "customization if we have to do a modification production and then to backboard if we have to back fix faster",
    "start": "1465200",
    "end": "1470600"
  },
  {
    "text": "and that uh authorizes also to have dependencies between our deployments that's really nice and it worked",
    "start": "1470600",
    "end": "1477260"
  },
  {
    "text": "perfectly well on our common operations we talk about",
    "start": "1477260",
    "end": "1483320"
  },
  {
    "text": "automatic rollout that's a feature that the operator bring that's mean when we we made the modification it will always",
    "start": "1483320",
    "end": "1490760"
  },
  {
    "text": "roll out in this order so that's mean if the historicals are impacted so historicals will restart one by one or",
    "start": "1490760",
    "end": "1497960"
  },
  {
    "text": "two by two depending on your update strategy and then it will take Zin zero",
    "start": "1497960",
    "end": "1505520"
  },
  {
    "text": "zero loads indexes so every time but the coordinators and the brokers",
    "start": "1505520",
    "end": "1511400"
  },
  {
    "text": "another part is the storage update in Grid we want to add datas sometimes and",
    "start": "1511400",
    "end": "1518360"
  },
  {
    "text": "metadatas 2 so we want to scale horizontally and vertically the operator",
    "start": "1518360",
    "end": "1524419"
  },
  {
    "text": "provides a volume expansion dynamically and it works really well",
    "start": "1524419",
    "end": "1530600"
  },
  {
    "text": "um and another point is recently we had two migrate from a CSI to another one",
    "start": "1530600",
    "end": "1536840"
  },
  {
    "text": "and the operator I'll post a lot to in this case",
    "start": "1536840",
    "end": "1544580"
  },
  {
    "text": "that's mean um when we are challenging the configuration only the CSI on the",
    "start": "1544580",
    "end": "1550039"
  },
  {
    "text": "new SDS State first we change and it will load the data one by one so we",
    "start": "1550039",
    "end": "1555860"
  },
  {
    "text": "don't have with our pickaxe",
    "start": "1555860",
    "end": "1559419"
  },
  {
    "text": "so yes we're going to give you other feedback as I said everything we did was perfect it was very nice I",
    "start": "1564140",
    "end": "1570440"
  },
  {
    "text": "congratulations we congratulated ourselves a lot so well no so we had a few shoes one of the issue",
    "start": "1570440",
    "end": "1578779"
  },
  {
    "text": "we encountered is performances so basically we are using",
    "start": "1578779",
    "end": "1583820"
  },
  {
    "text": "um SQL as it's a password is not maintained a",
    "start": "1583820",
    "end": "1588980"
  },
  {
    "text": "lot we have a lot of changes of teams and so we don't have a lot of skills",
    "start": "1588980",
    "end": "1594679"
  },
  {
    "text": "around using native queries right now and the problem when you do that is that you don't have control over what is",
    "start": "1594679",
    "end": "1600080"
  },
  {
    "text": "running on your clusters and sometimes you end up with big inefficiencies like you are running a bunch of goodbyes",
    "start": "1600080",
    "end": "1605360"
  },
  {
    "text": "you're running aggregations of aggregations while it's not done for that and you don't need that so that's",
    "start": "1605360",
    "end": "1612500"
  },
  {
    "text": "one of the points in the end we were able to make it do what we wanted by",
    "start": "1612500",
    "end": "1618020"
  },
  {
    "text": "playing a bit with a sequel by using the latest versions We which have a very",
    "start": "1618020",
    "end": "1623120"
  },
  {
    "text": "good translation and design to be honest it's really getting there so you can just use SQL and if you have bad",
    "start": "1623120",
    "end": "1629419"
  },
  {
    "text": "performances rewrite your SQL to adapt it to the to the use case and we'll also",
    "start": "1629419",
    "end": "1636559"
  },
  {
    "text": "add a lot of tweaking to do uh about the grid configuration as I mentioned there is a lot of good documentation so we",
    "start": "1636559",
    "end": "1642980"
  },
  {
    "text": "will explain what is default how you should do your calculation but you need to get in there and pull pull your your",
    "start": "1642980",
    "end": "1649520"
  },
  {
    "text": "calculator your Calculus your Excel sheet and do a bunch of operations on",
    "start": "1649520",
    "end": "1655159"
  },
  {
    "text": "how many threads will I need how many buffers which sizes what does it mean for my broker stuff like this which is",
    "start": "1655159",
    "end": "1662179"
  },
  {
    "text": "something really that you you can do a pretty good job on your own but in the",
    "start": "1662179",
    "end": "1667520"
  },
  {
    "text": "end you will need to test it to make sure that everything makes sense and that it runs properly also about the migration we ended up",
    "start": "1667520",
    "end": "1676100"
  },
  {
    "text": "with a big surprise in that when we migrated all our data we changed the",
    "start": "1676100",
    "end": "1682460"
  },
  {
    "text": "bucket we used underneath and we didn't we we didn't I didn't know actually that",
    "start": "1682460",
    "end": "1690679"
  },
  {
    "text": "uh in the database for all your data you have the location the bucket it uses",
    "start": "1690679",
    "end": "1695840"
  },
  {
    "text": "it's not needed because dude knows where in which bucket to to search for but",
    "start": "1695840",
    "end": "1702080"
  },
  {
    "text": "it's specified in there as well and we didn't change it so we we ended up with",
    "start": "1702080",
    "end": "1707480"
  },
  {
    "text": "the staging not popping up like that I wasn't there it took us uh took us a bit",
    "start": "1707480",
    "end": "1713240"
  },
  {
    "text": "of time to understand what was going on and we ended up running for two weeks on two different backups at the same time",
    "start": "1713240",
    "end": "1719419"
  },
  {
    "text": "which was very messy so we had to do a second migration and this time we were about to rectify as well and finally we",
    "start": "1719419",
    "end": "1728120"
  },
  {
    "text": "had some very very strange Behavior which uh by using uh preemptible spot",
    "start": "1728120",
    "end": "1735740"
  },
  {
    "text": "instances we had some very very down to the machine problems that",
    "start": "1735740",
    "end": "1741740"
  },
  {
    "text": "we weren't able to explain but it was very hard to debug because uh we were using Java 8 and it's not done for",
    "start": "1741740",
    "end": "1749840"
  },
  {
    "text": "kubernetes and so you're not able to monitor what it's doing with ram you can",
    "start": "1749840",
    "end": "1755000"
  },
  {
    "text": "just give it a huge chunk of ramen up for the best basically which is uh bits",
    "start": "1755000",
    "end": "1761240"
  },
  {
    "text": "of which is not the best because um the more RAM you have on your Druid",
    "start": "1761240",
    "end": "1766580"
  },
  {
    "text": "clusters the better it operates and especially with spare Ram it can cache segments stuff like this and it gets it",
    "start": "1766580",
    "end": "1773899"
  },
  {
    "text": "gets a lot of benefits out of this so we had a lot of issues on this now we'll talk about all the",
    "start": "1773899",
    "end": "1780919"
  },
  {
    "text": "improvements we see for our own clusters but to be honest it represents a bit of what is happening right now around with",
    "start": "1780919",
    "end": "1787640"
  },
  {
    "text": "I'd say yeah currently we are planning to migrate to 325 and to migrate to Java",
    "start": "1787640",
    "end": "1795620"
  },
  {
    "text": "17 because we had some issues with the javaid version currently just to precise",
    "start": "1795620",
    "end": "1803419"
  },
  {
    "text": "we got all the Rams that is cached by your given but not usable by the Tweet when we are learning on spot instance",
    "start": "1803419",
    "end": "1810020"
  },
  {
    "text": "that's what happened then we have some nuts that are running on arm and we are",
    "start": "1810020",
    "end": "1817940"
  },
  {
    "text": "planning to decrease the cost to migrate all the trade cluster on",
    "start": "1817940",
    "end": "1823100"
  },
  {
    "text": "I think in the next quarter we are we plan to add a process Credit in front of our SQL",
    "start": "1823100",
    "end": "1832520"
  },
  {
    "text": "Android and to run the 200 without Equippers that's mean we we plan to use",
    "start": "1832520",
    "end": "1839419"
  },
  {
    "text": "the tcd and the kubernetes API to get all the endpoint and all the information",
    "start": "1839419",
    "end": "1845000"
  },
  {
    "text": "that we need it's currently unstable but we",
    "start": "1845000",
    "end": "1850220"
  },
  {
    "text": "for from this documentation but we plan to test it on staging soon and we hope",
    "start": "1850220",
    "end": "1857059"
  },
  {
    "text": "to to have this tackle in the next months too long term I don't think we have any time",
    "start": "1857059",
    "end": "1864080"
  },
  {
    "text": "uh actually to talk about the long term uh anyway maybe we can finish with a",
    "start": "1864080",
    "end": "1869240"
  },
  {
    "text": "closing remark uh it's that in the end what we are doing is running uh stateful sets on kubernetes and really taking",
    "start": "1869240",
    "end": "1876559"
  },
  {
    "text": "advantage of it I would understand very well that you wouldn't want to do that but basically it's it's not something",
    "start": "1876559",
    "end": "1883580"
  },
  {
    "text": "that would run better on bare metal in my opinion so in the end maybe it's not",
    "start": "1883580",
    "end": "1889100"
  },
  {
    "text": "that well suited for kubernetes but what is it suited for in the end we don't",
    "start": "1889100",
    "end": "1895220"
  },
  {
    "text": "know and so we think we think kubernetes is the best really that can be done even",
    "start": "1895220",
    "end": "1901100"
  },
  {
    "text": "though it's not maybe that easy and finally we would like to address the special things to the dude and the dude",
    "start": "1901100",
    "end": "1908000"
  },
  {
    "text": "operational Community especially the maintainers with whom we have worked a lot and well thank you",
    "start": "1908000",
    "end": "1914960"
  },
  {
    "text": "foreign",
    "start": "1914960",
    "end": "1917960"
  }
]