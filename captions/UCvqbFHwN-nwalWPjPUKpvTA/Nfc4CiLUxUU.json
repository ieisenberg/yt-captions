[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "so I'm I'm John B from Google this is y Tang uh from Avanti and we'll talk to",
    "start": "320",
    "end": "6319"
  },
  {
    "text": "you a little bit about how uh you may or may not realize you're leaking some of your service information uh in your",
    "start": "6319",
    "end": "12840"
  },
  {
    "text": "kubernetes clusters and and what you might be able to do about it so Yang will take over from",
    "start": "12840",
    "end": "18600"
  },
  {
    "text": "here okay to to get started before we talk about uh Co service information and",
    "start": "20439",
    "end": "25800"
  },
  {
    "start": "21000",
    "end": "119000"
  },
  {
    "text": "the DNS Let's uh just briefly reveal role based Access Control in kues so",
    "start": "25800",
    "end": "31279"
  },
  {
    "text": "what is robas Access Control robas access control is to Define uh which user can do what in a kuet cluster uh",
    "start": "31279",
    "end": "39440"
  },
  {
    "text": "the principles of arack is to have list privilege that means you only want to",
    "start": "39440",
    "end": "46000"
  },
  {
    "text": "expose information to users that absolutely need to know uh the good",
    "start": "46000",
    "end": "51120"
  },
  {
    "text": "thing about roadbed access controling kubernetes is that it a wide interference in a shared environment",
    "start": "51120",
    "end": "57199"
  },
  {
    "text": "let's assume you have a shared cluster with teams each team has their own agenda has their own like features has",
    "start": "57199",
    "end": "64439"
  },
  {
    "text": "their own service me and uh uh let's say someday some one team decided to make",
    "start": "64439",
    "end": "71400"
  },
  {
    "text": "some deployment and they break up certain things and of course in whole clust you uh in a nonworking State then",
    "start": "71400",
    "end": "78920"
  },
  {
    "text": "everything will suffer right that's the so-called shared environment but in Ro SE control you actually want to have a",
    "start": "78920",
    "end": "85320"
  },
  {
    "text": "separation so if one team messed up it will not uh of course a trouble to",
    "start": "85320",
    "end": "90439"
  },
  {
    "text": "another team uh Ro based Access Control has been available in kubernetes since",
    "start": "90439",
    "end": "96079"
  },
  {
    "text": "1.8 so it has been adopted by uh by many companies uh across the board however uh",
    "start": "96079",
    "end": "104079"
  },
  {
    "text": "in today's session I'm going to discuss a little bit about a special information that is DS related information uh the",
    "start": "104079",
    "end": "111880"
  },
  {
    "text": "uniqueness about DS is that DS is actually uh outlier in commity",
    "start": "111880",
    "end": "118079"
  },
  {
    "text": "environment so how is that so first of all DNS information in Comm is always",
    "start": "118079",
    "end": "123159"
  },
  {
    "text": "going to be public that is DNS by itself serve as entry point for all the",
    "start": "123159",
    "end": "128599"
  },
  {
    "text": "services because they serve the purpose of service Discovery the services in kumes uh will be exposed to our clients",
    "start": "128599",
    "end": "135720"
  },
  {
    "text": "through DNS DNS uh have a DNS relies on UDP protocol which makes things even",
    "start": "135720",
    "end": "142959"
  },
  {
    "text": "worse because with UDP you'll have no authentication or authorization uh this is in great",
    "start": "142959",
    "end": "149480"
  },
  {
    "text": "conflict with the least privilege principle we discussed just a moment ago so you have to fix that uh so how do you",
    "start": "149480",
    "end": "156879"
  },
  {
    "text": "fix that there are several ways to fix and depending on your scenario there may be some very easy way to fix let's just",
    "start": "156879",
    "end": "164800"
  },
  {
    "start": "164000",
    "end": "337000"
  },
  {
    "text": "hypothetically say you work for small startup you have some great product you try to push and because it's a small",
    "start": "164800",
    "end": "171800"
  },
  {
    "text": "startup the growth uh uh the growth of the company is very high let's say 50%",
    "start": "171800",
    "end": "177599"
  },
  {
    "text": "year over year or even more and in this case your company's uh higher level",
    "start": "177599",
    "end": "183239"
  },
  {
    "text": "management uh are going to push for new features every day they care more about the the growth they care less about the",
    "start": "183239",
    "end": "190080"
  },
  {
    "text": "c or security and uh because of that normally the company will operate in in",
    "start": "190080",
    "end": "195920"
  },
  {
    "text": "a way such as you have smaller teams and each team working on their own pushing features every day constantly that's a",
    "start": "195920",
    "end": "202799"
  },
  {
    "text": "so-called fast deployment uh in a scenario like that you're going to say can we find an",
    "start": "202799",
    "end": "208720"
  },
  {
    "text": "either way to Wi the noisy evil neighbor situation we discussed just a moment ago uh of course",
    "start": "208720",
    "end": "215959"
  },
  {
    "text": "there have some there are some easy ways we talk about shared information we don't want the information to be shared",
    "start": "215959",
    "end": "221360"
  },
  {
    "text": "each other we also want to uh to aoid the scenario where one team's uh trouble",
    "start": "221360",
    "end": "228319"
  },
  {
    "text": "cause another team then let's just say let's give every team a standalone coate",
    "start": "228319",
    "end": "233360"
  },
  {
    "text": "cluster the problem is solved right you're going to say this solution seem",
    "start": "233360",
    "end": "238640"
  },
  {
    "text": "to be a little Dy right but the reality is uh at least from my experience when I",
    "start": "238640",
    "end": "245159"
  },
  {
    "text": "talk to different companies uh across the board I talk to different uh uh teams across different companies and",
    "start": "245159",
    "end": "252040"
  },
  {
    "text": "many companies actually operating this way uh that is because if you have a",
    "start": "252040",
    "end": "258120"
  },
  {
    "text": "company with let's say sever hundred engineers and you have let's say like 10 teams or 20 team each team only have",
    "start": "258120",
    "end": "264560"
  },
  {
    "text": "like 10 or 20 people uh you don't have a dedicated team you every team just work",
    "start": "264560",
    "end": "270759"
  },
  {
    "text": "on their own they just want to say okay so they don't want to share environment with another team because it's causing a",
    "start": "270759",
    "end": "276759"
  },
  {
    "text": "lot more trouble to communicate with so many teams uh it's causing a lot of trouble to do all kind of things besides",
    "start": "276759",
    "end": "284039"
  },
  {
    "text": "you know I'm working on my own I'm trying to push as as soon as possible so I want a dedicate cluster so you know so",
    "start": "284039",
    "end": "291199"
  },
  {
    "text": "I can release whenever I want that's a smaller company with Hyper growth uh and the problem is solved if",
    "start": "291199",
    "end": "298639"
  },
  {
    "text": "you just give everyone and uh kuet cluster uh by the way one thing I want",
    "start": "298639",
    "end": "304360"
  },
  {
    "text": "to mention is that we all know kubernetes can have a very big cluster uh on top of the you know the",
    "start": "304360",
    "end": "312360"
  },
  {
    "text": "biggest cluster you can handle in kuet environment it's like 5,000 notes but on",
    "start": "312360",
    "end": "318720"
  },
  {
    "text": "average when I talk to different companies for company less than thousand uh employes most of the Clusters are",
    "start": "318720",
    "end": "326160"
  },
  {
    "text": "ranging from let's say five notes all the way to maybe like 20 notes so that's another big cluster right so that's",
    "start": "326160",
    "end": "332400"
  },
  {
    "text": "that's the scenario we encounter with smaller companies but now we talk about",
    "start": "332400",
    "end": "337440"
  },
  {
    "start": "337000",
    "end": "391000"
  },
  {
    "text": "uh what what about if you work for a company that's uh getting a little",
    "start": "337440",
    "end": "342800"
  },
  {
    "text": "bigger and uh the growth has been slowed down when the growth of a company slows",
    "start": "342800",
    "end": "348520"
  },
  {
    "text": "down you realize the shared environment became necessity there are several reasons one the management your your",
    "start": "348520",
    "end": "355560"
  },
  {
    "text": "company's management it's pushing for profit versus the growth gr because there no no real growth anymore and the",
    "start": "355560",
    "end": "363000"
  },
  {
    "text": "F features will be released less frequently uh as expected and finally uh",
    "start": "363000",
    "end": "369560"
  },
  {
    "text": "if you work for a team and you have been maintaining this uh service for a long",
    "start": "369560",
    "end": "374599"
  },
  {
    "text": "time you probably already adopt a mindset of say if it's working don't",
    "start": "374599",
    "end": "379840"
  },
  {
    "text": "change it no one want to make a change right okay now we talk about share",
    "start": "379840",
    "end": "386319"
  },
  {
    "text": "classes and talk about uh dedicate dedicated team that's how great because",
    "start": "386319",
    "end": "392560"
  },
  {
    "start": "391000",
    "end": "515000"
  },
  {
    "text": "with a dedicate team with a shared clust there are several advantages one it",
    "start": "392560",
    "end": "397599"
  },
  {
    "text": "increase CPO uh CPO memory and GP utilization two you can update",
    "start": "397599",
    "end": "404000"
  },
  {
    "text": "kubernetes clusters infrastructure more fre uh more frequently uh this is going",
    "start": "404000",
    "end": "409440"
  },
  {
    "text": "to increase the security coverage and uh because you have one dedicated team to",
    "start": "409440",
    "end": "415039"
  },
  {
    "text": "do all the upgrade you are going to uh have less cross team coordinations one",
    "start": "415039",
    "end": "421199"
  },
  {
    "text": "issue I observe uh with the so-called distribut mode of each each team have",
    "start": "421199",
    "end": "427919"
  },
  {
    "text": "one cluster is that every team try to work on their own they are getting excited at the beginning of the",
    "start": "427919",
    "end": "434360"
  },
  {
    "text": "development cycle but when uh service get into a maintenance mode and you need to uh release security update you",
    "start": "434360",
    "end": "442720"
  },
  {
    "text": "realize that even just by sending notice to let's say 20 teams or 30 teams just",
    "start": "442720",
    "end": "447800"
  },
  {
    "text": "telling them hey there's a uh kuber neted version that's getting a little uh old and we had to upgrade to a new",
    "start": "447800",
    "end": "454440"
  },
  {
    "text": "version it's getting very Troublesome because every team has a their own uh schedule their own agenda they don't",
    "start": "454440",
    "end": "461440"
  },
  {
    "text": "want to be disrupted by someone saying you have to stop whatever you're doing and you have to upgrade right so in this",
    "start": "461440",
    "end": "468000"
  },
  {
    "text": "way if you have a dedicated uh dedicated team to maintain shared environment you",
    "start": "468000",
    "end": "473520"
  },
  {
    "text": "company normally have a uh have a better efficiency across the board now that's a",
    "start": "473520",
    "end": "481000"
  },
  {
    "text": "question we are going to ask if you are the class admin and you work for this dedicated team you you maintain let's",
    "start": "481000",
    "end": "488080"
  },
  {
    "text": "say 200 kuet cluster or maybe just maintaining one gigantic kuic cluster",
    "start": "488080",
    "end": "493400"
  },
  {
    "text": "with let's say 1,000 node so how I going to fix the kuet DNS information leakage",
    "start": "493400",
    "end": "500039"
  },
  {
    "text": "issue I'm going to hand over to J and he will do the magic",
    "start": "500039",
    "end": "505159"
  },
  {
    "text": "yeah thank you y thank you Yang um okay so let's I'm going to do little demo so",
    "start": "505159",
    "end": "510560"
  },
  {
    "text": "we'll see actually how this problem um how bad this problem is uh okay so I",
    "start": "510560",
    "end": "517599"
  },
  {
    "start": "515000",
    "end": "540000"
  },
  {
    "text": "have oops uh notice here um what I have",
    "start": "517599",
    "end": "522760"
  },
  {
    "text": "here is uh a kind cluster running um so",
    "start": "522760",
    "end": "529080"
  },
  {
    "text": "we can see uh it's got some stuff running in",
    "start": "529080",
    "end": "534680"
  },
  {
    "text": "it sorry um okay there's a bunch of services running",
    "start": "535959",
    "end": "541519"
  },
  {
    "start": "540000",
    "end": "745000"
  },
  {
    "text": "in it and um so you you know we can launch a",
    "start": "541519",
    "end": "548360"
  },
  {
    "text": "pod and what we see here is that that pod because it's I didn't give it any",
    "start": "548519",
    "end": "554760"
  },
  {
    "text": "particular service account or anything it's going to get the default service account in the default Nam space and",
    "start": "554760",
    "end": "561200"
  },
  {
    "text": "that is going to we have our back of course on our cluster and so we can see",
    "start": "561200",
    "end": "567120"
  },
  {
    "text": "um that we have no access to anything so the the scenario here is right do you",
    "start": "567120",
    "end": "573399"
  },
  {
    "text": "have a a shared cluster with as Yang was saying you know as your company grows",
    "start": "573399",
    "end": "578720"
  },
  {
    "text": "you may want to create shared multi tendent clusters um you have uh users",
    "start": "578720",
    "end": "584399"
  },
  {
    "text": "with access to that cluster they can create pods they can do do what they will but of course they have their their",
    "start": "584399",
    "end": "589680"
  },
  {
    "text": "access scoped to um whatever their particular rbac is and uh typically",
    "start": "589680",
    "end": "595320"
  },
  {
    "text": "that's to specific name spaces so here we've got a pod that's just has basically no privileges to the API but",
    "start": "595320",
    "end": "603200"
  },
  {
    "text": "um even so uh you know it does have privileges to query DNS so you can",
    "start": "603200",
    "end": "611240"
  },
  {
    "text": "actually use that to glean information so in the kubernetes DNS specification",
    "start": "611240",
    "end": "616680"
  },
  {
    "text": "because it's sort of coming from the DNS World it um it represents the the",
    "start": "616680",
    "end": "622880"
  },
  {
    "text": "services as this DNS schema and it will respond based upon um",
    "start": "622880",
    "end": "630720"
  },
  {
    "text": "things like if a namespace exists you'll get a new query for a record in that",
    "start": "630720",
    "end": "637440"
  },
  {
    "text": "namespace that means that the the DNS name for that namespace exists and so",
    "start": "637440",
    "end": "644000"
  },
  {
    "text": "there may not be a record so I'm going to query for just a an a record that is a record for an IP address that matches",
    "start": "644000",
    "end": "650760"
  },
  {
    "text": "a namespace name that doesn't exist um and you'll see that I get doesn't",
    "start": "650760",
    "end": "658560"
  },
  {
    "text": "exist but if I get if I query for one that does exist oops sorry it definitely",
    "start": "658560",
    "end": "665360"
  },
  {
    "text": "doesn't exist when it's completely wrong um still doesn't exist and then if",
    "start": "665360",
    "end": "672480"
  },
  {
    "text": "I query for one that does we know Cube system",
    "start": "672480",
    "end": "677320"
  },
  {
    "text": "exists all right I get no error I get no I I get no records so it's a non-error I",
    "start": "678800",
    "end": "684519"
  },
  {
    "text": "don't get the NX domain saying that there's no such domain so what this is telling me is it's leaking a little bit of information that if I can guess a",
    "start": "684519",
    "end": "691000"
  },
  {
    "text": "namespace name I can determine whether even if I have zero privileges on the actual API server I can determine",
    "start": "691000",
    "end": "697399"
  },
  {
    "text": "whether that namespace exists okay that's that's kind of interesting but guessing a bunch of namespace names",
    "start": "697399",
    "end": "702920"
  },
  {
    "text": "isn't really that easy to do but if I think about it I know I can probably use",
    "start": "702920",
    "end": "709200"
  },
  {
    "text": "that to look up this the API server itself and I can see well that's uh",
    "start": "709200",
    "end": "715920"
  },
  {
    "text": "that's got IP address 10961 you know what but uh that means that the service CER is probably",
    "start": "715920",
    "end": "723920"
  },
  {
    "text": "10966 so there's another thing that DNS can look up it can do reverse IP lookups",
    "start": "723920",
    "end": "730120"
  },
  {
    "text": "so if I know the the the CER I know the addresses I can do a bunch of reverse",
    "start": "730120",
    "end": "735360"
  },
  {
    "text": "lookups so you know a sl6 really isn't all that big these days I can actually",
    "start": "735360",
    "end": "741600"
  },
  {
    "text": "take this stupid script 10 lines of bash code not even bash shell um and uh I can",
    "start": "741600",
    "end": "749680"
  },
  {
    "start": "745000",
    "end": "894000"
  },
  {
    "text": "look through that whole sl16 and do reverse lookups so let's give it a",
    "start": "749680",
    "end": "756839"
  },
  {
    "text": "try oh okay found two things in in uh in the first sl24 and it's going to go",
    "start": "757839",
    "end": "766639"
  },
  {
    "text": "through and it takes about 6 seconds with this stupid little bash script for",
    "start": "766639",
    "end": "771839"
  },
  {
    "text": "each sl24 so that's about 20 minutes I don't want to sit around here and wait",
    "start": "771839",
    "end": "777199"
  },
  {
    "text": "20 minutes so we're going to cheat just a little bit cuz I happen to know",
    "start": "777199",
    "end": "782320"
  },
  {
    "text": "something that the attacker wouldn't know and that's that",
    "start": "782320",
    "end": "787959"
  },
  {
    "text": "um I can look here and I can just start at say",
    "start": "787959",
    "end": "794160"
  },
  {
    "text": "uh this particular sl24 and we can try that again and let's",
    "start": "794160",
    "end": "799880"
  },
  {
    "text": "see what we find okay so it's scanning and oh look at that okay so I have a service running here um gy gy is a open-",
    "start": "799880",
    "end": "810040"
  },
  {
    "text": "Source git provider like gitlab or GitHub it's a sort of Open Source clone",
    "start": "810040",
    "end": "815480"
  },
  {
    "text": "of GitHub um and so I can see now even though I I don't have any privileges to",
    "start": "815480",
    "end": "821240"
  },
  {
    "text": "the API server I can see that exists while I can actually go and I can query for an SRV record so an SRV record in",
    "start": "821240",
    "end": "829639"
  },
  {
    "text": "DNS is uh not just an IP address that it returns but it also returns a port so",
    "start": "829639",
    "end": "835959"
  },
  {
    "text": "every service that you publish in the kubernetes API service you typically will say what the ports",
    "start": "835959",
    "end": "841279"
  },
  {
    "text": "are for that service and those end up in an SRV record so if I do",
    "start": "841279",
    "end": "848560"
  },
  {
    "text": "that SBC cluster. loc I don't actually have to type all of it out I guess but",
    "start": "853199",
    "end": "859600"
  },
  {
    "text": "um we can see okay it's got Port uh 22 running that's for git SSH service and",
    "start": "859600",
    "end": "868079"
  },
  {
    "text": "Port 3000 I what that is well I know that gy tends to run HTTP so if maybe I",
    "start": "868079",
    "end": "874720"
  },
  {
    "text": "can just try and do a little this um that cluster.",
    "start": "874720",
    "end": "884959"
  },
  {
    "text": "local Port 3000 and oh look at that so here I am uh I can start to",
    "start": "886480",
    "end": "894320"
  },
  {
    "start": "894000",
    "end": "936000"
  },
  {
    "text": "explore uh all of my other tenants uh Services what they have running and I could start poking at them",
    "start": "894320",
    "end": "900519"
  },
  {
    "text": "and looking for ones that have vulnerabilities and then of course I can crack into those so we've leaked some",
    "start": "900519",
    "end": "906839"
  },
  {
    "text": "information that can be useful to attackers through DNS um",
    "start": "906839",
    "end": "913600"
  },
  {
    "text": "so what can we do about that how can we reduce the leakage well um in sorry I'm",
    "start": "913600",
    "end": "921600"
  },
  {
    "text": "checking my notes here so um we have a featuring cords it's not built in but",
    "start": "921600",
    "end": "928240"
  },
  {
    "text": "it's called the fire wall plugin and um I can switch back to the",
    "start": "928240",
    "end": "935399"
  },
  {
    "text": "slides um it's an external plugin and it's",
    "start": "935560",
    "end": "941480"
  },
  {
    "start": "936000",
    "end": "1014000"
  },
  {
    "text": "available but it's part of the cordance organization which means we maintain it as part of CNS um but when it's an",
    "start": "941480",
    "end": "947720"
  },
  {
    "text": "external plugin that means that the one that you're downloading off of uh that it's running in your eks cluster the",
    "start": "947720",
    "end": "953240"
  },
  {
    "text": "cord has or your a cluster or your uh Google gke on Prem cluster",
    "start": "953240",
    "end": "959519"
  },
  {
    "text": "um or I don't know uh if open shift uh runs cordion s uh or not I think they do",
    "start": "959519",
    "end": "966480"
  },
  {
    "text": "but I'm not sure um but anyway the one that's running there by default yeah okay and uh is is not going to have this",
    "start": "966480",
    "end": "973800"
  },
  {
    "text": "in it so you'd have to build your own custom image but basically what it does is uh it's a it's got a built-in",
    "start": "973800",
    "end": "980199"
  },
  {
    "text": "expression engine uh that allows you to write simple Expressions that look like",
    "start": "980199",
    "end": "985279"
  },
  {
    "text": "uh a little like C um and it you you can also alternatively integrate with",
    "start": "985279",
    "end": "991240"
  },
  {
    "text": "external engines like uh Opa but it allows um you",
    "start": "991240",
    "end": "998959"
  },
  {
    "text": "to kind of uh make policy decisions on the on the the requests that come in",
    "start": "998959",
    "end": "1005720"
  },
  {
    "text": "based upon metadata that we can associate with the individual request so",
    "start": "1005720",
    "end": "1011800"
  },
  {
    "text": "in the um in in cords we have another plugin",
    "start": "1011800",
    "end": "1017560"
  },
  {
    "start": "1014000",
    "end": "1234000"
  },
  {
    "text": "called metadata that is built in and when you enable the metadata plugin",
    "start": "1017560",
    "end": "1022880"
  },
  {
    "text": "it essentially tells other plugins hey add information to the context so",
    "start": "1022880",
    "end": "1030360"
  },
  {
    "text": "stepping back maybe a little bit I I didn't explain um the way core DNS Works",
    "start": "1030360",
    "end": "1036798"
  },
  {
    "text": "different from most DNS servers is that uh it's a bunch of it's a request",
    "start": "1036799",
    "end": "1042038"
  },
  {
    "text": "processing pipeline so you get a a DNS query in and it's accepted by C DNS and",
    "start": "1042039",
    "end": "1049280"
  },
  {
    "text": "based on your configuration it hands it to a plugin which will either look up B",
    "start": "1049280",
    "end": "1058559"
  },
  {
    "text": "the information in some data source to satisfy that query or will manipulate or",
    "start": "1058559",
    "end": "1063760"
  },
  {
    "text": "change the query in some way um and it or it will decide it doesn't have",
    "start": "1063760",
    "end": "1069039"
  },
  {
    "text": "anything to do with that query and it will pass it on to the next plugin in the chain so you have this whole chain of plugins metadata is one of these that",
    "start": "1069039",
    "end": "1076080"
  },
  {
    "text": "doesn't actually do anything to the query all it does is basically create an entry in the go context so in in the programing language",
    "start": "1076080",
    "end": "1083360"
  },
  {
    "text": "of go there's a a context that we typically pass down from one you know",
    "start": "1083360",
    "end": "1088480"
  },
  {
    "text": "down through a chain like this and that entry in the go context tells the other",
    "start": "1088480",
    "end": "1093840"
  },
  {
    "text": "plugins hey somebody's interested in metadata the reason we do that the reason you have to enable the metadata",
    "start": "1093840",
    "end": "1100480"
  },
  {
    "text": "plugin is that of course this has a performance cost and in DNS it's a very very hot Loop we want we want tens of",
    "start": "1100480",
    "end": "1106799"
  },
  {
    "text": "thousands of QPS and so on a from a single single core so anything going",
    "start": "1106799",
    "end": "1112840"
  },
  {
    "text": "into that that isn't needed we we we try to avoid so in any case um if you enable",
    "start": "1112840",
    "end": "1119000"
  },
  {
    "text": "the metadata plugin then when your request comes in on this diagram you see the request coming in from the left top there uh",
    "start": "1119000",
    "end": "1127080"
  },
  {
    "text": "it's from a particular client IP um it's for an SRV record for some some query",
    "start": "1127080",
    "end": "1132440"
  },
  {
    "text": "name the metadata plugin doesn't change the actual uh request at all it just",
    "start": "1132440",
    "end": "1138799"
  },
  {
    "text": "sort of adds this metadata placeholder for the context the firewall plugin on",
    "start": "1138799",
    "end": "1144240"
  },
  {
    "text": "the way in doesn't actually do anything because uh how we've configured it but",
    "start": "1144240",
    "end": "1149799"
  },
  {
    "text": "the kubernetes plug-in gets that request and says Ah okay that's for a cluster.",
    "start": "1149799",
    "end": "1156280"
  },
  {
    "text": "looc zone so I own this query so I'm going to resolve it and it goes and it",
    "start": "1156280",
    "end": "1162919"
  },
  {
    "text": "query it actually has a cache of the services it finds that in the the cache",
    "start": "1162919",
    "end": "1168760"
  },
  {
    "text": "but it sees that the metadata plug-in is enabled and so it actually adds a bunch of metadata on that request context now",
    "start": "1168760",
    "end": "1175919"
  },
  {
    "text": "this is a chain a function call chain right so on the way back every single uh every",
    "start": "1175919",
    "end": "1183240"
  },
  {
    "text": "single function plug-in sees the the response on the way back what the previous plug-in did so that request",
    "start": "1183240",
    "end": "1190000"
  },
  {
    "text": "that now has the response it's an a uh I guess I asked for an SRV and I'm giving an a record so my my slide isn't perfect",
    "start": "1190000",
    "end": "1196840"
  },
  {
    "text": "here but the response here is a record um the but it also has some metadata on",
    "start": "1196840",
    "end": "1203400"
  },
  {
    "text": "it the firewall now can take advantage of that so we can write a firewall rule",
    "start": "1203400",
    "end": "1208880"
  },
  {
    "text": "within the CNS uh configuration that says hey the client name",
    "start": "1208880",
    "end": "1214919"
  },
  {
    "text": "space does not match the name space of the service being requested so send an",
    "start": "1214919",
    "end": "1221080"
  },
  {
    "text": "NX domain instead of sending uh the actual response so that is actually what we I'll show the configuration and I'll",
    "start": "1221080",
    "end": "1227840"
  },
  {
    "text": "I'll show that demo uh in a moment or just now actually so",
    "start": "1227840",
    "end": "1233799"
  },
  {
    "text": "let's take another look here um so I exited out of that pod and I have here",
    "start": "1233799",
    "end": "1240360"
  },
  {
    "start": "1234000",
    "end": "1282000"
  },
  {
    "text": "uh a couple of files so if I you can see that here if we look",
    "start": "1240360",
    "end": "1247919"
  },
  {
    "text": "oops if we look this this is I just pulled out the deployment file this is you can see a special build right we're",
    "start": "1250480",
    "end": "1256720"
  },
  {
    "text": "not not using the standard cords default build because it doesn't include the plug-in firewall I had to make a special",
    "start": "1256720",
    "end": "1263280"
  },
  {
    "text": "build for it and then I just pulled out the config map that's used to configure",
    "start": "1263280",
    "end": "1269640"
  },
  {
    "text": "CNS in this kind cluster and then I edited it and so we can",
    "start": "1269640",
    "end": "1275840"
  },
  {
    "text": "see that there we go um okay so what did I change one I enabled the metadata",
    "start": "1279640",
    "end": "1287520"
  },
  {
    "text": "plugin two I switched to pods verified mode I'll explain that in a minute and",
    "start": "1287520",
    "end": "1294240"
  },
  {
    "text": "three I added this stanza for configuring the the firewall",
    "start": "1294240",
    "end": "1301400"
  },
  {
    "text": "uh policy so uh the basically it's saying allow this",
    "start": "1301400",
    "end": "1309080"
  },
  {
    "text": "query if the kubernetes main space equals the client name space or I'm",
    "start": "1309080",
    "end": "1314640"
  },
  {
    "text": "quering for something in Cube system or I'm quering for something in the default name space um otherwise block it all",
    "start": "1314640",
    "end": "1324080"
  },
  {
    "text": "right let's give it a try we have to first uh apply that",
    "start": "1324080",
    "end": "1333000"
  },
  {
    "text": "change okay that updates the uh the the config map and then just to be make it",
    "start": "1335320",
    "end": "1343120"
  },
  {
    "text": "faster so cords will actually reload the the core file on its own um but but it's",
    "start": "1343120",
    "end": "1349400"
  },
  {
    "text": "kind of whether it gets to each instance is a little bit um you know there can be",
    "start": "1349400",
    "end": "1355679"
  },
  {
    "text": "raise conditions let's just say that um and then so let's make sure that those",
    "start": "1355679",
    "end": "1361799"
  },
  {
    "text": "uh those pods restarted okay 19 seconds ago so those are restarted so let's do",
    "start": "1361799",
    "end": "1367799"
  },
  {
    "start": "1362000",
    "end": "1425000"
  },
  {
    "text": "our uh example again and let's try and",
    "start": "1367799",
    "end": "1377320"
  },
  {
    "text": "grab oh we do it SRV",
    "start": "1377440",
    "end": "1384080"
  },
  {
    "text": "again if I spell it wrong it's definitely going to",
    "start": "1386000",
    "end": "1393080"
  },
  {
    "text": "okay and boom an extra name so hey it actually works",
    "start": "1395120",
    "end": "1400919"
  },
  {
    "text": "amazing all right so",
    "start": "1400919",
    "end": "1407440"
  },
  {
    "text": "um all right that's interesting now I mentioned uh a moment ago sorry I just",
    "start": "1407440",
    "end": "1414760"
  },
  {
    "text": "scroll my notes here um I mentioned I had to enable pods",
    "start": "1414760",
    "end": "1420159"
  },
  {
    "text": "verified mode so let let's talk a little bit let's go back to the slide deck",
    "start": "1420159",
    "end": "1425600"
  },
  {
    "start": "1425000",
    "end": "1810000"
  },
  {
    "text": "um so you know it works it's great right why don't we just enable this why why do I have to build a special core DNS and",
    "start": "1425600",
    "end": "1432400"
  },
  {
    "text": "and then why do I have to to uh you know edit the standard DNS core file that's",
    "start": "1432400",
    "end": "1438720"
  },
  {
    "text": "in every single cluster out there in in the world and um and there's a few reasons one is",
    "start": "1438720",
    "end": "1445159"
  },
  {
    "text": "um the reason I need a special build is because that firewall pgon brings a",
    "start": "1445159",
    "end": "1450360"
  },
  {
    "text": "whole bunch of dependencies with it that we didn't want in the main application because it's a kind of special use case",
    "start": "1450360",
    "end": "1455960"
  },
  {
    "text": "so that's why we we keep it keep it out and we don't build it in by default but more to the point why don't we why don't",
    "start": "1455960",
    "end": "1462440"
  },
  {
    "text": "we build it why isn't it so important that we do it all by default well one is you need to use pods verified mode so",
    "start": "1462440",
    "end": "1467600"
  },
  {
    "text": "what pods verif ified mode does in the kubernetes plugin is um in the earliest",
    "start": "1467600",
    "end": "1474600"
  },
  {
    "text": "days of kubernetes there's a uh a feature where you could request um uh an",
    "start": "1474600",
    "end": "1481360"
  },
  {
    "text": "IP address like uh 10101010 pod. uh cluster. loal or",
    "start": "1481360",
    "end": "1487679"
  },
  {
    "text": "something like that and it would return 10101010 but it actually would return whatever you put for that IP address it",
    "start": "1487679",
    "end": "1494559"
  },
  {
    "text": "just always returned it because um because they didn't want to watch pods",
    "start": "1494559",
    "end": "1501159"
  },
  {
    "text": "so watching pods if you imagine um the way the kubernetes API server works is right you have a way say CNS works with",
    "start": "1501159",
    "end": "1508360"
  },
  {
    "text": "it and all other controllers is they create a persistent connection to the API server and they say I'm interested",
    "start": "1508360",
    "end": "1514279"
  },
  {
    "text": "in this information send me any send me all that information and then send me any changes about that information so in",
    "start": "1514279",
    "end": "1521320"
  },
  {
    "text": "a 10,000 node cluster with many many many tens of thousands of nodes um or",
    "start": "1521320",
    "end": "1527880"
  },
  {
    "text": "rather many many tens of thousands of PODS uh a watch on pods is extremely",
    "start": "1527880",
    "end": "1533080"
  },
  {
    "text": "expensive you're pulling all of those pods into memory and then every single change of any pod that runs anywhere in",
    "start": "1533080",
    "end": "1539039"
  },
  {
    "text": "the system basically gets pushed down to your process so pods verified",
    "start": "1539039",
    "end": "1546080"
  },
  {
    "text": "mode allows those pod queries so the default in there that we changed it from was pods insecure which basically",
    "start": "1546080",
    "end": "1553480"
  },
  {
    "text": "replicates the ancient Behavior the default which is with no pods in is just",
    "start": "1553480",
    "end": "1558880"
  },
  {
    "text": "don't support pod names and that's actually what most people do nowadays I think but in any case pods verified was",
    "start": "1558880",
    "end": "1566559"
  },
  {
    "text": "put in to say hey that's actually really insecure to just return from DNS any",
    "start": "1566559",
    "end": "1571960"
  },
  {
    "text": "name because DNS is often used as like a a a a a root trust type of thing so like",
    "start": "1571960",
    "end": "1580279"
  },
  {
    "text": "in your TLS negotiation if you can effectively spoof that you're have a DS",
    "start": "1580279",
    "end": "1585760"
  },
  {
    "text": "name you can probably play some tricks so we like we don't we don't want to do that we added this pods verified mode",
    "start": "1585760",
    "end": "1591480"
  },
  {
    "text": "puts a watch on pods only Returns the Pod uh IP address if there's actually a",
    "start": "1591480",
    "end": "1597240"
  },
  {
    "text": "pod with that IP so um that has terrible implications in large clusters this is",
    "start": "1597240",
    "end": "1603320"
  },
  {
    "text": "one of the reasons this is an on default there's also a more subtle issue and that's that the there's a race condition",
    "start": "1603320",
    "end": "1610880"
  },
  {
    "text": "so remember CNS connection to API server listening for things on pods you launch",
    "start": "1610880",
    "end": "1616799"
  },
  {
    "text": "a new pod the notification of that new pod to cords is an asynchronous process",
    "start": "1616799",
    "end": "1623600"
  },
  {
    "text": "so if you've got a pod that launches an immediately starts trying to make connections outbound and make DNS",
    "start": "1623600",
    "end": "1628679"
  },
  {
    "text": "queries outbound and you've implemented what I've shown you here today then those pods will initially fail those DNS",
    "start": "1628679",
    "end": "1635440"
  },
  {
    "text": "lookups so you'll actually get application failures until core DNS receives the watch event processes it",
    "start": "1635440",
    "end": "1642640"
  },
  {
    "text": "and puts it in its cach so if your workloads are finicky and don't don't",
    "start": "1642640",
    "end": "1647840"
  },
  {
    "text": "handle uh uh failure of DNS resolution very well then this solution wouldn't work",
    "start": "1647840",
    "end": "1654679"
  },
  {
    "text": "for you there either so two big reasons why this isn't on everywhere all the time so or you I guess I have a note",
    "start": "1654679",
    "end": "1661399"
  },
  {
    "text": "here you can solve that last one by failing open but which means like in our",
    "start": "1661399",
    "end": "1666840"
  },
  {
    "text": "policies that we write in the firewall allow unknown clients to access anything but that's kind of what's the point so",
    "start": "1666840",
    "end": "1675200"
  },
  {
    "text": "what in the world do we do about it well I'm not going to leave you here I will say there is one thing you could do",
    "start": "1675200",
    "end": "1682080"
  },
  {
    "text": "slightly better and that's per tenant DNS services and I just need to check",
    "start": "1682080",
    "end": "1687679"
  },
  {
    "text": "time here yeah lots of time I think um so the the idea here is you combine some of",
    "start": "1687679",
    "end": "1696640"
  },
  {
    "text": "this firewall Concepts that we've talked about today but you also segregate your",
    "start": "1696640",
    "end": "1702840"
  },
  {
    "text": "DNS instances per tenant this only works if you have sort of large tenants in and",
    "start": "1702840",
    "end": "1708640"
  },
  {
    "text": "say that that are going to be creating a lot of name spaces maybe and you can prefix those name spaces with the tenant",
    "start": "1708640",
    "end": "1714399"
  },
  {
    "text": "name or you know there's there's work you have to do to make this happen but the idea would be you combine a with a",
    "start": "1714399",
    "end": "1719799"
  },
  {
    "text": "mutating web hook so that you see when somebody's creating a pod in a name space that uh is belongs to one of these",
    "start": "1719799",
    "end": "1727120"
  },
  {
    "text": "tenants you mutate their DNS policy on the Pod so you change the DNS resolver",
    "start": "1727120",
    "end": "1734559"
  },
  {
    "text": "address that they're going to use for those pods and of course you have to run a separate CNS instance whose scope is",
    "start": "1734559",
    "end": "1741600"
  },
  {
    "text": "limited to the name spaces of that tenant So within the kubernetes plugin in CNS you can list a set of Nam spaces",
    "start": "1741600",
    "end": "1748600"
  },
  {
    "text": "and then we will only serve records for those name spaces so that works for the tenant as long as",
    "start": "1748600",
    "end": "1755720"
  },
  {
    "text": "the tenant doesn't need to access any common Services if they do",
    "start": "1755720",
    "end": "1761519"
  },
  {
    "text": "you need to write policies to sort of allow that um typically you'll also",
    "start": "1761519",
    "end": "1766799"
  },
  {
    "text": "still need to run your potential DNS for all your your platform Services which aren't tenant Services um and so you",
    "start": "1766799",
    "end": "1775640"
  },
  {
    "text": "know you have to craft those policies maybe to only allow look up of the the cube API server typically you know",
    "start": "1775640",
    "end": "1783519"
  },
  {
    "text": "another specific platform Services you want those tenants to see out of the central DNS uh and um or even use",
    "start": "1783519",
    "end": "1790760"
  },
  {
    "text": "Network policy potentially to limit visibility uh to the central DNS for the",
    "start": "1790760",
    "end": "1796120"
  },
  {
    "text": "uh the tenant pots so um that's what I've got for you any",
    "start": "1796120",
    "end": "1802519"
  },
  {
    "text": "any questions there's two mics here and here",
    "start": "1802519",
    "end": "1807919"
  },
  {
    "text": "to meet",
    "start": "1807919",
    "end": "1812360"
  }
]