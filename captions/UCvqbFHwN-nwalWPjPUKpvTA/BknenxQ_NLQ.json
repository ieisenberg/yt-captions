[
  {
    "start": "0",
    "end": "59000"
  },
  {
    "text": "hi welcome to our talk yeah so today we're going to be talking about how to",
    "start": "290",
    "end": "6060"
  },
  {
    "text": "measure your kubernetes components briefly my name is hon king I'm a",
    "start": "6060",
    "end": "11580"
  },
  {
    "text": "software engineer I lead the cluster ops team at Google which is a part of the",
    "start": "11580",
    "end": "17970"
  },
  {
    "text": "broader gke API machinery team in open source I contribute to both sync API",
    "start": "17970",
    "end": "24900"
  },
  {
    "text": "machinery instrumentation and I actually wasn't aware that I still had a Twitter",
    "start": "24900",
    "end": "32520"
  },
  {
    "text": "but Alayna correctly deduced that it was exactly the same as my github handle and",
    "start": "32520",
    "end": "38160"
  },
  {
    "text": "this is Elena yeah I'm Alana hashman I'm a principal site reliability engineer at Red Hat I'm",
    "start": "38160",
    "end": "43469"
  },
  {
    "text": "currently one of our tech leads on the Azure Red Hat OpenShift team I'm also a member of cig instrumentation since 2018",
    "start": "43469",
    "end": "51059"
  },
  {
    "text": "if you want to tweet at me my handle is a hash dn and my github is a hashman so",
    "start": "51059",
    "end": "59370"
  },
  {
    "start": "59000",
    "end": "59000"
  },
  {
    "text": "what are we going to cover today I'll start out by introducing how instrumentation works in kubernetes and",
    "start": "59370",
    "end": "65640"
  },
  {
    "text": "then Han will talk about the various components of kubernetes control plane and dive into more detail on their",
    "start": "65640",
    "end": "71520"
  },
  {
    "text": "instrumentation then we'll both cover some real-world debugging examples using the metrics and tools we've introduced",
    "start": "71520",
    "end": "77400"
  },
  {
    "text": "and last I'll talk a little bit about metrics usability and the sig instrumentation roadmap so let's start",
    "start": "77400",
    "end": "85049"
  },
  {
    "text": "from beginning how does kubernetes instrumentation work kubernetes components integrate with",
    "start": "85049",
    "end": "91619"
  },
  {
    "text": "Prometheus a time series based open-source monitoring and alerting toolkit your instrument is software will",
    "start": "91619",
    "end": "98280"
  },
  {
    "text": "use the Prometheus client to export metrics in Prometheus format on some endpoint which then can then be scraped",
    "start": "98280",
    "end": "104700"
  },
  {
    "text": "and stored in a time series database typically you're scraping mechanism is the open source Prometheus server but",
    "start": "104700",
    "end": "111479"
  },
  {
    "text": "there are other cloud and vendor tools that can scrape Prometheus metrics such as stack driver or Azure monitor all",
    "start": "111479",
    "end": "117090"
  },
  {
    "text": "kubernetes binaries are instrumented using Prometheus clients and export me Prometheus metrics by default and so as",
    "start": "117090",
    "end": "124229"
  },
  {
    "text": "soon as you set up a metric sync you can collect metrics on your cluster health without any additional configuration so",
    "start": "124229",
    "end": "131430"
  },
  {
    "start": "131000",
    "end": "131000"
  },
  {
    "text": "what do Prometheus metrics look like each time series has a name a series of",
    "start": "131430",
    "end": "136470"
  },
  {
    "text": "labels with values and an overall value in this example our time series is called up it has a label of job with",
    "start": "136470",
    "end": "143459"
  },
  {
    "text": "label value cube API server and label instance with label value API 1 and the",
    "start": "143459",
    "end": "148680"
  },
  {
    "text": "overall time series value is 1 presumably because our API server is up time series names and labels let us",
    "start": "148680",
    "end": "155549"
  },
  {
    "text": "query and drill down into our data Prometheus metrics will typically also have some associated help text which",
    "start": "155549",
    "end": "162540"
  },
  {
    "text": "describes what the metric means as well as a type annotation which indicates the data type of the time series value there",
    "start": "162540",
    "end": "170730"
  },
  {
    "text": "are 4 metric data types in Prometheus first counters whose values are monotonically increasing integers as",
    "start": "170730",
    "end": "177329"
  },
  {
    "text": "their name suggests counters are typically used to count a number of occurrences for example number of",
    "start": "177329",
    "end": "183569"
  },
  {
    "text": "requests or number of failures gages represent values that can increase or",
    "start": "183569",
    "end": "189030"
  },
  {
    "text": "decrease they are a very flexible metric type you might use them to track payload sizes or record Layton sees or represent",
    "start": "189030",
    "end": "196290"
  },
  {
    "text": "uptime like in our example time series here summaries and histograms are more complex data types that allow us to",
    "start": "196290",
    "end": "203010"
  },
  {
    "text": "track distributions of data and determine performance at a given percentile summaries are pre calculated",
    "start": "203010",
    "end": "209549"
  },
  {
    "text": "for specific percentiles whereas histograms are served in bucketed form for later aggregation",
    "start": "209549",
    "end": "214849"
  },
  {
    "text": "summaries are more accurate and efficient but can't be aggregated across many data sources for example if you",
    "start": "214849",
    "end": "221579"
  },
  {
    "text": "wanted to calculate the 99th percentile latency of pod launches across all cubelets in a cluster you would need to",
    "start": "221579",
    "end": "227880"
  },
  {
    "text": "use a histogram not a summary from a combination of these simple data types",
    "start": "227880",
    "end": "233069"
  },
  {
    "text": "of labels kubernetes developers have built metrics for each component that we can collect to help us measure the",
    "start": "233069",
    "end": "238769"
  },
  {
    "text": "health of our cluster when I quantify the quality of service of a cluster I like to consider the dimensions of",
    "start": "238769",
    "end": "244829"
  },
  {
    "text": "availability latency and capacity of my service as well as any errors what does",
    "start": "244829",
    "end": "250440"
  },
  {
    "text": "this look like in practice as an example let's consider some of the metrics available from the kubernetes api server",
    "start": "250440",
    "end": "256769"
  },
  {
    "text": "if we want to measure our API servers availability we can look at the uptime",
    "start": "256769",
    "end": "262109"
  },
  {
    "text": "series as we saw in the last slide which is a gauge with a value of 1 when the server is online and zero when it's offline to get",
    "start": "262109",
    "end": "269970"
  },
  {
    "text": "an idea of request latency we can use the API server request latency seconds histogram which allows us to examine the",
    "start": "269970",
    "end": "277020"
  },
  {
    "text": "distribution of server request latency to collect data on our API servers",
    "start": "277020",
    "end": "282060"
  },
  {
    "text": "capacity and throughput we can take a look at the API server request total counter from that metric we can derive",
    "start": "282060",
    "end": "288540"
  },
  {
    "text": "request rates and get a picture of load distribution last we might want to collect some data on errors in our API",
    "start": "288540",
    "end": "295260"
  },
  {
    "text": "servers many things can go wrong of course and what's going wrong is not always going to be obvious from a single",
    "start": "295260",
    "end": "301080"
  },
  {
    "text": "metric but here's a counter API server dropped requests total which we probably want to stay close to zero we can derive",
    "start": "301080",
    "end": "308670"
  },
  {
    "text": "the rate of dropped requests from this and perhaps build an alert if that rate exceeds zero for some period of time so",
    "start": "308670",
    "end": "316260"
  },
  {
    "start": "315000",
    "end": "315000"
  },
  {
    "text": "I've talked a little bit about what Prometheus metrics kubernetes exports but not how to use them without diving",
    "start": "316260",
    "end": "322410"
  },
  {
    "text": "into this too deeply I want to cover some use cases and open source tools that you can use to aid and measuring",
    "start": "322410",
    "end": "328050"
  },
  {
    "text": "cluster health and debugging the Prometheus query language prom QL can be",
    "start": "328050",
    "end": "333150"
  },
  {
    "text": "used for metric analysis and aggregation and we'll see some examples of an action later in this talk but where do we",
    "start": "333150",
    "end": "340050"
  },
  {
    "text": "actually write our queries if you're using a Prometheus server back-end to prototype and for one-off queries you",
    "start": "340050",
    "end": "346710"
  },
  {
    "text": "can use a Prometheus web UI which is deployed alongside the Prometheus server once you've figured out some stable",
    "start": "346710",
    "end": "353010"
  },
  {
    "text": "useful queries you can display the aggregated data in permanent dashboards graph on ax is an open source dashboard",
    "start": "353010",
    "end": "359730"
  },
  {
    "text": "and graph editor that's capable of accepting Prometheus as a data source if you want to be able to alert on your",
    "start": "359730",
    "end": "365970"
  },
  {
    "text": "queries you can configure alerting rules and setup the Prometheus alert manager to process them and last if you need if",
    "start": "365970",
    "end": "373170"
  },
  {
    "text": "you find you need a programmatic access to your data or you need to perform arbitrary queries you can also fetch",
    "start": "373170",
    "end": "379260"
  },
  {
    "text": "metrics from the Prometheus server in JSON format now an individual metric is",
    "start": "379260",
    "end": "385860"
  },
  {
    "start": "383000",
    "end": "383000"
  },
  {
    "text": "never going to tell you the whole story on your cluster lots of different issues may manifest the same way for example",
    "start": "385860",
    "end": "392850"
  },
  {
    "text": "there are many reasons why a node might go offline but in most cases you will see that nodes uptime metric drop to",
    "start": "392850",
    "end": "398550"
  },
  {
    "text": "zero a single symptom is not enough information to form a diagnosis maybe the node went offline because its",
    "start": "398550",
    "end": "404819"
  },
  {
    "text": "workload got too high and it can't report back or maybe there was a networking issue metrics can give us a",
    "start": "404819",
    "end": "410849"
  },
  {
    "text": "good idea of how something is failing but not necessarily why by exploring multiple metrics and data sources we can",
    "start": "410849",
    "end": "418650"
  },
  {
    "text": "pinpoint the specifics of a failure and figure out the why even if metrics don't show the whole",
    "start": "418650",
    "end": "425250"
  },
  {
    "start": "423000",
    "end": "423000"
  },
  {
    "text": "picture they can guide us to what we should look at next if a node is down we can take a look at system performance",
    "start": "425250",
    "end": "431220"
  },
  {
    "text": "metrics and see if there are any anomalies that might have led to the node dropping off wine we also don't",
    "start": "431220",
    "end": "437009"
  },
  {
    "text": "need to limit ourselves to metrics other tools we might use for cluster observability include log files audit",
    "start": "437009",
    "end": "443580"
  },
  {
    "text": "logs cluster events at CD dumps and more but in order to use our metrics most",
    "start": "443580",
    "end": "449699"
  },
  {
    "text": "effectively we need to understand the context in which they were produced without a mental model of how a",
    "start": "449699",
    "end": "456030"
  },
  {
    "text": "kubernetes cluster works it's much harder to pick up on the breadcrumb trail there leading us down to",
    "start": "456030",
    "end": "462900"
  },
  {
    "text": "illustrate this Han made me this chart of a cube API server that keeps going online and offline hey wait a minute is",
    "start": "462900",
    "end": "469889"
  },
  {
    "text": "this a Gruff hana chart it is indeed a rough hana chart now seeing this chart",
    "start": "469889",
    "end": "478680"
  },
  {
    "text": "you may be worried not just about the pun you might ask me those are everything in a kubernetes cluster",
    "start": "478680",
    "end": "483840"
  },
  {
    "text": "depend on a functioning API server this looks pretty bad and sure they do but it",
    "start": "483840",
    "end": "490199"
  },
  {
    "text": "turns out our applications doing just fine because kubernetes architecture does not necessarily require a",
    "start": "490199",
    "end": "495569"
  },
  {
    "text": "functioning control plane in order for the data plane to work there are more questions you should be asking me is",
    "start": "495569",
    "end": "501750"
  },
  {
    "text": "this the only API server in the cluster or are there multiple running and a high availability configuration is this",
    "start": "501750",
    "end": "507870"
  },
  {
    "text": "metric for an individual server or for the load balanced fqdn across all API servers the more systems context that we",
    "start": "507870",
    "end": "515490"
  },
  {
    "text": "have the better we can understand these signals and use them to assist us in debugging so in order to do that we're",
    "start": "515490",
    "end": "524130"
  },
  {
    "text": "going to try to pry apart the control plane just to get to the core of what we can actually observe let's start with a",
    "start": "524130",
    "end": "532260"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "little bit of context in kubernetes we basically have a hub-and-spoke model of network",
    "start": "532260",
    "end": "538470"
  },
  {
    "text": "communication which means that point-to-point interaction is not allowed between",
    "start": "538470",
    "end": "543689"
  },
  {
    "text": "components in principle the components of a control plane are neither aware nor",
    "start": "543689",
    "end": "549660"
  },
  {
    "text": "allowed to communicate with each other directly all of their interaction must be mediated by a hub which in our case",
    "start": "549660",
    "end": "557790"
  },
  {
    "text": "is going to be the cube API server on a slightly orthogonal dimension we have",
    "start": "557790",
    "end": "564300"
  },
  {
    "text": "the cubelet let's briefly go over the cube lit model there are three icons so",
    "start": "564300",
    "end": "572279"
  },
  {
    "text": "on the left you'll see the node in the middle the cubelet and on the right the",
    "start": "572279",
    "end": "577829"
  },
  {
    "text": "pods the nodes I like to think of as representing some sort of computational resource these can be VMs or even",
    "start": "577829",
    "end": "584999"
  },
  {
    "text": "physical machines on the right our pods which encapsulate a single deployable unit in kubernetes this can consist of a",
    "start": "584999",
    "end": "593160"
  },
  {
    "text": "container or multiple containers in the middle there's the couplet and that's",
    "start": "593160",
    "end": "598350"
  },
  {
    "text": "the daemon which runs on the node it is primarily responsible for making sure that the pods which are supposed to be",
    "start": "598350",
    "end": "603600"
  },
  {
    "text": "running are in fact up and healthy in most master configurations and by master",
    "start": "603600",
    "end": "610860"
  },
  {
    "start": "607000",
    "end": "607000"
  },
  {
    "text": "I mean a node which is running the components of your control plane these",
    "start": "610860",
    "end": "616170"
  },
  {
    "text": "master configurations are going to have a cubelet running your master components",
    "start": "616170",
    "end": "621300"
  },
  {
    "text": "as pods and because of this each of the master components is going to expose a",
    "start": "621300",
    "end": "627029"
  },
  {
    "text": "health endpoint which the cubelet can monitor so that it can try to restart the pod if it becomes unhealthy",
    "start": "627029",
    "end": "633199"
  },
  {
    "text": "specifically this is known as liveness probing because this health checking",
    "start": "633199",
    "end": "639990"
  },
  {
    "text": "model is such a common and pervasive way that to set up a control plane all of",
    "start": "639990",
    "end": "646470"
  },
  {
    "text": "our master components will have these health check endpoints and we can rely on this we can also rely on the fact",
    "start": "646470",
    "end": "652949"
  },
  {
    "text": "that we're going to have some metrics in the shape and the format that Elena just described and we're also going to have",
    "start": "652949",
    "end": "658980"
  },
  {
    "text": "logs and while we are always going to",
    "start": "658980",
    "end": "664800"
  },
  {
    "start": "661000",
    "end": "661000"
  },
  {
    "text": "have this baseline for it in acting the control plane not all kubernetes control components are going",
    "start": "664800",
    "end": "671529"
  },
  {
    "text": "to be equally in respectable some of them are going to be way better than others on what they can tell you about",
    "start": "671529",
    "end": "678250"
  },
  {
    "text": "your cluster or its operational state specifically what I have in mind are two",
    "start": "678250",
    "end": "683860"
  },
  {
    "text": "master components the cube API server and sed and by no means am I saying that",
    "start": "683860",
    "end": "689950"
  },
  {
    "text": "the controller manager and scheduler are not important they clearly are you can't have a functioning kubernetes control",
    "start": "689950",
    "end": "696850"
  },
  {
    "text": "plane without these things but the fact remains if you look at controller manager logs you will only ever see",
    "start": "696850",
    "end": "703209"
  },
  {
    "text": "controller manager things if you look at scheduler logs you will only ever see",
    "start": "703209",
    "end": "708279"
  },
  {
    "text": "scheduler things but what about the API server well since the API server is the",
    "start": "708279",
    "end": "715089"
  },
  {
    "start": "710000",
    "end": "710000"
  },
  {
    "text": "hub of the control plane every single component interaction must go through the cube API server to talk to",
    "start": "715089",
    "end": "720970"
  },
  {
    "text": "communicate the cube API server centralizes those interactions so if you",
    "start": "720970",
    "end": "726640"
  },
  {
    "text": "want to go digging for things which have happened in your cluster this is going to be a terrific place to start so how",
    "start": "726640",
    "end": "733480"
  },
  {
    "text": "do components communicate with API server well the cube api server provides",
    "start": "733480",
    "end": "738820"
  },
  {
    "text": "a restful HTTP api that the human the components will interact with and we can",
    "start": "738820",
    "end": "744040"
  },
  {
    "text": "actually examine this by adjusting the verbosity of any cube cuddle command so",
    "start": "744040",
    "end": "751600"
  },
  {
    "text": "if you set the verbosity of a cube cuttle command to nine or ten you can",
    "start": "751600",
    "end": "756699"
  },
  {
    "text": "see curl you can see the output which will let you copy and paste and directly",
    "start": "756699",
    "end": "766839"
  },
  {
    "text": "curl from the terminal but that's just the surface of cube api server int",
    "start": "766839",
    "end": "772930"
  },
  {
    "text": "respectability the fact is the cube api server is so central increases the value",
    "start": "772930",
    "end": "780910"
  },
  {
    "text": "of interest backing in since all component interactions will go through",
    "start": "780910",
    "end": "786519"
  },
  {
    "text": "this restful api we can see those interactions in our api server logs so",
    "start": "786519",
    "end": "791560"
  },
  {
    "text": "if we set our log verbosity high enough we're going to be able to see all of the requests that go through the",
    "start": "791560",
    "end": "799790"
  },
  {
    "text": "bi server this makes the cube API server logs a terrific source of data to mine",
    "start": "799790",
    "end": "805810"
  },
  {
    "text": "since unlike the controller manager and scheduler logs where we only see controller manager and scheduler things",
    "start": "805810",
    "end": "812440"
  },
  {
    "text": "in API server logs we will see a controller manager things scheduler things and basically anything else that",
    "start": "812440",
    "end": "818690"
  },
  {
    "text": "interacts with the cube API server all of these components are basically clients and the API server is going to",
    "start": "818690",
    "end": "825320"
  },
  {
    "text": "give us a record of these client interactions in fact we could often find various component interactions in the",
    "start": "825320",
    "end": "832130"
  },
  {
    "text": "metrics as well which are quite rich and we're going to see some of these in our examples later on we also have health",
    "start": "832130",
    "end": "839840"
  },
  {
    "text": "endpoints as you can see there's three of them and if you're wondering why we have so many that's because um until",
    "start": "839840",
    "end": "846380"
  },
  {
    "text": "quite recently the cube API server didn't ambigú eight between cube Ness",
    "start": "846380",
    "end": "851630"
  },
  {
    "text": "lightness and readiness probes and bundled everything up into a single endpoint which was healthy as of 116 the",
    "start": "851630",
    "end": "860720"
  },
  {
    "text": "cube API server supports lightness and readiness as distinct health checking mechanisms the cube API server can also",
    "start": "860720",
    "end": "868250"
  },
  {
    "text": "be introspective using audit logs depending on your cluster configuration while this is technically a security",
    "start": "868250",
    "end": "875570"
  },
  {
    "text": "feature our logs tend to be a terrific source of request information because",
    "start": "875570",
    "end": "880700"
  },
  {
    "text": "they are structured unlike our cube api server logs so if you want to query about request information it's going to",
    "start": "880700",
    "end": "887300"
  },
  {
    "text": "make your life a lot easier our next incredible source of cluster information",
    "start": "887300",
    "end": "893480"
  },
  {
    "text": "comes from a TD that CD is different from other kubernetes components in a",
    "start": "893480",
    "end": "899720"
  },
  {
    "text": "number of ways first it's a third-party component which isn't created from kubernetes proper and it's basically",
    "start": "899720",
    "end": "906950"
  },
  {
    "text": "vendored it for those of you who don't know about sedl just briefly describe it a TD is the persistence George litter",
    "start": "906950",
    "end": "914210"
  },
  {
    "text": "for kubernetes control plane it's a distributed key-value store and it uses the raft algorithm to provide strongly",
    "start": "914210",
    "end": "921860"
  },
  {
    "text": "consistent views of data stored across its members so that's what I like to",
    "start": "921860",
    "end": "927380"
  },
  {
    "text": "call the elevator description for a TD but more importantly",
    "start": "927380",
    "end": "932800"
  },
  {
    "text": "is a database it stores data its doors",
    "start": "932800",
    "end": "938620"
  },
  {
    "text": "specifically data about your cluster so if you want to look at what happened in your cluster this is going to be a",
    "start": "938620",
    "end": "945339"
  },
  {
    "text": "tremendous tremendously powerful tool for you to look at when you're trying to",
    "start": "945339",
    "end": "950769"
  },
  {
    "text": "figure out what's going on so here are some ways that you can actually look at",
    "start": "950769",
    "end": "956709"
  },
  {
    "text": "sed the first two items in the list are tools which allow you to directly access",
    "start": "956709",
    "end": "962920"
  },
  {
    "text": "data from a CD at CD cuttle is the one people are probably most familiar with",
    "start": "962920",
    "end": "968829"
  },
  {
    "text": "which is a client command-line client for interacting with SED it's super useful for ad-hoc exploration of sed",
    "start": "968829",
    "end": "975640"
  },
  {
    "text": "data the second one is auger aha which is probably a lesser known tool but it",
    "start": "975640",
    "end": "981519"
  },
  {
    "text": "shouldn't be it's a tool written by one of my friends and colleagues at Google seems to draw bets and auger the thing",
    "start": "981519",
    "end": "989589"
  },
  {
    "text": "about auger is that it can interact with a DB file directly this is super useful",
    "start": "989589",
    "end": "996579"
  },
  {
    "text": "because this means you can query for data without actually having a TD running and that ended of itself is",
    "start": "996579",
    "end": "1004170"
  },
  {
    "text": "valuable enough to justify using it but that's really not all there is to a ger",
    "start": "1004170",
    "end": "1009240"
  },
  {
    "text": "auger was specifically built to interact with kubernetes at CDs which means it",
    "start": "1009240",
    "end": "1016320"
  },
  {
    "text": "knows how to decode kubernetes objects so if you've ever used a CD cuddle to",
    "start": "1016320",
    "end": "1021510"
  },
  {
    "text": "try to look at a kubernetes object you'll will see that you'll end up with a screen of garbled protobuf it's",
    "start": "1021510",
    "end": "1028800"
  },
  {
    "text": "basically terrible Algar can help you actually read that data anyway with a TD",
    "start": "1028800",
    "end": "1037079"
  },
  {
    "text": "you also have metrics which has a ton of useful stuff like compaction and snapshot and Linc's and it also has a",
    "start": "1037079",
    "end": "1045030"
  },
  {
    "text": "health end point since sed isn't from kubernetes proper the health endpoint",
    "start": "1045030",
    "end": "1051750"
  },
  {
    "text": "doesn't really conform to the same format and you'll get JSON output back from the south end point and everything",
    "start": "1051750",
    "end": "1059429"
  },
  {
    "text": "else in the curie relays control plan kicks back plain text you can also look at the logs which has its own unique way",
    "start": "1059429",
    "end": "1065429"
  },
  {
    "text": "of marking sub levels it's going to look a bit different than your other kubernetes components so now that we have this",
    "start": "1065429",
    "end": "1072110"
  },
  {
    "text": "context let's see how we can apply this in the wild Elena would you mind walking us through the first example sure so Han",
    "start": "1072110",
    "end": "1081650"
  },
  {
    "text": "and I will show you a number of different examples to showcase how we can use the different metrics and tools we've shown you in order to debug some",
    "start": "1081650",
    "end": "1088250"
  },
  {
    "text": "common cluster issues I mentioned the scenario number of times because it's just so common so let's work through",
    "start": "1088250",
    "end": "1094820"
  },
  {
    "start": "1090000",
    "end": "1090000"
  },
  {
    "text": "this problem a node has gone offline I might find out about this in a number of different ways I see that a node is",
    "start": "1094820",
    "end": "1101510"
  },
  {
    "text": "reporting not ready or maybe the pod scheduled on a particular node are showing unknown status how might we detect this given",
    "start": "1101510",
    "end": "1109190"
  },
  {
    "text": "the metrics we have available well here's a sort of obvious solution look for all the nodes that Prometheus failed",
    "start": "1109190",
    "end": "1114950"
  },
  {
    "text": "to scream here we use our favorite Prometheus time series up since one",
    "start": "1114950",
    "end": "1120290"
  },
  {
    "text": "indicates a scrape target is on line this query it detects all nodes that are offline we could use this query to build",
    "start": "1120290",
    "end": "1126770"
  },
  {
    "text": "automation to check for nodes in the state for some period of time and then perhaps automatically reboot them so a",
    "start": "1126770",
    "end": "1132290"
  },
  {
    "text": "human doesn't have to intervene on node failures or really think about them at all of course usually things don't fail in",
    "start": "1132290",
    "end": "1139490"
  },
  {
    "text": "such a tidy way in production I'll often encounter grave failures where users",
    "start": "1139490",
    "end": "1144500"
  },
  {
    "text": "complain their pods aren't working quite right so I do some investigation and I find that all the affected pods happen",
    "start": "1144500",
    "end": "1151280"
  },
  {
    "text": "to be scheduled on a particular node so we can isolate that as a likely source of the problem but the node hasn't gone",
    "start": "1151280",
    "end": "1157010"
  },
  {
    "text": "offline so it wouldn't show up in our previous query and we're not quite sure what's wrong poking around some of the",
    "start": "1157010",
    "end": "1163460"
  },
  {
    "text": "Prometheus metrics I noticed that commonly when a cubelet is in a gray failure state and is acting slow it's",
    "start": "1163460",
    "end": "1170059"
  },
  {
    "text": "slow for everything not just pods and users so this shows up in our Prometheus",
    "start": "1170059",
    "end": "1175640"
  },
  {
    "text": "scrape duration data in this case we can detect such problems by looking at unusually high scrape time typically it",
    "start": "1175640",
    "end": "1183440"
  },
  {
    "text": "takes under a second to scrape data for a node so if the node metrics endpoint is regularly taking more than two",
    "start": "1183440",
    "end": "1188540"
  },
  {
    "text": "seconds to respond there's probably something a little off this query can help flag nodes that are not behaving",
    "start": "1188540",
    "end": "1194330"
  },
  {
    "text": "optimally both of these examples used knowledge of how Prometheus works and it's odd an automatically generated time series",
    "start": "1194330",
    "end": "1201500"
  },
  {
    "text": "to help us debug cluster issues prometheus knows whether or not it can successfully scrape a target and it",
    "start": "1201500",
    "end": "1207679"
  },
  {
    "text": "knows how long it took to scrape that target so we can use that information to our advantage here we don't rely on any",
    "start": "1207679",
    "end": "1213679"
  },
  {
    "text": "innate knowledge of how a node works to detect these issues so this is a great example of how you can do blackbox debugging with Prometheus okay so now",
    "start": "1213679",
    "end": "1224510"
  },
  {
    "text": "we're gonna go over an example which is quite near and dear to my heart because it's emerged out of the sweat blood in",
    "start": "1224510",
    "end": "1232429"
  },
  {
    "text": "tears of not just myself but a number of Engineers over Google not just once but",
    "start": "1232429",
    "end": "1238520"
  },
  {
    "text": "a number of times in varying incarnations so we're going to start with a crash lipping api server so this",
    "start": "1238520",
    "end": "1246260"
  },
  {
    "text": "is a pretty common failure mode which we should realize at this point because we know that the couplet is going to try to",
    "start": "1246260",
    "end": "1252559"
  },
  {
    "text": "restart broken containers in this case cube api server is unhealthy and it's",
    "start": "1252559",
    "end": "1257750"
  },
  {
    "text": "crashing for some reason and the cubelet is restarting it so how are we going to",
    "start": "1257750",
    "end": "1266809"
  },
  {
    "text": "figure out that this is happening hopefully we can become aware of this before someone tells us that it's",
    "start": "1266809",
    "end": "1272000"
  },
  {
    "text": "happening so one option since the cube API server exposes health a public API it's possible to set up uptime checks",
    "start": "1272000",
    "end": "1279080"
  },
  {
    "text": "against the help endpoints to generate some picture of availability this is pretty common alternatively we can",
    "start": "1279080",
    "end": "1287150"
  },
  {
    "text": "ingest metrics from the master cubelet which exposes counters for liveness and",
    "start": "1287150",
    "end": "1292820"
  },
  {
    "text": "readiness probes of the containers of the pods it's watching in this case the master cubelet is watching our QB API",
    "start": "1292820",
    "end": "1299210"
  },
  {
    "text": "server it's going to look something like this and as a disclaimer I've stripped out some of the labels to make it easier to",
    "start": "1299210",
    "end": "1305720"
  },
  {
    "text": "parse on screen the basic idea is you can set up a chart to display some of this information much in the way that",
    "start": "1305720",
    "end": "1313490"
  },
  {
    "text": "Elena had described earlier and configure some alerting to ensure that the number of field probes doesn't",
    "start": "1313490",
    "end": "1319640"
  },
  {
    "text": "exceed some pre-configured threshold so at this point we've detected that our",
    "start": "1319640",
    "end": "1326270"
  },
  {
    "text": "API servers crash looping but we need to ambigú eight between one of two situations so is the cubelet",
    "start": "1326270",
    "end": "1332419"
  },
  {
    "text": "to start a container when it sees that the container isn't running or is it",
    "start": "1332419",
    "end": "1338980"
  },
  {
    "text": "killing the process and trying to restart it in the first case the cubelet",
    "start": "1338980",
    "end": "1344330"
  },
  {
    "text": "is just in repair mode it's it sees that container which is supposed to be running isn't so it just starts the",
    "start": "1344330",
    "end": "1351289"
  },
  {
    "text": "container initialization process in the second case the cubelet is killing the",
    "start": "1351289",
    "end": "1357230"
  },
  {
    "text": "process in the in the case of a master node this means that the liveness pope has failed enough times consecutively to",
    "start": "1357230",
    "end": "1365119"
  },
  {
    "text": "trigger a cubelet restart so how can we figure out whether it's the former or",
    "start": "1365119",
    "end": "1370369"
  },
  {
    "text": "the latter there are a number of different ways we can look at the",
    "start": "1370369",
    "end": "1375739"
  },
  {
    "text": "failure account in our metrics probes endpoint that we looked at earlier possibly we can verify this by examining",
    "start": "1375739",
    "end": "1383539"
  },
  {
    "text": "the cuba logs and because it will tell you if in fact the cubelet did drive a restart in our case it's going to turn",
    "start": "1383539",
    "end": "1392239"
  },
  {
    "text": "out that the liveness it was a failure in the liveness endpoint we did have consecutive failures and the cubelet was",
    "start": "1392239",
    "end": "1397429"
  },
  {
    "text": "actively killing our API server process and restarting it we can further",
    "start": "1397429",
    "end": "1403249"
  },
  {
    "text": "investigate by curling our API server health check endpoint and passing an a",
    "start": "1403249",
    "end": "1409279"
  },
  {
    "text": "verbose flag this allows us to see all of the individual health checks are",
    "start": "1409279",
    "end": "1414590"
  },
  {
    "text": "configured for this health endpoint and as you can see here the API server health endpoint is comprised of a number",
    "start": "1414590",
    "end": "1421730"
  },
  {
    "text": "of more granular health checks and I bolded the one which is failing which is",
    "start": "1421730",
    "end": "1428570"
  },
  {
    "text": "the sed health check since the cubelet is using this endpoint for aliveness",
    "start": "1428570",
    "end": "1434570"
  },
  {
    "text": "probing that basically means if a TD is not healthy our API server is actually",
    "start": "1434570",
    "end": "1440749"
  },
  {
    "text": "going to be crash living so it turns out that our example isn't actually an API",
    "start": "1440749",
    "end": "1446600"
  },
  {
    "text": "server issue after all it's an sed issues and the slides from this point",
    "start": "1446600",
    "end": "1452779"
  },
  {
    "text": "will reflect that but now that we know it's an sed issue let's dig one of the",
    "start": "1452779",
    "end": "1458899"
  },
  {
    "start": "1458000",
    "end": "1458000"
  },
  {
    "text": "first things I do when I am debugging an sed issue is interestingly enough I look",
    "start": "1458899",
    "end": "1464270"
  },
  {
    "text": "a cube API server metric specifically I look at the STD object metric this",
    "start": "1464270",
    "end": "1471920"
  },
  {
    "text": "metric will tell you the number of objects that exist for is given kubernetes resource and believe me when",
    "start": "1471920",
    "end": "1479630"
  },
  {
    "text": "I tell you it is not uncommon for random third-party libraries that you install in your cluster to create a bazillion",
    "start": "1479630",
    "end": "1485059"
  },
  {
    "text": "objects so in this particular example there are a million such objects and",
    "start": "1485059",
    "end": "1493600"
  },
  {
    "text": "this can be more or less problematic depending on the size of the object let's say that the average object size",
    "start": "1493600",
    "end": "1500630"
  },
  {
    "text": "for this CRD is about one kilobyte doing some math if we have a million such",
    "start": "1500630",
    "end": "1505790"
  },
  {
    "text": "objects that's going to take about a gig of space if the the average size is",
    "start": "1505790",
    "end": "1511670"
  },
  {
    "text": "larger than that we really have a problem why because as we can see here",
    "start": "1511670",
    "end": "1517880"
  },
  {
    "text": "that the default size for STD is actually just two gigs so just looking",
    "start": "1517880",
    "end": "1523460"
  },
  {
    "text": "at this one metric can give us a huge hint at a potential problem which is",
    "start": "1523460",
    "end": "1529070"
  },
  {
    "text": "memory pressure issues recently we ran",
    "start": "1529070",
    "end": "1534290"
  },
  {
    "text": "into a far more insidious variant of this issue so let's take a look at these two metrics the first one is the sed",
    "start": "1534290",
    "end": "1541010"
  },
  {
    "text": "count metric and you can see that there's only one instance of this object that exists the second one gives us a",
    "start": "1541010",
    "end": "1548809"
  },
  {
    "text": "count of the number of updates for that specific resource so there is one object",
    "start": "1548809",
    "end": "1555020"
  },
  {
    "text": "but it has been updated 1200 times so why is this a problem so if we want to",
    "start": "1555020",
    "end": "1562580"
  },
  {
    "text": "correctly model this in our heads we have to take a step back we need a",
    "start": "1562580",
    "end": "1567620"
  },
  {
    "text": "proper mental model of how sed stores data so here we have a datum some object",
    "start": "1567620",
    "end": "1576260"
  },
  {
    "text": "that we store in sed there's an obvious dimension that we think about this which is the size of",
    "start": "1576260",
    "end": "1582710"
  },
  {
    "text": "that piece of that object but sed is a database it doesn't store just one",
    "start": "1582710",
    "end": "1588950"
  },
  {
    "text": "object it's a key value store it stores an object for each key there are many",
    "start": "1588950",
    "end": "1595100"
  },
  {
    "text": "objects so we can see that we have this dimension but if we were to take the",
    "start": "1595100",
    "end": "1601000"
  },
  {
    "text": "average size of objects and multiply it by the object count would that give us an approximate size of the data used by",
    "start": "1601000",
    "end": "1608410"
  },
  {
    "text": "a CD and the answer to that question is no at CD provides a watch mechanism so",
    "start": "1608410",
    "end": "1617680"
  },
  {
    "text": "that subscribers can see changes to objects over time which means that CD",
    "start": "1617680",
    "end": "1622900"
  },
  {
    "text": "must store a historical record of changed objects so if we revisit the",
    "start": "1622900",
    "end": "1629410"
  },
  {
    "text": "metrics from the last slide we can see that while there's actually only one instance of that object that 1,200",
    "start": "1629410",
    "end": "1636490"
  },
  {
    "text": "writes to that object means that there's actually 1200 versions of that object so",
    "start": "1636490",
    "end": "1642010"
  },
  {
    "text": "it's quite a bit different than what we initially thought so even though we have",
    "start": "1642010",
    "end": "1649000"
  },
  {
    "text": "1,200 objects we also now have to consider the size so let's just consider for instance that the size of this",
    "start": "1649000",
    "end": "1656110"
  },
  {
    "text": "object is large say 200 kilobytes let's",
    "start": "1656110",
    "end": "1661150"
  },
  {
    "text": "do some math so 200 kilobytes let's say API servers crash lipping once every",
    "start": "1661150",
    "end": "1668530"
  },
  {
    "text": "minute so we have 1200 writes in a minute that's roughly 20 writes a second",
    "start": "1668530",
    "end": "1673930"
  },
  {
    "text": "so if the size of the object is 200 kilobytes then that's 4 megabytes a",
    "start": "1673930",
    "end": "1680140"
  },
  {
    "text": "second or two hundred forty megabytes a minute which is about a quarter Giga minute now",
    "start": "1680140",
    "end": "1686500"
  },
  {
    "text": "SCD compacts every 5 minutes so by the time compaction occurs we have accumulated over a gig of data which is",
    "start": "1686500",
    "end": "1693790"
  },
  {
    "text": "problematic when you again consider the default at CD database size considering",
    "start": "1693790",
    "end": "1699190"
  },
  {
    "text": "that you are probably going to have other stuff in your STD besides just this one object so then what we really",
    "start": "1699190",
    "end": "1708520"
  },
  {
    "text": "need to be able to do is ballpark the size of the object if you can catch the API server while it's healthy you can",
    "start": "1708520",
    "end": "1714220"
  },
  {
    "text": "use cube cuddle but in our case Etsy D is actually not healthy and the cube API",
    "start": "1714220",
    "end": "1721540"
  },
  {
    "text": "server is not healthy so this is when it becomes super useful to use augur because you can operate on the bolt DB",
    "start": "1721540",
    "end": "1728230"
  },
  {
    "text": "file itself and that's how you would approximate the",
    "start": "1728230",
    "end": "1733600"
  },
  {
    "text": "size of an object using augur so in the nasty variant of this case that we encountered recently it turned out that",
    "start": "1733600",
    "end": "1740170"
  },
  {
    "text": "this custom resource had a number of base64 encoded images embedded into the",
    "start": "1740170",
    "end": "1745930"
  },
  {
    "text": "annotations field which drastically bloated the size of this object there",
    "start": "1745930",
    "end": "1751510"
  },
  {
    "text": "was also a custom controller which was going rogue updating the object due to a",
    "start": "1751510",
    "end": "1756730"
  },
  {
    "text": "bug in the Equality comparison operation for the CRT basically there was an an",
    "start": "1756730",
    "end": "1762130"
  },
  {
    "text": "unordered list which was always returning as unequal so the controller would make an update request because it",
    "start": "1762130",
    "end": "1768820"
  },
  {
    "text": "thought the object had changed so this high right rate led us to bump into IO",
    "start": "1768820",
    "end": "1774130"
  },
  {
    "text": "throttling limits but we were able to mitigate this by contacting the CR the",
    "start": "1774130",
    "end": "1779590"
  },
  {
    "text": "owners and getting them to reduce the size of the images that they were storing so the moral of the story",
    "start": "1779590",
    "end": "1784660"
  },
  {
    "text": "probably don't store images in your CR DS if you if you have to try to make",
    "start": "1784660",
    "end": "1791650"
  },
  {
    "text": "them as small as possible ultimately we got the custom controller fixed which",
    "start": "1791650",
    "end": "1796990"
  },
  {
    "text": "completely resolved the issue leading us to our happy ending well would you like to walk us to our",
    "start": "1796990",
    "end": "1802600"
  },
  {
    "text": "next example yeah last example so last fun problem for a period of time",
    "start": "1802600",
    "end": "1808930"
  },
  {
    "start": "1805000",
    "end": "1805000"
  },
  {
    "text": "I had an issue with API server access being slow both via cube cuddle and API",
    "start": "1808930",
    "end": "1814000"
  },
  {
    "text": "calls now the API server collected metrics for this so I figured I'd dig into that to",
    "start": "1814000",
    "end": "1819670"
  },
  {
    "text": "see if there was any wisdom I might gain perhaps the particular verb was slow like poster patch and that might",
    "start": "1819670",
    "end": "1825100"
  },
  {
    "text": "indicate some issue with write performance so this query allows us to plot out HTTP call Layton sees grouped",
    "start": "1825100",
    "end": "1831580"
  },
  {
    "text": "by HTTP verb and see if there's anything particularly slow so here's that query",
    "start": "1831580",
    "end": "1837400"
  },
  {
    "text": "applauded in Griffin against a production cluster representing p99 Layton sees of the API servers by HTTP",
    "start": "1837400",
    "end": "1843970"
  },
  {
    "text": "verb over a 24-hour period the Green Line is a marker for 5 seconds and this",
    "start": "1843970",
    "end": "1849280"
  },
  {
    "text": "graph has a linear scale it looks like there was a big spike in post latency towards the left well over 5 seconds now",
    "start": "1849280",
    "end": "1857230"
  },
  {
    "text": "you may notice something a little fishy about this data if I look at the Max and current values of",
    "start": "1857230",
    "end": "1863470"
  },
  {
    "text": "most of the verbs and the legend on the right the current HTTP latency clusters just below 125 milliseconds that's",
    "start": "1863470",
    "end": "1871419"
  },
  {
    "text": "pretty slow for an HTTP call and also kind of weird number the second thing",
    "start": "1871419",
    "end": "1876669"
  },
  {
    "text": "you may notice is the maximum recorded HTTP call latency is exactly eight seconds again",
    "start": "1876669",
    "end": "1882250"
  },
  {
    "text": "weird number so I dug into this and things got even stranger because I noticed that the histogram metrics being",
    "start": "1882250",
    "end": "1888730"
  },
  {
    "text": "exported did not match the summary metrics within orders of magnitude sometimes and it turns out that the",
    "start": "1888730",
    "end": "1895960"
  },
  {
    "text": "default API server histogram metrics in kubernetes were total nonsense because the min and Max bucket sizes were 125",
    "start": "1895960",
    "end": "1902260"
  },
  {
    "text": "milliseconds and eight seconds so I dug into the code at some point to confirm",
    "start": "1902260",
    "end": "1907690"
  },
  {
    "text": "this and here is that code with a fix to add better bucketing sizes between 50",
    "start": "1907690",
    "end": "1912760"
  },
  {
    "text": "milliseconds and 60 seconds instead of a hundred and 25 milliseconds at 8 seconds after getting involved in upstream and",
    "start": "1912760",
    "end": "1919510"
  },
  {
    "text": "voicing concerns around problems like this the fix landed in kubernetes 114 as part of the metrics overhaul enhancement",
    "start": "1919510",
    "end": "1925630"
  },
  {
    "text": "proposal I helped write so while that query didn't help me pinpoint the source",
    "start": "1925630",
    "end": "1930760"
  },
  {
    "text": "of the API server slowness it did help me confirm that things were slow and I was also able to confirm the issue and",
    "start": "1930760",
    "end": "1937090"
  },
  {
    "text": "suggest improvements upstream which is a good opportunity for me to jump into",
    "start": "1937090",
    "end": "1942429"
  },
  {
    "text": "talking a little more about upstream in our last example we saw there was an underlying issue with a metric that was",
    "start": "1942429",
    "end": "1948610"
  },
  {
    "start": "1945000",
    "end": "1945000"
  },
  {
    "text": "providing bad data to cluster operators and made it difficult for us to figure out what was going on how does the",
    "start": "1948610",
    "end": "1954789"
  },
  {
    "text": "instrumentation address these sorts of problems well it's a bit complicated updating metrics between releases could",
    "start": "1954789",
    "end": "1961900"
  },
  {
    "text": "break monitoring stacks for example fixing the API server latency buckets meant that users would potentially see",
    "start": "1961900",
    "end": "1967990"
  },
  {
    "text": "wildly different performance metrics between a 113 and 114 upgrade despite the actual performance not",
    "start": "1967990",
    "end": "1973840"
  },
  {
    "text": "necessarily changing this could result in anomaly detection throwing incorrect alerts between releases and there are",
    "start": "1973840",
    "end": "1980440"
  },
  {
    "text": "other issues beyond the metric data itself if a new metric is added that causes a memory leak or generates too",
    "start": "1980440",
    "end": "1986049"
  },
  {
    "text": "many time series there's no real way to disable it meaning a full bug fix and cluster upgrade rollout needs to be",
    "start": "1986049",
    "end": "1991299"
  },
  {
    "text": "performed when bad metrics are discovered so how can we coordinate developers to address this and",
    "start": "1991299",
    "end": "1996830"
  },
  {
    "text": "sure that end-users are fully informed about these sorts of problems Cygnus fermentation has spent a lot of",
    "start": "1996830",
    "end": "2002950"
  },
  {
    "start": "2001000",
    "end": "2001000"
  },
  {
    "text": "time identifying and cataloging broken metrics like in our API bucketing example but that's not the only one",
    "start": "2002950",
    "end": "2009909"
  },
  {
    "text": "for example the labels on C advisor metrics did not match the kubernetes instrumentation guidelines so they",
    "start": "2009909",
    "end": "2015970"
  },
  {
    "text": "couldn't be joined with other kubernetes metrics like those exported by Cuba State metrics there were also a number",
    "start": "2015970",
    "end": "2021789"
  },
  {
    "text": "of time series that use the wrong data types for example cubelets were only exporting spod start time in summary",
    "start": "2021789",
    "end": "2028480"
  },
  {
    "text": "form meaning you could not aggravate pod start times across a cluster in order to rank pod latencies by percentile you",
    "start": "2028480",
    "end": "2035500"
  },
  {
    "text": "need a histogram for that there were also issues with unit standardization some time series run in microseconds",
    "start": "2035500",
    "end": "2043029"
  },
  {
    "text": "others were in seconds and in some pretty bad cases they claimed they were in seconds when they actually were in",
    "start": "2043029",
    "end": "2049118"
  },
  {
    "text": "microseconds this did not conform with from easiest and kubernetes best practices for unit normalization so we",
    "start": "2049119",
    "end": "2056770"
  },
  {
    "text": "put together a cap detailing the problems and we fixed it most of those fixes rolled out in the 114 release now",
    "start": "2056770",
    "end": "2064419"
  },
  {
    "text": "for some metrics we could for follow an informal deprecation policy for example marking the time series with",
    "start": "2064419",
    "end": "2070148"
  },
  {
    "text": "non-standard units as deprecated in their help text and in the code and adding brand new time series with the",
    "start": "2070149",
    "end": "2075520"
  },
  {
    "text": "correct units but others like API server bucketing needs to be updated directly this might cause confusion to end users",
    "start": "2075520",
    "end": "2082030"
  },
  {
    "text": "of those metrics so how do we communicate that those metrics are deprecated and ensure users are able to",
    "start": "2082030",
    "end": "2087490"
  },
  {
    "text": "smoothly transition off them for these purposes sig instrumentation has developed a metric stability framework",
    "start": "2087490",
    "end": "2093970"
  },
  {
    "start": "2090000",
    "end": "2090000"
  },
  {
    "text": "and we're working to transition all kubernetes control plane metrics to use that framework the metric stability",
    "start": "2093970",
    "end": "2099580"
  },
  {
    "text": "proposal will allow us to treat metrics as a proper API and with an enforced automated multi release notice period",
    "start": "2099580",
    "end": "2105910"
  },
  {
    "text": "for changes to stable metrics it will also allow us to slowly phase out obsolete metrics across releases before",
    "start": "2105910",
    "end": "2112210"
  },
  {
    "text": "deletion and most importantly it will provide tooling to enforce stability for example via static analysis in practice",
    "start": "2112210",
    "end": "2120190"
  },
  {
    "start": "2119000",
    "end": "2119000"
  },
  {
    "text": "what this means is we've replaced all directs use of Prometheus metrics with a component base map wrapper which allows",
    "start": "2120190",
    "end": "2125349"
  },
  {
    "text": "us to annotate metrics with a stability level and deprecation status like you can see in the on the screen so it's a great first step",
    "start": "2125349",
    "end": "2133990"
  },
  {
    "text": "that we now have the capability to mark metrics as stable or deprecated but there's still a bunch more implementation to come now that we have",
    "start": "2133990",
    "end": "2140950"
  },
  {
    "text": "the metric stability framework in place we need to decide what metrics should actually be considered stable and to",
    "start": "2140950",
    "end": "2146680"
  },
  {
    "text": "accomplish that we need to develop criteria around what constitutes a stable metric and select some metrics to",
    "start": "2146680",
    "end": "2151990"
  },
  {
    "text": "be promoted to statement we'd also like to add flags to the various kubernetes components to allow for disabling",
    "start": "2151990",
    "end": "2158740"
  },
  {
    "text": "individual metrics at runtime so that if a new alpha metric causes stability issues by leaking memory or having",
    "start": "2158740",
    "end": "2164319"
  },
  {
    "text": "cardinality explosions we can just turn it off rather than having to do a full bug fix rollout you may have noticed we",
    "start": "2164319",
    "end": "2171160"
  },
  {
    "text": "didn't discuss tracing in this talk and there's a sig instrumentation cap under consideration for adding distributed",
    "start": "2171160",
    "end": "2176170"
  },
  {
    "text": "tracing support kubernetes components and of lot as of last Friday a new cap was also proposed to add structured logs",
    "start": "2176170",
    "end": "2183130"
  },
  {
    "text": "to all kubernetes components which is a thing I have been asking for for a year and finally we have more metric",
    "start": "2183130",
    "end": "2189849"
  },
  {
    "text": "improvements in the pipeline as we continue to identify suboptimal metrics and iterate on them so if you want to",
    "start": "2189849",
    "end": "2196420"
  },
  {
    "text": "learn more about any of this we invite you to intend the cig instrumentation intro and deep dive sessions which are",
    "start": "2196420",
    "end": "2201940"
  },
  {
    "text": "scheduled in the next slot today and the same timeslot tomorrow and I think we",
    "start": "2201940",
    "end": "2207549"
  },
  {
    "text": "don't have time for questions but you can come find us after the talk thank you",
    "start": "2207549",
    "end": "2213440"
  },
  {
    "text": "[Applause]",
    "start": "2213440",
    "end": "2217789"
  }
]