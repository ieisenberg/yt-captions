[
  {
    "start": "0",
    "end": "89000"
  },
  {
    "text": "okay hello everyone so my name is Tina and I'm gonna talk about party smart in",
    "start": "30",
    "end": "5580"
  },
  {
    "text": "communities so basically I'm going to give a technical deep dive into like I",
    "start": "5580",
    "end": "11130"
  },
  {
    "text": "know some internal of this project so for the rest of the presentation I'm",
    "start": "11130",
    "end": "16890"
  },
  {
    "text": "gonna use English actually so because for some of the terms really hard to translate them into Chinese a back and",
    "start": "16890",
    "end": "22890"
  },
  {
    "text": "forth so I'm gonna stick to stick to English for the rest of the presentation",
    "start": "22890",
    "end": "29570"
  },
  {
    "text": "okay so first I'm gonna start off to talk about some of the motivation behind",
    "start": "29720",
    "end": "34860"
  },
  {
    "text": "this project and also I will give a brief overview of the history of the project so falling that I will give you",
    "start": "34860",
    "end": "42390"
  },
  {
    "text": "know to technical deep dive into some of the internals of how everything works together so then we'll talk about some",
    "start": "42390",
    "end": "49950"
  },
  {
    "text": "of the existing features that we have already made into spark and okay book",
    "start": "49950",
    "end": "55680"
  },
  {
    "text": "overview of you know some of the future roadmap like you know so what do you can expect from the the community so what",
    "start": "55680",
    "end": "62609"
  },
  {
    "text": "kind of new feature you will see you know upcoming smart radius so finally I'll take some questions",
    "start": "62609",
    "end": "70159"
  },
  {
    "text": "so some background by myself I'm a software engineer on the GPU",
    "start": "70400",
    "end": "75540"
  },
  {
    "text": "workers team at Google so I also co-chair the community speak data I'm",
    "start": "75540",
    "end": "81479"
  },
  {
    "text": "also a Padres spark meter so mainly due to my work too you know the spark homies",
    "start": "81479",
    "end": "86909"
  },
  {
    "text": "in equation so like there's you know a",
    "start": "86909",
    "end": "94079"
  },
  {
    "start": "89000",
    "end": "89000"
  },
  {
    "text": "lot of reasons why people want to run spark natively on communities but I think these are the main reasons behind",
    "start": "94079",
    "end": "100380"
  },
  {
    "text": "the project so one of the main reasons like you know so the rapidly growing ecosystem around docker container and",
    "start": "100380",
    "end": "108450"
  },
  {
    "text": "kubernetes so people might have already been running like you know a lot of",
    "start": "108450",
    "end": "113490"
  },
  {
    "text": "these tuning for their online serving workloads were stateless we're closed so naturally they want to run all these",
    "start": "113490",
    "end": "120990"
  },
  {
    "text": "tooling you know also to a four patch kind of workloads or data processing workloads so they wanted like you know",
    "start": "120990",
    "end": "128399"
  },
  {
    "text": "leverage these rich tooling also for their batch workloads so another",
    "start": "128399",
    "end": "133770"
  },
  {
    "text": "like nurse are an increasing trend of like you know people adopting multi",
    "start": "133770",
    "end": "139050"
  },
  {
    "text": "cloud or hybrid cloud environment so you might have like you know you might use services from more than one cloud",
    "start": "139050",
    "end": "146070"
  },
  {
    "text": "providers like and you might run some services on Google or some part of the services on yeah for example Amazon so",
    "start": "146070",
    "end": "153810"
  },
  {
    "text": "also some companies actually day to hybrid set up so they'd run part of their workloads on in a cloud in the",
    "start": "153810",
    "end": "161400"
  },
  {
    "text": "meanwhile they also run some of their workouts on their own prime datacenters so in that case like a portability is",
    "start": "161400",
    "end": "167850"
  },
  {
    "text": "the key like I know so you cannot you want to achieve portability by basically you don't need to do anything then you can move your",
    "start": "167850",
    "end": "174030"
  },
  {
    "text": "workloads from one environment to the other so while kubernetes is the key to",
    "start": "174030",
    "end": "180120"
  },
  {
    "text": "achieve you know portability so naturally we want spark to be in a to be",
    "start": "180120",
    "end": "185460"
  },
  {
    "text": "able to natively run on kubernetes so last but not least we see a trend like",
    "start": "185460",
    "end": "193110"
  },
  {
    "text": "people wanted to consolidate their hardware resources so traditionally",
    "start": "193110",
    "end": "199709"
  },
  {
    "text": "people run big data workloads on things like yarn or missiles well we're seeing",
    "start": "199709",
    "end": "205290"
  },
  {
    "text": "recently like people wanted to move all these data processing workloads also to communities well for the reason like in",
    "start": "205290",
    "end": "211680"
  },
  {
    "text": "they're already operating coordinators class therefore there are online serving workloads so they don't want them in",
    "start": "211680",
    "end": "218400"
  },
  {
    "text": "like in a two operation team one for communities but then one for data so why not like you know just combine them",
    "start": "218400",
    "end": "224310"
  },
  {
    "text": "together so they could be just need to operate one environment for all their workloads so and also by combining you",
    "start": "224310",
    "end": "233700"
  },
  {
    "text": "know different kind of hardware requirements into one you can achieve better resource utilization you know you",
    "start": "233700",
    "end": "239670"
  },
  {
    "text": "can reduce cost all these kind of benefits so some background about spark",
    "start": "239670",
    "end": "246959"
  },
  {
    "start": "244000",
    "end": "244000"
  },
  {
    "text": "you know just in case you don't know that so it's currently the most popular framework for large-scale data",
    "start": "246959",
    "end": "253080"
  },
  {
    "text": "processing it's always often considered the next generation Hadoop",
    "start": "253080",
    "end": "258660"
  },
  {
    "text": "so it's actually widely used by some of the world largest companies like Tencent",
    "start": "258660",
    "end": "263910"
  },
  {
    "text": "like you know facebook / JD so it has several language bindings like",
    "start": "263910",
    "end": "270990"
  },
  {
    "text": "you can actually run spark you can write spark in Python in Scala in Java or in",
    "start": "270990",
    "end": "276449"
  },
  {
    "text": "our it also has a sequin interface so you can actually run sequel queries on spark natively so sometimes spark is",
    "start": "276449",
    "end": "285270"
  },
  {
    "text": "called the unified analytics engine well by that it means like you know you can",
    "start": "285270",
    "end": "290520"
  },
  {
    "text": "actually run all kinds of different batch workloads and streaming workloads and sometimes machine weren't learning",
    "start": "290520",
    "end": "295530"
  },
  {
    "text": "workloads and processing workloads all on spark so you have one shop for all",
    "start": "295530",
    "end": "301770"
  },
  {
    "text": "kinds of you know analytics needs so currently on github it has overnighting",
    "start": "301770",
    "end": "309509"
  },
  {
    "text": "cells and starts and over 17,000 Forks so come together with kubernetes they",
    "start": "309509",
    "end": "316800"
  },
  {
    "text": "are basically two of the most popular open source project you know on github so this is a like sort of like timeline",
    "start": "316800",
    "end": "326160"
  },
  {
    "start": "322000",
    "end": "322000"
  },
  {
    "text": "of the project history so in November 2016 we started the design and soon",
    "start": "326160",
    "end": "333479"
  },
  {
    "text": "after that in December with these we started that work on in one of English",
    "start": "333479",
    "end": "339330"
  },
  {
    "text": "or actually sell water reason why we chose to do it in a you know fork so you know we wanted to be iterative iterating",
    "start": "339330",
    "end": "346590"
  },
  {
    "text": "rolling fast so without you know doing all the code review stuff against the main repo so we decided to do it in a",
    "start": "346590",
    "end": "353610"
  },
  {
    "text": "fork so in meet 2017 we poured all the work from spark 2.1 to spark 2.2 but it",
    "start": "353610",
    "end": "361530"
  },
  {
    "text": "still you know fork and we did a beta release and so you know people can actually get a sense on what that was",
    "start": "361530",
    "end": "367440"
  },
  {
    "text": "for life and also gave us feedbacks on and you know issues so in September 2017",
    "start": "367440",
    "end": "375690"
  },
  {
    "text": "at the SP IP which stands for spark project improvement proposal passed so",
    "start": "375690",
    "end": "381120"
  },
  {
    "text": "to me Sacre we can start it we could start porting the work we did in a fork",
    "start": "381120",
    "end": "386159"
  },
  {
    "text": "to the man spark repo so around the timeframe of December 2017 and January",
    "start": "386159",
    "end": "393300"
  },
  {
    "text": "2018 so part of it part of the work that we did in the fork was upstream into",
    "start": "393300",
    "end": "398400"
  },
  {
    "text": "spark 2.3 so that's we that's one that the work was like you",
    "start": "398400",
    "end": "405430"
  },
  {
    "text": "know seen by the mainstream users so basically people started to talk about",
    "start": "405430",
    "end": "411669"
  },
  {
    "text": "like in the spark on kubernetes since the release of spark to point straight so in October well basically that you",
    "start": "411669",
    "end": "418569"
  },
  {
    "text": "know just like about a month ago so we added more featuring to spark 2.4 that",
    "start": "418569",
    "end": "424990"
  },
  {
    "text": "all we'll talk about in the later of part of the presentation so on a high",
    "start": "424990",
    "end": "433000"
  },
  {
    "start": "430000",
    "end": "430000"
  },
  {
    "text": "level the native integration with kubernetes in apache spark basically gives you another option to run your",
    "start": "433000",
    "end": "439960"
  },
  {
    "text": "spark jobs so it's it's besides standalone yarn and missiles that means",
    "start": "439960",
    "end": "446470"
  },
  {
    "text": "actually you can run now you have you have the forest choices like you know so you can actually run spark job",
    "start": "446470",
    "end": "452289"
  },
  {
    "text": "negatively on kubernetes like what do you run that on any other coarse-grained",
    "start": "452289",
    "end": "458979"
  },
  {
    "text": "scheduler backends so the native integration with kubernetes in spark",
    "start": "458979",
    "end": "465310"
  },
  {
    "start": "461000",
    "end": "461000"
  },
  {
    "text": "basically has two parts there's a community specific summation client and",
    "start": "465310",
    "end": "470560"
  },
  {
    "text": "also there's a kubernetes scheduler package that's running the driver so",
    "start": "470560",
    "end": "476259"
  },
  {
    "text": "when it comes to the submission client it sector pretty straightforward so it starts with a empty driver pass spec and",
    "start": "476259",
    "end": "485279"
  },
  {
    "text": "it applies a bunch of feature would we call feature steps to the MTA drivers",
    "start": "485279",
    "end": "491110"
  },
  {
    "text": "paw spec and finally it generates the the final driver pass back and it sends",
    "start": "491110",
    "end": "498279"
  },
  {
    "text": "it to the api server to create the driver pod so then Cooper need a master",
    "start": "498279",
    "end": "504250"
  },
  {
    "text": "or the scheduler basically schedules the driver power to run on some of the nodes",
    "start": "504250",
    "end": "509430"
  },
  {
    "text": "so some of the features tabs there are things like mounting volumes or mounting",
    "start": "509430",
    "end": "514719"
  },
  {
    "text": "secrets and also like you know adding environment variables specified by the",
    "start": "514719",
    "end": "519969"
  },
  {
    "text": "users those kind of things so basically each feature step adds some part of",
    "start": "519969",
    "end": "525430"
  },
  {
    "text": "customization into the driver prospect so the final result is a complete for",
    "start": "525430",
    "end": "531430"
  },
  {
    "text": "driver prospect that you can just send to the API server for Korea the pod so the other part is the",
    "start": "531430",
    "end": "540399"
  },
  {
    "start": "537000",
    "end": "537000"
  },
  {
    "text": "scheduler back-end which is a custom implementation of the coarse-grained scheduler back in spark it runs in the",
    "start": "540399",
    "end": "548470"
  },
  {
    "text": "driver so the responsibility of the scooter back-end is basically it it acts",
    "start": "548470",
    "end": "554889"
  },
  {
    "text": "as a sort of like a custom and controllers for executors pot so it",
    "start": "554889",
    "end": "560019"
  },
  {
    "text": "manages these two managers the life cycle of these executor pots so together",
    "start": "560019",
    "end": "565209"
  },
  {
    "text": "with the spark core it does these things like you know so they handle task scheduling so the sky dramatic back and",
    "start": "565209",
    "end": "573190"
  },
  {
    "text": "manages the executor pods so it also wants you know the executor power starts running so the schedule back and we'll",
    "start": "573190",
    "end": "580029"
  },
  {
    "text": "watch the events on these pots and take actions on certain events for example if",
    "start": "580029",
    "end": "587230"
  },
  {
    "text": "for whatever reason one of the exterior policy is deleted from the cluster so the schedule back and we'll try to",
    "start": "587230",
    "end": "593260"
  },
  {
    "text": "recreate that so the tobacco nature",
    "start": "593260",
    "end": "598480"
  },
  {
    "text": "takes at the spark configuration from the spark core and also it takes order to request a new executor or queue",
    "start": "598480",
    "end": "605860"
  },
  {
    "text": "existing executors from the spark court and thus this thing's accordingly by",
    "start": "605860",
    "end": "612490"
  },
  {
    "text": "talking to the APS over so on a cluster site so the kubernetes class are basically like analyze these very basic",
    "start": "612490",
    "end": "620170"
  },
  {
    "text": "services schedules and runs your driver and Exeter pods and it provides",
    "start": "620170",
    "end": "625420"
  },
  {
    "text": "networking support like and so you can actually so for example if you want to talk to and remote HDFS class there or",
    "start": "625420",
    "end": "632649"
  },
  {
    "text": "or it also provides networking support for the exit or pod to connect to the",
    "start": "632649",
    "end": "638800"
  },
  {
    "text": "driver and yeah it also provides like you know the login support so you can",
    "start": "638800",
    "end": "644079"
  },
  {
    "text": "actually use Co baccardo logs to to to fetch the logs of either your driver or",
    "start": "644079",
    "end": "649839"
  },
  {
    "text": "your executor pause so under the hood so",
    "start": "649839",
    "end": "654940"
  },
  {
    "start": "652000",
    "end": "652000"
  },
  {
    "text": "when someone like a user runs sparks on mate by giving it a master URL of the stars",
    "start": "654940",
    "end": "663670"
  },
  {
    "text": "with skaters so what happens under under nice is that",
    "start": "663670",
    "end": "669850"
  },
  {
    "text": "so the summation clients will run these steps that just mentioned to create a",
    "start": "669850",
    "end": "675560"
  },
  {
    "text": "driver pass back and send it to the API server so once the driver pod gets created this",
    "start": "675560",
    "end": "684020"
  },
  {
    "text": "the master will schedule that to run on some of the nodes in a cluster so once the driver part starts running",
    "start": "684020",
    "end": "690410"
  },
  {
    "text": "will request executor parts in mini batches so what writers start you know a",
    "start": "690410",
    "end": "696980"
  },
  {
    "text": "batch of Exeter pause whenever that possible so then the master will try to",
    "start": "696980",
    "end": "706460"
  },
  {
    "text": "schedule these Exeter parts onto some of the nodes on a cluster so once they",
    "start": "706460",
    "end": "713030"
  },
  {
    "text": "start they will connect to the driver pod - so though your application your",
    "start": "713030",
    "end": "719750"
  },
  {
    "text": "spark app is basically you know it's running like it would normally so",
    "start": "719750",
    "end": "726250"
  },
  {
    "text": "anything well so once the Exeter path start running so the driver pod will",
    "start": "726250",
    "end": "732280"
  },
  {
    "text": "start watching the events of the pod events on these secular paths and take",
    "start": "732280",
    "end": "737330"
  },
  {
    "text": "action so accordingly so the way or the",
    "start": "737330",
    "end": "742400"
  },
  {
    "text": "process the executor pass get created it's very similar so the driver politics",
    "start": "742400",
    "end": "747560"
  },
  {
    "text": "and empty excerpt or pass spec and then runs a bunch of steps on the MT Exeter",
    "start": "747560",
    "end": "755960"
  },
  {
    "text": "pops back to generate what are we called a final exterior prospect and then send",
    "start": "755960",
    "end": "761180"
  },
  {
    "text": "it today guess over so this will basically create on one executor pods so",
    "start": "761180",
    "end": "767780"
  },
  {
    "text": "again these steps do things like you know if you want to mount some secrets to your Exeter power so for example you",
    "start": "767780",
    "end": "774470"
  },
  {
    "text": "want to access some remote services or for example HDFS security defense the",
    "start": "774470",
    "end": "780200"
  },
  {
    "text": "secret might carry the delegation token in that so you want to actually mounted that into your your pods so they can",
    "start": "780200",
    "end": "787580"
  },
  {
    "text": "actually use that for act remote access so yeah so then once the",
    "start": "787580",
    "end": "794930"
  },
  {
    "text": "Exeter part gets rescheduled it starts running and we attach I emptied our Wallen to be used for as",
    "start": "794930",
    "end": "803009"
  },
  {
    "text": "the working directory for the executor so because of the way networking works",
    "start": "803009",
    "end": "811049"
  },
  {
    "start": "807000",
    "end": "807000"
  },
  {
    "text": "in kubernetes so the way the Exeter paws connect to the driver pod is also pretty",
    "start": "811049",
    "end": "816359"
  },
  {
    "text": "interesting so on yarn or on you know outer scheduler pack ins you're a",
    "start": "816359",
    "end": "821869"
  },
  {
    "text": "networking connectivity is a grunted like you know you you would assume so",
    "start": "821869",
    "end": "828209"
  },
  {
    "text": "the Exeter pods will be able to talk to the driver pod natively but kubernetes things are different so actually the",
    "start": "828209",
    "end": "836399"
  },
  {
    "text": "executor paths connect to the driver pass through a fully qualified DNS name so you know like you know on communities",
    "start": "836399",
    "end": "842850"
  },
  {
    "text": "in order to have to give apart a fully qualified DNS name you have to put the",
    "start": "842850",
    "end": "848970"
  },
  {
    "text": "part behind a service so this is actually handled by the submission client so the summation client before",
    "start": "848970",
    "end": "855809"
  },
  {
    "text": "creating a driver pod it actually creates a header a service for the pod",
    "start": "855809",
    "end": "861149"
  },
  {
    "text": "so this basically gives the driver pod a fully qualify DNS name so then so after",
    "start": "861149",
    "end": "867179"
  },
  {
    "text": "that you know once the executors pass start they can actually use this fqdn name to talk to the driver",
    "start": "867179",
    "end": "876389"
  },
  {
    "text": "so yeah this animation basically shows what happens like and so the driver so",
    "start": "876389",
    "end": "882389"
  },
  {
    "text": "the submission client basically sets up a service for endpoint and driver for",
    "start": "882389",
    "end": "888119"
  },
  {
    "text": "for for the driver and block manager in the driver process so this is actually I",
    "start": "888119",
    "end": "894809"
  },
  {
    "text": "had a headlight service which was pretty straightforward and then after that the",
    "start": "894809",
    "end": "900329"
  },
  {
    "text": "exterior will be able to connect to the driver using that service so again",
    "start": "900329",
    "end": "908839"
  },
  {
    "start": "906000",
    "end": "906000"
  },
  {
    "text": "because of the way cover needs works like so some seeing you naturally do in",
    "start": "908839",
    "end": "915809"
  },
  {
    "text": "outer without our scheduler backends like yarn or Miso's wouldn't work in",
    "start": "915809",
    "end": "921269"
  },
  {
    "text": "kubernetes one thing is like no Yukon is because the driver runs in the in the cluster in a pod",
    "start": "921269",
    "end": "927089"
  },
  {
    "text": "it doesn't have access to these application dependencies like jars or",
    "start": "927089",
    "end": "932279"
  },
  {
    "text": "data or config files on your locker submission machine so to solve these",
    "start": "932279",
    "end": "939749"
  },
  {
    "text": "problems actually we support these kind of like to dependency management options",
    "start": "939749",
    "end": "945329"
  },
  {
    "text": "one is we so we basically provides a bit a set of basic images you can use to",
    "start": "945329",
    "end": "951059"
  },
  {
    "text": "build upon so one way is you know you can always fake your dependencies into",
    "start": "951059",
    "end": "956519"
  },
  {
    "text": "your custom images then use these to run your your as like you know as the SPARC",
    "start": "956519",
    "end": "963480"
  },
  {
    "text": "driver or order executors which basically already have all the independencies locally visiting the",
    "start": "963480",
    "end": "969089"
  },
  {
    "text": "container or the other way that we all recommend for large amount of data is",
    "start": "969089",
    "end": "975029"
  },
  {
    "text": "that you just put your data or like you know large dependencies like jars onto",
    "start": "975029",
    "end": "980970"
  },
  {
    "text": "some remote storage system like HDFS or Google Cloud Storage or or or Amazon s3",
    "start": "980970",
    "end": "987180"
  },
  {
    "text": "then you you basically access these remote storage just like you usually do",
    "start": "987180",
    "end": "992879"
  },
  {
    "text": "so you give them like an HTTP endpoint or a HDFS URL so a spark itself knows",
    "start": "992879",
    "end": "1001939"
  },
  {
    "text": "how to you know access these files so",
    "start": "1001939",
    "end": "1007329"
  },
  {
    "text": "there's also some difference in terms of like you know so how remote application",
    "start": "1007329",
    "end": "1012589"
  },
  {
    "text": "dependencies get handled by the driver between spark 2.3 and spark 2.4 so in",
    "start": "1012589",
    "end": "1018829"
  },
  {
    "text": "spark 2 points 3 we actually used an in a container for downloading the remote",
    "start": "1018829",
    "end": "1024380"
  },
  {
    "text": "dependencies in the driver so what happens like you know so the summation client will ads and in a container into",
    "start": "1024380",
    "end": "1031250"
  },
  {
    "text": "the driver pod so these that in a container will be responsible for downloading these application",
    "start": "1031250",
    "end": "1037010"
  },
  {
    "text": "dependencies like jars or files onto a user specify location so once you know a",
    "start": "1037010",
    "end": "1042938"
  },
  {
    "text": "successfully downloads all these things it will as all the jars to the class pass so you can actually like it",
    "start": "1042939",
    "end": "1048740"
  },
  {
    "text": "naturally you can access the these jars in your spark program so in spark 2.4",
    "start": "1048740",
    "end": "1057350"
  },
  {
    "text": "actress since the release of spark 2.3 there are some debates over like net the use of in a container because well the",
    "start": "1057350",
    "end": "1063020"
  },
  {
    "text": "spark community was not very happy use of this inner container which asks",
    "start": "1063020",
    "end": "1068510"
  },
  {
    "text": "you know complexity also some you know more code to min 10 so then we decided",
    "start": "1068510",
    "end": "1075440"
  },
  {
    "text": "to get rid of the inner container so so the solution will be to actually run the",
    "start": "1075440",
    "end": "1081110"
  },
  {
    "text": "driver container so to run the submits sparks ma to command in climb mode in a",
    "start": "1081110",
    "end": "1087440"
  },
  {
    "text": "driver container so by doing that we can actually leverage the native support in",
    "start": "1087440",
    "end": "1092660"
  },
  {
    "text": "spark itself for downloading application dependencies because spark itself knows how to download dependencies before it",
    "start": "1092660",
    "end": "1098930"
  },
  {
    "text": "starts running your program so but with that change there's a very significant",
    "start": "1098930",
    "end": "1105140"
  },
  {
    "text": "change so first are no longer it downloaded to a user-specified location",
    "start": "1105140",
    "end": "1111710"
  },
  {
    "text": "but rather these files get downloaded into the local working directory and enter a randomly generated directory so",
    "start": "1111710",
    "end": "1120410"
  },
  {
    "text": "you can still access like you know things like jars natural light because these are these will be added to the class path but if you have some file you",
    "start": "1120410",
    "end": "1128780"
  },
  {
    "text": "want to access by a file path though so you have to use the spark files that get",
    "start": "1128780",
    "end": "1136130"
  },
  {
    "text": "to resolve these file paths basically so this basically gives the",
    "start": "1136130",
    "end": "1143510"
  },
  {
    "start": "1140000",
    "end": "1140000"
  },
  {
    "text": "list of existing feature as of spark 2.4 so we have you know we have the the",
    "start": "1143510",
    "end": "1149030"
  },
  {
    "text": "basic summation client and the native schedule back-end in English in a ripple",
    "start": "1149030",
    "end": "1156710"
  },
  {
    "text": "well so also supports these languages so you can actually use Java Scala Python or are it also supports running",
    "start": "1156710",
    "end": "1164570"
  },
  {
    "text": "sequel queries if for example you can actually set up the sequel for gotta",
    "start": "1164570",
    "end": "1172580"
  },
  {
    "text": "actor what's it what's it called but there's some kind of server you can actually run in spark then you can you can send sequel queries to so you can",
    "start": "1172580",
    "end": "1179960"
  },
  {
    "text": "also do that so currently I'm gonna support static resource allocation that means doctor you have to figure out like",
    "start": "1179960",
    "end": "1185360"
  },
  {
    "text": "you know how many executors you want and so you can't change the number of executors at runtime so it supports for",
    "start": "1185360",
    "end": "1193910"
  },
  {
    "text": "both container local and remote dependencies like what it what I just described",
    "start": "1193910",
    "end": "1198990"
  },
  {
    "text": "it also supports these basic you know pot customizations for example you can",
    "start": "1198990",
    "end": "1204179"
  },
  {
    "text": "add like you know you can specify hard to CPU and memory limits on the pot so",
    "start": "1204179",
    "end": "1209909"
  },
  {
    "text": "this is actually one thing you know the SPARC native in it coronated integration",
    "start": "1209909",
    "end": "1215130"
  },
  {
    "text": "it's different from the yarn or missiles integration is that you can actually use",
    "start": "1215130",
    "end": "1220649"
  },
  {
    "text": "fractional values for CPUs for both driver and executors for example you can actually use like a 0.5 or 100 meters if",
    "start": "1220649",
    "end": "1228059"
  },
  {
    "text": "use for your driver or executors which you cannot do in outer security beckons",
    "start": "1228059",
    "end": "1234299"
  },
  {
    "text": "so you can also say you know these standard things like label annotations",
    "start": "1234299",
    "end": "1239850"
  },
  {
    "text": "or environment variables for your pots you can also mount arbitrary kubernetes",
    "start": "1239850",
    "end": "1245610"
  },
  {
    "text": "secrets into into the pot so it also has limited support of these basic walden",
    "start": "1245610",
    "end": "1252539"
  },
  {
    "text": "types like empty their host pass or PVCs so you can also use image post secrets",
    "start": "1252539",
    "end": "1260190"
  },
  {
    "text": "to pull images from some private image registries yeah so it also comes to this",
    "start": "1260190",
    "end": "1266700"
  },
  {
    "text": "fool cluster and mobile support and also limited climb of support by that I will talk about that later so what I mean by",
    "start": "1266700",
    "end": "1273240"
  },
  {
    "text": "limited oh yeah so in SPARC 2.4 we added",
    "start": "1273240",
    "end": "1278309"
  },
  {
    "start": "1275000",
    "end": "1275000"
  },
  {
    "text": "the climber support this is actually one of the most requested features since the release of the SPARC 2.3 so with this",
    "start": "1278309",
    "end": "1286020"
  },
  {
    "text": "you can actually run interactive applications like notebooks like an attributer or jelly notebooks or spark",
    "start": "1286020",
    "end": "1292500"
  },
  {
    "text": "shell programs in with the Native communities integration so you can",
    "start": "1292500",
    "end": "1300179"
  },
  {
    "text": "actually run the driver both outside or inside the cluster but there's a tricky",
    "start": "1300179",
    "end": "1305580"
  },
  {
    "text": "part when you run the driver also at the cluster because users are responsible",
    "start": "1305580",
    "end": "1311279"
  },
  {
    "text": "for setting up the network connected connectivity from your executor pod running in the cluster to the driver pod",
    "start": "1311279",
    "end": "1317549"
  },
  {
    "text": "to the driver running outside the cluster so which is not always possible so you have to take that into",
    "start": "1317549",
    "end": "1323880"
  },
  {
    "text": "consideration so the recommended way is if you want to run like especially if",
    "start": "1323880",
    "end": "1329370"
  },
  {
    "text": "you want to run on notebook kind of applications you want to run these things in the class or so",
    "start": "1329370",
    "end": "1336600"
  },
  {
    "text": "the me structure you run your client in the English class or in a pod for example we actually run your",
    "start": "1336600",
    "end": "1342210"
  },
  {
    "text": "attributer notebook in a pod so then a networking connectivity part is much",
    "start": "1342210",
    "end": "1347489"
  },
  {
    "text": "easier to handle so what happens like you know so you can just create a",
    "start": "1347489",
    "end": "1352830"
  },
  {
    "text": "Heather service manually so this is because it's runs in a climb ode so the summation client won't be run at all so",
    "start": "1352830",
    "end": "1360570"
  },
  {
    "text": "what is the at least a part of that handles the creation of the driver part won't be run so you won't or so the",
    "start": "1360570",
    "end": "1368070"
  },
  {
    "text": "nabhi chakra there won't be an entity that will create the service for you so",
    "start": "1368070",
    "end": "1373379"
  },
  {
    "text": "you have to do that manually so your lay the process like you create a service first use you know some label selector",
    "start": "1373379",
    "end": "1383489"
  },
  {
    "text": "that you will later apply to the driver pod so then you start your trevor pod so then these the service you you create",
    "start": "1383489",
    "end": "1392850"
  },
  {
    "text": "previously will be bound to the driver pod so also if you run your driver pod",
    "start": "1392850",
    "end": "1399499"
  },
  {
    "text": "if you were drawing your driver in a client in a you know in it cluster you",
    "start": "1399499",
    "end": "1404700"
  },
  {
    "text": "can also leverage garbage collection so for example if you for whatever reason you delete your driver pod so all the",
    "start": "1404700",
    "end": "1411600"
  },
  {
    "text": "exterior paths created by the driver pot will be gone they due to a garbage collection so oh yeah so this is",
    "start": "1411600",
    "end": "1419399"
  },
  {
    "text": "basically like a no very wise how it works so you create a featherless service before you create your driver",
    "start": "1419399",
    "end": "1426840"
  },
  {
    "text": "pot so you know the server pockets a dns name from the service",
    "start": "1426840",
    "end": "1432809"
  },
  {
    "text": "so in Sparks 3.0 which will come next we",
    "start": "1432809",
    "end": "1438539"
  },
  {
    "start": "1433000",
    "end": "1433000"
  },
  {
    "text": "also added support for Kerberos authentication so you can actually use that for security fsx access I know like",
    "start": "1438539",
    "end": "1446309"
  },
  {
    "text": "you know most enterprises who run HDFS on Burrell actually will have",
    "start": "1446309",
    "end": "1451739"
  },
  {
    "text": "ostentation enabled for security purposes so this is also supported in",
    "start": "1451739",
    "end": "1457590"
  },
  {
    "text": "SPARC itself so to use this feature you have to give",
    "start": "1457590",
    "end": "1463320"
  },
  {
    "text": "the scheduler back and to sing one is the dedication token you use to access security FS that's the second is some",
    "start": "1463320",
    "end": "1471360"
  },
  {
    "text": "type of configuration that so the driver knows like you know how to talk to your",
    "start": "1471360",
    "end": "1478040"
  },
  {
    "text": "HDFS name note for our awesome vacation so when it comes to dedication token",
    "start": "1478040",
    "end": "1484620"
  },
  {
    "text": "actually there's two ways you can specify that one is you just tell the submission client hey this is actually",
    "start": "1484620",
    "end": "1490710"
  },
  {
    "text": "the Kerberos key type I want to use then the summation client will do it ossification for you and what generates",
    "start": "1490710",
    "end": "1497040"
  },
  {
    "text": "a secret that carries the dedication token so the months that into the part",
    "start": "1497040",
    "end": "1503490"
  },
  {
    "text": "or the second way it's like a now you can just use an existing if you already have a secret that Icarus your",
    "start": "1503490",
    "end": "1509040"
  },
  {
    "text": "dedication token you can just mount that into the past so when it comes to Hadoop configuration",
    "start": "1509040",
    "end": "1516300"
  },
  {
    "text": "again there's two ways the first is you just set the environment variable had to",
    "start": "1516300",
    "end": "1521850"
  },
  {
    "text": "come from there to your local Hadoop configuration directory so the summation",
    "start": "1521850",
    "end": "1527220"
  },
  {
    "text": "client will basically check the config in that directory and generates a config",
    "start": "1527220",
    "end": "1533520"
  },
  {
    "text": "map and mount that into your part well or you can also use your existing config",
    "start": "1533520",
    "end": "1540360"
  },
  {
    "text": "map that your to have some for example somehow the configuration unit so",
    "start": "1540360",
    "end": "1547610"
  },
  {
    "start": "1546000",
    "end": "1546000"
  },
  {
    "text": "additionally to the east like the Native communities integration name spark we",
    "start": "1547610",
    "end": "1553800"
  },
  {
    "text": "also create work on and creates this kubernetes operator for spark so this is",
    "start": "1553800",
    "end": "1560370"
  },
  {
    "text": "like you know so following the recent trend of doing extensions through CRT and custom controller so this is a seer",
    "start": "1560370",
    "end": "1567270"
  },
  {
    "text": "so it Hector has 230 types one for one shot up ignition and the other for cron",
    "start": "1567270",
    "end": "1572940"
  },
  {
    "text": "type applications and there's a custom controller that basically runs sparks I",
    "start": "1572940",
    "end": "1578340"
  },
  {
    "text": "made for you and once the application starts running or start monitoring these applications so it comes with so one",
    "start": "1578340",
    "end": "1586830"
  },
  {
    "text": "thing one good thing about this has actually support you know Paul customization all kinds of",
    "start": "1586830",
    "end": "1594250"
  },
  {
    "text": "customisations through a mutating admission webhook so that means dr. it",
    "start": "1594250",
    "end": "1600200"
  },
  {
    "text": "can support scenes that are not even supported by the native integration itself so it also comes with native cron",
    "start": "1600200",
    "end": "1607820"
  },
  {
    "text": "support for running schedule applications so it also does automatic setup for exporting like your",
    "start": "1607820",
    "end": "1615740"
  },
  {
    "text": "application level and Driver exceeder metrics to a Prometheus and also as",
    "start": "1615740",
    "end": "1621409"
  },
  {
    "text": "supports in college installation with helm and well get are seeing that comes",
    "start": "1621409",
    "end": "1626539"
  },
  {
    "text": "wizard is kamala to cost part control so you can actually use that to stage your",
    "start": "1626539",
    "end": "1632480"
  },
  {
    "text": "local dependency for example if you have some files on your local machine or jars on your local machine if you use part",
    "start": "1632480",
    "end": "1639230"
  },
  {
    "text": "control it automatically is stages these up dependencies on to a remote location",
    "start": "1639230",
    "end": "1646279"
  },
  {
    "text": "you specify for example you can actually automatically upload these dependency to an s3 bucket then you so and the smart",
    "start": "1646279",
    "end": "1655549"
  },
  {
    "text": "control command like to also automatically changes your dependency from your local ones to the ones on the",
    "start": "1655549",
    "end": "1661520"
  },
  {
    "text": "remote location so you don't need to handle all these things manually so all these are taken care for you yeah so",
    "start": "1661520",
    "end": "1669320"
  },
  {
    "text": "this is one of the example like you know so you can you can get a sense on how a",
    "start": "1669320",
    "end": "1674860"
  },
  {
    "text": "CR our customer resource for a spark application looks like so you can basically specify how the driver looks",
    "start": "1674860",
    "end": "1681649"
  },
  {
    "text": "like how director looks like and also he can enable monitoring uh or integration",
    "start": "1681649",
    "end": "1686659"
  },
  {
    "text": "with prometheus it seems like that okay so talking about road maps this is what",
    "start": "1686659",
    "end": "1695480"
  },
  {
    "start": "1691000",
    "end": "1691000"
  },
  {
    "text": "you would expect from the community you know moving forward that you probably",
    "start": "1695480",
    "end": "1701210"
  },
  {
    "text": "was seeing sparks 3.0 and later so one thing is to solve the problem so basic",
    "start": "1701210",
    "end": "1709370"
  },
  {
    "text": "all the pot customizations you just saw are danced through new spark config options but what the problem is like no",
    "start": "1709370",
    "end": "1717500"
  },
  {
    "text": "so because there are so many different things you can customize with smart power with community as part well so if",
    "start": "1717500",
    "end": "1723889"
  },
  {
    "text": "we keep doing that whoa we'll end up with so many different new options that's not something we want to",
    "start": "1723889",
    "end": "1729360"
  },
  {
    "text": "do because we have to maintain these spark off options moving forward so we decided to adopt new top a new approach",
    "start": "1729360",
    "end": "1737850"
  },
  {
    "text": "so we basically allow you to specify a plot template for poster driver and",
    "start": "1737850",
    "end": "1743250"
  },
  {
    "text": "exterior parts now you can do what kind of a custom customization or configuration you want so for example",
    "start": "1743250",
    "end": "1749040"
  },
  {
    "text": "you can actually specify pod affinity affinity or anti affinity or you can",
    "start": "1749040",
    "end": "1754530"
  },
  {
    "text": "like you know use tolerant aberrations or or like you know adder things like",
    "start": "1754530",
    "end": "1759840"
  },
  {
    "text": "security context so then you just give the summation client these templates so",
    "start": "1759840",
    "end": "1765420"
  },
  {
    "text": "the summation client will use these template to create the final driver and exterior parts back so the another thing",
    "start": "1765420",
    "end": "1774420"
  },
  {
    "text": "which is often requested is the support for dynamic resource allocation that",
    "start": "1774420",
    "end": "1779550"
  },
  {
    "text": "means actually we support changing the number of executors based on the word on",
    "start": "1779550",
    "end": "1785100"
  },
  {
    "text": "the load of the application at runtime so this were Act requires a new shuffle",
    "start": "1785100",
    "end": "1791340"
  },
  {
    "text": "service that can work natively with Sparta with communities we actually did",
    "start": "1791340",
    "end": "1797160"
  },
  {
    "text": "this in our fork but the way it was designed has some problems so we decided",
    "start": "1797160",
    "end": "1802740"
  },
  {
    "text": "to not upstream that into the main smart repo so we are currently working on a",
    "start": "1802740",
    "end": "1809280"
  },
  {
    "text": "new shuffle design that well features something we call these aggregated",
    "start": "1809280",
    "end": "1814500"
  },
  {
    "text": "storage that means actually use separate storage out from your compute so we also",
    "start": "1814500",
    "end": "1820020"
  },
  {
    "text": "want to work on better support for local dependency like what I just mentioned like you know so we currently only support two ways for using dependency",
    "start": "1820020",
    "end": "1826950"
  },
  {
    "text": "one is you bake did that mean to your custom images or you you know upload them to whatever",
    "start": "1826950",
    "end": "1834809"
  },
  {
    "text": "remote storage you want to use so another thing is we want to investigate",
    "start": "1834809",
    "end": "1840780"
  },
  {
    "text": "better scheduling support one of the issue was that the way the kubernetes",
    "start": "1840780",
    "end": "1846000"
  },
  {
    "text": "schedule works is you might have the resource to just run the driver pub but you don't have resource to run an aide",
    "start": "1846000",
    "end": "1852240"
  },
  {
    "text": "any of the executor parts so in our case your application basically haunts because the driver part is not doing it",
    "start": "1852240",
    "end": "1858780"
  },
  {
    "text": "won't do anything it was states stay there on tutors resources to run at least you",
    "start": "1858780",
    "end": "1864160"
  },
  {
    "text": "know one Exeter power to run your tasks so because I don't why I see the driver power just stay there forever so this is",
    "start": "1864160",
    "end": "1870010"
  },
  {
    "text": "something want to work on well this is particularly an issue for small class or",
    "start": "1870010",
    "end": "1875710"
  },
  {
    "text": "like you know you have a class or with only three or five notes you have very limited amount of resources in that case",
    "start": "1875710",
    "end": "1882760"
  },
  {
    "text": "if the driver is started but there's no resource for any of the executors pause then you you're in a very bad position",
    "start": "1882760",
    "end": "1890910"
  },
  {
    "text": "so if you want to get involved nurse doctor a couple like a couple ways so",
    "start": "1890910",
    "end": "1896080"
  },
  {
    "start": "1891000",
    "end": "1891000"
  },
  {
    "text": "the Dakotas enter the directory a resource manager slash Community Center you know the spark called ripple so the",
    "start": "1896080",
    "end": "1904780"
  },
  {
    "text": "documentation you can actually find the kubernetes specific documentation on the facial spark doc site so we also have",
    "start": "1904780",
    "end": "1913060"
  },
  {
    "text": "you know the spark user and that main enlist you can actually post questions or feature requests to to for things",
    "start": "1913060",
    "end": "1919600"
  },
  {
    "text": "like you know if you run into any of the issues or you want to have a new feature you can actually go to create JIRA",
    "start": "1919600",
    "end": "1927130"
  },
  {
    "text": "tickets so we will you know frequently check and take care of so we also have a",
    "start": "1927130",
    "end": "1934860"
  },
  {
    "text": "stock channel for seek big data that we we actually do it by a vacant meeting to",
    "start": "1934860",
    "end": "1941740"
  },
  {
    "text": "talk about like you know all different kind of big data we're closer on kubernetes like a no spark or flank or",
    "start": "1941740",
    "end": "1948600"
  },
  {
    "text": "outer HDFS for example okay so that's",
    "start": "1948600",
    "end": "1954310"
  },
  {
    "text": "all about my presentation so any questions",
    "start": "1954310",
    "end": "1959550"
  },
  {
    "text": "sorry just a quick question because before what we at least I will be taught is a busy Hadoop when they trying to",
    "start": "1965540",
    "end": "1972900"
  },
  {
    "text": "distribute the workload they were distribute to they will give you the workload to the closest the data where",
    "start": "1972900",
    "end": "1979500"
  },
  {
    "text": "the data resides but so far like if you know you have an interface between that when you assign",
    "start": "1979500",
    "end": "1985110"
  },
  {
    "text": "the part where you also you know inherit that kind of capability so yeah that's a",
    "start": "1985110",
    "end": "1992040"
  },
  {
    "text": "very good question actually so we did this in the fork but again so we saw the",
    "start": "1992040",
    "end": "1998760"
  },
  {
    "text": "design wasn't really very good so when we upstream that working to into the",
    "start": "1998760",
    "end": "2004310"
  },
  {
    "text": "sparkman ripple we decided to not assume that part we actually had support like you know so data locality supporting the",
    "start": "2004310",
    "end": "2011120"
  },
  {
    "text": "fork but it's not in the spark men ripple yet so that still needs to be",
    "start": "2011120",
    "end": "2016400"
  },
  {
    "text": "worked on yeah so right now if you access for example if you have an HDFS class or deployed on",
    "start": "2016400",
    "end": "2021920"
  },
  {
    "text": "kubernetes and you access that from and a spark job you won't you pretty much won't get any",
    "start": "2021920",
    "end": "2029090"
  },
  {
    "text": "data locality yeah",
    "start": "2029090",
    "end": "2033070"
  },
  {
    "text": "I have a quick question on the mapping between spark resources for example the",
    "start": "2039470",
    "end": "2046179"
  },
  {
    "text": "mapping between pop node to pas to executors and do you have any",
    "start": "2046179",
    "end": "2051648"
  },
  {
    "text": "suggestions from your experience are you",
    "start": "2051649",
    "end": "2059690"
  },
  {
    "text": "talking about mapping from pasta nodes or IC got it",
    "start": "2059690",
    "end": "2072230"
  },
  {
    "text": "so the question is about like when you want to like know so placement of these",
    "start": "2072230",
    "end": "2079069"
  },
  {
    "text": "driver or executor pass on to notes given the limited amount of resources on the node right so that's a wonderful",
    "start": "2079069",
    "end": "2088010"
  },
  {
    "text": "reason like the main reason why we wanted to so we actually saw in spark just two points three we didn't allow",
    "start": "2088010",
    "end": "2094419"
  },
  {
    "text": "fractional CPU adders for the exterior part so you have to use a minimum lei",
    "start": "2094419",
    "end": "2100040"
  },
  {
    "text": "once if you for your executor power then we discover like those so for small clusters there might not even be",
    "start": "2100040",
    "end": "2106640"
  },
  {
    "text": "resources to run your any of the extra parts so then we actually added support",
    "start": "2106640",
    "end": "2112069"
  },
  {
    "text": "for using fractional values for for CPU request and limits so you can like you",
    "start": "2112069",
    "end": "2117980"
  },
  {
    "text": "know maybe beam packing like in a more Exeter pass onto the same note oh this is what super help of for small clusters",
    "start": "2117980",
    "end": "2125660"
  },
  {
    "text": "so my suggestion would be to like you have to do benchmarking for your",
    "start": "2125660",
    "end": "2131630"
  },
  {
    "text": "application and see and you know given the limited CPU and memory requests for",
    "start": "2131630",
    "end": "2137119"
  },
  {
    "text": "the container whether you achieve you know good enough performance if it's not then you have to figure out like you",
    "start": "2137119",
    "end": "2142400"
  },
  {
    "text": "know so what would be well especially if you have a very limited amount of memory available you might run into GCC issues",
    "start": "2142400",
    "end": "2148670"
  },
  {
    "text": "if you have you know production like especially for production jobs you might",
    "start": "2148670",
    "end": "2154099"
  },
  {
    "text": "so your job might spend most of time like in between GC that's something you don't want to have a good one a wide so",
    "start": "2154099",
    "end": "2160430"
  },
  {
    "text": "yeah so well for especially like what I mentioned for small clusters you want to",
    "start": "2160430",
    "end": "2165710"
  },
  {
    "text": "benchmark your application see you know what's the best resource requests you want to give to",
    "start": "2165710",
    "end": "2173930"
  },
  {
    "text": "your container I have two questions why",
    "start": "2173930",
    "end": "2183650"
  },
  {
    "text": "is that can you explain a little bit more about your load map said here the last one is",
    "start": "2183650",
    "end": "2190599"
  },
  {
    "text": "the schedule enhancement in three three three three right so yeah can you",
    "start": "2190599",
    "end": "2198430"
  },
  {
    "text": "explain with what which topic which kind of forecast will be the enhancement for",
    "start": "2198430",
    "end": "2205640"
  },
  {
    "text": "scattering yeah another one is that I the spark job may be in my opinion",
    "start": "2205640",
    "end": "2213410"
  },
  {
    "text": "either way it is mostly that is batch job yeah so it may be Scipio heavy CP or",
    "start": "2213410",
    "end": "2223190"
  },
  {
    "text": "hey I oh yeah so especially for I all that kind of seeing that either will",
    "start": "2223190",
    "end": "2230019"
  },
  {
    "text": "affect the other jobs other containers running on the same node so what kind of",
    "start": "2230019",
    "end": "2236559"
  },
  {
    "text": "job have you already done here now on this project",
    "start": "2236559",
    "end": "2241730"
  },
  {
    "text": "yes thank you so for your first question",
    "start": "2241730",
    "end": "2247599"
  },
  {
    "text": "actually we we are we were talking to a project called cool be back I don't know",
    "start": "2247599",
    "end": "2253400"
  },
  {
    "text": "if you have heard of that or not it's actually a project undercover daily seek",
    "start": "2253400",
    "end": "2258559"
  },
  {
    "text": "scheduling so the goal of that is actually to support gun scheduling so",
    "start": "2258559",
    "end": "2263869"
  },
  {
    "text": "like you know so it's schedule a set of pot or at once were known so we talked",
    "start": "2263869",
    "end": "2271309"
  },
  {
    "text": "to them but it turns out like Eric didn't really work for a for our use cases because so what we really want is",
    "start": "2271309",
    "end": "2279890"
  },
  {
    "text": "there is a current II that you can actually schedule one driver pod and at least one Exeter pot so that's the",
    "start": "2279890",
    "end": "2285980"
  },
  {
    "text": "minimum requirement we have but what is the thing is like the Trevor pod and",
    "start": "2285980",
    "end": "2291079"
  },
  {
    "text": "extra pot they don't run they don't start at the same time so you can't roll a schedule them as a batch so you have",
    "start": "2291079",
    "end": "2297349"
  },
  {
    "text": "to start the Trevor Plouffe first and then the driver pod will you know create the exit so yeah well I guess what was still talk",
    "start": "2297349",
    "end": "2306160"
  },
  {
    "text": "was done like no see if there's any chance they can a support for this kind of I use cases I don't think this is",
    "start": "2306160",
    "end": "2311980"
  },
  {
    "text": "unique to Sports Park so for order like you know some master/slave kind of work clothes there might be so that the same",
    "start": "2311980",
    "end": "2319030"
  },
  {
    "text": "problem might apply to these use cases as well so we wanted to find out a",
    "start": "2319030",
    "end": "2324100"
  },
  {
    "text": "solution that can work for all these kind of use cases so that's the first question so the second question I think if you",
    "start": "2324100",
    "end": "2333730"
  },
  {
    "text": "want to achieve certain kind of isolation between jobs especially if you",
    "start": "2333730",
    "end": "2339460"
  },
  {
    "text": "have like for example you have to i/o intensive application running at the same time you don't want to schedule",
    "start": "2339460",
    "end": "2345610"
  },
  {
    "text": "them onto the same note because actually I'll go up become the bottleneck so in that case you can actually use pod",
    "start": "2345610",
    "end": "2352140"
  },
  {
    "text": "affinity or anti affinity to you know make sure that these job don't be a",
    "start": "2352140",
    "end": "2358090"
  },
  {
    "text": "one-piece scheduled onto the same set of nodes so to to do that actually",
    "start": "2358090",
    "end": "2364240"
  },
  {
    "text": "the singer just mentioned so in Sparks 3.0 we support using pala templates you",
    "start": "2364240",
    "end": "2369550"
  },
  {
    "text": "can actually specify a Napolitan place your part infinity or anti infinity to",
    "start": "2369550",
    "end": "2375690"
  },
  {
    "text": "separate you know pause from from the same jobs from different jobs out onto",
    "start": "2375690",
    "end": "2381610"
  },
  {
    "text": "you know to different set of nodes yeah so that's so if I would do that and that's that's the way I would go yeah",
    "start": "2381610",
    "end": "2389730"
  },
  {
    "text": "any other questions cool yes thanks",
    "start": "2390790",
    "end": "2397860"
  },
  {
    "text": "[Applause]",
    "start": "2398470",
    "end": "2402270"
  }
]