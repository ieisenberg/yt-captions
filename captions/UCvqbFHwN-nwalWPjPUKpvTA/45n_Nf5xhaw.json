[
  {
    "text": "all right cool um so what is Prometheus Prometheus is a",
    "start": "1520",
    "end": "7620"
  },
  {
    "text": "metrics-based monitoring and alerting stack uh when I say stack it's end to end as",
    "start": "7620",
    "end": "14160"
  },
  {
    "text": "in you get the instrumentation libraries to instrument your applications and also exporters to get data from other systems",
    "start": "14160",
    "end": "21900"
  },
  {
    "text": "that are not instrumented with Prometheus it also has the scraping which means the collection and the",
    "start": "21900",
    "end": "28439"
  },
  {
    "text": "storage layer and it also has a querying alerting dashboarding layer which is the UI and the way you interface with all",
    "start": "28439",
    "end": "35640"
  },
  {
    "text": "the data that you collect and this is for all levels of the stack as in you can monitor systems applications to your",
    "start": "35640",
    "end": "41700"
  },
  {
    "text": "hobby projects to weather to anything it's very generic what differentiates it from the older monitoring systems is",
    "start": "41700",
    "end": "49079"
  },
  {
    "text": "that it's made for highly Dynamic Cloud environments as in when for example communities where pods come and go",
    "start": "49079",
    "end": "55340"
  },
  {
    "text": "Prometheus is really really good for environments like that where you don't have static hosts but have pods that",
    "start": "55340",
    "end": "61260"
  },
  {
    "text": "come and go and change over time cool um Prometheus does not do any logging or",
    "start": "61260",
    "end": "68520"
  },
  {
    "text": "tracing it's very focused on metrics it does that one thing and does it really really well",
    "start": "68520",
    "end": "73979"
  },
  {
    "text": "um it doesn't do any automatic anomaly detection you have to write the alerts yourself we have a really powerful query",
    "start": "73979",
    "end": "80580"
  },
  {
    "text": "language as you'll see um but you have to write the alert queries yourself you do not going to suggest any anomaly detection or",
    "start": "80580",
    "end": "87840"
  },
  {
    "text": "anything it doesn't have scalable or durable storage so what does that mean so essentially",
    "start": "87840",
    "end": "94080"
  },
  {
    "text": "with Prometheus you can comfortably store up to a few terabytes of data as and you can only scale to a single node",
    "start": "94080",
    "end": "100320"
  },
  {
    "text": "and you can store uh you the typical attention is two weeks but you can also store a few months of data but typically",
    "start": "100320",
    "end": "106619"
  },
  {
    "text": "this is not for storing years and years of data or like scaling Beyond a single node",
    "start": "106619",
    "end": "111780"
  },
  {
    "text": "um yeah so it started off in 2012 at SoundCloud they were it was a bunch of",
    "start": "111780",
    "end": "118920"
  },
  {
    "text": "ex-googlers who moved to SoundCloud and they really missed the monitoring system that they were used to so they started working on Prometheus they started",
    "start": "118920",
    "end": "126299"
  },
  {
    "text": "adopting it internally at SoundCloud at scale and they finally published it end of 2015 I think",
    "start": "126299",
    "end": "131879"
  },
  {
    "text": "a joint cncf and v1.0 was released in 2016 v2.0 was released in 2017. you",
    "start": "131879",
    "end": "138239"
  },
  {
    "text": "might expect we three.org to come but we are at B 2.40 right now uh or something",
    "start": "138239",
    "end": "143879"
  },
  {
    "text": "so essentially we're still on v2.0 it works really really well um and yeah it was a complete rewrite of",
    "start": "143879",
    "end": "149459"
  },
  {
    "text": "the storage engine and it's kind of future proof so coming to the architecture so you",
    "start": "149459",
    "end": "155099"
  },
  {
    "text": "have your applications that you want to monitor that you want to collect uh Matrix from the way you do this is you",
    "start": "155099",
    "end": "160920"
  },
  {
    "text": "instrument your applications with Prometheus or Prometheus compatible client libraries and these client libraries expose the HTTP server so",
    "start": "160920",
    "end": "168599"
  },
  {
    "text": "essentially you need to expose your metrics on a HTTP server you do not push your Matrix to a central system you just",
    "start": "168599",
    "end": "173940"
  },
  {
    "text": "say oh if you hit this API endpoint which is slash Matrix typically you get",
    "start": "173940",
    "end": "179099"
  },
  {
    "text": "all the list of metrics and this this uh endpoint is big is",
    "start": "179099",
    "end": "184379"
  },
  {
    "text": "being hit regularly by Prometheus and that's how Prometheus collects the data now if you want to monitor things like",
    "start": "184379",
    "end": "190739"
  },
  {
    "text": "the Linux system or MySQL or other systems that are not instrumented with the Prometheus client library that are",
    "start": "190739",
    "end": "197760"
  },
  {
    "text": "not exposing this endpoint but they have their own way of exposing analytics for example with mySQL you cannot run a",
    "start": "197760",
    "end": "204360"
  },
  {
    "text": "bunch of queries and you can get the analytical data or you can get the metrics out of the number of connections the size of the DB and things like that",
    "start": "204360",
    "end": "211019"
  },
  {
    "text": "or for Linux you can basically scrape proc the proc file system to get the data out so we have exporters which you",
    "start": "211019",
    "end": "218340"
  },
  {
    "text": "run alongside these systems what these exporters do is they just basically go talk to the service in its own native",
    "start": "218340",
    "end": "224640"
  },
  {
    "text": "language uh in its own native format and take that Matrix data and convert it to Prometheus so it also exposes a HTTP",
    "start": "224640",
    "end": "231720"
  },
  {
    "text": "server the talks Prometheus now we have Prometheus which just basically goes to all of these services",
    "start": "231720",
    "end": "237599"
  },
  {
    "text": "and collects the data and stores it in the tstv which is the local storage engine of Prometheus",
    "start": "237599",
    "end": "244680"
  },
  {
    "text": "now Prometheus needs to discover what's what services exist and that's where",
    "start": "244680",
    "end": "250019"
  },
  {
    "text": "service Discovery comes into picture and this is something really really powerful because suppose you're running a",
    "start": "250019",
    "end": "256440"
  },
  {
    "text": "collection of services maybe say I wanna you say I want 45 of my services to run you have Auto scaling up that goes to 47",
    "start": "256440",
    "end": "263160"
  },
  {
    "text": "or 43 you have you have a dynamic number of services running and now you're monitoring them with a push-based system",
    "start": "263160",
    "end": "269340"
  },
  {
    "text": "so you don't know when only 41 of them are running it's really hard to detect of one or two services are",
    "start": "269340",
    "end": "274800"
  },
  {
    "text": "malfunctioning but with Prometheus it talks to the kubernetes API or it talks to the service Discovery provider it",
    "start": "274800",
    "end": "280440"
  },
  {
    "text": "knows oh there is 45 Services running and here's the eyepiece of all of these I'm gonna go and try to get metrics from",
    "start": "280440",
    "end": "286620"
  },
  {
    "text": "them if someone one of them is down we will know immediately because Prometheus will fail and it will have a metric that",
    "start": "286620",
    "end": "292139"
  },
  {
    "text": "says this service is down and we can we can alert on it so that's one of the advantages of the push the pull-based",
    "start": "292139",
    "end": "297660"
  },
  {
    "text": "system over push so that's the service Discovery part of Prometheus and then we have the uh UI",
    "start": "297660",
    "end": "305280"
  },
  {
    "text": "side of things so we have a web UI which is really powerful I still regularly use it even though I work for dafana",
    "start": "305280",
    "end": "312360"
  },
  {
    "text": "um and it works really well for debugging purposes and for dashboarding and for you to look at regular",
    "start": "312360",
    "end": "318120"
  },
  {
    "text": "dashboards we recommend using grafana there's also a lot of people who wrote a lot of automation with our API where you",
    "start": "318120",
    "end": "324180"
  },
  {
    "text": "just call Prometheus with fromql and it gets it gives you all the data so there's this uh you can say the data",
    "start": "324180",
    "end": "331380"
  },
  {
    "text": "usage side of things and also we have alerting so Prometheus has so you can write",
    "start": "331380",
    "end": "337740"
  },
  {
    "text": "alerts in Prometheus Prometheus will evaluate these alerts against the data in the Prometheus and then it sends it",
    "start": "337740",
    "end": "343560"
  },
  {
    "text": "to an alert manager which basically detoes and routes alerts so you can say oh for this namespace send it to this",
    "start": "343560",
    "end": "350100"
  },
  {
    "text": "slack Channel or for this uh for for a critical alert send it to pager duty all alerts into slack Channel you can do",
    "start": "350100",
    "end": "356160"
  },
  {
    "text": "this routing that's the alert Financial component one thing I do want to mention is the robust form of alerting that",
    "start": "356160",
    "end": "362220"
  },
  {
    "text": "happens with Prometheus because the very little amount of dependencies it has network is very prone to break and if",
    "start": "362220",
    "end": "368639"
  },
  {
    "text": "you put your alerting on a system on a distributed system or on a system that relies on networking it's less robust",
    "start": "368639",
    "end": "375120"
  },
  {
    "text": "than putting it in Prometheus and it's as close to data as possible that's really cool one really cool thing about Prometheus",
    "start": "375120",
    "end": "382560"
  },
  {
    "text": "yeah the selling points of Prometheus are its dimensional data models so when the data model came out uh it kind of",
    "start": "382560",
    "end": "389160"
  },
  {
    "text": "literally changed the world uh we will talk about that we have a really nice and Powerful query language built for",
    "start": "389160",
    "end": "395100"
  },
  {
    "text": "Prometheus and the Prometheus style data we have a simple and efficient server even though it's a single node server",
    "start": "395100",
    "end": "400680"
  },
  {
    "text": "that doesn't scale typically it really scales vertically on a single node and also our very powerful service Discovery",
    "start": "400680",
    "end": "407520"
  },
  {
    "text": "mechanisms that we are constantly adding new service Discovery mechanisms to and improving",
    "start": "407520",
    "end": "413039"
  },
  {
    "text": "so talking about the data model what is the time series in my mind time series is an identifier for a list of timestamp",
    "start": "413039",
    "end": "420419"
  },
  {
    "text": "values uh for example the stock price of a particular stock that's a Time series",
    "start": "420419",
    "end": "426360"
  },
  {
    "text": "because it's a timestamp there's a value the weather or temperature in a",
    "start": "426360",
    "end": "432240"
  },
  {
    "text": "particular City there's a lot of different cities for each City there's a weather dot I mean weather for Detroit",
    "start": "432240",
    "end": "437580"
  },
  {
    "text": "weather for Chicago and stuff like that that's a Time series but how do we identify this each time series that's",
    "start": "437580",
    "end": "444060"
  },
  {
    "text": "where Prometheus shines and it is it has this label based time series for example",
    "start": "444060",
    "end": "450419"
  },
  {
    "text": "HTTP request total with the labels that describe that a particular thing so it's",
    "start": "450419",
    "end": "455699"
  },
  {
    "text": "flexible it doesn't have hierarchy there's no Dots here it's not HTTP request total dot nginx dot 200 or",
    "start": "455699",
    "end": "462000"
  },
  {
    "text": "anything so there's no Dots here you can add and remove labels without your queries and dashboards breaking",
    "start": "462000",
    "end": "468840"
  },
  {
    "text": "uh which is really powerful and like in the previous model if you're used to graphite or stats T you can you have",
    "start": "468840",
    "end": "475319"
  },
  {
    "text": "like HTTP request total dot nginx dot 200 or something and you have to guess what the 200 is you have to guess what",
    "start": "475319",
    "end": "481500"
  },
  {
    "text": "engine exists but here it's a very explicit watch each of those labels mean and that's really powerful",
    "start": "481500",
    "end": "488280"
  },
  {
    "text": "for the querying we have our own language called promcule it's a functional query language it's built for",
    "start": "488280",
    "end": "494940"
  },
  {
    "text": "Prometheus and Prometheus style data it's not SQL which I think is a huge benefit",
    "start": "494940",
    "end": "500099"
  },
  {
    "text": "um it's it's very easy to reason with uh and once you get used to it you cannot",
    "start": "500099",
    "end": "505440"
  },
  {
    "text": "literally you can you cannot live without it and one really cool thing I want to mention is today if you are a",
    "start": "505440",
    "end": "511020"
  },
  {
    "text": "Google Cloud customer you can go to stackdriver and before you used to have this clunky UI where you have to click a",
    "start": "511020",
    "end": "516240"
  },
  {
    "text": "lot of things to understand but you can now use promptql to query that data it's really nice and really powerful and more",
    "start": "516240",
    "end": "521339"
  },
  {
    "text": "and more vendors are adding support for it um yeah for example let's see how selecting",
    "start": "521339",
    "end": "528480"
  },
  {
    "text": "all the partitions uh that are more than 100 GB capacity that are not mounted on root looks like so first we have the",
    "start": "528480",
    "end": "535500"
  },
  {
    "text": "node file system bytes total which is symmetric so if you select this this is going to give you ideally the amount of",
    "start": "535500",
    "end": "542339"
  },
  {
    "text": "disk that the file system is using and when I say not root you can say Mount Point not equal slash if you want it on",
    "start": "542339",
    "end": "549899"
  },
  {
    "text": "a particular route you just say Mount Point equals that particular path and you're going to get the data for that particular path",
    "start": "549899",
    "end": "555959"
  },
  {
    "text": "now this is in bytes and we have to convert it to GB so 1. E9 1 divided by",
    "start": "555959",
    "end": "561420"
  },
  {
    "text": "one E9 and that's greater than 100 so it's greater than 100 GB and it's going to give you a list of all the time",
    "start": "561420",
    "end": "567600"
  },
  {
    "text": "series or metrics that match that condition and here you can see okay uh on this particular instance this",
    "start": "567600",
    "end": "573779"
  },
  {
    "text": "particular Mount point is taking more than 100 GB Sigma oh 118 GB and things like that it's kind of very easy to",
    "start": "573779",
    "end": "579899"
  },
  {
    "text": "reason with uh at least the easier queries so you can write really ugly promcule but it's better than other",
    "start": "579899",
    "end": "586800"
  },
  {
    "text": "languages I think um yeah and here's what is the ratio of",
    "start": "586800",
    "end": "591959"
  },
  {
    "text": "requests across all my service instances so you can say um ratio of Errors so you can just say",
    "start": "591959",
    "end": "598380"
  },
  {
    "text": "hey give me the list of uh I mean give me the rate of errors and give divided by the total it will give me the ratio",
    "start": "598380",
    "end": "604260"
  },
  {
    "text": "of error so here it's like 2.9 percent of my requests are errors and then you",
    "start": "604260",
    "end": "609779"
  },
  {
    "text": "can also add custom groupings to it for example give it to be by path uh you just say sum by path and you can Group",
    "start": "609779",
    "end": "616620"
  },
  {
    "text": "by path and divide by path and you can see that okay here's my worst performing",
    "start": "616620",
    "end": "622080"
  },
  {
    "text": "endpoint which is basically about nine percent of my requests are errors on this topics endpoint so you can write a",
    "start": "622080",
    "end": "628860"
  },
  {
    "text": "query that says if my error threshold is more than this set me an alert you can get that query",
    "start": "628860",
    "end": "633899"
  },
  {
    "text": "your alertable page you you can get that query and then you can dive into it you can add Dimensions slice by Dimensions",
    "start": "633899",
    "end": "639300"
  },
  {
    "text": "to understand exactly what is causing that and alerting is very similar so",
    "start": "639300",
    "end": "645660"
  },
  {
    "text": "basically you write an expression that matches a list of Time series and whenever there's a there's a Time series there that alert condition is matched",
    "start": "645660",
    "end": "652440"
  },
  {
    "text": "you can send yourself an alert for example this if you say more than if I'm seeing more than five percent of Errors",
    "start": "652440",
    "end": "659339"
  },
  {
    "text": "send me an alert but one really cool thing is the four period here four or five minutes so you can have a",
    "start": "659339",
    "end": "666420"
  },
  {
    "text": "some weird freak networking incident where you know you just have like a blip of errors and then it's back to normal",
    "start": "666420",
    "end": "671640"
  },
  {
    "text": "it's fine it's just a small tiny blip of Errors you don't want to be woken up in the middle of the night for that you",
    "start": "671640",
    "end": "677279"
  },
  {
    "text": "will want to be ideally woken up if this is continuing regularly so you can configure a four period and only if this",
    "start": "677279",
    "end": "683399"
  },
  {
    "text": "condition is true for a particular amount of time you get paced and that's that's also really powerful it reduces",
    "start": "683399",
    "end": "689220"
  },
  {
    "text": "the number of pages that you get um and then you can add labels and then you can route by labels and you can do a",
    "start": "689220",
    "end": "695040"
  },
  {
    "text": "lot of really cool things for alerting yeah this is something uh that I'm also",
    "start": "695040",
    "end": "701760"
  },
  {
    "text": "really proud of it's very efficient and even though it's a single node system that's not distributed it a single node",
    "start": "701760",
    "end": "708540"
  },
  {
    "text": "works for a lot of the companies it can scale to 1 million samples a second with tens of millions of active",
    "start": "708540",
    "end": "715500"
  },
  {
    "text": "series uh and this is more than enough for most organizations uh you don't need",
    "start": "715500",
    "end": "720720"
  },
  {
    "text": "a distributed storage you don't need a complicated uh monitoring system to run you can just run Prometheus which is",
    "start": "720720",
    "end": "726959"
  },
  {
    "text": "single node extremely robust and focus on your applications not your monitoring that's really powerful and it's really",
    "start": "726959",
    "end": "733380"
  },
  {
    "text": "simple really simple to reason with and really simple to operate we also use the gorilla compression on our data which",
    "start": "733380",
    "end": "739920"
  },
  {
    "text": "means a 16 uh byte like sample which is a time stamp",
    "start": "739920",
    "end": "746579"
  },
  {
    "text": "and value compresses down to just one to two bytes like 1.6 bytes a sample on",
    "start": "746579",
    "end": "751860"
  },
  {
    "text": "average or 1.2 bytes of sample on average which means even with tens of millions of active series on a single",
    "start": "751860",
    "end": "758519"
  },
  {
    "text": "node you can keep like two to three weeks of data or even months of data without",
    "start": "758519",
    "end": "763680"
  },
  {
    "text": "having to resort to a distributed or network storage uh and that's really powerful",
    "start": "763680",
    "end": "769740"
  },
  {
    "text": "um some people keep years of data on it but they have you have to take regular backups and stuff but it is possible",
    "start": "769740",
    "end": "775560"
  },
  {
    "text": "there's nothing inherently broken in Prometheus that you cannot store more than this it's just the limitation of",
    "start": "775560",
    "end": "781380"
  },
  {
    "text": "disk space that is basically stopping you um yeah so one thing I want to highlight",
    "start": "781380",
    "end": "788220"
  },
  {
    "text": "is the exporter ecosystem again Prometheus is a pull-based system it",
    "start": "788220",
    "end": "793260"
  },
  {
    "text": "needs to go go and scrape metrics from data from things and these things might not be taught in the Prometheus protocol",
    "start": "793260",
    "end": "799760"
  },
  {
    "text": "and we have a huge ecosystem of exporters when I say huge I mean massive",
    "start": "799760",
    "end": "806339"
  },
  {
    "text": "like every time somebody is like how do I monitor this with Prometheus the first thing that Google is uh the thing",
    "start": "806339",
    "end": "811740"
  },
  {
    "text": "Prometheus exporter and I will always almost always find one including uh speedtest.net exporter or like a router",
    "start": "811740",
    "end": "819480"
  },
  {
    "text": "exporter for like Fritz box uh back in Germany uh you can you can export that that talks to that API converts things",
    "start": "819480",
    "end": "825959"
  },
  {
    "text": "to Prometheus and gives me data on my on how my router is performing the ecosystem is huge which means you can",
    "start": "825959",
    "end": "831720"
  },
  {
    "text": "monitor many or most of the popular services or things that you run with",
    "start": "831720",
    "end": "838200"
  },
  {
    "text": "Prometheus um so that's the exporter ecosystem we also have a lot of you know we have the",
    "start": "838200",
    "end": "844440"
  },
  {
    "text": "Json exporter which is very generic if an API returns Json and you want to convert that to metrics you can do that",
    "start": "844440",
    "end": "849600"
  },
  {
    "text": "with the Json exporter but this week you can also natively instrument a lot of the applications and libraries with",
    "start": "849600",
    "end": "855420"
  },
  {
    "text": "Prometheus and this is something we are seeing more and more of cool conclusion",
    "start": "855420",
    "end": "861540"
  },
  {
    "text": "um yeah Prometheus uh with its Dynamic data model uh the query language and",
    "start": "861540",
    "end": "867120"
  },
  {
    "text": "simplicity is a really really good uh monitoring system for cloud native environments also for your hobby",
    "start": "867120",
    "end": "873300"
  },
  {
    "text": "projects for everything across the stack um yeah use more of it if you are not",
    "start": "873300",
    "end": "879240"
  },
  {
    "text": "already using it so that's me with the intro and now Ganesh yeah so in the Deep time uh we will see",
    "start": "879240",
    "end": "887820"
  },
  {
    "text": "what is the few of the highlights what are new in Prometheus in the last one year we'll mostly look at the Prometheus",
    "start": "887820",
    "end": "894839"
  },
  {
    "text": "server itself though there are lots of other projects under promise which have done great stuff and we'll see what's",
    "start": "894839",
    "end": "901320"
  },
  {
    "text": "coming next recapping some features of from KL which",
    "start": "901320",
    "end": "906959"
  },
  {
    "text": "have been presented before uh two of the things which let you control the time of",
    "start": "906959",
    "end": "912360"
  },
  {
    "text": "a Time series so a query Works in a way where you mention the time when you want",
    "start": "912360",
    "end": "917579"
  },
  {
    "text": "to run the query but you don't always want all the data to be fetched for the",
    "start": "917579",
    "end": "922740"
  },
  {
    "text": "same time in the API sometimes you want to fetch data for the past sometime in the future sometimes you want to",
    "start": "922740",
    "end": "929040"
  },
  {
    "text": "pinpoint the exact time when you want for net query so Prometheus supported",
    "start": "929040",
    "end": "934380"
  },
  {
    "text": "offsets which lets you move back the time in the past but in version 2.25",
    "start": "934380",
    "end": "941300"
  },
  {
    "text": "we added support to move into the future because sometimes people like to focus",
    "start": "941300",
    "end": "946500"
  },
  {
    "text": "things and put that data into Prometheus so you can query data into the future and you can with the help of the at",
    "start": "946500",
    "end": "953220"
  },
  {
    "text": "modifier which is the orange text that you see as at end it",
    "start": "953220",
    "end": "960120"
  },
  {
    "text": "pins the time for that Vector selector that particular time so",
    "start": "960120",
    "end": "966300"
  },
  {
    "text": "the this one these were introduced were more than a year ago but it became stable in the last we announced it",
    "start": "966300",
    "end": "973380"
  },
  {
    "text": "stable in the last one year and recently we also added trigonometric functions in Prometheus and an operator",
    "start": "973380",
    "end": "981480"
  },
  {
    "text": "called arc tangent the first example shows uh the angle is in degrees so we",
    "start": "981480",
    "end": "988620"
  },
  {
    "text": "convert it into radian using the radian function then we take a sign of it yeah these were the new things in the",
    "start": "988620",
    "end": "995339"
  },
  {
    "text": "promql now before I talk about this particular feature",
    "start": "995339",
    "end": "1000500"
  },
  {
    "text": "Prometheus has a feature called remote write where you can configure Prometheus",
    "start": "1000500",
    "end": "1005720"
  },
  {
    "text": "to send all of it its data that it's getting to send it to a remote storage for example Thanos or cortex for long",
    "start": "1005720",
    "end": "1012079"
  },
  {
    "text": "term storage and in version 2.25 Prometheus received",
    "start": "1012079",
    "end": "1018500"
  },
  {
    "text": "the capability to receive the data from the remote right so you could have a Prometheus remote right into another",
    "start": "1018500",
    "end": "1024079"
  },
  {
    "text": "Prometheus so this also came more than a year ago but in version 2.33 which was",
    "start": "1024079",
    "end": "1029240"
  },
  {
    "text": "within last one year we declared it stable I think you have to still enable",
    "start": "1029240",
    "end": "1034459"
  },
  {
    "text": "it using a feature flag and talking about remote right mode you",
    "start": "1034459",
    "end": "1042260"
  },
  {
    "text": "don't always want to store all the data in Prometheus because it's possible you never query Prometheus",
    "start": "1042260",
    "end": "1048020"
  },
  {
    "text": "so in version 2.32 we added a new mode in Prometheus called the agent mode",
    "start": "1048020",
    "end": "1053960"
  },
  {
    "text": "where the Prometheus does not store any any data in a queryable fashion on the",
    "start": "1053960",
    "end": "1059000"
  },
  {
    "text": "disk which reduces the load and the storage required for the Prometheus and",
    "start": "1059000",
    "end": "1065419"
  },
  {
    "text": "it just scrapes the data stores it in a right log and forwards it to the remote",
    "start": "1065419",
    "end": "1070460"
  },
  {
    "text": "storage and clears the right head log in a regular fashion so this was added to make the remote",
    "start": "1070460",
    "end": "1077419"
  },
  {
    "text": "storage so easy and up to recently uh Prometheus",
    "start": "1077419",
    "end": "1084500"
  },
  {
    "text": "released a new version every six weeks and once a new version was released we",
    "start": "1084500",
    "end": "1090500"
  },
  {
    "text": "rarely ever put the bug fixes into older release and we always ask you to upgrade to the",
    "start": "1090500",
    "end": "1097820"
  },
  {
    "text": "latest release if you wanted a bug fix this was not really helpful lately and",
    "start": "1097820",
    "end": "1103460"
  },
  {
    "text": "starting tour 37 which was released in July we we have started releasing the",
    "start": "1103460",
    "end": "1109700"
  },
  {
    "text": "long term support releases first one is tour 37 which was released in July we",
    "start": "1109700",
    "end": "1116419"
  },
  {
    "text": "plan to support it until Jan and I think the plan is to have one or two long-term",
    "start": "1116419",
    "end": "1121520"
  },
  {
    "text": "support releases every year which will get critical bug fixes",
    "start": "1121520",
    "end": "1126880"
  },
  {
    "text": "and uh one another fallback before",
    "start": "1128480",
    "end": "1135460"
  },
  {
    "text": "this particular feature was that if for a Time series you got a sample let's say",
    "start": "1135460",
    "end": "1140480"
  },
  {
    "text": "for a timestamp thousand and for the same time series due to some timing issues a new sample came for a timestamp",
    "start": "1140480",
    "end": "1147860"
  },
  {
    "text": "let's say 900. Prometheus would reject it because it could not support timestamps that came out of order every",
    "start": "1147860",
    "end": "1154580"
  },
  {
    "text": "new sample had to be newer than the sample that came before but starting to rot starting version 2.39 we added an",
    "start": "1154580",
    "end": "1162980"
  },
  {
    "text": "experimental support in Prometheus where you can say if the auto 40 sample or the old sample that",
    "start": "1162980",
    "end": "1170179"
  },
  {
    "text": "comes within let's say two hours in this configured example we can ingest it in",
    "start": "1170179",
    "end": "1176179"
  },
  {
    "text": "Prometheus right now so Prometheus no longer will reject out of order samples and you can configure it to have let's",
    "start": "1176179",
    "end": "1183380"
  },
  {
    "text": "say 100 year old sample and it will still work fine and this is the last thing that I am",
    "start": "1183380",
    "end": "1190220"
  },
  {
    "text": "going to talk about in the Deep dive which I am very excited about in the data model you might have seen that a",
    "start": "1190220",
    "end": "1196280"
  },
  {
    "text": "sample has a timestamp as int and a value as float 64.",
    "start": "1196280",
    "end": "1201919"
  },
  {
    "text": "and we use this time series model to represent a histogram a histogram is a",
    "start": "1201919",
    "end": "1208760"
  },
  {
    "text": "distribution of values within buckets for example 100 counts within the number",
    "start": "1208760",
    "end": "1213919"
  },
  {
    "text": "of zero to one second and things like that and for every bucket we had a new",
    "start": "1213919",
    "end": "1219080"
  },
  {
    "text": "time series and we had a separate time series to denote the overall some overall count of the histogram but uh",
    "start": "1219080",
    "end": "1226580"
  },
  {
    "text": "with these native histograms which I just merged yesterday and will be part of release 2.40 next month",
    "start": "1226580",
    "end": "1234320"
  },
  {
    "text": "we are replacing the float with a complex data structure which stores uh sparse histograms in a sparse fashion in",
    "start": "1234320",
    "end": "1242000"
  },
  {
    "text": "a nicely compressed fashion where it will be a single time series and hundreds of buckets without uh",
    "start": "1242000",
    "end": "1249679"
  },
  {
    "text": "compromising on the storage efficiency so this is coming next month",
    "start": "1249679",
    "end": "1256120"
  },
  {
    "text": "so with this I would like to open the floors for Q and A I think we have",
    "start": "1256400",
    "end": "1261820"
  },
  {
    "text": "roughly 13 minutes for Q a",
    "start": "1261820",
    "end": "1266259"
  },
  {
    "text": "yep",
    "start": "1267919",
    "end": "1270460"
  },
  {
    "text": "thank you hi thank you for your explanation all of them and I'm happy to use that as a pro",
    "start": "1273200",
    "end": "1279860"
  },
  {
    "text": "medius and recently I found it there's the Kuba Pro middle stats and ham chart",
    "start": "1279860",
    "end": "1286580"
  },
  {
    "text": "I'm deployed it to some secret name is a little bit strange so I opened uh some",
    "start": "1286580",
    "end": "1292400"
  },
  {
    "text": "issue but nobody checking so could you check the issues that the issue number",
    "start": "1292400",
    "end": "1299539"
  },
  {
    "text": "is the I'm really sorry for that but the issue is nobody see this though issue",
    "start": "1299539",
    "end": "1305120"
  },
  {
    "text": "number is the 2400 2 530 33 so could you locate it as if you have",
    "start": "1305120",
    "end": "1313460"
  },
  {
    "text": "a time thank you [Laughter]",
    "start": "1313460",
    "end": "1321950"
  },
  {
    "text": "also while I walk back there I happen to be the a maintainer of the recruit Prometheus project so",
    "start": "1322039",
    "end": "1328460"
  },
  {
    "text": "um the helm chart is not maintained by the maintainers of the coup Prometheus stack so just one two say that",
    "start": "1328460",
    "end": "1335900"
  },
  {
    "text": "hello um I had an issue in production a little bit ago where we had a developer wanted",
    "start": "1335900",
    "end": "1344059"
  },
  {
    "text": "to add some custom metrics to their app which is yay fantastic um he didn't really have a lot of",
    "start": "1344059",
    "end": "1349760"
  },
  {
    "text": "oversight when doing it so he added a label for the path of the request coming",
    "start": "1349760",
    "end": "1355880"
  },
  {
    "text": "in the problem was some of the paths had unique IDs in them this went to prod",
    "start": "1355880",
    "end": "1361600"
  },
  {
    "text": "after about a week we ended up with a label that had a cardinality of like 500",
    "start": "1361600",
    "end": "1367340"
  },
  {
    "text": "000. as you can imagine Prometheus did not handle that well well actually I'm not sure if Prometheus didn't handle it",
    "start": "1367340",
    "end": "1373400"
  },
  {
    "text": "well our cluster didn't handle it well are there any I guess I'm not aware of it maybe there is a simple answer is",
    "start": "1373400",
    "end": "1379820"
  },
  {
    "text": "there a way to kind of put some guard rails around handling labels and high carnality like that or is that just a",
    "start": "1379820",
    "end": "1387320"
  },
  {
    "text": "hey don't do that again so we have something called relabeling",
    "start": "1387320",
    "end": "1393559"
  },
  {
    "text": "rules on scrape time but for the issue that you mentioned I don't think you can foresee the issue happening okay so we",
    "start": "1393559",
    "end": "1400820"
  },
  {
    "text": "now have scrape level limits where you can limit the number of Series in a script and you can be like okay if",
    "start": "1400820",
    "end": "1406100"
  },
  {
    "text": "somebody's sending me more than five thousand cities or ten thousand series depends on the script conflict you can just the script will just fail and then",
    "start": "1406100",
    "end": "1412880"
  },
  {
    "text": "you the developers will come to you and say I can't see my metrics and they can go to the targets page and see oh I'm",
    "start": "1412880",
    "end": "1418039"
  },
  {
    "text": "pushing a lot of data probably garbage so we have all those limits in place",
    "start": "1418039",
    "end": "1424340"
  },
  {
    "text": "um they're not part of the global so you have to add it to each script config but I have a PR out that I did during the",
    "start": "1424340",
    "end": "1429620"
  },
  {
    "text": "Contra Fest today which is going to add it to Global uh as well sorry what was that called again",
    "start": "1429620",
    "end": "1435500"
  },
  {
    "text": "um they're called I don't know but like if you go to scrape config and search for limit",
    "start": "1435500",
    "end": "1441260"
  },
  {
    "text": "you you'll there'll be a bunch of limits in there great thank you",
    "start": "1441260",
    "end": "1446299"
  },
  {
    "text": "it's sample limit",
    "start": "1446299",
    "end": "1449440"
  },
  {
    "text": "um is there any uh performance trade-off with using the out of order right feature that you mentioned like does it",
    "start": "1457760",
    "end": "1463820"
  },
  {
    "text": "use more resources because it has to cache in memory yeah so we have tested this in production for a bunch of",
    "start": "1463820",
    "end": "1470120"
  },
  {
    "text": "different loads uh the depending on the rate at which you are getting out of",
    "start": "1470120",
    "end": "1475460"
  },
  {
    "text": "order samples there will be a bit of increase in the CPU uh and the memory I don't think it",
    "start": "1475460",
    "end": "1482059"
  },
  {
    "text": "affects a lot but again it all depends on the percentage of samples percentage of Time series that are getting out of",
    "start": "1482059",
    "end": "1488780"
  },
  {
    "text": "order samples and the rate at which you are getting out of order samples but still the memory will be capped up to",
    "start": "1488780",
    "end": "1494539"
  },
  {
    "text": "some point because once a time series gets up to 30 out of 40 samples we just",
    "start": "1494539",
    "end": "1499700"
  },
  {
    "text": "flush it to the disk and cap the memory consumption of a particular time series uh yeah so you have to just try it out",
    "start": "1499700",
    "end": "1506120"
  },
  {
    "text": "and see how it performs in your environment so it it can there can be no difference there can be a little higher",
    "start": "1506120",
    "end": "1512240"
  },
  {
    "text": "difference so it all depends got it thank you",
    "start": "1512240",
    "end": "1516640"
  },
  {
    "text": "okay so uh when you're talking about the long term support the LTS uh you are",
    "start": "1519200",
    "end": "1525980"
  },
  {
    "text": "specifying about specific versions right is it that only in those versions you get the critical bugs bug fix or I I was",
    "start": "1525980",
    "end": "1533480"
  },
  {
    "text": "hearing something like that okay so the latest release which is out will always get the critical bug fixes and the long",
    "start": "1533480",
    "end": "1541460"
  },
  {
    "text": "term release is like for example 2.37 when which went out in July and which we",
    "start": "1541460",
    "end": "1547279"
  },
  {
    "text": "plan to support up to Jan it will get critical bug fixes up to Jan until the",
    "start": "1547279",
    "end": "1553100"
  },
  {
    "text": "next LTS release is released on top of that the latest release which",
    "start": "1553100",
    "end": "1558500"
  },
  {
    "text": "is out right now is to the 39 that will also get the critical work fix so it is like a patch release yes patch release",
    "start": "1558500",
    "end": "1564620"
  },
  {
    "text": "it won't get new features it won't get any experimental features or bug fixes with them but it will get critical for",
    "start": "1564620",
    "end": "1570799"
  },
  {
    "text": "this oh",
    "start": "1570799",
    "end": "1577419"
  },
  {
    "text": "by the way we also have our Prometheus quarterly Community call um and you we discuss all the latest and",
    "start": "1578900",
    "end": "1585919"
  },
  {
    "text": "greatest things there and you can also always always bring your questions and directly talk to the maintainers and community members in the community call",
    "start": "1585919",
    "end": "1592220"
  },
  {
    "text": "this Google for it Prometheus Community call and you'll find it and we also have monthly Dev Summits",
    "start": "1592220",
    "end": "1599059"
  },
  {
    "text": "where we discuss uh all the development that's happening in Prometheus and we openly take decision on what we want to",
    "start": "1599059",
    "end": "1605539"
  },
  {
    "text": "do next and if you have anything in your mind that you want to propose the doc is public so you can also search for",
    "start": "1605539",
    "end": "1611600"
  },
  {
    "text": "Prometheus Dev Summits and take part in them every month hey",
    "start": "1611600",
    "end": "1617659"
  },
  {
    "text": "um so it's been a while since I last checked I think it was like last month but I I ran into an issue and it was",
    "start": "1617659",
    "end": "1625760"
  },
  {
    "text": "I'll give some backstory here if anyone has ran istio and tried to scrape his",
    "start": "1625760",
    "end": "1631039"
  },
  {
    "text": "geometrics you know that the cardinality is a little ridiculous especially when you're doing a pod monitor with sidecars",
    "start": "1631039",
    "end": "1636320"
  },
  {
    "text": "right so in one in one particular cluster that we had we run around to",
    "start": "1636320",
    "end": "1641600"
  },
  {
    "text": "maybe 300 clusters our setup is we have Prometheus and everything everything remote writes to mimir right and kudos",
    "start": "1641600",
    "end": "1650360"
  },
  {
    "text": "to bemir I haven't been able to break it until this thing happened but so what essentially happened was it was it was",
    "start": "1650360",
    "end": "1657559"
  },
  {
    "text": "it was my fault because I I misconfigured the scrape config uh or the remote right config so I didn't",
    "start": "1657559",
    "end": "1663200"
  },
  {
    "text": "allow it enough capacity to write to write out so basically it was scraping",
    "start": "1663200",
    "end": "1668419"
  },
  {
    "text": "too much and it couldn't keep up with the load that was getting because my Max shards wasn't high enough and it the I",
    "start": "1668419",
    "end": "1673940"
  },
  {
    "text": "did some testing my Max requested shards it wanted like 30 million right but because of this it wasn't writing uh in",
    "start": "1673940",
    "end": "1681140"
  },
  {
    "text": "time and stuff was getting out of date and that caused a cascading failure across all of my clusters Prometheus",
    "start": "1681140",
    "end": "1688279"
  },
  {
    "text": "nodes uh or Prometheus pods right so now all of the prods are finally like trying to",
    "start": "1688279",
    "end": "1694340"
  },
  {
    "text": "send these metrics they're failing mimir's trying to ingest the metrics but it's sending I think I can't remember the Eric was like 490 or something like",
    "start": "1694340",
    "end": "1700460"
  },
  {
    "text": "that and basically this is the only thing that I've had that's ever taken down the mirror which is a surprise but",
    "start": "1700460",
    "end": "1705679"
  },
  {
    "text": "my thing is is there is there a possibility that we can input logic into the remote right",
    "start": "1705679",
    "end": "1712220"
  },
  {
    "text": "config that if it's out of date for a specific amount of time to just don't even try sending it just drop it right",
    "start": "1712220",
    "end": "1719240"
  },
  {
    "text": "away like uh because I feel like because the only solution to this particular issue and it's lucky because",
    "start": "1719240",
    "end": "1725539"
  },
  {
    "text": "I don't really care too much about the data on the pods so I'm just using empty dirs so what I did was I just ran it ran",
    "start": "1725539",
    "end": "1731720"
  },
  {
    "text": "a script through and just uh deleted all of the pods and had the staple set restart up and you know the fresh right",
    "start": "1731720",
    "end": "1737480"
  },
  {
    "text": "ahead log but if I was running PVS on there then we would still run in the same issue it'd be a huge problem so uh",
    "start": "1737480",
    "end": "1746059"
  },
  {
    "text": "again I remember there being an issue about this I haven't checked in about like a month and a half but is there is that being looked at or is that a",
    "start": "1746059",
    "end": "1752360"
  },
  {
    "text": "possibility that we can Implement into this um yeah I mean so first thing is you can",
    "start": "1752360",
    "end": "1757820"
  },
  {
    "text": "have an alert that says if I'm out of sync for more than 60 seconds page me that's first thing I would say and I",
    "start": "1757820",
    "end": "1765200"
  },
  {
    "text": "would run alerting on Prometheus not on memer uh because remember it doesn't get the data you can't alert it's more",
    "start": "1765200",
    "end": "1771080"
  },
  {
    "text": "robust to always do alerting on Prometheus that's one layer of things um and uh yeah I mean like I think you",
    "start": "1771080",
    "end": "1779539"
  },
  {
    "text": "can do a few things we want to improve this in a lot of different ways but I would have that alert and act on it",
    "start": "1779539",
    "end": "1785059"
  },
  {
    "text": "directly and hopefully not rely on this architecture that causes a cascading failure right yeah so if anyone's",
    "start": "1785059",
    "end": "1792320"
  },
  {
    "text": "curious the fix to it was increasing the capacity and also the max amount of shards so it didn't actually fall behind",
    "start": "1792320",
    "end": "1797539"
  },
  {
    "text": "and cause the cascading failure yes and also if you can also increase the batch size that helps yeah",
    "start": "1797539",
    "end": "1802600"
  },
  {
    "text": "the main remote right work for 99 percent of the use cases but like for",
    "start": "1802600",
    "end": "1807620"
  },
  {
    "text": "those small use cases you need to tweak and tweak maybe we can do some Auto tweaking but like for now we're gonna",
    "start": "1807620",
    "end": "1813320"
  },
  {
    "text": "stick to what it is um okay yeah um and another question too",
    "start": "1813320",
    "end": "1819200"
  },
  {
    "text": "um when it comes to the when it comes to scrape configs themselves there's the re-labeling configs and the metrics relabeling configs but there's also the",
    "start": "1819200",
    "end": "1826220"
  },
  {
    "text": "re the remote right relabeling config um when it when it comes to service",
    "start": "1826220",
    "end": "1831559"
  },
  {
    "text": "monitors and pod monitors right it would be nice if like uh we can kind of have",
    "start": "1831559",
    "end": "1837440"
  },
  {
    "text": "like The Best of Both Worlds of it storing some data locally uh but also",
    "start": "1837440",
    "end": "1844159"
  },
  {
    "text": "like the stuff that like is really like heavy like memory usage wise we send it but don't store it locally if that makes",
    "start": "1844159",
    "end": "1850520"
  },
  {
    "text": "sense kind of like uh we do them you do a metrics relabeling to keep uh certain metrics but then you would have to do",
    "start": "1850520",
    "end": "1857120"
  },
  {
    "text": "like at a global level stop like the remote right for some metrics but if we could put that into another section of",
    "start": "1857120",
    "end": "1864860"
  },
  {
    "text": "like the Pod monitor the service monitor is like remote right relabeling or something like that I think that would be really is are there plans for that or",
    "start": "1864860",
    "end": "1871340"
  },
  {
    "text": "is it just going to be set on the global level I don't think that I need plans for that",
    "start": "1871340",
    "end": "1876500"
  },
  {
    "text": "um but it's a feature request I would say that I haven't seen so just open an issue for it okay yeah cool",
    "start": "1876500",
    "end": "1883460"
  },
  {
    "text": "um yeah we still have a few minutes uh but let's try to keep the questions short we'll always be here all the",
    "start": "1883460",
    "end": "1888919"
  },
  {
    "text": "Prometheus people so you can always come to us and discuss also afterwards so yeah we have a couple of questions more",
    "start": "1888919",
    "end": "1894260"
  },
  {
    "text": "I think and we have a few minutes more",
    "start": "1894260",
    "end": "1897820"
  },
  {
    "text": "hi this is relating to uh cardinality uh I guess there's no support for string",
    "start": "1901460",
    "end": "1906919"
  },
  {
    "text": "metrics where the value is a string right everything else is a label",
    "start": "1906919",
    "end": "1913640"
  },
  {
    "text": "I have today if I want to keep track of a string I got to put it as a label which increases the cardinality right",
    "start": "1913640",
    "end": "1919820"
  },
  {
    "text": "could we add string metrics uh it's again a feature request I'm not",
    "start": "1919820",
    "end": "1925279"
  },
  {
    "text": "sure if you're being the plan anytime soon but we'll consider it thank you we are a",
    "start": "1925279",
    "end": "1931100"
  },
  {
    "text": "matrix monitoring system not a logs monitoring systems but yes I mean with the histogram stuff",
    "start": "1931100",
    "end": "1938179"
  },
  {
    "text": "we can make it a little more generic maybe in the future we'll support it but no plans as of today",
    "start": "1938179",
    "end": "1943960"
  },
  {
    "text": "hey um I've heard about some updates with prom ql language server as well as",
    "start": "1949039",
    "end": "1955039"
  },
  {
    "text": "another project called prom lens and I know some difficulty that folks have is with learning prom ql you know",
    "start": "1955039",
    "end": "1961220"
  },
  {
    "text": "particularly like with our developers and I was just wondering if there's any plans to like integrate that with Prometheus UI or graphonics not if it's",
    "start": "1961220",
    "end": "1968419"
  },
  {
    "text": "not already there but um but yeah yeah just looking to hear if there's any like status updates on that",
    "start": "1968419",
    "end": "1974179"
  },
  {
    "text": "um by the way prom lens was open source I think two days ago uh with the collaboration from the problem Labs I",
    "start": "1974179",
    "end": "1980659"
  },
  {
    "text": "mean I don't know what julius's company's problem Labs uh and chronosphere and we have full plans to",
    "start": "1980659",
    "end": "1985760"
  },
  {
    "text": "integrate it into Prometheus it's part of the prom like it's in the Prometheus org right now and you should see it in",
    "start": "1985760",
    "end": "1991640"
  },
  {
    "text": "the coming months yeah cool any more questions yes we have a",
    "start": "1991640",
    "end": "1998360"
  },
  {
    "text": "couple more we have one more minute who do you think was first uh the one with the back sorry",
    "start": "1998360",
    "end": "2005700"
  },
  {
    "text": "I know you guys uh didn't really mention the subject and I'm not really up to date but could you elaborate a little",
    "start": "2010059",
    "end": "2016299"
  },
  {
    "text": "bit more on Native High availability in Prometheus and what things you specifically don't want and what your",
    "start": "2016299",
    "end": "2022539"
  },
  {
    "text": "roadmap is Prometheus is highly available in that you run two Prometheus servers that",
    "start": "2022539",
    "end": "2028899"
  },
  {
    "text": "don't talk to each other that don't know anything about each other and you can do deduplication of alerting at the alert",
    "start": "2028899",
    "end": "2035679"
  },
  {
    "text": "manager level which is highly available so I would say Prometheus does solve the high availability problem in its own way",
    "start": "2035679",
    "end": "2042399"
  },
  {
    "text": "it's a it's a very workable solution that works for most people Prometheus is inexpensive enough that you can run two",
    "start": "2042399",
    "end": "2049240"
  },
  {
    "text": "Prometheus servers and be happy about it um if you go to a really distributed highly available system you'll probably",
    "start": "2049240",
    "end": "2055540"
  },
  {
    "text": "have to run three replicas please um that talk to each other they said Network dependency their split brain",
    "start": "2055540",
    "end": "2061658"
  },
  {
    "text": "problems we don't want to get into all of that we want to build a very robust server for alerting and we don't want to",
    "start": "2061659",
    "end": "2068679"
  },
  {
    "text": "complicate the architecture but if you do want a scalable highly available system there's other projects in the",
    "start": "2068679",
    "end": "2074800"
  },
  {
    "text": "ecosystem like cortex Thanos uh mirror and stuff that you can look into but Prometheus itself will be a single node",
    "start": "2074800",
    "end": "2081520"
  },
  {
    "text": "system that where the high availability is run to copy of it thanks",
    "start": "2081520",
    "end": "2087580"
  },
  {
    "text": "again even for the customers of mimir I always suggest run alerting in",
    "start": "2087580",
    "end": "2093220"
  },
  {
    "text": "Prometheus mainly because it's far more robust than running it in memory like",
    "start": "2093220",
    "end": "2098859"
  },
  {
    "text": "like they mentioned it's very common for the data to lag behind when I say very",
    "start": "2098859",
    "end": "2104020"
  },
  {
    "text": "common it happens once in three or four months but that's too frequent but if you're relying on alerting",
    "start": "2104020",
    "end": "2110200"
  },
  {
    "text": "yeah do we have time for one more yeah I think so I mean do we yeah okay we have",
    "start": "2110200",
    "end": "2116200"
  },
  {
    "text": "time uh you talked about scraping is the only way to push metrics into uh the tstv uh",
    "start": "2116200",
    "end": "2124359"
  },
  {
    "text": "what about things like otlp like open Telemetry line protocol to",
    "start": "2124359",
    "end": "2129400"
  },
  {
    "text": "so Ganesh also talked about the remote right receiver you can push remote right into it um and in The Collector I give a talk on",
    "start": "2129400",
    "end": "2137079"
  },
  {
    "text": "Monday it should be up already I think so basically you can use the collector to write otlp into the collector and",
    "start": "2137079",
    "end": "2142599"
  },
  {
    "text": "then right remote right into Prometheus so you can convert otlp data into Prometheus and Maps really really well",
    "start": "2142599",
    "end": "2149220"
  },
  {
    "text": "having said that I do I'm also part of the open Telemetry community and I always keep proposing adding otlp",
    "start": "2149220",
    "end": "2156099"
  },
  {
    "text": "ingestion so there's a Dev Summit in a couple of weeks that I'm going to propose again so let's see what the Prometheus Community says then but yeah",
    "start": "2156099",
    "end": "2163420"
  },
  {
    "text": "today if you want to do that you can do it through the collector there's full support that works really well",
    "start": "2163420",
    "end": "2169359"
  },
  {
    "text": "all right I think that's uh time but we will be here um so you can this other maintenance here so you can always come",
    "start": "2169359",
    "end": "2176079"
  },
  {
    "text": "talk to us yeah [Applause]",
    "start": "2176079",
    "end": "2181130"
  }
]