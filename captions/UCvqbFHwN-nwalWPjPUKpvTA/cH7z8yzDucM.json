[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "okay hello welcome to my session on running low latency workloads on top of",
    "start": "0",
    "end": "5100"
  },
  {
    "text": "kubernetes uh an alternative title this talk is how we run Spice TV without hiccups",
    "start": "5100",
    "end": "11400"
  },
  {
    "text": "um this is going to be documenting largely our journey as we've kind of learned to run our latency sensitive",
    "start": "11400",
    "end": "18539"
  },
  {
    "text": "workloads on top of kubernetes um uh [Music] guide you along your journey as well and",
    "start": "18539",
    "end": "25740"
  },
  {
    "text": "show you kind of The Primitives we use in kubernetes to guarantee the performance that we need out of our systems",
    "start": "25740",
    "end": "31800"
  },
  {
    "text": "um so without further Ado um we're going to need to go through some introductions I am Jimmy zielinski",
    "start": "31800",
    "end": "38700"
  },
  {
    "start": "37000",
    "end": "37000"
  },
  {
    "text": "so I am a co-founder of a company called auth Zed previously I worked at red hat and coreless I've been in all kinds of",
    "start": "38700",
    "end": "45540"
  },
  {
    "text": "roles product management engineering operations as well I've been SRE so",
    "start": "45540",
    "end": "51600"
  },
  {
    "text": "despite kind of my current title as Chief product officer I still carry a painter I still build the systems that",
    "start": "51600",
    "end": "58079"
  },
  {
    "text": "then get ran in production um I've been around in the cloud native ecosystem for a pretty long time",
    "start": "58079",
    "end": "65040"
  },
  {
    "text": "basically since the beginning um because I started at core OS before kubernetes or the cloud native",
    "start": "65040",
    "end": "70320"
  },
  {
    "text": "Foundation was actually created um so in that time I worked a lot on kind",
    "start": "70320",
    "end": "76020"
  },
  {
    "text": "of the container ecosystem um so thinking about Docker Registries um I worked on Quay which is the first",
    "start": "76020",
    "end": "83460"
  },
  {
    "text": "private Docker registry I worked on Claire which is the first container",
    "start": "83460",
    "end": "88740"
  },
  {
    "text": "static analysis tool for detecting vulnerabilities I am currently a maintainer of the open container",
    "start": "88740",
    "end": "94560"
  },
  {
    "text": "initiative so that is the standards body that dictates what an application container is",
    "start": "94560",
    "end": "99900"
  },
  {
    "text": "um well I was at red hat and core OS I worked on basically what ultimately",
    "start": "99900",
    "end": "106200"
  },
  {
    "text": "became the operator framework which is a cncf project now to help folks build and",
    "start": "106200",
    "end": "112140"
  },
  {
    "text": "run operators on clusters and then in general you'll probably have seen me on random GitHub issues around",
    "start": "112140",
    "end": "118200"
  },
  {
    "text": "the cloud native ecosystem I've never ever been a maintainer of kubernetes or anything Hands-On but I I've been a user",
    "start": "118200",
    "end": "125700"
  },
  {
    "text": "its whole existence and chime in Fairly regularly on different issues that I've hit in production personally",
    "start": "125700",
    "end": "132319"
  },
  {
    "text": "prior to all this stuff I was actually working on like the BitTorrent ecosystem so I'm author of some of the BitTorrent",
    "start": "132319",
    "end": "138360"
  },
  {
    "text": "standards and I worked on a project called chahaya which is a BitTorrent tracker but it is actually part of the",
    "start": "138360",
    "end": "144720"
  },
  {
    "text": "orchestration layer at Facebook internally that distributes their software server side from from rack to",
    "start": "144720",
    "end": "151440"
  },
  {
    "text": "Rack um so that was kind of like where I first began cutting my teeth on high performance and low latency go systems",
    "start": "151440",
    "end": "157860"
  },
  {
    "text": "and that will become relevant a bit further on um I figured I would add some of my",
    "start": "157860",
    "end": "162900"
  },
  {
    "text": "contact information here just in case folks want to come back if you do want to reach out to me at any point in time",
    "start": "162900",
    "end": "168300"
  },
  {
    "text": "any questions at all like related to this or anything that you can kind of think of email is my best option",
    "start": "168300",
    "end": "176400"
  },
  {
    "text": "um you can also see me on like the social networks or GitHub um but basically if you want to get a",
    "start": "176400",
    "end": "181800"
  },
  {
    "text": "guaranteed response you want me to see something email is definitely the uh the best venue for that",
    "start": "181800",
    "end": "188480"
  },
  {
    "text": "all right um so next up is an introduction for spice",
    "start": "188580",
    "end": "193800"
  },
  {
    "text": "DB spice DB is an authorization specific database so it's a database that stores",
    "start": "193800",
    "end": "199080"
  },
  {
    "text": "authorization data so authorization data is the data that you use in order to",
    "start": "199080",
    "end": "204120"
  },
  {
    "text": "determine whether someone has a permission to perform a particular operation",
    "start": "204120",
    "end": "209400"
  },
  {
    "text": "um the reason why you would uh usually want to kind of isolate all this stuff is so",
    "start": "209400",
    "end": "216239"
  },
  {
    "text": "that um you kind of have this centralized place where you have both the logic and data required to determine these",
    "start": "216239",
    "end": "223140"
  },
  {
    "text": "permissions that empowers you a lot it means if you have multiple applications",
    "start": "223140",
    "end": "228420"
  },
  {
    "text": "or multiple multiple applications implemented in different languages all",
    "start": "228420",
    "end": "233879"
  },
  {
    "text": "of those different programs can actually just query this one Central system to",
    "start": "233879",
    "end": "239099"
  },
  {
    "text": "perform an operation um to understand whether or not someone has access",
    "start": "239099",
    "end": "244980"
  },
  {
    "text": "um and so that means you're not writing uh tons of different logic uh security critical logic",
    "start": "244980",
    "end": "251099"
  },
  {
    "text": "um in your code duplicating it or like trying to make sure everything works everywhere um and uh we basically give you a",
    "start": "251099",
    "end": "259320"
  },
  {
    "text": "framework for describing these systems in a safe way so that guarantees that oh adding a new feature that needs to",
    "start": "259320",
    "end": "265919"
  },
  {
    "text": "change the model we can actually test that and guarantee to you that you have an open to security flaw in your",
    "start": "265919",
    "end": "272639"
  },
  {
    "text": "software and then also we give you kind of uh scalability guarantees so as long",
    "start": "272639",
    "end": "279240"
  },
  {
    "text": "as you build within this model we can guarantee performance at a very high scale that not only means that you don't",
    "start": "279240",
    "end": "286020"
  },
  {
    "text": "have to worry if you're like a massive company doing tons of traffic but also that means that you can get very very",
    "start": "286020",
    "end": "292800"
  },
  {
    "text": "fine grain with the actual permissions you're allowing you can ask very very specific questions like can",
    "start": "292800",
    "end": "300780"
  },
  {
    "text": "this particular regular API key access this Row in this other database",
    "start": "300780",
    "end": "306500"
  },
  {
    "text": "so that if you think about the order of magnitude for API Keys it's a multiple",
    "start": "306500",
    "end": "311880"
  },
  {
    "text": "of the number of users on your system if you think about the cardinality of rows in your database it's also",
    "start": "311880",
    "end": "317580"
  },
  {
    "text": "astronomically large so this system can scale to the cross product of that just absolutely massive numbers",
    "start": "317580",
    "end": "324240"
  },
  {
    "text": "um in a way that the the performance is predictable and always the same and low",
    "start": "324240",
    "end": "329340"
  },
  {
    "text": "latency um so those are kind of the high level reasons why you adopt something like this",
    "start": "329340",
    "end": "334680"
  },
  {
    "text": "um permission checks are super critical uh because as I kind of said earlier",
    "start": "334680",
    "end": "339900"
  },
  {
    "text": "before you can do any work whatsoever you first have to run a permission check",
    "start": "339900",
    "end": "345060"
  },
  {
    "text": "so that means that we're kind of in the critical path of absolutely everything and before our folks even kind of make",
    "start": "345060",
    "end": "351600"
  },
  {
    "text": "queries to their relational database their primary data stores this is a full request that has to happen at the exact",
    "start": "351600",
    "end": "358080"
  },
  {
    "text": "same time or ahead of time um that puts us super in the critical path",
    "start": "358080",
    "end": "363600"
  },
  {
    "text": "um thankfully we are the most mature solution outside of Google that is",
    "start": "363600",
    "end": "368759"
  },
  {
    "text": "inspired by the system that Google uses at scale um that software is called Zanzibar",
    "start": "368759",
    "end": "373860"
  },
  {
    "text": "um it's not available to anyone else and um we have basically taken the",
    "start": "373860",
    "end": "380639"
  },
  {
    "text": "um the idea behind that and made it kind of approachable to folks that are not",
    "start": "380639",
    "end": "387600"
  },
  {
    "text": "inside of Google um it's kind of well understood that Google has googleisms they have all of",
    "start": "387600",
    "end": "393900"
  },
  {
    "text": "their own internal software that they can rely on um the environment working inside of",
    "start": "393900",
    "end": "399180"
  },
  {
    "text": "Google is not the same as working outside of Google at another business an Enterprise software company you're not",
    "start": "399180",
    "end": "405600"
  },
  {
    "text": "going to have a lot of the availability and standardization around the tools that they use um so instead we have designed an open",
    "start": "405600",
    "end": "412979"
  },
  {
    "text": "source system that works around all these things and is very friendly towards kind of folks that live in the",
    "start": "412979",
    "end": "420060"
  },
  {
    "text": "real world and don't have access and the same guarantees that the software engineer is operating inside of Google",
    "start": "420060",
    "end": "425580"
  },
  {
    "text": "have um so that often means we have to give people really nice developer tools for",
    "start": "425580",
    "end": "431460"
  },
  {
    "text": "integrating their products rather than having to be able basically having a",
    "start": "431460",
    "end": "436500"
  },
  {
    "text": "manager top down say you're forced to use this solution so we have to entice developers with better tooling and",
    "start": "436500",
    "end": "442380"
  },
  {
    "text": "better workflows than what they currently already have um and just kind of prove it all out",
    "start": "442380",
    "end": "448020"
  },
  {
    "text": "we've recently run benchmarks and hit five milliseconds at the 95th percentile running a million QPS with 100 billion",
    "start": "448020",
    "end": "455940"
  },
  {
    "text": "relationships stored inside of space DB so I think it goes out saying that we have built a low latency system that can",
    "start": "455940",
    "end": "463080"
  },
  {
    "text": "check permissions largely at Google scale foreign",
    "start": "463080",
    "end": "469639"
  },
  {
    "text": "so given my background uh given my co-founders backgrounds um I think it goes kind of without",
    "start": "469639",
    "end": "475620"
  },
  {
    "text": "saying that uh our business is going to run it's by Stevie Young kubernetes it's",
    "start": "475620",
    "end": "480900"
  },
  {
    "text": "it's everything we have experience with operationalizing um it's a thing we know super in-depth um",
    "start": "480900",
    "end": "487680"
  },
  {
    "text": "and I have actually given another webinar where I kind of outlined how we deploy a database as a service as a high",
    "start": "487680",
    "end": "495419"
  },
  {
    "text": "level um on top of kubernetes that includes kind of like do workflows and a lot of the product requirements that folks",
    "start": "495419",
    "end": "501599"
  },
  {
    "text": "often forget um but also talks about how we lay out clusters how we subdivide them how we",
    "start": "501599",
    "end": "507840"
  },
  {
    "text": "manage rolling out different phases of stability to different users um this is very very high level",
    "start": "507840",
    "end": "515099"
  },
  {
    "text": "um but also talks about a lot about core Technologies we use so if you're interested in kind of like the overall",
    "start": "515099",
    "end": "520140"
  },
  {
    "text": "details of how one of these systems works I encourage you to check out this talk",
    "start": "520140",
    "end": "525540"
  },
  {
    "text": "you can find the url right there um but this journey wasn't easy",
    "start": "525540",
    "end": "531720"
  },
  {
    "text": "um and running spice DB optimally on top of kubernetes was non-trivial",
    "start": "531720",
    "end": "538200"
  },
  {
    "text": "um I'm going to talk a little bit about why that is the case so uh spicetv in order to get its low",
    "start": "538200",
    "end": "545459"
  },
  {
    "start": "541000",
    "end": "541000"
  },
  {
    "text": "latency and very very um like quick answers for folks uh is",
    "start": "545459",
    "end": "551580"
  },
  {
    "text": "massively parallel um so what actually happens is Spice TV is kubernetes aware",
    "start": "551580",
    "end": "557339"
  },
  {
    "text": "when you deploy a deployment of spicetv on top of kubernetes it knows how to",
    "start": "557339",
    "end": "563220"
  },
  {
    "text": "talk to the API server to discover pods in that same deployment and it will um then immediately connect to those",
    "start": "563220",
    "end": "569459"
  },
  {
    "text": "pods and start self-clustering so um what is the point of self-clustering",
    "start": "569459",
    "end": "575580"
  },
  {
    "text": "well what actually happens is as requests come in they come into a load balancer like Envoy",
    "start": "575580",
    "end": "581160"
  },
  {
    "text": "um as soon as they hit any of the existing space DB pods um that pod will break down that request",
    "start": "581160",
    "end": "587940"
  },
  {
    "text": "into a bunch of subrequests that are then um paralyzed and sharded across the",
    "start": "587940",
    "end": "594839"
  },
  {
    "text": "entire cluster um so we use consistent hashrin in order to decide",
    "start": "594839",
    "end": "600120"
  },
  {
    "text": "um where a request should belong and what that actually helps us do is it means caching for that particular",
    "start": "600120",
    "end": "607860"
  },
  {
    "text": "um answer like that particular subrequest will uh be far more likely to exist on",
    "start": "607860",
    "end": "614459"
  },
  {
    "text": "that particular pod um so that basically gives us um higher cash hit rates",
    "start": "614459",
    "end": "619560"
  },
  {
    "text": "um but then at the same time because we're doing so much of this um and we're kind of like recursively",
    "start": "619560",
    "end": "625500"
  },
  {
    "text": "breaking down these requests into more and more sub-requests that get parallelized",
    "start": "625500",
    "end": "631080"
  },
  {
    "text": "um we are just trying to get as much throughput as possible um and doing as much work in parallel as",
    "start": "631080",
    "end": "638100"
  },
  {
    "text": "possible so we're High users of um goes and then threading models that means we",
    "start": "638100",
    "end": "643140"
  },
  {
    "text": "spent up tons of go routines as much as possible within process it's also made parallel not just across the server and",
    "start": "643140",
    "end": "650040"
  },
  {
    "text": "all this will become relevant away later in the conversation when we start talking about kind of performance",
    "start": "650040",
    "end": "656160"
  },
  {
    "text": "implications once we've tightened a lot of the uh schedule layer down inside of kubernetes",
    "start": "656160",
    "end": "663199"
  },
  {
    "text": "um but we have to start from the very beginning so in the beginning when you go to the player software on top of",
    "start": "663480",
    "end": "669120"
  },
  {
    "text": "kubernetes there are the defaults so what do the defaults give you I have",
    "start": "669120",
    "end": "674339"
  },
  {
    "start": "672000",
    "end": "672000"
  },
  {
    "text": "this title are you even trying and that's not talking about the kubernetes developers or anything like that that's",
    "start": "674339",
    "end": "679500"
  },
  {
    "text": "about you are you actually trying um if you've just kind of deployed your software and you haven't really done",
    "start": "679500",
    "end": "685680"
  },
  {
    "text": "anything else um the guarantees that kubernetes gives you",
    "start": "685680",
    "end": "691560"
  },
  {
    "text": "by default is effectively just best effort um so the two real big points that I",
    "start": "691560",
    "end": "698220"
  },
  {
    "text": "want to highlight uh for uh this kind of best effort behavior is that um by",
    "start": "698220",
    "end": "703860"
  },
  {
    "text": "default there's absolutely no protection from consuming all of the memory on a node that means um there's nothing that",
    "start": "703860",
    "end": "710100"
  },
  {
    "text": "stops an individual pod from just allocating more and more and more memory until the node has none in which case",
    "start": "710100",
    "end": "717060"
  },
  {
    "text": "you get an out of memory error and something has to be killed on that node",
    "start": "717060",
    "end": "722360"
  },
  {
    "text": "so effectively then kubernetes is going to have to like move a pod and switch it to another node but that disruption is",
    "start": "722360",
    "end": "730800"
  },
  {
    "text": "still going to take place there's nothing that prevents uh a uh like",
    "start": "730800",
    "end": "737579"
  },
  {
    "text": "basically background job a less critical job from affecting a performance critical job um just because it has a memory leak it",
    "start": "737579",
    "end": "744180"
  },
  {
    "text": "can totally disrupt um disrupt your performance of your very very sensitive and workload",
    "start": "744180",
    "end": "749940"
  },
  {
    "text": "um so that that's like probably number one the biggest thing to look out for if",
    "start": "749940",
    "end": "755100"
  },
  {
    "text": "you're just using kubernetes defaults um and then next up uh basically uh",
    "start": "755100",
    "end": "761640"
  },
  {
    "text": "kind of like I just said where um you can have kind of non-critical",
    "start": "761640",
    "end": "766740"
  },
  {
    "text": "workloads scheduled onto the same node as a critical workload yep kubernetes is going to do its best to schedule",
    "start": "766740",
    "end": "773940"
  },
  {
    "text": "workloads on different nodes so it doesn't want to put two of the same kind of pod next to each other but like I",
    "start": "773940",
    "end": "780720"
  },
  {
    "text": "said everything is best effort if they can't find space in the cluster it's going to do that it's not going to throw an error it's not going to tell you that",
    "start": "780720",
    "end": "786600"
  },
  {
    "text": "you need to provision more resources it's just going to tell you hey it's not going to tell you anything it's just",
    "start": "786600",
    "end": "791760"
  },
  {
    "text": "going to schedule the two workloads next to each other if they're both latency sensitive they might actually then have",
    "start": "791760",
    "end": "797639"
  },
  {
    "text": "them performance implications on each other so at the end of the day there really isn't anything that kubernetes is",
    "start": "797639",
    "end": "803220"
  },
  {
    "text": "doing to prevent um the quote unquote Noisy Neighbor problem um and having like one pod affecting",
    "start": "803220",
    "end": "809639"
  },
  {
    "text": "other pod and you have absolutely no guarantees of which pods land where so",
    "start": "809639",
    "end": "815579"
  },
  {
    "text": "why don't we see what's available to us help the schedule and decide which pods",
    "start": "815579",
    "end": "821579"
  },
  {
    "text": "should land where so there's actually kind of two major Primitives we're going to explore for",
    "start": "821579",
    "end": "826980"
  },
  {
    "text": "giving the kubernetes scheduler more details and the first one does exactly what I just described and this is called",
    "start": "826980",
    "end": "833459"
  },
  {
    "start": "832000",
    "end": "832000"
  },
  {
    "text": "Kate's intolerations um the general kind of distributed systems uh concept for this is called",
    "start": "833459",
    "end": "840180"
  },
  {
    "text": "affinity and anti-affinity so you might see that reference if you're reading any material that's not about kubernetes",
    "start": "840180",
    "end": "845880"
  },
  {
    "text": "it's still talking about this General subject um but at the end of the day",
    "start": "845880",
    "end": "851160"
  },
  {
    "text": "um there basically teams and tolerations are two special kind of labels I would describe them as that basically a taint",
    "start": "851160",
    "end": "859320"
  },
  {
    "text": "labels a node and says you cannot schedule pods unless they can tolerate",
    "start": "859320",
    "end": "864959"
  },
  {
    "text": "this this taint um and then a toleration is a basically",
    "start": "864959",
    "end": "871740"
  },
  {
    "text": "a label on iPod that says I can actually tolerate this taint on a node",
    "start": "871740",
    "end": "877860"
  },
  {
    "text": "um and so you can use this to achieve certain things like only one pod can run",
    "start": "877860",
    "end": "882959"
  },
  {
    "text": "on one node um of a particular deployment type um and you can also extend it to go one",
    "start": "882959",
    "end": "890579"
  },
  {
    "text": "step further which is you can create a whole custom node pool um that's specifically optimized for your workload so in our case what we did",
    "start": "890579",
    "end": "897420"
  },
  {
    "text": "is we got very excited and then set a like spicy B specific tainted Toleration",
    "start": "897420",
    "end": "903120"
  },
  {
    "text": "and spun up a node pool um that was the only thing that had that taint on it and that Hardware could be",
    "start": "903120",
    "end": "910500"
  },
  {
    "text": "optimized for running spice DB specifically like I said um spicyb is very very parallel so",
    "start": "910500",
    "end": "916079"
  },
  {
    "text": "having more cores does a lot more for spice QB um than maybe your general purpose",
    "start": "916079",
    "end": "921500"
  },
  {
    "text": "instance types within kubernetes so um this gives you a bit better",
    "start": "921500",
    "end": "928199"
  },
  {
    "text": "guarantees but there still isn't um unless you finally start using anti-affinity as well which would",
    "start": "928199",
    "end": "934740"
  },
  {
    "text": "prevent basically use from scheduling on anything else then you can start to make",
    "start": "934740",
    "end": "940199"
  },
  {
    "text": "these guarantees that your nodes are sorry your pods are only going to run on the very very specific nodes that you've",
    "start": "940199",
    "end": "946620"
  },
  {
    "text": "provisioned um at this point in time we did notice performance improvements on Spice TV but",
    "start": "946620",
    "end": "952560"
  },
  {
    "text": "largely this is because of the optimized Hardware we were running it on and it",
    "start": "952560",
    "end": "958199"
  },
  {
    "text": "was kind of a side effect of having like nothing else run on these nodes purely",
    "start": "958199",
    "end": "963839"
  },
  {
    "text": "spice DB um all right so the next thing that you",
    "start": "963839",
    "end": "969839"
  },
  {
    "start": "967000",
    "end": "967000"
  },
  {
    "text": "wanted to try that is kind of recommended all throughout the kubernetes documentation",
    "start": "969839",
    "end": "975120"
  },
  {
    "text": "is requests and limits um requests are basically the ability for",
    "start": "975120",
    "end": "981240"
  },
  {
    "text": "you to specify what resources need to be available on a pod for a pod to be scheduled you can kind of think of this",
    "start": "981240",
    "end": "987480"
  },
  {
    "text": "also as like an affinity anti-affinity kind of thing except it's kind of aware of the actual resourcing Primitives like",
    "start": "987480",
    "end": "995160"
  },
  {
    "text": "that the node has available to it so it's a little bit more Dynamic um and then uh limits actually create a hard",
    "start": "995160",
    "end": "1002660"
  },
  {
    "text": "limit um for the Pod it says this pod if it starts to grow um in CPU or memory",
    "start": "1002660",
    "end": "1008959"
  },
  {
    "text": "um like we're going to actually prevent it from ever growing past a certain point so that kind of gives you guarantees for how you can kind of um",
    "start": "1008959",
    "end": "1015680"
  },
  {
    "text": "fit um these pods onto nodes in a way that they're not going to overextend and",
    "start": "1015680",
    "end": "1022279"
  },
  {
    "text": "impact uh start consuming some of the resources that another pod guarantees",
    "start": "1022279",
    "end": "1027678"
  },
  {
    "text": "um that's fantastic that's our so sober bullet right like we're totally done",
    "start": "1027679",
    "end": "1033740"
  },
  {
    "text": "um they're basically between these two things we have tons of guarantees now right um well there's actually a bunch of pros",
    "start": "1033740",
    "end": "1040760"
  },
  {
    "start": "1039000",
    "end": "1039000"
  },
  {
    "text": "and cons uh related to adopting these two bits of uh kind of well uh well",
    "start": "1040760",
    "end": "1047720"
  },
  {
    "text": "recommended uh configuration for kubernetes um on the pro side we totally eliminated",
    "start": "1047720",
    "end": "1053660"
  },
  {
    "text": "out of memory errors we're not going to see those ever because uh We've restricted the amount of memory that a",
    "start": "1053660",
    "end": "1060080"
  },
  {
    "text": "pod can can provision um that's fantastic and we've also guaranteed that the scheduler is going",
    "start": "1060080",
    "end": "1066440"
  },
  {
    "text": "to look for machines that have memory available before it schedules something that we know is going to grow",
    "start": "1066440",
    "end": "1071720"
  },
  {
    "text": "um you're going to have to basically spend some time while you kind of run your system without these limits",
    "start": "1071720",
    "end": "1078919"
  },
  {
    "text": "um to understand how it's consuming memory and where you want that threshold to be um to safely run your software but after",
    "start": "1078919",
    "end": "1084919"
  },
  {
    "text": "that kind of like initial configuration phase you you you have a good understanding of how to operate",
    "start": "1084919",
    "end": "1090919"
  },
  {
    "text": "operationalize your software you should be totally fine and then the limits actually make",
    "start": "1090919",
    "end": "1096260"
  },
  {
    "text": "performance very very predictable you know you're not going to have anything uh spiky and nothing is going to go uh",
    "start": "1096260",
    "end": "1102559"
  },
  {
    "text": "totally awry um but on the con side uh actually these limits uh artificially limit your",
    "start": "1102559",
    "end": "1109160"
  },
  {
    "text": "processing if you have available um cores uh basically you're going to",
    "start": "1109160",
    "end": "1114200"
  },
  {
    "text": "want to use that if there's nothing consuming that like if you have to burst ideally you would still be able to burst",
    "start": "1114200",
    "end": "1119840"
  },
  {
    "text": "so that's kind of a limiting factor um and then additionally we just generally saw that merely enabling these",
    "start": "1119840",
    "end": "1127520"
  },
  {
    "text": "limits we saw a drop in performance um I'm not sure directly what the",
    "start": "1127520",
    "end": "1132620"
  },
  {
    "text": "overhead is uh with regards to that but um just tightening these things up actually made things more predictable",
    "start": "1132620",
    "end": "1140360"
  },
  {
    "text": "but also had a performance cost so that's kind of like a trade-off you might be you might ultimately decide",
    "start": "1140360",
    "end": "1146480"
  },
  {
    "text": "that that's actually fine to like lose some of that efficiency But ultimately gain that predictability",
    "start": "1146480",
    "end": "1152539"
  },
  {
    "text": "um any other super important thing to note about limits specifically is that they",
    "start": "1152539",
    "end": "1157880"
  },
  {
    "text": "are reactive there is actually a API server flag where you can control the",
    "start": "1157880",
    "end": "1164000"
  },
  {
    "text": "rate at which um limits pull and what happens is it's",
    "start": "1164000",
    "end": "1169039"
  },
  {
    "text": "going to pull at that specific rate um and when it detects that a pod has",
    "start": "1169039",
    "end": "1174500"
  },
  {
    "text": "exceeded its limit um during one of those polls then that's when it's going to react and kind of",
    "start": "1174500",
    "end": "1181220"
  },
  {
    "text": "throttle that pot back um or or kill that pod um so uh that means that there is",
    "start": "1181220",
    "end": "1188720"
  },
  {
    "text": "fundamentally this reaction time um like it is reactive and not ProActive at preventing these bursts so you can",
    "start": "1188720",
    "end": "1196640"
  },
  {
    "text": "still burst outside of your limit um very much so enough to impact the",
    "start": "1196640",
    "end": "1202220"
  },
  {
    "text": "performance of a very low latency system so there's kind of this like battle",
    "start": "1202220",
    "end": "1208160"
  },
  {
    "text": "between the latency that your software has and then the latency at which um",
    "start": "1208160",
    "end": "1213280"
  },
  {
    "text": "this polling rate can actually react um and I think that you'll find that for",
    "start": "1213280",
    "end": "1218840"
  },
  {
    "text": "specifically low latency systems um it doesn't matter there's like a bottom basically limit for uh how often you can",
    "start": "1218840",
    "end": "1226340"
  },
  {
    "text": "set this polling rate and um fundamentally it's it's probably never going to be low enough and",
    "start": "1226340",
    "end": "1233780"
  },
  {
    "text": "actually what we do need is a proactive solution rather than a reactive solution um and then we kind of included this",
    "start": "1233780",
    "end": "1240860"
  },
  {
    "text": "last bullet here which is uh there's still going to be context switching costs so the scheduler can still move",
    "start": "1240860",
    "end": "1247039"
  },
  {
    "text": "things around um like stop you from what you're currently doing switch out um and uh because the processor is not",
    "start": "1247039",
    "end": "1255020"
  },
  {
    "text": "necessarily guaranteed um you're going to be running on different cores on the same machine so",
    "start": "1255020",
    "end": "1260539"
  },
  {
    "text": "um you can actually still have other processes running like other pods",
    "start": "1260539",
    "end": "1265940"
  },
  {
    "text": "impacting the CPU performance of your your software um but what exactly is contact switching",
    "start": "1265940",
    "end": "1273320"
  },
  {
    "text": "you might not have a background in this and why that's important um context switching is number one it's",
    "start": "1273320",
    "end": "1280760"
  },
  {
    "start": "1277000",
    "end": "1277000"
  },
  {
    "text": "a really expensive thing um that your your operating system and schedulers generally do",
    "start": "1280760",
    "end": "1287179"
  },
  {
    "text": "um to understand it basically there's these kind of two concepts that a lot of people often conflate um concurrency and",
    "start": "1287179",
    "end": "1294500"
  },
  {
    "text": "parallelism and so parallelism is actually when you have two different processes and they're running completely",
    "start": "1294500",
    "end": "1301400"
  },
  {
    "text": "um separate tasks um entirely independently um so uh that means that basically these",
    "start": "1301400",
    "end": "1308539"
  },
  {
    "text": "are two completely they can the workloads cannot affect each other because you actually have two workers",
    "start": "1308539",
    "end": "1314140"
  },
  {
    "text": "fundamentally running two different jobs um but concurrency is actually what",
    "start": "1314140",
    "end": "1321799"
  },
  {
    "text": "um you'll see in a lot of systems um specifically modern computers do both",
    "start": "1321799",
    "end": "1328039"
  },
  {
    "text": "concurrency and parallelism but if you're running on a single core machine for example um you can still run multiple programs",
    "start": "1328039",
    "end": "1334280"
  },
  {
    "text": "like a very a long time ago um computers could only run one program at the same time and they got fast",
    "start": "1334280",
    "end": "1340100"
  },
  {
    "text": "enough that they could do um what's called multitasking and multitasking is concurrency",
    "start": "1340100",
    "end": "1346520"
  },
  {
    "text": "basically it runs one program for a short amount of time and then it swaps",
    "start": "1346520",
    "end": "1351620"
  },
  {
    "text": "to running another program for a short amount of time and then it swaps back to running another program for another",
    "start": "1351620",
    "end": "1357020"
  },
  {
    "text": "short amount of time and these swaps while where it is actually like stopping",
    "start": "1357020",
    "end": "1363200"
  },
  {
    "text": "the execution of a program kind of saving the current state of the world restoring state of the world of another",
    "start": "1363200",
    "end": "1369679"
  },
  {
    "text": "program and then continuing to execute that second program that process is",
    "start": "1369679",
    "end": "1374780"
  },
  {
    "text": "called a context switch um sorry that's a cuckoo clock um and so context switches are super",
    "start": "1374780",
    "end": "1381799"
  },
  {
    "text": "expensive um and if we look on a very very deep level at like your your processor the",
    "start": "1381799",
    "end": "1388100"
  },
  {
    "text": "actual CPU executing this code um it actually has to do a lot of work it has to basically save those contacts",
    "start": "1388100",
    "end": "1396080"
  },
  {
    "text": "and restore the contacts but um what ends up happening too is that um",
    "start": "1396080",
    "end": "1401780"
  },
  {
    "text": "basically the CPU cache like a lot of the um memory that's inside the CPU",
    "start": "1401780",
    "end": "1407960"
  },
  {
    "text": "optimizing um the throughput of the CPU is going to be invalidated because now it's working",
    "start": "1407960",
    "end": "1413120"
  },
  {
    "text": "with a completely different memory it's working on a completely different problem um a lot of the predictions that you",
    "start": "1413120",
    "end": "1419299"
  },
  {
    "text": "would get uh a lot of your performance on your CPU um like a lot of that information just",
    "start": "1419299",
    "end": "1425059"
  },
  {
    "text": "has to be completely cleared as it swaps to the next thing um and not only is it kind of like at",
    "start": "1425059",
    "end": "1431059"
  },
  {
    "text": "the uh very much so Hardware level but at the like higher level the scheduler that's managing swapping between these",
    "start": "1431059",
    "end": "1438320"
  },
  {
    "text": "things that has to do a bunch of bookkeeping around it um oftentimes there's kind of rules around how things should be scheduled to",
    "start": "1438320",
    "end": "1445520"
  },
  {
    "text": "get fairness um and uh that kind of leads into the two different kinds of scheduling",
    "start": "1445520",
    "end": "1451940"
  },
  {
    "text": "there's cooperative and preemptive scheduling um Cooperative scheduling is uh",
    "start": "1451940",
    "end": "1457039"
  },
  {
    "text": "typically when the systems the two processes are working in tandem with each other or so",
    "start": "1457039",
    "end": "1463340"
  },
  {
    "text": "um you only yield to run another thing once you've basically got into a",
    "start": "1463340",
    "end": "1469159"
  },
  {
    "text": "stopping point and you say Hey Okay scheduler I'm actually ready to be paused here this is a safe place for me",
    "start": "1469159",
    "end": "1474860"
  },
  {
    "text": "to be paused and then you can run some other work um and basically uh by providing that",
    "start": "1474860",
    "end": "1482419"
  },
  {
    "text": "signal that's cooperating with the overall system um this is a pretty common system",
    "start": "1482419",
    "end": "1490159"
  },
  {
    "text": "um but it does have trade-offs um you can have basically uh processes that",
    "start": "1490159",
    "end": "1495740"
  },
  {
    "text": "never yield or very aggressive about not yielding to another process and consume",
    "start": "1495740",
    "end": "1501020"
  },
  {
    "text": "as much time as they want um so that way you can actually starve out other processes and this is really",
    "start": "1501020",
    "end": "1507080"
  },
  {
    "text": "where you start to see um kind of like the quote-unquote fairness of the scheduler being invalidated and",
    "start": "1507080",
    "end": "1516080"
  },
  {
    "text": "you start to see performance implications on processes that are not actually getting the time that they need",
    "start": "1516080",
    "end": "1521120"
  },
  {
    "text": "scheduled um and then the other one uh which my head is actually blocking a little bit",
    "start": "1521120",
    "end": "1526700"
  },
  {
    "text": "here is preemptive scheduling so this is actually a triggered basically at meantime",
    "start": "1526700",
    "end": "1532820"
  },
  {
    "text": "um so that means the context switch any point in time uh the overall scheduler can say okay I'm done running this I'm",
    "start": "1532820",
    "end": "1539059"
  },
  {
    "text": "going to pause it for a second and then swap over to another thing these systems have to be designed to be robust and",
    "start": "1539059",
    "end": "1544760"
  },
  {
    "text": "kind of like aware of the fact that they can get preempted at any time um but it does get a little bit more",
    "start": "1544760",
    "end": "1550220"
  },
  {
    "text": "control over uh basically guarantees around Fair scheduling",
    "start": "1550220",
    "end": "1555320"
  },
  {
    "text": "um so all that aside uh the question becomes",
    "start": "1555320",
    "end": "1561200"
  },
  {
    "text": "um how can I actually uh deal with these context swaps I don't want",
    "start": "1561200",
    "end": "1566840"
  },
  {
    "text": "um another process running on my node to affect my latency system um and it turns out there's actually one",
    "start": "1566840",
    "end": "1573080"
  },
  {
    "text": "feature left in kubernetes that's really critical to solving this exact problem um it's actually not one that's talked",
    "start": "1573080",
    "end": "1579620"
  },
  {
    "text": "about a lot and it's kind of more obscure and so I think this is actually going to be probably the thing that",
    "start": "1579620",
    "end": "1584919"
  },
  {
    "text": "surprises most people or most people are unfamiliar with however they may not",
    "start": "1584919",
    "end": "1590059"
  },
  {
    "text": "have arrived at a lot of the same conclusions we have for kind of like the trade-offs of adopting some of these",
    "start": "1590059",
    "end": "1595700"
  },
  {
    "text": "other Solutions um and that is the static CPU manager",
    "start": "1595700",
    "end": "1600860"
  },
  {
    "start": "1597000",
    "end": "1597000"
  },
  {
    "text": "policy um so this was actually made stable in kubernetes 1.26 and what this does is it",
    "start": "1600860",
    "end": "1609260"
  },
  {
    "text": "basically lets you if you create a request and limit",
    "start": "1609260",
    "end": "1615260"
  },
  {
    "text": "um for a pod CPU and you do it with this mode",
    "start": "1615260",
    "end": "1622340"
  },
  {
    "text": "configured with static CPU management configured if you use whole numbers like",
    "start": "1622340",
    "end": "1627440"
  },
  {
    "text": "dedicate a whole core it actually will give exclusive access to that process to",
    "start": "1627440",
    "end": "1633500"
  },
  {
    "text": "that core that means it's mapping it to a physical core and it's going to stay on that physical core that means no one",
    "start": "1633500",
    "end": "1640100"
  },
  {
    "text": "else is going to be able to use that physical core it's not going to context switch another process onto that core so",
    "start": "1640100",
    "end": "1647120"
  },
  {
    "text": "this actually gives you back a lot of those guarantees you're not going to get nearly as much of kind of like the Noisy",
    "start": "1647120",
    "end": "1653179"
  },
  {
    "text": "Neighbor problems because you actually have guaranteed Hardware now um there are like some kind of caveats",
    "start": "1653179",
    "end": "1660980"
  },
  {
    "text": "to note with this you can't just allocate all of your cores on a node as these exclusive cores because there are",
    "start": "1660980",
    "end": "1668000"
  },
  {
    "text": "still processes running on the system outside of kubernetes and the couplet",
    "start": "1668000",
    "end": "1673159"
  },
  {
    "text": "itself so kind of these system resources is need something basically to run on",
    "start": "1673159",
    "end": "1679279"
  },
  {
    "text": "and that's basically also provided as a flag and you need to allocate at least",
    "start": "1679279",
    "end": "1684860"
  },
  {
    "text": "one CPU for that um but you can also allocate more by configuring it with that flag",
    "start": "1684860",
    "end": "1691279"
  },
  {
    "text": "um and then kind of like the major trade-off here is that like these whole number of integers you can't schedule things smaller than that because that",
    "start": "1691279",
    "end": "1697940"
  },
  {
    "text": "would be sharing a particular core and that just arrives right back at the whole problem of context switching and",
    "start": "1697940",
    "end": "1703279"
  },
  {
    "text": "other processes running on that exact core so now you're kind of beholden to",
    "start": "1703279",
    "end": "1708440"
  },
  {
    "text": "restricting your your workloads to very very specific numbers of cores and whole",
    "start": "1708440",
    "end": "1714620"
  },
  {
    "text": "numbers of cores but this also has a major thing um",
    "start": "1714620",
    "end": "1720440"
  },
  {
    "text": "and its name gives it away it's static but we need Dynamic users are going to be provisioning clusters here and we're",
    "start": "1720440",
    "end": "1728480"
  },
  {
    "text": "not going to be able to we need to react to that we'll eventually run out of cores available and then then what",
    "start": "1728480",
    "end": "1734539"
  },
  {
    "text": "happens on our system um so with that there is kind of Auto",
    "start": "1734539",
    "end": "1740840"
  },
  {
    "text": "scaling functionality in kubernetes and this is what actually fills the Gap um we are big fans of this AWS project",
    "start": "1740840",
    "end": "1748760"
  },
  {
    "text": "which is totally open source and it is a kubernetes node Auto scaler called Carpenter",
    "start": "1748760",
    "end": "1753860"
  },
  {
    "text": "um the super cool thing with this is that it basically adds just in time capacity so that means that um we can",
    "start": "1753860",
    "end": "1762620"
  },
  {
    "text": "fully Leverage The static CPU mapping and when we create a new pod that needs",
    "start": "1762620",
    "end": "1769460"
  },
  {
    "text": "to be scheduled Carpenter is going to look at the results of the kubernetes scheduler and this kubernetes scheduler",
    "start": "1769460",
    "end": "1776000"
  },
  {
    "text": "is going to say I can't find a CPU to actually schedule this on um and when it does it's going to",
    "start": "1776000",
    "end": "1782299"
  },
  {
    "text": "actually provision a new virtual machine on our cloud provider like a new node that will have those cores available to",
    "start": "1782299",
    "end": "1789559"
  },
  {
    "text": "them within the limits that we configure for the size of the cluster be scalable um and then those uh that deployment of",
    "start": "1789559",
    "end": "1798200"
  },
  {
    "text": "spice DB can have the cores allocated directly for it so this kind of gives us the flexibility of expanding and",
    "start": "1798200",
    "end": "1805760"
  },
  {
    "text": "Contracting based on the workloads that we have scheduled but also the actual full dedication of those cores to our",
    "start": "1805760",
    "end": "1812240"
  },
  {
    "text": "workloads and if we run out of these cores we can provision more cores specifically dedicated just to this",
    "start": "1812240",
    "end": "1818179"
  },
  {
    "text": "workload um there are uh there are different like kind of cross Cloud",
    "start": "1818179",
    "end": "1824120"
  },
  {
    "text": "um alternatives to this uh so if you're on Google cloud gke has autopilot if",
    "start": "1824120",
    "end": "1829399"
  },
  {
    "text": "you're on Azure uh AKs has a cluster Auto scaler um the nice thing about uh Carpenter is",
    "start": "1829399",
    "end": "1835279"
  },
  {
    "text": "it's actually open source um and it has the ability uh for folks to implement backends eventually I",
    "start": "1835279",
    "end": "1841279"
  },
  {
    "text": "expect Carpenter to be fully fleshed out and support um both uh gke and AKs and that would",
    "start": "1841279",
    "end": "1848120"
  },
  {
    "text": "basically mean you have one kind of unified single way of kind of doing this Auto",
    "start": "1848120",
    "end": "1854000"
  },
  {
    "text": "scaling that's Cloud agnostic rather than kind of working with the specific",
    "start": "1854000",
    "end": "1859760"
  },
  {
    "text": "apis across the different Cloud providers so um it's really nice",
    "start": "1859760",
    "end": "1865100"
  },
  {
    "text": "um eks doesn't actually have a checkbox inside of Amazon for you to just enable auto scaling so this is their Cloud",
    "start": "1865100",
    "end": "1872360"
  },
  {
    "text": "solution and I'm actually really glad that that's the case because eventually we're going to get this neutral way of",
    "start": "1872360",
    "end": "1878120"
  },
  {
    "text": "actually doing Auto scaling cross-cloud um so with that that's been our journey",
    "start": "1878120",
    "end": "1884120"
  },
  {
    "text": "these are the super critical um bits of kubernetes that we think are vital to running low latency systems",
    "start": "1884120",
    "end": "1891260"
  },
  {
    "text": "um I'd like to give shout outs to our team at all said um who are the folks that largely have discovered and like work through this",
    "start": "1891260",
    "end": "1897799"
  },
  {
    "text": "process um Brad Eisen Evan Cordell and Victor um basically these folks are the ones",
    "start": "1897799",
    "end": "1903980"
  },
  {
    "text": "that have provisioned the infrastructure measured the infrastructure ran benchmarks Dove deep into kubernetes to",
    "start": "1903980",
    "end": "1911419"
  },
  {
    "text": "to understand exactly how all these Primitives are working so that we can understand the trade-offs um there's also this very helpful medium",
    "start": "1911419",
    "end": "1917960"
  },
  {
    "text": "article that kind of guided us at the very beginning of our journey and it basically is",
    "start": "1917960",
    "end": "1923779"
  },
  {
    "text": "um talking about like how folks just need to learn um the the actual implications of these",
    "start": "1923779",
    "end": "1929120"
  },
  {
    "text": "Primitives and use them properly because I think folks often just look at resource requests and limits and kind of",
    "start": "1929120",
    "end": "1936080"
  },
  {
    "text": "assume uh that these things are going to work for them and they're very simple but actually there's a lot of deep in",
    "start": "1936080",
    "end": "1941840"
  },
  {
    "text": "applications for adopting these such as the performance hit that we saw just by enabling CPU limits",
    "start": "1941840",
    "end": "1948980"
  },
  {
    "text": "so that concludes this talk if you have any questions at all with regards to",
    "start": "1948980",
    "end": "1954679"
  },
  {
    "text": "this topic with regards to space DB um just kubernetes in general",
    "start": "1954679",
    "end": "1960340"
  },
  {
    "text": "feel free to reach out to me via email but then we also have a community",
    "start": "1960340",
    "end": "1966320"
  },
  {
    "text": "Discord and you can see that URL at the bottom of the contact as well this is",
    "start": "1966320",
    "end": "1971360"
  },
  {
    "text": "primarily the community that does development and uses spice DB but we",
    "start": "1971360",
    "end": "1976580"
  },
  {
    "text": "also talk about generic distributed systems Concepts running a software on top of kubernetes operators development",
    "start": "1976580",
    "end": "1983179"
  },
  {
    "text": "on code that integrates deeply with kubernetes and then obviously as spice",
    "start": "1983179",
    "end": "1989000"
  },
  {
    "text": "DB is a low latency system we're often working with folks running space DV on their own Hardware or Cloud",
    "start": "1989000",
    "end": "1995480"
  },
  {
    "text": "configurations and helping them kind of discover The Primitives available to uh to them to optimize that experience",
    "start": "1995480",
    "end": "2002620"
  },
  {
    "text": "running low latency system and with that I'd like to thank you for",
    "start": "2002620",
    "end": "2008559"
  },
  {
    "text": "watching um and feel free to contact me at any point in time in the future thanks bye",
    "start": "2008559",
    "end": "2016260"
  }
]