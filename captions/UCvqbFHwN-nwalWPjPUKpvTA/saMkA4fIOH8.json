[
  {
    "start": "0",
    "end": "69000"
  },
  {
    "text": "please hold your questions till the end and don't forget to leave feedback on skate calm thank you I was gonna have",
    "start": "110",
    "end": "9420"
  },
  {
    "text": "Dan come up on the stage so that we could see each of our names but yeah yeah I'm Ellis I work at Google my main",
    "start": "9420",
    "end": "17430"
  },
  {
    "text": "job is on Google's managed cloud platform but I spend all my passion and energy on open source which is this",
    "start": "17430",
    "end": "23640"
  },
  {
    "text": "project my name is Dan I work at the",
    "start": "23640",
    "end": "32040"
  },
  {
    "text": "resins platform at the boom burg I'm also passionate for open source and also",
    "start": "32040",
    "end": "37460"
  },
  {
    "text": "in supermans to solve data science problems cool so chaos Irving is a new project in",
    "start": "37460",
    "end": "45030"
  },
  {
    "text": "the coop flow ecosystem ku flow you probably are aware of right now if you're coming to the ml track at coop",
    "start": "45030",
    "end": "51420"
  },
  {
    "text": "con but if you're not coop grows or could flow as a project in in the",
    "start": "51420",
    "end": "56610"
  },
  {
    "text": "kubernetes ecosystem for training managing deploying models on kubernetes",
    "start": "56610",
    "end": "62579"
  },
  {
    "text": "and KF serving is the serving story for coop flow so if you have a model what is",
    "start": "62579",
    "end": "71700"
  },
  {
    "start": "69000",
    "end": "69000"
  },
  {
    "text": "a model typically a data data scientist takes a bunch of data that they have and they write some model code maybe that's",
    "start": "71700",
    "end": "78090"
  },
  {
    "text": "tensorflow or pi/2 or XG boost they're different frameworks sometimes it's just some raw Python code and you take you",
    "start": "78090",
    "end": "84720"
  },
  {
    "text": "take your algorithm your model and you train it against some data and you eventually end up you're so vast oversimplification but eventually end up",
    "start": "84720",
    "end": "91920"
  },
  {
    "text": "with a some sort of asset that can be served for tensorflow this is a serialized asset called a SAV model",
    "start": "91920",
    "end": "98670"
  },
  {
    "text": "proto different frameworks have different serialization schemas but as a",
    "start": "98670",
    "end": "105000"
  },
  {
    "text": "data scientist you end up with this resource that you then need to figure out okay how do I start sending traffic to this thing and some data scientists",
    "start": "105000",
    "end": "113909"
  },
  {
    "text": "have started to come to kubernetes and say okay well kubernetes is good at surveying and I need to serve how do I",
    "start": "113909",
    "end": "119490"
  },
  {
    "text": "do that and this is what they need to learn",
    "start": "119490",
    "end": "123799"
  },
  {
    "text": "it's really scary kubernetes is incredibly scary to data scientists how",
    "start": "124759",
    "end": "130050"
  },
  {
    "text": "many of you here are data scientists and how many of you here are ops or",
    "start": "130050",
    "end": "135180"
  },
  {
    "text": "engineers or infrastructure people yeah so kubernetes okay how many of you are scared of kubernetes so this is your",
    "start": "135180",
    "end": "146099"
  },
  {
    "text": "domain you understand it kind of well but for data scientists it's very very terrifying and it's not even their job",
    "start": "146099",
    "end": "152880"
  },
  {
    "text": "right they need to focus on new algorithms new frameworks reading papers going to ml conferences not",
    "start": "152880",
    "end": "159000"
  },
  {
    "text": "infrastructure conferences these fields are also both changing incredibly quickly which is really really",
    "start": "159000",
    "end": "164700"
  },
  {
    "text": "challenging to stay on top of both so this is where KF serving it comes in we try to take all of the complexity",
    "start": "164700",
    "end": "170760"
  },
  {
    "start": "169000",
    "end": "169000"
  },
  {
    "text": "around infrastructure and package it up into a very very simple consumable interface for data scientists data",
    "start": "170760",
    "end": "178260"
  },
  {
    "text": "scientists our customer that means we're solving problems for them not infrastructure people it's an",
    "start": "178260",
    "end": "183390"
  },
  {
    "text": "infrastructure project and so infrastructure is always involved there's some handoff without some relationship there but as we build our",
    "start": "183390",
    "end": "189150"
  },
  {
    "text": "interface and as we build our features were focused on meeting data scientists where they are we're trying to build a",
    "start": "189150",
    "end": "195630"
  },
  {
    "text": "consistent experience for deploying models of any kind whether it's PI torch or XT boost we don't care we don't have",
    "start": "195630",
    "end": "201510"
  },
  {
    "text": "any opinions we want to support all of the popular frameworks we want a really simple use case but we want it to be",
    "start": "201510",
    "end": "207780"
  },
  {
    "text": "extensible as well that means on day 100 or a year in once your model is mission-critical you don't we don't want",
    "start": "207780",
    "end": "214170"
  },
  {
    "text": "to have our users have that moment of oh no how do I add Network policy that should just work we also don't think",
    "start": "214170",
    "end": "221400"
  },
  {
    "text": "that modern production features that you've kind of see with kubernetes like canary deployments tracing etc we don't",
    "start": "221400",
    "end": "227940"
  },
  {
    "text": "think that ml workloads shouldn't have those features because ml is so complicated how could we also do tracing",
    "start": "227940",
    "end": "233489"
  },
  {
    "text": "so we try and enable make sure that that story is complete a little bit of",
    "start": "233489",
    "end": "239790"
  },
  {
    "text": "history about the project some of you might have been to the talk by Tommy Lee and Animesh Singh on server las'",
    "start": "239790",
    "end": "246269"
  },
  {
    "text": "inferencing last year in Seattle I was sitting in the audience and I was kind of paying attention and kind of typing",
    "start": "246269",
    "end": "252209"
  },
  {
    "text": "away on a CRD I was playing around with and I was implementing a interface that was translating machine",
    "start": "252209",
    "end": "258410"
  },
  {
    "text": "learning models or the frameworks for serving machine learning models on 2k native resources and here come Tommy Lee",
    "start": "258410",
    "end": "265520"
  },
  {
    "text": "and animus Singh and this they're giving a talk on how to do that I then a couple",
    "start": "265520",
    "end": "271010"
  },
  {
    "text": "weeks later got introduced to Dan son who was doing the exact same thing at Bloomberg and Microsoft was doing a",
    "start": "271010",
    "end": "276860"
  },
  {
    "text": "similar thing and I thought okay god we've got a stop let's talk about what we're doing talk about what we're trying",
    "start": "276860",
    "end": "282320"
  },
  {
    "text": "to achieve and see if we can work together on this all of these projects were an open source but they weren't",
    "start": "282320",
    "end": "288680"
  },
  {
    "text": "really compatible with each other we weren't really paying attention to each other so we founded this working group that we called KF serving and we started",
    "start": "288680",
    "end": "296600"
  },
  {
    "text": "to develop an interface and a system that met all of our customers needs so we have a huge a diverse set of",
    "start": "296600",
    "end": "303380"
  },
  {
    "text": "customers at every set of companies and we're trying to solve the model serving problem once in a way that's general COO",
    "start": "303380",
    "end": "311930"
  },
  {
    "start": "311000",
    "end": "311000"
  },
  {
    "text": "flow was a really awesome community to help us build this coop flow has some really amazing principles around open",
    "start": "311930",
    "end": "318230"
  },
  {
    "text": "governance and open community it was really easy for people to trust coop flow as a standard and is a good place",
    "start": "318230",
    "end": "325250"
  },
  {
    "text": "to go develop ml on kubernetes it was it wasn't even that hard of a self people just started showing up to our weekly",
    "start": "325250",
    "end": "331220"
  },
  {
    "text": "meetings and before long we had a technical steering committee from five different companies the ones you see",
    "start": "331220",
    "end": "336230"
  },
  {
    "text": "here and we were having weekly discussions on how to develop AF serving",
    "start": "336230",
    "end": "341950"
  },
  {
    "start": "342000",
    "end": "342000"
  },
  {
    "text": "we started off with an interface this is I think one of the most important pieces",
    "start": "342430",
    "end": "347810"
  },
  {
    "text": "to making that day one experience for data scientists really easy here's three examples of our CRD one for scikit-learn",
    "start": "347810",
    "end": "355160"
  },
  {
    "text": "one for one for tensorflow and one for pi torch you can see that",
    "start": "355160",
    "end": "360290"
  },
  {
    "text": "this will get you three different applications each with an HTTP endpoint and each one serving a model of a",
    "start": "360290",
    "end": "367220"
  },
  {
    "text": "different framework in each of these examples there's a storage your eye that points somewhere in the cloud in this",
    "start": "367220",
    "end": "373130"
  },
  {
    "text": "case Google Cloud Storage to a serialized asset one for SK learn one",
    "start": "373130",
    "end": "378320"
  },
  {
    "text": "for tensorflow and one for pi torch the experience is consistent no matter what your framework is and there's a the the",
    "start": "378320",
    "end": "385970"
  },
  {
    "text": "serialization format is always different but you have a very common experience for each of these you might notice that in the PI torch",
    "start": "385970",
    "end": "392360"
  },
  {
    "text": "model there's an extra field called model class name we have these concepts of framework specs so there's a tensor",
    "start": "392360",
    "end": "399110"
  },
  {
    "text": "flow spec a PI torch spec and they're common enough in that they have the same specification for storage your eye",
    "start": "399110",
    "end": "405050"
  },
  {
    "text": "things like kubernetes resource requests etc but any extra fields that they need",
    "start": "405050",
    "end": "410210"
  },
  {
    "text": "for example tensorflow batching or pi torch class name we also wire those through the spec if",
    "start": "410210",
    "end": "419270"
  },
  {
    "text": "we dig a little bit deeper into this example and in here you have kind of the day one I just want an endpoint that",
    "start": "419270",
    "end": "425210"
  },
  {
    "text": "serves and here you want to actually start to exhibit some control over it we",
    "start": "425210",
    "end": "430789"
  },
  {
    "text": "start off with the the first extension which is a service account potentially you want to authenticate your request to",
    "start": "430789",
    "end": "436310"
  },
  {
    "text": "Google Cloud Storage because you don't want it to be public and you need an identity running on the container we",
    "start": "436310",
    "end": "441470"
  },
  {
    "text": "allow you to pass in a kubernetes identity that we mount on the container and it just wires up magically min and",
    "start": "441470",
    "end": "448009"
  },
  {
    "text": "max replicas are a way to control your provisioning to make sure that you don't drop requests and that you can meet demand while not overspending and",
    "start": "448009",
    "end": "454750"
  },
  {
    "text": "resource requests are obviously very important we have some defaults but most of the time you want to customize this",
    "start": "454750",
    "end": "461000"
  },
  {
    "text": "for your model we even allowed GPUs through this interface the second half",
    "start": "461000",
    "end": "466159"
  },
  {
    "text": "of the spec is a somewhat controversial but I think really powerful piece of KF serving if you think about how to do",
    "start": "466159",
    "end": "472279"
  },
  {
    "text": "canary and this is our canary mechanism how to do canary as an ops engineer typically you would",
    "start": "472279",
    "end": "479419"
  },
  {
    "text": "create multiple kubernetes deployments maybe set up a nist EO virtual service and route between them other systems",
    "start": "479419",
    "end": "485900"
  },
  {
    "text": "like k native allow you a separate interface that's fairly similar to do traffic splitting and canary deployments",
    "start": "485900",
    "end": "491840"
  },
  {
    "text": "but that can be really challenging especially for data scientists we've we basically simplified the n way traffic",
    "start": "491840",
    "end": "499279"
  },
  {
    "text": "split into a two way traffic split with the assumption that when doing any sort of deployment you only really need a new",
    "start": "499279",
    "end": "505819"
  },
  {
    "text": "and an old and that's exactly what this mechanism is built for if you use just the default you get a standard Bluegreen",
    "start": "505819",
    "end": "512270"
  },
  {
    "text": "deployment as most of you are familiar with with the kubernetes deployment resource if you do a canary with present",
    "start": "512270",
    "end": "518539"
  },
  {
    "text": "0 you get what we call a pinned mode where you have an addressable primary and",
    "start": "518539",
    "end": "523940"
  },
  {
    "text": "addressable default an addressable canary end point and that allows you to",
    "start": "523940",
    "end": "529400"
  },
  {
    "text": "send traffic or experimental traffic to the new endpoint but customer traffic isn't sent there yet",
    "start": "529400",
    "end": "534800"
  },
  {
    "text": "and then as you and the third option is canary which is this percentage here in",
    "start": "534800",
    "end": "539900"
  },
  {
    "text": "between the two specs and that allows users to or that allows deployers to",
    "start": "539900",
    "end": "545930"
  },
  {
    "text": "slowly increment the traffic to transparently pass request to the canary deployment in this example we're",
    "start": "545930",
    "end": "552320"
  },
  {
    "text": "experimenting with an iris v2 instead of iris and as we slowly increment this canary for traffic percent we can gain",
    "start": "552320",
    "end": "559700"
  },
  {
    "text": "confidence that our deployment is not going to break our users eventually we'd go to a hundred flip the canary and",
    "start": "559700",
    "end": "565640"
  },
  {
    "text": "default and then delete the canary afterwards that sounds like a lot of",
    "start": "565640",
    "end": "573350"
  },
  {
    "start": "571000",
    "end": "571000"
  },
  {
    "text": "complicated functionality and it is and I certainly don't want to implement all of it one of the most amazing pieces",
    "start": "573350",
    "end": "579830"
  },
  {
    "text": "about kubernetes is that all of these is that kubernetes itself is very extensible but so are the layers that",
    "start": "579830",
    "end": "586310"
  },
  {
    "text": "are built on top of kubernetes many of you are probably familiar with sto and Kay native which is which are projects",
    "start": "586310",
    "end": "592280"
  },
  {
    "text": "that we've chosen to be our foundational layers for server lists and networking KF serving is focused just on the",
    "start": "592280",
    "end": "599420"
  },
  {
    "text": "machine learning problems so when a data scientist comes to us and they say I need this network policy thing my ops",
    "start": "599420",
    "end": "605420"
  },
  {
    "text": "team is demanding this before we can serve this in production or oh my my auto scaling is too slow or my auto",
    "start": "605420",
    "end": "611810"
  },
  {
    "text": "scaling isn't responsive enough we don't have to implement that we can work with those data scientists move down the",
    "start": "611810",
    "end": "617750"
  },
  {
    "text": "stack and talk to our friends at K native and sto to implement those features that allows those features to",
    "start": "617750",
    "end": "623930"
  },
  {
    "text": "reach a higher purpose and also make sure that K F cert that that we don't reinvent the wheel at K F serving ml",
    "start": "623930",
    "end": "634370"
  },
  {
    "start": "631000",
    "end": "631000"
  },
  {
    "text": "surface meshes why do we need a service mesh for ml well like we said we think that modern production functionality is",
    "start": "634370",
    "end": "640490"
  },
  {
    "text": "necessary for all work loads including ml a service mesh is typically implemented as a proxy that's dropped",
    "start": "640490",
    "end": "646880"
  },
  {
    "text": "into every workload that proxy controls all traffic in and out of the pot and allows you to transparently implement a",
    "start": "646880",
    "end": "653180"
  },
  {
    "text": "whole bunch of features like tracing authentication HTTP upgrading Network policy all of those",
    "start": "653180",
    "end": "660530"
  },
  {
    "text": "things are maybe not something that you'd want on day one but on day 100 when your ops team comes to you and",
    "start": "660530",
    "end": "666080"
  },
  {
    "text": "gives you these requirements you don't have to do anything you can keep your KF serving set up working and just start",
    "start": "666080",
    "end": "672710"
  },
  {
    "text": "applying sto policies and it works magically this is that layering you saw in the previous slide and it's really",
    "start": "672710",
    "end": "679310"
  },
  {
    "text": "critical for how a KF serving maintained its agility and provides these features to data scientists can ativ is the next",
    "start": "679310",
    "end": "687980"
  },
  {
    "start": "686000",
    "end": "686000"
  },
  {
    "text": "layer I don't know if any of you have worked with K native before but it's a fairly complicated resource model I",
    "start": "687980",
    "end": "693740"
  },
  {
    "text": "think it's the right resource model but it's also a little bit too much for data scientists most people only need their",
    "start": "693740",
    "end": "700940"
  },
  {
    "text": "top-level resource which is a service but sometimes you might need or rather",
    "start": "700940",
    "end": "706010"
  },
  {
    "text": "the the underlying resources are just kind of overly complicated so we've abstracted K native but we still get all",
    "start": "706010",
    "end": "712670"
  },
  {
    "text": "the benefits around routing and queuing the K native has the most valuable piece",
    "start": "712670",
    "end": "718010"
  },
  {
    "text": "of K native for us is the KPA or the request based autoscaler if you're familiar with kubernetes horizontal",
    "start": "718010",
    "end": "724420"
  },
  {
    "text": "autoscaler you're probably familiar that you can set a CPU or memory target and",
    "start": "724420",
    "end": "730460"
  },
  {
    "text": "once you exceed that target you start to scale up this doesn't work that well for a lot of mo workloads especially ones",
    "start": "730460",
    "end": "737150"
  },
  {
    "text": "using accelerators if you're not CPU bound and not memory bound how do you know to scale up you could do something",
    "start": "737150",
    "end": "743780"
  },
  {
    "text": "like Nvidia does which is you prove Nvidia provides duty cycle which is a GPU metric but that's effectively power",
    "start": "743780",
    "end": "750470"
  },
  {
    "text": "on the GPU and it's often the case that the ML model isn't using the full power of the GPU and so it's really hard to",
    "start": "750470",
    "end": "757010"
  },
  {
    "text": "figure out exactly what metric to set there the request-based autoscaler for however works really well there's a",
    "start": "757010",
    "end": "764480"
  },
  {
    "text": "concept of desired concurrency and actual concurrency and so from machine learning models desired concurrency we",
    "start": "764480",
    "end": "771890"
  },
  {
    "text": "just set to one which tends to work really well because ml models tend to max out the accelerator what that means",
    "start": "771890",
    "end": "779810"
  },
  {
    "text": "is that if a single pod has 10x concurrency because that's how the load",
    "start": "779810",
    "end": "785390"
  },
  {
    "text": "balancer has routed the requests we know that we need 10x the number of pods and that allows",
    "start": "785390",
    "end": "791300"
  },
  {
    "text": "to autoscale in a really snappy way and it happens to work really well for machine learning workloads without any",
    "start": "791300",
    "end": "797899"
  },
  {
    "text": "additional configuration you can always pass in extra parameters to change that",
    "start": "797899",
    "end": "802940"
  },
  {
    "text": "desired concurrency if you need but for almost all workloads we found that one works incredibly well and that's a",
    "start": "802940",
    "end": "809149"
  },
  {
    "text": "really powerful simplification for data scientists and our interface so what we",
    "start": "809149",
    "end": "815060"
  },
  {
    "text": "have here is we've taken these eight lines of llam√≥ this very simple interface that's not a container it's",
    "start": "815060",
    "end": "821510"
  },
  {
    "text": "just a stored asset it meets the data scientist exactly where they are and you get all the features production features",
    "start": "821510",
    "end": "827870"
  },
  {
    "text": "of ml and everything you need to scale into the future that was cane that was",
    "start": "827870",
    "end": "833690"
  },
  {
    "start": "832000",
    "end": "832000"
  },
  {
    "text": "KF serving vo dot one we're now on KF serving ODOT too and we've continued to",
    "start": "833690",
    "end": "838940"
  },
  {
    "text": "listen to data scientists and continued to look at the industry to see what we could simplify and what we could automate we came up with these two extra",
    "start": "838940",
    "end": "846529"
  },
  {
    "text": "concepts of transformer and explainer you may be familiar with the field of explanation which is where you send",
    "start": "846529",
    "end": "852320"
  },
  {
    "text": "multiple requests to a predictor and you perturb the input several times to see",
    "start": "852320",
    "end": "858079"
  },
  {
    "text": "what predictions come back when you look at all the results based off of the perturbed input you can make some",
    "start": "858079",
    "end": "864470"
  },
  {
    "text": "reasoning or you can reason about those results to figure out what parts of the input were most important in getting",
    "start": "864470",
    "end": "870980"
  },
  {
    "text": "this prediction it's a really cool field and there's another talk on this by Selden later they have a explainer",
    "start": "870980",
    "end": "877520"
  },
  {
    "text": "library called alibi but we're basically just an architectural component of an",
    "start": "877520",
    "end": "883220"
  },
  {
    "text": "explainer so we have an out-of-the-box alibi explainer implementation that can be configured in Y amyl or you can",
    "start": "883220",
    "end": "889700"
  },
  {
    "text": "provide your own container implementation of an explainer and we wire up the routing so that you just",
    "start": "889700",
    "end": "895310"
  },
  {
    "text": "drop it in in the spec and it wires up automatically we've also seen the concept of a transformer which is",
    "start": "895310",
    "end": "900770"
  },
  {
    "text": "incredibly important maybe you want to take a request clean it or pre process it or translate a user ID into a bunch",
    "start": "900770",
    "end": "907310"
  },
  {
    "text": "of features about the user you need some hooks somewhere and often machine learning frameworks don't give you a",
    "start": "907310",
    "end": "913010"
  },
  {
    "text": "place to execute that code we've implemented this hook in KF serving as what we call a transformer so the",
    "start": "913010",
    "end": "919370"
  },
  {
    "text": "request comes in we translate a user ID into a whole bunch of features about that user the explainer",
    "start": "919370",
    "end": "925460"
  },
  {
    "text": "then takes all those features perturbs them sends many prediction requests the",
    "start": "925460",
    "end": "930710"
  },
  {
    "text": "pretty hello the predictor then does predictions on",
    "start": "930710",
    "end": "937550"
  },
  {
    "text": "each of those requests and the end of the day you get back your explained transformed requests all of this is",
    "start": "937550",
    "end": "944870"
  },
  {
    "text": "automatically wired up for data scientists and we've seen that this pattern and these use cases can be reused in all sorts of scenarios now",
    "start": "944870",
    "end": "953839"
  },
  {
    "start": "953000",
    "end": "953000"
  },
  {
    "text": "we're gonna go to Bloomberg to see exactly what their use cases are and how they've leveraged this to build their",
    "start": "953839",
    "end": "959300"
  },
  {
    "text": "data science platform so next I'm going to talk about how care service you sow",
    "start": "959300",
    "end": "965510"
  },
  {
    "text": "our Bloomberg data sense platform so Bloomberg data science platform is for",
    "start": "965510",
    "end": "971149"
  },
  {
    "text": "Bloomberg's machine learning practioners and if it runs on a computed class with a GPU notes",
    "start": "971149",
    "end": "977020"
  },
  {
    "text": "Bloomberg has a sizable number of data science and AI teams so almost a lot of",
    "start": "977020",
    "end": "983300"
  },
  {
    "text": "the team that need to runs your infant service production so studying models on",
    "start": "983300",
    "end": "996680"
  },
  {
    "text": "production is not a unique problem to Bloomberg as Alice just mentioned come",
    "start": "996680",
    "end": "1003100"
  },
  {
    "text": "company virus different company the currently doing a different way to serve models and even within organizations",
    "start": "1003100",
    "end": "1010950"
  },
  {
    "text": "teams are doing basically various different biome itself and so we reach",
    "start": "1010950",
    "end": "1018670"
  },
  {
    "text": "out to community early this year and we talked about with Ellis and a few leaders and we started K of serving to",
    "start": "1018670",
    "end": "1026110"
  },
  {
    "text": "really address address all these issues by providing a standardized and outbox",
    "start": "1026110",
    "end": "1031360"
  },
  {
    "text": "model serving across different model frameworks and to complete a have a",
    "start": "1031360",
    "end": "1037209"
  },
  {
    "text": "complete story we also support a model experiment a and inference graph with a/b testing and multi-armed bandit and",
    "start": "1037209",
    "end": "1045459"
  },
  {
    "text": "Cave serving can be easily drawn our current existing key need evidence to stack oh that's a pretty in nature move",
    "start": "1045459",
    "end": "1054669"
  },
  {
    "start": "1053000",
    "end": "1053000"
  },
  {
    "text": "for us to so this is a diagram for typical",
    "start": "1054669",
    "end": "1059780"
  },
  {
    "text": "lifecycle for Bloomberg data scientists so there are scientists spend many",
    "start": "1059780",
    "end": "1067010"
  },
  {
    "text": "cycles to trigger mode of long as your local terminal environment to dev and to",
    "start": "1067010",
    "end": "1072660"
  },
  {
    "text": "proud to achieve proper and good model and and many of the data science actress",
    "start": "1072660",
    "end": "1079650"
  },
  {
    "text": "thinks oh I have a model now my job is done and this is operational people's",
    "start": "1079650",
    "end": "1085350"
  },
  {
    "text": "problem to please the problem about to production actually the last step is very critical because in order to",
    "start": "1085350",
    "end": "1092010"
  },
  {
    "text": "realize that all these the business values of the models we have to observe the model production to serve the real",
    "start": "1092010",
    "end": "1097080"
  },
  {
    "text": "production traffic however this is a very challenging task to a lot of data scientists",
    "start": "1097080",
    "end": "1102590"
  },
  {
    "text": "how many see however you think like a serving waters very difficult on production yes so we found out a lot of",
    "start": "1102590",
    "end": "1116400"
  },
  {
    "text": "time like a lot of models are not getting shipped to production for various reasons for some they they don't",
    "start": "1116400",
    "end": "1125100"
  },
  {
    "text": "know how to handle the production load wasted recover which required latency",
    "start": "1125100",
    "end": "1130559"
  },
  {
    "text": "and its throughput and for some some settings and months is to production",
    "start": "1130559",
    "end": "1136140"
  },
  {
    "text": "lies model and sometimes it's just a figure out they are not very confident",
    "start": "1136140",
    "end": "1142049"
  },
  {
    "text": "about their new model and so as our",
    "start": "1142049",
    "end": "1148020"
  },
  {
    "text": "infrastructure its we really want to take this responsibility to help speed",
    "start": "1148020",
    "end": "1153059"
  },
  {
    "text": "up the production lines they are inferences and as well as increase their",
    "start": "1153059",
    "end": "1158640"
  },
  {
    "text": "confidence to further when you go out and you model they build so in addition",
    "start": "1158640",
    "end": "1167700"
  },
  {
    "start": "1166000",
    "end": "1166000"
  },
  {
    "text": "to all the the problem in the complexity at is just mentioned we also for Bloomberg we also have list of long list",
    "start": "1167700",
    "end": "1175200"
  },
  {
    "text": "of for production requirements we want to support a production user with 24 hours in a 70s report support and we",
    "start": "1175200",
    "end": "1183090"
  },
  {
    "text": "want a safe robot in robot low-back Pro Street procedure and we want to have",
    "start": "1183090",
    "end": "1188940"
  },
  {
    "text": "established a beaut process for the image get deployed to production and to have a probe approval process for",
    "start": "1188940",
    "end": "1196200"
  },
  {
    "text": "all the inference service department once inferences it brought on production",
    "start": "1196200",
    "end": "1201400"
  },
  {
    "text": "we also need to have other logins tracing available for people to debug their problems too as well to monitor",
    "start": "1201400",
    "end": "1209980"
  },
  {
    "text": "the latency and a throughput we also need like a reliable alerting we invert",
    "start": "1209980",
    "end": "1216610"
  },
  {
    "text": "the inference service goes wrong cave serving comes into picture and it",
    "start": "1216610",
    "end": "1223960"
  },
  {
    "text": "actually helps us to adjust a lot of the problems on the list since caves ovens",
    "start": "1223960",
    "end": "1229450"
  },
  {
    "text": "building on the server laser framework it abstractor way adopted to the de",
    "start": "1229450",
    "end": "1235090"
  },
  {
    "text": "product complexity for us and it offers canary a lot strategy and it also",
    "start": "1235090",
    "end": "1244120"
  },
  {
    "text": "provides a very clean and Itakura to interface which is very easy for people to review and which answers question",
    "start": "1244120",
    "end": "1251320"
  },
  {
    "text": "what is actually deploy to production are you rolling out a new model or you change some configurations or changes",
    "start": "1251320",
    "end": "1257260"
  },
  {
    "text": "normally instances and case serving also",
    "start": "1257260",
    "end": "1264300"
  },
  {
    "text": "expose a set of standard metrics for latency reports and concurrency so which",
    "start": "1264300",
    "end": "1272830"
  },
  {
    "text": "is very will be a critical for us to based on those metrics to set up our alerting so as you can see cave serving",
    "start": "1272830",
    "end": "1283030"
  },
  {
    "start": "1280000",
    "end": "1280000"
  },
  {
    "text": "is a very critical piece on our platform architecture which is built on top of",
    "start": "1283030",
    "end": "1288310"
  },
  {
    "text": "kubernetes so we spend on our last two to two years to establish a solid",
    "start": "1288310",
    "end": "1294120"
  },
  {
    "text": "training infrastructure to between allow users to chain of models with all these",
    "start": "1294120",
    "end": "1299860"
  },
  {
    "text": "various different the frameworks and now there's a lot of need to ship those",
    "start": "1299860",
    "end": "1305320"
  },
  {
    "text": "models to attack production and KEF serving as a very critical piece to",
    "start": "1305320",
    "end": "1311520"
  },
  {
    "text": "provide a standardized model serving across all these different model frameworks and with production quality",
    "start": "1311520",
    "end": "1323400"
  },
  {
    "start": "1322000",
    "end": "1322000"
  },
  {
    "text": "next time ago is to talk about the Bromberg use case for",
    "start": "1323400",
    "end": "1328910"
  },
  {
    "text": "what the production inference so the most important use case is to we want to",
    "start": "1328910",
    "end": "1334190"
  },
  {
    "text": "standardize automotive serving across the model frameworks and also we want we",
    "start": "1334190",
    "end": "1339860"
  },
  {
    "text": "don't want each team to revenge the area where to build their own model server over the time that is not performance",
    "start": "1339860",
    "end": "1346700"
  },
  {
    "text": "and hazard and has it's very not like a transparent to what is actually going on",
    "start": "1346700",
    "end": "1353870"
  },
  {
    "text": "in those servers pre and post-processing is a very common step in the inference",
    "start": "1353870",
    "end": "1360980"
  },
  {
    "text": "and as you know Bloomberg SSL has a lot of natural language processing use cases",
    "start": "1360980",
    "end": "1366890"
  },
  {
    "text": "so text classification model explain ability is also very interesting to us to help us build a chest of or a eye",
    "start": "1366890",
    "end": "1373940"
  },
  {
    "text": "product we also wanted to do inference on news Kafka streamys where all the",
    "start": "1373940",
    "end": "1380840"
  },
  {
    "text": "news comes to Kafka and we wanted to really return in for instance a new stories we also request like a GPU",
    "start": "1380840",
    "end": "1390230"
  },
  {
    "text": "impressed with tensorflow and paramount to meet our latency and a support",
    "start": "1390230",
    "end": "1395300"
  },
  {
    "text": "requirement we are also very working with closely with community to build out",
    "start": "1395300",
    "end": "1401120"
  },
  {
    "text": "GPU sharing solution for us to improve the utilization of GPUs and we were",
    "start": "1401120",
    "end": "1408410"
  },
  {
    "text": "there's a need for a be testing as well so well goes through a few some pros",
    "start": "1408410",
    "end": "1415510"
  },
  {
    "start": "1412000",
    "end": "1412000"
  },
  {
    "text": "spec we use cases we are currently using so this example where we want to use a",
    "start": "1415510",
    "end": "1423410"
  },
  {
    "text": "cave settings transformer concept let's just talk about so kept serving offers a",
    "start": "1423410",
    "end": "1430330"
  },
  {
    "text": "SDK where you can easily allows to implement your own pre and post",
    "start": "1430330",
    "end": "1436250"
  },
  {
    "text": "processing so you extend the cave servings base class and you can hook up",
    "start": "1436250",
    "end": "1443780"
  },
  {
    "text": "you as your own pre and post processing handler once you you build in Mitch and",
    "start": "1443780",
    "end": "1449450"
  },
  {
    "text": "you can add the transformer to your inference service pack once you submit",
    "start": "1449450",
    "end": "1455030"
  },
  {
    "text": "to the inference service you wiii cave serving x-rays to microservice my name's transformer",
    "start": "1455030",
    "end": "1461539"
  },
  {
    "text": "and one as a predictor we may request comes in it will hit the transformer",
    "start": "1461539",
    "end": "1466879"
  },
  {
    "text": "first invokes your pre process handler transformer which we are all then call",
    "start": "1466879",
    "end": "1473269"
  },
  {
    "text": "out to predict her to get the prediction result back and again Nadeau invokes the",
    "start": "1473269",
    "end": "1480440"
  },
  {
    "text": "post process which often the time people use it to deliver a more user-friendly",
    "start": "1480440",
    "end": "1485990"
  },
  {
    "text": "response back to the end-user we separated a concept of transformer and",
    "start": "1485990",
    "end": "1491870"
  },
  {
    "text": "predictor for a good reason and in this way you can actually keep your chance mark code while you want you can like a",
    "start": "1491870",
    "end": "1498529"
  },
  {
    "text": "switch a prettier ways more performing the motor server for example like Onyx if you want and also you can like a",
    "start": "1498529",
    "end": "1508070"
  },
  {
    "text": "scarer transformer and particular differently you can even run transformer Brown CPU and particular Allen GPU if",
    "start": "1508070",
    "end": "1520460"
  },
  {
    "start": "1519000",
    "end": "1519000"
  },
  {
    "text": "you have a needs like the way to a beauty or sometimes it may be the out",
    "start": "1520460",
    "end": "1525649"
  },
  {
    "text": "box model service route on the certified need you can always exchange kept serving an SDK to improve your own",
    "start": "1525649",
    "end": "1532309"
  },
  {
    "text": "prediction functions which is a fairly straightforward and you build an image",
    "start": "1532309",
    "end": "1538250"
  },
  {
    "text": "and you can shovel that image on to the cave serving custom spec and then you",
    "start": "1538250",
    "end": "1544129"
  },
  {
    "text": "get the goodness of all the KF serving offers with auto scaling Canary routes",
    "start": "1544129",
    "end": "1549500"
  },
  {
    "text": "and and other you get all the metrics out of a box so model explanation is are",
    "start": "1549500",
    "end": "1562129"
  },
  {
    "start": "1557000",
    "end": "1557000"
  },
  {
    "text": "becoming more and more important nowadays on the production if it's all",
    "start": "1562129",
    "end": "1567710"
  },
  {
    "text": "races are so alibi is a open source with from certain for model explanation and",
    "start": "1567710",
    "end": "1576019"
  },
  {
    "text": "interpretation it is supposed support a multiple use cases such as tabular text",
    "start": "1576019",
    "end": "1581480"
  },
  {
    "text": "and image data it's a black box model acceleration algorithm which is a",
    "start": "1581480",
    "end": "1588409"
  },
  {
    "text": "increment based on the paper called anchor Maude model agnostic explanations this",
    "start": "1588409",
    "end": "1601120"
  },
  {
    "start": "1600000",
    "end": "1600000"
  },
  {
    "text": "is the tech alibi text extra inspiration to us because we have a lot of NLP use",
    "start": "1601120",
    "end": "1609310"
  },
  {
    "text": "case where we we can easily integrate with our in-house and a peer model",
    "start": "1609310",
    "end": "1614830"
  },
  {
    "text": "server sings alibi Texas already needs is to have accessibility to a prediction",
    "start": "1614830",
    "end": "1621340"
  },
  {
    "text": "URL now and you are at explain around to the inference service pack you can you",
    "start": "1621340",
    "end": "1627640"
  },
  {
    "text": "can actually get back the explanation results so when you call the explanation",
    "start": "1627640",
    "end": "1632770"
  },
  {
    "text": "and part you will get the explanation back English into the prediction result",
    "start": "1632770",
    "end": "1638560"
  },
  {
    "text": "so here on the right side you can see a example response where it finds the",
    "start": "1638560",
    "end": "1644500"
  },
  {
    "text": "anchor with a word freshy and which means like that's the world has to if the ad word pretend in your",
    "start": "1644500",
    "end": "1650860"
  },
  {
    "text": "text it will insures a prediction regardless of all of other words in the",
    "start": "1650860",
    "end": "1655900"
  },
  {
    "text": "text we also have a used case where a",
    "start": "1655900",
    "end": "1663280"
  },
  {
    "start": "1659000",
    "end": "1659000"
  },
  {
    "text": "lot of news come into Kafka and we want to have a inference pipeline where goes",
    "start": "1663280",
    "end": "1670150"
  },
  {
    "text": "through a set of models and doing the name detection and duplication and then",
    "start": "1670150",
    "end": "1677380"
  },
  {
    "text": "after the last step it will the result get shuffled on to out Kafka topic and",
    "start": "1677380",
    "end": "1684040"
  },
  {
    "text": "then which can be further consumed to for the downstream end clients on",
    "start": "1684040",
    "end": "1693880"
  },
  {
    "start": "1691000",
    "end": "1691000"
  },
  {
    "text": "country we are also making appetizing proposal to the community where we were",
    "start": "1693880",
    "end": "1699150"
  },
  {
    "text": "in conscious - a canary road where it helps attest to the serve your service",
    "start": "1699150",
    "end": "1705040"
  },
  {
    "text": "performance or hers but sometimes you may also want to test the model accuracy",
    "start": "1705040",
    "end": "1710920"
  },
  {
    "text": "with the real production traffic so here we want to want to introduce this",
    "start": "1710920",
    "end": "1716380"
  },
  {
    "text": "concept called inference router where you can do it determines the routing with control and experiment group so the",
    "start": "1716380",
    "end": "1722980"
  },
  {
    "text": "group the request of Ron contribute a lot to a model 1 and the request from the",
    "start": "1722980",
    "end": "1728720"
  },
  {
    "text": "experiment group our route to model - so and then you basically started a channel",
    "start": "1728720",
    "end": "1734090"
  },
  {
    "text": "where you can start to collect the actual user feedback from from your",
    "start": "1734090",
    "end": "1739970"
  },
  {
    "text": "prediction result and once you accumulate enough examples and then you",
    "start": "1739970",
    "end": "1746539"
  },
  {
    "text": "stone into some database and and you can do a stat use statistical a significant",
    "start": "1746539",
    "end": "1753980"
  },
  {
    "text": "test between the two models to decide whether you want to use the new model or",
    "start": "1753980",
    "end": "1759230"
  },
  {
    "text": "not so after we are able to choose a",
    "start": "1759230",
    "end": "1765380"
  },
  {
    "start": "1762000",
    "end": "1762000"
  },
  {
    "text": "part of these features finally we want to have a simple and reliable the",
    "start": "1765380",
    "end": "1771559"
  },
  {
    "text": "process for data scientists to deploy your inference service to production so",
    "start": "1771559",
    "end": "1776840"
  },
  {
    "text": "we wanted a size 2 poster the machine only source code and as in 14 we provide",
    "start": "1776840",
    "end": "1782539"
  },
  {
    "text": "a Orchestra CI CD pipeline to help them to build images and versioning and",
    "start": "1782539",
    "end": "1789880"
  },
  {
    "text": "produce the motors so the design state can then create a cave serving the",
    "start": "1789880",
    "end": "1796940"
  },
  {
    "text": "inference service department will get a repo to write a cave serving spec we just showed so as you can every time the",
    "start": "1796940",
    "end": "1804919"
  },
  {
    "text": "turret you can update the image version or if they're using some of the custom",
    "start": "1804919",
    "end": "1810049"
  },
  {
    "text": "cave serving or pre-process images and whenever they wrote new models it can",
    "start": "1810049",
    "end": "1816559"
  },
  {
    "text": "also update the model version on the spec so and people can review and then",
    "start": "1816559",
    "end": "1821840"
  },
  {
    "text": "after review it again with it well then get the product to our kubernetes",
    "start": "1821840",
    "end": "1827750"
  },
  {
    "text": "cluster so current status we have been",
    "start": "1827750",
    "end": "1834679"
  },
  {
    "text": "irani is Tok Neff stack for almost a year and we started with a function",
    "start": "1834679",
    "end": "1841190"
  },
  {
    "text": "survey zone and now we are actually migrated him to care serving and we are",
    "start": "1841190",
    "end": "1848210"
  },
  {
    "text": "working with very hard with the community to get caps of in Turku flow 1.0 we started having user trying out",
    "start": "1848210",
    "end": "1856970"
  },
  {
    "text": "I would ever cross that are giving us a feedback and meanwhile we are working on the production deployment process and",
    "start": "1856970",
    "end": "1863270"
  },
  {
    "text": "aiming to get this production already hopefully early next year all right",
    "start": "1863270",
    "end": "1869960"
  },
  {
    "start": "1869000",
    "end": "1869000"
  },
  {
    "text": "demo time so the demo is about how you can build em to end inference pipeline",
    "start": "1869960",
    "end": "1877900"
  },
  {
    "text": "with k of serving so the demo here is",
    "start": "1877900",
    "end": "1888640"
  },
  {
    "text": "famous like a mini example so what I want to do here is I have a bucket here",
    "start": "1888640",
    "end": "1895700"
  },
  {
    "text": "and I want to upload a digit and then once I get a response back see I have",
    "start": "1895700",
    "end": "1903170"
  },
  {
    "text": "zero to nine here so once I get there is a prediction result back it will",
    "start": "1903170",
    "end": "1908690"
  },
  {
    "text": "automatically put into this one with the packet so in order to do this I",
    "start": "1908690",
    "end": "1915130"
  },
  {
    "text": "basically implement a transformer where there's a pretty process and post",
    "start": "1915130",
    "end": "1922220"
  },
  {
    "text": "process so the pre process basically so once I upload to the image it will stand",
    "start": "1922220",
    "end": "1930440"
  },
  {
    "text": "a Kafka event to the to the mean your",
    "start": "1930440",
    "end": "1936170"
  },
  {
    "text": "publisher Kafka event and it will get delivered to the inference arias and every service work how this process",
    "start": "1936170",
    "end": "1944120"
  },
  {
    "text": "function to download an image and then to an image transform after you get to",
    "start": "1944120",
    "end": "1951140"
  },
  {
    "text": "the result back it will invoke this post process where it gets the raw prediction",
    "start": "1951140",
    "end": "1956540"
  },
  {
    "text": "and then it will find the right bucket and then I upload to that bucket so I",
    "start": "1956540",
    "end": "1969380"
  },
  {
    "text": "just so our charge with the product if so I keep serving from the notebook so",
    "start": "1969380",
    "end": "1974690"
  },
  {
    "text": "we have a SDK where you can specify the",
    "start": "1974690",
    "end": "1979810"
  },
  {
    "text": "transformer so you I build the image with that pre and post processing and I",
    "start": "1979810",
    "end": "1985940"
  },
  {
    "text": "put the image here in space because my crust are stingy so I over at the request our image and then I'm going to",
    "start": "1985940",
    "end": "1994570"
  },
  {
    "text": "create a specify predictor with tensorflow so I have a model chain on s3 bucket so I",
    "start": "1994570",
    "end": "2009310"
  },
  {
    "text": "then created a service yeah so I will",
    "start": "2009310",
    "end": "2016560"
  },
  {
    "text": "take a few seconds to have the service pin up Oh check out the parts so you see",
    "start": "2016560",
    "end": "2029320"
  },
  {
    "text": "you have responded to parts here so actually it's like a 2k native service",
    "start": "2029320",
    "end": "2037679"
  },
  {
    "text": "so so I see no poo here so okay so this",
    "start": "2037770",
    "end": "2043570"
  },
  {
    "text": "service is really so it says readiness qu and a hundred percent traffic is it send it to the service and you get an",
    "start": "2043570",
    "end": "2052300"
  },
  {
    "text": "endpoint",
    "start": "2052300",
    "end": "2054510"
  },
  {
    "text": "so next thing I'm going to do is I'm going to hook up to the Kafka so as I",
    "start": "2062390",
    "end": "2068780"
  },
  {
    "text": "upload image it will publish event to the car car and then edit basically says",
    "start": "2068780",
    "end": "2074710"
  },
  {
    "text": "from this topic I connect to this cava",
    "start": "2074710",
    "end": "2079850"
  },
  {
    "text": "broker where I deployed in cluster and our delivered that event to this impress service which they've diminished",
    "start": "2079850",
    "end": "2088210"
  },
  {
    "text": "so here the you can see spinned up a",
    "start": "2097739",
    "end": "2104799"
  },
  {
    "text": "Kafka sauce part which is the basically a cover consumer it reads the events",
    "start": "2104799",
    "end": "2110769"
  },
  {
    "text": "from the Kafka queue and then calls out to the inference service so everything",
    "start": "2110769",
    "end": "2118029"
  },
  {
    "text": "is set up and then I can upload the file",
    "start": "2118029",
    "end": "2127980"
  },
  {
    "text": "okay and then they should go to well off",
    "start": "2129239",
    "end": "2134769"
  },
  {
    "text": "the bucket ready to go again",
    "start": "2134769",
    "end": "2149819"
  },
  {
    "text": "upload on please yeah yeah I'd won actually a window here I think I",
    "start": "2158650",
    "end": "2166099"
  },
  {
    "text": "appeared our bad model here it should be go to five but I went tonight so that's",
    "start": "2166099",
    "end": "2171890"
  },
  {
    "text": "why I need data signs to help because so",
    "start": "2171890",
    "end": "2177950"
  },
  {
    "text": "our leave this problem to data scientists I actually helped all right",
    "start": "2177950",
    "end": "2185769"
  },
  {
    "text": "well it's my smooth as the last clothing slides",
    "start": "2189069",
    "end": "2194529"
  },
  {
    "text": "so I think we have time for one or two questions and then we can take questions out in the hall if anyone has it I think",
    "start": "2202840",
    "end": "2208180"
  },
  {
    "text": "you were very fast I'll repeat",
    "start": "2208180",
    "end": "2218520"
  },
  {
    "text": "ah yeah I should have mentioned that",
    "start": "2220230",
    "end": "2227030"
  },
  {
    "text": "yeah so the question is once you have an end point made what does the request look like and this is a really big",
    "start": "2227030",
    "end": "2233640"
  },
  {
    "text": "question for us is we call it the data plane what how do you structure a request for prediction and it's a fairly",
    "start": "2233640",
    "end": "2240359"
  },
  {
    "text": "large question every single model server that exists in this history right now is similar and a little bit different so if",
    "start": "2240359",
    "end": "2247680"
  },
  {
    "text": "you look at how tensorflow serving which is basically the first and a lot of people followed their convention that",
    "start": "2247680",
    "end": "2253740"
  },
  {
    "text": "works in one way and onyx runtime works in another way and tensor RT inference server works in another way we've tried",
    "start": "2253740",
    "end": "2261510"
  },
  {
    "text": "to get all of these people open Vino's and other into a room and so we have a committee of data a data plane committee",
    "start": "2261510",
    "end": "2267570"
  },
  {
    "text": "that's working on this problem of how do you in a standard way send a request to any framework and that would enable you",
    "start": "2267570",
    "end": "2273750"
  },
  {
    "text": "to say swap out a psychic learn model that you've started implementing and maybe you ran into some performance",
    "start": "2273750",
    "end": "2279600"
  },
  {
    "text": "problems and did better with PI torch and you could swap that out under live traffic as long as the request format",
    "start": "2279600",
    "end": "2286050"
  },
  {
    "text": "was the same so for HTTP we've decided among the community including open vino",
    "start": "2286050",
    "end": "2292109"
  },
  {
    "text": "and tensorflow and tensor RT at Nvidia and a bunch of other I guess we have our own implementations of some of the",
    "start": "2292109",
    "end": "2298200"
  },
  {
    "text": "Python ones we've been able to standardize that around tensorflow as HTTP protocol and if you go in our Doc's",
    "start": "2298200",
    "end": "2304619"
  },
  {
    "text": "there's a simple structure of what that looks like and it's fairly straightforward there's some pretty serious problems especially with",
    "start": "2304619",
    "end": "2310590"
  },
  {
    "text": "encoding images into JSON you can't put bytes into JSON and images are typically",
    "start": "2310590",
    "end": "2316740"
  },
  {
    "text": "representative bytes so you have to base 64 encoded and that base64 encode decode",
    "start": "2316740",
    "end": "2322020"
  },
  {
    "text": "and JSON parsing is expensive and this is where in videos really helping us we're talking about a more performant",
    "start": "2322020",
    "end": "2327960"
  },
  {
    "text": "interface so we would expect in the future every single model server every single framework would support two",
    "start": "2327960",
    "end": "2333180"
  },
  {
    "text": "interfaces one very simple JSON one which is the tensor flows existing protocol but you could just curl like an",
    "start": "2333180",
    "end": "2339030"
  },
  {
    "text": "array of floats if you wanted to but if you needed to do something more performant like image processing you",
    "start": "2339030",
    "end": "2344520"
  },
  {
    "text": "could do something like proto before flatbuffers that's still in development so that's kind of the current state of",
    "start": "2344520",
    "end": "2350910"
  },
  {
    "text": "things we're hoping within several months that we have a really solid story around what the performance standardized data plane is alright we're",
    "start": "2350910",
    "end": "2360130"
  },
  {
    "text": "out of time [Applause]",
    "start": "2360130",
    "end": "2370349"
  }
]