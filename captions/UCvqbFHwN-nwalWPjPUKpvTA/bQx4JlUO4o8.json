[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "welcome everybody thanks a lot for attending the last session of the conference I know brace got all that's",
    "start": "179",
    "end": "7440"
  },
  {
    "text": "standing between you and going home or celebrating after the conference so I salute your dedication my name is Eric",
    "start": "7440",
    "end": "14910"
  },
  {
    "text": "and I work at Red Hat and my partner here is a he nan works at Google and two",
    "start": "14910",
    "end": "21630"
  },
  {
    "text": "of the co-chairs on the sig Big Data we talk about the sig today",
    "start": "21630",
    "end": "28460"
  },
  {
    "start": "28000",
    "end": "28000"
  },
  {
    "text": "so the basic landscape of the talk I'm just going to briefly introduce what the",
    "start": "28460",
    "end": "34079"
  },
  {
    "text": "sig is about I'm gonna we're gonna cover a couple of the longer-term engagements",
    "start": "34079",
    "end": "40020"
  },
  {
    "text": "we've had over the last couple of years Apache air flow and Apache spark then I'm gonna circle back to the cig and",
    "start": "40020",
    "end": "47820"
  },
  {
    "text": "talk a little bit about some of the trajectories and the possible futures and then you can see at the bottom here",
    "start": "47820",
    "end": "55230"
  },
  {
    "text": "I called that audience dialogue so we are hoping to leave a little bit of extra time to get feedback from you guys",
    "start": "55230",
    "end": "62460"
  },
  {
    "text": "about possible directions we can take the cig or new projects so the the main",
    "start": "62460",
    "end": "74159"
  },
  {
    "start": "71000",
    "end": "71000"
  },
  {
    "text": "function of the cig since 2017 has been to sort of serve as a community resource",
    "start": "74159",
    "end": "80900"
  },
  {
    "text": "for advising other projects in our case focusing on you know big data or data",
    "start": "80900",
    "end": "87299"
  },
  {
    "text": "science software projects and just to serve as a reservoir of expertise and",
    "start": "87299",
    "end": "93000"
  },
  {
    "text": "help for how helping these people integrate with kubernetes and then in",
    "start": "93000",
    "end": "98610"
  },
  {
    "text": "turn our other goal was to you know represent the concerns of these users",
    "start": "98610",
    "end": "104220"
  },
  {
    "text": "back to kubernetes in the form of you know driving possible new features or enhancements shepherding PRS etc and the",
    "start": "104220",
    "end": "117090"
  },
  {
    "text": "text you see above here is actually the exact text I submitted via this PR at",
    "start": "117090",
    "end": "122759"
  },
  {
    "text": "the bottom as the official cig mission and I'll be circling back to this PR",
    "start": "122759",
    "end": "128190"
  },
  {
    "text": "later at the end there are actually three chairs of the",
    "start": "128190",
    "end": "134930"
  },
  {
    "start": "131000",
    "end": "131000"
  },
  {
    "text": "cig currently on Arruda Ramanath on who is currently at rock set myself at Red",
    "start": "134930",
    "end": "141110"
  },
  {
    "text": "Hat and Eaton Holly at Google and the QR code there will actually take you to the",
    "start": "141110",
    "end": "147200"
  },
  {
    "text": "kubernetes community page for our cig if you want to visit that sort of a cig",
    "start": "147200",
    "end": "156320"
  },
  {
    "start": "153000",
    "end": "153000"
  },
  {
    "text": "come from there's actually a prehistory to the cig which i not particularly",
    "start": "156320",
    "end": "161360"
  },
  {
    "text": "familiar with but early in 2017 it got revived and part of the process of",
    "start": "161360",
    "end": "168620"
  },
  {
    "text": "creating a new community platform for the purpose of prototyping the kubernetes schedule or back-end on",
    "start": "168620",
    "end": "175880"
  },
  {
    "text": "apache spark since then we've actually",
    "start": "175880",
    "end": "182330"
  },
  {
    "start": "179000",
    "end": "179000"
  },
  {
    "text": "moved the coop scheduler backend upstream to apache spark community we've",
    "start": "182330",
    "end": "188000"
  },
  {
    "text": "also done some work on HDFS deployments inside kubernetes and also helped the",
    "start": "188000",
    "end": "195470"
  },
  {
    "text": "apache airflow community create operators and executor x' to work with coop we've had a pleasingly diverse",
    "start": "195470",
    "end": "205720"
  },
  {
    "text": "community involvement and the above seven companies here are like my own",
    "start": "205720",
    "end": "212209"
  },
  {
    "text": "scientific assessment of who's like been most consistent attending if i've left",
    "start": "212209",
    "end": "218300"
  },
  {
    "text": "anybody off that is only because i want to try and complete the talk on time so",
    "start": "218300",
    "end": "225410"
  },
  {
    "start": "224000",
    "end": "224000"
  },
  {
    "text": "i'm gonna go into the first engagement which was with a patchy airflow airflow",
    "start": "225410",
    "end": "232880"
  },
  {
    "text": "has kind of like an opinion about its compute model at these computation as",
    "start": "232880",
    "end": "239200"
  },
  {
    "text": "workflows of tasks in each task is allowed to have dependencies now if",
    "start": "239200",
    "end": "244520"
  },
  {
    "text": "anybody who comes out of computer science will know that the data structure represented add is a directed",
    "start": "244520",
    "end": "250280"
  },
  {
    "text": "acyclic graph you can see the arcs of the graph are pointing from right to",
    "start": "250280",
    "end": "256459"
  },
  {
    "text": "left as traditional and however when you actually run it execution logically",
    "start": "256459",
    "end": "263250"
  },
  {
    "text": "precedes them from left to right so anybody also familiar with the UNIX",
    "start": "263250",
    "end": "269700"
  },
  {
    "text": "make will realize that this is essentially UNIX make after taking a lot",
    "start": "269700",
    "end": "275340"
  },
  {
    "text": "of steroids and doping and such like",
    "start": "275340",
    "end": "281000"
  },
  {
    "start": "279000",
    "end": "279000"
  },
  {
    "text": "so the basic unit of work for airflows called an operator corresponds to like",
    "start": "281000",
    "end": "287940"
  },
  {
    "text": "executing a single command or a function generally associated with a unique task",
    "start": "287940",
    "end": "293280"
  },
  {
    "text": "ID and it is intended to contain all the parameters or other resources it needs",
    "start": "293280",
    "end": "298920"
  },
  {
    "text": "to execute operators come in many flavors you can literally run a bash",
    "start": "298920",
    "end": "306000"
  },
  {
    "start": "302000",
    "end": "302000"
  },
  {
    "text": "command invoke a Python function send an HTTP request to a rest and plying or",
    "start": "306000",
    "end": "312930"
  },
  {
    "text": "execute a sequel query it comes out-of-the-box with many many flavors of operator the airflow scheduler is the",
    "start": "312930",
    "end": "324140"
  },
  {
    "start": "319000",
    "end": "319000"
  },
  {
    "text": "component that's basically tasked with executing a workflow in the proper order",
    "start": "324140",
    "end": "329280"
  },
  {
    "text": "and well it does that it also tracks the success or failure of each task and",
    "start": "329280",
    "end": "335280"
  },
  {
    "text": "reports that back to a user interface it also keeps track of log log data and",
    "start": "335280",
    "end": "341520"
  },
  {
    "text": "other minutiae it also is concept of executor an executor is really nothing",
    "start": "341520",
    "end": "349350"
  },
  {
    "start": "344000",
    "end": "344000"
  },
  {
    "text": "but a selection of an environment that will actually run the task so the basic",
    "start": "349350",
    "end": "356340"
  },
  {
    "text": "one is literally just executing locally on a machine it also has one for executing tasks instead of mesos and now",
    "start": "356340",
    "end": "363870"
  },
  {
    "text": "more recently inside of kubernetes so",
    "start": "363870",
    "end": "369950"
  },
  {
    "text": "the kubrana is operator or kubernetes pod operator is I guess in my opinion",
    "start": "369950",
    "end": "376710"
  },
  {
    "text": "these sort of keystone of the integration with Kubb and you can see it",
    "start": "376710",
    "end": "381930"
  },
  {
    "text": "contains very common things common to all operators like a name a task ID",
    "start": "381930",
    "end": "388240"
  },
  {
    "text": "but then it also contains fields for container image and a command to run on",
    "start": "388240",
    "end": "394240"
  },
  {
    "text": "that image and of course that's basically the core functionality that allows you to run inside of the cuve",
    "start": "394240",
    "end": "400539"
  },
  {
    "text": "environment you can see also that there are other kubernetes features supported",
    "start": "400539",
    "end": "406030"
  },
  {
    "text": "labels secrets volume mounts so it's got",
    "start": "406030",
    "end": "411729"
  },
  {
    "text": "a fairly full-featured you know set of integrations for a typical kubernetes",
    "start": "411729",
    "end": "418660"
  },
  {
    "text": "cluster and upcoming there is going to",
    "start": "418660",
    "end": "426729"
  },
  {
    "start": "421000",
    "end": "421000"
  },
  {
    "text": "be support for a airflow communities operator and so it's in a sense the",
    "start": "426729",
    "end": "432220"
  },
  {
    "text": "fullest possible coop integration it actually runs the scheduler and the user",
    "start": "432220",
    "end": "437680"
  },
  {
    "text": "interface and the sequel database inside of kubernetes and so all the components",
    "start": "437680",
    "end": "444520"
  },
  {
    "text": "are actually running in Kubb and then executing the workflows also inside of the same cluster and I thought it also",
    "start": "444520",
    "end": "455349"
  },
  {
    "start": "454000",
    "end": "454000"
  },
  {
    "text": "briefly mentioned that a lot of the actual uses of airflow in the wild",
    "start": "455349",
    "end": "460509"
  },
  {
    "text": "involved very large DAGs they can run into the megabytes and a user could have",
    "start": "460509",
    "end": "467590"
  },
  {
    "text": "thousands of them as part of their workflows and so you're talking about actually large-scale data and one of the",
    "start": "467590",
    "end": "474699"
  },
  {
    "text": "things that the airflow community put a lot of effort and design into is supporting different ways to stage these",
    "start": "474699",
    "end": "481479"
  },
  {
    "text": "DAGs to make them available to containers running coop you know in a",
    "start": "481479",
    "end": "486520"
  },
  {
    "text": "scalable way and here just a few of the ones that you know you might see",
    "start": "486520",
    "end": "492460"
  },
  {
    "text": "yourself running into and it's also extensible so you can add your own ways of staging via hooks so that that's the",
    "start": "492460",
    "end": "501520"
  },
  {
    "text": "engagement we had with airflow and you know what I gave you is vastly",
    "start": "501520",
    "end": "507969"
  },
  {
    "text": "incomplete and if you're actually interested in airflow I encourage you to",
    "start": "507969",
    "end": "513070"
  },
  {
    "text": "check out the recording of Daniel in Berman's and Barney so the",
    "start": "513070",
    "end": "520560"
  },
  {
    "text": "theorem I think is his name I'm a butcher dad and I apologize but they",
    "start": "520560",
    "end": "525600"
  },
  {
    "text": "gave that talk yesterday and they go into much more detail to the Anya hair",
    "start": "525600",
    "end": "530940"
  },
  {
    "text": "flow on Cooper DS effort I'm gonna hand it over to a Ian on who's gonna talk",
    "start": "530940",
    "end": "538019"
  },
  {
    "start": "534000",
    "end": "534000"
  },
  {
    "text": "about our Apache spark engagement so I should before I start just wanna get it",
    "start": "538019",
    "end": "543329"
  },
  {
    "text": "since like you know how many users are Co how many of you are already running a spark on khorinis okay it's quite a few",
    "start": "543329",
    "end": "550200"
  },
  {
    "text": "okay great so uh before you know I dive into the details of running spark on",
    "start": "550200",
    "end": "557250"
  },
  {
    "text": "kubernetes I would just want to get you know a sense of what smart really is like you know I want to talk about a",
    "start": "557250",
    "end": "562709"
  },
  {
    "text": "smart computer model so on a high level like you know you have a spark job you have a bunch of input data to process",
    "start": "562709",
    "end": "568320"
  },
  {
    "text": "this is a really really simple example like you know you have a bunch of array of numbers as the input and you tell",
    "start": "568320",
    "end": "576990"
  },
  {
    "text": "like no spar ha how many executors I want in this example like you know so he wants reacts executors so the spark task",
    "start": "576990",
    "end": "584520"
  },
  {
    "text": "scheduler will try to like you know even for a distribute that that you know the",
    "start": "584520",
    "end": "589709"
  },
  {
    "text": "tasks among these executors so the driver is in a position to actually",
    "start": "589709",
    "end": "595170"
  },
  {
    "text": "manage you know the executors so saying except in this perfect example you know",
    "start": "595170",
    "end": "600870"
  },
  {
    "text": "each exterior gets three numbers to to to process so let's say like you know the compute compute we want to do is",
    "start": "600870",
    "end": "608100"
  },
  {
    "text": "really simple I can you just wanted to a doubles of the input numbers so each executor gets free numbers process then",
    "start": "608100",
    "end": "614399"
  },
  {
    "text": "you know they generate some output this is all orchestrated by the driver which",
    "start": "614399",
    "end": "621149"
  },
  {
    "text": "is sort of like you know the master kind of thing so when you put these things",
    "start": "621149",
    "end": "627810"
  },
  {
    "text": "onto a container that's basically how everything works now you have like you",
    "start": "627810",
    "end": "633240"
  },
  {
    "text": "know the driver and exterior executors all running in containers when these containers one on communities they basic",
    "start": "633240",
    "end": "640470"
  },
  {
    "text": "just running pots so the cluster mode in",
    "start": "640470",
    "end": "647010"
  },
  {
    "start": "643000",
    "end": "643000"
  },
  {
    "text": "spark is basically like your driver actually runs your so the",
    "start": "647010",
    "end": "654040"
  },
  {
    "text": "driver runs your your men program and you know is responsible for actually creating executors and managing them and",
    "start": "654040",
    "end": "660880"
  },
  {
    "text": "you know these are being a task to the executors so this is how you know",
    "start": "660880",
    "end": "666760"
  },
  {
    "text": "basically like the entire flow of what happens when you run the spark uncle",
    "start": "666760",
    "end": "673030"
  },
  {
    "text": "increment is done on a unicloset mode so the user runs the spark some eight",
    "start": "673030",
    "end": "678430"
  },
  {
    "text": "script so what happens like you know when you give a master URL that starts with Kade s so this is what have like",
    "start": "678430",
    "end": "685870"
  },
  {
    "text": "you know so the sparks am a script will called kubernetes specific submission client so this salvation submission",
    "start": "685870",
    "end": "692800"
  },
  {
    "text": "client will actually try to create a pass back from initial prospect and goes",
    "start": "692800",
    "end": "698350"
  },
  {
    "text": "through a bunch of steps to actually configure the pass spec towards you know the final state so in my you know there",
    "start": "698350",
    "end": "705370"
  },
  {
    "text": "might be steps for like in mounting wallings more monthly secrets weren't you know doing other kind of things so",
    "start": "705370",
    "end": "711640"
  },
  {
    "text": "basically you know so each step modifies the prospect in certain way so then the",
    "start": "711640",
    "end": "716890"
  },
  {
    "text": "final result will be sent to the API server which well you know basic as the",
    "start": "716890",
    "end": "724090"
  },
  {
    "text": "scheduler to actually schedule your driver pass to run on some node in a cluster so once the driver starts it",
    "start": "724090",
    "end": "731320"
  },
  {
    "text": "will be requesting exceeded pods from a PS server based on the user's request so",
    "start": "731320",
    "end": "738880"
  },
  {
    "text": "again like a node API server works with the scheduler to actually create and schedule your executor pass to run on",
    "start": "738880",
    "end": "745630"
  },
  {
    "text": "some notes so once these executor pass start they will register themselves to",
    "start": "745630",
    "end": "751240"
  },
  {
    "text": "the driver so this happens occurs through and fully qualified das name for",
    "start": "751240",
    "end": "758950"
  },
  {
    "text": "the driver so basically what the submission client will also create a headless terminated service for the",
    "start": "758950",
    "end": "766060"
  },
  {
    "text": "driver so it you know give us it a fqdn name okay so this is what we have done",
    "start": "766060",
    "end": "775060"
  },
  {
    "start": "773000",
    "end": "773000"
  },
  {
    "text": "so far like you know regarding the kubernetes schedule back in for spark so",
    "start": "775060",
    "end": "780160"
  },
  {
    "text": "we have the first release in spark 2.3 with initial support for cluster mode",
    "start": "780160",
    "end": "785740"
  },
  {
    "text": "with you know language binding for Java and Scala and also with support using",
    "start": "785740",
    "end": "791050"
  },
  {
    "text": "remote indices on for example HTTP servers were or s3 or HDFS or like you",
    "start": "791050",
    "end": "797020"
  },
  {
    "text": "know Google Cloud storage and we also support limited set of customizations on the on a pause on the driver and extra",
    "start": "797020",
    "end": "804670"
  },
  {
    "text": "parts for example you can actually specify an environment variables labels",
    "start": "804670",
    "end": "810070"
  },
  {
    "text": "annotations those kind of things so we you know actually suggest months ago",
    "start": "810070",
    "end": "816910"
  },
  {
    "text": "spark 2.4 was released so this release actor came with looking a more feature for the kubernetes schedule back-end for",
    "start": "816910",
    "end": "824620"
  },
  {
    "text": "example now you can actually run Python or are smart jobs on kubernetes",
    "start": "824620",
    "end": "830560"
  },
  {
    "text": "and this release also comes with limited climate support I will talk about that later so why I say it's limited so in",
    "start": "830560",
    "end": "838660"
  },
  {
    "text": "Sparks 3.0 which will be the next release of spark will have more features",
    "start": "838660",
    "end": "843820"
  },
  {
    "text": "in it like Ana so we'll have Kerberos support so you can actually use its",
    "start": "843820",
    "end": "849880"
  },
  {
    "text": "security FS for your input or output and we're also adding support for using pod",
    "start": "849880",
    "end": "858339"
  },
  {
    "text": "templates to do pod customization so we won't that means like a no we won't be",
    "start": "858339",
    "end": "863770"
  },
  {
    "text": "adding new spark config options for like in a doing communities plot pod",
    "start": "863770",
    "end": "869589"
  },
  {
    "text": "customizations so climb mode is actually pretty useful for a lot of seems like",
    "start": "869589",
    "end": "874660"
  },
  {
    "start": "871000",
    "end": "871000"
  },
  {
    "text": "you know you can use Hana mode to run spark shell or drew bitter notebooks for example so we actually support running",
    "start": "874660",
    "end": "882100"
  },
  {
    "text": "the drivers the driver pods or acting on did not pass like driver both inside and outside of clusters so when the driver",
    "start": "882100",
    "end": "889120"
  },
  {
    "text": "runs inside the cluster or acts would be running in a pod so when you actually",
    "start": "889120",
    "end": "894730"
  },
  {
    "text": "use the climb mode there's a few things to take into mind like you know so the first of all if you're running the",
    "start": "894730",
    "end": "899980"
  },
  {
    "text": "driver part inside a if you're running the driver in side the cluster so when",
    "start": "899980",
    "end": "906880"
  },
  {
    "text": "you are so you the better is that you give you know you tell spark what is the",
    "start": "906880",
    "end": "912040"
  },
  {
    "text": "name of your driver pod so in our case we actually set up like a no garbage owner references on the exterior parts",
    "start": "912040",
    "end": "919329"
  },
  {
    "text": "created by the driver so then this will allow a garbage collection to kickin when you know when when the job is done",
    "start": "919329",
    "end": "926230"
  },
  {
    "text": "so what world you know when we actually get rid of the driver part so the exterior parts will be deleted by a",
    "start": "926230",
    "end": "932920"
  },
  {
    "text": "garbage collector so when you are running the driver outside the cluster it won't be running apart so now what do",
    "start": "932920",
    "end": "940179"
  },
  {
    "text": "you need to do is actually you want to make sure that you have connectivity from your executor powers running in a",
    "start": "940179",
    "end": "945339"
  },
  {
    "text": "cluster to the driver which is you know now outside the cluster so we don't like",
    "start": "945339",
    "end": "952839"
  },
  {
    "text": "you know we're not really opinionated on how people will do connectivity so in",
    "start": "952839",
    "end": "957939"
  },
  {
    "text": "this case it's actually on your hand like you know so if you're running the driver inside apart inside the cluster",
    "start": "957939",
    "end": "963490"
  },
  {
    "text": "you want to have a health service for your driver so you have a fully",
    "start": "963490",
    "end": "970449"
  },
  {
    "text": "qualified DNS name for your driver so because actually the Acura P X series will be using our fqdn name to connect",
    "start": "970449",
    "end": "976749"
  },
  {
    "text": "to the driver so when you're running your driver outside the cluster well",
    "start": "976749",
    "end": "981910"
  },
  {
    "text": "it's you know it's much more trickier but still you're you have to make sure",
    "start": "981910",
    "end": "987459"
  },
  {
    "text": "that you know this connectivity is set up properly so the exterior of house can actually talk to the driver yeah so this",
    "start": "987459",
    "end": "995379"
  },
  {
    "text": "animation basically shows like you know if you have a driver inside a cluster you want to create a header this service",
    "start": "995379",
    "end": "1001019"
  },
  {
    "text": "for that so that you know executors can actually use that to talk to the driver",
    "start": "1001019",
    "end": "1007220"
  },
  {
    "text": "so Kerberos supports will coming sparks 3.0 so this is really you know",
    "start": "1007220",
    "end": "1012990"
  },
  {
    "start": "1008000",
    "end": "1008000"
  },
  {
    "text": "critical for using secure HDFS for input or output so to use this feature you",
    "start": "1012990",
    "end": "1018540"
  },
  {
    "text": "actually need two things one is the dedication token the other is actually a custom header configuration that works",
    "start": "1018540",
    "end": "1024808"
  },
  {
    "text": "with the way you set up you know security efforts so this feature doesn't",
    "start": "1024809",
    "end": "1031288"
  },
  {
    "text": "yet support dedicated totally new renewable that means extra you by",
    "start": "1031289",
    "end": "1037350"
  },
  {
    "text": "default dedicated telecom will expire in 24 hours it means that here you have to make sure that your job finish you know",
    "start": "1037350",
    "end": "1042360"
  },
  {
    "text": "in 24 hours because otherwise the dedication token will expire you won't have be able to access your HDFS class",
    "start": "1042360",
    "end": "1048000"
  },
  {
    "text": "or any anymore so this there's actually two",
    "start": "1048000",
    "end": "1053350"
  },
  {
    "text": "ways you can you know tell us part what kind of a so how do how to actually get",
    "start": "1053350",
    "end": "1058600"
  },
  {
    "text": "access to a dedicated token one ways you give the sparks of summation client a",
    "start": "1058600",
    "end": "1063940"
  },
  {
    "text": "key tab so the simply summation client will actually use that key tap to do the logging and then create a secret that",
    "start": "1063940",
    "end": "1073300"
  },
  {
    "text": "carries the dedicated token or optionally you can also bypass this you",
    "start": "1073300",
    "end": "1078730"
  },
  {
    "text": "can just tell spark okay this is factor the secret I want to use which had already has the dedication token stored",
    "start": "1078730",
    "end": "1085650"
  },
  {
    "text": "well the same clumps for configuration so one way is like you know so yeah you",
    "start": "1085650",
    "end": "1092140"
  },
  {
    "text": "basically set the environment verbal had took off I was sorry I forgot what's the",
    "start": "1092140",
    "end": "1097750"
  },
  {
    "text": "exactly the name for that environment verbal but also a sectoral hot cod had took oh yeah so it's actually had a",
    "start": "1097750",
    "end": "1108610"
  },
  {
    "text": "comforter comforter so you said are you burn but we're about to a local Hadoop conf directory so then the submission",
    "start": "1108610",
    "end": "1116170"
  },
  {
    "text": "client work for you start to read all the config files under that direction and create a config map to actress",
    "start": "1116170",
    "end": "1123730"
  },
  {
    "text": "stores you know the the config and then mount it into the driver and Exeter pods",
    "start": "1123730",
    "end": "1130030"
  },
  {
    "text": "so also optionally you can actually use your own config map like you know if you",
    "start": "1130030",
    "end": "1135400"
  },
  {
    "text": "have a config that map that stores your Hadoop config so you can also use that",
    "start": "1135400",
    "end": "1140520"
  },
  {
    "text": "so in addition to like the native integration of spark with kubernetes we",
    "start": "1140520",
    "end": "1146680"
  },
  {
    "start": "1141000",
    "end": "1141000"
  },
  {
    "text": "also working on this the so kakuni the operator for spark so this is actually like you know leveraging operator",
    "start": "1146680",
    "end": "1153400"
  },
  {
    "text": "pattern that was that's actually becoming really popular these days so basically you will have a CR D and you",
    "start": "1153400",
    "end": "1161020"
  },
  {
    "text": "know a custom controller directory one spark some it for you so you basically you define your application",
    "start": "1161020",
    "end": "1166720"
  },
  {
    "text": "declaratively llamo spec for example and you actually just you know create CRS to trigger spark submission so they",
    "start": "1166720",
    "end": "1174340"
  },
  {
    "text": "also like so it actually has you know a richer support for part because the",
    "start": "1174340",
    "end": "1180970"
  },
  {
    "text": "customization beyond was we're currently able to do that is actually done through a mutating web",
    "start": "1180970",
    "end": "1186970"
  },
  {
    "text": "hook so it also has native cron support for running time-based jobs also like",
    "start": "1186970",
    "end": "1192070"
  },
  {
    "text": "you know so integration knitting integration with prometheus for metrics",
    "start": "1192070",
    "end": "1197410"
  },
  {
    "text": "you can actually export like a knobos application metrics and also driver or",
    "start": "1197410",
    "end": "1202720"
  },
  {
    "text": "executors of metrics to Prometheus yeah so it also comes with a custom command",
    "start": "1202720",
    "end": "1210760"
  },
  {
    "text": "line tool called which is called Smart control for doing this things like now you can actually use that to do a port",
    "start": "1210760",
    "end": "1216400"
  },
  {
    "text": "forwarding really easily you don't even need to know the name of the pod so because actually so well sport control",
    "start": "1216400",
    "end": "1222010"
  },
  {
    "text": "and will get you know the driver power name from the CR their customer resource",
    "start": "1222010",
    "end": "1228310"
  },
  {
    "text": "so then we'll be able to actually to forwarding for you for example so this",
    "start": "1228310",
    "end": "1233410"
  },
  {
    "text": "is like a no soul I wrote a simple example of one spark a vacation spec ok",
    "start": "1233410",
    "end": "1239740"
  },
  {
    "start": "1239000",
    "end": "1239000"
  },
  {
    "text": "so in terms of roadmap like what I just said so in Sparks 2 points 3 will have support for pot I'm bleeding like an ax",
    "start": "1239740",
    "end": "1247240"
  },
  {
    "text": "so you can actually supply a pala template for the driver or executor so",
    "start": "1247240",
    "end": "1252580"
  },
  {
    "text": "you can do things like know like you can set a pot infinity or infinity or you can actually use past security context",
    "start": "1252580",
    "end": "1260260"
  },
  {
    "text": "for example so which are not currently supported so the mean structure will no",
    "start": "1260260",
    "end": "1265900"
  },
  {
    "text": "longer add more spark config options people well what the reason is we already have a bunch of you know",
    "start": "1265900",
    "end": "1271450"
  },
  {
    "text": "different kubernetes specific configure options we don't want to add more because that you know so but the more",
    "start": "1271450",
    "end": "1277240"
  },
  {
    "text": "you have them the more difficult to manage so dynamic resource allocation is",
    "start": "1277240",
    "end": "1284260"
  },
  {
    "text": "also on a road map like so with dynamic resource allocation you will be able to actually you know so spark will be able",
    "start": "1284260",
    "end": "1291490"
  },
  {
    "text": "to adjust the number of executors at runtime based on the load or than you know the amount of tasks so this also",
    "start": "1291490",
    "end": "1298330"
  },
  {
    "text": "requires the so called external shuffle service so we have a new shelf of",
    "start": "1298330",
    "end": "1303820"
  },
  {
    "text": "service design in progress actually so we also plan to work on you know better",
    "start": "1303820",
    "end": "1310870"
  },
  {
    "text": "support for local application dependencies so right now the way you do with dependencies is either you you know",
    "start": "1310870",
    "end": "1318700"
  },
  {
    "text": "you have a custom image that you bake your advances in or you put your dependencies somewhere remotely like an",
    "start": "1318700",
    "end": "1326110"
  },
  {
    "text": "oil on s3 or GCS so what if like a know so you have some dependencies on your",
    "start": "1326110",
    "end": "1332020"
  },
  {
    "text": "local machine you want to use those the first thing you have to do it like you know you have to stage somewhere either",
    "start": "1332020",
    "end": "1337210"
  },
  {
    "text": "you know make it into the inter York image or put it somewhere so we want to have you know provide better support for",
    "start": "1337210",
    "end": "1343000"
  },
  {
    "text": "these kind of things so you don't need to worry about like you know the staging at all so the other thing is like anaphors bar streaming on driver",
    "start": "1343000",
    "end": "1350110"
  },
  {
    "text": "resilience like or org is really important so you want to make sure that your driver stays up even though there's",
    "start": "1350110",
    "end": "1357310"
  },
  {
    "text": "like an ax so fader you there's restart and being handled",
    "start": "1357310",
    "end": "1362320"
  },
  {
    "text": "so we're also working out foot but with seek scheduling to you know have better",
    "start": "1362320",
    "end": "1370360"
  },
  {
    "text": "support for scheduling for batch we're closing general but that also will benefit spark spark is actually pretty",
    "start": "1370360",
    "end": "1376330"
  },
  {
    "text": "unique in the sense that if you only have resource to run the driver there's once the driver star you don't",
    "start": "1376330",
    "end": "1382600"
  },
  {
    "text": "have any resource to run your executors but the job will be hunting so basically job will not make any progress but the",
    "start": "1382600",
    "end": "1388360"
  },
  {
    "text": "driver is be is they're staying so we want to make sure that the schedule can",
    "start": "1388360",
    "end": "1394000"
  },
  {
    "text": "take this into consideration like now so you want to make sure that you have enough resources to run at least your",
    "start": "1394000",
    "end": "1399940"
  },
  {
    "text": "driver plus one executor so you can actually make progress yeah so if you",
    "start": "1399940",
    "end": "1406510"
  },
  {
    "start": "1405000",
    "end": "1405000"
  },
  {
    "text": "want to get involved in this project you can actually check out the code at you know this official spark repo what",
    "start": "1406510",
    "end": "1413650"
  },
  {
    "text": "Dakota we enter resource managers slash kubernetes and we have you know documentation zhan on a facial spark site and spark because",
    "start": "1413650",
    "end": "1422650"
  },
  {
    "text": "it's a party project it actually prefers using user and main enlisted floor you know questions or or general discussions",
    "start": "1422650",
    "end": "1431020"
  },
  {
    "text": "so we also have you know we also use JIRA for for feature requests or bug",
    "start": "1431020",
    "end": "1436960"
  },
  {
    "text": "reports there's also a slack channel for sigfig data you can actor check out so",
    "start": "1436960",
    "end": "1445120"
  },
  {
    "text": "with that I'm gonna hand over back to Eric",
    "start": "1445120",
    "end": "1449700"
  },
  {
    "text": "thanks T non so I think these two projects have shown an example of a kind",
    "start": "1450840",
    "end": "1459310"
  },
  {
    "start": "1451000",
    "end": "1451000"
  },
  {
    "text": "of common trajectory I've seen so far which is that once you kind of get over",
    "start": "1459310",
    "end": "1464710"
  },
  {
    "text": "the hump and get some pretty good feature full integration with Kubb you",
    "start": "1464710",
    "end": "1470290"
  },
  {
    "text": "tend to see like you know the discussion move away from our sig meetings and kind",
    "start": "1470290",
    "end": "1475750"
  },
  {
    "text": "of back upstream and I've been calling it you know graduating which some",
    "start": "1475750",
    "end": "1482710"
  },
  {
    "text": "projects actually have a concept of graduation there's no formal process of that at all but you know it's part of",
    "start": "1482710",
    "end": "1489340"
  },
  {
    "text": "the lifecycle and I'm actually pleased that it's happening but it does mean of course that we want to keep a sort of",
    "start": "1489340",
    "end": "1495790"
  },
  {
    "text": "pipeline of reaching out to new projects so just a couple weeks ago we got a demo",
    "start": "1495790",
    "end": "1504220"
  },
  {
    "start": "1497000",
    "end": "1497000"
  },
  {
    "text": "of a flank operator for kubernetes and this coming Wednesday hazal cast is going to be demoing their",
    "start": "1504220",
    "end": "1511930"
  },
  {
    "text": "in-memory data grid and jet stream processing on Kubb recently as I",
    "start": "1511930",
    "end": "1522640"
  },
  {
    "start": "1519000",
    "end": "1519000"
  },
  {
    "text": "mentioned earlier friend pull requests we submitted they have formal charter for sig big data and it's currently up",
    "start": "1522640",
    "end": "1530200"
  },
  {
    "text": "and you can see the QR code will take you to that URL and so far the main",
    "start": "1530200",
    "end": "1535960"
  },
  {
    "text": "feedback we've had is that kubernetes governance officially defines a sig as",
    "start": "1535960",
    "end": "1541420"
  },
  {
    "text": "owning some component or subsystem of a kubernetes and as you can see from our",
    "start": "1541420",
    "end": "1547450"
  },
  {
    "text": "description we actually haven't owned code and that way for the sig and so",
    "start": "1547450",
    "end": "1553600"
  },
  {
    "text": "we're currently discussions with the steering committee about what to do what",
    "start": "1553600",
    "end": "1558820"
  },
  {
    "start": "1557000",
    "end": "1557000"
  },
  {
    "text": "can we do we could acquire ownership of some code we might recategorize ourselves as a",
    "start": "1558820",
    "end": "1565600"
  },
  {
    "text": "kubernetes working group or sub project or possibly even a new kind of a some",
    "start": "1565600",
    "end": "1573220"
  },
  {
    "text": "new kind of user community that fits the description of what it is we're actually doing and then also even I discussed",
    "start": "1573220",
    "end": "1581710"
  },
  {
    "text": "with one of the other steering committee members grandfathering the some of the",
    "start": "1581710",
    "end": "1586870"
  },
  {
    "text": "older SIG's that never had this as well so all these are possible options were probably sort them out after all the",
    "start": "1586870",
    "end": "1593470"
  },
  {
    "text": "dust settles in the new year and we are continuing to meanwhile reach out to",
    "start": "1593470",
    "end": "1600700"
  },
  {
    "text": "people so if you know of a user community or project that wants to",
    "start": "1600700",
    "end": "1606040"
  },
  {
    "text": "integrate better with Kubb we'd be more than happy to have you guys attend and get in touch with us and there's our",
    "start": "1606040",
    "end": "1614980"
  },
  {
    "text": "contact handles for just email and then on kubernetes slack comm and I'm gonna",
    "start": "1614980",
    "end": "1620950"
  },
  {
    "text": "turn it over to you in the audience if you have an ideas for new communities to",
    "start": "1620950",
    "end": "1626500"
  },
  {
    "text": "engage with or questions or feedback on the airflow and spark or just the",
    "start": "1626500",
    "end": "1632110"
  },
  {
    "text": "general future of cig Big Data please feel free yes",
    "start": "1632110",
    "end": "1640919"
  },
  {
    "text": "so my name is Katun I work with lyft we've been working on the spark operator",
    "start": "1651270",
    "end": "1657150"
  },
  {
    "text": "within an and it's been it's been going great we will be has yourself you",
    "start": "1657150",
    "end": "1662400"
  },
  {
    "text": "mentioned flink operators some work going on on our side and then there's",
    "start": "1662400",
    "end": "1667890"
  },
  {
    "text": "some other stuff that we are actually doing that kind of replaces air flow for most of our cases but I think one of the",
    "start": "1667890",
    "end": "1676980"
  },
  {
    "text": "other sides on big data that we it does touch scheduling a little bit like six",
    "start": "1676980",
    "end": "1682650"
  },
  {
    "text": "scheduling so I don't really know where what ends and what starts because what happens is when you start running things",
    "start": "1682650",
    "end": "1688740"
  },
  {
    "text": "at scale they brake as you mentioned like you set apart and then it hangs",
    "start": "1688740",
    "end": "1694650"
  },
  {
    "text": "about the process just slows down tremendously and and so I think how",
    "start": "1694650",
    "end": "1702780"
  },
  {
    "text": "should we probably maybe it's it's like combining those two SIG's if if at all or keeping this sake and",
    "start": "1702780",
    "end": "1709680"
  },
  {
    "text": "also including like batch scheduling in this in this sake might be useful",
    "start": "1709680",
    "end": "1714990"
  },
  {
    "text": "because there are it's not just job like equipment is native jobs right do you",
    "start": "1714990",
    "end": "1720210"
  },
  {
    "text": "like for example if I want to schedule apart what happens if I'm running out at capacity what should be the behavior of",
    "start": "1720210",
    "end": "1726390"
  },
  {
    "text": "a spark driver should it crash or should it like continue on in currently it works I think but like I think we need",
    "start": "1726390",
    "end": "1734460"
  },
  {
    "text": "to think about that that's those are Mike it's like not a feedback but I don't know it's an open question no",
    "start": "1734460",
    "end": "1741930"
  },
  {
    "text": "those are definitely open questions I mean behavior of behavior of certain",
    "start": "1741930",
    "end": "1749040"
  },
  {
    "text": "categories especially of spark jobs like long-running streaming jobs restart",
    "start": "1749040",
    "end": "1754410"
  },
  {
    "text": "policies is still being actually debated and there's the concept of I believe you",
    "start": "1754410",
    "end": "1761520"
  },
  {
    "text": "know working group specifically designed to go across cig like that so that's",
    "start": "1761520",
    "end": "1767760"
  },
  {
    "text": "another possible direction we could take that yeah thanks for your question",
    "start": "1767760",
    "end": "1775429"
  },
  {
    "text": "so for a spark job was the job finished round does the driver kind of terminates",
    "start": "1781440",
    "end": "1786630"
  },
  {
    "text": "itself or still cannot keep running you can actually configure that now you can",
    "start": "1786630",
    "end": "1793560"
  },
  {
    "text": "set it so that if things crash or you know once they terminate the pods remain",
    "start": "1793560",
    "end": "1799070"
  },
  {
    "text": "so you can actually go back and inspect them its default behavior is basically",
    "start": "1799070",
    "end": "1805590"
  },
  {
    "text": "the you know what's like most pods when it's done it just goes away it gets collected so what happens is like so the",
    "start": "1805590",
    "end": "1812190"
  },
  {
    "text": "driver pod will always stay like a nacelle once they drop completes the drivers pod would be there well even",
    "start": "1812190",
    "end": "1817800"
  },
  {
    "text": "though it's not taking computer resource but it's after taking an SAT resource so but extra deposit so for example if if",
    "start": "1817800",
    "end": "1826710"
  },
  {
    "text": "one actuator just finishes successfully will be gone after be deleted I mean by",
    "start": "1826710",
    "end": "1832440"
  },
  {
    "text": "default I mean join work like a driver was always stay so I have to clean up myself so so there's actually a new",
    "start": "1832440",
    "end": "1838680"
  },
  {
    "text": "feature called here on part which will be alpha in 113 so you can actually use",
    "start": "1838680",
    "end": "1844920"
  },
  {
    "text": "that to set a TDR like you know so once the the driver path finishes will be gone after TDI expires",
    "start": "1844920",
    "end": "1850080"
  },
  {
    "text": "cool thanks oh yeah so if you actually",
    "start": "1850080",
    "end": "1857820"
  },
  {
    "text": "use the operator so the lifecycle of the pod the driver power will be tied to the",
    "start": "1857820",
    "end": "1863940"
  },
  {
    "text": "life cycle or if your CR so not might be easier to handle that those kind of",
    "start": "1863940",
    "end": "1869430"
  },
  {
    "text": "sites Thanks sorry a couple of questions",
    "start": "1869430",
    "end": "1875280"
  },
  {
    "text": "um I don't think I mentioned anything around data locality of sparked execution with",
    "start": "1875280",
    "end": "1881250"
  },
  {
    "text": "HDFS data is that something you've looked at that's a that's a good question so we actually had that",
    "start": "1881250",
    "end": "1888150"
  },
  {
    "text": "featuring in our fork before we have streamed on spark you know does so that the scheduler beckoning to the main",
    "start": "1888150",
    "end": "1895020"
  },
  {
    "text": "spark rebel so we I think we still need",
    "start": "1895020",
    "end": "1900180"
  },
  {
    "text": "to have that special you know given we know how am I gonna support for for",
    "start": "1900180",
    "end": "1905220"
  },
  {
    "text": "example for seeker sniff us cover support so EFSA also one of the project we're being",
    "start": "1905220",
    "end": "1910620"
  },
  {
    "text": "engaging with so that's really I think that's really important yeah that's",
    "start": "1910620",
    "end": "1916170"
  },
  {
    "text": "something why even though you know it's not listed there but that's something we also plan to to getting to into this",
    "start": "1916170",
    "end": "1923370"
  },
  {
    "text": "woman's spark revenue as well and then kind of related when you're using local",
    "start": "1923370",
    "end": "1931800"
  },
  {
    "text": "disks local Peavey's and actually hold your HDFS data are we at risk of getting",
    "start": "1931800",
    "end": "1938370"
  },
  {
    "text": "in race conditions with potentially the hosts getting moved around faster than we can get replicas rebuilt so each",
    "start": "1938370",
    "end": "1947700"
  },
  {
    "text": "cloud provider has the right to move our VMs when they're doing stamp evacuation",
    "start": "1947700",
    "end": "1954960"
  },
  {
    "text": "of them stuff like that and have we got the right integrations with each cloud",
    "start": "1954960",
    "end": "1961620"
  },
  {
    "text": "provider to ensure that if we're asked to evict that we can get our replicas rebuilt fast enough so first of all we",
    "start": "1961620",
    "end": "1970440"
  },
  {
    "text": "are not all a like touching any of these things like you know so like forget some",
    "start": "1970440",
    "end": "1976080"
  },
  {
    "text": "law can persist in water that's really up to the application developers they chose like analyze the kind of things",
    "start": "1976080",
    "end": "1981270"
  },
  {
    "text": "they want to use if they want to like you know touch me a local person bottom to their their jobs they can do that but",
    "start": "1981270",
    "end": "1987570"
  },
  {
    "text": "so if we didn't smart we don't do anything special about the hdfs replication we've been you know what we",
    "start": "1987570",
    "end": "1995160"
  },
  {
    "text": "have done has been primarily platform neutral so you know if you talk about like provider specific you know we",
    "start": "1995160",
    "end": "2004610"
  },
  {
    "text": "haven't really talked and that detail at all some of the upstream conversations have been you know more along the lines",
    "start": "2004610",
    "end": "2011990"
  },
  {
    "text": "of spark spark of course has native you just native the ability to download data",
    "start": "2011990",
    "end": "2020110"
  },
  {
    "text": "you know from a URI to during executive",
    "start": "2020110",
    "end": "2025400"
  },
  {
    "text": "that's not so fine yeah and there it may not answer your use case but a lot of",
    "start": "2025400",
    "end": "2030710"
  },
  {
    "text": "the you know upstream feedback we've gotten is maybe we should let it do that",
    "start": "2030710",
    "end": "2036740"
  },
  {
    "text": "go to s3 you know all right so do we not do are you aware of customers that are actually running",
    "start": "2036740",
    "end": "2043379"
  },
  {
    "text": "HDFS and relying on the local Peavey's and HDFS replication for durability so",
    "start": "2043379",
    "end": "2050608"
  },
  {
    "text": "the thing I'm aware of actually so we are so we actually have a helmet r44 HDFS that's based on a paper structure",
    "start": "2050609",
    "end": "2058378"
  },
  {
    "text": "based on the ha host pass volume if I'm not wrong so I think there's a plan to",
    "start": "2058379",
    "end": "2064289"
  },
  {
    "text": "actually you know try out like a lock of resistant water before 4-h TF has data nodes but that hasn't been done yet",
    "start": "2064289",
    "end": "2071069"
  },
  {
    "text": "right okay okay first question is if you",
    "start": "2071069",
    "end": "2082169"
  },
  {
    "text": "sub spark submit a job you can specify what's a cuter memory you prefer like 20",
    "start": "2082169",
    "end": "2088138"
  },
  {
    "text": "TB yes next job maybe I won't own one a 10 GB do they actually share the same",
    "start": "2088139",
    "end": "2095190"
  },
  {
    "text": "pot or is going to be different container allocated right now there's",
    "start": "2095190",
    "end": "2101430"
  },
  {
    "text": "not a lot of fine-grained control for which of those will happen one of the reasons we actually implemented the",
    "start": "2101430",
    "end": "2109769"
  },
  {
    "text": "ability to add customized templates was",
    "start": "2109769",
    "end": "2115079"
  },
  {
    "text": "because people wanted affinity control okay one more question which is their",
    "start": "2115079",
    "end": "2123779"
  },
  {
    "text": "equivalent of like a fair scheduler or capacity scheduler in this new that's a",
    "start": "2123779",
    "end": "2130619"
  },
  {
    "text": "really good question yeah so we are actually talking to sake scheduling",
    "start": "2130619",
    "end": "2136710"
  },
  {
    "text": "about us so they have a project called could be bash buffer if you guys have heard of it or not like that was",
    "start": "2136710",
    "end": "2142440"
  },
  {
    "text": "supposed to be you know doing gone scheduling but spark it's actually pretty unique in a sense that you the",
    "start": "2142440",
    "end": "2150779"
  },
  {
    "text": "driver in executors they don't start at the same time they're not like you know the same they're not the same so you can",
    "start": "2150779",
    "end": "2158160"
  },
  {
    "text": "related you treat them as the same group so the current way the coop about concept doesn't really work for spark",
    "start": "2158160",
    "end": "2166289"
  },
  {
    "text": "even though you know my work for machine learning workloads but now for spark so",
    "start": "2166289",
    "end": "2171299"
  },
  {
    "text": "actually I just how does discussion with the lead on that project harder today so I guess we will have",
    "start": "2171299",
    "end": "2178350"
  },
  {
    "text": "some plan to actually see how we can make work house of work for spark and or",
    "start": "2178350",
    "end": "2183540"
  },
  {
    "text": "generally like if you have a master walker kind of architecture and marks there and water they don't start at the",
    "start": "2183540",
    "end": "2189330"
  },
  {
    "text": "same time how you actually make that work for these kind of workers as well and also it so when it comes to",
    "start": "2189330",
    "end": "2197070"
  },
  {
    "text": "scheduling I think there's like like I see it's definitely you know increasing demands for things like the you know the",
    "start": "2197070",
    "end": "2205110"
  },
  {
    "text": "capacity or first coordinate scheduler from yarn for example in communities well we don't currently have anything",
    "start": "2205110",
    "end": "2210600"
  },
  {
    "text": "like those so that's um still something that we have to work on so is there any",
    "start": "2210600",
    "end": "2219660"
  },
  {
    "text": "plan to do a kind of data catalog thing on kubernetes for example something like",
    "start": "2219660",
    "end": "2224760"
  },
  {
    "text": "Adele brouse school or hive so you can when I run sprout job I can reference that metadata to even though I have",
    "start": "2224760",
    "end": "2231780"
  },
  {
    "text": "managed spark job so I have a kind of consciousness centralized metadata store so I know where the data is coming from",
    "start": "2231780",
    "end": "2239010"
  },
  {
    "text": "is there any plan for doing some sort of protocol that we have no current plans",
    "start": "2239010",
    "end": "2244430"
  },
  {
    "text": "but if you'd like to see something that yeah definitely um you know send us an email and we can you know try to get it",
    "start": "2244430",
    "end": "2251430"
  },
  {
    "text": "in as part of our discussions either on the cigarette stream okay cool",
    "start": "2251430",
    "end": "2258859"
  },
  {
    "text": "so it's basically like you know any costume resources you basically just",
    "start": "2268780",
    "end": "2274400"
  },
  {
    "text": "have your customer resource definitions store somewhere like you know so you can either you llamo or you create those",
    "start": "2274400",
    "end": "2281620"
  },
  {
    "text": "programmatically there's like you know so different ways of doing that or you",
    "start": "2281620",
    "end": "2287120"
  },
  {
    "text": "can act like that so you can also do it like you know for example like C get CS ad kind of thing like you know you have",
    "start": "2287120",
    "end": "2293030"
  },
  {
    "text": "your specs stored on github for example then once you make a change Walker",
    "start": "2293030",
    "end": "2298550"
  },
  {
    "text": "automatically trigger reapply something like that yeah so it's just you know so basic just customer resources well we're",
    "start": "2298550",
    "end": "2310760"
  },
  {
    "text": "at just just past 505 oh thank you we can know you've got more question",
    "start": "2310760",
    "end": "2318700"
  },
  {
    "text": "[Music]",
    "start": "2319200",
    "end": "2322260"
  },
  {
    "text": "than it is in providing that as part of the saying that hey this is what you should be running when you're running",
    "start": "2333530",
    "end": "2338700"
  },
  {
    "text": "things in production if I know this is fine but yeah like especially with cloud",
    "start": "2338700",
    "end": "2343950"
  },
  {
    "text": "meters",
    "start": "2343950",
    "end": "2346338"
  },
  {
    "text": "yeah exactly so I think one unique thing",
    "start": "2349920",
    "end": "2355320"
  },
  {
    "text": "about this sega's actually covers you know not only about computer but also storage and scheduling so there's not",
    "start": "2355320",
    "end": "2363240"
  },
  {
    "text": "like a whole bunch of scenes you have to consider when you run your data processing workflows on kubernetes yeah",
    "start": "2363240",
    "end": "2369480"
  },
  {
    "text": "so like what you said mmm yeah that's a good idea actual can you",
    "start": "2369480",
    "end": "2374820"
  },
  {
    "text": "know provide some kind of reference so people when people come to us like a know they know what will be the",
    "start": "2374820",
    "end": "2380810"
  },
  {
    "text": "recommendation that we have one it is true I think part of part of our mission",
    "start": "2380810",
    "end": "2386880"
  },
  {
    "text": "is to supply like best practices yes we have not codified that it's been you",
    "start": "2386880",
    "end": "2392790"
  },
  {
    "text": "know mostly just discussions already covered about minutes so yeah I mean",
    "start": "2392790",
    "end": "2399180"
  },
  {
    "text": "Mike yeah my own view is that that's kind of application dependent so I'm personally hesitant to like try to write",
    "start": "2399180",
    "end": "2405450"
  },
  {
    "text": "some kind of comprehensive document on that yeah actually we could do that and",
    "start": "2405450",
    "end": "2418740"
  },
  {
    "text": "of course some of them might actually want to resign now upon the Apache spark docs or for that are the airflow Doc's",
    "start": "2418740",
    "end": "2426380"
  },
  {
    "text": "we're trying there it's on it there comes a point where we try to like make sure that like we're directing",
    "start": "2426380",
    "end": "2433580"
  },
  {
    "text": "discussions to the right organization because of course the up streams want to make sure that like you know they're in",
    "start": "2433580",
    "end": "2440250"
  },
  {
    "text": "the loop on anything that happens before it becomes a PR that shows up on there mm-hmm",
    "start": "2440250",
    "end": "2447170"
  },
  {
    "text": "- thank you [Applause]",
    "start": "2450880",
    "end": "2458059"
  }
]