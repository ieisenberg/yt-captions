[
  {
    "text": "hello and welcome to my talk about how",
    "start": "80",
    "end": "2000"
  },
  {
    "text": "we converted our cortex data",
    "start": "2000",
    "end": "4000"
  },
  {
    "text": "to tsdb blocks i am peter and i work as",
    "start": "4000",
    "end": "7120"
  },
  {
    "text": "software engineer at grafana labs",
    "start": "7120",
    "end": "9120"
  },
  {
    "text": "where i work on the cortex project for",
    "start": "9120",
    "end": "12240"
  },
  {
    "text": "those of you who are not familiar with",
    "start": "12240",
    "end": "14000"
  },
  {
    "text": "cortex",
    "start": "14000",
    "end": "14960"
  },
  {
    "text": "here is a short introduction cortex is a",
    "start": "14960",
    "end": "17840"
  },
  {
    "text": "horizontally scalable",
    "start": "17840",
    "end": "19279"
  },
  {
    "text": "highly available multi-tenant long-term",
    "start": "19279",
    "end": "22080"
  },
  {
    "text": "storage for prometheus that provides",
    "start": "22080",
    "end": "23920"
  },
  {
    "text": "global view across all of your permit",
    "start": "23920",
    "end": "26160"
  },
  {
    "text": "use data",
    "start": "26160",
    "end": "27439"
  },
  {
    "text": "cortex features blazing fast queries and",
    "start": "27439",
    "end": "30400"
  },
  {
    "text": "is hundred percent compatible with prom",
    "start": "30400",
    "end": "32398"
  },
  {
    "text": "ql",
    "start": "32399",
    "end": "33600"
  },
  {
    "text": "cortex is currently an incubating",
    "start": "33600",
    "end": "35600"
  },
  {
    "text": "project at cncf",
    "start": "35600",
    "end": "37840"
  },
  {
    "text": "in short cortex acts as remote right",
    "start": "37840",
    "end": "40800"
  },
  {
    "text": "target for prometheus servers",
    "start": "40800",
    "end": "43200"
  },
  {
    "text": "and allow squaring the data back year",
    "start": "43200",
    "end": "46399"
  },
  {
    "text": "2020",
    "start": "46399",
    "end": "47440"
  },
  {
    "text": "was the year when we have introduced a",
    "start": "47440",
    "end": "49760"
  },
  {
    "text": "new storage engine into cortex",
    "start": "49760",
    "end": "52000"
  },
  {
    "text": "called the blocks engine the storage",
    "start": "52000",
    "end": "54879"
  },
  {
    "text": "engine",
    "start": "54879",
    "end": "55440"
  },
  {
    "text": "uses tsdb blocks exactly the same as",
    "start": "55440",
    "end": "58239"
  },
  {
    "text": "prometheus itself does",
    "start": "58239",
    "end": "60160"
  },
  {
    "text": "in fact we reuse the tsdb library from",
    "start": "60160",
    "end": "62879"
  },
  {
    "text": "prometheus to write the data",
    "start": "62879",
    "end": "65439"
  },
  {
    "text": "in addition to that cortex also reuses",
    "start": "65439",
    "end": "68080"
  },
  {
    "text": "some components from thanos",
    "start": "68080",
    "end": "69920"
  },
  {
    "text": "and we have optimized some of those",
    "start": "69920",
    "end": "71760"
  },
  {
    "text": "components for the benefit of both",
    "start": "71760",
    "end": "73600"
  },
  {
    "text": "cortex and thanos",
    "start": "73600",
    "end": "75600"
  },
  {
    "text": "why did we spend all this effort tsdb",
    "start": "75600",
    "end": "78720"
  },
  {
    "text": "blocks are simply",
    "start": "78720",
    "end": "79759"
  },
  {
    "text": "much more efficient at storing the same",
    "start": "79759",
    "end": "81840"
  },
  {
    "text": "data compared to the storage",
    "start": "81840",
    "end": "83840"
  },
  {
    "text": "used by cortex previously in addition to",
    "start": "83840",
    "end": "86960"
  },
  {
    "text": "that",
    "start": "86960",
    "end": "87600"
  },
  {
    "text": "we also get extra features that cortex",
    "start": "87600",
    "end": "89840"
  },
  {
    "text": "did not have before like",
    "start": "89840",
    "end": "91439"
  },
  {
    "text": "support for queries without the metric",
    "start": "91439",
    "end": "93600"
  },
  {
    "text": "name pertinent data retention",
    "start": "93600",
    "end": "96159"
  },
  {
    "text": "or much simpler data cleanup from the",
    "start": "96159",
    "end": "98320"
  },
  {
    "text": "long term storage",
    "start": "98320",
    "end": "101119"
  },
  {
    "text": "introducing the new data storage format",
    "start": "101119",
    "end": "103520"
  },
  {
    "text": "poses the question about",
    "start": "103520",
    "end": "105119"
  },
  {
    "text": "what to do with the old data stored in a",
    "start": "105119",
    "end": "107200"
  },
  {
    "text": "legacy format",
    "start": "107200",
    "end": "108640"
  },
  {
    "text": "and that is what this talk is going to",
    "start": "108640",
    "end": "110399"
  },
  {
    "text": "be about",
    "start": "110399",
    "end": "112560"
  },
  {
    "text": "original storage in cortex is called",
    "start": "112560",
    "end": "115119"
  },
  {
    "text": "chunk storage",
    "start": "115119",
    "end": "116320"
  },
  {
    "text": "it has two parts index and store for",
    "start": "116320",
    "end": "119680"
  },
  {
    "text": "chunks",
    "start": "119680",
    "end": "121040"
  },
  {
    "text": "on this slide we see example of three",
    "start": "121040",
    "end": "123040"
  },
  {
    "text": "chunks",
    "start": "123040",
    "end": "124159"
  },
  {
    "text": "each chunk contains samples for one time",
    "start": "124159",
    "end": "127280"
  },
  {
    "text": "series",
    "start": "127280",
    "end": "128479"
  },
  {
    "text": "for a specific time range typically a",
    "start": "128479",
    "end": "130879"
  },
  {
    "text": "few hours",
    "start": "130879",
    "end": "132480"
  },
  {
    "text": "in addition to those samples cortex",
    "start": "132480",
    "end": "135120"
  },
  {
    "text": "chunk",
    "start": "135120",
    "end": "136000"
  },
  {
    "text": "has submitted data about itself like",
    "start": "136000",
    "end": "138400"
  },
  {
    "text": "what series it belongs to",
    "start": "138400",
    "end": "140400"
  },
  {
    "text": "stored as a set of labels or minimum and",
    "start": "140400",
    "end": "143280"
  },
  {
    "text": "maximum time",
    "start": "143280",
    "end": "145599"
  },
  {
    "text": "each chunk is stored separately for",
    "start": "145599",
    "end": "148080"
  },
  {
    "text": "example as individual object in the",
    "start": "148080",
    "end": "150000"
  },
  {
    "text": "cloud storage",
    "start": "150000",
    "end": "150959"
  },
  {
    "text": "or individual cell in bigtable as you",
    "start": "150959",
    "end": "154080"
  },
  {
    "text": "can imagine",
    "start": "154080",
    "end": "154800"
  },
  {
    "text": "this generates a huge number of objects",
    "start": "154800",
    "end": "156800"
  },
  {
    "text": "or cells",
    "start": "156800",
    "end": "158480"
  },
  {
    "text": "due to the replication individual sample",
    "start": "158480",
    "end": "160800"
  },
  {
    "text": "may end up in multiple chunks",
    "start": "160800",
    "end": "162720"
  },
  {
    "text": "although cortex chunk storage does have",
    "start": "162720",
    "end": "164800"
  },
  {
    "text": "some features to reduce this duplicity",
    "start": "164800",
    "end": "168879"
  },
  {
    "text": "for cortex to be able to locate the",
    "start": "168879",
    "end": "170800"
  },
  {
    "text": "correct chunks during the queries",
    "start": "170800",
    "end": "173200"
  },
  {
    "text": "it uses index index is stored in no sql",
    "start": "173200",
    "end": "176959"
  },
  {
    "text": "database like",
    "start": "176959",
    "end": "177920"
  },
  {
    "text": "bigtable or dynamodb index needs to have",
    "start": "177920",
    "end": "181519"
  },
  {
    "text": "multiple features",
    "start": "181519",
    "end": "182879"
  },
  {
    "text": "first of all index needs to allow for",
    "start": "182879",
    "end": "185440"
  },
  {
    "text": "efficient lookups",
    "start": "185440",
    "end": "186640"
  },
  {
    "text": "based on time because promql queries are",
    "start": "186640",
    "end": "190080"
  },
  {
    "text": "always",
    "start": "190080",
    "end": "190640"
  },
  {
    "text": "restricted to some specific time range",
    "start": "190640",
    "end": "193680"
  },
  {
    "text": "to make them run fast we need to",
    "start": "193680",
    "end": "195519"
  },
  {
    "text": "restrict search to this time range only",
    "start": "195519",
    "end": "199680"
  },
  {
    "text": "index must support search by specific",
    "start": "199680",
    "end": "201920"
  },
  {
    "text": "label names and values",
    "start": "201920",
    "end": "204799"
  },
  {
    "text": "next index must also support",
    "start": "204799",
    "end": "206720"
  },
  {
    "text": "multi-tenancy",
    "start": "206720",
    "end": "207920"
  },
  {
    "text": "even though the index entries for all",
    "start": "207920",
    "end": "209920"
  },
  {
    "text": "users are stored in the same database",
    "start": "209920",
    "end": "213040"
  },
  {
    "text": "how exactly index looks like in cortex",
    "start": "213040",
    "end": "215840"
  },
  {
    "text": "has evolved in time",
    "start": "215840",
    "end": "217360"
  },
  {
    "text": "and cortex uses so-called schemas to",
    "start": "217360",
    "end": "219599"
  },
  {
    "text": "describe each version of the index",
    "start": "219599",
    "end": "222239"
  },
  {
    "text": "on the picture you can see simplified",
    "start": "222239",
    "end": "224400"
  },
  {
    "text": "view of so-called version 9 of the index",
    "start": "224400",
    "end": "227360"
  },
  {
    "text": "with entry types like label label value",
    "start": "227360",
    "end": "230080"
  },
  {
    "text": "or series",
    "start": "230080",
    "end": "232159"
  },
  {
    "text": "here is the fun challenge how to",
    "start": "232159",
    "end": "234400"
  },
  {
    "text": "efficiently return chunks for",
    "start": "234400",
    "end": "236239"
  },
  {
    "text": "cortex uptime metric with job names",
    "start": "236239",
    "end": "238560"
  },
  {
    "text": "starting with queue",
    "start": "238560",
    "end": "241680"
  },
  {
    "text": "let's take a look at tsdb blocks now",
    "start": "241680",
    "end": "244480"
  },
  {
    "text": "block consists",
    "start": "244480",
    "end": "245519"
  },
  {
    "text": "of multiple files on disk each block has",
    "start": "245519",
    "end": "248640"
  },
  {
    "text": "a unique identifier",
    "start": "248640",
    "end": "250400"
  },
  {
    "text": "that encodes timestamp when this",
    "start": "250400",
    "end": "252159"
  },
  {
    "text": "identifier was generated and",
    "start": "252159",
    "end": "254159"
  },
  {
    "text": "random part for uniqueness inside",
    "start": "254159",
    "end": "257199"
  },
  {
    "text": "chunks subdirectory there are so called",
    "start": "257199",
    "end": "259519"
  },
  {
    "text": "segment files",
    "start": "259519",
    "end": "261040"
  },
  {
    "text": "which are numbered in this image there",
    "start": "261040",
    "end": "263440"
  },
  {
    "text": "are two",
    "start": "263440",
    "end": "264960"
  },
  {
    "text": "block also has an index stored in a",
    "start": "264960",
    "end": "267199"
  },
  {
    "text": "single file",
    "start": "267199",
    "end": "268800"
  },
  {
    "text": "metadata are stored in a small file in",
    "start": "268800",
    "end": "271520"
  },
  {
    "text": "json format",
    "start": "271520",
    "end": "272639"
  },
  {
    "text": "and finally there are tombstones which",
    "start": "272639",
    "end": "274720"
  },
  {
    "text": "cortex currently doesn't use",
    "start": "274720",
    "end": "277520"
  },
  {
    "text": "this is what meta json file looks like",
    "start": "277520",
    "end": "280080"
  },
  {
    "text": "it contains some information about",
    "start": "280080",
    "end": "282000"
  },
  {
    "text": "the block most importantly the time",
    "start": "282000",
    "end": "284400"
  },
  {
    "text": "range that it covers",
    "start": "284400",
    "end": "286000"
  },
  {
    "text": "but also some stats and information",
    "start": "286000",
    "end": "288000"
  },
  {
    "text": "about compaction",
    "start": "288000",
    "end": "289050"
  },
  {
    "text": "[Music]",
    "start": "289050",
    "end": "290720"
  },
  {
    "text": "now let's take a look at segment files",
    "start": "290720",
    "end": "292880"
  },
  {
    "text": "those numbered files under chunks",
    "start": "292880",
    "end": "294960"
  },
  {
    "text": "directory",
    "start": "294960",
    "end": "296479"
  },
  {
    "text": "segment files contain individual chunks",
    "start": "296479",
    "end": "299919"
  },
  {
    "text": "these are similar to cortex chunks but",
    "start": "299919",
    "end": "302080"
  },
  {
    "text": "much simpler they only contain",
    "start": "302080",
    "end": "304000"
  },
  {
    "text": "samples chunks are stored one next to",
    "start": "304000",
    "end": "307039"
  },
  {
    "text": "another in the segment file",
    "start": "307039",
    "end": "308639"
  },
  {
    "text": "until they take roughly half of gigabyte",
    "start": "308639",
    "end": "310960"
  },
  {
    "text": "of space",
    "start": "310960",
    "end": "311919"
  },
  {
    "text": "and then another segment file is started",
    "start": "311919",
    "end": "315280"
  },
  {
    "text": "each chunk has so-called reference",
    "start": "315280",
    "end": "317440"
  },
  {
    "text": "number which is basically the position",
    "start": "317440",
    "end": "319759"
  },
  {
    "text": "or the offset of the chunk in the",
    "start": "319759",
    "end": "321280"
  },
  {
    "text": "segment file combined with the number of",
    "start": "321280",
    "end": "323600"
  },
  {
    "text": "segment",
    "start": "323600",
    "end": "324080"
  },
  {
    "text": "file 001 in this example",
    "start": "324080",
    "end": "327199"
  },
  {
    "text": "encoding these two values together gives",
    "start": "327199",
    "end": "329520"
  },
  {
    "text": "us a reference number",
    "start": "329520",
    "end": "332400"
  },
  {
    "text": "index in tsdb block is a single file and",
    "start": "332400",
    "end": "335280"
  },
  {
    "text": "it is quite complicated internally",
    "start": "335280",
    "end": "337360"
  },
  {
    "text": "for maximum efficiency but the basic",
    "start": "337360",
    "end": "340000"
  },
  {
    "text": "idea is simple",
    "start": "340000",
    "end": "342000"
  },
  {
    "text": "it contains information about all time",
    "start": "342000",
    "end": "344320"
  },
  {
    "text": "series in the block",
    "start": "344320",
    "end": "346479"
  },
  {
    "text": "that means list of labels list of chunks",
    "start": "346479",
    "end": "349840"
  },
  {
    "text": "or the reference numbers together with",
    "start": "349840",
    "end": "352160"
  },
  {
    "text": "minimum and maximum time",
    "start": "352160",
    "end": "353600"
  },
  {
    "text": "for each chunk index also contains",
    "start": "353600",
    "end": "357600"
  },
  {
    "text": "all label value pairs for fast lookup",
    "start": "357600",
    "end": "360639"
  },
  {
    "text": "and these label value pairs are then",
    "start": "360639",
    "end": "362720"
  },
  {
    "text": "mapped to series ids",
    "start": "362720",
    "end": "365199"
  },
  {
    "text": "and that's basically it now you know",
    "start": "365199",
    "end": "367360"
  },
  {
    "text": "what's inside your tsdb blocks",
    "start": "367360",
    "end": "370080"
  },
  {
    "text": "it's interesting to see that prometheus",
    "start": "370080",
    "end": "371840"
  },
  {
    "text": "does not encode series type like gauge",
    "start": "371840",
    "end": "374160"
  },
  {
    "text": "or counter in the index",
    "start": "374160",
    "end": "376720"
  },
  {
    "text": "what is important about the esdb blocks",
    "start": "376720",
    "end": "378880"
  },
  {
    "text": "is that they are standalone",
    "start": "378880",
    "end": "380560"
  },
  {
    "text": "each block is independent from any other",
    "start": "380560",
    "end": "382720"
  },
  {
    "text": "block",
    "start": "382720",
    "end": "383600"
  },
  {
    "text": "blocks can be merged together into",
    "start": "383600",
    "end": "385360"
  },
  {
    "text": "larger blocks through the process called",
    "start": "385360",
    "end": "387440"
  },
  {
    "text": "compaction",
    "start": "387440",
    "end": "388479"
  },
  {
    "text": "and this helps to save the disk space",
    "start": "388479",
    "end": "390560"
  },
  {
    "text": "because big",
    "start": "390560",
    "end": "391840"
  },
  {
    "text": "part of the index is typically the same",
    "start": "391840",
    "end": "394560"
  },
  {
    "text": "that is if you have the same label names",
    "start": "394560",
    "end": "396400"
  },
  {
    "text": "and values for",
    "start": "396400",
    "end": "397520"
  },
  {
    "text": "longer than couple of hours",
    "start": "397520",
    "end": "400879"
  },
  {
    "text": "to learn more about esdb blocks i can",
    "start": "400960",
    "end": "403120"
  },
  {
    "text": "highly recommend",
    "start": "403120",
    "end": "404240"
  },
  {
    "text": "series of blog posts by my colleague",
    "start": "404240",
    "end": "406560"
  },
  {
    "text": "ganesh vernicar",
    "start": "406560",
    "end": "408000"
  },
  {
    "text": "who maintains the sdb library in",
    "start": "408000",
    "end": "409759"
  },
  {
    "text": "prometheus",
    "start": "409759",
    "end": "411039"
  },
  {
    "text": "slides contain the link if you are",
    "start": "411039",
    "end": "412720"
  },
  {
    "text": "interested",
    "start": "412720",
    "end": "414800"
  },
  {
    "text": "so how are tsdb blocks used by cortex",
    "start": "414800",
    "end": "418960"
  },
  {
    "text": "cortex components put incoming data to",
    "start": "418960",
    "end": "421520"
  },
  {
    "text": "blocks",
    "start": "421520",
    "end": "422240"
  },
  {
    "text": "and upload them to object storage like",
    "start": "422240",
    "end": "424319"
  },
  {
    "text": "gcs rs3",
    "start": "424319",
    "end": "426319"
  },
  {
    "text": "we define our own structure for storing",
    "start": "426319",
    "end": "428639"
  },
  {
    "text": "blocks",
    "start": "428639",
    "end": "429520"
  },
  {
    "text": "basically each user has its own",
    "start": "429520",
    "end": "431039"
  },
  {
    "text": "directory full of blocks",
    "start": "431039",
    "end": "433440"
  },
  {
    "text": "and we also put little extra metadata",
    "start": "433440",
    "end": "436160"
  },
  {
    "text": "into meta json file",
    "start": "436160",
    "end": "438000"
  },
  {
    "text": "but otherwise we just use plenty sdb",
    "start": "438000",
    "end": "440560"
  },
  {
    "text": "blocks",
    "start": "440560",
    "end": "442000"
  },
  {
    "text": "in cortex we generate two hours blocks",
    "start": "442000",
    "end": "444800"
  },
  {
    "text": "at first",
    "start": "444800",
    "end": "445759"
  },
  {
    "text": "but later we compacted them into one day",
    "start": "445759",
    "end": "448479"
  },
  {
    "text": "blocks",
    "start": "448479",
    "end": "449039"
  },
  {
    "text": "that is each block covers 24 hours of",
    "start": "449039",
    "end": "451919"
  },
  {
    "text": "data",
    "start": "451919",
    "end": "454160"
  },
  {
    "text": "we have seen how cortex chunk storage",
    "start": "454160",
    "end": "456400"
  },
  {
    "text": "looks like",
    "start": "456400",
    "end": "457360"
  },
  {
    "text": "and what tsdb blocks look like",
    "start": "457360",
    "end": "460560"
  },
  {
    "text": "now how do we convert chunks or cortex",
    "start": "460560",
    "end": "463520"
  },
  {
    "text": "chunks",
    "start": "463520",
    "end": "464080"
  },
  {
    "text": "to tsdb blocks",
    "start": "464080",
    "end": "467280"
  },
  {
    "text": "to generate block we need to find series",
    "start": "467280",
    "end": "469440"
  },
  {
    "text": "and chunks that belong to this block",
    "start": "469440",
    "end": "472240"
  },
  {
    "text": "since we want one block per day per user",
    "start": "472240",
    "end": "475039"
  },
  {
    "text": "that means",
    "start": "475039",
    "end": "475599"
  },
  {
    "text": "finding series for specific user that",
    "start": "475599",
    "end": "477919"
  },
  {
    "text": "has samples",
    "start": "477919",
    "end": "478800"
  },
  {
    "text": "in that specific day unfortunately",
    "start": "478800",
    "end": "482080"
  },
  {
    "text": "cortex index was not designed with these",
    "start": "482080",
    "end": "484479"
  },
  {
    "text": "requirements in mind",
    "start": "484479",
    "end": "486319"
  },
  {
    "text": "maybe thinking what isn't that isn't",
    "start": "486319",
    "end": "488720"
  },
  {
    "text": "that what index is for",
    "start": "488720",
    "end": "491120"
  },
  {
    "text": "yeah well index is designed to do",
    "start": "491120",
    "end": "493759"
  },
  {
    "text": "lookups for",
    "start": "493759",
    "end": "494800"
  },
  {
    "text": "label and value pairs but not to find",
    "start": "494800",
    "end": "496960"
  },
  {
    "text": "all series for a user",
    "start": "496960",
    "end": "498960"
  },
  {
    "text": "in fact it's even difficult to quickly",
    "start": "498960",
    "end": "501120"
  },
  {
    "text": "find all users",
    "start": "501120",
    "end": "503759"
  },
  {
    "text": "in addition to that cortex uses",
    "start": "503759",
    "end": "505680"
  },
  {
    "text": "different versions of index",
    "start": "505680",
    "end": "507759"
  },
  {
    "text": "fortunately in our production databases",
    "start": "507759",
    "end": "511039"
  },
  {
    "text": "we have only used version 9 and later",
    "start": "511039",
    "end": "513599"
  },
  {
    "text": "which are compatible",
    "start": "513599",
    "end": "514880"
  },
  {
    "text": "so that is what we have focused on",
    "start": "514880",
    "end": "518399"
  },
  {
    "text": "on the other hand we can read the entire",
    "start": "518399",
    "end": "520959"
  },
  {
    "text": "index",
    "start": "520959",
    "end": "521839"
  },
  {
    "text": "and generate lists of series and chunks",
    "start": "521839",
    "end": "524240"
  },
  {
    "text": "that should be put into each block",
    "start": "524240",
    "end": "526640"
  },
  {
    "text": "and that is actually pretty easy and",
    "start": "526640",
    "end": "528480"
  },
  {
    "text": "efficient to do",
    "start": "528480",
    "end": "530080"
  },
  {
    "text": "it just requires single full scan of the",
    "start": "530080",
    "end": "532560"
  },
  {
    "text": "index",
    "start": "532560",
    "end": "533839"
  },
  {
    "text": "and this observation draw the design of",
    "start": "533839",
    "end": "535839"
  },
  {
    "text": "conversion tooling",
    "start": "535839",
    "end": "538720"
  },
  {
    "text": "our conversion tooling has three main",
    "start": "539279",
    "end": "541279"
  },
  {
    "text": "components",
    "start": "541279",
    "end": "542720"
  },
  {
    "text": "index scanner for doing full index scan",
    "start": "542720",
    "end": "545519"
  },
  {
    "text": "and generating plan files",
    "start": "545519",
    "end": "547279"
  },
  {
    "text": "which is basically a list of series and",
    "start": "547279",
    "end": "549440"
  },
  {
    "text": "chunks",
    "start": "549440",
    "end": "550320"
  },
  {
    "text": "that belong to a single block",
    "start": "550320",
    "end": "553440"
  },
  {
    "text": "block builder which uses single plan",
    "start": "553440",
    "end": "556320"
  },
  {
    "text": "file",
    "start": "556320",
    "end": "557440"
  },
  {
    "text": "downloads all chunks and constructs tsdb",
    "start": "557440",
    "end": "560080"
  },
  {
    "text": "blocks",
    "start": "560080",
    "end": "561200"
  },
  {
    "text": "and finally scheduler which monitors",
    "start": "561200",
    "end": "563360"
  },
  {
    "text": "available blend files",
    "start": "563360",
    "end": "564880"
  },
  {
    "text": "and distributes them to block builders",
    "start": "564880",
    "end": "568800"
  },
  {
    "text": "i will shortly talk about these",
    "start": "568800",
    "end": "570320"
  },
  {
    "text": "components now but",
    "start": "570320",
    "end": "571839"
  },
  {
    "text": "if you want even more details there is a",
    "start": "571839",
    "end": "574160"
  },
  {
    "text": "link to the design document",
    "start": "574160",
    "end": "577839"
  },
  {
    "text": "scanner scans the index tables in cortex",
    "start": "578240",
    "end": "582320"
  },
  {
    "text": "our index is divided into individual",
    "start": "582320",
    "end": "584800"
  },
  {
    "text": "tables",
    "start": "584800",
    "end": "585440"
  },
  {
    "text": "each table covers one week of data",
    "start": "585440",
    "end": "588560"
  },
  {
    "text": "when scanner scans the table it is",
    "start": "588560",
    "end": "591040"
  },
  {
    "text": "processing",
    "start": "591040",
    "end": "591680"
  },
  {
    "text": "all index entries in some order due to",
    "start": "591680",
    "end": "594800"
  },
  {
    "text": "how cortex",
    "start": "594800",
    "end": "595600"
  },
  {
    "text": "stores index entries this order is",
    "start": "595600",
    "end": "598160"
  },
  {
    "text": "basically random because",
    "start": "598160",
    "end": "599600"
  },
  {
    "text": "cortex prefixes each entry with the hash",
    "start": "599600",
    "end": "603200"
  },
  {
    "text": "for better key distribution at least",
    "start": "603200",
    "end": "606480"
  },
  {
    "text": "that is the case when cortex stores",
    "start": "606480",
    "end": "608800"
  },
  {
    "text": "index in bigtable which is what we have",
    "start": "608800",
    "end": "610720"
  },
  {
    "text": "used in our production",
    "start": "610720",
    "end": "611850"
  },
  {
    "text": "[Music]",
    "start": "611850",
    "end": "613279"
  },
  {
    "text": "while reading all index entries scanner",
    "start": "613279",
    "end": "616160"
  },
  {
    "text": "only selects",
    "start": "616160",
    "end": "616959"
  },
  {
    "text": "entries that describe the mapping",
    "start": "616959",
    "end": "618480"
  },
  {
    "text": "between series ids",
    "start": "618480",
    "end": "620079"
  },
  {
    "text": "and chunks and stores these mappings",
    "start": "620079",
    "end": "622880"
  },
  {
    "text": "into a file",
    "start": "622880",
    "end": "624399"
  },
  {
    "text": "we know minimum and maximum time of",
    "start": "624399",
    "end": "626240"
  },
  {
    "text": "chunk because it's part of the chunk id",
    "start": "626240",
    "end": "628959"
  },
  {
    "text": "so we know into which days the chunk",
    "start": "628959",
    "end": "630959"
  },
  {
    "text": "belongs",
    "start": "630959",
    "end": "632640"
  },
  {
    "text": "so in the end the scanner produces one",
    "start": "632640",
    "end": "635120"
  },
  {
    "text": "file per user and day",
    "start": "635120",
    "end": "637279"
  },
  {
    "text": "and this file contains complete list of",
    "start": "637279",
    "end": "639680"
  },
  {
    "text": "series ids and their chunks",
    "start": "639680",
    "end": "641839"
  },
  {
    "text": "and this is called plan file if you have",
    "start": "641839",
    "end": "645279"
  },
  {
    "text": "thousands of users",
    "start": "645279",
    "end": "646800"
  },
  {
    "text": "in the index scanning single table will",
    "start": "646800",
    "end": "649760"
  },
  {
    "text": "produce seven",
    "start": "649760",
    "end": "650640"
  },
  {
    "text": "times that number of users of pain files",
    "start": "650640",
    "end": "653200"
  },
  {
    "text": "one per day per user",
    "start": "653200",
    "end": "656079"
  },
  {
    "text": "plan files are then uploaded to the",
    "start": "656079",
    "end": "658160"
  },
  {
    "text": "object storage bucket",
    "start": "658160",
    "end": "659760"
  },
  {
    "text": "and scanner can continue with the next",
    "start": "659760",
    "end": "662399"
  },
  {
    "text": "table until it processes all of them",
    "start": "662399",
    "end": "665760"
  },
  {
    "text": "if new table appears which can happen",
    "start": "665760",
    "end": "667920"
  },
  {
    "text": "because entire conversion take",
    "start": "667920",
    "end": "670399"
  },
  {
    "text": "takes many days",
    "start": "670399",
    "end": "673440"
  },
  {
    "text": "and customers are pushing new data in in",
    "start": "673440",
    "end": "676240"
  },
  {
    "text": "this time",
    "start": "676240",
    "end": "677760"
  },
  {
    "text": "scanner can still process the new tables",
    "start": "677760",
    "end": "683360"
  },
  {
    "text": "originally scanner has supported",
    "start": "683680",
    "end": "685760"
  },
  {
    "text": "bigtable only but now it also supports",
    "start": "685760",
    "end": "687839"
  },
  {
    "text": "dynamodb",
    "start": "687839",
    "end": "689040"
  },
  {
    "text": "and there was some community work on",
    "start": "689040",
    "end": "690959"
  },
  {
    "text": "adding cassandra support",
    "start": "690959",
    "end": "693440"
  },
  {
    "text": "scanner is not horizontally scalable by",
    "start": "693440",
    "end": "695519"
  },
  {
    "text": "design",
    "start": "695519",
    "end": "696480"
  },
  {
    "text": "it's typically pretty fast already",
    "start": "696480",
    "end": "700160"
  },
  {
    "text": "block builder is the main component of",
    "start": "700560",
    "end": "702320"
  },
  {
    "text": "the tooling because it generates the sdb",
    "start": "702320",
    "end": "704640"
  },
  {
    "text": "blocks",
    "start": "704640",
    "end": "705839"
  },
  {
    "text": "block builder is told which plan to work",
    "start": "705839",
    "end": "708160"
  },
  {
    "text": "on",
    "start": "708160",
    "end": "709040"
  },
  {
    "text": "and then it downloads the plan and",
    "start": "709040",
    "end": "711360"
  },
  {
    "text": "fetches all the chunks from chunk store",
    "start": "711360",
    "end": "715360"
  },
  {
    "text": "quote builder then builds tsdb index and",
    "start": "715360",
    "end": "717680"
  },
  {
    "text": "segment files",
    "start": "717680",
    "end": "719120"
  },
  {
    "text": "and uploads generated tsdb block back",
    "start": "719120",
    "end": "721600"
  },
  {
    "text": "into bucket",
    "start": "721600",
    "end": "722639"
  },
  {
    "text": "where cortex can find it block builder",
    "start": "722639",
    "end": "726240"
  },
  {
    "text": "takes care of many details that",
    "start": "726240",
    "end": "728480"
  },
  {
    "text": "need to be right it deduplicates samples",
    "start": "728480",
    "end": "731839"
  },
  {
    "text": "from multiple chunks in the same series",
    "start": "731839",
    "end": "734560"
  },
  {
    "text": "chunks such chunks may appear because of",
    "start": "734560",
    "end": "737680"
  },
  {
    "text": "replication",
    "start": "737680",
    "end": "738560"
  },
  {
    "text": "used in cortex builder can also fix",
    "start": "738560",
    "end": "741760"
  },
  {
    "text": "some bugs introduced in cortex chunks",
    "start": "741760",
    "end": "744000"
  },
  {
    "text": "over time",
    "start": "744000",
    "end": "744880"
  },
  {
    "text": "for example duplicate labels builder",
    "start": "744880",
    "end": "748480"
  },
  {
    "text": "of course takes care of sorting series",
    "start": "748480",
    "end": "750560"
  },
  {
    "text": "before building index",
    "start": "750560",
    "end": "752399"
  },
  {
    "text": "otherwise index would not be correct and",
    "start": "752399",
    "end": "755760"
  },
  {
    "text": "that's so without getting unkilled",
    "start": "755760",
    "end": "759360"
  },
  {
    "text": "builder obviously knows how to produce",
    "start": "759360",
    "end": "761200"
  },
  {
    "text": "meta json file",
    "start": "761200",
    "end": "762720"
  },
  {
    "text": "with cortex metadata important point",
    "start": "762720",
    "end": "766480"
  },
  {
    "text": "about block builder is that once it has",
    "start": "766480",
    "end": "768399"
  },
  {
    "text": "the plan",
    "start": "768399",
    "end": "769440"
  },
  {
    "text": "it doesn't need to interact with cortex",
    "start": "769440",
    "end": "771440"
  },
  {
    "text": "index",
    "start": "771440",
    "end": "772800"
  },
  {
    "text": "it only needs to fetch chunks cortex",
    "start": "772800",
    "end": "776000"
  },
  {
    "text": "chunks already contain full information",
    "start": "776000",
    "end": "778160"
  },
  {
    "text": "about the series",
    "start": "778160",
    "end": "779519"
  },
  {
    "text": "like label names and values and that is",
    "start": "779519",
    "end": "782079"
  },
  {
    "text": "all that block builder needs",
    "start": "782079",
    "end": "785040"
  },
  {
    "text": "this also shows one reason why tsd is",
    "start": "785040",
    "end": "787519"
  },
  {
    "text": "more",
    "start": "787519",
    "end": "788160"
  },
  {
    "text": "space efficient than cortex chunk",
    "start": "788160",
    "end": "789839"
  },
  {
    "text": "storage namely",
    "start": "789839",
    "end": "791200"
  },
  {
    "text": "because the information about the same",
    "start": "791200",
    "end": "793120"
  },
  {
    "text": "series is not repeated in every chunk",
    "start": "793120",
    "end": "796320"
  },
  {
    "text": "when it is stored in tsdb instead the",
    "start": "796320",
    "end": "799360"
  },
  {
    "text": "series information is only stored once",
    "start": "799360",
    "end": "801440"
  },
  {
    "text": "in the tsdb",
    "start": "801440",
    "end": "802399"
  },
  {
    "text": "index block builder is horizontally",
    "start": "802399",
    "end": "805760"
  },
  {
    "text": "scalable",
    "start": "805760",
    "end": "806720"
  },
  {
    "text": "you can run many of them to get blocks",
    "start": "806720",
    "end": "808720"
  },
  {
    "text": "built faster",
    "start": "808720",
    "end": "811440"
  },
  {
    "text": "we have said before that index scanner",
    "start": "811440",
    "end": "813519"
  },
  {
    "text": "writes plans to the bucket",
    "start": "813519",
    "end": "815680"
  },
  {
    "text": "scheduler is the component that finds",
    "start": "815680",
    "end": "818000"
  },
  {
    "text": "those plans",
    "start": "818000",
    "end": "819199"
  },
  {
    "text": "and sends them to the blog builders",
    "start": "819199",
    "end": "822320"
  },
  {
    "text": "blog builders communicate their build",
    "start": "822320",
    "end": "824480"
  },
  {
    "text": "progress using another small file in the",
    "start": "824480",
    "end": "826639"
  },
  {
    "text": "bucket",
    "start": "826639",
    "end": "827199"
  },
  {
    "text": "which is regularly updated as long as",
    "start": "827199",
    "end": "829360"
  },
  {
    "text": "block builder is running",
    "start": "829360",
    "end": "830720"
  },
  {
    "text": "and working on the plan if scheduler",
    "start": "830720",
    "end": "833920"
  },
  {
    "text": "finds that the block builder hasn't",
    "start": "833920",
    "end": "835519"
  },
  {
    "text": "reported its progress recently",
    "start": "835519",
    "end": "837920"
  },
  {
    "text": "scheduler will consider such builder as",
    "start": "837920",
    "end": "840320"
  },
  {
    "text": "crashed",
    "start": "840320",
    "end": "841120"
  },
  {
    "text": "and will abort the build when",
    "start": "841120",
    "end": "844079"
  },
  {
    "text": "blockbuilder is done with the plan",
    "start": "844079",
    "end": "846160"
  },
  {
    "text": "it uploads finished file next to the",
    "start": "846160",
    "end": "848480"
  },
  {
    "text": "plan file",
    "start": "848480",
    "end": "849760"
  },
  {
    "text": "this tells scheduler that given plan is",
    "start": "849760",
    "end": "851839"
  },
  {
    "text": "finished",
    "start": "851839",
    "end": "853040"
  },
  {
    "text": "if build has failed blockbuilder will",
    "start": "853040",
    "end": "855199"
  },
  {
    "text": "upload failed file instead",
    "start": "855199",
    "end": "858560"
  },
  {
    "text": "this is a very simple orchestration",
    "start": "858560",
    "end": "860800"
  },
  {
    "text": "using object storage",
    "start": "860800",
    "end": "863120"
  },
  {
    "text": "note that the scanner doesn't need to",
    "start": "863120",
    "end": "864639"
  },
  {
    "text": "read the files it only issues list",
    "start": "864639",
    "end": "866800"
  },
  {
    "text": "commands",
    "start": "866800",
    "end": "867760"
  },
  {
    "text": "or information is encoded in file names",
    "start": "867760",
    "end": "871199"
  },
  {
    "text": "by doing single list scheduler knows",
    "start": "871199",
    "end": "874320"
  },
  {
    "text": "which plans are available",
    "start": "874320",
    "end": "876160"
  },
  {
    "text": "which are in progress or finished this",
    "start": "876160",
    "end": "878959"
  },
  {
    "text": "allows scheduler to update its",
    "start": "878959",
    "end": "880959"
  },
  {
    "text": "in-memory state every few minutes so",
    "start": "880959",
    "end": "883920"
  },
  {
    "text": "that when the blog builder",
    "start": "883920",
    "end": "885360"
  },
  {
    "text": "asks for the next plan scheduler can",
    "start": "885360",
    "end": "888160"
  },
  {
    "text": "return one",
    "start": "888160",
    "end": "890399"
  },
  {
    "text": "how did it work well it worked",
    "start": "890399",
    "end": "893680"
  },
  {
    "text": "we have successfully converted all",
    "start": "893680",
    "end": "895760"
  },
  {
    "text": "stored chunks data into tsdb blocks",
    "start": "895760",
    "end": "898959"
  },
  {
    "text": "and we could downscale our bigtable",
    "start": "898959",
    "end": "900880"
  },
  {
    "text": "instances",
    "start": "900880",
    "end": "902480"
  },
  {
    "text": "what have we learned that simple",
    "start": "902480",
    "end": "904880"
  },
  {
    "text": "distributed system is good",
    "start": "904880",
    "end": "907120"
  },
  {
    "text": "communication using buckets by listing",
    "start": "907120",
    "end": "909519"
  },
  {
    "text": "of files",
    "start": "909519",
    "end": "910160"
  },
  {
    "text": "works just fine and it's easy to",
    "start": "910160",
    "end": "912000"
  },
  {
    "text": "manipulate the state of jobs",
    "start": "912000",
    "end": "914000"
  },
  {
    "text": "by manipulating files in the bucket",
    "start": "914000",
    "end": "917199"
  },
  {
    "text": "for example if you delete the failure",
    "start": "917199",
    "end": "919040"
  },
  {
    "text": "file scheduler will see that the plan",
    "start": "919040",
    "end": "921920"
  },
  {
    "text": "needs to be built again",
    "start": "921920",
    "end": "923040"
  },
  {
    "text": "and will give it to the block builders",
    "start": "923040",
    "end": "926160"
  },
  {
    "text": "were there any problems of course there",
    "start": "926160",
    "end": "928800"
  },
  {
    "text": "were",
    "start": "928800",
    "end": "929600"
  },
  {
    "text": "first issue we have run into was related",
    "start": "929600",
    "end": "931920"
  },
  {
    "text": "to meta json files",
    "start": "931920",
    "end": "934079"
  },
  {
    "text": "in early versions of both builder",
    "start": "934079",
    "end": "937199"
  },
  {
    "text": "we did not correctly set all json file",
    "start": "937199",
    "end": "940079"
  },
  {
    "text": "details",
    "start": "940079",
    "end": "940880"
  },
  {
    "text": "especially compaction section cortex",
    "start": "940880",
    "end": "943680"
  },
  {
    "text": "compactor was then confused",
    "start": "943680",
    "end": "945440"
  },
  {
    "text": "by such box and deleted newly built",
    "start": "945440",
    "end": "948639"
  },
  {
    "text": "blocks soon after they were uploaded to",
    "start": "948639",
    "end": "950639"
  },
  {
    "text": "the bucket",
    "start": "950639",
    "end": "952000"
  },
  {
    "text": "we did not catch this in testing because",
    "start": "952000",
    "end": "954000"
  },
  {
    "text": "we didn't run compactor in cluster where",
    "start": "954000",
    "end": "956079"
  },
  {
    "text": "the test was done",
    "start": "956079",
    "end": "957680"
  },
  {
    "text": "but once fixed one once fixed we were",
    "start": "957680",
    "end": "961040"
  },
  {
    "text": "quickly able to just redo those blocks",
    "start": "961040",
    "end": "963440"
  },
  {
    "text": "again",
    "start": "963440",
    "end": "965120"
  },
  {
    "text": "another issue we have hit was related to",
    "start": "965120",
    "end": "967600"
  },
  {
    "text": "memory",
    "start": "967600",
    "end": "968720"
  },
  {
    "text": "when writing tsdb index we were trying",
    "start": "968720",
    "end": "971440"
  },
  {
    "text": "to sort",
    "start": "971440",
    "end": "972160"
  },
  {
    "text": "all series in memory but some of our",
    "start": "972160",
    "end": "974720"
  },
  {
    "text": "customers had so many series that we",
    "start": "974720",
    "end": "976639"
  },
  {
    "text": "could not fit all the labels in the",
    "start": "976639",
    "end": "978560"
  },
  {
    "text": "memory during the block build",
    "start": "978560",
    "end": "981279"
  },
  {
    "text": "after retrying the job several times",
    "start": "981279",
    "end": "983279"
  },
  {
    "text": "with more and more memory",
    "start": "983279",
    "end": "984959"
  },
  {
    "text": "we eventually gave up at 30 gigabytes",
    "start": "984959",
    "end": "987680"
  },
  {
    "text": "and stored series data on the disk",
    "start": "987680",
    "end": "989680"
  },
  {
    "text": "instead with some sorting afterwards",
    "start": "989680",
    "end": "992079"
  },
  {
    "text": "that fixed the issue and last point i",
    "start": "992079",
    "end": "995279"
  },
  {
    "text": "have",
    "start": "995279",
    "end": "996000"
  },
  {
    "text": "is horizontal scaling for the win by",
    "start": "996000",
    "end": "998639"
  },
  {
    "text": "using plan files we split the whole",
    "start": "998639",
    "end": "1000560"
  },
  {
    "text": "conversion task into many small sub",
    "start": "1000560",
    "end": "1002399"
  },
  {
    "text": "tasks",
    "start": "1002399",
    "end": "1003279"
  },
  {
    "text": "allowing block builder to be",
    "start": "1003279",
    "end": "1004560"
  },
  {
    "text": "horizontally scalable",
    "start": "1004560",
    "end": "1006880"
  },
  {
    "text": "that is we could just run more of them",
    "start": "1006880",
    "end": "1008639"
  },
  {
    "text": "to proceed faster",
    "start": "1008639",
    "end": "1010320"
  },
  {
    "text": "one of our clusters contains thousands",
    "start": "1010320",
    "end": "1013040"
  },
  {
    "text": "of tiny tenants",
    "start": "1013040",
    "end": "1014720"
  },
  {
    "text": "and number of plans on this cluster was",
    "start": "1014720",
    "end": "1016720"
  },
  {
    "text": "in millions",
    "start": "1016720",
    "end": "1018240"
  },
  {
    "text": "but we got it converted in just couple",
    "start": "1018240",
    "end": "1020160"
  },
  {
    "text": "of days by running many",
    "start": "1020160",
    "end": "1022160"
  },
  {
    "text": "many block builders if you have your own",
    "start": "1022160",
    "end": "1026319"
  },
  {
    "text": "chunks data that you would like to cover",
    "start": "1026319",
    "end": "1028079"
  },
  {
    "text": "to blocks all this tooling is part of",
    "start": "1028079",
    "end": "1030480"
  },
  {
    "text": "open source cortex codebase",
    "start": "1030480",
    "end": "1033760"
  },
  {
    "text": "in the last part of the talk i want to",
    "start": "1033760",
    "end": "1036000"
  },
  {
    "text": "show you",
    "start": "1036000",
    "end": "1037280"
  },
  {
    "text": "how you can write the sdb blocks by some",
    "start": "1037280",
    "end": "1039520"
  },
  {
    "text": "simple go code",
    "start": "1039520",
    "end": "1041520"
  },
  {
    "text": "of course if you want to convert some",
    "start": "1041520",
    "end": "1043120"
  },
  {
    "text": "data to tsdb blocks",
    "start": "1043120",
    "end": "1045038"
  },
  {
    "text": "you can also use the new prometheus",
    "start": "1045039",
    "end": "1047120"
  },
  {
    "text": "backfilling tool",
    "start": "1047120",
    "end": "1048319"
  },
  {
    "text": "but sometimes it may be more efficient",
    "start": "1048319",
    "end": "1050240"
  },
  {
    "text": "to write the sdb blocks directly",
    "start": "1050240",
    "end": "1052480"
  },
  {
    "text": "without converting data to open metrics",
    "start": "1052480",
    "end": "1054720"
  },
  {
    "text": "format first",
    "start": "1054720",
    "end": "1056400"
  },
  {
    "text": "and it's also easy and fun to generate",
    "start": "1056400",
    "end": "1059679"
  },
  {
    "text": "the sdb block",
    "start": "1059679",
    "end": "1060640"
  },
  {
    "text": "we need to just we need to generate",
    "start": "1060640",
    "end": "1063679"
  },
  {
    "text": "sorry we need to do three steps write",
    "start": "1063679",
    "end": "1066480"
  },
  {
    "text": "chunks into segment files",
    "start": "1066480",
    "end": "1068240"
  },
  {
    "text": "write index and red method json file",
    "start": "1068240",
    "end": "1071919"
  },
  {
    "text": "and we will use tsd library from pro",
    "start": "1071919",
    "end": "1074160"
  },
  {
    "text": "methods to do that",
    "start": "1074160",
    "end": "1076080"
  },
  {
    "text": "you can find full example at provided",
    "start": "1076080",
    "end": "1078840"
  },
  {
    "text": "link",
    "start": "1078840",
    "end": "1080880"
  },
  {
    "text": "this code example shows how to prepare",
    "start": "1080880",
    "end": "1083039"
  },
  {
    "text": "chunks for a single time series",
    "start": "1083039",
    "end": "1084960"
  },
  {
    "text": "from given list of samples each sample",
    "start": "1084960",
    "end": "1087600"
  },
  {
    "text": "has a timestamp",
    "start": "1087600",
    "end": "1088720"
  },
  {
    "text": "and a float64 value we need to iterate",
    "start": "1088720",
    "end": "1092240"
  },
  {
    "text": "through all samples",
    "start": "1092240",
    "end": "1094559"
  },
  {
    "text": "we convert the timestamp to the unix",
    "start": "1094559",
    "end": "1096320"
  },
  {
    "text": "timestamp in milliseconds",
    "start": "1096320",
    "end": "1098000"
  },
  {
    "text": "and because that is what prometheus",
    "start": "1098000",
    "end": "1100160"
  },
  {
    "text": "expects to find in tsd",
    "start": "1100160",
    "end": "1101760"
  },
  {
    "text": "block we check we have appender for",
    "start": "1101760",
    "end": "1104400"
  },
  {
    "text": "trunk",
    "start": "1104400",
    "end": "1104960"
  },
  {
    "text": "if not we create a new sort chunk which",
    "start": "1104960",
    "end": "1107360"
  },
  {
    "text": "is used by prometheus",
    "start": "1107360",
    "end": "1108960"
  },
  {
    "text": "and we get a pander for this chunk",
    "start": "1108960",
    "end": "1111760"
  },
  {
    "text": "appender is used to write",
    "start": "1111760",
    "end": "1113760"
  },
  {
    "text": "individual samples to the chunk we are",
    "start": "1113760",
    "end": "1116640"
  },
  {
    "text": "not only building chunks but",
    "start": "1116640",
    "end": "1118240"
  },
  {
    "text": "also chunk meta information with minimum",
    "start": "1118240",
    "end": "1120799"
  },
  {
    "text": "and maximum time",
    "start": "1120799",
    "end": "1122799"
  },
  {
    "text": "now that we have appender we can write",
    "start": "1122799",
    "end": "1124640"
  },
  {
    "text": "samples to chunk",
    "start": "1124640",
    "end": "1127520"
  },
  {
    "text": "we simply append sample to the appender",
    "start": "1127520",
    "end": "1129679"
  },
  {
    "text": "and update",
    "start": "1129679",
    "end": "1130799"
  },
  {
    "text": "max time in metadata in this step we",
    "start": "1130799",
    "end": "1133360"
  },
  {
    "text": "assume that timestamps",
    "start": "1133360",
    "end": "1134799"
  },
  {
    "text": "for samples are increasing that is how",
    "start": "1134799",
    "end": "1137840"
  },
  {
    "text": "prometheus expects to find samples in",
    "start": "1137840",
    "end": "1140240"
  },
  {
    "text": "the chunk",
    "start": "1140240",
    "end": "1142000"
  },
  {
    "text": "after writing 120 samples into a chunk",
    "start": "1142000",
    "end": "1145280"
  },
  {
    "text": "we finish the chunk append metadata to",
    "start": "1145280",
    "end": "1148000"
  },
  {
    "text": "our list of metadatas",
    "start": "1148000",
    "end": "1149360"
  },
  {
    "text": "and set appender to nil which will cause",
    "start": "1149360",
    "end": "1152240"
  },
  {
    "text": "creation of new chunk in the next",
    "start": "1152240",
    "end": "1153840"
  },
  {
    "text": "iteration",
    "start": "1153840",
    "end": "1155120"
  },
  {
    "text": "and that's it this creates chunks",
    "start": "1155120",
    "end": "1159039"
  },
  {
    "text": "chunks are written to the disk using",
    "start": "1159039",
    "end": "1160720"
  },
  {
    "text": "chunk writer one can get the chunk",
    "start": "1160720",
    "end": "1163120"
  },
  {
    "text": "writer from tsdb",
    "start": "1163120",
    "end": "1164400"
  },
  {
    "text": "chunks package and it currently has",
    "start": "1164400",
    "end": "1167360"
  },
  {
    "text": "these two methods",
    "start": "1167360",
    "end": "1168559"
  },
  {
    "text": "right chunks and close red chunks can be",
    "start": "1168559",
    "end": "1171919"
  },
  {
    "text": "called many times",
    "start": "1171919",
    "end": "1172960"
  },
  {
    "text": "we give it a slice of chunks to write",
    "start": "1172960",
    "end": "1175039"
  },
  {
    "text": "and after writing them all",
    "start": "1175039",
    "end": "1176799"
  },
  {
    "text": "we can close it there are few important",
    "start": "1176799",
    "end": "1180000"
  },
  {
    "text": "details",
    "start": "1180000",
    "end": "1180880"
  },
  {
    "text": "chunks must be sorted in the same order",
    "start": "1180880",
    "end": "1183360"
  },
  {
    "text": "as is the order of series in the index",
    "start": "1183360",
    "end": "1186320"
  },
  {
    "text": "prometheus does it this way and",
    "start": "1186320",
    "end": "1188559"
  },
  {
    "text": "prometheus may use this fact for future",
    "start": "1188559",
    "end": "1190840"
  },
  {
    "text": "optimizations",
    "start": "1190840",
    "end": "1192799"
  },
  {
    "text": "within single series chunks must be",
    "start": "1192799",
    "end": "1195200"
  },
  {
    "text": "sorted by increasing time",
    "start": "1195200",
    "end": "1197919"
  },
  {
    "text": "calling great chunks will update",
    "start": "1197919",
    "end": "1199600"
  },
  {
    "text": "reference field of each chunk",
    "start": "1199600",
    "end": "1201520"
  },
  {
    "text": "metastructure this reference field is",
    "start": "1201520",
    "end": "1204880"
  },
  {
    "text": "important in the next step",
    "start": "1204880",
    "end": "1206240"
  },
  {
    "text": "writing index index writer has two",
    "start": "1206240",
    "end": "1209520"
  },
  {
    "text": "important",
    "start": "1209520",
    "end": "1210159"
  },
  {
    "text": "methods first you must call add a symbol",
    "start": "1210159",
    "end": "1214480"
  },
  {
    "text": "with all symbols that is label names and",
    "start": "1214480",
    "end": "1217120"
  },
  {
    "text": "values",
    "start": "1217120",
    "end": "1218880"
  },
  {
    "text": "and these symbols must be sorted and",
    "start": "1218880",
    "end": "1220799"
  },
  {
    "text": "each symbol must be added exactly once",
    "start": "1220799",
    "end": "1224240"
  },
  {
    "text": "after writing all symbols to the index",
    "start": "1224240",
    "end": "1226480"
  },
  {
    "text": "we can finally start adding series to",
    "start": "1226480",
    "end": "1228640"
  },
  {
    "text": "the index",
    "start": "1228640",
    "end": "1229760"
  },
  {
    "text": "to add a series we need to pass in ref",
    "start": "1229760",
    "end": "1232720"
  },
  {
    "text": "number",
    "start": "1232720",
    "end": "1233760"
  },
  {
    "text": "set of labels that the series uses and",
    "start": "1233760",
    "end": "1236320"
  },
  {
    "text": "chunks",
    "start": "1236320",
    "end": "1237120"
  },
  {
    "text": "and the slice of chunks meta",
    "start": "1237120",
    "end": "1240159"
  },
  {
    "text": "structures these chunks meta structures",
    "start": "1240159",
    "end": "1243520"
  },
  {
    "text": "must have a reference field set and as",
    "start": "1243520",
    "end": "1245760"
  },
  {
    "text": "we have said on previous slide it is set",
    "start": "1245760",
    "end": "1248320"
  },
  {
    "text": "after chunks are written to the chunk",
    "start": "1248320",
    "end": "1250559"
  },
  {
    "text": "writer",
    "start": "1250559",
    "end": "1252400"
  },
  {
    "text": "the reference number passed as first",
    "start": "1252400",
    "end": "1254320"
  },
  {
    "text": "argument is somewhat strange",
    "start": "1254320",
    "end": "1255840"
  },
  {
    "text": "it's really not needed and the",
    "start": "1255840",
    "end": "1258080"
  },
  {
    "text": "implementation of of index writer in",
    "start": "1258080",
    "end": "1260400"
  },
  {
    "text": "tsdb library only uses it to check that",
    "start": "1260400",
    "end": "1262799"
  },
  {
    "text": "it's",
    "start": "1262799",
    "end": "1263360"
  },
  {
    "text": "increased between calls after writing",
    "start": "1263360",
    "end": "1266640"
  },
  {
    "text": "chunks and index we are almost finished",
    "start": "1266640",
    "end": "1268559"
  },
  {
    "text": "with the block",
    "start": "1268559",
    "end": "1269440"
  },
  {
    "text": "last piece that is missing is meta json",
    "start": "1269440",
    "end": "1271760"
  },
  {
    "text": "file",
    "start": "1271760",
    "end": "1274159"
  },
  {
    "text": "and this file is represented by block",
    "start": "1274400",
    "end": "1276720"
  },
  {
    "text": "meta type from the",
    "start": "1276720",
    "end": "1278000"
  },
  {
    "text": "tsdb library we need to fill all the",
    "start": "1278000",
    "end": "1280960"
  },
  {
    "text": "fields",
    "start": "1280960",
    "end": "1281760"
  },
  {
    "text": "and most of them are pretty",
    "start": "1281760",
    "end": "1283000"
  },
  {
    "text": "self-explanatory version is currently",
    "start": "1283000",
    "end": "1285440"
  },
  {
    "text": "one",
    "start": "1285440",
    "end": "1285840"
  },
  {
    "text": "unfortunately this constant is not",
    "start": "1285840",
    "end": "1289039"
  },
  {
    "text": "public in tsd library",
    "start": "1289039",
    "end": "1292720"
  },
  {
    "text": "compaction information must be set as",
    "start": "1292720",
    "end": "1294720"
  },
  {
    "text": "well and we simply set block source to",
    "start": "1294720",
    "end": "1297200"
  },
  {
    "text": "itself",
    "start": "1297200",
    "end": "1298159"
  },
  {
    "text": "the failure to do this may confuse",
    "start": "1298159",
    "end": "1299919"
  },
  {
    "text": "compactor",
    "start": "1299919",
    "end": "1301679"
  },
  {
    "text": "after writing meta.json file to disk we",
    "start": "1301679",
    "end": "1304159"
  },
  {
    "text": "have a complete block",
    "start": "1304159",
    "end": "1305520"
  },
  {
    "text": "that we can use with prometheus for",
    "start": "1305520",
    "end": "1308080"
  },
  {
    "text": "thanos and cortex",
    "start": "1308080",
    "end": "1309600"
  },
  {
    "text": "we need extra bit of metadata in the",
    "start": "1309600",
    "end": "1312159"
  },
  {
    "text": "meta json file but nothing too complex",
    "start": "1312159",
    "end": "1315760"
  },
  {
    "text": "you can find the fully functional",
    "start": "1315760",
    "end": "1317360"
  },
  {
    "text": "example code at this address",
    "start": "1317360",
    "end": "1319919"
  },
  {
    "text": "it contains a tiny program that",
    "start": "1319919",
    "end": "1321440"
  },
  {
    "text": "generates the usdb block with the single",
    "start": "1321440",
    "end": "1323440"
  },
  {
    "text": "series",
    "start": "1323440",
    "end": "1324880"
  },
  {
    "text": "if you put this generated block into",
    "start": "1324880",
    "end": "1326559"
  },
  {
    "text": "prometheus you can query it and",
    "start": "1326559",
    "end": "1328880"
  },
  {
    "text": "see a sine wave if you want to use this",
    "start": "1328880",
    "end": "1331919"
  },
  {
    "text": "approach to generate the sdb blocks",
    "start": "1331919",
    "end": "1334080"
  },
  {
    "text": "check out prometheus storage",
    "start": "1334080",
    "end": "1335600"
  },
  {
    "text": "documentation and section about",
    "start": "1335600",
    "end": "1337440"
  },
  {
    "text": "backfilling",
    "start": "1337440",
    "end": "1338480"
  },
  {
    "text": "there are some small things to keep in",
    "start": "1338480",
    "end": "1340159"
  },
  {
    "text": "mind when converting very recent data",
    "start": "1340159",
    "end": "1343200"
  },
  {
    "text": "and overlapping blocks on the last slide",
    "start": "1343200",
    "end": "1346880"
  },
  {
    "text": "you can find some links to learn",
    "start": "1346880",
    "end": "1348799"
  },
  {
    "text": "more about the blocks engine in cortex",
    "start": "1348799",
    "end": "1350880"
  },
  {
    "text": "and tsdb blocks",
    "start": "1350880",
    "end": "1353200"
  },
  {
    "text": "thank you very much for your attention",
    "start": "1353200",
    "end": "1357519"
  }
]