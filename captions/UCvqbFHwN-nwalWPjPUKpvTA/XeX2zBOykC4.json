[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "hello everyone welcome to this introduction to the kubernetes working group batch",
    "start": "10480",
    "end": "17920"
  },
  {
    "text": "i'm very excited to to do this presentation because this is a new working group",
    "start": "18960",
    "end": "24160"
  },
  {
    "text": "how many of you were in the batch day okay a few so you heard through uh you",
    "start": "24160",
    "end": "31119"
  },
  {
    "text": "heard about these working groups through that day i assume um how many of you",
    "start": "31119",
    "end": "36559"
  },
  {
    "text": "heard about the working group today through the keynotes",
    "start": "36559",
    "end": "41600"
  },
  {
    "text": "cool that's cool yeah thank you uh ricardo for this amazing presentation",
    "start": "42320",
    "end": "48800"
  },
  {
    "text": "uh that highlighted some of the stuff we've we have been doing um i'm gonna skip this slide",
    "start": "48800",
    "end": "54879"
  },
  {
    "start": "52000",
    "end": "297000"
  },
  {
    "text": "um i wanna start with the motivation why why did we start",
    "start": "54879",
    "end": "60160"
  },
  {
    "text": "this working group well the reason is",
    "start": "60160",
    "end": "65760"
  },
  {
    "text": "kubernetes as you may or may not know it had a humble beginning kubernetes was built",
    "start": "65760",
    "end": "73760"
  },
  {
    "text": "for serving applications uh stateless applications",
    "start": "73760",
    "end": "79119"
  },
  {
    "text": "and over time we we have been increasing support for stateful",
    "start": "79119",
    "end": "84159"
  },
  {
    "text": "stateful applications right still still of course uh there are still things to be done",
    "start": "84159",
    "end": "91040"
  },
  {
    "text": "but there was never a focus on other types of workloads uh which are um",
    "start": "91040",
    "end": "98640"
  },
  {
    "text": "very different from from serving applications and of course i'm referring to to batch workloads so yes you can run batch",
    "start": "98640",
    "end": "106479"
  },
  {
    "text": "workloads there is a job api and of course you can create your own crts",
    "start": "106479",
    "end": "112240"
  },
  {
    "text": "and run batch applications um but there are feature gaps",
    "start": "112240",
    "end": "118560"
  },
  {
    "text": "um back to the job api there is",
    "start": "118960",
    "end": "124399"
  },
  {
    "text": "there is very little um policies around uh",
    "start": "124399",
    "end": "130319"
  },
  {
    "text": "completion and failure modes in the job api so that's something uh that limits the the usage of of the job",
    "start": "130319",
    "end": "137599"
  },
  {
    "text": "api itself um in the cubelet or well not just the cubelet the overall kubernetes system",
    "start": "137599",
    "end": "145760"
  },
  {
    "text": "and there is a lack of some support for specialized devices yes",
    "start": "145760",
    "end": "150800"
  },
  {
    "text": "we have the device plugin api um but it doesn't have uh",
    "start": "150800",
    "end": "156640"
  },
  {
    "text": "if i want to uh say how much memory i want for my gpu you cannot do that or or",
    "start": "156640",
    "end": "162800"
  },
  {
    "text": "if you have pneuma cells in or numa nodes within a node you cannot um",
    "start": "162800",
    "end": "168400"
  },
  {
    "text": "well there is some progress i'm going to talk about it during the presentation but uh there are",
    "start": "168400",
    "end": "174160"
  },
  {
    "text": "all these uh different things about hardware that you might need for for high performance uh",
    "start": "174160",
    "end": "180000"
  },
  {
    "text": "which are lacking all or nothing but scheduling um",
    "start": "180000",
    "end": "187120"
  },
  {
    "text": "this um this has been a request for for for multiple",
    "start": "187280",
    "end": "192480"
  },
  {
    "text": "i would say years uh in the keep scheduler and yes today you cannot",
    "start": "192480",
    "end": "197519"
  },
  {
    "text": "schedule uh a group of pots together you have to schedule one by one",
    "start": "197519",
    "end": "203390"
  },
  {
    "text": "[Music] and",
    "start": "203390",
    "end": "208159"
  },
  {
    "text": "yes this is kind of related so once you have multiple parts trying to you have multiple jobs",
    "start": "208560",
    "end": "215120"
  },
  {
    "text": "and each job is a part of the job with multiple parts then we don't have but all or nothing",
    "start": "215120",
    "end": "221680"
  },
  {
    "text": "scheduling so we we also don't have job queueing so you either create a job or",
    "start": "221680",
    "end": "228400"
  },
  {
    "text": "or you don't so so far in kubernetes the status quo",
    "start": "228400",
    "end": "235040"
  },
  {
    "text": "was to support this feature through crds through replacing the scheduler",
    "start": "235040",
    "end": "241710"
  },
  {
    "text": "[Music] all sorts of ways around the problem",
    "start": "241710",
    "end": "249040"
  },
  {
    "text": "and this somehow works uh it's not i mean you can still do something but it",
    "start": "251040",
    "end": "257600"
  },
  {
    "text": "causes other problems uh such as fragmentation in the ecosystem",
    "start": "257600",
    "end": "262800"
  },
  {
    "text": "so for example of we have forked pot schedulers",
    "start": "262800",
    "end": "268160"
  },
  {
    "text": "we have four job apis and we even have",
    "start": "268160",
    "end": "274320"
  },
  {
    "text": "cris different cris to do the hardware support and",
    "start": "274320",
    "end": "280400"
  },
  {
    "text": "once once you have all of these uh things then it it becomes harder for for you to to",
    "start": "280400",
    "end": "287120"
  },
  {
    "text": "manage your cluster to install it and and and then you if you have two competing",
    "start": "287120",
    "end": "292240"
  },
  {
    "text": "schedulers they might take different decisions and then you have you run into into",
    "start": "292240",
    "end": "297440"
  },
  {
    "text": "some problems so yes the working group started",
    "start": "297440",
    "end": "302800"
  },
  {
    "text": "actually when i submitted the application for for this talk the working group was not",
    "start": "302800",
    "end": "308320"
  },
  {
    "text": "created uh i could do it i think because uh i'm a tln6 scheduling so i i was",
    "start": "308320",
    "end": "314960"
  },
  {
    "text": "allowed to to submit the application um but we had already asked",
    "start": "314960",
    "end": "321520"
  },
  {
    "text": "the the steering committee to to create this working group it looked like it was going to go unfortunately did in",
    "start": "321520",
    "end": "328240"
  },
  {
    "text": "february the working group was formally created",
    "start": "328240",
    "end": "333520"
  },
  {
    "text": "but the work didn't start this february we have been doing some work in",
    "start": "333520",
    "end": "338960"
  },
  {
    "text": "different uh sigs over over the past few years",
    "start": "338960",
    "end": "345280"
  },
  {
    "text": "um seacaps in caps we have been working on",
    "start": "345280",
    "end": "350800"
  },
  {
    "text": "well completing the job api making it support a wider range of use cases",
    "start": "350800",
    "end": "357120"
  },
  {
    "text": "for example we introduced index jobs which is now ga in 120",
    "start": "357120",
    "end": "364080"
  },
  {
    "text": "i forgot i think it was in 123 that or 124 that we make it ga",
    "start": "364080",
    "end": "372560"
  },
  {
    "text": "and it this is simple uh when uh when you have a",
    "start": "372720",
    "end": "378160"
  },
  {
    "text": "classic job all your pots look the same so",
    "start": "378160",
    "end": "383440"
  },
  {
    "text": "it's up to you to differentiate these spots among",
    "start": "383440",
    "end": "388560"
  },
  {
    "text": "themselves if you introduce an index so each pod gets a number it's very easy for you to",
    "start": "388560",
    "end": "394880"
  },
  {
    "text": "just front from within the pot access a certain",
    "start": "394880",
    "end": "400560"
  },
  {
    "text": "index of your data right and so you can do a static partitioning of your workload",
    "start": "400560",
    "end": "406880"
  },
  {
    "text": "and well it's much easier to to do parallel programming or distribute the programming",
    "start": "406880",
    "end": "414240"
  },
  {
    "text": "this way we introduced suspended jobs as well",
    "start": "414240",
    "end": "420479"
  },
  {
    "text": "this it might be hard to understand on itself but basically its ability to create jobs",
    "start": "420479",
    "end": "426000"
  },
  {
    "text": "uh but defer the pod creation to later and uh you'll see why this is important",
    "start": "426000",
    "end": "431520"
  },
  {
    "text": "in um the next slides um",
    "start": "431520",
    "end": "436720"
  },
  {
    "text": "so we also introduced a ttl after finish um jobs don't unnecessarily",
    "start": "436720",
    "end": "443599"
  },
  {
    "text": "once they finish they stay there in the api server consuming well etcd resources",
    "start": "443599",
    "end": "448639"
  },
  {
    "text": "so we introduced that this is a very important feature which",
    "start": "448639",
    "end": "454880"
  },
  {
    "text": "is still in beta there is a very um",
    "start": "454880",
    "end": "459919"
  },
  {
    "text": "there is a very bad bug in the in the job controller where we actually lose",
    "start": "459919",
    "end": "464960"
  },
  {
    "text": "progress of of the job of the job completion or number of failures",
    "start": "464960",
    "end": "470160"
  },
  {
    "text": "um and that has been there for forever and um the reason is because we uh we depended",
    "start": "470160",
    "end": "477280"
  },
  {
    "text": "on on the pods to stay in the api server to keep track of the completions and",
    "start": "477280",
    "end": "483520"
  },
  {
    "text": "so we introduced uh some approach uh basically using finalizers to do this",
    "start": "483520",
    "end": "488720"
  },
  {
    "text": "synchronization so we can remove pots but we don't lose progress on the on the job status um",
    "start": "488720",
    "end": "495520"
  },
  {
    "text": "this feature actually has gone through some uh ups and downs uh it was very hard and why i",
    "start": "495520",
    "end": "502639"
  },
  {
    "text": "i want to highlight that this is very hard and we don't want managing pot is very hard so we don't",
    "start": "502639",
    "end": "509280"
  },
  {
    "text": "want you as developers to have to manage spots kubernetes should do it right",
    "start": "509280",
    "end": "515839"
  },
  {
    "text": "so we hope that by solving this problem and by increasing the use cases of the job",
    "start": "515839",
    "end": "522000"
  },
  {
    "text": "api you don't have to think uh about this you can you can create your crds of course",
    "start": "522000",
    "end": "528880"
  },
  {
    "text": "but when it comes to managing bots you just create a job and you don't have to run into these",
    "start": "528880",
    "end": "535200"
  },
  {
    "text": "bugs again over and over we also introduce this small feature to",
    "start": "535200",
    "end": "541040"
  },
  {
    "text": "track the number of ready pods in deployments or sorry replica sets you have this field saying active bots",
    "start": "541040",
    "end": "549279"
  },
  {
    "text": "that was not in the job api we included it and crown job was",
    "start": "549279",
    "end": "556480"
  },
  {
    "text": "graduated to ga which is also very important there were several optimizations there",
    "start": "556480",
    "end": "563279"
  },
  {
    "text": "and somewhat related this might be more related to gaming or um",
    "start": "563279",
    "end": "569920"
  },
  {
    "text": "or if you have a i don't know i don't want to put other examples maybe",
    "start": "569920",
    "end": "575360"
  },
  {
    "text": "spark jobs actually if you if you want to reduce the size of your replica set um",
    "start": "575360",
    "end": "583440"
  },
  {
    "text": "there was no way to tell which bots should be removed pot decision cost doesn't provide all the guarantees but",
    "start": "583440",
    "end": "590399"
  },
  {
    "text": "it still gives some flexibility for you to decide when to",
    "start": "590399",
    "end": "596480"
  },
  {
    "text": "to which spots down scale quickly just want to",
    "start": "596720",
    "end": "603440"
  },
  {
    "start": "599000",
    "end": "640000"
  },
  {
    "text": "show how an index job looks like very simple we have our",
    "start": "603440",
    "end": "609760"
  },
  {
    "text": "different tasks that we have to accomplish right the five at the top and",
    "start": "609760",
    "end": "615279"
  },
  {
    "text": "so we have number of completions five and then we have number of parallelism two this means that at every uh each",
    "start": "615279",
    "end": "621920"
  },
  {
    "text": "point of time we only have two parts running but uh they will basically the first part will",
    "start": "621920",
    "end": "628399"
  },
  {
    "text": "take the first work item second part and second one and so on and so forth and",
    "start": "628399",
    "end": "633680"
  },
  {
    "text": "you get this window that progresses through the",
    "start": "633680",
    "end": "638720"
  },
  {
    "text": "through the data um",
    "start": "638720",
    "end": "644399"
  },
  {
    "text": "i actually forgot to introduce my my co-presenters uh one of them couldn't",
    "start": "644399",
    "end": "650000"
  },
  {
    "text": "make it to to to this uh to keep con but he sent me a",
    "start": "650000",
    "end": "656320"
  },
  {
    "text": "video so i'm gonna play this part",
    "start": "656320",
    "end": "661320"
  },
  {
    "text": "this is alex one one of our one of our main contributors to to the working group",
    "start": "661519",
    "end": "668079"
  },
  {
    "text": "batch initiative",
    "start": "668079",
    "end": "671399"
  },
  {
    "text": "there we go now you have my receipts",
    "start": "676720",
    "end": "682120"
  },
  {
    "text": "it's very",
    "start": "714079",
    "end": "716720"
  },
  {
    "text": "resource there if there is no gun strategy the diagram",
    "start": "729200",
    "end": "735279"
  },
  {
    "text": "on the left shows that two different jobs how we inform their respective last",
    "start": "735279",
    "end": "741600"
  },
  {
    "text": "point to be scanned so neither of them is able to start",
    "start": "741600",
    "end": "748560"
  },
  {
    "text": "and also they don't release the resources this is the result",
    "start": "748560",
    "end": "754880"
  },
  {
    "text": "and it will winston cluster resources we define a crd called support group to",
    "start": "754880",
    "end": "763760"
  },
  {
    "text": "annoy the user to define the minimum number of reports",
    "start": "763760",
    "end": "769279"
  },
  {
    "text": "required for job to start when creating jobs you can associate it with a",
    "start": "769279",
    "end": "776880"
  },
  {
    "text": "protocol by the naval import and the second one is capacity study",
    "start": "776880",
    "end": "786639"
  },
  {
    "text": "we define a new api called elastic quarter to declare the quotas from",
    "start": "786639",
    "end": "793600"
  },
  {
    "text": "different tenants users can define the guaranteed resources and mean and the maximum",
    "start": "793600",
    "end": "802320"
  },
  {
    "text": "resources and max by using this object and there is an example on the left",
    "start": "802320",
    "end": "812000"
  },
  {
    "text": "which makes it possible to share the resources dynamically between different",
    "start": "812320",
    "end": "819120"
  },
  {
    "text": "name spaces in your classroom and to improve the resource utilization",
    "start": "819120",
    "end": "825279"
  },
  {
    "text": "in the cluster that home book usage without the right",
    "start": "825279",
    "end": "830480"
  },
  {
    "text": "will names these two have three resources things with white can borrow resource",
    "start": "830480",
    "end": "836480"
  },
  {
    "text": "interactions with true to run one point and when things with true is resources",
    "start": "836480",
    "end": "843760"
  },
  {
    "text": "the borrowed will be reclaimed at which point both names with one and",
    "start": "843760",
    "end": "849600"
  },
  {
    "text": "names we too can use their guarantee to resources",
    "start": "849600",
    "end": "855440"
  },
  {
    "text": "and the last one is being had each one learns asked to scan your homes",
    "start": "855440",
    "end": "862399"
  },
  {
    "text": "on the notes that have the list and use the resources such as cpu memory and the",
    "start": "862399",
    "end": "870399"
  },
  {
    "text": "other extended resources like gpm in this way we can solve the problem of",
    "start": "870399",
    "end": "877199"
  },
  {
    "text": "resource recognition back on small tasks together and the",
    "start": "877199",
    "end": "884000"
  },
  {
    "text": "same spaces for the big one okay that's all",
    "start": "884000",
    "end": "890399"
  },
  {
    "text": "thanks to everyone",
    "start": "890399",
    "end": "893959"
  },
  {
    "text": "um thank you to alex for showing us these uh small plugins that were introduced in",
    "start": "899279",
    "end": "906160"
  },
  {
    "start": "901000",
    "end": "952000"
  },
  {
    "text": "the complementary six scheduling um plugins repository um",
    "start": "906160",
    "end": "913040"
  },
  {
    "text": "now uh back to uh sig note uh there were also some initiatives around uh",
    "start": "913040",
    "end": "920000"
  },
  {
    "text": "in particular around numa numa nodes uh for example we have a cubelets topology",
    "start": "920000",
    "end": "925199"
  },
  {
    "text": "manager where you can um [Music] you can tell cubelets to",
    "start": "925199",
    "end": "932480"
  },
  {
    "text": "send containers or the the entire pot to up to uh to one pneuma node or to a set",
    "start": "932480",
    "end": "938560"
  },
  {
    "text": "of of new nodes and this has the advantage of improving the the runtime because the processes",
    "start": "938560",
    "end": "946639"
  },
  {
    "text": "are closer to each other within the newman",
    "start": "946639",
    "end": "951199"
  },
  {
    "start": "952000",
    "end": "987000"
  },
  {
    "text": "now the problem is that uh the cubelet takes its decisions uh",
    "start": "953519",
    "end": "958639"
  },
  {
    "text": "but it doesn't uh coordinate those with the cube scheduler so for that",
    "start": "958639",
    "end": "964800"
  },
  {
    "text": "signal was working on this crd we're still working on this here the uh",
    "start": "964800",
    "end": "970560"
  },
  {
    "text": "for exposing the the decisions that the the topology manager manager did so that",
    "start": "970560",
    "end": "977199"
  },
  {
    "text": "uh a scheduling plugin could uh use that information to schedule based uh based",
    "start": "977199",
    "end": "984320"
  },
  {
    "text": "on that all right so",
    "start": "984320",
    "end": "989360"
  },
  {
    "start": "987000",
    "end": "1143000"
  },
  {
    "text": "this all happened before february um on february we finally",
    "start": "989360",
    "end": "995440"
  },
  {
    "text": "got approved to create the working group batch which has this mission discuss and",
    "start": "995440",
    "end": "1002240"
  },
  {
    "text": "enhance the support of batch workloads in core kubernetes the goal is to unify the way users deploy batch",
    "start": "1002240",
    "end": "1009199"
  },
  {
    "text": "workloads to improve portability and to simplify supportability to for",
    "start": "1009199",
    "end": "1014240"
  },
  {
    "text": "kubernetes providers so i'm going to quickly highlight here",
    "start": "1014240",
    "end": "1020639"
  },
  {
    "text": "what what do i need by batch workloads usually um",
    "start": "1020639",
    "end": "1025760"
  },
  {
    "text": "well we could be talking about ml we could be talking about hpc uh and we can be talking about uh maybe",
    "start": "1025760",
    "end": "1034160"
  },
  {
    "text": "something less obvious like cicd um um when you have",
    "start": "1034160",
    "end": "1039520"
  },
  {
    "text": "when you have ci um a ci you have a job just to run your tests for example so it",
    "start": "1039520",
    "end": "1045280"
  },
  {
    "text": "they behave similarly to uh these other types of workloads so we we want to improve the support for",
    "start": "1045280",
    "end": "1051039"
  },
  {
    "text": "for all of these things incorporated uh and unify",
    "start": "1051039",
    "end": "1056960"
  },
  {
    "text": "so i don't know how many of you know what a working group is uh maybe some of you",
    "start": "1056960",
    "end": "1063679"
  },
  {
    "text": "might not even know what a cig is i'll quickly explain the difference a sig owns code",
    "start": "1063679",
    "end": "1071760"
  },
  {
    "text": "a working group doesn't own code so it's more of a forum for different zigs to coordinate",
    "start": "1071760",
    "end": "1078840"
  },
  {
    "text": "changes uh that expand multiple dimensions so",
    "start": "1078840",
    "end": "1084000"
  },
  {
    "text": "of course we have seagaps as the particip as a participant stakeholder uh seagaps um",
    "start": "1084000",
    "end": "1090880"
  },
  {
    "text": "owns the the all the batch apis think auto scaling um",
    "start": "1090880",
    "end": "1096720"
  },
  {
    "text": "they are listed in alphabetical order so cigar to scaling of course um",
    "start": "1096720",
    "end": "1102640"
  },
  {
    "text": "when we have job work um when we have multiple jobs maybe our",
    "start": "1102640",
    "end": "1109039"
  },
  {
    "text": "small cluster is not enough so we want to ask how to scale and with that we we want to uh improve cost",
    "start": "1109039",
    "end": "1116640"
  },
  {
    "text": "right scale up scale down so we we wanted to make a co2 scale in part",
    "start": "1116640",
    "end": "1121679"
  },
  {
    "text": "of the initiating initiative as well signal we already saw some",
    "start": "1121679",
    "end": "1126799"
  },
  {
    "text": "some work being done there are multiple caps that i saw open um",
    "start": "1126799",
    "end": "1132640"
  },
  {
    "text": "around this area so very encouraging to to see and seek scheduling of course um",
    "start": "1132640",
    "end": "1139440"
  },
  {
    "text": "that's where where i come from so",
    "start": "1139440",
    "end": "1144640"
  },
  {
    "start": "1143000",
    "end": "1275000"
  },
  {
    "text": "yeah those are the stakeholders uh once we created a working group batch we",
    "start": "1144640",
    "end": "1150320"
  },
  {
    "text": "decided to kind of define what what things we want to work on",
    "start": "1150320",
    "end": "1155840"
  },
  {
    "text": "so the first work stream is advancing the job api so",
    "start": "1155840",
    "end": "1161120"
  },
  {
    "text": "we want you you saw already some improvements but we want to keep listening uh from",
    "start": "1161120",
    "end": "1166799"
  },
  {
    "text": "from the community what what they what they need from the job api um to to",
    "start": "1166799",
    "end": "1172320"
  },
  {
    "text": "to support a wider range of use cases we already saw static partitioning but there is still work to be done",
    "start": "1172320",
    "end": "1178960"
  },
  {
    "text": "for supporting mpi ml ai again we we",
    "start": "1178960",
    "end": "1184240"
  },
  {
    "text": "if you if you are using for example keyflow mpi jaw that's that's great but",
    "start": "1184240",
    "end": "1189440"
  },
  {
    "text": "again managing pot is hard so we want kubeflow mpi uh job to somehow use the the job api or",
    "start": "1189440",
    "end": "1197280"
  },
  {
    "text": "some other um uh apis that that we have for for groups",
    "start": "1197280",
    "end": "1202320"
  },
  {
    "text": "of bots um so job api is the",
    "start": "1202320",
    "end": "1207520"
  },
  {
    "text": "the main api we're working on but of course uh all the apis are open for discussion",
    "start": "1207520",
    "end": "1213440"
  },
  {
    "text": "um the second workstream is about well kind of scheduling we",
    "start": "1213440",
    "end": "1219360"
  },
  {
    "text": "i particularly don't like calling it scheduling because when we talk about scheduling in kubernetes we are",
    "start": "1219360",
    "end": "1225120"
  },
  {
    "text": "referring to pot pot to not scheduling so um i want i i prefer to call it job",
    "start": "1225120",
    "end": "1231840"
  },
  {
    "text": "management so we we want to work at the level of jobs and",
    "start": "1231840",
    "end": "1238400"
  },
  {
    "text": "at the level of jobs we want to do queueing we want to do auto provisioning",
    "start": "1238400",
    "end": "1245360"
  },
  {
    "text": "make the autoscaler create a provision more resources of course there will be",
    "start": "1245360",
    "end": "1252240"
  },
  {
    "text": "scheduling uh already set out to scale",
    "start": "1252240",
    "end": "1257120"
  },
  {
    "text": "and the the last work stream is around hardware we want to uh discuss what we can do",
    "start": "1258159",
    "end": "1264640"
  },
  {
    "text": "about runtime and scaling support for specialized hardware accelerators pneuma",
    "start": "1264640",
    "end": "1270159"
  },
  {
    "text": "rdma fpgas you you tell me",
    "start": "1270159",
    "end": "1276400"
  },
  {
    "start": "1275000",
    "end": "1498000"
  },
  {
    "text": "so where are we now um we started in february um what we",
    "start": "1276400",
    "end": "1282400"
  },
  {
    "text": "have been doing like was it three months we actually did some progress um",
    "start": "1282400",
    "end": "1289679"
  },
  {
    "text": "so six scheduling um started sponsoring an a new project for job queuing um",
    "start": "1289679",
    "end": "1296720"
  },
  {
    "text": "i don't have a presentation of q today uh because we already had one",
    "start": "1296720",
    "end": "1302880"
  },
  {
    "text": "on tuesday for the batch day uh i believe is the link on the right um",
    "start": "1302880",
    "end": "1309520"
  },
  {
    "text": "um this is a full presentation on on the uh on the system",
    "start": "1309520",
    "end": "1315120"
  },
  {
    "text": "um i'm gonna quickly explain what it is and the other link is for",
    "start": "1315120",
    "end": "1321200"
  },
  {
    "text": "ricardo's presentation the keynote in the morning so yeah go ahead and watch it it was",
    "start": "1321200",
    "end": "1327120"
  },
  {
    "text": "pretty pretty cool so cube it's a job level manager so",
    "start": "1327120",
    "end": "1333520"
  },
  {
    "text": "again we're not dealing with pots we're talking we're dealing with jobs and then it decides when the job should",
    "start": "1333520",
    "end": "1340240"
  },
  {
    "text": "start meaning that we can create bots or if there is no capacity and then i",
    "start": "1340240",
    "end": "1347600"
  },
  {
    "text": "have a high priority job i need to reclaim that capacity so i can stop a job uh and delete all those spots and uh",
    "start": "1347600",
    "end": "1355360"
  },
  {
    "text": "start my new job so that that's the um",
    "start": "1355360",
    "end": "1360400"
  },
  {
    "text": "that's what what q does on a high level um [Music] so for that i i showed you much earlier",
    "start": "1360400",
    "end": "1367679"
  },
  {
    "text": "there we added the suspend field in the job api so we did that to enable queue",
    "start": "1367679",
    "end": "1374559"
  },
  {
    "text": "yeah it's very simple you suspend the job no pots you you unsuspend the job",
    "start": "1374880",
    "end": "1382000"
  },
  {
    "text": "but are created so this way we prevent the scheduler from",
    "start": "1382000",
    "end": "1387120"
  },
  {
    "text": "scheduling something that might not fit and we take the decision in this other",
    "start": "1387120",
    "end": "1392400"
  },
  {
    "text": "controller at a much which has a view of all the jobs that",
    "start": "1392400",
    "end": "1397600"
  },
  {
    "text": "are running now i've been talking about jobs but in queue there is this concept of workload which is a smaller it's a",
    "start": "1397600",
    "end": "1406799"
  },
  {
    "text": "shadow representation of a job such that we can support not just a job",
    "start": "1406799",
    "end": "1412080"
  },
  {
    "text": "but we need to support crds because of course today job api doesn't uh doesn't support all the use",
    "start": "1412080",
    "end": "1418880"
  },
  {
    "text": "cases and maybe we'll there is always a chance for for crds so",
    "start": "1418880",
    "end": "1425039"
  },
  {
    "text": "we we want that to be included the main design principle of q is no",
    "start": "1425039",
    "end": "1430559"
  },
  {
    "text": "duplication so q coexists with everything we have already in kubernetes",
    "start": "1430559",
    "end": "1436080"
  },
  {
    "text": "q coexists with the q controller manager which knows how to manage manage spots",
    "start": "1436080",
    "end": "1441600"
  },
  {
    "text": "for a job which knows how to connect",
    "start": "1441600",
    "end": "1447279"
  },
  {
    "text": "assign a pot to a node uh and the cluster autoscaler which knows how to add a capacity in my",
    "start": "1447279",
    "end": "1454080"
  },
  {
    "text": "cluster so q coordinates with all of them rather than replacing the all of those",
    "start": "1454080",
    "end": "1460480"
  },
  {
    "text": "components so that's that's the main design principle the usage um",
    "start": "1460480",
    "end": "1466720"
  },
  {
    "text": "i mean the admins define cues with resource flavors",
    "start": "1466720",
    "end": "1471919"
  },
  {
    "text": "quotas and borrowing cohorts um so they have the the heavy job let's say um and users they don't change their",
    "start": "1471919",
    "end": "1479600"
  },
  {
    "text": "behavior they just create jobs they just say the queue uh and that's it",
    "start": "1479600",
    "end": "1486559"
  },
  {
    "text": "so we released a v01 on april uh we're working on the v02",
    "start": "1487039",
    "end": "1492880"
  },
  {
    "text": "where we want to add more metrics and make it robust",
    "start": "1492880",
    "end": "1498320"
  },
  {
    "start": "1498000",
    "end": "1571000"
  },
  {
    "text": "so go and watch the two presentations um what else is happening in uh what is",
    "start": "1498880",
    "end": "1505200"
  },
  {
    "text": "happening in in the um space we have a few a few of these debates we are talking about more",
    "start": "1505200",
    "end": "1513039"
  },
  {
    "text": "policies for jobs um we want we are thinking about how to do",
    "start": "1513039",
    "end": "1518720"
  },
  {
    "text": "still do all our all-or-nothing scheduling in in the keep scatter for the cases where we have a study cluster",
    "start": "1518720",
    "end": "1524880"
  },
  {
    "text": "with very tight resources it might still be needed uh an api to reserve no resources so the autoscaler can like",
    "start": "1524880",
    "end": "1532480"
  },
  {
    "text": "can preemptively add more capacity um",
    "start": "1532480",
    "end": "1537919"
  },
  {
    "text": "maybe a plugin model there was a proposal for a plugin model for the cubelet so we can more easily",
    "start": "1537919",
    "end": "1545840"
  },
  {
    "text": "define support other hardware we are welcoming presentations from all",
    "start": "1545840",
    "end": "1551600"
  },
  {
    "text": "of you if you have a batch project a crd um",
    "start": "1551600",
    "end": "1556960"
  },
  {
    "text": "we want to uh we want to understand what future apps you have",
    "start": "1556960",
    "end": "1563919"
  },
  {
    "text": "and we want to to keep the con the discussion going on um we of course",
    "start": "1563919",
    "end": "1572080"
  },
  {
    "start": "1571000",
    "end": "1610000"
  },
  {
    "text": "how how can you get involved uh well come present uh we have the links in the",
    "start": "1572080",
    "end": "1580000"
  },
  {
    "text": "the last link the working group batch in the official community repository there you can find the slack you can",
    "start": "1580000",
    "end": "1586400"
  },
  {
    "text": "find the mailing list well they are there and you can find our schedule uh",
    "start": "1586400",
    "end": "1592400"
  },
  {
    "text": "go ahead put your topic in the agenda and our lovely um",
    "start": "1592400",
    "end": "1599279"
  },
  {
    "text": "organizers uh who are sitting right here will uh will um",
    "start": "1599279",
    "end": "1605279"
  },
  {
    "text": "well um moderate you know the next in the next working group meeting um",
    "start": "1605279",
    "end": "1610880"
  },
  {
    "start": "1610000",
    "end": "1643000"
  },
  {
    "text": "and that's all oh yes uh before i forget uh google is running um",
    "start": "1610880",
    "end": "1617600"
  },
  {
    "text": "uh live uh panel uh in june 15th uh we",
    "start": "1617600",
    "end": "1622720"
  },
  {
    "text": "uh we invited uh the relevant leads from the different seeks",
    "start": "1622720",
    "end": "1628159"
  },
  {
    "text": "within kubernetes to um to discuss to chat more about uh what is missing in",
    "start": "1628159",
    "end": "1633840"
  },
  {
    "text": "kubernetes um to support a batch um so yeah i invite you to to go there and",
    "start": "1633840",
    "end": "1642080"
  },
  {
    "text": "register so questions before so i completely forgot about introducing my",
    "start": "1642080",
    "end": "1648880"
  },
  {
    "start": "1643000",
    "end": "1675000"
  },
  {
    "text": "co-presenters abdullah is right here for questions uh i'm aldo we're both from google and kikong or alex wong",
    "start": "1648880",
    "end": "1656960"
  },
  {
    "text": "from alibaba and you can find us in github um there with those",
    "start": "1656960",
    "end": "1663039"
  },
  {
    "text": "usernames thank you [Applause]",
    "start": "1663039",
    "end": "1671840"
  },
  {
    "text": "all right so we have about eight minutes for questions so if you want raise your hand and i will give you a microphone",
    "start": "1671840",
    "end": "1680360"
  },
  {
    "start": "1675000",
    "end": "1999000"
  },
  {
    "text": "hi thank you for the talk um i wanted to know how does it",
    "start": "1683200",
    "end": "1688559"
  },
  {
    "text": "integrate with some existing for example training operator formerly tf job operator because",
    "start": "1688559",
    "end": "1695760"
  },
  {
    "text": "i remember there was indexed pods already at the time so if you have like multiple replicas",
    "start": "1695760",
    "end": "1702159"
  },
  {
    "text": "for example the pods would be indexed and i don't know now if you if these now",
    "start": "1702159",
    "end": "1707840"
  },
  {
    "text": "features are brought by the batch api spec or batch api so how it will",
    "start": "1707840",
    "end": "1715520"
  },
  {
    "text": "integrate right so um the then df job right",
    "start": "1715520",
    "end": "1722640"
  },
  {
    "text": "that know df job would have to use the job api so the con the tf job controller would have",
    "start": "1722640",
    "end": "1729279"
  },
  {
    "text": "to create job objects instead of creating robots and then would benefit from the the index job um",
    "start": "1729279",
    "end": "1735760"
  },
  {
    "text": "we we engaged with the keyflop community we wanted to to get this done we wanted them to",
    "start": "1735760",
    "end": "1743039"
  },
  {
    "text": "to be able to uh use what we have been working on but it's still not possible",
    "start": "1743039",
    "end": "1748080"
  },
  {
    "text": "they have extra policies in terms of termination and failures so we we need",
    "start": "1748080",
    "end": "1753600"
  },
  {
    "text": "to include those in the job api so that they can uh support uh they can migrate",
    "start": "1753600",
    "end": "1758880"
  },
  {
    "text": "but yeah that's a discussion that is going on um what was",
    "start": "1758880",
    "end": "1764559"
  },
  {
    "text": "was that your question were you asking about queue yeah exactly okay perfect yeah i guess it will make uh make them",
    "start": "1764559",
    "end": "1770480"
  },
  {
    "text": "have less code on their side correct yes but still more constraints what i",
    "start": "1770480",
    "end": "1776480"
  },
  {
    "text": "usually say is like we have to solve the problem once and then everyone benefits",
    "start": "1776480",
    "end": "1782240"
  },
  {
    "text": "all right no questions [Applause]",
    "start": "1791600",
    "end": "1799759"
  },
  {
    "text": "i'm sorry i'm going to ask what i'm going to talk about tomorrow which is how does this relate to the cncf batch",
    "start": "1804480",
    "end": "1811279"
  },
  {
    "text": "initiative yes um well actually the two",
    "start": "1811279",
    "end": "1816559"
  },
  {
    "text": "proposals were kind of started at the same time",
    "start": "1816559",
    "end": "1823840"
  },
  {
    "text": "we have a little bit different objectives because we the working group batch is within the",
    "start": "1824480",
    "end": "1830640"
  },
  {
    "text": "kubernetes organization so we we want to um",
    "start": "1830640",
    "end": "1836559"
  },
  {
    "text": "improve what is already within the within the core right the job controller cube scheduler",
    "start": "1836559",
    "end": "1842559"
  },
  {
    "text": "um and we once uh we want",
    "start": "1842559",
    "end": "1848960"
  },
  {
    "text": "i don't know if it's going to be possible but we want to provide these smaller",
    "start": "1848960",
    "end": "1855679"
  },
  {
    "text": "you know core features that other batch schedulers can use so",
    "start": "1855679",
    "end": "1861360"
  },
  {
    "text": "kubernetes can focus on starting um starting jobs",
    "start": "1861360",
    "end": "1867440"
  },
  {
    "text": "and the batch schedulers can focus on well maybe maybe you don't like use fair",
    "start": "1867440",
    "end": "1873360"
  },
  {
    "text": "sharing you you can implement your own fair sharing algorithm but starting the jobs",
    "start": "1873360",
    "end": "1879600"
  },
  {
    "text": "um maybe some basic queueing fifo queueing you you just still use that and if we",
    "start": "1879600",
    "end": "1886000"
  },
  {
    "text": "are talking about multi-cluster right the the kubernetes core should support some of those operations",
    "start": "1886000",
    "end": "1894159"
  },
  {
    "text": "now the working the cncf uh batch working group is looking",
    "start": "1894159",
    "end": "1901120"
  },
  {
    "text": "at the problem at a let's say at a outside of quantities right they are",
    "start": "1901120",
    "end": "1906720"
  },
  {
    "text": "trying to integrate or they are trying to uniformize",
    "start": "1906720",
    "end": "1912799"
  },
  {
    "text": "multiple different batch projects uh which may or may not be kubernetes",
    "start": "1912799",
    "end": "1919039"
  },
  {
    "text": "so we hope to engage with with them so that we we are somehow aligned but uh",
    "start": "1919039",
    "end": "1924799"
  },
  {
    "text": "that's a conversation that hasn't started yet",
    "start": "1924799",
    "end": "1929640"
  },
  {
    "text": "um looking at what you're proposing uh a job is a fixed thing",
    "start": "1934799",
    "end": "1941679"
  },
  {
    "text": "so you create it beforehand but what happens if the job changes while it's running so it grows or shrinks so that",
    "start": "1941679",
    "end": "1949519"
  },
  {
    "text": "means that every single time you need to recreate something well if you look at the the tensorflow or the spark job",
    "start": "1949519",
    "end": "1956960"
  },
  {
    "text": "the driver picks up and says i need these kinds of resources so how does that fit in with this proposal",
    "start": "1956960",
    "end": "1964559"
  },
  {
    "text": "um so i suppose you're talking specifically about queue um",
    "start": "1964559",
    "end": "1970960"
  },
  {
    "text": "oh the job api um [Music] yeah so",
    "start": "1970960",
    "end": "1976480"
  },
  {
    "text": "when we are talking about um scaling uh job",
    "start": "1976480",
    "end": "1983039"
  },
  {
    "text": "the the main question here is which when we're scaling up all this all",
    "start": "1983039",
    "end": "1988720"
  },
  {
    "text": "is fine just add more pots when we're scaling down that's the question which one which pots do we",
    "start": "1988720",
    "end": "1996000"
  },
  {
    "text": "remove and simply the discussion is just",
    "start": "1996000",
    "end": "2002840"
  },
  {
    "start": "1999000",
    "end": "2248000"
  },
  {
    "text": "to we need to discuss this uh we need to come up with an api where we can safely",
    "start": "2002840",
    "end": "2008720"
  },
  {
    "text": "say which spots we can remove uh but abdullah you wanted to add something",
    "start": "2008720",
    "end": "2015440"
  },
  {
    "text": "yeah i just want to mention that in the job api i think one of the things that are fixed is the completions this",
    "start": "2015440",
    "end": "2021120"
  },
  {
    "text": "is what you're mentioning that like you define that my job is complete when i complete 10 instances of this pod",
    "start": "2021120",
    "end": "2027440"
  },
  {
    "text": "um but like you want to scale it up and down this is the same thing right like if we make completions a mutable field",
    "start": "2027440",
    "end": "2035279"
  },
  {
    "text": "it starts with one for example and then you increase it",
    "start": "2035279",
    "end": "2041039"
  },
  {
    "text": "you could achieve similar semantics probably now the the the case that i'm talking",
    "start": "2041760",
    "end": "2047679"
  },
  {
    "text": "about is i run a uh let's let's pick a spark job i give it an um a request",
    "start": "2047679",
    "end": "2054878"
  },
  {
    "text": "and based on the amount of data it processes it says i need 10 executors or",
    "start": "2054879",
    "end": "2061200"
  },
  {
    "text": "100 executors you don't know when you define the job the spark executor looks at the data or",
    "start": "2061200",
    "end": "2068480"
  },
  {
    "text": "the spark driver looks at the information that it needs to process and based on what it needs to do",
    "start": "2068480",
    "end": "2075839"
  },
  {
    "text": "dynamically generates the uh the pots that it needs so",
    "start": "2075919",
    "end": "2081839"
  },
  {
    "text": "you submit the job consists of just the driver and the rest is",
    "start": "2081839",
    "end": "2087440"
  },
  {
    "text": "code within the driver that decides on what it does so what you're asking",
    "start": "2087440",
    "end": "2093440"
  },
  {
    "text": "almost for is the driver to create job api",
    "start": "2093440",
    "end": "2098640"
  },
  {
    "text": "things to get the executors up and running and scheduled in a way so that's a huge uh",
    "start": "2098640",
    "end": "2105920"
  },
  {
    "text": "request that you put on the applications that are going to run using this",
    "start": "2105920",
    "end": "2111760"
  },
  {
    "text": "you you want to do it without changing the application that runs you don't want to change the spark",
    "start": "2111760",
    "end": "2119119"
  },
  {
    "text": "or tensorflow or things like that so the the spark jobs and",
    "start": "2119119",
    "end": "2126079"
  },
  {
    "text": "mpi jobs if you were talking about there is some elastic api jobs uh",
    "start": "2126079",
    "end": "2132480"
  },
  {
    "text": "so when we are talking about them it they 100 don't fit the job api today right",
    "start": "2132480",
    "end": "2140800"
  },
  {
    "text": "the job api is designed for completions and these jobs be",
    "start": "2140800",
    "end": "2146079"
  },
  {
    "text": "yes the entire thing is a job but the individual workers are not tasks they are kind of like small",
    "start": "2146079",
    "end": "2152960"
  },
  {
    "text": "servers um so they are more similar to services",
    "start": "2152960",
    "end": "2158560"
  },
  {
    "text": "so more similar to deployments and staple sets so the answer",
    "start": "2158560",
    "end": "2163680"
  },
  {
    "text": "my answer today is we don't have an api for that today the closest thing might be staple set but that's a discussion we",
    "start": "2163680",
    "end": "2170560"
  },
  {
    "text": "need we need to we we need to happen um and it didn't start yet",
    "start": "2170560",
    "end": "2176720"
  },
  {
    "text": "so the last question we got it from a virtual audience what is the performance of running jobs",
    "start": "2176720",
    "end": "2182880"
  },
  {
    "text": "in kubernetes is the overhead and rate at which pots can be created going to",
    "start": "2182880",
    "end": "2187920"
  },
  {
    "text": "improve um i suppose this uh this is asking about",
    "start": "2187920",
    "end": "2193520"
  },
  {
    "text": "the job controller so um it comes a lot to what well for starters",
    "start": "2193520",
    "end": "2201920"
  },
  {
    "text": "it depends a lot on on what's the api throttling that the jaw controller has",
    "start": "2201920",
    "end": "2209359"
  },
  {
    "text": "so if you increase that it's probably going to behave better but at the same time",
    "start": "2210000",
    "end": "2215359"
  },
  {
    "text": "that's something we want to work on um we recently introduced some load testing",
    "start": "2215359",
    "end": "2220640"
  },
  {
    "text": "for the job controller uh just the past release and",
    "start": "2220640",
    "end": "2225760"
  },
  {
    "text": "well we just started we put the load test we don't have a concrete answer today we want to",
    "start": "2225760",
    "end": "2231920"
  },
  {
    "text": "improve it um yes for that we welcome contributors",
    "start": "2231920",
    "end": "2238640"
  },
  {
    "text": "all right good we run out of time so thank you very much [Applause]",
    "start": "2238640",
    "end": "2250170"
  }
]