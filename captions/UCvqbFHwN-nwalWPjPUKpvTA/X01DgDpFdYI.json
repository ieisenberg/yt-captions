[
  {
    "text": "good morning we're here today to talk",
    "start": "160",
    "end": "2320"
  },
  {
    "text": "about a new project in the Kubernetes",
    "start": "2320",
    "end": "4040"
  },
  {
    "text": "ecosystem sponsored by the serving",
    "start": "4040",
    "end": "6480"
  },
  {
    "text": "working group called the gateway API",
    "start": "6480",
    "end": "8639"
  },
  {
    "text": "inference",
    "start": "8639",
    "end": "10360"
  },
  {
    "text": "extension it takes any Kubernetes",
    "start": "10360",
    "end": "12880"
  },
  {
    "text": "gateway and turns it into an inference",
    "start": "12880",
    "end": "15200"
  },
  {
    "text": "gateway and an inference gateway helps",
    "start": "15200",
    "end": "18560"
  },
  {
    "text": "large platform teams small platform",
    "start": "18560",
    "end": "20480"
  },
  {
    "text": "teams self-host large language models",
    "start": "20480",
    "end": "23199"
  },
  {
    "text": "efficiently in production it's informed",
    "start": "23199",
    "end": "25519"
  },
  {
    "text": "by our experiences at Google and Bite",
    "start": "25519",
    "end": "27920"
  },
  {
    "text": "Dance and we're very excited to talk to",
    "start": "27920",
    "end": "29920"
  },
  {
    "text": "you today about",
    "start": "29920",
    "end": "31160"
  },
  {
    "text": "it 10 years ago someone told me that",
    "start": "31160",
    "end": "34480"
  },
  {
    "text": "Kubernetes wasn't going to be relevant",
    "start": "34480",
    "end": "36079"
  },
  {
    "text": "to the majority of users we'd all be",
    "start": "36079",
    "end": "38160"
  },
  {
    "text": "using functions as a service or maybe 12",
    "start": "38160",
    "end": "41120"
  },
  {
    "text": "factor managed platform as a service uh",
    "start": "41120",
    "end": "43840"
  },
  {
    "text": "in the cloud now obviously as judging by",
    "start": "43840",
    "end": "47520"
  },
  {
    "text": "all of us here they were wrong but there",
    "start": "47520",
    "end": "50320"
  },
  {
    "text": "was a seat of truth in that it wasn't",
    "start": "50320",
    "end": "52559"
  },
  {
    "text": "yet clear that the majority of large",
    "start": "52559",
    "end": "54960"
  },
  {
    "text": "platform teams would have a diverse",
    "start": "54960",
    "end": "56719"
  },
  {
    "text": "range of workloads and that they would",
    "start": "56719",
    "end": "58960"
  },
  {
    "text": "need and demand the flexibility from of",
    "start": "58960",
    "end": "62600"
  },
  {
    "text": "Kubernetes as well as depend on a rich",
    "start": "62600",
    "end": "66080"
  },
  {
    "text": "ecosystem of composable automation",
    "start": "66080",
    "end": "68799"
  },
  {
    "text": "there's a similar timeline going on in",
    "start": "68799",
    "end": "70720"
  },
  {
    "text": "AI today two years ago it seemed that",
    "start": "70720",
    "end": "73840"
  },
  {
    "text": "all models would be proprietary in the",
    "start": "73840",
    "end": "76479"
  },
  {
    "text": "last year open models have dramatically",
    "start": "76479",
    "end": "79360"
  },
  {
    "text": "closed the",
    "start": "79360",
    "end": "80520"
  },
  {
    "text": "gap because larger models are more",
    "start": "80520",
    "end": "83080"
  },
  {
    "text": "flexible and smaller models are more",
    "start": "83080",
    "end": "85520"
  },
  {
    "text": "efficient to serve we believe there is a",
    "start": "85520",
    "end": "88159"
  },
  {
    "text": "meaningful and durable trade-off between",
    "start": "88159",
    "end": "90960"
  },
  {
    "text": "very smart frontier models and smaller",
    "start": "90960",
    "end": "94000"
  },
  {
    "text": "predictable building block open models",
    "start": "94000",
    "end": "97119"
  },
  {
    "text": "so we expect everyone hopefully will",
    "start": "97119",
    "end": "100560"
  },
  {
    "text": "eventually need to serve open models for",
    "start": "100560",
    "end": "102240"
  },
  {
    "text": "efficiency at scale while still continue",
    "start": "102240",
    "end": "105600"
  },
  {
    "text": "to depend on cutting edge models for",
    "start": "105600",
    "end": "107360"
  },
  {
    "text": "time to",
    "start": "107360",
    "end": "108280"
  },
  {
    "text": "market what flexibility and composable",
    "start": "108280",
    "end": "111079"
  },
  {
    "text": "automation will we all need when models",
    "start": "111079",
    "end": "113840"
  },
  {
    "text": "are a fundamental part of our",
    "start": "113840",
    "end": "115240"
  },
  {
    "text": "applications that's exactly the question",
    "start": "115240",
    "end": "117759"
  },
  {
    "text": "that we started the serving working",
    "start": "117759",
    "end": "119280"
  },
  {
    "text": "group in Kubernetes to answer a year ago",
    "start": "119280",
    "end": "121360"
  },
  {
    "text": "at",
    "start": "121360",
    "end": "122119"
  },
  {
    "text": "CubeConu and just like in the beginning",
    "start": "122119",
    "end": "124240"
  },
  {
    "text": "of Kubernetes we depend on experienced",
    "start": "124240",
    "end": "127520"
  },
  {
    "text": "platform",
    "start": "127520",
    "end": "128920"
  },
  {
    "text": "teams looking to build the next",
    "start": "128920",
    "end": "131120"
  },
  {
    "text": "generation of their platforms to guide",
    "start": "131120",
    "end": "133560"
  },
  {
    "text": "us we were very fortunate that Bite",
    "start": "133560",
    "end": "136160"
  },
  {
    "text": "Dance was ready to build the next",
    "start": "136160",
    "end": "137680"
  },
  {
    "text": "version of their platform they chose to",
    "start": "137680",
    "end": "139760"
  },
  {
    "text": "do it in the open with",
    "start": "139760",
    "end": "142959"
  },
  {
    "text": "us running LLM in Kubernetes sounds",
    "start": "143720",
    "end": "146800"
  },
  {
    "text": "simple in theory but in practice is",
    "start": "146800",
    "end": "149120"
  },
  {
    "text": "still challenging let's dive into",
    "start": "149120",
    "end": "151040"
  },
  {
    "text": "several production issues that shows LLM",
    "start": "151040",
    "end": "154000"
  },
  {
    "text": "inference challenges are truly unique",
    "start": "154000",
    "end": "156879"
  },
  {
    "text": "the first plot is from a bidance",
    "start": "156879",
    "end": "159120"
  },
  {
    "text": "production LM service capturing the",
    "start": "159120",
    "end": "161519"
  },
  {
    "text": "daily request distribution so as we can",
    "start": "161519",
    "end": "164239"
  },
  {
    "text": "see the request valuation is spiking the",
    "start": "164239",
    "end": "167360"
  },
  {
    "text": "input prompt size vary widely and the",
    "start": "167360",
    "end": "169760"
  },
  {
    "text": "output length are unpredictable this",
    "start": "169760",
    "end": "172640"
  },
  {
    "text": "actually highlights one of the biggest",
    "start": "172640",
    "end": "174800"
  },
  {
    "text": "differences between traditional",
    "start": "174800",
    "end": "176680"
  },
  {
    "text": "microservices each request is different",
    "start": "176680",
    "end": "179400"
  },
  {
    "text": "here's another demonstration we can see",
    "start": "179400",
    "end": "182560"
  },
  {
    "text": "sudden strikes in GPU compute activity",
    "start": "182560",
    "end": "185840"
  },
  {
    "text": "even under constant QPS so at that at",
    "start": "185840",
    "end": "189040"
  },
  {
    "text": "that time we figure it out a batch of",
    "start": "189040",
    "end": "191680"
  },
  {
    "text": "requests with super long uh prompts",
    "start": "191680",
    "end": "194959"
  },
  {
    "text": "sneak in so for LLM it's not just the",
    "start": "194959",
    "end": "198400"
  },
  {
    "text": "read of the request that matters but",
    "start": "198400",
    "end": "200720"
  },
  {
    "text": "also their shape the length of the",
    "start": "200720",
    "end": "202800"
  },
  {
    "text": "prompts the number of the generated",
    "start": "202800",
    "end": "204640"
  },
  {
    "text": "tokens and the prompt structure these",
    "start": "204640",
    "end": "207200"
  },
  {
    "text": "all significantly affect the GPU load",
    "start": "207200",
    "end": "209760"
  },
  {
    "text": "and make it",
    "start": "209760",
    "end": "211400"
  },
  {
    "text": "unpredictable let's see another",
    "start": "211400",
    "end": "213280"
  },
  {
    "text": "challenge in binance we serve many",
    "start": "213280",
    "end": "215760"
  },
  {
    "text": "models in production well their traffic",
    "start": "215760",
    "end": "218480"
  },
  {
    "text": "patterns vary widely some models are",
    "start": "218480",
    "end": "221280"
  },
  {
    "text": "critical production models while others",
    "start": "221280",
    "end": "223519"
  },
  {
    "text": "are experimental with near zero",
    "start": "223519",
    "end": "226440"
  },
  {
    "text": "traffic given how resource intensive",
    "start": "226440",
    "end": "229040"
  },
  {
    "text": "these models are this kind of traffic",
    "start": "229040",
    "end": "231519"
  },
  {
    "text": "screw makes the problem even more",
    "start": "231519",
    "end": "233920"
  },
  {
    "text": "problematic it also makes the life cycle",
    "start": "233920",
    "end": "236640"
  },
  {
    "text": "management and resource allocation",
    "start": "236640",
    "end": "239040"
  },
  {
    "text": "extremely challenging and highlight the",
    "start": "239040",
    "end": "241519"
  },
  {
    "text": "need for resource usage aware",
    "start": "241519",
    "end": "243840"
  },
  {
    "text": "orchestration rather than just relying",
    "start": "243840",
    "end": "246400"
  },
  {
    "text": "on static deployments another uh",
    "start": "246400",
    "end": "249360"
  },
  {
    "text": "challenge we face is hardware",
    "start": "249360",
    "end": "251239"
  },
  {
    "text": "heterogeneity due to the machine",
    "start": "251239",
    "end": "253200"
  },
  {
    "text": "delivery timelines coder policies and",
    "start": "253200",
    "end": "255920"
  },
  {
    "text": "availability requirements our inference",
    "start": "255920",
    "end": "258400"
  },
  {
    "text": "clusters commonly end up with a mix of",
    "start": "258400",
    "end": "261040"
  },
  {
    "text": "different GPU types as show in the table",
    "start": "261040",
    "end": "264160"
  },
  {
    "text": "in 15,000 GPU clusters we have over",
    "start": "264160",
    "end": "267360"
  },
  {
    "text": "eight GPU types even the heterogeneous",
    "start": "267360",
    "end": "270320"
  },
  {
    "text": "pool help us serve a range of different",
    "start": "270320",
    "end": "272600"
  },
  {
    "text": "workloads we noticed it brings the",
    "start": "272600",
    "end": "275639"
  },
  {
    "text": "complexity sometimes our model has to",
    "start": "275639",
    "end": "278560"
  },
  {
    "text": "run on different GPU types due to the",
    "start": "278560",
    "end": "280720"
  },
  {
    "text": "capacity issues well due to GPU's",
    "start": "280720",
    "end": "283759"
  },
  {
    "text": "different T flops and memory bandwidth",
    "start": "283759",
    "end": "286080"
  },
  {
    "text": "it's very hard to abstract away those",
    "start": "286080",
    "end": "288479"
  },
  {
    "text": "hardware differences which makes the",
    "start": "288479",
    "end": "290639"
  },
  {
    "text": "management like routing even more",
    "start": "290639",
    "end": "292639"
  },
  {
    "text": "difficult all of these challenges like",
    "start": "292639",
    "end": "295520"
  },
  {
    "text": "different request shape model traffic",
    "start": "295520",
    "end": "297759"
  },
  {
    "text": "screw or hardware hydrogenerity directly",
    "start": "297759",
    "end": "301040"
  },
  {
    "text": "impact one shared infrastructure layer",
    "start": "301040",
    "end": "303520"
  },
  {
    "text": "which is routing and that's why it",
    "start": "303520",
    "end": "306080"
  },
  {
    "text": "becomes a bottleneck we have to",
    "start": "306080",
    "end": "308600"
  },
  {
    "text": "rethink to handle the unique challenges",
    "start": "308600",
    "end": "311199"
  },
  {
    "text": "of LM inference we need a new class of",
    "start": "311199",
    "end": "313919"
  },
  {
    "text": "routing solutions that go beyond",
    "start": "313919",
    "end": "315840"
  },
  {
    "text": "traditional load balancers bance and",
    "start": "315840",
    "end": "318400"
  },
  {
    "text": "Google have been working together over",
    "start": "318400",
    "end": "320320"
  },
  {
    "text": "the last year to pull our experiences in",
    "start": "320320",
    "end": "322800"
  },
  {
    "text": "serving to make Kubernetes better for",
    "start": "322800",
    "end": "325120"
  },
  {
    "text": "LLM inference what we found is that LLM",
    "start": "325120",
    "end": "329199"
  },
  {
    "text": "serving success really depends on three",
    "start": "329199",
    "end": "331360"
  },
  {
    "text": "things denser faster and automated it",
    "start": "331360",
    "end": "335039"
  },
  {
    "text": "all comes down to the self-hosted LLM",
    "start": "335039",
    "end": "338039"
  },
  {
    "text": "serving that teams need more control",
    "start": "338039",
    "end": "341039"
  },
  {
    "text": "flexibility and speed than ever so let's",
    "start": "341039",
    "end": "344160"
  },
  {
    "text": "get started by digging into the first",
    "start": "344160",
    "end": "345759"
  },
  {
    "text": "part dancer",
    "start": "345759",
    "end": "347919"
  },
  {
    "text": "lora which stands for low rank",
    "start": "347919",
    "end": "349680"
  },
  {
    "text": "adaptation is a technique to fine-tune",
    "start": "349680",
    "end": "352080"
  },
  {
    "text": "the large pre-trained model efficiently",
    "start": "352080",
    "end": "354400"
  },
  {
    "text": "the core idea about the Laura is to",
    "start": "354400",
    "end": "356960"
  },
  {
    "text": "adapt the large pre-trained model to",
    "start": "356960",
    "end": "359520"
  },
  {
    "text": "specific tasks without needing to change",
    "start": "359520",
    "end": "361840"
  },
  {
    "text": "the entire model weight but just a small",
    "start": "361840",
    "end": "364160"
  },
  {
    "text": "set of the parameters called adapters so",
    "start": "364160",
    "end": "366800"
  },
  {
    "text": "these adapters commonly uh only add 1%",
    "start": "366800",
    "end": "370000"
  },
  {
    "text": "of the storage and memory overhead",
    "start": "370000",
    "end": "372319"
  },
  {
    "text": "compared to the original model waste",
    "start": "372319",
    "end": "374720"
  },
  {
    "text": "while at the same time it can maintain",
    "start": "374720",
    "end": "376880"
  },
  {
    "text": "the uh uh efficiencies and accuracies",
    "start": "376880",
    "end": "380560"
  },
  {
    "text": "without any",
    "start": "380560",
    "end": "381960"
  },
  {
    "text": "loss uh during the training phase Laura",
    "start": "381960",
    "end": "385680"
  },
  {
    "text": "frees the uh original model weights and",
    "start": "385680",
    "end": "388479"
  },
  {
    "text": "only fine-tune the two small matrix A",
    "start": "388479",
    "end": "390560"
  },
  {
    "text": "and B and uh in the inference phase it",
    "start": "390560",
    "end": "393680"
  },
  {
    "text": "get the output from the traditional uh",
    "start": "393680",
    "end": "395919"
  },
  {
    "text": "model and uh merge with the",
    "start": "395919",
    "end": "398479"
  },
  {
    "text": "multiplications of A and B to get final",
    "start": "398479",
    "end": "401600"
  },
  {
    "text": "uh inference results so that's the basic",
    "start": "401600",
    "end": "403840"
  },
  {
    "text": "of",
    "start": "403840",
    "end": "404680"
  },
  {
    "text": "Laura even Laura gives the resource uh",
    "start": "404680",
    "end": "407840"
  },
  {
    "text": "uh efficiencies and model flexibilities",
    "start": "407840",
    "end": "410720"
  },
  {
    "text": "managing it in Kubernetes is",
    "start": "410720",
    "end": "412639"
  },
  {
    "text": "surprisingly challenging why because",
    "start": "412639",
    "end": "415520"
  },
  {
    "text": "Laura has to be loaded alongside the",
    "start": "415520",
    "end": "417840"
  },
  {
    "text": "base model so that means Laura cannot be",
    "start": "417840",
    "end": "420639"
  },
  {
    "text": "deployed in separate containers those",
    "start": "420639",
    "end": "423039"
  },
  {
    "text": "breaks the Kubernetes principles right",
    "start": "423039",
    "end": "426319"
  },
  {
    "text": "now every single container may serve",
    "start": "426319",
    "end": "428720"
  },
  {
    "text": "multiple models that also brings the",
    "start": "428720",
    "end": "430880"
  },
  {
    "text": "problems to the uh request routing and",
    "start": "430880",
    "end": "434639"
  },
  {
    "text": "the original base models service cannot",
    "start": "434639",
    "end": "437360"
  },
  {
    "text": "be used to find the Laura anymore and",
    "start": "437360",
    "end": "439840"
  },
  {
    "text": "load balancer becomes even challenging",
    "start": "439840",
    "end": "442800"
  },
  {
    "text": "especially multiple Lauras contend for",
    "start": "442800",
    "end": "445360"
  },
  {
    "text": "the shared GPU resources within the same",
    "start": "445360",
    "end": "448599"
  },
  {
    "text": "pot luckily we solve these problems in",
    "start": "448599",
    "end": "451360"
  },
  {
    "text": "production let's use a concrete example",
    "start": "451360",
    "end": "453680"
  },
  {
    "text": "from Bidance to illustrate the benefits",
    "start": "453680",
    "end": "456160"
  },
  {
    "text": "of denser deployment in biteance we have",
    "start": "456160",
    "end": "459360"
  },
  {
    "text": "many database product like to integrate",
    "start": "459360",
    "end": "461759"
  },
  {
    "text": "AI capabilities to enhance user",
    "start": "461759",
    "end": "464240"
  },
  {
    "text": "productivity and lower the learning",
    "start": "464240",
    "end": "466479"
  },
  {
    "text": "curve text to SQL is one of the most",
    "start": "466479",
    "end": "469360"
  },
  {
    "text": "popular AI capabilities that translate",
    "start": "469360",
    "end": "471919"
  },
  {
    "text": "the uh natural language to SQL",
    "start": "471919",
    "end": "475080"
  },
  {
    "text": "queries in Binance we fine-tune the D6",
    "start": "475080",
    "end": "478319"
  },
  {
    "text": "33B model to support our text to SQL use",
    "start": "478319",
    "end": "481120"
  },
  {
    "text": "case however we have many business lines",
    "start": "481120",
    "end": "484400"
  },
  {
    "text": "they provide the SQL like query",
    "start": "484400",
    "end": "486160"
  },
  {
    "text": "scenarios such as log search or elastic",
    "start": "486160",
    "end": "489120"
  },
  {
    "text": "search while the query structure is very",
    "start": "489120",
    "end": "491840"
  },
  {
    "text": "similar to SQL but the syntax and the",
    "start": "491840",
    "end": "494720"
  },
  {
    "text": "semantics differ significantly so each",
    "start": "494720",
    "end": "497759"
  },
  {
    "text": "scenario needs their own uh fine-tune",
    "start": "497759",
    "end": "500240"
  },
  {
    "text": "model however if we support all these",
    "start": "500240",
    "end": "503199"
  },
  {
    "text": "scenarios with dedicated GPUs that will",
    "start": "503199",
    "end": "505599"
  },
  {
    "text": "be costly to address this resource issue",
    "start": "505599",
    "end": "509039"
  },
  {
    "text": "we adopt the Laura adapture solution we",
    "start": "509039",
    "end": "511840"
  },
  {
    "text": "fine-tune all these models in Laura way",
    "start": "511840",
    "end": "514159"
  },
  {
    "text": "and packing all the SQL like adapters",
    "start": "514159",
    "end": "518240"
  },
  {
    "text": "into one share deep models by following",
    "start": "518240",
    "end": "521680"
  },
  {
    "text": "this adapter sharing and routing",
    "start": "521680",
    "end": "523839"
  },
  {
    "text": "practice we can deploy the new adapters",
    "start": "523839",
    "end": "526640"
  },
  {
    "text": "in seconds achieve 1.5 to",
    "start": "526640",
    "end": "529959"
  },
  {
    "text": "4.7x GPU cost saving under different",
    "start": "529959",
    "end": "532880"
  },
  {
    "text": "traffic conditions so in this setup the",
    "start": "532880",
    "end": "535839"
  },
  {
    "text": "gateway plays a key role in minimizing",
    "start": "535839",
    "end": "538160"
  },
  {
    "text": "the number of the model servers and",
    "start": "538160",
    "end": "540160"
  },
  {
    "text": "intelligently select the least busy",
    "start": "540160",
    "end": "544319"
  },
  {
    "text": "instance bite Dance's experience",
    "start": "545080",
    "end": "547680"
  },
  {
    "text": "mirrored broad feedback from GKE's",
    "start": "547680",
    "end": "549920"
  },
  {
    "text": "generative AI customers large models are",
    "start": "549920",
    "end": "552240"
  },
  {
    "text": "slow and latency is important models",
    "start": "552240",
    "end": "555360"
  },
  {
    "text": "generate output tokens converted to text",
    "start": "555360",
    "end": "557760"
  },
  {
    "text": "at word or subword boundaries a bit",
    "start": "557760",
    "end": "560080"
  },
  {
    "text": "below human reading speed that's great",
    "start": "560080",
    "end": "562399"
  },
  {
    "text": "for chatbots but it doesn't work so well",
    "start": "562399",
    "end": "564560"
  },
  {
    "text": "for other use cases to hit a specific",
    "start": "564560",
    "end": "566880"
  },
  {
    "text": "latency objective in online serving you",
    "start": "566880",
    "end": "569120"
  },
  {
    "text": "need to understand your traffic",
    "start": "569120",
    "end": "570480"
  },
  {
    "text": "distribution in terms of input and",
    "start": "570480",
    "end": "572000"
  },
  {
    "text": "output tokens you need to choose a",
    "start": "572000",
    "end": "574000"
  },
  {
    "text": "foundation model sized to have",
    "start": "574000",
    "end": "576399"
  },
  {
    "text": "acceptable quality at the lowest compute",
    "start": "576399",
    "end": "578640"
  },
  {
    "text": "cost select an accelerator configuration",
    "start": "578640",
    "end": "581040"
  },
  {
    "text": "for both your model and your traffic",
    "start": "581040",
    "end": "582800"
  },
  {
    "text": "that is cost effective and reserve",
    "start": "582800",
    "end": "584720"
  },
  {
    "text": "enough accelerators to handle your base",
    "start": "584720",
    "end": "586480"
  },
  {
    "text": "load and hopefully cover your burst load",
    "start": "586480",
    "end": "589680"
  },
  {
    "text": "and sending more requests to an",
    "start": "589680",
    "end": "591920"
  },
  {
    "text": "accelerator at the same time increases",
    "start": "591920",
    "end": "594320"
  },
  {
    "text": "throughput and the latency of all other",
    "start": "594320",
    "end": "597440"
  },
  {
    "text": "requests leading to the curve up here so",
    "start": "597440",
    "end": "600399"
  },
  {
    "text": "it's your latency tolerance not just",
    "start": "600399",
    "end": "602560"
  },
  {
    "text": "your traffic load that determines the",
    "start": "602560",
    "end": "604399"
  },
  {
    "text": "cost to serve we worked with teams",
    "start": "604399",
    "end": "606560"
  },
  {
    "text": "trying to solve this problem in with",
    "start": "606560",
    "end": "608880"
  },
  {
    "text": "multiple variables repeatedly new models",
    "start": "608880",
    "end": "611200"
  },
  {
    "text": "new hardware better software and",
    "start": "611200",
    "end": "612720"
  },
  {
    "text": "increasing prompt and output lengths all",
    "start": "612720",
    "end": "614800"
  },
  {
    "text": "led to high toil how can we help",
    "start": "614800",
    "end": "617600"
  },
  {
    "text": "optimize production",
    "start": "617600",
    "end": "619720"
  },
  {
    "text": "serving we start where it hurts the most",
    "start": "619720",
    "end": "622720"
  },
  {
    "text": "where the very nature of large model",
    "start": "622720",
    "end": "624320"
  },
  {
    "text": "serving leads to wasted resources and a",
    "start": "624320",
    "end": "626720"
  },
  {
    "text": "human can't be in the loop load",
    "start": "626720",
    "end": "628240"
  },
  {
    "text": "balancing we are moving from",
    "start": "628240",
    "end": "630000"
  },
  {
    "text": "microservices and web apps with very",
    "start": "630000",
    "end": "632480"
  },
  {
    "text": "small and very predictable requests to",
    "start": "632480",
    "end": "635279"
  },
  {
    "text": "very expensive and highly variable LLM",
    "start": "635279",
    "end": "637440"
  },
  {
    "text": "queries roundrobin load balancing",
    "start": "637440",
    "end": "639680"
  },
  {
    "text": "doesn't cut it anymore some accelerators",
    "start": "639680",
    "end": "642160"
  },
  {
    "text": "would sit idle and some requests would",
    "start": "642160",
    "end": "644240"
  },
  {
    "text": "be stuck waiting longer to get processed",
    "start": "644240",
    "end": "647120"
  },
  {
    "text": "we need to look at each request and",
    "start": "647120",
    "end": "648880"
  },
  {
    "text": "estimate where it will fit we also need",
    "start": "648880",
    "end": "651200"
  },
  {
    "text": "to know how full the servers are and how",
    "start": "651200",
    "end": "653040"
  },
  {
    "text": "sending one more request to that server",
    "start": "653040",
    "end": "655440"
  },
  {
    "text": "will impact the latency of all other",
    "start": "655440",
    "end": "656959"
  },
  {
    "text": "requests",
    "start": "656959",
    "end": "658480"
  },
  {
    "text": "we believe we can automate a significant",
    "start": "658480",
    "end": "660480"
  },
  {
    "text": "amount of the toil and generative AI",
    "start": "660480",
    "end": "662680"
  },
  {
    "text": "serving just based on these two ideas at",
    "start": "662680",
    "end": "665200"
  },
  {
    "text": "the load balancer model the cost of an",
    "start": "665200",
    "end": "668000"
  },
  {
    "text": "incoming LLM request and how it'll",
    "start": "668000",
    "end": "670160"
  },
  {
    "text": "impact other requests on that server and",
    "start": "670160",
    "end": "672640"
  },
  {
    "text": "build a real time snapshot of the",
    "start": "672640",
    "end": "674800"
  },
  {
    "text": "performance of each backend by",
    "start": "674800",
    "end": "676640"
  },
  {
    "text": "continuously gathering metrics capturing",
    "start": "676640",
    "end": "678959"
  },
  {
    "text": "the complex relationships between",
    "start": "678959",
    "end": "681000"
  },
  {
    "text": "hardware concurrency of traffic and",
    "start": "681000",
    "end": "683680"
  },
  {
    "text": "client visible latency the extra CPU",
    "start": "683680",
    "end": "686399"
  },
  {
    "text": "time spent gathering metrics and",
    "start": "686399",
    "end": "688480"
  },
  {
    "text": "precisely scheduling requests lets us",
    "start": "688480",
    "end": "690959"
  },
  {
    "text": "use more of the accelerator and expose a",
    "start": "690959",
    "end": "693760"
  },
  {
    "text": "better operational view to the service",
    "start": "693760",
    "end": "696040"
  },
  {
    "text": "owner even with just the simplest",
    "start": "696040",
    "end": "698480"
  },
  {
    "text": "version of this loop we see good results",
    "start": "698480",
    "end": "701760"
  },
  {
    "text": "as the number of model servers grows",
    "start": "701760",
    "end": "703680"
  },
  {
    "text": "random load balancing of non-uniform",
    "start": "703680",
    "end": "705920"
  },
  {
    "text": "requests increases the chance that one",
    "start": "705920",
    "end": "708160"
  },
  {
    "text": "model server is going to get multiple",
    "start": "708160",
    "end": "710560"
  },
  {
    "text": "very long requests in a row since a",
    "start": "710560",
    "end": "713200"
  },
  {
    "text": "model server needs GPU memory to",
    "start": "713200",
    "end": "715200"
  },
  {
    "text": "generate output tokens when memory fills",
    "start": "715200",
    "end": "717839"
  },
  {
    "text": "up that model server has to stop",
    "start": "717839",
    "end": "719760"
  },
  {
    "text": "accepting new requests which increases",
    "start": "719760",
    "end": "722279"
  },
  {
    "text": "latency just by steering requests to the",
    "start": "722279",
    "end": "725440"
  },
  {
    "text": "model server with the most unused GPU",
    "start": "725440",
    "end": "728079"
  },
  {
    "text": "memory we can achieve over 30% higher",
    "start": "728079",
    "end": "731240"
  },
  {
    "text": "QPS at constant",
    "start": "731240",
    "end": "733480"
  },
  {
    "text": "latency over a uh predictable traffic",
    "start": "733480",
    "end": "736240"
  },
  {
    "text": "load getting us closer to the maximum",
    "start": "736240",
    "end": "738240"
  },
  {
    "text": "utilization of the accelerator and this",
    "start": "738240",
    "end": "740320"
  },
  {
    "text": "is just a representative chat agent",
    "start": "740320",
    "end": "741920"
  },
  {
    "text": "workload the more workloads you add to a",
    "start": "741920",
    "end": "744560"
  },
  {
    "text": "shared set of model servers and the more",
    "start": "744560",
    "end": "746480"
  },
  {
    "text": "traffic patterns that overlap the",
    "start": "746480",
    "end": "749200"
  },
  {
    "text": "benefit of an algorithmic approach to",
    "start": "749200",
    "end": "750880"
  },
  {
    "text": "load balancing increases while we",
    "start": "750880",
    "end": "753360"
  },
  {
    "text": "focused on a single dimension for",
    "start": "753360",
    "end": "754639"
  },
  {
    "text": "optimization here we both see a huge",
    "start": "754639",
    "end": "757680"
  },
  {
    "text": "number of possible uh optimizations and",
    "start": "757680",
    "end": "759920"
  },
  {
    "text": "research and ecosystem that could be",
    "start": "759920",
    "end": "761600"
  },
  {
    "text": "integrated but how can we bring a highly",
    "start": "761600",
    "end": "763760"
  },
  {
    "text": "distributed ML ecosystem together",
    "start": "763760",
    "end": "767360"
  },
  {
    "text": "we're here at CubeCon because what needs",
    "start": "767360",
    "end": "769120"
  },
  {
    "text": "to be done is more important than",
    "start": "769120",
    "end": "770880"
  },
  {
    "text": "algorithms or hardware we need common",
    "start": "770880",
    "end": "773279"
  },
  {
    "text": "ground for operationalizing large models",
    "start": "773279",
    "end": "776320"
  },
  {
    "text": "is just another workload if we're all",
    "start": "776320",
    "end": "778320"
  },
  {
    "text": "going to be running large models in",
    "start": "778320",
    "end": "779760"
  },
  {
    "text": "production as a fundamental part of our",
    "start": "779760",
    "end": "781200"
  },
  {
    "text": "application infrastructure in a few",
    "start": "781200",
    "end": "782839"
  },
  {
    "text": "years we need to identify the APIs and",
    "start": "782839",
    "end": "785760"
  },
  {
    "text": "components that can be standardized and",
    "start": "785760",
    "end": "787920"
  },
  {
    "text": "reused we need common ground to bring",
    "start": "787920",
    "end": "790639"
  },
  {
    "text": "the latest research to production and a",
    "start": "790639",
    "end": "792480"
  },
  {
    "text": "common framework for innovation that",
    "start": "792480",
    "end": "794160"
  },
  {
    "text": "everyone can take to production",
    "start": "794160",
    "end": "797040"
  },
  {
    "text": "if there's a standard dynamic and",
    "start": "797040",
    "end": "798639"
  },
  {
    "text": "extensible load balancer it's envoy we",
    "start": "798639",
    "end": "801519"
  },
  {
    "text": "chose to build our architecture around",
    "start": "801519",
    "end": "802959"
  },
  {
    "text": "Envoy because we knew it would work both",
    "start": "802959",
    "end": "804800"
  },
  {
    "text": "with and without Kubernetes and the rich",
    "start": "804800",
    "end": "807600"
  },
  {
    "text": "ecosystem of the gateway API would",
    "start": "807600",
    "end": "810079"
  },
  {
    "text": "ensure our extension could avoid",
    "start": "810079",
    "end": "812240"
  },
  {
    "text": "duplicating all of the regular load",
    "start": "812240",
    "end": "813680"
  },
  {
    "text": "balancing features that LLM service",
    "start": "813680",
    "end": "815839"
  },
  {
    "text": "owners also need we use the standard",
    "start": "815839",
    "end": "819120"
  },
  {
    "text": "envoy X proc callout mechanism to",
    "start": "819120",
    "end": "822480"
  },
  {
    "text": "decouple our algorithms from the load",
    "start": "822480",
    "end": "824959"
  },
  {
    "text": "balancer you can deploy them",
    "start": "824959",
    "end": "826440"
  },
  {
    "text": "independently this also gives us the",
    "start": "826440",
    "end": "828399"
  },
  {
    "text": "freedom to have multiple implementations",
    "start": "828399",
    "end": "830399"
  },
  {
    "text": "and to allow for forking and",
    "start": "830399",
    "end": "832040"
  },
  {
    "text": "experimentation when very large platform",
    "start": "832040",
    "end": "834480"
  },
  {
    "text": "teams need something that the open",
    "start": "834480",
    "end": "836160"
  },
  {
    "text": "source project doesn't provide yet we",
    "start": "836160",
    "end": "839199"
  },
  {
    "text": "also worked to standardize the metrics",
    "start": "839199",
    "end": "840800"
  },
  {
    "text": "we would need in the top model servers",
    "start": "840800",
    "end": "842399"
  },
  {
    "text": "so that operators have a consistent",
    "start": "842399",
    "end": "843920"
  },
  {
    "text": "experience across the ecosystem and",
    "start": "843920",
    "end": "845839"
  },
  {
    "text": "ouruler would need to do less our focus",
    "start": "845839",
    "end": "848720"
  },
  {
    "text": "is automating the boring parts of going",
    "start": "848720",
    "end": "850480"
  },
  {
    "text": "to production with LLM and bringing you",
    "start": "850480",
    "end": "852480"
  },
  {
    "text": "the best of ML research we plug into a",
    "start": "852480",
    "end": "855360"
  },
  {
    "text": "broad set of gateway solutions we don't",
    "start": "855360",
    "end": "857279"
  },
  {
    "text": "have any opinion on how you deploy your",
    "start": "857279",
    "end": "858720"
  },
  {
    "text": "model servers we build in support for",
    "start": "858720",
    "end": "861279"
  },
  {
    "text": "Laura for prioritization and fairness",
    "start": "861279",
    "end": "864399"
  },
  {
    "text": "and for standard model rollouts so you",
    "start": "864399",
    "end": "866959"
  },
  {
    "text": "can safely share your model servers",
    "start": "866959",
    "end": "868720"
  },
  {
    "text": "between many different workloads for",
    "start": "868720",
    "end": "870160"
  },
  {
    "text": "higher utilization we want to be a",
    "start": "870160",
    "end": "872320"
  },
  {
    "text": "loadbearing part of your serving",
    "start": "872320",
    "end": "874040"
  },
  {
    "text": "infrastructure orchestrate all of us can",
    "start": "874040",
    "end": "876480"
  },
  {
    "text": "depend on an ecosystem driving",
    "start": "876480",
    "end": "878399"
  },
  {
    "text": "optimization and the control you need",
    "start": "878399",
    "end": "880399"
  },
  {
    "text": "over your production journey",
    "start": "880399",
    "end": "883120"
  },
  {
    "text": "yeah 2025 makes the year of production",
    "start": "883120",
    "end": "885680"
  },
  {
    "text": "scaling leading inference uh projects",
    "start": "885680",
    "end": "888000"
  },
  {
    "text": "like VRM and control plan airs SG long",
    "start": "888000",
    "end": "891600"
  },
  {
    "text": "and tensor RT have all prioritize large",
    "start": "891600",
    "end": "894240"
  },
  {
    "text": "scale deployment in the road maps",
    "start": "894240",
    "end": "896399"
  },
  {
    "text": "putting efficient scalable inference at",
    "start": "896399",
    "end": "898800"
  },
  {
    "text": "the heart of their strategies so the",
    "start": "898800",
    "end": "900959"
  },
  {
    "text": "gateway API inference extension project",
    "start": "900959",
    "end": "903600"
  },
  {
    "text": "will play a critical role as the",
    "start": "903600",
    "end": "905680"
  },
  {
    "text": "foundation for LLM aware load balancer",
    "start": "905680",
    "end": "908160"
  },
  {
    "text": "in Kubernetes unlocking intelligent",
    "start": "908160",
    "end": "910560"
  },
  {
    "text": "traffic control for LM workloads looking",
    "start": "910560",
    "end": "913360"
  },
  {
    "text": "ahead we'll focus on enabling a full",
    "start": "913360",
    "end": "916399"
  },
  {
    "text": "suit of production readings features",
    "start": "916399",
    "end": "919120"
  },
  {
    "text": "including fairness for multi-tenencies",
    "start": "919120",
    "end": "921760"
  },
  {
    "text": "heterogeneous weighted routing adaptive",
    "start": "921760",
    "end": "924240"
  },
  {
    "text": "SLO driven routing and KV cache wire",
    "start": "924240",
    "end": "927040"
  },
  {
    "text": "routing to support production workloads",
    "start": "927040",
    "end": "929440"
  },
  {
    "text": "at scale and with that we'll turn it",
    "start": "929440",
    "end": "932560"
  },
  {
    "text": "back to you we hope that you try the if",
    "start": "932560",
    "end": "935120"
  },
  {
    "text": "you're thinking about LLM serving in",
    "start": "935120",
    "end": "936639"
  },
  {
    "text": "production give us a try give us",
    "start": "936639",
    "end": "938959"
  },
  {
    "text": "feedback and help become part of the",
    "start": "938959",
    "end": "940800"
  },
  {
    "text": "community thank you",
    "start": "940800",
    "end": "944160"
  },
  {
    "text": "thank you guys",
    "start": "945519",
    "end": "948839"
  }
]