[
  {
    "text": "so thanks so much for attending our session and this is all about AI gway",
    "start": "480",
    "end": "5879"
  },
  {
    "text": "specifically about Envoy and we're going to start by basically talk first of all",
    "start": "5879",
    "end": "11080"
  },
  {
    "text": "about why you need gway then understand why AI adoption in",
    "start": "11080",
    "end": "16560"
  },
  {
    "text": "Enterprise what we see working with all our customer what people doing there how it's usually come then we're going to",
    "start": "16560",
    "end": "23480"
  },
  {
    "text": "talk oops about Envoy as a I gway what did we do there implementation DET and",
    "start": "23480",
    "end": "29000"
  },
  {
    "text": "so on we'll have tons of demo and we're going to share with you some of our",
    "start": "29000",
    "end": "34280"
  },
  {
    "text": "learning so first of all we present ourselves so my name is ID LaVine I'm the founder in the SE of solo um and I'm",
    "start": "34280",
    "end": "41039"
  },
  {
    "text": "Eton yarmush um I thought the formatting is wrong whatever I tried um I'm a",
    "start": "41039",
    "end": "47520"
  },
  {
    "text": "senior architect here at solo and the AI Gateway lead exactly okay so and and",
    "start": "47520",
    "end": "54120"
  },
  {
    "text": "should we represent SAA I think they know yeah okay SAA is a company basically doing a lot of stuff around",
    "start": "54120",
    "end": "60440"
  },
  {
    "text": "networking API gway service Smash and and honestly everything we need to do in",
    "start": "60440",
    "end": "66040"
  },
  {
    "text": "order to make our customer and user successful and scaling um okay so let's start with",
    "start": "66040",
    "end": "72400"
  },
  {
    "text": "talking about it again just I think all of you know that but I just wanted to make the point is that you know the idea",
    "start": "72400",
    "end": "78640"
  },
  {
    "text": "with a gway is pretty simple if we were not going to do the gway that mean that",
    "start": "78640",
    "end": "84200"
  },
  {
    "text": "you need to put all those like operation code inside your micros service so or application and usually it's not healthy",
    "start": "84200",
    "end": "91200"
  },
  {
    "text": "because then if you have any problem you need to upgrade a library for instance",
    "start": "91200",
    "end": "96320"
  },
  {
    "text": "you will need to go and basically redeploy that application so it's really not healthy to put the business logic",
    "start": "96320",
    "end": "102079"
  },
  {
    "text": "with the Core Business logic of your application so ideally we will want to separate it and by the way by doing this",
    "start": "102079",
    "end": "110320"
  },
  {
    "text": "we kind of like nicely working with the with the um with the Persona because that way the application on user only",
    "start": "110320",
    "end": "117320"
  },
  {
    "text": "need to work with the co- business USIC but they know they really do not care about security and all the other stuff",
    "start": "117320",
    "end": "124079"
  },
  {
    "text": "and the platform engineering team getting back the power to their hand they can actually Force Security logging",
    "start": "124079",
    "end": "130879"
  },
  {
    "text": "monitoring and everything else that went need in order to make sure that the application will run well in production",
    "start": "130879",
    "end": "136920"
  },
  {
    "text": "okay so we understand this one let's first understand do we need a",
    "start": "136920",
    "end": "142879"
  },
  {
    "text": "new gway is there anything that we need of course we don't need a gway a new gway but do we need a new is there a new",
    "start": "142879",
    "end": "148400"
  },
  {
    "text": "use case even for a gway okay we can actually use a regular ones",
    "start": "148400",
    "end": "153879"
  },
  {
    "text": "and I think that even in Solo honestly it took us time to understand that this is a real thing right we're working with",
    "start": "153879",
    "end": "159840"
  },
  {
    "text": "our customer and actually have a lot of our customer coming to us and said we need this we understood that this is a",
    "start": "159840",
    "end": "165560"
  },
  {
    "text": "very different type of workflow and when you're talking about regular typical web",
    "start": "165560",
    "end": "170760"
  },
  {
    "text": "application traffic you know usually the request is very small it's quick um you",
    "start": "170760",
    "end": "176840"
  },
  {
    "text": "know you can do a lot of queries and pars that um it's not in terms of process and",
    "start": "176840",
    "end": "183560"
  },
  {
    "text": "request you know as soon as it's arrived boom it's basically processing it um the",
    "start": "183560",
    "end": "189319"
  },
  {
    "text": "time that it's taking to process it is usually in millisecond that's what we usually see uh similar request can uh",
    "start": "189319",
    "end": "196159"
  },
  {
    "text": "Ser from cash which mean that we even getting better performance because we don't really going Upstream to the",
    "start": "196159",
    "end": "202159"
  },
  {
    "text": "application and the request cost manage H within the beckon right so that's",
    "start": "202159",
    "end": "207720"
  },
  {
    "text": "that's basically what we usually using until today but that's not the case with AI right if you're looking at AI usually",
    "start": "207720",
    "end": "213360"
  },
  {
    "text": "the the request is way larger right you're using a lot of uh you know you're using single LM app you need to usually",
    "start": "213360",
    "end": "220560"
  },
  {
    "text": "ping it to a GPU or TPU so compute time there is some game here a request wait",
    "start": "220560",
    "end": "227760"
  },
  {
    "text": "for available so they're basically queuing it right so basically the request is actually waiting queuing in the application so that's interesting",
    "start": "227760",
    "end": "235319"
  },
  {
    "text": "that's different um in terms of variable processing from SEC again it's okay",
    "start": "235319",
    "end": "241680"
  },
  {
    "text": "right for AI you will expect it to take longer and therefore for second is fine even minute sometime it's fine and this",
    "start": "241680",
    "end": "247640"
  },
  {
    "text": "is actually very important piece because we see a lot of people actually using in AI models in some places like for",
    "start": "247640",
    "end": "254120"
  },
  {
    "text": "instance in the request part or they get right that doesn't make any sense you know my guys like no one will wait for two minutes until they will get the",
    "start": "254120",
    "end": "260280"
  },
  {
    "text": "request so that's really not actually what you need to do um request then you know um and sorry request often",
    "start": "260280",
    "end": "269240"
  },
  {
    "text": "guaranteed a unique content so there's no in terms of caching this is not going to work",
    "start": "269240",
    "end": "274759"
  },
  {
    "text": "very very simple and then last and not least in terms of traffic crowded",
    "start": "274759",
    "end": "280360"
  },
  {
    "text": "um it's depending on the request and that's very important piece so if we're",
    "start": "280360",
    "end": "285440"
  },
  {
    "text": "looking at all of this there is some use cases if you think about it a little bit even more you understand that we",
    "start": "285440",
    "end": "292639"
  },
  {
    "text": "actually need a little bit more logic and we will talk about way more we need to understand a little bit more the",
    "start": "292639",
    "end": "297880"
  },
  {
    "text": "logic of the request so maybe it's not enough for us to stay in the layer of maybe three we probably need to go a",
    "start": "297880",
    "end": "304240"
  },
  {
    "text": "little bit more and understand the request um itself in terms of you know",
    "start": "304240",
    "end": "309800"
  },
  {
    "text": "so so so okay so the workload are different but it's even more complex and why it's more complex because you also have a lot of people on your organ",
    "start": "309800",
    "end": "316759"
  },
  {
    "text": "organization a lot of application basically reaching out to a lot of models usually outside your organization",
    "start": "316759",
    "end": "323440"
  },
  {
    "text": "sometime inside the organization so there is a lot of traffic coming from everywhere so it's really really hard to",
    "start": "323440",
    "end": "329720"
  },
  {
    "text": "to maintain it and even if you want for instance I don't know to keep and we will talk about it more the you know you",
    "start": "329720",
    "end": "335280"
  },
  {
    "text": "want them to go to a specifically a a a model and then you want for instance to do God will there so then there is a lot",
    "start": "335280",
    "end": "342039"
  },
  {
    "text": "of it and it's just not consistent exactly the same problem that we have that we created a gway for to try to",
    "start": "342039",
    "end": "347759"
  },
  {
    "text": "create a consens consensus okay so again I think that the solution here is to put",
    "start": "347759",
    "end": "353400"
  },
  {
    "text": "a Gateway all the request is going to go to the Gateway and we giving us a lot of power to basically managing everything",
    "start": "353400",
    "end": "359720"
  },
  {
    "text": "that related to cost efficiency godwell semantic catching and a lot of",
    "start": "359720",
    "end": "365479"
  },
  {
    "text": "other interesting stuff that we are going to talk about in The Talk today so here's what we see with our customer",
    "start": "365479",
    "end": "371800"
  },
  {
    "text": "we're working quite a lot with customers that actually is working on real production work what we see is that most",
    "start": "371800",
    "end": "378840"
  },
  {
    "text": "of the project or most of the organization starting with the public models they're going their they're you",
    "start": "378840",
    "end": "385160"
  },
  {
    "text": "know playing with this they starting this is the easiest kind of like way to start using it but then they understand",
    "start": "385160",
    "end": "392759"
  },
  {
    "text": "a few things they understand that ay maybe those models are a little bit too big and therefore also very very",
    "start": "392759",
    "end": "401720"
  },
  {
    "text": "expensive and also some sometimes they have some private data that I cannot send there so what do you do you're",
    "start": "401720",
    "end": "408199"
  },
  {
    "text": "trying to take smaller models and put them on FR okay so that's a different",
    "start": "408199",
    "end": "413960"
  },
  {
    "text": "problem to Ender it right now you're running the infrastructure it's not open AI so how is that going to work showing",
    "start": "413960",
    "end": "419960"
  },
  {
    "text": "a lot of different H challenges that we need to solve and then what we see eventually happening is kind of like the",
    "start": "419960",
    "end": "427400"
  },
  {
    "text": "ibrid model so people starting with the public there's some data that will continue staying there right they",
    "start": "427400",
    "end": "433400"
  },
  {
    "text": "continue expose those use those those public models but they're also going to",
    "start": "433400",
    "end": "438680"
  },
  {
    "text": "use some internally and in their networking and how is all of this is going to work there is a lot of stuff",
    "start": "438680",
    "end": "445520"
  },
  {
    "text": "that we can do there and we will talk about a lot a lot of use case so again we will right now what we're going to do",
    "start": "445520",
    "end": "451120"
  },
  {
    "text": "the structure will be like this we will talk about start with public model we're going to explain what the challenges we",
    "start": "451120",
    "end": "456199"
  },
  {
    "text": "will go use case use use case then we show you a demo then we move to the next one the the private one again challenge",
    "start": "456199",
    "end": "463759"
  },
  {
    "text": "challenge challenge demo so that's going to be what we will do next Okie doie say then you take over all right so starting",
    "start": "463759",
    "end": "471199"
  },
  {
    "text": "from first principles when we talk to our customers the first thing that comes up every time is credential management",
    "start": "471199",
    "end": "478159"
  },
  {
    "text": "so we have seen over and over again that AI as AI adoption explodes all the teams that are using",
    "start": "478159",
    "end": "486080"
  },
  {
    "text": "these large llm providers are essentially given API keys to access",
    "start": "486080",
    "end": "492479"
  },
  {
    "text": "these providers and just reach out to them directly right so the apps themselves hold on to these Keys now",
    "start": "492479",
    "end": "498960"
  },
  {
    "text": "that is a huge problem for many reasons first of all how does key management work tracking revocation refreshing them",
    "start": "498960",
    "end": "506759"
  },
  {
    "text": "right client identity is managed Now by the providers which the the problem is",
    "start": "506759",
    "end": "511919"
  },
  {
    "text": "these providers they don't care about arbac they don't care about that's not their business their business is making",
    "start": "511919",
    "end": "517360"
  },
  {
    "text": "the best the biggest model ASAP right and so you now these applications are",
    "start": "517360",
    "end": "523080"
  },
  {
    "text": "essentially given free reign so how do we solve this well this is actually a",
    "start": "523080",
    "end": "528200"
  },
  {
    "text": "really typical Gateway use case right essentially you take you use your IDP",
    "start": "528200",
    "end": "534040"
  },
  {
    "text": "right to issue keys to your applications this is something that most likely everyone is already doing today the",
    "start": "534040",
    "end": "539560"
  },
  {
    "text": "Gateway holds on to the keys to access the llm providers and then you",
    "start": "539560",
    "end": "545320"
  },
  {
    "text": "essentially give you create rback rules to access those providers right so it's",
    "start": "545320",
    "end": "550640"
  },
  {
    "text": "the same thing that it's the same uh typical workflow that we're used to but just applied to these gateways uh to",
    "start": "550640",
    "end": "556560"
  },
  {
    "text": "these llm providers um so that's the first thing that we see come up all the",
    "start": "556560",
    "end": "561760"
  },
  {
    "text": "time so what's next so now we've gotten to the point where we have taken the",
    "start": "561760",
    "end": "567640"
  },
  {
    "text": "keys out of the applications right they're in the Gateway so that's good now we know where the keys are we know",
    "start": "567640",
    "end": "572680"
  },
  {
    "text": "what people are using awesome cool so now we have prompt management so this is a whole new class of problem that really",
    "start": "572680",
    "end": "578720"
  },
  {
    "text": "only exists for llms um so there's a whole bunch of things that can come up",
    "start": "578720",
    "end": "584079"
  },
  {
    "text": "here as you start to use these things more heavily so first of all you can have governing inappropriate use of",
    "start": "584079",
    "end": "591480"
  },
  {
    "text": "prompt submissions so imagine you have vulgar language you have jailbreaks right you need to make sure that as",
    "start": "591480",
    "end": "598959"
  },
  {
    "text": "these prompts are making it either uh to to these models right that they don't have any sensitive data that they don't",
    "start": "598959",
    "end": "604800"
  },
  {
    "text": "have um that they're not trying to access your databases in some bad way right the class of problems is entirely",
    "start": "604800",
    "end": "611160"
  },
  {
    "text": "different in fact oosp has released an entirely new top 10 list of security issues just for llm",
    "start": "611160",
    "end": "618680"
  },
  {
    "text": "apis um okay so how do we solve this right well again it's a similar problem",
    "start": "618680",
    "end": "625920"
  },
  {
    "text": "that we're all used to we're all used to wafs right a wa is essentially just a a guard mechanism that you put in front of",
    "start": "625920",
    "end": "632920"
  },
  {
    "text": "your apis now in the AI world I think",
    "start": "632920",
    "end": "638079"
  },
  {
    "text": "these things are are commonly referred to as guard rails but functionally it's the same thing right it's a set of",
    "start": "638079",
    "end": "644959"
  },
  {
    "text": "protections built into the gateway that give you the peace of mind right that",
    "start": "644959",
    "end": "650079"
  },
  {
    "text": "allow you to set up security rules such that these attacks uh cannot happen",
    "start": "650079",
    "end": "655560"
  },
  {
    "text": "right so there's prompt governance you can screen for unwanted texts and reject right you can establish consistency in",
    "start": "655560",
    "end": "661360"
  },
  {
    "text": "your prompt context right you can inject system prompts to make sure that you know you tell the system that it has to",
    "start": "661360",
    "end": "667000"
  },
  {
    "text": "be kind it has to be this it has to be that right you can make sure that there's a certain number of tokens um",
    "start": "667000",
    "end": "672639"
  },
  {
    "text": "you can reject or even transform the inappropriate content and this is all extendable because what we've seen with",
    "start": "672639",
    "end": "679920"
  },
  {
    "text": "all of our customers that this is actually right after they get the credential management done the next thing that they're worried about is data",
    "start": "679920",
    "end": "685519"
  },
  {
    "text": "exfiltration Numero Uno open Ai No nobody wants to give them nobody wants",
    "start": "685519",
    "end": "690760"
  },
  {
    "text": "to give them your data right that is that is Numero Uno issue for the cesos so okay so what's after that all right",
    "start": "690760",
    "end": "699279"
  },
  {
    "text": "so now all right we've made sure that the keys are locked we've made sure that",
    "start": "699279",
    "end": "705040"
  },
  {
    "text": "nothing bad is making it out but we still need to but now we need to figure out where is all this money going right",
    "start": "705040",
    "end": "712600"
  },
  {
    "text": "these models are so expensive whether you're running them privately or publicly that the cost per token is just",
    "start": "712600",
    "end": "718959"
  },
  {
    "text": "as astronomical right so how do we ensure that we can actually charge back",
    "start": "718959",
    "end": "725079"
  },
  {
    "text": "right the usage to the teams or individuals or business units that that we need to right so again a very typical",
    "start": "725079",
    "end": "732600"
  },
  {
    "text": "Gateway problem where we need to make sure that we have so we have access",
    "start": "732600",
    "end": "739360"
  },
  {
    "text": "logging right and again we're using our client identity our IDP right so we have",
    "start": "739360",
    "end": "744480"
  },
  {
    "text": "the client identity and so using a typical access logging format right glue",
    "start": "744480",
    "end": "749600"
  },
  {
    "text": "or the and the Gateway right can expose the number of tokens used by the request",
    "start": "749600",
    "end": "755120"
  },
  {
    "text": "and then you can add that and the client identity in so that you can correlate the usage and in addition you can use",
    "start": "755120",
    "end": "761199"
  },
  {
    "text": "typical rate limiting functionality but with a Twist so again rate limiting per",
    "start": "761199",
    "end": "766519"
  },
  {
    "text": "request is not 100% um useful here because the number of tokens actually",
    "start": "766519",
    "end": "772680"
  },
  {
    "text": "has a very large impact on the cost of the request and so we have a method of essentially using the number of tokens",
    "start": "772680",
    "end": "778920"
  },
  {
    "text": "in request as the rate limiting bucket um and so that allows you to have a a qu",
    "start": "778920",
    "end": "783959"
  },
  {
    "text": "like control to make sure that you're that these models are not being overwhelmed and that specific users or",
    "start": "783959",
    "end": "789160"
  },
  {
    "text": "specific teams are not um using up too much too quickly so with that in mind I'm going",
    "start": "789160",
    "end": "795880"
  },
  {
    "text": "to do a demo um and this specific demo is going to be about guard rails um so",
    "start": "795880",
    "end": "802519"
  },
  {
    "text": "as I mentioned so let me just apply my config I'm not going to go into too deep",
    "start": "802519",
    "end": "808560"
  },
  {
    "text": "uh detail about about the config cuz there's a lot of it and we don't have a lot of time but this repo I'm going to make public after the talk and in",
    "start": "808560",
    "end": "815160"
  },
  {
    "text": "addition I would love to talk to all of you um please uh come to our booth I think we're somewhere we're in the front",
    "start": "815160",
    "end": "821160"
  },
  {
    "text": "um come talk to us I'd love to explain all this in more detail uh it's",
    "start": "821160",
    "end": "827320"
  },
  {
    "text": "gone um but it's beautiful I I promise I you've never seen better yaml",
    "start": "830000",
    "end": "837440"
  },
  {
    "text": "um yeah yeah I'm gonna I'm gonna try turning it",
    "start": "837440",
    "end": "843480"
  },
  {
    "text": "off and on again hold on give me a second what yeah trying to that's what I",
    "start": "843480",
    "end": "849240"
  },
  {
    "text": "just",
    "start": "849240",
    "end": "851360"
  },
  {
    "text": "did oh you're almost there there's something was there and then he",
    "start": "857040",
    "end": "863320"
  },
  {
    "text": "left This is my yamel dance",
    "start": "863600",
    "end": "868440"
  },
  {
    "text": "yeah oh hey all right we [Applause] life this is why I don't do Hardware all",
    "start": "878399",
    "end": "885360"
  },
  {
    "text": "right um cool so as I mentioned right",
    "start": "885360",
    "end": "890519"
  },
  {
    "text": "it's very important when using these public models that your internal data doesn't get leaked so how do we protect that well GL um it's very important that",
    "start": "890519",
    "end": "898440"
  },
  {
    "text": "an AI Gateway ships with the like basic functionality to mask this now at the",
    "start": "898440",
    "end": "904519"
  },
  {
    "text": "same time we recognize that everyone's data is different right everyone's",
    "start": "904519",
    "end": "910199"
  },
  {
    "text": "requests are different and so the first guard rails um the first piece of",
    "start": "910199",
    "end": "915320"
  },
  {
    "text": "guardrails that we added into our code was an extension point because at the end of the day everyone's again",
    "start": "915320",
    "end": "921920"
  },
  {
    "text": "everyone's data is different and we know that there needs to be custom logic to filter out or potentially mask or block",
    "start": "921920",
    "end": "929600"
  },
  {
    "text": "the data that you care about so first of all I'm going to show off just a quick Rex so you know I'm going to tell uh",
    "start": "929600",
    "end": "936600"
  },
  {
    "text": "this is a bit of a contrived example but I'm going to tell openai to remove um",
    "start": "936600",
    "end": "942240"
  },
  {
    "text": "dashes from the following sentence now I'm currently telling it to block to mask any phone numbers and so you'll see",
    "start": "942240",
    "end": "948800"
  },
  {
    "text": "that when the request comes back it's actually going to be masked instead of having the phone number uh operated",
    "start": "948800",
    "end": "956240"
  },
  {
    "text": "on so give that a second and we see here that now let me actually pipe that into",
    "start": "956240",
    "end": "961920"
  },
  {
    "text": "JQ so it's a bit easier to read sorry um oh what happened okay one",
    "start": "961920",
    "end": "970959"
  },
  {
    "text": "second all right so now you see here that the so it says my phone number is phone number so essentially that's just",
    "start": "973720",
    "end": "979560"
  },
  {
    "text": "our system masking the phone number before it makes it out and you can do this additionally with social security",
    "start": "979560",
    "end": "986199"
  },
  {
    "text": "numbers email so I'll just do that quickly",
    "start": "986199",
    "end": "990920"
  },
  {
    "text": "oops and you see here it's the same thing my email is email address awesome",
    "start": "993839",
    "end": "999720"
  },
  {
    "text": "so but as I said right it's really important that we're able to do this in an extendable way so as a part of this",
    "start": "999720",
    "end": "1004920"
  },
  {
    "text": "demo I've deployed a little web hook that is running alongside and so I'm going to send",
    "start": "1004920",
    "end": "1010800"
  },
  {
    "text": "another request but I'm going to tell it now in in this case I I have set up this header such that I can tell it what to",
    "start": "1010800",
    "end": "1016800"
  },
  {
    "text": "do I would not recommend that in production um but it makes for a better demo so I can show you guys how the webook can",
    "start": "1016800",
    "end": "1023640"
  },
  {
    "text": "react so I'm going to go ahead and in this case I'm going to tell it that it needs to block if it finds any pii and",
    "start": "1023640",
    "end": "1030678"
  },
  {
    "text": "there we go so we've got pii detected in the request now I want to take a second",
    "start": "1030679",
    "end": "1036438"
  },
  {
    "text": "before I move on to the next section and explain how Envoy because again this",
    "start": "1036439",
    "end": "1042079"
  },
  {
    "text": "talk is all about Envoy right I haven't I haven't gone into Envoy yet so the beautiful thing about here let me just",
    "start": "1042079",
    "end": "1048160"
  },
  {
    "text": "go back uh open up the slides really fast the beautiful thing about what we're",
    "start": "1048160",
    "end": "1053600"
  },
  {
    "text": "presenting here is how easy it's almost how simple Envoy made building all of",
    "start": "1053600",
    "end": "1059520"
  },
  {
    "text": "this functionality if you notice right what did I talk about first I talked about injecting headers right because at",
    "start": "1059520",
    "end": "1065840"
  },
  {
    "text": "the end of the day authorization is mostly about injecting headers right envoy can do that super easily on any",
    "start": "1065840",
    "end": "1071720"
  },
  {
    "text": "route config you just inject headers right what did I talk about second I talked about prompt management so that",
    "start": "1071720",
    "end": "1077600"
  },
  {
    "text": "it gets a little more complic at because we're talking about messing around with the body right and so again Envoy about",
    "start": "1077600",
    "end": "1085240"
  },
  {
    "text": "a year ago introduced something called external processing external processing has been amazing amazing amazing amazing",
    "start": "1085240",
    "end": "1091760"
  },
  {
    "text": "we have found for solo and all of our customers because it's allowed us to implement logic in other languages very",
    "start": "1091760",
    "end": "1097600"
  },
  {
    "text": "very quickly and in I was even surprised in a very performant manner um and then",
    "start": "1097600",
    "end": "1103880"
  },
  {
    "text": "the third thing Oops why did we oh sorry hold on one second uh and the third thing we talked about was consumption",
    "start": "1103880",
    "end": "1109760"
  },
  {
    "text": "control and visibility again Envoy has an rate limiting a ton of rate limiting",
    "start": "1109760",
    "end": "1115159"
  },
  {
    "text": "functionality built in as well as a robust access logging system and actually to get the token weighted rate",
    "start": "1115159",
    "end": "1121799"
  },
  {
    "text": "limiting done uh for this functionality I had to Upstream a minor uh piece of",
    "start": "1121799",
    "end": "1126919"
  },
  {
    "text": "like a minor feature to get to be able to explain to Envoy how much to rate limit on but again Envoy having that",
    "start": "1126919",
    "end": "1133679"
  },
  {
    "text": "kind of flexibility built in has been amazing for us so awesome",
    "start": "1133679",
    "end": "1139400"
  },
  {
    "text": "so thanks so next we're going to talk about private models as I said they starting all in the cloud playing with",
    "start": "1139400",
    "end": "1145679"
  },
  {
    "text": "this having fun consuming those big H language model but then they want to come back to the infrastructure and then",
    "start": "1145679",
    "end": "1154400"
  },
  {
    "text": "there is different challenges so we will start with some of them so let talk about load balancing so we're talking",
    "start": "1154400",
    "end": "1160600"
  },
  {
    "text": "about load balancing we all know load balancing right it's pretty simple it's basically round robbing right that's the",
    "start": "1160600",
    "end": "1165960"
  },
  {
    "text": "algorithm very simple it's basically evenly spread it around the request but the thing is that this",
    "start": "1165960",
    "end": "1172919"
  },
  {
    "text": "application are a little bit different and there's some processes that are way way taking more long time so you don't",
    "start": "1172919",
    "end": "1178600"
  },
  {
    "text": "want it to do it fair like this because it's will basically cure in the application and basically it's not",
    "start": "1178600",
    "end": "1184400"
  },
  {
    "text": "really efficient right think about what's going to happen let's assume that you have any GPU and you have a lot a",
    "start": "1184400",
    "end": "1190280"
  },
  {
    "text": "lot of application queue there but it's time for it so you're moving it there the queue there will go longer versus",
    "start": "1190280",
    "end": "1196200"
  },
  {
    "text": "you probably was smarter for you to actually forwarded to a different uh GPU",
    "start": "1196200",
    "end": "1202240"
  },
  {
    "text": "so that's basically the Ida so what we doing there can talk about but basically what we're going to do there is",
    "start": "1202240",
    "end": "1207520"
  },
  {
    "text": "basically change the algorithm on the loot balancing we're going to get a response we will know the size and the",
    "start": "1207520",
    "end": "1213840"
  },
  {
    "text": "Quee of the the way the application is basically queuing we will get it back",
    "start": "1213840",
    "end": "1220000"
  },
  {
    "text": "and based on this we will change the algorithm and where we are basically routing so that's basically everything",
    "start": "1220000",
    "end": "1225480"
  },
  {
    "text": "that related to load balance balancing and I think this is actually a very unique feature I think that the",
    "start": "1225480",
    "end": "1230640"
  },
  {
    "text": "performance will go way faster and again you need to understand when you're going on Prem you're paying for those GPU they",
    "start": "1230640",
    "end": "1236760"
  },
  {
    "text": "are very very expensive you wanted to be very efficient about how much you're consuming it and how you actually making",
    "start": "1236760",
    "end": "1243960"
  },
  {
    "text": "a a the consumption really really um really really efficient so the same",
    "start": "1243960",
    "end": "1249320"
  },
  {
    "text": "thing right but it's not exactly relevant to this stock because we're talking not here talking about networking so much is everything that",
    "start": "1249320",
    "end": "1256039"
  },
  {
    "text": "related to scheduling and I think that this is something something that definitely we need to attack so the",
    "start": "1256039",
    "end": "1261440"
  },
  {
    "text": "beautiful of kubernetes is that it's actually extremely pluggable so it's really easy for me to actually change",
    "start": "1261440",
    "end": "1266880"
  },
  {
    "text": "for specific P the scheduling system again when you have gpus the thing that",
    "start": "1266880",
    "end": "1272960"
  },
  {
    "text": "everybody is doing is basically try to optimize the schedu to this GPU and",
    "start": "1272960",
    "end": "1278760"
  },
  {
    "text": "again it's different right because it's different workflow so this is not related to this because this is not a",
    "start": "1278760",
    "end": "1284360"
  },
  {
    "text": "networking there's no Envoy invol but I think this is something that is definitely something in people mind and",
    "start": "1284360",
    "end": "1290080"
  },
  {
    "text": "we you know we probably will adopt will attack it next dat you take next all",
    "start": "1290080",
    "end": "1295600"
  },
  {
    "text": "right so again so now we're talking about private models so it's it's even more important for us that we get as",
    "start": "1295600",
    "end": "1301600"
  },
  {
    "text": "much performance out of our models as possible so let's talk about that so first of all what are the challenges",
    "start": "1301600",
    "end": "1307480"
  },
  {
    "text": "that we face one we want to provide our models with the most accurate and upto-date information without retraining",
    "start": "1307480",
    "end": "1314159"
  },
  {
    "text": "training models is expensive it's difficult it requires a lot of time right these are these are processes that",
    "start": "1314159",
    "end": "1319320"
  },
  {
    "text": "we would rather avoid if possible uh we we want to be able to inject client specific and domain specific context",
    "start": "1319320",
    "end": "1326600"
  },
  {
    "text": "into these PRS right you think about the typical use case of chatbot right it's not that useful if you can't get data",
    "start": "1326600",
    "end": "1331679"
  },
  {
    "text": "about the customer uh we want to be able to minimize the risk of hallucinations",
    "start": "1331679",
    "end": "1336799"
  },
  {
    "text": "we've all seen chat GPT go a little crazy don't lie to me um and we also want to optimize response latency right",
    "start": "1336799",
    "end": "1344640"
  },
  {
    "text": "we can't cash like a standard uh HTTP call but we do want to be able to somehow cash right we want to figure out",
    "start": "1344640",
    "end": "1351120"
  },
  {
    "text": "how we can reduce the the latency okay so how do we solve this well the first one is a pretty solved",
    "start": "1351120",
    "end": "1357640"
  },
  {
    "text": "problem it's called rag or retrieval augmented generation now the cool thing",
    "start": "1357640",
    "end": "1363679"
  },
  {
    "text": "here is what we've decided is that actually this is this is the perfect place the Gateway is actually the",
    "start": "1363679",
    "end": "1368720"
  },
  {
    "text": "perfect place to do rag so if you think about it if you're not doing rag from the Gateway that means that you're",
    "start": "1368720",
    "end": "1374600"
  },
  {
    "text": "giving access uh again you're giving your applications access to this very private data and so you need to make",
    "start": "1374600",
    "end": "1380760"
  },
  {
    "text": "sure right again it's a it's an issue of who has access to the data um so that's",
    "start": "1380760",
    "end": "1386600"
  },
  {
    "text": "rag uh secondly and is semantic caching so I mentioned we can't do typical caching but we can do what's called",
    "start": "1386600",
    "end": "1392640"
  },
  {
    "text": "semantic caching and that means rather than uh matching on the exact request",
    "start": "1392640",
    "end": "1398320"
  },
  {
    "text": "right because this is natural language you're you're most likely not going to have the exact same request so you match",
    "start": "1398320",
    "end": "1403360"
  },
  {
    "text": "on something that's semantically similar and I'll explain that when I demo it in a minute um and the second is we yeah",
    "start": "1403360",
    "end": "1410559"
  },
  {
    "text": "and the third part actually I don't like this bullet point but we integrate with third party Vector DBS for both of these",
    "start": "1410559",
    "end": "1416840"
  },
  {
    "text": "features um so yeah we don't have a vector DB of our own okay so what is rag",
    "start": "1416840",
    "end": "1422880"
  },
  {
    "text": "rag is essentially um the ability to inject context specific information into",
    "start": "1422880",
    "end": "1429000"
  },
  {
    "text": "a prompt and again um this is so being able to do it from the AI Gateway means",
    "start": "1429000",
    "end": "1434440"
  },
  {
    "text": "that you can change the configuration on the fly right it's llm and database Bas agnostic right so you can perform this",
    "start": "1434440",
    "end": "1441200"
  },
  {
    "text": "operations uh these operations without your users knowing it right and without having to roll out any applications it's",
    "start": "1441200",
    "end": "1447000"
  },
  {
    "text": "just yaml uh and semantic caching um I just",
    "start": "1447000",
    "end": "1452159"
  },
  {
    "text": "again just to go over quickly it's the traditional caching mechanisms aren't as relevant here um and so we offer",
    "start": "1452159",
    "end": "1458360"
  },
  {
    "text": "semantic caching both manual and automatic um and this is based on",
    "start": "1458360",
    "end": "1463720"
  },
  {
    "text": "specific customer feedback that we received for a i for for uh for the Gateway",
    "start": "1463720",
    "end": "1469000"
  },
  {
    "text": "where essentially traditionally you would take a request and if it's not in the cach you immediately put it there",
    "start": "1469000",
    "end": "1476039"
  },
  {
    "text": "right but what if what if the the AI hallucinated and then all of a sudden you've got a bad request in there or a",
    "start": "1476039",
    "end": "1483120"
  },
  {
    "text": "bad response in your cash for an hour or a day or some large amount of time so in",
    "start": "1483120",
    "end": "1488919"
  },
  {
    "text": "order to fix that we allow manual control over the cash um and you can even separate caching by user to ensure",
    "start": "1488919",
    "end": "1495799"
  },
  {
    "text": "no leaking of that sensitive info that we mentioned earlier and I really like talking about santic",
    "start": "1495799",
    "end": "1501120"
  },
  {
    "text": "caching because the real world implications of this are huge okay if we talk about just just the the command",
    "start": "1501120",
    "end": "1507799"
  },
  {
    "text": "high right you're talking to a chat bot you say hi right that's one token and then it says back to you oh I didn't",
    "start": "1507799",
    "end": "1514520"
  },
  {
    "text": "write it down here but it says hello how are you doing that's 10 that's 10 tokens total right so per million calls that's",
    "start": "1514520",
    "end": "1520679"
  },
  {
    "text": "$10 okay so per billion tokens that's that's you're you're you're talking about",
    "start": "1520679",
    "end": "1526720"
  },
  {
    "text": "$90,000 right so if you have have 300 RPS on some simple token count all of a sudden you're talking about $90,000 so",
    "start": "1526720",
    "end": "1533000"
  },
  {
    "text": "why shouldn't High be cashed right why shouldn't your FAQ be cashed right there",
    "start": "1533000",
    "end": "1538840"
  },
  {
    "text": "are so many questions that we already know the answer to why aren't those already being put in the cash and saving",
    "start": "1538840",
    "end": "1544320"
  },
  {
    "text": "us a ton of money whether you're running private or public okay so that in mind let's move on to another demo I'm gonna",
    "start": "1544320",
    "end": "1551840"
  },
  {
    "text": "I'm going to move through these quite quickly because we're running out of time but it's all righty",
    "start": "1551840",
    "end": "1559200"
  },
  {
    "text": "okay so the first one I'm going to do is semantic caching um",
    "start": "1559240",
    "end": "1564799"
  },
  {
    "text": "so in order to show off semantic caching what I'm going to do is I'm G to ask openai what is GitHub and it's going to",
    "start": "1564799",
    "end": "1571600"
  },
  {
    "text": "come up with some pretty long-winded answer about open you know cicd and GID",
    "start": "1571600",
    "end": "1576880"
  },
  {
    "text": "and etc etc so yeah read me files very very long now I don't want my users to",
    "start": "1576880",
    "end": "1582600"
  },
  {
    "text": "see that that's a lot of data so what I want them to see is GitHub is my favorite website but I wish it wouldn't",
    "start": "1582600",
    "end": "1588320"
  },
  {
    "text": "go down so often so what I'm going to do is I'm going to take that and I'm going to throw it in the cash so let's do that",
    "start": "1588320",
    "end": "1595279"
  },
  {
    "text": "so I have we have a little service that we built which allows you again to manually update the cach so I'm going to",
    "start": "1595279",
    "end": "1602000"
  },
  {
    "text": "do that boom cash has been updated so let's ask that question",
    "start": "1602000",
    "end": "1607200"
  },
  {
    "text": "again there you go and if you notice it responded almost immediately so you cut down the response time massively and we",
    "start": "1608159",
    "end": "1614640"
  },
  {
    "text": "get the response that that we want and again remember I said this is a semantically similar cache so I probably",
    "start": "1614640",
    "end": "1620159"
  },
  {
    "text": "don't have to ask the same question I'm gonna ask it what is the purpose of GitHub again so as you can see the point",
    "start": "1620159",
    "end": "1628679"
  },
  {
    "text": "here is that semantically similar questions will get answered in the same way so with that in mind I'm going to",
    "start": "1628679",
    "end": "1635159"
  },
  {
    "text": "move on to the next demo and the next demo is going to be rag so before we jump into the rag demo",
    "start": "1635159",
    "end": "1641720"
  },
  {
    "text": "I want to show you a a passion of mine and that's French cheese now now there I",
    "start": "1641720",
    "end": "1649279"
  },
  {
    "text": "found this wonderful blog all about the top French cheeses okay and when I'm",
    "start": "1649279",
    "end": "1655279"
  },
  {
    "text": "setting up my chatot I want to make sure that they have access to this information so I've pre-populated a",
    "start": "1655279",
    "end": "1661840"
  },
  {
    "text": "vector database with the information from this article and we're going to use that for our rag so the first thing that",
    "start": "1661840",
    "end": "1668279"
  },
  {
    "text": "I want to point out is the top five cheeses okay those being Brie Cam and bear I'm not going to try and pronounce",
    "start": "1668279",
    "end": "1675120"
  },
  {
    "text": "this I'm really sorry or this one or that one but",
    "start": "1675120",
    "end": "1680120"
  },
  {
    "text": "but trust me they're great okay um so okay so let's ask let's",
    "start": "1680360",
    "end": "1686919"
  },
  {
    "text": "ask it something not related to French cheese right what's stepen Spielberg's favorite movie I don't know thank you thank you",
    "start": "1686919",
    "end": "1693880"
  },
  {
    "text": "openi that's awesome okay so now let's ask it what are the top five French",
    "start": "1693880",
    "end": "1699679"
  },
  {
    "text": "cheeses right that's that question that we care about that's the information that we have that open AI doesn't know",
    "start": "1699679",
    "end": "1706158"
  },
  {
    "text": "about boom as you you can see it has them Bri Cam and bear and then those",
    "start": "1706519",
    "end": "1712000"
  },
  {
    "text": "three that I didn't want to talk about so perfect amazing um all right so now",
    "start": "1712000",
    "end": "1718279"
  },
  {
    "text": "with that in mind let me delete this one so that we have clean slate um I had",
    "start": "1718279",
    "end": "1725960"
  },
  {
    "text": "something I was going to say but I don't remember so I'm gonna move on all right",
    "start": "1725960",
    "end": "1731880"
  },
  {
    "text": "okay and again the thing that I want to point out specifically here is ex",
    "start": "1731880",
    "end": "1737360"
  },
  {
    "text": "external process so it it has made our work on this so",
    "start": "1737360",
    "end": "1742519"
  },
  {
    "text": "much easier uh again all this being based on on Envoy do you want to talk about hybrid or do you want me",
    "start": "1742519",
    "end": "1749279"
  },
  {
    "text": "to I can start no no you know what you're doing exploding sorry oh my head",
    "start": "1749279",
    "end": "1756000"
  },
  {
    "text": "is exploding so the third use case we talked about remember we started with public then we talked about private and",
    "start": "1756000",
    "end": "1761320"
  },
  {
    "text": "now let's talk about hybrid cuz the reality is that they're both going to exist different llms have different use",
    "start": "1761320",
    "end": "1767200"
  },
  {
    "text": "cases and you or organizations are already we see this in our customers they're running in both places okay so",
    "start": "1767200",
    "end": "1774559"
  },
  {
    "text": "what does that mean for us well again it's a typical Gateway problem we need to load balance right maybe we are",
    "start": "1774559",
    "end": "1782240"
  },
  {
    "text": "running in the cloud we're running in opening eye right now but we've been testing out this local model and we",
    "start": "1782240",
    "end": "1787360"
  },
  {
    "text": "think it performs better so we're going to shift traffic over to it it's just a canary right we've seen this before",
    "start": "1787360",
    "end": "1793960"
  },
  {
    "text": "super simple so we're going to show that off another one and you can use that",
    "start": "1793960",
    "end": "1799880"
  },
  {
    "text": "same mechanism for cost control right these models are getting too expensive let's start shifting over let's start",
    "start": "1799880",
    "end": "1806080"
  },
  {
    "text": "shifting over some traffic seeing how it responds let's see how that goes and the second most and the second",
    "start": "1806080",
    "end": "1813799"
  },
  {
    "text": "important mechanism here is failover right we talk about Regional failover for for typical Services all resiliency",
    "start": "1813799",
    "end": "1820000"
  },
  {
    "text": "always having our services up it's the same thing for AI Services right for llms we need to make sure that no matter",
    "start": "1820000",
    "end": "1827080"
  },
  {
    "text": "what we don't have downtime now the interesting thing is the difference here is that typically",
    "start": "1827080",
    "end": "1833880"
  },
  {
    "text": "when a service goes down you get a 503 right that's that's typically the Response Code you're looking for 503 502",
    "start": "1833880",
    "end": "1839720"
  },
  {
    "text": "some sort of 5xx right but with AI use cases it's usually actually a 429",
    "start": "1839720",
    "end": "1846120"
  },
  {
    "text": "because the you're you have run out of tokens or for your uh usage of that",
    "start": "1846120",
    "end": "1852880"
  },
  {
    "text": "model right that's typical with these clouds with with Azure with AWS with all of them and we see this over and over",
    "start": "1852880",
    "end": "1858760"
  },
  {
    "text": "again so what we have done is essentially we allow you to do standard failover but on 429s and this is the",
    "start": "1858760",
    "end": "1866240"
  },
  {
    "text": "most important part of the talk if we have any Envoy maintainers in the audience please approve this",
    "start": "1866240",
    "end": "1874080"
  },
  {
    "text": "PR thank you um what I'll take a amazing love",
    "start": "1874120",
    "end": "1881880"
  },
  {
    "text": "you um and again and the last one very similar to failover but Regional aware",
    "start": "1881880",
    "end": "1887120"
  },
  {
    "text": "routing again this is something that Envoy does super well it's been in there since the beginning right Envoy is so so",
    "start": "1887120",
    "end": "1895039"
  },
  {
    "text": "so good at endpoint selection right locality aware routing making sure that you're going to the best pool of",
    "start": "1895039",
    "end": "1901200"
  },
  {
    "text": "endpoints it's the same thing it's the same thing and so what we can do using Envoy is we can enable all of these use",
    "start": "1901200",
    "end": "1908240"
  },
  {
    "text": "cases which are useful in general but especially useful for llms and so um we don't have time for",
    "start": "1908240",
    "end": "1915760"
  },
  {
    "text": "this slide but what is the most interesting okay okay about you okay so the last thing I",
    "start": "1915760",
    "end": "1923240"
  },
  {
    "text": "want to talk about when it comes to hybrid is what we're calling semantic analysis and what semantic analysis is",
    "start": "1923240",
    "end": "1929039"
  },
  {
    "text": "is basically when you I don't know who has used chat GPT here but often when it responds it'll give you multiple options",
    "start": "1929039",
    "end": "1935840"
  },
  {
    "text": "and it says which one do you like best now what we realized is that that we could do that but across multiple",
    "start": "1935840",
    "end": "1942440"
  },
  {
    "text": "providers right and so what what what we're able to do and this is with",
    "start": "1942440",
    "end": "1948399"
  },
  {
    "text": "feedback from our customers is basically combine multiple provider uh options into a single request and either allow",
    "start": "1948399",
    "end": "1957039"
  },
  {
    "text": "the user right the actual client to pick the best one or perform some sort of",
    "start": "1957039",
    "end": "1962120"
  },
  {
    "text": "async processing later and decide which one the organization feels as",
    "start": "1962120",
    "end": "1967600"
  },
  {
    "text": "best so with that in mind let's do move on to our last demo and this one is",
    "start": "1967600",
    "end": "1973200"
  },
  {
    "text": "going to be uh failover and load balancing",
    "start": "1973200",
    "end": "1978320"
  },
  {
    "text": "so I've set up two routes for this demo the first one I'm calling hybrid and",
    "start": "1978320",
    "end": "1983600"
  },
  {
    "text": "what hybrid does is it has a 50/50 split between open Ai and a local olama model",
    "start": "1983600",
    "end": "1990159"
  },
  {
    "text": "that I'm running so I'm going to run this query and I'm going to JQ out the the model that it used so first one we",
    "start": "1990159",
    "end": "1996799"
  },
  {
    "text": "get GPT 3.5 this one and now we've got quen so the quen is the local model and I'll",
    "start": "1996799",
    "end": "2003799"
  },
  {
    "text": "just show I think this one is actually Worth showing the AML this is just a typical Gateway API HTP route with 50-50",
    "start": "2003799",
    "end": "2012679"
  },
  {
    "text": "traffic splitting there is no magic here super simple and we enable you to do this very easily um now you'll notice",
    "start": "2012679",
    "end": "2019240"
  },
  {
    "text": "the backend refs are um are not using typical services and that's because",
    "start": "2019240",
    "end": "2024440"
  },
  {
    "text": "there's a little logic added on top but the fundamentals are the same um and so the next one that I have",
    "start": "2024440",
    "end": "2030399"
  },
  {
    "text": "here is a failover route and the failover route is going to indicate what I was showing earlier about if things",
    "start": "2030399",
    "end": "2036919"
  },
  {
    "text": "are returning for 29's right we're going to we're going to go down to the LA to our last option so here what I've done",
    "start": "2036919",
    "end": "2044039"
  },
  {
    "text": "is I've set up two open AI options and then the and then if those both fail we're going to use our our local model",
    "start": "2044039",
    "end": "2050720"
  },
  {
    "text": "and in order to prove that this is working first of all we're going to see here that this answer was returned Again",
    "start": "2050720",
    "end": "2056358"
  },
  {
    "text": "by our local o AMA model but if I go ahead and look I actually have a little",
    "start": "2056359",
    "end": "2061878"
  },
  {
    "text": "service here that's receiving the the traffic and returning 429s and we can see here that there's",
    "start": "2061879",
    "end": "2068320"
  },
  {
    "text": "two requests that came in first was for GPT where is it GPT 40 mini and the",
    "start": "2068320",
    "end": "2075118"
  },
  {
    "text": "second was for GPT 3.5 turbo right so we've explained to the system that it should try those first before moving on",
    "start": "2075119",
    "end": "2081839"
  },
  {
    "text": "and with that in mind I'm going to give it back to Ed to finish off yeah the",
    "start": "2081839",
    "end": "2087079"
  },
  {
    "text": "presentation how we wrap up oh on sorry sorry where is yeah okay okay so I will",
    "start": "2087079",
    "end": "2096280"
  },
  {
    "text": "finish it quick because my other is killed it's killing me but when you're talking about AI there is I don't know",
    "start": "2096280",
    "end": "2102000"
  },
  {
    "text": "if you're aware of it but there is this model called the Tre Ag and basically when you starting attacking a problem in",
    "start": "2102000",
    "end": "2108400"
  },
  {
    "text": "AI um the Tre age are helpful right",
    "start": "2108400",
    "end": "2113520"
  },
  {
    "text": "that's talking about the quality of the answers and that's where semantic analysis could be very interesting right",
    "start": "2113520",
    "end": "2120760"
  },
  {
    "text": "the second one is honest right we really need to make sure that there's no hallucination in the models that where r",
    "start": "2120760",
    "end": "2127839"
  },
  {
    "text": "is kind of like attacking that problem very very nice and then the last one is armless and that's basically everything",
    "start": "2127839",
    "end": "2134079"
  },
  {
    "text": "that related to safe and security and that's where God R is basically playing a so when you're starting a project in",
    "start": "2134079",
    "end": "2139960"
  },
  {
    "text": "AI keep this in mind and understand that those API getways actually can help you",
    "start": "2139960",
    "end": "2145760"
  },
  {
    "text": "with each of those AG but I will finish with this slide because I think it's important so you",
    "start": "2145760",
    "end": "2152440"
  },
  {
    "text": "know we talk about what we see with our customers but I wanted to show you what we usually seen the beginning when we",
    "start": "2152440",
    "end": "2159280"
  },
  {
    "text": "started working with them with customers in Enterprise there's a lot of POC out",
    "start": "2159280",
    "end": "2164520"
  },
  {
    "text": "there people are starting and they're pcing and they are very excited and they're doing getting quickly kind of",
    "start": "2164520",
    "end": "2170880"
  },
  {
    "text": "like a result that are really impressive but then they want to go to production right and what we see is that and this",
    "start": "2170880",
    "end": "2179560"
  },
  {
    "text": "is some of aan demo show the men in the middle these things right now the AI",
    "start": "2179560",
    "end": "2186760"
  },
  {
    "text": "there is a lot of involvement with human you saw the idea for instance for one of our customer that we did basically the",
    "start": "2186760",
    "end": "2193880"
  },
  {
    "text": "caching the semantic caching that we did there is basically a person on the other Sid that is going to handle the cach in",
    "start": "2193880",
    "end": "2201400"
  },
  {
    "text": "the beginning right this is people people is going to manage it so though we kind of look very excited about Ai",
    "start": "2201400",
    "end": "2207119"
  },
  {
    "text": "and there's a good reason we have a big believer in it we need to understand that right now the that actually",
    "start": "2207119",
    "end": "2212800"
  },
  {
    "text": "happening in those organization it's a lot of human intervention and I think that this is something something that we",
    "start": "2212800",
    "end": "2218079"
  },
  {
    "text": "need to know because it's really really important okay so I will finish with this I don't know if you guys saw the keynote but we just actually donate a",
    "start": "2218079",
    "end": "2226079"
  },
  {
    "text": "pool request at of our Gateway uh so we have a gateway called glue it's a very",
    "start": "2226079",
    "end": "2232079"
  },
  {
    "text": "mature Gateway we have some of a customer there so they're using it in a lot of their",
    "start": "2232079",
    "end": "2237160"
  },
  {
    "text": "organization and we basically donated to the cncf as a as the name under the name",
    "start": "2237160",
    "end": "2242960"
  },
  {
    "text": "katees Gateway as part of this we also kind going to take the entire solution",
    "start": "2242960",
    "end": "2249119"
  },
  {
    "text": "of the AI get that we have and put it Upstream as well which mean that this is not an empty repositories you know",
    "start": "2249119",
    "end": "2256200"
  },
  {
    "text": "there's so many BS in the market right now people are announcing a lot of initiative that basically is nothing",
    "start": "2256200",
    "end": "2262680"
  },
  {
    "text": "besides marketing this is not the case you know we have some of our customer that can tell you here we are working",
    "start": "2262680",
    "end": "2269640"
  },
  {
    "text": "them we are getting them to production there is a lot of very interesting stuff that happening right now and it's very",
    "start": "2269640",
    "end": "2275760"
  },
  {
    "text": "very educational to us and we feel that if we all going to join together as a community probably can even get more",
    "start": "2275760",
    "end": "2281119"
  },
  {
    "text": "data right and B basically work on this together so again everything that Dayan was talking about we will Upstream I",
    "start": "2281119",
    "end": "2287400"
  },
  {
    "text": "think that would be very exciting and and we would love if you'll join us to make it even better so yeah thank you so",
    "start": "2287400",
    "end": "2293480"
  },
  {
    "text": "much for having us and I believe was useful",
    "start": "2293480",
    "end": "2298560"
  }
]