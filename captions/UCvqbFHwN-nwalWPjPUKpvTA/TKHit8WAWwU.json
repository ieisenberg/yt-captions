[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "[Music] all right so welcome everyone to another session of our research user group",
    "start": "560",
    "end": "7680"
  },
  {
    "text": "so today's topic has been uh requested for a long time and we never actually got to cover it",
    "start": "7680",
    "end": "14960"
  },
  {
    "text": "so elements and we have jamie and scott from g research to",
    "start": "14960",
    "end": "22640"
  },
  {
    "text": "present how they are handling this and and teach us how to how we should do it",
    "start": "22640",
    "end": "28640"
  },
  {
    "text": "so yeah onto youtube thank you all right thanks right let me see if i can work out to share my screen",
    "start": "28640",
    "end": "36320"
  },
  {
    "text": "don't think i have shared on this platform before wow there we go all right can you see",
    "start": "37520",
    "end": "45120"
  },
  {
    "text": "that before we kick off yeah it looks great great okay all right so um",
    "start": "45120",
    "end": "51600"
  },
  {
    "text": "yeah this is uh me and scott are going to talk to you a little bit about uh bare metal kubernetes at g research so",
    "start": "51600",
    "end": "59120"
  },
  {
    "text": "not necessarily telling you exactly how to do it or how the only way it can be done but just talking a bit about",
    "start": "59120",
    "end": "64720"
  },
  {
    "text": "our adventures with it what we're up to uh what we've learned along the way um so in terms of introductions uh we've",
    "start": "64720",
    "end": "71439"
  },
  {
    "text": "actually both of us been to cern to visit um ricardo and co and managed to get the obligatory photo",
    "start": "71439",
    "end": "77040"
  },
  {
    "text": "in front of um i think that's alice um so we put those next to each other but yeah i'm jamie paul um most of you",
    "start": "77040",
    "end": "82960"
  },
  {
    "text": "probably know me because i co-host this with ricardo every other week anyway although haven't been around for a few weeks so apologies for that",
    "start": "82960",
    "end": "89680"
  },
  {
    "text": "and i'm the compute platform engineering manager here at g research so responsible for all things kubernetes",
    "start": "89680",
    "end": "96159"
  },
  {
    "text": "and batch compute and calc farm and that kind of thing and then i've got scott",
    "start": "96159",
    "end": "101600"
  },
  {
    "text": "here with me who i'll play introducing hi uh yeah i'm scott so i'm cloud engineer i work",
    "start": "101600",
    "end": "106880"
  },
  {
    "text": "mostly on openstack uh but yeah uh so yeah a lot of that is ironic",
    "start": "106880",
    "end": "113520"
  },
  {
    "text": "cool um very very brief bit about georesearch for those who don't know so we're a fintech company based in london",
    "start": "113520",
    "end": "120000"
  },
  {
    "start": "119000",
    "end": "119000"
  },
  {
    "text": "we run a large distributed research platform for teams of quants to look for",
    "start": "120000",
    "end": "125439"
  },
  {
    "text": "patterns in real world noisy data sets of financial data uh looking for for",
    "start": "125439",
    "end": "130479"
  },
  {
    "text": "patterns for for our clients um and currently we're we're still saying for a while but it's still true",
    "start": "130479",
    "end": "136720"
  },
  {
    "text": "migrating large amounts of our batch compute workloads from windows and hd condor onto",
    "start": "136720",
    "end": "143040"
  },
  {
    "text": "kubernetes and linux and containerization and all that good stuff uh so yeah without further ado go",
    "start": "143040",
    "end": "149520"
  },
  {
    "text": "straight into the ironic portion of the presentation which i'll hand over to scott for",
    "start": "149520",
    "end": "154560"
  },
  {
    "text": "okay so uh so yeah a little bit of a background first of all so what is ironic ironic is",
    "start": "154560",
    "end": "161120"
  },
  {
    "start": "160000",
    "end": "160000"
  },
  {
    "text": "an integrated openstack service which aims to provision bare metal machines instead of virtual machines uh so ironic",
    "start": "161120",
    "end": "167280"
  },
  {
    "text": "supports using vendor-specific plugins which implement additional functionality such as moving machines between",
    "start": "167280",
    "end": "173040"
  },
  {
    "text": "different networks um so yeah and the main things about well the",
    "start": "173040",
    "end": "178319"
  },
  {
    "text": "main thing for this talk really is to focus on the different states we have in ironic it's not limited to these but the",
    "start": "178319",
    "end": "184159"
  },
  {
    "text": "main ones are rolling cleaning holding and provisioning so how does it work under the hood um so",
    "start": "184159",
    "end": "191680"
  },
  {
    "text": "ironic it's pretty straightforward so what it is it does ipmi and pxe and a",
    "start": "191680",
    "end": "198480"
  },
  {
    "text": "mix of mix that and around disk image um and then it turns machines on and off and moves in between different networks",
    "start": "198480",
    "end": "205519"
  },
  {
    "text": "as they move through different parts of the build so ironic can be can be deployed",
    "start": "205519",
    "end": "210799"
  },
  {
    "text": "standalone but most common way to do it and probably like in a production environment um it sort of sits beside",
    "start": "210799",
    "end": "217760"
  },
  {
    "text": "um other openstack projects such as like nova neutral and glance just a bit of background so nova is used for deploying",
    "start": "217760",
    "end": "225760"
  },
  {
    "text": "like vms like virtual machines neutron is your network and and then glance is like an image catalog",
    "start": "225760",
    "end": "231680"
  },
  {
    "text": "um so yeah uh ironic will use those uh different services",
    "start": "231680",
    "end": "236879"
  },
  {
    "text": "um to to get images or change networks or whatever it needs to do so the good thing as well is when a bare",
    "start": "236879",
    "end": "243519"
  },
  {
    "text": "metal machine is deleted uh by the user it's cleaned and then um it's just returned back into the",
    "start": "243519",
    "end": "248799"
  },
  {
    "text": "available pool and then someone can else just someone else can just pick out that that pool",
    "start": "248799",
    "end": "254400"
  },
  {
    "start": "254000",
    "end": "254000"
  },
  {
    "text": "so this is really high level um diagram just to sort of show um",
    "start": "254400",
    "end": "260000"
  },
  {
    "text": "the sort of enrollment uh stage that we've got so if you look on the left there there's",
    "start": "260000",
    "end": "265040"
  },
  {
    "text": "a few open source products we use um so one is kyobi which is a",
    "start": "265040",
    "end": "270160"
  },
  {
    "text": "sort of a subproject of the color ansible project in openstack and that's used to deploy",
    "start": "270160",
    "end": "276560"
  },
  {
    "text": "also use it to deploy new bare metal nodes into ironic as well so essentially it's just a bunch of",
    "start": "276560",
    "end": "283040"
  },
  {
    "text": "ansible and we use um muse jenkins to sort of uh to",
    "start": "283040",
    "end": "289759"
  },
  {
    "text": "orchestrate that i guess so if we look at what the enrollment phase",
    "start": "289759",
    "end": "295440"
  },
  {
    "start": "293000",
    "end": "293000"
  },
  {
    "text": "actually does so we go through pre-inspection first of all it's just like the pre-steps before you can",
    "start": "295440",
    "end": "301440"
  },
  {
    "text": "actually look at the nodes um and work out what what's going on in there so we create a record of it in the",
    "start": "301440",
    "end": "306960"
  },
  {
    "text": "openstack api then we set the resource class we buy we apply some baseline bios",
    "start": "306960",
    "end": "312479"
  },
  {
    "text": "and ilo settings the important bit here is the resource class so a resource class essentially is just like um",
    "start": "312479",
    "end": "318479"
  },
  {
    "text": "defines like what i know like a type of node so you might have like a certain type of gpu node or cpu node or like",
    "start": "318479",
    "end": "325919"
  },
  {
    "text": "specialist hardware um you need to find that as a resource class it basically says this is what my server should look",
    "start": "325919",
    "end": "332240"
  },
  {
    "text": "like you just have this much ram these disks and all that kind of all that stuff so um we define all that and we",
    "start": "332240",
    "end": "340160"
  },
  {
    "text": "say this is what i expect these new nodes to look like and then we go through to the next phase which is",
    "start": "340160",
    "end": "346240"
  },
  {
    "text": "inspection which is an ironic sort of state so it will turn the server on pixie boot",
    "start": "346240",
    "end": "351680"
  },
  {
    "text": "into the ram disk and then it will discover what hardware is there check for things like cabling issues and",
    "start": "351680",
    "end": "357840"
  },
  {
    "text": "identify the switch it's plugged into so when we move it we know which switch to log into to actually move the port",
    "start": "357840",
    "end": "364160"
  },
  {
    "text": "and then it will create those ports in ironic um so what that allows us to do is then basically cross check between the",
    "start": "364160",
    "end": "370479"
  },
  {
    "text": "resource class and what the um server actually actually has inside it because",
    "start": "370479",
    "end": "376720"
  },
  {
    "text": "if you've got if you think if you've got a big pool of servers and when you hand one back and you take a new one you want to make sure that you're getting the",
    "start": "376720",
    "end": "382880"
  },
  {
    "text": "same server back well not literally the same but one that has the same spec",
    "start": "382880",
    "end": "388560"
  },
  {
    "text": "so once we've done the inspection we can we then know where it's plugged in and which switchboard so",
    "start": "388560",
    "end": "395759"
  },
  {
    "text": "neutral will then move it to what's known as the cleaning vlan which is essentially the same as that the cleaning state",
    "start": "395759",
    "end": "402160"
  },
  {
    "text": "and then we go for like a what's known as cleaning so cleaning runs inside the ram disk image again and",
    "start": "402160",
    "end": "409120"
  },
  {
    "text": "what it does is uh it will just boot into it and it will have like a set of steps basically python scripts and it",
    "start": "409120",
    "end": "414560"
  },
  {
    "text": "will just run through those in order of priority so we do things like updating firmware verifying that the ilo settings",
    "start": "414560",
    "end": "421199"
  },
  {
    "text": "are all correct ntp um set up the storage we wipe the hard disks and then we check if there's gpus",
    "start": "421199",
    "end": "428000"
  },
  {
    "text": "in there we'll check their health as well once it's done that it should be good to go almost so we",
    "start": "428000",
    "end": "434319"
  },
  {
    "text": "just finally run some tests on it so we run some burning tests and then we move it to the holding vlan",
    "start": "434319",
    "end": "440080"
  },
  {
    "text": "and then it goes from cleaning into the holding state that essentially means it's ready for a user to pick it up the",
    "start": "440080",
    "end": "445360"
  },
  {
    "text": "other side so if we just look back at that diagram you shall understand this a little bit more now um so nodes come in from the",
    "start": "445360",
    "end": "451919"
  },
  {
    "text": "left for our automation into ironic api we inspect them and then they get moved",
    "start": "451919",
    "end": "458240"
  },
  {
    "text": "into where they are in the data center so a conductor basically is a microservice in ironic which is um which",
    "start": "458240",
    "end": "465440"
  },
  {
    "text": "its purpose is to sort of look after a group of nodes so you might have like a common set of nodes or",
    "start": "465440",
    "end": "472080"
  },
  {
    "text": "like an area in a data center like an availability zone or something like that um so they all just get bunched up and",
    "start": "472080",
    "end": "477759"
  },
  {
    "text": "yeah and then it's all ready for ready for people to use the other side",
    "start": "477759",
    "end": "482800"
  },
  {
    "text": "so moving on onto the deployment side um so there's a person there they will",
    "start": "482800",
    "end": "488879"
  },
  {
    "start": "483000",
    "end": "483000"
  },
  {
    "text": "pick um a flavor and a network and an a-z in an image um and then that transforms into",
    "start": "488879",
    "end": "495840"
  },
  {
    "text": "some novus uh some stuff that happens in iran in openstack and then out pops a bare metal node the other side",
    "start": "495840",
    "end": "502240"
  },
  {
    "text": "so we'll just take a little bit of a closer look at that happens is the user requests the new bare metal machine via terraform in our",
    "start": "502240",
    "end": "509039"
  },
  {
    "text": "case but i mean you can just do it by the api if you really want to um and then the flavor selected is the",
    "start": "509039",
    "end": "514959"
  },
  {
    "text": "thing that maps to the resource class so earlier when i said you've got a resource class it's like a type of server the flavor is basically the user",
    "start": "514959",
    "end": "521599"
  },
  {
    "text": "defining what type of server that they actually want to pull from the pool and then the network and the",
    "start": "521599",
    "end": "527120"
  },
  {
    "text": "availability zone that they select maps to some sort of location within a data center that allows you to sort of scale",
    "start": "527120",
    "end": "532480"
  },
  {
    "text": "this to quite a lot um quite a lot of servers",
    "start": "532480",
    "end": "537680"
  },
  {
    "text": "so yeah if we look back here that's just that first bit just up here so the user selects and they go",
    "start": "537680",
    "end": "543200"
  },
  {
    "text": "into the openstack process so the openstack side of the process um",
    "start": "543200",
    "end": "550320"
  },
  {
    "text": "you hit that you hit the nova api then that will talk to placement and the scheduler and that will basically look",
    "start": "550320",
    "end": "556560"
  },
  {
    "text": "in the pool and it will say what's available give me the first node of the top or the first hundred nodes or first thousand nodes whatever when you select",
    "start": "556560",
    "end": "564000"
  },
  {
    "text": "neutron although then go and move all of those into the provision in vlan so then they go into that provisioning state so",
    "start": "564000",
    "end": "569600"
  },
  {
    "text": "we go from holding back into provisioning and then this is the state where we get ready for the user to use",
    "start": "569600",
    "end": "576000"
  },
  {
    "text": "so in machine provisioning we turn the server on um",
    "start": "576000",
    "end": "581279"
  },
  {
    "text": "using ipmi we pixie boot into the round disk image and then we apply a few bios settings that might be like um hyper",
    "start": "581279",
    "end": "588240"
  },
  {
    "text": "threading on or off that's probably the most common one but you can you can configure anything you want um as long as it's available via the api",
    "start": "588240",
    "end": "595839"
  },
  {
    "text": "um and then we pull the user's image from glance so the user will specify that image when they actually build the",
    "start": "595839",
    "end": "601920"
  },
  {
    "text": "machine they don't want to run the ram disk image because that's what all our tools in it hasn't got their tools in it so this would um in our case be flat car",
    "start": "601920",
    "end": "609519"
  },
  {
    "text": "would get pulled from glance which is the image service in openstack so that basically explains the blocks",
    "start": "609519",
    "end": "615920"
  },
  {
    "text": "there on the right so yeah request comes in schedules nova compute will coordinate",
    "start": "615920",
    "end": "622079"
  },
  {
    "text": "um some stuff in neutron to move it to the right uh vlan and then the ironic conductor will pull the image down",
    "start": "622079",
    "end": "628880"
  },
  {
    "text": "put it on the node then and then from there all we need to do is move the vlan again into the the",
    "start": "628880",
    "end": "636640"
  },
  {
    "text": "requested um vlan that the user wanted and then we just restart the server and",
    "start": "636640",
    "end": "641920"
  },
  {
    "text": "then the server will just boot into it into an os and then present a prompt screen that the user can log into",
    "start": "641920",
    "end": "649120"
  },
  {
    "text": "that means yeah then hopefully we've got everyone got their metal servers and they're happy to go and use their um",
    "start": "649120",
    "end": "654959"
  },
  {
    "text": "their fleet of servers now that is all good until the users then is",
    "start": "654959",
    "end": "660160"
  },
  {
    "text": "finished with the server so the idea between ironic is sort of cattle not pets so you use a server for",
    "start": "660160",
    "end": "667120"
  },
  {
    "text": "a lot of time or however long you need it and then you hand it back go through cleaning and then it goes available",
    "start": "667120",
    "end": "673440"
  },
  {
    "text": "ready for someone else to use so it's really really flexible so yeah the user deletes the server",
    "start": "673440",
    "end": "678800"
  },
  {
    "text": "neutron uh goes and moves the server to cleaning we go through those same cleaning steps so if",
    "start": "678800",
    "end": "684480"
  },
  {
    "text": "the firmware has changed since um since users handed back the machine then that will get updated wipes all the disks so",
    "start": "684480",
    "end": "691440"
  },
  {
    "text": "it's all nice and secure when the new user use uh gets given the server um and yeah we checked that it hasn't",
    "start": "691440",
    "end": "697920"
  },
  {
    "text": "been tampered with or anything like that as well just for extra security checks uh and then yeah neutron will finally",
    "start": "697920",
    "end": "703200"
  },
  {
    "text": "move it into holding and then then it becomes available again so just to recap on those states so",
    "start": "703200",
    "end": "710560"
  },
  {
    "text": "first of all you roll the node into ironic then it sits there and it's ready for the user to use um and then we've",
    "start": "710560",
    "end": "717120"
  },
  {
    "text": "got cleaning between holding and provisioning really so yeah enrolling",
    "start": "717120",
    "end": "722399"
  },
  {
    "text": "cleaning holding provisioning that's about it really ot thank you all right and then more on to the sort",
    "start": "722399",
    "end": "728880"
  },
  {
    "text": "of how we use this for kubernetes and research purposes side of things i suppose so we",
    "start": "728880",
    "end": "735200"
  },
  {
    "text": "uh historically we've always run kubernetes in g research on openstack um",
    "start": "735200",
    "end": "740480"
  },
  {
    "text": "for a long time we've been doing it on on vms but more recently we started moving on to building clusters using",
    "start": "740480",
    "end": "746480"
  },
  {
    "text": "bare metal uh but this process is pretty much the same regardless we just use a different flavor as scott was talking",
    "start": "746480",
    "end": "752399"
  },
  {
    "text": "about so the way we tend to do it is we define our clusters in um terraform",
    "start": "752399",
    "end": "758800"
  },
  {
    "text": "terraform code in github we then use terraform enterprise in our case to build the clusters",
    "start": "758800",
    "end": "765440"
  },
  {
    "text": "into openstack using ironic machines are built and configured using the flat car operating system",
    "start": "765440",
    "end": "771600"
  },
  {
    "text": "and then flat car uses ignition to pull down user data and configure a very minimal",
    "start": "771600",
    "end": "778079"
  },
  {
    "text": "kubernetes installation so our initial bootstrap task basically gets us a small bare metal server uh sorry collection of",
    "start": "778079",
    "end": "785120"
  },
  {
    "text": "servers running a kubernetes cluster uh pretty vanilla um currently we still do",
    "start": "785120",
    "end": "790720"
  },
  {
    "text": "um the lcd nodes as virtual machines but for our larger and higher performance",
    "start": "790720",
    "end": "796160"
  },
  {
    "text": "clusters we now use ironic and bare metal for the master nodes and all the worker nodes as well",
    "start": "796160",
    "end": "802880"
  },
  {
    "text": "once we have a minimal cluster then we apply our more detailed kubernetes configuration on top typically we do",
    "start": "802880",
    "end": "809200"
  },
  {
    "text": "that these days using jenkins or arc cd or combination of the two actually sort",
    "start": "809200",
    "end": "814800"
  },
  {
    "text": "of undergoing a bit of migration at the moment and that's just how we then deploy all of our sort of desired state",
    "start": "814800",
    "end": "820720"
  },
  {
    "text": "kubernetes configuration on top so things along the lines of ingress controllers and calico and all",
    "start": "820720",
    "end": "826240"
  },
  {
    "text": "the other bits and pieces and things that we want to have in our clusters to make them look and feel like our desired gr clusters",
    "start": "826240",
    "end": "833199"
  },
  {
    "text": "uh once we've done that we then deploy armada so this is an application which i've talked about a bunch of times at",
    "start": "833199",
    "end": "839920"
  },
  {
    "text": "this uh in this forum so i won't go into too much detail now but this is just the overall architecture diagram of the",
    "start": "839920",
    "end": "846639"
  },
  {
    "text": "the application which we typically deploy on top of these clusters so you can see here the the blue boxes at the",
    "start": "846639",
    "end": "852480"
  },
  {
    "text": "bottom of the screen are kubernetes clusters in this sort of new world of metal these are all but high",
    "start": "852480",
    "end": "858480"
  },
  {
    "text": "performance bare metal clusters uh quite a large number of nodes we tend to scale up to about a thousand uh and then one",
    "start": "858480",
    "end": "864240"
  },
  {
    "text": "our model server sitting on top which allows our users users to submit jobs to uh run on the hardware",
    "start": "864240",
    "end": "870480"
  },
  {
    "text": "um a couple of notes just on some benefits we've seen so far so that this is really the",
    "start": "870480",
    "end": "876000"
  },
  {
    "start": "871000",
    "end": "871000"
  },
  {
    "text": "reasons for us moving to this model in the first place so it's still early days but the things sorts of things we've seen are",
    "start": "876000",
    "end": "881760"
  },
  {
    "text": "increased stability so certainly for things like gpu intensive workloads we had seen some issues when",
    "start": "881760",
    "end": "887199"
  },
  {
    "text": "we were running on virtualization that have just completely evaporated since movement spare metal uh it's certainly",
    "start": "887199",
    "end": "892240"
  },
  {
    "text": "been a lot simpler than trying to debug sort of uh kernel level issues within the within the virtualization layer just to move to",
    "start": "892240",
    "end": "898880"
  },
  {
    "text": "their metal and not really worry about it um some other benefits we've seen are things like increased network throughput",
    "start": "898880",
    "end": "904480"
  },
  {
    "text": "between nodes and external resources uh being able to use bgp peering very easily uh can be done with vms but it's",
    "start": "904480",
    "end": "910560"
  },
  {
    "text": "a little bit more complex um for us as well typically we end up with much larger nodes because your bare metal",
    "start": "910560",
    "end": "917120"
  },
  {
    "text": "servers tend to be a bit bigger than your average virtual machine by definition i suppose um and for us it's just simpler estate",
    "start": "917120",
    "end": "923040"
  },
  {
    "text": "management as well so we have fewer layers between our our workloads and our hardware um fewer machines fewer bigger",
    "start": "923040",
    "end": "930079"
  },
  {
    "text": "machines tend to be slightly easier to manage than tens of thousands of smaller virtual machines",
    "start": "930079",
    "end": "936480"
  },
  {
    "start": "936000",
    "end": "936000"
  },
  {
    "text": "uh however there are limitations as well so some of the things we've noticed so far certainly um a slower provisioning",
    "start": "936480",
    "end": "941759"
  },
  {
    "text": "time which is actually completely expected as you can imagine when you're provisioning a bare metal server you're",
    "start": "941759",
    "end": "946959"
  },
  {
    "text": "actually basically turning on a real machine and you have to wait for it to power on um with a virtual machine all",
    "start": "946959",
    "end": "952240"
  },
  {
    "text": "of that sort of abstracted away from you you don't really see it um there's a lot more precise quota",
    "start": "952240",
    "end": "957279"
  },
  {
    "text": "management required i think you can be a little bit more fast and loose when you're running a large virtual estate you can oversubscribe things and uh you",
    "start": "957279",
    "end": "964320"
  },
  {
    "text": "know over subscribe cpus and things like that it's much harder to do in a bare metal environment you're very much constrained by the physical resource you",
    "start": "964320",
    "end": "970399"
  },
  {
    "text": "actually have um it is a little bit less flexible in some ways and there's some features of virtualization which we don't get as a",
    "start": "970399",
    "end": "977279"
  },
  {
    "text": "side effect so things like being at a snapshot of vm are quite useful you can't do that",
    "start": "977279",
    "end": "982399"
  },
  {
    "text": "natively using a bare metal server so you have to sort of roll something or use some other",
    "start": "982399",
    "end": "987519"
  },
  {
    "text": "tool to do that and we have also noticed in a kubernetes world it can be a little bit tricky sometimes to mix and match in",
    "start": "987519",
    "end": "992720"
  },
  {
    "text": "cluster having virtual machines and physical machines so we've tended to take the approach of just starting from",
    "start": "992720",
    "end": "998160"
  },
  {
    "text": "scratch and building new clusters as bare metal from the beginning rather than trying to add it into existing",
    "start": "998160",
    "end": "1003920"
  },
  {
    "text": "virtual clusters but yeah in summary um",
    "start": "1003920",
    "end": "1008959"
  },
  {
    "start": "1006000",
    "end": "1006000"
  },
  {
    "text": "for us we're now using bare metal kubernetes for our highest performance workloads um we're still also making",
    "start": "1008959",
    "end": "1014880"
  },
  {
    "text": "heavy use of virtualization where appropriate so from all sorts of classic kubernetes clusters if you like for",
    "start": "1014880",
    "end": "1020240"
  },
  {
    "text": "services and so forth we're still making good use of vms but for the clusters where we really",
    "start": "1020240",
    "end": "1025678"
  },
  {
    "text": "care about performance and we're running lots and lots of high throughput jobs then we are now moving to my metal and openstack ironic is our metal as a",
    "start": "1025679",
    "end": "1032880"
  },
  {
    "text": "service choice i think is it little whistles doctor but are",
    "start": "1032880",
    "end": "1038880"
  },
  {
    "start": "1036000",
    "end": "1036000"
  },
  {
    "text": "there any questions awesome thank you jamie and scott it was uh",
    "start": "1038880",
    "end": "1046000"
  },
  {
    "text": "nice nice summary um anyone has any questions feel free to",
    "start": "1046000",
    "end": "1052880"
  },
  {
    "text": "just go for it and ask i will have a couple but uh i'll leave",
    "start": "1052880",
    "end": "1057919"
  },
  {
    "text": "the floor for two others first i'm here i have a question um did you",
    "start": "1057919",
    "end": "1064720"
  },
  {
    "text": "look at any other tool suites uh besides um um ironic or was were you set on ironic",
    "start": "1064720",
    "end": "1072240"
  },
  {
    "text": "because uh i believe it's an open stack project right it is yeah okay we have looked at some other things um",
    "start": "1072240",
    "end": "1079200"
  },
  {
    "text": "we're relatively opinionated about it i suppose because we're already got quite a foothold in ironic",
    "start": "1079200",
    "end": "1084320"
  },
  {
    "text": "uh sorry in openstack using lots of other openstack um uh services as scott mentioned",
    "start": "1084320",
    "end": "1090799"
  },
  {
    "text": "uh we have actually on i think independently ahead of ironic rolled our own metal as a service",
    "start": "1090799",
    "end": "1098880"
  },
  {
    "text": "system internally which does work as well but it's kind of nice to be able to use the",
    "start": "1098880",
    "end": "1103919"
  },
  {
    "text": "off-the-shelf open source tooling that fits in nicely with the rest of our ecosystem okay but i know there's other",
    "start": "1103919",
    "end": "1110000"
  },
  {
    "text": "things as well like mass and others that we we haven't evaluated at",
    "start": "1110000",
    "end": "1115360"
  },
  {
    "text": "in depth but yeah ironic seems to work well for us",
    "start": "1115360",
    "end": "1119840"
  },
  {
    "text": "thank you all right",
    "start": "1121520",
    "end": "1126960"
  },
  {
    "text": "okay so alex did you have a question as well so you want me to add some point",
    "start": "1126960",
    "end": "1132880"
  },
  {
    "text": "i was wondering whether they had um looked at the",
    "start": "1132880",
    "end": "1138000"
  },
  {
    "text": "you know you mentioned the provisioning times the slow um whether there was any",
    "start": "1138000",
    "end": "1143679"
  },
  {
    "text": "uh looking at pre-provisioning sort of expected images that you're gonna spin up um i know that when we had",
    "start": "1143679",
    "end": "1152080"
  },
  {
    "text": "the uh on metal service at rackspace before we switched over to ironic that was part",
    "start": "1152080",
    "end": "1157120"
  },
  {
    "text": "of the whole plot was to pre-spin up these uh bare metal servers",
    "start": "1157120",
    "end": "1163600"
  },
  {
    "text": "it was supposed to come back in ironic but and that was years ago but i don't know whether that's actually come back so",
    "start": "1163600",
    "end": "1170240"
  },
  {
    "text": "it's not something we've used yet i mean certainly some things we've looked at in our processes where we can save time",
    "start": "1170480",
    "end": "1177679"
  },
  {
    "text": "with earth maybe you can cover this but some of the things like bios settings and things where we want to make sure we eliminate the",
    "start": "1177679",
    "end": "1182880"
  },
  {
    "text": "requirement for reboots and things like that i suppose during the process yeah so um yeah there's a little bit of that",
    "start": "1182880",
    "end": "1188480"
  },
  {
    "text": "we can trim around but um yeah so",
    "start": "1188480",
    "end": "1193600"
  },
  {
    "text": "the way it's designed is you have one big pool of data sorry of nodes and you can sort of have multiple tenants using",
    "start": "1193600",
    "end": "1199600"
  },
  {
    "text": "that where we don't have that um there's some stuff that we can sort of pull out like for example the buyer",
    "start": "1199600",
    "end": "1205760"
  },
  {
    "text": "settings um they're they're applied at like a provision time if they're static then you just don't have to reapply them",
    "start": "1205760",
    "end": "1212159"
  },
  {
    "text": "every time you just have to make sure in cleaning and i was tampered with them and then you can save a bit of time there also",
    "start": "1212159",
    "end": "1218240"
  },
  {
    "text": "there's lots of things you can do about caching images and that kind of thing and actually for us that's something",
    "start": "1218240",
    "end": "1223520"
  },
  {
    "text": "that has been relatively easy because it's um because these these kubernetes uh",
    "start": "1223520",
    "end": "1229440"
  },
  {
    "text": "clusters tend to use the same image and then we have lots of them that use the same image so everything gets cached and",
    "start": "1229440",
    "end": "1235600"
  },
  {
    "text": "it's all kind of hot at all times pretty much so um there's more things if that becomes slow in the future we can move",
    "start": "1235600",
    "end": "1242400"
  },
  {
    "text": "glance closer to the actual bare metal nodes but at the moment we're finding that um the cache is pretty warm and um",
    "start": "1242400",
    "end": "1249919"
  },
  {
    "text": "yeah well yeah it's performing pretty well one thing which i noticed which surprised me actually in my own reaction",
    "start": "1249919",
    "end": "1257280"
  },
  {
    "text": "to it i mean is uh the first time i saw it it took 20 minutes to to build a server i was like oh this is a",
    "start": "1257280",
    "end": "1263360"
  },
  {
    "text": "nightmare this is going to make everything really slow and difficult um because i'm used to a vm spinning up in 30 seconds or something but actually",
    "start": "1263360",
    "end": "1270720"
  },
  {
    "text": "when you're used to it and you're doing things at large scale and in bulk it doesn't really matter if one server takes 20 minutes if you can build",
    "start": "1270720",
    "end": "1278559"
  },
  {
    "text": "hundreds or thousands simultaneously um you actually end up caring a lot more about reliability and being comfortable",
    "start": "1278559",
    "end": "1284799"
  },
  {
    "text": "that your automation will just work and you can walk away from it and come back later and everything will be up and running it will be much worse if it was",
    "start": "1284799",
    "end": "1291520"
  },
  {
    "text": "faster but less reliable so i always sort of er on the side of reliability over performance personally of the build that",
    "start": "1291520",
    "end": "1298880"
  },
  {
    "text": "is once it's up and running we want performance as well obviously",
    "start": "1298880",
    "end": "1304120"
  },
  {
    "text": "i mean for us if we have long-running jobs that take days 20 minutes is neither here nor there",
    "start": "1304240",
    "end": "1312240"
  },
  {
    "text": "yeah i mean 20 minutes is slightly uh anecdotal i would say that's that's our current experience for",
    "start": "1314799",
    "end": "1320880"
  },
  {
    "text": "a certain type of flavor but it's uh it's of the order of minutes no longer seconds but in that way",
    "start": "1320880",
    "end": "1328399"
  },
  {
    "text": "so is is that the mode of operation that everybody want so if uh somebody submits",
    "start": "1330880",
    "end": "1338480"
  },
  {
    "text": "to to run a particular workload they get provisioned a particular resource or",
    "start": "1338480",
    "end": "1344000"
  },
  {
    "text": "resource type it's not that some things are long lived and people kind of swap",
    "start": "1344000",
    "end": "1349039"
  },
  {
    "text": "or you know interchangeably use the same standing resource it depends how you choose to use it in",
    "start": "1349039",
    "end": "1355520"
  },
  {
    "text": "our model what we do is we have a bunch of hardware and built into clusters ahead of time which sit there",
    "start": "1355520",
    "end": "1362000"
  },
  {
    "text": "and are used relatively constantly by a collection of different users so it affects the hardware as well being a",
    "start": "1362000",
    "end": "1367600"
  },
  {
    "text": "large pool of hardware is all being shared by lots of different people um we're quite lucky in the sense that we've got relatively in the grand scheme",
    "start": "1367600",
    "end": "1374159"
  },
  {
    "text": "of things a relatively small pool of researchers all doing quite a similar thing so we can be quite",
    "start": "1374159",
    "end": "1380159"
  },
  {
    "text": "uh prescriptive about the hardware that they'll get so we have a smallish number of flavors of cpu nodes and similar gpu",
    "start": "1380159",
    "end": "1387600"
  },
  {
    "text": "nodes and potentially in future other accelerators um it might be the case that in other",
    "start": "1387600",
    "end": "1394000"
  },
  {
    "text": "companies who do or other organizations even who want to do a more like",
    "start": "1394000",
    "end": "1399840"
  },
  {
    "text": "i guess offer metal as a service or cluster as a service up to users to actually create their own that that would be a possibility but for us we we",
    "start": "1399840",
    "end": "1406640"
  },
  {
    "text": "take the more sort of we provision it we being the infrastructure and platform teams and",
    "start": "1406640",
    "end": "1412799"
  },
  {
    "text": "then our our users within our organization then just use what we've provisioned for them",
    "start": "1412799",
    "end": "1418480"
  },
  {
    "text": "but this would lend people provisioning their own if they wanted to how does it work that they they submit a",
    "start": "1418480",
    "end": "1424159"
  },
  {
    "text": "ticket and you you take care of that uh it depends what you mean so",
    "start": "1424159",
    "end": "1431200"
  },
  {
    "text": "generally speaking the way we we have is we have a these pools of compute which we understand uh the",
    "start": "1431200",
    "end": "1437279"
  },
  {
    "text": "sort of flavors and qualities of and then we have a bunch of tools and software which allow users then run jobs on them so they don't it's not a",
    "start": "1437279",
    "end": "1443520"
  },
  {
    "text": "ticketing system it's really a case of they can just they are already set up with access to",
    "start": "1443520",
    "end": "1449120"
  },
  {
    "text": "this large pool of compute and then they can submit jobs to then use the hardware as they see fit so effectively run",
    "start": "1449120",
    "end": "1455120"
  },
  {
    "text": "run jobs as pods in kubernetes ultimately is what happens on top of the hardware that happens to be provisioned through ironic",
    "start": "1455120",
    "end": "1461440"
  },
  {
    "text": "okay so to get a sense of their time scale",
    "start": "1461440",
    "end": "1466880"
  },
  {
    "text": "how long do the clusters live and or if it's like a dynamic cluster how long does the uh the nodes typically",
    "start": "1466880",
    "end": "1473279"
  },
  {
    "text": "live like you know yeah six months on a cluster and every few weeks things shrink and grow or which was yeah so it",
    "start": "1473279",
    "end": "1480400"
  },
  {
    "start": "1478000",
    "end": "1478000"
  },
  {
    "text": "looks loosely like this so we actually have multiple of this whole picture in fact but if we just look at one of these as an example imagine this is a data",
    "start": "1480400",
    "end": "1486799"
  },
  {
    "text": "center we have many of these clusters under here each one of these clusters itself but the cluster i suppose in it",
    "start": "1486799",
    "end": "1494480"
  },
  {
    "text": "in in of itself may even last for years we might create it you know a couple of",
    "start": "1494480",
    "end": "1500000"
  },
  {
    "text": "years ago saying it's still running now and we'll still be running jobs on it the nodes themselves we tend to quite frequently rebuild um because",
    "start": "1500000",
    "end": "1507200"
  },
  {
    "text": "i think we actually have a bit of a fetish for sort of rebuilding stuff in gr making sure everything comes back clean and tidy",
    "start": "1507200",
    "end": "1513120"
  },
  {
    "text": "so we actually have a separate project at the moment going on to ensure we're constantly rebuilding things and making sure there's a maximum",
    "start": "1513120",
    "end": "1519360"
  },
  {
    "text": "lifetime of the actual nodes but clusters themselves can last for quite a long time um we probably also eventually i think",
    "start": "1519360",
    "end": "1525200"
  },
  {
    "text": "we'll move into a more rolling cluster rebuild process as well because obviously that's long-lived state itself",
    "start": "1525200",
    "end": "1530640"
  },
  {
    "text": "which could get dirty or out of sync somehow it shouldn't but as possible but no generally speaking the clusters",
    "start": "1530640",
    "end": "1537120"
  },
  {
    "text": "themselves live for quite a long time and then the nodes within them are of the order of tens of days",
    "start": "1537120",
    "end": "1542799"
  },
  {
    "text": "maximum couple of months so this architecture is really for um at that facilitator level where you're you're",
    "start": "1542799",
    "end": "1549200"
  },
  {
    "text": "building environments for individuals and you're you're keeping it you're moving with whatever the ongoing",
    "start": "1549200",
    "end": "1555520"
  },
  {
    "text": "research is and the reason yeah short time scales on the pods and whatever yes yeah exactly so we've got",
    "start": "1555520",
    "end": "1562320"
  },
  {
    "text": "like time scales then time scales the pods themselves are anything from uh seconds up to a couple of weeks say",
    "start": "1562320",
    "end": "1568799"
  },
  {
    "text": "and then the nodes and the clusters last for a lot longer and they were they're just sort of running this primal deal soup of of user",
    "start": "1568799",
    "end": "1575919"
  },
  {
    "text": "workload um but also just using ironic or any kind",
    "start": "1575919",
    "end": "1581039"
  },
  {
    "text": "of metal as a service is also just a useful thing if people have sort of high performance requirements or",
    "start": "1581039",
    "end": "1587120"
  },
  {
    "text": "just have a different estate management process i suppose so i know ricardo and the guys at cern don't do",
    "start": "1587120",
    "end": "1592400"
  },
  {
    "text": "this model so we have a model where we create clusters and then effectively offer you can think of it as like",
    "start": "1592400",
    "end": "1597679"
  },
  {
    "text": "namespace as a service so that tenancy is the thing which we offer people on the existing clusters whereas i think or certainly when last",
    "start": "1597679",
    "end": "1604640"
  },
  {
    "text": "time we were talking about it over in cern they're doing more sort of cluster as a service so people can ask for their own clusters which",
    "start": "1604640",
    "end": "1610640"
  },
  {
    "text": "then may use something like ironic in fact do you do that ricardo do you have ironic as an option yeah under classes",
    "start": "1610640",
    "end": "1616240"
  },
  {
    "text": "as well yes you do exactly that yeah you can even have like mixed clusters with",
    "start": "1616240",
    "end": "1621520"
  },
  {
    "text": "node groups or node pools in vms and additional node pools using",
    "start": "1621520",
    "end": "1627840"
  },
  {
    "text": "bare metal makes sense",
    "start": "1627840",
    "end": "1631600"
  },
  {
    "text": "i had a question because you mentioned it's a kind of follow up for the last one which is uh",
    "start": "1633440",
    "end": "1640080"
  },
  {
    "text": "you you describe the workflow with github and then the provisioning using terraform um do you also use this for like cluster",
    "start": "1640080",
    "end": "1647039"
  },
  {
    "text": "upgrades or or is this like you just redeploy from scratch and you cluster",
    "start": "1647039",
    "end": "1653039"
  },
  {
    "text": "that's a good question so we tend to use uh the cluster bootstrap thing is kind of a one-time thing to build a cluster",
    "start": "1653039",
    "end": "1659279"
  },
  {
    "text": "uh if we have quite a long loop cluster then we can actually do all of our upgrades then from this point onwards if",
    "start": "1659279",
    "end": "1664880"
  },
  {
    "text": "this makes sense so things like upgrading kubernetes itself um we have a bunch of tooling to do that so we can do",
    "start": "1664880",
    "end": "1671440"
  },
  {
    "text": "it in place cluster upgrades um even the kubelet on all the nodes as well because that itself is",
    "start": "1671440",
    "end": "1676720"
  },
  {
    "text": "containerized um and similarly then uh operating system upgrades we can do in a rolling fashion",
    "start": "1676720",
    "end": "1682559"
  },
  {
    "text": "because we have this model where here underneath the long lift cluster the nodes get rebuilt sort of",
    "start": "1682559",
    "end": "1688720"
  },
  {
    "text": "sequentially underneath the cluster uh with error budgets and so forth so that we don't do the whole thing at once",
    "start": "1688720",
    "end": "1694960"
  },
  {
    "text": "um but we have options we can also if we want just completely blow you know coordinate wait for stuff to drain and",
    "start": "1694960",
    "end": "1700000"
  },
  {
    "text": "then blow it away and rebuild it all if we want to do upgrades but yeah we tend to just",
    "start": "1700000",
    "end": "1706080"
  },
  {
    "text": "i guess the separation we have is terraform tends to be used for the note the cluster slash node build",
    "start": "1706080",
    "end": "1711120"
  },
  {
    "text": "process and everything afterwards is through all right jenkins and okay",
    "start": "1711120",
    "end": "1717200"
  },
  {
    "text": "okay and uh the other the other question i had was uh like uh there's quite a lot of activity",
    "start": "1717200",
    "end": "1724799"
  },
  {
    "text": "in this try to to kind of uh manage the clusters from",
    "start": "1724799",
    "end": "1729919"
  },
  {
    "text": "as if they were kubernetes resources and then just build on things like argo to kind of make everything uniform",
    "start": "1729919",
    "end": "1736720"
  },
  {
    "text": "yes is this something that you've looked into and is because i was just searching now for",
    "start": "1736720",
    "end": "1742880"
  },
  {
    "text": "the integration of like metal as service components into a cluster api is this something that would simplify or that",
    "start": "1742880",
    "end": "1749039"
  },
  {
    "text": "you would not consider i would definitely consider it and i'm very excited about it and i would like to do it at some point but it's just",
    "start": "1749039",
    "end": "1754960"
  },
  {
    "text": "never quite been up the priority list enough for us in our world i think what it would end up doing is effectively replacing terraform yeah i think",
    "start": "1754960",
    "end": "1761600"
  },
  {
    "text": "basically we would be going straight from github well i suppose we'd have something to bootstrap our initial cluster somehow and then cluster api",
    "start": "1761600",
    "end": "1767600"
  },
  {
    "text": "would then go off and talk straight to openstack but hopefully everything we've already done would then",
    "start": "1767600",
    "end": "1772720"
  },
  {
    "text": "continue to integrate nicely and we would just use that directly so i think it's really a question of how well supported",
    "start": "1772720",
    "end": "1778559"
  },
  {
    "text": "openstack is by the cluster api right i haven't checked recently but yeah it would be very interesting to do",
    "start": "1778559",
    "end": "1784799"
  },
  {
    "text": "that cool certainly a limitation we found not",
    "start": "1784799",
    "end": "1790080"
  },
  {
    "text": "bare metal specifically but as soon as you get to large scale kubernetes or any configurations in fact within terraform",
    "start": "1790080",
    "end": "1796080"
  },
  {
    "text": "it is a bit slow it has effectively maintains a big graph of resources which it has to have",
    "start": "1796080",
    "end": "1803360"
  },
  {
    "start": "1800000",
    "end": "1800000"
  },
  {
    "text": "to walk walk the graph every time you make any kind of change and especially when every resource",
    "start": "1803360",
    "end": "1808480"
  },
  {
    "text": "is actually a remote thing that has to go off and be checked then you can imagine that ends up translating into a lot of api calls which can be quite slow",
    "start": "1808480",
    "end": "1814720"
  },
  {
    "text": "and expensive so if we could turn that into something a bit more elegant using kubernetes itself i'm all for it",
    "start": "1814720",
    "end": "1823120"
  },
  {
    "text": "makes sense checking here if there's other questions",
    "start": "1823120",
    "end": "1829200"
  },
  {
    "text": "in the chat of someone ah there you go um when you uh i have another question um so",
    "start": "1829200",
    "end": "1834960"
  },
  {
    "text": "uh do you does your team manage the networking equipment um and do you have",
    "start": "1834960",
    "end": "1840000"
  },
  {
    "text": "like kind of broad control over that or do you work with the networking team uh we have a networking team who's more",
    "start": "1840000",
    "end": "1846000"
  },
  {
    "text": "responsible for that so within our organization we have a few different functions and different areas",
    "start": "1846000",
    "end": "1852399"
  },
  {
    "text": "responsible for different things so we've got an infrastructure function and a platform function me and scott from",
    "start": "1852399",
    "end": "1858080"
  },
  {
    "text": "both of those respectively there is a team within the infrastructure function who deals with networking specifically",
    "start": "1858080",
    "end": "1864399"
  },
  {
    "text": "um but what we're definitely finding is having more cross-functional teams is really powerful so i've got people in my team who have got really strong",
    "start": "1864399",
    "end": "1870080"
  },
  {
    "text": "networking skills and understand that kind of stuff including down to the hardware um and i actually suspect over time",
    "start": "1870080",
    "end": "1876399"
  },
  {
    "text": "we're probably going to need to develop some kind of special cross-functional team that just looks at performance and tuning of",
    "start": "1876399",
    "end": "1883840"
  },
  {
    "text": "the estate basically because we need to be able to do it all the way from top to bottom really especially when we're now dealing",
    "start": "1883840",
    "end": "1889039"
  },
  {
    "text": "with metal you know we actually need to understand how everything is configured all the way down to the bios yeah um we you know we use a lot of but",
    "start": "1889039",
    "end": "1896720"
  },
  {
    "text": "we have a lot of bare metal and uh vms um and we just there's this friction with the same networking team",
    "start": "1896720",
    "end": "1902559"
  },
  {
    "text": "professional disagreements maybe over how the switches should be uh managed uh and i saw like in during your",
    "start": "1902559",
    "end": "1908720"
  },
  {
    "text": "presentation you were switching vlans and uh at the beginning and it seemed like you had a decent amount of control",
    "start": "1908720",
    "end": "1915519"
  },
  {
    "text": "yes yeah we i think i think we do um our networking team is quite sort of up to",
    "start": "1915519",
    "end": "1921279"
  },
  {
    "text": "speed with everything that we're doing as well and uh i mean like you say though there was always friction sometimes between teams",
    "start": "1921279",
    "end": "1926880"
  },
  {
    "text": "because different teams sometimes operate at different rates and when we've got responsibility shared across groups it can be tricky but",
    "start": "1926880",
    "end": "1932880"
  },
  {
    "text": "um we've got quite a sort of singular purpose at the moment so there's a particular large project happening at the moment which involves a",
    "start": "1932880",
    "end": "1938960"
  },
  {
    "text": "lot of this stuff so we've got a lot of people from different teams all working together to make it happen so it's quite nice",
    "start": "1938960",
    "end": "1944960"
  },
  {
    "text": "and these clusters are fairly large there are dozens or hundreds of servers is that what you were saying yeah yeah i",
    "start": "1944960",
    "end": "1950799"
  },
  {
    "text": "mean up to about a thousand nodes in the given cluster um we could go we've actually decided",
    "start": "1950799",
    "end": "1956720"
  },
  {
    "text": "arbitrarily to sort of stop about there but that was in fact one of the reasons for the armada architecture so that we",
    "start": "1956720",
    "end": "1962559"
  },
  {
    "text": "could have many of these things um because we're aware that past a certain limit kubernetes can't really",
    "start": "1962559",
    "end": "1967760"
  },
  {
    "text": "scale much further i think the official limit is still 5000 nodes but i know from i think anyone knows from",
    "start": "1967760",
    "end": "1973120"
  },
  {
    "text": "going to conferences and things that you have to do quite a lot to get that far and then over backwards so we have a",
    "start": "1973120",
    "end": "1978799"
  },
  {
    "text": "model where we just go to about a thousand and then just plug in more clusters horizontally and scales quite well that way",
    "start": "1978799",
    "end": "1985600"
  },
  {
    "text": "and is armada um a g um a g research project or is that um yes",
    "start": "1985600",
    "end": "1992159"
  },
  {
    "text": "okay so it's an open source but yes it's come from g research i'll tell you there's a probably further back in the uh",
    "start": "1992159",
    "end": "1999440"
  },
  {
    "text": "list of um meetings there'll be a recording of some stuff we've done specifically on this if you're interested okay",
    "start": "1999440",
    "end": "2007039"
  },
  {
    "start": "2009000",
    "end": "2009000"
  },
  {
    "text": "i have one more um you may you mentioned the issues with gpus and stability and",
    "start": "2010399",
    "end": "2018320"
  },
  {
    "text": "improvements by moving to bare metal yeah were you doing a pci pass-through i guess and in vms",
    "start": "2018320",
    "end": "2026480"
  },
  {
    "text": "and do you remember which which specific issues you had and how did they think we were",
    "start": "2026480",
    "end": "2033679"
  },
  {
    "text": "um yes yes um i can't remember the specific issues but",
    "start": "2033679",
    "end": "2039360"
  },
  {
    "text": "we were basically getting unexpected errors things were being reported as um you know not a number and that kind of thing just mathematical errors which",
    "start": "2039360",
    "end": "2046240"
  },
  {
    "text": "shouldn't have been happening um under quite nice circumstances as well when we were running out of memory and you know you'd have to have a few",
    "start": "2046240",
    "end": "2052480"
  },
  {
    "text": "different failures happening a certain way but somehow we managed to always hit this scenario quite frequently and then we",
    "start": "2052480",
    "end": "2059440"
  },
  {
    "text": "just thought well hey look rather than try and debug all this let's just see what happens if we run on bare metal and lo",
    "start": "2059440",
    "end": "2065040"
  },
  {
    "text": "and behold the problem went away so sometimes it's just not worth sort of yeah",
    "start": "2065040",
    "end": "2070800"
  },
  {
    "text": "i ask is because we we have been seeing simulations with virtual machines recently right",
    "start": "2070800",
    "end": "2077118"
  },
  {
    "text": "and uh yeah that that is a tempting solution",
    "start": "2077119",
    "end": "2082638"
  },
  {
    "text": "i guess i mean it makes sense doesn't it if you think you're just going through this whole extra layer that maybe you don't really need to and",
    "start": "2082639",
    "end": "2088720"
  },
  {
    "text": "there's a lot more software involved isn't there ultimately yeah the the issue is how many gpus do you have per",
    "start": "2088720",
    "end": "2094720"
  },
  {
    "text": "node on average for us uh up to about eight all right",
    "start": "2094720",
    "end": "2100480"
  },
  {
    "text": "it's the issue is that for kubernetes clusters this is easy to handle but for",
    "start": "2100480",
    "end": "2106000"
  },
  {
    "text": "if you have a mix of vms and kubernetes clusters using those gpus actually",
    "start": "2106000",
    "end": "2111119"
  },
  {
    "text": "virtualization allows you to like expose um is on a multi-gpu node quite easily",
    "start": "2111119",
    "end": "2118160"
  },
  {
    "text": "yeah um well if you just dedicate like parameter nodes to to to people directly",
    "start": "2118160",
    "end": "2124960"
  },
  {
    "text": "as you would do with vms then you basically like potentially",
    "start": "2124960",
    "end": "2130240"
  },
  {
    "text": "giving them a a really nice way to waste uh pressure reserve resources yeah",
    "start": "2130240",
    "end": "2135520"
  },
  {
    "text": "that's true yeah i think i think that's that's the reason why like for coordinates cost is kind of",
    "start": "2135520",
    "end": "2141359"
  },
  {
    "text": "a no-brainer that you can go bare metal for gpus and just schedule",
    "start": "2141359",
    "end": "2146640"
  },
  {
    "text": "directly",
    "start": "2146640",
    "end": "2149119"
  },
  {
    "text": "all right that's pretty pretty um cool have i have one one question",
    "start": "2153119",
    "end": "2160480"
  },
  {
    "text": "i don't see anyone raising like if i i guess the question that a lot of people will have is if i have a bunch of nodes",
    "start": "2160480",
    "end": "2166560"
  },
  {
    "text": "arriving on a new data center or whatever on premises",
    "start": "2166560",
    "end": "2172000"
  },
  {
    "text": "what would be the suggestion if i just want to do kubernetes um",
    "start": "2172000",
    "end": "2178400"
  },
  {
    "text": "on bare metal like what's the the best option or",
    "start": "2178400",
    "end": "2184400"
  },
  {
    "text": "and the least uh complicated option to get stuff up and running you know sort of",
    "start": "2184400",
    "end": "2191280"
  },
  {
    "text": "yeah um quick fashion i mean i i don't know because i know",
    "start": "2191280",
    "end": "2197040"
  },
  {
    "text": "what we do and we've got obviously quite opinionated about using openstack and ironic i'm sure i i think one thing that",
    "start": "2197040",
    "end": "2203599"
  },
  {
    "text": "probably can be said of ironic at openstack is it can be quite complex and it's probably quite difficult to get up and running from uh also there's two",
    "start": "2203599",
    "end": "2210720"
  },
  {
    "text": "projects that you probably if you want to have a guide to look at um which sort of lower that barrier to entry so one is",
    "start": "2210720",
    "end": "2217200"
  },
  {
    "text": "called bifrost which will allow you to just um sort of run um like ironic from like just laptop",
    "start": "2217200",
    "end": "2224640"
  },
  {
    "text": "that's good for like bootstrapping new environments where you don't already have a control plane um and then the other is what we actually use here to",
    "start": "2224640",
    "end": "2231200"
  },
  {
    "text": "deploy all of our openstack which is color ansible um that is it's basically a collection",
    "start": "2231200",
    "end": "2236480"
  },
  {
    "text": "of uh ansible uh roles um and basically a lot of the hard work's been done for you um so a lot of",
    "start": "2236480",
    "end": "2242960"
  },
  {
    "text": "it just kind of works out the box and you can deploy it vanilla open stack really easily to like a couple of vms on",
    "start": "2242960",
    "end": "2249119"
  },
  {
    "text": "your machine or if you've got a couple of um bare metal nodes you can deploy a control plane there with relatively",
    "start": "2249119",
    "end": "2255680"
  },
  {
    "text": "little openstack experience um tuning it and getting it um to large",
    "start": "2255680",
    "end": "2260880"
  },
  {
    "text": "scales that takes a lot of time and experience and working through issues and that kind of",
    "start": "2260880",
    "end": "2266160"
  },
  {
    "text": "thing but um yeah if you want to get started the barrier to entry is not really that high on",
    "start": "2266160",
    "end": "2271760"
  },
  {
    "text": "yeah either by frost or color answer really i'll be really interested if we could do",
    "start": "2271760",
    "end": "2277280"
  },
  {
    "text": "some kind of questionnaire for a wider group obviously our group as wide as we can get it to find out when",
    "start": "2277280",
    "end": "2283119"
  },
  {
    "text": "people are using metal as a service product what they're using because even knowing what's out there is a",
    "start": "2283119",
    "end": "2289280"
  },
  {
    "text": "challenge sometimes yeah i think that comes back to this idea that we've had for a while which is",
    "start": "2289280",
    "end": "2294880"
  },
  {
    "text": "to do these recipes for different sorts of uh workloads that are kind of specific for",
    "start": "2294880",
    "end": "2300160"
  },
  {
    "text": "research environments i think like the deployment on premises and norman bare metal",
    "start": "2300160",
    "end": "2306400"
  },
  {
    "text": "is is something that is not like super common maybe because like",
    "start": "2306400",
    "end": "2311920"
  },
  {
    "text": "most users will be using public cloud providers or some sort of commercial virtualization solution that",
    "start": "2311920",
    "end": "2318079"
  },
  {
    "text": "is already available so i don't know for research institutions that want to have a bunch",
    "start": "2318079",
    "end": "2324640"
  },
  {
    "text": "of notes and want to to get something up and running maybe there's there's something we can provide",
    "start": "2324640",
    "end": "2330960"
  },
  {
    "text": "uh with some ideas or pointers i guess we don't even have to have a",
    "start": "2330960",
    "end": "2336560"
  },
  {
    "text": "recipe for it and say do this we but we can say we as a collective have done these things yeah we know that they work",
    "start": "2336560",
    "end": "2343440"
  },
  {
    "text": "well it can can be made to work uh but yeah that's good she didn't mention that in this presentation all of",
    "start": "2343440",
    "end": "2348800"
  },
  {
    "text": "our computers is on-prem yeah i suppose anyone using a",
    "start": "2348800",
    "end": "2354400"
  },
  {
    "text": "cloud provider can also just use bare metal through through whatever they support as well i think they will all do now",
    "start": "2354400",
    "end": "2360000"
  },
  {
    "text": "i think that'd be very valuable for the university community from what i can tell most of the uh the bare metal clusters are hand created",
    "start": "2360000",
    "end": "2368400"
  },
  {
    "text": "from various various methods so uh what works and what works well would be definitely useful yeah i think as",
    "start": "2368400",
    "end": "2375520"
  },
  {
    "text": "soon as you do it any kind of scale then the hand crank method just sort of doesn't work spend your whole time doing it i i guess",
    "start": "2375520",
    "end": "2382640"
  },
  {
    "text": "the the dream is really this idea of the cluster api where you you put some effort into the bootstrap cluster",
    "start": "2382640",
    "end": "2388960"
  },
  {
    "text": "that you do by hand but then everything else is kind of coming automatically",
    "start": "2388960",
    "end": "2394000"
  },
  {
    "text": "via the cluster api i don't know how far it gets because then you still need this kind of metal as a service component",
    "start": "2394000",
    "end": "2399680"
  },
  {
    "text": "somewhere yeah there's there's enough bits of surrounding infrastructure you need still yeah would have to be set up by",
    "start": "2399680",
    "end": "2406319"
  },
  {
    "text": "something but maybe some of this will become more ubiquitous as time goes on i don't know",
    "start": "2406319",
    "end": "2412000"
  },
  {
    "text": "yeah but yeah but so maybe maybe we take this as an action just to to send around uh",
    "start": "2412000",
    "end": "2418400"
  },
  {
    "text": "like a survey like we did the last couple of times with uh asking specifically about parameter",
    "start": "2418400",
    "end": "2425200"
  },
  {
    "text": "deployments yeah that'd be good also be for those going to kubecon actually to do some research they will be really",
    "start": "2425200",
    "end": "2430240"
  },
  {
    "text": "valuable yeah yeah actually we we don't have a talk",
    "start": "2430240",
    "end": "2436880"
  },
  {
    "text": "this time for the group i don't know if there are other questions on the topic let me see",
    "start": "2436880",
    "end": "2442720"
  },
  {
    "text": "if there's any i'll give probably got time for one more if there is one more otherwise i'll stop sharing",
    "start": "2442720",
    "end": "2448800"
  },
  {
    "text": "i think we're good five second rule fine good all right thanks a lot again jamie and scott",
    "start": "2451119",
    "end": "2456800"
  },
  {
    "text": "that was pretty interesting we don't have anything else today so but one one thing i was going to mention is",
    "start": "2456800",
    "end": "2462960"
  },
  {
    "text": "uh for kubecon we still have another session in two weeks but we don't have uh talk this time but we should probably",
    "start": "2462960",
    "end": "2470480"
  },
  {
    "text": "just circulate uh like a slot lunch time or something where we",
    "start": "2470480",
    "end": "2476160"
  },
  {
    "text": "all get together and yes a it all started in barcelona so we might",
    "start": "2476160",
    "end": "2481520"
  },
  {
    "text": "as well get together i'm yeah i'm i'm not actually going i'm a bit gutted i've got three people from my team are going to be there though so",
    "start": "2481520",
    "end": "2488000"
  },
  {
    "text": "i'll um make sure i send them your way still escaping uh this jam session i can",
    "start": "2488000",
    "end": "2495040"
  },
  {
    "text": "see i know i'm sorry i'm saving up for detroit",
    "start": "2495040",
    "end": "2500640"
  },
  {
    "text": "oh that sounds good",
    "start": "2500640",
    "end": "2503599"
  },
  {
    "text": "all right that's um i i don't have anything else for today",
    "start": "2506960",
    "end": "2513119"
  },
  {
    "text": "and if anyone else wants to raise something",
    "start": "2513119",
    "end": "2517599"
  },
  {
    "text": "otherwise",
    "start": "2520160",
    "end": "2523160"
  },
  {
    "text": "all right otherwise we have a container sage uh in two weeks um",
    "start": "2530800",
    "end": "2536640"
  },
  {
    "text": "and after that could come so yeah thanks everyone for attending and we'll follow up also in the in this live channel",
    "start": "2536640",
    "end": "2544160"
  },
  {
    "text": "thank you everybody great to see you yeah thank you everyone",
    "start": "2544160",
    "end": "2550280"
  }
]