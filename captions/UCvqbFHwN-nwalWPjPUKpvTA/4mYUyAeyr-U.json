[
  {
    "text": "okay that is time so let's get started hi everyone thank you for joining me my name is Eddie zeneski uh thank you for",
    "start": "280",
    "end": "7480"
  },
  {
    "text": "sticking around on the the last day of talks I know this is right after lunch so hopefully we'll have some fun in here",
    "start": "7480",
    "end": "12920"
  },
  {
    "text": "uh today we're going to be talking about the cluster killer bug also known as learning API in priority and fairness to Hardway quick bit about me you can find",
    "start": "12920",
    "end": "20160"
  },
  {
    "text": "me on the internet at Ed Zay I live in Denver Colorado I like to climb mountains I'm a co-chair and Tech lead",
    "start": "20160",
    "end": "25439"
  },
  {
    "text": "for kubernetes Sig CLI so I maintain Cube control customize all those CLI things under the the umbrella there uh",
    "start": "25439",
    "end": "32520"
  },
  {
    "text": "and I'm a Staff de and open source engineer and I need a new job so hire me thank you so we're going to flash back to",
    "start": "32520",
    "end": "40000"
  },
  {
    "text": "February 1st uh 2023 at Cloud native security con was anyone here at Cloud",
    "start": "40000",
    "end": "45079"
  },
  {
    "text": "native security con no that's okay it was out in Seattle it was the first one uh cncf decided to kind of like split",
    "start": "45079",
    "end": "52199"
  },
  {
    "text": "off an event uh into its own and it was a good time and I was out with a bunch",
    "start": "52199",
    "end": "58000"
  },
  {
    "text": "of co-workers doing some karaoke late at night and I had a media interview with",
    "start": "58000",
    "end": "65000"
  },
  {
    "text": "the news stack the following day so we were going to go through some of the new stuff that we were working on some of",
    "start": "65000",
    "end": "70280"
  },
  {
    "text": "the new controller stuff we were building and I snuck out of karaoke to",
    "start": "70280",
    "end": "75479"
  },
  {
    "text": "go do some last minute prep in my hotel room at night uh and just like all good",
    "start": "75479",
    "end": "81439"
  },
  {
    "text": "things nothing was working uh I had no idea what was going on I thought it was a network I thought it was a computer uh",
    "start": "81439",
    "end": "88360"
  },
  {
    "text": "my requests were timing out none of my demos were working everything just kind of seemed broken and so I did what anyone would",
    "start": "88360",
    "end": "95759"
  },
  {
    "text": "normally do I deleted the cluster and made a new one uh and everything worked and I was pretty fat happy with that and",
    "start": "95759",
    "end": "102399"
  },
  {
    "text": "then I went to bed and I woke up in the morning and tested it out again and it was uh broken",
    "start": "102399",
    "end": "109200"
  },
  {
    "text": "again so here I am probably like two or three hours before I'm supposed to do an interview uh and my demo wasn't working",
    "start": "109200",
    "end": "118240"
  },
  {
    "text": "uh so we we push the interview back uh to the afternoon thankfully uh new",
    "start": "118240",
    "end": "123360"
  },
  {
    "text": "Stack's awesome if you ever work with them did a bunch of triage just trying to figure out what was wrong really",
    "start": "123360",
    "end": "128399"
  },
  {
    "text": "quickly learned a lot of things about what to do in the moment when you have to get on stage and do a demo uh and",
    "start": "128399",
    "end": "135959"
  },
  {
    "text": "thankfully we managed to get everything working still didn't know what was quite wrong uh but I was able to do the demo",
    "start": "135959",
    "end": "141760"
  },
  {
    "text": "after basically barely any sleep and a very stressful day so I have a little",
    "start": "141760",
    "end": "146840"
  },
  {
    "text": "clip I'll play for you of uh the beginning of see if you could see how frazzled I was during this",
    "start": "146840",
    "end": "152800"
  },
  {
    "text": "interview action there used to be this advertisement that was like popular in the Northwest for Rainer beer they go",
    "start": "152800",
    "end": "160930"
  },
  {
    "text": "[Music] Rainer beer is that ring a bell no okay",
    "start": "160930",
    "end": "166519"
  },
  {
    "text": "but I want some now so advertising works okay so Alex from the new stack is great",
    "start": "166519",
    "end": "171560"
  },
  {
    "text": "if you've never met him before uh so like I said thankfully we were able to pull this off but it was a long stressful two days trying to figure out",
    "start": "171560",
    "end": "177800"
  },
  {
    "text": "what was going on play again uh and so I have some early",
    "start": "177800",
    "end": "183159"
  },
  {
    "text": "takeaways from all this and we'll get into the bug we'll get into everything that was going on but the early takeaways was I should not have deleted",
    "start": "183159",
    "end": "189440"
  },
  {
    "text": "that cluster from the beginning I kind of should have put it on the back burner and like left it around to introspect later so I could compare and contrast so",
    "start": "189440",
    "end": "196560"
  },
  {
    "text": "don't be too like quick to to pull the trigger and blow it away I know that's what we all want to do is does it work if you turn it off and on again right uh",
    "start": "196560",
    "end": "204040"
  },
  {
    "text": "try not to do that start ruling out what you can as soon as possible so going",
    "start": "204040",
    "end": "209360"
  },
  {
    "text": "through the logs going through the metrics going through everything I was trying to rule out like what could actually be wrong with what I was",
    "start": "209360",
    "end": "214920"
  },
  {
    "text": "seeing think out loud whether this is with people around you or in slack just kind of type out your thoughts",
    "start": "214920",
    "end": "221040"
  },
  {
    "text": "screenshots past and Snippets of metrics and stuff uh do this as you're going uh",
    "start": "221040",
    "end": "226280"
  },
  {
    "text": "and then glance at the logs and metrics I say glance here because especially if",
    "start": "226280",
    "end": "231920"
  },
  {
    "text": "you don't quite know what you're looking for and you're your logs aren't always obvious uh you can spend a lot of time",
    "start": "231920",
    "end": "237120"
  },
  {
    "text": "reading log lines that you've never seen before and think that every single log line could be what the problem is so",
    "start": "237120",
    "end": "243640"
  },
  {
    "text": "take a good glance but don't get too caught up in the logs and metrics when you're first trying to get through this stuff and your goal is to try to",
    "start": "243640",
    "end": "250079"
  },
  {
    "text": "reproduce the issue that you're running into uh you don't Tech really have to understand what's going on again I'm",
    "start": "250079",
    "end": "255760"
  },
  {
    "text": "coming this at a frame of you know I have like three hours before I need to go and demo this thing so I can't",
    "start": "255760",
    "end": "261320"
  },
  {
    "text": "actually dig into everything that's going on so you want to try to figure out like what causes this bug to happen so you can build and work and avoid",
    "start": "261320",
    "end": "268600"
  },
  {
    "text": "it so what was I demoing so the controller uh it was a uh admission uh",
    "start": "268600",
    "end": "274360"
  },
  {
    "text": "controller that we built uh it was built around the Sig store policy controller anyone familiar with sigstore it's uh a",
    "start": "274360",
    "end": "280600"
  },
  {
    "text": "Linux Foundation project part of the open ssf for signing and verifying software artifacts so you can do supply",
    "start": "280600",
    "end": "286000"
  },
  {
    "text": "chain security policies to say I only want containers that are signed by these images that don't contain these known",
    "start": "286000",
    "end": "291479"
  },
  {
    "text": "vulnerabilities or dependencies so it was built around the sigstore policy controller which is open",
    "start": "291479",
    "end": "296840"
  },
  {
    "text": "source uh it opened a lot of Long Live connections to the API server it did what's called a watch if you're familiar",
    "start": "296840",
    "end": "303680"
  },
  {
    "text": "uh it's basically just like a very long lived request that kind of streams responses back from the kubernetes API",
    "start": "303680",
    "end": "310039"
  },
  {
    "text": "server it was built with K native serving uh which anyone built a controller before or use K native it's a",
    "start": "310039",
    "end": "316880"
  },
  {
    "text": "lot of fun it's a pretty building a controller from scratch is fun you kind of learn all the knobs and and then you",
    "start": "316880",
    "end": "323120"
  },
  {
    "text": "reach out for K native and never think about The Primitives ever again because K native does everything for you so there was a bit of magic baked in there",
    "start": "323120",
    "end": "331080"
  },
  {
    "text": "uh and the very unique thing about this controller is it had the ability to run we called Agent lists um we were a",
    "start": "331080",
    "end": "337520"
  },
  {
    "text": "security company and when you have a security product you generally need to run a sidecar or an agent on your",
    "start": "337520",
    "end": "344400"
  },
  {
    "text": "customers servers or clusters uh and this was something we heard from a lot of customers was that they didn't want",
    "start": "344400",
    "end": "350440"
  },
  {
    "text": "to have to run these side cars or these agents they they really didn't want to have to install yet another agent on",
    "start": "350440",
    "end": "356120"
  },
  {
    "text": "their their clusters so we designed a way to run these AR controller agent lists so stepping back a bit what does a",
    "start": "356120",
    "end": "363120"
  },
  {
    "text": "controller actually look like uh well we have the kubernetes API server which is the heart of all things uh and we have",
    "start": "363120",
    "end": "369560"
  },
  {
    "text": "the controller which in you know our case was a go binary but it you know could be built with anything but it's",
    "start": "369560",
    "end": "374960"
  },
  {
    "text": "these ideas are these few components that make up a controller so you have your your client that can talk to the API server you have your work CU that's",
    "start": "374960",
    "end": "381720"
  },
  {
    "text": "kind of listening to those events coming in and putting them on a queue that the controller can process and reconcile you",
    "start": "381720",
    "end": "387080"
  },
  {
    "text": "have these informers and liser caches uh this is kind of just uh opening those long live connections to the API server",
    "start": "387080",
    "end": "394360"
  },
  {
    "text": "and kind of listening for all those pod events all those node events all those config Maps you're creating uh and then",
    "start": "394360",
    "end": "399840"
  },
  {
    "text": "it caches them uh when you do a list operation on a kubernetes API server it",
    "start": "399840",
    "end": "405080"
  },
  {
    "text": "has to go to ETD and query all the values out of ETD and it's probably one",
    "start": "405080",
    "end": "410160"
  },
  {
    "text": "of the most you know U consuming request that you can make Tod is a list you have",
    "start": "410160",
    "end": "416280"
  },
  {
    "text": "to get back a ton of information uh you can't use like ND is a key value store right so it's not you can just look up a",
    "start": "416280",
    "end": "422199"
  },
  {
    "text": "nice single key of A O of one operation you have to get a huge list and so that's what we have this like Informer",
    "start": "422199",
    "end": "428240"
  },
  {
    "text": "does caching and then everything runs in a reconciler loop so this is the loop saying I'm going to take something off",
    "start": "428240",
    "end": "433400"
  },
  {
    "text": "the que uh do some work that needs to be done maybe like monitor a new thing or enforce a new policy and then pop it off",
    "start": "433400",
    "end": "441240"
  },
  {
    "text": "and so the way we made this agent list was we actually built a proxy that sat in front of everything and we registered",
    "start": "441240",
    "end": "448199"
  },
  {
    "text": "an off cluster uh mutating admission web hook so kubernetes if you're not familiar with mutating admission web",
    "start": "448199",
    "end": "454039"
  },
  {
    "text": "hooks or any sort of admission controllers you can register one with your cluster it sends events and",
    "start": "454039",
    "end": "459960"
  },
  {
    "text": "requests to it and you respond whether or not something should be come in so if you say I don't want a pod Named Dave to",
    "start": "459960",
    "end": "466639"
  },
  {
    "text": "ever run on my cluster you can write a admission controller for that you get a request when a new pod comes in and you",
    "start": "466639",
    "end": "472479"
  },
  {
    "text": "can check the name see if it's Dave rejected or not so we are able to run this cluster run this controller in one",
    "start": "472479",
    "end": "479080"
  },
  {
    "text": "of our managed tenant clusters you see there some other cluster bit and by having that proxy where the API server",
    "start": "479080",
    "end": "485960"
  },
  {
    "text": "was reaching out sending all those events to us through this tunnel uh we able to run our agent but off your",
    "start": "485960",
    "end": "491400"
  },
  {
    "text": "cluster it's very novel it worked pretty well so what we saw when this bug was happening was the cluster appeared just",
    "start": "491400",
    "end": "498199"
  },
  {
    "text": "super dead just like all those requests were timing out we weren't able to pull any of the metrics up we weren't able to",
    "start": "498199",
    "end": "504240"
  },
  {
    "text": "pull an logs none of the pods were responding to requests everything just appeared dead",
    "start": "504240",
    "end": "510479"
  },
  {
    "text": "client requests were timing out so if you made a cube control request or any sort of other client request it would",
    "start": "510479",
    "end": "515719"
  },
  {
    "text": "just time out it hit with a one minute context deadline and just pop you out with the",
    "start": "515719",
    "end": "520800"
  },
  {
    "text": "504 the gke dashboard was dead this was the very interesting one have you ever",
    "start": "520800",
    "end": "526000"
  },
  {
    "text": "seen the GK dashboard for it could show you like your nodes and your storage and your pods uh when we loaded it up I have",
    "start": "526000",
    "end": "531880"
  },
  {
    "text": "I have a slide for it in a bit but it was just it was dead it was showing us no data was available and throwing some",
    "start": "531880",
    "end": "537920"
  },
  {
    "text": "errors and so so after some debugging triaging we realized that this happened after 4 to 6 hours of our controller",
    "start": "537920",
    "end": "544640"
  },
  {
    "text": "being installed on your cluster so 4 to6 hours later is when we'd start seeing the spikes and the traffic",
    "start": "544640",
    "end": "552040"
  },
  {
    "text": "dropping so we named this the cluster killer bug because it would brick a kubernetes cluster as far as we could",
    "start": "552040",
    "end": "558320"
  },
  {
    "text": "tell uh it visibly only affected kubernetes 125 which was a Unique",
    "start": "558320",
    "end": "563600"
  },
  {
    "text": "Piece uh it only happened on gke we also did a bit of AWS testing the whole thing",
    "start": "563600",
    "end": "571040"
  },
  {
    "text": "with the agent list was it did some funny I am impersonation to be able to do that off cluster traffic and that",
    "start": "571040",
    "end": "576880"
  },
  {
    "text": "proxying and we were only able to actually see this with gke and it only affected agent list",
    "start": "576880",
    "end": "583600"
  },
  {
    "text": "right so when the when the cluster was running the agent was running off the cluster as opposed to on",
    "start": "583600",
    "end": "589040"
  },
  {
    "text": "it and the only solution seemed to be deleting the cluster and uh recreating",
    "start": "589040",
    "end": "595399"
  },
  {
    "text": "and re-enrolling it and when you're a services company and a security company uh this is not an acceptable solution",
    "start": "595399",
    "end": "601519"
  },
  {
    "text": "for your customers who clusters you've bricked right we can't be bricking customer clusters uh so this became a a",
    "start": "601519",
    "end": "607880"
  },
  {
    "text": "pretty top priority to figure out uh thankfully we didn't have anyone using agent list on gke at the time so we had",
    "start": "607880",
    "end": "613959"
  },
  {
    "text": "a little bit of of Headway uh we had AWS agentless users so it was it was good",
    "start": "613959",
    "end": "619320"
  },
  {
    "text": "for the time uh this is what it looked like when the gke dashboard died you can see there",
    "start": "619320",
    "end": "624880"
  },
  {
    "text": "it's like we can't make any requests uh kind of strange",
    "start": "624880",
    "end": "630680"
  },
  {
    "text": "um looking through the logs you you know when you're looking at logs that you're not familiar with that you really haven't dug into before every single log",
    "start": "630680",
    "end": "637440"
  },
  {
    "text": "line that you see could be what's wrong uh this was something that was going on with the list and Watcher cache of the",
    "start": "637440",
    "end": "643480"
  },
  {
    "text": "sigar policy controller it was giving us like a timeout allowed request uh and",
    "start": "643480",
    "end": "649200"
  },
  {
    "text": "then initializing so we spent a bunch of time trying to dig into was it the policy controller and what's going on",
    "start": "649200",
    "end": "655279"
  },
  {
    "text": "with the K native listers you then you wind up with log lines like this uh this is just kind of",
    "start": "655279",
    "end": "662120"
  },
  {
    "text": "like a full-on stack Trace which uh is great to have when you're debugging not something you want to see returned in a",
    "start": "662120",
    "end": "668680"
  },
  {
    "text": "response sometimes right we try not to leak State like that um I'm not even going to try to explain what's in there",
    "start": "668680",
    "end": "675040"
  },
  {
    "text": "it's just a full stack trace of the API server uh looking at the metrics the only thing that was notable uh was the",
    "start": "675040",
    "end": "681959"
  },
  {
    "text": "API server memory uh it would kind of Spike and then drop down and Spike and drop down and this is over the period of",
    "start": "681959",
    "end": "688800"
  },
  {
    "text": "what is that 27 days in April of a cluster um and it's it looks relatively",
    "start": "688800",
    "end": "696519"
  },
  {
    "text": "normal you know you have some spiky traffic bits and then you have some lows but this kind of gave us a a good",
    "start": "696519",
    "end": "704079"
  },
  {
    "text": "place to start was like why is the API server uh oing itself right and and obviously you know you assume that those",
    "start": "704079",
    "end": "709800"
  },
  {
    "text": "dips are the API server being oom killed or restarted so our first Theory uh we dug",
    "start": "709800",
    "end": "716000"
  },
  {
    "text": "into the change log between 125 and 126 because again we couldn't reproduce this on 126 uh and so we found a change that",
    "start": "716000",
    "end": "724360"
  },
  {
    "text": "came through it was the admission controllers can cause unnecessary significant load on API server uh spend",
    "start": "724360",
    "end": "730680"
  },
  {
    "text": "a ton of time trying to dig in and understand this change uh really couldn't uh see how this would impact",
    "start": "730680",
    "end": "737440"
  },
  {
    "text": "what we were doing so we ruled this out uh just so some lessons from all of",
    "start": "737440",
    "end": "742600"
  },
  {
    "text": "that so have a baseline of what your logs and metrics look like uh obviously if you're doing a demo cluster you're",
    "start": "742600",
    "end": "748399"
  },
  {
    "text": "not going to have this but like I said keep those around um when I went in there to debug those logs and I saw",
    "start": "748399",
    "end": "755199"
  },
  {
    "text": "those random stack traces and those random reconciler errors uh to me who had not worked on this product before I",
    "start": "755199",
    "end": "762240"
  },
  {
    "text": "was not aware of are these normal or are these not normal so it's good to have a baseline of like what you're comparing",
    "start": "762240",
    "end": "767800"
  },
  {
    "text": "against and of course being able to compare like historically with your metrics uh helps a",
    "start": "767800",
    "end": "773760"
  },
  {
    "text": "ton slow down try not to rush like I said this was a we're kind of RAC ing",
    "start": "773760",
    "end": "779079"
  },
  {
    "text": "the clock here where you know thankfully there were no customers that would encounter this at the time but you know we were a small startup that was growing",
    "start": "779079",
    "end": "785560"
  },
  {
    "text": "quickly and who knew who would want to use it so take your time slow down kind of like think",
    "start": "785560",
    "end": "790600"
  },
  {
    "text": "through get creative and experiment uh this is ultimately what led us to figure out what was going on so just throw",
    "start": "790600",
    "end": "796720"
  },
  {
    "text": "at the wall and see what sticks try things that sound crazy um so this is going to be kind of small",
    "start": "796720",
    "end": "803160"
  },
  {
    "text": "but uh the try and crazy piece is how we were able to actually like get a lead here and so I don't think I can zoom in",
    "start": "803160",
    "end": "811639"
  },
  {
    "text": "too much no uh but what you can't see at the Top Line there is it is a HTTP get",
    "start": "811639",
    "end": "817199"
  },
  {
    "text": "to API Fubar uh this was me just randomly throwing things out of cube",
    "start": "817199",
    "end": "822279"
  },
  {
    "text": "control at the API client with a little raw query uh and we saw a this log line",
    "start": "822279",
    "end": "828279"
  },
  {
    "text": "show up in the API server and this is a this was different than all the other timeouts and log lines we seeing this",
    "start": "828279",
    "end": "833600"
  },
  {
    "text": "was a 4 or4 not found uh which compared to all the other responses we were seeing when when the cluster was",
    "start": "833600",
    "end": "840320"
  },
  {
    "text": "seemingly bricked the fact that we were able to see a a response like this was was a lead so we knew that the API",
    "start": "840320",
    "end": "846720"
  },
  {
    "text": "server was listening it was responding but it was able to tell us that the resource was not",
    "start": "846720",
    "end": "852160"
  },
  {
    "text": "found uh and then we got even crazier we wound up sshing into the uh the nodes uh",
    "start": "852160",
    "end": "858519"
  },
  {
    "text": "set up some searchs was able to talk to the API server directly when no other cluster traffic could talk to the API",
    "start": "858519",
    "end": "864600"
  },
  {
    "text": "server so the nodes were perfectly healthy that was the one of the other things with all the metrics everything",
    "start": "864600",
    "end": "869839"
  },
  {
    "text": "looked healthy aside from that little bit of memory spiking all the request times all of the the processing the CPU",
    "start": "869839",
    "end": "876320"
  },
  {
    "text": "utilization nothing really seemed out of the ordinary we're just kind of in a a weird Magic Black Box state so when we",
    "start": "876320",
    "end": "883240"
  },
  {
    "text": "were able to talk to the API server and have instant responses uh this gave us a good lead and from here we were actually",
    "start": "883240",
    "end": "889959"
  },
  {
    "text": "able to dump all of the metrics so if you haven't done with Cube control before you can do a cube control get--",
    "start": "889959",
    "end": "896199"
  },
  {
    "text": "raw and you can make raw API requests and it includes your alt headers so we able to make requests to you know D-",
    "start": "896199",
    "end": "903120"
  },
  {
    "text": "rmetrics logs and get all those actual metrics and logs we weren't able to get",
    "start": "903120",
    "end": "908160"
  },
  {
    "text": "from the gke dashboard and log exporter and then we found a little bit",
    "start": "908160",
    "end": "913880"
  },
  {
    "text": "more bread clums uh once we started being able to pull these logs the one that really stuck out to us was this too",
    "start": "913880",
    "end": "920240"
  },
  {
    "text": "many requests line uh and from what we were seeing with again we weren't getting responses from the API server or",
    "start": "920240",
    "end": "927040"
  },
  {
    "text": "they were timing out uh and then to see too many requests coming in well what",
    "start": "927040",
    "end": "932079"
  },
  {
    "text": "was that what is telling us that there's too many requests were there too many retries the API server didn't look",
    "start": "932079",
    "end": "938000"
  },
  {
    "text": "flooded so what was going on with this too many requests so to take us a little step",
    "start": "938000",
    "end": "943440"
  },
  {
    "text": "back uh I want to talk briefly about API server tuning uh this is something that most people probably are never going to",
    "start": "943440",
    "end": "949399"
  },
  {
    "text": "have to touch or have access to uh the two flags that you would set on the API",
    "start": "949399",
    "end": "954959"
  },
  {
    "text": "server are Max request in flight and Max mutating request request in flight and this is how the API server can be set to",
    "start": "954959",
    "end": "962240"
  },
  {
    "text": "the max number of inbound requests at a time uh these are hidden from you if",
    "start": "962240",
    "end": "967639"
  },
  {
    "text": "you're using a managed kubernetes service you usually don't get access to the logs uh thankfully if you look at",
    "start": "967639",
    "end": "974440"
  },
  {
    "text": "gke uh you can see these logs these flags printed out at the top of your uh",
    "start": "974440",
    "end": "981680"
  },
  {
    "text": "API logs so if you do like Cube control getlog ai- server you can actually get",
    "start": "981680",
    "end": "989279"
  },
  {
    "text": "the logs and so here's actually all the flags that are set on my gke cluster when it starts up with the API server I",
    "start": "989279",
    "end": "996120"
  },
  {
    "text": "don't know if there's many other providers that do this but I wish they did this saves a ton of time so you can see and if we look in here we can see",
    "start": "996120",
    "end": "1003199"
  },
  {
    "text": "that Max what is it Max Max request inlight is sent to 60 by",
    "start": "1003199",
    "end": "1011680"
  },
  {
    "text": "default and Max requests inflight uh mutating is set to zero so zero here means unlimited um so we should",
    "start": "1011680",
    "end": "1019920"
  },
  {
    "text": "technically have these unlimited requests coming in uh and the max refle request in Flight is 60 uh that's pretty",
    "start": "1019920",
    "end": "1028120"
  },
  {
    "text": "decent especially for you know the response time the round trip uh and so these are the two dials",
    "start": "1028120",
    "end": "1034199"
  },
  {
    "text": "that you would be able to tune if you had access to the server Flags but you can't uh so out of that we uh built a",
    "start": "1034199",
    "end": "1040839"
  },
  {
    "text": "feature into kubernetes called API priority and fairness anyone familiar with this few people cool uh quick",
    "start": "1040839",
    "end": "1048520"
  },
  {
    "text": "summary from the docs the API priority and fairness feature is an alternative to the those two flags I mentioned that",
    "start": "1048520",
    "end": "1054799"
  },
  {
    "text": "improves upon before mentioned infly limitations APF classifies and isolates requests in a more fine grain way it",
    "start": "1054799",
    "end": "1061919"
  },
  {
    "text": "also introduces a limited amount of queuing so that no requests are rejected in cases of brief",
    "start": "1061919",
    "end": "1069559"
  },
  {
    "text": "bursts without APF enabled overall concurrency in the API server is limited",
    "start": "1069600",
    "end": "1074720"
  },
  {
    "text": "by those two flags and with APF enabled the concurrency limits defined by these flags are summed and their sum is",
    "start": "1074720",
    "end": "1082000"
  },
  {
    "text": "divided up among a configural set of priority levels each request each incoming request is assigned to a single",
    "start": "1082000",
    "end": "1088480"
  },
  {
    "text": "priority level and each priority level will only dispatch as many current concurrent requests as its particular",
    "start": "1088480",
    "end": "1094360"
  },
  {
    "text": "limit allows so we came across this while looking through API tuning uh rate",
    "start": "1094360",
    "end": "1099520"
  },
  {
    "text": "limiting in the cluster uh this obviously sounded like something that we were seeing",
    "start": "1099520",
    "end": "1104840"
  },
  {
    "text": "right so quick background on APF uh it was uh beta feature that we released in kubernetes 120 it's currently at a uh",
    "start": "1104840",
    "end": "1112320"
  },
  {
    "text": "beta V1 V3 still um I don't know when it's playing for ga uh it is enabled by",
    "start": "1112320",
    "end": "1118520"
  },
  {
    "text": "default as of 120 so your clusters will have this unless they're opted out I don't think any of the cloud providers opt out so you're most likely have this",
    "start": "1118520",
    "end": "1125159"
  },
  {
    "text": "feature enabled it's how you control traffic in an overloaded situation so this is",
    "start": "1125159",
    "end": "1130760"
  },
  {
    "text": "intended for you to prioritize different types of traffic uh so that when you are having backed up or or massive burst of",
    "start": "1130760",
    "end": "1137400"
  },
  {
    "text": "requests to your cluster that those important requests get through uh think in this case of your metrics endpoint",
    "start": "1137400",
    "end": "1142919"
  },
  {
    "text": "your logs endpoint you always want those responses to be I mean those endpoints to be available uh even when the API",
    "start": "1142919",
    "end": "1149640"
  },
  {
    "text": "server is bogged down trying to list a thousand different config Maps or something and what I've really taken",
    "start": "1149640",
    "end": "1155919"
  },
  {
    "text": "away from APF is it doesn't matter until it does right this is one of those features that probably 90% of people who",
    "start": "1155919",
    "end": "1161760"
  },
  {
    "text": "use kubernetes are never going to have to think about or touch uh until you run into it and it becomes an issue so spoil",
    "start": "1161760",
    "end": "1168480"
  },
  {
    "text": "if you couldn't tell from the title this is actually what we were running into with the bug we were seeing so there's two resources that are",
    "start": "1168480",
    "end": "1175559"
  },
  {
    "text": "part of the flow control schema for API priority and fairness uh",
    "start": "1175559",
    "end": "1181080"
  },
  {
    "text": "the U the flow schema is what defines uh The Who and the what of your cluster uh",
    "start": "1181080",
    "end": "1186640"
  },
  {
    "text": "traffic so this is very similar to how arbac does it so you have your subjects which are user service accounts or",
    "start": "1186640",
    "end": "1191880"
  },
  {
    "text": "groups uh and then resourcesresources which are your group version resources so your pods",
    "start": "1191880",
    "end": "1197280"
  },
  {
    "text": "deployments and your paths so SL metrics uh it has a matching precedence built in",
    "start": "1197280",
    "end": "1202919"
  },
  {
    "text": "so this is where you can Elevate certain policies for certain requests um and there's some built-in ones that we'll",
    "start": "1202919",
    "end": "1209000"
  },
  {
    "text": "take a look at in a second and so the flow schema maps to a priority level configuration so you",
    "start": "1209000",
    "end": "1215120"
  },
  {
    "text": "define this flow schema here's an example of what it looks like so this is going to map everything from the service",
    "start": "1215120",
    "end": "1220600"
  },
  {
    "text": "account demo uh every single type of requests so non-resource URLs and resourced URLs uh and it's going to map",
    "start": "1220600",
    "end": "1227919"
  },
  {
    "text": "all of these requests at the 1,000 priority level to the priority",
    "start": "1227919",
    "end": "1233159"
  },
  {
    "text": "configuration name demo you see that mapping there priority level configuration so",
    "start": "1233159",
    "end": "1241280"
  },
  {
    "text": "this is the uh it has two different types that you can configure at the top level it's either limited or exempt so",
    "start": "1241280",
    "end": "1246840"
  },
  {
    "text": "you can exempt traffic from a policy so you can say hey this none of this stuff ever gets ritim it should always have",
    "start": "1246840",
    "end": "1254159"
  },
  {
    "text": "priority and then when a rate limit hits or the concurrency limit hits you can choose between queuing those requests or",
    "start": "1254159",
    "end": "1260480"
  },
  {
    "text": "rejecting them two different types uh and these are the knobs to turn",
    "start": "1260480",
    "end": "1265799"
  },
  {
    "text": "for your tuning so here's what one of those policies looks like you can see the Q The Limited and then all those",
    "start": "1265799",
    "end": "1272000"
  },
  {
    "text": "knobs that we're going to talk about so the queuing configuration has",
    "start": "1272000",
    "end": "1277200"
  },
  {
    "text": "four main knobs that you tune uh and so this is the V2 uh beta V2 version uh",
    "start": "1277200",
    "end": "1283080"
  },
  {
    "text": "beta B3 made a few changes which we'll talk about in a little bit but this one's a bit easier to understand so",
    "start": "1283080",
    "end": "1288400"
  },
  {
    "text": "assured concurrency shares these are kind of like your seats for your concurrent connections these are the the",
    "start": "1288400",
    "end": "1293480"
  },
  {
    "text": "number of uh concurrent requests that are allowed to be executing uh and we say seats Loosely here and we'll talk",
    "start": "1293480",
    "end": "1300240"
  },
  {
    "text": "about that in a second hand size so the way this works is with a fair queuing algorithm uh there's lots of white",
    "start": "1300240",
    "end": "1306600"
  },
  {
    "text": "papers and Wikipedia pages that I tried to understand uh essentially what hand size means is out of all of your cues",
    "start": "1306600",
    "end": "1313480"
  },
  {
    "text": "it's going to randomly pick one and from how it picks one is it is dealt a hand so it will pick maybe your hand size is",
    "start": "1313480",
    "end": "1320320"
  },
  {
    "text": "four so it will pick four cues and then randomly pick from those and this is hopefully to stop it from starving out",
    "start": "1320320",
    "end": "1325760"
  },
  {
    "text": "particular Q's uh your Q length limit is how big the Q is so how many requests can be in a given q and q's is your",
    "start": "1325760",
    "end": "1333039"
  },
  {
    "text": "number of Q's that you have so to when you want to increase",
    "start": "1333039",
    "end": "1338080"
  },
  {
    "text": "cues you reduce the rate of collisions between those different flows at the cost of increased memory usage a value",
    "start": "1338080",
    "end": "1344279"
  },
  {
    "text": "of one here effectively disables Fair queing logic but it still allows request to queed increasing Q limit allows larger",
    "start": "1344279",
    "end": "1351760"
  },
  {
    "text": "burst traffic so this is where you're responding to larger burst requests coming through again it's hopefully so",
    "start": "1351760",
    "end": "1357440"
  },
  {
    "text": "you don't drop any requests at the cost of uh higher latency while those uh requests are in Q and more memory usage",
    "start": "1357440",
    "end": "1364080"
  },
  {
    "text": "to hold them in Q and changing hand size allows you to adjust the probability of collisions uh so you have you know a",
    "start": "1364080",
    "end": "1370840"
  },
  {
    "text": "different hand size to choose from out of all those cues so there's two built-in levels uh",
    "start": "1370840",
    "end": "1376640"
  },
  {
    "text": "well there's several built-in levels but there's there two that are worth talking about the workload low priority level is for requests from any other service",
    "start": "1376640",
    "end": "1383480"
  },
  {
    "text": "account which will typically include all requests from controllers running in pods and then there is the global",
    "start": "1383480",
    "end": "1389520"
  },
  {
    "text": "default priority level uh this is basically all other uh traffic from your",
    "start": "1389520",
    "end": "1395679"
  },
  {
    "text": "uh clients and stuff so if you make a cube control command from your laptop it is going to hit Global default this is",
    "start": "1395679",
    "end": "1401360"
  },
  {
    "text": "for like authenticated users there's also a catch all bucket and that is usually for like unauthenticated types",
    "start": "1401360",
    "end": "1407000"
  },
  {
    "text": "of uh requests so these are the two uh that you really work with most of your traffic that runs on your cluster inside",
    "start": "1407000",
    "end": "1414320"
  },
  {
    "text": "of your cluster your controllers are going to hit that workload low priority",
    "start": "1414320",
    "end": "1419799"
  },
  {
    "text": "queue there's a few special cases to talk about uh when I said that the assured concurrency shares the a list",
    "start": "1419799",
    "end": "1428200"
  },
  {
    "text": "request is a is a very consuming request to the API server andd because again it",
    "start": "1428200",
    "end": "1433799"
  },
  {
    "text": "has to walk over those keys the uh way list work is it it kind of estimates how",
    "start": "1433799",
    "end": "1438960"
  },
  {
    "text": "many keys are going to be returned and it kind of like picks a number between that to figure out how many shares a",
    "start": "1438960",
    "end": "1444120"
  },
  {
    "text": "single list request will use so when you think of these they don't map one to one generally uh and then there's the",
    "start": "1444120",
    "end": "1450240"
  },
  {
    "text": "watches watches are those long live requests to the API server to listen for those events back uh the watches",
    "start": "1450240",
    "end": "1455799"
  },
  {
    "text": "generally take up a a few seats when they first start and then they are considered not taking seats while they",
    "start": "1455799",
    "end": "1462320"
  },
  {
    "text": "are sitting and waiting for events to come through uh and then there's special groups like the system Masters uh which",
    "start": "1462320",
    "end": "1468720"
  },
  {
    "text": "always get prioritized to top level traffic so this is a policy from the",
    "start": "1468720",
    "end": "1473880"
  },
  {
    "text": "docs this isn't one that's included by default but this is one they recommend everyone have uh which maybe should be",
    "start": "1473880",
    "end": "1479880"
  },
  {
    "text": "installed by default but we could discuss that so this is what would prioritize all of your health checks",
    "start": "1479880",
    "end": "1485159"
  },
  {
    "text": "Readiness checks and liveness checks to be always allowed by System unauthenticated users so this is for",
    "start": "1485159",
    "end": "1491360"
  },
  {
    "text": "your your scraping of your metrics and your health checking uh so this is a policy that you probably want to have in",
    "start": "1491360",
    "end": "1496840"
  },
  {
    "text": "your cluster if you don't already it's not there by default uh and this would exempt all of that traffic so uh it does",
    "start": "1496840",
    "end": "1503039"
  },
  {
    "text": "open the possibility for you to be ddosed or or smacked down on these end points but uh again it if something",
    "start": "1503039",
    "end": "1509399"
  },
  {
    "text": "happens you do want to be able to prioritize these these paths quick look at what these levels",
    "start": "1509399",
    "end": "1515960"
  },
  {
    "text": "look like you can see the catch all Global default ones I mentioned in there uh the assured concurrency shares all",
    "start": "1515960",
    "end": "1522080"
  },
  {
    "text": "the cues and stuff so you kind of see the scale of of what we're talking about what these look like by default",
    "start": "1522080",
    "end": "1528799"
  },
  {
    "text": "uh the way that the uh concurrency limit is calculated in the Clusters we sum up",
    "start": "1528799",
    "end": "1533919"
  },
  {
    "text": "these concurrency shares so in our example here we have 245 and then we take our two flags that",
    "start": "1533919",
    "end": "1542520"
  },
  {
    "text": "we had before uh the default values I'm using here are 400 and",
    "start": "1542520",
    "end": "1547640"
  },
  {
    "text": "200 you pick a priority level which workload low has 100 concurrency shares",
    "start": "1547640",
    "end": "1554919"
  },
  {
    "text": "and then you do this equation so you take the two numbers add them divide them by uh the ACs value for the total",
    "start": "1554919",
    "end": "1561799"
  },
  {
    "text": "cluster multiply that by the ACs for the priority level and that's how many requests per second your particular",
    "start": "1561799",
    "end": "1567799"
  },
  {
    "text": "priority level is allowed in your cluster so this is much more visible and configurable for you as a",
    "start": "1567799",
    "end": "1574880"
  },
  {
    "text": "user uh so for example what happens when you change these values right so if we say that we add another priority level",
    "start": "1574880",
    "end": "1581840"
  },
  {
    "text": "with an ACs of 55 uh that brings our total ACS to 300 do the math we have 200 requests per",
    "start": "1581840",
    "end": "1589320"
  },
  {
    "text": "second from the uh the previous uh total and now once we have our new uh share in",
    "start": "1589320",
    "end": "1596279"
  },
  {
    "text": "we have 110 right so we go from that 244 request a second down to the 200 so we",
    "start": "1596279",
    "end": "1603279"
  },
  {
    "text": "lost 45 or whatever it was requests per second 44 requests per second so adding",
    "start": "1603279",
    "end": "1608679"
  },
  {
    "text": "more priority levels you can prioritize traffic to different levels but it",
    "start": "1608679",
    "end": "1614000"
  },
  {
    "text": "brings all of your overall Max request per second down so that's something to",
    "start": "1614000",
    "end": "1619480"
  },
  {
    "text": "consider uh in 126 it actually again it doesn't really matter it makes it a lot better so I have a demo to show",
    "start": "1619480",
    "end": "1627919"
  },
  {
    "text": "you I have a quick program I [Music]",
    "start": "1627919",
    "end": "1633080"
  },
  {
    "text": "wrote uh this is a kind of a just a blaster go program pulls out client go",
    "start": "1633080",
    "end": "1639399"
  },
  {
    "text": "disables all the local client side limiting uh and then fires up 100 workers and makes a ton of infinite uh",
    "start": "1639399",
    "end": "1647080"
  },
  {
    "text": "list requ requests so we're going to fire this off at my cluster oh and I'll show you my the APF",
    "start": "1647080",
    "end": "1654360"
  },
  {
    "text": "policy we have configured right now so we have 30 shares Q size blah blah blah",
    "start": "1654360",
    "end": "1659559"
  },
  {
    "text": "uh and so this is just targeting My Demo user so this is the one you saw before so this is what to set",
    "start": "1659559",
    "end": "1666159"
  },
  {
    "text": "up so if we fire this off uh we're going to open up our watch",
    "start": "1666919",
    "end": "1674880"
  },
  {
    "text": "here and internet works cool so what you're looking for in this",
    "start": "1675000",
    "end": "1681120"
  },
  {
    "text": "request is this number going up right here which I think the internet's",
    "start": "1681120",
    "end": "1686919"
  },
  {
    "text": "fighting me here but you can see it climbing so this is requests the total requests that are being made to my cluster for this particular flow and you",
    "start": "1686919",
    "end": "1694120"
  },
  {
    "text": "can see that that number is climbing and then uh we have the number of requests",
    "start": "1694120",
    "end": "1700960"
  },
  {
    "text": "currently executing up here so there's seven and then inq requests they are there's 64 right now right so this is",
    "start": "1700960",
    "end": "1706679"
  },
  {
    "text": "just refreshing as we go so currently we're handling the load actually I'm not getting any errors reported my queuing",
    "start": "1706679",
    "end": "1713120"
  },
  {
    "text": "is working up this is finally tuned for this application but now if we start to kind of tweak those",
    "start": "1713120",
    "end": "1720600"
  },
  {
    "text": "numbers and we say let's just make everything",
    "start": "1720720",
    "end": "1725639"
  },
  {
    "text": "one and so now we're going to start you can",
    "start": "1736399",
    "end": "1743240"
  },
  {
    "text": "see we immediately drop and that's one of the cool things is this is all applied and reconfigured on the Fly we love kubernetes but you can see now that",
    "start": "1743240",
    "end": "1750080"
  },
  {
    "text": "my requests are being rejected and my reason bucket here for the Q full is",
    "start": "1750080",
    "end": "1755279"
  },
  {
    "text": "starting to rapidly climb and if we pop back to our our sample app we can see that we are getting uh this is a 429 or",
    "start": "1755279",
    "end": "1763440"
  },
  {
    "text": "4 I think it's 429 some 409 the servers received too many requests and asked us",
    "start": "1763440",
    "end": "1768600"
  },
  {
    "text": "to try again later right so this is APF working as intended uh my policy is rate limiting and I am getting responses back",
    "start": "1768600",
    "end": "1775519"
  },
  {
    "text": "on my client cool and",
    "start": "1775519",
    "end": "1782320"
  },
  {
    "text": "so we could play with that and tune that but you get the idea uh so what was the original bug well off cluster traffic",
    "start": "1782480",
    "end": "1789320"
  },
  {
    "text": "was getting bucketed into Global default right that is what happens with global default uh our our controllers traffic",
    "start": "1789320",
    "end": "1796480"
  },
  {
    "text": "should have been in the the workload low bucket as we saw and so what was happening was all of our requests were",
    "start": "1796480",
    "end": "1802679"
  },
  {
    "text": "kind of backing up and all of our our retries were were getting stuck and we",
    "start": "1802679",
    "end": "1808120"
  },
  {
    "text": "filled up the global default bucket uh so when we went to make requests from Cube control when we went to go look at",
    "start": "1808120",
    "end": "1813840"
  },
  {
    "text": "our external monitoring uh everything seemed dead because the requests were just sitting in queue and getting kicked",
    "start": "1813840",
    "end": "1819159"
  },
  {
    "text": "out uh the fix f is to create APF resources um pretty straightforward",
    "start": "1819159",
    "end": "1825360"
  },
  {
    "text": "again not something you need to worry or know about you do and so the question is when would you actually want to create these resources well definitely when",
    "start": "1825360",
    "end": "1832000"
  },
  {
    "text": "you're being clever and uh you know running things off of a cluster but the a good example would be if you have a",
    "start": "1832000",
    "end": "1838399"
  },
  {
    "text": "controller that constantly makes L list requests to your uh your API server",
    "start": "1838399",
    "end": "1843559"
  },
  {
    "text": "right you might want to bucket those or cue those up so they don't starve the controller and maybe only want one of those executing at any given time to",
    "start": "1843559",
    "end": "1850480"
  },
  {
    "text": "list all of your you know thousands of config Maps or whatever you have U so there is a is a reason to know about",
    "start": "1850480",
    "end": "1856679"
  },
  {
    "text": "this um to tune for things that you're seeing in your metrics so when you're looking through your your your grafana",
    "start": "1856679",
    "end": "1862080"
  },
  {
    "text": "and your your logs and you notice things might be backing up or failing uh this is when you be able to apply a policy to",
    "start": "1862080",
    "end": "1867919"
  },
  {
    "text": "kind of even that traffic out a bit and again you can do this by service account or user or",
    "start": "1867919",
    "end": "1874039"
  },
  {
    "text": "namespace so a couple other questions why did this not work on 12 yeah why did we not see this on 126 well that's",
    "start": "1874039",
    "end": "1880919"
  },
  {
    "text": "because 126 introduced a new feature for APF called borrowing uh borrowing is uh",
    "start": "1880919",
    "end": "1886799"
  },
  {
    "text": "added few new Fields so uh the borrowing limit percent and lendable percent this allowed uh APF policies to borrow and",
    "start": "1886799",
    "end": "1894559"
  },
  {
    "text": "share from other buckets that weren't full or whose cues were empty uh you could specify a percentage on these uh",
    "start": "1894559",
    "end": "1901399"
  },
  {
    "text": "values to tell you how much you could borrow so uh again we we were uh in 126",
    "start": "1901399",
    "end": "1909159"
  },
  {
    "text": "our bucket was getting full but because it could borrow from all the other buckets uh we didn't actually see this Behavior so this is rad it also changes",
    "start": "1909159",
    "end": "1916240"
  },
  {
    "text": "the nominal concurrency shares from the ACs to the NCS uh and there's a complicated formula that's explained but",
    "start": "1916240",
    "end": "1923200"
  },
  {
    "text": "the tldr is bigger numbers mean a larger nominal concurrency limit at the expense of every other limit being uh",
    "start": "1923200",
    "end": "1929720"
  },
  {
    "text": "prioritized down so uh last question why did the gke dashboard die well this was",
    "start": "1929720",
    "end": "1935880"
  },
  {
    "text": "a fun one uh I don't know if you can see that but the gke dashboard was L Landing",
    "start": "1935880",
    "end": "1942240"
  },
  {
    "text": "requests into the global default bucket uh I think this is fixed now uh but yeah",
    "start": "1942240",
    "end": "1947760"
  },
  {
    "text": "that's that's why when we would load the gke dashboard up uh it would look like it was dead and led to our believing the",
    "start": "1947760",
    "end": "1954399"
  },
  {
    "text": "cluster was dead because their requests were getting bucketed with all of the other off cluster traffic uh so again",
    "start": "1954399",
    "end": "1960919"
  },
  {
    "text": "I'm pretty sure that they fixed this now which is good uh there was a fun side quest where",
    "start": "1960919",
    "end": "1967080"
  },
  {
    "text": "uh our uh our API token didn't have the email scope permission so we're getting back these Google guy IDs these are like",
    "start": "1967080",
    "end": "1973960"
  },
  {
    "text": "internal IDs for Google uh you know not like a problem or anything but when we",
    "start": "1973960",
    "end": "1979039"
  },
  {
    "text": "were trying to write a policy we ran into numbers instead of the surface account emails we were looking for so if",
    "start": "1979039",
    "end": "1984360"
  },
  {
    "text": "you ever run into that just always include the the cloud scope in your Google and the email scope um couple",
    "start": "1984360",
    "end": "1992080"
  },
  {
    "text": "minutes left the more Lessons Learned being clever means you need to do more research uh this is something that we didn't know about before we ran into it",
    "start": "1992080",
    "end": "1998960"
  },
  {
    "text": "so um yep just you have to spend time to figure out and understand things we need",
    "start": "1998960",
    "end": "2005240"
  },
  {
    "text": "new people to test these features and give feedback um in writing and preparing for this talk I was going to do this on a 128 cluster but I actually",
    "start": "2005240",
    "end": "2012279"
  },
  {
    "text": "found two new bugs with APF that were going to get filed and fixed which is cool uh so if you all haven't played",
    "start": "2012279",
    "end": "2017880"
  },
  {
    "text": "with these knobs or any knobs in general go out there uh we as maintainers don't really have a direct line to the",
    "start": "2017880",
    "end": "2024080"
  },
  {
    "text": "community other than the people who show up and Report bugs so when you read those blog posts or those release notes",
    "start": "2024080",
    "end": "2029559"
  },
  {
    "text": "and they say hey we need people to test these features we really do so please help us out uh read the docs and release",
    "start": "2029559",
    "end": "2036360"
  },
  {
    "text": "notes to understand you know what's changing between versions uh and again get involved Upstream uh there's lots of",
    "start": "2036360",
    "end": "2043000"
  },
  {
    "text": "us that work on fun cool stuff like this so if you want to know where to get started feel free to hit me up or",
    "start": "2043000",
    "end": "2049398"
  },
  {
    "text": "anything uh this isn't um solved actually uh we there's still an open bug",
    "start": "2049399",
    "end": "2056079"
  },
  {
    "text": "that we need to figure out uh we were not getting back 429s like we should have we were getting back 504 timeouts",
    "start": "2056079",
    "end": "2062839"
  },
  {
    "text": "uh so I'm pretty sure that the switch to sharing the buckets for 12 6 is simply",
    "start": "2062839",
    "end": "2068760"
  },
  {
    "text": "hiding a bug that I ran into in 125 so uh hopefully no one else runs into this",
    "start": "2068760",
    "end": "2073878"
  },
  {
    "text": "but we are going to dig into it at some point but there's still a bug out there uh a couple shout outs to a bunch of people who help me while solving and",
    "start": "2073879",
    "end": "2080118"
  },
  {
    "text": "fixing this Jordan Mike and Billy they're all awesome and I am out of time actually right on the dot so I'll hang",
    "start": "2080119",
    "end": "2086679"
  },
  {
    "text": "around around here outside to answer any questions uh please scan the QR code and give feedback and yeah thanks for coming",
    "start": "2086679",
    "end": "2092760"
  },
  {
    "text": "and listening to me ramble this was fun",
    "start": "2092760",
    "end": "2098879"
  }
]