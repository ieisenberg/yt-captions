[
  {
    "text": "thanks everybody for joining us today in this session about KDA and how to unlock",
    "start": "240",
    "end": "6480"
  },
  {
    "text": "new visions and how to use KDA in production First of all I'm going to",
    "start": "6480",
    "end": "11599"
  },
  {
    "text": "introduce myself I'm Jorge Dorado I work as principal SR at",
    "start": "11599",
    "end": "17400"
  },
  {
    "text": "CRM the international half of SWARS group and I'm one of KDA maintainers I'm",
    "start": "17400",
    "end": "24400"
  },
  {
    "text": "also CNCF ambassador and Microsoft MVP and he is my employee Nice Introduce",
    "start": "24400",
    "end": "30720"
  },
  {
    "text": "yourself Thank you Horge Thank you Great So Horge is done for for today and the rest of the talk is on me So my name is",
    "start": "30720",
    "end": "37600"
  },
  {
    "text": "Beign I know it's hard to pronounce and don't feel bad about it U I'm also KA",
    "start": "37600",
    "end": "43040"
  },
  {
    "text": "maintainer Uh I'm with the project since the inception So uh already even it was",
    "start": "43040",
    "end": "48239"
  },
  {
    "text": "before the CNCF donation So we together work with a couple of other folks on Ka",
    "start": "48239",
    "end": "54160"
  },
  {
    "text": "Um on on top of that I'm also founder and CTO uh of Kifi It's a company built",
    "start": "54160",
    "end": "60000"
  },
  {
    "text": "around KDA and we provide enterprise autoscaling solutions uh around Kada for our customers So let's talk about Ka",
    "start": "60000",
    "end": "67600"
  },
  {
    "text": "today This is the agenda we have I hope we will be able to get it in time So we will have some interaction talk about",
    "start": "67600",
    "end": "74400"
  },
  {
    "text": "some features uh discuss best practices and some challenges that you can face with uh dealing with autoscaling So",
    "start": "74400",
    "end": "81520"
  },
  {
    "text": "before we start uh may I ask uh the audience if you can raise your hand if you already use SCADA in production",
    "start": "81520",
    "end": "89119"
  },
  {
    "text": "Nice It's like a more than half right Uh is there anybody who doesn't know what KDA is or maybe just heard the name but",
    "start": "89119",
    "end": "96240"
  },
  {
    "text": "is not aware of the capabilities Okay we have bunch of folks and uh is there",
    "start": "96240",
    "end": "102479"
  },
  {
    "text": "anybody who knows what KA is but doesn't use it would like to use it Okay perfect",
    "start": "102479",
    "end": "108159"
  },
  {
    "text": "So we have a coverage for all the stuff Uh so let's start right",
    "start": "108159",
    "end": "117399"
  },
  {
    "text": "So why do we need to use SCADA Why do we need to use auto scaring For the folks that already use SCADA this will be",
    "start": "117399",
    "end": "123920"
  },
  {
    "text": "repetition of the already known but I would like to cover this kind of stuff in a in a short time So why would we",
    "start": "123920",
    "end": "131120"
  },
  {
    "text": "need autoscaling on our clusters First we would like to save resources because we would like to run only the workloads",
    "start": "131120",
    "end": "136879"
  },
  {
    "text": "uh when they are needed or and also at the same time we would like to improve performance of our applications How we",
    "start": "136879",
    "end": "143360"
  },
  {
    "text": "can do that We can enable autoscaling because we can autoscale our workloads based on the actual demand So let's take",
    "start": "143360",
    "end": "150000"
  },
  {
    "text": "a look at this problem We have a Kubernetes application This application is consuming some data from some external source It in this case it could",
    "start": "150000",
    "end": "156879"
  },
  {
    "text": "be Rabbit MQ So the consumer application is consuming data and we would like to",
    "start": "156879",
    "end": "162640"
  },
  {
    "text": "autoscale it because we just discussed that autoscaling is great right So the naive approach sorry uh would be to plug",
    "start": "162640",
    "end": "169760"
  },
  {
    "text": "HPA into the mix HPA is already built in Kubernetes It works well It's perfect tool and what it does it monitors CPU",
    "start": "169760",
    "end": "176560"
  },
  {
    "text": "memory usage on the application and based on these metrics it scales out the application This is great for a lot of",
    "start": "176560",
    "end": "182400"
  },
  {
    "text": "use cases but for a lot of use cases this might not be the most ideal solution Why Because consuming messages",
    "start": "182400",
    "end": "190560"
  },
  {
    "text": "from the external source uh doesn't reflect in the in the resources consumption on the workload What does it",
    "start": "190560",
    "end": "196720"
  },
  {
    "text": "mean our application is pulling data from rabbit MQ and this doesn't reflect in the CPU or memory load or maybe the",
    "start": "196720",
    "end": "203040"
  },
  {
    "text": "load is coming a little bit after much wiser would be to actually scale the application based on the metrics coming",
    "start": "203040",
    "end": "208640"
  },
  {
    "text": "from Rabbit MQ Kafka Prometheus you name it how we can do that yeah you guess it",
    "start": "208640",
    "end": "214319"
  },
  {
    "text": "right we plug it into the mix and with this solution we can uh we can scrape the metrics directly from the external",
    "start": "214319",
    "end": "220080"
  },
  {
    "text": "source and then scale the workload directly we can also scale to zero",
    "start": "220080",
    "end": "226959"
  },
  {
    "text": "Yeah it's your turn now No I will hold your microphone So let's",
    "start": "226959",
    "end": "234159"
  },
  {
    "text": "show you a demo The good part of KDA is that you don't need to be aware about complex",
    "start": "234159",
    "end": "240879"
  },
  {
    "text": "stuff related with custommetric servers and how Kubernetes handles metrics and and the management of the metrics for",
    "start": "240879",
    "end": "247519"
  },
  {
    "text": "the HPA Can you see my No we cannot see your beep",
    "start": "247519",
    "end": "255200"
  },
  {
    "text": "It is what it is It happens Now I'm showing the right screen and instead of",
    "start": "255200",
    "end": "261199"
  },
  {
    "text": "using an HPA with boring custom metrics with adapters and a lot of boring stuff",
    "start": "261199",
    "end": "266320"
  },
  {
    "text": "you just need to deploy Keta with in this case I have a an authentication",
    "start": "266320",
    "end": "271520"
  },
  {
    "text": "source just for the secret It's not rocket science At the end of the day we are not putting humans on Mars So it's",
    "start": "271520",
    "end": "278800"
  },
  {
    "text": "just a reference to a secret And I'm going to deploy the demo And if I go to the demon",
    "start": "278800",
    "end": "287560"
  },
  {
    "text": "namespace Now I have start I I have published some messages on a rabbit Q",
    "start": "287560",
    "end": "293360"
  },
  {
    "text": "And then automatically it has started from zero to one And eventually it will",
    "start": "293360",
    "end": "298400"
  },
  {
    "text": "scale now for now for two to four Then it will scale to eight and so on and so",
    "start": "298400",
    "end": "304400"
  },
  {
    "text": "on and until all the messages are consumed Perfect Nice I can dance a bit if you",
    "start": "304400",
    "end": "310880"
  },
  {
    "text": "want in No no no Please don't Thanks I appreciate it Now the P the messages are",
    "start": "310880",
    "end": "318000"
  },
  {
    "text": "being consumed and in a couple of seconds when all the messages end we could see how it scaled to zero",
    "start": "318000",
    "end": "325560"
  },
  {
    "text": "again No it doesn't work It is what it is But you are the cutest support for",
    "start": "325560",
    "end": "330639"
  },
  {
    "text": "the microphone that I have Thank you What's your wife about it",
    "start": "330639",
    "end": "336600"
  },
  {
    "text": "Nice So it's finishing Let me check how",
    "start": "336600",
    "end": "341759"
  },
  {
    "text": "close is It's almost there And in just a second no problem We can",
    "start": "341759",
    "end": "348400"
  },
  {
    "text": "continue with our session because just another second Blah blah blah blah Nah",
    "start": "348400",
    "end": "354960"
  },
  {
    "text": "we have 24 minutes left 24 No no problem bro No problem We are almost there N I",
    "start": "354960",
    "end": "362800"
  },
  {
    "text": "don't Yeah Scaling to zero again So we can reduce the money I was I was afraid",
    "start": "362800",
    "end": "368479"
  },
  {
    "text": "about doing a demo that doesn't work just in the beginning to be honest Let's",
    "start": "368479",
    "end": "373520"
  },
  {
    "text": "continue with a shift F5 Perfect Okay so this is Ka in action I",
    "start": "373520",
    "end": "381680"
  },
  {
    "text": "hope it was somehow understandable So what is Kada Ka is a project that uh builts on top of Kubernetes Uh we try to",
    "start": "381680",
    "end": "388960"
  },
  {
    "text": "reuse Kubernetes internals You can scale any workloads uh Kubernetes jobs whatever based on different metrics not",
    "start": "388960",
    "end": "395280"
  },
  {
    "text": "just CPU memory We have 70 plus different uh event sources So Rabbit MQ",
    "start": "395280",
    "end": "401039"
  },
  {
    "text": "Prometheus whatever Uh we have a community a lot of lot of folks using it We have some good",
    "start": "401039",
    "end": "407800"
  },
  {
    "text": "contributors Uh we have a Ka user survey So if you are K user please fill the",
    "start": "407800",
    "end": "413039"
  },
  {
    "text": "survey so we can know uh we can understand what is your opinion on the project So architecture it's your turn",
    "start": "413039",
    "end": "421120"
  },
  {
    "text": "Ka it's my turn Okay Oh thanks",
    "start": "421120",
    "end": "427440"
  },
  {
    "text": "Thank you very much So marketing stuff for the CTO",
    "start": "427440",
    "end": "432639"
  },
  {
    "text": "technical part for the SR right the life the world So the point is how it ka how",
    "start": "432639",
    "end": "438880"
  },
  {
    "text": "how keta works basically ka is built on top of kubernetes is it's not replacing",
    "start": "438880",
    "end": "444400"
  },
  {
    "text": "the kubernetes mechanisms for the autoscaling is just extending them why because keta doesn't replace the hpa",
    "start": "444400",
    "end": "451440"
  },
  {
    "text": "basically keta extends the APA the the HPA thanks to ex to another extension",
    "start": "451440",
    "end": "457840"
  },
  {
    "text": "for metrics through external metrics that can be done thanks to the",
    "start": "457840",
    "end": "463520"
  },
  {
    "text": "scale object scale object is just a grapper on top on top of the HPA to extend to HPA capabilities But one thing",
    "start": "463520",
    "end": "470960"
  },
  {
    "text": "that we have noticed over the years is that not all the workloads can be horizontal horizontally",
    "start": "470960",
    "end": "477639"
  },
  {
    "text": "autoscaled relying on the HPA because if your workload have some kind of state",
    "start": "477639",
    "end": "483280"
  },
  {
    "text": "and can't be randomly killed during the scaleown process probably you will face with troubles for instance if you have",
    "start": "483280",
    "end": "490160"
  },
  {
    "text": "your GitHub runners or any kind of CI runners that doesn't match with an",
    "start": "490160",
    "end": "495599"
  },
  {
    "text": "horizontal autoscaling process as it is because you cannot guarantee that The guild bot is the empty pot For that we",
    "start": "495599",
    "end": "503720"
  },
  {
    "text": "developed the scale job which it's almost the same but instead of relying",
    "start": "503720",
    "end": "509120"
  },
  {
    "text": "on the HPA and the horizontal pot autoscaler controller we spawn from KDA",
    "start": "509120",
    "end": "514800"
  },
  {
    "text": "different jobs So the job has to end before it's removed Apart of these",
    "start": "514800",
    "end": "520159"
  },
  {
    "text": "approaches KDA extends the horizontal autoscaling capabilities supporting the",
    "start": "520159",
    "end": "525760"
  },
  {
    "text": "scale to zero which is something that you could say but Kubernetes does support",
    "start": "525760",
    "end": "531480"
  },
  {
    "text": "it Yes but not you need to enable a filter gate and you need to provide your",
    "start": "531480",
    "end": "536720"
  },
  {
    "text": "custom metric in your own way So you need to deal with all the problems and boring stuff related with custommetrics",
    "start": "536720",
    "end": "544240"
  },
  {
    "text": "server API In this case KDA does it on your",
    "start": "544240",
    "end": "550399"
  },
  {
    "text": "behalf and how an ancal object looks because HPA is like a common and and a",
    "start": "551160",
    "end": "560160"
  },
  {
    "text": "well-known manifest within Kubernetes But if you check the current scale object approach probably there are a lot",
    "start": "560160",
    "end": "567839"
  },
  {
    "text": "of similarities and a lot and a lot of things that you can relate with the HPA",
    "start": "567839",
    "end": "572880"
  },
  {
    "text": "There is a scale target ref There is the max and minimal replicas that you want And instead of metrics you have triggers",
    "start": "572880",
    "end": "579360"
  },
  {
    "text": "Which is a trigger A trigger is the scaler that you want to use And you can use one or multiple triggers Doesn't",
    "start": "579360",
    "end": "586160"
  },
  {
    "text": "matter You can include all the information that you need to make your autoscaling accurate with your",
    "start": "586160",
    "end": "592000"
  },
  {
    "text": "necessities not just CPU or memory and or all excuse me and almost the same",
    "start": "592000",
    "end": "600640"
  },
  {
    "text": "for the jobs but with some small difference Instead of referring a workload in in general you can specify",
    "start": "600640",
    "end": "608480"
  },
  {
    "text": "your job spec and keta will spawn jobs based on your rules during the scaling",
    "start": "608480",
    "end": "613920"
  },
  {
    "text": "process As I said this can bring you more power if your warloads have a state",
    "start": "613920",
    "end": "619200"
  },
  {
    "text": "or if you are running some long running execution You need to keep the process",
    "start": "619200",
    "end": "624880"
  },
  {
    "text": "alive until it ends It's not just a stateless application Probably this is",
    "start": "624880",
    "end": "629920"
  },
  {
    "text": "your best fit for your necessities It's like a chron job on steroids basically",
    "start": "629920",
    "end": "635440"
  },
  {
    "text": "Yeah it's like a good chron job features Nice features",
    "start": "635440",
    "end": "644160"
  },
  {
    "text": "This year has been a year probably with not super fancy features but all those",
    "start": "644160",
    "end": "651040"
  },
  {
    "text": "features that you could need for really really enterprise scenarios What I mean",
    "start": "651040",
    "end": "657120"
  },
  {
    "text": "probably if you or if you check other years we could have said no 30 new",
    "start": "657120",
    "end": "663519"
  },
  {
    "text": "scalar sources and a lot of out methods extra created But nowadays in enterprise",
    "start": "663519",
    "end": "669600"
  },
  {
    "text": "application in distributed and really huge application you need a good observability you need a good eventing",
    "start": "669600",
    "end": "676720"
  },
  {
    "text": "about the the things that are happening We have been working on cloud events on",
    "start": "676720",
    "end": "681839"
  },
  {
    "text": "supporting the cloud event specification in order to publish all the things that",
    "start": "681839",
    "end": "687040"
  },
  {
    "text": "are happening in Kella but wherever you need them just publishing them through",
    "start": "687040",
    "end": "692160"
  },
  {
    "text": "HTTP through as grid and we are working on new new targets that you can use to",
    "start": "692160",
    "end": "698800"
  },
  {
    "text": "receive old events related with keta wherever you need",
    "start": "698800",
    "end": "704279"
  },
  {
    "text": "them Also we have been working on improving observability in terms of",
    "start": "704279",
    "end": "709600"
  },
  {
    "text": "aligning all the metrics that we expose and all the the flows that you can use with the with the community standards",
    "start": "709600",
    "end": "716720"
  },
  {
    "text": "because if you work with Prometheus or at least in my case when I work with Prometheus I don't like to see different",
    "start": "716720",
    "end": "723839"
  },
  {
    "text": "kind of h metrics named like execution hours no hour is not a",
    "start": "723839",
    "end": "729920"
  },
  {
    "text": "community standard the community standard is second underscore second So why should I work with ours Why KDA",
    "start": "729920",
    "end": "736720"
  },
  {
    "text": "comes here to break the standards and make the things that they want No we have refactor all those metrics We have",
    "start": "736720",
    "end": "743040"
  },
  {
    "text": "been working on aligning all the standards with the community and also Yeah Yeah Yeah Do you want to continue I",
    "start": "743040",
    "end": "750000"
  },
  {
    "text": "need to drink No continue please Yeah Yeah Yeah Perfect Yes sir",
    "start": "750000",
    "end": "756560"
  },
  {
    "text": "We have been working also on improving the experience when you work with KDA to",
    "start": "756560",
    "end": "761680"
  },
  {
    "text": "detect the problems as soon as they happen Working on admission web hooks admission web hooks were there last year",
    "start": "761680",
    "end": "768079"
  },
  {
    "text": "but we have improved what we which checks we do We have improved multiple",
    "start": "768079",
    "end": "773360"
  },
  {
    "text": "fixes that we have discovered over the year and please go ahead go ahead We have been working also on",
    "start": "773360",
    "end": "781880"
  },
  {
    "text": "improving all the authentication stuffs We have improved the AWS Thanks I",
    "start": "781880",
    "end": "787600"
  },
  {
    "text": "appreciate it Yeah Yeah I'll continue So basically a lot of improvements on the authentication area because autoscaling",
    "start": "787600",
    "end": "794000"
  },
  {
    "text": "is one part but uh securing the authentication and credentials and this kind of stuff is also important because",
    "start": "794000",
    "end": "799680"
  },
  {
    "text": "when we are talking to these external services you need sometimes or usually you need authentication So we need a",
    "start": "799680",
    "end": "805360"
  },
  {
    "text": "proper way how to uh how to communicate to the services Okay So this was a a",
    "start": "805360",
    "end": "810399"
  },
  {
    "text": "very brief uh brief uh overview of KDA and some new features and new stuff Uh",
    "start": "810399",
    "end": "815839"
  },
  {
    "text": "let's talk about some best practices actually which might be interesting to you guys So the first thing it might be",
    "start": "815839",
    "end": "822600"
  },
  {
    "text": "obvious please use HPA scaling behavior What is it It's a it's a feature native",
    "start": "822600",
    "end": "828639"
  },
  {
    "text": "to HPA which we are using under the hood but is important This setting is important for one to end scaling It",
    "start": "828639",
    "end": "835760"
  },
  {
    "text": "basically controls how HPA responds to the actual load So with the with its",
    "start": "835760",
    "end": "841199"
  },
  {
    "text": "proper setting on this on this on this configuration we can avoid fluctuations on the replicas So number of replicas So",
    "start": "841199",
    "end": "848560"
  },
  {
    "text": "you see high load low load So you can have a stabilization window policies etc etc You can also define how many pots",
    "start": "848560",
    "end": "855360"
  },
  {
    "text": "you would like to add a single iteration So basically when we are creating more pots where we are scaling out we can",
    "start": "855360",
    "end": "860560"
  },
  {
    "text": "change this kind of settings So this is very very important setting and this could really change the change the uh the stuff for you if you configure it",
    "start": "860560",
    "end": "867800"
  },
  {
    "text": "properly The other other let's say best practice or maybe uh I would say one of",
    "start": "867800",
    "end": "873199"
  },
  {
    "text": "the most repeating questions we got from our uh from our users So they say okay we are using chron scaler So what what",
    "start": "873199",
    "end": "879600"
  },
  {
    "text": "is chron scaler In chron scaler you can define a schedule and you can basically tell okay at this during this schedule I",
    "start": "879600",
    "end": "885760"
  },
  {
    "text": "would like to have this amount of replicas This is useful I don't know if you would like to scale out the workload",
    "start": "885760",
    "end": "891440"
  },
  {
    "text": "uh different uh in a different ways uh during the day So for example work hours and during the night you would like to",
    "start": "891440",
    "end": "897120"
  },
  {
    "text": "scale a little bit differently And the typical thing that usually people do is",
    "start": "897120",
    "end": "902160"
  },
  {
    "text": "to define that they would like to scale to zero outside of working hours The",
    "start": "902160",
    "end": "907279"
  },
  {
    "text": "best way how to do that is not to define the design replicas to zero in the chron scaler because the how the chron scaler",
    "start": "907279",
    "end": "913199"
  },
  {
    "text": "works it tells hpa when it should scale So it should be the opposite So in the chron scaler we define the uh the",
    "start": "913199",
    "end": "920240"
  },
  {
    "text": "minimum that we would like to have outside of this of this zero schedule So this is the this is the important thing",
    "start": "920240",
    "end": "925920"
  },
  {
    "text": "to do actually you can use this feature that you are going to explain to achieve",
    "start": "925920",
    "end": "932240"
  },
  {
    "text": "the behavior of the previous slide just in case Exactly And uh the very uh this",
    "start": "932240",
    "end": "939199"
  },
  {
    "text": "is also let's say newer feature maybe not everybody's aware of it It's called scaly modifiers What is it about Keta",
    "start": "939199",
    "end": "946480"
  },
  {
    "text": "internally is relying on HPA to drive the scaling and you can as mentioned you can define multiple different scalers",
    "start": "946480",
    "end": "953440"
  },
  {
    "text": "The different the default behavior is that the scaler that reports the largest value is been selected to to drive the",
    "start": "953440",
    "end": "960560"
  },
  {
    "text": "scaling So imagine you have two scalers One scaler give you metric to scale to two replicas The other scaler tell you",
    "start": "960560",
    "end": "966800"
  },
  {
    "text": "uh to scale to five replicas So HPA by default will select the five as the maximum We cannot do about it For",
    "start": "966800",
    "end": "972800"
  },
  {
    "text": "example you would like to have average You have like the minimum or stuff like that U because it's hard to modify this",
    "start": "972800",
    "end": "979279"
  },
  {
    "text": "in the HPA because the way how HPA works Uh we develop scaling modifiers where",
    "start": "979279",
    "end": "984399"
  },
  {
    "text": "you can um you can define the needed behavior So you can specify a",
    "start": "984399",
    "end": "990079"
  },
  {
    "text": "mathematical formula or nested conditions and you can really play with the with a different metrics So you can",
    "start": "990079",
    "end": "995600"
  },
  {
    "text": "use different sources different uh triggers and scale them out properly for",
    "start": "995600",
    "end": "1000720"
  },
  {
    "text": "I'll show you it uh this kind of stuff in action Nice This time I have done",
    "start": "1000720",
    "end": "1007000"
  },
  {
    "text": "it Thanks So in this case we have a super",
    "start": "1007000",
    "end": "1013120"
  },
  {
    "text": "super simple scale object Easy peasy just with two different chrome triggers",
    "start": "1013120",
    "end": "1019360"
  },
  {
    "text": "for the demo Doesn't doesn't matter which trigger you use And if we check",
    "start": "1019360",
    "end": "1024480"
  },
  {
    "text": "the metrics if we go to the HPA we can see that there are two metrics with one",
    "start": "1024480",
    "end": "1029678"
  },
  {
    "text": "in average This is just a super example but if you want to add them instead of",
    "start": "1029679",
    "end": "1034959"
  },
  {
    "text": "executing a max between them because you are mixing some provider Q and another",
    "start": "1034959",
    "end": "1040319"
  },
  {
    "text": "provider Q whatever you want to apply HPA doesn't support it As Siniac explained you can go to your scale",
    "start": "1040319",
    "end": "1047280"
  },
  {
    "text": "object and you can define an advanced section scaling modifiers where you can",
    "start": "1047280",
    "end": "1054000"
  },
  {
    "text": "directly write down your custom formula and KDA will do the magic under the hood",
    "start": "1054000",
    "end": "1059440"
  },
  {
    "text": "to provide the scaling as you want In this case I'm going to say scaler a plus",
    "start": "1059440",
    "end": "1065360"
  },
  {
    "text": "instead of four plus",
    "start": "1065360",
    "end": "1071000"
  },
  {
    "text": " for 42 Easy peasy And I'm going to",
    "start": "1071000",
    "end": "1076480"
  },
  {
    "text": "apply it And now I'm going to check again the",
    "start": "1076480",
    "end": "1084000"
  },
  {
    "text": "HPA The same HPA has automatically been updated Now instead of of seeing two",
    "start": "1084000",
    "end": "1089520"
  },
  {
    "text": "triggers I see one composite trigger and the result of it is one + one for two",
    "start": "1089520",
    "end": "1096400"
  },
  {
    "text": "four is the value This is a super stupid example but it show the power of this",
    "start": "1096400",
    "end": "1101520"
  },
  {
    "text": "feature because for the previous case when I want to scale to zero I can just define zero as the chron and multiply",
    "start": "1101520",
    "end": "1108960"
  },
  {
    "text": "for the working hours that I want to scale or one for it or those kind of formulas The limit is your imagination",
    "start": "1108960",
    "end": "1117600"
  },
  {
    "text": "in this case Yeah perfect And also I would like to highlight that this feature is not useful only when you have multiple scalers or triggers only If you",
    "start": "1117600",
    "end": "1124799"
  },
  {
    "text": "have just one one trigger and you would like to just modify the modify the setting on this particular trigger you",
    "start": "1124799",
    "end": "1129919"
  },
  {
    "text": "can do it with with this uh with this powerful powerful engine So if you if you haven't u used it please try to",
    "start": "1129919",
    "end": "1136799"
  },
  {
    "text": "think about it because it's very powerful Okay So this was the demo right Demo No no no no no Enough Okay so this",
    "start": "1136799",
    "end": "1145360"
  },
  {
    "text": "was the best practices some just the from our observation in the community what people usually ask what is the",
    "start": "1145360",
    "end": "1151280"
  },
  {
    "text": "common issue and um this is all nice right autoscaling works everything is perfect ka is perfect kubernetes is",
    "start": "1151280",
    "end": "1158320"
  },
  {
    "text": "perfect right actually but it is not so with the autoscaling and uh especially",
    "start": "1158320",
    "end": "1165039"
  },
  {
    "text": "in a larger scale you can you can see uh you can see bunch of challenges and bunch of issues in the environment and I",
    "start": "1165039",
    "end": "1170799"
  },
  {
    "text": "will try to list a few of them so the first fun This is a really good good",
    "start": "1170799",
    "end": "1176440"
  },
  {
    "text": "quic actually and uh uh so as we discussed ka supports multiple different",
    "start": "1176440",
    "end": "1182559"
  },
  {
    "text": "uh event sources one of them is Prometheus and I would say this is one of the most popular scar that we have",
    "start": "1182559",
    "end": "1187840"
  },
  {
    "text": "out there because it's like a flexible it allows you to do a lot of lot of things under the hood because you have",
    "start": "1187840",
    "end": "1192960"
  },
  {
    "text": "already some monitoring solution into in your system right so why don't use the metrics to drive the scaling as well",
    "start": "1192960",
    "end": "1199200"
  },
  {
    "text": "this is good for a lot of cases this is good but there are certain scenarios I",
    "start": "1199200",
    "end": "1205520"
  },
  {
    "text": "would start the start with the first and the permits is just example It could be any other monitoring solution It could be data dog data trace Basically you",
    "start": "1205520",
    "end": "1212400"
  },
  {
    "text": "have the metrics stored somewhere outside the cluster and you are just pulling them So the first problem is if",
    "start": "1212400",
    "end": "1219120"
  },
  {
    "text": "you look at the diagram you can see that the workload is you know exposing some metrics and our Prometheus instance is",
    "start": "1219120",
    "end": "1224640"
  },
  {
    "text": "scraping them So first we need to send the metrics to Prometheus and then KDA can scrape those metrics from Prometheus",
    "start": "1224640",
    "end": "1231120"
  },
  {
    "text": "again So there is like this delay which could be matter of seconds maybe sometimes a minute and it depends on the",
    "start": "1231120",
    "end": "1237039"
  },
  {
    "text": "configuration For some cases this is completely okay because you are doing some uh some complex permitous query",
    "start": "1237039",
    "end": "1243679"
  },
  {
    "text": "that uh takes average over a long time But if you need more real time realtime traffic this is a problem because you",
    "start": "1243679",
    "end": "1249520"
  },
  {
    "text": "are introducing the delay for the scaling So it would be better to instead uh scrape those metrics directly from",
    "start": "1249520",
    "end": "1256000"
  },
  {
    "text": "the workload The other problem with this solution is that especially larger scale if you have many clusters and for",
    "start": "1256000",
    "end": "1262240"
  },
  {
    "text": "example single uh let's say um observability tool or prometers instance somewhere outside the cluster you are",
    "start": "1262240",
    "end": "1269120"
  },
  {
    "text": "you are causing lot of traffic outside the cluster and back into the cluster imagine you have application you have",
    "start": "1269120",
    "end": "1274480"
  },
  {
    "text": "the metric in the in the in the kubernetes cluster then you send the metric there in the somewhere in the wild and then back to keta sometimes it",
    "start": "1274480",
    "end": "1281280"
  },
  {
    "text": "could be uh cross zone traffic you know crosszone traffic is uh is expensive as well on a on on the call providers right",
    "start": "1281280",
    "end": "1288080"
  },
  {
    "text": "So you might you might cause a lot of lot of traffic unnecessary traffic into your system Other thing since you are",
    "start": "1288080",
    "end": "1295520"
  },
  {
    "text": "trying to trying to uh be more real time so you try to you know configure the scraping periods to be you know as low",
    "start": "1295520",
    "end": "1302159"
  },
  {
    "text": "as possible This is doable Maybe your prometers instance is capable of handling This is fine But once you start",
    "start": "1302159",
    "end": "1309039"
  },
  {
    "text": "overloading this primes instance you are starting getting uh connection times in Ka This is very very typical typical",
    "start": "1309039",
    "end": "1316480"
  },
  {
    "text": "issue we see from users that they are complaining okay we are getting lot of this context that line exceed it and",
    "start": "1316480",
    "end": "1322240"
  },
  {
    "text": "stuff like that So yeah thank you So and this is this is really issue and you should you should really think about",
    "start": "1322240",
    "end": "1328000"
  },
  {
    "text": "when you were designing your solution how you where you host the host the metrics The other thing which is kind of",
    "start": "1328000",
    "end": "1333200"
  },
  {
    "text": "related is that uh you are your application is scale based on some some metrics and these applications it could",
    "start": "1333200",
    "end": "1339360"
  },
  {
    "text": "be multiple different applications and they are all same talking to the same uh infrastructure piece It could be the",
    "start": "1339360",
    "end": "1344880"
  },
  {
    "text": "database or your service that you are hosting yourself You might actually buy this autoscaling stuff which is awesome",
    "start": "1344880",
    "end": "1351440"
  },
  {
    "text": "You might overload this database So you should think about really how to properly configure the database maybe",
    "start": "1351440",
    "end": "1356559"
  },
  {
    "text": "try to scale the infra somehow little bit or control how many replicas of each applications uh can can connect to the",
    "start": "1356559",
    "end": "1363520"
  },
  {
    "text": "same instance The other other thing cluster autoscalers because if you would like to",
    "start": "1363520",
    "end": "1368720"
  },
  {
    "text": "enable autoscaling of kubernetes it's important to correctly set up both port level autoscaling and the cluster",
    "start": "1368720",
    "end": "1374000"
  },
  {
    "text": "autoscaling It needs to be every time you know configured together because this is the key key to uh enable",
    "start": "1374000",
    "end": "1380919"
  },
  {
    "text": "dynamicity Cluster autoscaler is much slower than pot autoscaler because it take much longer to schedule a new node",
    "start": "1380919",
    "end": "1386960"
  },
  {
    "text": "So you should really think about how to how to mitigate this solution There are bunch of ways how we can do that It's a",
    "start": "1386960",
    "end": "1393440"
  },
  {
    "text": "little bit harder because cluster autoscalers they don't have any API uh for us to tell cluster autoscaler okay",
    "start": "1393440",
    "end": "1399200"
  },
  {
    "text": "let's start scaling out creating new node so this is a little bit harder because there is no APIs but there are",
    "start": "1399200",
    "end": "1404640"
  },
  {
    "text": "ways how to how to achieve that the other thing AI is everywhere right so you are hosting your or for",
    "start": "1404640",
    "end": "1412400"
  },
  {
    "text": "example you would you wouldn't like to host your host the models in in some service in for example open AI or",
    "start": "1412400",
    "end": "1418640"
  },
  {
    "text": "somewhere else you would like to host the models in your kubernetes cluster in your infra for compliance security",
    "start": "1418640",
    "end": "1424559"
  },
  {
    "text": "issues or maybe cost So autoscaling applies also to these workloads and it's",
    "start": "1424559",
    "end": "1429760"
  },
  {
    "text": "even more important because GPUs are expensive So we need to properly think about using the proper metrics to to",
    "start": "1429760",
    "end": "1435440"
  },
  {
    "text": "scale out the the applications to have the good latency and throughput I recommend you a good talk on this on",
    "start": "1435440",
    "end": "1440480"
  },
  {
    "text": "this on this problem They are using uh keta kurf to try to mitigate this problem and it's tomorrow uh the last",
    "start": "1440480",
    "end": "1447120"
  },
  {
    "text": "talk of the day Definitely recommend this stuff if you are interested in running your AI workloads on Kubernetes",
    "start": "1447120",
    "end": "1452480"
  },
  {
    "text": "Okay So for the road map uh we have a predictive scaling which is something we are trying to do in a long time What is",
    "start": "1452480",
    "end": "1458240"
  },
  {
    "text": "the what is the uh benefit Ka is getting lot of these metrics about your workloads So the idea is uh to to get",
    "start": "1458240",
    "end": "1466960"
  },
  {
    "text": "those metrics and then based on the patterns we see in those metrics anticipate the load So we can scale a",
    "start": "1466960",
    "end": "1472960"
  },
  {
    "text": "little bit in advance before the actual load is happening Uh also authentications the stuff we we",
    "start": "1472960",
    "end": "1478400"
  },
  {
    "text": "discussed before it's important So uh this is the stuff also we would like to extend and will continue",
    "start": "1478400",
    "end": "1484720"
  },
  {
    "text": "Yeah and we are in in talk with AVM just to extend new platforms uh more",
    "start": "1484720",
    "end": "1491520"
  },
  {
    "text": "platforms for KDA just to support more use cases and the but don't troll me Ah",
    "start": "1491520",
    "end": "1499200"
  },
  {
    "text": "trollers want to troll The last but not least the HTTP scaling All the things that we have explained fit really really",
    "start": "1499200",
    "end": "1505919"
  },
  {
    "text": "nice with asynchronous processes But when you are dealing with synchronous processes like HTTP request you need to",
    "start": "1505919",
    "end": "1512880"
  },
  {
    "text": "do the things in a different way You cannot just no the request is on the queue we will process it Not if you want",
    "start": "1512880",
    "end": "1519440"
  },
  {
    "text": "to scale h from zero We are in hurry Please pass the next slide",
    "start": "1519440",
    "end": "1526840"
  },
  {
    "text": " HR will contact you This is more or less the draft If",
    "start": "1526840",
    "end": "1532880"
  },
  {
    "text": "you see the blue the blue are the colors Yeah The green part is your application",
    "start": "1532880",
    "end": "1538559"
  },
  {
    "text": "The blue part is the client the the customer's calling And the red part is how KDA works Keta can interact in the",
    "start": "1538559",
    "end": "1546000"
  },
  {
    "text": "middle of your workload without modifying your workload So it's not so intrusive in terms of needing to change",
    "start": "1546000",
    "end": "1553440"
  },
  {
    "text": "all your deployment way just to use the HTTP uh the HTTP scaling features And",
    "start": "1553440",
    "end": "1559520"
  },
  {
    "text": "basically it will deploy the interceptor in the middle Yeah Yeah I'm on it So",
    "start": "1559520",
    "end": "1565760"
  },
  {
    "text": "thanks Welcome Yeah I would like to add that this is also applies to the stuff I",
    "start": "1565760",
    "end": "1570799"
  },
  {
    "text": "mentioned before because HTTP traffic is more um more you know it's import more",
    "start": "1570799",
    "end": "1576559"
  },
  {
    "text": "special Yeah And uh the real time aspect is more special because you have the timeouts and stuff like that So you need really need to be fast when you are",
    "start": "1576559",
    "end": "1582720"
  },
  {
    "text": "scaling based on the HTTP traffic So ending with the last demo If I go to the",
    "start": "1582720",
    "end": "1588320"
  },
  {
    "text": "HTTP demo there isn't any pot here serving my request But as soon as I",
    "start": "1588320",
    "end": "1595559"
  },
  {
    "text": "browse I could need please work Yeah it's working I can see",
    "start": "1595559",
    "end": "1601320"
  },
  {
    "text": "that Oh I open the ground chat I see the pot here So it has a scale and",
    "start": "1601320",
    "end": "1609360"
  },
  {
    "text": "when the warload is not needed it will just scale to zero after some time So if",
    "start": "1609360",
    "end": "1615039"
  },
  {
    "text": "I come here again and refresh I will see exactly the same behavior scaling out",
    "start": "1615039",
    "end": "1620640"
  },
  {
    "text": "reprocessing the request and scaling in saving money in the pro in the process",
    "start": "1620640",
    "end": "1626080"
  },
  {
    "text": "And it has been all Thanks everybody for joining us today for this session If you",
    "start": "1626080",
    "end": "1631919"
  },
  {
    "text": "have any question this is the moment If not if you are afraid about asking",
    "start": "1631919",
    "end": "1637039"
  },
  {
    "text": "publicly you can reach us during the process you can ping us by Slack by however",
    "start": "1637039",
    "end": "1645000"
  },
  {
    "text": "Thank you",
    "start": "1646919",
    "end": "1650480"
  },
  {
    "text": "Hi thank you That was really nice You mentioned something about cluster autoscaler and I was aware",
    "start": "1654799",
    "end": "1661919"
  },
  {
    "text": "I I know what KDA does maybe not so well after that presentation but you mentioned something about cluster",
    "start": "1661919",
    "end": "1668159"
  },
  {
    "text": "autoscaler How would Kada work with you know a concept like cluster to scaling Yeah So so we we have a P on that at",
    "start": "1668159",
    "end": "1676720"
  },
  {
    "text": "Kify We build a um we are using cluster API because cluster autoscalers",
    "start": "1676720",
    "end": "1682480"
  },
  {
    "text": "carpenter cluster autoscalers it doesn't have an API to tell it to add a new new node So what we did uh we have a cluster",
    "start": "1682480",
    "end": "1688640"
  },
  {
    "text": "API defined on a on a on a separate node group So you can run your carpenter or",
    "start": "1688640",
    "end": "1694240"
  },
  {
    "text": "cluster skill on the cluster you separate you create a separate uh node group and we can target this node group",
    "start": "1694240",
    "end": "1699679"
  },
  {
    "text": "to create new nodes based on those metrics all the metrics we discussed So you can specify for example I can see",
    "start": "1699679",
    "end": "1704880"
  },
  {
    "text": "this this the benefit is that the cross auto scale carpenter this kind of it's kind of reactive so it based on the",
    "start": "1704880",
    "end": "1712159"
  },
  {
    "text": "based on the resource uh utilization and schedulable pots that that's that's all yeah so we try to do that uh in a uh",
    "start": "1712159",
    "end": "1720000"
  },
  {
    "text": "faster way through through cluster API so you said that's in PLC right now yeah yeah we can we can talk about it later",
    "start": "1720000",
    "end": "1725760"
  },
  {
    "text": "on but we have we have PC on that thank you thank you very much anybody else has a question there is a",
    "start": "1725760",
    "end": "1733840"
  },
  {
    "text": "I'm sorry for for the guy",
    "start": "1733840",
    "end": "1738360"
  },
  {
    "text": "Hi a question regarding the interceptors for HTTP traffic Can uh you also",
    "start": "1740799",
    "end": "1747880"
  },
  {
    "text": "utilize existing interceptors like from a service mesh because I think the service mesh would be kind of",
    "start": "1747880",
    "end": "1754559"
  },
  {
    "text": "interceptor anyway So yes yes yes and no because for especially for the scale",
    "start": "1754559",
    "end": "1761840"
  },
  {
    "text": "from 0 to one you need to hold the first request and catch it and wait until the",
    "start": "1761840",
    "end": "1766960"
  },
  {
    "text": "application is scaled out which is bit harder with with the with an",
    "start": "1766960",
    "end": "1773840"
  },
  {
    "text": "interceptor any other question",
    "start": "1774919",
    "end": "1778799"
  },
  {
    "text": "anybody we have the he's sorry for that single",
    "start": "1781159",
    "end": "1787520"
  },
  {
    "text": "Mike do you want to take that one",
    "start": "1787640",
    "end": "1792440"
  },
  {
    "text": "Hi Uh I know that cluster autoscaler recently introduced something called provisioning requests Sorry can you hear",
    "start": "1792679",
    "end": "1799000"
  },
  {
    "text": "me Um cluster autoscaler recently introduced something called provisioning requests Um which I know like Q is using",
    "start": "1799000",
    "end": "1806480"
  },
  {
    "text": "to be to basically like submit uh upscales like before a job is running I'm curious if you've like thought about",
    "start": "1806480",
    "end": "1812720"
  },
  {
    "text": "integrating provisioning requests with Kada at all because you talked a little bit about like an API to like pre-upscale Um I know it's a little bit",
    "start": "1812720",
    "end": "1818720"
  },
  {
    "text": "more challenging with predictive scaling to Kada but I'm curious if you've explored provisioning requests at all",
    "start": "1818720",
    "end": "1825399"
  },
  {
    "text": "Yes Yes we we did but nothing nothing concrete at the moment but yeah we we we uh we we saw that Yeah So good good",
    "start": "1826000",
    "end": "1832000"
  },
  {
    "text": "input Yeah Yeah I I will let you do the mic Yeah",
    "start": "1832000",
    "end": "1837600"
  },
  {
    "text": "So you you mentioned earlier that Kada works also with other custom resources",
    "start": "1837600",
    "end": "1843200"
  },
  {
    "text": "other than deployments Is that right Yeah Yeah Uh so do you know a use case",
    "start": "1843200",
    "end": "1849360"
  },
  {
    "text": "where it is used for CI workloads for example with GitHub actions self-hosted",
    "start": "1849360",
    "end": "1855279"
  },
  {
    "text": "runners does it work well with those kind of situations So so you mean like the worker like in the GitHub uh like",
    "start": "1855279",
    "end": "1865080"
  },
  {
    "text": "a runners So I have GitHub action self for runners running as ports in my but",
    "start": "1865080",
    "end": "1872080"
  },
  {
    "text": "but they are managed with a custom resource Uh so can I use keta to",
    "start": "1872080",
    "end": "1878000"
  },
  {
    "text": "autoscale them The yeah the question is if KDA can manage custom resource",
    "start": "1878000",
    "end": "1884399"
  },
  {
    "text": "definition or can scale custom resources because there are he has a use case",
    "start": "1884399",
    "end": "1889440"
  },
  {
    "text": "where they use a custom resour an operator based on a custom resource to",
    "start": "1889440",
    "end": "1895279"
  },
  {
    "text": "spin up some workloads there The answer is yes Keta can scale whatever that",
    "start": "1895279",
    "end": "1900519"
  },
  {
    "text": "support/scale is an extension that you need to implement on your CRDs and as soon as your CRD implement SLS scale",
    "start": "1900519",
    "end": "1909039"
  },
  {
    "text": "easy pec will do it Doesn't matter which CRD is is the the affected one If it",
    "start": "1909039",
    "end": "1916000"
  },
  {
    "text": "implements last scale ka will do it any [Music]",
    "start": "1916000",
    "end": "1923920"
  },
  {
    "text": "other Uh thank you for the presentation Um does the HTTP autoscaler also take um",
    "start": "1924919",
    "end": "1931519"
  },
  {
    "text": "open websockets connections into account",
    "start": "1931519",
    "end": "1936360"
  },
  {
    "text": "I'm not fully sure right now because there is a in progress PR for supporting",
    "start": "1937120",
    "end": "1942799"
  },
  {
    "text": "is not merged yet No no it is not like the the HTTP I don't that we presented It's still alpha beta version I would",
    "start": "1942799",
    "end": "1949919"
  },
  {
    "text": "say So it is so the websockets are missing there Yeah",
    "start": "1949919",
    "end": "1955519"
  },
  {
    "text": "Yeah I remember it's not merge expecting end to end test before merging the feature Yep It's not true ported the",
    "start": "1955519",
    "end": "1962240"
  },
  {
    "text": "websocket yet The mic does the extensions are in in or",
    "start": "1962240",
    "end": "1969760"
  },
  {
    "text": "GPA GPC based stuff what what exactly the extensions are So runs inside the",
    "start": "1969760",
    "end": "1975279"
  },
  {
    "text": "kada binary or outside or add-ons what what how how the whole framework actual",
    "start": "1975279",
    "end": "1981440"
  },
  {
    "text": "extension framework works What which extension the scalers all of them are already built in",
    "start": "1981440",
    "end": "1988000"
  },
  {
    "text": "and you can extend them through a G a gRPC interface that we expose You can",
    "start": "1988000",
    "end": "1994720"
  },
  {
    "text": "use any built-in scaler the six the more than 70 scalers that we said are built",
    "start": "1994720",
    "end": "2000000"
  },
  {
    "text": "in and you can use it without any other change or you can develop your own gRPC",
    "start": "2000000",
    "end": "2006240"
  },
  {
    "text": "server exposing the interface that we bring and extend with any business logic",
    "start": "2006240",
    "end": "2011760"
  },
  {
    "text": "that you want if the current scalers and the scaling modifiers feature if they",
    "start": "2011760",
    "end": "2016960"
  },
  {
    "text": "don't match your own necessities you can even develop your own server and connect",
    "start": "2016960",
    "end": "2022640"
  },
  {
    "text": "it to KDA using your server to to manage the business logic and relying on KDA to",
    "start": "2022640",
    "end": "2028640"
  },
  {
    "text": "manage the Kubernetes part of the",
    "start": "2028640",
    "end": "2032480"
  },
  {
    "text": "integration So you manage the challenge uh of delay when you're scraping the the",
    "start": "2037480",
    "end": "2043519"
  },
  {
    "text": "matrix from Prometheus somewhere far from the workload Then what maybe I missed this during the",
    "start": "2043519",
    "end": "2050158"
  },
  {
    "text": "presentation but what's your recommendation to target this challenge Uh there is not a single recommendation",
    "start": "2050159",
    "end": "2056158"
  },
  {
    "text": "There are multiple ways how to you can do that Um one of the things is uh we",
    "start": "2056159",
    "end": "2062158"
  },
  {
    "text": "are also posing uh open telemetry scaler So instead of sending those metrics over",
    "start": "2062159",
    "end": "2069440"
  },
  {
    "text": "to the Prometheus uh we deploy small open telemetry collector and it directly",
    "start": "2069440",
    "end": "2075919"
  },
  {
    "text": "fetches the metrics from your workloads So it doesn't leave the the Kubernetes cluster and it's been directly pushed uh",
    "start": "2075919",
    "end": "2082398"
  },
  {
    "text": "pushed to Ka So this is one of the one of the solutions how you can avoid that Uh the other is maybe you can try the",
    "start": "2082399",
    "end": "2089358"
  },
  {
    "text": "different scaler already built in Ka that would fit the purpose better because the the Prometheus instance",
    "start": "2089359",
    "end": "2096079"
  },
  {
    "text": "might be overloaded And the last um last recommendation on this front is that we",
    "start": "2096079",
    "end": "2101440"
  },
  {
    "text": "have also a feature which is called uh matrix caching And this uh feature can",
    "start": "2101440",
    "end": "2107200"
  },
  {
    "text": "not improve the delay but it could little bit uh ease the load that's coming to the Prometheus because we can",
    "start": "2107200",
    "end": "2113440"
  },
  {
    "text": "reduce the number of requests coming to the Prometheus So uh I recommend using this one And about this uh auto",
    "start": "2113440",
    "end": "2120960"
  },
  {
    "text": "collector PLC when when this auto collector PLC is going to we have we",
    "start": "2120960",
    "end": "2126560"
  },
  {
    "text": "have it open in our in our repository like in the in the Kify repository and",
    "start": "2126560",
    "end": "2131760"
  },
  {
    "text": "we will be probably thinking about moving it into the Ka Ka organization if",
    "start": "2131760",
    "end": "2137040"
  },
  {
    "text": "we discuss that and it will be separate separate add-on basically so it won't be back in Keta but it will be there So if",
    "start": "2137040",
    "end": "2144160"
  },
  {
    "text": "you you know we can talk about this offline I suppose",
    "start": "2144160",
    "end": "2149400"
  },
  {
    "text": "Okay Okay There is a question right Do we have a one Yeah Do we have one more",
    "start": "2154000",
    "end": "2159119"
  },
  {
    "text": "mic Okay",
    "start": "2159119",
    "end": "2162519"
  },
  {
    "text": "Just wondering if uh you guys had plans for any more AWS services integrating with those",
    "start": "2165520",
    "end": "2172400"
  },
  {
    "text": "Do you have an example of of a service does not cover it",
    "start": "2172400",
    "end": "2177839"
  },
  {
    "text": "It's supported Is is there SQS You will ask it No just any others that you guys were working No it's this is usually",
    "start": "2177839",
    "end": "2185520"
  },
  {
    "text": "driven by users Yeah actually we don't have any road map but it's a communitydriven project If you have any",
    "start": "2185520",
    "end": "2192400"
  },
  {
    "text": "service that you would like to see supported just open a feature request The point is that the amount of",
    "start": "2192400",
    "end": "2197920"
  },
  {
    "text": "possibilities is almost infinite across all the services ac across all the projects So we implement them as soon as",
    "start": "2197920",
    "end": "2205520"
  },
  {
    "text": "someone actually need them instead of just implementing a thousand of them We",
    "start": "2205520",
    "end": "2211440"
  },
  {
    "text": "just implement on demand or even if you need it you can open the the the is the",
    "start": "2211440",
    "end": "2217520"
  },
  {
    "text": "the feature request and you can open a PR adding the support and it will be implemented for sure Okay So I think we",
    "start": "2217520",
    "end": "2225440"
  },
  {
    "text": "we need to we need to end here So if you have more questions feel free to reach out to us I'll be at Karify booth or CE",
    "start": "2225440",
    "end": "2231200"
  },
  {
    "text": "will be around the venue So thank you for for attendance [Applause]",
    "start": "2231200",
    "end": "2239079"
  }
]