[
  {
    "text": "well welcome everybody to Rook storage for kubernetes I'm impressed to see this many of you here late on a Friday",
    "start": "280",
    "end": "7080"
  },
  {
    "text": "afternoon so congratulations storage must be important to you",
    "start": "7080",
    "end": "13000"
  },
  {
    "text": "yes great so I'm Travis neelen U I'm with IBM storage team with the team we",
    "start": "13000",
    "end": "18880"
  },
  {
    "text": "were at Red Hat we moved over to IBM a couple years ago but ultimately uh one of the original Rook maintainers",
    "start": "18880",
    "end": "24599"
  },
  {
    "text": "creators of of the project and happy to talk about rook and seph today um and all about what we're doing with it so",
    "start": "24599",
    "end": "31359"
  },
  {
    "text": "blae yeah I'm bla Gardner I'm also an engineer on the IBM's uh data Foundation",
    "start": "31359",
    "end": "37120"
  },
  {
    "text": "storage team uh I've been working with the Rook project for seven years a maintainer for I think around six hi I'm",
    "start": "37120",
    "end": "43800"
  },
  {
    "text": "Anette and I was introduced to Rook by Travis about seven years ago and uh",
    "start": "43800",
    "end": "49719"
  },
  {
    "text": "I've been a Believer ever since and work also on the data Foundation effort in the IBM storage",
    "start": "49719",
    "end": "56199"
  },
  {
    "text": "Team all right let's let's dive in um so today we want to go through an",
    "start": "56199",
    "end": "61600"
  },
  {
    "text": "introduction you know what is Rook what features does it have why would you use it for your storage platform and get",
    "start": "61600",
    "end": "67960"
  },
  {
    "text": "into some details some interesting things around well what should you consider about in your topology when you're deploying uh after youve deployed",
    "start": "67960",
    "end": "74640"
  },
  {
    "text": "what are some maintenance best practices and then what does the project health and the community look like U we'll get",
    "start": "74640",
    "end": "81040"
  },
  {
    "text": "into some new features for our latest release 1.15 which was uh a couple months ago and then our road map and",
    "start": "81040",
    "end": "87439"
  },
  {
    "text": "features we're kind of working on for our next release 1 16 to finish it off we'll have a nice topic by Anette on",
    "start": "87439",
    "end": "94200"
  },
  {
    "text": "application disaster recovery so just to get an idea of who's in the audience here could you raise",
    "start": "94200",
    "end": "100600"
  },
  {
    "text": "your hand if you're learning about Rook for the first time anybody okay we got a bunch great um who's experimented with",
    "start": "100600",
    "end": "107079"
  },
  {
    "text": "Rook we've got a a number who's running rook in production and also a bunch and if for",
    "start": "107079",
    "end": "113479"
  },
  {
    "text": "the real die hards who is going to sephon next month in Geneva okay I'll be there if you're",
    "start": "113479",
    "end": "119479"
  },
  {
    "text": "there be happy to see you there okay so introduction to Rook um",
    "start": "119479",
    "end": "124799"
  },
  {
    "text": "going back to history long time ago even before kubernetes was a big thing over eight years ago I was with a team where",
    "start": "124799",
    "end": "131760"
  },
  {
    "text": "we were looking at Cloud native storage we wanted to say what what really should storage look like what do we want to take a bet on and we saw that cloud",
    "start": "131760",
    "end": "139519"
  },
  {
    "text": "providers of course have their storage platforms but what about your own data center what do you do for storage do you",
    "start": "139519",
    "end": "144599"
  },
  {
    "text": "have to you have to buy somebody else's Appliance and plug it into your data center so traditionally in that fashion",
    "start": "144599",
    "end": "151400"
  },
  {
    "text": "is not part of your cluster you buy it you plug it in but why does it have to be external to to kubernetes why not",
    "start": "151400",
    "end": "157800"
  },
  {
    "text": "manage storage as any other kubernetes application so you can have similar",
    "start": "157800",
    "end": "164000"
  },
  {
    "text": "maintenance um patterns so we said okay we're going to",
    "start": "164000",
    "end": "169239"
  },
  {
    "text": "do this we're going to have storage for kubernetes but what storage platform do we want to trust what are we going to going to take a a bet on I was not with",
    "start": "169239",
    "end": "176599"
  },
  {
    "text": "the the seph team at the time but we we looked at seph and and I was not at the same company or anything just looking at",
    "start": "176599",
    "end": "182360"
  },
  {
    "text": "open source we wanted a solution that was open source that was Enterprise ready already running in production we",
    "start": "182360",
    "end": "188560"
  },
  {
    "text": "didn't want to go write a new storage platform and we after looking at Seth we decided okay this is this is where we",
    "start": "188560",
    "end": "195519"
  },
  {
    "text": "want to go and I'll explain more about why that is in the coming",
    "start": "195519",
    "end": "201040"
  },
  {
    "text": "slides so as we started creating Rook so what is it so it it really brings Seth",
    "start": "201040",
    "end": "206799"
  },
  {
    "text": "storage into your kubernetes cluster Rook will manage the SE storage with an",
    "start": "206799",
    "end": "212200"
  },
  {
    "text": "operator and and crds so you can so you define with your the crds how you want",
    "start": "212200",
    "end": "217599"
  },
  {
    "text": "to deploy storage in your cluster and Rook will go make it happen Rook will automate the deployment configuration",
    "start": "217599",
    "end": "223480"
  },
  {
    "text": "all the upgrades to keep your data online and available even during upgrades and then your applications will",
    "start": "223480",
    "end": "230840"
  },
  {
    "text": "get to consume the storage just like any other um storage with CSI with storage classes PVCs there's a CSI driver to",
    "start": "230840",
    "end": "238519"
  },
  {
    "text": "hook it all up so it looks to your applications just like any other storage platform um Rook is open source based on",
    "start": "238519",
    "end": "247079"
  },
  {
    "text": "Apache 2. um and seph is also open source so the entire stack the storage platform is",
    "start": "247079",
    "end": "254480"
  },
  {
    "text": "open source so now what is seph and why did we take a beted on it so seph is a",
    "start": "254480",
    "end": "261000"
  },
  {
    "text": "distributed software defined storage solution and it really fits a lot of the",
    "start": "261000",
    "end": "266320"
  },
  {
    "text": "kubernetes paradigms where it's distributed it needs to scale it needs to be ready for",
    "start": "266320",
    "end": "271520"
  },
  {
    "text": "Enterprises and SE advantages of seph it's allinone open- Source platform so",
    "start": "271520",
    "end": "277960"
  },
  {
    "text": "we have seph RBD that's gives us block storage usually used with read write once",
    "start": "277960",
    "end": "284240"
  },
  {
    "text": "volumes you have sefs which gives you a shared file system usually for read",
    "start": "284240",
    "end": "289400"
  },
  {
    "text": "write many volumes and then also to top it off object storage with st rgw the",
    "start": "289400",
    "end": "294919"
  },
  {
    "text": "Ratt Gateway which gives you access to S3 buckets uh the yeah the GW is um is",
    "start": "294919",
    "end": "301680"
  },
  {
    "text": "S3 compliant and feature for feature it it keeps up with S3 pretty",
    "start": "301680",
    "end": "308240"
  },
  {
    "text": "well uh again it's open source and Seth itself was first deployed in production",
    "start": "308240",
    "end": "314440"
  },
  {
    "text": "and first stable in July 2012 so what are we over 12 years now one of the first big customers was CERN which is",
    "start": "314440",
    "end": "321840"
  },
  {
    "text": "where sephon will be next month with running in their you know their collider just terabytes and terabytes of data I'm",
    "start": "321840",
    "end": "329000"
  },
  {
    "text": "not sure how many curious to look forward to to see what they have to tell us next month on how big their data set",
    "start": "329000",
    "end": "334120"
  },
  {
    "text": "really is um but again seph is really designed for scalability so terab there",
    "start": "334120",
    "end": "339240"
  },
  {
    "text": "are many production clusters running multiple and many terabytes of",
    "start": "339240",
    "end": "344919"
  },
  {
    "text": "data SEF is designed for durability so SEF storage when you put data in SEF",
    "start": "344919",
    "end": "351360"
  },
  {
    "text": "it's consistent as opposed to eventually consistent with some other storage platforms so as soon as the storage acts",
    "start": "351360",
    "end": "358800"
  },
  {
    "text": "that it's stored it is stored and safe the data is sharded by SEF across azs",
    "start": "358800",
    "end": "364479"
  },
  {
    "text": "racks nodes you can choose your failure domain and how you want SE to spread that",
    "start": "364479",
    "end": "369960"
  },
  {
    "text": "data um and we'll talk about what that means for resiliency a little bit later",
    "start": "369960",
    "end": "376880"
  },
  {
    "text": "so um even in extreme disasters the data can be recovered manually as well in",
    "start": "376880",
    "end": "383280"
  },
  {
    "text": "addition to the replication mode eraser coding is also supported by SEF it's most most commonly used for object",
    "start": "383280",
    "end": "389080"
  },
  {
    "text": "storage but it also works with block and shared file system where performance isn't as",
    "start": "389080",
    "end": "396120"
  },
  {
    "text": "critical so just to kind of understand how the pieces are are fitting together here so Rook again is the operator that",
    "start": "396120",
    "end": "402720"
  },
  {
    "text": "really deploys and and manages Seth so seph for those of you who who know it",
    "start": "402720",
    "end": "408560"
  },
  {
    "text": "well it's not known for its simple management it is a rather complex storage system so from the start rook's",
    "start": "408560",
    "end": "415479"
  },
  {
    "text": "goal was to make seph as simple as possible it may not be the simplest still but but we do our best and we make",
    "start": "415479",
    "end": "421440"
  },
  {
    "text": "it so you can deploy and have have hope to manage stuff the C there's a CSI driver then",
    "start": "421440",
    "end": "428560"
  },
  {
    "text": "that helps your applications provision and mount the storage into your your application pods but at the end of the",
    "start": "428560",
    "end": "434840"
  },
  {
    "text": "day the the core data layer is seph uh providing that direct storage at that",
    "start": "434840",
    "end": "441919"
  },
  {
    "text": "layer so so how do you install Rook how do you get started there are Helm charts",
    "start": "441919",
    "end": "447240"
  },
  {
    "text": "uh there or if you like to to work with the Amo directly there are just example manifests for many different",
    "start": "447240",
    "end": "453400"
  },
  {
    "text": "configurations there's a quick start guide go to our website r.o and and get",
    "start": "453400",
    "end": "459520"
  },
  {
    "text": "started so where would you install Rook where would you really it's wherever you need storage which is wherever",
    "start": "460240",
    "end": "466360"
  },
  {
    "text": "kubernetes runs right where when we started the project we thought well we want to run storage in the most common",
    "start": "466360",
    "end": "472840"
  },
  {
    "text": "scenario in your own data center because there is no other storage option But ultimately we'll see there are Cloud",
    "start": "472840",
    "end": "479000"
  },
  {
    "text": "scenarios too even to to enhance the cloud provider storage But ultimately it",
    "start": "479000",
    "end": "484919"
  },
  {
    "text": "can be in the cloud on Prem know virtual bare metal hardware any off-the-shelf commodity Hardware seph is designed to",
    "start": "484919",
    "end": "491080"
  },
  {
    "text": "work with uh the and the underlying storage so we basically take local",
    "start": "491080",
    "end": "496639"
  },
  {
    "text": "storage or um or Cloud volumes even loop back devices for testing we take those",
    "start": "496639",
    "end": "501800"
  },
  {
    "text": "individual devices and a SE Aggregates them for for the storage",
    "start": "501800",
    "end": "507840"
  },
  {
    "text": "platform so now why would we deploy a rook in the cloud well there Cloud providers have certain limitations that",
    "start": "507840",
    "end": "515479"
  },
  {
    "text": "that make it make certain scenarios difficult for example um storage not being available across az's or the",
    "start": "515479",
    "end": "522440"
  },
  {
    "text": "number of PVS per note is limited to 32 I don't know if that's still the number but there's some limit typically uh or",
    "start": "522440",
    "end": "530120"
  },
  {
    "text": "if you have small PVS they have poor performance so seph helps you and Rook helps you overcome these by you put seph",
    "start": "530120",
    "end": "537480"
  },
  {
    "text": "on top of um off on top of your cloud storage you can have virtually unlimited",
    "start": "537480",
    "end": "543880"
  },
  {
    "text": "PVS you can have the data available across az's so your your applications",
    "start": "543880",
    "end": "549600"
  },
  {
    "text": "can be rescheduled across az's no problem um and again and if you provision Rook with the these larger PVS",
    "start": "549600",
    "end": "557399"
  },
  {
    "text": "under the covers you'll get the performance benefits of of the larger PVS even if you have hundreds or",
    "start": "557399",
    "end": "562680"
  },
  {
    "text": "thousands of smaller PVS on top of it and and it is a consistent storage",
    "start": "562680",
    "end": "568760"
  },
  {
    "text": "platform then if you worked with mul multiple clouds you could have similar support across",
    "start": "568760",
    "end": "575720"
  },
  {
    "text": "them so a bit about you how to consider do I need dedicated nodes do I need a",
    "start": "575720",
    "end": "581440"
  },
  {
    "text": "dedicated environment how do I do this is it really safe to run it with my applications in the same cluster I know",
    "start": "581440",
    "end": "587160"
  },
  {
    "text": "I've Heard lots of comments like is that a good idea or not well by default we have our hypercon converge mode which",
    "start": "587160",
    "end": "594360"
  },
  {
    "text": "does um where you say okay just run it in the same cluster it runs on the same nodes with your",
    "start": "594360",
    "end": "599920"
  },
  {
    "text": "applications and you can set resource requests and limits on on the rook and",
    "start": "599920",
    "end": "605040"
  },
  {
    "text": "and Seth pods so and also in your applications so that each of them get get their the resources that that they",
    "start": "605040",
    "end": "611800"
  },
  {
    "text": "need and as long as everybody has resources set really they can run together and and it works well in you",
    "start": "611800",
    "end": "619600"
  },
  {
    "text": "know in all the scenarios I've heard of where things are properly configured like that um but in higher performance",
    "start": "619600",
    "end": "627000"
  },
  {
    "text": "applications or where you really want to make sure there's never a chance the the Noisy Neighbor problem with applications",
    "start": "627000",
    "end": "633040"
  },
  {
    "text": "will affect your storage uh you in the same kubernetes cluster well just have",
    "start": "633040",
    "end": "638639"
  },
  {
    "text": "some dedicated storage nodes you can taint them so the applications don't run on them and then you can tell Rook okay",
    "start": "638639",
    "end": "644360"
  },
  {
    "text": "or only run the the SEF storage on these dedicated storage nodes and you can",
    "start": "644360",
    "end": "650360"
  },
  {
    "text": "ensure the storage has absolute absolutely dedicated resources to run",
    "start": "650360",
    "end": "655839"
  },
  {
    "text": "there and never be impacted by applications the third mode to consider is well what",
    "start": "655839",
    "end": "663200"
  },
  {
    "text": "if I really don't want to run the storage platform in kubernetes that's great it does have advantages for being",
    "start": "663200",
    "end": "669079"
  },
  {
    "text": "separate which is uh you can run the storage platform completely outside of",
    "start": "669079",
    "end": "674279"
  },
  {
    "text": "kubernetes if you have like a RF cluster you can manage it separately from Rook",
    "start": "674279",
    "end": "679519"
  },
  {
    "text": "or kubernetes and that allows you to connect kubernetes to it and you can you can even connect",
    "start": "679519",
    "end": "686680"
  },
  {
    "text": "multiple kues clusters to the same storage cluster so here's a diagram of what that might look like for an",
    "start": "686680",
    "end": "692920"
  },
  {
    "text": "external cluster so in your kubernetes cluster really From rook's perspective all you have is the operator and that",
    "start": "692920",
    "end": "700079"
  },
  {
    "text": "deploys the CSI driver and simplifies that the CSI driver and it's just a simple connection that then helps you",
    "start": "700079",
    "end": "706560"
  },
  {
    "text": "connect to the external Z cluster with the same storage classes and PVCs and",
    "start": "706560",
    "end": "711959"
  },
  {
    "text": "mechanisms that that you're already that you're already familiar with and again you can connect multiple of client",
    "start": "711959",
    "end": "717800"
  },
  {
    "text": "clusters to this and also even though it's one SEF cluster you can isolate the",
    "start": "717800",
    "end": "723160"
  },
  {
    "text": "storage so to make it multi-tenant so seph has mechanisms for making sure U",
    "start": "723160",
    "end": "729880"
  },
  {
    "text": "that clients are isolated they've got what they call ratos name spaces and and some sub volume groups other",
    "start": "729880",
    "end": "737199"
  },
  {
    "text": "Concepts so what does a CSI driver provide so that whenever you provision a",
    "start": "737199",
    "end": "742240"
  },
  {
    "text": "volume in se you get a thinly provisioned volume meaning you you can tell it a size and the size can can grow",
    "start": "742240",
    "end": "749839"
  },
  {
    "text": "you can expand the volume they're very Dynamic that way if as the Seth cluster",
    "start": "749839",
    "end": "755880"
  },
  {
    "text": "runs low on Space you can add more storage to Seth itself the CSI driver is topology aware",
    "start": "755880",
    "end": "763240"
  },
  {
    "text": "so if you're if you have azs you can read the data from the nearest client and then what you consider",
    "start": "763240",
    "end": "770120"
  },
  {
    "text": "standard CSI Behavior with snapshots and cloning dealing with the uh provisioning",
    "start": "770120",
    "end": "775360"
  },
  {
    "text": "FML volumes and another feature coming soon is group snapshots or the ability",
    "start": "775360",
    "end": "781800"
  },
  {
    "text": "to say I want these volumes to be snapshotted at the same",
    "start": "781800",
    "end": "787160"
  },
  {
    "text": "time along with CSI we have some add-ons which are really more advanced features",
    "start": "787440",
    "end": "793000"
  },
  {
    "text": "to to help enhance those some of these features are for um for Dr and mirroring",
    "start": "793000",
    "end": "798399"
  },
  {
    "text": "which will Annette will talk more about and some other things reclaiming space handling offline nodes and yeah volume",
    "start": "798399",
    "end": "806760"
  },
  {
    "text": "replication another feature is so with with rwo and rwx volumes block and",
    "start": "806760",
    "end": "813560"
  },
  {
    "text": "file you're used to provisioning them with a CSI driver mounting it to your pod well what about for the S3 endpoint",
    "start": "813560",
    "end": "820199"
  },
  {
    "text": "when you're using object storage there Kon doesn't have this pattern well the",
    "start": "820199",
    "end": "825600"
  },
  {
    "text": "if you're familiar with the kazy effort container object storage interface that is an effort to to define a standard way",
    "start": "825600",
    "end": "831240"
  },
  {
    "text": "in kubernetes of accessing buckets and provisioning them uh so this is currently experimental Rook has a full",
    "start": "831240",
    "end": "837600"
  },
  {
    "text": "implementation um and another way of doing it which we've had for longer is the object bucket claims which is",
    "start": "837600",
    "end": "844079"
  },
  {
    "text": "basically just way a way of saying I've got a storage class let me provision a bucket instead of the the normal volume",
    "start": "844079",
    "end": "850480"
  },
  {
    "text": "and then you can have access to the the bucket from your application",
    "start": "850480",
    "end": "856040"
  },
  {
    "text": "pod another topic on maintenance things to consider as far as data resiliency",
    "start": "856360",
    "end": "862759"
  },
  {
    "text": "and well I need to do upgrades I need to upgrade the nodes of my kubernetes cluster how will that affect the storage",
    "start": "862759",
    "end": "868680"
  },
  {
    "text": "if it's running in the same cluster so if we assume you have three",
    "start": "868680",
    "end": "874759"
  },
  {
    "text": "generally three um failure domains say they're azs any one of those three azs",
    "start": "874759",
    "end": "882079"
  },
  {
    "text": "can be taken down and all data remains available and for both reads and writes",
    "start": "882079",
    "end": "887240"
  },
  {
    "text": "so your cluster is online even while you're maintaining one entire a we use pdbs the Pod disruption budgets",
    "start": "887240",
    "end": "895160"
  },
  {
    "text": "to ensure that the node drains are aware of what they can drain and what can't",
    "start": "895160",
    "end": "901360"
  },
  {
    "text": "can't be drained if you try and drain a node that has that would make your storage unavailable the pdb says Nope",
    "start": "901360",
    "end": "907519"
  },
  {
    "text": "you better uh you better hold off on that uh The Rook up upgrades ourselves",
    "start": "907519",
    "end": "913959"
  },
  {
    "text": "we do ensure only one failure domain is affected at a time to to do our best for no",
    "start": "913959",
    "end": "919440"
  },
  {
    "text": "downtime and then kind of a the worst case scenario if disaster hits you power",
    "start": "919440",
    "end": "925000"
  },
  {
    "text": "loss to to everything you have running if the entire cluster goes down just be",
    "start": "925000",
    "end": "931000"
  },
  {
    "text": "be just know that data is safe as long as you can bring up at least one of those failure domains back so because",
    "start": "931000",
    "end": "938480"
  },
  {
    "text": "the the data is replicated to each each failure domain just bring one of them back you don't need all three you can",
    "start": "938480",
    "end": "944360"
  },
  {
    "text": "get the data back and coup cutle plugin",
    "start": "944360",
    "end": "949600"
  },
  {
    "text": "troubleshooting and Mt maintenance does require things that don't really fit the crd pattern very well but you need",
    "start": "949600",
    "end": "955880"
  },
  {
    "text": "oneoff um status tell me the status do this oneoff operation remove",
    "start": "955880",
    "end": "962240"
  },
  {
    "text": "osds uh do things with monum some of these Advanced operations we do have this CBE cutle plugin that that helps",
    "start": "962240",
    "end": "968560"
  },
  {
    "text": "with those scenarios and we'd love feedback in this area too what would you like to see automated that are manual",
    "start": "968560",
    "end": "974120"
  },
  {
    "text": "tasks today and now I'll turn the time over to",
    "start": "974120",
    "end": "980120"
  },
  {
    "text": "Blaine yeah I'm blae and I'd love to talk to you about our community so I mean being an open source project our",
    "start": "982880",
    "end": "989560"
  },
  {
    "text": "community is really important to us um I think out of all the stuff on this Slide",
    "start": "989560",
    "end": "995319"
  },
  {
    "text": "the thing that I'm and we are most excited excited about is um having a new",
    "start": "995319",
    "end": "1000639"
  },
  {
    "text": "contributing company stepping in so uh klyo um is has really been contributing",
    "start": "1000639",
    "end": "1008440"
  },
  {
    "text": "some development uh lately and it's been really fantastic having",
    "start": "1008440",
    "end": "1014360"
  },
  {
    "text": "them um in addition to the three companies that have been involved for a while while helping with uh uh",
    "start": "1014360",
    "end": "1021120"
  },
  {
    "text": "maintainership as well uh Rook is a graduated project",
    "start": "1021120",
    "end": "1026558"
  },
  {
    "text": "we've been graduated for four years now um and Rook declared itself stable six",
    "start": "1026559",
    "end": "1033319"
  },
  {
    "text": "years ago uh in 2018 um that stability is and and has",
    "start": "1033319",
    "end": "1040360"
  },
  {
    "text": "been our number one Focus uh so we have I I could not even",
    "start": "1040360",
    "end": "1045880"
  },
  {
    "text": "tell you how many Upstream users running in production um and you know Downstream from a",
    "start": "1045880",
    "end": "1052280"
  },
  {
    "text": "corporate sense we have tons and tons of Downstream deployments that are all running very",
    "start": "1052280",
    "end": "1058440"
  },
  {
    "text": "successfully um we've had users come up and talk to us at The Rook booth and some of the anecdotes that I think are",
    "start": "1058440",
    "end": "1064240"
  },
  {
    "text": "are really amazing or we have people telling us that their Rook clusters run great on nvme with infiniband or just",
    "start": "1064240",
    "end": "1073160"
  },
  {
    "text": "some spare old desktops with 100 USB drives",
    "start": "1073160",
    "end": "1079480"
  },
  {
    "text": "uh we try to release about every six months uh our last release was in August our next release is going to be in uh",
    "start": "1080280",
    "end": "1087480"
  },
  {
    "text": "just next month in December and we try to have regular patch updates just so",
    "start": "1087480",
    "end": "1093080"
  },
  {
    "text": "whatever is the latest we have going on everyone can get those every two weeks and we do put those out uh more urgently",
    "start": "1093080",
    "end": "1100360"
  },
  {
    "text": "if there is need um uh for those project updates in",
    "start": "1100360",
    "end": "1106280"
  },
  {
    "text": "the latest in the most recent release one 15 uh we officially welcome support for",
    "start": "1106280",
    "end": "1112480"
  },
  {
    "text": "seph's latest version named squid and we also have some just kind of",
    "start": "1112480",
    "end": "1118640"
  },
  {
    "text": "quality of life improvements around uh day two uh disk parameter changes uh and",
    "start": "1118640",
    "end": "1125200"
  },
  {
    "text": "we uh had some community community members I'm sorry contribute uh Keystone",
    "start": "1125200",
    "end": "1130400"
  },
  {
    "text": "and Swift integration um so this is primarily beneficial for those running open stack especially if you're trying",
    "start": "1130400",
    "end": "1135960"
  },
  {
    "text": "to run Open Stacks like control plane in kubernets uh We've also begun uh experimenting",
    "start": "1135960",
    "end": "1143240"
  },
  {
    "text": "with uh kind of pulling out the CSI uh operations out of Rook itself into its",
    "start": "1143240",
    "end": "1149200"
  },
  {
    "text": "own operator so that we can hopefully give people better flexibility around what they're doing uh with rook and with",
    "start": "1149200",
    "end": "1156360"
  },
  {
    "text": "CSI um if you're a multis user uh I don't expect too many of you uh but",
    "start": "1156360",
    "end": "1161440"
  },
  {
    "text": "we've had some feedback about uh just like maintenance challenges Ono with",
    "start": "1161440",
    "end": "1166480"
  },
  {
    "text": "upgrades around uh what we've called a holder pods uh so we began the deprecation process for those um and",
    "start": "1166480",
    "end": "1173559"
  },
  {
    "text": "we're also beginning some work in 1.5 on uh Object Store pool",
    "start": "1173559",
    "end": "1180799"
  },
  {
    "text": "placements um looking forward to the next release and next releases uh I think the overall theme that we're",
    "start": "1181280",
    "end": "1187520"
  },
  {
    "text": "seeing is kind of starting to see interest in and desired support for",
    "start": "1187520",
    "end": "1193640"
  },
  {
    "text": "users with more complex needs they really do want to like get back some of the complexity of uh seph's like just",
    "start": "1193640",
    "end": "1202200"
  },
  {
    "text": "infinite configurability um one of the things we have specifically on the docket is uh",
    "start": "1202200",
    "end": "1209080"
  },
  {
    "text": "mirroring for ratos Block pool name spaces um so this is something that like can tie into Disaster Recovery",
    "start": "1209080",
    "end": "1216200"
  },
  {
    "text": "efforts um and we're seeing a lot more interest in and push behind object",
    "start": "1216200",
    "end": "1222120"
  },
  {
    "text": "storage um so two of the many things going on with object storage are support",
    "start": "1222120",
    "end": "1227520"
  },
  {
    "text": "for S3 storage classes those are distinct from kubernetes Storage classes um so don't",
    "start": "1227520",
    "end": "1234080"
  },
  {
    "text": "get tripped up there like we all do um and also adding more uh auditing and",
    "start": "1234080",
    "end": "1239200"
  },
  {
    "text": "logging for S3 accesses so as administrators you can keep track of what users are doing with",
    "start": "1239200",
    "end": "1245080"
  },
  {
    "text": "S3 uh the holder pods I mentioned will be fully deprecated probably with",
    "start": "1245080",
    "end": "1250559"
  },
  {
    "text": "116 and uh also uh seph's two releases back v19 Quincy is nearing the end of",
    "start": "1250559",
    "end": "1257559"
  },
  {
    "text": "its life so we'll be stopping support for that uh but now I'm really excited to pass it off to",
    "start": "1257559",
    "end": "1264000"
  },
  {
    "text": "Anette thank you BL okay so what I want to do um is sort",
    "start": "1264000",
    "end": "1270880"
  },
  {
    "text": "of show you how rook and SEF can integrate into an actual uh solution and",
    "start": "1270880",
    "end": "1277039"
  },
  {
    "text": "the solution we want to talk about is disaster recovery at an application Level so the first thing is to um",
    "start": "1277039",
    "end": "1284760"
  },
  {
    "text": "discuss sort of the measurements of disaster recovery so we have the um recovery Point objective and we",
    "start": "1284760",
    "end": "1292400"
  },
  {
    "text": "have the recovery time objective so the recovery Point objective is how much",
    "start": "1292400",
    "end": "1297520"
  },
  {
    "text": "data can I afford to lose if this application uh goes down so that is",
    "start": "1297520",
    "end": "1303919"
  },
  {
    "text": "usually measured in time so you might say my you know I mean there is a case",
    "start": "1303919",
    "end": "1309120"
  },
  {
    "text": "for RPO equal to zero uh and that can be designed but there's a lot of",
    "start": "1309120",
    "end": "1315279"
  },
  {
    "text": "limitations so um recovery time OB itive even if RPO is equal to zero meaning you",
    "start": "1315279",
    "end": "1321279"
  },
  {
    "text": "don't lose any data it's still going to take time unless you have a highly available application to recover that",
    "start": "1321279",
    "end": "1329279"
  },
  {
    "text": "application so in terms of how rook and and seph support um and I'm going to",
    "start": "1329279",
    "end": "1336799"
  },
  {
    "text": "speak about asynchronous replication um seph just out of the box already does",
    "start": "1336799",
    "end": "1342240"
  },
  {
    "text": "synchronous replication but what I'm talking about here is asynchronous replication between peer CL kubernetes",
    "start": "1342240",
    "end": "1349640"
  },
  {
    "text": "clusters so we have a couple crds that stf offers that rook orchestrates and um",
    "start": "1349640",
    "end": "1356720"
  },
  {
    "text": "keeps track of we have the SEF RBD mirror this is going to create a Damon",
    "start": "1356720",
    "end": "1362039"
  },
  {
    "text": "that is used to do the replication on a volume level usually and then we can we're going to",
    "start": "1362039",
    "end": "1369320"
  },
  {
    "text": "uh in Cube objects these crds now are available which are volume replication",
    "start": "1369320",
    "end": "1375559"
  },
  {
    "text": "and volume replication class so the volume application what it does is it",
    "start": "1375559",
    "end": "1381159"
  },
  {
    "text": "will go ahead instead of you having to go into SEF and do the SEF commands to enable a volume for mirring or an image",
    "start": "1381159",
    "end": "1388679"
  },
  {
    "text": "for mirring you're going to instead use a volume replication so there'll be a",
    "start": "1388679",
    "end": "1394039"
  },
  {
    "text": "volume replication um resource for every volume that the application is using and these",
    "start": "1394039",
    "end": "1400760"
  },
  {
    "text": "are for Block volumes so the volume replication class is going to specify",
    "start": "1400760",
    "end": "1406039"
  },
  {
    "text": "the replication interval so you might you know have a f minute interval of",
    "start": "1406039",
    "end": "1411600"
  },
  {
    "text": "which you're going to replicate the Delta Data for each image we also have SEF uh file system",
    "start": "1411600",
    "end": "1419480"
  },
  {
    "text": "mirring and it's it's similar um by default it is not enabled um so and it",
    "start": "1419480",
    "end": "1428279"
  },
  {
    "text": "doesn't currently have any any Cube resources like the volume replication that enable it but um you can in SEF",
    "start": "1428279",
    "end": "1436400"
  },
  {
    "text": "configure it and this is all documented in the Rook documentation similar to block volumes",
    "start": "1436400",
    "end": "1443360"
  },
  {
    "text": "you have a SEF file system mirror and um you know there's there's quite a few",
    "start": "1443360",
    "end": "1449120"
  },
  {
    "text": "steps right now to set the bootstrapping up but once you get the mirring there then you know you can go ahead and start",
    "start": "1449120",
    "end": "1455679"
  },
  {
    "text": "um replicating file volumes this is experimental so uh we don't you know",
    "start": "1455679",
    "end": "1461640"
  },
  {
    "text": "like I said it's fully documented you're certainly welcome to try it Downstream we have not um introduced this into",
    "start": "1461640",
    "end": "1468559"
  },
  {
    "text": "stream yet we have a different way of replicating file volumes using uh V Sync",
    "start": "1468559",
    "end": "1474120"
  },
  {
    "text": "right now Downstream so once you've got your your",
    "start": "1474120",
    "end": "1479200"
  },
  {
    "text": "replication set up you got your replication interval then you're ready um to basically apply a policy and be",
    "start": "1479200",
    "end": "1487720"
  },
  {
    "text": "able to do a fail over or a fail back the difference between a failover and a",
    "start": "1487720",
    "end": "1492799"
  },
  {
    "text": "fail back is a failover assumes that one of your kubernetes clusters or at Le Le",
    "start": "1492799",
    "end": "1498600"
  },
  {
    "text": "the application on that cluster is no longer accessible you know you don't",
    "start": "1498600",
    "end": "1503679"
  },
  {
    "text": "know if it's running but you cannot get to it it doesn't appear to be running so that's when you're going to issue a um",
    "start": "1503679",
    "end": "1511880"
  },
  {
    "text": "basically use a a volume replication group um to go ahead and initiate that",
    "start": "1511880",
    "end": "1519640"
  },
  {
    "text": "failover it's going to demote the storage well actually in a failover it won't demote because it can't talk to it",
    "start": "1519640",
    "end": "1526320"
  },
  {
    "text": "but it's going to promote the image that's sitting on the alternate cluster to be primary and remember that image",
    "start": "1526320",
    "end": "1533399"
  },
  {
    "text": "has been updated based on the replication interval so it's sitting there waiting to become",
    "start": "1533399",
    "end": "1540760"
  },
  {
    "text": "primary you're on the failed back this is a situation where you want",
    "start": "1540760",
    "end": "1547320"
  },
  {
    "text": "to just move the application to another cluster maybe to move it after a disaster or after you had a failover in",
    "start": "1547320",
    "end": "1555399"
  },
  {
    "text": "this case both clusters are healthy and you're going to scale down the",
    "start": "1555399",
    "end": "1560799"
  },
  {
    "text": "application before so then your RPO is going to be equal to zero because all the rights will be replicated before you",
    "start": "1560799",
    "end": "1568320"
  },
  {
    "text": "fail the application or before you migrate the",
    "start": "1568320",
    "end": "1573840"
  },
  {
    "text": "application so when we want to sort of orchestrate this and make it a little",
    "start": "1573840",
    "end": "1579760"
  },
  {
    "text": "bit easier than doing self commands or even um you know doing like volume",
    "start": "1579760",
    "end": "1585559"
  },
  {
    "text": "replication um there's some we can use some combination of projects so one of",
    "start": "1585559",
    "end": "1591640"
  },
  {
    "text": "them that we've integrated with definitely Downstream but also um we",
    "start": "1591640",
    "end": "1597760"
  },
  {
    "text": "have a Upstream capability to do this is open cluster management if you've heard",
    "start": "1597760",
    "end": "1603520"
  },
  {
    "text": "Downstream is called Advanced cluster Management on open shift but Upstream um",
    "start": "1603520",
    "end": "1610799"
  },
  {
    "text": "this allows you to import the the peer clusters and to orchestrate be able to deploy applications and it also is",
    "start": "1610799",
    "end": "1618240"
  },
  {
    "text": "integrated with the project of the team I work on in the data Foundation um development team which is",
    "start": "1618240",
    "end": "1625760"
  },
  {
    "text": "ramen drr so if you don't remember um",
    "start": "1625760",
    "end": "1630840"
  },
  {
    "text": "you know a lot of this that's okay but if you want to look at just look up Ramen Dr GitHub and um a lot of what",
    "start": "1630840",
    "end": "1638000"
  },
  {
    "text": "I've said will will be documented there um in addition to the Rook dos which",
    "start": "1638000",
    "end": "1643640"
  },
  {
    "text": "also are very good so the ramen crds the really that are new and come with the",
    "start": "1643640",
    "end": "1650000"
  },
  {
    "text": "ramen project are Dr policy Dr clusters Dr placement control so these are all B",
    "start": "1650000",
    "end": "1656039"
  },
  {
    "text": "all would be all in a what we call a hub cluster and I'll show you that in a minute um Dr policy is going to apply a",
    "start": "1656039",
    "end": "1664360"
  },
  {
    "text": "policy on a per application Level Dr and the policy is going to include what are",
    "start": "1664360",
    "end": "1669679"
  },
  {
    "text": "the two peer clusters and what is the replication interval the Dr clusters are",
    "start": "1669679",
    "end": "1675519"
  },
  {
    "text": "which two clusters are peered right now we do everything in pairs of two and the",
    "start": "1675519",
    "end": "1681080"
  },
  {
    "text": "Dr placement control is going to be used to say what Dr action do I want to do um",
    "start": "1681080",
    "end": "1687159"
  },
  {
    "text": "we use two actions fail over and relocate relocate is fail",
    "start": "1687159",
    "end": "1692360"
  },
  {
    "text": "back so the vrg volume replication group is a a custom resource that as soon as",
    "start": "1692360",
    "end": "1698519"
  },
  {
    "text": "you create a drpc and apply a policy then the vrg is",
    "start": "1698519",
    "end": "1703760"
  },
  {
    "text": "on the manage cluster or the cluster that has the application and it's going",
    "start": "1703760",
    "end": "1708840"
  },
  {
    "text": "to then basically do the actions to to initiate that failover",
    "start": "1708840",
    "end": "1715399"
  },
  {
    "text": "relocate and then like I said if you there's the Upstream project for ramen Dr that you can easily find so if we",
    "start": "1715399",
    "end": "1723720"
  },
  {
    "text": "look at it just um in terms of you know an architecture or how it looks uh in",
    "start": "1723720",
    "end": "1730600"
  },
  {
    "text": "the middle we have a so this is a three kubernetes cluster solution in the",
    "start": "1730600",
    "end": "1735840"
  },
  {
    "text": "middle we have what we call the Hub cluster that where ocm would be deployed it's also where we would have our Dr",
    "start": "1735840",
    "end": "1742600"
  },
  {
    "text": "operators so we have you know that would create the the um custom resources I",
    "start": "1742600",
    "end": "1748720"
  },
  {
    "text": "discussed Dr policy Dr clusters Dr placement control they would all live on the Hub and the reason they live on the",
    "start": "1748720",
    "end": "1755679"
  },
  {
    "text": "Hub is we can't assume that that a cluster is going to be available where",
    "start": "1755679",
    "end": "1761320"
  },
  {
    "text": "the application is so I show the replication the asynchronous replication",
    "start": "1761320",
    "end": "1766480"
  },
  {
    "text": "going both directions remember this is per application not per cluster so you",
    "start": "1766480",
    "end": "1772000"
  },
  {
    "text": "don't have to have a cluster just sitting there doing nothing you can do a blue green kind of thing where some of",
    "start": "1772000",
    "end": "1778000"
  },
  {
    "text": "your applications are primary on one and some of them are primary on the other that way if you lose one then you only",
    "start": "1778000",
    "end": "1785200"
  },
  {
    "text": "have to fail over the ones that are down and in terms of distance limitations",
    "start": "1785200",
    "end": "1790600"
  },
  {
    "text": "with asynchronous you don't really have a distance limitation um compared to a synchronous like SEF that we spoke about",
    "start": "1790600",
    "end": "1797600"
  },
  {
    "text": "before um but if you know if your if your latency is high or then your replication",
    "start": "1797600",
    "end": "1806039"
  },
  {
    "text": "takes longer per per volume so this is how it would look",
    "start": "1806039",
    "end": "1813159"
  },
  {
    "text": "um if you take the components we've talked about today Ramen um rook and",
    "start": "1813159",
    "end": "1819480"
  },
  {
    "text": "then the CSI uh like I said does require a hub um cluster there are solutions where you",
    "start": "1819480",
    "end": "1826240"
  },
  {
    "text": "can put if you only have two sides you can put a hub a active Hub on one side",
    "start": "1826240",
    "end": "1831760"
  },
  {
    "text": "and a passive Hub on the other side and then you know it's still a four that would be a four kubernetes cluster",
    "start": "1831760",
    "end": "1837799"
  },
  {
    "text": "solution so you don't necessarily have to have three sides but um in terms of",
    "start": "1837799",
    "end": "1843159"
  },
  {
    "text": "what I'm showing you here this is like a three side the the object store in the middle um really is living on there's",
    "start": "1843159",
    "end": "1850320"
  },
  {
    "text": "Object Store uh a bucket on each one of the Clusters and what we do is we store",
    "start": "1850320",
    "end": "1856360"
  },
  {
    "text": "metadata to be able and in some cases Cube objects um to be able to recreate",
    "start": "1856360",
    "end": "1862600"
  },
  {
    "text": "the app on the alternate cluster so there'll be a bucket on each one that sort of you know cross saves assuming",
    "start": "1862600",
    "end": "1870519"
  },
  {
    "text": "that but you know if if both clusters go down then obviously you don't have a failover you just have a lot of down",
    "start": "1870519",
    "end": "1877559"
  },
  {
    "text": "applications um oh yeah I missed that so",
    "start": "1877559",
    "end": "1882600"
  },
  {
    "text": "we the team I'm on dev team for for these Dr solution Downstream um but",
    "start": "1882600",
    "end": "1888840"
  },
  {
    "text": "Upstream we do almost all of our development upstream and we have a tool which has been developed called Dr EnV",
    "start": "1888840",
    "end": "1896200"
  },
  {
    "text": "so that we can quickly create using ocm um Ramen Dr um rook and SEF and O and I",
    "start": "1896200",
    "end": "1904559"
  },
  {
    "text": "said ocm anyway it creates the whole thing I showed you here and it allows",
    "start": "1904559",
    "end": "1910240"
  },
  {
    "text": "you to uh go ahead and test failover with essentially just running it on a machine a single machine and using mini",
    "start": "1910240",
    "end": "1917039"
  },
  {
    "text": "Cube so at the bottom there um is a link to that user guide to set that up if you",
    "start": "1917039",
    "end": "1923080"
  },
  {
    "text": "want to try it and I think that's all I have thank",
    "start": "1923080",
    "end": "1928679"
  },
  {
    "text": "you just in conclusion uh please go to our website Rook doio from there you'll",
    "start": "1931320",
    "end": "1937240"
  },
  {
    "text": "see a link to the documentation all the documentation helps you get started troubleshoot set it up everything uh",
    "start": "1937240",
    "end": "1944000"
  },
  {
    "text": "join our slack there's a link from the website as well to join the slack uh you know we we love questions through slack",
    "start": "1944000",
    "end": "1950120"
  },
  {
    "text": "whether you or we have GitHub discussions GitHub issues you can open whenever you find things you you'd like",
    "start": "1950120",
    "end": "1955200"
  },
  {
    "text": "to improve um anyway hoping for your feedback we we do really strive to put",
    "start": "1955200",
    "end": "1960919"
  },
  {
    "text": "Upstream first that and we've grown and appreciate the community so thank you for all of your involvement for those",
    "start": "1960919",
    "end": "1967039"
  },
  {
    "text": "who are already involved and looking forward to working with more more of you so I think we have a couple minutes for",
    "start": "1967039",
    "end": "1974559"
  },
  {
    "text": "questions uh hi uh thanks for the talk it was excellent um my question is whether or not you can use Rook as a",
    "start": "1976320",
    "end": "1982919"
  },
  {
    "text": "control plane for multiple external um stf clusters um so how would you see or",
    "start": "1982919",
    "end": "1989720"
  },
  {
    "text": "or or maybe you could maybe describe the pattern that you would use for managing multiple you know SEF uh rook and like",
    "start": "1989720",
    "end": "1995880"
  },
  {
    "text": "Rook operators and SEF clusters across say like multiple regions hundreds uh of",
    "start": "1995880",
    "end": "2001919"
  },
  {
    "text": "clusters thank you right so so you're saying you have multiple Seth clusters and you want to manage it with with one",
    "start": "2001919",
    "end": "2008320"
  },
  {
    "text": "Rook or you've got one cluster that's right or or or if you or if you or if there is a pattern for managing multiple",
    "start": "2008320",
    "end": "2014480"
  },
  {
    "text": "SEF clusters with Rook um what would that look like right so Rook does have",
    "start": "2014480",
    "end": "2019639"
  },
  {
    "text": "the ability to manage multiple Seth clusters in the same uh same kubernetes cluster if you really want to Super",
    "start": "2019639",
    "end": "2027519"
  },
  {
    "text": "isolation from you know this SE cluster for these applications and this Seth cluster for those applications I'd say",
    "start": "2027519",
    "end": "2034600"
  },
  {
    "text": "it manages a handful of Seth clusters well at the same time it's just the op Rook operator does not manage them in",
    "start": "2034600",
    "end": "2040679"
  },
  {
    "text": "parallel so there there can be bottlenecks if you try and do too many of those but there's uh we we have a",
    "start": "2040679",
    "end": "2046880"
  },
  {
    "text": "design consideration where we'll we'll separate make make that more parallel to",
    "start": "2046880",
    "end": "2052200"
  },
  {
    "text": "handle that better um but yeah does that answer the question or anything to add",
    "start": "2052200",
    "end": "2058118"
  },
  {
    "text": "anybody um I I guess if if you're considering um Standalone SEF clusters",
    "start": "2058119",
    "end": "2064638"
  },
  {
    "text": "uh that may be a question that you want to ask like the SE ADM team from the SEF",
    "start": "2064639",
    "end": "2069720"
  },
  {
    "text": "Community as well um they may have some like fun like ideas about what or how to",
    "start": "2069720",
    "end": "2076200"
  },
  {
    "text": "do that I will thank you yeah next",
    "start": "2076200",
    "end": "2081760"
  },
  {
    "text": "question uh I have two question so first question regarding the replication",
    "start": "2081760",
    "end": "2086800"
  },
  {
    "text": "performance so looks like um there must be some outad to support the replication",
    "start": "2086800",
    "end": "2092280"
  },
  {
    "text": "C uh clusters do you man to give kind of some guidance regarding what what",
    "start": "2092280",
    "end": "2098079"
  },
  {
    "text": "resource we need to reserve for replication support I'm not sure I'm not sure",
    "start": "2098079",
    "end": "2105400"
  },
  {
    "text": "understand the question maybe we could talk after um okay so okay then next",
    "start": "2105400",
    "end": "2111240"
  },
  {
    "text": "question so in order to support uh you have h a cluster you also have two a",
    "start": "2111240",
    "end": "2117560"
  },
  {
    "text": "cluster so in the case of region outage and Fa over to cross region does it",
    "start": "2117560",
    "end": "2123640"
  },
  {
    "text": "require the three cluster in three different region or what's the recommendation to deploy these three",
    "start": "2123640",
    "end": "2130839"
  },
  {
    "text": "clusters yeah um the I think the question is if I have a hub cluster and two manage clusters do they need to be",
    "start": "2130839",
    "end": "2137480"
  },
  {
    "text": "in three distinct locations and the answer is yes um",
    "start": "2137480",
    "end": "2143800"
  },
  {
    "text": "because if you lose the Hub and you have a fail if you collocate two of them",
    "start": "2143800",
    "end": "2150440"
  },
  {
    "text": "right the Hub and the manage cluster and you have a failure you have no way to fail over the other ones because your",
    "start": "2150440",
    "end": "2156119"
  },
  {
    "text": "Hub is not available you can like I said have four kubernetes clusters two at one",
    "start": "2156119",
    "end": "2162000"
  },
  {
    "text": "site two at the other in that case you have two hubs active passive and if you",
    "start": "2162000",
    "end": "2167079"
  },
  {
    "text": "have a failure of site one you recover The Hub at site two and then you can",
    "start": "2167079",
    "end": "2172319"
  },
  {
    "text": "fail over the applications that are are on site one so",
    "start": "2172319",
    "end": "2178240"
  },
  {
    "text": "in that case is there any kind of recommendation to support to make the Hub cluster itself resident in the C",
    "start": "2178240",
    "end": "2186119"
  },
  {
    "text": "region because there's no on that but that seems like a kind of key uh",
    "start": "2186119",
    "end": "2191240"
  },
  {
    "text": "coordinator so that should be some conss um applied to that hop for uh like uh",
    "start": "2191240",
    "end": "2199240"
  },
  {
    "text": "residency uh or F over within that region right it cannot be stand along",
    "start": "2199240",
    "end": "2204520"
  },
  {
    "text": "single cluster think about that cluster is in CL yeah yeah I mean if you assume",
    "start": "2204520",
    "end": "2211240"
  },
  {
    "text": "that the The Hub could go down you can run the managed clusters with the",
    "start": "2211240",
    "end": "2216960"
  },
  {
    "text": "application headless and recover The Hub that's not I mean you can do that you",
    "start": "2216960",
    "end": "2222119"
  },
  {
    "text": "just won't be able to install any new apps using uh ocm but I think the more",
    "start": "2222119",
    "end": "2229480"
  },
  {
    "text": "serious thing is losing a cluster with applications and if you believe the Hub",
    "start": "2229480",
    "end": "2234599"
  },
  {
    "text": "you want the Hub to be resilient then you would do an active passive Hub uh",
    "start": "2234599",
    "end": "2240760"
  },
  {
    "text": "situation okay I hopefully I answered the question yeah and we can talk more",
    "start": "2240760",
    "end": "2246200"
  },
  {
    "text": "yeah we can talk more yeah thank thank you okay U I think we're officially out",
    "start": "2246200",
    "end": "2251640"
  },
  {
    "text": "of time but feel free to come up and ask questions as well and thank you for your time thank you",
    "start": "2251640",
    "end": "2260559"
  }
]