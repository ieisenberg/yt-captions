[
  {
    "text": "good morning uh Welcome to our s session um my name is Olivia T I'm a principal",
    "start": "80",
    "end": "6919"
  },
  {
    "text": "uh research scientist and manager at IBM and today I'm joined with by abishek malvankar who is a senior software",
    "start": "6919",
    "end": "13320"
  },
  {
    "text": "developer at IBM research and the two of us are very excited to tell you uh about",
    "start": "13320",
    "end": "19480"
  },
  {
    "text": "our experience about the lessons we learned trying to use Dynamic resource allocation for improving GPU",
    "start": "19480",
    "end": "25480"
  },
  {
    "text": "utilizations on our uh kuties clusters so",
    "start": "25480",
    "end": "30560"
  },
  {
    "text": "uh in this talk I'll start by briefly uh talking about motivations uh inference",
    "start": "30560",
    "end": "37320"
  },
  {
    "text": "servers the the the workload that you know led us on this journey towards using Dynamic resource allocation I'll",
    "start": "37320",
    "end": "43960"
  },
  {
    "text": "talk about multi-instance gpus and then we'll dive into Dynamic resource",
    "start": "43960",
    "end": "49160"
  },
  {
    "text": "allocation I'll talk about what it changes from the perspective of kuani users uh or cluster owners and I'll",
    "start": "49160",
    "end": "57320"
  },
  {
    "text": "introduce insta slice which is a piece of code work contributing to the",
    "start": "57320",
    "end": "62480"
  },
  {
    "text": "community that is you know par paradoxically trying to make sure that nothing changes uh that will become",
    "start": "62480",
    "end": "68360"
  },
  {
    "text": "clear in a second and after that abishek will walk you through uh not just uh",
    "start": "68360",
    "end": "75080"
  },
  {
    "text": "What uh jir is doing but how it's working its mechanics and what we've",
    "start": "75080",
    "end": "81040"
  },
  {
    "text": "been trying to do to improve on the outof thebox experience with scheduling latencies and placement",
    "start": "81040",
    "end": "88479"
  },
  {
    "text": "strategies so in the last couple of years at IBM research we've been developing uh an inference server",
    "start": "88479",
    "end": "95320"
  },
  {
    "text": "service a large you know inference service that I think today is ranging",
    "start": "95320",
    "end": "101720"
  },
  {
    "text": "over about 100 he100 Nvidia gpus and we use the service as a platform for",
    "start": "101720",
    "end": "107200"
  },
  {
    "text": "experimentation with models with serving technology and so on and it serves uh",
    "start": "107200",
    "end": "112360"
  },
  {
    "text": "constantly varying evolving collection of machine learning models some of them very big with you know hundreds of",
    "start": "112360",
    "end": "118560"
  },
  {
    "text": "billions of parameters and some of them relatively small by today's standards with just a let's say a couple million",
    "start": "118560",
    "end": "124520"
  },
  {
    "text": "parameters we've talked already twice this week about this uh inference service here's one of the talk on the",
    "start": "124520",
    "end": "131400"
  },
  {
    "text": "slides there's another one if you want more uh specifics more details about this workload and this service you can",
    "start": "131400",
    "end": "137360"
  },
  {
    "text": "find a lot more statistics in these stocks but from the purpose of these stocks what is of interest uh to me",
    "start": "137360",
    "end": "143440"
  },
  {
    "text": "today is that some of these models are small they can uh they only require a fraction of a GPU to run and so",
    "start": "143440",
    "end": "150360"
  },
  {
    "text": "of course in order to maximize GP utilization we like to uh pack many",
    "start": "150360",
    "end": "156280"
  },
  {
    "text": "models on uh the same GPU and you have different ways to go after that but this",
    "start": "156280",
    "end": "162000"
  },
  {
    "text": "is cubec Con So today we're going to talk about kubernetes Native mechanism to do that hi he how can I run multiple",
    "start": "162000",
    "end": "169760"
  },
  {
    "text": "containers or multiple pods on a single GPU uh for that I need to introduce",
    "start": "169760",
    "end": "175800"
  },
  {
    "text": "multi-instance gpus which is a feature of uh Nvidia data center class gpus like",
    "start": "175800",
    "end": "182720"
  },
  {
    "text": "a100 gpus or h100 gpus uh essentially it's the idea that I can divide uh one GPU into a number of",
    "start": "182720",
    "end": "190480"
  },
  {
    "text": "slots seven today and if I want to use these uh Seven slots or slices my gpus",
    "start": "190480",
    "end": "197920"
  },
  {
    "text": "uh as if they were independent devices but I can also group them in pairs in",
    "start": "197920",
    "end": "203280"
  },
  {
    "text": "groups of three or four or seven so that I can get the GPU size if you want that",
    "start": "203280",
    "end": "208760"
  },
  {
    "text": "is exactly right from my workload right and the nice thing about uh m is that I",
    "start": "208760",
    "end": "214400"
  },
  {
    "text": "can mix and match so I can get two slots uh instance with a one slot instance a",
    "start": "214400",
    "end": "220040"
  },
  {
    "text": "one slot instance and three slot instance or I can do something completely different what's not so",
    "start": "220040",
    "end": "225920"
  },
  {
    "text": "convenient though about m is that this is not just a virtual partition of the chip this is really about deciding what",
    "start": "225920",
    "end": "232280"
  },
  {
    "text": "you do with physical spaces or areas on the Chip And so what that means for instance is if I just want a single slot",
    "start": "232280",
    "end": "240840"
  },
  {
    "text": "and uh I choose one in the middle or one at the end of the GPU it will make no difference to the workload that is",
    "start": "240840",
    "end": "246959"
  },
  {
    "text": "running on these slots it's going to get the same amount of memory the same amount of compute and so on but it's",
    "start": "246959",
    "end": "252159"
  },
  {
    "text": "actually going to dramatically change what I can do later with the remainder of the GPU because it introduces",
    "start": "252159",
    "end": "257280"
  },
  {
    "text": "different fragmentation patterns with the GPU right now understanding this kind of resource allocation constraints",
    "start": "257280",
    "end": "264960"
  },
  {
    "text": "and how to best make use of those is not quite something that kubernetes resource",
    "start": "264960",
    "end": "270840"
  },
  {
    "text": "management and schedule can do today which is why the community has been working on Dynamic resource location or one of the reasons the community has",
    "start": "270840",
    "end": "276680"
  },
  {
    "text": "been working on Dynamic allocation resource allocation we're going to talk about that but um you know before I get",
    "start": "276680",
    "end": "283120"
  },
  {
    "text": "there you know if you like me you like Tetris you can think of MiG as a puzzle if you want gpus is is kind of this",
    "start": "283120",
    "end": "288880"
  },
  {
    "text": "weirdly shaped thing and you're trying to cover it with these weirdly shaped pieces and you can play this game at",
    "start": "288880",
    "end": "295080"
  },
  {
    "text": "home with your kids so how does this help us again I said this before because",
    "start": "295080",
    "end": "300160"
  },
  {
    "text": "if we can fit the model in a small slice we can increase density we can run more models on the GPU everything is good of",
    "start": "300160",
    "end": "307919"
  },
  {
    "text": "course as you might guess uh performance is kind of a different story because as I reduce the side of the slice I'm also",
    "start": "307919",
    "end": "315520"
  },
  {
    "text": "reducing the amount of compute the amount of memory I can use for caching and therefore I'm kind of limiting the",
    "start": "315520",
    "end": "320759"
  },
  {
    "text": "throughput I can get out of the model so as a result uh the optimal slice for",
    "start": "320759",
    "end": "327639"
  },
  {
    "text": "particular model notm only depends on the model size parameters you know um you know a bit width but also on the",
    "start": "327639",
    "end": "334120"
  },
  {
    "text": "loads and of course you know load varies over days because model popularity changes because load again vary",
    "start": "334120",
    "end": "340919"
  },
  {
    "text": "throughout the day and so what that means is we want to do a good job at uh",
    "start": "340919",
    "end": "346720"
  },
  {
    "text": "uh maximizing density while preserving our service levels uh agreement objectives for this service we have to",
    "start": "346720",
    "end": "353240"
  },
  {
    "text": "dynamically vary the size of the slice for you know small models and you you",
    "start": "353240",
    "end": "359720"
  },
  {
    "text": "could think of doing this as horizontal scaling by having fewer or more slices but this is wasteful because you have to",
    "start": "359720",
    "end": "366800"
  },
  {
    "text": "duplicate the model waste and it's actually uh in this case vertical scaling gives better performance and is",
    "start": "366800",
    "end": "373120"
  },
  {
    "text": "what what we like to have which is why we need a mechanism to dynamically scale dynamically slice uh uh gpus making",
    "start": "373120",
    "end": "381680"
  },
  {
    "text": "changes for one particular model without necessarily affecting other models so can you do this today with",
    "start": "381680",
    "end": "388560"
  },
  {
    "text": "kubernetes mainstream you know stable releases kind of but not exactly what",
    "start": "388560",
    "end": "395400"
  },
  {
    "text": "what you can do is you can deploy the Nvidia GPU operator to your clusters I'm sure many of you are familiar with that",
    "start": "395400",
    "end": "401440"
  },
  {
    "text": "now the Nvidia GPU comes with M support it comes with something called uh Mig",
    "start": "401440",
    "end": "406960"
  },
  {
    "text": "manager and what it let you do is it let an admin go and label notes on your clusters select a profile a layout for",
    "start": "406960",
    "end": "414639"
  },
  {
    "text": "your gpus and then when you when you set this label or when you deploy the operation",
    "start": "414639",
    "end": "420680"
  },
  {
    "text": "uh what you will end what the operator will end up doing is actually slicing the gpus configuring the gpus according",
    "start": "420680",
    "end": "427360"
  },
  {
    "text": "to specification you've provided for instance in that case uh building these kind of uh distribution of slices on the",
    "start": "427360",
    "end": "434560"
  },
  {
    "text": "GPU and then uh the uh nodes will start",
    "start": "434560",
    "end": "439879"
  },
  {
    "text": "advertising matching resources we call these things extended resources because they're not just CPU memory and then you",
    "start": "439879",
    "end": "446879"
  },
  {
    "text": "can request for particular p one of those slices by just including a",
    "start": "446879",
    "end": "452360"
  },
  {
    "text": "resource request or resource limits on your P spe okay so this is great again",
    "start": "452360",
    "end": "459199"
  },
  {
    "text": "what's great about it is that it just works um it's uh stable uh you know all",
    "start": "459199",
    "end": "465759"
  },
  {
    "text": "the all the slicers are available at the same time because this is a proper partitioning of the GPU it's actually",
    "start": "465759",
    "end": "470919"
  },
  {
    "text": "even support Dynamic partitioning because an admin can come in and change the label on the nodes and the GPU",
    "start": "470919",
    "end": "477599"
  },
  {
    "text": "operator will uh do the rests right propagating and making configuration changes on the gpus in your cluster",
    "start": "477599",
    "end": "485360"
  },
  {
    "text": "what's not so great about it for use case is that there's no partial work configuration no incremental work",
    "start": "485360",
    "end": "490639"
  },
  {
    "text": "configuration it's slow right so by no partial work configuration what I means is if I change this label all my",
    "start": "490639",
    "end": "497240"
  },
  {
    "text": "workloads on my GPU are going to be evicted and I will need to start again so I cannot make small you know limited",
    "start": "497240",
    "end": "504759"
  },
  {
    "text": "changes on my GPU such as merging the two green partitions into a blue one if I want",
    "start": "504759",
    "end": "510479"
  },
  {
    "text": "it's also no Inc not incremental you know if I just get a single pod coming to my cluster and I need to allocate a",
    "start": "510479",
    "end": "516640"
  },
  {
    "text": "slice on the gpus for that pod I need to decide ahead of time what I'm going to do with the rest of the GPU so I need to",
    "start": "516640",
    "end": "522680"
  },
  {
    "text": "guess what the next workload is going to need which in general you know I might",
    "start": "522680",
    "end": "527720"
  },
  {
    "text": "get wrong and finally when I do this I have to wait a minute or two if I do a reconfiguration the way the GPU operator",
    "start": "527720",
    "end": "535360"
  },
  {
    "text": "works is such that I need a minute or two to actually uh go and um observe the",
    "start": "535360",
    "end": "543320"
  },
  {
    "text": "changes and be able to use those changes so this is where Dynamic",
    "start": "543320",
    "end": "549079"
  },
  {
    "text": "resource allocation enters a picture or one of the reason it does right uh I'm sure many of you have seen the Keynotes",
    "start": "549079",
    "end": "555480"
  },
  {
    "text": "from Patrick from uh Kevin yesterday and so Dr is an attempt from the communities",
    "start": "555480",
    "end": "562360"
  },
  {
    "text": "to standardize access to UND demand resources where everything is trickier than what we're used to right the",
    "start": "562360",
    "end": "568480"
  },
  {
    "text": "resource description themselves is tricky because for instance in the case of me they combine you know valid pairs",
    "start": "568480",
    "end": "574640"
  },
  {
    "text": "of memory footprint and compute Footprints they have custom satisfiability rules right I if I want",
    "start": "574640",
    "end": "580800"
  },
  {
    "text": "to decide whether GPU can satisfy a resource request or not I need to have a deep understanding of how mix Works what",
    "start": "580800",
    "end": "586760"
  },
  {
    "text": "are the conflicts between the different layouts of partitions on on the GPU there's also custom initialization and",
    "start": "586760",
    "end": "592800"
  },
  {
    "text": "cleanup because once I decide that yes a GPU can satisfy request I still have to go and configure the GPU so that it",
    "start": "592800",
    "end": "599600"
  },
  {
    "text": "exposes exactly the device I've logically uh you know given my pod so",
    "start": "599600",
    "end": "605360"
  },
  {
    "text": "it's it's much more than that it's also about sharing not just sharing by partitioning but sharing by having two",
    "start": "605360",
    "end": "610720"
  },
  {
    "text": "pods share the same slice and things like that it's not just for gpus you know G comes uh with a concept of",
    "start": "610720",
    "end": "617360"
  },
  {
    "text": "resource claims and it follows from persistent volumes persistent volume claims so it's about storage it's about",
    "start": "617360",
    "end": "623480"
  },
  {
    "text": "Network it's about topologies there's a lot of potential there again Kevin and Patrick have a list of six 10 11 12 use",
    "start": "623480",
    "end": "630680"
  },
  {
    "text": "cases that you can use this for this is great now the last thing to mention about this though is that this is an",
    "start": "630680",
    "end": "636519"
  },
  {
    "text": "alpha feature this has been an alpha feature for a while and this is going to be an alpha feature for a while longer",
    "start": "636519",
    "end": "643040"
  },
  {
    "text": "because this is really important and the community want to get this rights so as a disclaimer I should also say that us",
    "start": "643040",
    "end": "648760"
  },
  {
    "text": "at IBM are eager to deploy da in in lots of clusters we are working to get there",
    "start": "648760",
    "end": "655880"
  },
  {
    "text": "and facilitate that but until the specification stabilizers than us we're",
    "start": "655880",
    "end": "661399"
  },
  {
    "text": "are not going to do that yet so how do you use d uh this is how you use the",
    "start": "661399",
    "end": "668160"
  },
  {
    "text": "Nvidia GPU operator I've seen I've shown this before way you use D is you comment",
    "start": "668160",
    "end": "673760"
  },
  {
    "text": "out these three red lines of yamal and you replace this with the green stuff so",
    "start": "673760",
    "end": "680160"
  },
  {
    "text": "I know what you're going to say but before we get there in fairness to Dr there's a reason for this abundance of",
    "start": "680160",
    "end": "686519"
  },
  {
    "text": "green again Dr is expressive flexible powerful so we have to pay this price",
    "start": "686519",
    "end": "693000"
  },
  {
    "text": "somewhere now having said that ouch right I don't want to be having to",
    "start": "693000",
    "end": "698839"
  },
  {
    "text": "explain to my users that the yamal they've been happy with for the last few years they have to change it uh and they",
    "start": "698839",
    "end": "705480"
  },
  {
    "text": "have to replace every yaml or every yaml generation tool to do the the green instead of the red I don't want to tell",
    "start": "705480",
    "end": "711560"
  },
  {
    "text": "them I don't know if I can point this that if you look at this slide sometimes you need a DOT between 1G and 5gb and",
    "start": "711560",
    "end": "718079"
  },
  {
    "text": "sometimes you need a hyphen and if you get this wrong nothing is going to work I really don't want to do that which is",
    "start": "718079",
    "end": "724200"
  },
  {
    "text": "the first reasons we've been working on Insta slice insta slice if you think of G as the medicine that kubernetes needs",
    "start": "724200",
    "end": "731880"
  },
  {
    "text": "think of insta slice as the sugar coating that we need to make the medicine go down insta slice is a bunch",
    "start": "731880",
    "end": "737440"
  },
  {
    "text": "of things uh abishek will talk about the more interesting maybe more advanced features of insta slice but to start",
    "start": "737440",
    "end": "743440"
  },
  {
    "text": "with it's all about making you forget the previous slide and you've never seen that you never think that anything has",
    "start": "743440",
    "end": "749199"
  },
  {
    "text": "to change right and the way we do this concretely is we've implemented a part admission controller a mutating webbook",
    "start": "749199",
    "end": "756480"
  },
  {
    "text": "that looks for you know P specs and sees uh resource Mig resource request in",
    "start": "756480",
    "end": "762600"
  },
  {
    "text": "these P specs essentially rewrite them as the equivalent claims right so this",
    "start": "762600",
    "end": "767760"
  },
  {
    "text": "is available today this is open source you can go you can download it you can run it on your cluster you should give",
    "start": "767760",
    "end": "773800"
  },
  {
    "text": "it a star not because it's rocket science not because you think deploying a pod ad mission controller on your",
    "start": "773800",
    "end": "779800"
  },
  {
    "text": "cluster is a great IDE you know hundreds of way it can kill your clusters and your productivity but because together",
    "start": "779800",
    "end": "786160"
  },
  {
    "text": "we can send a clear message the community that these kind of user experience questions these kind of",
    "start": "786160",
    "end": "791560"
  },
  {
    "text": "migration questions these are not second order concern right this is really something that the Dr proposal has to",
    "start": "791560",
    "end": "799680"
  },
  {
    "text": "embrace right how are we going to help our user migrates from the old world to the new world is it something for NVIDIA",
    "start": "799680",
    "end": "806199"
  },
  {
    "text": "to solve is you know how do we do that we have to discuss us but this is something we need to do and ideally I",
    "start": "806199",
    "end": "811839"
  },
  {
    "text": "don't want to maintain this code for the rest of an eternity so um before I leave I really",
    "start": "811839",
    "end": "818760"
  },
  {
    "text": "like to do a demo I think I'm running out of time so you can go and watch this online offline after the talk on YouTube",
    "start": "818760",
    "end": "826399"
  },
  {
    "text": "I'm just going to uh tell you briefly what you're going to find there um but essentially everything I've talked about",
    "start": "826399",
    "end": "833079"
  },
  {
    "text": "so far in this demo I oh I'm not seeing this um let me share",
    "start": "833079",
    "end": "841839"
  },
  {
    "text": "this I guess",
    "start": "843360",
    "end": "846839"
  },
  {
    "text": "because can you see this now nope speakers this is the other screen",
    "start": "848680",
    "end": "855519"
  },
  {
    "text": "yes okay so what you can what you will find in this demo is uh uh essentially a",
    "start": "855519",
    "end": "862440"
  },
  {
    "text": "demonstration of how you can create a kubernetes cluster or you can on this kubernetes cluster deploy and use the V",
    "start": "862440",
    "end": "869399"
  },
  {
    "text": "GPU operator to slice your gpus ahead of time so here you know I'm configuring my",
    "start": "869399",
    "end": "874800"
  },
  {
    "text": "gpus so that they are sliced uh in seven homogeneous slice of the same type I can",
    "start": "874800",
    "end": "881759"
  },
  {
    "text": "then deploy the workload the thing I have on the right on the cluster it's going to work halfway",
    "start": "881759",
    "end": "887279"
  },
  {
    "text": "because one of the pods uh for one of the pods I can satisfy the make request for another one I cannot do that because",
    "start": "887279",
    "end": "894279"
  },
  {
    "text": "the kind of slice I'm asking for is not available in the cluster which is why we want Z in the first place",
    "start": "894279",
    "end": "899600"
  },
  {
    "text": "so what I do like in this demo is reconfigure the Mig operator so that you know how to do this but more importantly",
    "start": "899600",
    "end": "905120"
  },
  {
    "text": "how you can instead use theay which uh will then let me deploy the same",
    "start": "905120",
    "end": "910360"
  },
  {
    "text": "workload on the cluster as is and if I do that if you've been following F carefully what's going to happen well",
    "start": "910360",
    "end": "917639"
  },
  {
    "text": "nothing because D doesn't understand extended resources but finally I can",
    "start": "917639",
    "end": "923399"
  },
  {
    "text": "combine the in the slice and Eureka I get the best of both worlds I can use",
    "start": "923399",
    "end": "928680"
  },
  {
    "text": "this same workloads as before I can deploy it on my cluster it will now run",
    "start": "928680",
    "end": "934000"
  },
  {
    "text": "and instead of having to create GPU slices ahead of time I'm going to create",
    "start": "934000",
    "end": "940040"
  },
  {
    "text": "the combination of insta slice and G will create the slic that you need exactly the one you need exactly when",
    "start": "940040",
    "end": "945839"
  },
  {
    "text": "you need them right that's uh I think concludes my part of the talk and I'll",
    "start": "945839",
    "end": "951399"
  },
  {
    "text": "try to bring back the PowerPoint here",
    "start": "951399",
    "end": "956839"
  },
  {
    "text": "yes I just need to we have yeah thank you yeah um thank you Olivier",
    "start": "956839",
    "end": "965160"
  },
  {
    "text": "um so uh definitely Olivier took us through a journey of uh the the device",
    "start": "965160",
    "end": "971480"
  },
  {
    "text": "plugin way or the GPU operator way of doing things and then now um we drop",
    "start": "971480",
    "end": "976880"
  },
  {
    "text": "down to uh the Dr which is uh certainly the new way um of doing things uh so",
    "start": "976880",
    "end": "983959"
  },
  {
    "text": "let's learn maybe this is a refresher so let's learn a bit uh Concepts about uh dr's uh",
    "start": "983959",
    "end": "991279"
  },
  {
    "text": "initial implementation which worked in tandem with the scheduler and uh the interaction",
    "start": "991279",
    "end": "997920"
  },
  {
    "text": "happened via a new new object called uh pod scheduling context uh Dr has or had uh two modes um",
    "start": "997920",
    "end": "1008279"
  },
  {
    "text": "and uh we'll go through that so wait for first consumer mode um we believe this",
    "start": "1008279",
    "end": "1014079"
  },
  {
    "text": "mode uh is a lazy mode um so if you do not know what uh resources your workload",
    "start": "1014079",
    "end": "1021680"
  },
  {
    "text": "or pod needs a prior um then this is a",
    "start": "1021680",
    "end": "1027000"
  },
  {
    "text": "useful mode and uh what we what we mean by that is uh let's try to understand",
    "start": "1027000",
    "end": "1032880"
  },
  {
    "text": "this uh with a use case so consider Technologies like uh cxl right and these",
    "start": "1032880",
    "end": "1038160"
  },
  {
    "text": "these Technologies help you attach resources that are not local to the node",
    "start": "1038160",
    "end": "1044918"
  },
  {
    "text": "and you can attach these resources on the Fly uh based upon What U uh the",
    "start": "1044919",
    "end": "1050559"
  },
  {
    "text": "requirement of a pod or a workload is uh so certainly in those those scenarios uh",
    "start": "1050559",
    "end": "1056400"
  },
  {
    "text": "wait for first consumer uh plays uh a good role and is the mode uh to be",
    "start": "1056400",
    "end": "1062480"
  },
  {
    "text": "working with uh the second mode is immediate mode um this is much simpler",
    "start": "1062480",
    "end": "1068600"
  },
  {
    "text": "and eager mode um it does soft allocation per se and uh this allocation",
    "start": "1068600",
    "end": "1076480"
  },
  {
    "text": "happens before the Pod arrives in the system um in in our scenario or the",
    "start": "1076480",
    "end": "1082559"
  },
  {
    "text": "system that we designed and we'll learn um a bit more about soft allocation um",
    "start": "1082559",
    "end": "1088919"
  },
  {
    "text": "in the demo um the reason immediate mode works for us is uh most of our workloads",
    "start": "1088919",
    "end": "1095600"
  },
  {
    "text": "are inference here in this cluster and all they ask is gpus or a mix slice and",
    "start": "1095600",
    "end": "1102559"
  },
  {
    "text": "and these are node local resources and we know a prior um uh we can inspect",
    "start": "1102559",
    "end": "1107799"
  },
  {
    "text": "them and and and we and we know what the Pod is requesting so that's why um for our use case uh immediate mode",
    "start": "1107799",
    "end": "1116960"
  },
  {
    "text": "flies Okay so we sought two Dr modes uh they both have overheads so let's try to",
    "start": "1117760",
    "end": "1126000"
  },
  {
    "text": "understand overhead between both the modes um in wait for first consumer mode",
    "start": "1126000",
    "end": "1132159"
  },
  {
    "text": "overhead comes from multiple interactions uh between the controller",
    "start": "1132159",
    "end": "1137600"
  },
  {
    "text": "and the scheduler um via Port scheduling context in",
    "start": "1137600",
    "end": "1143200"
  },
  {
    "text": "immediate mode we interact with the port scheduling context just",
    "start": "1143200",
    "end": "1148360"
  },
  {
    "text": "once um you can see as Illustrated in the uh workflow uh chart uh the first",
    "start": "1148360",
    "end": "1155080"
  },
  {
    "text": "picture here shows uh wait for first consumer mode um that does a back and",
    "start": "1155080",
    "end": "1160559"
  },
  {
    "text": "forth um with the scheduler and the Dr controller uh to find a suitable node",
    "start": "1160559",
    "end": "1166760"
  },
  {
    "text": "while the seg mode which is the uh second picture which is the immediate mode",
    "start": "1166760",
    "end": "1172720"
  },
  {
    "text": "uh just consumes the placement that is already existing in the system so in",
    "start": "1172720",
    "end": "1180159"
  },
  {
    "text": "comparison immediate mode has less overhead to enable immediate mode",
    "start": "1180159",
    "end": "1187159"
  },
  {
    "text": "placement and resource check becomes key to provide uh valid",
    "start": "1187159",
    "end": "1193880"
  },
  {
    "text": "allocations hence uh now further since we have uh talked about uh placement as",
    "start": "1195799",
    "end": "1202799"
  },
  {
    "text": "being important um let's let's first Define what do we mean by placement um",
    "start": "1202799",
    "end": "1209280"
  },
  {
    "text": "for our use case placement is finding a node and a GPU and then provisioning a",
    "start": "1209280",
    "end": "1215600"
  },
  {
    "text": "mix Slice on that um GPU is what we Define as",
    "start": "1215600",
    "end": "1222120"
  },
  {
    "text": "placement so with Mig enabled Hardware what we see is fragmentation is",
    "start": "1222120",
    "end": "1227919"
  },
  {
    "text": "easy due to the constraints um that it puts on so the the hardware has a",
    "start": "1227919",
    "end": "1234520"
  },
  {
    "text": "constraint that vertical overlapping of slices uh is is not",
    "start": "1234520",
    "end": "1241159"
  },
  {
    "text": "possible on the contrary if you do some sort of smart placement it can help with",
    "start": "1241159",
    "end": "1246960"
  },
  {
    "text": "fragmentation and optimize GPU utilization let's try to understand uh",
    "start": "1246960",
    "end": "1253640"
  },
  {
    "text": "this um with an example assume you have a queue somewhere and you have two",
    "start": "1253640",
    "end": "1258960"
  },
  {
    "text": "workloads requesting 3G slice and a 4G slice u in that particular order uh",
    "start": "1258960",
    "end": "1266760"
  },
  {
    "text": "according to uh uh the hardware uh supported U uh Dimensions placing 4G",
    "start": "1266760",
    "end": "1275320"
  },
  {
    "text": "slice ahead of 3G slice almost always makes sense and that's uh right to left",
    "start": "1275320",
    "end": "1282080"
  },
  {
    "text": "placement uh which we Define in our system to achieve such placement uh an",
    "start": "1282080",
    "end": "1288799"
  },
  {
    "text": "external entity typically would be needed uh that provides um such better",
    "start": "1288799",
    "end": "1295240"
  },
  {
    "text": "placement decisions and works with the existing queuing system uh in the Target",
    "start": "1295240",
    "end": "1302520"
  },
  {
    "text": "environment hence connecting the dot between our learnings and the use case",
    "start": "1303559",
    "end": "1309480"
  },
  {
    "text": "we now present another feature of insta slice uh the placement that works with the Dr",
    "start": "1309480",
    "end": "1316600"
  },
  {
    "text": "controller via immediate claim insta slice selection at this point is",
    "start": "1316600",
    "end": "1321880"
  },
  {
    "text": "first fit algorithm placement of work workload happens uh right to left almost all the",
    "start": "1321880",
    "end": "1329760"
  },
  {
    "text": "time for our use cases um since uh we are going after packing",
    "start": "1329760",
    "end": "1337120"
  },
  {
    "text": "optimization future feature of insta slice would would be to enable Autos",
    "start": "1337320",
    "end": "1342600"
  },
  {
    "text": "scaling via possibly seeing pending claims uh using machine sets uh uh which",
    "start": "1342600",
    "end": "1348799"
  },
  {
    "text": "is an API provided by the cluster API project below picture shows the design",
    "start": "1348799",
    "end": "1354640"
  },
  {
    "text": "and the interaction so a high level overview would be insta slice controller",
    "start": "1354640",
    "end": "1359960"
  },
  {
    "text": "send placement information with claims to the Dr controller Dr controller",
    "start": "1359960",
    "end": "1365440"
  },
  {
    "text": "updates placements in its Nas object via immediate allocation Cube scheduler reads the port scheduling context to",
    "start": "1365440",
    "end": "1371840"
  },
  {
    "text": "bind ports and finally workload starts running insta slice also manages the",
    "start": "1371840",
    "end": "1377559"
  },
  {
    "text": "life cycle of the CL with the workload meaning if the workload is terminated or",
    "start": "1377559",
    "end": "1383279"
  },
  {
    "text": "completed insta size would clean up the previous claim to make room for the next",
    "start": "1383279",
    "end": "1388600"
  },
  {
    "text": "claim in the system the whole setup has been running on open shift for us and we share with",
    "start": "1388600",
    "end": "1395520"
  },
  {
    "text": "you few pointers on the installation process we would like to thank uh Vali",
    "start": "1395520",
    "end": "1400720"
  },
  {
    "text": "from red hat for helping uh with the",
    "start": "1400720",
    "end": "1405400"
  },
  {
    "text": "enablement we would also like to few gas that we encountered when building the system to summarize we faced hiccups",
    "start": "1406520",
    "end": "1415200"
  },
  {
    "text": "with consistency orchestration and performance for our setup um and we list",
    "start": "1415200",
    "end": "1421159"
  },
  {
    "text": "them out so please reach out if you need more details on",
    "start": "1421159",
    "end": "1426279"
  },
  {
    "text": "this okay now the demo",
    "start": "1426279",
    "end": "1430600"
  },
  {
    "text": "time all right uh welcome to insta slice demo",
    "start": "1437440",
    "end": "1442880"
  },
  {
    "text": "on open shift with uh Dr enabled let's quickly see the the console URL and you",
    "start": "1442880",
    "end": "1448679"
  },
  {
    "text": "see uh oh to move it to the other scen",
    "start": "1448679",
    "end": "1455720"
  },
  {
    "text": "okay to the right right yeah",
    "start": "1455720",
    "end": "1462080"
  },
  {
    "text": "right okay all right thank you for that so",
    "start": "1467360",
    "end": "1474480"
  },
  {
    "text": "what we see uh in the cluster here is um the cluster URL um and once the cluster has been set",
    "start": "1474480",
    "end": "1483000"
  },
  {
    "text": "up we would also like to show what operators we installed to enable Dr uh",
    "start": "1483000",
    "end": "1488120"
  },
  {
    "text": "so first what we see is uh the NFD operator which is used to um uh label um",
    "start": "1488120",
    "end": "1495080"
  },
  {
    "text": "accelerator enabled nodes and then the GPU operator is used to install GPU binaries uh on",
    "start": "1495080",
    "end": "1501840"
  },
  {
    "text": "the target nodes uh in the cluster now we see uh the number of nodes in the",
    "start": "1501840",
    "end": "1507039"
  },
  {
    "text": "cluster for this simple setup we have single node open shift cluster which operates as a control plane master and",
    "start": "1507039",
    "end": "1513840"
  },
  {
    "text": "the worker we see that the insta slice controller is is running in the namespace insta slice system and we also",
    "start": "1513840",
    "end": "1520840"
  },
  {
    "text": "need the Nvidia Dr driver now let's see the nas object uh",
    "start": "1520840",
    "end": "1527600"
  },
  {
    "text": "for a c Clean Slate",
    "start": "1527600",
    "end": "1530799"
  },
  {
    "text": "cluster trust me this works this is a video so I think it's a playback speed",
    "start": "1533360",
    "end": "1538520"
  },
  {
    "text": "issue so we see the nas object here uh the status here is ready what we also see here is the m partition that is",
    "start": "1538520",
    "end": "1545039"
  },
  {
    "text": "exposed by the nas object for an A1 40gb device the interesting thing over here",
    "start": "1545039",
    "end": "1550760"
  },
  {
    "text": "is this device has been managed by the Nvidia Dr uh plug-in driver and",
    "start": "1550760",
    "end": "1556320"
  },
  {
    "text": "obviously this is the node allocation state object or the nas let's see if the cluster has any resource claims uh so",
    "start": "1556320",
    "end": "1564279"
  },
  {
    "text": "yes we don't have any resource claims so we start with clean slate we submit a new kind of first kind of resource claim",
    "start": "1564279",
    "end": "1571039"
  },
  {
    "text": "which is wait for first consumer claim and it created bunch of objects uh specifically GPU claim parameter that",
    "start": "1571039",
    "end": "1577880"
  },
  {
    "text": "tries to acquire the GPU and make device claim parameter which will realize the Meg on the acquired GPU now remember",
    "start": "1577880",
    "end": "1585760"
  },
  {
    "text": "this this this mode is lazy right so we do want to see what's the status of the",
    "start": "1585760",
    "end": "1590960"
  },
  {
    "text": "wait for first consumer uh claim so by this command we would see",
    "start": "1590960",
    "end": "1597640"
  },
  {
    "text": "the status U of the wait for first consumer claim and no surpris is there the state here is pending and the reason",
    "start": "1597640",
    "end": "1604600"
  },
  {
    "text": "being is there's no pod in the system right so it's lazy it will do nothing just create the claim okay so let's",
    "start": "1604600",
    "end": "1611960"
  },
  {
    "text": "verify this by now again reinspect the nas object and we see that the nas",
    "start": "1611960",
    "end": "1617360"
  },
  {
    "text": "object at this point is just ready and it's clean slate okay so hence we prove",
    "start": "1617360",
    "end": "1623799"
  },
  {
    "text": "that wait for first consumer is is late lazy according to our understanding now let's delete uh the claim and start uh",
    "start": "1623799",
    "end": "1632559"
  },
  {
    "text": "with a clean slate system now here we submit something else we submit here a regular pod or a workload and a bunch of",
    "start": "1632559",
    "end": "1639960"
  },
  {
    "text": "things will happen insta slice web hook will mutate the Pod will create the",
    "start": "1639960",
    "end": "1645000"
  },
  {
    "text": "desired resource claims and also provide place ments so let's see what kind of",
    "start": "1645000",
    "end": "1650919"
  },
  {
    "text": "claims did insta slice controller create for this pod submission so let's see so",
    "start": "1650919",
    "end": "1657640"
  },
  {
    "text": "we see now A different kind of claim that is created by the insta slice controller which is immediate and it's",
    "start": "1657640",
    "end": "1663559"
  },
  {
    "text": "in the allocated State okay what about the placements right so you can see the",
    "start": "1663559",
    "end": "1668720"
  },
  {
    "text": "placement on in the label section so we have all the placement information in",
    "start": "1668720",
    "end": "1674679"
  },
  {
    "text": "the label section which is consumed by the Dr controller here to make uh the",
    "start": "1674679",
    "end": "1680279"
  },
  {
    "text": "make placement on your target GPU let's see the nas object now did it change so",
    "start": "1680279",
    "end": "1686679"
  },
  {
    "text": "Nas object now has two new sections allocated claim which we call as the intent and prepared claims which we call",
    "start": "1686679",
    "end": "1694399"
  },
  {
    "text": "as the realization of the intent so both of them are in sync this just means that",
    "start": "1694399",
    "end": "1699600"
  },
  {
    "text": "the partition has been created on the target GPU let's try to view the logs",
    "start": "1699600",
    "end": "1705320"
  },
  {
    "text": "what does the workload see so no surprises there we do see that",
    "start": "1705320",
    "end": "1711000"
  },
  {
    "text": "the workload is seeing the correct GPU with the correct uh mix slices now let's do something",
    "start": "1711000",
    "end": "1717600"
  },
  {
    "text": "interesting let's submit few more workloads and see incremental M creation",
    "start": "1717600",
    "end": "1723000"
  },
  {
    "text": "happening in process uh if we compare this with the device plug-in world uh",
    "start": "1723000",
    "end": "1729000"
  },
  {
    "text": "with without any reconfiguration this won't be possible but with Dr this",
    "start": "1729000",
    "end": "1735120"
  },
  {
    "text": "enables you um let's again reinspect the nas object now and see what happened",
    "start": "1735120",
    "end": "1741240"
  },
  {
    "text": "when we submitted more workloads uh to the",
    "start": "1741240",
    "end": "1746320"
  },
  {
    "text": "system what we now see is that the prepared claim sections has lot more content or meaning lot more claims and",
    "start": "1746320",
    "end": "1753399"
  },
  {
    "text": "usually the prepared claim SE section will play a catchup game with the",
    "start": "1753399",
    "end": "1758559"
  },
  {
    "text": "allocated claim section um and and and soon after they both would be in in in",
    "start": "1758559",
    "end": "1764760"
  },
  {
    "text": "sync let's see now what's the status of the resource claims we do see that the",
    "start": "1764760",
    "end": "1770159"
  },
  {
    "text": "resource claim now have been moved from allocated to reserved meaning some of the pods in the system are are consuming",
    "start": "1770159",
    "end": "1777200"
  },
  {
    "text": "those immediate uh resource claims let's do something more more interesting now is try to incrementally",
    "start": "1777200",
    "end": "1785120"
  },
  {
    "text": "delete pods not claims we are trying to delete workloads and what iniz",
    "start": "1785120",
    "end": "1790880"
  },
  {
    "text": "controller is doing is it has a relationship between workload to the claim so it's trying to clean up the",
    "start": "1790880",
    "end": "1797159"
  },
  {
    "text": "claim to make room for the newer claims and if we see the resource claim now we",
    "start": "1797159",
    "end": "1802399"
  },
  {
    "text": "see that no resource claim exist because there are no workloads in the system if",
    "start": "1802399",
    "end": "1808000"
  },
  {
    "text": "we see the nas object the nas object again is clean slate so insta slice",
    "start": "1808000",
    "end": "1813399"
  },
  {
    "text": "manages the claim life cycle with the workload and in this demo we show how",
    "start": "1813399",
    "end": "1818760"
  },
  {
    "text": "instant slies can influence decisions um of the Dr",
    "start": "1818760",
    "end": "1824919"
  },
  {
    "text": "controller",
    "start": "1826960",
    "end": "1829960"
  },
  {
    "text": "okay okay um to summarize uh Dr is the future and",
    "start": "1841559",
    "end": "1849240"
  },
  {
    "text": "it is still in progress we have seen and learned that insta slice implements",
    "start": "1849240",
    "end": "1856159"
  },
  {
    "text": "extended resources API on top of Dr improves scheduling latency and",
    "start": "1856159",
    "end": "1862559"
  },
  {
    "text": "placements by adopting immediate mode provides placement from right to",
    "start": "1862559",
    "end": "1869600"
  },
  {
    "text": "left for initial Dr implementation one key requirement what we observed is",
    "start": "1869600",
    "end": "1875360"
  },
  {
    "text": "exposing an API which could be consumed by external controller like insta slice",
    "start": "1875360",
    "end": "1881600"
  },
  {
    "text": "to influence uh Dr placement I hope you enjoyed this presentation um thank you",
    "start": "1881600",
    "end": "1888279"
  },
  {
    "text": "for [Applause]",
    "start": "1888279",
    "end": "1893099"
  },
  {
    "text": "listening before we take questions just a a couple of comments on the demo you've seen here right we have two demos",
    "start": "1895679",
    "end": "1902480"
  },
  {
    "text": "here because the first one is just using plain kubernetes and using the plain Upstream D driver from Nvidia and you",
    "start": "1902480",
    "end": "1910240"
  },
  {
    "text": "can find all the details the scripts behind the demo on the insta scale repository and you can therefore not",
    "start": "1910240",
    "end": "1916120"
  },
  {
    "text": "just you know replay the demo at home home but you can recreate it at home if you want uh assuming you have a100 gpus",
    "start": "1916120",
    "end": "1922760"
  },
  {
    "text": "in your basement uh call me if you do right um the second demo is an open shift demo it shows more advanced",
    "start": "1922760",
    "end": "1929760"
  },
  {
    "text": "features that require changes to the deer driver these are things we're working on publishing with our",
    "start": "1929760",
    "end": "1935240"
  },
  {
    "text": "colleagues at rat and it should be available shortly",
    "start": "1935240",
    "end": "1940960"
  },
  {
    "text": "[Applause] thanks uh youer from Nvidia yeah Kevin",
    "start": "1942170",
    "end": "1949519"
  },
  {
    "text": "konik so it's great to see the improvements and enhancement in the mix",
    "start": "1949519",
    "end": "1955320"
  },
  {
    "text": "uh very nice work so I just wondering and uh can you comment and compare on",
    "start": "1955320",
    "end": "1960679"
  },
  {
    "text": "the proposed Dynamic and the slicing with other existing GPU sharing",
    "start": "1960679",
    "end": "1966519"
  },
  {
    "text": "mechanism like time slicing and multiprocess service and do you foresee",
    "start": "1966519",
    "end": "1972080"
  },
  {
    "text": "and any use cases in your practice for them thank you yes of course I you know",
    "start": "1972080",
    "end": "1977600"
  },
  {
    "text": "uh I don't want to give the misimpression here that we think that Mig is the one and only way to do",
    "start": "1977600",
    "end": "1983000"
  },
  {
    "text": "slicing or sharing for inference we choose to focus on Ming this stock",
    "start": "1983000",
    "end": "1988200"
  },
  {
    "text": "because it's really the poster child for Dr but we're also considering time sharing MPS for inference and we think",
    "start": "1988200",
    "end": "1996480"
  },
  {
    "text": "that you know we probably even think that time sharing is in the end most likely more valuable than just me for",
    "start": "1996480",
    "end": "2002760"
  },
  {
    "text": "inference okay great look forward to see more use cases for all thank you",
    "start": "2002760",
    "end": "2008799"
  },
  {
    "text": "a question yeah great presentation as well thanks a lot I am curious your cluster",
    "start": "2008799",
    "end": "2015080"
  },
  {
    "text": "is a fixed size do you have any interoperability with node claims",
    "start": "2015080",
    "end": "2020159"
  },
  {
    "text": "for um yeah currently in our scenario we are working on on fix size clusters I",
    "start": "2020159",
    "end": "2026919"
  },
  {
    "text": "mean I'm not sure if you're alluding to autoscaling uh we yeah I was thinking about Carpenter and note claims is there",
    "start": "2026919",
    "end": "2033639"
  },
  {
    "text": "any interoperability between that and the GPU claims I mean the thing there is",
    "start": "2033639",
    "end": "2038679"
  },
  {
    "text": "I I don't think Carpenter is there yet in understanding the dynamic claim creation and and how how the information",
    "start": "2038679",
    "end": "2045480"
  },
  {
    "text": "gets propagated we are in talks with the car one of the carpenter maintainers yesterday and and there is work to do on",
    "start": "2045480",
    "end": "2053760"
  },
  {
    "text": "understanding the mix so right now I think both the not us neither us nor them are are ready but we are in talks",
    "start": "2053760",
    "end": "2060760"
  },
  {
    "text": "to make a it seems like there's some confusion between you're claiming a GPU on the one hand the Autos scal is",
    "start": "2060760",
    "end": "2066800"
  },
  {
    "text": "claiming a node on the other other hand how does that interoperate I guess is",
    "start": "2066800",
    "end": "2071919"
  },
  {
    "text": "yeah I mean we we need to figure a strategy out currently we we don't have one yeah okay thanks a lot and uh this",
    "start": "2071919",
    "end": "2079440"
  },
  {
    "text": "is also something that the next revision of Dr with structure parameters should make it more uh less opaque and",
    "start": "2079440",
    "end": "2086720"
  },
  {
    "text": "therefore more amenable to have the autoscaler understand those things oh yeah good point yeah thanks a lot",
    "start": "2086720",
    "end": "2093398"
  },
  {
    "text": "yeah hello thank you for the talk it was very informative uh I wanted to ask did",
    "start": "2093520",
    "end": "2099280"
  },
  {
    "text": "I understand correctly that uh instance wise is actually doing an injection you might say so a mutating web hook that",
    "start": "2099280",
    "end": "2105880"
  },
  {
    "text": "would update the uh information that you showed uh uh for the Nvidia driver which",
    "start": "2105880",
    "end": "2111680"
  },
  {
    "text": "is yeah very long and hard to understand yes that's correct that that's what it",
    "start": "2111680",
    "end": "2117040"
  },
  {
    "text": "does at at this point in time it's specific to the Nvidia you know it essentially has a table that says you",
    "start": "2117040",
    "end": "2122839"
  },
  {
    "text": "know this is where to slice is called um uh if you're using extended resources",
    "start": "2122839",
    "end": "2129599"
  },
  {
    "text": "this is what the profile names for the claims this this is the equivalent GPU",
    "start": "2129599",
    "end": "2134720"
  },
  {
    "text": "parameter that you need if you're using Dray so it it it should be possible to extend uh I think as I joked around also",
    "start": "2134720",
    "end": "2141880"
  },
  {
    "text": "in the talk I'm not convinced that long term uh you know I don't want to convince my my bread at colleagues that",
    "start": "2141880",
    "end": "2147480"
  },
  {
    "text": "they want to deploy pod admission controllers on all their clusters so I think we need better Solutions but you",
    "start": "2147480",
    "end": "2153560"
  },
  {
    "text": "know that's a at least that's a stop Gap solution and the point is we need this right we need this to be supported by",
    "start": "2153560",
    "end": "2159280"
  },
  {
    "text": "the framework out of the box yeah it's it's a good idea definitely especially for someone that doesn't want to look",
    "start": "2159280",
    "end": "2165280"
  },
  {
    "text": "into every tiny detail but in the event that you do have to do some optimization",
    "start": "2165280",
    "end": "2171040"
  },
  {
    "text": "to the Nvidia one does instance wise allow it like with some sort of annotation or yeah so I mean from a code",
    "start": "2171040",
    "end": "2178880"
  },
  {
    "text": "perspective we are um trying to open source the placement logic and there is an interface you can hook your custom",
    "start": "2178880",
    "end": "2185079"
  },
  {
    "text": "policies on top of that interface and have your own custom placements whatever you want so that's that's in progress we",
    "start": "2185079",
    "end": "2192119"
  },
  {
    "text": "we have haven't released it because the new Dr version came in just yesterday or two days ago so we are still in flux to",
    "start": "2192119",
    "end": "2199119"
  },
  {
    "text": "to decide yeah okay thank you yeah hi uh Al from Red Hat so thank you",
    "start": "2199119",
    "end": "2206760"
  },
  {
    "text": "for the great talk so I I saw you have web Hooks and the scheduler is now",
    "start": "2206760",
    "end": "2212760"
  },
  {
    "text": "watching for new resources does this means I want to understand first the impact of web hooks on this architecture",
    "start": "2212760",
    "end": "2219040"
  },
  {
    "text": "second if we need to extend the schedular to watch for new resources",
    "start": "2219040",
    "end": "2225200"
  },
  {
    "text": "apart from Parts no okay and so so U",
    "start": "2225200",
    "end": "2230839"
  },
  {
    "text": "maybe this is a core di question so J relies on extensions of the scheduler where it now understand claims that is",
    "start": "2230839",
    "end": "2237040"
  },
  {
    "text": "something they don't have if you don't enable the dynamic resource allocation feature on your schuer so there's",
    "start": "2237040",
    "end": "2243119"
  },
  {
    "text": "definitely an underlying change in the Schuler uh that is that we leverage here so the first part of instant slice the",
    "start": "2243119",
    "end": "2250000"
  },
  {
    "text": "webbook indeed is not changing anything in the schedular it's something that sits before the P reaches the scheduler",
    "start": "2250000",
    "end": "2256880"
  },
  {
    "text": "and make the necessary changes to your pod spe essentially okay great and then the second part as you mentioned H go",
    "start": "2256880",
    "end": "2263040"
  },
  {
    "text": "ahead I mean you you mentioned change in the scheduler I mean part scheduling context is a new object that the",
    "start": "2263040",
    "end": "2268920"
  },
  {
    "text": "scheduler consumes to get the placements so that's that's a change implicit in the initial version of the Dr and and",
    "start": "2268920",
    "end": "2276319"
  },
  {
    "text": "and that's that was Alpha feature Upstream so that's that's the change for the schedu all right and and then the",
    "start": "2276319",
    "end": "2282800"
  },
  {
    "text": "second part is you mentioned cluster API I didn't get the context so Auto scaling",
    "start": "2282800",
    "end": "2288160"
  },
  {
    "text": "here we are bouncing around ideas uh you can so currently the auto scaler works on pending pods but we believe Autos",
    "start": "2288160",
    "end": "2294960"
  },
  {
    "text": "scaling could also work on pending claims and once you know the claims then all you can do is you know B Bunch them",
    "start": "2294960",
    "end": "2303520"
  },
  {
    "text": "together and ask for a node using the KP project and then somehow stuff those claims on that node",
    "start": "2303520",
    "end": "2311520"
  },
  {
    "text": "so that pods could you know attract to those node and that could be one Auto scaling policy and the reason we say",
    "start": "2311520",
    "end": "2318560"
  },
  {
    "text": "this is because I mean if you go the traditional Auto scaling way you would have to rebuild the binary for your own",
    "start": "2318560",
    "end": "2324359"
  },
  {
    "text": "custom scheduler and other things so it's a it's a long path so there are multiple Solutions it depends what you",
    "start": "2324359",
    "end": "2330960"
  },
  {
    "text": "want to explore all right thank you very",
    "start": "2330960",
    "end": "2335400"
  },
  {
    "text": "much well if there's no more questions thanks again thank",
    "start": "2336440",
    "end": "2341880"
  },
  {
    "text": "you",
    "start": "2341880",
    "end": "2344880"
  }
]