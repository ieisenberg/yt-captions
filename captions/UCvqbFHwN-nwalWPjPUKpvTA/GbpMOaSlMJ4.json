[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "just in case you guys were wondering these slides are posted so you don't take photos of the slides just future",
    "start": "30",
    "end": "6029"
  },
  {
    "text": "reference it's easier that way okay so I'll get started thank you all",
    "start": "6029",
    "end": "11250"
  },
  {
    "text": "for coming I'm Milan Fallon anko and I'm a software engineer at Bloomberg working on the data science infrastructure team and",
    "start": "11250",
    "end": "17850"
  },
  {
    "text": "today we'll be talking about scaling securing spark on kubernetes at Bloomberg so I'll begin by talking about",
    "start": "17850",
    "end": "25230"
  },
  {
    "text": "data science at Bloomberg then I'll discuss the part that spark plays in the detail pipelines that we have and how we",
    "start": "25230",
    "end": "31650"
  },
  {
    "text": "work to secure and scale it with disaggregated compute and then some future work that lays ahead so machine",
    "start": "31650",
    "end": "39840"
  },
  {
    "start": "39000",
    "end": "39000"
  },
  {
    "text": "learning workloads come in many shapes and sizes but the challenges that ml teams face are quite common even in",
    "start": "39840",
    "end": "45390"
  },
  {
    "text": "established enterprises some of these concerns include data permissioning scheduling and Rito's monitoring for",
    "start": "45390",
    "end": "53039"
  },
  {
    "text": "tasks that include feature generation model training and model development in an effort to solve some of these",
    "start": "53039",
    "end": "59460"
  },
  {
    "text": "challenges specifically regarding ml questions and problems Bloomberg has developed a versatile data science",
    "start": "59460",
    "end": "65128"
  },
  {
    "text": "platform that provides sophisticated compute environment for engineers to manage their ETL pipelines and they're",
    "start": "65129",
    "end": "70979"
  },
  {
    "text": "modeled with model development life cycles to do this the solution presents",
    "start": "70979",
    "end": "76020"
  },
  {
    "text": "a variety of ETL and training jobs that run through various runtimes and identity management solution that",
    "start": "76020",
    "end": "82200"
  },
  {
    "text": "makes security a first-class citizen resource governance for teams and forms of sharing a shared workspaces and",
    "start": "82200",
    "end": "87900"
  },
  {
    "text": "inference solutions that are designed with lambdas using K native for building serving and monitoring models one",
    "start": "87900",
    "end": "96479"
  },
  {
    "start": "96000",
    "end": "96000"
  },
  {
    "text": "integral component of most ETL pipelines is Apache spark so just as a show of",
    "start": "96479",
    "end": "101579"
  },
  {
    "text": "reference how many people know of Apache spark in this room okay awesome that's really great and how many of you",
    "start": "101579",
    "end": "106950"
  },
  {
    "text": "actually use it within your kubernetes environments okay even better now we're",
    "start": "106950",
    "end": "112290"
  },
  {
    "text": "gonna discuss that a little bit so just for those who didn't raise their hand spark functions as a robust framework",
    "start": "112290",
    "end": "117780"
  },
  {
    "text": "that uses a general execution engine to provide distributed data processing and one of Sparx powerful applications in",
    "start": "117780",
    "end": "123719"
  },
  {
    "text": "fee is feature transformation and generation in essence converting large under start unstructured data into",
    "start": "123719",
    "end": "129810"
  },
  {
    "text": "something more structured and spark runs as independent processes on a cluster coordinated by the spark context over",
    "start": "129810",
    "end": "135900"
  },
  {
    "text": "here that's running in your driver program the spark context then can connect to",
    "start": "135900",
    "end": "141300"
  },
  {
    "text": "several types of cluster managers which includes standalone meso CR or core",
    "start": "141300",
    "end": "146370"
  },
  {
    "text": "kubernetes and these cluster managers in essence just allocate resources across applications now upon receiving Pro",
    "start": "146370",
    "end": "153870"
  },
  {
    "text": "provision executors Sparkman sends your application code to the executors and then after which the spark context would",
    "start": "153870",
    "end": "159840"
  },
  {
    "text": "then send tasks to to the executors to run that the executor will then use a variety of caching strategies for",
    "start": "159840",
    "end": "166470"
  },
  {
    "text": "various SPARC files that are traditionally stored on the local file system so that would be for example that's displayed over here and",
    "start": "166470",
    "end": "173000"
  },
  {
    "text": "persistence is also traditionally done to local HDFS data nodes for stuff like hive tables we just orderly here the",
    "start": "173000",
    "end": "179850"
  },
  {
    "text": "same time executors might need to communicate with each other in order to read from their other local caches now",
    "start": "179850",
    "end": "185850"
  },
  {
    "text": "storage relating to the data that you wish to process is also traditionally co-located for a data locality speed ups",
    "start": "185850",
    "end": "191610"
  },
  {
    "text": "as such you can see how the storage and compute are on the same node thereby co-located however this design is",
    "start": "191610",
    "end": "199050"
  },
  {
    "text": "constraining when we try to extend kubernetes as a cluster manager for SPARC which we will elaborate on later",
    "start": "199050",
    "end": "204720"
  },
  {
    "text": "but also it isn't consistent with the with the industry's trend towards promoting disaggregated compute but what",
    "start": "204720",
    "end": "211380"
  },
  {
    "start": "211000",
    "end": "211000"
  },
  {
    "text": "is this aggregated compute or Disick so this AGG is the cluster separation of storage and compute and as Bloomberg's",
    "start": "211380",
    "end": "218400"
  },
  {
    "text": "technical problems grow in scope so does our data and unsurprisingly compute nodes that aren't designed for large",
    "start": "218400",
    "end": "223980"
  },
  {
    "text": "disk space can't handle certain workloads of SPARC is doing its file caching by writing too slow to its local",
    "start": "223980",
    "end": "230160"
  },
  {
    "text": "file system and unsurprisingly compute nodes aren't designed in this way will",
    "start": "230160",
    "end": "236130"
  },
  {
    "text": "now be started taking advantage of the hardware that you can now specialize so for example your compute nodes will now be RAM heavy while your storage nodes",
    "start": "236130",
    "end": "242400"
  },
  {
    "text": "can be disk adding this aggregated compute will also allow your cluster management and heart and hardware upgrade cycles to be done separately",
    "start": "242400",
    "end": "248670"
  },
  {
    "text": "especially within Blumberg where we have specialized teams that manage storage this is particularly appealing this AG",
    "start": "248670",
    "end": "255269"
  },
  {
    "text": "will also allow you to persist your storage nodes while you're while your compute nodes can be brought up elastically and this is especially",
    "start": "255269",
    "end": "261150"
  },
  {
    "text": "useful in cloud environments where you can like scale up and down ec2 instances",
    "start": "261150",
    "end": "266870"
  },
  {
    "text": "so Bloomberg has incorporated this dis Agra proach in the design of its data science platform and what you can",
    "start": "266870",
    "end": "272760"
  },
  {
    "text": "immediately notice is the separation between storage and compute our users store their data in s3 HDFS or some",
    "start": "272760",
    "end": "279360"
  },
  {
    "text": "other data stores of their choosing and one goal in building this platform is to make it easy to access data and various",
    "start": "279360",
    "end": "284910"
  },
  {
    "text": "data sources so on the bottom right we have our compute cluster which is made up of many CPU and GPU nodes which all",
    "start": "284910",
    "end": "290880"
  },
  {
    "text": "which has all bits resource scheduling and management done via kubernetes a user then submits a platform job to this",
    "start": "290880",
    "end": "298320"
  },
  {
    "text": "compute cluster we provide various pieces of tooling and ml facilities around the compute cluster for our users",
    "start": "298320",
    "end": "303660"
  },
  {
    "text": "and these and some of this tool includes a custom job CLI and a job history UI now let's dig let's dig into the",
    "start": "303660",
    "end": "310470"
  },
  {
    "text": "Eternals of the job the platform job so the life cycle looks something like this",
    "start": "310470",
    "end": "315840"
  },
  {
    "start": "314000",
    "end": "314000"
  },
  {
    "text": "was traditional for like an operator controller architecture so user submits an API resource to the kubernetes api",
    "start": "315840",
    "end": "322380"
  },
  {
    "text": "server in a declarative format like Y Amal then there's a platform controller which watches for changes to a resource",
    "start": "322380",
    "end": "328710"
  },
  {
    "text": "of interest it's worth to note that this controller runs in a privileged namespace the controller then acts like",
    "start": "328710",
    "end": "335010"
  },
  {
    "text": "a state machine by reading the updates that happen in an appropriate way by looking at the current state compared to",
    "start": "335010",
    "end": "340140"
  },
  {
    "text": "the current to the desired state of the world now because our controller runs in a privileged namespace it can now submit",
    "start": "340140",
    "end": "346440"
  },
  {
    "text": "relevant changes and requests back to the API server so this could come in the form of a pod resource for example the",
    "start": "346440",
    "end": "352980"
  },
  {
    "text": "API server will then schedule the pods appropriately and as such the SPARC job",
    "start": "352980",
    "end": "358260"
  },
  {
    "start": "356000",
    "end": "356000"
  },
  {
    "text": "can now be thought also as a platform job thereby the controller's responsible",
    "start": "358260",
    "end": "363450"
  },
  {
    "text": "for requesting resources needed to spin up a spark class spark cluster now you guys probably seen this diagram a lot of times but let's talk a little bit about",
    "start": "363450",
    "end": "369420"
  },
  {
    "text": "how this kind of fits into the kubernetes world so the in the initial days that of the data science platform",
    "start": "369420",
    "end": "375270"
  },
  {
    "text": "we decided to launch spark via stand-alone which means that the driver communicates with a master pod as",
    "start": "375270",
    "end": "380370"
  },
  {
    "text": "opposed to the kubernetes api server directly as such by using an in in container the driver would wait until",
    "start": "380370",
    "end": "385620"
  },
  {
    "text": "the master and worker nodes are provisioned and reachable before launch in a spark application defined in the",
    "start": "385620",
    "end": "390900"
  },
  {
    "text": "job spec it is worth to note that our storage nodes are actually separated",
    "start": "390900",
    "end": "396540"
  },
  {
    "text": "which contain most the user application data from the from a computer however the cache layer as you can see here is still",
    "start": "396540",
    "end": "403009"
  },
  {
    "text": "sitting on our compute nodes now some drawbacks become very apparent the first of which is that you're unable to",
    "start": "403009",
    "end": "408860"
  },
  {
    "text": "elastically scale within a spark application as there's no direct communication with the API server the",
    "start": "408860",
    "end": "414410"
  },
  {
    "text": "second was that the master node and worker knows needed to be set up before launching the spark application so if",
    "start": "414410",
    "end": "419960"
  },
  {
    "text": "you guys use a lot of Jupiter this is particularly problematic when trying to create a spark context within Jupiter as a as well B require that you pre launch",
    "start": "419960",
    "end": "426710"
  },
  {
    "text": "a headless spark job that you need to interact with now this traditionally we are scaling up our user account we found",
    "start": "426710",
    "end": "432889"
  },
  {
    "text": "that this lets a lot of problems with the resource management as sometimes these properties clusters weren't properly reap reaps upon notebook",
    "start": "432889",
    "end": "439760"
  },
  {
    "text": "termination so to explain that better pretty much users wouldn't users who would launch a spark job within their",
    "start": "439760",
    "end": "446030"
  },
  {
    "text": "Jupiter notebook wouldn't would need to be responsible to terminate that same spark job and as you can imagine that's not always the case so to solve this",
    "start": "446030",
    "end": "453710"
  },
  {
    "start": "453000",
    "end": "453000"
  },
  {
    "text": "Bloomberg actively work to introduce Cabrini's native integration into spark 2 3 + 2 4 to solve these very issues now",
    "start": "453710",
    "end": "461030"
  },
  {
    "text": "we can see the architecture suddenly change by having direct by having the driver directly communicate with the API",
    "start": "461030",
    "end": "466729"
  },
  {
    "text": "server so as such you can now think of the driver as a controller in the architecture of the date of the data science platform so this is the current",
    "start": "466729",
    "end": "473780"
  },
  {
    "text": "state of where the community is at with in terms of spark and kubernetes however there were issues when we started",
    "start": "473780",
    "end": "479270"
  },
  {
    "text": "integrating this with in our production Bloomberg environment so now I'm gonna identify some of these issues and how we work to the community to solve them so",
    "start": "479270",
    "end": "487729"
  },
  {
    "text": "because of the security restrictions imposed by our cluster administrators due to the sensitivity and regulations",
    "start": "487729",
    "end": "493340"
  },
  {
    "text": "of the space that we worked in users are unable to launch pods from their own namespaces so instead users need to",
    "start": "493340",
    "end": "499159"
  },
  {
    "text": "interact with pause that are launched by a controller that is running in a demand namespace now this use case might be one",
    "start": "499159",
    "end": "505789"
  },
  {
    "text": "that is employed in your environments as well therefore to have a driver function as a controller is not possible within",
    "start": "505789",
    "end": "512779"
  },
  {
    "text": "our environment because it's creating pods secondly the vast array of data sources that Bloomberg uses are all",
    "start": "512779",
    "end": "518330"
  },
  {
    "text": "secured and the process of data retrieval within our compute jobs lack to manage identity service that",
    "start": "518330",
    "end": "523820"
  },
  {
    "text": "integrates with Bloomberg jobs especially the SPARC and lastly Sparx caching on local disk makes enable",
    "start": "523820",
    "end": "531110"
  },
  {
    "text": "a fully disaggregated spark environment impossible just go back a tiny but this cash handling here",
    "start": "531110",
    "end": "536720"
  },
  {
    "text": "because it's being wrote to the local file system restricts us from doing a fully disaggregated environment so let's",
    "start": "536720",
    "end": "545390"
  },
  {
    "text": "start with security as we said earlier the ability for pods to be created from the users of namespaces forbidden but",
    "start": "545390",
    "end": "551450"
  },
  {
    "start": "547000",
    "end": "547000"
  },
  {
    "text": "even if it were allowed via some form of restricting restricted web hooks these web hooks will be applied to all pods including system pods which could bring",
    "start": "551450",
    "end": "558440"
  },
  {
    "text": "a nose will slow down furthermore there's no ability to do an all or nothing pod creation we're like all pods",
    "start": "558440",
    "end": "565070"
  },
  {
    "text": "required for a spark cluster would wait to be registered with the API server until the reaches quota is available",
    "start": "565070",
    "end": "570260"
  },
  {
    "text": "this use case is important for different resource scheduling scenarios where you would wish to tune when to create a",
    "start": "570260",
    "end": "575990"
  },
  {
    "text": "batch of executors based on the cluster load so this customization would solve some complications that could arise with",
    "start": "575990",
    "end": "582709"
  },
  {
    "text": "regards to resource deadlocks between competing spark jobs on a single workspace so Bloomberg developed a",
    "start": "582709",
    "end": "588740"
  },
  {
    "text": "solution by extending an API that we developed that hooks into the kubernetes cluster scheduler back end and this API",
    "start": "588740",
    "end": "594700"
  },
  {
    "text": "this API is default implementation extends the current logic which we just",
    "start": "594700",
    "end": "600290"
  },
  {
    "text": "launched pods but internally we extend this API to create an update a custom resource that we call the exec pod",
    "start": "600290",
    "end": "605420"
  },
  {
    "text": "scalar but users for this API if you the committee wants the user can extend any form of custom resource that they choose",
    "start": "605420",
    "end": "612459"
  },
  {
    "text": "so so let's jump into an example spark job and to see how we leverage the exec",
    "start": "612459",
    "end": "617660"
  },
  {
    "start": "613000",
    "end": "613000"
  },
  {
    "text": "pod scalar custom resource definition so the user who lives in the users namespace he would begin by submitting a",
    "start": "617660",
    "end": "623480"
  },
  {
    "text": "spark job to the API server using some declarative format like Hamill for example a controller living in a",
    "start": "623480",
    "end": "629900"
  },
  {
    "text": "privileged namespace would watch for custom resources and upon seeing a request for a spark job with provision and Driver pod with all the appropriate",
    "start": "629900",
    "end": "635360"
  },
  {
    "text": "web hook and modifications and upon there being enough resources in the users quota for their driver and the",
    "start": "635360",
    "end": "640550"
  },
  {
    "text": "executors within the controller would then launch a driver pod now by measuring the quota needed for the",
    "start": "640550",
    "end": "646100"
  },
  {
    "text": "driver and executors this allows for us to avoid hanging drivers who are deadlocked from launching as computers",
    "start": "646100",
    "end": "651500"
  },
  {
    "text": "when competing spark jobs are all running in the same instance which is pretty important because there's a lot of situations where you might have a",
    "start": "651500",
    "end": "657170"
  },
  {
    "text": "heavy cluster load and drivers that are just waiting so you now have the overpopulation of too many drivers the",
    "start": "657170",
    "end": "664100"
  },
  {
    "text": "driver pod will now run the spark application and upon it requesting executors and its back-end it would create an exact pod",
    "start": "664100",
    "end": "669620"
  },
  {
    "text": "scalar custom resource which the controller will also be watching for upon seeing requests for an exact pod",
    "start": "669620",
    "end": "676220"
  },
  {
    "text": "scalar custom resource the controller were then provision the executors with the appropriate web hooks and modifications upon seen again the",
    "start": "676220",
    "end": "682910"
  },
  {
    "text": "appropriate resource quota being available for the request and executors this is the handle of race condition in",
    "start": "682910",
    "end": "688250"
  },
  {
    "text": "which multiple SPARC jobs could have been started after the period in which the driver plus executors resource count was available to note all executors are",
    "start": "688250",
    "end": "696259"
  },
  {
    "text": "created at once upon the availability of resources as opposed to having only a portion of the executors created and subsequently blocking simultaneous spark",
    "start": "696259",
    "end": "703040"
  },
  {
    "text": "jobs who might be in the same deadlock scenario so as you can see now the",
    "start": "703040",
    "end": "708380"
  },
  {
    "text": "driver does not create the executors which would then give the ability for users to cut to customize their own pods but instead the driver will setting the",
    "start": "708380",
    "end": "715220"
  },
  {
    "text": "executors pods that are to be created and by filtering on unique spark application pods the driver will then",
    "start": "715220",
    "end": "720769"
  },
  {
    "text": "send the spark application information such as the application specs tasks to the executors so after solving one of",
    "start": "720769",
    "end": "728959"
  },
  {
    "start": "727000",
    "end": "727000"
  },
  {
    "text": "the challenges with compute security another one else presented was with relation to storage security Bloomberg",
    "start": "728959",
    "end": "734420"
  },
  {
    "text": "has a variety of data sources that he uses for its model development each of which are highly secured and to simplify the experience of using moods at the",
    "start": "734420",
    "end": "741110"
  },
  {
    "text": "data science platform when interacting with for example kerberized HDFS or s3 the data science platform designed to",
    "start": "741110",
    "end": "747500"
  },
  {
    "text": "manage token service for all jobs the token service is serviceable via the simple inclusion of the user identity",
    "start": "747500",
    "end": "754130"
  },
  {
    "text": "name that is specify on your job specification and one of the features that it automatically handles Hadoop",
    "start": "754130",
    "end": "759139"
  },
  {
    "text": "token of renewal and retrieval for HDFS high of HBase and the like now by",
    "start": "759139",
    "end": "764329"
  },
  {
    "text": "putting the token service behind the controller this allows for reusability and standardization across a variety of",
    "start": "764329",
    "end": "769939"
  },
  {
    "text": "jobs including spark and also another popular job that uses HDFS data which is tensorflow so let's see how it works so",
    "start": "769939",
    "end": "779060"
  },
  {
    "text": "the controller from an admin namespace communicates out to a variety of data sources and management tools which in",
    "start": "779060",
    "end": "784759"
  },
  {
    "text": "sparks case includes HDFS and s3 the user begins by creating an identity custom resource and a controller will",
    "start": "784759",
    "end": "791180"
  },
  {
    "text": "then read the custom resource and have the users privileged credentials stored as a secret for jobs in the users namespace to",
    "start": "791180",
    "end": "796989"
  },
  {
    "text": "when a job is looking for a particular identity the controller would then handle bootstrapping the job with that at any resource it automatically handles",
    "start": "796989",
    "end": "803989"
  },
  {
    "text": "updating expiring tokens when dealing with long-running jobs now these long-running jobs are quite",
    "start": "803989",
    "end": "809179"
  },
  {
    "text": "common in Bloomberg's ecosystem as I'm sure it is in yours when we deal with spark ml as well spark streaming jobs",
    "start": "809179",
    "end": "814220"
  },
  {
    "text": "that could run for weeks so how will that update look in the world of secure HDFS well the controller within function",
    "start": "814220",
    "end": "821779"
  },
  {
    "text": "as a proxy user on behalf of the job user and using the controller's own principle it would receive an HDFS",
    "start": "821779",
    "end": "827509"
  },
  {
    "text": "service ticket by communicating with the Kerberos node next we'll use that ticket to receive a delegation token that's",
    "start": "827509",
    "end": "834079"
  },
  {
    "text": "bounded to the job users principle to ensure that the job user only is actually the job user data and we also",
    "start": "834079",
    "end": "839779"
  },
  {
    "text": "provide additional security if I put in a token and the secret only privy to that user therefore the expire token would be",
    "start": "839779",
    "end": "846379"
  },
  {
    "text": "updated in the secret the driver and executors based on kubernetes will automatically detect updates to the",
    "start": "846379",
    "end": "852259"
  },
  {
    "text": "secrets and have their tokens renewed as a result however despite detecting",
    "start": "852259",
    "end": "857299"
  },
  {
    "text": "updates to the token file to hook into spark we added an additional feature to spark a doob utils it allows for the",
    "start": "857299",
    "end": "863809"
  },
  {
    "text": "driver upon changes to the token secret to send token updates to the executors to then kick off the same UGI process",
    "start": "863809",
    "end": "870589"
  },
  {
    "text": "that is consistent across all resource managers so in addition we saw that in other jobs like tensorflow which lactose",
    "start": "870589",
    "end": "876739"
  },
  {
    "text": "had a beauty on the soju butyl UGI feature we needed to support renewal",
    "start": "876739",
    "end": "882079"
  },
  {
    "text": "to be done via delegation tokens and so you propose a patch to Hadoop core to support this so the reason for this is",
    "start": "882079",
    "end": "888410"
  },
  {
    "text": "the following on the implicit the implicit assumption in the old mutation was that yarn functions as a scheduling",
    "start": "888410",
    "end": "895279"
  },
  {
    "text": "agent with access to appropriate key tabs for the renewal of Kerberos tickets and delegation tokens however jobs I",
    "start": "895279",
    "end": "902449"
  },
  {
    "text": "would interact with kerberized Hadoop services like HDFS hiver HBase and use an external schedule like",
    "start": "902449",
    "end": "908239"
  },
  {
    "text": "communities typically don't have access to key tabs so in these specific cases delegation tokens are a logical choice",
    "start": "908239",
    "end": "914509"
  },
  {
    "text": "for interacting with a kerberized cluster as tokens could be issued based on some sort of external off mechanism",
    "start": "914509",
    "end": "921139"
  },
  {
    "text": "like cube l dev for example so solving the first two challenges we listed the",
    "start": "921139",
    "end": "926660"
  },
  {
    "text": "last one we wanted to discuss was scaling spark by enabling too to compute in a traditionally co-located",
    "start": "926660",
    "end": "932230"
  },
  {
    "text": "co-located application so let's go a little to the spark internals to allow",
    "start": "932230",
    "end": "937610"
  },
  {
    "start": "935000",
    "end": "935000"
  },
  {
    "text": "for us to understand where they're strict where the restrictions exist stopping us from enabling to zag so",
    "start": "937610",
    "end": "942740"
  },
  {
    "text": "let's define an example user story so you have your example spark application so let's you have a large data set a few",
    "start": "942740",
    "end": "949339"
  },
  {
    "text": "petabytes in size for example and you wish to run a couple simple transformations on it like adding +1 to",
    "start": "949339",
    "end": "954920"
  },
  {
    "text": "your values and a few more complex operations like sort for example before collecting the results into the driver",
    "start": "954920",
    "end": "960939"
  },
  {
    "text": "so spark leverage is an operator graph for users to allow for them to apply a variety of transformations that won't",
    "start": "960939",
    "end": "967490"
  },
  {
    "text": "get sent to the dag scheduler until an action like collect for example is performed so in this diagram we show how",
    "start": "967490",
    "end": "972529"
  },
  {
    "text": "operations are broken up into stages which stage can contain tasks abused",
    "start": "972529",
    "end": "977569"
  },
  {
    "text": "tasks based on data partitions now there so there could be many tasks Piatt",
    "start": "977569",
    "end": "983689"
  },
  {
    "text": "pipeline together in a single stage and in the end the stages are passed into the task scheduler what you said earlier",
    "start": "983689",
    "end": "988879"
  },
  {
    "text": "is responsible for distributing the task to executors so you pay attention to the space in between stage and and n plus",
    "start": "988879",
    "end": "995480"
  },
  {
    "text": "one you can see an alt all communication that is called a shuffle operation so this alt all communication is critical",
    "start": "995480",
    "end": "1001269"
  },
  {
    "text": "for operations like sorting which is done by your example spark application above as I said earlier in the user story so in technical terms shuffles",
    "start": "1001269",
    "end": "1009370"
  },
  {
    "text": "when you move data from one stage to another while partitioning your data so the reducers receive their respective",
    "start": "1009370",
    "end": "1015550"
  },
  {
    "text": "partitions so there are two specific important things to note about the following shuffle example illustrate the",
    "start": "1015550",
    "end": "1022810"
  },
  {
    "text": "first of which is that SPARC implements shuffle using files on disk now this was designed with a strong fault tolerance",
    "start": "1022810",
    "end": "1028058"
  },
  {
    "text": "guarantees as well as a simple reasoning that not all shuffle data could necessarily fit in memory which as you",
    "start": "1028059",
    "end": "1033788"
  },
  {
    "text": "can imagine in the use case I just gave with petabytes in size just cut that's quite common the second is that in stage",
    "start": "1033789",
    "end": "1039428"
  },
  {
    "text": "n we have four mappers for the four partitions and in stage n plus one we have three reducers for the three of",
    "start": "1039429",
    "end": "1045010"
  },
  {
    "text": "resulting partitions so this serves as a valuable introduction to some concept called dynamic resource allocation which",
    "start": "1045010",
    "end": "1051940"
  },
  {
    "text": "is when you have a spark application miss giving back resources to the cluster if they no longer need those",
    "start": "1051940",
    "end": "1057010"
  },
  {
    "text": "resources they can also request them back again later when they needed based on their demands as you can see we could actually give",
    "start": "1057010",
    "end": "1063669"
  },
  {
    "text": "back a work resource went after completing stage and as we moved to stage n +1 this is",
    "start": "1063669",
    "end": "1068840"
  },
  {
    "text": "valuable as executors that aren't given tasks should be repurpose for other spark jobs so in the case that we want",
    "start": "1068840",
    "end": "1074150"
  },
  {
    "text": "to rescale our executor talent we need to ensure that these shuffle files aren't lost when executors are scaled down and if these shuffle files are",
    "start": "1074150",
    "end": "1080660"
  },
  {
    "text": "available for newly created executors now what kind of files are used by SPARC",
    "start": "1080660",
    "end": "1086360"
  },
  {
    "start": "1084000",
    "end": "1084000"
  },
  {
    "text": "to enable shuffle the first kind of temporary file that SPARC uses are spill files these files are created when a",
    "start": "1086360",
    "end": "1091730"
  },
  {
    "text": "task running on an executor spills to disk after being unable to store it in memory and as you can imagine this can",
    "start": "1091730",
    "end": "1097100"
  },
  {
    "text": "happen quite often as many spill files can be created on a given stage the next kind of file that can be created as a",
    "start": "1097100",
    "end": "1102740"
  },
  {
    "text": "shuffle file which as I said earlier are created whenever you want to do a shuffle operation between stages this",
    "start": "1102740",
    "end": "1108110"
  },
  {
    "text": "file is created by the mapper via shuffle right which the reducer would then read via shuffle fetch but only parts of partitions and the case that",
    "start": "1108110",
    "end": "1115220"
  },
  {
    "text": "spill files do exist the mapper will then merge all the Spill files that happen in the map stage and put that",
    "start": "1115220",
    "end": "1120230"
  },
  {
    "text": "merge content into the shuffle file for later stages to then have so in the",
    "start": "1120230",
    "end": "1127040"
  },
  {
    "start": "1126000",
    "end": "1126000"
  },
  {
    "text": "current implementation of shuffle shuffle files involve two files when it comes to shuffle fetches by later stages",
    "start": "1127040",
    "end": "1132970"
  },
  {
    "text": "the first of which are the they are the data files which are written by the mapper and the second is an index file",
    "start": "1132970",
    "end": "1138890"
  },
  {
    "text": "which are purply indexes where in the data file the partition begins and ends so for reading shuffle bytes you would",
    "start": "1138890",
    "end": "1145340"
  },
  {
    "text": "first read the index file determine wearing the data file you then seek and read so as you can imagine in the case",
    "start": "1145340",
    "end": "1151190"
  },
  {
    "text": "of a large index file this could have long seek times just for retrieving indexes and obviously in the case of",
    "start": "1151190",
    "end": "1156620"
  },
  {
    "text": "large data files this would have a magnitudes larger seek time now remember when we discuss",
    "start": "1156620",
    "end": "1162560"
  },
  {
    "text": "dynamic resource allocation where you don't necessarily need for execute is a feeling of three partitions well this scaling of your executors is crucial for",
    "start": "1162560",
    "end": "1168890"
  },
  {
    "text": "scaling any production spark environment oh man is critical at Bloomberg specifically so the way that the",
    "start": "1168890",
    "end": "1174320"
  },
  {
    "text": "external shuffle service enables dynamic of resource allocation is that instead of directly communicating with executors",
    "start": "1174320",
    "end": "1179450"
  },
  {
    "text": "for the shuffle files via RPC you could when which could be potentially scaled-down for example in the case of a",
    "start": "1179450",
    "end": "1186110"
  },
  {
    "text": "dynamic environment you would then talk to a shuffle service instance instead now in a traditional spark on yarn",
    "start": "1186110",
    "end": "1192770"
  },
  {
    "text": "environment this shuffle service instance would live on every node and function as a file server for reading",
    "start": "1192770",
    "end": "1197990"
  },
  {
    "text": "data files as well as a cache server for index files and as executors upon finishing its shuffle write they would",
    "start": "1197990",
    "end": "1203780"
  },
  {
    "text": "register those written blocks with the shuffle service and later the shuffle fetches are enabled by the shuffle",
    "start": "1203780",
    "end": "1208880"
  },
  {
    "text": "service by streaming data from the London nodes local disks to the appropriate executors now there are",
    "start": "1208880",
    "end": "1214880"
  },
  {
    "text": "numerous problems with this that we soon found the first of which is that there's a lack of resource isolation so if the",
    "start": "1214880",
    "end": "1220910"
  },
  {
    "text": "shuffle service uses too much memory on the node the whole node goes down and then takes on all the executors that are running on that given node there's also",
    "start": "1220910",
    "end": "1227480"
  },
  {
    "text": "a lack of replication in shovel service instances so without multiple shovel",
    "start": "1227480",
    "end": "1232550"
  },
  {
    "text": "service instances per node you'd have a case where a collocated shuffle instance sales for example and thus require",
    "start": "1232550",
    "end": "1238220"
  },
  {
    "text": "replaying of the entire dag for that for the executors on that node so that the shuffle files can be recreated",
    "start": "1238220",
    "end": "1244130"
  },
  {
    "text": "however they're all sending a local disk anyway so there's no reason for you to recreate them furthermore the continuous",
    "start": "1244130",
    "end": "1250700"
  },
  {
    "text": "uptime of the shuffle service is required for the node to then accept executors to run so if a shovel service",
    "start": "1250700",
    "end": "1256190"
  },
  {
    "text": "goes down on that node then no executors can be schedulable on that node and lastly what happens we move this to",
    "start": "1256190",
    "end": "1261500"
  },
  {
    "text": "kubernetes for example so in kubernetes world there'd be an assumption to",
    "start": "1261500",
    "end": "1266570"
  },
  {
    "text": "container volume will be shared between the executor and the shuffle service now this design breaks in certain cluster",
    "start": "1266570",
    "end": "1272570"
  },
  {
    "text": "configurations where co-located storage is not allowed which I assume is pretty consistent across this room as such",
    "start": "1272570",
    "end": "1278120"
  },
  {
    "text": "there needed to be a solution where we could redesign the communication between SPARC and the external shuffle service to the Slee enable dynamic resource",
    "start": "1278120",
    "end": "1284630"
  },
  {
    "text": "allocation so the solution Blumberg developed with Palantir along many",
    "start": "1284630",
    "end": "1290270"
  },
  {
    "start": "1287000",
    "end": "1287000"
  },
  {
    "text": "developers from many companies would be an api within the current shuffle implementation that allows for pluggable",
    "start": "1290270",
    "end": "1295430"
  },
  {
    "text": "writing and reading of shuffle bytes so this API allows for developers to extend",
    "start": "1295430",
    "end": "1300710"
  },
  {
    "text": "their shuffle storage with any preferred backing storage layer as well as a database layer that they can plug in",
    "start": "1300710",
    "end": "1306790"
  },
  {
    "text": "thereby promoting disaggregated compute for shuffle files specifically the EPI",
    "start": "1306790",
    "end": "1311840"
  },
  {
    "text": "leverage is a plugging tree that defines various components and enabling shuffle the driver components allow for",
    "start": "1311840",
    "end": "1317960"
  },
  {
    "text": "application initialization and cleaning the executor components provide flexibility towards writing strategy may",
    "start": "1317960",
    "end": "1323870"
  },
  {
    "text": "be output streams or read about byte channels for Java neo optimizations like file channel or",
    "start": "1323870",
    "end": "1329270"
  },
  {
    "text": "sokka channel and flexibility for reading as well via the shuffle readers and the shuffle locations allow",
    "start": "1329270",
    "end": "1335180"
  },
  {
    "text": "developers the flexibility to create unique solutions that can be comprised of primary and backup locations so where",
    "start": "1335180",
    "end": "1342050"
  },
  {
    "text": "does it become powerful so given the problems that we address little earlier we developed some solutions",
    "start": "1342050",
    "end": "1348130"
  },
  {
    "start": "1348000",
    "end": "1348000"
  },
  {
    "text": "so both Bloomberg and Palantir have been actually working on various example implementations I'm going to go through",
    "start": "1348130",
    "end": "1353180"
  },
  {
    "text": "some of them as there could be used case where it becomes very helpful for you guys and would love to hear your comments on them the first of which is",
    "start": "1353180",
    "end": "1360620"
  },
  {
    "text": "using individual file servers so in essence you're treating the shuffle services as file servers that accept",
    "start": "1360620",
    "end": "1365750"
  },
  {
    "text": "streams of bytes as input for writing shuffle data and providing endpoints for reading files as byte streams the way",
    "start": "1365750",
    "end": "1373550"
  },
  {
    "text": "this will work is that the executors begin by writing as shuffle blocks to an external shuffle service instance that",
    "start": "1373550",
    "end": "1379130"
  },
  {
    "text": "will function as a file server for its own files for its own file system which is in the case will be here the executor",
    "start": "1379130",
    "end": "1385880"
  },
  {
    "text": "can then optionally handle replication by doing a second rewrite to another file server for example the one over here and what's unique about that one",
    "start": "1385880",
    "end": "1392180"
  },
  {
    "text": "for the sake of example is that instead of writing it's blocked to its local file storage it's writing to an external",
    "start": "1392180",
    "end": "1397730"
  },
  {
    "text": "file system like HDFS upon completion of its writing the executor will then",
    "start": "1397730",
    "end": "1402770"
  },
  {
    "text": "report below the backup would then report all of its primary and backup in",
    "start": "1402770",
    "end": "1408080"
  },
  {
    "text": "secondary locations to the driver now something to note in this specific design is that there are two network",
    "start": "1408080",
    "end": "1413930"
  },
  {
    "text": "transfers if the file server is communicating with an external file system the first network transfer will",
    "start": "1413930",
    "end": "1420290"
  },
  {
    "text": "then happen with actually communicating with the file server itself and the second will be communicating with HDFS",
    "start": "1420290",
    "end": "1427960"
  },
  {
    "text": "however there's some clear advantages to this design that cover the problems that were addressed every external shuffle",
    "start": "1427960",
    "end": "1434150"
  },
  {
    "text": "service instance can be run in its own hosts or in an or in isolated container so if any external shuffle service",
    "start": "1434150",
    "end": "1439880"
  },
  {
    "text": "instance fails only the container running the shuffle servers will be affected so therefore it's not necessary",
    "start": "1439880",
    "end": "1444980"
  },
  {
    "text": "but in its implementation you would not need to collocate your shuffle services with your SPARC executors which is important because the whole point is",
    "start": "1444980",
    "end": "1450740"
  },
  {
    "text": "that are trying to separate the two so as the number of executors grow in the system you're also able to also",
    "start": "1450740",
    "end": "1456230"
  },
  {
    "text": "independently scale your shut your shuffle service instances obviously once you put the shuffle service instance",
    "start": "1456230",
    "end": "1462590"
  },
  {
    "text": "behind Roxi you then have another network hop you have to do but that's if you want to do the external shuffle service there's",
    "start": "1462590",
    "end": "1470029"
  },
  {
    "text": "also not need to share local storage with the executors and in fact the shuffle services themselves could be completely stateless if they communicate",
    "start": "1470029",
    "end": "1476779"
  },
  {
    "text": "with an external external storage like they do on the red fork and in the case of persisting to an external storage the",
    "start": "1476779",
    "end": "1483739"
  },
  {
    "text": "shuffle service instance can be shut down as well and the underlying data can be serviced by any remaining external",
    "start": "1483739",
    "end": "1489139"
  },
  {
    "text": "shuffle service instances another example on this design we are trying to",
    "start": "1489139",
    "end": "1495080"
  },
  {
    "start": "1493000",
    "end": "1493000"
  },
  {
    "text": "asynchronously back up our locations as we're writing our shuffle blocks to local disk so upon writing our shuffle",
    "start": "1495080",
    "end": "1500210"
  },
  {
    "text": "blocks index files to the executors local file system the executor with an asynchronously backup it's shuffle",
    "start": "1500210",
    "end": "1505369"
  },
  {
    "text": "blocks index files to a stable DFS now because it is async the executor can",
    "start": "1505369",
    "end": "1510649"
  },
  {
    "text": "immediately report to the driver the primary and backup locations upon finishing it's local right however what happens when we do the read",
    "start": "1510649",
    "end": "1517970"
  },
  {
    "text": "from the backup and it's not done writing which is quite common probably one strategy is to do an asynchronous shuffle fetch such that you would open",
    "start": "1517970",
    "end": "1524690"
  },
  {
    "text": "up concurrent input streams across all your file locations both primary and backups and take the fastest response",
    "start": "1524690",
    "end": "1530239"
  },
  {
    "text": "and otherwise you would just block if there's no response so this solution is also satisfying for the problems that we",
    "start": "1530239",
    "end": "1536359"
  },
  {
    "text": "listed since we removed the external shuffle service here if one executor fails would only impact the application",
    "start": "1536359",
    "end": "1541909"
  },
  {
    "text": "that executors are part of also the backup storage layer would be quite resilient and highly available and the",
    "start": "1541909",
    "end": "1549200"
  },
  {
    "text": "solution is compatible with containerized storage since the local disk does not need to be shared between any two processes in the system",
    "start": "1549200",
    "end": "1555039"
  },
  {
    "text": "exchanging shuffle data would be done either via RPC between executors or between RPC between an executor and the",
    "start": "1555039",
    "end": "1561529"
  },
  {
    "text": "remote file servers sorry I'm in the remote backup storage so the last",
    "start": "1561529",
    "end": "1567440"
  },
  {
    "start": "1566000",
    "end": "1566000"
  },
  {
    "text": "publication as an example is to do a direct write to distributed file system or some kind of stable DFS so where the",
    "start": "1567440",
    "end": "1574489"
  },
  {
    "text": "driver would maintain all of its own shuffle metadata without using a shuffle service and the executor can then",
    "start": "1574489",
    "end": "1580249"
  },
  {
    "text": "circumvent the need for a file system and would directly write its shuffle blocks index files to the stable of DFS",
    "start": "1580249",
    "end": "1585320"
  },
  {
    "text": "so something to note here in contrast the individual file server example that was earlier you now only require one one",
    "start": "1585320",
    "end": "1591950"
  },
  {
    "text": "network transfer as you directly communicate with the DIA and for further optimizations you could also leverage an in-memory key value",
    "start": "1591950",
    "end": "1598490"
  },
  {
    "text": "store like ignite or Redis and you actually remove the need for index file completely by just having the partition",
    "start": "1598490",
    "end": "1604399"
  },
  {
    "text": "to be part of that key this example also solves the problems they were addressing earlier because it provides the",
    "start": "1604399",
    "end": "1610519"
  },
  {
    "text": "necessary resource isolation also with the scalability factor that the scalability factor only relies on the",
    "start": "1610519",
    "end": "1615950"
  },
  {
    "text": "scalability of the shuffle operation as well as the scalability of the the data storage system which in this case is",
    "start": "1615950",
    "end": "1621139"
  },
  {
    "text": "going to meet quite performant so upon completion of a proposed API and despite",
    "start": "1621139",
    "end": "1627350"
  },
  {
    "text": "making some headway on trying to run some of our SPARC deployments you use these implementations there's still a lot of work that needs to be done",
    "start": "1627350",
    "end": "1632929"
  },
  {
    "text": "obviously so the API and some of the implementations I present here are formalized in an SPI P under the JIRA",
    "start": "1632929",
    "end": "1640340"
  },
  {
    "text": "tickets SPARC two five two nine nine and is an active discussion of the other spark'd of mailing lists so many",
    "start": "1640340",
    "end": "1645559"
  },
  {
    "text": "different shuffle implementations specifically with remote pluggable shuffle are presented at various conferences and it main focus of those",
    "start": "1645559",
    "end": "1651769"
  },
  {
    "text": "workers try to unify these implementations with this form of a generic API however in the work of",
    "start": "1651769",
    "end": "1657799"
  },
  {
    "text": "enabling a fully disaggregated SPARC environment there are still challenges that remain so the work the slim needs",
    "start": "1657799",
    "end": "1662899"
  },
  {
    "text": "to be done involves fetch failed exception handling and the dag scheduler for failed shovel fetches which is quite",
    "start": "1662899",
    "end": "1669320"
  },
  {
    "text": "common so these shall fetch failures our primary sources of query latency in",
    "start": "1669320",
    "end": "1674690"
  },
  {
    "text": "SPARC and as such we will like the we would like the implementer to customize the exception that is thrown for their given specific implementations so for",
    "start": "1674690",
    "end": "1681830"
  },
  {
    "text": "example in the case of your direct writing to DFS your have situation where you don't want",
    "start": "1681830",
    "end": "1687590"
  },
  {
    "text": "necessarily do a fetch failure because you want to do generic exception because the chances of you being unable to freshen DFS is quite low because of the",
    "start": "1687590",
    "end": "1693980"
  },
  {
    "text": "scalability of your vacuum store the next thing we have to do is extend the API to handle spills and cache files so",
    "start": "1693980",
    "end": "1700129"
  },
  {
    "text": "cache files is another kind of temporary file that I didn't get into in this talk luckily these two api's are quite",
    "start": "1700129",
    "end": "1706279"
  },
  {
    "text": "orthogonal to our work and so we don't we still need to necessarily extend them",
    "start": "1706279",
    "end": "1711590"
  },
  {
    "text": "to make this fully supported but luckily we also pretty much those are the last two things that are needed to fully move",
    "start": "1711590",
    "end": "1716840"
  },
  {
    "text": "that cash flow caching layer outside of our compute nodes didn't fully promote",
    "start": "1716840",
    "end": "1722000"
  },
  {
    "text": "to sag and the last things to do performance benchmarking across different implementations before deciding which one is appropriate",
    "start": "1722000",
    "end": "1727910"
  },
  {
    "text": "Sparks environment as always we encourage collaboration as we move towards disaggregated compute I'm sorry",
    "start": "1727910",
    "end": "1733660"
  },
  {
    "text": "as we move towards the disaggregate compute and I am excited to see where this work will go and how we can work",
    "start": "1733660",
    "end": "1738830"
  },
  {
    "text": "together thank you okay yeah any",
    "start": "1738830",
    "end": "1749030"
  },
  {
    "text": "questions awesome hello thanks for the presentation I've",
    "start": "1749030",
    "end": "1757160"
  },
  {
    "text": "got one question maybe you know why all the old work from spark on Burnett's",
    "start": "1757160",
    "end": "1763840"
  },
  {
    "text": "branch was removed in spark to perf 2.4 because there was official spark on",
    "start": "1763840",
    "end": "1770600"
  },
  {
    "text": "kubernetes project and there was dynamic resource allocation there and external",
    "start": "1770600",
    "end": "1776660"
  },
  {
    "text": "shuffle ok so the question was on the removal of the fork so so I was part of",
    "start": "1776660",
    "end": "1782150"
  },
  {
    "text": "the team that made the fork and so as a removing upstream there need to be a certain considerations that were",
    "start": "1782150",
    "end": "1787610"
  },
  {
    "text": "required for us to move upstream and that was to be consistent across the cluster schedulers um and our approach",
    "start": "1787610",
    "end": "1793730"
  },
  {
    "text": "was very specific for containerized environments for example we removed the unit container from a lot of our logic",
    "start": "1793730",
    "end": "1799130"
  },
  {
    "text": "for the driver um which included some of the work that was necessary for I believe we also had",
    "start": "1799130",
    "end": "1805070"
  },
  {
    "text": "a file server that you can actually write most of your files he wanted to do local file reads and so a lot of that",
    "start": "1805070",
    "end": "1812450"
  },
  {
    "text": "was in pretty much you're using too many kubernetes specific things in an environment that's supposed to be agnostic to cluster managers um so there",
    "start": "1812450",
    "end": "1818330"
  },
  {
    "text": "was there was just back and forth between the community where we needed to redesign it and as I discussed earlier the new design of the generic API",
    "start": "1818330",
    "end": "1824780"
  },
  {
    "text": "promotes the ability for us to have various different shuffle mutations just leverage these wrappers allow for you to",
    "start": "1824780",
    "end": "1831320"
  },
  {
    "text": "directly call write output stream read output stream and various shuffle locations that could be any location ok",
    "start": "1831320",
    "end": "1841040"
  },
  {
    "text": "we maybe we should keep this in the center ok",
    "start": "1841040",
    "end": "1844870"
  },
  {
    "text": "Thanks you mentioned that you have quote",
    "start": "1850940",
    "end": "1856649"
  },
  {
    "text": "a location for the spawning of executors could you talk a bit more about that how",
    "start": "1856649",
    "end": "1862320"
  },
  {
    "text": "you manage that quota to do what sorry for the users to spawn executors mm-hmm",
    "start": "1862320",
    "end": "1867360"
  },
  {
    "text": "so this quota management resource quota management okay so you pretty much",
    "start": "1867360",
    "end": "1872429"
  },
  {
    "text": "define a resource quota or your name stays so our workspaces are defined around a namespace and then based not",
    "start": "1872429",
    "end": "1877500"
  },
  {
    "text": "use calculate the size of your cluster that you're specifying like let's say when you launch a spark job you request",
    "start": "1877500",
    "end": "1882720"
  },
  {
    "text": "executors memory and Driver memory just based on that calculation you would then determine if this job is too large and",
    "start": "1882720",
    "end": "1888299"
  },
  {
    "text": "the question of how you would handle scheduling the gang scheduling or FIFO or whatever of that nature between spark jobs that's still under investigation",
    "start": "1888299",
    "end": "1894620"
  },
  {
    "text": "whether there's an appropriate place for that in terms of how you schedule jobs in terms of reach or scheduling but the",
    "start": "1894620",
    "end": "1900809"
  },
  {
    "text": "simplest approach would then be just like whether you're over prevent your pretty much you're giving too many reasons to your spark jobs and be",
    "start": "1900809",
    "end": "1906389"
  },
  {
    "text": "blocked from launching if that makes sense I think there was another question",
    "start": "1906389",
    "end": "1917299"
  },
  {
    "text": "hello we are currently running spark on young but most the reason is because we",
    "start": "1917600",
    "end": "1924720"
  },
  {
    "text": "have a pool of resource which is purely just the full spark and we have many researchers running in Sparks your",
    "start": "1924720",
    "end": "1930779"
  },
  {
    "text": "opponent's for young they have this ability to preempt jobs so when the",
    "start": "1930779",
    "end": "1937679"
  },
  {
    "text": "cluster is empty we allow one researcher to just fully use the whole cluster but when the second researchers join singing",
    "start": "1937679",
    "end": "1944700"
  },
  {
    "text": "young will make sure it split this cluster half and a half to make it fair for the two researchers but on Cuban",
    "start": "1944700",
    "end": "1952139"
  },
  {
    "text": "artists it doesn't look like we can do this and is it the feature you're going to support in the future so fairness",
    "start": "1952139",
    "end": "1958350"
  },
  {
    "text": "scheduling is that internal spark are you talking about to kubernetes specifically well it's its own",
    "start": "1958350",
    "end": "1964320"
  },
  {
    "text": "communities so we want to use spark on communities but for this reason we have to run it to be done",
    "start": "1964320",
    "end": "1971809"
  },
  {
    "text": "I think that could be promoted for",
    "start": "1973420",
    "end": "1980200"
  },
  {
    "text": "Anna's scheduling should be able to be done logic about it's not a spark internal thing so I guess it's based on the cluster manager itself I would say",
    "start": "1980200",
    "end": "1988200"
  },
  {
    "text": "I'll take that offline I'll have to look into that a little bit thank you so much",
    "start": "1988200",
    "end": "1993780"
  },
  {
    "text": "that we have one question the mech",
    "start": "2000110",
    "end": "2003799"
  },
  {
    "text": "yes so this this is the slide over here so this work is going to be is already",
    "start": "2008770",
    "end": "2019640"
  },
  {
    "start": "2019000",
    "end": "2019000"
  },
  {
    "text": "running up for spark two five two nine nine and so we already submitted a an SB",
    "start": "2019640",
    "end": "2026210"
  },
  {
    "text": "IP for that and right now it's in discussion and then we're gonna start a voting soon and then it's a matter of whether we can get in a spark 300 or",
    "start": "2026210",
    "end": "2032419"
  },
  {
    "text": "spark 31 I guess the whole function is that it's gonna be an upstream work and then the implementations would probably",
    "start": "2032419",
    "end": "2038360"
  },
  {
    "text": "be surfaced as a spork so that you can for example leverage our implementations",
    "start": "2038360",
    "end": "2043610"
  },
  {
    "text": "of the individual file server or calendars and petitions of the direct DFS writing or whatnot",
    "start": "2043610",
    "end": "2050560"
  },
  {
    "text": "and a question",
    "start": "2065580",
    "end": "2068630"
  },
  {
    "text": "okay got one in the back go for key pass",
    "start": "2073970",
    "end": "2079260"
  },
  {
    "text": "the mic please",
    "start": "2079260",
    "end": "2082398"
  },
  {
    "text": "my question is about performance currently the shuffle service is is is a",
    "start": "2094040",
    "end": "2101310"
  },
  {
    "text": "file server that tries to the local file system and it's one of the of the",
    "start": "2101310",
    "end": "2106680"
  },
  {
    "text": "biggest issue for spark graphs perform performance so when you move to writing",
    "start": "2106680",
    "end": "2113520"
  },
  {
    "text": "directly to HDFS Torn external HDFS cluster that does it have you use it on",
    "start": "2113520",
    "end": "2120090"
  },
  {
    "text": "big workloads on big spark drops so as the problem question I guess comes down",
    "start": "2120090",
    "end": "2126270"
  },
  {
    "text": "to like query latency in general like when you have how long does it usually take most of the time that's spent on",
    "start": "2126270",
    "end": "2131760"
  },
  {
    "text": "shuffle and so becoming an issue with regards to fetch failed exceptions so your fetch shell exception is going to",
    "start": "2131760",
    "end": "2137670"
  },
  {
    "text": "cause a predominantly large portion of variability in your spark job times and so to stabilize at least that by",
    "start": "2137670",
    "end": "2144540"
  },
  {
    "text": "ensuring they're writing to a stable DFS that should at least provide users",
    "start": "2144540",
    "end": "2151610"
  },
  {
    "text": "consistency in their spark jobs as well as the ability for it to be faster we also see that network and network itself",
    "start": "2151610",
    "end": "2160620"
  },
  {
    "text": "has becoming more and more developed in general so we're saying that our network costs are gonna go down well our",
    "start": "2160620",
    "end": "2166320"
  },
  {
    "text": "determining our disk throughputs we're seeing that we're not gonna we were focusing more on network than disk throughput and so we're seeing that when",
    "start": "2166320",
    "end": "2172170"
  },
  {
    "text": "he comes to larger jobs we want to focus more on not relying on disk throughput as that's hot you'd say the industry's",
    "start": "2172170",
    "end": "2179490"
  },
  {
    "text": "not moving more towards that as opposed to network the other thing is that there's some jobs that physically you",
    "start": "2179490",
    "end": "2184830"
  },
  {
    "text": "can't so there were some other talks um by Facebook during there and during spark summit where they were discussing",
    "start": "2184830",
    "end": "2190290"
  },
  {
    "text": "write amplification do you just um shuffles and so in some cases you're for example one petabyte of shuffle",
    "start": "2190290",
    "end": "2198000"
  },
  {
    "text": "operation can results in three times that so three petabytes of shuffle files in spell files written right so that",
    "start": "2198000",
    "end": "2204090"
  },
  {
    "text": "physically the the ability to write that all in disk is sometimes impossible and so there is a need then to write to DFS",
    "start": "2204090",
    "end": "2211790"
  },
  {
    "text": "if that answers your question so there's it's caught it's a combination of there's no way to do it for large jobs",
    "start": "2211790",
    "end": "2217440"
  },
  {
    "text": "as well as the industry is moving more towards making Network faster because this clear foot is not getting",
    "start": "2217440",
    "end": "2223260"
  },
  {
    "text": "better",
    "start": "2223260",
    "end": "2225350"
  },
  {
    "text": "okay going once twice okay awesome thank you guys so much appreciate it",
    "start": "2235800",
    "end": "2241700"
  },
  {
    "text": "[Applause]",
    "start": "2241700",
    "end": "2247630"
  }
]