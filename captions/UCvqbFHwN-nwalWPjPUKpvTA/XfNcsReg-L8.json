[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "hello everyone thank you for joining me",
    "start": "80",
    "end": "2080"
  },
  {
    "text": "to this session",
    "start": "2080",
    "end": "3199"
  },
  {
    "text": "i'm yoda shafril and today we are going",
    "start": "3199",
    "end": "6160"
  },
  {
    "text": "to discuss about",
    "start": "6160",
    "end": "7279"
  },
  {
    "text": "fail scheduling for deep learning",
    "start": "7279",
    "end": "8800"
  },
  {
    "text": "workloads in kubernetes",
    "start": "8800",
    "end": "11599"
  },
  {
    "text": "but first i would like to introduce",
    "start": "11599",
    "end": "13200"
  },
  {
    "text": "myself i'm a software team lead at run",
    "start": "13200",
    "end": "15759"
  },
  {
    "text": "ai",
    "start": "15759",
    "end": "16800"
  },
  {
    "text": "running i provide the compute management",
    "start": "16800",
    "end": "18720"
  },
  {
    "text": "platform for ai workloads",
    "start": "18720",
    "end": "20960"
  },
  {
    "text": "i am also a contributor of kubernetes",
    "start": "20960",
    "end": "23840"
  },
  {
    "text": "and",
    "start": "23840",
    "end": "24240"
  },
  {
    "text": "i live and work from tel aviv",
    "start": "24240",
    "end": "27439"
  },
  {
    "start": "27000",
    "end": "123000"
  },
  {
    "text": "as you probably know data scientists run",
    "start": "27439",
    "end": "29920"
  },
  {
    "text": "their workloads on gpus",
    "start": "29920",
    "end": "31840"
  },
  {
    "text": "the gpus are shared in the cluster",
    "start": "31840",
    "end": "33840"
  },
  {
    "text": "between the data scientists",
    "start": "33840",
    "end": "35600"
  },
  {
    "text": "and we see more and more clusters that",
    "start": "35600",
    "end": "37760"
  },
  {
    "text": "are moving to use kubernetes",
    "start": "37760",
    "end": "40320"
  },
  {
    "text": "now in deep learning workloads can run",
    "start": "40320",
    "end": "42559"
  },
  {
    "text": "for days or even weeks",
    "start": "42559",
    "end": "44559"
  },
  {
    "text": "so when the data scientists share the",
    "start": "44559",
    "end": "46800"
  },
  {
    "text": "gpu resources",
    "start": "46800",
    "end": "48160"
  },
  {
    "text": "deciding which user should be able to",
    "start": "48160",
    "end": "49840"
  },
  {
    "text": "get the next gpu can be critical",
    "start": "49840",
    "end": "52559"
  },
  {
    "text": "a wrong decision can cause starvation",
    "start": "52559",
    "end": "55600"
  },
  {
    "text": "in this talk i will explain how does",
    "start": "55600",
    "end": "57920"
  },
  {
    "text": "kubernetes scheduler",
    "start": "57920",
    "end": "59280"
  },
  {
    "text": "decide what pod should be allocated next",
    "start": "59280",
    "end": "62079"
  },
  {
    "text": "and discuss with you",
    "start": "62079",
    "end": "63600"
  },
  {
    "text": "how can we achieve fairness between",
    "start": "63600",
    "end": "65600"
  },
  {
    "text": "users when sharing gpus in kubernetes",
    "start": "65600",
    "end": "70000"
  },
  {
    "text": "let's start with explaining the way",
    "start": "70000",
    "end": "72080"
  },
  {
    "text": "kubernetes schedule pods",
    "start": "72080",
    "end": "74159"
  },
  {
    "text": "in a very simplified way the scheduler",
    "start": "74159",
    "end": "76640"
  },
  {
    "text": "picks a pending pod",
    "start": "76640",
    "end": "78000"
  },
  {
    "text": "and then attempts to bind the pod to the",
    "start": "78000",
    "end": "80080"
  },
  {
    "text": "most suitable node",
    "start": "80080",
    "end": "82720"
  },
  {
    "text": "we will be focusing today on the part",
    "start": "82720",
    "end": "84880"
  },
  {
    "text": "where the scheduler picks the right part",
    "start": "84880",
    "end": "86720"
  },
  {
    "text": "for scheduling",
    "start": "86720",
    "end": "89520"
  },
  {
    "text": "the default scheduler stores the pending",
    "start": "89759",
    "end": "91680"
  },
  {
    "text": "pods in a heap",
    "start": "91680",
    "end": "93360"
  },
  {
    "text": "the ip is sorted by the priority and by",
    "start": "93360",
    "end": "95840"
  },
  {
    "text": "the creation time of the pods",
    "start": "95840",
    "end": "98320"
  },
  {
    "text": "so the next pod to be allocated will be",
    "start": "98320",
    "end": "101040"
  },
  {
    "text": "stored at the head of the heap",
    "start": "101040",
    "end": "103840"
  },
  {
    "text": "this is a very simplified way of",
    "start": "103840",
    "end": "105520"
  },
  {
    "text": "presenting how does the default",
    "start": "105520",
    "end": "107439"
  },
  {
    "text": "scheduler decide what pod should be",
    "start": "107439",
    "end": "109200"
  },
  {
    "text": "allocated next",
    "start": "109200",
    "end": "110880"
  },
  {
    "text": "there is also another leap called pod",
    "start": "110880",
    "end": "113119"
  },
  {
    "text": "back-off queue",
    "start": "113119",
    "end": "114159"
  },
  {
    "text": "and a map for the unscheduleable pods",
    "start": "114159",
    "end": "116640"
  },
  {
    "text": "but they are not relevant to the purpose",
    "start": "116640",
    "end": "118399"
  },
  {
    "text": "of what i'm about to explain",
    "start": "118399",
    "end": "120000"
  },
  {
    "text": "so we will not get into them",
    "start": "120000",
    "end": "123520"
  },
  {
    "text": "using the default scheduler of",
    "start": "123520",
    "end": "124880"
  },
  {
    "text": "kubernetes for sharing a cluster with",
    "start": "124880",
    "end": "127280"
  },
  {
    "text": "limited number",
    "start": "127280",
    "end": "128000"
  },
  {
    "text": "of gpus can cause fairness issues",
    "start": "128000",
    "end": "130560"
  },
  {
    "text": "because",
    "start": "130560",
    "end": "131200"
  },
  {
    "text": "data scientists can monopolize the",
    "start": "131200",
    "end": "133200"
  },
  {
    "text": "cluster",
    "start": "133200",
    "end": "135040"
  },
  {
    "text": "one way for users to monopolize the gpus",
    "start": "135040",
    "end": "137599"
  },
  {
    "text": "is to submit pods with the highest",
    "start": "137599",
    "end": "139200"
  },
  {
    "text": "priority",
    "start": "139200",
    "end": "139920"
  },
  {
    "text": "they are allowed to submit as users this",
    "start": "139920",
    "end": "142879"
  },
  {
    "text": "would make their pods move to the top of",
    "start": "142879",
    "end": "144800"
  },
  {
    "text": "the hip",
    "start": "144800",
    "end": "145520"
  },
  {
    "text": "causing their pods to be allocated first",
    "start": "145520",
    "end": "148560"
  },
  {
    "text": "as you can see in this example bob",
    "start": "148560",
    "end": "150640"
  },
  {
    "text": "submits pods with higher priority than",
    "start": "150640",
    "end": "152720"
  },
  {
    "text": "alice",
    "start": "152720",
    "end": "153440"
  },
  {
    "text": "so his pods will be at the top of the",
    "start": "153440",
    "end": "155920"
  },
  {
    "text": "hip",
    "start": "155920",
    "end": "156480"
  },
  {
    "text": "and therefore will be scheduled first",
    "start": "156480",
    "end": "159519"
  },
  {
    "text": "eventually bob can monopolize and use",
    "start": "159519",
    "end": "162000"
  },
  {
    "text": "all the gpus in the cluster",
    "start": "162000",
    "end": "165040"
  },
  {
    "text": "another way to monopolize the gpus is to",
    "start": "165040",
    "end": "167280"
  },
  {
    "text": "submit as many pods as possible",
    "start": "167280",
    "end": "169280"
  },
  {
    "text": "this will cause more pods to be in the",
    "start": "169280",
    "end": "171120"
  },
  {
    "text": "hip and even have smaller creation time",
    "start": "171120",
    "end": "174560"
  },
  {
    "text": "as you can see in this example bob",
    "start": "174560",
    "end": "176720"
  },
  {
    "text": "submitted",
    "start": "176720",
    "end": "177840"
  },
  {
    "text": "many pods and the ip contains mostly",
    "start": "177840",
    "end": "180560"
  },
  {
    "text": "ease parts",
    "start": "180560",
    "end": "181760"
  },
  {
    "text": "and therefore you will also have more",
    "start": "181760",
    "end": "183920"
  },
  {
    "text": "gpus allocated",
    "start": "183920",
    "end": "187040"
  },
  {
    "text": "so our motivation is to build the first",
    "start": "187040",
    "end": "189519"
  },
  {
    "text": "scheduler",
    "start": "189519",
    "end": "190560"
  },
  {
    "text": "one that knows to share the gpu",
    "start": "190560",
    "end": "192239"
  },
  {
    "text": "resources between the users in a fair",
    "start": "192239",
    "end": "194319"
  },
  {
    "text": "way",
    "start": "194319",
    "end": "195040"
  },
  {
    "text": "regardless of the creation time and the",
    "start": "195040",
    "end": "197040"
  },
  {
    "text": "pod priorities",
    "start": "197040",
    "end": "199200"
  },
  {
    "text": "so we want that even in in our examples",
    "start": "199200",
    "end": "202560"
  },
  {
    "text": "where ellie submits less sports and she",
    "start": "202560",
    "end": "204640"
  },
  {
    "text": "submits pods with",
    "start": "204640",
    "end": "206000"
  },
  {
    "text": "lower priority she would still get the",
    "start": "206000",
    "end": "208480"
  },
  {
    "text": "same amount of gpus",
    "start": "208480",
    "end": "209840"
  },
  {
    "text": "as bob now let's discuss how can we",
    "start": "209840",
    "end": "213360"
  },
  {
    "text": "achieve",
    "start": "213360",
    "end": "213920"
  },
  {
    "text": "such a scheduler in v115 of kubernetes",
    "start": "213920",
    "end": "218480"
  },
  {
    "start": "216000",
    "end": "271000"
  },
  {
    "text": "the architecture of the scheduler was",
    "start": "218480",
    "end": "220560"
  },
  {
    "text": "changed",
    "start": "220560",
    "end": "221200"
  },
  {
    "text": "to use the scheduling framework the",
    "start": "221200",
    "end": "224000"
  },
  {
    "text": "scheduling framework",
    "start": "224000",
    "end": "225200"
  },
  {
    "text": "is more pluggable and provides several",
    "start": "225200",
    "end": "227200"
  },
  {
    "text": "extension points to change the behavior",
    "start": "227200",
    "end": "229120"
  },
  {
    "text": "of the scheduler",
    "start": "229120",
    "end": "231200"
  },
  {
    "text": "it was a big game changer for building a",
    "start": "231200",
    "end": "233360"
  },
  {
    "text": "different scheduler",
    "start": "233360",
    "end": "234959"
  },
  {
    "text": "rather than the default scheduler",
    "start": "234959",
    "end": "236799"
  },
  {
    "text": "because it reduces the need for",
    "start": "236799",
    "end": "238560"
  },
  {
    "text": "developers",
    "start": "238560",
    "end": "239519"
  },
  {
    "text": "to build their own scheduler from sketch",
    "start": "239519",
    "end": "242239"
  },
  {
    "text": "and try to keep up with all the",
    "start": "242239",
    "end": "243920"
  },
  {
    "text": "kubernetes features",
    "start": "243920",
    "end": "246799"
  },
  {
    "text": "we have examined the scheduler framework",
    "start": "246799",
    "end": "248640"
  },
  {
    "text": "for implementing a first scheduler",
    "start": "248640",
    "end": "250720"
  },
  {
    "text": "and we decided not to do it",
    "start": "250720",
    "end": "253760"
  },
  {
    "text": "the reason we decided not to use the",
    "start": "253760",
    "end": "255360"
  },
  {
    "text": "framework was that we wanted to use the",
    "start": "255360",
    "end": "257359"
  },
  {
    "text": "different data structure for the pending",
    "start": "257359",
    "end": "259040"
  },
  {
    "text": "parts",
    "start": "259040",
    "end": "259680"
  },
  {
    "text": "rather than just a simple hip of parts",
    "start": "259680",
    "end": "262400"
  },
  {
    "text": "and the framework",
    "start": "262400",
    "end": "263600"
  },
  {
    "text": "simply doesn't let you change this data",
    "start": "263600",
    "end": "266000"
  },
  {
    "text": "structure",
    "start": "266000",
    "end": "267440"
  },
  {
    "text": "so we decided to build our own scheduler",
    "start": "267440",
    "end": "271680"
  },
  {
    "start": "271000",
    "end": "443000"
  },
  {
    "text": "the data structure we use is a",
    "start": "271680",
    "end": "273440"
  },
  {
    "text": "two-dimensional hip",
    "start": "273440",
    "end": "275199"
  },
  {
    "text": "in the manip every node represents a",
    "start": "275199",
    "end": "278400"
  },
  {
    "text": "user",
    "start": "278400",
    "end": "279040"
  },
  {
    "text": "and we sort the ip by the number of the",
    "start": "279040",
    "end": "281120"
  },
  {
    "text": "allocated gpus",
    "start": "281120",
    "end": "282400"
  },
  {
    "text": "this user uses now",
    "start": "282400",
    "end": "285919"
  },
  {
    "text": "every node of a user is associated to",
    "start": "285919",
    "end": "288479"
  },
  {
    "text": "another rip",
    "start": "288479",
    "end": "289440"
  },
  {
    "text": "of ease pending parts the pending pods",
    "start": "289440",
    "end": "292880"
  },
  {
    "text": "are sorted just as in kubernetes by",
    "start": "292880",
    "end": "295440"
  },
  {
    "text": "priority",
    "start": "295440",
    "end": "296160"
  },
  {
    "text": "and by creation time so if once again i",
    "start": "296160",
    "end": "299919"
  },
  {
    "text": "compare this",
    "start": "299919",
    "end": "300800"
  },
  {
    "text": "data structure to the hip of the",
    "start": "300800",
    "end": "302400"
  },
  {
    "text": "scheduling framework",
    "start": "302400",
    "end": "304000"
  },
  {
    "text": "now we can associate depending pods to a",
    "start": "304000",
    "end": "306560"
  },
  {
    "text": "specific user",
    "start": "306560",
    "end": "307840"
  },
  {
    "text": "and sort the users according to their",
    "start": "307840",
    "end": "309840"
  },
  {
    "text": "load on the cluster",
    "start": "309840",
    "end": "311360"
  },
  {
    "text": "rather than just using a simple hip of",
    "start": "311360",
    "end": "313759"
  },
  {
    "text": "pods",
    "start": "313759",
    "end": "315840"
  },
  {
    "text": "the way we allocate pause is this first",
    "start": "315840",
    "end": "319039"
  },
  {
    "text": "we pop the first user from the user zip",
    "start": "319039",
    "end": "322160"
  },
  {
    "text": "then we pop the first part of this user",
    "start": "322160",
    "end": "325120"
  },
  {
    "text": "and allocate this part",
    "start": "325120",
    "end": "327759"
  },
  {
    "text": "in our case in the following example",
    "start": "327759",
    "end": "330800"
  },
  {
    "text": "alice is the first user in the user zip",
    "start": "330800",
    "end": "334560"
  },
  {
    "text": "so we will allocate air first pod and",
    "start": "334560",
    "end": "337600"
  },
  {
    "text": "update the number of allocated gpus",
    "start": "337600",
    "end": "340000"
  },
  {
    "text": "alice currently uses once we will do",
    "start": "340000",
    "end": "342960"
  },
  {
    "text": "that",
    "start": "342960",
    "end": "343440"
  },
  {
    "text": "the heap will be updated and bobo will",
    "start": "343440",
    "end": "346400"
  },
  {
    "text": "now move",
    "start": "346400",
    "end": "347039"
  },
  {
    "text": "to the top of the heap notice that",
    "start": "347039",
    "end": "350720"
  },
  {
    "text": "at the end of the user zip we always",
    "start": "350720",
    "end": "352960"
  },
  {
    "text": "have the most styled user",
    "start": "352960",
    "end": "355360"
  },
  {
    "text": "this helps us in in the goal of",
    "start": "355360",
    "end": "358080"
  },
  {
    "text": "achieving fairness because we know",
    "start": "358080",
    "end": "359840"
  },
  {
    "text": "who is the most starved user and is the",
    "start": "359840",
    "end": "362400"
  },
  {
    "text": "one that should get the next gpu",
    "start": "362400",
    "end": "365440"
  },
  {
    "text": "once again we pop the first pod from",
    "start": "365440",
    "end": "367680"
  },
  {
    "text": "bobspots",
    "start": "367680",
    "end": "368639"
  },
  {
    "text": "and allocate this pod we will continue",
    "start": "368639",
    "end": "371520"
  },
  {
    "text": "to do this flow",
    "start": "371520",
    "end": "372479"
  },
  {
    "text": "until there are no more free gpus in the",
    "start": "372479",
    "end": "374400"
  },
  {
    "text": "cluster so",
    "start": "374400",
    "end": "376560"
  },
  {
    "text": "once we will update the ip alice will",
    "start": "376560",
    "end": "378800"
  },
  {
    "text": "move once again to",
    "start": "378800",
    "end": "380000"
  },
  {
    "text": "the top of the hip and we will allocate",
    "start": "380000",
    "end": "382000"
  },
  {
    "text": "a pod",
    "start": "382000",
    "end": "383280"
  },
  {
    "text": "and we will do the same for bob",
    "start": "383280",
    "end": "386720"
  },
  {
    "text": "as you can see now the gpus are shared",
    "start": "386720",
    "end": "389520"
  },
  {
    "text": "fairly between alice and bob",
    "start": "389520",
    "end": "392479"
  },
  {
    "text": "this solution will provide fairness when",
    "start": "392479",
    "end": "394319"
  },
  {
    "text": "deciding which pods",
    "start": "394319",
    "end": "395759"
  },
  {
    "text": "should be allocated next but",
    "start": "395759",
    "end": "398960"
  },
  {
    "text": "is this solution good enough in order to",
    "start": "398960",
    "end": "400880"
  },
  {
    "text": "provide fairness between the users",
    "start": "400880",
    "end": "403280"
  },
  {
    "text": "let's examine the following scenario",
    "start": "403280",
    "end": "406160"
  },
  {
    "text": "let's assume that bob submitted many",
    "start": "406160",
    "end": "408080"
  },
  {
    "text": "long running pods when the gpus were",
    "start": "408080",
    "end": "410080"
  },
  {
    "text": "free",
    "start": "410080",
    "end": "412080"
  },
  {
    "text": "because he was the only active user we",
    "start": "412080",
    "end": "414319"
  },
  {
    "text": "want to allow bob to use the whole",
    "start": "414319",
    "end": "415840"
  },
  {
    "text": "cluster",
    "start": "415840",
    "end": "416639"
  },
  {
    "text": "and maximize due to the utilization of",
    "start": "416639",
    "end": "419280"
  },
  {
    "text": "the gpus",
    "start": "419280",
    "end": "420319"
  },
  {
    "text": "so we will allocate all the gpus to bob",
    "start": "420319",
    "end": "424479"
  },
  {
    "text": "now when alice submits spots the pods",
    "start": "424479",
    "end": "426960"
  },
  {
    "text": "will remain in pending state",
    "start": "426960",
    "end": "428560"
  },
  {
    "text": "and alice will be starred until bob will",
    "start": "428560",
    "end": "430639"
  },
  {
    "text": "finish to use the gpus",
    "start": "430639",
    "end": "432720"
  },
  {
    "text": "bob can now run the gpus for weeks so",
    "start": "432720",
    "end": "435840"
  },
  {
    "text": "alice can be starred for a very long",
    "start": "435840",
    "end": "437440"
  },
  {
    "text": "time",
    "start": "437440",
    "end": "438639"
  },
  {
    "text": "this means that user can still",
    "start": "438639",
    "end": "440400"
  },
  {
    "text": "monopolize the gpus",
    "start": "440400",
    "end": "443520"
  },
  {
    "start": "443000",
    "end": "526000"
  },
  {
    "text": "in order to solve this problem we added",
    "start": "443520",
    "end": "445680"
  },
  {
    "text": "another algorithm",
    "start": "445680",
    "end": "447039"
  },
  {
    "text": "to enable preemption between the users",
    "start": "447039",
    "end": "450639"
  },
  {
    "text": "the way we handle the preemption between",
    "start": "450639",
    "end": "452479"
  },
  {
    "text": "the users is the following",
    "start": "452479",
    "end": "454400"
  },
  {
    "text": "we calculate the number of gpus every",
    "start": "454400",
    "end": "456800"
  },
  {
    "text": "active user deserves at a given moment",
    "start": "456800",
    "end": "459759"
  },
  {
    "text": "then we run a simulation where we",
    "start": "459759",
    "end": "461919"
  },
  {
    "text": "preempt the pods",
    "start": "461919",
    "end": "463039"
  },
  {
    "text": "of the users who are monopolizing the",
    "start": "463039",
    "end": "464960"
  },
  {
    "text": "gpus and attempt to allocate the pods",
    "start": "464960",
    "end": "467680"
  },
  {
    "text": "of the starved user if the simulation",
    "start": "467680",
    "end": "470720"
  },
  {
    "text": "succeeds",
    "start": "470720",
    "end": "471520"
  },
  {
    "text": "we preamp the pods and simply let our",
    "start": "471520",
    "end": "474800"
  },
  {
    "text": "location algorithm to apply",
    "start": "474800",
    "end": "477199"
  },
  {
    "text": "let's see an example of this flow",
    "start": "477199",
    "end": "480479"
  },
  {
    "text": "so we add four gpus in the cluster",
    "start": "480479",
    "end": "483520"
  },
  {
    "text": "so we calculate how many gpus every user",
    "start": "483520",
    "end": "486080"
  },
  {
    "text": "should have",
    "start": "486080",
    "end": "486720"
  },
  {
    "text": "that's two gpus each and it also means",
    "start": "486720",
    "end": "489520"
  },
  {
    "text": "that bob",
    "start": "489520",
    "end": "490160"
  },
  {
    "text": "uses two gpus more than it should",
    "start": "490160",
    "end": "493680"
  },
  {
    "text": "so we preempt two of bob's pods",
    "start": "493680",
    "end": "496960"
  },
  {
    "text": "and move those pods back to the pending",
    "start": "496960",
    "end": "499440"
  },
  {
    "text": "pod's hip",
    "start": "499440",
    "end": "501840"
  },
  {
    "text": "now we have two free gpus and we simply",
    "start": "501840",
    "end": "504960"
  },
  {
    "text": "apply our allocation algorithm",
    "start": "504960",
    "end": "508080"
  },
  {
    "text": "so we will allocate ali's first pod and",
    "start": "508080",
    "end": "511039"
  },
  {
    "text": "update the number of allocated gpus",
    "start": "511039",
    "end": "513360"
  },
  {
    "text": "but she will remain at the top of the",
    "start": "513360",
    "end": "515039"
  },
  {
    "text": "hip because she is still",
    "start": "515039",
    "end": "516479"
  },
  {
    "text": "starved so we'll allocate a second bar",
    "start": "516479",
    "end": "520399"
  },
  {
    "text": "as you can see bob and alex are now",
    "start": "520399",
    "end": "522640"
  },
  {
    "text": "sharing the",
    "start": "522640",
    "end": "523440"
  },
  {
    "text": "cluster fairly to summarize",
    "start": "523440",
    "end": "527600"
  },
  {
    "start": "526000",
    "end": "606000"
  },
  {
    "text": "with the allocation and preemption",
    "start": "527600",
    "end": "529200"
  },
  {
    "text": "algorithms user can get",
    "start": "529200",
    "end": "531120"
  },
  {
    "text": "the most of the gpus when the gpus are",
    "start": "531120",
    "end": "534160"
  },
  {
    "text": "free",
    "start": "534160",
    "end": "535440"
  },
  {
    "text": "when the users submit pods they are able",
    "start": "535440",
    "end": "538000"
  },
  {
    "text": "to get the gpus they deserve",
    "start": "538000",
    "end": "539680"
  },
  {
    "text": "and they will not be starved",
    "start": "539680",
    "end": "542959"
  },
  {
    "text": "let's see an example of all this",
    "start": "542959",
    "end": "544959"
  },
  {
    "text": "explanation",
    "start": "544959",
    "end": "546240"
  },
  {
    "text": "from one of our customers",
    "start": "546240",
    "end": "549440"
  },
  {
    "text": "when you looked at the mark time window",
    "start": "549440",
    "end": "551440"
  },
  {
    "text": "you can see that the blue user uses",
    "start": "551440",
    "end": "553120"
  },
  {
    "text": "about",
    "start": "553120",
    "end": "553600"
  },
  {
    "text": "20 gpus while the pink user uses about",
    "start": "553600",
    "end": "556480"
  },
  {
    "text": "10 gpus",
    "start": "556480",
    "end": "558080"
  },
  {
    "text": "we allow the blue user to get 20 gpus",
    "start": "558080",
    "end": "560480"
  },
  {
    "text": "because",
    "start": "560480",
    "end": "561440"
  },
  {
    "text": "the gpus are available and we want to",
    "start": "561440",
    "end": "564080"
  },
  {
    "text": "get the maximum utilization out of the",
    "start": "564080",
    "end": "566000"
  },
  {
    "text": "cluster",
    "start": "566000",
    "end": "567920"
  },
  {
    "text": "after a few hours in the new marked",
    "start": "567920",
    "end": "570480"
  },
  {
    "text": "window",
    "start": "570480",
    "end": "572080"
  },
  {
    "text": "the pink user submits more pods so what",
    "start": "572080",
    "end": "575200"
  },
  {
    "text": "we do",
    "start": "575200",
    "end": "576399"
  },
  {
    "text": "is that we preempt the blue user and",
    "start": "576399",
    "end": "579519"
  },
  {
    "text": "we let the pink user use those gpus",
    "start": "579519",
    "end": "583760"
  },
  {
    "text": "as you can see the lines are getting",
    "start": "583760",
    "end": "585519"
  },
  {
    "text": "closer to each other this is because we",
    "start": "585519",
    "end": "587600"
  },
  {
    "text": "took the gpus from the blue user",
    "start": "587600",
    "end": "589680"
  },
  {
    "text": "and gave it to the pink user so this is",
    "start": "589680",
    "end": "592160"
  },
  {
    "text": "how the",
    "start": "592160",
    "end": "592880"
  },
  {
    "text": "fairness actually looks like",
    "start": "592880",
    "end": "596320"
  },
  {
    "text": "thank you all for your time you are more",
    "start": "596800",
    "end": "598560"
  },
  {
    "text": "than welcome to contact me",
    "start": "598560",
    "end": "600000"
  },
  {
    "text": "if you have any question or want to",
    "start": "600000",
    "end": "601360"
  },
  {
    "text": "discuss about scheduling with me",
    "start": "601360",
    "end": "603760"
  },
  {
    "text": "thank you",
    "start": "603760",
    "end": "607920"
  }
]