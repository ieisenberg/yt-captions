[
  {
    "text": "all right um so hi everyone I'm Aditya",
    "start": "199",
    "end": "2879"
  },
  {
    "text": "I'm a graduate student from uiu and uh",
    "start": "2879",
    "end": "5759"
  },
  {
    "text": "today I'll be talking about our",
    "start": "5759",
    "end": "7040"
  },
  {
    "text": "experiences running Cham Plus+ on",
    "start": "7040",
    "end": "8760"
  },
  {
    "text": "kubernetes cloud so before I start I'd",
    "start": "8760",
    "end": "11880"
  },
  {
    "text": "like to thank the IBM lenoi Discovery",
    "start": "11880",
    "end": "13759"
  },
  {
    "text": "accelerator Institute for funding this",
    "start": "13759",
    "end": "15400"
  },
  {
    "text": "project and all the collabor",
    "start": "15400",
    "end": "17160"
  },
  {
    "text": "collaborators from both UI and IBM for",
    "start": "17160",
    "end": "19800"
  },
  {
    "text": "their",
    "start": "19800",
    "end": "21519"
  },
  {
    "text": "contributions so over the last few years",
    "start": "21519",
    "end": "24119"
  },
  {
    "text": "we have seen an increasing availability",
    "start": "24119",
    "end": "26119"
  },
  {
    "text": "and Adoption of HBC Cloud resources",
    "start": "26119",
    "end": "28840"
  },
  {
    "text": "according to this study that that was",
    "start": "28840",
    "end": "30240"
  },
  {
    "text": "conducted in 2023 over the next 5 years",
    "start": "30240",
    "end": "33440"
  },
  {
    "text": "uh the cloud HPC spending is expected to",
    "start": "33440",
    "end": "35600"
  },
  {
    "text": "grow at a much higher rate than the HPC",
    "start": "35600",
    "end": "37840"
  },
  {
    "text": "on on premises spending but according to",
    "start": "37840",
    "end": "40719"
  },
  {
    "text": "the same study out of the 60% of the HBC",
    "start": "40719",
    "end": "44520"
  },
  {
    "text": "sites that are currently using Cloud",
    "start": "44520",
    "end": "47000"
  },
  {
    "text": "only 29% of their HPC workload runtime",
    "start": "47000",
    "end": "49719"
  },
  {
    "text": "was in fact running on the cloud and so",
    "start": "49719",
    "end": "51920"
  },
  {
    "text": "while we have seen an increasing",
    "start": "51920",
    "end": "53160"
  },
  {
    "text": "availability anded option there is still",
    "start": "53160",
    "end": "55160"
  },
  {
    "text": "ample room for",
    "start": "55160",
    "end": "56680"
  },
  {
    "text": "growth the demand for HPC Cloud",
    "start": "56680",
    "end": "59199"
  },
  {
    "text": "resources is now even higher with an",
    "start": "59199",
    "end": "61280"
  },
  {
    "text": "increasing demand for AI workloads",
    "start": "61280",
    "end": "63199"
  },
  {
    "text": "running on the cloud and so now more",
    "start": "63199",
    "end": "65920"
  },
  {
    "text": "than ever we need a way to run HPC",
    "start": "65920",
    "end": "67720"
  },
  {
    "text": "applications with efficient utilization",
    "start": "67720",
    "end": "69560"
  },
  {
    "text": "of cloud resources for our study we used",
    "start": "69560",
    "end": "72479"
  },
  {
    "text": "champ Plus+ which is a parallel",
    "start": "72479",
    "end": "74360"
  },
  {
    "text": "programming framework that provides a",
    "start": "74360",
    "end": "76360"
  },
  {
    "text": "programming model that matches well with",
    "start": "76360",
    "end": "77920"
  },
  {
    "text": "the cloud",
    "start": "77920",
    "end": "79240"
  },
  {
    "text": "philosophy I'll start with a quick",
    "start": "79240",
    "end": "81119"
  },
  {
    "text": "background of Cham Plus+ it is a",
    "start": "81119",
    "end": "83479"
  },
  {
    "text": "parallel programming framework with an",
    "start": "83479",
    "end": "84960"
  },
  {
    "text": "Adaptive runtime system applications",
    "start": "84960",
    "end": "87360"
  },
  {
    "text": "here uh something like MPI but instead",
    "start": "87360",
    "end": "90640"
  },
  {
    "text": "of being written in terms of ranks and",
    "start": "90640",
    "end": "92600"
  },
  {
    "text": "processes are written in terms of",
    "start": "92600",
    "end": "94439"
  },
  {
    "text": "objects which are also called as chars",
    "start": "94439",
    "end": "96720"
  },
  {
    "text": "and then the mapping of these objects to",
    "start": "96720",
    "end": "98479"
  },
  {
    "text": "physical processors is managed by the",
    "start": "98479",
    "end": "100479"
  },
  {
    "text": "runtime system Cham Plus+ is used in",
    "start": "100479",
    "end": "103600"
  },
  {
    "text": "highly scalable scientific applications",
    "start": "103600",
    "end": "105399"
  },
  {
    "text": "such as namd which is a molecular",
    "start": "105399",
    "end": "107320"
  },
  {
    "text": "Dynamic simulation software changa and",
    "start": "107320",
    "end": "109479"
  },
  {
    "text": "astrophysics simulation software and",
    "start": "109479",
    "end": "111079"
  },
  {
    "text": "there are many others champ Plus+ is",
    "start": "111079",
    "end": "113360"
  },
  {
    "text": "available in both C++ and python in",
    "start": "113360",
    "end": "115680"
  },
  {
    "text": "Python it's called Cham for p and while",
    "start": "115680",
    "end": "117840"
  },
  {
    "text": "Cham Plus+ has been traditionally used",
    "start": "117840",
    "end": "119479"
  },
  {
    "text": "for scientific applications it is more",
    "start": "119479",
    "end": "121640"
  },
  {
    "text": "recently being explored for AI",
    "start": "121640",
    "end": "123719"
  },
  {
    "text": "applications as well so let's look at",
    "start": "123719",
    "end": "126600"
  },
  {
    "text": "take a quick look at how CH Plus+ works",
    "start": "126600",
    "end": "128679"
  },
  {
    "text": "so imagine you have a system with four",
    "start": "128679",
    "end": "130239"
  },
  {
    "text": "processors and you want to write your",
    "start": "130239",
    "end": "132440"
  },
  {
    "text": "program with six objects you start by",
    "start": "132440",
    "end": "135239"
  },
  {
    "text": "creating HR array or a collection of",
    "start": "135239",
    "end": "137680"
  },
  {
    "text": "objects of size six these six objects",
    "start": "137680",
    "end": "140360"
  },
  {
    "text": "are then distributed by the runtime",
    "start": "140360",
    "end": "142080"
  },
  {
    "text": "system on these four",
    "start": "142080",
    "end": "144080"
  },
  {
    "text": "processors in order to communicate",
    "start": "144080",
    "end": "146080"
  },
  {
    "text": "between these objects you can call a",
    "start": "146080",
    "end": "147959"
  },
  {
    "text": "function on one of the objects from",
    "start": "147959",
    "end": "149879"
  },
  {
    "text": "anywhere in the system so for example in",
    "start": "149879",
    "end": "152080"
  },
  {
    "text": "this case a the function fu is called on",
    "start": "152080",
    "end": "155200"
  },
  {
    "text": "a of two from a of one and internally",
    "start": "155200",
    "end": "157560"
  },
  {
    "text": "the runtime system will send a message",
    "start": "157560",
    "end": "159280"
  },
  {
    "text": "from processor zero which is where a of",
    "start": "159280",
    "end": "161280"
  },
  {
    "text": "one resides to processor one which is",
    "start": "161280",
    "end": "163000"
  },
  {
    "text": "where a of two resides and then the",
    "start": "163000",
    "end": "164840"
  },
  {
    "text": "function Fu will be executed on",
    "start": "164840",
    "end": "166239"
  },
  {
    "text": "processor one there can also be multiple",
    "start": "166239",
    "end": "169040"
  },
  {
    "text": "types of char arrays um in this case",
    "start": "169040",
    "end": "171159"
  },
  {
    "text": "there are two types of char arrays A and",
    "start": "171159",
    "end": "173879"
  },
  {
    "text": "B and these chars or objects are",
    "start": "173879",
    "end": "177159"
  },
  {
    "text": "migratable which means the runtime",
    "start": "177159",
    "end": "178720"
  },
  {
    "text": "system can move around these objects um",
    "start": "178720",
    "end": "181040"
  },
  {
    "text": "between processors without the user",
    "start": "181040",
    "end": "183120"
  },
  {
    "text": "having to write explicit code for it and",
    "start": "183120",
    "end": "186480"
  },
  {
    "text": "this enables some neat features such as",
    "start": "186480",
    "end": "188319"
  },
  {
    "text": "Dynamic rescaling so let's say you're IM",
    "start": "188319",
    "end": "190920"
  },
  {
    "text": "let's say you're running a program with",
    "start": "190920",
    "end": "192280"
  },
  {
    "text": "four processors and you want to in the",
    "start": "192280",
    "end": "194440"
  },
  {
    "text": "middle of the execution change it to run",
    "start": "194440",
    "end": "196519"
  },
  {
    "text": "on just three processors the runtime",
    "start": "196519",
    "end": "198519"
  },
  {
    "text": "system can move objects from one of the",
    "start": "198519",
    "end": "200760"
  },
  {
    "text": "processors and distribute it across",
    "start": "200760",
    "end": "202599"
  },
  {
    "text": "others and then just continue execution",
    "start": "202599",
    "end": "204519"
  },
  {
    "text": "with three processors the migr ability",
    "start": "204519",
    "end": "207200"
  },
  {
    "text": "concept also results in some other",
    "start": "207200",
    "end": "209159"
  },
  {
    "text": "features such as Dynamic load balancing",
    "start": "209159",
    "end": "210920"
  },
  {
    "text": "and fall tolerance which I won't talk",
    "start": "210920",
    "end": "212480"
  },
  {
    "text": "about in this",
    "start": "212480",
    "end": "214599"
  },
  {
    "text": "talk now why are we trying to run CH",
    "start": "214599",
    "end": "216920"
  },
  {
    "text": "plus CL on kubernetes uh kubernetes has",
    "start": "216920",
    "end": "219319"
  },
  {
    "text": "become a defacto standard for container",
    "start": "219319",
    "end": "221159"
  },
  {
    "text": "orchestration on the cloud over the last",
    "start": "221159",
    "end": "222959"
  },
  {
    "text": "few years and it provides operators for",
    "start": "222959",
    "end": "225920"
  },
  {
    "text": "job scheduling and life cycle Management",
    "start": "225920",
    "end": "228120"
  },
  {
    "text": "in particular for this project we have",
    "start": "228120",
    "end": "230319"
  },
  {
    "text": "used Cube flows MPI operator quite",
    "start": "230319",
    "end": "232599"
  },
  {
    "text": "extensively MPI operator is used for",
    "start": "232599",
    "end": "234879"
  },
  {
    "text": "running MPI applications and we found",
    "start": "234879",
    "end": "237120"
  },
  {
    "text": "that minimal changes to the MPI operator",
    "start": "237120",
    "end": "239159"
  },
  {
    "text": "were required to to be able to run CH",
    "start": "239159",
    "end": "241680"
  },
  {
    "text": "Plus+ applications as",
    "start": "241680",
    "end": "243760"
  },
  {
    "text": "well kubernetes also supports Dynamic",
    "start": "243760",
    "end": "246319"
  },
  {
    "text": "rescaling of resources and while we only",
    "start": "246319",
    "end": "249120"
  },
  {
    "text": "support CPUs right now uh we plan to",
    "start": "249120",
    "end": "251200"
  },
  {
    "text": "support gpus in the future as well uh on",
    "start": "251200",
    "end": "253720"
  },
  {
    "text": "the left is an example of an MPI",
    "start": "253720",
    "end": "255959"
  },
  {
    "text": "operator raml file uh I won't go into",
    "start": "255959",
    "end": "258600"
  },
  {
    "text": "each of each of these specs in detail",
    "start": "258600",
    "end": "260639"
  },
  {
    "text": "but a couple of things that I'll point",
    "start": "260639",
    "end": "262000"
  },
  {
    "text": "out uh first you define a launcher and a",
    "start": "262000",
    "end": "265400"
  },
  {
    "text": "worker the launcher is where your MPI",
    "start": "265400",
    "end": "268120"
  },
  {
    "text": "run command is defined in in the workers",
    "start": "268120",
    "end": "270240"
  },
  {
    "text": "you define a number of replicas and that",
    "start": "270240",
    "end": "271919"
  },
  {
    "text": "is the number of workers that will be",
    "start": "271919",
    "end": "273199"
  },
  {
    "text": "launched so in this case there there's",
    "start": "273199",
    "end": "275320"
  },
  {
    "text": "going to be four workers and one",
    "start": "275320",
    "end": "276720"
  },
  {
    "text": "launcher where you run the MPI run",
    "start": "276720",
    "end": "279199"
  },
  {
    "text": "command now coming to the efficiency",
    "start": "279199",
    "end": "281759"
  },
  {
    "text": "part of the equation how do we get um",
    "start": "281759",
    "end": "284360"
  },
  {
    "text": "how do we use MPI operator to be able to",
    "start": "284360",
    "end": "286360"
  },
  {
    "text": "efficiently use cloud resources so the",
    "start": "286360",
    "end": "288840"
  },
  {
    "text": "objective here is to maximize job",
    "start": "288840",
    "end": "290759"
  },
  {
    "text": "throughput on a fixed size cluster while",
    "start": "290759",
    "end": "292880"
  },
  {
    "text": "minimizing weight time for high priority",
    "start": "292880",
    "end": "294800"
  },
  {
    "text": "jobs and in order to do this we modified",
    "start": "294800",
    "end": "297199"
  },
  {
    "text": "the MPI operator crd to include a couple",
    "start": "297199",
    "end": "299320"
  },
  {
    "text": "of ADD additional Fields firstly we",
    "start": "299320",
    "end": "301560"
  },
  {
    "text": "added a job priority field and secondly",
    "start": "301560",
    "end": "303800"
  },
  {
    "text": "we changed the worker spec to uh replace",
    "start": "303800",
    "end": "307199"
  },
  {
    "text": "the replicas filled with Min replicas",
    "start": "307199",
    "end": "309080"
  },
  {
    "text": "and Max replicas which is the Min and",
    "start": "309080",
    "end": "310880"
  },
  {
    "text": "maximum number of workers that can be",
    "start": "310880",
    "end": "313160"
  },
  {
    "text": "launched for a job and the number of",
    "start": "313160",
    "end": "315880"
  },
  {
    "text": "workers that are launched is determined",
    "start": "315880",
    "end": "317880"
  },
  {
    "text": "by the MP operators controller at",
    "start": "317880",
    "end": "319800"
  },
  {
    "text": "runtime based on the state of the",
    "start": "319800",
    "end": "321120"
  },
  {
    "text": "cluster we also modified the controller",
    "start": "321120",
    "end": "323479"
  },
  {
    "text": "for elastic scheduling so let's look at",
    "start": "323479",
    "end": "326440"
  },
  {
    "text": "how this elastic scheduling works when a",
    "start": "326440",
    "end": "329280"
  },
  {
    "text": "new job is submitted we first check if",
    "start": "329280",
    "end": "331240"
  },
  {
    "text": "we have enough available CPUs for",
    "start": "331240",
    "end": "333160"
  },
  {
    "text": "running this new job at its Min replicas",
    "start": "333160",
    "end": "335639"
  },
  {
    "text": "configuration if we do then we just",
    "start": "335639",
    "end": "337680"
  },
  {
    "text": "start running this new job if not then",
    "start": "337680",
    "end": "340160"
  },
  {
    "text": "we check if there are any running jobs",
    "start": "340160",
    "end": "342479"
  },
  {
    "text": "that are a lower priority than this new",
    "start": "342479",
    "end": "344319"
  },
  {
    "text": "job if this new job is the lowest",
    "start": "344319",
    "end": "346479"
  },
  {
    "text": "priority job that is submitted to the",
    "start": "346479",
    "end": "348000"
  },
  {
    "text": "system then we have to queue this job",
    "start": "348000",
    "end": "349919"
  },
  {
    "text": "because we can't scale down any of the",
    "start": "349919",
    "end": "351400"
  },
  {
    "text": "higher priority jobs but if there are",
    "start": "351400",
    "end": "353479"
  },
  {
    "text": "some lower priority jobs that are",
    "start": "353479",
    "end": "354919"
  },
  {
    "text": "running on the system we iterate over",
    "start": "354919",
    "end": "357120"
  },
  {
    "text": "those lower priority jobs and we scale",
    "start": "357120",
    "end": "359080"
  },
  {
    "text": "them down to to M replicas to their Min",
    "start": "359080",
    "end": "361120"
  },
  {
    "text": "replicas configuration to free up some",
    "start": "361120",
    "end": "364080"
  },
  {
    "text": "CPUs if we are able to free up enough",
    "start": "364080",
    "end": "366680"
  },
  {
    "text": "CPUs then we run this new job otherwise",
    "start": "366680",
    "end": "369000"
  },
  {
    "text": "we queue it and similarly when a job",
    "start": "369000",
    "end": "372080"
  },
  {
    "text": "finishes execution we it frees up some",
    "start": "372080",
    "end": "375599"
  },
  {
    "text": "CPUs that can be reassigned to some of",
    "start": "375599",
    "end": "377360"
  },
  {
    "text": "the other uh jobs so we iterate over all",
    "start": "377360",
    "end": "380800"
  },
  {
    "text": "of the running and queued jobs in",
    "start": "380800",
    "end": "382400"
  },
  {
    "text": "decreasing order of priority and we",
    "start": "382400",
    "end": "384599"
  },
  {
    "text": "reassign these CPUs by scaling up either",
    "start": "384599",
    "end": "387759"
  },
  {
    "text": "scaling up running jobs or by starting",
    "start": "387759",
    "end": "389880"
  },
  {
    "text": "some new C",
    "start": "389880",
    "end": "392120"
  },
  {
    "text": "jobs so here's a quick example of what",
    "start": "392120",
    "end": "394840"
  },
  {
    "text": "this actually looks",
    "start": "394840",
    "end": "396280"
  },
  {
    "text": "like so I'm going to start by submitting",
    "start": "396280",
    "end": "399800"
  },
  {
    "text": "a uh lower priority job which is going",
    "start": "399800",
    "end": "402800"
  },
  {
    "text": "to start running with four",
    "start": "402800",
    "end": "406759"
  },
  {
    "text": "workers and then I'm going to submit a",
    "start": "407400",
    "end": "409560"
  },
  {
    "text": "higher priority",
    "start": "409560",
    "end": "412319"
  },
  {
    "text": "job when I submit a higher priority job",
    "start": "412319",
    "end": "415479"
  },
  {
    "text": "the lower priority job is getting",
    "start": "415479",
    "end": "417440"
  },
  {
    "text": "rescaled to run with just two workers",
    "start": "417440",
    "end": "419360"
  },
  {
    "text": "and instead of four and the two freed",
    "start": "419360",
    "end": "422120"
  },
  {
    "text": "workers are now assigned to this new uh",
    "start": "422120",
    "end": "424599"
  },
  {
    "text": "higher priority",
    "start": "424599",
    "end": "427360"
  },
  {
    "text": "job later when this higher priority job",
    "start": "430840",
    "end": "433319"
  },
  {
    "text": "finishes execution the workers that get",
    "start": "433319",
    "end": "435560"
  },
  {
    "text": "freed are now reassigned back to the",
    "start": "435560",
    "end": "438240"
  },
  {
    "text": "older low priority",
    "start": "438240",
    "end": "440240"
  },
  {
    "text": "job like we see Happening Here and Now",
    "start": "440240",
    "end": "444000"
  },
  {
    "text": "this older low priority job is again",
    "start": "444000",
    "end": "446000"
  },
  {
    "text": "rescaled up to run with four workers",
    "start": "446000",
    "end": "448680"
  },
  {
    "text": "instead of two",
    "start": "448680",
    "end": "451520"
  },
  {
    "text": "we also measured the rescaling overhead",
    "start": "452280",
    "end": "454720"
  },
  {
    "text": "for a 2d stencil problem which is a",
    "start": "454720",
    "end": "456840"
  },
  {
    "text": "typical problem typ typical scientific",
    "start": "456840",
    "end": "459520"
  },
  {
    "text": "problem used for",
    "start": "459520",
    "end": "461319"
  },
  {
    "text": "benchmarking uh and we saw that the",
    "start": "461319",
    "end": "463879"
  },
  {
    "text": "rescaling overhead was pretty high when",
    "start": "463879",
    "end": "465840"
  },
  {
    "text": "we use smaller problem sizes Because the",
    "start": "465840",
    "end": "468240"
  },
  {
    "text": "actual run time itself was pretty small",
    "start": "468240",
    "end": "470440"
  },
  {
    "text": "but as the problem size increased the",
    "start": "470440",
    "end": "472319"
  },
  {
    "text": "overhead actually dropped to much",
    "start": "472319",
    "end": "474680"
  },
  {
    "text": "smaller than",
    "start": "474680",
    "end": "476759"
  },
  {
    "text": "10% and uh that's all for my talk today",
    "start": "476759",
    "end": "480360"
  },
  {
    "text": "uh here's the links to my group web page",
    "start": "480360",
    "end": "482840"
  },
  {
    "text": "uh if you want to learn more about Shan",
    "start": "482840",
    "end": "484520"
  },
  {
    "text": "Plus+ and feel free to reach out to us",
    "start": "484520",
    "end": "486919"
  },
  {
    "text": "our group is a traditional HPC group and",
    "start": "486919",
    "end": "489759"
  },
  {
    "text": "uh so we really value your feedback um",
    "start": "489759",
    "end": "492960"
  },
  {
    "text": "on how we can make this thing uh better",
    "start": "492960",
    "end": "496080"
  },
  {
    "text": "thank you",
    "start": "496080",
    "end": "497640"
  },
  {
    "text": "[Applause]",
    "start": "497640",
    "end": "501079"
  }
]