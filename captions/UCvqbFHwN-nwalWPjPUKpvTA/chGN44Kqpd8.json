[
  {
    "text": "all right while the speakers make their way to the front hi I'm Doug Davis chocos for this",
    "start": "30",
    "end": "5370"
  },
  {
    "text": "session I see we talked about you based search on kubernetes a couple of things first there is a survey with associate",
    "start": "5370",
    "end": "15929"
  },
  {
    "text": "with each session so please after the session is over please rate the presentation and of course the presenters much appreciated and with",
    "start": "15929",
    "end": "22740"
  },
  {
    "text": "that now that they're here or knurl miked you're all set all right right here not you guys all right thank",
    "start": "22740",
    "end": "29300"
  },
  {
    "text": "you hi everyone my name is Manish I'm a developer with eBay search team and this",
    "start": "29300",
    "end": "35989"
  },
  {
    "text": "is my colleague Ashwin and he's a developer with eBay's platform team which is responsible for all things kubernetes within eBay and will walk you",
    "start": "35989",
    "end": "43640"
  },
  {
    "text": "through a journey of how we took eBay store Hugh's case and ran it on top of kubernetes so if there's nothing else",
    "start": "43640",
    "end": "52220"
  },
  {
    "text": "that you remember from this talk at least remember that it's possible to run a large-scale latency-sensitive",
    "start": "52220",
    "end": "57309"
  },
  {
    "text": "application like a search application and run it on the kubernetes platform natively and hopefully with that said",
    "start": "57309",
    "end": "65299"
  },
  {
    "text": "I'll sort of walk you through what search looks like in eBay what searches scale is and then my partner will go",
    "start": "65299",
    "end": "71540"
  },
  {
    "text": "into detail on what our current design looks like when we model this use case on top of kubernetes so let's look at",
    "start": "71540",
    "end": "80360"
  },
  {
    "text": "searches background eBay has its own custom search engine built in-house it's called Cassini it's a distributed",
    "start": "80360",
    "end": "88670"
  },
  {
    "text": "search engine it says various use cases within eBay one of the most prominent use cases is what we call as active",
    "start": "88670",
    "end": "95539"
  },
  {
    "text": "listings and to give you an idea at any point in time this search engine is",
    "start": "95539",
    "end": "101329"
  },
  {
    "text": "serving around 1.4 billion active listening so listening that what we call the things that you can transact on site",
    "start": "101329",
    "end": "107030"
  },
  {
    "text": "so buy sell them etc and active is what is currently available as opposed to there are other use cases like sole",
    "start": "107030",
    "end": "113030"
  },
  {
    "text": "listings which have our historical data and you can search on the history and so",
    "start": "113030",
    "end": "119270"
  },
  {
    "text": "we receive around 300 K QPS in one of our data centers at any given point in",
    "start": "119270",
    "end": "124549"
  },
  {
    "text": "time in the day hopefully these two numbers together at least give you a rough idea that it's not possible to",
    "start": "124549",
    "end": "130520"
  },
  {
    "text": "load all of this data on a couple of machines and be able to serve the query volume that we are talking about within",
    "start": "130520",
    "end": "136100"
  },
  {
    "text": "the stipulated SLS for our queries and so in fact within eBay search use case",
    "start": "136100",
    "end": "144460"
  },
  {
    "text": "typically takes around 30 to 40% of our data certain footprints so that's how many servers thousands of servers we are",
    "start": "144460",
    "end": "150020"
  },
  {
    "text": "talking about we have five nines of availability of SLS that we have to",
    "start": "150020",
    "end": "155269"
  },
  {
    "text": "maintain every second search is down is actually costing eBay money because you can't use our sight anymore and so lot",
    "start": "155269",
    "end": "163240"
  },
  {
    "text": "of our design is governed keeping this in mind that we have to be available all the time with that said let's dive a",
    "start": "163240",
    "end": "172120"
  },
  {
    "text": "little bit deeper into how a based search architecture looks like so broadly you can think of eBay search",
    "start": "172120",
    "end": "179110"
  },
  {
    "text": "architecture divided into five layers so when a user comes into ebay.com and types in a query USD sweatshirt for",
    "start": "179110",
    "end": "187420"
  },
  {
    "text": "example it goes to our first layer which is the software load balance earlier these are custom inbuilt software load",
    "start": "187420",
    "end": "193450"
  },
  {
    "text": "balancers and as the name suggests their job is very simple they find the least",
    "start": "193450",
    "end": "199000"
  },
  {
    "text": "loaded node in the downstream layer and forward the query to that particular node which brings the query to the",
    "start": "199000",
    "end": "206050"
  },
  {
    "text": "second layer which is the query transformation layer so the co a",
    "start": "206050",
    "end": "211900"
  },
  {
    "text": "transformation layers main responsibility is to take the users query and enhance it and annotate it",
    "start": "211900",
    "end": "217570"
  },
  {
    "text": "what I mean by this eBay analyzes a lot of user data click data that's going on and be mind what is",
    "start": "217570",
    "end": "223990"
  },
  {
    "text": "the demand and supply for various things so for example we have might have noticed that people who type in USD",
    "start": "223990",
    "end": "229570"
  },
  {
    "text": "sweatshirt are also interested and click on hoodies and tend to buy OD so we will add that keyword to the original query",
    "start": "229570",
    "end": "235990"
  },
  {
    "text": "and add hoodies for something we can also do spell correction at this layer add brands etc to this coil and",
    "start": "235990",
    "end": "242470"
  },
  {
    "text": "transform it and then we send this query transform query the next layer is what we called us the top-level aggregators",
    "start": "242470",
    "end": "250470"
  },
  {
    "text": "the top-level aggregators responsibility is to take this user query and fan it",
    "start": "250470",
    "end": "255760"
  },
  {
    "text": "out to multiple use cases if the intent is such so it can send the same query to",
    "start": "255760",
    "end": "260890"
  },
  {
    "text": "our active listings and also to our sold listings and then on the way back it",
    "start": "260890",
    "end": "266320"
  },
  {
    "text": "merges the results for the upstream layers to look to get the whole relevance and recall for that particular",
    "start": "266320",
    "end": "272560"
  },
  {
    "text": "query the top-level aggregators once they fan out send it to low-level aggregators or ll s they are essentially",
    "start": "272560",
    "end": "281050"
  },
  {
    "text": "the same as the top-level aggregators but they work within the use case so they don't go out to multiple use cases",
    "start": "281050",
    "end": "287080"
  },
  {
    "text": "they work within the use case and fan the query out for the next layer which brings me",
    "start": "287080",
    "end": "293290"
  },
  {
    "text": "to the most important layer which is the fifth layer which we call as the quarry serving stat this is the layer which is",
    "start": "293290",
    "end": "299260"
  },
  {
    "text": "essentially responsible for taking the query running that query across the billion items that we have figuring out",
    "start": "299260",
    "end": "305920"
  },
  {
    "text": "the relevance of the intended relevance of the incoming query to the item and",
    "start": "305920",
    "end": "311380"
  },
  {
    "text": "then score them rank them and send it back upstream to the user so this essentially is the most complex layer",
    "start": "311380",
    "end": "317710"
  },
  {
    "text": "for us and our idea was that if we tackle this layer correctly other layers",
    "start": "317710",
    "end": "323860"
  },
  {
    "text": "will take care of itself or themselves so this is the layer that we'll be focusing this talk on as we saw earlier",
    "start": "323860",
    "end": "331360"
  },
  {
    "text": "we have around 1.4 billion listings so we cannot all fit this in the same machine so we shard this data so you can",
    "start": "331360",
    "end": "339280"
  },
  {
    "text": "think of shards as columns which hold a percentage of our inventory so every or buckets rather which hold a percentage",
    "start": "339280",
    "end": "346120"
  },
  {
    "text": "of our inventory and then we have replicas within the shard or what we call rows for handling the query volume",
    "start": "346120",
    "end": "352810"
  },
  {
    "text": "so essentially the query serving stack that we have you can think of it as a matrix with the set number of columns",
    "start": "352810",
    "end": "360310"
  },
  {
    "text": "for sharding and then rows for handling the query volume that we have with that",
    "start": "360310",
    "end": "366430"
  },
  {
    "text": "said eBay's search use cases a little different than the traditional search use case or a web-based search use case",
    "start": "366430",
    "end": "372700"
  },
  {
    "text": "where web pages tend to be static or data and inventory is changing every",
    "start": "372700",
    "end": "377800"
  },
  {
    "text": "second items are getting bought items are getting sold prices are changing and so what we do in our design today is we",
    "start": "377800",
    "end": "385630"
  },
  {
    "text": "take a snapshot of the whole inventory and we ship it to this query serving grid grid multiple times a day what this",
    "start": "385630",
    "end": "392950"
  },
  {
    "text": "means is whenever the query serving grid has to load this new snapshot it has to",
    "start": "392950",
    "end": "397960"
  },
  {
    "text": "be restart the grid while serving traffic so that some servers have loaded this new snapshot and the rest of",
    "start": "397960",
    "end": "403450"
  },
  {
    "text": "working on working its way through also from the point the snapshot was taken we",
    "start": "403450",
    "end": "409270"
  },
  {
    "text": "continuously send all of these servers data messages that tell you the current state because not everything is captured",
    "start": "409270",
    "end": "415360"
  },
  {
    "text": "in the snapshot things are happening and this is only happening few times a day so prices could be changing every second",
    "start": "415360",
    "end": "420610"
  },
  {
    "text": "so then we continuously keep sending it data to keep the query serving grid up to",
    "start": "420610",
    "end": "426150"
  },
  {
    "text": "this factors in this complexity of shipping data and restarting the grid while serving traffic factors in and is",
    "start": "426150",
    "end": "433080"
  },
  {
    "text": "an important part of how we model this use case when we ran it on top of kubernetes so essentially remember that the",
    "start": "433080",
    "end": "439920"
  },
  {
    "text": "inventory that that column is solving is the state for that column or is the state for that shot so we looked at how",
    "start": "439920",
    "end": "450630"
  },
  {
    "text": "search works and so I searched 101 for eBay and now let's look at the kubernetes fit went within eBay so the",
    "start": "450630",
    "end": "457830"
  },
  {
    "text": "team that handles all the kubernetes clusters within eBay is called tests at i/o we have around 60 plus production",
    "start": "457830",
    "end": "464820"
  },
  {
    "text": "clusters of various sizes some ranging from hundreds of nodes to some that are in upwards of 2000 nodes in totality we",
    "start": "464820",
    "end": "473040"
  },
  {
    "text": "are handling hundred and sixty thousand pods application pods across 30,000 bare-metal machines today and these",
    "start": "473040",
    "end": "479370"
  },
  {
    "text": "clusters serve multiple internal and external use cases so external use cases are things like search which are user",
    "start": "479370",
    "end": "485700"
  },
  {
    "text": "facing so you are handling that and internal use cases is something like what Hadoop pipelines for our big data",
    "start": "485700",
    "end": "491220"
  },
  {
    "text": "or AI ml pipelines which use GPUs to create data models so all of this today",
    "start": "491220",
    "end": "496860"
  },
  {
    "text": "is handled by kubernetes running within eBay the these clusters are also spread",
    "start": "496860",
    "end": "503820"
  },
  {
    "text": "across the world into around 15 regions what this gives applications is an",
    "start": "503820",
    "end": "509190"
  },
  {
    "text": "opportunity to be closer to the user closer to the edge so some of our kubernetes clusters are deployed on the",
    "start": "509190",
    "end": "514830"
  },
  {
    "text": "edge and we are using on web proxies and software load balancers to give them that capability so which brings me to",
    "start": "514830",
    "end": "524190"
  },
  {
    "text": "the why why did search decide to run on kubernetes why can't it stay for what",
    "start": "524190",
    "end": "529590"
  },
  {
    "text": "where it is right now so what happens is whenever a new use case wants to onboard on top of our search engine on Cassini",
    "start": "529590",
    "end": "536480"
  },
  {
    "text": "it takes us a lot of time today we go from talking to the hardware capacity",
    "start": "536480",
    "end": "542100"
  },
  {
    "text": "team to get us the servers to we get the nodes reimage to the OS we want we deploy the software we deploy the data",
    "start": "542100",
    "end": "548760"
  },
  {
    "text": "and this is the same process if we have to add capacity for let's say the holiday period because we're going to get more traffic and so this takes time",
    "start": "548760",
    "end": "556260"
  },
  {
    "text": "and we wanted it such that reduce the friction when we move on to kubernetes er and so that was one of the",
    "start": "556260",
    "end": "562140"
  },
  {
    "text": "main driving factors behind trying to trying to run a task or search scale so",
    "start": "562140",
    "end": "569940"
  },
  {
    "text": "we saw earlier that our kubernetes clusters are deployed to the edge which means that if we are able to offload or",
    "start": "569940",
    "end": "575730"
  },
  {
    "text": "if you are able to design our workload on top of kubernetes it also gives eBay's search to be closer to the edge",
    "start": "575730",
    "end": "581940"
  },
  {
    "text": "by just being on run by being able to run on kubernetes so there are lots of",
    "start": "581940",
    "end": "587010"
  },
  {
    "text": "use cases if you think you are typing iPhone and you see all these completions that happen right those things make",
    "start": "587010",
    "end": "592320"
  },
  {
    "text": "sense to be closer to the user because every millisecond matters for search",
    "start": "592320",
    "end": "598010"
  },
  {
    "text": "automation so as I said one of the main factors for search in designing is",
    "start": "599660",
    "end": "604890"
  },
  {
    "text": "availability we have to be available all the time which means we lay out our grid our topology the matrix that we talked",
    "start": "604890",
    "end": "611430"
  },
  {
    "text": "about in a very specific manner for example when we create a column or a chart we make sure that every single",
    "start": "611430",
    "end": "618030"
  },
  {
    "text": "replica is not on the same rack just because if a rat goes down and it takes",
    "start": "618030",
    "end": "623130"
  },
  {
    "text": "the whole shard with it that part of the inventory essentially is invisible to the user so it took out the whole that chunk of the the bucket",
    "start": "623130",
    "end": "629910"
  },
  {
    "text": "basically is out and so we cannot show the items that were in that bucket so things like that so what we did was we",
    "start": "629910",
    "end": "636270"
  },
  {
    "text": "use Ford affinity anti affinity rules that kubernetes provides and bake this into the platform as opposed to",
    "start": "636270",
    "end": "641870"
  },
  {
    "text": "carefully manually hand-holding how we layout the topology flexibility as we",
    "start": "641870",
    "end": "649620"
  },
  {
    "text": "saw search is you can think of it as a matrix multiple dimensional matrix we",
    "start": "649620",
    "end": "655020"
  },
  {
    "text": "want it to be able to stretch it in all dimensions so for example when we have more traffic you can scale it up with",
    "start": "655020",
    "end": "661470"
  },
  {
    "text": "more replicas when there's less traffic you scale replicas down if we have more data and more inventory we want it to be",
    "start": "661470",
    "end": "667890"
  },
  {
    "text": "able to expand the grid to have more shots and if we have less data around during the year then maybe want to",
    "start": "667890",
    "end": "673110"
  },
  {
    "text": "shrink the grid so basically by being on the edge and by designing the our search",
    "start": "673110",
    "end": "679110"
  },
  {
    "text": "use case on top of kubernetes now it allows us to achieve all of this and which is why we tried to do this on",
    "start": "679110",
    "end": "685950"
  },
  {
    "text": "kubernetes and were successful so I'll hand it over to my partner Josh one who's going to die",
    "start": "685950",
    "end": "691410"
  },
  {
    "text": "deeper into how our design looks like on top of kubernetes I give my niche the",
    "start": "691410",
    "end": "696629"
  },
  {
    "text": "first part of this journey was to convert this search application into micro services so the first important",
    "start": "696629",
    "end": "706079"
  },
  {
    "text": "micro service which we call as a query serving pot this has three major containers the first container is a",
    "start": "706079",
    "end": "711990"
  },
  {
    "text": "query serving container this contains all the application logic that is required to serve the end user queries",
    "start": "711990",
    "end": "718649"
  },
  {
    "text": "the second container is the log exporter container this container pushes all the",
    "start": "718649",
    "end": "724889"
  },
  {
    "text": "application logs into our logging system the third is a matrix exporter this",
    "start": "724889",
    "end": "730910"
  },
  {
    "text": "container converts all the application matrix into Prometheus format and ships",
    "start": "730910",
    "end": "736079"
  },
  {
    "text": "to our metric system the second important micro service is the data",
    "start": "736079",
    "end": "741149"
  },
  {
    "text": "distribution agent for the primary responsibility of this part is to download the data on to the node this",
    "start": "741149",
    "end": "749040"
  },
  {
    "text": "data is used by the query serving fault to serve the user queries we deploy this",
    "start": "749040",
    "end": "754379"
  },
  {
    "text": "part is a daemon set on all our caged nodes we also have few metric collection",
    "start": "754379",
    "end": "759959"
  },
  {
    "text": "paths that collect both the container matrix and the node level metrics for",
    "start": "759959",
    "end": "765779"
  },
  {
    "text": "the KH norm we store this data on our local persistent volumes this persistent",
    "start": "765779",
    "end": "771930"
  },
  {
    "text": "volume is shared both by the data distribution agent pod and the query serving for now the next part of the",
    "start": "771930",
    "end": "779579"
  },
  {
    "text": "journey is to layout the search grid further for the search use case when we",
    "start": "779579",
    "end": "785189"
  },
  {
    "text": "looked into the case ecosystem we could not we could not find any operator or a",
    "start": "785189",
    "end": "791279"
  },
  {
    "text": "pattern that would help us to deploy the application in the form of a matrix the existing patterns such as deployments",
    "start": "791279",
    "end": "798540"
  },
  {
    "text": "run stateful cells are uni-dimensional so what we did was we set out to create an operator to achieve this use case we",
    "start": "798540",
    "end": "807180"
  },
  {
    "text": "call this as a matrix deployment operator so when an operator wants to",
    "start": "807180",
    "end": "812579"
  },
  {
    "text": "deploy a search grid for a specific use case she goes and creates a CRD for the",
    "start": "812579",
    "end": "818819"
  },
  {
    "text": "matrix deployed as you can see in this particular example spec",
    "start": "818819",
    "end": "824200"
  },
  {
    "text": "the operator wants to create a matrix with four columns and 3 rows he had columns referred to the stateful",
    "start": "824200",
    "end": "831579"
  },
  {
    "text": "sets in the search grid and the rows referred to the replicas for each column this also this bike also has additional",
    "start": "831579",
    "end": "839110"
  },
  {
    "text": "information like the container versions which need to be deployed and also the",
    "start": "839110",
    "end": "844449"
  },
  {
    "text": "data packages that need to go as part of the pod we also track the status of this",
    "start": "844449",
    "end": "849839"
  },
  {
    "text": "deployment grid so once the CRD instance is created the",
    "start": "849839",
    "end": "855130"
  },
  {
    "text": "controller kicks in and starts creating the stateful sets as Mohnish mentioned the charts of each column needs to be",
    "start": "855130",
    "end": "862810"
  },
  {
    "text": "distributed across the racks so we use a combination of pod and DF 3d and",
    "start": "862810",
    "end": "867880"
  },
  {
    "text": "affinity rules to achieve this kind of layout so the statehood sets are created",
    "start": "867880",
    "end": "875350"
  },
  {
    "text": "on and the grid has formed as we just illustrated since the operator has asked",
    "start": "875350",
    "end": "882339"
  },
  {
    "text": "for four columns and 3 rows we create a search grid with this configuration now",
    "start": "882339",
    "end": "888880"
  },
  {
    "text": "while these parts are getting created the service controller kicks in and creates the headless services for each",
    "start": "888880",
    "end": "895149"
  },
  {
    "text": "of these pots this we create headless services because we want the upper level",
    "start": "895149",
    "end": "900790"
  },
  {
    "text": "stack to be oblivious of the infrastructure changes underneath them now as you can see this design helps us",
    "start": "900790",
    "end": "907959"
  },
  {
    "text": "to be very flexible with our search grids it provides us the speed for",
    "start": "907959",
    "end": "913510"
  },
  {
    "text": "example if we have want to increase the grid size all the operator needs to do is change the spike it also provides us",
    "start": "913510",
    "end": "920680"
  },
  {
    "text": "the flexibility of moving our infrastructure for example if you know that a specific use case is going to get",
    "start": "920680",
    "end": "927670"
  },
  {
    "text": "more traffic while the other doesn't get that much traffic so we can decrease the",
    "start": "927670",
    "end": "932829"
  },
  {
    "text": "search grid for the other use case and move the infrastructure here so it actually provides the flexibility in",
    "start": "932829",
    "end": "939370"
  },
  {
    "text": "terms of infrastructure movement we also monitor our search grids using the",
    "start": "939370",
    "end": "945550"
  },
  {
    "text": "metric server whenever there we look at the CPU and the memory utilization and",
    "start": "945550",
    "end": "951250"
  },
  {
    "text": "whenever it goes beyond a certain percentage we use the pod auto scaling capabilities to increase have a good",
    "start": "951250",
    "end": "957939"
  },
  {
    "text": "sighs now that we have laid out the grid and the next part of the journey is to",
    "start": "957939",
    "end": "964289"
  },
  {
    "text": "distribute the data since this is a stateful application it is very",
    "start": "964289",
    "end": "969720"
  },
  {
    "text": "important for us to maintain the right state on these parts so we set out and",
    "start": "969720",
    "end": "975309"
  },
  {
    "text": "wrote an other operator which we called as a file distribution operator to distribute the data across the grid so",
    "start": "975309",
    "end": "982869"
  },
  {
    "text": "whenever there is new data for the use case that in a deployment Orchestrator",
    "start": "982869",
    "end": "987909"
  },
  {
    "text": "gets the grid metadata for that particular use case this includes information like number of rows number",
    "start": "987909",
    "end": "993279"
  },
  {
    "text": "of columns and so on the orchestrator then creates the spec on the ATA server",
    "start": "993279",
    "end": "1000529"
  },
  {
    "text": "for example in this spec we are mentioning that for shard 1 what data needs to be downloaded and where it",
    "start": "1000529",
    "end": "1007439"
  },
  {
    "text": "needs to be downloaded once this we also track the status of the data",
    "start": "1007439",
    "end": "1012509"
  },
  {
    "text": "distribution using this spec things like the download speed and the progress of",
    "start": "1012509",
    "end": "1018299"
  },
  {
    "text": "the download is tracked here which is in turn used by the orchestrator for the distribution of the data once this year",
    "start": "1018299",
    "end": "1027569"
  },
  {
    "text": "is created the data distribution controller starts to push data in suddenly to all the columns in the",
    "start": "1027569",
    "end": "1033779"
  },
  {
    "text": "search grid once the data lands on the top of the column pod it gets replicated",
    "start": "1033779",
    "end": "1041579"
  },
  {
    "text": "to its charts which is robeast this is done by the data distribution agent pod",
    "start": "1041579",
    "end": "1049279"
  },
  {
    "text": "now mohnish mentioned that we push the data a few times in a day as well as the",
    "start": "1049279",
    "end": "1055049"
  },
  {
    "text": "incremental data to the search grid both these mechanisms happen to look through the same process once the data",
    "start": "1055049",
    "end": "1062070"
  },
  {
    "text": "is downloaded to all the pods we again go back to the matrix controller to",
    "start": "1062070",
    "end": "1067379"
  },
  {
    "text": "actually update the spec with the latest innovations this change in the spec is",
    "start": "1067379",
    "end": "1072600"
  },
  {
    "text": "again propagated to all the stateful sets by the matrix operator one of the",
    "start": "1072600",
    "end": "1080789"
  },
  {
    "text": "problems that we ran while designing this particular grid based application is that the same volume needs to be",
    "start": "1080789",
    "end": "1087360"
  },
  {
    "text": "shared by two different parts the solutions that Cuban at his office",
    "start": "1087360",
    "end": "1092730"
  },
  {
    "text": "today doesn't help us to solve the problem the way we Internet do in order",
    "start": "1092730",
    "end": "1098130"
  },
  {
    "text": "to solve this problem we created a mutating weapon to make sure the same PVC it was distributed is shared by both",
    "start": "1098130",
    "end": "1106320"
  },
  {
    "text": "the pots let me walk through how it how this goes so the data distribution agent",
    "start": "1106320",
    "end": "1112680"
  },
  {
    "text": "Paul as I mentioned earlier is running as a daemon set on all our cases notes when a query serving part lands on that",
    "start": "1112680",
    "end": "1120030"
  },
  {
    "text": "particular node we create a PVC by binding a little pv to that particular",
    "start": "1120030",
    "end": "1125520"
  },
  {
    "text": "PVC once this query serving part is up and running we go ahead and we start the",
    "start": "1125520",
    "end": "1131490"
  },
  {
    "text": "data read my side during the restart process the mutating webbook kicks in",
    "start": "1131490",
    "end": "1136890"
  },
  {
    "text": "and what it does is it gets all the list of all the PVCs that are created on that",
    "start": "1136890",
    "end": "1142890"
  },
  {
    "text": "particular node by the query civic pod we get the PVCs and attached to the",
    "start": "1142890",
    "end": "1149160"
  },
  {
    "text": "speck in the mutating maple so when the data distribution demented part gets",
    "start": "1149160",
    "end": "1155910"
  },
  {
    "text": "recreated on the node it has a list of all the PVCs that it needs to share with",
    "start": "1155910",
    "end": "1161220"
  },
  {
    "text": "the query serving pot this this is one of the solutions that we have built in",
    "start": "1161220",
    "end": "1168150"
  },
  {
    "text": "order to achieve this particular grid pattern once we have laid out the grid",
    "start": "1168150",
    "end": "1174750"
  },
  {
    "text": "pattern that the next thing we set out to do is to look at the performance of",
    "start": "1174750",
    "end": "1180390"
  },
  {
    "text": "this particular application right out of the box we saw a 10 to 12 percent",
    "start": "1180390",
    "end": "1186180"
  },
  {
    "text": "degradation in the performance at around 20 percent CP utilization we wanted to",
    "start": "1186180",
    "end": "1193080"
  },
  {
    "text": "be as close to our bare metal performance because this is a latency sensitive application and we cannot",
    "start": "1193080",
    "end": "1199230"
  },
  {
    "text": "compromise on the performance of the application so we set out to look at the",
    "start": "1199230",
    "end": "1205440"
  },
  {
    "text": "performance of the of our case node that we on which the search is hosted on so",
    "start": "1205440",
    "end": "1212850"
  },
  {
    "text": "what helped us move the curve there are three important things that helped us move the curve the kernel CPU and",
    "start": "1212850",
    "end": "1219660"
  },
  {
    "text": "networking we made sure all our Colonels are upgraded to the latest version on order",
    "start": "1219660",
    "end": "1226410"
  },
  {
    "text": "KH nodes the second factor is the CPU we started",
    "start": "1226410",
    "end": "1232740"
  },
  {
    "text": "with the CPU manager which is a feature which is available in Cuba natives as many of you are aware this actually",
    "start": "1232740",
    "end": "1240090"
  },
  {
    "text": "helps in better distribution of the workloads by exclusively pinning the CPUs to that workload however this",
    "start": "1240090",
    "end": "1246840"
  },
  {
    "text": "particular change did not help us much we then disable the irq balancer on the",
    "start": "1246840",
    "end": "1252330"
  },
  {
    "text": "node this is to make sure we disable the hardware interrupts this did not know",
    "start": "1252330",
    "end": "1257400"
  },
  {
    "text": "the calibrator then we set out to tune our p state and c state parameters to",
    "start": "1257400",
    "end": "1262860"
  },
  {
    "text": "enable the turbo boost technology on the bare meter this actually had the smoother curve significantly high",
    "start": "1262860",
    "end": "1269600"
  },
  {
    "text": "however we were not close to the bare metal performance here so we wanted to",
    "start": "1269600",
    "end": "1274710"
  },
  {
    "text": "get as much close as possible we started looking into our networking on the gates",
    "start": "1274710",
    "end": "1279870"
  },
  {
    "text": "not for normal workloads we use OBS bridge however when the network packet",
    "start": "1279870",
    "end": "1286230"
  },
  {
    "text": "transfer happens from the bare metal host to the pod interface we see that there are a lot of soft irq in droves",
    "start": "1286230",
    "end": "1292260"
  },
  {
    "text": "happening during this process thereby reducing the network throughput and also",
    "start": "1292260",
    "end": "1297960"
  },
  {
    "text": "increasing the CPU usage so we switch to IP wheel and networking this networking",
    "start": "1297960",
    "end": "1304260"
  },
  {
    "text": "change reduced the number of soft irq interrupts thereby increasing the network throughput on the node so this",
    "start": "1304260",
    "end": "1311010"
  },
  {
    "text": "brought us closer to the performance that we wanted to after making these",
    "start": "1311010",
    "end": "1316020"
  },
  {
    "text": "changes we were nearly able to match the bare metal performance at around 80% CPU",
    "start": "1316020",
    "end": "1321810"
  },
  {
    "text": "utilization at 20% and 50% we are exactly the 80% we are close to the",
    "start": "1321810",
    "end": "1328470"
  },
  {
    "text": "pavement of performance so in short we were able to achieve the speed flexibility of the search application",
    "start": "1328470",
    "end": "1337530"
  },
  {
    "text": "without compromising more much on the performance of that particular application for the lessons learned as",
    "start": "1337530",
    "end": "1344670"
  },
  {
    "text": "part of this journey I'll hand over back to my thanks",
    "start": "1344670",
    "end": "1350090"
  },
  {
    "text": "so Cassini is all now it's it's new for us but it's still older so it's like",
    "start": "1350260",
    "end": "1355840"
  },
  {
    "text": "it's been around for seven eight years that we know of so over the years we've accumulated a lot of TechNet it was",
    "start": "1355840",
    "end": "1361630"
  },
  {
    "text": "really difficult for us to take this monolithic application and break this down into micro services or containers",
    "start": "1361630",
    "end": "1368080"
  },
  {
    "text": "and trying to figure out how the containers will communicate to each other it was painful it was really painful so",
    "start": "1368080",
    "end": "1375190"
  },
  {
    "text": "that is one of the things that I would suggest you start early in their design about that this is not a short journey",
    "start": "1375190",
    "end": "1383950"
  },
  {
    "text": "for us it's not something that will happen in a matter of few months that all of the search will suddenly move on",
    "start": "1383950",
    "end": "1389290"
  },
  {
    "text": "top of kubernetes it's a long journey that's going to take a few years so we wanted to make sure that we minimize the",
    "start": "1389290",
    "end": "1396280"
  },
  {
    "text": "disruption to our clients our users and so we have to do things in a way that",
    "start": "1396280",
    "end": "1401530"
  },
  {
    "text": "sort of doesn't impact a lot of our clients what I mean is we are writing matrix in the kubernetes world infra",
    "start": "1401530",
    "end": "1409299"
  },
  {
    "text": "meatiest format but we're also writing them in the old format we are setting out alerts on top of the Prometheus",
    "start": "1409299",
    "end": "1414429"
  },
  {
    "text": "metrics but we are also setting alerts on top of the older format just to make sure that our tools and capabilities",
    "start": "1414429",
    "end": "1419740"
  },
  {
    "text": "that we have already worked but this also what this also did was sort of maintain gave us a chance to get the old",
    "start": "1419740",
    "end": "1427900"
  },
  {
    "text": "system and the new system so we had apples and apples comparison that we could do now with between both the",
    "start": "1427900",
    "end": "1432940"
  },
  {
    "text": "systems one more example is we had to develop a proxy layer which abstracts",
    "start": "1432940",
    "end": "1439840"
  },
  {
    "text": "out for our clients all of the underlying topology or the layout of the",
    "start": "1439840",
    "end": "1445390"
  },
  {
    "text": "grid our clients code to that proxy layer using restful services and this proxy layer then hides out whether",
    "start": "1445390",
    "end": "1451690"
  },
  {
    "text": "search is running on bare metals or is it running on kubernetes so essentially",
    "start": "1451690",
    "end": "1457090"
  },
  {
    "text": "if we ever now decide to change technologies again hopefully it's causes less disruption for all of our clients",
    "start": "1457090",
    "end": "1462190"
  },
  {
    "text": "and we can just change the proxy layer and it's ignores and the clients above are agnostic so we spent a lot of time",
    "start": "1462190",
    "end": "1467590"
  },
  {
    "text": "developing this layer the next one and the most controversial one was running",
    "start": "1467590",
    "end": "1473950"
  },
  {
    "text": "the data distribution agent Fahd as a separate entity as part of a daemon set as we saw rather than as a sidecar along",
    "start": "1473950",
    "end": "1481120"
  },
  {
    "text": "with all the pods the co a serving pods this sort of brought in a lot of design",
    "start": "1481120",
    "end": "1487150"
  },
  {
    "text": "changes for example as you saw we had to create a mutating web book so that we can mount the PVCs with on these two",
    "start": "1487150",
    "end": "1493690"
  },
  {
    "text": "separate parts now but it was important for our teams to do it this way because the life cycles of those two things is",
    "start": "1493690",
    "end": "1499809"
  },
  {
    "text": "completely separate and they wanted to keep them separate and not intermingle than with one another if they have to do",
    "start": "1499809",
    "end": "1505570"
  },
  {
    "text": "upgrades and such or such things so we had to jump through hoops to basically make this work and sort of mutating",
    "start": "1505570",
    "end": "1511720"
  },
  {
    "text": "where books saved us their notes",
    "start": "1511720",
    "end": "1518950"
  },
  {
    "text": "eventually died things go wrong what we had naively assumed is that we are",
    "start": "1518950",
    "end": "1524679"
  },
  {
    "text": "running on top of kubernetes if a pod dies the stateful set will spawn it again not quite true so we didn't sort",
    "start": "1524679",
    "end": "1530860"
  },
  {
    "text": "of get this design thoroughly when we started this journey we are not using networked PVCs we are using local PVCs",
    "start": "1530860",
    "end": "1536770"
  },
  {
    "text": "for performance reasons and when a node dies exactly when is it okay to now",
    "start": "1536770",
    "end": "1542230"
  },
  {
    "text": "delete the PVC so that the stateful set can launch another pod on another node that is available it's still something",
    "start": "1542230",
    "end": "1548919"
  },
  {
    "text": "that we are working on an ironing out so that is not something that we fully vetted when we started this journey and",
    "start": "1548919",
    "end": "1556510"
  },
  {
    "text": "of course performance you can never start early with performance it took us a lot of trial and error a lot of runs",
    "start": "1556510",
    "end": "1561760"
  },
  {
    "text": "of our search grid hats off to our performance guys over here sort of figuring out what works and what",
    "start": "1561760",
    "end": "1567159"
  },
  {
    "text": "doesn't work and especially for a latency sensitive application where every millisecond counts even though we",
    "start": "1567159",
    "end": "1574360"
  },
  {
    "text": "had started early in the design that we're gonna measure everything from the get-go you can never start early you",
    "start": "1574360",
    "end": "1579730"
  },
  {
    "text": "know so yeah that's one of the lessons learned future work so this brings me",
    "start": "1579730",
    "end": "1585970"
  },
  {
    "text": "what we are excited to work on next now that we've proved that search can run on top of kubernetes and we've modeled our",
    "start": "1585970",
    "end": "1591760"
  },
  {
    "text": "workload you saw earlier in the slides we have this data Orchestrator layer that we use to to sort of download this",
    "start": "1591760",
    "end": "1600850"
  },
  {
    "text": "new snapshot of the inventory to our search grid we also have to restart all of these nodes when the new snapshot is",
    "start": "1600850",
    "end": "1607059"
  },
  {
    "text": "available this takes time in the default strategies that are available with stateful sets when we started because",
    "start": "1607059",
    "end": "1613539"
  },
  {
    "text": "when you update the stateful set it go it went pod by four which meant it could take tens of hours",
    "start": "1613539",
    "end": "1619120"
  },
  {
    "text": "for our size of search grid to finish that just one data upgrade so are we use",
    "start": "1619120",
    "end": "1625659"
  },
  {
    "text": "the data or test return to parallel East we start by rows now in the newer",
    "start": "1625659",
    "end": "1630940"
  },
  {
    "text": "versions of kubernetes we have this max unavailable strategy and so we are excited to sort of try and use that and",
    "start": "1630940",
    "end": "1636460"
  },
  {
    "text": "offload some of more work to the platform itself recently if also hard",
    "start": "1636460",
    "end": "1642009"
  },
  {
    "text": "talks about volume cloning so you saw earlier that whenever there is load on the search grid we scale and bring up",
    "start": "1642009",
    "end": "1648820"
  },
  {
    "text": "new pods but the data still has to come to the new pods and that takes time",
    "start": "1648820",
    "end": "1654389"
  },
  {
    "text": "sometimes can take hours so we are going to experiment and see if we can",
    "start": "1654389",
    "end": "1659559"
  },
  {
    "text": "implement something around volume cloning with when local PVCs are installed such that whenever a new pod",
    "start": "1659559",
    "end": "1666009"
  },
  {
    "text": "comes in you can download this data from a pure pod rather than going across to our Duplessis where the data is",
    "start": "1666009",
    "end": "1674190"
  },
  {
    "text": "yeah no remediation with local PVC really needs to be ironed out for us at which piece in the node remediation",
    "start": "1674190",
    "end": "1681370"
  },
  {
    "text": "workflow do we listen for an event and say ok the return of investment on now deleting this data is good enough that",
    "start": "1681370",
    "end": "1688570"
  },
  {
    "text": "we should delete it now so that the pod can come up on another node versus maybe a few restarts we'll fix it and it'll",
    "start": "1688570",
    "end": "1695440"
  },
  {
    "text": "come up in a matter of seconds so we are still going to work on ironing out that thing as search on boards more and more",
    "start": "1695440",
    "end": "1702250"
  },
  {
    "text": "on top of kubernetes or cluster sizes are going to grow we have use cases that require sometimes 7000 servers and we",
    "start": "1702250",
    "end": "1710649"
  },
  {
    "text": "want this to be more manageable we might have to break it into multiple clusters which means that the controllers and",
    "start": "1710649",
    "end": "1716740"
  },
  {
    "text": "operators that we write should have the ability to have this uber view across the clusters and so the like say make",
    "start": "1716740",
    "end": "1723789"
  },
  {
    "text": "matrix deployment controller or the data distribution controllers that we have we want to add multi cluster support to it",
    "start": "1723789",
    "end": "1730888"
  },
  {
    "text": "not also use cases are very latency specific typically for latency sensitive",
    "start": "1731129",
    "end": "1736779"
  },
  {
    "text": "applications with our use cases for search we don't share our cluster with anybody else but not all the CLA's our",
    "start": "1736779",
    "end": "1744009"
  },
  {
    "text": "use cases are that sensitive which means that we could run in a generic kubernetes cluster but we would still",
    "start": "1744009",
    "end": "1749529"
  },
  {
    "text": "rely on some sort of poor priority or preemption such that even when those use cases need more",
    "start": "1749529",
    "end": "1756280"
  },
  {
    "text": "parts to be spawned the other workers that are running in the cluster can be preempted so we haven't played with this",
    "start": "1756280",
    "end": "1762580"
  },
  {
    "text": "much but we intend to play with this in the next few months all right so which",
    "start": "1762580",
    "end": "1769360"
  },
  {
    "text": "brings me to the end of the talk so reiterating back it's absolutely possible to run a latency-sensitive",
    "start": "1769360",
    "end": "1774420"
  },
  {
    "text": "large-scale application state full application on top of kubernetes while keeping all of the performance or while",
    "start": "1774420",
    "end": "1781900"
  },
  {
    "text": "keeping the performance as close to bare metal as possible and leveraging all the agility flexibility that kubernetes",
    "start": "1781900",
    "end": "1787900"
  },
  {
    "text": "platforms brings thank you",
    "start": "1787900",
    "end": "1792059"
  },
  {
    "text": "[Applause]",
    "start": "1793810",
    "end": "1799050"
  },
  {
    "text": "get the mic turned on thank you have you",
    "start": "1805690",
    "end": "1811390"
  },
  {
    "text": "used elasticsearch or solar in or any",
    "start": "1811390",
    "end": "1816690"
  },
  {
    "text": "existing solutions for your search stack no when we started this journey",
    "start": "1816690",
    "end": "1822490"
  },
  {
    "text": "none of those products existed in a form that they could be they could be run on",
    "start": "1822490",
    "end": "1827500"
  },
  {
    "text": "scale so we have our own homegrown custom search engine and you mentioned Hadoop so you use sort of behind the",
    "start": "1827500",
    "end": "1834970"
  },
  {
    "text": "scenes - yes yeah so the data that goes on to these nodes so basically the search data that it's needed to serve",
    "start": "1834970",
    "end": "1841780"
  },
  {
    "text": "the relevance and type with the relevance and all the listings back up the stack we use Hadoop to create those",
    "start": "1841780",
    "end": "1847720"
  },
  {
    "text": "and that's not running on kubernetes is you Hadoop is one of the things that we run and how do I get it oh so how do P",
    "start": "1847720",
    "end": "1854350"
  },
  {
    "text": "is also part of you so as I mention like one of the slides we mentioned that there are some external and internal use cases search is an external user facing",
    "start": "1854350",
    "end": "1860799"
  },
  {
    "text": "but there's also internal use cases like Hadoop or Rai pipelines that are also running on kubernetes now all right",
    "start": "1860799",
    "end": "1866110"
  },
  {
    "text": "thank you hi thank you for the perspective one",
    "start": "1866110",
    "end": "1873010"
  },
  {
    "text": "question is in your future plans do you have a planning to move towards the service mesh architecture or is that not",
    "start": "1873010",
    "end": "1879760"
  },
  {
    "text": "not so the immediate roadmap it's something that we have just started",
    "start": "1879760",
    "end": "1885700"
  },
  {
    "text": "talking about but it's not immediately something that we are working going to work on in the next year or so because",
    "start": "1885700",
    "end": "1892179"
  },
  {
    "text": "there's lots of migration effort from our side that needs to happen before we can think start thinking about using",
    "start": "1892179",
    "end": "1897549"
  },
  {
    "text": "service mass for architecture another question so you're running Hadoop on",
    "start": "1897549",
    "end": "1903490"
  },
  {
    "text": "kubernetes right are you using any operator to do that or how I can answer",
    "start": "1903490",
    "end": "1911830"
  },
  {
    "text": "that question so managing Hadoop within eBay is we use custom tooling and those",
    "start": "1911830",
    "end": "1920679"
  },
  {
    "text": "tools actually managed how do on top of cuban ideas so we integrated cuban it is",
    "start": "1920679",
    "end": "1929200"
  },
  {
    "text": "API servers without it all right any other questions Oh",
    "start": "1929200",
    "end": "1936100"
  },
  {
    "text": "hi Meno what what is your sharding policy how do you actually share your loads",
    "start": "1938180",
    "end": "1944210"
  },
  {
    "text": "across that's a bigger that pick was a bigger answer I can meet you both",
    "start": "1944210",
    "end": "1949940"
  },
  {
    "text": "orchestra for that yeah and and the question is you mentioned that your data sets are constantly updated multiple",
    "start": "1949940",
    "end": "1957170"
  },
  {
    "text": "times a day or something like that right yeah so is there a chance of your data getting state the search getting stale",
    "start": "1957170",
    "end": "1963080"
  },
  {
    "text": "yes so search is an eventual consistent system we never guarantee that you will",
    "start": "1963080",
    "end": "1968420"
  },
  {
    "text": "have full consistency so at any point in time one so let's say there are two snapshots of an inventory right we don't",
    "start": "1968420",
    "end": "1975440"
  },
  {
    "text": "be comment load the that snapshot inventory inventory in a single short across all the servers so at any given",
    "start": "1975440",
    "end": "1981020"
  },
  {
    "text": "time some servers will have the newer snapshot some servers will have the older snapshot but we are also incrementally sending it data all the",
    "start": "1981020",
    "end": "1988970"
  },
  {
    "text": "time right so from the point that snapshot was taken to now we are continuously sending it data so we send",
    "start": "1988970",
    "end": "1995120"
  },
  {
    "text": "data to the older so it will switch from the older snap short time and for the newer servers we send continued uh from",
    "start": "1995120",
    "end": "2000520"
  },
  {
    "text": "the new snapshot time last question what's the storage behind for the query",
    "start": "2000520",
    "end": "2007300"
  },
  {
    "text": "serving grid it's all in memory the data is stored in memory I think it's a great",
    "start": "2007300",
    "end": "2012640"
  },
  {
    "text": "time for one more question if there is one nope okay I think that's it that you",
    "start": "2012640",
    "end": "2018649"
  },
  {
    "text": "guys very much and don't forget to take the survey ping thank thank you thank you [Applause]",
    "start": "2018649",
    "end": "2024690"
  }
]