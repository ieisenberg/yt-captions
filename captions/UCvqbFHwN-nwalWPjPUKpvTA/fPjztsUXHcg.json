[
  {
    "start": "0",
    "end": "9000"
  },
  {
    "text": "hi everyone so today we're going to be",
    "start": "0",
    "end": "1680"
  },
  {
    "text": "talking about how here at foreign",
    "start": "1680",
    "end": "3679"
  },
  {
    "text": "intelligence we are making complex art",
    "start": "3679",
    "end": "5680"
  },
  {
    "text": "forecast applications into production",
    "start": "5680",
    "end": "7839"
  },
  {
    "text": "using our go workflow so just to give a",
    "start": "7839",
    "end": "10080"
  },
  {
    "start": "9000",
    "end": "51000"
  },
  {
    "text": "little bit of a background we started as",
    "start": "10080",
    "end": "12080"
  },
  {
    "text": "an economics consultancy company that",
    "start": "12080",
    "end": "15280"
  },
  {
    "text": "focus mainly on time series data",
    "start": "15280",
    "end": "18080"
  },
  {
    "text": "and we are now and we have already a",
    "start": "18080",
    "end": "20640"
  },
  {
    "text": "first version developing what we call",
    "start": "20640",
    "end": "22560"
  },
  {
    "text": "forecast as a service the idea is that",
    "start": "22560",
    "end": "25680"
  },
  {
    "text": "people with little or no background on",
    "start": "25680",
    "end": "28240"
  },
  {
    "text": "statistics or economics can run analysis",
    "start": "28240",
    "end": "31119"
  },
  {
    "text": "very sophisticated running from",
    "start": "31119",
    "end": "32960"
  },
  {
    "text": "traditional statistical algorithms up to",
    "start": "32960",
    "end": "36000"
  },
  {
    "text": "automl functions",
    "start": "36000",
    "end": "38480"
  },
  {
    "text": "and they can do that with just a few",
    "start": "38480",
    "end": "40719"
  },
  {
    "text": "clicks and we also focus on having very",
    "start": "40719",
    "end": "43600"
  },
  {
    "text": "good",
    "start": "43600",
    "end": "44399"
  },
  {
    "text": "front-end applications where the person",
    "start": "44399",
    "end": "46719"
  },
  {
    "text": "can easily access all of the results and",
    "start": "46719",
    "end": "48879"
  },
  {
    "text": "have a good understanding of what's",
    "start": "48879",
    "end": "50480"
  },
  {
    "text": "going on in there as we started to shift",
    "start": "50480",
    "end": "53120"
  },
  {
    "start": "51000",
    "end": "110000"
  },
  {
    "text": "from consultancy focus to a technology",
    "start": "53120",
    "end": "55680"
  },
  {
    "text": "company we had a lot of challenges the",
    "start": "55680",
    "end": "58320"
  },
  {
    "text": "first one was that we were used to using",
    "start": "58320",
    "end": "61600"
  },
  {
    "text": "r in all of our developments so if you",
    "start": "61600",
    "end": "64320"
  },
  {
    "text": "look at the background of our scientists",
    "start": "64320",
    "end": "67119"
  },
  {
    "text": "data scientists they are mostly from",
    "start": "67119",
    "end": "69360"
  },
  {
    "text": "statistics economics engineers and use",
    "start": "69360",
    "end": "72799"
  },
  {
    "text": "our on a daily basis so we needed to",
    "start": "72799",
    "end": "75280"
  },
  {
    "text": "bring archer production we needed to use",
    "start": "75280",
    "end": "78080"
  },
  {
    "text": "that team know-how that we had and we",
    "start": "78080",
    "end": "80159"
  },
  {
    "text": "also wanted to do this while optimizing",
    "start": "80159",
    "end": "82159"
  },
  {
    "text": "our cost",
    "start": "82159",
    "end": "83759"
  },
  {
    "text": "in the same line we wanted to have",
    "start": "83759",
    "end": "86240"
  },
  {
    "text": "simpler and faster deploys because in",
    "start": "86240",
    "end": "89360"
  },
  {
    "text": "the beginning it would take hours maybe",
    "start": "89360",
    "end": "91600"
  },
  {
    "text": "on a friday night friday evening and we",
    "start": "91600",
    "end": "94880"
  },
  {
    "text": "knew that it was something that we",
    "start": "94880",
    "end": "96240"
  },
  {
    "text": "needed to do",
    "start": "96240",
    "end": "97840"
  },
  {
    "text": "but one thing that we're not willing to",
    "start": "97840",
    "end": "99680"
  },
  {
    "text": "let go is the reliability of our results",
    "start": "99680",
    "end": "102320"
  },
  {
    "text": "we really focus on having accurate",
    "start": "102320",
    "end": "105040"
  },
  {
    "text": "forecasts and while bringing",
    "start": "105040",
    "end": "107840"
  },
  {
    "text": "interpretability to of the results to",
    "start": "107840",
    "end": "110000"
  },
  {
    "start": "110000",
    "end": "186000"
  },
  {
    "text": "our clients but how did we get there so",
    "start": "110000",
    "end": "113040"
  },
  {
    "text": "our data scientists working on different",
    "start": "113040",
    "end": "115360"
  },
  {
    "text": "projects would see the need of a",
    "start": "115360",
    "end": "116960"
  },
  {
    "text": "different approach or a different",
    "start": "116960",
    "end": "118799"
  },
  {
    "text": "methodology that was new or something",
    "start": "118799",
    "end": "120799"
  },
  {
    "text": "like that and they would implement this",
    "start": "120799",
    "end": "122719"
  },
  {
    "text": "in our writing functions and put it in",
    "start": "122719",
    "end": "125520"
  },
  {
    "text": "putting it inside our pipeline",
    "start": "125520",
    "end": "128319"
  },
  {
    "text": "the idea was that once you have this in",
    "start": "128319",
    "end": "130800"
  },
  {
    "text": "the pipeline other teammates could help",
    "start": "130800",
    "end": "133280"
  },
  {
    "text": "you debug",
    "start": "133280",
    "end": "134400"
  },
  {
    "text": "and also use on their projects but the",
    "start": "134400",
    "end": "137040"
  },
  {
    "text": "problem that we faced sometimes was that",
    "start": "137040",
    "end": "139680"
  },
  {
    "text": "there was no version in control maybe i",
    "start": "139680",
    "end": "142000"
  },
  {
    "text": "was working on a version that i debugged",
    "start": "142000",
    "end": "144879"
  },
  {
    "text": "and a teammate was working on a previous",
    "start": "144879",
    "end": "147200"
  },
  {
    "text": "one we started doing super repos where",
    "start": "147200",
    "end": "150239"
  },
  {
    "text": "we put everything in there and we would",
    "start": "150239",
    "end": "152000"
  },
  {
    "text": "have maybe version one two three but",
    "start": "152000",
    "end": "155120"
  },
  {
    "text": "there was no actual version in control",
    "start": "155120",
    "end": "157840"
  },
  {
    "text": "at the point that we started doing that",
    "start": "157840",
    "end": "159760"
  },
  {
    "text": "thing doing that things got a lot better",
    "start": "159760",
    "end": "162560"
  },
  {
    "text": "for us so another big issue that we",
    "start": "162560",
    "end": "164640"
  },
  {
    "text": "faced was that we were running the codes",
    "start": "164640",
    "end": "167040"
  },
  {
    "text": "locally and",
    "start": "167040",
    "end": "168879"
  },
  {
    "text": "with all of the time series that we had",
    "start": "168879",
    "end": "170879"
  },
  {
    "text": "maybe we were running a thousand of them",
    "start": "170879",
    "end": "173599"
  },
  {
    "text": "with very exhaustive cross-validation it",
    "start": "173599",
    "end": "176480"
  },
  {
    "text": "would take a lot of time and once you",
    "start": "176480",
    "end": "179040"
  },
  {
    "text": "find a bug you have to start all over",
    "start": "179040",
    "end": "181280"
  },
  {
    "text": "and we saw the need to go to the cloud",
    "start": "181280",
    "end": "183760"
  },
  {
    "text": "it was something that we couldn't",
    "start": "183760",
    "end": "185519"
  },
  {
    "text": "postpone anymore once we lift and",
    "start": "185519",
    "end": "187680"
  },
  {
    "start": "186000",
    "end": "277000"
  },
  {
    "text": "shifted our coach the cloud there were a",
    "start": "187680",
    "end": "189760"
  },
  {
    "text": "lot of gains because we noticed that the",
    "start": "189760",
    "end": "192080"
  },
  {
    "text": "productivity of our data scientists grew",
    "start": "192080",
    "end": "195040"
  },
  {
    "text": "a lot we also were a lot more sure about",
    "start": "195040",
    "end": "197920"
  },
  {
    "text": "the reliability of the results we knew",
    "start": "197920",
    "end": "200480"
  },
  {
    "text": "that",
    "start": "200480",
    "end": "201440"
  },
  {
    "text": "that it went through our entire pipeline",
    "start": "201440",
    "end": "203760"
  },
  {
    "text": "from pre-processing feature selection",
    "start": "203760",
    "end": "206560"
  },
  {
    "text": "modeling",
    "start": "206560",
    "end": "207680"
  },
  {
    "text": "and post modeling all of the analysis",
    "start": "207680",
    "end": "211040"
  },
  {
    "text": "that we have we were sure that they were",
    "start": "211040",
    "end": "213680"
  },
  {
    "text": "being done properly and we did all of",
    "start": "213680",
    "end": "216159"
  },
  {
    "text": "that using r if you look at the way that",
    "start": "216159",
    "end": "219040"
  },
  {
    "text": "we used to do things we used art for api",
    "start": "219040",
    "end": "221840"
  },
  {
    "text": "orchestrating outputting and",
    "start": "221840",
    "end": "223599"
  },
  {
    "text": "communicating with google cloud even our",
    "start": "223599",
    "end": "225760"
  },
  {
    "text": "front end was developed in r",
    "start": "225760",
    "end": "227920"
  },
  {
    "text": "we also had a monolithic structures if i",
    "start": "227920",
    "end": "231040"
  },
  {
    "text": "was running maybe 97 it's a very classic",
    "start": "231040",
    "end": "234000"
  },
  {
    "text": "case for us we were running 97 time",
    "start": "234000",
    "end": "236560"
  },
  {
    "text": "series and i sent the request via api we",
    "start": "236560",
    "end": "240400"
  },
  {
    "text": "would run things on batch so we would",
    "start": "240400",
    "end": "242799"
  },
  {
    "text": "send that 97 jobs to one virtual machine",
    "start": "242799",
    "end": "247200"
  },
  {
    "text": "and we had four at the time we had a big",
    "start": "247200",
    "end": "250560"
  },
  {
    "text": "problem with lines but still from",
    "start": "250560",
    "end": "253360"
  },
  {
    "text": "running locally to going to the cloud we",
    "start": "253360",
    "end": "255599"
  },
  {
    "text": "had a great improvement but we started",
    "start": "255599",
    "end": "258000"
  },
  {
    "text": "having new problems with getting too big",
    "start": "258000",
    "end": "261280"
  },
  {
    "text": "for this structure being run in r and",
    "start": "261280",
    "end": "264639"
  },
  {
    "text": "that's when we started implementing the",
    "start": "264639",
    "end": "266800"
  },
  {
    "text": "new infrastructure using argo and the",
    "start": "266800",
    "end": "268880"
  },
  {
    "text": "kubernetes so as the lines started to",
    "start": "268880",
    "end": "272240"
  },
  {
    "text": "grow and",
    "start": "272240",
    "end": "273600"
  },
  {
    "text": "the jobs would take a lot of time to run",
    "start": "273600",
    "end": "275680"
  },
  {
    "text": "we started looking for new solutions as",
    "start": "275680",
    "end": "278080"
  },
  {
    "start": "277000",
    "end": "311000"
  },
  {
    "text": "our focus started shifting scalability",
    "start": "278080",
    "end": "280479"
  },
  {
    "text": "became a bottleneck so we needed to be",
    "start": "280479",
    "end": "283280"
  },
  {
    "text": "able to scale what we're doing so we",
    "start": "283280",
    "end": "285600"
  },
  {
    "text": "could grow",
    "start": "285600",
    "end": "286960"
  },
  {
    "text": "and",
    "start": "286960",
    "end": "288160"
  },
  {
    "text": "what we wanted to do we wanted our code",
    "start": "288160",
    "end": "290160"
  },
  {
    "text": "to be generic and accurate so we want to",
    "start": "290160",
    "end": "293919"
  },
  {
    "text": "run as different time series as possible",
    "start": "293919",
    "end": "297280"
  },
  {
    "text": "we want to have great forecasts but we",
    "start": "297280",
    "end": "300000"
  },
  {
    "text": "want to do it fast",
    "start": "300000",
    "end": "301919"
  },
  {
    "text": "how to make it work in an efficient",
    "start": "301919",
    "end": "303680"
  },
  {
    "text": "cloud environment should we keep using r",
    "start": "303680",
    "end": "306080"
  },
  {
    "text": "should we change both infra and",
    "start": "306080",
    "end": "307840"
  },
  {
    "text": "algorithms that's what we had to decide",
    "start": "307840",
    "end": "310240"
  },
  {
    "text": "at that point",
    "start": "310240",
    "end": "311840"
  },
  {
    "start": "311000",
    "end": "367000"
  },
  {
    "text": "at this point we decided to look at the",
    "start": "311840",
    "end": "313680"
  },
  {
    "text": "pros and cons of r of keeping using r",
    "start": "313680",
    "end": "316479"
  },
  {
    "text": "the advantages were first that we had a",
    "start": "316479",
    "end": "318880"
  },
  {
    "text": "legacy all of our codes were written in",
    "start": "318880",
    "end": "321280"
  },
  {
    "text": "our we were very comfortable doing that",
    "start": "321280",
    "end": "324320"
  },
  {
    "text": "the team know how even from developers",
    "start": "324320",
    "end": "326800"
  },
  {
    "text": "to users they all knew how to use r very",
    "start": "326800",
    "end": "329759"
  },
  {
    "text": "fluently and we also have the fact that",
    "start": "329759",
    "end": "332400"
  },
  {
    "text": "r is largely used in academics we",
    "start": "332400",
    "end": "335840"
  },
  {
    "text": "focus on having new implementations and",
    "start": "335840",
    "end": "338880"
  },
  {
    "text": "most of the statistical new",
    "start": "338880",
    "end": "340320"
  },
  {
    "text": "implementations are available in r very",
    "start": "340320",
    "end": "342720"
  },
  {
    "text": "fast so we could have new approaches new",
    "start": "342720",
    "end": "346000"
  },
  {
    "text": "methodologies within clicks and the",
    "start": "346000",
    "end": "348639"
  },
  {
    "text": "drawbacks are that there's very limited",
    "start": "348639",
    "end": "351280"
  },
  {
    "text": "native support to cloud environments and",
    "start": "351280",
    "end": "354000"
  },
  {
    "text": "once we started needing",
    "start": "354000",
    "end": "355919"
  },
  {
    "text": "help from other developers outside of",
    "start": "355919",
    "end": "358319"
  },
  {
    "text": "this academic world they were not",
    "start": "358319",
    "end": "360400"
  },
  {
    "text": "familiarized with r they used more",
    "start": "360400",
    "end": "362560"
  },
  {
    "text": "python or other languages so we had to",
    "start": "362560",
    "end": "365280"
  },
  {
    "text": "make sure that we were on the same page",
    "start": "365280",
    "end": "367759"
  },
  {
    "start": "367000",
    "end": "391000"
  },
  {
    "text": "we got to the point where we map our",
    "start": "367759",
    "end": "369600"
  },
  {
    "text": "solution needs we needed scalability so",
    "start": "369600",
    "end": "372160"
  },
  {
    "text": "we need to be able to run",
    "start": "372160",
    "end": "374479"
  },
  {
    "text": "lots of time series at the same time and",
    "start": "374479",
    "end": "376960"
  },
  {
    "text": "also long ones",
    "start": "376960",
    "end": "378639"
  },
  {
    "text": "and within be ready within a few minutes",
    "start": "378639",
    "end": "382240"
  },
  {
    "text": "we need a resilience we need to have",
    "start": "382240",
    "end": "384319"
  },
  {
    "text": "some retry strategy we need a decoupling",
    "start": "384319",
    "end": "387280"
  },
  {
    "text": "dockerization cost efficiency monitoring",
    "start": "387280",
    "end": "390240"
  },
  {
    "text": "and microservice friendly so next we're",
    "start": "390240",
    "end": "392319"
  },
  {
    "start": "391000",
    "end": "422000"
  },
  {
    "text": "going to talk a little bit about the",
    "start": "392319",
    "end": "393600"
  },
  {
    "text": "solutions that we showed in the previous",
    "start": "393600",
    "end": "395919"
  },
  {
    "text": "slide and we start with the decoder step",
    "start": "395919",
    "end": "398560"
  },
  {
    "text": "once a job is sent via api it goes to",
    "start": "398560",
    "end": "401440"
  },
  {
    "text": "the decoder which divides the jobs into",
    "start": "401440",
    "end": "404400"
  },
  {
    "text": "different vms it parallelizes and scales",
    "start": "404400",
    "end": "407600"
  },
  {
    "text": "up the jaw the machines on demand at",
    "start": "407600",
    "end": "410319"
  },
  {
    "text": "this point you can see that we kept r",
    "start": "410319",
    "end": "413120"
  },
  {
    "text": "for all of the modeling but we are",
    "start": "413120",
    "end": "415120"
  },
  {
    "text": "introducing python in here the decoder",
    "start": "415120",
    "end": "417759"
  },
  {
    "text": "is done in python which is now sent to",
    "start": "417759",
    "end": "420560"
  },
  {
    "text": "the different virtual machines",
    "start": "420560",
    "end": "422800"
  },
  {
    "start": "422000",
    "end": "484000"
  },
  {
    "text": "so what we did at this point is that we",
    "start": "422800",
    "end": "425840"
  },
  {
    "text": "fit our sessions in small images we",
    "start": "425840",
    "end": "428479"
  },
  {
    "text": "start we took that big images and we",
    "start": "428479",
    "end": "430960"
  },
  {
    "text": "divided it so we could be faster and",
    "start": "430960",
    "end": "433680"
  },
  {
    "text": "lighter",
    "start": "433680",
    "end": "434560"
  },
  {
    "text": "it's good to mention that we got a lot",
    "start": "434560",
    "end": "436000"
  },
  {
    "text": "of help from the our minimum community",
    "start": "436000",
    "end": "438479"
  },
  {
    "text": "they helped us build a specific version",
    "start": "438479",
    "end": "440639"
  },
  {
    "text": "that we needed and once we started",
    "start": "440639",
    "end": "442800"
  },
  {
    "text": "working with the images we faced a very",
    "start": "442800",
    "end": "445199"
  },
  {
    "text": "non challenge for our users is that we",
    "start": "445199",
    "end": "448400"
  },
  {
    "text": "had package dependencies that conflicted",
    "start": "448400",
    "end": "451599"
  },
  {
    "text": "so maybe i needed a specific version of",
    "start": "451599",
    "end": "454160"
  },
  {
    "text": "a package for one package and for",
    "start": "454160",
    "end": "455840"
  },
  {
    "text": "another i would need a different one and",
    "start": "455840",
    "end": "457919"
  },
  {
    "text": "with those conflicts we realized that",
    "start": "457919",
    "end": "459840"
  },
  {
    "text": "maybe it was best for us to build our",
    "start": "459840",
    "end": "461840"
  },
  {
    "text": "own image and with these images we",
    "start": "461840",
    "end": "464400"
  },
  {
    "text": "decided to use as little external",
    "start": "464400",
    "end": "466560"
  },
  {
    "text": "packages as possible to avoid conflict",
    "start": "466560",
    "end": "470000"
  },
  {
    "text": "let to leave our image lighter and give",
    "start": "470000",
    "end": "473360"
  },
  {
    "text": "us more flexibility inside the",
    "start": "473360",
    "end": "475360"
  },
  {
    "text": "implementations",
    "start": "475360",
    "end": "477680"
  },
  {
    "text": "so this lighter image helped us decrease",
    "start": "477680",
    "end": "480560"
  },
  {
    "text": "our building time from three hours to",
    "start": "480560",
    "end": "482560"
  },
  {
    "text": "five minutes it was a great plus for us",
    "start": "482560",
    "end": "484879"
  },
  {
    "start": "484000",
    "end": "894000"
  },
  {
    "text": "so now pedro is going to continue the",
    "start": "484879",
    "end": "486479"
  },
  {
    "text": "presentation talking about our",
    "start": "486479",
    "end": "487919"
  },
  {
    "text": "architecture solution thanks natalia",
    "start": "487919",
    "end": "491759"
  },
  {
    "text": "so after we optimized our",
    "start": "491759",
    "end": "494560"
  },
  {
    "text": "our image we had to decide how the new",
    "start": "494560",
    "end": "497520"
  },
  {
    "text": "architecture would look like coming from",
    "start": "497520",
    "end": "499440"
  },
  {
    "text": "the previously mentioned architecture we",
    "start": "499440",
    "end": "501759"
  },
  {
    "text": "decided that easier approach was to turn",
    "start": "501759",
    "end": "504160"
  },
  {
    "text": "it into a cloud run application",
    "start": "504160",
    "end": "506639"
  },
  {
    "text": "so",
    "start": "506639",
    "end": "507440"
  },
  {
    "text": "we wrapped all the our pipeline in a in",
    "start": "507440",
    "end": "510479"
  },
  {
    "text": "a python api and we plugged it after the",
    "start": "510479",
    "end": "513360"
  },
  {
    "text": "decoder what it did for us",
    "start": "513360",
    "end": "516159"
  },
  {
    "text": "is that it greatly increased our",
    "start": "516159",
    "end": "518640"
  },
  {
    "text": "performance the",
    "start": "518640",
    "end": "520479"
  },
  {
    "text": "all the jobs and the pipeline was",
    "start": "520479",
    "end": "522080"
  },
  {
    "text": "running much faster but we had some",
    "start": "522080",
    "end": "524880"
  },
  {
    "text": "issues with cost control we had some",
    "start": "524880",
    "end": "526720"
  },
  {
    "text": "problems with cost control",
    "start": "526720",
    "end": "528480"
  },
  {
    "text": "so we decided to change the focus and",
    "start": "528480",
    "end": "530080"
  },
  {
    "text": "look for another solution so that's",
    "start": "530080",
    "end": "532080"
  },
  {
    "text": "where we landed on kubernetes which is a",
    "start": "532080",
    "end": "534320"
  },
  {
    "text": "much more",
    "start": "534320",
    "end": "535519"
  },
  {
    "text": "modern approach to this kind of pipeline",
    "start": "535519",
    "end": "537920"
  },
  {
    "text": "and uh for that we decided to use the",
    "start": "537920",
    "end": "540000"
  },
  {
    "text": "argo workflow",
    "start": "540000",
    "end": "542000"
  },
  {
    "text": "what argo did for us",
    "start": "542000",
    "end": "543680"
  },
  {
    "text": "is that it",
    "start": "543680",
    "end": "545040"
  },
  {
    "text": "enabled all the micro servers like",
    "start": "545040",
    "end": "548000"
  },
  {
    "text": "structure with a scalable and more",
    "start": "548000",
    "end": "551760"
  },
  {
    "text": "cost controlled architecture",
    "start": "551760",
    "end": "554080"
  },
  {
    "text": "all running in an isolated ecosystem so",
    "start": "554080",
    "end": "556320"
  },
  {
    "text": "we can run",
    "start": "556320",
    "end": "557600"
  },
  {
    "text": "python processes and r processes all",
    "start": "557600",
    "end": "560399"
  },
  {
    "text": "that within argo and argo will deal",
    "start": "560399",
    "end": "563440"
  },
  {
    "text": "with all the communication with the",
    "start": "563440",
    "end": "566320"
  },
  {
    "text": "google cloud platform",
    "start": "566320",
    "end": "568000"
  },
  {
    "text": "regarding the costs",
    "start": "568000",
    "end": "569839"
  },
  {
    "text": "argo allows us to",
    "start": "569839",
    "end": "571839"
  },
  {
    "text": "customize our clusters and our pods our",
    "start": "571839",
    "end": "574480"
  },
  {
    "text": "nodes to",
    "start": "574480",
    "end": "576880"
  },
  {
    "text": "know how much we will spend",
    "start": "576880",
    "end": "578959"
  },
  {
    "text": "in certain",
    "start": "578959",
    "end": "580640"
  },
  {
    "text": "situations in",
    "start": "580640",
    "end": "582000"
  },
  {
    "text": "we can",
    "start": "582000",
    "end": "583680"
  },
  {
    "text": "predefine all memory usages and how much",
    "start": "583680",
    "end": "585839"
  },
  {
    "text": "machines we're running uh regarding the",
    "start": "585839",
    "end": "588240"
  },
  {
    "text": "resilience part",
    "start": "588240",
    "end": "589600"
  },
  {
    "text": "what what it allowed us to do is to",
    "start": "589600",
    "end": "592399"
  },
  {
    "text": "implement retry and failure strategies",
    "start": "592399",
    "end": "594800"
  },
  {
    "text": "and disaster recover",
    "start": "594800",
    "end": "597040"
  },
  {
    "text": "tools as well",
    "start": "597040",
    "end": "598320"
  },
  {
    "text": "and also the application became",
    "start": "598320",
    "end": "600160"
  },
  {
    "text": "stateless and much more fault tolerant",
    "start": "600160",
    "end": "602160"
  },
  {
    "text": "which",
    "start": "602160",
    "end": "602959"
  },
  {
    "text": "all of that",
    "start": "602959",
    "end": "604399"
  },
  {
    "text": "combined formed a much more bulletproof",
    "start": "604399",
    "end": "606720"
  },
  {
    "text": "architecture regarding the tracking i",
    "start": "606720",
    "end": "608880"
  },
  {
    "text": "think it was one of the our biggest",
    "start": "608880",
    "end": "610640"
  },
  {
    "text": "improvements was that now we have uh",
    "start": "610640",
    "end": "612560"
  },
  {
    "text": "completely the couple log files so when",
    "start": "612560",
    "end": "615519"
  },
  {
    "text": "an error occurs we know exactly where",
    "start": "615519",
    "end": "617600"
  },
  {
    "text": "the error is so if it's an",
    "start": "617600",
    "end": "619440"
  },
  {
    "text": "infrastructure error or a pipeline error",
    "start": "619440",
    "end": "621680"
  },
  {
    "text": "and that allows us to",
    "start": "621680",
    "end": "624000"
  },
  {
    "text": "call the right people to solve the right",
    "start": "624000",
    "end": "625760"
  },
  {
    "text": "problems it provides us with a lot of",
    "start": "625760",
    "end": "627920"
  },
  {
    "text": "less time debugging and looking for what",
    "start": "627920",
    "end": "630560"
  },
  {
    "text": "exactly is the problem and what",
    "start": "630560",
    "end": "632880"
  },
  {
    "text": "uh is occurring and also we implemented",
    "start": "632880",
    "end": "636160"
  },
  {
    "text": "real-time monitoring",
    "start": "636160",
    "end": "637600"
  },
  {
    "text": "uh with argo workflow dashboards and",
    "start": "637600",
    "end": "640000"
  },
  {
    "text": "prometheus and grafana one of the",
    "start": "640000",
    "end": "641440"
  },
  {
    "text": "biggest",
    "start": "641440",
    "end": "642240"
  },
  {
    "text": "accomplishments that argo brings to us",
    "start": "642240",
    "end": "644880"
  },
  {
    "text": "is that it allows us",
    "start": "644880",
    "end": "647200"
  },
  {
    "text": "to move much more towards a",
    "start": "647200",
    "end": "650560"
  },
  {
    "text": "micro service approach and uh",
    "start": "650560",
    "end": "652800"
  },
  {
    "text": "it allows us to plug more and more",
    "start": "652800",
    "end": "655440"
  },
  {
    "text": "application that",
    "start": "655440",
    "end": "656800"
  },
  {
    "text": "makes the all the pipeline much less",
    "start": "656800",
    "end": "660079"
  },
  {
    "text": "language dependent so for example we",
    "start": "660079",
    "end": "662480"
  },
  {
    "text": "started everything in a pipeline with r",
    "start": "662480",
    "end": "664399"
  },
  {
    "text": "now we have machine learning models in",
    "start": "664399",
    "end": "666399"
  },
  {
    "text": "python we have deep learning models in",
    "start": "666399",
    "end": "667760"
  },
  {
    "text": "python all that can work well with argo",
    "start": "667760",
    "end": "673040"
  },
  {
    "text": "to build a pipeline that brings the best",
    "start": "673120",
    "end": "675440"
  },
  {
    "text": "of both worlds so it can have the best",
    "start": "675440",
    "end": "677680"
  },
  {
    "text": "r can provide and the best python can",
    "start": "677680",
    "end": "679519"
  },
  {
    "text": "provide we can have the latest",
    "start": "679519",
    "end": "681279"
  },
  {
    "text": "implementations at the newest solutions",
    "start": "681279",
    "end": "683920"
  },
  {
    "text": "also what what it brings to us is that",
    "start": "683920",
    "end": "686399"
  },
  {
    "text": "it allows us to have multiple processes",
    "start": "686399",
    "end": "688720"
  },
  {
    "text": "with continuous deployment and",
    "start": "688720",
    "end": "690560"
  },
  {
    "text": "continuous integration with much more",
    "start": "690560",
    "end": "692640"
  },
  {
    "text": "reliability and now moving on to how our",
    "start": "692640",
    "end": "695360"
  },
  {
    "text": "architecture actually looks",
    "start": "695360",
    "end": "697040"
  },
  {
    "text": "uh this is how it works the our user",
    "start": "697040",
    "end": "700720"
  },
  {
    "text": "it can be both an external user",
    "start": "700720",
    "end": "704000"
  },
  {
    "text": "or an internal client such as our data",
    "start": "704000",
    "end": "706560"
  },
  {
    "text": "science teams",
    "start": "706560",
    "end": "708000"
  },
  {
    "text": "they can send their jobs",
    "start": "708000",
    "end": "709839"
  },
  {
    "text": "uh",
    "start": "709839",
    "end": "710720"
  },
  {
    "text": "directly",
    "start": "710720",
    "end": "712160"
  },
  {
    "text": "to or forecast as a service or they can",
    "start": "712160",
    "end": "714800"
  },
  {
    "text": "use our created time series that we",
    "start": "714800",
    "end": "717519"
  },
  {
    "text": "provide both types of clients",
    "start": "717519",
    "end": "719839"
  },
  {
    "text": "to",
    "start": "719839",
    "end": "720560"
  },
  {
    "text": "better improve their models it will go",
    "start": "720560",
    "end": "722320"
  },
  {
    "text": "through an api that will trigger uh a",
    "start": "722320",
    "end": "725360"
  },
  {
    "text": "pub sub event then then we'll start all",
    "start": "725360",
    "end": "728160"
  },
  {
    "text": "the process so the client can send",
    "start": "728160",
    "end": "731279"
  },
  {
    "text": "hundreds and hundreds of series at once",
    "start": "731279",
    "end": "734079"
  },
  {
    "text": "the",
    "start": "734079",
    "end": "734880"
  },
  {
    "text": "need to enter the decoder the decoder",
    "start": "734880",
    "end": "737040"
  },
  {
    "text": "break it apart send it to the modeling",
    "start": "737040",
    "end": "739519"
  },
  {
    "text": "pipeline",
    "start": "739519",
    "end": "741040"
  },
  {
    "text": "and it will make all those processes",
    "start": "741040",
    "end": "743120"
  },
  {
    "text": "using the argo events to trigger all the",
    "start": "743120",
    "end": "745360"
  },
  {
    "text": "argo workflow events uh so all the",
    "start": "745360",
    "end": "747760"
  },
  {
    "text": "models will occur and it will output our",
    "start": "747760",
    "end": "751200"
  },
  {
    "text": "final result",
    "start": "751200",
    "end": "752880"
  },
  {
    "text": "and trigger all the events that send",
    "start": "752880",
    "end": "755279"
  },
  {
    "text": "the email to our client and uh that let",
    "start": "755279",
    "end": "758000"
  },
  {
    "text": "our front end for example know that the",
    "start": "758000",
    "end": "759920"
  },
  {
    "text": "job is finished so",
    "start": "759920",
    "end": "761440"
  },
  {
    "text": "the person can consume it both",
    "start": "761440",
    "end": "763839"
  },
  {
    "text": "in their own",
    "start": "763839",
    "end": "765040"
  },
  {
    "text": "ide are using our front end for post",
    "start": "765040",
    "end": "767440"
  },
  {
    "text": "processing",
    "start": "767440",
    "end": "768880"
  },
  {
    "text": "so with the architecture defined we saw",
    "start": "768880",
    "end": "771040"
  },
  {
    "text": "the need to set the parameters to allow",
    "start": "771040",
    "end": "772720"
  },
  {
    "text": "it to scale in an efficient way first of",
    "start": "772720",
    "end": "774959"
  },
  {
    "text": "all we use aggro conflict map to limit",
    "start": "774959",
    "end": "777200"
  },
  {
    "text": "the rays at which pods are created to",
    "start": "777200",
    "end": "779440"
  },
  {
    "text": "avoid overloading the case api we also",
    "start": "779440",
    "end": "782079"
  },
  {
    "text": "limited the maximum number of incomplete",
    "start": "782079",
    "end": "784000"
  },
  {
    "text": "workflows and the max total parallel",
    "start": "784000",
    "end": "786399"
  },
  {
    "text": "workflow that can be executed at the",
    "start": "786399",
    "end": "787920"
  },
  {
    "text": "same time to avoid using excessive",
    "start": "787920",
    "end": "790000"
  },
  {
    "text": "resources to mitigate ip extrusion we",
    "start": "790000",
    "end": "792800"
  },
  {
    "text": "limited the maximum pods per node",
    "start": "792800",
    "end": "795600"
  },
  {
    "text": "and",
    "start": "795600",
    "end": "796399"
  },
  {
    "text": "now",
    "start": "796399",
    "end": "797519"
  },
  {
    "text": "with a dedicated envelope team we now",
    "start": "797519",
    "end": "799839"
  },
  {
    "text": "have automated tests and deploys and",
    "start": "799839",
    "end": "802160"
  },
  {
    "text": "builds using a plethora or",
    "start": "802160",
    "end": "804639"
  },
  {
    "text": "of applications",
    "start": "804639",
    "end": "806240"
  },
  {
    "text": "which allowed us to finally reach our",
    "start": "806240",
    "end": "808079"
  },
  {
    "text": "github's ci cd",
    "start": "808079",
    "end": "810240"
  },
  {
    "text": "which",
    "start": "810240",
    "end": "811200"
  },
  {
    "text": "in turn also led to a much more stable",
    "start": "811200",
    "end": "814639"
  },
  {
    "text": "level of cost optimization",
    "start": "814639",
    "end": "816720"
  },
  {
    "text": "and also",
    "start": "816720",
    "end": "819199"
  },
  {
    "text": "designing tests for vulnerability in",
    "start": "819519",
    "end": "822480"
  },
  {
    "text": "contain in both containers and package",
    "start": "822480",
    "end": "824560"
  },
  {
    "text": "dependencies",
    "start": "824560",
    "end": "826000"
  },
  {
    "text": "we are now running all our application",
    "start": "826000",
    "end": "828560"
  },
  {
    "text": "in private clusters with no external",
    "start": "828560",
    "end": "831279"
  },
  {
    "text": "access at all and uh all our processes",
    "start": "831279",
    "end": "834079"
  },
  {
    "text": "run asynchronous so we have as many uh",
    "start": "834079",
    "end": "838320"
  },
  {
    "text": "different series",
    "start": "838320",
    "end": "839680"
  },
  {
    "text": "as memory allows us in a different",
    "start": "839680",
    "end": "842240"
  },
  {
    "text": "virtual machine so we are not locked to",
    "start": "842240",
    "end": "844320"
  },
  {
    "text": "a one job per viral machine",
    "start": "844320",
    "end": "847120"
  },
  {
    "text": "and",
    "start": "847120",
    "end": "848320"
  },
  {
    "text": "i think to wrap it up we also have",
    "start": "848320",
    "end": "850880"
  },
  {
    "text": "now",
    "start": "850880",
    "end": "851600"
  },
  {
    "text": "running it on a multi-region",
    "start": "851600",
    "end": "854399"
  },
  {
    "text": "privacy key cluster which also brings",
    "start": "854399",
    "end": "856720"
  },
  {
    "text": "more resiliency so to wrap it up",
    "start": "856720",
    "end": "859839"
  },
  {
    "text": "we",
    "start": "859839",
    "end": "860880"
  },
  {
    "text": "change far from",
    "start": "860880",
    "end": "863040"
  },
  {
    "text": "entirely built in our application in the",
    "start": "863040",
    "end": "865360"
  },
  {
    "text": "cloud with third parties",
    "start": "865360",
    "end": "867290"
  },
  {
    "text": "[Music]",
    "start": "867290",
    "end": "868800"
  },
  {
    "text": "packages doing the communication to a",
    "start": "868800",
    "end": "870880"
  },
  {
    "text": "completely cloud-native solution uh",
    "start": "870880",
    "end": "874079"
  },
  {
    "text": "which is completely",
    "start": "874079",
    "end": "876160"
  },
  {
    "text": "uh",
    "start": "876160",
    "end": "877680"
  },
  {
    "text": "independent from which code we're using",
    "start": "877680",
    "end": "879519"
  },
  {
    "text": "in our process and now we can plug in",
    "start": "879519",
    "end": "882480"
  },
  {
    "text": "different applications and",
    "start": "882480",
    "end": "884800"
  },
  {
    "text": "work",
    "start": "884800",
    "end": "885920"
  },
  {
    "text": "without any bound to which technology is",
    "start": "885920",
    "end": "888160"
  },
  {
    "text": "trending at the moment thank you very",
    "start": "888160",
    "end": "889839"
  },
  {
    "text": "much for watching thank you so much for",
    "start": "889839",
    "end": "891680"
  },
  {
    "text": "watching i hope you join us at the q a",
    "start": "891680",
    "end": "895839"
  }
]