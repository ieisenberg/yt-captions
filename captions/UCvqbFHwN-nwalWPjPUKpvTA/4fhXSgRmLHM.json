[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "hi I'm welcome to my talk chronics is a long-term story for prometheus my name",
    "start": "120",
    "end": "5190"
  },
  {
    "text": "is mullets Kimura I work at a software engineer at QA we're in Munich Germany",
    "start": "5190",
    "end": "10430"
  },
  {
    "text": "that's where Oktoberfest is if you want to contact me just shoot me a tweet at",
    "start": "10430",
    "end": "16919"
  },
  {
    "text": "at pH xql the bottom left of the slide",
    "start": "16919",
    "end": "22789"
  },
  {
    "text": "so I assume you never heard about chronics but that's no problem because",
    "start": "22789",
    "end": "28320"
  },
  {
    "text": "I'm going to introduce it to you and that's the current state of the art for",
    "start": "28320",
    "end": "33570"
  },
  {
    "start": "30000",
    "end": "166000"
  },
  {
    "text": "monitoring cloud native applications you have your running cloud native applications and use Prometheus to pull",
    "start": "33570",
    "end": "40110"
  },
  {
    "text": "the matrix in there and then you can use it for real-time monitoring and alerting and the default retention day is about",
    "start": "40110",
    "end": "46800"
  },
  {
    "text": "14 days in prometheus after that the data gets discarded so",
    "start": "46800",
    "end": "56480"
  },
  {
    "text": "nothing more to do because everything is covered we don't think so because",
    "start": "56480",
    "end": "63060"
  },
  {
    "text": "imagine you want to store it more than 14 days for example storage half a year",
    "start": "63060",
    "end": "70289"
  },
  {
    "text": "or stored a year or something like that then you could use another time series",
    "start": "70289",
    "end": "77280"
  },
  {
    "text": "database behind Prometheus so Prometheus is grabbing those metrics and we'd use",
    "start": "77280",
    "end": "83520"
  },
  {
    "text": "the default retention day of 14 days to store the data and it also pushes the data into your other database and that's",
    "start": "83520",
    "end": "91439"
  },
  {
    "text": "where chronics come into play because chronics it's a lossless long term",
    "start": "91439",
    "end": "96479"
  },
  {
    "text": "storage and it can store your data literally forever and it's as long as",
    "start": "96479",
    "end": "103469"
  },
  {
    "text": "you give it enough hard disk space it can store it we have we let us see the",
    "start": "103469",
    "end": "110249"
  },
  {
    "text": "storage requirements for that ok why",
    "start": "110249",
    "end": "115649"
  },
  {
    "text": "every created chronics that's one of our products and our company helps other",
    "start": "115649",
    "end": "125279"
  },
  {
    "text": "companies to find problems in their production you'll see a graph of a",
    "start": "125279",
    "end": "132390"
  },
  {
    "text": "running a system here and there are some hiccups in there and now imagine you don't want",
    "start": "132390",
    "end": "141240"
  },
  {
    "text": "to analyze just the last 14 days but like a year or something like that",
    "start": "141240",
    "end": "147480"
  },
  {
    "text": "to find problems before that and imagine",
    "start": "147480",
    "end": "153720"
  },
  {
    "text": "you're not monitoring just one system or some systems but you're monitoring your",
    "start": "153720",
    "end": "158970"
  },
  {
    "text": "fleet of IOT devices or something like that which generates huge amounts of metric points so that's 40 motivation",
    "start": "158970",
    "end": "169310"
  },
  {
    "text": "today I'm gonna talk a little bit of electronics how we build it from what's",
    "start": "169310",
    "end": "174720"
  },
  {
    "text": "in there and the performance how we do the integration with Prometheus and then",
    "start": "174720",
    "end": "181200"
  },
  {
    "text": "I have a showcase for you which combines Prometheus the chronic sin gesture which is needed for the Prometheus chronics",
    "start": "181200",
    "end": "189690"
  },
  {
    "text": "and the graph on a dashboard which we heard and the talk before so that's a",
    "start": "189690",
    "end": "198570"
  },
  {
    "text": "high level architecture view of chronics and I start at the top at the bottom",
    "start": "198570",
    "end": "205910"
  },
  {
    "text": "that's how that data gets into chronics and we have you can for example collect",
    "start": "205910",
    "end": "212220"
  },
  {
    "text": "data from logstash or from fluent e or from collecti or using the ingestion bridge the ingestion",
    "start": "212220",
    "end": "219269"
  },
  {
    "text": "bridge is if you have a software which generates for example kairos DB metrics",
    "start": "219269",
    "end": "226530"
  },
  {
    "text": "you can point that des oft where to the ingestion bridge endpoint and that will",
    "start": "226530",
    "end": "232160"
  },
  {
    "text": "make the translation between Kairos TV formats into chronics format so i can",
    "start": "232160",
    "end": "238290"
  },
  {
    "text": "just point it anywhere it's also supports open tiers to be in flux to be Cairo's graphite or the native",
    "start": "238290",
    "end": "245690"
  },
  {
    "text": "Prometheus format that's the Prometheus text format not the proto buffers one",
    "start": "245690",
    "end": "252440"
  },
  {
    "text": "then the chronic score it consists of three systems they are not panel",
    "start": "254230",
    "end": "262280"
  },
  {
    "text": "together you can use them at their own for example chronic storage it's based",
    "start": "262280",
    "end": "267470"
  },
  {
    "text": "on leucine leucine is the job of framework for indexing text data something like that the chronic storage",
    "start": "267470",
    "end": "276110"
  },
  {
    "text": "for example if you don't need a big cluster server system but want to run it",
    "start": "276110",
    "end": "281840"
  },
  {
    "text": "embed it you can use the chronic storage part the chronic server part is based on",
    "start": "281840",
    "end": "288280"
  },
  {
    "text": "Apache Solr which is a no SQL data store",
    "start": "288280",
    "end": "295210"
  },
  {
    "text": "which which you can scale on multiple clusters high availability fellow but",
    "start": "295210",
    "end": "300950"
  },
  {
    "text": "something that the chronic server Intelli uses the chronic storage format but also utilizes solar to store the",
    "start": "300950",
    "end": "308480"
  },
  {
    "text": "data and on top of that if you have really if you have many data you can",
    "start": "308480",
    "end": "316100"
  },
  {
    "text": "also use the SPARC integration SPARC is to massively multi analysis of of",
    "start": "316100",
    "end": "326960"
  },
  {
    "text": "tunneling solar and on top of that you can use Apache Zeppelin for example with",
    "start": "326960",
    "end": "333800"
  },
  {
    "text": "in combination with spark or you can use the chronic analytics suit which is ongoing work or you can use the graph on",
    "start": "333800",
    "end": "341600"
  },
  {
    "text": "a dashboard and today I'm going to show you the ingestion bridge how to get data",
    "start": "341600",
    "end": "347060"
  },
  {
    "text": "from Prometheus into chronics we also take a look at the chronic server part and we take a look at the graph owner in",
    "start": "347060",
    "end": "353570"
  },
  {
    "text": "the showcase so I talked about a time",
    "start": "353570",
    "end": "359450"
  },
  {
    "start": "355000",
    "end": "781000"
  },
  {
    "text": "series database what's the time series database my first definition is a sample",
    "start": "359450",
    "end": "368230"
  },
  {
    "text": "and a sample consists of a timestamp and the well you the value could be any kind",
    "start": "368230",
    "end": "375020"
  },
  {
    "text": "of object it's it's not just a double or something like that it can everything",
    "start": "375020",
    "end": "381229"
  },
  {
    "text": "my second definition is what is a time series and a time series is an arbitrary",
    "start": "381229",
    "end": "386759"
  },
  {
    "text": "list of chronological ordered samples so the time series consists of multiple of",
    "start": "386759",
    "end": "392550"
  },
  {
    "text": "those short definition is what's a chunk and the chunk is just a part of a time",
    "start": "392550",
    "end": "400529"
  },
  {
    "text": "series so you take that split it and then you get multiple chunks and my last",
    "start": "400529",
    "end": "407580"
  },
  {
    "text": "definition Flipboard is a little bit in a way a time series database what's the",
    "start": "407580",
    "end": "413279"
  },
  {
    "text": "time series data base it's a specialized database which is which is specialized",
    "start": "413279",
    "end": "421740"
  },
  {
    "text": "for storing and retrieving time series and efficient and optimized - oh you can take your time series and put it in go",
    "start": "421740",
    "end": "428370"
  },
  {
    "text": "to your time series database",
    "start": "428370",
    "end": "431960"
  },
  {
    "text": "that's a high-level overview of the chronics architecture it consists of",
    "start": "435860",
    "end": "440930"
  },
  {
    "text": "multiple steps the first step which is optional is symmetric transformation",
    "start": "440930",
    "end": "447760"
  },
  {
    "text": "there is 68 billion data points and for example you can use the semantic",
    "start": "447760",
    "end": "453950"
  },
  {
    "text": "transformation to downsample that if you don't need the resolution you can you can shrink it or you can enrich the data",
    "start": "453950",
    "end": "460310"
  },
  {
    "text": "with from additional data sources the",
    "start": "460310",
    "end": "466880"
  },
  {
    "text": "next step is it creates chunks from that 68 billion points because to store 68",
    "start": "466880",
    "end": "473900"
  },
  {
    "text": "building points in a NOAA scale data base isn't such a great idea if you get 68 billion documents that's too much for",
    "start": "473900",
    "end": "480980"
  },
  {
    "text": "them to handle so you create chunks for example we created 1 million chunks",
    "start": "480980",
    "end": "486470"
  },
  {
    "text": "which all consists of 68,000 points and the chunks have attributes for example",
    "start": "486470",
    "end": "493600"
  },
  {
    "text": "from which house they came which metrics taste or something like that and the",
    "start": "493600",
    "end": "500300"
  },
  {
    "text": "third step it applies basic compression so we take the chunk and we just compress it and because it's timestamp",
    "start": "500300",
    "end": "509030"
  },
  {
    "text": "value pairs it achieves a very high compression rate in that case 69 percent",
    "start": "509030",
    "end": "515450"
  },
  {
    "text": "compression we see some details later and then you take a multi-dimensional storage and store them in our example if",
    "start": "515450",
    "end": "524630"
  },
  {
    "text": "you use the chronic server it uses solar to store them",
    "start": "524630",
    "end": "530680"
  },
  {
    "text": "how's the record look like here you have a your compressed chunk a chunk consists",
    "start": "533760",
    "end": "541120"
  },
  {
    "text": "of a time series which is a timestamp and a value and that case in America",
    "start": "541120",
    "end": "547149"
  },
  {
    "text": "come later that Fidel or you can also store traces for example a time stamp",
    "start": "547149",
    "end": "553120"
  },
  {
    "text": "and in an exception or you can store locks which is the time stamp and your",
    "start": "553120",
    "end": "559329"
  },
  {
    "text": "lock data or any data you want chronics isn't interested in what your store in",
    "start": "559329",
    "end": "565779"
  },
  {
    "text": "it then you have some technical fields and you'll need them for solo for",
    "start": "565779",
    "end": "571899"
  },
  {
    "text": "example a document ID and a version for optimistic locking that's a start and an",
    "start": "571899",
    "end": "578380"
  },
  {
    "text": "end because you store chance in it that's the start point of the first time",
    "start": "578380",
    "end": "585040"
  },
  {
    "text": "series point in the chunk and that's the one of the last and then you have optional attributes for example from",
    "start": "585040",
    "end": "591339"
  },
  {
    "text": "which hosted data came from rich process the data came to which group it belongs which metrics is stored and what's the",
    "start": "591339",
    "end": "598930"
  },
  {
    "text": "max value it's the pre computed max value of that chunk you can add actually",
    "start": "598930",
    "end": "608459"
  },
  {
    "text": "arbitrary attributes crank doesn't matter the attributes are all annexed",
    "start": "608459",
    "end": "613750"
  },
  {
    "text": "through solar and you can search for them and like I said the max is a pre",
    "start": "613750",
    "end": "619870"
  },
  {
    "text": "calculated volume now only storing is a bit boring so",
    "start": "619870",
    "end": "628509"
  },
  {
    "text": "chronics also useless aggregations for example you can cure it for the min",
    "start": "628509",
    "end": "634759"
  },
  {
    "text": "value of the metrics for the max value of the metrics we have an average you can sum them you can count them you can",
    "start": "634759",
    "end": "640370"
  },
  {
    "text": "calculate percentiles and the deviations first of all you the last value or",
    "start": "640370",
    "end": "645490"
  },
  {
    "text": "scientific and then it also supports transformations which is for example the",
    "start": "645490",
    "end": "652850"
  },
  {
    "text": "bottom five values or the top five values of dead metrics you can calculate moving averages for about in a five",
    "start": "652850",
    "end": "660620"
  },
  {
    "text": "minute window for example you can scale them up scale them down you can also",
    "start": "660620",
    "end": "666920"
  },
  {
    "text": "don't down sample them I said here you",
    "start": "666920",
    "end": "673279"
  },
  {
    "text": "can you can down sample them before you put them into the data store you can downside with them afterwards it also",
    "start": "673279",
    "end": "682850"
  },
  {
    "text": "supports supports analysis which is that trend analysis using a linear regression model to determine if your for example",
    "start": "682850",
    "end": "691790"
  },
  {
    "text": "if you're using or if you're plotting your memory usage if the memory usage is",
    "start": "691790",
    "end": "698689"
  },
  {
    "text": "is increasing over time that is your trend analysis or you can find an",
    "start": "698689",
    "end": "704180"
  },
  {
    "text": "outlier analysis for example to find",
    "start": "704180",
    "end": "709930"
  },
  {
    "text": "time windows where the memory consumption is much higher than other",
    "start": "709930",
    "end": "717050"
  },
  {
    "text": "times you can also use a frequency analysis which is very cool for example",
    "start": "717050",
    "end": "722509"
  },
  {
    "text": "if you're measuring your exception rate and it's like one exception in every",
    "start": "722509",
    "end": "728839"
  },
  {
    "text": "five minutes and to use the frequency analysis on that matrix and then you have in your five-minute window",
    "start": "728839",
    "end": "735380"
  },
  {
    "text": "100 exceptions it will get flagged so you can see the time windows where where",
    "start": "735380",
    "end": "741889"
  },
  {
    "text": "you have more exceptions on your production systems you can you also use fast dynamic time warping warping which",
    "start": "741889",
    "end": "749029"
  },
  {
    "text": "is used to compare time series if you have time series from two different",
    "start": "749029",
    "end": "755600"
  },
  {
    "text": "services since we want to see if they are in identical if they are behaving the same",
    "start": "755600",
    "end": "760640"
  },
  {
    "text": "you can use that method or you can use two symbolic I could get approximation",
    "start": "760640",
    "end": "766400"
  },
  {
    "text": "which is the if you search for it it it's also called",
    "start": "766400",
    "end": "771620"
  },
  {
    "text": "si X that's used for a similarity and pattern search so I said you can store",
    "start": "771620",
    "end": "784220"
  },
  {
    "text": "anything in chronics how does that work here's the Java interface for the time",
    "start": "784220",
    "end": "790400"
  },
  {
    "text": "series converter and here you get its it's a generic over T and here we have a",
    "start": "790400",
    "end": "797990"
  },
  {
    "text": "time series chunk which is the generic T and it just it has to create a binary",
    "start": "797990",
    "end": "803450"
  },
  {
    "text": "time series the binary time series is that part which is stored into junk and that's the other way around you get the",
    "start": "803450",
    "end": "810650"
  },
  {
    "text": "binary time series you get a query start and a query end that that's what the user wants to start an end because it's",
    "start": "810650",
    "end": "818510"
  },
  {
    "text": "junk and then you get your generic back so you can store anything in chronics chronics by default because that's the",
    "start": "818510",
    "end": "826070"
  },
  {
    "text": "main use case for us has support for numeric values it's just here over",
    "start": "826070",
    "end": "831770"
  },
  {
    "text": "orbital so you can put there in a chunk of doubles or a list of doubles and it",
    "start": "831770",
    "end": "836780"
  },
  {
    "text": "gives you the binary representation but there is more to come",
    "start": "836780",
    "end": "841720"
  },
  {
    "text": "okay how to run that stuff",
    "start": "844870",
    "end": "851490"
  },
  {
    "text": "here's your computer you need a Java 8",
    "start": "851490",
    "end": "856500"
  },
  {
    "text": "you don't load the chronics distribution the chronics distribution includes a solar and the solar entirely uses",
    "start": "856500",
    "end": "865810"
  },
  {
    "text": "leucine and chronics is implemented a solar plugins so you have a crew if you",
    "start": "865810",
    "end": "870820"
  },
  {
    "text": "know solar you have a Curie Handler you have the injection handler which translates the data from those databases",
    "start": "870820",
    "end": "877060"
  },
  {
    "text": "into chronics you have a compaction handler which is used to compact data into chunks more on that later and you",
    "start": "877060",
    "end": "884740"
  },
  {
    "text": "have to retention handle off there you can specify throw away data after one year of down sample data after one year",
    "start": "884740",
    "end": "892900"
  },
  {
    "text": "and if you want to make queries you have a go client and you have a Java client",
    "start": "892900",
    "end": "899020"
  },
  {
    "text": "and this is the ingestion interface for the other databases to import data into",
    "start": "899020",
    "end": "905440"
  },
  {
    "text": "chronics a little bit cold how to get",
    "start": "905440",
    "end": "912280"
  },
  {
    "text": "data all of chronics so first you just create a solar client and then you wrap",
    "start": "912280",
    "end": "917920"
  },
  {
    "text": "that solar client into a chronics client and that's your converter that's this is",
    "start": "917920",
    "end": "924220"
  },
  {
    "text": "the implementation of the interface I showed you two slides before that converts the binary data into a list of",
    "start": "924220",
    "end": "931060"
  },
  {
    "text": "Tuttle's and then you can trade a solar Curie which says I'm interested in all",
    "start": "931060",
    "end": "938260"
  },
  {
    "text": "metrics which contains load and then you call the stream method and you get a Java eight stream and you can work with",
    "start": "938260",
    "end": "944650"
  },
  {
    "text": "the data or you can also add some aggregations or functions that's all",
    "start": "944650",
    "end": "952450"
  },
  {
    "text": "that stuff here for example we are interested in the max value of the",
    "start": "952450",
    "end": "958870"
  },
  {
    "text": "metrics and the min value of the matrix how many metrics there are and signed difference here it's an example if your",
    "start": "958870",
    "end": "966760"
  },
  {
    "text": "first metric values 20 and your last metric value is minus 80 you will get minus 80 so it falls over time and then",
    "start": "966760",
    "end": "975310"
  },
  {
    "text": "you also call the stream method and you get a Java 8 stream yes",
    "start": "975310",
    "end": "981670"
  },
  {
    "text": "yes",
    "start": "990720",
    "end": "993350"
  },
  {
    "text": "if you run solar clustered then you have a solar master note and that is where the aggregations were are aggregated",
    "start": "1014350",
    "end": "1023790"
  },
  {
    "text": "yeah but you can also use if you have many data you can also use spark which",
    "start": "1025200",
    "end": "1034290"
  },
  {
    "text": "distributes that whole over your solar clusters in the case of percentile is a vault word but in Maxwell uses visible",
    "start": "1034290",
    "end": "1041170"
  },
  {
    "text": "work",
    "start": "1041170",
    "end": "1043500"
  },
  {
    "start": "1047000",
    "end": "1193000"
  },
  {
    "text": "so there are many time series databases why should you use chronics chronic Sisk",
    "start": "1047930",
    "end": "1059420"
  },
  {
    "text": "created an in cooperation with a German university if you're interested in the",
    "start": "1059420",
    "end": "1065060"
  },
  {
    "text": "art fact that's from their paper yeah",
    "start": "1065060",
    "end": "1070310"
  },
  {
    "text": "if you're interested chunk just come to me after the talk",
    "start": "1070310",
    "end": "1076390"
  },
  {
    "text": "we are evaluated chronics against influx to be open TCP and Kairos to be and",
    "start": "1076900",
    "end": "1082540"
  },
  {
    "text": "that's important one all databases are configured as a single null database they are not running cluster they're",
    "start": "1082540",
    "end": "1087950"
  },
  {
    "text": "running on one machine yeah",
    "start": "1087950",
    "end": "1091779"
  },
  {
    "text": "I think so yeah yeah the databases all support our",
    "start": "1098210",
    "end": "1105330"
  },
  {
    "text": "clustering but they are configured to run as a single node if you take at the",
    "start": "1105330",
    "end": "1115290"
  },
  {
    "text": "query times and the paper a Curie mix test if is was defined about usual",
    "start": "1115290",
    "end": "1126180"
  },
  {
    "text": "curious which are run against time series databases and in this query mix Cronus's and the bottom tier 70 is three",
    "start": "1126180",
    "end": "1134040"
  },
  {
    "text": "percent to a ninety time 92 percent faster on data retrieval and it's in the",
    "start": "1134040",
    "end": "1142320"
  },
  {
    "text": "query mixed case 82 97 percent faster if you want to query data out of chronics",
    "start": "1142320",
    "end": "1147830"
  },
  {
    "text": "if you take the storage demand for you--even 108 gigabyte rossi use full",
    "start": "1147830",
    "end": "1153600"
  },
  {
    "text": "file and you want to import it in the database and in chronics it uses 8.7",
    "start": "1153600",
    "end": "1159570"
  },
  {
    "text": "gigabytes space on disk and that is compared to the other databases 20 time",
    "start": "1159570",
    "end": "1164880"
  },
  {
    "text": "to a 20% to 84 percent less than your databases and if you're concerned about",
    "start": "1164880",
    "end": "1172070"
  },
  {
    "text": "main memory chronics takes one to one",
    "start": "1172070",
    "end": "1177300"
  },
  {
    "text": "six time less memory than the best alternative these are the hard facts all",
    "start": "1177300",
    "end": "1184440"
  },
  {
    "text": "in that research paper if you're interested in that paper just come to me",
    "start": "1184440",
    "end": "1189570"
  },
  {
    "text": "afterwards",
    "start": "1189570",
    "end": "1192019"
  },
  {
    "start": "1193000",
    "end": "1618000"
  },
  {
    "text": "okay now I've talked about chronics what about Prometheus so to get it work",
    "start": "1195070",
    "end": "1204330"
  },
  {
    "text": "Prometheus it collects the metrics from your service it writes them to the",
    "start": "1204330",
    "end": "1209380"
  },
  {
    "text": "default storage and it also writes them in our configuration through the",
    "start": "1209380",
    "end": "1214570"
  },
  {
    "text": "standard remote write interface to electronics and Jasta the chronics and gesture which sits in the middle between",
    "start": "1214570",
    "end": "1219730"
  },
  {
    "text": "chronics and your Prometheus and it's used to collect the samples from the",
    "start": "1219730",
    "end": "1225220"
  },
  {
    "text": "Prometheus and write them into Prometheus in batch mode it does that",
    "start": "1225220",
    "end": "1233170"
  },
  {
    "text": "because chronics isn't so good if you feed it one sample at a time it prefers",
    "start": "1233170",
    "end": "1239320"
  },
  {
    "text": "to get them batch and that's what the chronics ingest the does it also writes checkpoints to the disk before a chunk",
    "start": "1239320",
    "end": "1246250"
  },
  {
    "text": "is full to prevent if that is crashing the you lose your metrics",
    "start": "1246250",
    "end": "1254130"
  },
  {
    "text": "now some scenarios this is running Prometheus using chronics on a single",
    "start": "1258000",
    "end": "1265560"
  },
  {
    "text": "host use the Prometheus to get your metrics from your systems it writes them",
    "start": "1265560",
    "end": "1273000"
  },
  {
    "text": "it's right to samples using your remote right in the face in the chronics and gesture this some batching and writes",
    "start": "1273000",
    "end": "1279480"
  },
  {
    "text": "the rights the batches into the chronic server afterwards you can query the",
    "start": "1279480",
    "end": "1284850"
  },
  {
    "text": "chronic server for data here you have if you have multiple Prometheus you just",
    "start": "1284850",
    "end": "1293010"
  },
  {
    "text": "configure multiple chronics and just as they are all do the batching and they pipe into one chronic server you can",
    "start": "1293010",
    "end": "1301050"
  },
  {
    "text": "also do that you have Prometheus another host running you use the chronics and gesture and the chronic server on a",
    "start": "1301050",
    "end": "1306570"
  },
  {
    "text": "single node you just point them into one chronic sin gesture the stuff the batching and writes it to electronics",
    "start": "1306570",
    "end": "1313130"
  },
  {
    "text": "yes in that case chronics is a single node",
    "start": "1313130",
    "end": "1320370"
  },
  {
    "text": "but we come to the full cloud mode where you Charlotte you can also use multiple",
    "start": "1320370",
    "end": "1329520"
  },
  {
    "text": "nodes for chronics you can define it and",
    "start": "1329520",
    "end": "1336450"
  },
  {
    "text": "in solar you can shard on the metrics field or you can shard on some ID fields",
    "start": "1336450",
    "end": "1342240"
  },
  {
    "text": "or something like that because that's based on solar and we we all get the the",
    "start": "1342240",
    "end": "1348720"
  },
  {
    "text": "solar charging features for free",
    "start": "1348720",
    "end": "1352190"
  },
  {
    "text": "here you can see a lopa Lansing example you have multiple Prometheus you all point them to an engineer slope will",
    "start": "1357060",
    "end": "1362460"
  },
  {
    "text": "answer in balances them over to chronics and justice they do the batching and",
    "start": "1362460",
    "end": "1367890"
  },
  {
    "text": "they write it through the chronics server or the full cloud mode you use",
    "start": "1367890",
    "end": "1374400"
  },
  {
    "text": "Prometheus type them into nginx load balancer which shot which balance them over multiple chronic system you write",
    "start": "1374400",
    "end": "1382530"
  },
  {
    "text": "the data to a chronic smoker or a solar Master and solar duster Sharleen and",
    "start": "1382530",
    "end": "1388010"
  },
  {
    "text": "distributes them over multiple chronic service so what's the chronic sin jester",
    "start": "1388010",
    "end": "1399110"
  },
  {
    "text": "trans congested is just a small go program and test 700 lines of code",
    "start": "1399110",
    "end": "1405530"
  },
  {
    "text": "it's because it's go program you can just copy and execute it it sits between",
    "start": "1405530",
    "end": "1413400"
  },
  {
    "text": "your Prometheus and your chronic server I'm using a remote write interface and",
    "start": "1413400",
    "end": "1421260"
  },
  {
    "text": "you just configure your host into port and ingest or and Prometheus remote",
    "start": "1421260",
    "end": "1426870"
  },
  {
    "text": "write off the rest the main feature is it batches to samples in memory so here",
    "start": "1426870",
    "end": "1433230"
  },
  {
    "text": "you get samples outcomes batches for the chronic server because chronics needs",
    "start": "1433230",
    "end": "1439800"
  },
  {
    "text": "needs large chunks to work well and you can also configure the max patch age",
    "start": "1439800",
    "end": "1446610"
  },
  {
    "text": "it's question restart with resilience for example your piping samples in there",
    "start": "1446610",
    "end": "1453240"
  },
  {
    "text": "and any crashes and that hasn't read all the batch your data isn't gone because it writes check points to disk and",
    "start": "1453240",
    "end": "1458820"
  },
  {
    "text": "restore some automatically if you restarted",
    "start": "1458820",
    "end": "1462740"
  },
  {
    "text": "so you can see all that work in comes the samples it stores them in an",
    "start": "1465660",
    "end": "1471640"
  },
  {
    "text": "in-progress trunk and if that junk is full and ready to write to chronics it gets written all to chronics and",
    "start": "1471640",
    "end": "1477820"
  },
  {
    "text": "chronics can work with the data it indexes that and that's it so that's the",
    "start": "1477820",
    "end": "1487330"
  },
  {
    "text": "reason why Prometheus and cryonics are really good to work with each other",
    "start": "1487330",
    "end": "1493030"
  },
  {
    "text": "because Prometheus uses so-called labels to store the management values and if",
    "start": "1493030",
    "end": "1498400"
  },
  {
    "text": "you compare it to chronic that uses the attributes which are also key value pairs to store dimension values so that",
    "start": "1498400",
    "end": "1504100"
  },
  {
    "text": "the only difference is is called labels and it's called attributes it's the same feature the the labels are edited",
    "start": "1504100",
    "end": "1511690"
  },
  {
    "text": "dynamically and in chronics a debate is based on solar you can use a schema but",
    "start": "1511690",
    "end": "1520120"
  },
  {
    "text": "you can also use the schema less mode which adds dynamic fields so they are",
    "start": "1520120",
    "end": "1526000"
  },
  {
    "text": "also added dynamically and Prometheus",
    "start": "1526000",
    "end": "1531220"
  },
  {
    "text": "store sample pairs for a pair of a timestamp at the world you and this is",
    "start": "1531220",
    "end": "1536380"
  },
  {
    "text": "also supported in chronics it's puzzles stores a timestamp NW and that's how",
    "start": "1536380",
    "end": "1544980"
  },
  {
    "text": "that's why the chronics and gesture is so simple it just takes the data it matches it writes a dog it has it",
    "start": "1544980",
    "end": "1553240"
  },
  {
    "text": "doesn't have to do some transformation or something like that that's a chronic",
    "start": "1553240",
    "end": "1559000"
  },
  {
    "text": "schema that some solar internals it's not very important here you see the",
    "start": "1559000",
    "end": "1565480"
  },
  {
    "text": "fields for example the ID field the star field the unfilled the data field which",
    "start": "1565480",
    "end": "1570820"
  },
  {
    "text": "is binary and here you see a dynamic",
    "start": "1570820",
    "end": "1575920"
  },
  {
    "text": "field which is used to store the text so if you store the Prometheus label it",
    "start": "1575920",
    "end": "1585040"
  },
  {
    "text": "gets translated into a chronic level for example if you use the premises label host it gets the dynamic fields host or",
    "start": "1585040",
    "end": "1591760"
  },
  {
    "text": "underscore as for a dynamic field and then you can query for that",
    "start": "1591760",
    "end": "1598440"
  },
  {
    "text": "now my showcase its uses Prometheus this writes the samples into electronics and",
    "start": "1599879",
    "end": "1605799"
  },
  {
    "text": "gesture and that writes the batches into the qur'anic server and we put a graph on a dashboard which reads data from",
    "start": "1605799",
    "end": "1613210"
  },
  {
    "text": "Prometheus and which also reads data from the chronic server a few words",
    "start": "1613210",
    "end": "1620619"
  },
  {
    "start": "1618000",
    "end": "1928000"
  },
  {
    "text": "about our performance it's 11 days of data in the slides in my life",
    "start": "1620619",
    "end": "1626950"
  },
  {
    "text": "presentation it's it's more data but here they were 12 mil 112 million",
    "start": "1626950",
    "end": "1633190"
  },
  {
    "text": "samples in Prometheus that's about 786 megabyte on disk and in chronics it's",
    "start": "1633190",
    "end": "1638710"
  },
  {
    "text": "265 megabyte on disk and without compaction I mentioned compaction",
    "start": "1638710",
    "end": "1646330"
  },
  {
    "text": "earlier compaction works by you specify how many points you want per chunk and",
    "start": "1646330",
    "end": "1654779"
  },
  {
    "text": "chronics takes those data points and chunks them and here you can see how",
    "start": "1654779",
    "end": "1661419"
  },
  {
    "text": "that has an effect on your disk usage size that's the default chunking the",
    "start": "1661419",
    "end": "1670629"
  },
  {
    "text": "default means no chunking it just takes the batches as they come in yeah",
    "start": "1670629",
    "end": "1676409"
  },
  {
    "text": "it just likes 100 data points but shrunk and then cuts the chunk and here you can",
    "start": "1694900",
    "end": "1703090"
  },
  {
    "text": "see if you run 100 points per charge you end up with 357 megabytes and if you",
    "start": "1703090",
    "end": "1708340"
  },
  {
    "text": "run 500 thousands points and junk you all knew just get 119 megabytes because",
    "start": "1708340",
    "end": "1714790"
  },
  {
    "text": "the compression is more efficient the caveat is if you store 500,000 data",
    "start": "1714790",
    "end": "1721180"
  },
  {
    "text": "points per chunk your query time gets higher because it has to unpack 500,000",
    "start": "1721180",
    "end": "1729100"
  },
  {
    "text": "data points if you corrode your data",
    "start": "1729100",
    "end": "1732960"
  },
  {
    "text": "yeah",
    "start": "1735120",
    "end": "1738120"
  },
  {
    "text": "No",
    "start": "1740279",
    "end": "1743279"
  },
  {
    "text": "on a technical level it's all stored in a solid document and a solid document is",
    "start": "1745760",
    "end": "1752090"
  },
  {
    "text": "like a row in a relational database and",
    "start": "1752090",
    "end": "1756549"
  },
  {
    "text": "here in the chunk field it determines how many data points of your time series",
    "start": "1758170",
    "end": "1764540"
  },
  {
    "text": "are stored in there if you store one point third document and if you store 68",
    "start": "1764540",
    "end": "1771470"
  },
  {
    "text": "billion points then you end up with 68 billion solar documents which is not",
    "start": "1771470",
    "end": "1776720"
  },
  {
    "text": "very good for scale here but if you use",
    "start": "1776720",
    "end": "1784520"
  },
  {
    "text": "a chunk size of 68,000 points it a creates your your 68 billion points into",
    "start": "1784520",
    "end": "1793010"
  },
  {
    "text": "1 million documents because every document contains 68,000 points and that's much better for your scalability",
    "start": "1793010",
    "end": "1800030"
  },
  {
    "text": "of your solar yeah",
    "start": "1800030",
    "end": "1810730"
  },
  {
    "text": "here I'm not sure that default config",
    "start": "1840790",
    "end": "1846660"
  },
  {
    "text": "okay",
    "start": "1848670",
    "end": "1851670"
  },
  {
    "text": "okay",
    "start": "1853900",
    "end": "1856590"
  },
  {
    "text": "okay your story",
    "start": "1861190",
    "end": "1866460"
  },
  {
    "text": "yeah and it's gzip compressed protocol",
    "start": "1866460",
    "end": "1872480"
  },
  {
    "text": "buffers",
    "start": "1872480",
    "end": "1874840"
  },
  {
    "text": "yeah they here there are many magic it's",
    "start": "1881540",
    "end": "1887410"
  },
  {
    "text": "Delta encoding and something like that",
    "start": "1887410",
    "end": "1892300"
  },
  {
    "text": "no I'm not sure no no no they are not at the moment there are not options they're",
    "start": "1897280",
    "end": "1903170"
  },
  {
    "text": "hard-coded in the software in the research paper I showed earlier they are",
    "start": "1903170",
    "end": "1909759"
  },
  {
    "text": "they run multiple benchmarks with different compression algorithms different compression settings and they",
    "start": "1909790",
    "end": "1917030"
  },
  {
    "text": "end up with gzip and the block size of I don't know you have to read the paper but they are hard-coded at the moment so",
    "start": "1917030",
    "end": "1930890"
  },
  {
    "start": "1928000",
    "end": "2158000"
  },
  {
    "text": "that the junking and in that case we disabled to Chungking or we disabled to",
    "start": "1930890",
    "end": "1936980"
  },
  {
    "text": "compaction of chunking and that's a screenshot from our software kaki tool",
    "start": "1936980",
    "end": "1944240"
  },
  {
    "text": "which I showed earlier and here you can see the red lines are the CPU usage for",
    "start": "1944240",
    "end": "1951110"
  },
  {
    "text": "the chronic Sanjay sir the green lines are for Prometheus itself the CPU usage",
    "start": "1951110",
    "end": "1959060"
  },
  {
    "text": "it's max at 400% because we have for cost and the blue one is the chronic",
    "start": "1959060",
    "end": "1967940"
  },
  {
    "text": "server that's the memory consumption of",
    "start": "1967940",
    "end": "1973910"
  },
  {
    "text": "our chronic sin gesture it tops as about 120 megabytes",
    "start": "1973910",
    "end": "1979870"
  },
  {
    "text": "so I can show it in a live demo um here I have my graph Anna I have a chronic",
    "start": "1982789",
    "end": "1988879"
  },
  {
    "text": "status source and the Prometheus data source and here is the graph on a",
    "start": "1988879",
    "end": "1994729"
  },
  {
    "text": "dashboard for our in Chester and the in jester also publishes Prometheus metrics",
    "start": "1994729",
    "end": "2000789"
  },
  {
    "text": "so you can scrape your ingest up with Prometheus itself here's our",
    "start": "2000789",
    "end": "2006789"
  },
  {
    "text": "configuration okay",
    "start": "2006789",
    "end": "2012778"
  },
  {
    "text": "that's the important part it's the remote right which points to our chronic sin gesture and it scraped some data the",
    "start": "2015720",
    "end": "2024899"
  },
  {
    "text": "localhost there locally running from UTS and some demo Prometheus's that's the",
    "start": "2024899",
    "end": "2034009"
  },
  {
    "text": "default solar Korean face and if you run a query against it we can see it found 1",
    "start": "2034009",
    "end": "2041549"
  },
  {
    "text": "million and something solar documents and here you can see the fields I",
    "start": "2041549",
    "end": "2047460"
  },
  {
    "text": "mentioned earlier for example the ID the start date the end date and here is for",
    "start": "2047460",
    "end": "2052858"
  },
  {
    "text": "example a Prometheus tag called tender and it's it is stored in the handler",
    "start": "2052859",
    "end": "2058108"
  },
  {
    "text": "underscore s a field of that solar document and here is the compressed",
    "start": "2058109",
    "end": "2063440"
  },
  {
    "text": "chunk",
    "start": "2063440",
    "end": "2066440"
  },
  {
    "text": "so this is our Prometheus it's running",
    "start": "2070780",
    "end": "2077220"
  },
  {
    "text": "showing the memory chunks of our hand gesture and now the interesting thing I",
    "start": "2077220",
    "end": "2083529"
  },
  {
    "text": "have a dashboard which is called pro-mix it combines the lower data is coming",
    "start": "2083529",
    "end": "2090489"
  },
  {
    "text": "from Prometheus and the upper data is coming from chronic sit that's this exact same matrix also that is coming",
    "start": "2090489",
    "end": "2097180"
  },
  {
    "text": "from the Prometheus data store and that is the data stored in chronics and here we can see that's about the 14 day mark",
    "start": "2097180",
    "end": "2104609"
  },
  {
    "text": "Prometheus throw the data away and in chronic state it's also stored and also",
    "start": "2104609",
    "end": "2111940"
  },
  {
    "text": "interesting is where are the spikes there are some spikes and I assume",
    "start": "2111940",
    "end": "2117880"
  },
  {
    "text": "that's because Prometheus does some down sampling if you are not zoomed in if you",
    "start": "2117880",
    "end": "2124180"
  },
  {
    "text": "zoom in the spikes appear and the graphs look identical",
    "start": "2124180",
    "end": "2131430"
  },
  {
    "text": "and that's it crunches Oprah sauce you can find on chronic style and it's",
    "start": "2141560",
    "end": "2148580"
  },
  {
    "text": "hosted on github if you have questions to us just ask them here or Twitter us",
    "start": "2148580",
    "end": "2155990"
  },
  {
    "text": "and we also have a slight channel and that's it thank you very much",
    "start": "2155990",
    "end": "2161300"
  },
  {
    "text": "what any questions yeah yeah yeah right",
    "start": "2161300",
    "end": "2184300"
  },
  {
    "text": "yeah you need to use the solar query language right",
    "start": "2184300",
    "end": "2196960"
  },
  {
    "text": "I'm not aware of that but I definitely remember that okay",
    "start": "2212300",
    "end": "2220040"
  },
  {
    "text": "you mean here",
    "start": "2239210",
    "end": "2242660"
  },
  {
    "text": "and you could configure that this in gesture is also is always writing in",
    "start": "2251130",
    "end": "2256529"
  },
  {
    "text": "that chronic server but not based on the metric so all metrics from that and",
    "start": "2256529",
    "end": "2262890"
  },
  {
    "text": "gesture will get in that chronic server if you point it directly to that chronic server if you point it to the master",
    "start": "2262890",
    "end": "2268680"
  },
  {
    "text": "node it shards on your folding policy for example the ID field or the metrics",
    "start": "2268680",
    "end": "2274589"
  },
  {
    "text": "field or something that",
    "start": "2274589",
    "end": "2277460"
  },
  {
    "text": "if you're Cleary chronics afterwards it does the ordering for you it doesn't",
    "start": "2283020",
    "end": "2289950"
  },
  {
    "text": "matter in which in which ordering you store the data you'll always get it's in",
    "start": "2289950",
    "end": "2298140"
  },
  {
    "text": "chronic chronological ordering if you curry it afterwards that's that yeah",
    "start": "2298140",
    "end": "2304710"
  },
  {
    "text": "okay",
    "start": "2304710",
    "end": "2307040"
  },
  {
    "text": "back yeah yeah and that's one big use case",
    "start": "2311650",
    "end": "2319760"
  },
  {
    "text": "because we are not getting these metrics with Prometheus but with an self-written",
    "start": "2319760",
    "end": "2327770"
  },
  {
    "text": "collector which rights use file files and we import them this year's file files into our chronic so that's all",
    "start": "2327770",
    "end": "2334730"
  },
  {
    "text": "data from the past and chronics sort salt ordering and then you get it in the",
    "start": "2334730",
    "end": "2340160"
  },
  {
    "text": "right chronological order",
    "start": "2340160",
    "end": "2343299"
  },
  {
    "text": "I'm I'm not sure okay",
    "start": "2360620",
    "end": "2364690"
  },
  {
    "text": "yeah it's",
    "start": "2406730",
    "end": "2414700"
  },
  {
    "text": "yeah okay I hear ya",
    "start": "2418370",
    "end": "2423700"
  },
  {
    "text": "you know at the moment it's it's done it just uses the start and the end fields",
    "start": "2436650",
    "end": "2443320"
  },
  {
    "text": "to drop data before some days it's not aware of charts or something like that",
    "start": "2443320",
    "end": "2449040"
  },
  {
    "text": "no they are not currently implemented",
    "start": "2450210",
    "end": "2454710"
  },
  {
    "text": "yeah",
    "start": "2467960",
    "end": "2470530"
  },
  {
    "text": "with something",
    "start": "2489880",
    "end": "2493238"
  },
  {
    "text": "that or that one this one",
    "start": "2531180",
    "end": "2537800"
  },
  {
    "text": "okay thank you",
    "start": "2559280",
    "end": "2563020"
  }
]