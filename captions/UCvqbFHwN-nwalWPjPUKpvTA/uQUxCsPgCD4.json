[
  {
    "text": "all right so I think we can start and we",
    "start": "560",
    "end": "2639"
  },
  {
    "text": "give ourselves a couple of minutes so",
    "start": "2639",
    "end": "5120"
  },
  {
    "text": "next we have um shrand and he'll be",
    "start": "5120",
    "end": "8679"
  },
  {
    "text": "talking to us about multiplayer machine",
    "start": "8679",
    "end": "11320"
  },
  {
    "text": "learning with metaflow open AI whisper",
    "start": "11320",
    "end": "14040"
  },
  {
    "text": "and",
    "start": "14040",
    "end": "15160"
  },
  {
    "text": "kubernetes thank you thank you hi",
    "start": "15160",
    "end": "17520"
  },
  {
    "text": "everyone my name is Shri Zer I am an",
    "start": "17520",
    "end": "21080"
  },
  {
    "text": "engineer at outabounds outabounds is one",
    "start": "21080",
    "end": "23039"
  },
  {
    "text": "of the main companies behind the open-",
    "start": "23039",
    "end": "25039"
  },
  {
    "text": "source metaflow project today we are",
    "start": "25039",
    "end": "27160"
  },
  {
    "text": "going to talk about multiplayer machine",
    "start": "27160",
    "end": "29039"
  },
  {
    "text": "learning essentially with uh metaflow",
    "start": "29039",
    "end": "31759"
  },
  {
    "text": "open Ai and kubernetes I know lots of",
    "start": "31759",
    "end": "34680"
  },
  {
    "text": "jargon lots of terms we'll try and",
    "start": "34680",
    "end": "36879"
  },
  {
    "text": "justify the existence of each of",
    "start": "36879",
    "end": "39160"
  },
  {
    "text": "them so let's start with metaflow so",
    "start": "39160",
    "end": "42200"
  },
  {
    "text": "what is metaflow so metaflow is an",
    "start": "42200",
    "end": "44000"
  },
  {
    "text": "open-source project um metaflow makes it",
    "start": "44000",
    "end": "46760"
  },
  {
    "text": "easy to access resources needed for any",
    "start": "46760",
    "end": "49199"
  },
  {
    "text": "data science and data intensive",
    "start": "49199",
    "end": "51039"
  },
  {
    "text": "application so typically you would",
    "start": "51039",
    "end": "52440"
  },
  {
    "text": "require compute workflows data and",
    "start": "52440",
    "end": "54840"
  },
  {
    "text": "versioning so for doing any of these if",
    "start": "54840",
    "end": "57039"
  },
  {
    "text": "you want to use a python based API",
    "start": "57039",
    "end": "58960"
  },
  {
    "text": "metaflow is one of the the projects you",
    "start": "58960",
    "end": "60440"
  },
  {
    "text": "can use metaflow began as an open source",
    "start": "60440",
    "end": "62559"
  },
  {
    "text": "project at Netflix it's still actually",
    "start": "62559",
    "end": "64320"
  },
  {
    "text": "uh open sourced under the Netflix group",
    "start": "64320",
    "end": "66400"
  },
  {
    "text": "of projects you can go to github.com",
    "start": "66400",
    "end": "67960"
  },
  {
    "text": "Netflix metaflow to learn more about",
    "start": "67960",
    "end": "71080"
  },
  {
    "text": "metaflow so metaflow makes it very",
    "start": "71080",
    "end": "73200"
  },
  {
    "text": "simple to create python based data",
    "start": "73200",
    "end": "75240"
  },
  {
    "text": "science projects python is one of the",
    "start": "75240",
    "end": "77080"
  },
  {
    "text": "most commonly used languages for data",
    "start": "77080",
    "end": "78560"
  },
  {
    "text": "science here I have a quick example of",
    "start": "78560",
    "end": "80400"
  },
  {
    "text": "what a metaflow flow looks like so on",
    "start": "80400",
    "end": "83040"
  },
  {
    "text": "the left hand side if you see kind of",
    "start": "83040",
    "end": "84360"
  },
  {
    "text": "sort of the the terminal based black and",
    "start": "84360",
    "end": "85880"
  },
  {
    "text": "white screen um is a simple example of",
    "start": "85880",
    "end": "88280"
  },
  {
    "text": "what a flow looks like you have a three",
    "start": "88280",
    "end": "90079"
  },
  {
    "text": "step flow where someone creates a hello",
    "start": "90079",
    "end": "92920"
  },
  {
    "text": "flow or any class which actually uh",
    "start": "92920",
    "end": "95399"
  },
  {
    "text": "implements this flow spec that meta flow",
    "start": "95399",
    "end": "97560"
  },
  {
    "text": "has and there are three steps the start",
    "start": "97560",
    "end": "99479"
  },
  {
    "text": "step as you can tell is the first step",
    "start": "99479",
    "end": "101040"
  },
  {
    "text": "that begins the flow this one says the",
    "start": "101040",
    "end": "103600"
  },
  {
    "text": "next step is the work step so the work",
    "start": "103600",
    "end": "105680"
  },
  {
    "text": "step that's called Next and then work",
    "start": "105680",
    "end": "107360"
  },
  {
    "text": "says the next one to do is end and then",
    "start": "107360",
    "end": "108799"
  },
  {
    "text": "end actually completes the flow you can",
    "start": "108799",
    "end": "110680"
  },
  {
    "text": "do sequential or parallel execution of",
    "start": "110680",
    "end": "112600"
  },
  {
    "text": "these steps you can have data passed",
    "start": "112600",
    "end": "114040"
  },
  {
    "text": "from one step to another or can have",
    "start": "114040",
    "end": "115439"
  },
  {
    "text": "data passed from one step to multiple",
    "start": "115439",
    "end": "117320"
  },
  {
    "text": "steps that run in parallel again take a",
    "start": "117320",
    "end": "118880"
  },
  {
    "text": "look at the open source document",
    "start": "118880",
    "end": "119799"
  },
  {
    "text": "mentation for more information about",
    "start": "119799",
    "end": "121280"
  },
  {
    "text": "this but the best part about metaflow",
    "start": "121280",
    "end": "123399"
  },
  {
    "text": "actually is that you can run these steps",
    "start": "123399",
    "end": "125680"
  },
  {
    "text": "locally as well as remotely so as you go",
    "start": "125680",
    "end": "128160"
  },
  {
    "text": "through the process of actually writing",
    "start": "128160",
    "end": "129599"
  },
  {
    "text": "the code and actually building the flows",
    "start": "129599",
    "end": "132160"
  },
  {
    "text": "or building your data science code you",
    "start": "132160",
    "end": "134040"
  },
  {
    "text": "can keep running this thing locally",
    "start": "134040",
    "end": "135440"
  },
  {
    "text": "using something like hello.py run so you",
    "start": "135440",
    "end": "137760"
  },
  {
    "text": "keep running this locally once you think",
    "start": "137760",
    "end": "139760"
  },
  {
    "text": "you're ready to actually run this at",
    "start": "139760",
    "end": "141280"
  },
  {
    "text": "scale on a bigger back end like",
    "start": "141280",
    "end": "142720"
  },
  {
    "text": "kubernetes where you have access to more",
    "start": "142720",
    "end": "144840"
  },
  {
    "text": "memory more CPU or maybe more U or GPU",
    "start": "144840",
    "end": "147599"
  },
  {
    "text": "resources or maybe access to data that",
    "start": "147599",
    "end": "149920"
  },
  {
    "text": "is only available in kubernetes you can",
    "start": "149920",
    "end": "151920"
  },
  {
    "text": "actually just run this with something",
    "start": "151920",
    "end": "153360"
  },
  {
    "text": "called like hello hello.py run with",
    "start": "153360",
    "end": "155640"
  },
  {
    "text": "kubernetes and then if you want to",
    "start": "155640",
    "end": "157480"
  },
  {
    "text": "actually deploy them and have these run",
    "start": "157480",
    "end": "159319"
  },
  {
    "text": "at a particular Cadence or run it based",
    "start": "159319",
    "end": "161800"
  },
  {
    "text": "on events you can use Argo workflows for",
    "start": "161800",
    "end": "163720"
  },
  {
    "text": "doing this and you know those are the",
    "start": "163720",
    "end": "165040"
  },
  {
    "text": "two commands again all of this is",
    "start": "165040",
    "end": "166519"
  },
  {
    "text": "available in the",
    "start": "166519",
    "end": "168080"
  },
  {
    "text": "documentation okay so then let's move to",
    "start": "168080",
    "end": "170239"
  },
  {
    "text": "the next part of the of the the multiple",
    "start": "170239",
    "end": "172280"
  },
  {
    "text": "jargons that we had open a whisper",
    "start": "172280",
    "end": "174920"
  },
  {
    "text": "openai whisper is actually a machine",
    "start": "174920",
    "end": "176519"
  },
  {
    "text": "learning model created by openai that",
    "start": "176519",
    "end": "178360"
  },
  {
    "text": "does speech to text translation so the",
    "start": "178360",
    "end": "181200"
  },
  {
    "text": "simple thing that it does is given an",
    "start": "181200",
    "end": "183080"
  },
  {
    "text": "audio file give me the text output of",
    "start": "183080",
    "end": "186000"
  },
  {
    "text": "that audio so that's what it does it",
    "start": "186000",
    "end": "188159"
  },
  {
    "text": "supports multiple languages and another",
    "start": "188159",
    "end": "190440"
  },
  {
    "text": "interesting facet of openi whisper is",
    "start": "190440",
    "end": "192400"
  },
  {
    "text": "that this model or the model weights",
    "start": "192400",
    "end": "194000"
  },
  {
    "text": "rather are available in multiple sizes",
    "start": "194000",
    "end": "196360"
  },
  {
    "text": "they have t-shirt sizing so you",
    "start": "196360",
    "end": "198280"
  },
  {
    "text": "basically pick as a user you have to",
    "start": "198280",
    "end": "200200"
  },
  {
    "text": "pick whether you use the tiny model or",
    "start": "200200",
    "end": "202200"
  },
  {
    "text": "the small model or the large model or",
    "start": "202200",
    "end": "204120"
  },
  {
    "text": "the medium model and that size of the",
    "start": "204120",
    "end": "206760"
  },
  {
    "text": "model decides a few other things so this",
    "start": "206760",
    "end": "209760"
  },
  {
    "text": "size of the model actually decides how",
    "start": "209760",
    "end": "211680"
  },
  {
    "text": "much resources you will need when you",
    "start": "211680",
    "end": "213400"
  },
  {
    "text": "actually use the model for transcription",
    "start": "213400",
    "end": "215400"
  },
  {
    "text": "so if you use the tiny model obviously",
    "start": "215400",
    "end": "216920"
  },
  {
    "text": "you'll need fewer CPU fewer memory if",
    "start": "216920",
    "end": "219319"
  },
  {
    "text": "you use the large model you'll you'll",
    "start": "219319",
    "end": "221200"
  },
  {
    "text": "need maximum amount of CPU and maximum",
    "start": "221200",
    "end": "222959"
  },
  {
    "text": "amount of memory just so that the model",
    "start": "222959",
    "end": "224680"
  },
  {
    "text": "actually fits into memory this actually",
    "start": "224680",
    "end": "226760"
  },
  {
    "text": "decides so the size of the model decides",
    "start": "226760",
    "end": "228280"
  },
  {
    "text": "the resources it decides the accuracy of",
    "start": "228280",
    "end": "230640"
  },
  {
    "text": "the generated transcript so you know how",
    "start": "230640",
    "end": "233480"
  },
  {
    "text": "accurate is the transcript and then of",
    "start": "233480",
    "end": "234879"
  },
  {
    "text": "course it also decides the time required",
    "start": "234879",
    "end": "237360"
  },
  {
    "text": "for actual transcription so again given",
    "start": "237360",
    "end": "239319"
  },
  {
    "text": "a file like how long does it take to",
    "start": "239319",
    "end": "240720"
  },
  {
    "text": "actually transcribe it size of the model",
    "start": "240720",
    "end": "242480"
  },
  {
    "text": "has an input on that so so we came up",
    "start": "242480",
    "end": "245959"
  },
  {
    "text": "with this fun challenge that okay",
    "start": "245959",
    "end": "247360"
  },
  {
    "text": "metaflow says that you can do all these",
    "start": "247360",
    "end": "248920"
  },
  {
    "text": "kind of sort of cool dag related uh",
    "start": "248920",
    "end": "251079"
  },
  {
    "text": "interesting stuff using um just Python",
    "start": "251079",
    "end": "253640"
  },
  {
    "text": "apis and Whisper is a model that can do",
    "start": "253640",
    "end": "256199"
  },
  {
    "text": "transcription with multiple sizes can we",
    "start": "256199",
    "end": "258680"
  },
  {
    "text": "do something like this if you see this",
    "start": "258680",
    "end": "260720"
  },
  {
    "text": "flow from left to right we have a start",
    "start": "260720",
    "end": "263120"
  },
  {
    "text": "step and there what we want to do is we",
    "start": "263120",
    "end": "265040"
  },
  {
    "text": "want to transcribe three URLs so",
    "start": "265040",
    "end": "266960"
  },
  {
    "text": "basically three files that are available",
    "start": "266960",
    "end": "268800"
  },
  {
    "text": "as urls and in each of these cases we",
    "start": "268800",
    "end": "271479"
  },
  {
    "text": "want to use the tiny model and the large",
    "start": "271479",
    "end": "274520"
  },
  {
    "text": "model so we have two the tiny open a",
    "start": "274520",
    "end": "277320"
  },
  {
    "text": "whisper model and the large model",
    "start": "277320",
    "end": "279360"
  },
  {
    "text": "transcribe the same file with both of",
    "start": "279360",
    "end": "281600"
  },
  {
    "text": "these and then do a join just so that",
    "start": "281600",
    "end": "284039"
  },
  {
    "text": "you know you kind of sort of get the",
    "start": "284039",
    "end": "285360"
  },
  {
    "text": "results in theory at this join step you",
    "start": "285360",
    "end": "287680"
  },
  {
    "text": "could do some sort of postprocessing to",
    "start": "287680",
    "end": "289680"
  },
  {
    "text": "see whether the output of the tiny was",
    "start": "289680",
    "end": "291360"
  },
  {
    "text": "better or the large model was better or",
    "start": "291360",
    "end": "293240"
  },
  {
    "text": "the time difference between the two or",
    "start": "293240",
    "end": "294759"
  },
  {
    "text": "what have you so you could do some sort",
    "start": "294759",
    "end": "296520"
  },
  {
    "text": "of post processing I have a demo in",
    "start": "296520",
    "end": "298120"
  },
  {
    "text": "which this post- processing does not",
    "start": "298120",
    "end": "299360"
  },
  {
    "text": "happen happen but it is something that",
    "start": "299360",
    "end": "300960"
  },
  {
    "text": "could be done and then you complete the",
    "start": "300960",
    "end": "302600"
  },
  {
    "text": "step the the whole flow so let's see how",
    "start": "302600",
    "end": "305320"
  },
  {
    "text": "we can actually go about doing this so I",
    "start": "305320",
    "end": "307919"
  },
  {
    "text": "have the source code for all of this",
    "start": "307919",
    "end": "309639"
  },
  {
    "text": "here um where we have the start step we",
    "start": "309639",
    "end": "313680"
  },
  {
    "text": "we decide or we actually set up three",
    "start": "313680",
    "end": "315960"
  },
  {
    "text": "URLs in this case of course the number",
    "start": "315960",
    "end": "317600"
  },
  {
    "text": "of URLs in this case is three but it",
    "start": "317600",
    "end": "319280"
  },
  {
    "text": "could be any number then you call this",
    "start": "319280",
    "end": "321680"
  },
  {
    "text": "transcribe transcribe says that when I",
    "start": "321680",
    "end": "324319"
  },
  {
    "text": "at transcribe call Two Steps called tiny",
    "start": "324319",
    "end": "326800"
  },
  {
    "text": "and small so I know I mentioned large",
    "start": "326800",
    "end": "329240"
  },
  {
    "text": "mod model in the previous step but",
    "start": "329240",
    "end": "330520"
  },
  {
    "text": "instead of large I'm using a small model",
    "start": "330520",
    "end": "332039"
  },
  {
    "text": "it's just is easier to do it um so you",
    "start": "332039",
    "end": "334479"
  },
  {
    "text": "have two steps that are happening in",
    "start": "334479",
    "end": "335600"
  },
  {
    "text": "parallel for each URL for each URL you",
    "start": "335600",
    "end": "338280"
  },
  {
    "text": "call this tiny and uh small steps and",
    "start": "338280",
    "end": "341199"
  },
  {
    "text": "then they both at the end say join so",
    "start": "341199",
    "end": "343720"
  },
  {
    "text": "they combine the results and then join",
    "start": "343720",
    "end": "345880"
  },
  {
    "text": "finally calls the end step uh which",
    "start": "345880",
    "end": "347919"
  },
  {
    "text": "actually completes so now let's take a",
    "start": "347919",
    "end": "349759"
  },
  {
    "text": "look oh by the way what other what would",
    "start": "349759",
    "end": "351759"
  },
  {
    "text": "be the best way to run this obviously",
    "start": "351759",
    "end": "353360"
  },
  {
    "text": "this will be on kubernetes right I mean",
    "start": "353360",
    "end": "354840"
  },
  {
    "text": "that's what brings all of us here the",
    "start": "354840",
    "end": "356639"
  },
  {
    "text": "interesting bit here is that you can use",
    "start": "356639",
    "end": "358039"
  },
  {
    "text": "kubernetes decorators uh in metaflow and",
    "start": "358039",
    "end": "360880"
  },
  {
    "text": "you can pick and choose how much",
    "start": "360880",
    "end": "362400"
  },
  {
    "text": "resources you can provide to each of",
    "start": "362400",
    "end": "364039"
  },
  {
    "text": "these steps so for the tiny step I'm",
    "start": "364039",
    "end": "366160"
  },
  {
    "text": "actually giving it two CPUs and 1 gig of",
    "start": "366160",
    "end": "368000"
  },
  {
    "text": "memory because that's what the open a",
    "start": "368000",
    "end": "369400"
  },
  {
    "text": "documentation talks about and then for",
    "start": "369400",
    "end": "371400"
  },
  {
    "text": "the small step I'm actually giving it",
    "start": "371400",
    "end": "373199"
  },
  {
    "text": "four CPUs and 8 gigs of memory just",
    "start": "373199",
    "end": "375639"
  },
  {
    "text": "because that's the amount of memory",
    "start": "375639",
    "end": "376759"
  },
  {
    "text": "needed to actually run the step so and",
    "start": "376759",
    "end": "379720"
  },
  {
    "text": "this will automatically get taken care",
    "start": "379720",
    "end": "381080"
  },
  {
    "text": "of when this runs on kubernetes the",
    "start": "381080",
    "end": "382479"
  },
  {
    "text": "corresponding pods that gets spawned up",
    "start": "382479",
    "end": "384400"
  },
  {
    "text": "metaflow will make sure that these spawn",
    "start": "384400",
    "end": "386000"
  },
  {
    "text": "up with these resources so once this",
    "start": "386000",
    "end": "388000"
  },
  {
    "text": "actually happens you all you have to do",
    "start": "388000",
    "end": "389280"
  },
  {
    "text": "is basically run this uh with kubernetes",
    "start": "389280",
    "end": "392560"
  },
  {
    "text": "in this case um and and this will",
    "start": "392560",
    "end": "396080"
  },
  {
    "text": "actually spawn each of those steps one",
    "start": "396080",
    "end": "398479"
  },
  {
    "text": "after the other or some in sequence some",
    "start": "398479",
    "end": "400280"
  },
  {
    "text": "in parallel depending on how the flow",
    "start": "400280",
    "end": "401639"
  },
  {
    "text": "was actually written and then each of",
    "start": "401639",
    "end": "403919"
  },
  {
    "text": "those steps will take its time we won't",
    "start": "403919",
    "end": "405800"
  },
  {
    "text": "have time for actually waiting for this",
    "start": "405800",
    "end": "407280"
  },
  {
    "text": "to complete even though it just takes",
    "start": "407280",
    "end": "408479"
  },
  {
    "text": "about a couple of minutes to complete I",
    "start": "408479",
    "end": "410319"
  },
  {
    "text": "had a previous run that was actually uh",
    "start": "410319",
    "end": "413080"
  },
  {
    "text": "here that you can see so this multi-",
    "start": "413080",
    "end": "415120"
  },
  {
    "text": "audio transcription flow is this is the",
    "start": "415120",
    "end": "416720"
  },
  {
    "text": "one that is actually running right now",
    "start": "416720",
    "end": "418360"
  },
  {
    "text": "for the last what 15 16 seconds and this",
    "start": "418360",
    "end": "420720"
  },
  {
    "text": "one was the run that I ran just before",
    "start": "420720",
    "end": "422759"
  },
  {
    "text": "uh for some time but you can see in this",
    "start": "422759",
    "end": "425039"
  },
  {
    "text": "case this is the metaflow UI this also",
    "start": "425039",
    "end": "427479"
  },
  {
    "text": "is an open part of the open source",
    "start": "427479",
    "end": "429120"
  },
  {
    "text": "metaflow project you can see that the",
    "start": "429120",
    "end": "431280"
  },
  {
    "text": "tiny step here complet like um had its",
    "start": "431280",
    "end": "434560"
  },
  {
    "text": "output where this is the actual",
    "start": "434560",
    "end": "436599"
  },
  {
    "text": "transcription of the of the of the first",
    "start": "436599",
    "end": "439479"
  },
  {
    "text": "audio file that it received and then",
    "start": "439479",
    "end": "441520"
  },
  {
    "text": "this is the small model this is also",
    "start": "441520",
    "end": "443919"
  },
  {
    "text": "like it'll also have its own output and",
    "start": "443919",
    "end": "445879"
  },
  {
    "text": "it's kind of sort of the same output if",
    "start": "445879",
    "end": "447199"
  },
  {
    "text": "you look at the overall dag for this",
    "start": "447199",
    "end": "450080"
  },
  {
    "text": "um you can see that um you can see that",
    "start": "450080",
    "end": "452879"
  },
  {
    "text": "that the steps started in parallel and",
    "start": "452879",
    "end": "454720"
  },
  {
    "text": "how much time they took and so on and so",
    "start": "454720",
    "end": "456360"
  },
  {
    "text": "forth the dag here actually shows that",
    "start": "456360",
    "end": "458360"
  },
  {
    "text": "we had the start step then transcribe",
    "start": "458360",
    "end": "460560"
  },
  {
    "text": "then multiple of these where you had the",
    "start": "460560",
    "end": "461879"
  },
  {
    "text": "tiny and small steps running in parallel",
    "start": "461879",
    "end": "463879"
  },
  {
    "text": "the join and then the post join so this",
    "start": "463879",
    "end": "466759"
  },
  {
    "text": "actually kubernetes is a great mechanism",
    "start": "466759",
    "end": "468840"
  },
  {
    "text": "for actually running this and metaflow",
    "start": "468840",
    "end": "470240"
  },
  {
    "text": "makes it super easy to be able to use",
    "start": "470240",
    "end": "472280"
  },
  {
    "text": "these kinds of models and these kinds of",
    "start": "472280",
    "end": "474560"
  },
  {
    "text": "um resources for the use cases like",
    "start": "474560",
    "end": "476919"
  },
  {
    "text": "model inference in this case",
    "start": "476919",
    "end": "480159"
  },
  {
    "text": "so that's awesome uh what can we do with",
    "start": "480159",
    "end": "483240"
  },
  {
    "text": "this like this was like a cool demo but",
    "start": "483240",
    "end": "485039"
  },
  {
    "text": "what can we do with this and how can we",
    "start": "485039",
    "end": "486440"
  },
  {
    "text": "extend it further well the we had a",
    "start": "486440",
    "end": "489599"
  },
  {
    "text": "fixed set of three URLs or three files",
    "start": "489599",
    "end": "491919"
  },
  {
    "text": "in this case imagine that this pop this",
    "start": "491919",
    "end": "494159"
  },
  {
    "text": "list of audios that you want to actually",
    "start": "494159",
    "end": "496159"
  },
  {
    "text": "transcribe is actually getting populated",
    "start": "496159",
    "end": "498000"
  },
  {
    "text": "based on some other input that you go",
    "start": "498000",
    "end": "499520"
  },
  {
    "text": "out and look at a YouTube channel or",
    "start": "499520",
    "end": "501560"
  },
  {
    "text": "find like search or for audio file",
    "start": "501560",
    "end": "503560"
  },
  {
    "text": "somewhere and actually or look at your",
    "start": "503560",
    "end": "505000"
  },
  {
    "text": "Zoom call history and find all the zoom",
    "start": "505000",
    "end": "506560"
  },
  {
    "text": "recordings that you have so you can",
    "start": "506560",
    "end": "508440"
  },
  {
    "text": "scale out the number URLs that that you",
    "start": "508440",
    "end": "510319"
  },
  {
    "text": "are actually transcribing at any point",
    "start": "510319",
    "end": "511680"
  },
  {
    "text": "in time you could use gpus so in this",
    "start": "511680",
    "end": "514360"
  },
  {
    "text": "case we we saw that we could use gpus uh",
    "start": "514360",
    "end": "516200"
  },
  {
    "text": "sorry CPUs but you can easily use gpus",
    "start": "516200",
    "end": "518518"
  },
  {
    "text": "for this the amount of time it takes for",
    "start": "518519",
    "end": "521080"
  },
  {
    "text": "transcribing audio to text using gpus is",
    "start": "521080",
    "end": "524320"
  },
  {
    "text": "orders of magnitude less than what it",
    "start": "524320",
    "end": "526600"
  },
  {
    "text": "takes for CPUs you can schedule these",
    "start": "526600",
    "end": "529000"
  },
  {
    "text": "flows periodically so you can have these",
    "start": "529000",
    "end": "530800"
  },
  {
    "text": "let's say run every night and you have",
    "start": "530800",
    "end": "532320"
  },
  {
    "text": "some uh YouTube video that gets updated",
    "start": "532320",
    "end": "534839"
  },
  {
    "text": "every day so you can actually transcribe",
    "start": "534839",
    "end": "536680"
  },
  {
    "text": "videos every day or every night and then",
    "start": "536680",
    "end": "538839"
  },
  {
    "text": "run the these flows based on events that",
    "start": "538839",
    "end": "540880"
  },
  {
    "text": "actually the is the really cool part so",
    "start": "540880",
    "end": "542800"
  },
  {
    "text": "I really want to do this where there is",
    "start": "542800",
    "end": "544640"
  },
  {
    "text": "a zoom call uh we it gets recorded or",
    "start": "544640",
    "end": "547160"
  },
  {
    "text": "Google meet call it gets recorded the",
    "start": "547160",
    "end": "548560"
  },
  {
    "text": "moment the recording actually completes",
    "start": "548560",
    "end": "550720"
  },
  {
    "text": "it actually triggers a flow that",
    "start": "550720",
    "end": "552320"
  },
  {
    "text": "actually takes the video transcribes it",
    "start": "552320",
    "end": "556079"
  },
  {
    "text": "uses another the other like",
    "start": "556079",
    "end": "557760"
  },
  {
    "text": "summarization machine learning models",
    "start": "557760",
    "end": "560079"
  },
  {
    "text": "passes the entire transcription to the",
    "start": "560079",
    "end": "561839"
  },
  {
    "text": "summ summarization model generates a",
    "start": "561839",
    "end": "564040"
  },
  {
    "text": "summary and sends it as a slack message",
    "start": "564040",
    "end": "566279"
  },
  {
    "text": "to the team saying this was the summary",
    "start": "566279",
    "end": "567920"
  },
  {
    "text": "of the call that you just completed that",
    "start": "567920",
    "end": "570160"
  },
  {
    "text": "would be a fun thing to do and it'll be",
    "start": "570160",
    "end": "571959"
  },
  {
    "text": "much much more fun if you do it in",
    "start": "571959",
    "end": "574959"
  },
  {
    "text": "metaflow um so thanks a lot that's all I",
    "start": "574959",
    "end": "577399"
  },
  {
    "text": "had in mind uh metaflow is also",
    "start": "577399",
    "end": "579240"
  },
  {
    "text": "available U there is a slack Channel",
    "start": "579240",
    "end": "580800"
  },
  {
    "text": "open to everyone so feel free to join us",
    "start": "580800",
    "end": "582480"
  },
  {
    "text": "on the slack Channel if you have more",
    "start": "582480",
    "end": "583600"
  },
  {
    "text": "questions about metaflow and the company",
    "start": "583600",
    "end": "586160"
  },
  {
    "text": "that uh I mentioned that we are behind",
    "start": "586160",
    "end": "587959"
  },
  {
    "text": "metaflow is called outer bounds we have",
    "start": "587959",
    "end": "590200"
  },
  {
    "text": "a booth called uh Booth z20 so come join",
    "start": "590200",
    "end": "594000"
  },
  {
    "text": "us and talk to us at the booth we also",
    "start": "594000",
    "end": "595920"
  },
  {
    "text": "have an event uh day after tomorrow in",
    "start": "595920",
    "end": "598399"
  },
  {
    "text": "the evening so if you want to join us",
    "start": "598399",
    "end": "600079"
  },
  {
    "text": "for the event uh feel free to come talk",
    "start": "600079",
    "end": "601959"
  },
  {
    "text": "to us at the booth or you can scan the",
    "start": "601959",
    "end": "603440"
  },
  {
    "text": "QR code and uh join us there thank",
    "start": "603440",
    "end": "607760"
  },
  {
    "text": "you",
    "start": "609959",
    "end": "612959"
  }
]