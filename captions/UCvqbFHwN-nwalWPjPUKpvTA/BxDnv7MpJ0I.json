[
  {
    "text": "so welcome everyone to Sikh Network intro I see a few familiar faces who here is already involved with zigge",
    "start": "30",
    "end": "6390"
  },
  {
    "text": "Network all right you're probably watch me embarrass myself to everyone else welcome",
    "start": "6390",
    "end": "12620"
  },
  {
    "text": "so sig network is responsible for more or less anything to do with networking",
    "start": "12620",
    "end": "18270"
  },
  {
    "text": "in kubernetes so we own some broad areas of responsibility like can pause talk to",
    "start": "18270",
    "end": "24119"
  },
  {
    "text": "other pods can we have traffic going into the cluster and so on and this translates into a number of specific",
    "start": "24119",
    "end": "30029"
  },
  {
    "text": "components that we own as well and maintain so I'll get like at the end more into",
    "start": "30029",
    "end": "36780"
  },
  {
    "text": "how to get involved to the sig but we meet every two weeks we have like a video call we talk or argue around a",
    "start": "36780",
    "end": "43739"
  },
  {
    "text": "document and then we also have like our mailing lists our slack lists like the other SIG's if you've interacted with",
    "start": "43739",
    "end": "50039"
  },
  {
    "text": "them where we have more like day to day coordination a discussion",
    "start": "50039",
    "end": "55489"
  },
  {
    "text": "so some of the components we maintain is the CMI implementation so being able to",
    "start": "56030",
    "end": "61379"
  },
  {
    "text": "have different network drivers we're responsible for the base layer of that we maintain services and endpoints to be",
    "start": "61379",
    "end": "68880"
  },
  {
    "text": "able to have higher level abstractions towards having instance groups that we",
    "start": "68880",
    "end": "74040"
  },
  {
    "text": "load balanced - we own the coop proxy which is a fun little low-level mechanic",
    "start": "74040",
    "end": "79920"
  },
  {
    "text": "for routing traffic internally around our cluster DNS so we can actually have discovery of",
    "start": "79920",
    "end": "88140"
  },
  {
    "text": "our services as well as some functionality towards like outside world DNS we have the ingress which is a fun",
    "start": "88140",
    "end": "95400"
  },
  {
    "text": "and somewhat controversial way of routing traffic into our clusters we're currently looking at GAR existing",
    "start": "95400",
    "end": "103409"
  },
  {
    "text": "ingress spec with some tweaks and looking at ingress v2 but Tim we'll get more into that with the deep dive and then lastly we also own the network",
    "start": "103409",
    "end": "111210"
  },
  {
    "text": "policy spec which is just a spec we don't have a controller for it currently in core",
    "start": "111210",
    "end": "117409"
  },
  {
    "text": "so we get questions like this a lot lately last couple months we've been",
    "start": "117409",
    "end": "122700"
  },
  {
    "text": "doing a triage at the beginning of each session and almost always we get a question like this help my pods can't",
    "start": "122700",
    "end": "128099"
  },
  {
    "text": "reach google.com help my service doesn't run so in that spirit I thought I would",
    "start": "128099",
    "end": "133230"
  },
  {
    "text": "go over kind of the components and steps involved in service to service",
    "start": "133230",
    "end": "138450"
  },
  {
    "text": "networking in kubernetes so this is like how we how we like to",
    "start": "138450",
    "end": "144480"
  },
  {
    "text": "whiteboard it or think about it more first introducing the concept we have some pod it goes to something goes to",
    "start": "144480",
    "end": "150840"
  },
  {
    "text": "another pod and it just works but there's many many moving parts involved in there's many ways that you're add-ons",
    "start": "150840",
    "end": "157740"
  },
  {
    "text": "can interfere your configuration can be wrong or we might have a bug in the base stack it happens",
    "start": "157740",
    "end": "165290"
  },
  {
    "text": "thank you all thanks everyone",
    "start": "183560",
    "end": "188000"
  },
  {
    "text": "oh wow that's a lot of people okay welcome no pressure",
    "start": "193069",
    "end": "200859"
  },
  {
    "text": "so as I was saying this is like our nice super high-level way that we'd like to plan this the stock works this is even",
    "start": "202150",
    "end": "209810"
  },
  {
    "text": "still a very high level view of what's actually going on so we have DNS we've got everything going through a proxy we",
    "start": "209810",
    "end": "216109"
  },
  {
    "text": "have Services was not even traffic going through services they're like an abstract management concept that creates",
    "start": "216109",
    "end": "222139"
  },
  {
    "text": "and manages a lot of other resources so let's kind of step through this in a conceptual order",
    "start": "222139",
    "end": "228579"
  },
  {
    "text": "firstly I'm going to look at what the service controller actually does so when you create some service object in your",
    "start": "228579",
    "end": "235879"
  },
  {
    "text": "cluster this is what it's doing services are a high level constructor",
    "start": "235879",
    "end": "241489"
  },
  {
    "text": "applaud networking so you have some pool of pods and you want able to route",
    "start": "241489",
    "end": "246680"
  },
  {
    "text": "traffic to them without having to manually track every pod and load balance and wrote and discover it all",
    "start": "246680",
    "end": "252950"
  },
  {
    "text": "yourself that would be very messy this is one of the like distinguishing characteristics of kubernetes evolving",
    "start": "252950",
    "end": "258769"
  },
  {
    "text": "at a borg because borg who mostly relied on just publishing that endpoint data around pods and then clients would kind",
    "start": "258769",
    "end": "266300"
  },
  {
    "text": "of decide how to handle it on their own so we expose a group of pods with a set",
    "start": "266300",
    "end": "272300"
  },
  {
    "text": "of ports and protocols and so on so this might be just like exposing a tea it might be exposing a tea for for three on",
    "start": "272300",
    "end": "277909"
  },
  {
    "text": "the same abstract service and you just hit a service and it ideally works and",
    "start": "277909",
    "end": "286430"
  },
  {
    "text": "you don't have to fuss around any of the backend implementation so this is a snippet out of a service",
    "start": "286430",
    "end": "294710"
  },
  {
    "text": "spec it's a core object and we give it a name we give it a set of ports and those",
    "start": "294710",
    "end": "301699"
  },
  {
    "text": "ports map onto something on the container side so here in this example we have a tea as",
    "start": "301699",
    "end": "307990"
  },
  {
    "text": "the port that we're exposing to the world that's pretty typical and then like we often do we run our web server",
    "start": "307990",
    "end": "314360"
  },
  {
    "text": "on some arbitrary port on a container in this case we're using TCP there's",
    "start": "314360",
    "end": "320280"
  },
  {
    "text": "another very important part in the spec called the selector and the selector is what makes sure that we actually hit",
    "start": "320280",
    "end": "326880"
  },
  {
    "text": "some pods so it's often a frequent complaint of like initially users when they're looking at the docs they're",
    "start": "326880",
    "end": "332789"
  },
  {
    "text": "confused why is services not built into deployments and the answer is that services don't necessarily map onto a",
    "start": "332789",
    "end": "339630"
  },
  {
    "text": "deployment so selectors can be changed and they can",
    "start": "339630",
    "end": "344910"
  },
  {
    "text": "correspond to multiple things for example you might choose to do a blue green deploy of your application using",
    "start": "344910",
    "end": "351780"
  },
  {
    "text": "labels so you have deployment a deployment be same label the same",
    "start": "351780",
    "end": "357690"
  },
  {
    "text": "service hits them even though the different deployment objects you can also play around with like evicting and",
    "start": "357690",
    "end": "362729"
  },
  {
    "text": "adding pods but that's another story so this is what it looks like in",
    "start": "362729",
    "end": "368070"
  },
  {
    "text": "practice we have our service and it the service controller finds and selects",
    "start": "368070",
    "end": "374070"
  },
  {
    "text": "every single pod with that matching selector set so this is watching every single pod but",
    "start": "374070",
    "end": "381389"
  },
  {
    "text": "that's not necessarily helpful on its own because for example what if a pod is just starting up what if it's still",
    "start": "381389",
    "end": "386729"
  },
  {
    "text": "creating what if like the web server hasn't loaded yet we're still missing sign to make sure that we have proper",
    "start": "386729",
    "end": "393479"
  },
  {
    "text": "service discovery because that's an integral part",
    "start": "393479",
    "end": "397909"
  },
  {
    "text": "so another useful thing that services do is having DNS records so it creates a",
    "start": "398810",
    "end": "405240"
  },
  {
    "text": "DNS record corresponding to the service name this means that we have song it's very intuitive to access and we don't",
    "start": "405240",
    "end": "412020"
  },
  {
    "text": "have to use something that's not a terminus t'k for example if it created an IP we'd need to create an object then",
    "start": "412020",
    "end": "419190"
  },
  {
    "text": "check what the IP is and then try to work with that that's not very kubernetes so instead we have the",
    "start": "419190",
    "end": "426449"
  },
  {
    "text": "service DNS record there's a particular path that's globally accessible where we",
    "start": "426449",
    "end": "433289"
  },
  {
    "text": "create records it's the service name space and then SBC cost or local as well",
    "start": "433289",
    "end": "439440"
  },
  {
    "text": "as within a namespace we typically just access it by its name and it's a standard DNS record so we just access it",
    "start": "439440",
    "end": "447700"
  },
  {
    "text": "reports protocols however we desire the code for the controller lives in",
    "start": "447700",
    "end": "453940"
  },
  {
    "text": "Korea's core most of it is in package controller service and it's run by",
    "start": "453940",
    "end": "459310"
  },
  {
    "text": "something called coop controller manager coop controller manager is responsible for basically bundling up a lot of the",
    "start": "459310",
    "end": "466510"
  },
  {
    "text": "control plane controllers and running them all together in one process it's a",
    "start": "466510",
    "end": "472000"
  },
  {
    "text": "bit of a weird and magical hack and I can't pretend to fully know the context behind it",
    "start": "472000",
    "end": "478710"
  },
  {
    "text": "so next up we have endpoints I said earlier that services aren't directly",
    "start": "479070",
    "end": "484840"
  },
  {
    "text": "resolved for handle they aren't directly involved in handling any traffic they create DNS records and they create",
    "start": "484840",
    "end": "491230"
  },
  {
    "text": "endpoints what endpoints are our routes to individual pods so it's a one-to-one",
    "start": "491230",
    "end": "497620"
  },
  {
    "text": "relationship of a pod and an IP specifically it's a one-to-one",
    "start": "497620",
    "end": "503470"
  },
  {
    "text": "relationship of ready pods so until a pod is ready it doesn't wind up in the",
    "start": "503470",
    "end": "509290"
  },
  {
    "text": "endpoint list min Han from sig networks did a pretty good talk yesterday on pod readiness and",
    "start": "509290",
    "end": "517000"
  },
  {
    "text": "he talked a lot about the endpoint controller and some of the logistics there",
    "start": "517000",
    "end": "522599"
  },
  {
    "text": "so the endpoint object is what gets consumed internally for doing",
    "start": "522690",
    "end": "530260"
  },
  {
    "text": "load balancing and serves discovery and it's also something that you can custom use for example one thing that I've been",
    "start": "530260",
    "end": "536170"
  },
  {
    "text": "working on is doing endpoint level load balancing and I know a lot of companies like Google also have something like",
    "start": "536170",
    "end": "541810"
  },
  {
    "text": "that where you wrote out a load balancer level directly to pods versus going",
    "start": "541810",
    "end": "547270"
  },
  {
    "text": "through the standard mechanisms later I'll talk more about how those are implemented an interesting thing about endpoints is",
    "start": "547270",
    "end": "554680"
  },
  {
    "text": "recently we had an enhancement proposal come up around adding a concept of an",
    "start": "554680",
    "end": "560710"
  },
  {
    "text": "endpoint slice so incriminate is when we have some kind of",
    "start": "560710",
    "end": "565900"
  },
  {
    "text": "object list it's one massive list of everything so this might be a list of",
    "start": "565900",
    "end": "571800"
  },
  {
    "text": "many many many hundreds of pause there in a deployed and we certain scalability",
    "start": "571800",
    "end": "577900"
  },
  {
    "text": "concerns with this at some point because we either have too many objects and @cd has a scaling problem for the key or",
    "start": "577900",
    "end": "585220"
  },
  {
    "text": "perhaps it's just too much churn too much data going around the cluster at once for example if you have a list of",
    "start": "585220",
    "end": "591400"
  },
  {
    "text": "endpoints in this case every time endpoints are coming or going you have a massive list of changes to sync out to",
    "start": "591400",
    "end": "598360"
  },
  {
    "text": "your controllers this one's I having performance impacts not great",
    "start": "598360",
    "end": "603490"
  },
  {
    "text": "I think Tim will get more into the endpoint slices proposal and his deep dive but there's a debate to be had",
    "start": "603490",
    "end": "610120"
  },
  {
    "text": "about that model because it's new to kubernetes we don't really have slices or partitioning of any other resources",
    "start": "610120",
    "end": "616210"
  },
  {
    "text": "yet and this proposal could get messy so there's a discussion to be had around what",
    "start": "616210",
    "end": "623440"
  },
  {
    "text": "compromises we make and how much we try to eat performance or change some of our other models around having a singular",
    "start": "623440",
    "end": "629920"
  },
  {
    "text": "endpoint list so endpoint controller again is another",
    "start": "629920",
    "end": "635410"
  },
  {
    "text": "a bit of coop controller manager magic and it's in package controller endpoint",
    "start": "635410",
    "end": "642269"
  },
  {
    "text": "so next up DNS a lot of the DNS stuff we actually don't own anymore because it",
    "start": "643560",
    "end": "651160"
  },
  {
    "text": "runs as core DNS rather than our own implementation so we manage a fairly",
    "start": "651160",
    "end": "656710"
  },
  {
    "text": "simple system where we deployed DNS to the cluster via just a deployment and",
    "start": "656710",
    "end": "663370"
  },
  {
    "text": "then use both to help my head I think it's the couplet to inject information around",
    "start": "663370",
    "end": "670500"
  },
  {
    "text": "pointing to that DNS server but we don't actually maintain the core DNS server",
    "start": "670500",
    "end": "677080"
  },
  {
    "text": "code anymore",
    "start": "677080",
    "end": "679560"
  },
  {
    "text": "so I believe we do still have the old DNS entry but using it as highly",
    "start": "684990",
    "end": "690640"
  },
  {
    "text": "ill-advised unless they have a very specific reason we're kind of going for this pattern more and more in",
    "start": "690640",
    "end": "695860"
  },
  {
    "text": "communities of rather than providing a core implementation of something provide an interface or provide a standard way",
    "start": "695860",
    "end": "702820"
  },
  {
    "text": "and then outsourcing either to like it kind of for lack of a better way of",
    "start": "702820",
    "end": "708430"
  },
  {
    "text": "putting it singular winner with coordinates because it's just the standard or something like Network policy where there's many different",
    "start": "708430",
    "end": "713830"
  },
  {
    "text": "options with their own trade-offs so last component that I'm going to get",
    "start": "713830",
    "end": "720820"
  },
  {
    "text": "into is my favorite one and by that I mean the one that I've spent the most time poking at it's the cube proxy so",
    "start": "720820",
    "end": "728770"
  },
  {
    "text": "the cube proxy is responsible for a lot of magic behind the scenes kind of sticking some iptables glue in here and",
    "start": "728770",
    "end": "734320"
  },
  {
    "text": "there and just making sure that everything Ravitz",
    "start": "734320",
    "end": "739110"
  },
  {
    "text": "so I've done this diagram of roughly what the flow is of data around pot ipys",
    "start": "740010",
    "end": "746680"
  },
  {
    "text": "we start off with once pods are ready the Kubla which is responsible for",
    "start": "746680",
    "end": "752230"
  },
  {
    "text": "tracking pod data submits that signal on the pod status",
    "start": "752230",
    "end": "758500"
  },
  {
    "text": "now the endpoint controller is watching the pods and when it sees pods become",
    "start": "758500",
    "end": "764170"
  },
  {
    "text": "ready it adds the corresponding pod IP to its own list so if you're not very",
    "start": "764170",
    "end": "770080"
  },
  {
    "text": "familiar with kubernetes architecture there's a lot of this kind of back and forth pattern that happens between controllers where we have massive lists",
    "start": "770080",
    "end": "777760"
  },
  {
    "text": "of things and we essentially watching maintain independent states so it's very it's very broken up because we want to",
    "start": "777760",
    "end": "784630"
  },
  {
    "text": "be able to have very simple controllers with very simple message passing as opposed to something",
    "start": "784630",
    "end": "789940"
  },
  {
    "text": "that's perhaps much more complex and much more stateful so we have the endpoint ads the list",
    "start": "789940",
    "end": "796620"
  },
  {
    "text": "that's something that route proxy is constantly pulling for when Clube proxy sees the endpoint",
    "start": "796620",
    "end": "802780"
  },
  {
    "text": "that's available it then adds some Network rules in the host",
    "start": "802780",
    "end": "808230"
  },
  {
    "text": "so the network rules are things like IP IP tables or IP vs the several different",
    "start": "808230",
    "end": "813850"
  },
  {
    "text": "modes that I'll talk about briefly later that then provide on host networking so",
    "start": "813850",
    "end": "819220"
  },
  {
    "text": "let's see what that looks like ku proxy is a daemon set on every single",
    "start": "819220",
    "end": "824230"
  },
  {
    "text": "node that specifically is used to configure the host network so although we call it",
    "start": "824230",
    "end": "832600"
  },
  {
    "text": "a proxy other than like the legacy mode that I hope no one's using anymore it doesn't actually proxy data itself it",
    "start": "832600",
    "end": "839560"
  },
  {
    "text": "just sets up the host proxying and maintains that constantly so what ku proxy uses for capturing",
    "start": "839560",
    "end": "847900"
  },
  {
    "text": "traffic is something called a cluster IP which is a set of virtual IPs so we",
    "start": "847900",
    "end": "853690"
  },
  {
    "text": "create a cluster IP and every single KU proxy instance in the cluster maintains",
    "start": "853690",
    "end": "859330"
  },
  {
    "text": "that and then it acts as an IP sync for doing additional routing because when",
    "start": "859330",
    "end": "864670"
  },
  {
    "text": "you make a request for the cluster IP your network rules will hit that",
    "start": "864670",
    "end": "870870"
  },
  {
    "text": "so specifically a cluster IP corresponds to one service we create a DNS record",
    "start": "870990",
    "end": "877120"
  },
  {
    "text": "for the service so that it's nicely determined it has thick and easy to access but this is the real world with",
    "start": "877120",
    "end": "883990"
  },
  {
    "text": "an IP address behind that so this is meant to be a stable interface for connecting to pods",
    "start": "883990",
    "end": "890700"
  },
  {
    "text": "and then ku proxy is also responsible for not just owning this but specifically",
    "start": "890700",
    "end": "897310"
  },
  {
    "text": "doing the load balancing although behavioral around load balancing really varies depending on the way they have KU",
    "start": "897310",
    "end": "903850"
  },
  {
    "text": "proxy configured so ku proxy right now has four modes",
    "start": "903850",
    "end": "910620"
  },
  {
    "text": "there's the older user space one which is a legacy mode it essentially ran as a",
    "start": "910620",
    "end": "917020"
  },
  {
    "text": "ghost server that would terminate connections and open new connections this one I am too young in the project",
    "start": "917020",
    "end": "924400"
  },
  {
    "text": "who I've worked with but it was much more feature-rich especially than IP tables because working in user space and",
    "start": "924400",
    "end": "931270"
  },
  {
    "text": "working programmatically with everything you could do a lot of complex things so it had major scalability issues if you",
    "start": "931270",
    "end": "938410"
  },
  {
    "text": "look back and github you can see many complaints about this around throughput issues and cluster size but IP tables in",
    "start": "938410",
    "end": "945280"
  },
  {
    "text": "some ways was a regression around load balancing functionality piping tables is now our default mode so if memory serves",
    "start": "945280",
    "end": "953610"
  },
  {
    "text": "we don't have much loading balancing functionality it's a bit done with her",
    "start": "953610",
    "end": "958620"
  },
  {
    "text": "notes things but it works especially with IP tables speed ups under the hood going on but",
    "start": "958620",
    "end": "965520"
  },
  {
    "text": "that said it can still be a performance constraint just because once you have so many hundreds and hundreds of IP tables",
    "start": "965520",
    "end": "971520"
  },
  {
    "text": "and rules in any system we start to have issues IP vs I'm not 100% sure if it's actually",
    "start": "971520",
    "end": "978960"
  },
  {
    "text": "beta anymore but it's our okay so IP V SSgA now it's a new system and there's",
    "start": "978960",
    "end": "986790"
  },
  {
    "text": "also the windows module which I can't pretend to know much about but the entire Windows Network stock is",
    "start": "986790",
    "end": "992910"
  },
  {
    "text": "different so we have an entirely different mode for that we started talking about ways to",
    "start": "992910",
    "end": "998390"
  },
  {
    "text": "split these modes out more and maybe deprecated user space just because there's so many bugs that have come up",
    "start": "998390",
    "end": "1004730"
  },
  {
    "text": "in the interplay of these various modes like one particular thing that I did",
    "start": "1004730",
    "end": "1009920"
  },
  {
    "text": "recently was deprecating automatic cross mode cleanup because we found that there was some",
    "start": "1009920",
    "end": "1015950"
  },
  {
    "text": "there's some weird intersection between all of these for example user space and ipbs actually create some iptables rules",
    "start": "1015950",
    "end": "1022460"
  },
  {
    "text": "so this means that we can't just wipe the IP table stuff clean because there might be some other implications",
    "start": "1022460",
    "end": "1031329"
  },
  {
    "text": "so there are several different kinds of service types and cube proxy is",
    "start": "1032770",
    "end": "1038089"
  },
  {
    "text": "responsible for the implementation details of how they actually work first one I'm talking about is just a",
    "start": "1038089",
    "end": "1044510"
  },
  {
    "text": "very simple one it's external name so that's just a cname to some other system",
    "start": "1044510",
    "end": "1049900"
  },
  {
    "text": "so there's no cluster IPA doesn't map to any pods it's just a way of like taking some external service of yours and",
    "start": "1049900",
    "end": "1056180"
  },
  {
    "text": "making it look kind of like a kubernetes service",
    "start": "1056180",
    "end": "1060490"
  },
  {
    "text": "next one is the main one for services that we is the most is the cluster IP",
    "start": "1061480",
    "end": "1066670"
  },
  {
    "text": "so the cluster IP winds up with service DNS pointing to it and then it acts as a",
    "start": "1066670",
    "end": "1073400"
  },
  {
    "text": "load balancer between requests to the rest of the cluster so we make a request to say one dot one",
    "start": "1073400",
    "end": "1080600"
  },
  {
    "text": "dot one dot one that's a cluster IP iptables rules on the host hijacks that",
    "start": "1080600",
    "end": "1086710"
  },
  {
    "text": "the IP tables rules contains a list of 50 pods or however many ready pods you",
    "start": "1086710",
    "end": "1092990"
  },
  {
    "text": "have and that picks which IP within the cluster to send that request to",
    "start": "1092990",
    "end": "1098440"
  },
  {
    "text": "a question that we get a lot is why don't we use dns for this kind of thing why do we have this IP table set up the",
    "start": "1098440",
    "end": "1105740"
  },
  {
    "text": "short answer is DNS sucks it's more complicated than that but as anyone who's tried to load balance stuff with",
    "start": "1105740",
    "end": "1112190"
  },
  {
    "text": "DNS is found there's a lot of pitfalls to that there's often unpredictable behavior on client caching where things",
    "start": "1112190",
    "end": "1118910"
  },
  {
    "text": "can happen if your connectivity gets cut off and a lot of clients don't respect",
    "start": "1118910",
    "end": "1124690"
  },
  {
    "text": "that kind of load balancing it's kind of up to whatever library is using",
    "start": "1124690",
    "end": "1130250"
  },
  {
    "text": "it so you'll often find that you get like your first index and through nice record running very hot because",
    "start": "1130250",
    "end": "1135800"
  },
  {
    "text": "everything is just picking the first record out of many rather than picking a random one",
    "start": "1135800",
    "end": "1141790"
  },
  {
    "text": "next time we have is the node ports the note port can be thought of as like a very simple building block for doing",
    "start": "1141790",
    "end": "1148850"
  },
  {
    "text": "external connectivity it just opens a port on the host and that maps to any local containers if I",
    "start": "1148850",
    "end": "1156080"
  },
  {
    "text": "remember correctly it actually fails if you don't have any containers with that port open and running",
    "start": "1156080",
    "end": "1163030"
  },
  {
    "text": "the more advanced combination of a cluster IP and a node port is called the",
    "start": "1164710",
    "end": "1169910"
  },
  {
    "text": "service type load balancer so this is a combination of creating cluster IPS opening node ports pointing to them and",
    "start": "1169910",
    "end": "1176660"
  },
  {
    "text": "then running some kind of cloud provider code to spin up an external load balancer so the result of this is if you",
    "start": "1176660",
    "end": "1183380"
  },
  {
    "text": "hit any class any node in your cluster it'll contain a node port that you can",
    "start": "1183380",
    "end": "1189530"
  },
  {
    "text": "access and that node port will route to a cluster IP which then wrote somewhere in your cluster this is where potta",
    "start": "1189530",
    "end": "1197240"
  },
  {
    "text": "where load balancing which I mentioned earlier starts to become desirable because you have a high amount of",
    "start": "1197240",
    "end": "1202340"
  },
  {
    "text": "abstraction here you have things just kind of bouncing around and this requires going through some",
    "start": "1202340",
    "end": "1208460"
  },
  {
    "text": "like iptables rules that might not be strictly necessary and it also means that you might be taking an extra hop of",
    "start": "1208460",
    "end": "1214730"
  },
  {
    "text": "going one host into your cluster and that host redirects you to a different host it's not something that's critical",
    "start": "1214730",
    "end": "1221270"
  },
  {
    "text": "but if you're very constrained about performance or you have very high scale systems that really starts to matter",
    "start": "1221270",
    "end": "1229269"
  },
  {
    "text": "so the coop proxy runs as its own binary we're looking at splitting out of kaykai",
    "start": "1229630",
    "end": "1235550"
  },
  {
    "text": "because we're having a bit of a mono repo problem in kubernetes that we're very slowly tackling but the main binary",
    "start": "1235550",
    "end": "1243920"
  },
  {
    "text": "is in CMD to proxy CMD is what holds most the binaries we create and then",
    "start": "1243920",
    "end": "1249800"
  },
  {
    "text": "most of it logic is lumped into package proxy",
    "start": "1249800",
    "end": "1255220"
  },
  {
    "text": "so that's the stack involved in service service what else didn't recover",
    "start": "1255640",
    "end": "1262750"
  },
  {
    "text": "well is the ingress which is our layer seven way of writing traffic to services",
    "start": "1262750",
    "end": "1268720"
  },
  {
    "text": "under those although low-level routing concerns so for example you could get almost infinitely deep when talking",
    "start": "1268720",
    "end": "1275330"
  },
  {
    "text": "about coop proxy all the way down the IP stack as to how exactly things work and",
    "start": "1275330",
    "end": "1282590"
  },
  {
    "text": "what kind of fan tangling goes on and there's also aspects like CNI",
    "start": "1282590",
    "end": "1287980"
  },
  {
    "text": "that we can get into as well somewhat notably is we don't really work",
    "start": "1287980",
    "end": "1294380"
  },
  {
    "text": "with network overlays in our sake ourselves so we get many many questions in fact I",
    "start": "1294380",
    "end": "1301010"
  },
  {
    "text": "would say probably one person a day bouncing into the channel saying hey I ran something I installed flannel now my",
    "start": "1301010",
    "end": "1307880"
  },
  {
    "text": "notes don't work fix it and there's some very lovely people who try",
    "start": "1307880",
    "end": "1313160"
  },
  {
    "text": "to help if they can but it's not something it's our purview and it's not something that we make",
    "start": "1313160",
    "end": "1319450"
  },
  {
    "text": "so I'm guessing that most of you are these curious about getting involved since you're here how do we do that",
    "start": "1320560",
    "end": "1328120"
  },
  {
    "text": "the easiest open lowest friction way is just to get involved with github issues starting with adding your own if you",
    "start": "1328120",
    "end": "1335210"
  },
  {
    "text": "have a problem or if you want something so easiest ones are just file feature",
    "start": "1335210",
    "end": "1341910"
  },
  {
    "text": "requests or file a bug you can also if you are more familiar with like how urban edits works or you're familiar",
    "start": "1341910",
    "end": "1348570"
  },
  {
    "text": "with the code base itself you can file cleanup cleanup recommendations we have",
    "start": "1348570",
    "end": "1354120"
  },
  {
    "text": "in many areas of kubernetes but it feels like a lot in Signet work we have a lot",
    "start": "1354120",
    "end": "1359220"
  },
  {
    "text": "of deliberately incurred tech debt the project tried to grow very quickly it's how we want the cluster orchestration",
    "start": "1359220",
    "end": "1365490"
  },
  {
    "text": "battle but now there's always stuff to kind of clean up and there's a slow tale of making sure that we don't break",
    "start": "1365490",
    "end": "1371340"
  },
  {
    "text": "things in the process of that so we do have",
    "start": "1371340",
    "end": "1376880"
  },
  {
    "text": "I'm gonna say this on camera to hold myself accountable we do need to get better with tagging things as it first",
    "start": "1376880",
    "end": "1383220"
  },
  {
    "text": "issue or Help Wanted but we do have issues that are meant to be things that aren't too time critical and are fairly",
    "start": "1383220",
    "end": "1389030"
  },
  {
    "text": "approachable without a whole lot of in-depth knowledge to fix for example the very first ticket I did was just",
    "start": "1389030",
    "end": "1395179"
  },
  {
    "text": "moving some test files around to be more consistent with our code based conventions",
    "start": "1395179",
    "end": "1401990"
  },
  {
    "text": "triaging issues is another good one we thanks to Tim went through from hundreds",
    "start": "1403010",
    "end": "1408540"
  },
  {
    "text": "and hundreds of open question mark issues to having only about 50 right now",
    "start": "1408540",
    "end": "1414300"
  },
  {
    "text": "ountry ozt and most of that massive backlog closed but that said there's",
    "start": "1414300",
    "end": "1419820"
  },
  {
    "text": "always more work to do people are still using kubernetes tickets are still being filed",
    "start": "1419820",
    "end": "1425960"
  },
  {
    "text": "enhancements is a bigger way to get involved so who's familiar with kubernetes enhancements",
    "start": "1427520",
    "end": "1434809"
  },
  {
    "text": "yeah of course you are Tim enhancements are basically non-trivial",
    "start": "1434870",
    "end": "1441809"
  },
  {
    "text": "features or things that require coordination across things or things",
    "start": "1441809",
    "end": "1447330"
  },
  {
    "text": "that require like multiple lifecycle stages as we go across releases so there are things that are notable and user",
    "start": "1447330",
    "end": "1454530"
  },
  {
    "text": "facing as opposed to like individual bugs or just small bits of cleanup",
    "start": "1454530",
    "end": "1459800"
  },
  {
    "text": "so enhancements are kind of where all the cool ideas come for example the ingress GA changes the endpoint slice",
    "start": "1459800",
    "end": "1467570"
  },
  {
    "text": "lots of other stuff so you can go through the communities",
    "start": "1467570",
    "end": "1473260"
  },
  {
    "text": "enhancements repo not sure if there's a shortcut for it but github comm career daddies",
    "start": "1473260",
    "end": "1479500"
  },
  {
    "text": "enhancements will take you there and you can look at the proposals that we have merged which means that we've decided",
    "start": "1479500",
    "end": "1486549"
  },
  {
    "text": "that we agree upon that phase of it and then you can also see the pull request of the stuff that we're still bickering",
    "start": "1486549",
    "end": "1492220"
  },
  {
    "text": "about so enhancement proposals go through a couple different phases starting with just provisional which",
    "start": "1492220",
    "end": "1498309"
  },
  {
    "text": "means we want to do this we think it's reasonable we kind of have an idea of how to do it but we're not necessarily",
    "start": "1498309",
    "end": "1505750"
  },
  {
    "text": "ready to do it we don't have a specific plan to implementable which is we have a solid plan would agreed on this API team",
    "start": "1505750",
    "end": "1513039"
  },
  {
    "text": "doesn't have a problem there's no big logistical issues and so on as we get",
    "start": "1513039",
    "end": "1518559"
  },
  {
    "text": "sawing into the codebase and then eventually all the way to GA when it's done",
    "start": "1518559",
    "end": "1524309"
  },
  {
    "text": "and you can submit an some proposals of your own if you want I suggest starting with",
    "start": "1524309",
    "end": "1530830"
  },
  {
    "text": "an issue that's like our first way to get traction and decide if something is worth pursuing",
    "start": "1530830",
    "end": "1537179"
  },
  {
    "text": "but there's nothing stopping you as for ongoing areas of work there's",
    "start": "1537179",
    "end": "1543429"
  },
  {
    "text": "always tons to do the biggest thing that comes to mind for me is just a cleanup of existing components and tech debt",
    "start": "1543429",
    "end": "1550059"
  },
  {
    "text": "Tim often likes to say that if you spend 15 20 minutes looking at any part of the codebase you'll find something that",
    "start": "1550059",
    "end": "1555280"
  },
  {
    "text": "looks unpleasant and it's fixing it took me a while to become familiar enough with a lot of things but I definitely",
    "start": "1555280",
    "end": "1561100"
  },
  {
    "text": "agree with that statement it's a big codebase that's lost its old its how programming works",
    "start": "1561100",
    "end": "1567240"
  },
  {
    "text": "that's also a good way to get involved because it's something where it's not very quote/unquote competitive not many",
    "start": "1567240",
    "end": "1574000"
  },
  {
    "text": "people are racing to solve the exact same issue and there's a huge scope between all the components in areas",
    "start": "1574000",
    "end": "1582480"
  },
  {
    "text": "there's ingress v1 GA so we're still deciding on exactly what tweaks were going to make to ingress before GA it so",
    "start": "1582840",
    "end": "1590230"
  },
  {
    "text": "there's a discussion on that and there's going to be a lot of work implementing that and slowly getting it out the door",
    "start": "1590230",
    "end": "1597090"
  },
  {
    "text": "there's a couple things around Louisville networking features like yes it's GA but still could do with some",
    "start": "1597090",
    "end": "1605940"
  },
  {
    "text": "tuning was work around dual stock IP before ipv6 and so on",
    "start": "1605940",
    "end": "1611600"
  },
  {
    "text": "there's always work to do so if you're interested in joining in I",
    "start": "1611600",
    "end": "1617900"
  },
  {
    "text": "have links here to our main page on github it's kind of like our man readme",
    "start": "1617900",
    "end": "1624210"
  },
  {
    "text": "of a reminder of what we do as well as where we talk so it's got the mailing",
    "start": "1624210",
    "end": "1630000"
  },
  {
    "text": "list links lock link things like that it's linked to from the main community SIG's page and honestly if you google",
    "start": "1630000",
    "end": "1636270"
  },
  {
    "text": "kubernetes sig network it should be the top result I admit that's shamefully often how I find the video link for",
    "start": "1636270",
    "end": "1641850"
  },
  {
    "text": "meetings so there's also our slack and our mailing list communications are kind of",
    "start": "1641850",
    "end": "1648420"
  },
  {
    "text": "split between slack and the mailing list almost based on preference and communication style and then every two",
    "start": "1648420",
    "end": "1655620"
  },
  {
    "text": "weeks we have our zoom call where we have a specific agenda of what we're going to deal with as well as",
    "start": "1655620",
    "end": "1662040"
  },
  {
    "text": "doing some just cleanup of our backlog going through some triage and",
    "start": "1662040",
    "end": "1668240"
  },
  {
    "text": "most of those should be posted on youtube if you want to go through the archive and kind of see what's going on",
    "start": "1668240",
    "end": "1673380"
  },
  {
    "text": "if you're planning on getting involved I strongly recommend either just mostly",
    "start": "1673380",
    "end": "1678900"
  },
  {
    "text": "silently attending some meetings are watching some YouTube videos to kind of get an idea of like our interactions",
    "start": "1678900",
    "end": "1684420"
  },
  {
    "text": "what's going on what we're working on and try to get up to speed they're getting a news feed on github can only",
    "start": "1684420",
    "end": "1689940"
  },
  {
    "text": "get you so far in my opinion that's it for the main content are there",
    "start": "1689940",
    "end": "1695880"
  },
  {
    "text": "any questions specifically around what we've covered here or getting about with the community Tim will be doing the deep",
    "start": "1695880",
    "end": "1701940"
  },
  {
    "text": "dive afterwards for the really nasty questions if you've got them",
    "start": "1701940",
    "end": "1708440"
  },
  {
    "text": "yep",
    "start": "1710780",
    "end": "1713780"
  },
  {
    "text": "sorry I can't hear you super well from up from up here I PBS improvements",
    "start": "1718430",
    "end": "1724250"
  },
  {
    "text": "um I don't specifically know I just know that that's an area that's been brought up",
    "start": "1726740",
    "end": "1732650"
  },
  {
    "text": "there's been a bunch of bugs in ipbs that we're finding like all good",
    "start": "1732650",
    "end": "1738150"
  },
  {
    "text": "software nobody uses it until it's released so now that it's GA people are starting to find the bugs with it and so",
    "start": "1738150",
    "end": "1745560"
  },
  {
    "text": "we're working through those bugs still they're not none of them have been super critical bugs but there there are nonzero bugs",
    "start": "1745560",
    "end": "1753169"
  },
  {
    "text": "yep a",
    "start": "1754520",
    "end": "1757790"
  },
  {
    "text": "separate traffic interface what do you mean by that I",
    "start": "1762050",
    "end": "1766670"
  },
  {
    "text": "can take that one the short answer is no",
    "start": "1774350",
    "end": "1780680"
  },
  {
    "text": "no the longer answer is I want I'm still learning I'm still hearing the requirements that what people are trying",
    "start": "1780680",
    "end": "1787890"
  },
  {
    "text": "to do with these multiple interfaces I don't have a like fundamental problem with the idea",
    "start": "1787890",
    "end": "1793610"
  },
  {
    "text": "concerned about how the implementation evolves and where it ends up",
    "start": "1793610",
    "end": "1799430"
  },
  {
    "text": "one of the things I will not ask is I keep finding like a lot of the ingress",
    "start": "1806309",
    "end": "1811509"
  },
  {
    "text": "is just one layer seven what what layer four like I have use cases where I want to go into the system looking to the",
    "start": "1811509",
    "end": "1816699"
  },
  {
    "text": "cluster like using layer four but node port doesn't really suit me because I",
    "start": "1816699",
    "end": "1821889"
  },
  {
    "text": "just wanna have more control of where the traffic goes to so is any plans to do is a load balancing for",
    "start": "1821889",
    "end": "1829869"
  },
  {
    "text": "layer four traffic",
    "start": "1829869",
    "end": "1832559"
  },
  {
    "text": "are there more plans to layer four uh load balancing is strictly outside the",
    "start": "1835439",
    "end": "1841719"
  },
  {
    "text": "bounds of what committees does so we have the internal cluster load balancer right which is intentionally more of a load sprayer than a load balancer",
    "start": "1841719",
    "end": "1848759"
  },
  {
    "text": "we when we talk about bringing stuff in from the outside which is what I think you're getting to we have to talk about which load",
    "start": "1848759",
    "end": "1856209"
  },
  {
    "text": "balancer are you using and how does that work and what does it understand if you hold that question I'm happy to I think",
    "start": "1856209",
    "end": "1862569"
  },
  {
    "text": "we've got some of the stuff in the deep dive where we talk about some of the stuff that's happening we talked a little bit about it at my talk yesterday",
    "start": "1862569",
    "end": "1869339"
  },
  {
    "text": "I'll be happy to go back over that yeah any questions on that I would love to",
    "start": "1869339",
    "end": "1875139"
  },
  {
    "text": "get see if anybody's got other questions on the the intro section and then we can get into the deep dive where we get a",
    "start": "1875139",
    "end": "1880179"
  },
  {
    "text": "little bit more more up to the elbows gory",
    "start": "1880179",
    "end": "1884459"
  },
  {
    "text": "if there's no cluster IP then yeah",
    "start": "1893250",
    "end": "1900679"
  },
  {
    "text": "I'm trying to remember what happens with that yes the short answer is yes we won't",
    "start": "1902780",
    "end": "1909030"
  },
  {
    "text": "allocate you an IP if you ask for none",
    "start": "1909030",
    "end": "1912980"
  },
  {
    "text": "all right any other questions",
    "start": "1925340",
    "end": "1928789"
  },
  {
    "text": "that's something that I'm looking at right now very broadly we're trying to",
    "start": "1942049",
    "end": "1947250"
  },
  {
    "text": "do that kind of thing across cabernet is getting different components split out",
    "start": "1947250",
    "end": "1953450"
  },
  {
    "text": "sorry I'm the question it was like are we splitting up coup proxy to its own",
    "start": "1953450",
    "end": "1958740"
  },
  {
    "text": "repository and the answer is almost certainly one day hopefully not in the",
    "start": "1958740",
    "end": "1964470"
  },
  {
    "text": "very distant future there's many many mechanics involved with that because we have an extreme number of dependencies",
    "start": "1964470",
    "end": "1971070"
  },
  {
    "text": "including kind of circular dependencies in the project so crude proxy should be theoretically easy in practice like",
    "start": "1971070",
    "end": "1977870"
  },
  {
    "text": "Lucas and I were stepping over it this afternoon oh there's some fun imports we're importing all kinds of stuff from",
    "start": "1977870",
    "end": "1983909"
  },
  {
    "text": "like API server and so on that we're going to need to detangle if we want to separate things",
    "start": "1983909",
    "end": "1990409"
  },
  {
    "text": "there already is an interface its services yeah cube services and implants",
    "start": "2011230",
    "end": "2016610"
  },
  {
    "text": "if you know weep if I can pontificate for a moment kubernetes we talk about controllers in",
    "start": "2016610",
    "end": "2022640"
  },
  {
    "text": "the controller pattern and how that really drives everything all cue proxy is is a controller for services and endpoints it watches services endpoints",
    "start": "2022640",
    "end": "2029690"
  },
  {
    "text": "and it makes them appear on a machine so if you can replace that with a different controller cool yeah I'll coup proxy is",
    "start": "2029690",
    "end": "2037400"
  },
  {
    "text": "doing is creating a cluster IP when it sees a service that should have a cluster IP and then populating end",
    "start": "2037400",
    "end": "2043700"
  },
  {
    "text": "points from the endpoints object when it sees endpoints so there's a lot of fan",
    "start": "2043700",
    "end": "2049610"
  },
  {
    "text": "dangling around in the plumbing of like using IP vs or using IP tables but fundamentally if you do like a flow",
    "start": "2049610",
    "end": "2056750"
  },
  {
    "text": "diagram of it it's doing a very very simple task",
    "start": "2056750",
    "end": "2061210"
  },
  {
    "text": "where do plugins actually live in the system",
    "start": "2086860",
    "end": "2092050"
  },
  {
    "text": "I've seen a number of different ways of installing plugins I'm honestly not sure what the best way to install a network",
    "start": "2094750",
    "end": "2100640"
  },
  {
    "text": "plug-in is I've seen people do things on the host I've seen people use CRT is the",
    "start": "2100640",
    "end": "2106120"
  },
  {
    "text": "the more abstract answer is we have an interface between cubelet on each node",
    "start": "2106120",
    "end": "2112490"
  },
  {
    "text": "and a plug-in and there's configuration that you feed to cubelet that tells it how to which plug-in to run there's an",
    "start": "2112490",
    "end": "2119930"
  },
  {
    "text": "interface called CNI which is defined it's an exec based interface right now I",
    "start": "2119930",
    "end": "2125060"
  },
  {
    "text": "will exec your plug-in with a set of comparable",
    "start": "2125060",
    "end": "2129820"
  },
  {
    "text": "so there's of course somebody who's going to get more pedantic than me it is",
    "start": "2130780",
    "end": "2136310"
  },
  {
    "text": "in fact in the container runtime interface which is called by cubelet and",
    "start": "2136310",
    "end": "2142330"
  },
  {
    "text": "that execs your plugin sets up the container interface and then returns the IP address that it allocated for you",
    "start": "2142330",
    "end": "2148720"
  },
  {
    "text": "just way down at the bottom of the stack I'm sorry",
    "start": "2148720",
    "end": "2156160"
  },
  {
    "text": "yeah so let's let's if that's the question is should we should be do questions or should we do the deep dive",
    "start": "2158140",
    "end": "2163989"
  },
  {
    "text": "I would love to get into the deep dive yeah we're at about time anyways yeah but there's other sort of intro",
    "start": "2163989",
    "end": "2170650"
  },
  {
    "text": "questions I'm happy to take those too I want to push ahead a little bit with the deep dive I have way more material than",
    "start": "2170650",
    "end": "2177130"
  },
  {
    "text": "I do time so we'll pick and choose what we talk about and we'll do questions as we go through there so we had a Signet",
    "start": "2177130",
    "end": "2183609"
  },
  {
    "text": "work meeting in person on Monday and we spent something like 2 or 3 hours in a room and we did not finish everything we",
    "start": "2183609",
    "end": "2189099"
  },
  {
    "text": "want to talk about so there's a lot going on thank you thank you",
    "start": "2189099",
    "end": "2197038"
  },
  {
    "text": "[Applause]",
    "start": "2199170",
    "end": "2202839"
  },
  {
    "text": "I",
    "start": "2215630",
    "end": "2217660"
  },
  {
    "text": "don't leave it's not it's not that scary",
    "start": "2225820",
    "end": "2230680"
  },
  {
    "text": "thank you all for coming here I honestly end-of-day last day of the conference I expected about five people to show up so",
    "start": "2230920",
    "end": "2239480"
  },
  {
    "text": "it's amazing they were probably all gonna be like people I know so thank you all for coming I have like I said sort",
    "start": "2239480",
    "end": "2247340"
  },
  {
    "text": "of way more material than I do time I picked a few topics from this stream of things that are happening and I thought",
    "start": "2247340",
    "end": "2253370"
  },
  {
    "text": "I'd go into them but if these are not really interesting we can also talk about other things so you know obviously",
    "start": "2253370",
    "end": "2259340"
  },
  {
    "text": "there's a lot of questions here so you know I love questions stop me just wave your hand and scream if you have a",
    "start": "2259340",
    "end": "2265970"
  },
  {
    "text": "question as I'm going through and we'll do it that so",
    "start": "2265970",
    "end": "2271150"
  },
  {
    "text": "landmark you guys might have seen this week we love to think about the network kubernetes as like this like it's almost",
    "start": "2271150",
    "end": "2279020"
  },
  {
    "text": "done but really I think of it more like this",
    "start": "2279020",
    "end": "2284410"
  },
  {
    "text": "there's we got a lot of work to do the general scaffolding is in place but there's a lot of finishing work",
    "start": "2284410",
    "end": "2291970"
  },
  {
    "text": "so I picked five topics for today I'm happy to talk about these or not talk about these as you see fit I could",
    "start": "2291970",
    "end": "2297950"
  },
  {
    "text": "probably do this in an hour and a half and I've got 40 ish minutes so and I",
    "start": "2297950",
    "end": "2303170"
  },
  {
    "text": "don't know if they're gonna kick us out at 5:20 but we'll see I was going to talk about the ingress version one so",
    "start": "2303170",
    "end": "2311180"
  },
  {
    "text": "anybody who's used ingress in kubernetes maybe has wondered why it's a v1 beta one for the last three and a half years",
    "start": "2311180",
    "end": "2318040"
  },
  {
    "text": "we're gonna talk about what we're going to do to move it into GA and call it version one I was going to talk a little",
    "start": "2318040",
    "end": "2323720"
  },
  {
    "text": "bit about a DNS per node cache that we're working on which will hopefully fix a lot of the problems that people",
    "start": "2323720",
    "end": "2330230"
  },
  {
    "text": "are having with DNS across the kubernetes clusters I was going to talk about service topology",
    "start": "2330230",
    "end": "2336110"
  },
  {
    "text": "the idea of having control over which endpoints of a service you choose to talk to based on where you exist like",
    "start": "2336110",
    "end": "2342740"
  },
  {
    "text": "same node I was going to give a quick update on IP VI ipv6 and dual stack",
    "start": "2342740",
    "end": "2348230"
  },
  {
    "text": "support and I was going to talk about the endpoints API reboot that is in flight the Valerie mentioned earlier I don't",
    "start": "2348230",
    "end": "2356210"
  },
  {
    "text": "have any particular preference order Oh apparently we have people waiting can we",
    "start": "2356210",
    "end": "2361970"
  },
  {
    "text": "again scooch in there's been a bunch of empty people who have left so can we have people sort of move in from the",
    "start": "2361970",
    "end": "2368119"
  },
  {
    "text": "outsides towards the middle if if there are empty seats between you",
    "start": "2368119",
    "end": "2374359"
  },
  {
    "text": "in the middle of the room please scooch thank you all",
    "start": "2374359",
    "end": "2380589"
  },
  {
    "text": "there was a bunch of people standing in the back so I'm gonna jump in here again if you",
    "start": "2380589",
    "end": "2387410"
  },
  {
    "text": "have questions please just raise your hands and scream so ingress version one",
    "start": "2387410",
    "end": "2392660"
  },
  {
    "text": "why hasn't grass been stuck at v1 beta one for the last three years wide why do we care now",
    "start": "2392660",
    "end": "2399579"
  },
  {
    "text": "so there's there's two main reasons one there's a big press to get rid of the",
    "start": "2399579",
    "end": "2405740"
  },
  {
    "text": "extensions API group which is where ingress lives we thought that was a good idea turns out it was not a good idea to",
    "start": "2405740",
    "end": "2412099"
  },
  {
    "text": "put things all in this extensions group we want to move it out of that group so we've started the process of moving out",
    "start": "2412099",
    "end": "2417800"
  },
  {
    "text": "we're basically the last API in the group and they want to turn the lights off but we're holding them back from",
    "start": "2417800",
    "end": "2423260"
  },
  {
    "text": "doing that we could replace it with an entirely new API which we actually really want to do",
    "start": "2423260",
    "end": "2428869"
  },
  {
    "text": "and you could see this in some of the other talks earlier this week but that's gonna take a really long time and like",
    "start": "2428869",
    "end": "2434900"
  },
  {
    "text": "let's be realistic you know to get an API design and approved by all the",
    "start": "2434900",
    "end": "2440569"
  },
  {
    "text": "implementations and then implemented and then through alpha beta GA is going to take a year or more the extensions a peg",
    "start": "2440569",
    "end": "2447230"
  },
  {
    "text": "group has to go away so we figured now is a really good time to push it through",
    "start": "2447230",
    "end": "2453369"
  },
  {
    "text": "the other reason here though is the perpetual beta status make some of our customers unhappy a lot of people are",
    "start": "2454480",
    "end": "2461390"
  },
  {
    "text": "using it and they they say what is it gonna go away are you going to delete it",
    "start": "2461390",
    "end": "2466520"
  },
  {
    "text": "are you gonna abandon it are you gonna change it in really fundamental ways",
    "start": "2466520",
    "end": "2471609"
  },
  {
    "text": "ingress has more users than there are people on earth and",
    "start": "2471819",
    "end": "2477250"
  },
  {
    "text": "about a dozen different implementations that I know of people are using it like it is a de",
    "start": "2477250",
    "end": "2483000"
  },
  {
    "text": "facto ga API so we're gonna just admit that right but you know if we're gonna change the API version we're gonna",
    "start": "2483000",
    "end": "2488820"
  },
  {
    "text": "change the API a little bit along the way because hey why why not so in the cap that Bowie wrote I don't",
    "start": "2488820",
    "end": "2496770"
  },
  {
    "text": "always hear but I the kept that we wrote for this we've proposed a half-dozen ish",
    "start": "2496770",
    "end": "2503220"
  },
  {
    "text": "things that we want to fix along the way for the API some of which are really sort of obvious and some of which are",
    "start": "2503220",
    "end": "2508560"
  },
  {
    "text": "actually really complicated and I'm gonna just dig into these a little bit if anybody's used ingress before maybe",
    "start": "2508560",
    "end": "2513600"
  },
  {
    "text": "you'll be nodding your head going yeah I needed that so the first one is the the path field",
    "start": "2513600",
    "end": "2520770"
  },
  {
    "text": "of the ingress type it's documented as a POSIX regex the number of controllers",
    "start": "2520770",
    "end": "2528180"
  },
  {
    "text": "that implement it as a positive edge X is zero",
    "start": "2528180",
    "end": "2534410"
  },
  {
    "text": "it is it has come to have implementation specific meaning so now as a user of an",
    "start": "2534410",
    "end": "2540359"
  },
  {
    "text": "abstract API you have to know what implementation you're using and how it treats this path that's not cool so it",
    "start": "2540359",
    "end": "2546570"
  },
  {
    "text": "is sort of simultaneously under specified and over specified so we are going to fix that in the v1",
    "start": "2546570",
    "end": "2553020"
  },
  {
    "text": "API and we're gonna fix it by defining it to a subset of the feature set that we can actually implement across all",
    "start": "2553020",
    "end": "2559020"
  },
  {
    "text": "these controllers specifically either exact match or prefix match and it gets",
    "start": "2559020",
    "end": "2565109"
  },
  {
    "text": "tricky though because kubernetes api is have a rule that you can round trip between versions without losing",
    "start": "2565109",
    "end": "2570150"
  },
  {
    "text": "information so if I define it as a prefix API and you round-trip it into the beta API and then back you could",
    "start": "2570150",
    "end": "2577260"
  },
  {
    "text": "lose information or if you defined it in the beta API and you bring it into the v1 API you could have a field that",
    "start": "2577260",
    "end": "2584130"
  },
  {
    "text": "doesn't work right it could fail validation so we've got to define it a little bit carefully and how we can round-trip that the exact details of the",
    "start": "2584130",
    "end": "2590790"
  },
  {
    "text": "API are still sort of being decided but this is a pretty straightforward one",
    "start": "2590790",
    "end": "2597410"
  },
  {
    "text": "also in the ingress specification we have a host field and people have asked",
    "start": "2597650",
    "end": "2603180"
  },
  {
    "text": "us time immemorial does it support wild cards and we didn't specify it to support wild cards I don't know if any",
    "start": "2603180",
    "end": "2609450"
  },
  {
    "text": "of the implementations support wild cards but it's come up often enough and it seems to be supported all of the",
    "start": "2609450",
    "end": "2614830"
  },
  {
    "text": "implementations we're gonna make sure that it does support simple wildcards for host names",
    "start": "2614830",
    "end": "2620730"
  },
  {
    "text": "ingress back-end this is a really easy one I've had so many people ask me what is this field for I don't understand",
    "start": "2622050",
    "end": "2628150"
  },
  {
    "text": "what this field is for well it's your default back-end why is it called back-end it's the default back-end so we're just",
    "start": "2628150",
    "end": "2634720"
  },
  {
    "text": "gonna rename it I don't know what we're gonna name it but probably default back in",
    "start": "2634720",
    "end": "2640589"
  },
  {
    "text": "so anybody here runs ingress in like a cloud provider and ends up running to",
    "start": "2641970",
    "end": "2647050"
  },
  {
    "text": "ingresses yeah yeah and now you have to specify this annotation that tells it",
    "start": "2647050",
    "end": "2653140"
  },
  {
    "text": "which ingress you want to use on a particular ingress instance yeah so we're gonna formalize that we have this",
    "start": "2653140",
    "end": "2659200"
  },
  {
    "text": "pattern within kubernetes of a something class which is a way for your cluster administrator to advertise to users what",
    "start": "2659200",
    "end": "2666010"
  },
  {
    "text": "types of a thing are available we see this in like storage class so we're gonna follow that same pattern and",
    "start": "2666010",
    "end": "2672400"
  },
  {
    "text": "implement an ingress class so you'll be able to specify I want to ingress and I want the cheap one or I want an ingress",
    "start": "2672400",
    "end": "2678850"
  },
  {
    "text": "and I want the good one and you don't have to actually know what that means you don't even know whether that's",
    "start": "2678850",
    "end": "2684280"
  },
  {
    "text": "Google or nginx and you can decide which one of those is good which one of those is cheap and as an administrator you can",
    "start": "2684280",
    "end": "2690190"
  },
  {
    "text": "actually change this so you can move it to a different cluster like a different topology a different cloud provider and",
    "start": "2690190",
    "end": "2695200"
  },
  {
    "text": "not have to worry about your users having to change this annotation right so better Portability",
    "start": "2695200",
    "end": "2700920"
  },
  {
    "text": "basically it's this the users can now go shopping they can say hey what ingress classes are actually available in my",
    "start": "2700920",
    "end": "2706450"
  },
  {
    "text": "cluster we give them back a list of ingress classes I feel like I had to have pictures in the slides and then",
    "start": "2706450",
    "end": "2712240"
  },
  {
    "text": "they can go create their ingress with an abstract class and now the controllers",
    "start": "2712240",
    "end": "2717250"
  },
  {
    "text": "aren't gonna fight over it anymore ingress status this is a basically",
    "start": "2717250",
    "end": "2723640"
  },
  {
    "text": "non-existent we've had a lot of requests for people to get feedback from their ingress implementation back to the users",
    "start": "2723640",
    "end": "2730270"
  },
  {
    "text": "about what's happening or why or why it's not happening most of the controllers will throw events into",
    "start": "2730270",
    "end": "2735520"
  },
  {
    "text": "kubernetes and you can sort of see that way what's happening but events are not a programmatic API they're more of a human API so",
    "start": "2735520",
    "end": "2742850"
  },
  {
    "text": "then that wouldn't be an API there a human interface so so we're gonna add a status field we're working",
    "start": "2742850",
    "end": "2748880"
  },
  {
    "text": "through the details of whether we're going to implement it as a CR D with a",
    "start": "2748880",
    "end": "2754130"
  },
  {
    "text": "type specific details or whether it'll be sort of a string to string map that's just generic to carry some information",
    "start": "2754130",
    "end": "2760010"
  },
  {
    "text": "but we've got options but I think a status field is pretty important at this point",
    "start": "2760010",
    "end": "2766540"
  },
  {
    "text": "health checks let me do something",
    "start": "2766540",
    "end": "2774220"
  },
  {
    "text": "sorry health checks so every cloud API out",
    "start": "2774490",
    "end": "2780860"
  },
  {
    "text": "there if you're like Google cloud Amazon whatever you're setting up your load balancers and the API requires you to",
    "start": "2780860",
    "end": "2787880"
  },
  {
    "text": "establish a health check right and ingress doesn't have any way for you to specify what that health check is so in",
    "start": "2787880",
    "end": "2794570"
  },
  {
    "text": "most cases we just assume that it'll be the /url and that'll be fine we'll hit your /url and if you return 200 you must",
    "start": "2794570",
    "end": "2800840"
  },
  {
    "text": "be alive right and that works for a lot of people but also not for a lot of",
    "start": "2800840",
    "end": "2806240"
  },
  {
    "text": "people and so some implications go even farther and say well I'll look at the service and I'll look at the pods behind",
    "start": "2806240",
    "end": "2812720"
  },
  {
    "text": "the service and I'll look and see if there's a readiness probe and if there's a readiness probe on the pod then I'll assume when it's type HTTP then I'll",
    "start": "2812720",
    "end": "2819440"
  },
  {
    "text": "take that path and I'll use it in my load balancer instead if any of you followed that I'm impressed",
    "start": "2819440",
    "end": "2824720"
  },
  {
    "text": "it's kind of a mess it explodes far more often than I'm happy about so I think we're going to try to formalize a just a",
    "start": "2824720",
    "end": "2832100"
  },
  {
    "text": "very simple concept of if this is an ingress what is the path to the health check for this for this back-end service",
    "start": "2832100",
    "end": "2838330"
  },
  {
    "text": "and protocol you mean scheme like HTTP HTTPS okay",
    "start": "2838330",
    "end": "2845590"
  },
  {
    "text": "and an expected error code I think Bowie Bowie are you taking notes",
    "start": "2847030",
    "end": "2852579"
  },
  {
    "text": "so interesting we'll have to we'll have to work through the details we don't act expected error code is something that's been asked for before on readiness",
    "start": "2852579",
    "end": "2859630"
  },
  {
    "text": "probes and we've so far declined to implement that mostly because if you have control of it",
    "start": "2859630",
    "end": "2866260"
  },
  {
    "text": "why don't you just convert it like we're at a new URL path but I would love to hear the the use cases for why we should",
    "start": "2866260",
    "end": "2873160"
  },
  {
    "text": "add every new field we add to the API I'm gonna fight over so",
    "start": "2873160",
    "end": "2878460"
  },
  {
    "text": "yes yes correct so that's as an excellent point",
    "start": "2882839",
    "end": "2888369"
  },
  {
    "text": "thank you for saying that I'm not trying to make a new ingress API here I'm trying to make sure that the one we've",
    "start": "2888369",
    "end": "2894339"
  },
  {
    "text": "got the one that everybody's out there is using is stable and reliable and we",
    "start": "2894339",
    "end": "2899950"
  },
  {
    "text": "can call it GA with a straight face right we'll do a version two and the version",
    "start": "2899950",
    "end": "2905500"
  },
  {
    "text": "two will be so amazing you can't imagine what version one was ever useful for but that's next cube cond",
    "start": "2905500",
    "end": "2912779"
  },
  {
    "text": "so another thing that comes up a lot is the ability to have a load balancer that",
    "start": "2912779",
    "end": "2918099"
  },
  {
    "text": "fronts things that are not services there's an assumption today that everything behind and ingress is a kubernetes service and we've had",
    "start": "2918099",
    "end": "2924640"
  },
  {
    "text": "requests for well I'm gonna target an external host like I want to add as act as a proxy to another host or I want to",
    "start": "2924640",
    "end": "2931809"
  },
  {
    "text": "target a storage back-end like in Google cloud you can set up a GCS bucket I think in Amazon you can set up an s3",
    "start": "2931809",
    "end": "2937390"
  },
  {
    "text": "bucket I want to be able to target those things from my from my ingress and because of the lack of capability there",
    "start": "2937390",
    "end": "2944049"
  },
  {
    "text": "people end up going behind the scenes into the the underlying cloud and changing things and then they get really",
    "start": "2944049",
    "end": "2949630"
  },
  {
    "text": "mad when we undo it through the ingress controller and they call us and they yell at my boss so I'm proposing that we",
    "start": "2949630",
    "end": "2956380"
  },
  {
    "text": "fix this again the details of the API are to be decided but we've got this lovely CRD pattern that we use in a",
    "start": "2956380",
    "end": "2962289"
  },
  {
    "text": "million places in kubernetes now this feels like a ripe opportunity to say you know my back-end is this other thing and",
    "start": "2962289",
    "end": "2968650"
  },
  {
    "text": "if you understand what this other thing is then you can implement it",
    "start": "2968650",
    "end": "2974220"
  },
  {
    "text": "it comes up often enough that when we made the list of things to fix this was on the list of like top 10 yeah",
    "start": "2975359",
    "end": "2983999"
  },
  {
    "text": "now to be fair there's also a list of things that we're not gonna fix right now this may fall between those two",
    "start": "2983999",
    "end": "2990339"
  },
  {
    "text": "depending on how hard it is to implement",
    "start": "2990339",
    "end": "2993569"
  },
  {
    "text": "yeah yeah I don't want it to become the full feature set of h8 proxy but I do want to make sure that it's useful for",
    "start": "2996449",
    "end": "3002729"
  },
  {
    "text": "the people who are actually using it right it may be that what we do with this is just leave room for it and come",
    "start": "3002729",
    "end": "3009029"
  },
  {
    "text": "back to it later right but today there's just no room for it so there's a list of things that we can",
    "start": "3009029",
    "end": "3015209"
  },
  {
    "text": "fix at a later point things like I want my backends to all be",
    "start": "3015209",
    "end": "3020369"
  },
  {
    "text": "HTTPS right a bunch of the implementations have annotations that they support for this it seems like a relatively simple thing but it didn't",
    "start": "3020369",
    "end": "3026969"
  },
  {
    "text": "come up in the most critical list of things to fix and it also seems like it's relatively easy to fix later it's",
    "start": "3026969",
    "end": "3033239"
  },
  {
    "text": "strictly additive right we're not changing things we've had a lot of questions about restricting host names like I want you",
    "start": "3033239",
    "end": "3040229"
  },
  {
    "text": "to be able to set up an ingress but I don't want you to be able to set up an ingress for google.com right and so",
    "start": "3040229",
    "end": "3045749"
  },
  {
    "text": "maybe a way to sort of grant you access to some host names to delegate down to a namespace",
    "start": "3045749",
    "end": "3052249"
  },
  {
    "text": "per back-end timeouts this comes up a lot there's a lot of annotations for this I think this is one of the most",
    "start": "3052249",
    "end": "3058289"
  },
  {
    "text": "popular nginx annotations that people use and oh sorry I messed up the last",
    "start": "3058289",
    "end": "3065309"
  },
  {
    "text": "one is the backend protocol HTTP the first one is only HTTPS like don't take",
    "start": "3065309",
    "end": "3070589"
  },
  {
    "text": "HTTP traffic right this comes up surprisingly often too and then we have",
    "start": "3070589",
    "end": "3076380"
  },
  {
    "text": "a short list of things that are interesting and exciting but really hard to fix without fundamental changes to",
    "start": "3076380",
    "end": "3082140"
  },
  {
    "text": "the API so automatic protocol upgrades like the way ingress is currently implemented with",
    "start": "3082140",
    "end": "3089190"
  },
  {
    "text": "all the cloud providers it's very difficult to guarantee that this is going to work on all the implementations we may just punt that one to the next",
    "start": "3089190",
    "end": "3095249"
  },
  {
    "text": "version of the API cross namespace secrets there's a massive potential security hole there",
    "start": "3095249",
    "end": "3101459"
  },
  {
    "text": "but I also understand the use case for people having shared secret excuse me shared secrets that they want to use",
    "start": "3101459",
    "end": "3108180"
  },
  {
    "text": "their organization without having to copy them into every namespace is there a question no",
    "start": "3108180",
    "end": "3114859"
  },
  {
    "text": "back end affinity there are so many different implementations of back end affinity that it's hard to describe an",
    "start": "3114859",
    "end": "3120750"
  },
  {
    "text": "abstract API for that there's so many things that we'd love to add like those other things but that",
    "start": "3120750",
    "end": "3127380"
  },
  {
    "text": "there would be optional this was an idea was why don't we add them and just say that they're optional right and I'm really concerned about the portability",
    "start": "3127380",
    "end": "3133530"
  },
  {
    "text": "of this I really don't want to add features that people use that would couple them to their implementation",
    "start": "3133530",
    "end": "3139170"
  },
  {
    "text": "without them being aware that they're choosing to get couple to their implementation so we've discussed a",
    "start": "3139170",
    "end": "3145319"
  },
  {
    "text": "little bit of how how would we make like rings of conformance like there's a core",
    "start": "3145319",
    "end": "3150720"
  },
  {
    "text": "set that everybody implements there's an optional set that you can choose to implement and then there's other stuff that the implementations are specific I",
    "start": "3150720",
    "end": "3157890"
  },
  {
    "text": "don't know how to do that yet and I don't think it fits really well into the API that we've got and I don't want to do a fundamentally new API today",
    "start": "3157890",
    "end": "3166970"
  },
  {
    "text": "explicit sharing or non sharing in my piece this has come up a lot like Google Cloud ingress will Caray will create a",
    "start": "3167000",
    "end": "3172859"
  },
  {
    "text": "new IP address for every ingress and the engine s engine X implementation will share an IP address for every ingress",
    "start": "3172859",
    "end": "3178710"
  },
  {
    "text": "and what if I want the opposite of that right on a GCP instance I could share an IP address across ingress is and save",
    "start": "3178710",
    "end": "3184950"
  },
  {
    "text": "some money on nginx maybe I want a dedicated IP address so I can set up separate firewall rules or other things",
    "start": "3184950",
    "end": "3191540"
  },
  {
    "text": "it's right now it's not possible to specify that that's not gonna fit within this API and then TCP support there's",
    "start": "3191540",
    "end": "3198359"
  },
  {
    "text": "really not a ton of demand for this so far we tried to leave room for it but we didn't spend a lot of time thinking",
    "start": "3198359",
    "end": "3203400"
  },
  {
    "text": "about it so I just want to sort of put on the record I don't think we're gonna do it in ingress at this point I'm just",
    "start": "3203400",
    "end": "3209160"
  },
  {
    "text": "not giving it any thought and so I'm probably gonna make mistakes that make it hard to do",
    "start": "3209160",
    "end": "3215270"
  },
  {
    "text": "so the status of this is is at the kept stage",
    "start": "3215270",
    "end": "3220130"
  },
  {
    "text": "implement eight so there is an imp TCP support at the nginx ingress I'm saying I'm not gonna formalize it I'm not gonna",
    "start": "3224850",
    "end": "3231130"
  },
  {
    "text": "take it away from the implementations but I'm just not gonna formalize it as part of the API",
    "start": "3231130",
    "end": "3236520"
  },
  {
    "text": "oops it is at the kept stage the enhancement proposal stage you can find it I made some short links because I",
    "start": "3236520",
    "end": "3242500"
  },
  {
    "text": "don't like pasting three lines of link but it is at a kept stage you can go",
    "start": "3242500",
    "end": "3247750"
  },
  {
    "text": "take a look at the proposal there's some debate about the exact shape of the API is if you're an ingress user I saw a lot",
    "start": "3247750",
    "end": "3253210"
  },
  {
    "text": "of hands in here and you're interested in these things please come and weigh in and tell us what you think about these",
    "start": "3253210",
    "end": "3258430"
  },
  {
    "text": "requirements for example protocol we'd love to get those on the record because honestly I'll probably forget it",
    "start": "3258430",
    "end": "3264220"
  },
  {
    "text": "between now and the time I get back to California remember that the round-trip requirement",
    "start": "3264220",
    "end": "3269560"
  },
  {
    "text": "is maybe the the hardest part of all of this so anything we add has to be able to round-trip",
    "start": "3269560",
    "end": "3275070"
  },
  {
    "text": "definitely could use help especially if for people who are implementing ingress is if you're hearing you're an",
    "start": "3275070",
    "end": "3280330"
  },
  {
    "text": "implementer I would love to hear your thoughts about how this will map to you questions on",
    "start": "3280330",
    "end": "3287970"
  },
  {
    "text": "yes so the question is",
    "start": "3301270",
    "end": "3306850"
  },
  {
    "text": "nginx implementation does support regex as a path so I'm not going to take away the implementation specific option but",
    "start": "3306850",
    "end": "3313040"
  },
  {
    "text": "I'm gonna make the default be more consistently portable so the new API for example I'm not saying this is the API",
    "start": "3313040",
    "end": "3319460"
  },
  {
    "text": "but it might look like imagine there were three separate fields you have to pick one path prefix implementation",
    "start": "3319460",
    "end": "3325970"
  },
  {
    "text": "specific and in the implementation specific you're saying well I know nginx I know this can be interpreted as a",
    "start": "3325970",
    "end": "3331370"
  },
  {
    "text": "regex so that's fine right and then I can round-trip it through the from the beta if you specified path in the beta",
    "start": "3331370",
    "end": "3338270"
  },
  {
    "text": "that's gonna go straight to the implementation specific field and in the new API if you specified the the proper",
    "start": "3338270",
    "end": "3344840"
  },
  {
    "text": "path that'll go to a new field in the beta so we'll have to map and convert them but I'm not gonna take it I'm not",
    "start": "3344840",
    "end": "3349940"
  },
  {
    "text": "gonna take it away the intention here is not to break anybody right it's actually the opposite of the intention the intention is to not break people",
    "start": "3349940",
    "end": "3359590"
  },
  {
    "text": "yes I think la is probably here in the room or he was here at the conference anyway so question",
    "start": "3360790",
    "end": "3368710"
  },
  {
    "text": "right so the the question was do I some of the",
    "start": "3386940",
    "end": "3393970"
  },
  {
    "text": "implementations are already starting to do their own CR DS because we haven't had a lot of this and do I have a",
    "start": "3393970",
    "end": "3399760"
  },
  {
    "text": "comparison of this versus the CR DS as we prepared for this we we asked",
    "start": "3399760",
    "end": "3404830"
  },
  {
    "text": "Alejandro the nginx maintainer what are the top most used nginx annotations the",
    "start": "3404830",
    "end": "3411340"
  },
  {
    "text": "sort of crazy things that people are doing that are outside of the ingress scope and we got a list he actually was great he gave us a fold like sorted list",
    "start": "3411340",
    "end": "3417970"
  },
  {
    "text": "and we went through those and that's what informed these these things so I would imagine that these are the same",
    "start": "3417970",
    "end": "3423670"
  },
  {
    "text": "things that are going into those CRTs I haven't looked at the specific CRTs if you have a link to one of the CR DS that's being developed I'd love to see",
    "start": "3423670",
    "end": "3430000"
  },
  {
    "text": "it traffic splitting right so traffic splitting I think would fall into the",
    "start": "3430000",
    "end": "3435940"
  },
  {
    "text": "heart enough to implement in the existing API that it probably wouldn't be in this API it would be in like the the v2 API but now we are looking at how",
    "start": "3435940",
    "end": "3443590"
  },
  {
    "text": "to do exactly those sorts of things in the v2 API where we are considering you know maybe",
    "start": "3443590",
    "end": "3448840"
  },
  {
    "text": "those old cloud provider implementations that that can to implement these features like maybe they don't count anymore right one of the advantages of",
    "start": "3448840",
    "end": "3456520"
  },
  {
    "text": "having two generic ingress API over a bunch of implementation specific API is is you can then very simply iterate over",
    "start": "3456520",
    "end": "3464110"
  },
  {
    "text": "all the ingress --is in your cluster if I have an engine X ingress and a traffic ingress and a Google ingress and an",
    "start": "3464110",
    "end": "3470110"
  },
  {
    "text": "Amazon ingress it's very hard for me to generically say show me all the ingress --is so this is the reason that I'm",
    "start": "3470110",
    "end": "3476110"
  },
  {
    "text": "trying to keep the generic one usable question",
    "start": "3476110",
    "end": "3480600"
  },
  {
    "text": "any-anything 4g RPC no i haven't put any thought into how a RPC would fit with the existing ingress again for the v2",
    "start": "3485170",
    "end": "3492050"
  },
  {
    "text": "version I would love to think about if there's more we can do there yeah",
    "start": "3492050",
    "end": "3498370"
  },
  {
    "text": "so I said behind every ingress as a service but a lot of the ingresses actually go to the backend pods how do",
    "start": "3505060",
    "end": "3510290"
  },
  {
    "text": "they find those pods via the endpoints which are related to the service so that's what I don't mean",
    "start": "3510290",
    "end": "3517160"
  },
  {
    "text": "they actually go through the service IP they go through the service concept right through the grouping concert services like three api's and one the",
    "start": "3517160",
    "end": "3525200"
  },
  {
    "text": "you know the there's the the grouping concept they goes into concept and the",
    "start": "3525200",
    "end": "3530300"
  },
  {
    "text": "policy concept there's another question over here no all right",
    "start": "3530300",
    "end": "3536590"
  },
  {
    "text": "let's talk about dns anybody's ever had a problem with dns and their kubernetes clusters oh ouch",
    "start": "3536590",
    "end": "3543280"
  },
  {
    "text": "so we've we've heard you there's some problems they tell me if",
    "start": "3543280",
    "end": "3549320"
  },
  {
    "text": "you've heard these before UDP can drop packets connection tracking for UDP tends to",
    "start": "3549320",
    "end": "3555410"
  },
  {
    "text": "fill up connection tracking tables because all we can do is time them out there's no connection to track and it",
    "start": "3555410",
    "end": "3561950"
  },
  {
    "text": "turns out most dns connections are open a connection send a request get a respond or like open a socket send a",
    "start": "3561950",
    "end": "3567560"
  },
  {
    "text": "connection get a response do nothing next time open a new socket send a request get a response you fill the",
    "start": "3567560",
    "end": "3574070"
  },
  {
    "text": "connection tracking table with these single-use entries that have like a two minute timeout and it's it's terrible",
    "start": "3574070",
    "end": "3580540"
  },
  {
    "text": "there are actually bugs in the UDP connection tracking logic in the kernel which have been sort of highlighted over",
    "start": "3580540",
    "end": "3587120"
  },
  {
    "text": "the last year as a source of major problems where if two connection tracking records try to get written at",
    "start": "3587120",
    "end": "3592610"
  },
  {
    "text": "roughly the same time they can contend and one of them just gets lost and if you were hoping for a DNS response on",
    "start": "3592610",
    "end": "3598730"
  },
  {
    "text": "that you're going to be disappointed and of course anybody knows how long the DNS",
    "start": "3598730",
    "end": "3603830"
  },
  {
    "text": "timeout is by default five seconds so if you experience these",
    "start": "3603830",
    "end": "3609620"
  },
  {
    "text": "latency graphs where your latency is really good and you got this outlier five seconds they were 5.000 one right",
    "start": "3609620",
    "end": "3615570"
  },
  {
    "text": "that's me and you know DNS we run DNS in the",
    "start": "3615570",
    "end": "3621360"
  },
  {
    "text": "cluster for kubernetes services and unfortunately even though we try to auto scale it you have to at some point be",
    "start": "3621360",
    "end": "3627780"
  },
  {
    "text": "aware of it right because we can't get all the coefficients right for your particular use case so we try to tune it",
    "start": "3627780",
    "end": "3633060"
  },
  {
    "text": "for the average use case and it turns out for a lot of our larger customers they end up having to either add more",
    "start": "3633060",
    "end": "3638340"
  },
  {
    "text": "replicas or change the size of their DNS or the the memory allocations for their DNS which isn't really a fun experience",
    "start": "3638340",
    "end": "3645690"
  },
  {
    "text": "for them to have and the last problem which was really surprising to me but once I took it apart not-so-surprising",
    "start": "3645690",
    "end": "3652370"
  },
  {
    "text": "upstream DNS servers tend to have quotas based on source IP and if we fan in all",
    "start": "3652370",
    "end": "3658200"
  },
  {
    "text": "the DNS queries into our DNS server and then use that to proxy to the upstream DNS we eventually hit throttling and all",
    "start": "3658200",
    "end": "3664950"
  },
  {
    "text": "of your DNS in your cluster gets gets slowed down right the idea was originally all these if you have a",
    "start": "3664950",
    "end": "3670290"
  },
  {
    "text": "thousand VMs and a thousand VMs are asking for DNS find no problem but if you fan in a thousand to one IP it turns",
    "start": "3670290",
    "end": "3677310"
  },
  {
    "text": "out to be really bad so we want to fix all of these problems in one fell swoop",
    "start": "3677310",
    "end": "3682970"
  },
  {
    "text": "we want to lower the latency we want to get a high cache hit rate we want to get rid of connection tracking for DNS",
    "start": "3683090",
    "end": "3689040"
  },
  {
    "text": "altogether avoid the bugs avoid the timeouts I want to use TCP to the upstream DNS server so",
    "start": "3689040",
    "end": "3696030"
  },
  {
    "text": "we don't have to think about whether there was a lost back in and I want to distribute all the load among those",
    "start": "3696030",
    "end": "3701490"
  },
  {
    "text": "those thousand VMs so there's a design and alpha implementation that's ready to",
    "start": "3701490",
    "end": "3707580"
  },
  {
    "text": "go beta I gave the buried the lede we have a per node daemon set cache that",
    "start": "3707580",
    "end": "3714180"
  },
  {
    "text": "you can run it's optional you don't have to run it but if you do run it it's it's core DNS with a tiny little cache it's",
    "start": "3714180",
    "end": "3720210"
  },
  {
    "text": "like a couple megabytes of cache the cache size isn't the thing that actually matters here it's the locality",
    "start": "3720210",
    "end": "3725870"
  },
  {
    "text": "when you create your pods you pass that IP address and we create a dummy IP address on the machine you pass that to",
    "start": "3725870",
    "end": "3731730"
  },
  {
    "text": "cubelet which passes it to all the pods so when your pods start up it's going to talk to the local cache first if we",
    "start": "3731730",
    "end": "3737760"
  },
  {
    "text": "don't find a hit in the cache we'll upstream to the cluster DNS server just like we always did except we'll upstream",
    "start": "3737760",
    "end": "3743910"
  },
  {
    "text": "over TCP and we'll configure IP Able's to ignore any connections to that local cache so now all of the UDP connections",
    "start": "3743910",
    "end": "3751390"
  },
  {
    "text": "are not going to be connection tracked and we don't have to deal with almost all the bugs we want and just like that",
    "start": "3751390",
    "end": "3757750"
  },
  {
    "text": "we've seen a really great response from the people who are using this that it's made a ton of their bugs go away it's",
    "start": "3757750",
    "end": "3763330"
  },
  {
    "text": "made their latency more predictable they're no longer experiencing these five-second timeouts everybody's happy",
    "start": "3763330",
    "end": "3769020"
  },
  {
    "text": "as I so it is alpha now we're moving to beta with very minimal changes the",
    "start": "3769020",
    "end": "3775600"
  },
  {
    "text": "results look really fantastic so if you've had DNS problems you might take a look at this it should go beta in 115",
    "start": "3775600",
    "end": "3783040"
  },
  {
    "text": "what's next 115 we're at 15 releases already and take a look at this Pavitra",
    "start": "3783040",
    "end": "3790210"
  },
  {
    "text": "I don't know if she's here today she's here at the condo he's in the back she's doing all the work there if you have questions I'm sure she'll be happy to",
    "start": "3790210",
    "end": "3796450"
  },
  {
    "text": "talk to you afterwards in the future the things we're looking at you know we're thinking about H a",
    "start": "3796450",
    "end": "3801520"
  },
  {
    "text": "once you put a node agent in the data path things get squirrely if it goes down you can affect everybody on the",
    "start": "3801520",
    "end": "3807190"
  },
  {
    "text": "machine so we're looking at how to make that a little more reliable and we've got some future work that we're looking",
    "start": "3807190",
    "end": "3812980"
  },
  {
    "text": "at once we have this that could we could enable things like automatic path expansion on the DNS server because we",
    "start": "3812980",
    "end": "3818110"
  },
  {
    "text": "could use the cache as an intermediary for a more customized upstream protocol",
    "start": "3818110",
    "end": "3824280"
  },
  {
    "text": "service topology other questions so far before I get in that all right",
    "start": "3824280",
    "end": "3830530"
  },
  {
    "text": "how are we for time time is it",
    "start": "3830530",
    "end": "3835650"
  },
  {
    "text": "all right we got it we got about 20 minutes awesome service topology how",
    "start": "3835650",
    "end": "3840970"
  },
  {
    "text": "many people here have ever filed a bug asking for node local services same node",
    "start": "3840970",
    "end": "3846280"
  },
  {
    "text": "services I know there's at least a hundred of you because there's thousands of these bugs right",
    "start": "3846280",
    "end": "3853020"
  },
  {
    "text": "many times we have people who want to access this service but they want to",
    "start": "3853920",
    "end": "3858940"
  },
  {
    "text": "access the one that's on the same machine that they are right whether that's logging or monitoring or whatever it's important that they access the one",
    "start": "3858940",
    "end": "3865030"
  },
  {
    "text": "that's on the same machine there's another use case though like we have clusters within Google and and",
    "start": "3865030",
    "end": "3870610"
  },
  {
    "text": "other cloud providers that are spread across different zones within a region right so you get better failure modes if",
    "start": "3870610",
    "end": "3876580"
  },
  {
    "text": "one's whole zone goes out you're going to be survive that outed but traffic between zones is charged",
    "start": "3876580",
    "end": "3883359"
  },
  {
    "text": "traffic within zone is free and so people say well I would like to access the service that's in my zone as long as",
    "start": "3883359",
    "end": "3889749"
  },
  {
    "text": "there is one right seems like a totally reasonable thing to do and it also helps you to maintain failure domains right so",
    "start": "3889749",
    "end": "3896529"
  },
  {
    "text": "if you set up topology and say look I'm always gonna access the one that's in my rack right or the top of rack switch or",
    "start": "3896529",
    "end": "3902229"
  },
  {
    "text": "whatever your whatever topology you have because topology is really an arbitrary thing if that racks which goes out",
    "start": "3902229",
    "end": "3907539"
  },
  {
    "text": "you'll be affected but you're back into your back and will be dead but you'll be dead too so who cares",
    "start": "3907539",
    "end": "3913499"
  },
  {
    "text": "so the design of this we actually we had a kept on like a year ago and then we",
    "start": "3913499",
    "end": "3919599"
  },
  {
    "text": "said hold on we got to go and investigate this it might be bigger than this and we went off and we investigated",
    "start": "3919599",
    "end": "3924640"
  },
  {
    "text": "it I don't know if John is here but John did a bunch of investigation and he came back and said good news the simplest",
    "start": "3924640",
    "end": "3930339"
  },
  {
    "text": "possible API seems like the best one which is always my favorite sentence in the world it's one field it's an array",
    "start": "3930339",
    "end": "3938199"
  },
  {
    "text": "of strings and the idea is that you specify a list of topology keys that you care about what",
    "start": "3938199",
    "end": "3945069"
  },
  {
    "text": "do I mean by topology keys they are label keys on the node that you are running on so all nodes are labeled with",
    "start": "3945069",
    "end": "3952150"
  },
  {
    "text": "kubernetes io / hostname I can now specify my topology keys to polymerase",
    "start": "3952150",
    "end": "3958420"
  },
  {
    "text": "at io / hostname now it will always try to find me a back-end on my same hostname yeah you can also specify",
    "start": "3958420",
    "end": "3965650"
  },
  {
    "text": "create topology dr. Bernie's that I ozone and it will always try to find you one in your same zone and if at the end",
    "start": "3965650",
    "end": "3970989"
  },
  {
    "text": "of the list you know you decide well I don't care pick me one then you can put a star and it'll pick you one from any",
    "start": "3970989",
    "end": "3976569"
  },
  {
    "text": "from any topology that you can get to but it's nice because you can actually choose to fail well rather than pick",
    "start": "3976569",
    "end": "3981969"
  },
  {
    "text": "something in a different region or different rack or whatever right so you have your choice on this so here's just",
    "start": "3981969",
    "end": "3988299"
  },
  {
    "text": "a quick example where you can throw your topology keys here and say always pick me one on the same zone on the same node",
    "start": "3988299",
    "end": "3993579"
  },
  {
    "text": "right I'm gonna look at your hostname I'm gonna look at the target hostname in this case its hostname is really easy or",
    "start": "3993579",
    "end": "3999400"
  },
  {
    "text": "pick me one if there's none available on my host name then pick me one in the same zone and if there's none available",
    "start": "3999400",
    "end": "4004709"
  },
  {
    "text": "in my zone then pick any end point",
    "start": "4004709",
    "end": "4008719"
  },
  {
    "text": "yet question",
    "start": "4010190",
    "end": "4013609"
  },
  {
    "text": "so how will services know the client requests so the good thing about services being implemented on every node",
    "start": "4022220",
    "end": "4028500"
  },
  {
    "text": "is they know what node you came from because you're coming from the same node so I'm going to assume if I'm looking",
    "start": "4028500",
    "end": "4034950"
  },
  {
    "text": "for the same hostname I'm looking for an endpoint that has a hostname that matches me right or if I'm looking for the same zone I'm looking for an",
    "start": "4034950",
    "end": "4040740"
  },
  {
    "text": "endpoint that has a zone that matches me it gets interesting when we talk about",
    "start": "4040740",
    "end": "4046400"
  },
  {
    "text": "DNS here to get through all this yeah so it's interesting when you get into the design because things like DNS with a",
    "start": "4046400",
    "end": "4053130"
  },
  {
    "text": "headless service you want to give you a response that only includes the IPS that you would have gotten if you had",
    "start": "4053130",
    "end": "4059040"
  },
  {
    "text": "accessed the virtual IP but DNS doesn't know who you are right so we have to",
    "start": "4059040",
    "end": "4064230"
  },
  {
    "text": "build a reverse mapping thing so we've got a couple of different ideas that are open here one of the ideas well so one",
    "start": "4064230",
    "end": "4073320"
  },
  {
    "text": "thing is you can have every cube proxy watch every node and now it's gonna get a mapping of all the nodes and all their",
    "start": "4073320",
    "end": "4079140"
  },
  {
    "text": "labels right that sounds great but it's really expensive nodes node is a really big",
    "start": "4079140",
    "end": "4085290"
  },
  {
    "text": "object like several tens of kilobytes and it turns fairly frequently today because it gets updated whenever a",
    "start": "4085290",
    "end": "4091350"
  },
  {
    "text": "container image is polled or whenever the resources change or whenever the heartbeat changes so we're moving the",
    "start": "4091350",
    "end": "4097290"
  },
  {
    "text": "heartbeat out we're reducing the churn there but it's still a very big object when all you really want is the labels",
    "start": "4097290",
    "end": "4103080"
  },
  {
    "text": "so we've got a couple of different ideas one idea is to introduce a new resource",
    "start": "4103080",
    "end": "4108600"
  },
  {
    "text": "called the pod locator which is essentially a reverse mapping for your pod given a pods name I can look at the",
    "start": "4108600",
    "end": "4114298"
  },
  {
    "text": "locator with the same name and I can take it back and say well what is your node name and what is your IP address",
    "start": "4114299",
    "end": "4119520"
  },
  {
    "text": "right and then given that I can either selectively watch nodes or I can build I",
    "start": "4119520",
    "end": "4126600"
  },
  {
    "text": "can actually take the node labels and stick them into that locator and so we have an object that is much lower churn",
    "start": "4126600",
    "end": "4132480"
  },
  {
    "text": "like it won't ever like node labels don't change very often right so this thing will not change and all the",
    "start": "4132480",
    "end": "4138630"
  },
  {
    "text": "scalability problems that we had get sort of erased through this another option that we looked at is",
    "start": "4138630",
    "end": "4144548"
  },
  {
    "text": "whether we could teach the API machinery to be smarter and only watch the metadata and so we would only get",
    "start": "4144549",
    "end": "4151750"
  },
  {
    "text": "updated on the watch Channel with the metadata right API machinery team said oh my god you're crazy we're so busy",
    "start": "4151750",
    "end": "4157508"
  },
  {
    "text": "leave us alone and then Justin went often like in about 30 minutes prototype tit so maybe we'll get this maybe we",
    "start": "4157509",
    "end": "4165040"
  },
  {
    "text": "won't we'll see we'll keep working through this this design was sort of co-designed with the scalability sig so",
    "start": "4165040",
    "end": "4170679"
  },
  {
    "text": "that we would make sure we don't reinvent some of the problems of the past so now two headless services in DNS",
    "start": "4170679",
    "end": "4177310"
  },
  {
    "text": "or any arbitrary other thing that wants to comprehend services I can now watch pod locators and I can tell based on",
    "start": "4177310",
    "end": "4183969"
  },
  {
    "text": "your pods IP address what node you're coming from and that's actually a relatively low overhead operation now",
    "start": "4183969",
    "end": "4191159"
  },
  {
    "text": "interestingly it'll also give us the ability to do reverse dns for pods",
    "start": "4191159",
    "end": "4196170"
  },
  {
    "text": "so the status on this one while I have all the details in my head we're at a",
    "start": "4196170",
    "end": "4201489"
  },
  {
    "text": "camp we're pre or we are pre alpha kept nobody is currently implementing this so we have the design mostly worked out but",
    "start": "4201489",
    "end": "4209070"
  },
  {
    "text": "like all good things in open-source it needs a contributor it's gotten some",
    "start": "4209070",
    "end": "4214210"
  },
  {
    "text": "movement then it got stalled and you know this is how open-source works so if this is an interesting topic to you",
    "start": "4214210",
    "end": "4219639"
  },
  {
    "text": "please come talk to us and we'll be happy to see if we can get people engaged and working on it question",
    "start": "4219639",
    "end": "4226079"
  },
  {
    "text": "how do we actually implement this with IP tables or IPs with IP tables it's easy I just filtered the list of IP",
    "start": "4227969",
    "end": "4234070"
  },
  {
    "text": "addresses I would put in your back-end set to just the IPS that are available",
    "start": "4234070",
    "end": "4239368"
  },
  {
    "text": "well so I'm not going to there's an order to keep repeating the question there's an order to them I'm not even",
    "start": "4239610",
    "end": "4244900"
  },
  {
    "text": "going to put the ones that are out of consideration so if there's any IP address on the same node I'll just use",
    "start": "4244900",
    "end": "4250090"
  },
  {
    "text": "that one and none of the other ones I'm not gonna try to dynamically balance it that's a problem for like a service mesh",
    "start": "4250090",
    "end": "4255639"
  },
  {
    "text": "or something right that's way too hard I can't I can't teach the kernel to do that so it's a very strictly ordered list and",
    "start": "4255639",
    "end": "4263199"
  },
  {
    "text": "that's why I was trying to be very clear here right if there are none on my node then I will consider the zone all right",
    "start": "4263199",
    "end": "4270250"
  },
  {
    "text": "and if there are none in the zone then it will consider the rest there's no fallback options here it's",
    "start": "4270250",
    "end": "4277280"
  },
  {
    "text": "it's not a dynamic thing other questions on this",
    "start": "4277280",
    "end": "4283210"
  },
  {
    "text": "is there overlap with what scheduling is working on with topology systems I hope that they are complementary topology has",
    "start": "4288520",
    "end": "4294980"
  },
  {
    "text": "started to pervade basically every piece of the system we've already got storage topology we know which nodes you can",
    "start": "4294980",
    "end": "4301250"
  },
  {
    "text": "mount a disk on scheduling is adding more like spreading across topology they've already got some",
    "start": "4301250",
    "end": "4306860"
  },
  {
    "text": "built-in spreading they're working on a new API for allowing you to specify better spreading I hope that these",
    "start": "4306860",
    "end": "4314360"
  },
  {
    "text": "things are all complementary so that if you care about availability or regionality or whatever you should be",
    "start": "4314360",
    "end": "4320120"
  },
  {
    "text": "able to say I want my deployment and I wanted to run three replicas in every zone of my cluster right",
    "start": "4320120",
    "end": "4326540"
  },
  {
    "text": "that's not expressible today but when we talked to the workloads people they say yeah we want that we were going to do",
    "start": "4326540",
    "end": "4331940"
  },
  {
    "text": "that at some point then you could say well you can already Express I want a daemon set that runs on",
    "start": "4331940",
    "end": "4337520"
  },
  {
    "text": "every node but why couldn't you say I want a daemon said that runs in every zone right there was another question",
    "start": "4337520",
    "end": "4343840"
  },
  {
    "text": "No",
    "start": "4343840",
    "end": "4346840"
  },
  {
    "text": "is that gonna work with multi cluster what an awesome question I don't know I",
    "start": "4349080",
    "end": "4355330"
  },
  {
    "text": "it's it's a hot topic today whether we should introduce multi cluster concepts",
    "start": "4355330",
    "end": "4361630"
  },
  {
    "text": "into kubernetes proper or whether we should be building those strictly on top it's literally I don't know how I feel",
    "start": "4361630",
    "end": "4368200"
  },
  {
    "text": "about this on the one hand I want kubernetes to stay small and get boring so we stop adding new concepts at the",
    "start": "4368200",
    "end": "4375790"
  },
  {
    "text": "same time I want to make sure that multi cluster is a viable thing because I think everybody is going to be in that",
    "start": "4375790",
    "end": "4381100"
  },
  {
    "text": "situation so we're trying to figure out the exact boundaries there we you know at Google we're pushing on a couple",
    "start": "4381100",
    "end": "4387190"
  },
  {
    "text": "things that within the sig we're pushing on a couple things we've got some multi cluster ingress capabilities at Google",
    "start": "4387190",
    "end": "4393310"
  },
  {
    "text": "that we're happy with a little bit but not happy with a little bit so we're gonna keep evolving those ideas there's another question in the back",
    "start": "4393310",
    "end": "4401010"
  },
  {
    "text": "with their would you have room for an API from the local proxy to local",
    "start": "4419369",
    "end": "4424840"
  },
  {
    "text": "service I'm not sure I understand what you're getting - well what do you what are you trying to solve with that I",
    "start": "4424840",
    "end": "4431999"
  },
  {
    "text": "see what you mean like so like publishing republishing that information to local agents",
    "start": "4439079",
    "end": "4444809"
  },
  {
    "text": "that's not a terrible idea I haven't really thought much about it but it's not a bad idea I kind of like it PR is",
    "start": "4444809",
    "end": "4451389"
  },
  {
    "text": "welcome I mean I mean keps welcome please start with a cab",
    "start": "4451389",
    "end": "4457679"
  },
  {
    "text": "alright let me restate the question see if I can get it right are there any requests for more is it more dynamic",
    "start": "4479660",
    "end": "4488090"
  },
  {
    "text": "right everything you can imagine has been asked for at least once",
    "start": "4490190",
    "end": "4496280"
  },
  {
    "text": "whether that's a reasonable thing to implement is a different question so when we looked at whatever he was asking",
    "start": "4496280",
    "end": "4502200"
  },
  {
    "text": "for here we looked through the use cases that were really present in front of us the vast majority of them fit into those two patterns that I mentioned either",
    "start": "4502200",
    "end": "4508530"
  },
  {
    "text": "give me something on my same node or try your best to stay in the same zone and the other use cases you know failover or",
    "start": "4508530",
    "end": "4515850"
  },
  {
    "text": "spillover are a lot more dynamic and a lot more complicated to implement I'm gonna punt those off to is do or service",
    "start": "4515850",
    "end": "4523290"
  },
  {
    "text": "mash or whatever dual stack anybody here ipv6",
    "start": "4523290",
    "end": "4529520"
  },
  {
    "text": "nice so kubernetes has had ipv6 support for a while like a year and a half or",
    "start": "4529520",
    "end": "4535890"
  },
  {
    "text": "something but it's stuck in alpha state because we need continuous integration for it and the team that was working on",
    "start": "4535890",
    "end": "4542430"
  },
  {
    "text": "it like all good things an open-source got sort of repurposed and we need help so we took a step back and we're looking",
    "start": "4542430",
    "end": "4550830"
  },
  {
    "text": "at it and you know v6 single stack is interesting but it's not really the goal the real goal was to get to dual stack",
    "start": "4550830",
    "end": "4556470"
  },
  {
    "text": "so a team has now stepped up from Microsoft mostly but other contributors",
    "start": "4556470",
    "end": "4562770"
  },
  {
    "text": "to that are trying to drive the goal for dual stack and we'll focus on CI in",
    "start": "4562770",
    "end": "4568980"
  },
  {
    "text": "regards to dual stack so there yes we've got a kept yeah",
    "start": "4568980",
    "end": "4575550"
  },
  {
    "text": "question do Oh",
    "start": "4575550",
    "end": "4581540"
  },
  {
    "text": "Bob you work me right into my next couple of slides the question was can",
    "start": "4581870",
    "end": "4587370"
  },
  {
    "text": "you explain what I mean by dual-stack I will do exactly that in 30 seconds there's a cap it's a huge cap it is",
    "start": "4587370",
    "end": "4595620"
  },
  {
    "text": "incredibly complicated cap it touches tons of the system it has forced us to",
    "start": "4595620",
    "end": "4600780"
  },
  {
    "text": "go out and define new API conventions for how to do things and how to evolve this is part of why it's taken so long",
    "start": "4600780",
    "end": "4606480"
  },
  {
    "text": "to get here it touches almost every binary component of the system and it touches a half-dozen different API",
    "start": "4606480",
    "end": "4612750"
  },
  {
    "text": "resources so we have broken up the development into two stages the first",
    "start": "4612750",
    "end": "4618570"
  },
  {
    "text": "stage is let's go through all those fields in our API that talk about an IP address in a",
    "start": "4618570",
    "end": "4625530"
  },
  {
    "text": "singular and make them plural and we'll make them ready for two families of IP",
    "start": "4625530",
    "end": "4630810"
  },
  {
    "text": "address right so this is there's probably six or eight fields throughout the different API resources that say",
    "start": "4630810",
    "end": "4637380"
  },
  {
    "text": "address now they have to say address is so we had to invent a convention in kubernetes to pluralize an existing",
    "start": "4637380",
    "end": "4643860"
  },
  {
    "text": "field and still be compatible question",
    "start": "4643860",
    "end": "4648020"
  },
  {
    "text": "so the question is why I only have two addresses so",
    "start": "4660640",
    "end": "4665800"
  },
  {
    "text": "yeah so we're not stopping at two interfaces but we're starting at two interfaces so I'm not gonna say there's",
    "start": "4666400",
    "end": "4672710"
  },
  {
    "text": "now address 1 and address - it's gonna be an addresses list those it'll all be lists right so we're making room for",
    "start": "4672710",
    "end": "4678260"
  },
  {
    "text": "multiple addresses later but we're not focusing on that use case right now right we're right now we're assuming in",
    "start": "4678260",
    "end": "4683540"
  },
  {
    "text": "that list is going to be at most one v4 address in one v6 address yeah question",
    "start": "4683540",
    "end": "4688820"
  },
  {
    "text": "I",
    "start": "4688820",
    "end": "4691000"
  },
  {
    "text": "[Music] am NOT a v6 I'm not so the question is",
    "start": "4694250",
    "end": "4699920"
  },
  {
    "text": "do I expect that people will use unique link locals or public addresses I don't know I'm not a v6 expert honestly and so",
    "start": "4699920",
    "end": "4708440"
  },
  {
    "text": "I'm looking to the people who are v6 experts to tell me how this works how it works best for for these",
    "start": "4708440",
    "end": "4714860"
  },
  {
    "text": "situations from what I've seen so far I think they'll be using the public addresses not the link locals",
    "start": "4714860",
    "end": "4721120"
  },
  {
    "text": "so this touches a dozen a half dozen ish API resources it touches almost every",
    "start": "4721120",
    "end": "4727670"
  },
  {
    "text": "binary has at least one flag that has something to do with an IP address whether it's a listen address or publishing address or something like",
    "start": "4727670",
    "end": "4734180"
  },
  {
    "text": "that so these things all need to become pluralized we need to fix all the validation that checks for v4 addresses",
    "start": "4734180",
    "end": "4739640"
  },
  {
    "text": "and make sure that we can handle these six addresses and this will make all the pods be dual stack so now your CNI",
    "start": "4739640",
    "end": "4747380"
  },
  {
    "text": "plugins which can already actually return two IP addresses will actually be able to take those addresses and publish them all the way up into pods and when",
    "start": "4747380",
    "end": "4754130"
  },
  {
    "text": "you go look at your pods through the API server you'll be able to see that there are two different addresses",
    "start": "4754130",
    "end": "4759280"
  },
  {
    "text": "there's also the hope to make host ports be dual stack so host ports are",
    "start": "4759280",
    "end": "4764630"
  },
  {
    "text": "implemented today by mostly by cubelet and that would set up forwarding rules between the host and the pods and that's",
    "start": "4764630",
    "end": "4772640"
  },
  {
    "text": "it that's phase one right I want to get that in debugged and established phase two let's make endpoints support",
    "start": "4772640",
    "end": "4781280"
  },
  {
    "text": "multiply Peas and multiple families will add DNS support so you'll have a and quad-a records now some of these things",
    "start": "4781280",
    "end": "4787070"
  },
  {
    "text": "might flip between phases depending on how hard they are to actually implement we'll talk about making node ports and",
    "start": "4787070",
    "end": "4794139"
  },
  {
    "text": "ingress controllers and probes all dual-stack there's some talk about",
    "start": "4794139",
    "end": "4799310"
  },
  {
    "text": "whether we need two options to control the dual stack nough Slyke if my application only understands before",
    "start": "4799310",
    "end": "4805190"
  },
  {
    "text": "should I be able to say that so that nobody tries to reach me on v6 or vice versa and then do we make services dual stack",
    "start": "4805190",
    "end": "4812659"
  },
  {
    "text": "are we gonna actually take two different service IP range and allocate different ciders for v4 and v6 that is a to be",
    "start": "4812659",
    "end": "4820370"
  },
  {
    "text": "decided question that is essentially gravy once you've done all the rest of the work that that becomes a relatively",
    "start": "4820370",
    "end": "4826400"
  },
  {
    "text": "simple thing to do if we want to do it I anticipate that this work will take the",
    "start": "4826400",
    "end": "4832190"
  },
  {
    "text": "better part of the rest of this calendar year apparently there's a pull request in my mailbox right now that locky was",
    "start": "4832190",
    "end": "4837770"
  },
  {
    "text": "yelling at me to go review before code freeze next week so great we're seeing real progress here which is fantastic",
    "start": "4837770",
    "end": "4844210"
  },
  {
    "text": "questions yes",
    "start": "4844210",
    "end": "4851350"
  },
  {
    "text": "so how do I expect DNS to work I will expect it to work however that whatever",
    "start": "4862440",
    "end": "4868300"
  },
  {
    "text": "the client is going to ask for like we don't really have control over whether they ask for a and quad-a in parallel or",
    "start": "4868300",
    "end": "4873610"
  },
  {
    "text": "if they ask for them in sequence right we've been bitten before by trying to assume too much about what in client",
    "start": "4873610",
    "end": "4879160"
  },
  {
    "text": "implementations of dns are going to do there is zero consistency across clients so I don't think we have a whole lot of",
    "start": "4879160",
    "end": "4886060"
  },
  {
    "text": "control over that",
    "start": "4886060",
    "end": "4888660"
  },
  {
    "text": "my pv6 will probably work like before yeah I guess I assume so",
    "start": "4901710",
    "end": "4906780"
  },
  {
    "text": "IP management becomes easier already many hosts already get a easier slash 64",
    "start": "4906780",
    "end": "4912580"
  },
  {
    "text": "/ 96 assigned to them so the IP management the stuff that we have to do with cloud providers today gets a little",
    "start": "4912580",
    "end": "4919930"
  },
  {
    "text": "bit easier for the v6 side I think we're gonna feel this out as we go",
    "start": "4919930",
    "end": "4926430"
  },
  {
    "text": "yes yes so we didn't talk yesterday on alias IPS in Google cloud Google cloud",
    "start": "4926670",
    "end": "4932950"
  },
  {
    "text": "does not support ipv6 yet so I don't know exactly how our implementation will look when and if we get there",
    "start": "4932950",
    "end": "4939250"
  },
  {
    "text": "Microsoft does support ipv6 so they're they're driving this work right now this",
    "start": "4939250",
    "end": "4944470"
  },
  {
    "text": "is the beauty of open source communities the last topic that I prepared for today though I'm happy to talk about other",
    "start": "4944470",
    "end": "4950470"
  },
  {
    "text": "things that people want to talk about other things was a rethink of the endpoints API so let's look real quickly",
    "start": "4950470",
    "end": "4956440"
  },
  {
    "text": "at the problem today our endpoints API is a monolithic resource all of the endpoints for a single service are in a",
    "start": "4956440",
    "end": "4962950"
  },
  {
    "text": "single object this can get very large we have customers who are trying to run thousands or tens of thousands of",
    "start": "4962950",
    "end": "4969130"
  },
  {
    "text": "endpoints behind a single service each endpoint takes up a few hundred bytes of that resource you end up with research",
    "start": "4969130",
    "end": "4975580"
  },
  {
    "text": "with endpoint resources that are now literally mega almost a megabyte big which causes pain for SCD maybe has a",
    "start": "4975580",
    "end": "4983880"
  },
  {
    "text": "limit in the amount of memory that it's going to store resources and it causes pain for API servers which have to",
    "start": "4983880",
    "end": "4990250"
  },
  {
    "text": "serialize and deserialize the resources to to and from proto or JSON as an",
    "start": "4990250",
    "end": "4996420"
  },
  {
    "text": "example of the math if you have a thousand replicas service which isn't that unreasonable right for a large user",
    "start": "4996420",
    "end": "5002330"
  },
  {
    "text": "and you a thousand node cluster I'm going to generate over 250 gigabytes of traffic just to do a rolling update of",
    "start": "5002330",
    "end": "5008600"
  },
  {
    "text": "that service just updating the endpoints resource over and over and over again I'm going to serialize and deserialize that thousands of times right that's a",
    "start": "5008600",
    "end": "5017210"
  },
  {
    "text": "little too much so we work together with sig Network and",
    "start": "5017210",
    "end": "5022880"
  },
  {
    "text": "sig scalability to try to find a new endpoint API that would sort of trade-off between this and some of the",
    "start": "5022880",
    "end": "5029330"
  },
  {
    "text": "other scalability issues that API server and sed will have the result is a dock I've linked it here",
    "start": "5029330",
    "end": "5036730"
  },
  {
    "text": "min I don't know if you're in the room but min hun row at the dock and we'll be turning it into a kept any minute now",
    "start": "5036730",
    "end": "5042310"
  },
  {
    "text": "the short story is we're gonna take this model at the endpoint and break it up into chunks and the actual size of the",
    "start": "5042310",
    "end": "5049790"
  },
  {
    "text": "chunks is now a parameter so we can actually experiment with this and say we can set this parameter all the way down",
    "start": "5049790",
    "end": "5055100"
  },
  {
    "text": "to 1 we could set the parameter up to a hundred or something like that we've chosen 100 right now as a default",
    "start": "5055100",
    "end": "5060460"
  },
  {
    "text": "because most services are less than a hundred endpoints you know looking at data talking to people which means for",
    "start": "5060460",
    "end": "5066740"
  },
  {
    "text": "most services the actual behavior isn't going to change and for those large services you're going to get this",
    "start": "5066740",
    "end": "5072110"
  },
  {
    "text": "reduction in throughput and SCD rights and and these",
    "start": "5072110",
    "end": "5077270"
  },
  {
    "text": "other operations the dock is great it has a nice table comparing different trade-off values and there's basically an elbow in the graph and",
    "start": "5077270",
    "end": "5085030"
  },
  {
    "text": "this will replace the endpoints API for entry users so specifically cube proxy",
    "start": "5085030",
    "end": "5092000"
  },
  {
    "text": "but we're not going to get rid of the endpoints API because that's a ga supported API that has a multi-year deprecation plan and there are people",
    "start": "5092000",
    "end": "5099470"
  },
  {
    "text": "outside of communities who use that but if we were fixed all the queue proxies we're gonna get rid of 90 plus percent",
    "start": "5099470",
    "end": "5105770"
  },
  {
    "text": "of the pain by just fixing our entry users so we'll actually synchronize services to both sets of endpoints for",
    "start": "5105770",
    "end": "5112190"
  },
  {
    "text": "the foreseeable future at least until the core API group gets removed and Lord knows when that will be so hopefully",
    "start": "5112190",
    "end": "5119450"
  },
  {
    "text": "we'll have a kept very soon and then we can argue about it there if you guys have ever bumped into this I'd love to",
    "start": "5119450",
    "end": "5126660"
  },
  {
    "text": "hear from you and hear your thoughts on this if you're building higher-level systems that use endpoints for a service",
    "start": "5126660",
    "end": "5131820"
  },
  {
    "text": "discovery or mesh or whatever I would also love to talk to you about how this fits with your higher-level systems that",
    "start": "5131820",
    "end": "5138570"
  },
  {
    "text": "use those api's Wow I actually got through all that material that's amazing I know this is",
    "start": "5138570",
    "end": "5145260"
  },
  {
    "text": "really dense I didn't spend too much time trying to make it fluffy I figured the people who are here on Thursday",
    "start": "5145260",
    "end": "5150810"
  },
  {
    "text": "afternoon at 4 o'clock 5 o'clock have chosen to be here so blame yourself",
    "start": "5150810",
    "end": "5158210"
  },
  {
    "text": "if you take away just a short message from this we have a lot going on and",
    "start": "5158210",
    "end": "5163980"
  },
  {
    "text": "signal work this is not the full stream of projects that are going on or the full gamut of ideas that we've got on",
    "start": "5163980",
    "end": "5169140"
  },
  {
    "text": "the table we have a million or not a million so I guess a couple hundred now bugs that are open we've done the triage",
    "start": "5169140",
    "end": "5176460"
  },
  {
    "text": "thank thank the Lord but there are still dozens and dozens of real bugs that are",
    "start": "5176460",
    "end": "5181739"
  },
  {
    "text": "open so if you're interested in getting involved in Signet work you can jump in and try to help reproduce or fix bugs if",
    "start": "5181739",
    "end": "5188340"
  },
  {
    "text": "you're experiencing bugs please file them we have lots of cleanups that need",
    "start": "5188340",
    "end": "5193500"
  },
  {
    "text": "to be doing code refactoring or new new smaller features that we can use help with if you're interested in getting",
    "start": "5193500",
    "end": "5200730"
  },
  {
    "text": "involved in Signet work the first thing to do is join get on the mailing list jump on the slack channel jump on our",
    "start": "5200730",
    "end": "5207300"
  },
  {
    "text": "zoom calls they're not a really euro friendly time right now but I'm open to moving that time if enough Europeans",
    "start": "5207300",
    "end": "5213000"
  },
  {
    "text": "decide they want to jump on our on our zoom I'm happy to take more questions I think we're pretty close to time right",
    "start": "5213000",
    "end": "5218550"
  },
  {
    "text": "now so we're at about time so I'm happy to take",
    "start": "5218550",
    "end": "5224190"
  },
  {
    "text": "questions I don't think they're gonna kick us out because I think we're the last session so I'll just hang out here",
    "start": "5224190",
    "end": "5229400"
  },
  {
    "text": "sorry you know okay well we get kicked out the hardest I guess but I'm gonna sit here",
    "start": "5229400",
    "end": "5235620"
  },
  {
    "text": "until they drag me away so if anybody's got questions or wants to talk about stuff or go into things that we didn't",
    "start": "5235620",
    "end": "5241380"
  },
  {
    "text": "talk about here now's your chance",
    "start": "5241380",
    "end": "5244520"
  },
  {
    "text": "all right [Applause]",
    "start": "5247610",
    "end": "5252680"
  }
]