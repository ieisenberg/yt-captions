[
  {
    "text": "okay um yeah my name is uh Abdullah as you know um so I'll be moderating this panel so",
    "start": "1140",
    "end": "8639"
  },
  {
    "text": "we've been discussing a lot since like the whole batch working group",
    "start": "8639",
    "end": "15540"
  },
  {
    "text": "the idea of coming with a bachelor is that because you always thought that kubernetes",
    "start": "15540",
    "end": "22080"
  },
  {
    "text": "was mostly optimized for service type workloads we've been a lot of optimizations related to",
    "start": "22080",
    "end": "29460"
  },
  {
    "text": "um you know services like you know um topology-based routing rolling updates",
    "start": "29460",
    "end": "37820"
  },
  {
    "text": "and all these kind of things we're focused on microservices and now as we've seen recently",
    "start": "37820",
    "end": "44340"
  },
  {
    "text": "now that like kubernetes become pretty much a de facto orchestrator for containers and proved",
    "start": "44340",
    "end": "52860"
  },
  {
    "text": "to be quite nice to run service type workloads the community is trying to convert other types of workloads on that",
    "start": "52860",
    "end": "60660"
  },
  {
    "text": "same platform but because we're having focused as a community on running batch workloads on",
    "start": "60660",
    "end": "66900"
  },
  {
    "text": "kubernetes there are a lot of gaps that we know that um and so we thought like okay maybe we",
    "start": "66900",
    "end": "75000"
  },
  {
    "text": "should discuss this and have a look we we have these discussions a lot right like during the day we have multiple talks it's giving us like basically a",
    "start": "75000",
    "end": "82439"
  },
  {
    "text": "laundry list of gaps um but people are still running batch workloads and kubernetes even though we",
    "start": "82439",
    "end": "88500"
  },
  {
    "text": "have all these gaps um and so we thought like what about bring four different uh platforms that",
    "start": "88500",
    "end": "95400"
  },
  {
    "text": "were built on top of kubernetes we have a platform from G research Armada we've",
    "start": "95400",
    "end": "101040"
  },
  {
    "text": "got unicorn we have mcad from IBM and we have qflux or flux I guess uh from Los",
    "start": "101040",
    "end": "109979"
  },
  {
    "text": "Angeles National Labs it's affluence from Lawrence Livermore in IBM yeah",
    "start": "109979",
    "end": "116759"
  },
  {
    "text": "um and so the panel I guess we we could discuss",
    "start": "116759",
    "end": "122159"
  },
  {
    "text": "here is the what are the gaps that we have in kubernetes to run batch workloads and",
    "start": "122159",
    "end": "128160"
  },
  {
    "text": "how we can um Bridge these gaps um and I guess we can start",
    "start": "128160",
    "end": "135060"
  },
  {
    "text": "um from Daniel sure yeah um so I just want to start off",
    "start": "135060",
    "end": "141360"
  },
  {
    "text": "by saying my background is in HPC and bare metal HPC I've kind of been introduced to the kubernetes ecosystem",
    "start": "141360",
    "end": "148260"
  },
  {
    "text": "relatively recently and been really really excited by its capabilities and its abilities to enhance HPC itself",
    "start": "148260",
    "end": "154819"
  },
  {
    "text": "so I think one thing one thing that I like to think about in terms of HPC",
    "start": "154819",
    "end": "160319"
  },
  {
    "text": "versus kubernetes is declarative versus imperative models and from my point of",
    "start": "160319",
    "end": "165540"
  },
  {
    "text": "view HPC comes from a much more imperative model in that the user wants to is cust is customized or is used to",
    "start": "165540",
    "end": "173300"
  },
  {
    "text": "specifying exactly how things will run exactly where because they assume that",
    "start": "173300",
    "end": "178440"
  },
  {
    "text": "they know best how to get the most performance out of their particular application versus",
    "start": "178440",
    "end": "184140"
  },
  {
    "text": "the kubernetes model is much more declarative oriented where basically you want to maintain a particular state so",
    "start": "184140",
    "end": "190560"
  },
  {
    "text": "how basically can we merge these two models together and can you is that even possible to begin with",
    "start": "190560",
    "end": "196980"
  },
  {
    "text": "um I think that's that's a real big challenge I don't know if there that's a gap per se in in batch for kubernetes",
    "start": "196980",
    "end": "202920"
  },
  {
    "text": "it's I think more of a clash of models that hopefully will converge on one another",
    "start": "202920",
    "end": "209519"
  },
  {
    "text": "there you want to introduce yourself and why you're interested batch on kubernetes",
    "start": "209519",
    "end": "216239"
  },
  {
    "text": "thank you Alvin do you want to introduce yourself and sure my name is Albin and I work with",
    "start": "346560",
    "end": "353180"
  },
  {
    "text": "Jamie on their mother project for the presentation that he had earlier",
    "start": "353180",
    "end": "358560"
  },
  {
    "text": "and so let's go into three things that we have sort of figured out where it's",
    "start": "358560",
    "end": "363600"
  },
  {
    "text": "challenging with running our batch workloads on top of kubernetes and indeed these are some of the things that we try to alleviate with with Armada",
    "start": "363600",
    "end": "370740"
  },
  {
    "text": "and so one of these things is that we're really scared about overloading it City and our model is originally designed as",
    "start": "370740",
    "end": "377280"
  },
  {
    "text": "a buffer to be placed in front of etcd such that when one of the users submits 100 000 jobs within say 10 seconds we",
    "start": "377280",
    "end": "385139"
  },
  {
    "text": "don't cause hcd to completely fall over at because that city does total order broadcast if any one user completely",
    "start": "385139",
    "end": "392039"
  },
  {
    "text": "overloads at CD there's no service for anyone on the cluster anymore",
    "start": "392039",
    "end": "397199"
  },
  {
    "text": "um the second thing is that as discussed previously sort of HPC workloads are often designed in a very much in a when",
    "start": "397199",
    "end": "404880"
  },
  {
    "text": "this happens to something else and so there are events in kubernetes right but",
    "start": "404880",
    "end": "409919"
  },
  {
    "text": "these things are sort of informational and not really designed to build automation on top of",
    "start": "409919",
    "end": "417000"
  },
  {
    "text": "in particular something that we are sort of struggling between now is determining from outside a cluster if a port was",
    "start": "417000",
    "end": "423300"
  },
  {
    "text": "preempted as it turns out that the cube schedule will preemptible and then after that create an event saying that the port was",
    "start": "423300",
    "end": "429960"
  },
  {
    "text": "preempted but there's no guarantees that this event will actually be produced or indeed on any ordering between that",
    "start": "429960",
    "end": "435479"
  },
  {
    "text": "event and other events and so looking into the cluster it can be difficult to determine what happens to your pods and",
    "start": "435479",
    "end": "441599"
  },
  {
    "text": "to act accordingly and finally um you know because kubernetes is sort",
    "start": "441599",
    "end": "447539"
  },
  {
    "text": "of grew up in the microservices world there's still a lot of tooling missing for example gang scheduling there's a",
    "start": "447539",
    "end": "452880"
  },
  {
    "text": "lot of fragmentation around gang scheduling and this is something that we really want to support in our model and it's not really clear how we will do",
    "start": "452880",
    "end": "458160"
  },
  {
    "text": "this as many options just thank you Wilfred yeah so Wilfred",
    "start": "458160",
    "end": "464819"
  },
  {
    "text": "spikenberg from Apache unicorn um we started working on Apache unicorn",
    "start": "464819",
    "end": "471199"
  },
  {
    "text": "from the yarn perspective so light way way I I come from the yarn perspective",
    "start": "471199",
    "end": "477780"
  },
  {
    "text": "we've got a lot of these things like job queuing hierarchical cues user quotas",
    "start": "477780",
    "end": "486300"
  },
  {
    "text": "um that's all there so when we see people that want to move to kubernetes",
    "start": "486300",
    "end": "492300"
  },
  {
    "text": "the first question is what they ask is so are you going to give me all the",
    "start": "492300",
    "end": "498060"
  },
  {
    "text": "functionality that I've got on yarn so do you do you do you do queuing do you",
    "start": "498060",
    "end": "503099"
  },
  {
    "text": "do quotas do you do all that stuff on the Fly um and do you implement strict security",
    "start": "503099",
    "end": "510860"
  },
  {
    "text": "uh on Hadoop we run with kerbros enabled",
    "start": "510860",
    "end": "516000"
  },
  {
    "text": "every single thing is Kerberos all the way through the Pod or the ga VM that",
    "start": "516000",
    "end": "522479"
  },
  {
    "text": "you run on the system so that principle is there and when you look at kubernetes",
    "start": "522479",
    "end": "528839"
  },
  {
    "text": "from from our perspective was also like we're missing that user information going through the scheduling cycle so",
    "start": "528839",
    "end": "535560"
  },
  {
    "text": "accounting for resource usage and I've heard a number of people say that that's",
    "start": "535560",
    "end": "541200"
  },
  {
    "text": "difficult so how do you get that through there preemption from from batch jobs it often",
    "start": "541200",
    "end": "551040"
  },
  {
    "text": "matters exactly which Bots you preempt if you preempt the driver bot from spark",
    "start": "551040",
    "end": "556800"
  },
  {
    "text": "your whole spark application is gone so it could be far more costly to PM that",
    "start": "556800",
    "end": "564120"
  },
  {
    "text": "one driver Port than preempting 10 executor Bots overall so and that that",
    "start": "564120",
    "end": "572459"
  },
  {
    "text": "difference you you can't make the difference the scheduler is not aware of that information so we're trying to get",
    "start": "572459",
    "end": "579720"
  },
  {
    "text": "that kind of information into the system so that's missing from from from our",
    "start": "579720",
    "end": "585120"
  },
  {
    "text": "perspective so on top of a number of the other things that are already mentioned it's",
    "start": "585120",
    "end": "591959"
  },
  {
    "text": "two different worlds almost got a lot to talk about here so we've",
    "start": "591959",
    "end": "597779"
  },
  {
    "text": "got multi-cluster we've got quota and queuing and Bim packing",
    "start": "597779",
    "end": "603180"
  },
  {
    "text": "um so before uh going to the to the audience the questions I have like one high level question here which is do we",
    "start": "603180",
    "end": "611100"
  },
  {
    "text": "believe like since you've been working with kubernetes uh for a while now do it",
    "start": "611100",
    "end": "616860"
  },
  {
    "text": "there's something fundamentally wrong with kubernetes that makes it",
    "start": "616860",
    "end": "622080"
  },
  {
    "text": "wrong or like limiting that makes it hard to build on top of to build batch",
    "start": "622080",
    "end": "628220"
  },
  {
    "text": "Frameworks on top of it but we have to do it because well it's the cool thing out there and we have to basically unify",
    "start": "628220",
    "end": "636180"
  },
  {
    "text": "our infrastructure around something you know Common I think I can give a quick hook into",
    "start": "636180",
    "end": "644519"
  },
  {
    "text": "Etc the when you're looking at batch jobs and batch things there's users that that",
    "start": "644519",
    "end": "650760"
  },
  {
    "text": "will submit hundreds of jobs at one go which could be thousands of bots so",
    "start": "650760",
    "end": "657360"
  },
  {
    "text": "overloading Etc D is a real problem and",
    "start": "657360",
    "end": "662399"
  },
  {
    "text": "you need to be really careful and that's also why what Apple did what I hear",
    "start": "662399",
    "end": "668339"
  },
  {
    "text": "um we had a meet up last week Pinterest also they all stopped scaling at 800",
    "start": "668339",
    "end": "674880"
  },
  {
    "text": "2000 notes they say we don't go any further we do not want to get into problems with Etc D so that's a real",
    "start": "674880",
    "end": "682140"
  },
  {
    "text": "thing that we've all got um",
    "start": "682140",
    "end": "687860"
  },
  {
    "text": "yeah sorry completely agree",
    "start": "690019",
    "end": "693920"
  },
  {
    "text": "okay let's try again okay it seems safe now yeah so I say like it's something that",
    "start": "698519",
    "end": "704760"
  },
  {
    "text": "we're really concerned about um and indeed it looks like at least that sort of for the system that we try",
    "start": "704760",
    "end": "710459"
  },
  {
    "text": "to build relying only on its city is essentially like an impossibility because indeed we have used the submit",
    "start": "710459",
    "end": "716579"
  },
  {
    "text": "say 100 000 jobs simultaneously as a result instead we're relying on",
    "start": "716579",
    "end": "722100"
  },
  {
    "text": "pulsar which does sort of a partition total or the broadcaster which allows us",
    "start": "722100",
    "end": "727200"
  },
  {
    "text": "to scale to say two orders of magnitude higher than that City the other thing that is a fundamental",
    "start": "727200",
    "end": "732300"
  },
  {
    "text": "problem with kubernetes that will be very difficult to change is the events because the events so kubernetes will",
    "start": "732300",
    "end": "738480"
  },
  {
    "text": "lose something and then afterwards publish an event that it did it and there are many components that",
    "start": "738480",
    "end": "743760"
  },
  {
    "text": "concurrently publish these events because of this architecture you can never guarantee event ordering or",
    "start": "743760",
    "end": "749100"
  },
  {
    "text": "delivery and so they will never be reliable so it's also something that you try to leave it with almada whereby Armada will",
    "start": "749100",
    "end": "756420"
  },
  {
    "text": "Ramada is itself event driven so Ramada does guarantee event delivery and ordering but at the point at which we",
    "start": "756420",
    "end": "763560"
  },
  {
    "text": "interact with kubernetes we don't have these guarantees and we have to sort of try to do the best we can",
    "start": "763560",
    "end": "769820"
  },
  {
    "text": "[Music] foreign",
    "start": "831830",
    "end": "838700"
  },
  {
    "text": "yes I'd like to piggyback off of what several of the other presenters or panelists have said so far one of the",
    "start": "846120",
    "end": "851820"
  },
  {
    "text": "things I mentioned toward the end of my presentation on fluence was a project that we're working on called the flux",
    "start": "851820",
    "end": "857220"
  },
  {
    "text": "operator and the idea behind this is to instantiate a mini cluster inside of kubernetes that will provide",
    "start": "857220",
    "end": "864240"
  },
  {
    "text": "full-featured batch scheduling so this is the scheduling the queuing basically everything inside of it and there are",
    "start": "864240",
    "end": "870300"
  },
  {
    "text": "some difficulties that that we've run into so far and I said before that there aren't really limitations that that's",
    "start": "870300",
    "end": "876300"
  },
  {
    "text": "not exactly true so one of the things that that we've run into is the difficulty in figuring out when some of",
    "start": "876300",
    "end": "883320"
  },
  {
    "text": "the worker pods are ready state so the the way that it flux the way that flux",
    "start": "883320",
    "end": "889680"
  },
  {
    "text": "will wire up is that the workers will start and then communicate with via a",
    "start": "889680",
    "end": "895320"
  },
  {
    "text": "tree base overlay Network to a broker rank zero and they basically signal that",
    "start": "895320",
    "end": "900420"
  },
  {
    "text": "they are ready over that particular Network so if there were an internal kind of a intermediate state or a state",
    "start": "900420",
    "end": "906000"
  },
  {
    "text": "after running that was more specific to that than maybe that broker could query that state from within this group of",
    "start": "906000",
    "end": "912899"
  },
  {
    "text": "PODS and then transition to running where it could actually execute the application MPI or whatever it was",
    "start": "912899",
    "end": "919440"
  },
  {
    "text": "afterwards",
    "start": "919440",
    "end": "922040"
  },
  {
    "text": "I have some follow-ups on each item here but I want to open it first to",
    "start": "925260",
    "end": "930720"
  },
  {
    "text": "the audience if anybody has any like questions",
    "start": "930720",
    "end": "936800"
  },
  {
    "text": "uh more of an observation so I don't have an HPC batch background I have been",
    "start": "942660",
    "end": "947699"
  },
  {
    "text": "working with kubernetes since 2017 running in production in a big Bank and as an architect I get a thought",
    "start": "947699",
    "end": "956459"
  },
  {
    "text": "should you want to do a basically lift and shift from the traditional",
    "start": "956459",
    "end": "961500"
  },
  {
    "text": "world to the kubernetes world or should you put some of the changes at",
    "start": "961500",
    "end": "966959"
  },
  {
    "text": "the application side as well and not try to fix all the problems",
    "start": "966959",
    "end": "972440"
  },
  {
    "text": "um yes you you you can shift some of the work towards the um the submitted",
    "start": "982380",
    "end": "988440"
  },
  {
    "text": "towards the application side of things um but if you look at",
    "start": "988440",
    "end": "993959"
  },
  {
    "text": "um let's say a spark job when you process data",
    "start": "993959",
    "end": "1000100"
  },
  {
    "text": "you do not know exactly how many executors you will use or will will have",
    "start": "1000100",
    "end": "1006800"
  },
  {
    "text": "to use until you know what your data is so",
    "start": "1006800",
    "end": "1012139"
  },
  {
    "text": "you don't want to take that Dynam that Dynamic capability away because if you",
    "start": "1012139",
    "end": "1018740"
  },
  {
    "text": "take it away then you either extend the processing time or us",
    "start": "1018740",
    "end": "1024740"
  },
  {
    "text": "resources because you're you're only using half of the executors so pulling",
    "start": "1024740",
    "end": "1030860"
  },
  {
    "text": "that information out and then saying oh if you put that in a Gateway kind of thing and you do it all beforehand then",
    "start": "1030860",
    "end": "1038780"
  },
  {
    "text": "we can make it easier for kubernetes to work with",
    "start": "1038780",
    "end": "1045100"
  },
  {
    "text": "yeah your mileage may vary so you will you will get somewhere I know from",
    "start": "1045679",
    "end": "1051320"
  },
  {
    "text": "um a pechicon that there are a couple of companies that are working on on pulling",
    "start": "1051320",
    "end": "1057020"
  },
  {
    "text": "that information all into an application outside but they also say that's a continual",
    "start": "1057020",
    "end": "1064640"
  },
  {
    "text": "process and they analyze every single spark job that",
    "start": "1064640",
    "end": "1070880"
  },
  {
    "text": "they've run so the overhead becomes also pretty big on your on your side",
    "start": "1070880",
    "end": "1077679"
  },
  {
    "text": "Arabic so I'll just comment so I think sort of",
    "start": "1079520",
    "end": "1084740"
  },
  {
    "text": "a modern computer is one of the sort of successful design principles has been to make computers so that it appears to the",
    "start": "1084740",
    "end": "1090559"
  },
  {
    "text": "applications running on them as they have the whole computer for themselves then there's no contention as on and one can make the same argument for",
    "start": "1090559",
    "end": "1097400"
  },
  {
    "text": "the modern accumulators and distributed world right let's try to make the platform such that to the from the point",
    "start": "1097400",
    "end": "1102559"
  },
  {
    "text": "of view of the applications right it's like running on your local PC right but maybe we reached a limit where",
    "start": "1102559",
    "end": "1107960"
  },
  {
    "text": "that's no longer feasible and we have to just fundamentally write new applications built for this new world",
    "start": "1107960",
    "end": "1113240"
  },
  {
    "text": "and I think there's some truth to that",
    "start": "1113240",
    "end": "1116860"
  },
  {
    "text": "now it's working so I I think one of you mentioned this um",
    "start": "1123080",
    "end": "1128380"
  },
  {
    "text": "challenge that we have that I frequently we we here mentioned in the context of HPC that users want to control",
    "start": "1128380",
    "end": "1135140"
  },
  {
    "text": "everything and they want to like Drive everything but then interestingly I think two talks before we've seen that",
    "start": "1135140",
    "end": "1141260"
  },
  {
    "text": "funny slide from from that check university presentation where like we",
    "start": "1141260",
    "end": "1146299"
  },
  {
    "text": "were was showing like how bad users are in like efficiency of their workloads which is I to my experience at least I'm",
    "start": "1146299",
    "end": "1154039"
  },
  {
    "text": "going to do the presentation how bad most users are in in controlling these",
    "start": "1154039",
    "end": "1160760"
  },
  {
    "text": "things so I'm wondering whether our drive towards all of this very sophisticated features towards",
    "start": "1160760",
    "end": "1166640"
  },
  {
    "text": "controlling the processes controlling scheduling aren't we what's your opinion aren't we actually over indexing on a",
    "start": "1166640",
    "end": "1174380"
  },
  {
    "text": "very small fraction of power users versus delivering something that is going to be more broadly power powerful",
    "start": "1174380",
    "end": "1180799"
  },
  {
    "text": "for for broader community so at least sort of at the research our",
    "start": "1180799",
    "end": "1187940"
  },
  {
    "text": "users are absolutely terrible in knowing what they want on the other hand they're using",
    "start": "1187940",
    "end": "1193820"
  },
  {
    "text": "state-of-the-art machine learning Frameworks and the authors of those Frameworks know exactly what they're",
    "start": "1193820",
    "end": "1200120"
  },
  {
    "text": "supposed to need and so I think that's where you want to put the smarts of figuring out exactly",
    "start": "1200120",
    "end": "1205820"
  },
  {
    "text": "sort of what Newmar topology awareness is necessary for this particular application and so on because I know today they don't really write normal",
    "start": "1205820",
    "end": "1211820"
  },
  {
    "text": "general purpose codes for submitting into our clusters they just Express their problem in terms of some high",
    "start": "1211820",
    "end": "1217700"
  },
  {
    "text": "level framework and that high level framework you can do all of this for them I believe",
    "start": "1217700",
    "end": "1223539"
  },
  {
    "text": "okay great",
    "start": "1228740",
    "end": "1235600"
  },
  {
    "text": "thank you",
    "start": "1260660",
    "end": "1262780"
  },
  {
    "text": "all right okay I hear myself now so let me restart I think that's a",
    "start": "1265880",
    "end": "1270980"
  },
  {
    "text": "really interesting question I think that it's true that a lot of users don't understand how to squeeze the most",
    "start": "1270980",
    "end": "1276860"
  },
  {
    "text": "amount of uh capability out of a given set of Hardware but I think on the other hand that there are power users as well",
    "start": "1276860",
    "end": "1283480"
  },
  {
    "text": "that run at extremely large scale like exascale and that is something that that",
    "start": "1283480",
    "end": "1289460"
  },
  {
    "text": "users need to be able to do in order to squeeze as much capability out of you",
    "start": "1289460",
    "end": "1294679"
  },
  {
    "text": "know really cutting edge machines to do science at massive massive scales um so I think I think kubernetes needs",
    "start": "1294679",
    "end": "1301400"
  },
  {
    "text": "to be able to do both in that sense because I see that there's an increasing amount of interest for these large-scale",
    "start": "1301400",
    "end": "1307760"
  },
  {
    "text": "complex scientific workflows to run anywhere not just on bare metal and HPC",
    "start": "1307760",
    "end": "1313100"
  },
  {
    "text": "but also in kubernetes in the cloud thank you",
    "start": "1313100",
    "end": "1319280"
  },
  {
    "text": "yeah I wanted to go back to something earlier that you know it's a common pattern that people use multi-cluster",
    "start": "1319280",
    "end": "1325820"
  },
  {
    "text": "because one one control plane can't take it that actually really speaks to the",
    "start": "1325820",
    "end": "1331340"
  },
  {
    "text": "fact that either we accept that or and or we invest to try to make one control plane",
    "start": "1331340",
    "end": "1337159"
  },
  {
    "text": "larger which is probably never going to work but as a batch community what can we do to make Federated",
    "start": "1337159",
    "end": "1343520"
  },
  {
    "text": "clusters because it's such a common pattern literally all of us seem to have a solution for",
    "start": "1343520",
    "end": "1348860"
  },
  {
    "text": "and start to foment something that is reusable between Solutions",
    "start": "1348860",
    "end": "1355419"
  },
  {
    "text": "okay um so yeah I think that's that's a great",
    "start": "1366980",
    "end": "1372620"
  },
  {
    "text": "observation and and something we've been thinking about for for some time",
    "start": "1372620",
    "end": "1377780"
  },
  {
    "text": "um I do think that uh at some level we need",
    "start": "1377780",
    "end": "1383299"
  },
  {
    "text": "to start to think about how we do this Federation for batch jobs across the the",
    "start": "1383299",
    "end": "1390559"
  },
  {
    "text": "different clusters but in an upstream point of view whether we",
    "start": "1390559",
    "end": "1396700"
  },
  {
    "text": "enable the ability to provision clusters just for scheduling",
    "start": "1396700",
    "end": "1403220"
  },
  {
    "text": "in other words you you have a scheduling sort of mini cluster uh there's some interesting work with",
    "start": "1403220",
    "end": "1411260"
  },
  {
    "text": "KPC which provides an API a kubernetes AP I don't know if folks are familiar",
    "start": "1411260",
    "end": "1417380"
  },
  {
    "text": "with that which which is something we may be able to to look at which would be able to put a scheduler",
    "start": "1417380",
    "end": "1424580"
  },
  {
    "text": "behind that that would would Federate workloads across the different clusters",
    "start": "1424580",
    "end": "1431299"
  },
  {
    "text": "um but I do I think it should be part of the batch work group",
    "start": "1431299",
    "end": "1436960"
  },
  {
    "text": "agenda at least to evaluate how we might approach it but I think there's a lot",
    "start": "1436960",
    "end": "1443419"
  },
  {
    "text": "more work that needs to be done for that I just have one quick comment about this",
    "start": "1443419",
    "end": "1450080"
  },
  {
    "text": "as well like all the solutions that we've seen were mostly related to specific use cases right like we had the",
    "start": "1450080",
    "end": "1456620"
  },
  {
    "text": "spark implemented for spark uh we are Implement for specific type workloads embarrassingly parallel for example the",
    "start": "1456620",
    "end": "1463820"
  },
  {
    "text": "challenge is actually coming up with something extremely genetic and acceptable by the community that is I in",
    "start": "1463820",
    "end": "1469760"
  },
  {
    "text": "my in my opinion that's like 10x more complex um so you can definitely solve it for",
    "start": "1469760",
    "end": "1475460"
  },
  {
    "text": "one or two problems but then if you want to make it generic that's where the complexity",
    "start": "1475460",
    "end": "1481700"
  },
  {
    "text": "starts to shoot up let me add on to what Abdullah said also it's not just the",
    "start": "1481700",
    "end": "1487460"
  },
  {
    "text": "cluster that needs to scale then but if you use for instance the spark operator or something else",
    "start": "1487460",
    "end": "1493280"
  },
  {
    "text": "all these things need to scale up and what we've seen with our users that have",
    "start": "1493280",
    "end": "1500360"
  },
  {
    "text": "been using for instance the spark operator it does not scale up that high either so you've already got multiple",
    "start": "1500360",
    "end": "1506720"
  },
  {
    "text": "instances of the operator in the cluster of a thousand nodes so",
    "start": "1506720",
    "end": "1513380"
  },
  {
    "text": "it is yeah you can you can do one little bit but if you don't have every single",
    "start": "1513380",
    "end": "1519320"
  },
  {
    "text": "little bit in your whole chain you don't scale up anyway so",
    "start": "1519320",
    "end": "1525980"
  },
  {
    "text": "foreign so um quick question on the math involved so different chips have",
    "start": "1525980",
    "end": "1531799"
  },
  {
    "text": "different some of them don't follow the IEEE standard so if you look at the cell architecture for instance for the PS",
    "start": "1531799",
    "end": "1537440"
  },
  {
    "text": "PlayStations they don't follow it and they're not the only ones that have different architecture for floating point",
    "start": "1537440",
    "end": "1543320"
  },
  {
    "text": "how do you think we should move forward to make sure that the math is correct if we're looking for actual accuracy I",
    "start": "1543320",
    "end": "1550400"
  },
  {
    "text": "understand training doesn't always matter but other workloads do",
    "start": "1550400",
    "end": "1555100"
  },
  {
    "text": "the queue has this really a q project that is really nice feature which is called node flavors I believe you can",
    "start": "1556400",
    "end": "1563000"
  },
  {
    "text": "comment on this I guess you know better me whereby you have some set of labels I",
    "start": "1563000",
    "end": "1568279"
  },
  {
    "text": "believe right and a particular set of labels that have the same value denotes",
    "start": "1568279",
    "end": "1573440"
  },
  {
    "text": "a particular node flavor and so for example scheduling could guarantee that all of your nodes are scheduled across",
    "start": "1573440",
    "end": "1580220"
  },
  {
    "text": "nodes with identical node flavors which I think would be an elegant solution to problems such as this",
    "start": "1580220",
    "end": "1586700"
  },
  {
    "text": "at least then you are consistently incorrect if you are incorrect",
    "start": "1586700",
    "end": "1591700"
  },
  {
    "text": "hi guys see if this works uh so what thing that seems to be kind of an",
    "start": "1600980",
    "end": "1607059"
  },
  {
    "text": "assumption is that something else has to get bolted in to do a lot of coordination either",
    "start": "1607059",
    "end": "1613880"
  },
  {
    "text": "Federation between different clusters different scheduling Frameworks loading",
    "start": "1613880",
    "end": "1619700"
  },
  {
    "text": "things into the scheduling framework and the operator stack my question kind of goes to the start",
    "start": "1619700",
    "end": "1626900"
  },
  {
    "text": "before you get to that layer what is the expected user interface for just coming",
    "start": "1626900",
    "end": "1631940"
  },
  {
    "text": "in and throwing batch workload at a system and then how does that feed back into",
    "start": "1631940",
    "end": "1638900"
  },
  {
    "text": "those designs and how you tackle user identities versus our back",
    "start": "1638900",
    "end": "1647360"
  },
  {
    "text": "do you have any sort of approach that starts to tackle that uh and kind of where does that then spill over into the",
    "start": "1647360",
    "end": "1653779"
  },
  {
    "text": "rest of this architecture",
    "start": "1653779",
    "end": "1656860"
  },
  {
    "text": "so from a unicorn perspective we have taken the approach that we want to",
    "start": "1660919",
    "end": "1667100"
  },
  {
    "text": "support things as simple as possible no cids nothing we allow you to uh we",
    "start": "1667100",
    "end": "1674480"
  },
  {
    "text": "schedule based on annotations that you put on the pot and that's it the rest we",
    "start": "1674480",
    "end": "1681679"
  },
  {
    "text": "use the standard kubernetes objects so we've tried to keep it as simple as possible as close to",
    "start": "1681679",
    "end": "1688100"
  },
  {
    "text": "uh kubernetes as we can that's that was the doll setup that we",
    "start": "1688100",
    "end": "1693919"
  },
  {
    "text": "we started with so and from user perspective",
    "start": "1693919",
    "end": "1699860"
  },
  {
    "text": "um we're working on use equalities at the moment we're working on the design dock so that's that's coming up and",
    "start": "1699860",
    "end": "1708919"
  },
  {
    "text": "a thing that's going Upstream um probably in the next two weeks or so",
    "start": "1708919",
    "end": "1715520"
  },
  {
    "text": "so design docs should be around on the Apache unicorn website and how we",
    "start": "1715520",
    "end": "1722480"
  },
  {
    "text": "think about tackling that it's a whole different story",
    "start": "1722480",
    "end": "1727840"
  },
  {
    "text": "yeah so in our motherland we have the notion of a job that we use to start off",
    "start": "1728299",
    "end": "1734240"
  },
  {
    "text": "essentially a bag of kubernetes objects and so you'll give us your bag of kubernetes objects and you say one of",
    "start": "1734240",
    "end": "1739880"
  },
  {
    "text": "them is my main objects and whenever that object then dies for whatever reason typically support that runs",
    "start": "1739880",
    "end": "1745279"
  },
  {
    "text": "through completion we just kill all of the other resources and user identities is based entirely",
    "start": "1745279",
    "end": "1751159"
  },
  {
    "text": "around queues so when you give us your back objects you tell them put all of these bags these bag objects into this",
    "start": "1751159",
    "end": "1756200"
  },
  {
    "text": "queue and then that maps to your single namespace inside of kubernetes",
    "start": "1756200",
    "end": "1762200"
  },
  {
    "text": "and so in this way I think it's sort of we try to keep it fairly kubernetes native while still making it appear as",
    "start": "1762200",
    "end": "1768380"
  },
  {
    "text": "to be sort of a batch style job with a life cycle that is easy to reason about",
    "start": "1768380",
    "end": "1775840"
  },
  {
    "text": "world um we're about to release a quota management solution",
    "start": "1779360",
    "end": "1785960"
  },
  {
    "text": "where it's hierarchical there's borrowing and sharing and essentially",
    "start": "1785960",
    "end": "1791899"
  },
  {
    "text": "you can express it as a an rvac identity",
    "start": "1791899",
    "end": "1797059"
  },
  {
    "text": "right you define how you want to label all of this quotas related to it there's",
    "start": "1797059",
    "end": "1804620"
  },
  {
    "text": "no specific integration with actually reading the hardback objects but uh that's something that we",
    "start": "1804620",
    "end": "1814100"
  },
  {
    "text": "could look at but in the initially we allow the what we're calling the code of",
    "start": "1814100",
    "end": "1819140"
  },
  {
    "text": "admin to be able to Define what those definitions are and then set the limits",
    "start": "1819140",
    "end": "1824360"
  },
  {
    "text": "and then with that obviously you can manage the users in that way",
    "start": "1824360",
    "end": "1831158"
  },
  {
    "text": "I guess one like one major difference between of course Services The Bachelor because the services like you have I",
    "start": "1832039",
    "end": "1837740"
  },
  {
    "text": "don't know the user has there's a service which is a web page right with that the user is actually trying to do",
    "start": "1837740",
    "end": "1843260"
  },
  {
    "text": "something with the system directly they're trying to submit something and at least what I observed in for example",
    "start": "1843260",
    "end": "1848659"
  },
  {
    "text": "machine learning they have sdks right like they start from an SDK they Define everything in the code in Python Etc",
    "start": "1848659",
    "end": "1855140"
  },
  {
    "text": "that's their API for submitting jobs and then everything",
    "start": "1855140",
    "end": "1860600"
  },
  {
    "text": "after that is handled by the SDK it creates kubernetes objects etc etc",
    "start": "1860600",
    "end": "1866179"
  },
  {
    "text": "I don't know if that like model can translate to something like with slurs the same thing you have probably these",
    "start": "1866179",
    "end": "1872419"
  },
  {
    "text": "command line interfaces that people grow like used to like Slayer submit and all these things and",
    "start": "1872419",
    "end": "1878840"
  },
  {
    "text": "I guess it depends on the type of workload but I feel with batch we need an SDK we need these command",
    "start": "1878840",
    "end": "1884960"
  },
  {
    "text": "line tools and then those translate into something else it depends on how you set the system",
    "start": "1884960",
    "end": "1892720"
  },
  {
    "text": "hmm to extend to his question",
    "start": "1892880",
    "end": "1898039"
  },
  {
    "text": "do you think like a batch need to support like a multi-tenant support",
    "start": "1898039",
    "end": "1905440"
  },
  {
    "text": "yes there's a huge push for multi-tenancy support and that's why we're also",
    "start": "1914179",
    "end": "1921020"
  },
  {
    "text": "looking at the decided to use the qualities and all that kind of stuff so we see that push from from",
    "start": "1921020",
    "end": "1928399"
  },
  {
    "text": "the uses that we've got and then multi-tenancy doesn't mean",
    "start": "1928399",
    "end": "1934580"
  },
  {
    "text": "for for them multiple companies but even within companies there's different",
    "start": "1934580",
    "end": "1939919"
  },
  {
    "text": "groups that get a quota assigned or different users that have got a quota assigned so yeah we we do see that and",
    "start": "1939919",
    "end": "1948799"
  },
  {
    "text": "that's huge pressure",
    "start": "1948799",
    "end": "1953020"
  },
  {
    "text": "yeah indeed also in Armada maybe we have many users internally and there's a lot",
    "start": "1954559",
    "end": "1960919"
  },
  {
    "text": "of like it's very important obviously to the users that each of them get their fair share because they will come and",
    "start": "1960919",
    "end": "1966500"
  },
  {
    "text": "complain to us if we don't give them their fair share and so for this reason we do have all of these things we have sort of per user",
    "start": "1966500",
    "end": "1972620"
  },
  {
    "text": "quotas we have per user quotas per scheduling round so they're not a single user can hook too much capacity over a",
    "start": "1972620",
    "end": "1978919"
  },
  {
    "text": "short span of time um and then yeah also the sort of total",
    "start": "1978919",
    "end": "1984620"
  },
  {
    "text": "capacity of each particular cluster per user as well and then we schedule jobs in an order",
    "start": "1984620",
    "end": "1991220"
  },
  {
    "text": "determined by which who usually has the lowest fraction of their quota currently",
    "start": "1991220",
    "end": "1997820"
  },
  {
    "text": "and then we also have per priority class quotas so that if you have some",
    "start": "1997820",
    "end": "2003159"
  },
  {
    "text": "preemptible jobs we can give you more resources than if your jobs are non-preemptable",
    "start": "2003159",
    "end": "2009240"
  },
  {
    "text": "uh yeah just an observation on all these things we're designing is it the case that really we're all",
    "start": "2010840",
    "end": "2017320"
  },
  {
    "text": "just sort of architecting and optimizing around the fact that well a few things we're all scared of breaking out CD",
    "start": "2017320",
    "end": "2024640"
  },
  {
    "text": "and if that's scaled just imagine it scaled infinitely then that wouldn't be a problem and then secondly the multi-cluster",
    "start": "2024640",
    "end": "2031960"
  },
  {
    "text": "thing I think we all want that because that's a sensible thing to have for operational and resilience reasons if",
    "start": "2031960",
    "end": "2037299"
  },
  {
    "text": "there was just an automatic way that kubernetes clusters could be somehow aware of each other if we imagine both",
    "start": "2037299",
    "end": "2043240"
  },
  {
    "text": "those two problems were solved then we would all be kind of Unshackled to just build whatever Solutions we want",
    "start": "2043240",
    "end": "2049118"
  },
  {
    "text": "on top of it and it would be more obvious what the right one was for everyone that's just my observation I don't know",
    "start": "2049119",
    "end": "2055060"
  },
  {
    "text": "what you guys think about that should we focus on scaling LCD first or looking at different back end for",
    "start": "2055060",
    "end": "2060760"
  },
  {
    "text": "kubernetes maybe and having it use it in a more controlled way that wouldn't just allow you a rogue user to",
    "start": "2060760",
    "end": "2067300"
  },
  {
    "text": "destroy it with a single single button click sorry we haven't",
    "start": "2067300",
    "end": "2074138"
  },
  {
    "text": "really looked um at least our group hasn't really looked at the replacing the back end but definitely the you know",
    "start": "2074139",
    "end": "2081940"
  },
  {
    "text": "that's the big scare that's the motivation that some of our end users have multi-clusters is just essentially",
    "start": "2081940",
    "end": "2089080"
  },
  {
    "text": "to handle those limitations so um it's a good point and I don't know any if",
    "start": "2089080",
    "end": "2095679"
  },
  {
    "text": "there's any work around that but um it's very valid and I think you're right",
    "start": "2095679",
    "end": "2100900"
  },
  {
    "text": "once that's the biggest driver for multi-clusters on our end",
    "start": "2100900",
    "end": "2106559"
  },
  {
    "text": "I should I should say that uh part of the motivation for creating the the flux operator was to uh was to avoid a",
    "start": "2111460",
    "end": "2117940"
  },
  {
    "text": "perceived limitation of FCD as well in the sense that we would create this batch system uh with inside of",
    "start": "2117940",
    "end": "2123940"
  },
  {
    "text": "kubernetes it then can subdivide the resources and handle um the same number of tasks that flux",
    "start": "2123940",
    "end": "2130420"
  },
  {
    "text": "natively can handle which is tens hundreds of thousands perhaps millions of tasks",
    "start": "2130420",
    "end": "2137400"
  },
  {
    "text": "I mean this goes back to the first question they asked is actually hcd a fundamental part of kubernetes Canada is",
    "start": "2138220",
    "end": "2144040"
  },
  {
    "text": "not right like it's just the storage solution that they chose at the beginning but it can be replaced with",
    "start": "2144040",
    "end": "2149260"
  },
  {
    "text": "something that is you know infinitely scalable globally scalable right we have such Solutions in",
    "start": "2149260",
    "end": "2156040"
  },
  {
    "text": "our Cloud multiple Cloud providers if we just adopt the hcd API to those storage solutions that are globally scalable",
    "start": "2156040",
    "end": "2163660"
  },
  {
    "text": "it will solve most of these problems maybe we can have kubernetes on pulsar thank you",
    "start": "2163660",
    "end": "2171160"
  },
  {
    "text": "hi I'm just wondering if there's an opportunity for other projects like slurm to to actually",
    "start": "2171160",
    "end": "2179680"
  },
  {
    "text": "work with the kubernetes community to develop you know cues and different apis",
    "start": "2179680",
    "end": "2185500"
  },
  {
    "text": "so that slurm could actually do a lot of the scheduling and resource control that people are asking from the batch",
    "start": "2185500",
    "end": "2191500"
  },
  {
    "text": "Community but treat a kubernetes cluster as a resource and you know then you know you could",
    "start": "2191500",
    "end": "2198339"
  },
  {
    "text": "filter a lot of the problems I agree with that CD but I think that's a solvable problem but if you had",
    "start": "2198339",
    "end": "2203800"
  },
  {
    "text": "something like slurm that was extended you know people could still submit jobs as they're used to today in the HPC",
    "start": "2203800",
    "end": "2210820"
  },
  {
    "text": "Community or some internal communities I know a lot of people are also lsf users they'll dump hundreds of jobs over",
    "start": "2210820",
    "end": "2218500"
  },
  {
    "text": "thousands of nodes but that could be an opportunity to to merge those two",
    "start": "2218500",
    "end": "2223720"
  },
  {
    "text": "communities or or work with those two communities to solve some of these problems because they've already solved them",
    "start": "2223720",
    "end": "2229060"
  },
  {
    "text": "for you know bare metal clusters or other types of compute",
    "start": "2229060",
    "end": "2235079"
  },
  {
    "text": "but I guess is this most of these schedulers they did they were not built for kubernetes I guess unicore was was",
    "start": "2243540",
    "end": "2250540"
  },
  {
    "text": "unique like unicorn is component outside kubernetes um same thing with uh with your",
    "start": "2250540",
    "end": "2257619"
  },
  {
    "text": "scheduler you just use the scheduler plugins to basically offload the",
    "start": "2257619",
    "end": "2264220"
  },
  {
    "text": "scheduling or like call call outside into an external service can be done the same thing with slur",
    "start": "2264220",
    "end": "2269619"
  },
  {
    "text": "so like I mean what I'm trying to say is that that model is actually being adopted in multiple",
    "start": "2269619",
    "end": "2276040"
  },
  {
    "text": "other projects other than slur",
    "start": "2276040",
    "end": "2279480"
  },
  {
    "text": "um first I wanted to comment on FCB I remember",
    "start": "2283599",
    "end": "2288640"
  },
  {
    "text": "one of the topics of the last computer Summit in Valencia was that at CV was in",
    "start": "2288640",
    "end": "2293800"
  },
  {
    "text": "a crisis trying to find a maintainers so I guess my question is are we not doing",
    "start": "2293800",
    "end": "2300940"
  },
  {
    "text": "enough as a batch Community to you know go back to to hcd in this case and",
    "start": "2300940",
    "end": "2307800"
  },
  {
    "text": "improve it for our needs and same about",
    "start": "2307800",
    "end": "2313240"
  },
  {
    "text": "applications um like spark or yarn and uh MPI those are",
    "start": "2313240",
    "end": "2323079"
  },
  {
    "text": "all very old Frameworks uh that kind of assume that they have like",
    "start": "2323079",
    "end": "2329500"
  },
  {
    "text": "certain characteristics and in like they don't know about Parts they just assume they're running in bare metal even",
    "start": "2329500",
    "end": "2337660"
  },
  {
    "text": "um they assume everything is uh you only run when everything is already",
    "start": "2337660",
    "end": "2343980"
  },
  {
    "text": "set up and should we be reaching back",
    "start": "2343980",
    "end": "2349480"
  },
  {
    "text": "to let's say open API Intel API or Spark",
    "start": "2349480",
    "end": "2358540"
  },
  {
    "text": "um maintainers to actually help them or like help them be more Cloud native to",
    "start": "2358540",
    "end": "2367060"
  },
  {
    "text": "be more resilient to failure I recently was in a discussion also with",
    "start": "2367060",
    "end": "2372579"
  },
  {
    "text": "Tim Hawking uh where there was a cap about discussing a",
    "start": "2372579",
    "end": "2379300"
  },
  {
    "text": "startup order of containers and Tim cooking was saying well the",
    "start": "2379300",
    "end": "2385359"
  },
  {
    "text": "containers should be resilient to not having all the setup already there but",
    "start": "2385359",
    "end": "2391060"
  },
  {
    "text": "that's not the reality right MPI operator has that problem um",
    "start": "2391060",
    "end": "2396760"
  },
  {
    "text": "and I don't know about spark too much but uh so back to my question like are we",
    "start": "2396760",
    "end": "2403300"
  },
  {
    "text": "doing enough reaching back to those communities are we um helping them move forward so it they",
    "start": "2403300",
    "end": "2410320"
  },
  {
    "text": "simplify the things for us as kubernetes developers and so on",
    "start": "2410320",
    "end": "2417300"
  },
  {
    "text": "so I just wanted to comment on the the MPI at least I think that's a really really interesting point",
    "start": "2419740",
    "end": "2425440"
  },
  {
    "text": "um I I think there are efforts in MPI to make it more to make the communicator more flexible in a sense of dynamically",
    "start": "2425440",
    "end": "2432579"
  },
  {
    "text": "adding and removing ranks but I I don't know where that is I don't think that's production ready I think that's mainly",
    "start": "2432579",
    "end": "2438099"
  },
  {
    "text": "research at this point but I think that could be hugely hugely uh transformative",
    "start": "2438099",
    "end": "2444760"
  },
  {
    "text": "foreign one of our use cases we're using the array framework and we're very involved",
    "start": "2444760",
    "end": "2451839"
  },
  {
    "text": "with the with them in helping deliver some of our cueing and and um",
    "start": "2451839",
    "end": "2459579"
  },
  {
    "text": "dispatching policies with them so um as far as Ray is concerned we are",
    "start": "2459579",
    "end": "2466839"
  },
  {
    "text": "um not not enough I don't know enough about this work and",
    "start": "2466839",
    "end": "2472780"
  },
  {
    "text": "spark from its side is pretty flexible so you can tell it to start up with a",
    "start": "2476820",
    "end": "2483040"
  },
  {
    "text": "minimum number of executors and grow to a maximum number of executors and do all that kind of stuff for you in the",
    "start": "2483040",
    "end": "2489280"
  },
  {
    "text": "background but the end user needs to ask for it and if",
    "start": "2489280",
    "end": "2495640"
  },
  {
    "text": "they don't do it then you don't get it so it's it's partially also a education",
    "start": "2495640",
    "end": "2502839"
  },
  {
    "text": "of the end users to say this is how you submit and this is what you set up that",
    "start": "2502839",
    "end": "2509260"
  },
  {
    "text": "makes it easier for you um we still see people that run spark jobs",
    "start": "2509260",
    "end": "2516520"
  },
  {
    "text": "and then all the work is done in the driver but they're asking for 10 other",
    "start": "2516520",
    "end": "2522160"
  },
  {
    "text": "executors and waste an enormous amount of resources and then you tell them and say hey this",
    "start": "2522160",
    "end": "2529240"
  },
  {
    "text": "is what you're doing and they say oh wasn't the way that I was doing that",
    "start": "2529240",
    "end": "2534339"
  },
  {
    "text": "they don't have the view they don't look at it um",
    "start": "2534339",
    "end": "2539440"
  },
  {
    "text": "my way said hiding a lot of the things and and they just want to submit the job",
    "start": "2539440",
    "end": "2544780"
  },
  {
    "text": "and want to run quickly anybody can get the results back that's a lot of the end users that you",
    "start": "2544780",
    "end": "2550359"
  },
  {
    "text": "get when you want you run the spark things and I think that's the same with the guys from Armada",
    "start": "2550359",
    "end": "2557680"
  },
  {
    "text": "the people just submit the job they want to get the result back they don't look at how have our submitted this and how",
    "start": "2557680",
    "end": "2564880"
  },
  {
    "text": "many resources database up until you tell them here's the bill in the end and then they start",
    "start": "2564880",
    "end": "2571240"
  },
  {
    "text": "complaining so but they they don't get that because there's no use of accounting there's no",
    "start": "2571240",
    "end": "2577480"
  },
  {
    "text": "so there's if we provide that information back to them we can educate them and they will run better",
    "start": "2577480",
    "end": "2584140"
  },
  {
    "text": "so",
    "start": "2584140",
    "end": "2586619"
  },
  {
    "text": "thanks is this okay yeah um so I came into this very much from a",
    "start": "2589660",
    "end": "2594880"
  },
  {
    "text": "Services background and my work at a finance company and we're starting you know doing a lot of batch I'm thinking about kubernetes for that and it kind of",
    "start": "2594880",
    "end": "2601300"
  },
  {
    "text": "the reason why I mentioned that is that like you don't really care about users in services and we've talked a lot about",
    "start": "2601300",
    "end": "2607000"
  },
  {
    "text": "how impact you really do and it's you know we talked about queuing and scheduling and so on um and then I kind of it feels like a",
    "start": "2607000",
    "end": "2614260"
  },
  {
    "text": "lot of the solutions like there's nothing native in kubernetes to correlate the idea of a scheduling user to a workload like you kind of schedule",
    "start": "2614260",
    "end": "2621819"
  },
  {
    "text": "it and then it's never associated with you again and so we're all kind of building tools around it to help with that and I'm kind of curious about we",
    "start": "2621819",
    "end": "2628660"
  },
  {
    "text": "talked a lot about scheduling but storage is another problem that I'm really struggling with this I've spoken to people here about it already I'm kind",
    "start": "2628660",
    "end": "2635260"
  },
  {
    "text": "of curious about that idea of do we think there's a way to better correlate the like scheduling user or information",
    "start": "2635260",
    "end": "2641500"
  },
  {
    "text": "about the user into kubernetes itself or do we feel like we have to keep relying on building external things around it and kind of parallel onto that like",
    "start": "2641500",
    "end": "2648700"
  },
  {
    "text": "how are you guys or what do we think the solutions are for kind of interacting with non-api driven object storage so",
    "start": "2648700",
    "end": "2656440"
  },
  {
    "text": "posix file systems shared clustered file systems in a way that might actually work because the people I've spoken to",
    "start": "2656440",
    "end": "2662079"
  },
  {
    "text": "here don't seem very happy with the solutions they've built even if they've managed to make it work",
    "start": "2662079",
    "end": "2667480"
  },
  {
    "text": "um",
    "start": "2667480",
    "end": "2669540"
  },
  {
    "text": "yeah so we are very much in this situation where our users use a lot of file systems for various operations to",
    "start": "2675160",
    "end": "2681640"
  },
  {
    "text": "load data and so on and indeed this is one of the main issues with stability",
    "start": "2681640",
    "end": "2686740"
  },
  {
    "text": "and performance on our platform whereby say all of your users simultaneously try",
    "start": "2686740",
    "end": "2691780"
  },
  {
    "text": "to get that the same file on a file share and then everything falls over",
    "start": "2691780",
    "end": "2696940"
  },
  {
    "text": "so I think what we would really like in the long term is some way for kubernetes to be aware of the health of",
    "start": "2696940",
    "end": "2703540"
  },
  {
    "text": "the underlying health and capacity of the underlying file storage systems and to be able to share those resources",
    "start": "2703540",
    "end": "2709420"
  },
  {
    "text": "effectively because really this is something that we don't have at all and it's really causing us a lot of pain",
    "start": "2709420",
    "end": "2715859"
  },
  {
    "text": "is your question sometimes in the HPC World those sorts of problems are solved via tiered",
    "start": "2720099",
    "end": "2727060"
  },
  {
    "text": "storaging like burst buffers and that sort of thing so I wonder if there'd be some interest in the community kubernetes community or integrating that",
    "start": "2727060",
    "end": "2734500"
  },
  {
    "text": "if it hasn't already been done",
    "start": "2734500",
    "end": "2737640"
  },
  {
    "text": "using the fact that nobody wants to ask a question I just wanted to throw a comment defending at CD now this is this",
    "start": "2741339",
    "end": "2746920"
  },
  {
    "text": "is uh as I'm working on Google kubernetes engine and our internet testing pipelines test clusters up to 15",
    "start": "2746920",
    "end": "2753819"
  },
  {
    "text": "000 nodes by cluster the largest production clusters that our customers have are to have 12 000 nodes all spots",
    "start": "2753819",
    "end": "2760960"
  },
  {
    "text": "so very heavy on like Recycling and preemption so like this is a very",
    "start": "2760960",
    "end": "2766060"
  },
  {
    "text": "powerful tool like we need to fix a couple of problems like that Aldo mentioned like on quality of maintainability reliability with new",
    "start": "2766060",
    "end": "2772720"
  },
  {
    "text": "releases and that stuff but like I wouldn't like go with one of the",
    "start": "2772720",
    "end": "2778960"
  },
  {
    "text": "impression that it's it's a really bad storage system it's a really good one the community may need a little bit help",
    "start": "2778960",
    "end": "2784300"
  },
  {
    "text": "but but yeah thank you",
    "start": "2784300",
    "end": "2790380"
  },
  {
    "text": "so as there are no questions that I was going to do make a similar comment which is a couple of months ago people",
    "start": "2791079",
    "end": "2797980"
  },
  {
    "text": "realized that that city was left with one maintainer but that's that was a problem with the project but it was also",
    "start": "2797980",
    "end": "2805119"
  },
  {
    "text": "a show of the community that has been built around these tools because as soon as there was a call for help there were",
    "start": "2805119",
    "end": "2811780"
  },
  {
    "text": "a bunch of companies that jumped in and actually volunteered resources to fix it and maybe at CD is the current problem",
    "start": "2811780",
    "end": "2818740"
  },
  {
    "text": "but this problems keep repeating themselves with a bunch of tools that after the hype and when they become kind",
    "start": "2818740",
    "end": "2824200"
  },
  {
    "text": "of stable lose interest from a lot of people so we are seeing it with etcd but",
    "start": "2824200",
    "end": "2829720"
  },
  {
    "text": "we'll see to it any number of tools that will keep coming so jumping out of a",
    "start": "2829720",
    "end": "2835000"
  },
  {
    "text": "solution is sometimes not the best option maybe maybe there is room to fix it and then I had another comment which was",
    "start": "2835000",
    "end": "2842200"
  },
  {
    "text": "uh regarding the the opportunity of running batch on kubernetes there are",
    "start": "2842200",
    "end": "2848140"
  },
  {
    "text": "many options as we've heard today but I had a question because I come from an",
    "start": "2848140",
    "end": "2853240"
  },
  {
    "text": "environment where we were doing massive scale Computing before the cloud appeared and we built a bunch of custom",
    "start": "2853240",
    "end": "2859240"
  },
  {
    "text": "tools for two decades that we still run and uh moving those to kubernetes has",
    "start": "2859240",
    "end": "2865540"
  },
  {
    "text": "been a massive simplification and it also democratizes a bit the access to a large amount of resources with the",
    "start": "2865540",
    "end": "2871540"
  },
  {
    "text": "public Cloud I'm I wonder um how much of an effort should be in",
    "start": "2871540",
    "end": "2878560"
  },
  {
    "text": "these common apis given that we already have the kubernetes API as a base to",
    "start": "2878560",
    "end": "2884200"
  },
  {
    "text": "access all these resources you have options on premises you have options on all the major hyperscalers",
    "start": "2884200",
    "end": "2890800"
  },
  {
    "text": "um how do you see the the benefit of actually putting a lot of effort into having this single extensions to the job",
    "start": "2890800",
    "end": "2898180"
  },
  {
    "text": "API and and some sort of single scheduler instead of just relying on the",
    "start": "2898180",
    "end": "2904540"
  },
  {
    "text": "base kubernetes API and um and then people choose whatever they choose on top how much effort do you think we get",
    "start": "2904540",
    "end": "2912880"
  },
  {
    "text": "how much benefit we you think we get from this uh potential effort",
    "start": "2912880",
    "end": "2918960"
  },
  {
    "text": "so from our point of view I think it's a lot of benefit",
    "start": "2924060",
    "end": "2929440"
  },
  {
    "text": "um you can as you hear the talks today and from the other fellow panelists there's",
    "start": "2929440",
    "end": "2936339"
  },
  {
    "text": "a lot of common challenges that we have and to be able to solve as many of those",
    "start": "2936339",
    "end": "2942160"
  },
  {
    "text": "we can with one code base and have a community that supports that same code base",
    "start": "2942160",
    "end": "2947740"
  },
  {
    "text": "I think it becomes more effective we're become more efficient",
    "start": "2947740",
    "end": "2953020"
  },
  {
    "text": "right and um uh we can evolve it I think a lot",
    "start": "2953020",
    "end": "2958359"
  },
  {
    "text": "quicker so I think there's a lot of benefit I don't know if we'll be able to be able",
    "start": "2958359",
    "end": "2963640"
  },
  {
    "text": "to provide all the different features and fancy things that we've maybe individually in our projects have",
    "start": "2963640",
    "end": "2971079"
  },
  {
    "text": "provided but I think if we can try to gather as much of the common",
    "start": "2971079",
    "end": "2976260"
  },
  {
    "text": "functions and features I think it'll benefit the community very much so",
    "start": "2976260",
    "end": "2982980"
  },
  {
    "text": "we have time for one last question you have a comment",
    "start": "2983460",
    "end": "2989220"
  },
  {
    "text": "I guess a little bit more on the NCD thing I actually think like for us it's always never been NCD the API server can",
    "start": "2991720",
    "end": "2997780"
  },
  {
    "text": "go down right because you have like a Thundering Herd because you have so many nodes and you know there's a lot of",
    "start": "2997780",
    "end": "3004020"
  },
  {
    "text": "these lessons we've learned internally we've learned from each other just inside of our company unfortunately we mostly work proprietary",
    "start": "3004020",
    "end": "3011040"
  },
  {
    "text": "I kind of have a question like this has been an incredible Forum but it's been like one one day maybe one two days out",
    "start": "3011040",
    "end": "3016920"
  },
  {
    "text": "of the year I would really love to be able to communicate and be able to have these",
    "start": "3016920",
    "end": "3021960"
  },
  {
    "text": "kind of wonderful showcases what else can we do to encourage and to have more forms and more opportunities for folks",
    "start": "3021960",
    "end": "3028859"
  },
  {
    "text": "to you know be able to present and have and talk like this so Alex Cameron is raising his hands in",
    "start": "3028859",
    "end": "3034859"
  },
  {
    "text": "the back",
    "start": "3034859",
    "end": "3037160"
  },
  {
    "text": "all right I think we have run out of time do you any final remarks",
    "start": "3044040",
    "end": "3050000"
  },
  {
    "text": "I'll just mention that there are um there are containers in HPC and orchestrated containers in HBC workshops",
    "start": "3050400",
    "end": "3057059"
  },
  {
    "text": "at the super Computing conference as well and that's starting to gain traction we're starting to see a lot more users in that so coming from that",
    "start": "3057059",
    "end": "3063240"
  },
  {
    "text": "direction would be really interesting too I think those users that are used to kind of bare metal traditional HPC would",
    "start": "3063240",
    "end": "3069960"
  },
  {
    "text": "benefit a lot from hearing some of these presentations too Diana final remarks",
    "start": "3069960",
    "end": "3077180"
  },
  {
    "text": "I want to apologize for laughing but Alex was back there waving his hands so",
    "start": "3081240",
    "end": "3086280"
  },
  {
    "text": "yeah I I uh I want to thank everybody for um doing the presentations and",
    "start": "3086280",
    "end": "3092400"
  },
  {
    "text": "coming in and sharing their their thoughts ideas requests and I again I",
    "start": "3092400",
    "end": "3099420"
  },
  {
    "text": "just want to encourage you to come to the batch work group and help us",
    "start": "3099420",
    "end": "3105559"
  },
  {
    "text": "at least try to solve some of your issues or note be able to identify those",
    "start": "3105559",
    "end": "3110760"
  },
  {
    "text": "so we can make recommendations or hopefully Upstream some changes and I think that's it thank you",
    "start": "3110760",
    "end": "3118579"
  },
  {
    "text": "all right yeah last Mark also",
    "start": "3118680",
    "end": "3127319"
  },
  {
    "text": "I think also from an Apache unicorn we'll we're seeing way more traction out of the old big data kind of workloads",
    "start": "3127319",
    "end": "3135480"
  },
  {
    "text": "kind of moves over onto the kubernetes side so there's um",
    "start": "3135480",
    "end": "3140640"
  },
  {
    "text": "there is demand now from that side so we'll see more of that and I think when",
    "start": "3140640",
    "end": "3147180"
  },
  {
    "text": "I looked at all the presentations today we all have the same",
    "start": "3147180",
    "end": "3152700"
  },
  {
    "text": "kind of limitation on either the job API or so it's it's not that we want to do",
    "start": "3152700",
    "end": "3159839"
  },
  {
    "text": "different things we just need to get together in the group and then move on",
    "start": "3159839",
    "end": "3166760"
  },
  {
    "text": "yeah so similar for me we've built Armada because we needed to address what",
    "start": "3167339",
    "end": "3172380"
  },
  {
    "text": "we perceive to be limitations in kubernetes but over time as kubernetes can natively sort of address this we",
    "start": "3172380",
    "end": "3179700"
  },
  {
    "text": "would love to come back closer to just straight up regular kubernetes again as",
    "start": "3179700",
    "end": "3185579"
  },
  {
    "text": "close as we possibly can because we want to use the standard interfaces",
    "start": "3185579",
    "end": "3190160"
  },
  {
    "text": "all right thank you so much [Applause]",
    "start": "3191240",
    "end": "3199840"
  }
]