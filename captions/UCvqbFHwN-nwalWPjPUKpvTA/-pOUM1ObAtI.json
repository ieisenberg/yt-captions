[
  {
    "text": "we are both very excited and grateful to",
    "start": "40",
    "end": "2040"
  },
  {
    "text": "uh have this opportunity to share some",
    "start": "2040",
    "end": "3719"
  },
  {
    "text": "of our recent work and in particular how",
    "start": "3719",
    "end": "6160"
  },
  {
    "text": "we leverage celium in order to uh secure",
    "start": "6160",
    "end": "8960"
  },
  {
    "text": "the network connectivity of our",
    "start": "8960",
    "end": "10400"
  },
  {
    "text": "workloads amongst multiple communities",
    "start": "10400",
    "end": "12799"
  },
  {
    "text": "clusters uh so OS and I we both work at",
    "start": "12799",
    "end": "15400"
  },
  {
    "text": "data dog uh and we are working a team in",
    "start": "15400",
    "end": "18520"
  },
  {
    "text": "charge of uh the communities Network",
    "start": "18520",
    "end": "20199"
  },
  {
    "text": "platform there and in case you don't",
    "start": "20199",
    "end": "21840"
  },
  {
    "text": "know about us uh we are a software",
    "start": "21840",
    "end": "24119"
  },
  {
    "text": "Service uh observ observability platform",
    "start": "24119",
    "end": "28039"
  },
  {
    "text": "and we help our customers uh to get",
    "start": "28039",
    "end": "30759"
  },
  {
    "text": "better visibility into the applications",
    "start": "30759",
    "end": "32840"
  },
  {
    "text": "and infrastructure to give you a bit of",
    "start": "32840",
    "end": "36200"
  },
  {
    "text": "scale a sense of scale has of now we",
    "start": "36200",
    "end": "38360"
  },
  {
    "text": "handle about well over 100 trillion",
    "start": "38360",
    "end": "41079"
  },
  {
    "text": "events a day data points if you will and",
    "start": "41079",
    "end": "43760"
  },
  {
    "text": "in order to manage this kind of traffic",
    "start": "43760",
    "end": "46239"
  },
  {
    "text": "uh we uh do so over a multicloud kues",
    "start": "46239",
    "end": "49680"
  },
  {
    "text": "based environment and of course to get",
    "start": "49680",
    "end": "52480"
  },
  {
    "text": "our Network packets flowing smoothly",
    "start": "52480",
    "end": "54840"
  },
  {
    "text": "between our hundreds of thousands of pod",
    "start": "54840",
    "end": "56719"
  },
  {
    "text": "amongst hundreds of clusters uh we",
    "start": "56719",
    "end": "58680"
  },
  {
    "text": "decided to use C to do so so as an",
    "start": "58680",
    "end": "62079"
  },
  {
    "text": "agenda for today we're going to start",
    "start": "62079",
    "end": "63760"
  },
  {
    "text": "with some refresher about what why",
    "start": "63760",
    "end": "65960"
  },
  {
    "text": "Network policies are important and we",
    "start": "65960",
    "end": "67680"
  },
  {
    "text": "why we need them in the first place uh",
    "start": "67680",
    "end": "70640"
  },
  {
    "text": "and how they are being implemented in",
    "start": "70640",
    "end": "72360"
  },
  {
    "text": "within celum uh most importantly uh",
    "start": "72360",
    "end": "74920"
  },
  {
    "text": "we'll then iterate over what Solutions",
    "start": "74920",
    "end": "76520"
  },
  {
    "text": "are available uh when we uh look to make",
    "start": "76520",
    "end": "79479"
  },
  {
    "text": "them work amongst multiple clusters and",
    "start": "79479",
    "end": "82240"
  },
  {
    "text": "finally we'll give you a bit of insights",
    "start": "82240",
    "end": "84640"
  },
  {
    "text": "around what we felt was interesting to",
    "start": "84640",
    "end": "86759"
  },
  {
    "text": "share about uh and keep in mind when",
    "start": "86759",
    "end": "89479"
  },
  {
    "text": "operating such a",
    "start": "89479",
    "end": "91240"
  },
  {
    "text": "setup so to illustrate our thoughts uh",
    "start": "91240",
    "end": "94479"
  },
  {
    "text": "we' like to start with a a simple web",
    "start": "94479",
    "end": "96840"
  },
  {
    "text": "application design that is composed of a",
    "start": "96840",
    "end": "98439"
  },
  {
    "text": "few components we have a front end a",
    "start": "98439",
    "end": "100119"
  },
  {
    "text": "backend a database a potential third",
    "start": "100119",
    "end": "102600"
  },
  {
    "text": "party service that uh we might need uh",
    "start": "102600",
    "end": "105240"
  },
  {
    "text": "to access and security being a priority",
    "start": "105240",
    "end": "107640"
  },
  {
    "text": "controlling what these components uh can",
    "start": "107640",
    "end": "109840"
  },
  {
    "text": "communicate with is Paramount in order",
    "start": "109840",
    "end": "111920"
  },
  {
    "text": "to uh reduce the potential attack",
    "start": "111920",
    "end": "114119"
  },
  {
    "text": "surface of our application and whether",
    "start": "114119",
    "end": "117039"
  },
  {
    "text": "the solution is being deployed on",
    "start": "117039",
    "end": "118360"
  },
  {
    "text": "premises or over a cloud provider uh",
    "start": "118360",
    "end": "121360"
  },
  {
    "text": "there is most likely existing Primitives",
    "start": "121360",
    "end": "123560"
  },
  {
    "text": "that uh we can leverage in order to",
    "start": "123560",
    "end": "125840"
  },
  {
    "text": "secure uh such uh the communication of",
    "start": "125840",
    "end": "128920"
  },
  {
    "text": "those components so for instance if that",
    "start": "128920",
    "end": "131120"
  },
  {
    "text": "application were to run on AWS uh",
    "start": "131120",
    "end": "134080"
  },
  {
    "text": "natively uh we could leverage security",
    "start": "134080",
    "end": "136400"
  },
  {
    "text": "groups or am policies in order to",
    "start": "136400",
    "end": "138680"
  },
  {
    "text": "explicitly authorize uh the connectivity",
    "start": "138680",
    "end": "141040"
  },
  {
    "text": "permissions uh between them all however",
    "start": "141040",
    "end": "144519"
  },
  {
    "text": "as soon as we add kubes into the mix it",
    "start": "144519",
    "end": "146720"
  },
  {
    "text": "becomes much trickier because our",
    "start": "146720",
    "end": "148760"
  },
  {
    "text": "components will from now on be",
    "start": "148760",
    "end": "150480"
  },
  {
    "text": "containerized running as pods uh",
    "start": "150480",
    "end": "152519"
  },
  {
    "text": "potentially over the same instances that",
    "start": "152519",
    "end": "154920"
  },
  {
    "text": "we used to have at first but ultimately",
    "start": "154920",
    "end": "157280"
  },
  {
    "text": "what we'll want to do is to make most of",
    "start": "157280",
    "end": "159800"
  },
  {
    "text": "uh the resources that we are paying for",
    "start": "159800",
    "end": "161879"
  },
  {
    "text": "and that can translate potentially by",
    "start": "161879",
    "end": "163480"
  },
  {
    "text": "using larger instance types uh and bpack",
    "start": "163480",
    "end": "166640"
  },
  {
    "text": "some components that do not necessarily",
    "start": "166640",
    "end": "168480"
  },
  {
    "text": "belong to the same application and",
    "start": "168480",
    "end": "170519"
  },
  {
    "text": "therefore not supposed to talk to their",
    "start": "170519",
    "end": "173000"
  },
  {
    "text": "neighbors or other pods uh across uh the",
    "start": "173000",
    "end": "175920"
  },
  {
    "text": "cluster and if we look at the",
    "start": "175920",
    "end": "177599"
  },
  {
    "text": "recommendation that AWS make uh around",
    "start": "177599",
    "end": "180080"
  },
  {
    "text": "the operation of an NS cluster uh",
    "start": "180080",
    "end": "182440"
  },
  {
    "text": "although this is valid for most",
    "start": "182440",
    "end": "183680"
  },
  {
    "text": "providers well they basically advise us",
    "start": "183680",
    "end": "185920"
  },
  {
    "text": "to allow all traffic between each node",
    "start": "185920",
    "end": "188000"
  },
  {
    "text": "of the cluster but how can we keep",
    "start": "188000",
    "end": "190200"
  },
  {
    "text": "communication uh secure in such case",
    "start": "190200",
    "end": "192440"
  },
  {
    "text": "then well clearly what they are",
    "start": "192440",
    "end": "195239"
  },
  {
    "text": "encouraging us to do here is to think of",
    "start": "195239",
    "end": "197400"
  },
  {
    "text": "using higher Primitives in order to do",
    "start": "197400",
    "end": "199720"
  },
  {
    "text": "so and this is what network policies are",
    "start": "199720",
    "end": "202239"
  },
  {
    "text": "here for well they're here to enable us",
    "start": "202239",
    "end": "205720"
  },
  {
    "text": "with finer grain control mechanisms that",
    "start": "205720",
    "end": "207799"
  },
  {
    "text": "are based upon ku's uh Primitives such",
    "start": "207799",
    "end": "210840"
  },
  {
    "text": "as resource types labels or any sort of",
    "start": "210840",
    "end": "213519"
  },
  {
    "text": "metadata that we can relate to through",
    "start": "213519",
    "end": "216000"
  },
  {
    "text": "communities abstractions and they enable",
    "start": "216000",
    "end": "218959"
  },
  {
    "text": "us so by letting us reference uh our",
    "start": "218959",
    "end": "221280"
  },
  {
    "text": "pods with their actual representation",
    "start": "221280",
    "end": "223280"
  },
  {
    "text": "within a communties cluster not through",
    "start": "223280",
    "end": "225439"
  },
  {
    "text": "their actual Network implementation",
    "start": "225439",
    "end": "227799"
  },
  {
    "text": "details uh such as the IP addresses or",
    "start": "227799",
    "end": "231040"
  },
  {
    "text": "uh the subnets they are part of but most",
    "start": "231040",
    "end": "233920"
  },
  {
    "text": "importantly when we Define a network",
    "start": "233920",
    "end": "236000"
  },
  {
    "text": "policy what we do is that we determine",
    "start": "236000",
    "end": "237959"
  },
  {
    "text": "which pod uh this policy applies to what",
    "start": "237959",
    "end": "241120"
  },
  {
    "text": "can connect to these pods and what those",
    "start": "241120",
    "end": "243640"
  },
  {
    "text": "pods can connect to and that's the real",
    "start": "243640",
    "end": "245760"
  },
  {
    "text": "key here as using such abstractions",
    "start": "245760",
    "end": "248959"
  },
  {
    "text": "gives us much more control on what we",
    "start": "248959",
    "end": "250879"
  },
  {
    "text": "want to Target as well as enables us",
    "start": "250879",
    "end": "254040"
  },
  {
    "text": "with with much more tightly coupled uh",
    "start": "254040",
    "end": "256680"
  },
  {
    "text": "configuration",
    "start": "256680",
    "end": "258120"
  },
  {
    "text": "capabilities so the first thing we'd",
    "start": "258120",
    "end": "259919"
  },
  {
    "text": "like to mention uh is something that we",
    "start": "259919",
    "end": "262440"
  },
  {
    "text": "which we had started with uh earlier on",
    "start": "262440",
    "end": "265479"
  },
  {
    "text": "and we are now migrating towards uh and",
    "start": "265479",
    "end": "267880"
  },
  {
    "text": "that would be starting from a denied by",
    "start": "267880",
    "end": "269960"
  },
  {
    "text": "default kind of approach where we would",
    "start": "269960",
    "end": "271759"
  },
  {
    "text": "have our users explicitly Define the",
    "start": "271759",
    "end": "274479"
  },
  {
    "text": "accesses that their components need uh",
    "start": "274479",
    "end": "277039"
  },
  {
    "text": "and that's because it's always much",
    "start": "277039",
    "end": "278600"
  },
  {
    "text": "easier to ask or to give permission to",
    "start": "278600",
    "end": "281400"
  },
  {
    "text": "someone rather than reclaiming it back",
    "start": "281400",
    "end": "283199"
  },
  {
    "text": "much later on and I won't get into",
    "start": "283199",
    "end": "285080"
  },
  {
    "text": "details into uh how to achieve so but",
    "start": "285080",
    "end": "288560"
  },
  {
    "text": "what I want to talk to you about now",
    "start": "288560",
    "end": "290320"
  },
  {
    "text": "well but know that there are many ways",
    "start": "290320",
    "end": "292280"
  },
  {
    "text": "to achieve to achieve it uh and what I",
    "start": "292280",
    "end": "294800"
  },
  {
    "text": "want to talk to you about now is how all",
    "start": "294800",
    "end": "297320"
  },
  {
    "text": "of this is being implemented Within",
    "start": "297320",
    "end": "298880"
  },
  {
    "text": "celum",
    "start": "298880",
    "end": "300440"
  },
  {
    "text": "well when we use Network policies with",
    "start": "300440",
    "end": "302320"
  },
  {
    "text": "celium there are two very important",
    "start": "302320",
    "end": "304280"
  },
  {
    "text": "Concepts which are essential to",
    "start": "304280",
    "end": "307120"
  },
  {
    "text": "understand how those policies are being",
    "start": "307120",
    "end": "308880"
  },
  {
    "text": "enforced and those two concepts are",
    "start": "308880",
    "end": "311560"
  },
  {
    "text": "identities and end points so an identity",
    "start": "311560",
    "end": "314840"
  },
  {
    "text": "in celium is basically a unique",
    "start": "314840",
    "end": "316840"
  },
  {
    "text": "identifier that is assigned within the",
    "start": "316840",
    "end": "318759"
  },
  {
    "text": "cluster it's derived from a set of",
    "start": "318759",
    "end": "321199"
  },
  {
    "text": "labels but in short what they do is that",
    "start": "321199",
    "end": "324319"
  },
  {
    "text": "they allow us to represent a set of pods",
    "start": "324319",
    "end": "326840"
  },
  {
    "text": "which are fairly similar that can be",
    "start": "326840",
    "end": "329080"
  },
  {
    "text": "part of the same deployment of or",
    "start": "329080",
    "end": "330919"
  },
  {
    "text": "replica set for instance and in",
    "start": "330919",
    "end": "332919"
  },
  {
    "text": "communities as pods may come and go",
    "start": "332919",
    "end": "335600"
  },
  {
    "text": "quite easily so will their IP addresses",
    "start": "335600",
    "end": "339080"
  },
  {
    "text": "that are associated to them and having",
    "start": "339080",
    "end": "341400"
  },
  {
    "text": "such abstractions makes it much easier",
    "start": "341400",
    "end": "343360"
  },
  {
    "text": "to scale out uh the enforcement of uh",
    "start": "343360",
    "end": "345880"
  },
  {
    "text": "our policies and I will dig into that a",
    "start": "345880",
    "end": "348440"
  },
  {
    "text": "bit later on endpoints on their own they",
    "start": "348440",
    "end": "350960"
  },
  {
    "text": "are here to keep track of the necessary",
    "start": "350960",
    "end": "353199"
  },
  {
    "text": "mappings between the cidi abstractions",
    "start": "353199",
    "end": "355759"
  },
  {
    "text": "and the actual Network implementation",
    "start": "355759",
    "end": "357400"
  },
  {
    "text": "details of the pods so if you briefly",
    "start": "357400",
    "end": "359120"
  },
  {
    "text": "look into how they are composed we can",
    "start": "359120",
    "end": "361280"
  },
  {
    "text": "notice that first there is a onetoone",
    "start": "361280",
    "end": "363120"
  },
  {
    "text": "mapping with a pod that the endpoint is",
    "start": "363120",
    "end": "366240"
  },
  {
    "text": "uh referencing they also have their own",
    "start": "366240",
    "end": "368720"
  },
  {
    "text": "unique ID within the cluster and uh",
    "start": "368720",
    "end": "372560"
  },
  {
    "text": "they have also uh interestingly a",
    "start": "372560",
    "end": "375960"
  },
  {
    "text": "onetoone mapping with an existing",
    "start": "375960",
    "end": "377880"
  },
  {
    "text": "identity that uh matches the labels of",
    "start": "377880",
    "end": "380639"
  },
  {
    "text": "the Pod that is being referenced and",
    "start": "380639",
    "end": "382639"
  },
  {
    "text": "finally what we can see as well is that",
    "start": "382639",
    "end": "384440"
  },
  {
    "text": "they contain the IP address of the Pod",
    "start": "384440",
    "end": "386280"
  },
  {
    "text": "and the node that the Pod is running on",
    "start": "386280",
    "end": "389360"
  },
  {
    "text": "now if you look at the bigger picture",
    "start": "389360",
    "end": "391240"
  },
  {
    "text": "here to enforce Network policies each",
    "start": "391240",
    "end": "393840"
  },
  {
    "text": "celium agent has to keep track of all of",
    "start": "393840",
    "end": "397199"
  },
  {
    "text": "the endpoints and identities that are",
    "start": "397199",
    "end": "399360"
  },
  {
    "text": "part of the cluster however as you can",
    "start": "399360",
    "end": "401599"
  },
  {
    "text": "see there as well there is no",
    "start": "401599",
    "end": "403319"
  },
  {
    "text": "peer-to-peer sharing of that information",
    "start": "403319",
    "end": "405240"
  },
  {
    "text": "instead they all rely onto a centralized",
    "start": "405240",
    "end": "407440"
  },
  {
    "text": "source of Truth and that source of Truth",
    "start": "407440",
    "end": "409639"
  },
  {
    "text": "can be implemented in a couple ways",
    "start": "409639",
    "end": "411919"
  },
  {
    "text": "either through the Clusters API server",
    "start": "411919",
    "end": "414440"
  },
  {
    "text": "uh leveraging some custom resource",
    "start": "414440",
    "end": "416800"
  },
  {
    "text": "definitions which is now the default",
    "start": "416800",
    "end": "418720"
  },
  {
    "text": "method or they can leverage a dedicated",
    "start": "418720",
    "end": "421199"
  },
  {
    "text": "key Value Store such as an itcd cluster",
    "start": "421199",
    "end": "423240"
  },
  {
    "text": "for example and why is there two methods",
    "start": "423240",
    "end": "425879"
  },
  {
    "text": "though well there is a bit of history",
    "start": "425879",
    "end": "428120"
  },
  {
    "text": "behind it and Emos will cover that",
    "start": "428120",
    "end": "430240"
  },
  {
    "text": "shortly but Cent mostly it has to do",
    "start": "430240",
    "end": "433440"
  },
  {
    "text": "with the way they",
    "start": "433440",
    "end": "435280"
  },
  {
    "text": "scale so let's imagine we are using the",
    "start": "435280",
    "end": "437560"
  },
  {
    "text": "default method we have each identity and",
    "start": "437560",
    "end": "440039"
  },
  {
    "text": "endpoint being stored as an object",
    "start": "440039",
    "end": "442160"
  },
  {
    "text": "through a custom resource definition on",
    "start": "442160",
    "end": "443919"
  },
  {
    "text": "the cluster communties clusters API",
    "start": "443919",
    "end": "445879"
  },
  {
    "text": "server and let's say this is a fairly",
    "start": "445879",
    "end": "447919"
  },
  {
    "text": "large cluster composed of about five",
    "start": "447919",
    "end": "449520"
  },
  {
    "text": "5,000 nodes well in that case let's say",
    "start": "449520",
    "end": "452919"
  },
  {
    "text": "we also have someone which is attempting",
    "start": "452919",
    "end": "454720"
  },
  {
    "text": "to scale out a deployment and add up an",
    "start": "454720",
    "end": "457319"
  },
  {
    "text": "additional 100 pods well in such case",
    "start": "457319",
    "end": "460840"
  },
  {
    "text": "this simple upscale apparently",
    "start": "460840",
    "end": "463120"
  },
  {
    "text": "reasonable at First Sight will actually",
    "start": "463120",
    "end": "465759"
  },
  {
    "text": "result in triggering half a million",
    "start": "465759",
    "end": "468159"
  },
  {
    "text": "Watch updates from the API servers as",
    "start": "468159",
    "end": "470720"
  },
  {
    "text": "each celium agent will need to know",
    "start": "470720",
    "end": "472960"
  },
  {
    "text": "about these newon points which are being",
    "start": "472960",
    "end": "474639"
  },
  {
    "text": "created and as you can imagine having",
    "start": "474639",
    "end": "477159"
  },
  {
    "text": "such uh activity can rap L become an",
    "start": "477159",
    "end": "480159"
  },
  {
    "text": "issue if we can't have an efficient way",
    "start": "480159",
    "end": "482159"
  },
  {
    "text": "to manage that thankfully though there",
    "start": "482159",
    "end": "485159"
  },
  {
    "text": "are various ways uh to reduce this",
    "start": "485159",
    "end": "487199"
  },
  {
    "text": "pressure for example to name one we can",
    "start": "487199",
    "end": "489560"
  },
  {
    "text": "use celum and point slices since",
    "start": "489560",
    "end": "492080"
  },
  {
    "text": "v111 uh and that can greatly reduce uh",
    "start": "492080",
    "end": "495400"
  },
  {
    "text": "the amount of updates that are required",
    "start": "495400",
    "end": "497479"
  },
  {
    "text": "by batching them up",
    "start": "497479",
    "end": "499280"
  },
  {
    "text": "together at data dog we currently have",
    "start": "499280",
    "end": "502199"
  },
  {
    "text": "both uh Solutions so most of our",
    "start": "502199",
    "end": "505400"
  },
  {
    "text": "clusters which are fairly small in size",
    "start": "505400",
    "end": "508080"
  },
  {
    "text": "under a thousand nodes uh are running",
    "start": "508080",
    "end": "510479"
  },
  {
    "text": "under the default approach using the crd",
    "start": "510479",
    "end": "512800"
  },
  {
    "text": "based uh approach and our bigger ones of",
    "start": "512800",
    "end": "516839"
  },
  {
    "text": "have dedicated etcd clusters associated",
    "start": "516839",
    "end": "519200"
  },
  {
    "text": "with them and as our Noe count can",
    "start": "519200",
    "end": "521279"
  },
  {
    "text": "evolve over time we can now easily",
    "start": "521279",
    "end": "523200"
  },
  {
    "text": "switch from one to another thanks to uh",
    "start": "523200",
    "end": "526040"
  },
  {
    "text": "great contributions that our colleague",
    "start": "526040",
    "end": "527839"
  },
  {
    "text": "Anon made recently which allows us to uh",
    "start": "527839",
    "end": "531040"
  },
  {
    "text": "write to both modes uh in parallel which",
    "start": "531040",
    "end": "534360"
  },
  {
    "text": "makes the migration path uh very",
    "start": "534360",
    "end": "536519"
  },
  {
    "text": "straightforward from now",
    "start": "536519",
    "end": "538279"
  },
  {
    "text": "on now now if you'd like to get further",
    "start": "538279",
    "end": "540320"
  },
  {
    "text": "information about how those storage",
    "start": "540320",
    "end": "542160"
  },
  {
    "text": "modes compares to each other there is",
    "start": "542160",
    "end": "543680"
  },
  {
    "text": "also a great tool that that heos and",
    "start": "543680",
    "end": "545720"
  },
  {
    "text": "Marcel did during last cucon where they",
    "start": "545720",
    "end": "548680"
  },
  {
    "text": "dug into uh into",
    "start": "548680",
    "end": "550959"
  },
  {
    "text": "it so ultimately though there is only",
    "start": "550959",
    "end": "554200"
  },
  {
    "text": "aite number of no and pods that a KU",
    "start": "554200",
    "end": "557279"
  },
  {
    "text": "kues clusters uh is able to um support",
    "start": "557279",
    "end": "561399"
  },
  {
    "text": "So when your applications are about to",
    "start": "561399",
    "end": "563680"
  },
  {
    "text": "outgrow those numbers well it might be a",
    "start": "563680",
    "end": "566079"
  },
  {
    "text": "good time to think Beyond and where a",
    "start": "566079",
    "end": "568200"
  },
  {
    "text": "multicluster setup might become um",
    "start": "568200",
    "end": "571680"
  },
  {
    "text": "Paramount in order to keep your business",
    "start": "571680",
    "end": "574160"
  },
  {
    "text": "growing but what about our Network",
    "start": "574160",
    "end": "576320"
  },
  {
    "text": "policies though because as we can think",
    "start": "576320",
    "end": "578399"
  },
  {
    "text": "of our apps can now be running over",
    "start": "578399",
    "end": "580920"
  },
  {
    "text": "different clusters so would their",
    "start": "580920",
    "end": "583000"
  },
  {
    "text": "respective endpoints and",
    "start": "583000",
    "end": "585240"
  },
  {
    "text": "identities so then we need to think",
    "start": "585240",
    "end": "587079"
  },
  {
    "text": "about our",
    "start": "587079",
    "end": "588120"
  },
  {
    "text": "options first we could assume assuming",
    "start": "588120",
    "end": "590800"
  },
  {
    "text": "that our infrastructure provider can",
    "start": "590800",
    "end": "592680"
  },
  {
    "text": "handle the underlying network uh",
    "start": "592680",
    "end": "594399"
  },
  {
    "text": "connectivity and routing between our",
    "start": "594399",
    "end": "596240"
  },
  {
    "text": "nodes and pause across cluster we could",
    "start": "596240",
    "end": "598720"
  },
  {
    "text": "consider going back to some simpler good",
    "start": "598720",
    "end": "600839"
  },
  {
    "text": "old networking uh practices such as",
    "start": "600839",
    "end": "603000"
  },
  {
    "text": "layer 3 filtering however by doing so",
    "start": "603000",
    "end": "605399"
  },
  {
    "text": "that would also mean trading off those",
    "start": "605399",
    "end": "607440"
  },
  {
    "text": "uh final grain control that everyone is",
    "start": "607440",
    "end": "609480"
  },
  {
    "text": "now used to so that's not necessarily uh",
    "start": "609480",
    "end": "612120"
  },
  {
    "text": "an interesting uh fit in a similar vein",
    "start": "612120",
    "end": "615120"
  },
  {
    "text": "uh we could also think of leveraging",
    "start": "615120",
    "end": "616720"
  },
  {
    "text": "policies that can interact directly with",
    "start": "616720",
    "end": "619079"
  },
  {
    "text": "the underlying uh uh provider but this",
    "start": "619079",
    "end": "622600"
  },
  {
    "text": "is also not necessarily something that",
    "start": "622600",
    "end": "624240"
  },
  {
    "text": "is available for all providers but most",
    "start": "624240",
    "end": "626440"
  },
  {
    "text": "importantly that would also mean that uh",
    "start": "626440",
    "end": "629320"
  },
  {
    "text": "our policies are not so provider",
    "start": "629320",
    "end": "631519"
  },
  {
    "text": "agnostic anymore which is not",
    "start": "631519",
    "end": "634200"
  },
  {
    "text": "ideal otherwise if our endpoints are uh",
    "start": "634200",
    "end": "638120"
  },
  {
    "text": "resolvable globally onto our Network",
    "start": "638120",
    "end": "640360"
  },
  {
    "text": "potentially we could think of using DNS",
    "start": "640360",
    "end": "642560"
  },
  {
    "text": "policies to manage uh permissions this",
    "start": "642560",
    "end": "645440"
  },
  {
    "text": "would uh work potentially nicely for",
    "start": "645440",
    "end": "648360"
  },
  {
    "text": "egress unfortunately for Ingress the",
    "start": "648360",
    "end": "650600"
  },
  {
    "text": "support is still quite limited so uh",
    "start": "650600",
    "end": "653040"
  },
  {
    "text": "that would still remain tricky though",
    "start": "653040",
    "end": "655760"
  },
  {
    "text": "another idea that came to our mind when",
    "start": "655760",
    "end": "657279"
  },
  {
    "text": "we looked into it was the service smash",
    "start": "657279",
    "end": "659600"
  },
  {
    "text": "kind of approach so where we could place",
    "start": "659600",
    "end": "662040"
  },
  {
    "text": "some gateways on both edges of our",
    "start": "662040",
    "end": "664120"
  },
  {
    "text": "clusters that would uh end the Ingress",
    "start": "664120",
    "end": "666880"
  },
  {
    "text": "and the ESS traffic going from one",
    "start": "666880",
    "end": "669000"
  },
  {
    "text": "cluster to another but at data do though",
    "start": "669000",
    "end": "671839"
  },
  {
    "text": "our reports are already globally uh",
    "start": "671839",
    "end": "674079"
  },
  {
    "text": "rooted across clusters and Cloud",
    "start": "674079",
    "end": "675680"
  },
  {
    "text": "providers using the uh Native Primitives",
    "start": "675680",
    "end": "678120"
  },
  {
    "text": "that uh we obtain from them and so",
    "start": "678120",
    "end": "680160"
  },
  {
    "text": "adding gateways in the sole purpose of",
    "start": "680160",
    "end": "682399"
  },
  {
    "text": "enforcing security boundaries did not",
    "start": "682399",
    "end": "685040"
  },
  {
    "text": "seems like did not seem like the most",
    "start": "685040",
    "end": "686920"
  },
  {
    "text": "pertinent option at the time first uh",
    "start": "686920",
    "end": "690600"
  },
  {
    "text": "independently of which solution we would",
    "start": "690600",
    "end": "692279"
  },
  {
    "text": "pick we would have to ask our users to",
    "start": "692279",
    "end": "694800"
  },
  {
    "text": "change their existing policies which",
    "start": "694800",
    "end": "696680"
  },
  {
    "text": "would lead to frustration and also would",
    "start": "696680",
    "end": "698560"
  },
  {
    "text": "require some coordination in order to",
    "start": "698560",
    "end": "700760"
  },
  {
    "text": "roll that out and secondly by being so",
    "start": "700760",
    "end": "702959"
  },
  {
    "text": "loosely coupled those new approaches",
    "start": "702959",
    "end": "705000"
  },
  {
    "text": "would make our day two operations much",
    "start": "705000",
    "end": "706839"
  },
  {
    "text": "trickier to",
    "start": "706839",
    "end": "708279"
  },
  {
    "text": "support so the dream for us there would",
    "start": "708279",
    "end": "710760"
  },
  {
    "text": "actually to be able to let our users",
    "start": "710760",
    "end": "712600"
  },
  {
    "text": "migrate their workloads from one cluster",
    "start": "712600",
    "end": "714399"
  },
  {
    "text": "to the other without necessarily having",
    "start": "714399",
    "end": "716440"
  },
  {
    "text": "to change a single line of their",
    "start": "716440",
    "end": "717959"
  },
  {
    "text": "existing policies",
    "start": "717959",
    "end": "719839"
  },
  {
    "text": "and this is why we felt that investing",
    "start": "719839",
    "end": "722000"
  },
  {
    "text": "into cluster mesh seemed to be like the",
    "start": "722000",
    "end": "723880"
  },
  {
    "text": "most pertinent option for us and I will",
    "start": "723880",
    "end": "726200"
  },
  {
    "text": "not let Emos continue explaining you",
    "start": "726200",
    "end": "730279"
  },
  {
    "text": "why all right uh thanks Maxim so as we",
    "start": "734560",
    "end": "739240"
  },
  {
    "text": "just saw selum has a bunch of options",
    "start": "739240",
    "end": "742040"
  },
  {
    "text": "that allow you to cross that cluster",
    "start": "742040",
    "end": "743760"
  },
  {
    "text": "boundary today but all of those features",
    "start": "743760",
    "end": "746240"
  },
  {
    "text": "we saw until now are built with very",
    "start": "746240",
    "end": "748600"
  },
  {
    "text": "specific requirements in mind right but",
    "start": "748600",
    "end": "750959"
  },
  {
    "text": "if you wanted the kubernetes native",
    "start": "750959",
    "end": "753120"
  },
  {
    "text": "Network policy experience that you're",
    "start": "753120",
    "end": "754880"
  },
  {
    "text": "already used to within a single cluster",
    "start": "754880",
    "end": "757079"
  },
  {
    "text": "and expand that between multiple",
    "start": "757079",
    "end": "758800"
  },
  {
    "text": "clusters cluster mesh is the solution to",
    "start": "758800",
    "end": "761320"
  },
  {
    "text": "go so actually a quick show off hands",
    "start": "761320",
    "end": "763920"
  },
  {
    "text": "how many of you are already using a",
    "start": "763920",
    "end": "766000"
  },
  {
    "text": "variant of cluster",
    "start": "766000",
    "end": "768160"
  },
  {
    "text": "mesh okay good not not too many so we'll",
    "start": "768160",
    "end": "772440"
  },
  {
    "text": "start with some Basics so um so cluster",
    "start": "772440",
    "end": "775920"
  },
  {
    "text": "mesh at the Crux of it all cluster mesh",
    "start": "775920",
    "end": "778760"
  },
  {
    "text": "is doing is taking some of the state",
    "start": "778760",
    "end": "781279"
  },
  {
    "text": "that is required for celium to enforce a",
    "start": "781279",
    "end": "783279"
  },
  {
    "text": "network policy from one cluster and",
    "start": "783279",
    "end": "785680"
  },
  {
    "text": "moving that to other clusters that are",
    "start": "785680",
    "end": "787720"
  },
  {
    "text": "part of the mesh right so that's just",
    "start": "787720",
    "end": "789880"
  },
  {
    "text": "the Crux of cluster mesh and there are",
    "start": "789880",
    "end": "791480"
  },
  {
    "text": "multiple ways of doing it right and we",
    "start": "791480",
    "end": "793839"
  },
  {
    "text": "primarily at data dog focus on uniform",
    "start": "793839",
    "end": "796199"
  },
  {
    "text": "Network policy enforcement but there are",
    "start": "796199",
    "end": "798199"
  },
  {
    "text": "other features like global Service load",
    "start": "798199",
    "end": "800240"
  },
  {
    "text": "balancing you can take advantage",
    "start": "800240",
    "end": "802680"
  },
  {
    "text": "of so if you try to get started with",
    "start": "802680",
    "end": "805240"
  },
  {
    "text": "cluster mesh today there are a lot of",
    "start": "805240",
    "end": "808399"
  },
  {
    "text": "options and sometimes it can get really",
    "start": "808399",
    "end": "810519"
  },
  {
    "text": "confusing on which mode to pick and but",
    "start": "810519",
    "end": "813480"
  },
  {
    "text": "there are actually really good reasons",
    "start": "813480",
    "end": "815160"
  },
  {
    "text": "on why there are that many different",
    "start": "815160",
    "end": "817399"
  },
  {
    "text": "cluster mesh modes so to understand some",
    "start": "817399",
    "end": "819760"
  },
  {
    "text": "of those design decisions we need to",
    "start": "819760",
    "end": "821480"
  },
  {
    "text": "take a little look at the evolution of",
    "start": "821480",
    "end": "824160"
  },
  {
    "text": "cluster mesh so an interesting tidbit",
    "start": "824160",
    "end": "826480"
  },
  {
    "text": "here is that all the way back to celium",
    "start": "826480",
    "end": "829000"
  },
  {
    "text": "1.2 which was released around August",
    "start": "829000",
    "end": "831600"
  },
  {
    "text": "2018 celum actually had support for",
    "start": "831600",
    "end": "834440"
  },
  {
    "text": "cluster mesh and in this mode uh at this",
    "start": "834440",
    "end": "837120"
  },
  {
    "text": "point in time kubernetes actually did",
    "start": "837120",
    "end": "839240"
  },
  {
    "text": "not have custom resource definitions",
    "start": "839240",
    "end": "841040"
  },
  {
    "text": "right so all of the data that is",
    "start": "841040",
    "end": "842880"
  },
  {
    "text": "required by celium is stored in a",
    "start": "842880",
    "end": "844600"
  },
  {
    "text": "dedicated uh KV store either with at CD",
    "start": "844600",
    "end": "847519"
  },
  {
    "text": "or console",
    "start": "847519",
    "end": "849959"
  },
  {
    "text": "and almost an year after that celum",
    "start": "849959",
    "end": "852959"
  },
  {
    "text": "introduced a Mode called CD mode where",
    "start": "852959",
    "end": "855079"
  },
  {
    "text": "you can store the identity and endpoint",
    "start": "855079",
    "end": "856800"
  },
  {
    "text": "information within kubernetes itself at",
    "start": "856800",
    "end": "858720"
  },
  {
    "text": "crds this is almost a month before",
    "start": "858720",
    "end": "861279"
  },
  {
    "text": "kubernetes stabilized",
    "start": "861279",
    "end": "863399"
  },
  {
    "text": "crds and starting around uh celium 1 1.9",
    "start": "863399",
    "end": "867160"
  },
  {
    "text": "and 1.10 we started seeing a new",
    "start": "867160",
    "end": "869399"
  },
  {
    "text": "component called cluster mesh API server",
    "start": "869399",
    "end": "872040"
  },
  {
    "text": "that was basically introduced to take",
    "start": "872040",
    "end": "874759"
  },
  {
    "text": "the original cluster mesh which was",
    "start": "874759",
    "end": "876360"
  },
  {
    "text": "built to work with the KV store mode and",
    "start": "876360",
    "end": "878959"
  },
  {
    "text": "make that uh and support C mode as well",
    "start": "878959",
    "end": "882360"
  },
  {
    "text": "right and fast forward to 2023 with",
    "start": "882360",
    "end": "885440"
  },
  {
    "text": "celum 114 we started another Mode called",
    "start": "885440",
    "end": "888199"
  },
  {
    "text": "KV stor mesh mode which is today the",
    "start": "888199",
    "end": "890920"
  },
  {
    "text": "most scalable of all of these options",
    "start": "890920",
    "end": "893240"
  },
  {
    "text": "right and starting from 116 this is Now",
    "start": "893240",
    "end": "895720"
  },
  {
    "text": "the default if you try to install",
    "start": "895720",
    "end": "897320"
  },
  {
    "text": "cluster mesh",
    "start": "897320",
    "end": "899360"
  },
  {
    "text": "so if you go back to the original",
    "start": "899360",
    "end": "900839"
  },
  {
    "text": "cluster mesh uh architecture how it",
    "start": "900839",
    "end": "903839"
  },
  {
    "text": "worked is every celium agent within your",
    "start": "903839",
    "end": "906079"
  },
  {
    "text": "cluster connected to not just your local",
    "start": "906079",
    "end": "908399"
  },
  {
    "text": "hcd but also to all the remote hcds that",
    "start": "908399",
    "end": "911240"
  },
  {
    "text": "are part of the mesh so what this really",
    "start": "911240",
    "end": "913199"
  },
  {
    "text": "means from a single hcd perspective is",
    "start": "913199",
    "end": "915920"
  },
  {
    "text": "if a single Endo is updated at CD needs",
    "start": "915920",
    "end": "918959"
  },
  {
    "text": "to push out that watch update not just",
    "start": "918959",
    "end": "921079"
  },
  {
    "text": "to every single celium agent within the",
    "start": "921079",
    "end": "922880"
  },
  {
    "text": "cluster but to every other agent in",
    "start": "922880",
    "end": "925560"
  },
  {
    "text": "the rest of the cluster mesh right and",
    "start": "925560",
    "end": "928480"
  },
  {
    "text": "this can very quickly introduce a lot of",
    "start": "928480",
    "end": "930480"
  },
  {
    "text": "load on hcd and if you want to quantify",
    "start": "930480",
    "end": "933360"
  },
  {
    "text": "it if you have M clusters in the mesh",
    "start": "933360",
    "end": "935240"
  },
  {
    "text": "and N agents per cluster it's mcross and",
    "start": "935240",
    "end": "937880"
  },
  {
    "text": "updates that CD needs to push out for",
    "start": "937880",
    "end": "940519"
  },
  {
    "text": "every single endpoint update and that's",
    "start": "940519",
    "end": "942240"
  },
  {
    "text": "a",
    "start": "942240",
    "end": "943040"
  },
  {
    "text": "lot and if you look at the previous",
    "start": "943040",
    "end": "946720"
  },
  {
    "text": "architecture it did not really work with",
    "start": "946720",
    "end": "948959"
  },
  {
    "text": "C mode because KU uh because celium",
    "start": "948959",
    "end": "951360"
  },
  {
    "text": "started with the KV stored mode and in",
    "start": "951360",
    "end": "953519"
  },
  {
    "text": "order to support cluster mesh we have",
    "start": "953519",
    "end": "956040"
  },
  {
    "text": "this new component called cluster mesh",
    "start": "956040",
    "end": "957600"
  },
  {
    "text": "API server which is primarily made up of",
    "start": "957600",
    "end": "960040"
  },
  {
    "text": "two components so there's one component",
    "start": "960040",
    "end": "962199"
  },
  {
    "text": "that call I'll call it KS Sync here but",
    "start": "962199",
    "end": "964720"
  },
  {
    "text": "it's called API server in the helm chart",
    "start": "964720",
    "end": "967000"
  },
  {
    "text": "so all it does is it connects to your",
    "start": "967000",
    "end": "968759"
  },
  {
    "text": "kubernetes control plane reads all of",
    "start": "968759",
    "end": "970839"
  },
  {
    "text": "that endpoint and identity information",
    "start": "970839",
    "end": "972920"
  },
  {
    "text": "and writes that to a local HD that's",
    "start": "972920",
    "end": "975480"
  },
  {
    "text": "part of the cluster mesh API server pod",
    "start": "975480",
    "end": "977759"
  },
  {
    "text": "right and after this it's very similar",
    "start": "977759",
    "end": "980160"
  },
  {
    "text": "to the original cluster mesh uh",
    "start": "980160",
    "end": "982160"
  },
  {
    "text": "architecture and the scalability",
    "start": "982160",
    "end": "984240"
  },
  {
    "text": "characteristics are pretty much the same",
    "start": "984240",
    "end": "986160"
  },
  {
    "text": "right you're still having to push out",
    "start": "986160",
    "end": "987839"
  },
  {
    "text": "the M cross n update on every endpoint",
    "start": "987839",
    "end": "990079"
  },
  {
    "text": "update",
    "start": "990079",
    "end": "991639"
  },
  {
    "text": "right so in ebpf Summit 2022 there was a",
    "start": "991639",
    "end": "995600"
  },
  {
    "text": "great talk by Arthur from ctrip.com",
    "start": "995600",
    "end": "998000"
  },
  {
    "text": "where he rightly pointed out that what",
    "start": "998000",
    "end": "1000199"
  },
  {
    "text": "if we had a controller that connected to",
    "start": "1000199",
    "end": "1002399"
  },
  {
    "text": "remote hcds discovered all of that data",
    "start": "1002399",
    "end": "1005240"
  },
  {
    "text": "and made it available to me locally so",
    "start": "1005240",
    "end": "1007319"
  },
  {
    "text": "that all my local agents can discover uh",
    "start": "1007319",
    "end": "1009600"
  },
  {
    "text": "information from that and fast forward",
    "start": "1009600",
    "end": "1012040"
  },
  {
    "text": "to",
    "start": "1012040",
    "end": "1012920"
  },
  {
    "text": "cm4 uh there was a new mode introduced",
    "start": "1012920",
    "end": "1015399"
  },
  {
    "text": "called KV stor mesh that basically",
    "start": "1015399",
    "end": "1017560"
  },
  {
    "text": "implemented this architecture",
    "start": "1017560",
    "end": "1019360"
  },
  {
    "text": "and in this architecture you see a new",
    "start": "1019360",
    "end": "1021319"
  },
  {
    "text": "component called KV store mesh within",
    "start": "1021319",
    "end": "1023399"
  },
  {
    "text": "the cluster mesh API server originally",
    "start": "1023399",
    "end": "1025438"
  },
  {
    "text": "we only had two components there was a k",
    "start": "1025439",
    "end": "1027199"
  },
  {
    "text": "a sync component and a local CD but now",
    "start": "1027199",
    "end": "1030038"
  },
  {
    "text": "we have a new component called KV St",
    "start": "1030039",
    "end": "1031880"
  },
  {
    "text": "mesh that's connecting to remote CD",
    "start": "1031880",
    "end": "1034199"
  },
  {
    "text": "clusters reading that information and",
    "start": "1034199",
    "end": "1036558"
  },
  {
    "text": "making it available to your locald right",
    "start": "1036559",
    "end": "1039880"
  },
  {
    "text": "so with this architecture you went from",
    "start": "1039880",
    "end": "1042199"
  },
  {
    "text": "an M cross n updates to M plus n updates",
    "start": "1042199",
    "end": "1045640"
  },
  {
    "text": "right because instead of having all of",
    "start": "1045640",
    "end": "1047600"
  },
  {
    "text": "the remote agents connecting to your xcd",
    "start": "1047600",
    "end": "1050120"
  },
  {
    "text": "you only have one connection per remote",
    "start": "1050120",
    "end": "1052400"
  },
  {
    "text": "cluster right so this is a lot better",
    "start": "1052400",
    "end": "1055080"
  },
  {
    "text": "now and if you want to know more about",
    "start": "1055080",
    "end": "1057880"
  },
  {
    "text": "like the exact differences between both",
    "start": "1057880",
    "end": "1059520"
  },
  {
    "text": "of these modes and to get some metrics",
    "start": "1059520",
    "end": "1062640"
  },
  {
    "text": "on how this actually impacts the",
    "start": "1062640",
    "end": "1064240"
  },
  {
    "text": "endpoint propagation latency there's a",
    "start": "1064240",
    "end": "1065960"
  },
  {
    "text": "good talk by Ryan in selum Con Chicago I",
    "start": "1065960",
    "end": "1069039"
  },
  {
    "text": "highly recommend watching",
    "start": "1069039",
    "end": "1071080"
  },
  {
    "text": "it all right so if you go back to the KV",
    "start": "1071080",
    "end": "1074039"
  },
  {
    "text": "store mesh algorithm and sorry",
    "start": "1074039",
    "end": "1075880"
  },
  {
    "text": "architecture and take a closer look at",
    "start": "1075880",
    "end": "1077679"
  },
  {
    "text": "it this one one looks like it's designed",
    "start": "1077679",
    "end": "1079919"
  },
  {
    "text": "only to work with crd mode right so you",
    "start": "1079919",
    "end": "1082400"
  },
  {
    "text": "see a dedicated uh you see a collocated",
    "start": "1082400",
    "end": "1084720"
  },
  {
    "text": "at CD pod container but you don't see",
    "start": "1084720",
    "end": "1087840"
  },
  {
    "text": "the dedicated s CD in KV store mode but",
    "start": "1087840",
    "end": "1090280"
  },
  {
    "text": "at data dog we've been running two",
    "start": "1090280",
    "end": "1092120"
  },
  {
    "text": "variants of celium you can either run it",
    "start": "1092120",
    "end": "1093960"
  },
  {
    "text": "in C mode or with dedicated hcd and all",
    "start": "1093960",
    "end": "1097320"
  },
  {
    "text": "of our large clusters run with dedicated",
    "start": "1097320",
    "end": "1099400"
  },
  {
    "text": "hcd mode so we really wanted to mesh",
    "start": "1099400",
    "end": "1102080"
  },
  {
    "text": "some of our large clusters together",
    "start": "1102080",
    "end": "1104080"
  },
  {
    "text": "especially in environments where we have",
    "start": "1104080",
    "end": "1105640"
  },
  {
    "text": "default deny because you have no way to",
    "start": "1105640",
    "end": "1107600"
  },
  {
    "text": "cross that cluster boundary if you we",
    "start": "1107600",
    "end": "1108880"
  },
  {
    "text": "don't have cluster mesh so we were quite",
    "start": "1108880",
    "end": "1111600"
  },
  {
    "text": "puzzled by the fact that this did not",
    "start": "1111600",
    "end": "1113320"
  },
  {
    "text": "support KV store mode right so we",
    "start": "1113320",
    "end": "1115760"
  },
  {
    "text": "created a test spot and started looking",
    "start": "1115760",
    "end": "1117760"
  },
  {
    "text": "at the actual data that that is written",
    "start": "1117760",
    "end": "1119799"
  },
  {
    "text": "to hcd and compared the format uh",
    "start": "1119799",
    "end": "1122520"
  },
  {
    "text": "between the data format that is present",
    "start": "1122520",
    "end": "1125080"
  },
  {
    "text": "in the local at CD cluster in KV store",
    "start": "1125080",
    "end": "1127320"
  },
  {
    "text": "mode and compared that against the data",
    "start": "1127320",
    "end": "1129799"
  },
  {
    "text": "that clust shi server writes to its",
    "start": "1129799",
    "end": "1132280"
  },
  {
    "text": "local at CD container and turns out",
    "start": "1132280",
    "end": "1134520"
  },
  {
    "text": "thanks to cm's architecture they're",
    "start": "1134520",
    "end": "1136600"
  },
  {
    "text": "exactly the same right so",
    "start": "1136600",
    "end": "1139240"
  },
  {
    "text": "what that means is maybe we can point",
    "start": "1139240",
    "end": "1141039"
  },
  {
    "text": "this cluster mesh API server container",
    "start": "1141039",
    "end": "1143440"
  },
  {
    "text": "pod and point that to our persistent at",
    "start": "1143440",
    "end": "1145799"
  },
  {
    "text": "CD cluster and maybe everything would",
    "start": "1145799",
    "end": "1147679"
  },
  {
    "text": "work out of the box and internally we",
    "start": "1147679",
    "end": "1149840"
  },
  {
    "text": "even started calling this KV KV mesh",
    "start": "1149840",
    "end": "1151960"
  },
  {
    "text": "mode but we're open to suggestions on",
    "start": "1151960",
    "end": "1154520"
  },
  {
    "text": "other names so uh to summarize this",
    "start": "1154520",
    "end": "1157840"
  },
  {
    "text": "we're trying to go from a cluster mesh",
    "start": "1157840",
    "end": "1159640"
  },
  {
    "text": "API server which had three containers",
    "start": "1159640",
    "end": "1162320"
  },
  {
    "text": "and get rid of the ks in component get",
    "start": "1162320",
    "end": "1164360"
  },
  {
    "text": "rid of the local hcd and connect it to",
    "start": "1164360",
    "end": "1166559"
  },
  {
    "text": "the dedicated hcd cluster right",
    "start": "1166559",
    "end": "1169400"
  },
  {
    "text": "and when we tried that out it actually",
    "start": "1169400",
    "end": "1171080"
  },
  {
    "text": "surprisingly worked really well except",
    "start": "1171080",
    "end": "1173320"
  },
  {
    "text": "for like one small uh issue where um so",
    "start": "1173320",
    "end": "1177520"
  },
  {
    "text": "every time a given cluster needs to",
    "start": "1177520",
    "end": "1180480"
  },
  {
    "text": "discover information about remote",
    "start": "1180480",
    "end": "1182440"
  },
  {
    "text": "clusters things like uh cluster ID or",
    "start": "1182440",
    "end": "1185320"
  },
  {
    "text": "cluster name all of that data is present",
    "start": "1185320",
    "end": "1187559"
  },
  {
    "text": "in a against a given key and that data",
    "start": "1187559",
    "end": "1190520"
  },
  {
    "text": "needs to be replicated to other clusters",
    "start": "1190520",
    "end": "1192320"
  },
  {
    "text": "in order to discover that information",
    "start": "1192320",
    "end": "1194200"
  },
  {
    "text": "and this was maintained by the API",
    "start": "1194200",
    "end": "1195760"
  },
  {
    "text": "server component which we were getting",
    "start": "1195760",
    "end": "1197200"
  },
  {
    "text": "rid of so we had to move that uh key uh",
    "start": "1197200",
    "end": "1200320"
  },
  {
    "text": "Key Management to the celum operator and",
    "start": "1200320",
    "end": "1203400"
  },
  {
    "text": "once we backported that uh we were able",
    "start": "1203400",
    "end": "1205520"
  },
  {
    "text": "to run out of the box and if you're",
    "start": "1205520",
    "end": "1208440"
  },
  {
    "text": "trying to use this uh if you if you're",
    "start": "1208440",
    "end": "1211600"
  },
  {
    "text": "trying to deploy this using the Upstream",
    "start": "1211600",
    "end": "1213919"
  },
  {
    "text": "help chart you likely run into some",
    "start": "1213919",
    "end": "1215919"
  },
  {
    "text": "validation issues and it will not work",
    "start": "1215919",
    "end": "1217720"
  },
  {
    "text": "out of the box because it's not designed",
    "start": "1217720",
    "end": "1219280"
  },
  {
    "text": "to be run this way but internally at",
    "start": "1219280",
    "end": "1221000"
  },
  {
    "text": "data dog we run with our custom Helm",
    "start": "1221000",
    "end": "1223080"
  },
  {
    "text": "chart so we're able to make this",
    "start": "1223080",
    "end": "1225360"
  },
  {
    "text": "work and this was the original KV St",
    "start": "1225360",
    "end": "1228720"
  },
  {
    "text": "mesh uh architecture and we were able to",
    "start": "1228720",
    "end": "1230799"
  },
  {
    "text": "simplify that to something that looks",
    "start": "1230799",
    "end": "1232640"
  },
  {
    "text": "like this and at this point we were",
    "start": "1232640",
    "end": "1234640"
  },
  {
    "text": "really comfortable with uh being able to",
    "start": "1234640",
    "end": "1236919"
  },
  {
    "text": "scale our total uh mesh size because we",
    "start": "1236919",
    "end": "1240039"
  },
  {
    "text": "have a dedicated CD cluster we could",
    "start": "1240039",
    "end": "1242200"
  },
  {
    "text": "play with and if you want to scale up",
    "start": "1242200",
    "end": "1243799"
  },
  {
    "text": "our mesh size we can simply upscale our",
    "start": "1243799",
    "end": "1246400"
  },
  {
    "text": "hcd and we will be able to handle the",
    "start": "1246400",
    "end": "1248159"
  },
  {
    "text": "additional",
    "start": "1248159",
    "end": "1249480"
  },
  {
    "text": "load so to summarize it really comes",
    "start": "1249480",
    "end": "1252080"
  },
  {
    "text": "down to what kind of identity allocation",
    "start": "1252080",
    "end": "1254960"
  },
  {
    "text": "mode you're using you're using crd you",
    "start": "1254960",
    "end": "1257360"
  },
  {
    "text": "have two options if you're doing",
    "start": "1257360",
    "end": "1259200"
  },
  {
    "text": "dedicated at CD you also have two",
    "start": "1259200",
    "end": "1261640"
  },
  {
    "text": "options so over the course of course of",
    "start": "1261640",
    "end": "1264240"
  },
  {
    "text": "last 6 months we've been migrating and",
    "start": "1264240",
    "end": "1266400"
  },
  {
    "text": "mesing together some of our large",
    "start": "1266400",
    "end": "1268039"
  },
  {
    "text": "clusters and we've learned some lessons",
    "start": "1268039",
    "end": "1270279"
  },
  {
    "text": "uh while doing so so if you're trying to",
    "start": "1270279",
    "end": "1272960"
  },
  {
    "text": "do this with large clusters I highly",
    "start": "1272960",
    "end": "1275559"
  },
  {
    "text": "recommend trying to get a little",
    "start": "1275559",
    "end": "1276840"
  },
  {
    "text": "familiar with what is the data that is",
    "start": "1276840",
    "end": "1278840"
  },
  {
    "text": "actually being replicated between the",
    "start": "1278840",
    "end": "1280360"
  },
  {
    "text": "Clusters and you need to also be",
    "start": "1280360",
    "end": "1282120"
  },
  {
    "text": "cognizant of how fast are you shuffling",
    "start": "1282120",
    "end": "1284360"
  },
  {
    "text": "that information right because too much",
    "start": "1284360",
    "end": "1286360"
  },
  {
    "text": "endpoint churn in one cluster too much",
    "start": "1286360",
    "end": "1288559"
  },
  {
    "text": "identity CH in one cluster can impact",
    "start": "1288559",
    "end": "1290760"
  },
  {
    "text": "your other clusters right so you don't",
    "start": "1290760",
    "end": "1292320"
  },
  {
    "text": "want that to happen and what if my",
    "start": "1292320",
    "end": "1294279"
  },
  {
    "text": "cluster mesh API server dies will I lose",
    "start": "1294279",
    "end": "1296799"
  },
  {
    "text": "my cross cluster connectivity soon after",
    "start": "1296799",
    "end": "1298600"
  },
  {
    "text": "the cluster mesh API server dies so",
    "start": "1298600",
    "end": "1301440"
  },
  {
    "text": "luckily for the last problem from celm",
    "start": "1301440",
    "end": "1303720"
  },
  {
    "text": "116 we have a high availability mode for",
    "start": "1303720",
    "end": "1306200"
  },
  {
    "text": "customiz API server so you can run",
    "start": "1306200",
    "end": "1308720"
  },
  {
    "text": "another replica and if you lose one of",
    "start": "1308720",
    "end": "1310720"
  },
  {
    "text": "the nodes the other replica should pick",
    "start": "1310720",
    "end": "1312360"
  },
  {
    "text": "up",
    "start": "1312360",
    "end": "1313520"
  },
  {
    "text": "immediately and one of the key",
    "start": "1313520",
    "end": "1315320"
  },
  {
    "text": "difference I want to highlight",
    "start": "1315320",
    "end": "1316799"
  },
  {
    "text": "especially if you're going from a single",
    "start": "1316799",
    "end": "1318279"
  },
  {
    "text": "class to multicluster mesh is the way",
    "start": "1318279",
    "end": "1321320"
  },
  {
    "text": "your stale keys are being garbage",
    "start": "1321320",
    "end": "1323400"
  },
  {
    "text": "collected so in a single cluster mode",
    "start": "1323400",
    "end": "1325400"
  },
  {
    "text": "the celum operator actually garbage",
    "start": "1325400",
    "end": "1327080"
  },
  {
    "text": "collects stale keys but when you mesh",
    "start": "1327080",
    "end": "1329880"
  },
  {
    "text": "together multiple clusters garbage",
    "start": "1329880",
    "end": "1331919"
  },
  {
    "text": "collection can take a lot of time so",
    "start": "1331919",
    "end": "1334360"
  },
  {
    "text": "instead of doing garbage collection with",
    "start": "1334360",
    "end": "1335919"
  },
  {
    "text": "the operator celium relies on Lees and",
    "start": "1335919",
    "end": "1338480"
  },
  {
    "text": "ttls so every key that is added from",
    "start": "1338480",
    "end": "1341240"
  },
  {
    "text": "remote uh clusters has a corresponding",
    "start": "1341240",
    "end": "1344080"
  },
  {
    "text": "Lee attached to it and it has a",
    "start": "1344080",
    "end": "1345720"
  },
  {
    "text": "corresponding TTL attached to it so what",
    "start": "1345720",
    "end": "1348120"
  },
  {
    "text": "this means is the maximum amount of time",
    "start": "1348120",
    "end": "1350559"
  },
  {
    "text": "your cluster mesh API server can go down",
    "start": "1350559",
    "end": "1353559"
  },
  {
    "text": "is equal to the value of the TTL so the",
    "start": "1353559",
    "end": "1356200"
  },
  {
    "text": "default currently is 15 minutes so if",
    "start": "1356200",
    "end": "1357919"
  },
  {
    "text": "you lose the cluster M API server for 15",
    "start": "1357919",
    "end": "1360360"
  },
  {
    "text": "minutes you'll lose the cross cluster",
    "start": "1360360",
    "end": "1362159"
  },
  {
    "text": "cross cluster",
    "start": "1362159",
    "end": "1364799"
  },
  {
    "text": "connectivity so luckily for that part we",
    "start": "1369279",
    "end": "1371760"
  },
  {
    "text": "have a flag called KV store lease TTL it",
    "start": "1371760",
    "end": "1374880"
  },
  {
    "text": "was marked as hidden historic for",
    "start": "1374880",
    "end": "1376679"
  },
  {
    "text": "historical reasons it was originally",
    "start": "1376679",
    "end": "1378559"
  },
  {
    "text": "introduced to only work with CI but I",
    "start": "1378559",
    "end": "1381000"
  },
  {
    "text": "find it uh we find it really handy to",
    "start": "1381000",
    "end": "1383240"
  },
  {
    "text": "actually tune that so if you're okay",
    "start": "1383240",
    "end": "1385000"
  },
  {
    "text": "with taking on a slightly extra load on",
    "start": "1385000",
    "end": "1387640"
  },
  {
    "text": "the number of keys in your hcd cluster",
    "start": "1387640",
    "end": "1390159"
  },
  {
    "text": "we recommend that be bumped and you can",
    "start": "1390159",
    "end": "1392559"
  },
  {
    "text": "also play with the KV store QPS settings",
    "start": "1392559",
    "end": "1395159"
  },
  {
    "text": "both for the bootstrap and regular uh",
    "start": "1395159",
    "end": "1397559"
  },
  {
    "text": "QPS so that you can actually control",
    "start": "1397559",
    "end": "1399919"
  },
  {
    "text": "when there's a lot of identity or",
    "start": "1399919",
    "end": "1401799"
  },
  {
    "text": "endpoint CH in remote clusters you are",
    "start": "1401799",
    "end": "1404360"
  },
  {
    "text": "capping that with the QPS",
    "start": "1404360",
    "end": "1406559"
  },
  {
    "text": "settings and originally we mentioned",
    "start": "1406559",
    "end": "1408760"
  },
  {
    "text": "that we wanted workloads to move between",
    "start": "1408760",
    "end": "1410799"
  },
  {
    "text": "uh clusters but if you have existing",
    "start": "1410799",
    "end": "1412640"
  },
  {
    "text": "clusters and you're meshing them",
    "start": "1412640",
    "end": "1414159"
  },
  {
    "text": "together and you have Network policies",
    "start": "1414159",
    "end": "1416120"
  },
  {
    "text": "that are already configured your cnps",
    "start": "1416120",
    "end": "1419279"
  },
  {
    "text": "might be considering them as World",
    "start": "1419279",
    "end": "1421039"
  },
  {
    "text": "identities and now they're going to",
    "start": "1421039",
    "end": "1422760"
  },
  {
    "text": "become known identities so your cnps",
    "start": "1422760",
    "end": "1424840"
  },
  {
    "text": "need to factor that in as",
    "start": "1424840",
    "end": "1426679"
  },
  {
    "text": "well and here's a snapshot of one of our",
    "start": "1426679",
    "end": "1429200"
  },
  {
    "text": "dashboard that we use to monitor the",
    "start": "1429200",
    "end": "1431400"
  },
  {
    "text": "entire cluster mesh but some of the",
    "start": "1431400",
    "end": "1433200"
  },
  {
    "text": "interesting metrics that are really",
    "start": "1433200",
    "end": "1434880"
  },
  {
    "text": "useful are hcd mvcc lease expiration",
    "start": "1434880",
    "end": "1437919"
  },
  {
    "text": "metrics so if you see a lot of keys that",
    "start": "1437919",
    "end": "1440520"
  },
  {
    "text": "are expiring uh that's something you",
    "start": "1440520",
    "end": "1442480"
  },
  {
    "text": "need to keep an eye on and celum also",
    "start": "1442480",
    "end": "1445240"
  },
  {
    "text": "has some native connectivity metrics",
    "start": "1445240",
    "end": "1447320"
  },
  {
    "text": "between clusters so if you lose",
    "start": "1447320",
    "end": "1449200"
  },
  {
    "text": "connectivity from one cluster to the",
    "start": "1449200",
    "end": "1450799"
  },
  {
    "text": "other we highly recommend like adding",
    "start": "1450799",
    "end": "1453000"
  },
  {
    "text": "some alerts on this and finally you can",
    "start": "1453000",
    "end": "1455559"
  },
  {
    "text": "also monitor the QPS settings so that",
    "start": "1455559",
    "end": "1457480"
  },
  {
    "text": "you can be really sure if the QPS is",
    "start": "1457480",
    "end": "1459440"
  },
  {
    "text": "working out well for you and if you do",
    "start": "1459440",
    "end": "1462080"
  },
  {
    "text": "get alerted inevitably there are also",
    "start": "1462080",
    "end": "1464360"
  },
  {
    "text": "some on demand commands that you can run",
    "start": "1464360",
    "end": "1466039"
  },
  {
    "text": "called KV store debug troubleshoot that",
    "start": "1466039",
    "end": "1468480"
  },
  {
    "text": "will on demand try to establish",
    "start": "1468480",
    "end": "1470039"
  },
  {
    "text": "connections to remote hcd and if you",
    "start": "1470039",
    "end": "1472520"
  },
  {
    "text": "have some certificates expiring or",
    "start": "1472520",
    "end": "1474159"
  },
  {
    "text": "things like that you'll actually be able",
    "start": "1474159",
    "end": "1475520"
  },
  {
    "text": "to see them here and to summarize",
    "start": "1475520",
    "end": "1478679"
  },
  {
    "text": "everything um and there's some future",
    "start": "1478679",
    "end": "1480960"
  },
  {
    "text": "work that we think would be helpful for",
    "start": "1480960",
    "end": "1483760"
  },
  {
    "text": "the entire K stor mesh ecosystem so uh",
    "start": "1483760",
    "end": "1486919"
  },
  {
    "text": "the first point is that as we discussed",
    "start": "1486919",
    "end": "1489120"
  },
  {
    "text": "there's a lot of data that's being",
    "start": "1489120",
    "end": "1490440"
  },
  {
    "text": "replicated between clusters right so",
    "start": "1490440",
    "end": "1492760"
  },
  {
    "text": "depending on the number of features that",
    "start": "1492760",
    "end": "1494520"
  },
  {
    "text": "you're actually using in your production",
    "start": "1494520",
    "end": "1496279"
  },
  {
    "text": "environments all of the data that is",
    "start": "1496279",
    "end": "1497960"
  },
  {
    "text": "being replicated between these clusters",
    "start": "1497960",
    "end": "1500039"
  },
  {
    "text": "might not actually be necessary right so",
    "start": "1500039",
    "end": "1502480"
  },
  {
    "text": "we can make celum smarter to replicate",
    "start": "1502480",
    "end": "1504600"
  },
  {
    "text": "only that information that is really",
    "start": "1504600",
    "end": "1506679"
  },
  {
    "text": "important for you and the second point",
    "start": "1506679",
    "end": "1509120"
  },
  {
    "text": "is uh folks from Google are working on",
    "start": "1509120",
    "end": "1511279"
  },
  {
    "text": "this concept called operator managed",
    "start": "1511279",
    "end": "1512960"
  },
  {
    "text": "celium identities where we are trying to",
    "start": "1512960",
    "end": "1516000"
  },
  {
    "text": "update celum to create identities only",
    "start": "1516000",
    "end": "1518640"
  },
  {
    "text": "if a network policy actually uses it so",
    "start": "1518640",
    "end": "1521039"
  },
  {
    "text": "we can extend a similar concept to uh KV",
    "start": "1521039",
    "end": "1523640"
  },
  {
    "text": "store mesh as well so that we export",
    "start": "1523640",
    "end": "1526039"
  },
  {
    "text": "identities that are only relevant for",
    "start": "1526039",
    "end": "1527720"
  },
  {
    "text": "your uh policy",
    "start": "1527720",
    "end": "1529520"
  },
  {
    "text": "enforcement so hopefully this uh this",
    "start": "1529520",
    "end": "1533080"
  },
  {
    "text": "disambiguated all the different cluster",
    "start": "1533080",
    "end": "1534679"
  },
  {
    "text": "mesh modes that are available and you",
    "start": "1534679",
    "end": "1536520"
  },
  {
    "text": "have some good insights on how you could",
    "start": "1536520",
    "end": "1538039"
  },
  {
    "text": "tune your uh cluster mesh system and",
    "start": "1538039",
    "end": "1541039"
  },
  {
    "text": "that's pretty much what we had and both",
    "start": "1541039",
    "end": "1542840"
  },
  {
    "text": "Maxim and me are available on Twitter or",
    "start": "1542840",
    "end": "1545039"
  },
  {
    "text": "CDM slack and if you have any questions",
    "start": "1545039",
    "end": "1547279"
  },
  {
    "text": "don't hesitate to reach out to Sig",
    "start": "1547279",
    "end": "1548960"
  },
  {
    "text": "scalability or Sig gluster mesh thank",
    "start": "1548960",
    "end": "1551480"
  },
  {
    "text": "you",
    "start": "1551480",
    "end": "1553960"
  }
]