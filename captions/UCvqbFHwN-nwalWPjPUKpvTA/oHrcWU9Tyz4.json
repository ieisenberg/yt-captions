[
  {
    "start": "0",
    "end": "24000"
  },
  {
    "text": "hello my name is Kevin Lynch and I'm going to be telling you about kubernetes",
    "start": "380",
    "end": "5670"
  },
  {
    "text": "in the data center I'm going to be telling you about square spaces journey towards self-service infrastructure I've",
    "start": "5670",
    "end": "12360"
  },
  {
    "text": "been working at Squarespace for about three years so I've seen our entire",
    "start": "12360",
    "end": "17730"
  },
  {
    "text": "transition from a monolith application to micro services and on to kubernetes",
    "start": "17730",
    "end": "24260"
  },
  {
    "start": "24000",
    "end": "65000"
  },
  {
    "text": "just some background for those of you who may not be familiar Squarespace was founded in 2003 and we",
    "start": "24260",
    "end": "30810"
  },
  {
    "text": "provide a platform for people to host their websites their commerce platform and we host domains for them as well and",
    "start": "30810",
    "end": "37829"
  },
  {
    "text": "we have over a million customers and going back to that founded in 2003 means",
    "start": "37829",
    "end": "43980"
  },
  {
    "text": "that we run out of data centers we predate AWS publicly so we have three",
    "start": "43980",
    "end": "50280"
  },
  {
    "text": "data centers over 5,000 hosts in these data centers and for scale comparisons",
    "start": "50280",
    "end": "56969"
  },
  {
    "text": "we have about 40 million metrics that we generate every minute so there's a lot that's going on in these data centers",
    "start": "56969",
    "end": "62940"
  },
  {
    "text": "all the time back in 2013 we were less",
    "start": "62940",
    "end": "68850"
  },
  {
    "start": "65000",
    "end": "97000"
  },
  {
    "text": "than 50 engineers and we had a traditional stack where we had a monolithic application and you know",
    "start": "68850",
    "end": "76560"
  },
  {
    "text": "there was some background jobs and they communicated to a database a lot of the world looked like this back then and",
    "start": "76560",
    "end": "82369"
  },
  {
    "text": "these these engineers you know they needed their goal was to introduce new features to build the product and to",
    "start": "82369",
    "end": "89759"
  },
  {
    "text": "grow fast and really what ended up happening was you know whatever works was was what we went with you know we",
    "start": "89759",
    "end": "98820"
  },
  {
    "start": "97000",
    "end": "141000"
  },
  {
    "text": "grew a little bit then in 2014 you know we had about 75 engineers and we",
    "start": "98820",
    "end": "105450"
  },
  {
    "text": "realized that whatever works wasn't really working for us anymore you know engineers wanted to introduce",
    "start": "105450",
    "end": "111659"
  },
  {
    "text": "new features but there ended up being too much firefighting you know people",
    "start": "111659",
    "end": "116909"
  },
  {
    "text": "were unsure of the reliability and stability of the code they were introducing because there was so much",
    "start": "116909",
    "end": "122729"
  },
  {
    "text": "going on in these monolithic applications so like a lot of companies at that time you know we started down",
    "start": "122729",
    "end": "128849"
  },
  {
    "text": "that microservices route and it was great we introduced a few services and we formed a very stable definition of",
    "start": "128849",
    "end": "139290"
  },
  {
    "text": "service at Squarespace however you know 2016 we have a hundred plus engineers",
    "start": "139290",
    "end": "147090"
  },
  {
    "start": "141000",
    "end": "168000"
  },
  {
    "text": "you know the platform was scalable and reliable and developers could move faster and Squarespace could move faster",
    "start": "147090",
    "end": "153299"
  },
  {
    "text": "because of that you know we had all these services that were communicating with each other and taking traffic from",
    "start": "153299",
    "end": "159900"
  },
  {
    "text": "from the internet and all of these services wasn't really working for",
    "start": "159900",
    "end": "166919"
  },
  {
    "text": "operations so this is our typical workflow for provisioning new machines",
    "start": "166919",
    "end": "173219"
  },
  {
    "start": "168000",
    "end": "238000"
  },
  {
    "text": "in in our data centers and whether it's iron or virtualized it it started with a",
    "start": "173219",
    "end": "180239"
  },
  {
    "text": "very manual process of finding resources you know CPU RAM disk wherever we could",
    "start": "180239",
    "end": "185489"
  },
  {
    "text": "create this virtual machine we'd have to assign make sure we could assign networking we'd have to make sure",
    "start": "185489",
    "end": "190590"
  },
  {
    "text": "the networking worked we had to add all of these hosts who are in ansible inventory and then we could kick off an",
    "start": "190590",
    "end": "198120"
  },
  {
    "text": "automated process where you know we would update DNS and and pixie boot the VM and install the OS and configure the",
    "start": "198120",
    "end": "205049"
  },
  {
    "text": "OS with all of these dependencies that we've built up we'd have to configure monitoring for them and install all the",
    "start": "205049",
    "end": "211349"
  },
  {
    "text": "application the dependencies as well and finally we could install the application this took this process at best took",
    "start": "211349",
    "end": "219479"
  },
  {
    "text": "about 30 minutes because of all of the manual operations and in the pixie booting oftentimes it would take a lot",
    "start": "219479",
    "end": "226590"
  },
  {
    "text": "more because you know sometimes it would be painful to find these resources or painful to you know run into firewall",
    "start": "226590",
    "end": "234419"
  },
  {
    "text": "issues and whatnot so there had to be a better way the big takeaway from this",
    "start": "234419",
    "end": "240540"
  },
  {
    "start": "238000",
    "end": "282000"
  },
  {
    "text": "was static infrastructure and micro services do not mix it was always difficult to find all these resources",
    "start": "240540",
    "end": "246659"
  },
  {
    "text": "was very slow to provisioning scale and because we're running our data centers we do have you know pets in this in this",
    "start": "246659",
    "end": "255239"
  },
  {
    "text": "environment every machine has is a special case right and we were trying to",
    "start": "255239",
    "end": "261479"
  },
  {
    "text": "shoehorn the this micro service cattle mentality into all of these these",
    "start": "261479",
    "end": "266970"
  },
  {
    "text": "at these physical pets that we love and also this system was way too complex for",
    "start": "266970",
    "end": "273150"
  },
  {
    "text": "any new engineers that came on board whether they were developer development focused or operations",
    "start": "273150",
    "end": "278760"
  },
  {
    "text": "focused so we had to come up with a better solution in 2017 we have about",
    "start": "278760",
    "end": "285450"
  },
  {
    "start": "282000",
    "end": "309000"
  },
  {
    "text": "200 engineers a little over that and we started down this this path of",
    "start": "285450",
    "end": "292440"
  },
  {
    "text": "self-service infrastructure finally with with the help of self-service compute networking metrics and storage we're",
    "start": "292440",
    "end": "299880"
  },
  {
    "text": "able operations is finally able to move as fast as our developers and sports players can actually move as fast as we",
    "start": "299880",
    "end": "306180"
  },
  {
    "text": "want it to so for self-service compute we use",
    "start": "306180",
    "end": "312360"
  },
  {
    "start": "309000",
    "end": "332000"
  },
  {
    "text": "kubernetes you know we all love containers that's why we're here and it's very simple you know you just say",
    "start": "312360",
    "end": "317910"
  },
  {
    "text": "keep control apply and in some description of what service you want",
    "start": "317910",
    "end": "322919"
  },
  {
    "text": "applied and it just works however there's there's a lot of pain points that we ran into that we were",
    "start": "322919",
    "end": "329760"
  },
  {
    "text": "aware of at the time to take a step back our our service definitions that we",
    "start": "329760",
    "end": "336150"
  },
  {
    "start": "332000",
    "end": "393000"
  },
  {
    "text": "deploy into kubernetes closely match what we deploy is is a service on a VM",
    "start": "336150",
    "end": "342020"
  },
  {
    "text": "we have a Java spring boot based service model which relies on a lot of the",
    "start": "342020",
    "end": "349350"
  },
  {
    "text": "Netflix utilities for service discovery and in routing of requests we use",
    "start": "349350",
    "end": "356340"
  },
  {
    "text": "console for service discovery and key values we use fluent D for logging to ship all of these service to ship off",
    "start": "356340",
    "end": "362610"
  },
  {
    "text": "the or logs to a centralized elq stack and we also assign resources to these",
    "start": "362610",
    "end": "370950"
  },
  {
    "text": "Java services very similarly to how we assign them to our VM so each Java",
    "start": "370950",
    "end": "377460"
  },
  {
    "text": "services on is typically given two cores or four gigs of and four gigs of RAM we",
    "start": "377460",
    "end": "384060"
  },
  {
    "text": "do we do adjust this per for certain services as well but this is the default",
    "start": "384060",
    "end": "389520"
  },
  {
    "text": "that we provision for all of them however we were running into problems",
    "start": "389520",
    "end": "395760"
  },
  {
    "text": "with the Java micro service and to really understand what was going on",
    "start": "395760",
    "end": "401340"
  },
  {
    "text": "we needed to understand how all these pods are deployed to kubernetes so in",
    "start": "401340",
    "end": "407430"
  },
  {
    "text": "kubernetes each each container mapped to a c group and these c groups are assigned to to each of these containers",
    "start": "407430",
    "end": "414960"
  },
  {
    "text": "and there's they're given the resources based off of the kubernetes requests and",
    "start": "414960",
    "end": "421319"
  },
  {
    "text": "limits and we assign requests equals to limits for all of our services just to",
    "start": "421319",
    "end": "427949"
  },
  {
    "text": "make things easier for us and once once this is done we were running into issues",
    "start": "427949",
    "end": "435900"
  },
  {
    "text": "where we weren't seeing the performance that we saw in the pods as the VMS and",
    "start": "435900",
    "end": "441539"
  },
  {
    "text": "to really understand how this works we needed to understand how the the scheduler schedules these so we map so",
    "start": "441539",
    "end": "450150"
  },
  {
    "text": "kubernetes in linux maps the cpu requests to CPU shares and it's able to",
    "start": "450150",
    "end": "457979"
  },
  {
    "text": "throttle the the resources of base of the service based off of the cpu quota",
    "start": "457979",
    "end": "464250"
  },
  {
    "text": "limits this is implemented in kubernetes",
    "start": "464250",
    "end": "469789"
  },
  {
    "start": "466000",
    "end": "547000"
  },
  {
    "text": "in linux by calculating the number of shares that are assigned to this based",
    "start": "469789",
    "end": "476310"
  },
  {
    "text": "off of the cpu request x 1024 and kubernetes is aware of all of the",
    "start": "476310",
    "end": "482060"
  },
  {
    "text": "available shares based off of the total number of cores on a system on over a",
    "start": "482060",
    "end": "487979"
  },
  {
    "text": "kubernetes nodes we run 64 core boxes so",
    "start": "487979",
    "end": "494009"
  },
  {
    "text": "this would be assigned to about 65,000 shares for the entire box now each",
    "start": "494009",
    "end": "500039"
  },
  {
    "text": "service would then be throttled based off of the limit so it's given an allotment of the CPU limit times 100",
    "start": "500039",
    "end": "507240"
  },
  {
    "text": "milliseconds and that's calculated over a hundred milliseconds so if you wanted to us to assign a container roughly two",
    "start": "507240",
    "end": "514740"
  },
  {
    "text": "cores it would turn into a CPU limit of 2 of 200 milliseconds over 100",
    "start": "514740",
    "end": "521669"
  },
  {
    "text": "milliseconds so you could have two concurrent threads operating at the same time so as an example on our 64 core",
    "start": "521669",
    "end": "529380"
  },
  {
    "text": "machine boxes our Java process would be given 2048 shares over",
    "start": "529380",
    "end": "534600"
  },
  {
    "text": "sixty-five thousand so it's guaranteed at least that amount of resources to operate and it's able to run for up to",
    "start": "534600",
    "end": "544319"
  },
  {
    "text": "two threads during that period however we were running into painful scenarios",
    "start": "544319",
    "end": "552060"
  },
  {
    "start": "547000",
    "end": "596000"
  },
  {
    "text": "where the world would just stop when we were doing our stress testing and the",
    "start": "552060",
    "end": "557699"
  },
  {
    "text": "reason for this was because the the jvm garbage collector threads were using up",
    "start": "557699",
    "end": "563730"
  },
  {
    "text": "all of the cpu quota so we had to figure out why this was happening so the the",
    "start": "563730",
    "end": "570060"
  },
  {
    "text": "JVM we when we analyzed this we saw that there were 64 garbage collector threads",
    "start": "570060",
    "end": "576199"
  },
  {
    "text": "128 dead e threads for doing HTTP traffic and then there were 64 KVM for",
    "start": "576199",
    "end": "584759"
  },
  {
    "text": "queuing threads for parallel operations these numbers seemed strange they seemed",
    "start": "584759",
    "end": "591630"
  },
  {
    "text": "to match the number of cores so we had to figure out what was going on so all of the libraries in in Java",
    "start": "591630",
    "end": "600269"
  },
  {
    "start": "596000",
    "end": "636000"
  },
  {
    "text": "are a lot of them are configured by based on the number of available processors Keti does this the JVM does",
    "start": "600269",
    "end": "607350"
  },
  {
    "text": "this and configures the 420 employees pulls in the DC thread pools and then there could be various other libraries",
    "start": "607350",
    "end": "614009"
  },
  {
    "text": "that would access this to automatically tune and scale the number of threads for",
    "start": "614009",
    "end": "620519"
  },
  {
    "text": "a given operation and the JVM is able to detect this by running a assist control",
    "start": "620519",
    "end": "627990"
  },
  {
    "text": "call to get the number of online processors we found that this wasn't being restricted by C groups so we had",
    "start": "627990",
    "end": "634889"
  },
  {
    "text": "to figure out what to do there so we came up with a solution of providing a base Java container that would was able",
    "start": "634889",
    "end": "641759"
  },
  {
    "start": "636000",
    "end": "673000"
  },
  {
    "text": "to calculate all these resources and thankfully because the C groups is",
    "start": "641759",
    "end": "647100"
  },
  {
    "text": "mounted inside of every container and provides the values of all of these we're able to pretty much",
    "start": "647100",
    "end": "653610"
  },
  {
    "text": "reverse-engineer the number of cores that are assigned to this by just looking at the the number of quota",
    "start": "653610",
    "end": "660079"
  },
  {
    "text": "microseconds available and dividing that by the by the period and then we",
    "start": "660079",
    "end": "666260"
  },
  {
    "text": "automatically tune the JV by passing in KVM flags where were",
    "start": "666260",
    "end": "671400"
  },
  {
    "text": "explicitly setting this however this doesn't solve all the problems we were",
    "start": "671400",
    "end": "677640"
  },
  {
    "start": "673000",
    "end": "712000"
  },
  {
    "text": "we still needed a way to override the available processors call and we did",
    "start": "677640",
    "end": "682920"
  },
  {
    "text": "this by basically relying on a C shared library where we pretty much override",
    "start": "682920",
    "end": "689130"
  },
  {
    "text": "the JVM active processor count which is ultimately what the KVM cause and and we",
    "start": "689130",
    "end": "695910"
  },
  {
    "text": "shim this in with a Linux preload hook inside of the container so then when the",
    "start": "695910",
    "end": "701940"
  },
  {
    "text": "JVM cause available processors we would return the number of cores that we've assigned to this via via an environment",
    "start": "701940",
    "end": "709590"
  },
  {
    "text": "variable that the base container calculated so now we're able to actually",
    "start": "709590",
    "end": "714630"
  },
  {
    "start": "712000",
    "end": "728000"
  },
  {
    "text": "have self-service compute where the developer doesn't really need to know what the what the underlying mechanism",
    "start": "714630",
    "end": "722340"
  },
  {
    "text": "is all they need to say is I want a Java service with two cores and they're able to get this now going back to how we",
    "start": "722340",
    "end": "731760"
  },
  {
    "start": "728000",
    "end": "773000"
  },
  {
    "text": "configure our how our Java services look like I said we use spring boot for the",
    "start": "731760",
    "end": "737730"
  },
  {
    "text": "core container and we use all of the Netflix utilities for doing you know we",
    "start": "737730",
    "end": "743520"
  },
  {
    "text": "use Netflix ribbon for doing automatic retries or we could do client-side load balancing directly in the application so",
    "start": "743520",
    "end": "751560"
  },
  {
    "text": "every Java service is shipped with all of this logic and then we can do circuit",
    "start": "751560",
    "end": "756750"
  },
  {
    "text": "breaking with history and service discovery with console however we were in a world where we wanted to to have",
    "start": "756750",
    "end": "764880"
  },
  {
    "text": "our VM infrastructure still communicate with our with our pods that were",
    "start": "764880",
    "end": "771840"
  },
  {
    "text": "deploying into kubernetes and you know each of the the console agents need to",
    "start": "771840",
    "end": "777330"
  },
  {
    "start": "773000",
    "end": "838000"
  },
  {
    "text": "communicate with each other for discovery the Java services will ultimately need to communicate with each other directly",
    "start": "777330",
    "end": "782850"
  },
  {
    "text": "so we leverage calico for this kubernetes provides a pluggable",
    "start": "782850",
    "end": "789500"
  },
  {
    "text": "container network interface and there are many options there's flannel calico",
    "start": "789500",
    "end": "795600"
  },
  {
    "text": "weave cube net VX LAN probably a lot more than I'm aware of on this list",
    "start": "795600",
    "end": "801470"
  },
  {
    "text": "we ended up going with calculate because it gave us a few benefits for running in",
    "start": "801470",
    "end": "807260"
  },
  {
    "text": "our data center calcio provides Software Defined Networking we can use that to",
    "start": "807260",
    "end": "813230"
  },
  {
    "text": "configure nap Network policy IP tables access rules and it gives us the benefit",
    "start": "813230",
    "end": "818420"
  },
  {
    "text": "of not needing network overlays in our data centers so that means we don't have",
    "start": "818420",
    "end": "824180"
  },
  {
    "text": "any performance impact from doing encapsulation it eliminates any MTU",
    "start": "824180",
    "end": "829640"
  },
  {
    "text": "overhead that we may have and it also gives us the ability to have seamless ingress and egress because of our",
    "start": "829640",
    "end": "837170"
  },
  {
    "text": "network setup so in our data center we've deployed what we what's referred",
    "start": "837170",
    "end": "843440"
  },
  {
    "start": "838000",
    "end": "955000"
  },
  {
    "text": "to as a layer three kloss topology so this is a spine and leaf architecture",
    "start": "843440",
    "end": "849500"
  },
  {
    "text": "and all of these Leafs are layer three networking so each each of those each",
    "start": "849500",
    "end": "857720"
  },
  {
    "text": "rack only communicates with each other rack over layer three and this makes it",
    "start": "857720",
    "end": "864440"
  },
  {
    "text": "very simple to to understand because the spines aren't doing any any processing",
    "start": "864440",
    "end": "870290"
  },
  {
    "text": "and all of the Leafs are their own layer two domains so any any mac addresses don't really aren't seen by any other",
    "start": "870290",
    "end": "876560"
  },
  {
    "text": "racks and this makes it really easy to scale out and predictable and consistent",
    "start": "876560",
    "end": "881780"
  },
  {
    "text": "for any network communication because anything communicating in one rack is going to have at most two hops to get to",
    "start": "881780",
    "end": "888470"
  },
  {
    "text": "another rack and it this also gives us the ability of having anycast support as well so all the work is performed at the",
    "start": "888470",
    "end": "896390"
  },
  {
    "text": "leaf switches and and each of these these racks are assigned there and bgp",
    "start": "896390",
    "end": "903500"
  },
  {
    "text": "domain which calculi x' upon and this also gives us the added benefit of not having to worry about any any issues",
    "start": "903500",
    "end": "910280"
  },
  {
    "text": "like spanning tree protocol issues there's no convergence timer or loops that could be accidentally caused by",
    "start": "910280",
    "end": "916700"
  },
  {
    "text": "this so like I said each of these has",
    "start": "916700",
    "end": "922700"
  },
  {
    "text": "their own bgp domain which means they have their own layer 3 network slice so",
    "start": "922700",
    "end": "930740"
  },
  {
    "text": "we can assign networking a slash 24 now to each of these and we can also anycast",
    "start": "930740",
    "end": "938629"
  },
  {
    "text": "the same IP across all of these so this makes it a lot easier to to to have",
    "start": "938629",
    "end": "946610"
  },
  {
    "text": "services communicate with just any any service IP and it will be routed to any",
    "start": "946610",
    "end": "952399"
  },
  {
    "text": "healthy instance that can respond to that so we use calcio to do bgp peering",
    "start": "952399",
    "end": "958639"
  },
  {
    "text": "directly with these leaf switches and we as we pair that with all of our",
    "start": "958639",
    "end": "963850"
  },
  {
    "text": "kubernetes nodes as well and this allows us to seamlessly communicate whether",
    "start": "963850",
    "end": "969410"
  },
  {
    "text": "it's vm running on on a rack in a different leaf switch it can communicate with any pod that's running in",
    "start": "969410",
    "end": "977389"
  },
  {
    "text": "kubernetes so this is a typical kubernetes architecture where we have",
    "start": "977389",
    "end": "983990"
  },
  {
    "start": "979000",
    "end": "1062000"
  },
  {
    "text": "semesters and some nodes running some pod workers using calcio we're able to",
    "start": "983990",
    "end": "990139"
  },
  {
    "text": "announce directly the pod IPS and these are representatives slash 26 subnets that Calico assigns directly to these",
    "start": "990139",
    "end": "997490"
  },
  {
    "text": "and the calculation would then pick these up and announce them to the to the",
    "start": "997490",
    "end": "1002769"
  },
  {
    "text": "top of racks which likewise we're able to announce directly the service IP range so all we have to do is add these",
    "start": "1002769",
    "end": "1010749"
  },
  {
    "text": "to the loopback interface and tell Calico to announce these and all of a sudden we have the ability to for",
    "start": "1010749",
    "end": "1018369"
  },
  {
    "text": "anything whether it's inside of kubernetes are outside of kubernetes to communicate directly with this service",
    "start": "1018369",
    "end": "1024938"
  },
  {
    "text": "IP range which makes it a lot easier to for outside services or developers to",
    "start": "1024939",
    "end": "1030730"
  },
  {
    "text": "access and likewise we can announce the week we can assign an any cast address",
    "start": "1030730",
    "end": "1037630"
  },
  {
    "text": "for the API server that we then bind to the API server to so we don't need any any middleman for communicating with the",
    "start": "1037630",
    "end": "1045339"
  },
  {
    "text": "API server we just bind each API server that we want to this IP address and we",
    "start": "1045339",
    "end": "1050559"
  },
  {
    "text": "can communicate with that IP which makes it a lot easier in case one of the master nodes goes down we don't lose any",
    "start": "1050559",
    "end": "1057760"
  },
  {
    "text": "traffic to the API servers",
    "start": "1057760",
    "end": "1062940"
  },
  {
    "start": "1062000",
    "end": "1106000"
  },
  {
    "text": "so this allows a developer from their laptop to communicate directly with a",
    "start": "1064210",
    "end": "1069340"
  },
  {
    "text": "pod IP to any any of these pods that are running in kubernetes they can",
    "start": "1069340",
    "end": "1075789"
  },
  {
    "text": "communicate with the master IP which is then anycast and likewise they can talk to a service",
    "start": "1075789",
    "end": "1083259"
  },
  {
    "text": "IP as well so this gives us a really powerful way for our developers to have",
    "start": "1083259",
    "end": "1091240"
  },
  {
    "text": "self-service networking without any interaction with us and then we can leverage the network policies as well to",
    "start": "1091240",
    "end": "1098019"
  },
  {
    "text": "automatically assign firewall rules for instance if we want for these services",
    "start": "1098019",
    "end": "1104830"
  },
  {
    "text": "as well we the other benefit of this is",
    "start": "1104830",
    "end": "1110830"
  },
  {
    "start": "1106000",
    "end": "1132000"
  },
  {
    "text": "we can also rely on this for federating as well because we have two data you",
    "start": "1110830",
    "end": "1117129"
  },
  {
    "text": "know multiple data centers we can seamlessly communicate from the pod",
    "start": "1117129",
    "end": "1122470"
  },
  {
    "text": "network from one data center to another data center and using the Federation API we can then control both of them at the",
    "start": "1122470",
    "end": "1129340"
  },
  {
    "text": "same time so the other the other",
    "start": "1129340",
    "end": "1136080"
  },
  {
    "start": "1132000",
    "end": "1221000"
  },
  {
    "text": "self-service tool that we decided to to use was Prometheus historically we use",
    "start": "1136080",
    "end": "1143679"
  },
  {
    "text": "graphite for all of our VM based metrics this had some problems",
    "start": "1143679",
    "end": "1150509"
  },
  {
    "text": "unfortunately the application and collecti is are sending metrics either",
    "start": "1150509",
    "end": "1157299"
  },
  {
    "text": "from the application or from the from the system and there's forwarding this to graphite unfortunately we can easily",
    "start": "1157299",
    "end": "1167049"
  },
  {
    "text": "run into scenarios where the application developer just added a new endpoint to",
    "start": "1167049",
    "end": "1173440"
  },
  {
    "text": "generate metrics and we have this exponential explosion of metrics as we",
    "start": "1173440",
    "end": "1179049"
  },
  {
    "text": "deploy them to all of our services and all of these are aggregated all these metrics are aggregated in graphite where",
    "start": "1179049",
    "end": "1185470"
  },
  {
    "text": "each metric ends up becoming its own file so which is slow to create on all",
    "start": "1185470",
    "end": "1191799"
  },
  {
    "text": "on in this cluster and also really slow expensive to",
    "start": "1191799",
    "end": "1197830"
  },
  {
    "text": "to clean up so when we have all of these ephemeral pods we can't really rely on this because we end up overloading the",
    "start": "1197830",
    "end": "1204760"
  },
  {
    "text": "graphite cluster and bringing it down graphite also has some other problems as",
    "start": "1204760",
    "end": "1210220"
  },
  {
    "text": "well like there's loss of position when it does aggregation aggregated roll-ups of all of these metrics as well and it's",
    "start": "1210220",
    "end": "1217269"
  },
  {
    "text": "it's kind of inefficient to do aggregated calculations across all of these we then use sensu to actually do a",
    "start": "1217269",
    "end": "1225309"
  },
  {
    "text": "learning based off of off of the sensor client that runs on the host and we can",
    "start": "1225309",
    "end": "1232179"
  },
  {
    "text": "also trigger we can also have that query graphite as well however we were running",
    "start": "1232179",
    "end": "1238389"
  },
  {
    "text": "into some problems with this the the application and in the system are very tightly coupled it then becomes really",
    "start": "1238389",
    "end": "1245440"
  },
  {
    "text": "difficult for us to route alerts to it what happens when the application goes",
    "start": "1245440",
    "end": "1250480"
  },
  {
    "text": "down who do we route the alert to the application dev what happens when the system goes when the whole host goes",
    "start": "1250480",
    "end": "1257380"
  },
  {
    "text": "down do we route that to the application developer do we route that to the team",
    "start": "1257380",
    "end": "1264070"
  },
  {
    "text": "that's in charge of running that host and what happens if the whole hypervisor goes down who do we alert so we really",
    "start": "1264070",
    "end": "1271000"
  },
  {
    "text": "want these our application level alerts to be service level based however when",
    "start": "1271000",
    "end": "1278200"
  },
  {
    "text": "we pair sensitive with graphite it becomes really confusing to create because we had this central I this other",
    "start": "1278200",
    "end": "1284820"
  },
  {
    "text": "repository where devs would have to go into and add all of these checks and the",
    "start": "1284820",
    "end": "1290320"
  },
  {
    "text": "checks were actually really expensive because doing all these aggregated checks on graphite is painful so in",
    "start": "1290320",
    "end": "1298630"
  },
  {
    "start": "1297000",
    "end": "1358000"
  },
  {
    "text": "comes Prometheus Prometheus ties in really well with our kubernetes",
    "start": "1298630",
    "end": "1303850"
  },
  {
    "text": "infrastructure it gives us the ability to do automatic discovery of all of our containers whether it's using the",
    "start": "1303850",
    "end": "1310090"
  },
  {
    "text": "kubernetes api to communicate with pods or it could communicate with console and discover an AVM based application as",
    "start": "1310090",
    "end": "1317649"
  },
  {
    "text": "well there's no loss of precision because it just keeps appending metrics and it's really great at storing tag",
    "start": "1317649",
    "end": "1325779"
  },
  {
    "text": "data so we can store a metric space with tags of whatever the service are running",
    "start": "1325779",
    "end": "1331419"
  },
  {
    "text": "what hod they're running on which endpoint we're using to collect these metrics from and so forth and it's really really",
    "start": "1331419",
    "end": "1339460"
  },
  {
    "text": "great for these efficient for these ephemeral instances so when a new pod comes up aw is doing is modifying the",
    "start": "1339460",
    "end": "1347500"
  },
  {
    "text": "tags that are four that are being generated for that for that metric so",
    "start": "1347500",
    "end": "1353799"
  },
  {
    "text": "all of those would be aggregated and ultimately the same Prometheus file so",
    "start": "1353799",
    "end": "1359740"
  },
  {
    "start": "1358000",
    "end": "1407000"
  },
  {
    "text": "we used two Prometheus operator for this so we assign each team that wants to",
    "start": "1359740",
    "end": "1367210"
  },
  {
    "text": "deploy services to kubernetes urn namespace we also give them their own Prometheus instance as well so this",
    "start": "1367210",
    "end": "1374289"
  },
  {
    "text": "gives us a few benefits because we are able to then separate out each team's",
    "start": "1374289",
    "end": "1383350"
  },
  {
    "text": "names each team's Prometheus metric collector and that that gives us the ability for you know when one of these",
    "start": "1383350",
    "end": "1390879"
  },
  {
    "text": "Prometheus goes down we don't lose metrics for everything we just would lose that those metrics temporarily for",
    "start": "1390879",
    "end": "1397000"
  },
  {
    "text": "that one team so the Prometheus operator controls all of these Prometheus instances and they'll look at they'll",
    "start": "1397000",
    "end": "1404889"
  },
  {
    "text": "collect metrics from all of those team services and we provide a centralized",
    "start": "1404889",
    "end": "1410259"
  },
  {
    "start": "1407000",
    "end": "1427000"
  },
  {
    "text": "alert manager which is also configured through the Prometheus operator that would then route the alerts to pager",
    "start": "1410259",
    "end": "1415960"
  },
  {
    "text": "duty so at that point all the service owners have to do is define their own alerts for all these services and those",
    "start": "1415960",
    "end": "1423370"
  },
  {
    "text": "alerts are the Prometheus alert manager specification that look like this all we",
    "start": "1423370",
    "end": "1429549"
  },
  {
    "start": "1427000",
    "end": "1460000"
  },
  {
    "text": "have to do all they have to do is provide a service level style check",
    "start": "1429549",
    "end": "1435429"
  },
  {
    "text": "where for instance if we wanted to look at the error rates for a given service all you have to do is look at the",
    "start": "1435429",
    "end": "1442000"
  },
  {
    "text": "response the check for response codes of 500 and if that is you know high for",
    "start": "1442000",
    "end": "1450940"
  },
  {
    "text": "five minutes then we would just send an alert to whatever team that is with a",
    "start": "1450940",
    "end": "1456549"
  },
  {
    "text": "page that says this is critical please look at this",
    "start": "1456549",
    "end": "1461070"
  },
  {
    "start": "1460000",
    "end": "1534000"
  },
  {
    "text": "so the final the final tool that we provide for our developers is",
    "start": "1461580",
    "end": "1467989"
  },
  {
    "text": "self-service storage historically we have a centralized NFS cluster for all",
    "start": "1467989",
    "end": "1475559"
  },
  {
    "text": "of our files and we were running into a lot of problems where it was very difficult to spin up a new VM based",
    "start": "1475559",
    "end": "1484049"
  },
  {
    "text": "service that would have access to this all the access controls were manual",
    "start": "1484049",
    "end": "1490350"
  },
  {
    "text": "manual manually configured in the NFS cluster and we could also often run into issues depending on the application with",
    "start": "1490350",
    "end": "1498299"
  },
  {
    "text": "you know things like file locking issues where you know some service would be locking a file it would die and wouldn't",
    "start": "1498299",
    "end": "1505259"
  },
  {
    "text": "be able to come up again because that that file was locked on NFS we also",
    "start": "1505259",
    "end": "1511940"
  },
  {
    "text": "would leverage ESX local storage as well however this had some problems of you",
    "start": "1511940",
    "end": "1518730"
  },
  {
    "text": "know slow migrations if we needed to spin up a if we needed to move the VM elsewhere we would have to transfer all",
    "start": "1518730",
    "end": "1525029"
  },
  {
    "text": "of the data from one host to another and there was no replication so in order to really provide self-service storage",
    "start": "1525029",
    "end": "1532139"
  },
  {
    "text": "infrastructure we needed something else that's where SEF comes in Stef gives us",
    "start": "1532139",
    "end": "1537919"
  },
  {
    "start": "1534000",
    "end": "1577000"
  },
  {
    "text": "the ability to deploy on commodity hardware so all we so we don't need",
    "start": "1537919",
    "end": "1545009"
  },
  {
    "text": "these expensive NFS machines and all we have to do is scale out the system",
    "start": "1545009",
    "end": "1553830"
  },
  {
    "text": "whenever we need more storage set provides automatic replication and it",
    "start": "1553830",
    "end": "1559769"
  },
  {
    "text": "provides multiple access patterns as well so we can access either block storage directly we can access you know",
    "start": "1559769",
    "end": "1566820"
  },
  {
    "text": "sefa fast eviction is very similar to NFS and we can also access an object",
    "start": "1566820",
    "end": "1572190"
  },
  {
    "text": "store it provides an s3 compatible API",
    "start": "1572190",
    "end": "1577850"
  },
  {
    "start": "1577000",
    "end": "1607000"
  },
  {
    "text": "so again we to rely on this we weave in",
    "start": "1577999",
    "end": "1584489"
  },
  {
    "text": "kubernetes we have automatic provisioners that are based off of the",
    "start": "1584489",
    "end": "1589499"
  },
  {
    "text": "storage class concept all we have to do is define a default storage class for",
    "start": "1589499",
    "end": "1595540"
  },
  {
    "text": "the block device and provide a provisioner that has the ability to access Seth and create all of these all",
    "start": "1595540",
    "end": "1603010"
  },
  {
    "text": "of these blocks storage devices as they're requested so all a developer",
    "start": "1603010",
    "end": "1609580"
  },
  {
    "start": "1607000",
    "end": "1649000"
  },
  {
    "text": "would have to do is say you know I need a stateful set and here's the the persistent volume claim that goes along",
    "start": "1609580",
    "end": "1615940"
  },
  {
    "text": "with that and then kubernetes and the automatic provisioner take care of the",
    "start": "1615940",
    "end": "1621250"
  },
  {
    "text": "rest the RBD provisionary would then detect the creation of this of this pod of this",
    "start": "1621250",
    "end": "1627790"
  },
  {
    "text": "persistent volume claim and then create the block device and then allow the stateful set to then mount this this",
    "start": "1627790",
    "end": "1636300"
  },
  {
    "text": "persistent volume which is then mapped directly to the to that block device and it will manage the the lifecycle of that",
    "start": "1636300",
    "end": "1642970"
  },
  {
    "text": "as well so when you get rid of the stateful set the block storage is cleaned up as well so we rely on this",
    "start": "1642970",
    "end": "1651550"
  },
  {
    "start": "1649000",
    "end": "1680000"
  },
  {
    "text": "for some tools so we have Prometheus all of the Prometheus instances are relying",
    "start": "1651550",
    "end": "1658630"
  },
  {
    "text": "on these purpose persistent volume claims to to store all of their metric",
    "start": "1658630",
    "end": "1664090"
  },
  {
    "text": "data for some amount of time we also have some deployments of MongoDB and",
    "start": "1664090",
    "end": "1670450"
  },
  {
    "text": "Postgres relying on this as well and they've been running they've been",
    "start": "1670450",
    "end": "1678310"
  },
  {
    "text": "running great for us so far likewise we can rely on this sefa FS shared storage",
    "start": "1678310",
    "end": "1686320"
  },
  {
    "start": "1680000",
    "end": "1708000"
  },
  {
    "text": "for this this gives us the ability of having multiple services access the same",
    "start": "1686320",
    "end": "1692110"
  },
  {
    "text": "NFS pool by creating just a single PVC that any service instance is able to",
    "start": "1692110",
    "end": "1699250"
  },
  {
    "text": "access and then the Ceph FS provisioner would create a shared mount that all",
    "start": "1699250",
    "end": "1705460"
  },
  {
    "text": "these are able to access and share data across the final way for accessing of",
    "start": "1705460",
    "end": "1711520"
  },
  {
    "text": "course would be for the service to directly communicate with the s3 like",
    "start": "1711520",
    "end": "1717130"
  },
  {
    "text": "API so all that has to do is you know create a bucket and then access anything",
    "start": "1717130",
    "end": "1723160"
  },
  {
    "text": "in there and then this would be a completely separate of any of the",
    "start": "1723160",
    "end": "1729130"
  },
  {
    "text": "kubernetes automatic provisioners so with the help of kubernetes Calico",
    "start": "1729130",
    "end": "1735520"
  },
  {
    "start": "1732000",
    "end": "1809000"
  },
  {
    "text": "prometheus and seth we're able to provide self-service infrastructure for all of our developers and we've seen a",
    "start": "1735520",
    "end": "1742780"
  },
  {
    "text": "lot of benefits from this our existing services are migrated very quickly all",
    "start": "1742780",
    "end": "1748360"
  },
  {
    "text": "it takes is a couple hours of work to to migrate a service verify that it's healthy and create the appropriate",
    "start": "1748360",
    "end": "1755110"
  },
  {
    "text": "Prometheus dashboards for it we're also seeing a lot more adoption from",
    "start": "1755110",
    "end": "1760330"
  },
  {
    "text": "developers we're seeing about 20 new services that I'm aware of that are being planned for q1 which is a lot",
    "start": "1760330",
    "end": "1768520"
  },
  {
    "text": "larger than what we've seen before for one quarter and we're really seeing true",
    "start": "1768520",
    "end": "1774190"
  },
  {
    "text": "micro service adoption so we're seeing a lot of developers who want to spin up you know small experiments that they",
    "start": "1774190",
    "end": "1782080"
  },
  {
    "text": "otherwise wouldn't have been able to do in our infrastructure because of the long process for creating all of these",
    "start": "1782080",
    "end": "1788440"
  },
  {
    "text": "VMs so so finally Squarespace is able to",
    "start": "1788440",
    "end": "1793900"
  },
  {
    "text": "move as quickly as we want it to well thank you for listening to me are there",
    "start": "1793900",
    "end": "1800080"
  },
  {
    "text": "any questions [Applause]",
    "start": "1800080",
    "end": "1809230"
  },
  {
    "start": "1809000",
    "end": "1839000"
  },
  {
    "text": "yeah so we run the databases in kubernetes so we have some stateful sets",
    "start": "1809530",
    "end": "1816950"
  },
  {
    "text": "for and we've got some staples that's for Postgres and like I said like I said earlier all they have to do is",
    "start": "1816950",
    "end": "1823640"
  },
  {
    "text": "define that PVC and then they have shared storage so then when the pod goes",
    "start": "1823640",
    "end": "1829130"
  },
  {
    "text": "down it's just migrated to another host and comes back online with the same data",
    "start": "1829130",
    "end": "1835300"
  },
  {
    "start": "1839000",
    "end": "1865000"
  },
  {
    "text": "so we run all of the Prometheus instances in the same cluster we then",
    "start": "1840130",
    "end": "1845150"
  },
  {
    "text": "federal of the metric data to an external system as well that has a lot",
    "start": "1845150",
    "end": "1851030"
  },
  {
    "text": "more capacity for storage",
    "start": "1851030",
    "end": "1854950"
  },
  {
    "start": "1865000",
    "end": "1938000"
  },
  {
    "text": "so we don't abstract that we are right",
    "start": "1865199",
    "end": "1871839"
  },
  {
    "text": "now the service developers are writing the the yam well the kubernetes yeah",
    "start": "1871839",
    "end": "1877509"
  },
  {
    "text": "mellow descriptions but we do have a generator tool for the new services that will generate an entire spring",
    "start": "1877509",
    "end": "1885039"
  },
  {
    "text": "beautification and along with that we generate all of the the prescribed deployment and and alerts for all those",
    "start": "1885039",
    "end": "1892719"
  },
  {
    "text": "services",
    "start": "1892719",
    "end": "1894929"
  },
  {
    "text": "I'm sorry I didn't yeah",
    "start": "1906220",
    "end": "1921030"
  },
  {
    "text": "with the number of policies you mean",
    "start": "1924660",
    "end": "1928710"
  },
  {
    "text": "yeah yeah yeah so it's connected to -",
    "start": "1930030",
    "end": "1940840"
  },
  {
    "text": "yeah yeah so so we use so there's",
    "start": "1940840",
    "end": "1952240"
  },
  {
    "start": "1947000",
    "end": "1990000"
  },
  {
    "text": "redundancy at the at the network connection layer and then there's redundancy with with each of the each of",
    "start": "1952240",
    "end": "1960100"
  },
  {
    "text": "the leafs communicating with with two spines yeah so sorry",
    "start": "1960100",
    "end": "1967830"
  },
  {
    "start": "1990000",
    "end": "2033000"
  },
  {
    "text": "yeah spring brute we're not relying on console for the configuration itself so",
    "start": "1990720",
    "end": "1997890"
  },
  {
    "text": "we're using so someone there deployed is VMs we just use a just a regular",
    "start": "1997890",
    "end": "2004160"
  },
  {
    "text": "configure mo file that we've deployed to each of those hosts along with that code we can also use environment variables as",
    "start": "2004160",
    "end": "2011450"
  },
  {
    "text": "well which is what we use in the kubernetes environment if if there's any any information that the service once",
    "start": "2011450",
    "end": "2018530"
  },
  {
    "text": "itself it would communicate with the key value store directly with console yeah",
    "start": "2018530",
    "end": "2026890"
  },
  {
    "start": "2033000",
    "end": "2077000"
  },
  {
    "text": "so we haven't we haven't had the need yet to really like squeeze performance",
    "start": "2034360",
    "end": "2040070"
  },
  {
    "text": "out of it so we haven't gone down that path yet we just wanted to make sure that there was no performance loss",
    "start": "2040070",
    "end": "2046220"
  },
  {
    "text": "moving to kubernetes so I think that's something we're going to be exploring",
    "start": "2046220",
    "end": "2051980"
  },
  {
    "text": "later on like really how small can we can we get all of these services and and and what's the trade-off between",
    "start": "2051980",
    "end": "2058690"
  },
  {
    "text": "instance size and number of instances because you know we're dealing with the",
    "start": "2058690",
    "end": "2064700"
  },
  {
    "text": "JVM which isn't the friendliest of beasts",
    "start": "2064700",
    "end": "2069220"
  },
  {
    "start": "2077000",
    "end": "2126000"
  },
  {
    "text": "now the the memory side seemed fine because we were well I mean we pass we",
    "start": "2077830",
    "end": "2087710"
  },
  {
    "text": "pass in very similarly in that base container we calculate the number of the",
    "start": "2087710",
    "end": "2093639"
  },
  {
    "text": "the amount of RAM that's assigned to that and then we scale the heap appropriately so I think it's it's",
    "start": "2093640",
    "end": "2102770"
  },
  {
    "text": "tunable but I think by default we give the heap 50% and allow 50% of non heap",
    "start": "2102770",
    "end": "2109210"
  },
  {
    "text": "but that's tunable per service as well",
    "start": "2109210",
    "end": "2114040"
  },
  {
    "text": "cool",
    "start": "2117070",
    "end": "2120070"
  },
  {
    "start": "2126000",
    "end": "2189000"
  },
  {
    "text": "no we didn't go down that path we did use maces for a little bit our data team",
    "start": "2127660",
    "end": "2135260"
  },
  {
    "text": "was using that as kind of an internal test and that was right around the same",
    "start": "2135260",
    "end": "2141650"
  },
  {
    "text": "time that kubernetes really really was becoming popular so we saw a lot of benefit in switching that early on",
    "start": "2141650",
    "end": "2149470"
  },
  {
    "text": "no we didn't what's that",
    "start": "2168450",
    "end": "2175490"
  },
  {
    "text": "No",
    "start": "2179690",
    "end": "2182690"
  },
  {
    "text": "all right thank you [Applause]",
    "start": "2184700",
    "end": "2190989"
  }
]