[
  {
    "text": "hi everyone I'm very happy to be here",
    "start": "0",
    "end": "2460"
  },
  {
    "text": "today in Shanghai giving this talk to",
    "start": "2460",
    "end": "4259"
  },
  {
    "text": "all of you",
    "start": "4259",
    "end": "5160"
  },
  {
    "text": "so my talk is yet another talk about AI",
    "start": "5160",
    "end": "7980"
  },
  {
    "text": "it's such a buzzword now",
    "start": "7980",
    "end": "10500"
  },
  {
    "text": "but I don't think AI is just a buzzword",
    "start": "10500",
    "end": "12660"
  },
  {
    "text": "because we finally have models that",
    "start": "12660",
    "end": "14580"
  },
  {
    "text": "actually work models that can improve",
    "start": "14580",
    "end": "17160"
  },
  {
    "text": "your productivity and since most of us",
    "start": "17160",
    "end": "19500"
  },
  {
    "text": "here are probably programmers one of",
    "start": "19500",
    "end": "21840"
  },
  {
    "text": "these tools is code generation models",
    "start": "21840",
    "end": "23880"
  },
  {
    "text": "these are models that can also complete",
    "start": "23880",
    "end": "26460"
  },
  {
    "text": "your code it can be a function or a",
    "start": "26460",
    "end": "29039"
  },
  {
    "text": "class or a whole code block just like",
    "start": "29039",
    "end": "31199"
  },
  {
    "text": "GitHub compilers for example",
    "start": "31199",
    "end": "34200"
  },
  {
    "text": "so these models have been there for a",
    "start": "34200",
    "end": "36239"
  },
  {
    "text": "while but they've only been more trendy",
    "start": "36239",
    "end": "38399"
  },
  {
    "text": "and interested in research when codex",
    "start": "38399",
    "end": "40200"
  },
  {
    "text": "was released it's a co-generational",
    "start": "40200",
    "end": "42059"
  },
  {
    "text": "model by openai which is being used for",
    "start": "42059",
    "end": "44760"
  },
  {
    "text": "the vs code extension for GitHub Pilots",
    "start": "44760",
    "end": "47100"
  },
  {
    "text": "by Microsoft",
    "start": "47100",
    "end": "48600"
  },
  {
    "text": "before this model there were some small",
    "start": "48600",
    "end": "50579"
  },
  {
    "text": "code generation models like code GPT",
    "start": "50579",
    "end": "52920"
  },
  {
    "text": "code bird but these were small models",
    "start": "52920",
    "end": "55320"
  },
  {
    "text": "that were trained on a little data so",
    "start": "55320",
    "end": "57719"
  },
  {
    "text": "they weren't very good at the task",
    "start": "57719",
    "end": "59760"
  },
  {
    "text": "but when codex was released they kind of",
    "start": "59760",
    "end": "62100"
  },
  {
    "text": "showed that you can just train a code",
    "start": "62100",
    "end": "63840"
  },
  {
    "text": "model like you do for a language model",
    "start": "63840",
    "end": "65400"
  },
  {
    "text": "you take a Transformer that is large",
    "start": "65400",
    "end": "67740"
  },
  {
    "text": "enough and you'd see this a lot of code",
    "start": "67740",
    "end": "69600"
  },
  {
    "text": "data and just learns how to program",
    "start": "69600",
    "end": "72060"
  },
  {
    "text": "then deepmind worked on Alpha code and",
    "start": "72060",
    "end": "74520"
  },
  {
    "text": "AWS worked on code Whisperer which was",
    "start": "74520",
    "end": "77580"
  },
  {
    "text": "great but these models are all closed",
    "start": "77580",
    "end": "80280"
  },
  {
    "text": "models meaning you have to send your API",
    "start": "80280",
    "end": "83159"
  },
  {
    "text": "your data to an external API you also",
    "start": "83159",
    "end": "85740"
  },
  {
    "text": "can't fine tune these models so that's",
    "start": "85740",
    "end": "87840"
  },
  {
    "text": "when the open source Community tried to",
    "start": "87840",
    "end": "89640"
  },
  {
    "text": "step in and train some good code",
    "start": "89640",
    "end": "91560"
  },
  {
    "text": "generation models for example code",
    "start": "91560",
    "end": "93600"
  },
  {
    "text": "powers from hacking face encoder from",
    "start": "93600",
    "end": "95460"
  },
  {
    "text": "meta and code gen from Salesforce",
    "start": "95460",
    "end": "98579"
  },
  {
    "text": "however there are still some open",
    "start": "98579",
    "end": "100740"
  },
  {
    "text": "questions about these models first",
    "start": "100740",
    "end": "103079"
  },
  {
    "text": "regarding the performance because these",
    "start": "103079",
    "end": "105360"
  },
  {
    "text": "models weren't at the level of codex yet",
    "start": "105360",
    "end": "107220"
  },
  {
    "text": "and there was also a focus on Python and",
    "start": "107220",
    "end": "110159"
  },
  {
    "text": "not the other programming languages",
    "start": "110159",
    "end": "111540"
  },
  {
    "text": "there's also an open question about",
    "start": "111540",
    "end": "113759"
  },
  {
    "text": "transparency which data were these",
    "start": "113759",
    "end": "116399"
  },
  {
    "text": "models trained on for example",
    "start": "116399",
    "end": "118320"
  },
  {
    "text": "there are also questions about",
    "start": "118320",
    "end": "119820"
  },
  {
    "text": "evaluation and user experience so these",
    "start": "119820",
    "end": "122579"
  },
  {
    "text": "are questions that we try to address",
    "start": "122579",
    "end": "125159"
  },
  {
    "text": "so now let's see what does hugging phase",
    "start": "125159",
    "end": "127259"
  },
  {
    "text": "have to do with this we have the hub for",
    "start": "127259",
    "end": "129479"
  },
  {
    "text": "sharing data sets and models but we also",
    "start": "129479",
    "end": "131940"
  },
  {
    "text": "train models",
    "start": "131940",
    "end": "133379"
  },
  {
    "text": "last year we released code pirate which",
    "start": "133379",
    "end": "135959"
  },
  {
    "text": "is an educational tool for training",
    "start": "135959",
    "end": "137700"
  },
  {
    "text": "these models showing you how to scrape",
    "start": "137700",
    "end": "139440"
  },
  {
    "text": "data how to train these models and",
    "start": "139440",
    "end": "141300"
  },
  {
    "text": "evaluate them so it was more for",
    "start": "141300",
    "end": "143099"
  },
  {
    "text": "education and the performance was not",
    "start": "143099",
    "end": "144840"
  },
  {
    "text": "very great a few months ago we released",
    "start": "144840",
    "end": "147480"
  },
  {
    "text": "star coder which is a strong code",
    "start": "147480",
    "end": "149280"
  },
  {
    "text": "generation model trained on more than 80",
    "start": "149280",
    "end": "151440"
  },
  {
    "text": "programming languages and it surpasses",
    "start": "151440",
    "end": "153720"
  },
  {
    "text": "the performance of codex on python but",
    "start": "153720",
    "end": "156060"
  },
  {
    "text": "also on other languages the model is",
    "start": "156060",
    "end": "158160"
  },
  {
    "text": "also open access",
    "start": "158160",
    "end": "160500"
  },
  {
    "text": "so today I'm gonna talk to you about the",
    "start": "160500",
    "end": "162540"
  },
  {
    "text": "projects where we developed this and",
    "start": "162540",
    "end": "164280"
  },
  {
    "text": "about the architecture and deployment of",
    "start": "164280",
    "end": "166080"
  },
  {
    "text": "these models",
    "start": "166080",
    "end": "168120"
  },
  {
    "text": "so we decided to create a big",
    "start": "168120",
    "end": "170040"
  },
  {
    "text": "collaboration called Big code where we",
    "start": "170040",
    "end": "172319"
  },
  {
    "text": "have more than 500 participants from",
    "start": "172319",
    "end": "174540"
  },
  {
    "text": "over 30 countries this collaboration is",
    "start": "174540",
    "end": "176879"
  },
  {
    "text": "led by hacking face in service now but",
    "start": "176879",
    "end": "178800"
  },
  {
    "text": "is open for anyone to join so if anyone",
    "start": "178800",
    "end": "181080"
  },
  {
    "text": "in the audience today would like to help",
    "start": "181080",
    "end": "182580"
  },
  {
    "text": "we're very happy to have you our goal is",
    "start": "182580",
    "end": "185220"
  },
  {
    "text": "to train strong code generation models",
    "start": "185220",
    "end": "187260"
  },
  {
    "text": "but in an open and responsible approach",
    "start": "187260",
    "end": "191400"
  },
  {
    "text": "when you work starts with the code is to",
    "start": "191400",
    "end": "193560"
  },
  {
    "text": "address some of the bad challenges of",
    "start": "193560",
    "end": "195900"
  },
  {
    "text": "closed large language models development",
    "start": "195900",
    "end": "197760"
  },
  {
    "text": "this starts with not disclosing your",
    "start": "197760",
    "end": "200280"
  },
  {
    "text": "training data in the sources that were",
    "start": "200280",
    "end": "202260"
  },
  {
    "text": "used",
    "start": "202260",
    "end": "203220"
  },
  {
    "text": "for example today there are a lot of",
    "start": "203220",
    "end": "204900"
  },
  {
    "text": "great Open Access models but we don't",
    "start": "204900",
    "end": "206940"
  },
  {
    "text": "know on which data they were trained",
    "start": "206940",
    "end": "209159"
  },
  {
    "text": "the other bad practice is not releasing",
    "start": "209159",
    "end": "211260"
  },
  {
    "text": "the model weights at all so if you want",
    "start": "211260",
    "end": "213180"
  },
  {
    "text": "to fine tune these models you can't and",
    "start": "213180",
    "end": "215459"
  },
  {
    "text": "if you want to deploy them on premise",
    "start": "215459",
    "end": "216959"
  },
  {
    "text": "you also can't and you will always have",
    "start": "216959",
    "end": "218940"
  },
  {
    "text": "to send your data which might be",
    "start": "218940",
    "end": "220379"
  },
  {
    "text": "sensitive to third parties",
    "start": "220379",
    "end": "222900"
  },
  {
    "text": "another thing that is needless to say is",
    "start": "222900",
    "end": "226140"
  },
  {
    "text": "that all of this makes results not",
    "start": "226140",
    "end": "227819"
  },
  {
    "text": "reproducible and does it Foster research",
    "start": "227819",
    "end": "229920"
  },
  {
    "text": "in open source so what we think open",
    "start": "229920",
    "end": "232500"
  },
  {
    "text": "development of these models should look",
    "start": "232500",
    "end": "234120"
  },
  {
    "text": "like is exactly the opposite of all of",
    "start": "234120",
    "end": "236220"
  },
  {
    "text": "that first you should make your training",
    "start": "236220",
    "end": "238379"
  },
  {
    "text": "data public and you should give",
    "start": "238379",
    "end": "240000"
  },
  {
    "text": "inspection tools so that people can look",
    "start": "240000",
    "end": "242040"
  },
  {
    "text": "into the data and see what's in there",
    "start": "242040",
    "end": "243840"
  },
  {
    "text": "and what these models were trained on",
    "start": "243840",
    "end": "245459"
  },
  {
    "text": "you should also create opt-out Tools in",
    "start": "245459",
    "end": "248340"
  },
  {
    "text": "case people want to be removed because",
    "start": "248340",
    "end": "250439"
  },
  {
    "text": "for example you as authors of some data",
    "start": "250439",
    "end": "252780"
  },
  {
    "text": "or some code you should have the right",
    "start": "252780",
    "end": "254340"
  },
  {
    "text": "to say no I don't want to be in these",
    "start": "254340",
    "end": "256440"
  },
  {
    "text": "model trainings",
    "start": "256440",
    "end": "258060"
  },
  {
    "text": "the other thing does model weight should",
    "start": "258060",
    "end": "260040"
  },
  {
    "text": "be public so that people can fine tune",
    "start": "260040",
    "end": "261840"
  },
  {
    "text": "them and also deploy them on premise and",
    "start": "261840",
    "end": "264180"
  },
  {
    "text": "you should also give full documentation",
    "start": "264180",
    "end": "265620"
  },
  {
    "text": "of the whole process to make it easy to",
    "start": "265620",
    "end": "267840"
  },
  {
    "text": "use and reproducible",
    "start": "267840",
    "end": "270979"
  },
  {
    "text": "so let's see what what it takes to train",
    "start": "271040",
    "end": "273660"
  },
  {
    "text": "these models from scratch it takes",
    "start": "273660",
    "end": "275880"
  },
  {
    "text": "hundreds maybe thousands of GPU hours",
    "start": "275880",
    "end": "278400"
  },
  {
    "text": "but also terabytes of data but that's",
    "start": "278400",
    "end": "280919"
  },
  {
    "text": "not just all and I will try to show you",
    "start": "280919",
    "end": "283020"
  },
  {
    "text": "what goes behind the scenes for training",
    "start": "283020",
    "end": "284940"
  },
  {
    "text": "these models",
    "start": "284940",
    "end": "287040"
  },
  {
    "text": "everything I'm gonna show you is about",
    "start": "287040",
    "end": "288840"
  },
  {
    "text": "star coder this code generation model",
    "start": "288840",
    "end": "291000"
  },
  {
    "text": "that we released this summer and for",
    "start": "291000",
    "end": "292800"
  },
  {
    "text": "Star Wars fans this was named after",
    "start": "292800",
    "end": "294479"
  },
  {
    "text": "recent wasn't released on May the 4th",
    "start": "294479",
    "end": "297600"
  },
  {
    "text": "everything starts with good data so if",
    "start": "297600",
    "end": "300780"
  },
  {
    "text": "you want to train a language model let's",
    "start": "300780",
    "end": "302460"
  },
  {
    "text": "say on Chinese or English you train it",
    "start": "302460",
    "end": "304560"
  },
  {
    "text": "on Chinese and English text so it learns",
    "start": "304560",
    "end": "306600"
  },
  {
    "text": "the language if you train want to train",
    "start": "306600",
    "end": "308880"
  },
  {
    "text": "a language model how to code you need to",
    "start": "308880",
    "end": "310979"
  },
  {
    "text": "train it on code and what's the base",
    "start": "310979",
    "end": "312780"
  },
  {
    "text": "source for that it's GitHub so we cloned",
    "start": "312780",
    "end": "315780"
  },
  {
    "text": "all of GitHub repositories and then we",
    "start": "315780",
    "end": "318300"
  },
  {
    "text": "did a lot of data cleaning to remove",
    "start": "318300",
    "end": "319979"
  },
  {
    "text": "extensions we're not interested in and",
    "start": "319979",
    "end": "322440"
  },
  {
    "text": "also we did some license filtering",
    "start": "322440",
    "end": "324120"
  },
  {
    "text": "because if you take a code repository on",
    "start": "324120",
    "end": "326280"
  },
  {
    "text": "GitHub it can either have a permissive",
    "start": "326280",
    "end": "328380"
  },
  {
    "text": "license like MIT or Apache too it can",
    "start": "328380",
    "end": "331139"
  },
  {
    "text": "have a copy left license like GPL or no",
    "start": "331139",
    "end": "333600"
  },
  {
    "text": "license at all for us we only took the",
    "start": "333600",
    "end": "335820"
  },
  {
    "text": "first category which allows us to use",
    "start": "335820",
    "end": "337560"
  },
  {
    "text": "this code",
    "start": "337560",
    "end": "338520"
  },
  {
    "text": "we also built a data inspection tool so",
    "start": "338520",
    "end": "341580"
  },
  {
    "text": "for example you can go to our tool you",
    "start": "341580",
    "end": "343560"
  },
  {
    "text": "type your GitHub username and if it",
    "start": "343560",
    "end": "345419"
  },
  {
    "text": "tells you if any of your repositories",
    "start": "345419",
    "end": "347160"
  },
  {
    "text": "are in the stack and if you don't want",
    "start": "347160",
    "end": "349020"
  },
  {
    "text": "to be included in our future trainings",
    "start": "349020",
    "end": "350880"
  },
  {
    "text": "you just have to fill a simple Google",
    "start": "350880",
    "end": "352500"
  },
  {
    "text": "form and we'll make sure not to use your",
    "start": "352500",
    "end": "354539"
  },
  {
    "text": "data",
    "start": "354539",
    "end": "355919"
  },
  {
    "text": "we also did a lot of data curation to",
    "start": "355919",
    "end": "359280"
  },
  {
    "text": "make sure that our data was clean and",
    "start": "359280",
    "end": "360960"
  },
  {
    "text": "that the model could actually benefit",
    "start": "360960",
    "end": "362340"
  },
  {
    "text": "from it the first thing was to select a",
    "start": "362340",
    "end": "364919"
  },
  {
    "text": "smaller set of languages and not keep",
    "start": "364919",
    "end": "366900"
  },
  {
    "text": "all 300 because some languages are no",
    "start": "366900",
    "end": "369539"
  },
  {
    "text": "longer used or maintained we also added",
    "start": "369539",
    "end": "372000"
  },
  {
    "text": "other sources for example GitHub issues",
    "start": "372000",
    "end": "374699"
  },
  {
    "text": "conversations git commits and Jupiter",
    "start": "374699",
    "end": "377220"
  },
  {
    "text": "notebooks the other step for cleaning we",
    "start": "377220",
    "end": "379620"
  },
  {
    "text": "did is the duplication because Studies",
    "start": "379620",
    "end": "381840"
  },
  {
    "text": "have shown that feature in the model on",
    "start": "381840",
    "end": "384360"
  },
  {
    "text": "many copies of the same files This is",
    "start": "384360",
    "end": "386400"
  },
  {
    "text": "Gonna Hurt performance so it's important",
    "start": "386400",
    "end": "388259"
  },
  {
    "text": "to only keep one copy of each file we",
    "start": "388259",
    "end": "390720"
  },
  {
    "text": "also did decontamination this means that",
    "start": "390720",
    "end": "393180"
  },
  {
    "text": "if you have an evaluation Benchmark that",
    "start": "393180",
    "end": "395580"
  },
  {
    "text": "you know you're gonna evaluate on after",
    "start": "395580",
    "end": "397020"
  },
  {
    "text": "training you need to make sure they're",
    "start": "397020",
    "end": "398940"
  },
  {
    "text": "not a new training set the last step we",
    "start": "398940",
    "end": "401520"
  },
  {
    "text": "did for cleaning was removing personal",
    "start": "401520",
    "end": "403500"
  },
  {
    "text": "identifiable information this is believe",
    "start": "403500",
    "end": "406199"
  },
  {
    "text": "it or not there's still API keys in SSH",
    "start": "406199",
    "end": "408419"
  },
  {
    "text": "keys and public repositories on GitHub",
    "start": "408419",
    "end": "410520"
  },
  {
    "text": "and we didn't want to train our models",
    "start": "410520",
    "end": "412680"
  },
  {
    "text": "on that so that during inference people",
    "start": "412680",
    "end": "414780"
  },
  {
    "text": "wouldn't exploit the this and use",
    "start": "414780",
    "end": "416819"
  },
  {
    "text": "people's secretive data",
    "start": "416819",
    "end": "419039"
  },
  {
    "text": "if you're interested in how you can run",
    "start": "419039",
    "end": "421680"
  },
  {
    "text": "all of these processings on very large",
    "start": "421680",
    "end": "423720"
  },
  {
    "text": "data sets then hacking face datasets",
    "start": "423720",
    "end": "426060"
  },
  {
    "text": "Library comes to the rescue this is the",
    "start": "426060",
    "end": "428160"
  },
  {
    "text": "library we used for all our cleanings it",
    "start": "428160",
    "end": "430319"
  },
  {
    "text": "has some very nice features like",
    "start": "430319",
    "end": "431759"
  },
  {
    "text": "applying filters and Transformations",
    "start": "431759",
    "end": "433819"
  },
  {
    "text": "using a method called map which allows",
    "start": "433819",
    "end": "436440"
  },
  {
    "text": "multi-processing and my favorite feature",
    "start": "436440",
    "end": "438780"
  },
  {
    "text": "is batched mapping when you can Leverage",
    "start": "438780",
    "end": "440880"
  },
  {
    "text": "The Power of map and using batches and",
    "start": "440880",
    "end": "443099"
  },
  {
    "text": "make the filtering go so much faster",
    "start": "443099",
    "end": "446400"
  },
  {
    "text": "Okay so we've seen how to prepare the",
    "start": "446400",
    "end": "448680"
  },
  {
    "text": "data now how do we train these models",
    "start": "448680",
    "end": "450599"
  },
  {
    "text": "how they leverage a lot of gpus and make",
    "start": "450599",
    "end": "453060"
  },
  {
    "text": "sure our model is optimized",
    "start": "453060",
    "end": "456060"
  },
  {
    "text": "our model is a decoder model it has the",
    "start": "456060",
    "end": "458880"
  },
  {
    "text": "same architecture as gpt3 it has 15",
    "start": "458880",
    "end": "461639"
  },
  {
    "text": "billion parameters with some code",
    "start": "461639",
    "end": "463319"
  },
  {
    "text": "optimizations but we did some changes to",
    "start": "463319",
    "end": "466020"
  },
  {
    "text": "kind of address what people actually",
    "start": "466020",
    "end": "467699"
  },
  {
    "text": "want from a code generation model",
    "start": "467699",
    "end": "469979"
  },
  {
    "text": "for example we use the technique called",
    "start": "469979",
    "end": "471840"
  },
  {
    "text": "multi-query attention to make sure that",
    "start": "471840",
    "end": "474360"
  },
  {
    "text": "the memory footprint is reduced in",
    "start": "474360",
    "end": "476220"
  },
  {
    "text": "inference especially when you have large",
    "start": "476220",
    "end": "477720"
  },
  {
    "text": "batches we also used an 8000 context",
    "start": "477720",
    "end": "481440"
  },
  {
    "text": "length which is long and can allow you",
    "start": "481440",
    "end": "483419"
  },
  {
    "text": "to feed more contacts into your model",
    "start": "483419",
    "end": "485419"
  },
  {
    "text": "this is a decoder model so usually it",
    "start": "485419",
    "end": "488220"
  },
  {
    "text": "can only process context that's on the",
    "start": "488220",
    "end": "490080"
  },
  {
    "text": "left but we have used the technique",
    "start": "490080",
    "end": "491940"
  },
  {
    "text": "called fill in the middle which allows",
    "start": "491940",
    "end": "493680"
  },
  {
    "text": "your model to attend also to text that",
    "start": "493680",
    "end": "495780"
  },
  {
    "text": "is on the right for example if you have",
    "start": "495780",
    "end": "497699"
  },
  {
    "text": "some code and you want to edit it in the",
    "start": "497699",
    "end": "499500"
  },
  {
    "text": "middle you can do that with stockholder",
    "start": "499500",
    "end": "502379"
  },
  {
    "text": "and for the training setup it only took",
    "start": "502379",
    "end": "505699"
  },
  {
    "text": "512 gpus from the hugging phase cluster",
    "start": "505699",
    "end": "508440"
  },
  {
    "text": "for 24 days it was kind of a smooth",
    "start": "508440",
    "end": "510960"
  },
  {
    "text": "sailing where the last loss kept going",
    "start": "510960",
    "end": "513180"
  },
  {
    "text": "down and we've only had a few automatic",
    "start": "513180",
    "end": "515399"
  },
  {
    "text": "restarts we also used Megatron LM from",
    "start": "515399",
    "end": "517919"
  },
  {
    "text": "Nvidia for the training",
    "start": "517919",
    "end": "520380"
  },
  {
    "text": "so if these numbers scared you don't",
    "start": "520380",
    "end": "522300"
  },
  {
    "text": "worry you don't need all of that to do",
    "start": "522300",
    "end": "524099"
  },
  {
    "text": "fine tuning because with libraries like",
    "start": "524099",
    "end": "525959"
  },
  {
    "text": "pepft from hacking face that use",
    "start": "525959",
    "end": "527399"
  },
  {
    "text": "techniques like Laura you only lead",
    "start": "527399",
    "end": "529380"
  },
  {
    "text": "little computational resources to do the",
    "start": "529380",
    "end": "531720"
  },
  {
    "text": "fine tuning the idea is that if you take",
    "start": "531720",
    "end": "534420"
  },
  {
    "text": "your very large Transformer which has",
    "start": "534420",
    "end": "536339"
  },
  {
    "text": "billions of parameters you freeze all of",
    "start": "536339",
    "end": "538680"
  },
  {
    "text": "them and you only add a small number of",
    "start": "538680",
    "end": "541080"
  },
  {
    "text": "trainable parameters and you only train",
    "start": "541080",
    "end": "543060"
  },
  {
    "text": "those and you will get the performance",
    "start": "543060",
    "end": "545160"
  },
  {
    "text": "that is matching as if you were to do",
    "start": "545160",
    "end": "547019"
  },
  {
    "text": "full fine tuning this also lowers the",
    "start": "547019",
    "end": "549480"
  },
  {
    "text": "storage cost because you only need to",
    "start": "549480",
    "end": "551279"
  },
  {
    "text": "store those extra adapter weights",
    "start": "551279",
    "end": "555320"
  },
  {
    "text": "so let's see what the big code ecosystem",
    "start": "555320",
    "end": "557760"
  },
  {
    "text": "looks like it started from a small",
    "start": "557760",
    "end": "559500"
  },
  {
    "text": "family tree where we had the stack data",
    "start": "559500",
    "end": "561480"
  },
  {
    "text": "set and some starkoder model and became",
    "start": "561480",
    "end": "563580"
  },
  {
    "text": "a whole ecosystem nowadays everyone",
    "start": "563580",
    "end": "565920"
  },
  {
    "text": "almost everyone that wants to train a",
    "start": "565920",
    "end": "567839"
  },
  {
    "text": "code model they would start from the",
    "start": "567839",
    "end": "569459"
  },
  {
    "text": "stack that was the case for example for",
    "start": "569459",
    "end": "571260"
  },
  {
    "text": "stability with stable code for",
    "start": "571260",
    "end": "573240"
  },
  {
    "text": "Salesforce and the replays for example",
    "start": "573240",
    "end": "575040"
  },
  {
    "text": "there's also Community fine tunings",
    "start": "575040",
    "end": "577260"
  },
  {
    "text": "where people start from Star coder as a",
    "start": "577260",
    "end": "579360"
  },
  {
    "text": "base model and in fight unions on",
    "start": "579360",
    "end": "581160"
  },
  {
    "text": "instruction data sets for example we",
    "start": "581160",
    "end": "583200"
  },
  {
    "text": "have wizard coder from The Wizard lm10",
    "start": "583200",
    "end": "585300"
  },
  {
    "text": "and the pangu coder from Huawei team",
    "start": "585300",
    "end": "588899"
  },
  {
    "text": "okay so let's say we now have a good",
    "start": "588899",
    "end": "591899"
  },
  {
    "text": "code generation model that we trained",
    "start": "591899",
    "end": "593640"
  },
  {
    "text": "how can we turn that into an actual",
    "start": "593640",
    "end": "595620"
  },
  {
    "text": "product for example a vs code extension",
    "start": "595620",
    "end": "598080"
  },
  {
    "text": "that can serve hundreds of thousands of",
    "start": "598080",
    "end": "600060"
  },
  {
    "text": "users that's when deploying large",
    "start": "600060",
    "end": "602220"
  },
  {
    "text": "language models for cloud becomes",
    "start": "602220",
    "end": "603839"
  },
  {
    "text": "important",
    "start": "603839",
    "end": "605279"
  },
  {
    "text": "a tagging phase we have the inference",
    "start": "605279",
    "end": "607080"
  },
  {
    "text": "endpoints so if you don't want to take",
    "start": "607080",
    "end": "609360"
  },
  {
    "text": "care of the infrastructure and the",
    "start": "609360",
    "end": "611399"
  },
  {
    "text": "envelopes Behind These deployments you",
    "start": "611399",
    "end": "613620"
  },
  {
    "text": "can just use these endpoints and you can",
    "start": "613620",
    "end": "615360"
  },
  {
    "text": "query them to generate text or code",
    "start": "615360",
    "end": "617820"
  },
  {
    "text": "and what these endpoints use behind the",
    "start": "617820",
    "end": "620279"
  },
  {
    "text": "hood is text generation inference this",
    "start": "620279",
    "end": "622800"
  },
  {
    "text": "is our inference library that we use in",
    "start": "622800",
    "end": "624720"
  },
  {
    "text": "production it is available on GitHub it",
    "start": "624720",
    "end": "627000"
  },
  {
    "text": "has a lot of optimizations it also",
    "start": "627000",
    "end": "629399"
  },
  {
    "text": "serves most of the popular language",
    "start": "629399",
    "end": "630839"
  },
  {
    "text": "models",
    "start": "630839",
    "end": "632940"
  },
  {
    "text": "and what is cool about TGI is that it is",
    "start": "632940",
    "end": "635279"
  },
  {
    "text": "production ready it has a lot of metrics",
    "start": "635279",
    "end": "637500"
  },
  {
    "text": "and a good tracing mechanism so you can",
    "start": "637500",
    "end": "639720"
  },
  {
    "text": "follow your latency your throughputs the",
    "start": "639720",
    "end": "642060"
  },
  {
    "text": "request that you receive and be notified",
    "start": "642060",
    "end": "643920"
  },
  {
    "text": "when there's an anomaly it also has a",
    "start": "643920",
    "end": "646260"
  },
  {
    "text": "warm-up system we run the full pipeline",
    "start": "646260",
    "end": "648600"
  },
  {
    "text": "before you put it in production so that",
    "start": "648600",
    "end": "650459"
  },
  {
    "text": "we make sure it never crashes",
    "start": "650459",
    "end": "653160"
  },
  {
    "text": "it also have some optimizations so TGI",
    "start": "653160",
    "end": "656339"
  },
  {
    "text": "is focused on optimizing latency or some",
    "start": "656339",
    "end": "658920"
  },
  {
    "text": "other inference libraries might be",
    "start": "658920",
    "end": "660300"
  },
  {
    "text": "reporting through a post but isn't",
    "start": "660300",
    "end": "662040"
  },
  {
    "text": "necessarily what you want because what",
    "start": "662040",
    "end": "663779"
  },
  {
    "text": "you care about is serving as many users",
    "start": "663779",
    "end": "665940"
  },
  {
    "text": "as you want as fast as possible so",
    "start": "665940",
    "end": "667980"
  },
  {
    "text": "caring about the speed is important it",
    "start": "667980",
    "end": "670500"
  },
  {
    "text": "also has continuous batching for",
    "start": "670500",
    "end": "672180"
  },
  {
    "text": "handling concurrent requests meaning",
    "start": "672180",
    "end": "674040"
  },
  {
    "text": "that your batch size varies based on the",
    "start": "674040",
    "end": "676500"
  },
  {
    "text": "number of requests that you receive it",
    "start": "676500",
    "end": "678839"
  },
  {
    "text": "also has token streaming so for example",
    "start": "678839",
    "end": "681480"
  },
  {
    "text": "let's say you want your model to",
    "start": "681480",
    "end": "683100"
  },
  {
    "text": "generate 200 tokens instead of waiting",
    "start": "683100",
    "end": "686279"
  },
  {
    "text": "for the whole generation to be complete",
    "start": "686279",
    "end": "688079"
  },
  {
    "text": "and then displaying the results you can",
    "start": "688079",
    "end": "690300"
  },
  {
    "text": "display them token by token and this",
    "start": "690300",
    "end": "692760"
  },
  {
    "text": "reduces the perceived latency because",
    "start": "692760",
    "end": "694860"
  },
  {
    "text": "the reusers see the results earlier",
    "start": "694860",
    "end": "697800"
  },
  {
    "text": "it also makes your UI more interactive",
    "start": "697800",
    "end": "700860"
  },
  {
    "text": "because of the users don't like the",
    "start": "700860",
    "end": "702420"
  },
  {
    "text": "generation they can just stop it in the",
    "start": "702420",
    "end": "704399"
  },
  {
    "text": "middle and ask for a new one so these",
    "start": "704399",
    "end": "706740"
  },
  {
    "text": "are some of the optimizations that are",
    "start": "706740",
    "end": "708240"
  },
  {
    "text": "in TGI",
    "start": "708240",
    "end": "710160"
  },
  {
    "text": "some of the users are hugging chat our",
    "start": "710160",
    "end": "712800"
  },
  {
    "text": "UI for interacting with chat models like",
    "start": "712800",
    "end": "715200"
  },
  {
    "text": "Falcon chat llama 270b and the vs code",
    "start": "715200",
    "end": "718800"
  },
  {
    "text": "extension which by the way serves star",
    "start": "718800",
    "end": "721320"
  },
  {
    "text": "coder and other code models like",
    "start": "721320",
    "end": "723000"
  },
  {
    "text": "codelama which is hfl code autocomplete",
    "start": "723000",
    "end": "726000"
  },
  {
    "text": "open assistance also uses TGI as well as",
    "start": "726000",
    "end": "729360"
  },
  {
    "text": "not.dev",
    "start": "729360",
    "end": "732000"
  },
  {
    "text": "so for my last slide how can I be at",
    "start": "732000",
    "end": "734579"
  },
  {
    "text": "kubecon and Cloud native consummate and",
    "start": "734579",
    "end": "736620"
  },
  {
    "text": "not talk about kubernetes when",
    "start": "736620",
    "end": "738300"
  },
  {
    "text": "kubernetes is what powers all of the",
    "start": "738300",
    "end": "740399"
  },
  {
    "text": "hugging phase infrastructure we use it",
    "start": "740399",
    "end": "742680"
  },
  {
    "text": "for all our services The Hub the API",
    "start": "742680",
    "end": "745260"
  },
  {
    "text": "endpoints data set servers and spaces we",
    "start": "745260",
    "end": "748440"
  },
  {
    "text": "have eight production clusters in 500 in",
    "start": "748440",
    "end": "751079"
  },
  {
    "text": "the 800 nodes the Clusters are very",
    "start": "751079",
    "end": "754019"
  },
  {
    "text": "dense because for example if you want to",
    "start": "754019",
    "end": "756660"
  },
  {
    "text": "build a space which is a demo on hugging",
    "start": "756660",
    "end": "758760"
  },
  {
    "text": "face we usually give each user two CPUs",
    "start": "758760",
    "end": "760740"
  },
  {
    "text": "but these CPUs are usually most of the",
    "start": "760740",
    "end": "763079"
  },
  {
    "text": "time not used so what we want to do is",
    "start": "763079",
    "end": "765180"
  },
  {
    "text": "to be able to fit a lot of pods in one",
    "start": "765180",
    "end": "766980"
  },
  {
    "text": "cluster and we can have up to 25 250",
    "start": "766980",
    "end": "770700"
  },
  {
    "text": "pods in one node how we do that is using",
    "start": "770700",
    "end": "773700"
  },
  {
    "text": "memory swap feature where we swap the",
    "start": "773700",
    "end": "776459"
  },
  {
    "text": "disk space for the RAM and we're able to",
    "start": "776459",
    "end": "778620"
  },
  {
    "text": "fit a lot of codes in one node one other",
    "start": "778620",
    "end": "781380"
  },
  {
    "text": "thing and that's our team implemented is",
    "start": "781380",
    "end": "783720"
  },
  {
    "text": "the recompilation of container D to pull",
    "start": "783720",
    "end": "785880"
  },
  {
    "text": "images faster so for example if a user",
    "start": "785880",
    "end": "788579"
  },
  {
    "text": "wants to respond to restart their demo",
    "start": "788579",
    "end": "790620"
  },
  {
    "text": "or space they would need to pull the",
    "start": "790620",
    "end": "792360"
  },
  {
    "text": "image of the container again so who can",
    "start": "792360",
    "end": "794459"
  },
  {
    "text": "make this faster this improves the user",
    "start": "794459",
    "end": "796320"
  },
  {
    "text": "experience",
    "start": "796320",
    "end": "797279"
  },
  {
    "text": "so what the use of the our Engineers did",
    "start": "797279",
    "end": "799620"
  },
  {
    "text": "is to improve the checksum operations so",
    "start": "799620",
    "end": "801540"
  },
  {
    "text": "it is 30 faster",
    "start": "801540",
    "end": "803399"
  },
  {
    "text": "so this is an overview of how we use",
    "start": "803399",
    "end": "805680"
  },
  {
    "text": "kubernetes attacking phase",
    "start": "805680",
    "end": "808500"
  },
  {
    "text": "I hope this gave you an understanding of",
    "start": "808500",
    "end": "811620"
  },
  {
    "text": "how we're able to go from raw source",
    "start": "811620",
    "end": "814260"
  },
  {
    "text": "code that is laying there on GitHub to",
    "start": "814260",
    "end": "816720"
  },
  {
    "text": "actual products like hacking chat or HF",
    "start": "816720",
    "end": "819540"
  },
  {
    "text": "autocomplete extension and that's behind",
    "start": "819540",
    "end": "822120"
  },
  {
    "text": "the two there's what I like to describe",
    "start": "822120",
    "end": "824399"
  },
  {
    "text": "as an iceberg where we only see the tip",
    "start": "824399",
    "end": "826860"
  },
  {
    "text": "it's a lot of gpus but that's not",
    "start": "826860",
    "end": "828899"
  },
  {
    "text": "everything there's a lot that is",
    "start": "828899",
    "end": "830700"
  },
  {
    "text": "underneath the Augsburg this starts with",
    "start": "830700",
    "end": "833160"
  },
  {
    "text": "a lot of data curation you need to spend",
    "start": "833160",
    "end": "835440"
  },
  {
    "text": "time actually inspecting what's in your",
    "start": "835440",
    "end": "837540"
  },
  {
    "text": "data and trying to remove as much noise",
    "start": "837540",
    "end": "839700"
  },
  {
    "text": "as you can there's also a lot of data",
    "start": "839700",
    "end": "842100"
  },
  {
    "text": "governance and this was at the heart of",
    "start": "842100",
    "end": "843959"
  },
  {
    "text": "the big code project for example in the",
    "start": "843959",
    "end": "846240"
  },
  {
    "text": "training we only selected permissive",
    "start": "846240",
    "end": "848220"
  },
  {
    "text": "licenses and then when we released the",
    "start": "848220",
    "end": "851040"
  },
  {
    "text": "vs code extension for example we",
    "start": "851040",
    "end": "852779"
  },
  {
    "text": "implemented a code attribution tool so",
    "start": "852779",
    "end": "855120"
  },
  {
    "text": "if the model generates code that was",
    "start": "855120",
    "end": "857519"
  },
  {
    "text": "copied verbatim from the training data",
    "start": "857519",
    "end": "859500"
  },
  {
    "text": "it is highlighted in red and it spawns",
    "start": "859500",
    "end": "861660"
  },
  {
    "text": "you to the original repository in case",
    "start": "861660",
    "end": "863700"
  },
  {
    "text": "you want to give attribution to the",
    "start": "863700",
    "end": "865200"
  },
  {
    "text": "author because the permissible licenses",
    "start": "865200",
    "end": "867600"
  },
  {
    "text": "doesn't exempt you for attributing the",
    "start": "867600",
    "end": "869760"
  },
  {
    "text": "author in case the code is the same we",
    "start": "869760",
    "end": "872639"
  },
  {
    "text": "also worked on inference a lot as you've",
    "start": "872639",
    "end": "874500"
  },
  {
    "text": "seen because a lot goes behind the hood",
    "start": "874500",
    "end": "876660"
  },
  {
    "text": "so for example the vs code extension now",
    "start": "876660",
    "end": "878639"
  },
  {
    "text": "serves up to I think 12 000 users which",
    "start": "878639",
    "end": "882180"
  },
  {
    "text": "is really important as well as for the",
    "start": "882180",
    "end": "884100"
  },
  {
    "text": "end points and then there's a lot that",
    "start": "884100",
    "end": "885959"
  },
  {
    "text": "goes in evaluation because you need to",
    "start": "885959",
    "end": "888480"
  },
  {
    "text": "make sure that you evaluate your model",
    "start": "888480",
    "end": "890579"
  },
  {
    "text": "as much as possible to and have a",
    "start": "890579",
    "end": "892740"
  },
  {
    "text": "comprehensive understanding of your tool",
    "start": "892740",
    "end": "895320"
  },
  {
    "text": "so that was my talk and I hope you",
    "start": "895320",
    "end": "897420"
  },
  {
    "text": "enjoyed it thank you everyone",
    "start": "897420",
    "end": "900480"
  },
  {
    "text": "foreign",
    "start": "900480",
    "end": "902660"
  }
]