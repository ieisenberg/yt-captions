[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "hello everyone my name is gorov Kumar I work for the Uber's compute platform",
    "start": "40",
    "end": "5640"
  },
  {
    "text": "team in Amsterdam uh unfortunately my colleague Amit uh couldn't be here today due to",
    "start": "5640",
    "end": "11000"
  },
  {
    "text": "logistical reasons so I'll be leading this session solo uh in my roles and responsibilities",
    "start": "11000",
    "end": "17119"
  },
  {
    "text": "at compute platform we are mostly focused on uh running different kinds of workloads and uh in this particular talk",
    "start": "17119",
    "end": "24320"
  },
  {
    "text": "I'll be talking about uh scheduling GPU workloads on kubernetes and the kind of insights that we have had so far while",
    "start": "24320",
    "end": "30759"
  },
  {
    "text": "scheduling such workloads so yeah let's get started uh as the outline of the talk I",
    "start": "30759",
    "end": "37719"
  },
  {
    "start": "35000",
    "end": "105000"
  },
  {
    "text": "just want to clarify what this talk is not about so this talk is not about how to set up GPU workloads uh uh at the",
    "start": "37719",
    "end": "45559"
  },
  {
    "text": "beginning so basically uh I'll not talk about how to set up your clusters to run GPU workloads rather once you've set",
    "start": "45559",
    "end": "52559"
  },
  {
    "text": "your clusters up what sort of challenges and uh requirements that you face along the way I would go through a brief",
    "start": "52559",
    "end": "59280"
  },
  {
    "text": "overview of how compute works at Uber and uh I'll give an overview of kubernetes clusters for gpus at Uber uh",
    "start": "59280",
    "end": "67119"
  },
  {
    "text": "after that I'll covered the sections about efficiency and precision what are the goals and the challenges and the",
    "start": "67119",
    "end": "74119"
  },
  {
    "text": "solutions uh that we implemented to achieve achieve uh these uh efficiency",
    "start": "74119",
    "end": "79479"
  },
  {
    "text": "and precision targets I'll also cover a little bit about benefits for scheduling",
    "start": "79479",
    "end": "85400"
  },
  {
    "text": "uh efficiently and precisely on the kubernetes Clusters we'll covers some common pitfalls that we encountered",
    "start": "85400",
    "end": "91720"
  },
  {
    "text": "along the way uh to enable such GPU workloads I'll briefly touch on future",
    "start": "91720",
    "end": "97360"
  },
  {
    "text": "work that we have in the plans and if we have times uh we'll cover like a short",
    "start": "97360",
    "end": "102479"
  },
  {
    "text": "Q&A session as well this is the overview of compute at",
    "start": "102479",
    "end": "107759"
  },
  {
    "start": "105000",
    "end": "167000"
  },
  {
    "text": "Uber at the top we have two Federation layers uh we have a stateless Federation",
    "start": "107759",
    "end": "113439"
  },
  {
    "text": "layer and a badge Federation layers these Federation layers manage uh resources efficiently and uh they they",
    "start": "113439",
    "end": "120640"
  },
  {
    "text": "are responsible for different types of workloads so for stateless we have a federation layer known as up for batch",
    "start": "120640",
    "end": "127680"
  },
  {
    "text": "we have our own Federation layer known as batch federator I would recommend everyone to see the talk on batch",
    "start": "127680",
    "end": "132959"
  },
  {
    "text": "Federation on cubec con uh 2023 North America which uh in which my colleagues",
    "start": "132959",
    "end": "138200"
  },
  {
    "text": "talked about the batch Federation layer our uh compute platform has been con uh consistently evolving throughout uh we",
    "start": "138200",
    "end": "146120"
  },
  {
    "text": "were based on uh msos and our in-house scheduler known as as uh pelaton and",
    "start": "146120",
    "end": "151200"
  },
  {
    "text": "over the year we have transitioned to kubernetes below the stack we have our presence across multiple Cloud providers",
    "start": "151200",
    "end": "158519"
  },
  {
    "text": "such as oci and gcp and we have our on Prem data centers as well where we run",
    "start": "158519",
    "end": "164280"
  },
  {
    "text": "our own uh hosts the overview for GPU use uh at",
    "start": "164280",
    "end": "171239"
  },
  {
    "start": "167000",
    "end": "247000"
  },
  {
    "text": "Uber is uh like this we use uh we use gpus for model training uh for our AI ml",
    "start": "171239",
    "end": "178440"
  },
  {
    "text": "workloads we have have uh data science notebooks which are used for exploratory data science work so for example if",
    "start": "178440",
    "end": "184519"
  },
  {
    "text": "you're a machine learning scientist or a data scientist and you need like access to a GPU you can just launch a session",
    "start": "184519",
    "end": "190840"
  },
  {
    "text": "work on that session have the access to the GPU and return it back to the cluster uh some like uh use cases",
    "start": "190840",
    "end": "197519"
  },
  {
    "text": "include uh for the models that we train like ETA prediction so you'll you'll be able to see on the app like how long",
    "start": "197519",
    "end": "203480"
  },
  {
    "text": "will it take for your uh cab or food to arrive or how long will you be uh",
    "start": "203480",
    "end": "209200"
  },
  {
    "text": "reaching to your destination so we have ETA prediction models we also have llm based chatbots which will do uh",
    "start": "209200",
    "end": "215680"
  },
  {
    "text": "automated customer support and we have llm use cases for our own internal uh",
    "start": "215680",
    "end": "221400"
  },
  {
    "text": "internal users uh in terms of scale we have more than 4,000 gpus and we have",
    "start": "221400",
    "end": "227360"
  },
  {
    "text": "about 50,000 jobs which run every week on those gpus uh in terms of GPU models it's kind",
    "start": "227360",
    "end": "233079"
  },
  {
    "text": "of like a mixed bag we have Nvidia h100s Nvidia A1 100s some Quadro RTX 4000 and",
    "start": "233079",
    "end": "240120"
  },
  {
    "text": "some 1080 ti so it's uh like modern gpus as well as some Legacy gpus as",
    "start": "240120",
    "end": "247120"
  },
  {
    "start": "247000",
    "end": "293000"
  },
  {
    "text": "well so yeah this is what a cluster level overview for a GPU uh cluster",
    "start": "247120",
    "end": "253680"
  },
  {
    "text": "looks like at Uber so we have a in-house component uh known as kubernetes object",
    "start": "253680",
    "end": "259280"
  },
  {
    "text": "Pusher which is responsible for syncing the Nvidia device uh plug-in demon set spec to all the",
    "start": "259280",
    "end": "265560"
  },
  {
    "text": "Clusters uh it reads from uh uh a repository or we describe the spec for",
    "start": "265560",
    "end": "271240"
  },
  {
    "text": "the device plugin uh once that spec is sync uh the device plugin demon set",
    "start": "271240",
    "end": "276400"
  },
  {
    "text": "partt get La gets launched on the Node and we are able to advertise GPU resources to uh to uh to the to the",
    "start": "276400",
    "end": "286039"
  },
  {
    "text": "cluster uh the device plug-in and the cuet communicate via Unix socket as",
    "start": "286039",
    "end": "292360"
  },
  {
    "text": "standard on the Node level uh this is what it looks like we use the Nvidia",
    "start": "292360",
    "end": "298680"
  },
  {
    "start": "293000",
    "end": "328000"
  },
  {
    "text": "container toolkit uh which uh contains the implementation of the alternate uh container runtime",
    "start": "298680",
    "end": "306199"
  },
  {
    "text": "our our container runtime on production is mostly based out of container D so we have configured container D with uh uh",
    "start": "306199",
    "end": "313840"
  },
  {
    "text": "to be able to hook into the Nvidia container runtime whenever we receive a pod which has uh the runtime class name",
    "start": "313840",
    "end": "320680"
  },
  {
    "text": "as Nvidia if it does not uh it goes through the default run l so those are mostly stateless SP and non GPU",
    "start": "320680",
    "end": "328360"
  },
  {
    "start": "328000",
    "end": "365000"
  },
  {
    "text": "workloads we also use uh C advisor for being able to uh monitor GPU metrics uh",
    "start": "328360",
    "end": "336319"
  },
  {
    "text": "we have a rapper written on top of C advisor to be able to uh label the",
    "start": "336319",
    "end": "342680"
  },
  {
    "text": "metrics on a workload level because we want to do attribution of uh what jobs",
    "start": "342680",
    "end": "347800"
  },
  {
    "text": "are basically using what gpus so for example here there's a a here's a screenshot of a job which is using four",
    "start": "347800",
    "end": "354800"
  },
  {
    "text": "gpus and those four gpus are uh like they span across two pods and those two",
    "start": "354800",
    "end": "360800"
  },
  {
    "text": "two pods each have two gpus across different nodes yeah so now that you have set up",
    "start": "360800",
    "end": "367400"
  },
  {
    "text": "your clusters to be able to use gpus life should be fine but uh unfortunately",
    "start": "367400",
    "end": "373440"
  },
  {
    "text": "it isn't you will uh once you're able to just set up your clusters for GPU workloads you come into requirements for",
    "start": "373440",
    "end": "378840"
  },
  {
    "text": "efficiency and precision I'll get into detail for uh how that happens here so first you need to support a",
    "start": "378840",
    "end": "385919"
  },
  {
    "start": "383000",
    "end": "451000"
  },
  {
    "text": "heterogeneous cluster so a cluster can have CPU only and GPU nodes as well and",
    "start": "385919",
    "end": "392840"
  },
  {
    "text": "theoretically uh CPU workloads can run on GPU nodes it's not the best utilization of them so uh your uh your",
    "start": "392840",
    "end": "400160"
  },
  {
    "text": "cluster should be able to support heterogeneous nodes that that would be one requirement uh We've uh came into",
    "start": "400160",
    "end": "406400"
  },
  {
    "text": "this requirement uh mostly when uh our our machine learning scientists sort of",
    "start": "406400",
    "end": "412120"
  },
  {
    "text": "wanted to use Ray jobs Ray is a framework by any scale where you can launch a job which can have CPU only",
    "start": "412120",
    "end": "419720"
  },
  {
    "text": "workloads on some pods and GPU dedicated workloads on some other pods you have to be able to support multiple stock",
    "start": "419720",
    "end": "426680"
  },
  {
    "text": "keeping units for gpus so GPU workloads should be scheduled on the right skew",
    "start": "426680",
    "end": "431919"
  },
  {
    "text": "based on what the customer is asking for and the workloads which do not require a specific GPU skew should not be running",
    "start": "431919",
    "end": "438440"
  },
  {
    "text": "on those uh specialized GPU SKS you would also need to minimize the fragmentation of GPU resources to be",
    "start": "438440",
    "end": "444680"
  },
  {
    "text": "able to schedule efficiently so I'll get into details of all of these sections and the slides",
    "start": "444680",
    "end": "451759"
  },
  {
    "start": "451000",
    "end": "506000"
  },
  {
    "text": "later yeah so when you have to support heterogeneous clusters the key points here is that GPU resources are limited",
    "start": "451759",
    "end": "459319"
  },
  {
    "text": "and they are costly so you don't want to like waste uh GPU Resources by running",
    "start": "459319",
    "end": "464919"
  },
  {
    "text": "non GPU workloads on them and uh here as you can see in this uh example we have a",
    "start": "464919",
    "end": "470560"
  },
  {
    "text": "cluster where a CPU only pod is scheduled on a GPU node this tends to",
    "start": "470560",
    "end": "476520"
  },
  {
    "text": "happen if you just like let the cluster run a mark and you don't have any guard rails in place to uh avoid this so as",
    "start": "476520",
    "end": "483599"
  },
  {
    "text": "you can see a CPU pod is running on a GPU node and a general GPU pod will anyways run on a GPU node uh there's",
    "start": "483599",
    "end": "490639"
  },
  {
    "text": "also one catch that you need your device plug-in to be able to run on only GPU nodes and not CPU nodes because that's",
    "start": "490639",
    "end": "498360"
  },
  {
    "text": "not optimal also your device plugin will just keep crashing in the case uh in the in that particular case so how do you",
    "start": "498360",
    "end": "505280"
  },
  {
    "text": "solve that well there can be multiple solutions to do this uh first you can use to uh first you can node uh label",
    "start": "505280",
    "end": "513120"
  },
  {
    "start": "506000",
    "end": "605000"
  },
  {
    "text": "your gpus to be able to advertise that you have a GPU on this node and what",
    "start": "513120",
    "end": "518560"
  },
  {
    "text": "kinds of uh gpus are there uh node labeling uh in our case I mean you can",
    "start": "518560",
    "end": "524039"
  },
  {
    "text": "use different solutions as I think uh Kevin and Shiva mentioned you can use uh",
    "start": "524039",
    "end": "529920"
  },
  {
    "text": "NFD uh for us we use a custom way of uh labeling uh GPU nodes we already have an",
    "start": "529920",
    "end": "535920"
  },
  {
    "text": "on-host demon which uses Nvidia libraries to fetch the information about the gpus at boot time when we uh launch",
    "start": "535920",
    "end": "543120"
  },
  {
    "text": "the cubelet there's a wrapper which calls to that demon gets the information about the GPU nodes uh parses it and",
    "start": "543120",
    "end": "549839"
  },
  {
    "text": "labels the node uh uh once the cubet is booted once once you have labeled your",
    "start": "549839",
    "end": "555160"
  },
  {
    "text": "GPU nodes you need to add the necessary uh node selectors to the PODS of your",
    "start": "555160",
    "end": "560480"
  },
  {
    "text": "device plugin so that your device plugin only runs on those uh uh runs on those uh GPU",
    "start": "560480",
    "end": "566839"
  },
  {
    "text": "nodes and uh you need to you can use a GPU management filter",
    "start": "566839",
    "end": "572200"
  },
  {
    "text": "plugin to basically ensure resource isolation between CPU workloads and uh",
    "start": "572200",
    "end": "578399"
  },
  {
    "text": "GPU workloads just a bit of context here what do I mean when I say A GPU",
    "start": "578399",
    "end": "583560"
  },
  {
    "text": "management filter plugin so kubernetes allows you to write uh plugins for various points in the uh in the",
    "start": "583560",
    "end": "590720"
  },
  {
    "text": "scheduling cycle of a pod so when when you when I say I write a filter plugin what I mean is like a filter plugin will",
    "start": "590720",
    "end": "597360"
  },
  {
    "text": "basically uh filter out any candidate notes which are uh not suitable for this particular pods uh pod",
    "start": "597360",
    "end": "604600"
  },
  {
    "text": "scheduling so here it uh what it looks like uh when I say if we if you have to",
    "start": "604600",
    "end": "610519"
  },
  {
    "start": "605000",
    "end": "652000"
  },
  {
    "text": "implement a GPU filter plugin so if you have a pod which is not requesting gpus",
    "start": "610519",
    "end": "617600"
  },
  {
    "text": "uh the GPU filter plugin will filter out all GPU notes from its possible candidate placements and uh your uh your",
    "start": "617600",
    "end": "625040"
  },
  {
    "text": "CPU only workloads will only run on your CPU noes if you have a pod which is requesting for for gpus it will uh it",
    "start": "625040",
    "end": "631959"
  },
  {
    "text": "will be a noop and it will directly get uh scheduled on uh GPU nodes uh so the",
    "start": "631959",
    "end": "638000"
  },
  {
    "text": "GPU management filter plugin is sort of a noop for GPU resources well once you've uh ensured",
    "start": "638000",
    "end": "645839"
  },
  {
    "text": "the isolation between CPU and GPU nodes you think that uh again life should be comfortable but again unfortunately it",
    "start": "645839",
    "end": "652959"
  },
  {
    "start": "652000",
    "end": "758000"
  },
  {
    "text": "is not the reason for that is now you have another problem where you have to support multiple types of uh GPU skews",
    "start": "652959",
    "end": "659959"
  },
  {
    "text": "in the same clusters so your uh product teams would come up to you saying that now we want to train llms and llm",
    "start": "659959",
    "end": "666480"
  },
  {
    "text": "training requires specialized hardware for example Nvidia A1 100s and the best",
    "start": "666480",
    "end": "672000"
  },
  {
    "text": "use case for training those llms uh like best use case for using those uh uh",
    "start": "672000",
    "end": "677800"
  },
  {
    "text": "specialized Hardware is to train like certain GPU workloads and not let anything uh else run on that for for",
    "start": "677800",
    "end": "684800"
  },
  {
    "text": "some reason well and uh I mean you could potentially uh run uh generic GPU",
    "start": "684800",
    "end": "692560"
  },
  {
    "text": "workloads on those specialized Hardware but again it's not a very uh very optimal use case for for those uh",
    "start": "692560",
    "end": "700399"
  },
  {
    "text": "workloads also GPU features like uh differ completely on the skew types so",
    "start": "700399",
    "end": "707639"
  },
  {
    "text": "not all gpus are created equal some gpus have more vram some GPU have have",
    "start": "707639",
    "end": "712800"
  },
  {
    "text": "different kinds of compute each of the gpus cost uh a different amount so you",
    "start": "712800",
    "end": "717920"
  },
  {
    "text": "sort of have to do this C cost optimization when you're a platform and uh one more key thing was that model",
    "start": "717920",
    "end": "724320"
  },
  {
    "text": "accuracy is highly dependent on the kind of skews that you use so if you train your workloads on a certain kind of skew",
    "start": "724320",
    "end": "730240"
  },
  {
    "text": "uh we've seen in in production that uh the model accuracy varies by a few percentage",
    "start": "730240",
    "end": "735519"
  },
  {
    "text": "points so as you can see here in the cluster we have since we have ensured that CPU only workloads will run on CPU",
    "start": "735519",
    "end": "742720"
  },
  {
    "text": "nodes and GPU workloads will run on GPU nodes the CPU pods are fine but if you have a generic GPU workload it can get",
    "start": "742720",
    "end": "749959"
  },
  {
    "text": "scheduled on Specialized gpus but you have to uh like prevent that so again uh",
    "start": "749959",
    "end": "756519"
  },
  {
    "text": "how how how can you address this problem well you can again write a new filter",
    "start": "756519",
    "end": "762720"
  },
  {
    "start": "758000",
    "end": "806000"
  },
  {
    "text": "filter plug-in which is called the specialized GPU management filter plugin so plug-in one will be your uh Global",
    "start": "762720",
    "end": "769839"
  },
  {
    "text": "GPU management filter plugin it filters out all GP all GPU nodes from a pod which does not request those the plugin",
    "start": "769839",
    "end": "776800"
  },
  {
    "text": "two will be a specialized uh GPU management filter plugin which will",
    "start": "776800",
    "end": "781839"
  },
  {
    "text": "filter out all the specialized gpus from any pod which is just requesting for",
    "start": "781839",
    "end": "787320"
  },
  {
    "text": "General uh GPU resources and does not want to be placed on a specific skew and at the end like uh we have uh placement",
    "start": "787320",
    "end": "794880"
  },
  {
    "text": "strategies is based on node selectors so if your pod is specially requesting for let's say I want to run on an a100 you",
    "start": "794880",
    "end": "801120"
  },
  {
    "text": "put a node selector there and uh your pod will accordingly get",
    "start": "801120",
    "end": "806560"
  },
  {
    "start": "806000",
    "end": "840000"
  },
  {
    "text": "placed what does this do so once once you have done this you are able to isolate between CPU nodes generic GPU",
    "start": "806760",
    "end": "814279"
  },
  {
    "text": "nodes and specialized GPU nodes this ensures better Hardware utilization in our clusters and obviously once we are",
    "start": "814279",
    "end": "821760"
  },
  {
    "text": "able to schedule uh workloads on the right uh nodes we are able to achieve",
    "start": "821760",
    "end": "826800"
  },
  {
    "text": "better model accuracy this also gives us cost savings because uh obviously we are",
    "start": "826800",
    "end": "832000"
  },
  {
    "text": "able to utilize the hardware better uh the last time we did some evaluation on the kind of cost savings that it gives",
    "start": "832000",
    "end": "837279"
  },
  {
    "text": "us it was around half a million dollar a year okay now since you've done that you're",
    "start": "837279",
    "end": "843040"
  },
  {
    "start": "840000",
    "end": "974000"
  },
  {
    "text": "able to isolate resources uh you should be able to like rest and uh rest rest on",
    "start": "843040",
    "end": "849800"
  },
  {
    "text": "your laal but again you see another problem and the problem is uh based on",
    "start": "849800",
    "end": "855720"
  },
  {
    "text": "GPU fragmentation what tends to happen in our use case uh with uh scheduling",
    "start": "855720",
    "end": "860880"
  },
  {
    "text": "other kinds of workload was that we used to prefer load ofare placement when we say load ofare placement we try to place",
    "start": "860880",
    "end": "867680"
  },
  {
    "text": "the workloads on the no which are uh not being utilized uh to",
    "start": "867680",
    "end": "872920"
  },
  {
    "text": "the fullest so that we ensure like there's a uniform utilization of resources across our clusters uh this",
    "start": "872920",
    "end": "879639"
  },
  {
    "text": "works well in the uh in the CPU world but in the GPU world what that results in is fragmentation so here you can see",
    "start": "879639",
    "end": "887519"
  },
  {
    "text": "if you have a cluster uh uh we have nine gpus available to uh to schedule your",
    "start": "887519",
    "end": "894800"
  },
  {
    "text": "workloads but if you want to place a pod which requires more than 2G gpus you",
    "start": "894800",
    "end": "899839"
  },
  {
    "text": "won't be able to place uh the particular pod uh in the cluster and we've had complaints from our customers uh saying",
    "start": "899839",
    "end": "907240"
  },
  {
    "text": "that even though I have GPU resources on the cluster I I'm not able to schedule",
    "start": "907240",
    "end": "912399"
  },
  {
    "text": "uh my pods because uh of this fragmentation issue so H how do you uh",
    "start": "912399",
    "end": "918399"
  },
  {
    "text": "mitigate fragmentation well you could use something like bin packing placement so",
    "start": "918399",
    "end": "923800"
  },
  {
    "text": "kubernetes uh by default uh has a no resource fit plugin that node resource",
    "start": "923800",
    "end": "929920"
  },
  {
    "text": "fit plugin uh like you can uh alter the node resource fit plugin to use the most",
    "start": "929920",
    "end": "935079"
  },
  {
    "text": "allocated strategy so that you're able to bin pack your workloads uh closer and",
    "start": "935079",
    "end": "941399"
  },
  {
    "text": "then as you can see you have uh nodes available with more than uh 3 GPU so",
    "start": "941399",
    "end": "947480"
  },
  {
    "text": "your uh your pod can be placed on those nodes uh with that being said I not I'm",
    "start": "947480",
    "end": "954800"
  },
  {
    "text": "not saying that we have completely solved the fragmentation problem in our clusters the reason for that is it's",
    "start": "954800",
    "end": "960079"
  },
  {
    "text": "more of a art than a science because you have multiple scorers uh inside your uh",
    "start": "960079",
    "end": "965199"
  },
  {
    "text": "cluster competing for the uh placement and then you have to tune the weights of different kinds of schedule plugins that",
    "start": "965199",
    "end": "970680"
  },
  {
    "text": "you write so that uh you don't run into issues well so now you are able to",
    "start": "970680",
    "end": "977959"
  },
  {
    "text": "isolate resources you have to you're able to schedule uh workloads on the appropriate skews you're uh and you able",
    "start": "977959",
    "end": "985199"
  },
  {
    "text": "to bin pack your workloads you should be fine with most of the uh most of the",
    "start": "985199",
    "end": "991319"
  },
  {
    "text": "scenarios that you see but then like life is not again so easy you run into",
    "start": "991319",
    "end": "996560"
  },
  {
    "text": "some common pitfalls uh pitfalls is just a fancy name for bugs that we encountered along the way so yeah uh one",
    "start": "996560",
    "end": "1004319"
  },
  {
    "start": "1002000",
    "end": "1035000"
  },
  {
    "text": "of the pitfalls that we found maybe like after a week of deploying the plugins",
    "start": "1004319",
    "end": "1009440"
  },
  {
    "text": "was that uh yeah we uh the device plug-in pod and the filter plug-in do",
    "start": "1009440",
    "end": "1015720"
  },
  {
    "text": "not play very well uh and the reason for that was really simple uh I'll get into the reason first so what basically",
    "start": "1015720",
    "end": "1022079"
  },
  {
    "text": "happens is like when your node joins uh the cluster uh it it is effectively like",
    "start": "1022079",
    "end": "1027438"
  },
  {
    "text": "a CPU node you don't have uh GPU resources being advertised on your uh node and you have the uh GPU filter",
    "start": "1027439",
    "end": "1034199"
  },
  {
    "text": "plugin in place once uh the demon set spec is synced uh and Demon set part",
    "start": "1034199",
    "end": "1040558"
  },
  {
    "text": "gets scheduled on that particular node you start to advertise uh the GPU uh",
    "start": "1040559",
    "end": "1045640"
  },
  {
    "text": "resources on that particular node and all as well but what happens when that",
    "start": "1045640",
    "end": "1051039"
  },
  {
    "text": "particular uh Nvidia demon set pod for for some reason let's say crashes or",
    "start": "1051039",
    "end": "1056880"
  },
  {
    "text": "you're trying to upgrade your nvdia demon Set uh device plug-in demon set",
    "start": "1056880",
    "end": "1062080"
  },
  {
    "text": "you run into the issue of uh the the GPU filter plugin sort of interrupting uh",
    "start": "1062080",
    "end": "1067919"
  },
  {
    "text": "your Nvidia device plug-in port to be placed so basically the the GPU filter plugin was like is the uh was was making",
    "start": "1067919",
    "end": "1075080"
  },
  {
    "text": "a decision that is the Pod requesting for gpus and is is the node a GPU node",
    "start": "1075080",
    "end": "1080640"
  },
  {
    "text": "and if uh that's the case uh then I don't want to schedule uh pod on this node because device plugin essentially",
    "start": "1080640",
    "end": "1087480"
  },
  {
    "text": "is a CPU only workload well that's not ideal so for for uh just just for the",
    "start": "1087480",
    "end": "1095919"
  },
  {
    "start": "1092000",
    "end": "1113000"
  },
  {
    "text": "Simplicity of the solution you can add an exception for your device plug-in pod in your uh in your uh GPU management",
    "start": "1095919",
    "end": "1102919"
  },
  {
    "text": "filter plugin and it's just not only applies to device plugin pod it applies to all the system pods that need to run",
    "start": "1102919",
    "end": "1108480"
  },
  {
    "text": "on your GPU node but those are not CPU not GPU",
    "start": "1108480",
    "end": "1113960"
  },
  {
    "start": "1113000",
    "end": "1175000"
  },
  {
    "text": "workloads another common Pitfall that we encountered was that pods requesting for more than gpus that than uh they are",
    "start": "1113960",
    "end": "1120799"
  },
  {
    "text": "available on a node so for example we have a cluster here it's a theoretical cluster we have nodes with 4 gpus 6 gpus",
    "start": "1120799",
    "end": "1127600"
  },
  {
    "text": "and 8 gpus and if a pod tries to place a workload which contains overall 10 gpus",
    "start": "1127600",
    "end": "1134760"
  },
  {
    "text": "uh what used to happen was like the it would go through through uh it will go",
    "start": "1134760",
    "end": "1140240"
  },
  {
    "text": "through the admission control and uh we'll try to place the workload on the cluster and we'll we'll go through",
    "start": "1140240",
    "end": "1146640"
  },
  {
    "text": "multiple scheduling attempts and we will keep on facing placement failures uh",
    "start": "1146640",
    "end": "1152000"
  },
  {
    "text": "because essentially no node can accommodate this pod well and and uh if if no node can",
    "start": "1152000",
    "end": "1160559"
  },
  {
    "text": "accommodate this particular pod uh you get alerted that your job has been stuck",
    "start": "1160559",
    "end": "1165880"
  },
  {
    "text": "for a while and you you cannot run your uh run your workloads well what you could possibly",
    "start": "1165880",
    "end": "1172799"
  },
  {
    "text": "do is to prevent the admission of such pods in",
    "start": "1172799",
    "end": "1178280"
  },
  {
    "start": "1175000",
    "end": "1258000"
  },
  {
    "text": "your cluster which contain more than what uh which contain the request for",
    "start": "1178280",
    "end": "1183400"
  },
  {
    "text": "resources more than of what your particular cluster at that point time",
    "start": "1183400",
    "end": "1188559"
  },
  {
    "text": "point of time can uh provide uh this can also be solved at the Federation layer but I'm just talking about on a cluster",
    "start": "1188559",
    "end": "1194400"
  },
  {
    "text": "level so if if you try to place a pod which requests for 10 gpus and your",
    "start": "1194400",
    "end": "1200799"
  },
  {
    "text": "cluster does not have a node which can accommodate 10 gpus you just deny admission of that particular",
    "start": "1200799",
    "end": "1206880"
  },
  {
    "text": "Port well it's again not so simple because you can't just hard code saying",
    "start": "1206880",
    "end": "1212520"
  },
  {
    "text": "that okay in my particular cluster I have pod which has I have node which can support eight uh gpus so I'll just",
    "start": "1212520",
    "end": "1219919"
  },
  {
    "text": "hardcode eight and then ask uh users to send their workloads and uh uh once the",
    "start": "1219919",
    "end": "1226559"
  },
  {
    "text": "scheduling happens I'll be able to uh deny admission to uh to pods which contain more than uh eight uh gpus",
    "start": "1226559",
    "end": "1234520"
  },
  {
    "text": "because your cluster essentially is a living entity nodes keep coming in and nodes keep going out so tomorrow you can",
    "start": "1234520",
    "end": "1241440"
  },
  {
    "text": "have a node which contains 16 gpus or I don't know 10 gpus and uh or or your",
    "start": "1241440",
    "end": "1248559"
  },
  {
    "text": "eight node uh sorry 8 GPU node can move out of the cluster for some repairs or",
    "start": "1248559",
    "end": "1254559"
  },
  {
    "text": "like in maintenance so how do you uh do that well you can write an admission check",
    "start": "1254559",
    "end": "1261679"
  },
  {
    "start": "1258000",
    "end": "1325000"
  },
  {
    "text": "using a node Informer so you will watch of all like on this particular cluster I'll watch for node updates I'll keep",
    "start": "1261679",
    "end": "1268679"
  },
  {
    "text": "account of the maximum number of gpus that a node can have uh that a one node",
    "start": "1268679",
    "end": "1275080"
  },
  {
    "text": "has in this particular cluster my admission uh admission control check",
    "start": "1275080",
    "end": "1280120"
  },
  {
    "text": "will just validate that is my uh is my pod requesting for uh number of gpus",
    "start": "1280120",
    "end": "1288120"
  },
  {
    "text": "which is higher than a node can supply if yes uh then I just like deny",
    "start": "1288120",
    "end": "1294279"
  },
  {
    "text": "admission if not then I allow the partt to be admitted this is a very simplified",
    "start": "1294279",
    "end": "1300799"
  },
  {
    "text": "uh simplified solution we also need to take into account the fact that your node uh your Port spec also contains",
    "start": "1300799",
    "end": "1306880"
  },
  {
    "text": "init containers and when you have init containers uh it you need to evaluate",
    "start": "1306880",
    "end": "1312240"
  },
  {
    "text": "the maximum of an inet container as as opposed to like the sum of uh the number",
    "start": "1312240",
    "end": "1317520"
  },
  {
    "text": "of gpus that are are uh being requested by your pods requested by your",
    "start": "1317520",
    "end": "1326080"
  },
  {
    "start": "1325000",
    "end": "1379000"
  },
  {
    "text": "containers okay uh another thing that uh you would probably run into is the fact that you have uh privileged containers",
    "start": "1327039",
    "end": "1334720"
  },
  {
    "text": "running in your clusters so just for a bit of context a privilege container uh is a container which has elevated",
    "start": "1334720",
    "end": "1341200"
  },
  {
    "text": "Privileges and these are not limited by your container run time if you run privileged containers on your uh on your",
    "start": "1341200",
    "end": "1348240"
  },
  {
    "text": "cluster apart from just the security risks that are associated with uh running such containers you'll be uh",
    "start": "1348240",
    "end": "1355320"
  },
  {
    "text": "seeing that a privilege container has access to basically all the gpus on a",
    "start": "1355320",
    "end": "1360799"
  },
  {
    "text": "particular node and uh this affects resource attribution it also affects uh",
    "start": "1360799",
    "end": "1366279"
  },
  {
    "text": "your uh metrics in terms of what gpus are accessible to this particular uh",
    "start": "1366279",
    "end": "1372200"
  },
  {
    "text": "container and uh yeah you run into problems well we took a decision",
    "start": "1372200",
    "end": "1379159"
  },
  {
    "start": "1379000",
    "end": "1420000"
  },
  {
    "text": "uh to avoid s running such containers into our clusters the the necessary use",
    "start": "1379159",
    "end": "1385840"
  },
  {
    "text": "cases for privileged containers are governed by some other route of deployment rather than uh kubernetes so",
    "start": "1385840",
    "end": "1392720"
  },
  {
    "text": "we do another admission check we say that if the Pod has uh privileged capabilities for example it has capsus",
    "start": "1392720",
    "end": "1399919"
  },
  {
    "text": "admin or capsus CH rout or anything like that uh we just deny admission to it",
    "start": "1399919",
    "end": "1406320"
  },
  {
    "text": "also if the Pod security context allow ows it to its privileges to be escalated",
    "start": "1406320",
    "end": "1411480"
  },
  {
    "text": "we do not uh uh admit such pods into our clusters as well and if everything is no",
    "start": "1411480",
    "end": "1417240"
  },
  {
    "text": "then it's a no op and your pods can get placed so yeah I'll just briefly talk",
    "start": "1417240",
    "end": "1423080"
  },
  {
    "start": "1420000",
    "end": "1467000"
  },
  {
    "text": "about the future work that we plan on doing uh we plan on supporting fractional gpus so in our current setup",
    "start": "1423080",
    "end": "1429440"
  },
  {
    "text": "gpus cannot be shared by containers uh with that being said we do train",
    "start": "1429440",
    "end": "1435279"
  },
  {
    "text": "multiple models on our uh single GP use using software techniques such as uh low",
    "start": "1435279",
    "end": "1441320"
  },
  {
    "text": "rank resource attribution uh workloads uh if they are",
    "start": "1441320",
    "end": "1446600"
  },
  {
    "text": "not able to like fully utilize the GPU will essentially waste resources so we",
    "start": "1446600",
    "end": "1452559"
  },
  {
    "text": "are planning to use migs in order to uh in order to enhance the uh like",
    "start": "1452559",
    "end": "1457960"
  },
  {
    "text": "utilization of gpus in our clusters we found use cases of certain inference workloads which scale well with mix so",
    "start": "1457960",
    "end": "1464679"
  },
  {
    "text": "yeah the plan is to support this in the future we also uh plan to",
    "start": "1464679",
    "end": "1470080"
  },
  {
    "start": "1467000",
    "end": "1502000"
  },
  {
    "text": "support multiple GPU vendors like Intel and AMD the few reasons that we are",
    "start": "1470080",
    "end": "1475520"
  },
  {
    "text": "planning to do this is because of price to Performance ratios of certain workloads with these different uh GPU",
    "start": "1475520",
    "end": "1482679"
  },
  {
    "text": "providers these providers have open source drivers in stack and the device plug-in for these uh uh these uh vendors",
    "start": "1482679",
    "end": "1490919"
  },
  {
    "text": "are also open source so you can uh just check them out with that being said I don't expect it to be very smooth uh",
    "start": "1490919",
    "end": "1497960"
  },
  {
    "text": "just like like uh the experience that we've had with Nvidia gpus just a bit of a Shameless plug this",
    "start": "1497960",
    "end": "1504279"
  },
  {
    "start": "1502000",
    "end": "1513000"
  },
  {
    "text": "is the team that has worked on uh most of these features feel free to reach out to us on uh LinkedIn uh if you have any",
    "start": "1504279",
    "end": "1511679"
  },
  {
    "text": "questions or feedback and uh with that being said I'll open the floor for questions if you",
    "start": "1511679",
    "end": "1517480"
  },
  {
    "start": "1513000",
    "end": "1846000"
  },
  {
    "text": "have any and please scan the QR code if you have any uh feedback for the session",
    "start": "1517480",
    "end": "1522559"
  },
  {
    "text": "thank you",
    "start": "1522559",
    "end": "1530320"
  },
  {
    "text": "yeah you have mentioned Ray are you using Cube Ray for Ray and are using",
    "start": "1538600",
    "end": "1546480"
  },
  {
    "text": "gray serve I'm sorry uh can you repeat the question yes you have mentioned Ray at the beginning I guess that you using",
    "start": "1546480",
    "end": "1554080"
  },
  {
    "text": "the the ray framework on kubernetes Cube Ray yeah so we are using uh uh yeah Ray",
    "start": "1554080",
    "end": "1561840"
  },
  {
    "text": "workloads uh those are running on our clusters to train the machine learning",
    "start": "1561840",
    "end": "1567600"
  },
  {
    "text": "uh uh models so we uh we have a multiple uh sorts of bad jobs that run on run on",
    "start": "1567600",
    "end": "1574480"
  },
  {
    "text": "our clusters one of them is Ray which is like a framework by any scale exactly",
    "start": "1574480",
    "end": "1580360"
  },
  {
    "text": "and I think that there is an option to use functional gpus with Ray right and",
    "start": "1580360",
    "end": "1585399"
  },
  {
    "text": "are using anything other than Ray serve because currently what we are using is",
    "start": "1585399",
    "end": "1591279"
  },
  {
    "text": "mainly serving the ml models with race serve nothing else so I guess that you're using also Ray jobs yes there are",
    "start": "1591279",
    "end": "1598399"
  },
  {
    "text": "Ray jobs uh there are also spark jobs uh so like yeah it's it's it's uh multiple",
    "start": "1598399",
    "end": "1604480"
  },
  {
    "text": "sort of uh jobs that run on the batch platform it's not just uh R yeah super nice thank you so much thank",
    "start": "1604480",
    "end": "1612000"
  },
  {
    "text": "you thanks for the talk uh I just want to I have like two questions the first",
    "start": "1612000",
    "end": "1617640"
  },
  {
    "text": "one um are you separating Federation layer based on CPU and GPU workloads is that what you meant on the first slide",
    "start": "1617640",
    "end": "1624760"
  },
  {
    "text": "oh so yeah our Federation layer uh like so the the question is that are we",
    "start": "1624760",
    "end": "1630600"
  },
  {
    "text": "separating GPU and CPU clusters uh like when you said you have separate Federation layer yeah we have separate",
    "start": "1630600",
    "end": "1637720"
  },
  {
    "text": "Federation layer but it's uh mostly on job types so we have a uh Federation",
    "start": "1637720",
    "end": "1643120"
  },
  {
    "text": "layer for stateless workloads which run like microservices and uh other things",
    "start": "1643120",
    "end": "1648760"
  },
  {
    "text": "and we have a separate Federation layer for batch workloads which run these uh",
    "start": "1648760",
    "end": "1653880"
  },
  {
    "text": "like bad jobs like spark jobs Ray jobs and other machine learning models I see see for for batch workload uh how are",
    "start": "1653880",
    "end": "1660399"
  },
  {
    "text": "you guys approaching like queing uh batch workloads on the Federation layer",
    "start": "1660399",
    "end": "1666240"
  },
  {
    "text": "I see so uh that's a good question uh for for uh we already have uh badge",
    "start": "1666240",
    "end": "1673159"
  },
  {
    "text": "Federation will through something called a multicluster federator so what that fed Federation uh Federation layer does",
    "start": "1673159",
    "end": "1679399"
  },
  {
    "text": "is it admits the job from the customers which uh and then it's it tries to",
    "start": "1679399",
    "end": "1685120"
  },
  {
    "text": "distribute the uh job to the appropriate clusters depending on the you uh",
    "start": "1685120",
    "end": "1691840"
  },
  {
    "text": "resources available uh in those clusters uh in the in this sort of like like a",
    "start": "1691840",
    "end": "1697120"
  },
  {
    "text": "multicluster queue so it will redirect the job to the appropriate cluster depending on the uh amount of resources",
    "start": "1697120",
    "end": "1703799"
  },
  {
    "text": "that particular cluster has so yeah I would encourage you to watch the cbec on Chicago uh North America talk if you",
    "start": "1703799",
    "end": "1711279"
  },
  {
    "text": "want to learn more about the Federation layer for bats that we have at Uber I'll link it in the in the presentation as",
    "start": "1711279",
    "end": "1717080"
  },
  {
    "text": "well awesome thanks just just one final follow-up question sure um if you could",
    "start": "1717080",
    "end": "1722320"
  },
  {
    "text": "share more on like what is what are you guys prioritizing in terms of IQ are you",
    "start": "1722320",
    "end": "1728000"
  },
  {
    "text": "guys prioritizing Fair sharing or maximum GPU utilization rate Etc yes we",
    "start": "1728000",
    "end": "1736519"
  },
  {
    "text": "have that fair sharing model uh I can maybe uh like get back to you on the",
    "start": "1736519",
    "end": "1742120"
  },
  {
    "text": "details of how that fair sharing works on a cluster level I am mostly part of the platform team which runs uh uh like",
    "start": "1742120",
    "end": "1749880"
  },
  {
    "text": "the the workloads on kubernetes clusters that that uh Federation lay sets on top",
    "start": "1749880",
    "end": "1755000"
  },
  {
    "text": "of the platform awesome thank you thank you very much sure hi thank you so much for a",
    "start": "1755000",
    "end": "1762840"
  },
  {
    "text": "great talk um I have a question I'm just curious if you follow all the best practices and avoid those bit Falls yeah",
    "start": "1762840",
    "end": "1770679"
  },
  {
    "text": "what is the highest CPU and GPU utilization in the cluster that you've",
    "start": "1770679",
    "end": "1775880"
  },
  {
    "text": "seen oh so that's a really nice question uh when it comes to uh CPU utilization",
    "start": "1775880",
    "end": "1784399"
  },
  {
    "text": "we are highly over-provisioned we are still working on uh like uh improving",
    "start": "1784399",
    "end": "1791840"
  },
  {
    "text": "the overall uh C CPU utilization across our clusters uh but just to answer your",
    "start": "1791840",
    "end": "1799960"
  },
  {
    "text": "question uh majority of our workloads are probably like the number of uh the",
    "start": "1799960",
    "end": "1806159"
  },
  {
    "text": "peak CPU utilization that reaches in a cluster is close to like 80% so yeah you",
    "start": "1806159",
    "end": "1812600"
  },
  {
    "text": "can always you can never really reach like uh a very high CPU utilization",
    "start": "1812600",
    "end": "1818120"
  },
  {
    "text": "because uh uh then you will run into issues with the services being able to not perform as expected uh and when it",
    "start": "1818120",
    "end": "1824360"
  },
  {
    "text": "comes to the GPU utilization I think we we were like like when it comes to",
    "start": "1824360",
    "end": "1830559"
  },
  {
    "text": "utilization more than 50 to 70 but like never never Beyond 80 So yeah thank you",
    "start": "1830559",
    "end": "1837440"
  },
  {
    "text": "so much okay thank you very much for your",
    "start": "1837440",
    "end": "1844039"
  },
  {
    "text": "time guys",
    "start": "1844039",
    "end": "1848279"
  }
]