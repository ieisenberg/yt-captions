[
  {
    "text": "uh we're gonna go ahead and get started so I'm Sarah Moore and I'm Derek",
    "start": "539",
    "end": "7080"
  },
  {
    "text": "Kavanaugh uh this is our first talk at kubecon so we're super excited to be here",
    "start": "7080",
    "end": "12660"
  },
  {
    "text": "and today we are going to take you with us on a journey about default configuration in your log in stack and",
    "start": "12660",
    "end": "20039"
  },
  {
    "text": "how shipping the defaults could cause unforeseen challenges so let's start",
    "start": "20039",
    "end": "25500"
  },
  {
    "text": "with a little story the date was November 9th we received a",
    "start": "25500",
    "end": "30840"
  },
  {
    "text": "message from a fellow engineer saying um I can't seem to find my logs in grafana",
    "start": "30840",
    "end": "37140"
  },
  {
    "text": "huh that's not good so we start poking around a little bit and we confirmed the",
    "start": "37140",
    "end": "43379"
  },
  {
    "text": "problem the logs have gone missing today we're going to walk you through the case of the missing logs and how we",
    "start": "43379",
    "end": "50640"
  },
  {
    "text": "solved it before we get too far into the details of solving this problem let's go through",
    "start": "50640",
    "end": "55800"
  },
  {
    "text": "an overview of the plg stack so the plg stat consists of three main",
    "start": "55800",
    "end": "62820"
  },
  {
    "text": "components promtel for shipping logs Loki for aggregating and storing logs and grafana",
    "start": "62820",
    "end": "68640"
  },
  {
    "text": "for interfacing with logs promtel runs as a Daemon set in any",
    "start": "68640",
    "end": "74700"
  },
  {
    "text": "cluster you pull logs from by default promptel will pull pod logs that are",
    "start": "74700",
    "end": "80100"
  },
  {
    "text": "logging to standard out or standard error and ship them to Loki",
    "start": "80100",
    "end": "85580"
  },
  {
    "text": "Loki is the log aggregation component Loki exposes an API for prompt till the",
    "start": "85979",
    "end": "92220"
  },
  {
    "text": "push logs to it will ship both the raw logs and an index to a storage back end",
    "start": "92220",
    "end": "97740"
  },
  {
    "text": "and that's typically a storage bucket we'll talk more about indexes in a",
    "start": "97740",
    "end": "103140"
  },
  {
    "text": "little bit grafana is the UI interfacing with Loki",
    "start": "103140",
    "end": "108600"
  },
  {
    "text": "so we can create a Loki data store our data source in grafana in order to run queries against the logs",
    "start": "108600",
    "end": "115140"
  },
  {
    "text": "this gives us the ability to interact with the logs in a number of ways from simple Auto queries to complex",
    "start": "115140",
    "end": "121259"
  },
  {
    "text": "dashboards with integrated alerting all right so back to the case of the",
    "start": "121259",
    "end": "126960"
  },
  {
    "text": "missing logs so at this point what we've done is confirm the problem the logs are",
    "start": "126960",
    "end": "132599"
  },
  {
    "text": "indeed missing so what's next well Derek you're a good SRE",
    "start": "132599",
    "end": "138480"
  },
  {
    "text": "what would you do in this situation well Sarah I would probably query Loki to look at",
    "start": "138480",
    "end": "146280"
  },
  {
    "text": "the logs for Loki yeah did we mention we work at a company",
    "start": "146280",
    "end": "151319"
  },
  {
    "text": "called recursion so looking at the Pod logs for Loki we",
    "start": "151319",
    "end": "157020"
  },
  {
    "text": "see a few errors that look like this maximum active stream limit exceeded reduce the number of active streams or",
    "start": "157020",
    "end": "164160"
  },
  {
    "text": "contact your Loki administrator to see if the limit can be increased hey Derek do you know who the Loki",
    "start": "164160",
    "end": "170040"
  },
  {
    "text": "administrators are I thought that was you wait I thought that was you",
    "start": "170040",
    "end": "175620"
  },
  {
    "text": " I guess it's us so as talented Loki administrators we",
    "start": "175620",
    "end": "181260"
  },
  {
    "text": "decide to start by Rolling the Loki pods I mean let's just see if these errors resolve themselves",
    "start": "181260",
    "end": "188760"
  },
  {
    "text": "you can guess how that went when we continue to see these errors we do a bit",
    "start": "188760",
    "end": "193800"
  },
  {
    "text": "of searching and decide hey if we bit the maximum active streams let's just like increase that limit",
    "start": "193800",
    "end": "200300"
  },
  {
    "text": "if you don't know what streams are we'll talk about that in a minute",
    "start": "200300",
    "end": "205860"
  },
  {
    "text": "so we take the action to increase the maximum active streams and everything",
    "start": "205860",
    "end": "210900"
  },
  {
    "text": "starts working again logs are showing up in grafana developers are joyous we're not seeing this error from promptel",
    "start": "210900",
    "end": "217260"
  },
  {
    "text": "anymore okay sweet we're going to wash our hands of that and Mark this as another case closed",
    "start": "217260",
    "end": "223860"
  },
  {
    "text": "yeah just kidding it only took a few days for the problem to start happening again",
    "start": "223860",
    "end": "230040"
  },
  {
    "text": "For Us increasing the limit of active streams masked the real issue and only worked for a short amount of time",
    "start": "230040",
    "end": "236099"
  },
  {
    "text": "at this point we knew we needed to dig in and find the true root cause",
    "start": "236099",
    "end": "241500"
  },
  {
    "text": "so before we do that we need to go over some fundamentals about how promptail and Loki work we're going to cover indexes cardinality",
    "start": "241500",
    "end": "249060"
  },
  {
    "text": "active streams and chunks as they relate to the plg stack so let's start with indexes",
    "start": "249060",
    "end": "257160"
  },
  {
    "text": "so this is an example of what a log line might look like on the left we have some labels",
    "start": "257160",
    "end": "264780"
  },
  {
    "text": "and on the right we have the content of the log line unlike other logging aggregators such as",
    "start": "264780",
    "end": "270000"
  },
  {
    "text": "elasticsearch Loki does not index the entire text of the log instead Loki only indexes the",
    "start": "270000",
    "end": "276780"
  },
  {
    "text": "labels associated with the log this means smaller indexes which in turn",
    "start": "276780",
    "end": "282360"
  },
  {
    "text": "make Loki consume less memory and run more efficiently so to repeat Loki only indexes the",
    "start": "282360",
    "end": "288960"
  },
  {
    "text": "labels of the log line creating smaller memory efficient indexes that increase scalability and",
    "start": "288960",
    "end": "294360"
  },
  {
    "text": "performance so now let's define cardinality",
    "start": "294360",
    "end": "299759"
  },
  {
    "text": "by definition cardinality is measured by the number of unique elements in a group",
    "start": "299759",
    "end": "305820"
  },
  {
    "text": "here we are looking at some logs each log line is unique because the Pod label is different in each line",
    "start": "305820",
    "end": "313139"
  },
  {
    "text": "this means we have high cardinality if we were only to look at the app label",
    "start": "313139",
    "end": "319080"
  },
  {
    "text": "the cardinality would be low due to the label being less dynamic",
    "start": "319080",
    "end": "324600"
  },
  {
    "text": "so cardinality can be measured by how static or dynamic your label sets are",
    "start": "324600",
    "end": "331220"
  },
  {
    "text": "next let's take a look at active streams active active streams are log lines",
    "start": "332160",
    "end": "338400"
  },
  {
    "text": "being received by Loki with the same index so here we have a pod whose logs",
    "start": "338400",
    "end": "344160"
  },
  {
    "text": "are being sent to Loki we can see the index for each log line is the same so",
    "start": "344160",
    "end": "349440"
  },
  {
    "text": "these will create an active stream if we add a few more deployments we get a few more active streams so here",
    "start": "349440",
    "end": "356400"
  },
  {
    "text": "in this picture we have three active streams are Logs with the same",
    "start": "356400",
    "end": "362100"
  },
  {
    "text": "index being received by Loki so finally that brings us to chunks",
    "start": "362100",
    "end": "369780"
  },
  {
    "text": "so as streams of logs are being sent to Loki it will chunk up groups of logs and",
    "start": "369780",
    "end": "375000"
  },
  {
    "text": "push them to long-term storage this is often just a bucket so in this picture this group of logs",
    "start": "375000",
    "end": "381180"
  },
  {
    "text": "goes into chunk one for this index as more logs come in more chunks can be",
    "start": "381180",
    "end": "387240"
  },
  {
    "text": "stored when the active stream is closed no more chunks for this index are added to the bucket",
    "start": "387240",
    "end": "393780"
  },
  {
    "text": "additional or new index or new active streams result in new chunks being written",
    "start": "393780",
    "end": "400879"
  },
  {
    "text": "so chunks are blocks of log aggregated log data aggregated for a subset of logs",
    "start": "401280",
    "end": "407100"
  },
  {
    "text": "with matching indexes so let's get back to our case of the",
    "start": "407100",
    "end": "412380"
  },
  {
    "text": "missing logs so where were we we were seeing an error in Loki that looked like this we tried rolling the pods we tried",
    "start": "412380",
    "end": "419759"
  },
  {
    "text": "increasing the number of active streams now what so previously we were looking at the",
    "start": "419759",
    "end": "425819"
  },
  {
    "text": "first part of the log line the part that says hey you've reached your maximum stream limit",
    "start": "425819",
    "end": "431580"
  },
  {
    "text": "now that we understand some of the basics we can better understand the second part here that says reduce the",
    "start": "431580",
    "end": "437819"
  },
  {
    "text": "active streams AKA labels great so what labels do we have that are",
    "start": "437819",
    "end": "444180"
  },
  {
    "text": "causing this problem well at recursion we do Tech enabled drug discovery which means we have a",
    "start": "444180",
    "end": "450419"
  },
  {
    "text": "machine learning models looking at images of cells that come out of our lab at random times of the day so this means",
    "start": "450419",
    "end": "457199"
  },
  {
    "text": "we have very bursty patterns of usage to handle this we've built very Dynamic",
    "start": "457199",
    "end": "462300"
  },
  {
    "text": "clusters for example our machine learning clusters can scale up to thousands of nodes and tens of thousands",
    "start": "462300",
    "end": "468180"
  },
  {
    "text": "of pots so for us what this means is that if Loki tries to index our pod labels which",
    "start": "468180",
    "end": "475139"
  },
  {
    "text": "it does by default and we have tens of thousands of PODS running at any given time",
    "start": "475139",
    "end": "480960"
  },
  {
    "text": "that's going to lead to a lot of indexes and a lot of active streams this is problematic but hey we're",
    "start": "480960",
    "end": "487979"
  },
  {
    "text": "Engineers right Derek how do we fix this so let's just take a look at how we can",
    "start": "487979",
    "end": "495599"
  },
  {
    "text": "fix a simple High cardinality issue the following log line is indexing off",
    "start": "495599",
    "end": "500819"
  },
  {
    "text": "of four labels app cluster pod and file name for the most part we would consider app",
    "start": "500819",
    "end": "507300"
  },
  {
    "text": "and cluster labels fairly dynamic or failure fairly static on the other hand depending on how often",
    "start": "507300",
    "end": "513419"
  },
  {
    "text": "the PODS of this workload are created and or destroyed the Pod and file name labels can be",
    "start": "513419",
    "end": "519120"
  },
  {
    "text": "fairly dynamic in order to decrease cardinality it may",
    "start": "519120",
    "end": "524700"
  },
  {
    "text": "be a good idea to drop the Pod and file name labels so how would we go about doing that",
    "start": "524700",
    "end": "532100"
  },
  {
    "text": "this is where promptel pipelines come into play as mentioned earlier prom tells",
    "start": "532320",
    "end": "537620"
  },
  {
    "text": "responsibility is to poll logs from a cluster and ship them to Loki prior to promptel shipping logs to Loki",
    "start": "537620",
    "end": "544800"
  },
  {
    "text": "it has the ability to transform logs via Pipelines so there are four stage types that a",
    "start": "544800",
    "end": "553140"
  },
  {
    "text": "pipeline can Leverage the parsing stage type is used to parse",
    "start": "553140",
    "end": "558480"
  },
  {
    "text": "and extract data from the current log line extracted data is available for use",
    "start": "558480",
    "end": "563580"
  },
  {
    "text": "by other stages the transform stage type is used to",
    "start": "563580",
    "end": "568980"
  },
  {
    "text": "transform extracted data from the parsing stage the action stage type is used to take",
    "start": "568980",
    "end": "576420"
  },
  {
    "text": "extracted data from the previous stages and perform one or more modifications to it",
    "start": "576420",
    "end": "582839"
  },
  {
    "text": "the filtering stage type is used to run custom stages or drop log entries based",
    "start": "582839",
    "end": "588420"
  },
  {
    "text": "on a specified filter",
    "start": "588420",
    "end": "591440"
  },
  {
    "text": "so thinking about our log line we want to drop the Pod and file name labels",
    "start": "593820",
    "end": "600060"
  },
  {
    "text": "So based on what we just discussed there should be a stage type that we can leverage to drop labels",
    "start": "600060",
    "end": "605580"
  },
  {
    "text": "luckily for us there's a built-in action stage called label drop hey Sarah what do you think",
    "start": "605580",
    "end": "612420"
  },
  {
    "text": "this stage does I don't know Derek maybe it uh drops labels how did you know",
    "start": "612420",
    "end": "618959"
  },
  {
    "text": "so this stage allows us to pass a list of labels we would like to drop",
    "start": "618959",
    "end": "625040"
  },
  {
    "text": "let's take a look at some promptel config and make this happen",
    "start": "625080",
    "end": "630200"
  },
  {
    "text": "so here we are looking at scrape scrape configs block for promptel configuration",
    "start": "630720",
    "end": "636000"
  },
  {
    "text": "file if you've worked with Prometheus scrape configs this should look pretty familiar",
    "start": "636000",
    "end": "643040"
  },
  {
    "text": "this is where we can Define pipeline stages by leveraging the pipeline stages block you can see that along with",
    "start": "643560",
    "end": "650279"
  },
  {
    "text": "pipeline stages there is a job name and kubernetes sdconfigs block",
    "start": "650279",
    "end": "657500"
  },
  {
    "text": "uh so there can be multiple jobs in a scrape config performing different tasks so the job name block uniquely uniquely",
    "start": "658740",
    "end": "666720"
  },
  {
    "text": "defines each job based on the name of this job kubernetes pods we can assume it's used to scrape",
    "start": "666720",
    "end": "672720"
  },
  {
    "text": "pod logs and then the kubernetes SD config block",
    "start": "672720",
    "end": "678079"
  },
  {
    "text": "is the service Discovery Block it's just informing promptail to scrape logs from",
    "start": "678079",
    "end": "684480"
  },
  {
    "text": "pod objects so let's dive into the pipeline stages",
    "start": "684480",
    "end": "689760"
  },
  {
    "text": "block the first time in this list is a parsing type stage",
    "start": "689760",
    "end": "695339"
  },
  {
    "text": "we don't need to worry too much about this but just as if you're using the containerd runtime use CRI to properly",
    "start": "695339",
    "end": "702720"
  },
  {
    "text": "parse the log line if you're still using Docker runtime then this would be Docker instead",
    "start": "702720",
    "end": "709640"
  },
  {
    "text": "finally we can see the label drop stage this accepts a list of label keys that",
    "start": "710100",
    "end": "715320"
  },
  {
    "text": "we would like to drop from the label set before shipping them off to Loki so if",
    "start": "715320",
    "end": "721140"
  },
  {
    "text": "either label file name and or pod exist in the label set drop them",
    "start": "721140",
    "end": "728180"
  },
  {
    "text": "with our label label drop configuration in place our logs now have we'll now",
    "start": "728279",
    "end": "734100"
  },
  {
    "text": "have the Pod and file name labels dropped",
    "start": "734100",
    "end": "738019"
  },
  {
    "text": "our label set now has much lower cardinality and should provide us with better performance and resilience",
    "start": "740399",
    "end": "748279"
  },
  {
    "text": "gotten rid of the problematic labels for Loki but what if we still want to have those values in our logs so instead of",
    "start": "750420",
    "end": "756899"
  },
  {
    "text": "dropping the labels we can move the labels to the log content where it's not",
    "start": "756899",
    "end": "762360"
  },
  {
    "text": "indexed again we can leverage the promptel pipeline to accomplish this",
    "start": "762360",
    "end": "768720"
  },
  {
    "text": "so we use the built-in parser stage called replace which is used to manipulate the content of the log line",
    "start": "768720",
    "end": "775860"
  },
  {
    "text": "which is the part that's not indexed let's build on the previous configuration to make this happen",
    "start": "775860",
    "end": "784560"
  },
  {
    "text": "so here's our previous configuration we're going to inject the replace action type between the CRI and label drop box",
    "start": "784560",
    "end": "791639"
  },
  {
    "text": "blocks so the replace block accepts two arguments expression and replace the",
    "start": "791639",
    "end": "798660"
  },
  {
    "text": "expression argument allows us to use a regex to pull specific parts of the log entry we would like to manipulate so",
    "start": "798660",
    "end": "805079"
  },
  {
    "text": "here we pass like a DOT star expression wrapped in parentheses that means we're capturing the entire log line",
    "start": "805079",
    "end": "812399"
  },
  {
    "text": "the log entry is then saved as a capture Group which we can reference in the replace argument with the dot value",
    "start": "812399",
    "end": "819660"
  },
  {
    "text": "variable and the replace argument we're prefixing the prod and file name labels into the",
    "start": "819660",
    "end": "826980"
  },
  {
    "text": "log entry so let's go back to the log entry and see what effect this would",
    "start": "826980",
    "end": "832079"
  },
  {
    "text": "have so you can see that the Pod and file name",
    "start": "832079",
    "end": "840240"
  },
  {
    "text": "labels are now part of the log line and they're dropped with the label drop action type",
    "start": "840240",
    "end": "848360"
  },
  {
    "text": "so with this newfound knowledge we decided to run an experiment and gather some metrics on just how impactful these",
    "start": "851160",
    "end": "857700"
  },
  {
    "text": "configuration configuration changes can be so we deployed two instances of promptel",
    "start": "857700",
    "end": "863279"
  },
  {
    "text": "as well as two instances of Loki on one prontel instance promptel a in the",
    "start": "863279",
    "end": "868620"
  },
  {
    "text": "diagram above we used the default scrape configs while the other promptel B was",
    "start": "868620",
    "end": "875459"
  },
  {
    "text": "configured with the drop pod and file name labels we then created a deployment with 5000",
    "start": "875459",
    "end": "882240"
  },
  {
    "text": "replicas this was a simple workload that just spit out some random logs",
    "start": "882240",
    "end": "887820"
  },
  {
    "text": "so that's five thousand different pods with different pod and file name labels",
    "start": "887820",
    "end": "894540"
  },
  {
    "text": "now that we have 5000 pods running how can we analyze that data",
    "start": "894540",
    "end": "900000"
  },
  {
    "text": "so grafana offers a CLI tool called log CLI which allows us to interact with the",
    "start": "900000",
    "end": "906240"
  },
  {
    "text": "low-key API in a number of ways with this tool we can Leverage The",
    "start": "906240",
    "end": "912839"
  },
  {
    "text": "analyze labels command to audit label audit labels and stream counts",
    "start": "912839",
    "end": "919940"
  },
  {
    "text": "here's the full commands what this translates to is pull all the little labels from the past hour from Loki the",
    "start": "920399",
    "end": "928380"
  },
  {
    "text": "Loki ampoint and run an analysis",
    "start": "928380",
    "end": "932660"
  },
  {
    "text": "if we query the Loki cluster with no configuration changes you can see that the total number of streams is over five",
    "start": "934980",
    "end": "941220"
  },
  {
    "text": "thousand this is due to the Pod and file name labels being highly dynamic",
    "start": "941220",
    "end": "948320"
  },
  {
    "text": "so let's now take a look at the other Loki cluster where promptel is configured to drop the Pod and file name",
    "start": "949620",
    "end": "955860"
  },
  {
    "text": "labels keep in mind that both clusters are aggregating the exact same logs",
    "start": "955860",
    "end": "961800"
  },
  {
    "text": "as you can see by just dropping those two labels the number of active streams",
    "start": "961800",
    "end": "967199"
  },
  {
    "text": "dropped from over 5000 to just 54. that is an improvement of about 100x",
    "start": "967199",
    "end": "974579"
  },
  {
    "text": "this illustrates how quickly cardinality can get out of hand",
    "start": "974579",
    "end": "979819"
  },
  {
    "text": "now let's run some log queries and take a look at performance there so again we",
    "start": "984060",
    "end": "989279"
  },
  {
    "text": "can Leverage The Log CLI query for Loki um so both for both Loki clusters we are",
    "start": "989279",
    "end": "996899"
  },
  {
    "text": "running the same query looking at the same set of logs so first we can see the number of chunks downloaded from our",
    "start": "996899",
    "end": "1003079"
  },
  {
    "text": "data bucket of the default Loki had to download a lot more chunks than the optimize Loki",
    "start": "1003079",
    "end": "1008779"
  },
  {
    "text": "at 730 chunks versus only four we can also see how much faster the",
    "start": "1008779",
    "end": "1015680"
  },
  {
    "text": "optimized Loki is close to two seconds versus just microseconds",
    "start": "1015680",
    "end": "1021620"
  },
  {
    "text": "so big improvements here with just this one little config change dropping the the",
    "start": "1021620",
    "end": "1028600"
  },
  {
    "text": "Pod and the file name labels it's like 40 times faster and 97 percent",
    "start": "1028600",
    "end": "1035600"
  },
  {
    "text": "less data being fetched",
    "start": "1035600",
    "end": "1039520"
  },
  {
    "text": "so to close out the case of the missing logs we reduce the cardinality of our log",
    "start": "1041480",
    "end": "1048319"
  },
  {
    "text": "labels and in turn Loki was able to ingest logs again this not only closed the case of the",
    "start": "1048319",
    "end": "1054020"
  },
  {
    "text": "missing logs but as you could see by the previous numbers it increased our performance and efficiency as well",
    "start": "1054020",
    "end": "1062679"
  },
  {
    "text": "so here are some of our key takeaways first off every cluster is unique what",
    "start": "1065600",
    "end": "1072140"
  },
  {
    "text": "worked for us might not necessarily work for you we recommend running the log CLI cardinality test against your clusters",
    "start": "1072140",
    "end": "1078980"
  },
  {
    "text": "before you try to make any changes upping your max active stream count",
    "start": "1078980",
    "end": "1084320"
  },
  {
    "text": "isn't always the right solution and you might try looking at cardinality first",
    "start": "1084320",
    "end": "1089660"
  },
  {
    "text": "finally observability is the lifeblood of your systems it's your ability to diagnose and fix problems so it's worth",
    "start": "1089660",
    "end": "1096860"
  },
  {
    "text": "spending the time to fine-tune your configuration for your use case",
    "start": "1096860",
    "end": "1102799"
  },
  {
    "text": "so before we go we want to say we don't believe the defaults for the system are bad and in many ways they make a lot of",
    "start": "1102799",
    "end": "1109340"
  },
  {
    "text": "sense and honestly we want to give a shout out to the awesome developers who are building and working on and",
    "start": "1109340",
    "end": "1114919"
  },
  {
    "text": "supporting the tooling and the plg stack genuinely thank you and also we just say",
    "start": "1114919",
    "end": "1121580"
  },
  {
    "text": "how amazing this error log is um it's pretty much the best error log ever",
    "start": "1121580",
    "end": "1127400"
  },
  {
    "text": "it's not very often that we have error logs that exactly describe what the problem is and how to fix it so mad",
    "start": "1127400",
    "end": "1134179"
  },
  {
    "text": "props there um we also have an awesome team here who",
    "start": "1134179",
    "end": "1140179"
  },
  {
    "text": "came all the way from Toronto and Salt Lake City to be here for us and then stayed up really late with us last night",
    "start": "1140179",
    "end": "1146480"
  },
  {
    "text": "as we were rehearsing and practicing and they're awesome so thanks to our team as well",
    "start": "1146480",
    "end": "1152660"
  },
  {
    "text": "and thank you all for attending our talk and we hope you learned something along the way and we hope you enjoy the rest",
    "start": "1152660",
    "end": "1159320"
  },
  {
    "text": "of the conference if there's any questions feel free to ask us on the",
    "start": "1159320",
    "end": "1164480"
  },
  {
    "text": "mics or if you're more comfortable coming up after the talk you can do that as well [Applause]",
    "start": "1164480",
    "end": "1177500"
  },
  {
    "text": "yeah so there are mics on the side of the room we have this is being recorded",
    "start": "1177500",
    "end": "1182539"
  },
  {
    "text": "So if you want to come step up to the mic so that we can hear you on the recording we would love to answer questions",
    "start": "1182539",
    "end": "1189820"
  },
  {
    "text": "hi can I have a question uh how the reducing number of active streams help",
    "start": "1193340",
    "end": "1199640"
  },
  {
    "text": "you reduce the number of queried logs because number of lines I would expect",
    "start": "1199640",
    "end": "1205580"
  },
  {
    "text": "is the same even if there is another difference so it's about the chunk size then or yeah",
    "start": "1205580",
    "end": "1213380"
  },
  {
    "text": "you can explain a little bit Yeah so the question was if you decrease the active",
    "start": "1213380",
    "end": "1218720"
  },
  {
    "text": "stream counts how does that proportionally affect the chunk size and the number of chunks so when when",
    "start": "1218720",
    "end": "1225520"
  },
  {
    "text": "promptail ghost can compute the index and the stream it's creating a hash of",
    "start": "1225520",
    "end": "1232940"
  },
  {
    "text": "the Lego the labels so every different label set will create a different stream",
    "start": "1232940",
    "end": "1238880"
  },
  {
    "text": "which means it's sending to a different chunk so the actual size of the logs may be the",
    "start": "1238880",
    "end": "1246020"
  },
  {
    "text": "same but the number of chunks will drastically increase and will actually be more inefficient because ideally you",
    "start": "1246020",
    "end": "1253700"
  },
  {
    "text": "want chunk size to be full of logs before you ship them off to your bucket it's a lot more efficient for Loki to",
    "start": "1253700",
    "end": "1260419"
  },
  {
    "text": "query so smart when you have a lot of active streams you have a lot of chunks",
    "start": "1260419",
    "end": "1266299"
  },
  {
    "text": "with very little logs in them and it creates an additional overhead for the queries yeah thank you so basically if",
    "start": "1266299",
    "end": "1272539"
  },
  {
    "text": "we have uh most of the time full chunks it will not help us to reduce the query",
    "start": "1272539",
    "end": "1278539"
  },
  {
    "text": "speed it will only help us reduce the cardinality if we had full chunks of",
    "start": "1278539",
    "end": "1283880"
  },
  {
    "text": "data yeah yeah for the most part yes thank you very much yes thank you",
    "start": "1283880",
    "end": "1290200"
  },
  {
    "text": "hi um if I want to write some queries that were based on the",
    "start": "1291260",
    "end": "1297140"
  },
  {
    "text": "labels that were previously in the index that we took out how would that affect my query speed now that they're no",
    "start": "1297140",
    "end": "1304280"
  },
  {
    "text": "longer indexed so the question is um we move we move a part that's previously",
    "start": "1304280",
    "end": "1314360"
  },
  {
    "text": "been indexed into the content of the log line how does that impact performance if",
    "start": "1314360",
    "end": "1319700"
  },
  {
    "text": "we actually want to query on this piece that's been indexed",
    "start": "1319700",
    "end": "1324980"
  },
  {
    "text": "yeah so of course if your log is fully is a full",
    "start": "1324980",
    "end": "1330500"
  },
  {
    "text": "text in index it's going to be fast right but you have to think about with a",
    "start": "1330500",
    "end": "1336799"
  },
  {
    "text": "platform like elasticsearch your index sizes are going to be massive sometimes",
    "start": "1336799",
    "end": "1341960"
  },
  {
    "text": "even larger than the data sets and a lot of the times that's stored in memory so that's it's going to cost a lot of money",
    "start": "1341960",
    "end": "1348700"
  },
  {
    "text": "so when you take the label sets out of the log and say you want to query those what grafana has come up with is query",
    "start": "1348700",
    "end": "1357860"
  },
  {
    "text": "parallelization so when you query you can add to your query string",
    "start": "1357860",
    "end": "1364539"
  },
  {
    "text": "maybe the label you're looking for in your text field and in the back end uh",
    "start": "1364539",
    "end": "1372080"
  },
  {
    "text": "Loki is parallelizing those queries across multiple back-end pods to make",
    "start": "1372080",
    "end": "1377960"
  },
  {
    "text": "that performance a lot faster and you you can customize that query yourself or",
    "start": "1377960",
    "end": "1383539"
  },
  {
    "text": "that parallelization you can split it up by time so you could say for each queries you're splitting up do it by",
    "start": "1383539",
    "end": "1389539"
  },
  {
    "text": "every hour that you're querying and the nice thing is that you can just",
    "start": "1389539",
    "end": "1394940"
  },
  {
    "text": "horizontally scale to your heart's content to make your query performance very fast and then scale that back down",
    "start": "1394940",
    "end": "1401780"
  },
  {
    "text": "so instead of having all of this data in in memory as an index and wasting money",
    "start": "1401780",
    "end": "1408740"
  },
  {
    "text": "24 7 you can now say for example you could use maybe Keta autoscaler and",
    "start": "1408740",
    "end": "1414320"
  },
  {
    "text": "schedule scaling up and down starting in the morning scale up a bunch",
    "start": "1414320",
    "end": "1419960"
  },
  {
    "text": "scale down at night or you could use some like an HPA if you wanted to so it",
    "start": "1419960",
    "end": "1425600"
  },
  {
    "text": "just makes a lot more sense from a cost perspective to scale in and out and just only query or only have the resources",
    "start": "1425600",
    "end": "1433220"
  },
  {
    "text": "available on demand thank you",
    "start": "1433220",
    "end": "1439360"
  },
  {
    "text": "thanks for the presentation may ask when you start seeing this kind of issue how",
    "start": "1448280",
    "end": "1453980"
  },
  {
    "text": "many unique value account for example for the past seven days or for the past one day is that I also I want what I",
    "start": "1453980",
    "end": "1461059"
  },
  {
    "text": "would like to now is like how many resource do you use to run the Loki is that possible to share",
    "start": "1461059",
    "end": "1468320"
  },
  {
    "text": "are you it's hard it's a little hard to hear you um are you you're asking how many resources you need to run Loki yes",
    "start": "1468320",
    "end": "1476059"
  },
  {
    "text": "so how many resources you use to run lucky what is like the how many dogs you ingest per day and",
    "start": "1476059",
    "end": "1482419"
  },
  {
    "text": "when you start facing this kind of issue as you mentioned you have like how many",
    "start": "1482419",
    "end": "1487760"
  },
  {
    "text": "value different unique value count you have yeah so I think you know the the way",
    "start": "1487760",
    "end": "1495860"
  },
  {
    "text": "obviously the first thing we we deploy this uh Loki through their Helm chart",
    "start": "1495860",
    "end": "1501919"
  },
  {
    "text": "and if you're using it in production you definitely wanted to use the distributed",
    "start": "1501919",
    "end": "1507440"
  },
  {
    "text": "model which basically each component in Loki is split out into microservices",
    "start": "1507440",
    "end": "1513039"
  },
  {
    "text": "I would for me I would start with the defaults kind of like what we talked about and see",
    "start": "1513039",
    "end": "1520340"
  },
  {
    "text": "um see how things perform right are you having",
    "start": "1520340",
    "end": "1526279"
  },
  {
    "text": "like um kills with your pods um any sort of Crash Loop backoffs maybe you need to start by just adjusting and",
    "start": "1526279",
    "end": "1533419"
  },
  {
    "text": "tweaking some limits or the replica counts on the on the cluster um and then maybe go from there and like",
    "start": "1533419",
    "end": "1539779"
  },
  {
    "text": "we said using the um uh the analyze argument with log CLI is",
    "start": "1539779",
    "end": "1546980"
  },
  {
    "text": "really helpful to see if you are running into these cardinality type issues",
    "start": "1546980",
    "end": "1552039"
  },
  {
    "text": "and it it's okay if you you have a lot",
    "start": "1552039",
    "end": "1557480"
  },
  {
    "text": "of active streams as long as it's not affecting cardinality in too much of a",
    "start": "1557480",
    "end": "1563539"
  },
  {
    "text": "negative way so like I said every use case is different and you know we're more than happy to",
    "start": "1563539",
    "end": "1570679"
  },
  {
    "text": "talk more about this and your specific use case if you want to chat with us after this",
    "start": "1570679",
    "end": "1576519"
  },
  {
    "text": "So when you say defaults where are the defaults coming from",
    "start": "1577580",
    "end": "1583340"
  },
  {
    "text": "we're just talking about if you deploy the helm chart just do like a Helmand",
    "start": "1583340",
    "end": "1588860"
  },
  {
    "text": "style of Loki without any overriding values so like the the open source Helm",
    "start": "1588860",
    "end": "1596240"
  },
  {
    "text": "chart yeah do you do you attempt to monitor uh for",
    "start": "1596240",
    "end": "1603200"
  },
  {
    "text": "example from Prometheus if you're approaching some limits or do you use some kind of proactive not getting into",
    "start": "1603200",
    "end": "1610580"
  },
  {
    "text": "the scenario where you have to come back and work really fast to fix the issue",
    "start": "1610580",
    "end": "1615740"
  },
  {
    "text": "and maybe not losing logs what kind of monitoring do you or parameters do you",
    "start": "1615740",
    "end": "1620900"
  },
  {
    "text": "monitor yeah so the question was what kind of monitoring do we have in place to make sure that Loki is healthy",
    "start": "1620900",
    "end": "1627620"
  },
  {
    "text": "um they actually provide a really nice dashboard from Loki that gives you a lot",
    "start": "1627620",
    "end": "1635900"
  },
  {
    "text": "of different metrics so you should be able to find it in their repo somewhere so we just have that and it has a lot of",
    "start": "1635900",
    "end": "1644000"
  },
  {
    "text": "great information and you can definitely use grafana alerting to alert on any",
    "start": "1644000",
    "end": "1650480"
  },
  {
    "text": "metrics that you're concerned about it also does a really good job of showing you kind of how your chunk data",
    "start": "1650480",
    "end": "1657559"
  },
  {
    "text": "allocation is looking make sure you're optimized there shows query latency all of the goodies that you would expect to",
    "start": "1657559",
    "end": "1664400"
  },
  {
    "text": "be able to proactively respond to these types of issues also we have Engineers who use the",
    "start": "1664400",
    "end": "1670279"
  },
  {
    "text": "system so who needs monitoring when you they can just tell you when your stuff's broken right like they'll let us know",
    "start": "1670279",
    "end": "1676100"
  },
  {
    "text": "eventually I'm kidding um by the way",
    "start": "1676100",
    "end": "1681500"
  },
  {
    "text": "I was amazed by this presentation because well in particular because I attended also rejects uh this weekend",
    "start": "1681500",
    "end": "1689779"
  },
  {
    "text": "and there was a similar talk about basically similar issues which we",
    "start": "1689779",
    "end": "1696020"
  },
  {
    "text": "observe with Prometheus and metrics in general so the high children",
    "start": "1696020",
    "end": "1703240"
  },
  {
    "text": "then the amount of labels generated by kubernetes in general because to do its",
    "start": "1703240",
    "end": "1710059"
  },
  {
    "text": "layering model is huge and it's it's amazing to see that basically on the",
    "start": "1710059",
    "end": "1715159"
  },
  {
    "text": "same field well in a similar field of observability uh similar tools faces same issues and I guess my question is",
    "start": "1715159",
    "end": "1722779"
  },
  {
    "text": "because for this kubecon for example observability and open Telemetry are the",
    "start": "1722779",
    "end": "1728360"
  },
  {
    "text": "hot topics and we see that tools like metrics logs and",
    "start": "1728360",
    "end": "1734900"
  },
  {
    "text": "tracing trying to get as close well to",
    "start": "1734900",
    "end": "1740059"
  },
  {
    "text": "each other so are there any future developments which allow you well",
    "start": "1740059",
    "end": "1745400"
  },
  {
    "text": "at least for Prometheus and Loki to address the same problem maybe in",
    "start": "1745400",
    "end": "1750980"
  },
  {
    "text": "similar ways or maybe working together to yeah um so are you asking like how to",
    "start": "1750980",
    "end": "1756860"
  },
  {
    "text": "how to combine those two types of tools are you saying well combining those tools and basically both tools although",
    "start": "1756860",
    "end": "1764360"
  },
  {
    "text": "working in slightly different areas with facing the same issue of having too many labels sometimes and basically we just",
    "start": "1764360",
    "end": "1772700"
  },
  {
    "text": "get flooded yeah yeah you can most definitely have very similar issues with Prometheus in regards and solutions are",
    "start": "1772700",
    "end": "1779659"
  },
  {
    "text": "also kind of the same it's quite funny to see that you're also dropping yeah for sure so yeah um same same thing you",
    "start": "1779659",
    "end": "1787340"
  },
  {
    "text": "you should definitely audit your cardinality with Prometheus as well to ensure you don't run into those issues",
    "start": "1787340",
    "end": "1794080"
  },
  {
    "text": "I'd also say take a look at Tempo which is another grafana tool the integrates",
    "start": "1794080",
    "end": "1799820"
  },
  {
    "text": "distributed tracing logs and metrics all together and allows you to quickly jump between all of your metrics and logs and",
    "start": "1799820",
    "end": "1807020"
  },
  {
    "text": "traces so I think that's a pretty cool tool that we're currently investigating and actually that was one of the",
    "start": "1807020",
    "end": "1813140"
  },
  {
    "text": "questions to a current presentation because with metrics among other things I see the port because I see that this",
    "start": "1813140",
    "end": "1820279"
  },
  {
    "text": "particular port with this particular Port ID misbehaving and actually from Loki I usually expect to be able to find",
    "start": "1820279",
    "end": "1827539"
  },
  {
    "text": "that particular port with that ID envelopes and actually Trace what's happening so the fact that you kind of",
    "start": "1827539",
    "end": "1835059"
  },
  {
    "text": "put it aside was like why why would you do it I mean file name is okay but for",
    "start": "1835059",
    "end": "1842059"
  },
  {
    "text": "that is so yeah it's quite essential I think for us like it ends up being that",
    "start": "1842059",
    "end": "1847100"
  },
  {
    "text": "we don't really care that much about the Pod label because the more interesting thing in our queries is is the app label",
    "start": "1847100",
    "end": "1854960"
  },
  {
    "text": "I'll go back until we find something so for us getting rid of this pod ID like",
    "start": "1854960",
    "end": "1860960"
  },
  {
    "text": "it doesn't really harm us much like we don't find that our users have dashboards built off of this",
    "start": "1860960",
    "end": "1866539"
  },
  {
    "text": "um it's it's I would say rather uninteresting in our use case yeah",
    "start": "1866539",
    "end": "1872000"
  },
  {
    "text": "so yeah and so I think when we made this change we wanted to make sure we were",
    "start": "1872000",
    "end": "1877700"
  },
  {
    "text": "doing it with as low friction as possible for our Engineers so if there are one or two Engineers that actually cared about that pod label or the Pod ID",
    "start": "1877700",
    "end": "1885260"
  },
  {
    "text": "then it was there for them if they needed it so",
    "start": "1885260",
    "end": "1889840"
  },
  {
    "text": "um I think we're going to wrap up but thank you so much again [Applause]",
    "start": "1891080",
    "end": "1900059"
  }
]