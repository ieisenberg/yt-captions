[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "text": "all right so let's start so we are in the sick API",
    "start": "240",
    "end": "5359"
  },
  {
    "text": "Machinery talk today and um we have two speakers it's Abu who's here and um Mike",
    "start": "5359",
    "end": "12480"
  },
  {
    "text": "spryer who prepared a video which Abu will play and it's about priority and",
    "start": "12480",
    "end": "17960"
  },
  {
    "text": "fairness and for those of you who have no idea what this means a few years ago",
    "start": "17960",
    "end": "23640"
  },
  {
    "text": "when we didn't have that one controller could basically kill the API server by",
    "start": "23640",
    "end": "29320"
  },
  {
    "text": "sending two any requests and basically we depended on client side throttling to be configured correctly and we are in a",
    "start": "29320",
    "end": "36520"
  },
  {
    "text": "different world today 2024 and the work for that is basically in PRI and fairness and this feature on the C",
    "start": "36520",
    "end": "43079"
  },
  {
    "text": "server and Abu and Mike are the ones who have basically pushed that and imple implemented that so let's welcome",
    "start": "43079",
    "end": "50719"
  },
  {
    "text": "Abu thank you Stefan um hello everyone welcome to the IPA machinary seek dip",
    "start": "50719",
    "end": "57000"
  },
  {
    "text": "dive um so um my Curr beh in person um we have a pre-recorded",
    "start": "57000",
    "end": "62920"
  },
  {
    "text": "video from him um so I'll start uh by playing his",
    "start": "62920",
    "end": "68880"
  },
  {
    "start": "70000",
    "end": "370000"
  },
  {
    "text": "video hi I'm Mike spritzer here to give you a brief overview of the API priority",
    "start": "70119",
    "end": "75720"
  },
  {
    "text": "and fairness feature in kubernetes this is a feature in the C API server and in",
    "start": "75720",
    "end": "80920"
  },
  {
    "text": "the generic API server library that we use to build similar API servers this",
    "start": "80920",
    "end": "86320"
  },
  {
    "text": "feature regulates the load of API server in terms of the number of requests that is actively serving at a given",
    "start": "86320",
    "end": "93040"
  },
  {
    "text": "moment the purpose of this feature is to protect the API server from the clients and to protect the clients from each",
    "start": "93040",
    "end": "99680"
  },
  {
    "text": "other this feature is based on the on attributes of a request that participate",
    "start": "99680",
    "end": "105600"
  },
  {
    "text": "in authentication and access control so that it cannot be easily fooled this feature thus is taking into",
    "start": "105600",
    "end": "113680"
  },
  {
    "text": "account both the request rate and the time it takes to serve each request because the product of these two is on",
    "start": "113680",
    "end": "120000"
  },
  {
    "text": "average the number of concurrent requests this is one-dimensional regulation this is an approximate",
    "start": "120000",
    "end": "126719"
  },
  {
    "text": "technique it's a good approximation and there are a few TW tweaks to make it better which you'll see later this",
    "start": "126719",
    "end": "133239"
  },
  {
    "text": "feature replaces the Max and flight filter which is an earlier filter that is simpler it classifies each request",
    "start": "133239",
    "end": "140000"
  },
  {
    "text": "into one of just two categories mutating or read only and treats all request of a given category of the same APF is more",
    "start": "140000",
    "end": "147840"
  },
  {
    "text": "granular it's also configur and it introduces queuing and APF",
    "start": "147840",
    "end": "153319"
  },
  {
    "text": "introduces some fairness between clients like the Max and flight filter APF uh",
    "start": "153319",
    "end": "159560"
  },
  {
    "text": "rejects request using in the standard HTTP way which is the status code",
    "start": "159560",
    "end": "165680"
  },
  {
    "text": "429 let's look at where the APF feature fits into the API server Stephan",
    "start": "166200",
    "end": "172400"
  },
  {
    "text": "shimansky gave a good talk a few years ago at cpcom about the overall structure of the coup API server here we're just",
    "start": "172400",
    "end": "178800"
  },
  {
    "text": "going to think about the hand chain of handlers uh so-called filters that do",
    "start": "178800",
    "end": "185000"
  },
  {
    "text": "some general purpose processing of each request on its way into the cirle the APF filter starts by doing",
    "start": "185000",
    "end": "192280"
  },
  {
    "text": "classification for each request this determines the priority level that the request belongs to and puts the request",
    "start": "192280",
    "end": "199519"
  },
  {
    "text": "into one of the cues at that priority level each priority level has a certain number of cues that it is configured",
    "start": "199519",
    "end": "206040"
  },
  {
    "text": "with and has a dispatcher whose job is to take requests from the right Queue at the right time and send them on for",
    "start": "206040",
    "end": "211760"
  },
  {
    "text": "further processing in a way it keeps the server as busy as requested and allowed",
    "start": "211760",
    "end": "217040"
  },
  {
    "text": "but no more so and do so with some fairness let's look at request",
    "start": "217040",
    "end": "224200"
  },
  {
    "text": "classification this starts with configured objects called flow schemas",
    "start": "224200",
    "end": "229640"
  },
  {
    "text": "that say how requests are to be classified each flow schema has a name and a numeric matching precedence which",
    "start": "229640",
    "end": "236200"
  },
  {
    "text": "is used to put the flow schemas in an ordered list classifying request consists of starts",
    "start": "236200",
    "end": "243680"
  },
  {
    "text": "with uh comparing each request to the flow schemas in order to find the first",
    "start": "243680",
    "end": "250239"
  },
  {
    "text": "one that matches each flow schema is configured with some matching rules that that say which request matching the",
    "start": "250239",
    "end": "257639"
  },
  {
    "text": "result of this is two things one is it says which priority the the flow the",
    "start": "257639",
    "end": "263479"
  },
  {
    "text": "request belongs to and the other is it classifies the request into a flow in",
    "start": "263479",
    "end": "269240"
  },
  {
    "text": "other words for each priority level the requests are classified into flows a",
    "start": "269240",
    "end": "274360"
  },
  {
    "text": "flow is identified by a pair of strings the first string is the name of the flow schem that the request matched the",
    "start": "274360",
    "end": "281160"
  },
  {
    "text": "second string is extracted from the request according to a a rule which is a",
    "start": "281160",
    "end": "286639"
  },
  {
    "text": "choice that the flow schema is configured to make a flow schema can make one of three choices one is the",
    "start": "286639",
    "end": "293199"
  },
  {
    "text": "second string in the flow identifier can be the name space of the object that the request will act on another choice is",
    "start": "293199",
    "end": "301000"
  },
  {
    "text": "that the second string will be the name of the user that issu request the third choice is that the second string will be",
    "start": "301000",
    "end": "307880"
  },
  {
    "text": "the empty string the flow identifier is input to shuffle sh which is used to put",
    "start": "307880",
    "end": "314240"
  },
  {
    "text": "the request into one of the cues in the priority level a priority level is configured with a fixed number of cues",
    "start": "314240",
    "end": "320880"
  },
  {
    "text": "and a so-called hand size which is the size of the subset of the cues that will be considered for a given request",
    "start": "320880",
    "end": "327720"
  },
  {
    "text": "Shuffle sharting will use the identifier as a source of random of entropy to make",
    "start": "327720",
    "end": "333039"
  },
  {
    "text": "a pseudo random choice to pick that subset of cues the request then gets put",
    "start": "333039",
    "end": "339080"
  },
  {
    "text": "in the Q in that subset that has the shortest length so you see there is a",
    "start": "339080",
    "end": "345120"
  },
  {
    "text": "distinction here between the number of cues and the number of flows the number of flows is not so well controlled it is",
    "start": "345120",
    "end": "352360"
  },
  {
    "text": "dynamic and it can be quite large impractically large to service the number of cues Shuffle shorting is a",
    "start": "352360",
    "end": "359080"
  },
  {
    "text": "neat technique technique for mapping a large number of flows onto a small number of cues in such a way that any",
    "start": "359080",
    "end": "365080"
  },
  {
    "text": "one or few big flows did not crowd out the other",
    "start": "365080",
    "end": "370400"
  },
  {
    "start": "370000",
    "end": "483000"
  },
  {
    "text": "flows now let's look at dispatching each priority level has a current concurrency",
    "start": "370720",
    "end": "376520"
  },
  {
    "text": "limit that is roughly the maximum number of requests that the server can actually be working on at a given time of that",
    "start": "376520",
    "end": "382680"
  },
  {
    "text": "priority level these are based on nominal concurrency limits the nominal",
    "start": "382680",
    "end": "387880"
  },
  {
    "text": "concurrency limits are derived from server capacity and configured concurrency shares each priority level",
    "start": "387880",
    "end": "394599"
  },
  {
    "text": "is configured with a number of nominal concurrency shares the servers capacity",
    "start": "394599",
    "end": "400120"
  },
  {
    "text": "is divided amongst the priority levels in proportion to their nominal concurrency shares these nominal",
    "start": "400120",
    "end": "406639"
  },
  {
    "text": "concurrency limits are a baseline that is then tweaked periodically by borrowing to produce the current",
    "start": "406639",
    "end": "412400"
  },
  {
    "text": "concurrency limit the purpose of borrowing is to allow a relatively lightly loaded uh priority loev a level",
    "start": "412400",
    "end": "420000"
  },
  {
    "text": "that is lightly loaded at the moment to be able to lend some of its concurrency to other priority levels that are",
    "start": "420000",
    "end": "425199"
  },
  {
    "text": "heavily loaded at the moment each priority level is configured with a number of cues and there is a",
    "start": "425199",
    "end": "431520"
  },
  {
    "text": "dispatching algorithm that is inspired by the fair queuing technique from networking the uh detail we had to adapt",
    "start": "431520",
    "end": "438759"
  },
  {
    "text": "that a bit to our use here the details are in the cap and this this algorithm is the thing",
    "start": "438759",
    "end": "445560"
  },
  {
    "text": "that takes requests from the quees and chooses which CU to take request from and and send it on to further",
    "start": "445560",
    "end": "451759"
  },
  {
    "text": "processing a priority level can be configured to not q and instead just",
    "start": "451759",
    "end": "456879"
  },
  {
    "text": "reject exess requests like the Max and flight filter did also there are limits on queuing each priority level is",
    "start": "456879",
    "end": "464280"
  },
  {
    "text": "configured with a limit on the length of its cues and it's also there's also a limit on the time that a request can",
    "start": "464280",
    "end": "470879"
  },
  {
    "text": "spend waiting in a queue so a request can be rejected due to either of those limits as well finally there is one um",
    "start": "470879",
    "end": "479599"
  },
  {
    "text": "priority level that is exempt from regulation as mentioned earlier there",
    "start": "479599",
    "end": "486080"
  },
  {
    "start": "483000",
    "end": "643000"
  },
  {
    "text": "are some additional considerations one is that APF will consider some requests to occupy more than one seat that is to",
    "start": "486080",
    "end": "493080"
  },
  {
    "text": "say be relatively expensive or heavy weight the leading example of this is a",
    "start": "493080",
    "end": "498879"
  },
  {
    "text": "list request that returns a large number of objects such a request compared to others that take the same amount of time",
    "start": "498879",
    "end": "505360"
  },
  {
    "text": "to execute is exceptionally expensive to handle and so we APF will consider such",
    "start": "505360",
    "end": "511919"
  },
  {
    "text": "a request to uh be worth uh more than one",
    "start": "511919",
    "end": "517159"
  },
  {
    "text": "request the next consideration is watch requests whereas the Max and flight",
    "start": "517159",
    "end": "522440"
  },
  {
    "text": "filter simply did not regulate them APF does regulate them a watch request is a",
    "start": "522440",
    "end": "528000"
  },
  {
    "text": "request to do one or two things the main focus of the watch request is to keep the client appraised of changes to a",
    "start": "528000",
    "end": "535279"
  },
  {
    "text": "collection of objects on an ongoing basis over some period of time",
    "start": "535279",
    "end": "540399"
  },
  {
    "text": "some watch requests additionally will start by informing the client of all the pre-existing objects this first phase",
    "start": "540399",
    "end": "548399"
  },
  {
    "text": "which is or is not there depending on details of the request is much like the ordinary shortlived transaction requests",
    "start": "548399",
    "end": "555720"
  },
  {
    "text": "and APF will manage the pH that first phase as such and once that phase is over APF",
    "start": "555720",
    "end": "562800"
  },
  {
    "text": "considers the request to be done even though in fact it continues on to notify the client of ongoing changes",
    "start": "562800",
    "end": "570040"
  },
  {
    "text": "the costs for those notifications then uh APF Associates with the other",
    "start": "570040",
    "end": "575360"
  },
  {
    "text": "requests that are actually making the changes so for request that writes or mutates an object the even though the um",
    "start": "575360",
    "end": "584279"
  },
  {
    "text": "reply goes back to the requester in uh promptly without waiting APF will",
    "start": "584279",
    "end": "591079"
  },
  {
    "text": "consider the request to execute for a while longer to account for the cost of sending those watch notifications out to",
    "start": "591079",
    "end": "597360"
  },
  {
    "text": "the watching clients another additional consideration is for",
    "start": "597360",
    "end": "602519"
  },
  {
    "text": "the exempt priority level that has a nominal concurrency limit and participates in borrowing this can be",
    "start": "602519",
    "end": "608760"
  },
  {
    "text": "used to somewhat even out its effects on the other priority levels finally uh the last one I want to",
    "start": "608760",
    "end": "616880"
  },
  {
    "text": "mention is rejections the 429 status code one of its standard features in HTTP is a",
    "start": "616880",
    "end": "624519"
  },
  {
    "text": "recommended time for the client to wait before trying again for a long time kubernetes always set that to 1 second",
    "start": "624519",
    "end": "631040"
  },
  {
    "text": "recently we've made that adaptive so the clients can do a better job of backing off so that completes the brief overview",
    "start": "631040",
    "end": "638600"
  },
  {
    "text": "and now Abu will take over and give you some more interesting details all right let me just quickly um",
    "start": "638600",
    "end": "646160"
  },
  {
    "start": "643000",
    "end": "760000"
  },
  {
    "text": "skip through the slides M just covered",
    "start": "646160",
    "end": "652519"
  },
  {
    "text": "okay all right so let's do a quick recap on how APF handles a request",
    "start": "652959",
    "end": "660120"
  },
  {
    "text": "um so a new request arrives we find a matching flow schema and we compute the",
    "start": "660120",
    "end": "665680"
  },
  {
    "text": "flow of the request um the request is enced using Shuffle sharting and then we",
    "start": "665680",
    "end": "671959"
  },
  {
    "text": "ask the scheduler to dispatch so the schedular uh dispatches the request that",
    "start": "671959",
    "end": "678120"
  },
  {
    "text": "should be executed next uh using the fair queuing uh technique at this moment",
    "start": "678120",
    "end": "684120"
  },
  {
    "text": "the scheduler will also dispatch as many requests as possible um",
    "start": "684120",
    "end": "689959"
  },
  {
    "text": "and then the request basically um waits in the queue for a decision right if the",
    "start": "689959",
    "end": "696200"
  },
  {
    "text": "decision is accept um the request will get executed",
    "start": "696200",
    "end": "701920"
  },
  {
    "text": "um if the Q wait time threshold exceeds and the scheduler cannot accommodate the",
    "start": "701920",
    "end": "707639"
  },
  {
    "text": "request right this will be the request will be removed from the queue and then rejected right so that's kind of the of",
    "start": "707639",
    "end": "714200"
  },
  {
    "text": "like how APF handles a request um next slide so um APF is highly configurable",
    "start": "714200",
    "end": "724079"
  },
  {
    "text": "uh via API objects it ships with a set of flow schema and priority level",
    "start": "724079",
    "end": "729240"
  },
  {
    "text": "configuration objects we refer to them as bootstrap configuration uh the left",
    "start": "729240",
    "end": "734360"
  },
  {
    "text": "column shows the um flow schema objects available um in order of their matching",
    "start": "734360",
    "end": "740760"
  },
  {
    "text": "precedence um on the right we have the available priority level configuration objects um a flow schema object is",
    "start": "740760",
    "end": "748720"
  },
  {
    "text": "assigned to exactly one priority level on the other hand a priority level can be shared by",
    "start": "748720",
    "end": "756040"
  },
  {
    "text": "multiple flow schemas okay the next slide um so let's go through some of the",
    "start": "756040",
    "end": "763360"
  },
  {
    "start": "760000",
    "end": "1104000"
  },
  {
    "text": "um bootstrap flow schema objects and uh why they're there um so exempt this Floy",
    "start": "763360",
    "end": "771040"
  },
  {
    "text": "schema matches all requests belonging to the system Masters group uh that means",
    "start": "771040",
    "end": "776800"
  },
  {
    "text": "if you're are a cluster admin you fall into that category and these requests are always exempt from FF",
    "start": "776800",
    "end": "783480"
  },
  {
    "text": "regulations uh next we have system leader election it matches the leader election requests from CP controller",
    "start": "783480",
    "end": "791199"
  },
  {
    "text": "manager and CP scheduler right this traffic is critical for cluster",
    "start": "791199",
    "end": "796639"
  },
  {
    "text": "availability right uh then we have endpoint controller it matches the requests coming from the endpoint",
    "start": "796639",
    "end": "802959"
  },
  {
    "text": "controller uh that manages the endpoint objects um I think this is critical to",
    "start": "802959",
    "end": "808600"
  },
  {
    "text": "keep the service network um functional um then we have system node high this",
    "start": "808600",
    "end": "816360"
  },
  {
    "text": "flow schema matches the hard bits from the uh from the nodes I mean cuet uh this we require",
    "start": "816360",
    "end": "824160"
  },
  {
    "text": "we need this for system self-maintenance right and then we have CP controller manager CP",
    "start": "824160",
    "end": "830399"
  },
  {
    "text": "scheduler um these matches the TR the leftover traffic from their respective",
    "start": "830399",
    "end": "837800"
  },
  {
    "text": "components right uh we have service accounts it matches any leftover in",
    "start": "837800",
    "end": "843480"
  },
  {
    "text": "cluster traffic so if you have a workload that's running as a pod most",
    "start": "843480",
    "end": "848519"
  },
  {
    "text": "likely the the requests coming from your workload will match this flow schema um",
    "start": "848519",
    "end": "854759"
  },
  {
    "text": "Global default matches any leftover traffic right including uh any",
    "start": "854759",
    "end": "860560"
  },
  {
    "text": "unauthenticated traffic or any traffic that is external to the cluster right catch all it serves as a catch all for",
    "start": "860560",
    "end": "868839"
  },
  {
    "text": "any unmatched traffic um under normal conditions no requests would match this",
    "start": "868839",
    "end": "874360"
  },
  {
    "text": "flowy schema because um the flow schemas that are that are above like act as a",
    "start": "874360",
    "end": "880720"
  },
  {
    "text": "net to match like all requests okay so we talked a bit about",
    "start": "880720",
    "end": "887079"
  },
  {
    "text": "oh sorry um so um during startup the KU server uh ensures that all bootstrap",
    "start": "887079",
    "end": "895040"
  },
  {
    "text": "configuration objects exist on the cluster right it also periodic Ally scans these objects uh and applies any",
    "start": "895040",
    "end": "902480"
  },
  {
    "text": "update necessary right that means any changes to the spec of a configuration",
    "start": "902480",
    "end": "908759"
  },
  {
    "text": "object will be stomped by the API server um for the changes to stick the cluster",
    "start": "908759",
    "end": "914680"
  },
  {
    "text": "operators must set the auto update spec annotations annotation to fals um the",
    "start": "914680",
    "end": "922000"
  },
  {
    "text": "goal is to enable the kuay server um to update these bootst step objects",
    "start": "922000",
    "end": "928079"
  },
  {
    "text": "installed by the prev releases um also at the same time not overriding the",
    "start": "928079",
    "end": "933480"
  },
  {
    "text": "changes made by the cluster operators um all right so cluster operators can add their",
    "start": "933480",
    "end": "941279"
  },
  {
    "text": "own configurations if needed for example um if you have a buggy workload uh that",
    "start": "941279",
    "end": "946519"
  },
  {
    "text": "is known to run AOK and flood the API server uh you can actually Define uh a",
    "start": "946519",
    "end": "951600"
  },
  {
    "text": "dedicated flow schema that matches the requests coming from your workload and then assign it to a prior level with",
    "start": "951600",
    "end": "958560"
  },
  {
    "text": "like your small concurrency shaper right um all right so we talked bit about the",
    "start": "958560",
    "end": "966240"
  },
  {
    "text": "flow schemas now let's switch to um priority levels so we'll start with server concurrency limit uh it is the",
    "start": "966240",
    "end": "973360"
  },
  {
    "text": "maximum number of seats uh the inflight requests can occupy at any moment on the",
    "start": "973360",
    "end": "979240"
  },
  {
    "text": "server right so APF uses it as a fence for protection so it'll try to prevent",
    "start": "979240",
    "end": "986560"
  },
  {
    "text": "the load on the API server going beyond that fence and Mike just mentioned it's",
    "start": "986560",
    "end": "991600"
  },
  {
    "text": "the kind of approximation right so um server congren limit is uh calculated",
    "start": "991600",
    "end": "998399"
  },
  {
    "text": "by summing these two server run options uh if not modified by the cluster",
    "start": "998399",
    "end": "1004199"
  },
  {
    "text": "operator the default server concurrency limit is 600 um on any kbpa server",
    "start": "1004199",
    "end": "1012240"
  },
  {
    "text": "instance it's also worth mentioning that a request on the server can occupy one",
    "start": "1012399",
    "end": "1018920"
  },
  {
    "text": "or more seats um it gives us a granular mechanism to deal with um requests that",
    "start": "1018920",
    "end": "1025918"
  },
  {
    "text": "have variable costs for example Mike mentioned getting one object versus um",
    "start": "1025919",
    "end": "1031640"
  },
  {
    "text": "getting a list of hundreds of uh objects right",
    "start": "1031640",
    "end": "1038038"
  },
  {
    "text": "um so each priority level has a property called nominal concurrency limit um that",
    "start": "1038319",
    "end": "1046438"
  },
  {
    "text": "is basically the number of execution seats available to it right so uh let's",
    "start": "1046439",
    "end": "1052320"
  },
  {
    "text": "see how we actually um distribute the server concurrency limit among these different priority levels so each",
    "start": "1052320",
    "end": "1060360"
  },
  {
    "text": "priority level API object has um a field called nominal concurrency shares it's",
    "start": "1060360",
    "end": "1067559"
  },
  {
    "text": "it's basically prescribes a fraction of the server concurrency limit uh that is",
    "start": "1067559",
    "end": "1072640"
  },
  {
    "text": "available to that level uh we use it to compute the nominal concurrency limit of",
    "start": "1072640",
    "end": "1078480"
  },
  {
    "text": "that level level um and more and and higher value of nominal concurrency",
    "start": "1078480",
    "end": "1083840"
  },
  {
    "text": "shares basically means more um nominal concurrency limit to that level uh and",
    "start": "1083840",
    "end": "1090280"
  },
  {
    "text": "it at the expans of every other priority level um the pie chart here shows uh the",
    "start": "1090280",
    "end": "1096679"
  },
  {
    "text": "default distribution of the server concurrency limit um on a vanilla",
    "start": "1096679",
    "end": "1102200"
  },
  {
    "text": "cluster okay so next slide okay so uh each priority level like and en forces",
    "start": "1102200",
    "end": "1109400"
  },
  {
    "start": "1104000",
    "end": "1224000"
  },
  {
    "text": "its concurrency limit uh independently of the other other levels which",
    "start": "1109400",
    "end": "1114520"
  },
  {
    "text": "introduces like this utilization challenge right so if we um look at the",
    "start": "1114520",
    "end": "1120120"
  },
  {
    "text": "figure um priority level B uh is underutilized it is um running well",
    "start": "1120120",
    "end": "1128120"
  },
  {
    "text": "below its concurrency limit right on the other hand priority level a is saturated",
    "start": "1128120",
    "end": "1134559"
  },
  {
    "text": "right if you look at the YOLO line uh it shows a moment where the number of",
    "start": "1134559",
    "end": "1139640"
  },
  {
    "text": "executing requests for priority level a has reached the concurrency limit and",
    "start": "1139640",
    "end": "1145960"
  },
  {
    "text": "there are an access number of requests waiting in the queue so what happens to those access requests for a right they",
    "start": "1145960",
    "end": "1152880"
  },
  {
    "text": "will either wait longer in the queue before served and this will introduce additional latency in the response time",
    "start": "1152880",
    "end": "1159200"
  },
  {
    "text": "right or uh in the worst case they will be rejected by APF right um so how do we",
    "start": "1159200",
    "end": "1165120"
  },
  {
    "text": "solve this problem um if we allow a to borrow from from B some stats right then",
    "start": "1165120",
    "end": "1171240"
  },
  {
    "text": "uh we can help resolve the situation so APF um basically allows um borrowing",
    "start": "1171240",
    "end": "1177600"
  },
  {
    "text": "among priority levels um there are two fields that basically prescribe how many",
    "start": "1177600",
    "end": "1184039"
  },
  {
    "text": "seats a priority level can lend to or borrow from other levels uh so lendable",
    "start": "1184039",
    "end": "1189600"
  },
  {
    "text": "percent is the fraction of the nominal concurrency limit of of a priority level",
    "start": "1189600",
    "end": "1195159"
  },
  {
    "text": "that other levels can borrow from it um borrow in limit percent is basically uh",
    "start": "1195159",
    "end": "1200679"
  },
  {
    "text": "the limit on how many seats a a priority level can borrow from other levels right",
    "start": "1200679",
    "end": "1206360"
  },
  {
    "text": "uh this table here shows um the um borrowing configuration for some of the",
    "start": "1206360",
    "end": "1211960"
  },
  {
    "text": "boot level bootstrap uh priority levels for example literal action this priority",
    "start": "1211960",
    "end": "1217520"
  },
  {
    "text": "level uh it doesn't land to any other level but it can borrow without limit um",
    "start": "1217520",
    "end": "1225080"
  },
  {
    "start": "1224000",
    "end": "1398000"
  },
  {
    "text": "so another interesting topic what mentioning is prior inversion right it",
    "start": "1225080",
    "end": "1230240"
  },
  {
    "text": "is a case where in the course of serving a request some other request gets get",
    "start": "1230240",
    "end": "1237360"
  },
  {
    "text": "spawned to the K server for example um in the figure below um there we have a",
    "start": "1237360",
    "end": "1242880"
  },
  {
    "text": "cluster extended by aggregation right so user user sends a request a CA server",
    "start": "1242880",
    "end": "1249640"
  },
  {
    "text": "proxies the request to the aggregated APA server the aggregated API server uh",
    "start": "1249640",
    "end": "1255159"
  },
  {
    "text": "spawns a new request B uh in order to serve request a right um so B is subject",
    "start": "1255159",
    "end": "1261960"
  },
  {
    "text": "to APF regulations uh independent of a so if B is rejected by APF as a cons as",
    "start": "1261960",
    "end": "1270159"
  },
  {
    "text": "a consequence a will fail right um some examples are delegated",
    "start": "1270159",
    "end": "1278159"
  },
  {
    "text": "authorization where the aggregated AP server um is in response to serving a",
    "start": "1278159",
    "end": "1284960"
  },
  {
    "text": "request it actually issues um a subject access view to the go API server um",
    "start": "1284960",
    "end": "1291400"
  },
  {
    "text": "there are other examples um kapi server issuing requests to itself uh over local",
    "start": "1291400",
    "end": "1297440"
  },
  {
    "text": "loopback um client um I think there's also examples of",
    "start": "1297440",
    "end": "1303440"
  },
  {
    "text": "external uh admission webbook server issuing requests to the KU API server right um which itself is acting",
    "start": "1303440",
    "end": "1310120"
  },
  {
    "text": "basically serving call outs from the kui server um so how do we solve uh priority",
    "start": "1310120",
    "end": "1316279"
  },
  {
    "text": "inversion um a F doesn't have any mechanism to detect these spawn requests",
    "start": "1316279",
    "end": "1323480"
  },
  {
    "text": "um the way we're solving priority inversion is basically um exempting uh the spawn requests so basically uh you",
    "start": "1323480",
    "end": "1330640"
  },
  {
    "text": "have a flow schema that matches the um the requests um the the spond requests",
    "start": "1330640",
    "end": "1338120"
  },
  {
    "text": "and assign the flow schema to an exempt priority level um for example um for the",
    "start": "1338120",
    "end": "1344400"
  },
  {
    "text": "delegate authorization uh we here we are matching the the traffic right so subject access",
    "start": "1344400",
    "end": "1351240"
  },
  {
    "text": "reviews are matched from that aggregated API server and we are assigning them to",
    "start": "1351240",
    "end": "1356840"
  },
  {
    "text": "uh the exempt priority level okay okay so let's talk about uh",
    "start": "1356840",
    "end": "1363000"
  },
  {
    "text": "client en Rich R uh so when APF rejects a request it sends um 429 status code",
    "start": "1363000",
    "end": "1370240"
  },
  {
    "text": "which means too many requests it also adds um the retry after header uh in the",
    "start": "1370240",
    "end": "1376200"
  },
  {
    "text": "response um the value of the header basically um tells the client how long it should wait before the next rry right",
    "start": "1376200",
    "end": "1384840"
  },
  {
    "text": "um so if if you workload is built with K go um you don't have to worry about um",
    "start": "1384840",
    "end": "1390320"
  },
  {
    "text": "rise kle go automatically rise uh the requests",
    "start": "1390320",
    "end": "1395720"
  },
  {
    "text": "um okay I'll stop that um I'll talk about a couple of",
    "start": "1395720",
    "end": "1401679"
  },
  {
    "text": "situations where APF can help um so in most production environments KU server",
    "start": "1401679",
    "end": "1408240"
  },
  {
    "text": "runs is a pod and cuet um probes it periodically uh so that it knows when to",
    "start": "1408240",
    "end": "1413640"
  },
  {
    "text": "restart it right uh if the aps server is overloaded it may reject the health",
    "start": "1413640",
    "end": "1419120"
  },
  {
    "text": "probes from cubet uh further degrading the cluster right and we can prevent this situation by exempting um the",
    "start": "1419120",
    "end": "1426640"
  },
  {
    "text": "livess probes to the kui server there is another situation um",
    "start": "1426640",
    "end": "1432880"
  },
  {
    "text": "this is a wat storm incident um it shows um basically from a H cluster with three",
    "start": "1432880",
    "end": "1439440"
  },
  {
    "text": "kuas of instances um the graph shows the number of watches over time each line",
    "start": "1439440",
    "end": "1446039"
  },
  {
    "text": "represents a unique CI server instance um and it shows the number of watch",
    "start": "1446039",
    "end": "1451760"
  },
  {
    "text": "requests active on that particular server instance and the gap on the line shows a Time window where um the AP that",
    "start": "1451760",
    "end": "1460960"
  },
  {
    "text": "API server instance was unavailable for the duration if you look at the highlighted area um it it shows that the",
    "start": "1460960",
    "end": "1468360"
  },
  {
    "text": "blue instance died and all the watch requests from the blue instance almost",
    "start": "1468360",
    "end": "1474640"
  },
  {
    "text": "immediately reestablish on the green instance so this sort of watch storm can",
    "start": "1474640",
    "end": "1480600"
  },
  {
    "text": "like overload the AP server or in some instance can crash it right so this is one of the many uh cases of cluster",
    "start": "1480600",
    "end": "1486440"
  },
  {
    "text": "degradation where APF can come to the protection of the uh API",
    "start": "1486440",
    "end": "1491919"
  },
  {
    "text": "server all right I think um APF um adds",
    "start": "1491919",
    "end": "1496960"
  },
  {
    "text": "um as I said metrics for observability uh I'll just quickly go through some of",
    "start": "1496960",
    "end": "1502000"
  },
  {
    "text": "them um so this one is a rate of request dispatched um for each priority level so",
    "start": "1502000",
    "end": "1508840"
  },
  {
    "text": "we can see how many requests is serving per second the next one is",
    "start": "1508840",
    "end": "1515360"
  },
  {
    "text": "um this one shows the number of requests currently executing for each priority",
    "start": "1515360",
    "end": "1522600"
  },
  {
    "text": "level this one shows number requests currently waiting in the queue before they are served",
    "start": "1523360",
    "end": "1530320"
  },
  {
    "text": "and this histogram shows like how much time a request waits in it queue before",
    "start": "1533200",
    "end": "1538880"
  },
  {
    "text": "being executed and it shows it uh for each priority level I think that's it with this I'll",
    "start": "1538880",
    "end": "1545679"
  },
  {
    "text": "stop for uh",
    "start": "1545679",
    "end": "1548559"
  },
  {
    "text": "questions oh how many times oh the default so it's C setting",
    "start": "1551200",
    "end": "1558799"
  },
  {
    "text": "the default value is 10 but I think um an author can actually um modify that",
    "start": "1558799",
    "end": "1566080"
  },
  {
    "text": "settings are you are you referring to the um all oh I see yes no um yes you",
    "start": "1584320",
    "end": "1591960"
  },
  {
    "text": "can actually change the these are actually oh sorry your question was",
    "start": "1591960",
    "end": "1599360"
  },
  {
    "text": "can yeah I'll go back to the slide",
    "start": "1599360",
    "end": "1603760"
  },
  {
    "text": "yeah yeah oh this one okay there you go no this sorry this one I think yes yeah",
    "start": "1617240",
    "end": "1626320"
  },
  {
    "text": "so the question was like all the different configuration you said there's default value from like API machiner",
    "start": "1626320",
    "end": "1631799"
  },
  {
    "text": "that's set in the core right now um You said operators can reset them or set them to different values if they want to",
    "start": "1631799",
    "end": "1637559"
  },
  {
    "text": "is there any specific type of use case you see people having for this type of cluster you want to build you want to",
    "start": "1637559",
    "end": "1643360"
  },
  {
    "text": "set different values or so these are designed in a way that",
    "start": "1643360",
    "end": "1649960"
  },
  {
    "text": "um is enough for uh or approximately",
    "start": "1649960",
    "end": "1655039"
  },
  {
    "text": "enough to you know prevent um to have the aps functioning even during the degradation right um You can a clust r",
    "start": "1655039",
    "end": "1663840"
  },
  {
    "text": "bin can actually change those object these are API objects basically and a cluster operator can change his objects",
    "start": "1663840",
    "end": "1671039"
  },
  {
    "text": "um but it's not recommended I think only U use case is you are you you you",
    "start": "1671039",
    "end": "1678320"
  },
  {
    "text": "experience experiencing and degradation in the cluster and you find out that",
    "start": "1678320",
    "end": "1684000"
  },
  {
    "text": "some priority level here is does not have enough room maybe you can tweak it",
    "start": "1684000",
    "end": "1689039"
  },
  {
    "text": "to make room temporarily um this is maybe one scen are I can think of but um",
    "start": "1689039",
    "end": "1695799"
  },
  {
    "text": "usually we um we recommend that the operators um first identify the request",
    "start": "1695799",
    "end": "1702159"
  },
  {
    "text": "that are valuable to them right and then try to see if there is a flow schema that matches it otherwise you can you",
    "start": "1702159",
    "end": "1708120"
  },
  {
    "text": "can create your own Floy scheme object and then you can assign them to the desired priority level yeah that was",
    "start": "1708120",
    "end": "1715320"
  },
  {
    "text": "going to be the followup I think in the graphs you showed one of the example was like special type like open shift one",
    "start": "1715320",
    "end": "1721720"
  },
  {
    "text": "that was a custom boost configuration probably made as well like in the in the graphs that you",
    "start": "1721720",
    "end": "1727640"
  },
  {
    "text": "had shown I think one of the custom type was like that that was going to be the followup by I think you already answered it I see okay they can create custom",
    "start": "1727640",
    "end": "1733240"
  },
  {
    "text": "configuration okay all right thank you um is there any",
    "start": "1733240",
    "end": "1740039"
  },
  {
    "text": "is there any kind of guidance on how much to tweak the amount of seats available there was 600 as the default",
    "start": "1740799",
    "end": "1747880"
  },
  {
    "text": "from the maximum flight but as a function of U course Ram you allocate",
    "start": "1747880",
    "end": "1755880"
  },
  {
    "start": "1748000",
    "end": "2229000"
  },
  {
    "text": "yeah that's a very good question I think we are still trying to find that answer because this is all like a very uh rough",
    "start": "1755880",
    "end": "1761960"
  },
  {
    "text": "approximation right so when we say that APF uses this as a fence for protection",
    "start": "1761960",
    "end": "1767360"
  },
  {
    "text": "but it's definitely not an accurate fence right so um 600 means um you can",
    "start": "1767360",
    "end": "1772880"
  },
  {
    "text": "use the number of cores to map to this value um and you can run experiments and",
    "start": "1772880",
    "end": "1778840"
  },
  {
    "text": "see uh like how well it performs uh it also depends on the workload you have",
    "start": "1778840",
    "end": "1784760"
  },
  {
    "text": "right some like not all workloads will act like similar uh on the aps of side",
    "start": "1784760",
    "end": "1790519"
  },
  {
    "text": "right so because requests have different costs yeah",
    "start": "1790519",
    "end": "1796880"
  },
  {
    "text": "thanks so request traditionally had a time out of 30 seconds what can I expect here how",
    "start": "1797039",
    "end": "1805399"
  },
  {
    "text": "long does my request stay in those cues okay so uh we made a recent change um",
    "start": "1805399",
    "end": "1812200"
  },
  {
    "text": "the Q wait time is uh depends on the request timeout so um how do you specify",
    "start": "1812200",
    "end": "1818679"
  },
  {
    "text": "time out on request um a user can specify in the request parameter like time out equal to 10 second right um I",
    "start": "1818679",
    "end": "1825799"
  },
  {
    "text": "think we limit the request timeout at uh on the server side to be at most 60 second for the regular requests",
    "start": "1825799",
    "end": "1834640"
  },
  {
    "text": "and the Q8 timeout like APF Q8 timeout is basically 1/4 of the request",
    "start": "1834640",
    "end": "1842600"
  },
  {
    "text": "timeout uh previously it was like hardcoded 15 seconds now it's like based on the request actual request timeout if",
    "start": "1842600",
    "end": "1849760"
  },
  {
    "text": "the user doesn't specify a Timeout on the server side we default it to 60 second so it'll be 15 second um for",
    "start": "1849760",
    "end": "1857320"
  },
  {
    "text": "those requests",
    "start": "1857320",
    "end": "1859919"
  },
  {
    "text": "hey thanks for the talk um I just have a quick series of questions so the Boost sharp config I assume you can't delete",
    "start": "1868159",
    "end": "1875039"
  },
  {
    "text": "cuz you're saying if you make edits it reconciles unless you override yes um if you delete the controller will recreate",
    "start": "1875039",
    "end": "1882000"
  },
  {
    "text": "it okay great uh second question did you drop then in Klein go the request per",
    "start": "1882000",
    "end": "1888760"
  },
  {
    "text": "second client side throttling we have not yet but we did an experiment in um",
    "start": "1888760",
    "end": "1894559"
  },
  {
    "text": "kuci in Upstream CI by disabling uh the client throttling for uh everything in",
    "start": "1894559",
    "end": "1901679"
  },
  {
    "text": "KK uh and we have some results to like share for that like uh but the result",
    "start": "1901679",
    "end": "1907760"
  },
  {
    "text": "looked good but it's not in production I think the in production Cent go still enforces the the the client rate limiter",
    "start": "1907760",
    "end": "1914919"
  },
  {
    "text": "do you know when you'll drop uh we will will want to see how it performs in production usually um the production",
    "start": "1914919",
    "end": "1921480"
  },
  {
    "text": "environments are not up to date with the latest releases so I think there'll will be some soak time and then we'll",
    "start": "1921480",
    "end": "1927600"
  },
  {
    "text": "probably have to figure out from so you're saying there's no Upstream tests for that production environment we had a",
    "start": "1927600",
    "end": "1934039"
  },
  {
    "text": "we had a test uh I think it ran on the 5,000 node",
    "start": "1934039",
    "end": "1939080"
  },
  {
    "text": "test okay uh awesome for you said kle go does retries when does the 429 conflict",
    "start": "1939080",
    "end": "1946320"
  },
  {
    "text": "or the the rate Limited does that stall um like does Cent go wait and",
    "start": "1946320",
    "end": "1954840"
  },
  {
    "text": "retry their Quest so essentially if you're a controller author does that mean you'd be stalling reconciliation",
    "start": "1954840",
    "end": "1960080"
  },
  {
    "text": "when that happens yes so clang go if it sees uh a 429 or any 5xx and it also sees a retry",
    "start": "1960080",
    "end": "1969279"
  },
  {
    "text": "after header uh and the header has a value like today's integer so it will sleep for that number of seconds before",
    "start": "1969279",
    "end": "1976760"
  },
  {
    "text": "the next R TR is it possible to configure Kango to pass that through to the controller",
    "start": "1976760",
    "end": "1982600"
  },
  {
    "text": "author so they could intially recue the yes I think an author can um",
    "start": "1982600",
    "end": "1988799"
  },
  {
    "text": "like set the uh retry um the retry count uh but the actual weit is basically",
    "start": "1988799",
    "end": "1995480"
  },
  {
    "text": "instructed by the server right so server sends a header it says you should wait like 3 seconds and the client will wait",
    "start": "1995480",
    "end": "2001559"
  },
  {
    "text": "3 seconds before it's trying yeah the the reason I'm asking is cuz like for some controllers you have like two",
    "start": "2001559",
    "end": "2008600"
  },
  {
    "text": "concurrent reconciliations at a time as an example so if you have two of those hitting a potential retry because you're",
    "start": "2008600",
    "end": "2015440"
  },
  {
    "text": "hitting some resource um end point there could be other work to be done while the",
    "start": "2015440",
    "end": "2022000"
  },
  {
    "text": "while it's waiting for those retri that's why I was wondering if um what your thoughts are on that um if I",
    "start": "2022000",
    "end": "2027760"
  },
  {
    "text": "remember it correctly I think the the request that the client object that",
    "start": "2027760",
    "end": "2033440"
  },
  {
    "text": "sends a request um it's I think the same threat is same go routine as the um as",
    "start": "2033440",
    "end": "2041440"
  },
  {
    "text": "uh like um as the application so I guess there would be this I don't think there",
    "start": "2041440",
    "end": "2046919"
  },
  {
    "text": "is any way for you to to avoid that that latency that block you're saying okay um",
    "start": "2046919",
    "end": "2055079"
  },
  {
    "text": "I think that's all I had thanks but if you have an application that is built to you know be concurrent um like",
    "start": "2055079",
    "end": "2061200"
  },
  {
    "text": "independently that's something I think the author has to consider I",
    "start": "2061200",
    "end": "2066720"
  },
  {
    "text": "guess",
    "start": "2066720",
    "end": "2069720"
  },
  {
    "text": "yeah that's that was a good point um I guess adding to that just it would maybe",
    "start": "2073159",
    "end": "2078358"
  },
  {
    "text": "make sense to have an error type with timeout for those if you set retry to one then it would give you",
    "start": "2078359",
    "end": "2086520"
  },
  {
    "text": "an error instead saying that well you should the controller should wait or re you after a minute or something if",
    "start": "2086520",
    "end": "2094000"
  },
  {
    "text": "that's a the thing so that is something we probably could add to client go I think in client go",
    "start": "2094000",
    "end": "2102480"
  },
  {
    "text": "um just inform the controller that I see okay but in client go I think if you specify the timeout right uh the server",
    "start": "2102480",
    "end": "2111280"
  },
  {
    "text": "enforces the Timeout on the server side but the client also will wait at most that many seconds so let's say if you",
    "start": "2111280",
    "end": "2117200"
  },
  {
    "text": "say time are equal to 30 seconds um even if the server takes longer the client",
    "start": "2117200",
    "end": "2122640"
  },
  {
    "text": "will time out after that 30 second even if you have retry count",
    "start": "2122640",
    "end": "2128680"
  },
  {
    "text": "available to you for retries I think it'll just it'll be like context timing out basically after 30",
    "start": "2128680",
    "end": "2135280"
  },
  {
    "text": "seconds I don't have a question I just wanted to say thanks for working on something really really hard and subtle",
    "start": "2136480",
    "end": "2142480"
  },
  {
    "text": "thank you so much yes",
    "start": "2142480",
    "end": "2146200"
  },
  {
    "text": "thanks hey it's me again I have one more um",
    "start": "2154599",
    "end": "2159640"
  },
  {
    "text": "if I do something like similar to the endpoint controller and do endpoint management should I be reusing that",
    "start": "2159720",
    "end": "2167839"
  },
  {
    "text": "um uh that priority level or do you recommend I create my",
    "start": "2167839",
    "end": "2174119"
  },
  {
    "text": "own that that's a very good question um if you have your own flow schema and you",
    "start": "2174119",
    "end": "2180560"
  },
  {
    "text": "want to you want to assign it to any of like this um very critical priority level uh you run a risk because those",
    "start": "2180560",
    "end": "2188319"
  },
  {
    "text": "prior levels will be shared to some extent right um and you your if your workload is you know can ramak and flat",
    "start": "2188319",
    "end": "2196359"
  },
  {
    "text": "API server it could impact those more important requests",
    "start": "2196359",
    "end": "2201800"
  },
  {
    "text": "right uh I would say like you know if you have if you're not sure I think the best bet is to create a new par level",
    "start": "2201800",
    "end": "2210000"
  },
  {
    "text": "and assign your flow schema to that level and start with that",
    "start": "2210000",
    "end": "2215040"
  },
  {
    "text": "configuration thanks",
    "start": "2215040",
    "end": "2219520"
  },
  {
    "text": "all right we're done oh thank you so much for coming",
    "start": "2224640",
    "end": "2231480"
  }
]