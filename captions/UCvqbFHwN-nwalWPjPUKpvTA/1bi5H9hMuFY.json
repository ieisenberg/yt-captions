[
  {
    "start": "0",
    "end": "69000"
  },
  {
    "text": "hello uh we're very happy to be there with you today uh we'd",
    "start": "719",
    "end": "6399"
  },
  {
    "text": "rather be live with you and hopefully we might be able to uh in los angeles right",
    "start": "6399",
    "end": "11920"
  },
  {
    "text": "uh so we are lujan eric and we're going to share stories about low-level problems",
    "start": "11920",
    "end": "18560"
  },
  {
    "text": "uh we face from kubernetes we showed a few stories in the past about control plane challenges but today we're",
    "start": "18560",
    "end": "25359"
  },
  {
    "text": "going to focus on what happens on nodes so in case you don't know",
    "start": "25359",
    "end": "30480"
  },
  {
    "text": "datadog uh we're a monitoring company but today we're not going to talk about the",
    "start": "30480",
    "end": "36480"
  },
  {
    "text": "product we're going to talk about the infrastructure behind the products and",
    "start": "36480",
    "end": "41680"
  },
  {
    "text": "uh it's pretty large uh we're running tens of thousands of hosts and dozens of clusters",
    "start": "41680",
    "end": "46960"
  },
  {
    "text": "uh with up to four thousand nodes uh and it comes with uh multiple challenges",
    "start": "46960",
    "end": "53920"
  },
  {
    "text": "and this is what we're going to talk about today so we have a few stories uh not",
    "start": "53920",
    "end": "60719"
  },
  {
    "text": "completely related but they they all happen locally on notes",
    "start": "60719",
    "end": "65760"
  },
  {
    "text": "and we're going to to focus on that so let's start uh with this first story",
    "start": "65760",
    "end": "72400"
  },
  {
    "start": "69000",
    "end": "580000"
  },
  {
    "text": "about disappearing containers so it started uh with this simple graph",
    "start": "72400",
    "end": "78400"
  },
  {
    "text": "here when we migrated an application from a virtual machine to kubernetes and we noticed",
    "start": "78400",
    "end": "84240"
  },
  {
    "text": "that the cpu profile was very different which was a bit worrying right",
    "start": "84240",
    "end": "90560"
  },
  {
    "text": "so we looked at uh the pods running this application and we discovered that several of them",
    "start": "90560",
    "end": "97040"
  },
  {
    "text": "were in great container error actually about 10 of them so they were not ready and not processing traffic",
    "start": "97040",
    "end": "103600"
  },
  {
    "text": "we found very easy solutions to address this by editing the bird or researching the",
    "start": "103600",
    "end": "108960"
  },
  {
    "text": "cubelet but it wasn't very satisfying so the first thing we did was try to",
    "start": "108960",
    "end": "115280"
  },
  {
    "text": "understand what this error was so we looked at the qubit logs and basically the qubit is getting an error",
    "start": "115280",
    "end": "122320"
  },
  {
    "text": "for the runtime containing in our case uh because it's trying it seems to be trying to create a",
    "start": "122320",
    "end": "128720"
  },
  {
    "text": "container that already exists according to the runtime and it's doing this a lot as you can see",
    "start": "128720",
    "end": "133920"
  },
  {
    "text": "here with 62 attempts right and it keeps retrying and getting the exact same results",
    "start": "133920",
    "end": "140799"
  },
  {
    "text": "so of course we wonder well who is right uh who is wrong so the cubit believes it",
    "start": "141280",
    "end": "147599"
  },
  {
    "text": "needs to create a container what about continuity so if we have container d this container is actually running completely fine",
    "start": "147599",
    "end": "157680"
  },
  {
    "text": "the next step was to try and reproduce a problem in a test environment and we had noticed that it seemed to be",
    "start": "157680",
    "end": "163519"
  },
  {
    "text": "related with cubelet restarts so we did a very simple script that we started the cubelet frequently",
    "start": "163519",
    "end": "170000"
  },
  {
    "text": "and after only an hour as you can see all the parts in the deployment were in",
    "start": "170000",
    "end": "175519"
  },
  {
    "text": "the same era so we were able to very easily reproduce it right",
    "start": "175519",
    "end": "181840"
  },
  {
    "text": "so what is this error exactly so we went to the cubelet code and we",
    "start": "181840",
    "end": "187360"
  },
  {
    "text": "looked for it and we found that this error is is appearing in the runtime manager where",
    "start": "187360",
    "end": "195120"
  },
  {
    "text": "uh in the syncpod function so it seems that the cubelet wants to create this",
    "start": "195120",
    "end": "201920"
  },
  {
    "text": "container and entails how is this function called",
    "start": "201920",
    "end": "210080"
  },
  {
    "text": "so this function is called in in sync bud after computing both actions which is",
    "start": "210799",
    "end": "215840"
  },
  {
    "text": "basically uh the reconciliation that needs to happen between the pod spec and what's in the",
    "start": "215840",
    "end": "220879"
  },
  {
    "text": "runtime and in this case here it seems that the results of this computation is wrong how could that",
    "start": "220879",
    "end": "228080"
  },
  {
    "text": "be we looked at clogged but we couldn't",
    "start": "228080",
    "end": "233200"
  },
  {
    "text": "find much in logs even when we increased the velocity level so we did a",
    "start": "233200",
    "end": "238560"
  },
  {
    "text": "very simple thing and we added print statements in in the code and it turned out it was very efficient",
    "start": "238560",
    "end": "245120"
  },
  {
    "text": "because as you can see at the bottom of the screen um a container of the container 15 that",
    "start": "245120",
    "end": "251200"
  },
  {
    "text": "is completely fine in the first output statement is missing in the second one but the container is still",
    "start": "251200",
    "end": "257199"
  },
  {
    "text": "running so it seemed that container statuses which has the statuses of all the containers in a",
    "start": "257199",
    "end": "263440"
  },
  {
    "text": "pod is actually wrong so we wanted to understand where this",
    "start": "263440",
    "end": "270560"
  },
  {
    "text": "container status was coming from and how it could be wrong so here is an",
    "start": "270560",
    "end": "275759"
  },
  {
    "text": "overview of the main component involved in this data path in the cubelets",
    "start": "275759",
    "end": "281440"
  },
  {
    "text": "so everything starts with a single iteration which is the main sync loop in the cubelet this",
    "start": "281440",
    "end": "287840"
  },
  {
    "text": "sync loop receives events on channels um and these events can be event from api server such",
    "start": "287840",
    "end": "293759"
  },
  {
    "text": "as a spec change event from the lifecycle advanced generator which has",
    "start": "293759",
    "end": "299759"
  },
  {
    "text": "which are events from the runtime and other types of events when there is a change to a pod it's",
    "start": "299759",
    "end": "307680"
  },
  {
    "text": "then going to call pod workers which are going to get the status from the pod cache and",
    "start": "307680",
    "end": "314240"
  },
  {
    "text": "called the syncpod function which is the function that was failing when it was calling the runtime before",
    "start": "314240",
    "end": "319680"
  },
  {
    "text": "so it seemed that when the status is retrieved from the podcache it's wrong which triggered the problem",
    "start": "319680",
    "end": "325360"
  },
  {
    "text": "down underground so what is the pod cache the pod cache",
    "start": "325360",
    "end": "331039"
  },
  {
    "text": "is a cache of all the pods in the cubelets and how is it maintained so the podcast",
    "start": "331039",
    "end": "337520"
  },
  {
    "text": "is maintained by the pod lifecycle event generator plague which is the component responsible for",
    "start": "337520",
    "end": "344479"
  },
  {
    "text": "observing what's happening with the runtime updating the cache and generating events",
    "start": "344479",
    "end": "350160"
  },
  {
    "text": "that will then be received by sync to preparation and it seemed that this cache gets",
    "start": "350160",
    "end": "356479"
  },
  {
    "text": "corrupted somehow so here is what we know uh",
    "start": "356479",
    "end": "363360"
  },
  {
    "text": "the data in the podcast is wrong the podcast is managed by the plague and there's nothing in the play that",
    "start": "363360",
    "end": "370479"
  },
  {
    "text": "seems to indicate situations where we can get corruption so what's",
    "start": "370479",
    "end": "375680"
  },
  {
    "text": "happening and at that point we we were wondering well what is something else is modifying the",
    "start": "375680",
    "end": "382319"
  },
  {
    "text": "content of the cache in particular the container status is filled which is uh what's wrong here so we looked at",
    "start": "382319",
    "end": "390880"
  },
  {
    "text": "um the code of the cubelet and after a lot of digging we found this function which is sorted",
    "start": "390880",
    "end": "398639"
  },
  {
    "text": "which is sorting the container statuses and what's important here is that this function is actually sorting in place",
    "start": "398639",
    "end": "405600"
  },
  {
    "text": "and modifying the slides so we now have another call in the cube that is",
    "start": "405600",
    "end": "410639"
  },
  {
    "text": "modifying the podcast it's not only the plague anymore and so we looked at where this function",
    "start": "410639",
    "end": "415680"
  },
  {
    "text": "is called and here you have the sequence of functions involved and",
    "start": "415680",
    "end": "421199"
  },
  {
    "text": "what is important here is syncpod and sql iteration which are two functions we saw before",
    "start": "421199",
    "end": "426639"
  },
  {
    "text": "are actually ending up calling this uh this this sort operation so",
    "start": "426639",
    "end": "433759"
  },
  {
    "text": "let's go back to the cubelets to get back to the function that can",
    "start": "433759",
    "end": "440160"
  },
  {
    "text": "actually modify the pod cache so as we've seen before the player can modify the pod cache but also the function generate api put",
    "start": "440160",
    "end": "447840"
  },
  {
    "text": "status that can be called in two different cases one when there is a product date",
    "start": "447840",
    "end": "453360"
  },
  {
    "text": "and one when there is a plague event where the contender dives",
    "start": "453360",
    "end": "458720"
  },
  {
    "text": "and the problem is the handlers for these events are running in separate",
    "start": "458720",
    "end": "463840"
  },
  {
    "text": "core routines so they can race and which can lead to uh cash corruption",
    "start": "463840",
    "end": "470960"
  },
  {
    "text": "and uh here is a summary right we can have a concurrency between either syncpod if there's bot update from the",
    "start": "471280",
    "end": "478080"
  },
  {
    "text": "api server if a container dies on the plague or during a plague release if there was",
    "start": "478080",
    "end": "483919"
  },
  {
    "text": "something happening on the runtime so we had a theory and we",
    "start": "483919",
    "end": "489280"
  },
  {
    "text": "wanted to validate it so what we did is instead of sorting the slice in place",
    "start": "489280",
    "end": "495280"
  },
  {
    "text": "we made a copy of it and then did all the operation on the copy",
    "start": "495280",
    "end": "500960"
  },
  {
    "text": "and look everything is running now even if we restart the cubelet every few seconds so we",
    "start": "500960",
    "end": "508800"
  },
  {
    "text": "created a pr upstream and we're pretty happy because we've found and",
    "start": "508800",
    "end": "515360"
  },
  {
    "text": "fixed the the issue i also wanted to to credit nayef for doing all the work on this investigation",
    "start": "515360",
    "end": "522640"
  },
  {
    "text": "and uh diving very deep inside the cubelet",
    "start": "522640",
    "end": "527920"
  },
  {
    "text": "so key takeaways from this is that the current internals are complex",
    "start": "527920",
    "end": "532959"
  },
  {
    "text": "but they're accessible if you spend the time since the internals are complex we're",
    "start": "532959",
    "end": "539519"
  },
  {
    "text": "not 100 sure that our fix is the best one and that the data path i described is the full",
    "start": "539519",
    "end": "546160"
  },
  {
    "text": "one but what we know is the fix definitely works and solved our problem",
    "start": "546160",
    "end": "551279"
  },
  {
    "text": "and the pr has not been much upstream but we hope it will soon or an equivalent pr fixing thinking the",
    "start": "551279",
    "end": "557839"
  },
  {
    "text": "problem we're a bit lucky on this one because we cut the problem in staging which allowed us to",
    "start": "557839",
    "end": "563279"
  },
  {
    "text": "depot the production completely completely fine which was good news",
    "start": "563279",
    "end": "568480"
  },
  {
    "text": "so in in in this first um story we had a story about the cubelets and and now",
    "start": "568480",
    "end": "574160"
  },
  {
    "text": "eric is going to dive even deeper into something that's happening in the runtime itself",
    "start": "574160",
    "end": "580879"
  },
  {
    "start": "580000",
    "end": "952000"
  },
  {
    "text": "yeah thanks man so this one is titled random failures which as we'll see is a double pun on the actual problem",
    "start": "581440",
    "end": "591839"
  },
  {
    "text": "so the the problem we saw was that occasionally we had a database crash and",
    "start": "592880",
    "end": "599120"
  },
  {
    "text": "it was preceded by a rather puzzling log the pro the database couldn't open their",
    "start": "599120",
    "end": "605120"
  },
  {
    "text": "view random and it was getting an operation not permitted error and this is happening occasionally i",
    "start": "605120",
    "end": "611279"
  },
  {
    "text": "mean clearly the database works most of the time and then all of a sudden it can't open debut random so what's",
    "start": "611279",
    "end": "617760"
  },
  {
    "text": "happening if we look inside the container everything is fine the",
    "start": "617760",
    "end": "623680"
  },
  {
    "text": "file permissions on debut random are perfectly open to everyone to read everyone right",
    "start": "623680",
    "end": "629440"
  },
  {
    "text": "everyone we don't really see why we should be having a problem opening devi ran",
    "start": "629440",
    "end": "636160"
  },
  {
    "text": "so we write a straightforward very stupid reproducer very blunt",
    "start": "636640",
    "end": "642880"
  },
  {
    "text": "it just sits in a loop and keeps opening debug random and errors if it fails to do so",
    "start": "642880",
    "end": "649680"
  },
  {
    "text": "and very quickly we see that we're able to reproduce the problem we get the operation not permitted message and what's more interesting",
    "start": "649680",
    "end": "657040"
  },
  {
    "text": "maybe is that this happens every 10 or so seconds so it's extremely regular",
    "start": "657040",
    "end": "664320"
  },
  {
    "text": "if we search a little bit more for logs that the cubelet might be emitting for for this for this pod we",
    "start": "665839",
    "end": "672800"
  },
  {
    "text": "realized that in fact every 10 seconds the cpu manager component is emitting a reconcile state log",
    "start": "672800",
    "end": "680560"
  },
  {
    "text": "message and if we look at the details of that message we see that it's actually includes a configuration",
    "start": "680560",
    "end": "687920"
  },
  {
    "text": "for the cpu set so i need to explain a little bit more what the cpu set",
    "start": "687920",
    "end": "693760"
  },
  {
    "text": "is in this situation our pod is in guaranteed qrs class meaning that",
    "start": "693760",
    "end": "700959"
  },
  {
    "text": "it has requests and limits which are all specified and all equal for all the resources it requires",
    "start": "700959",
    "end": "708560"
  },
  {
    "text": "it also makes an integer request for cpus so six in this example",
    "start": "708560",
    "end": "714720"
  },
  {
    "text": "and finally we use a cubelet configuration where the cpu manager policy is static now the static cpu manager",
    "start": "714720",
    "end": "723839"
  },
  {
    "text": "policy is there to allow requesting and setting aside a specific set of cpus for a container",
    "start": "723839",
    "end": "732639"
  },
  {
    "text": "and doing so using the cpu set control group",
    "start": "732639",
    "end": "740000"
  },
  {
    "text": "so now if we look at the flow of configuration here so in the container runtime so we have the cubelet which",
    "start": "741040",
    "end": "747760"
  },
  {
    "text": "talks to container d over grpc so telling container d what containers need",
    "start": "747760",
    "end": "753120"
  },
  {
    "text": "to be launched what uh configuration what characteristics they need to have container indeed container",
    "start": "753120",
    "end": "758720"
  },
  {
    "text": "d in turn uh fork ex container shims which are per container and communicates",
    "start": "758720",
    "end": "765440"
  },
  {
    "text": "them over time through grpc also and the container the shim fork execs run c to actually start",
    "start": "765440",
    "end": "773279"
  },
  {
    "text": "containers and potentially update them again over time so for instance it will update the c group files of the containers",
    "start": "773279",
    "end": "782160"
  },
  {
    "text": "so if we follow the smoking gun through that flow we look at container d now and we see that container d also",
    "start": "783920",
    "end": "790480"
  },
  {
    "text": "is every 10 seconds reflecting an update to container resources not a big surprise at this point we",
    "start": "790480",
    "end": "800160"
  },
  {
    "text": "follow further down and we trace the shim we see that the shim every 10 seconds launches run c",
    "start": "800160",
    "end": "806240"
  },
  {
    "text": "and in particular launches run c with an update [Music] instruction tracing the run c",
    "start": "806240",
    "end": "813680"
  },
  {
    "text": "execution itself we see something rather surprising on the other hand we see that the um",
    "start": "813680",
    "end": "821360"
  },
  {
    "text": "update to control group devices section is starting with a deny",
    "start": "821360",
    "end": "828639"
  },
  {
    "text": "on all devices and then followed by individual allows for the rest of the devices for",
    "start": "828639",
    "end": "834639"
  },
  {
    "text": "the ones that we actually should be allowed to access and so we found a race in effect there's a window where for a",
    "start": "834639",
    "end": "842000"
  },
  {
    "text": "certain amount of time all devices are not allowed and progressively some will be re-allowed",
    "start": "842000",
    "end": "848160"
  },
  {
    "text": "until finally everything that should be allowed is allowed but there's a definite window of opportunity here",
    "start": "848160",
    "end": "853920"
  },
  {
    "text": "to attempt to open a device and to find that we're not allowed to do so because of the default denial",
    "start": "853920",
    "end": "862000"
  },
  {
    "text": "so we found the issue and in fact what we're in luck because the problem has actually been",
    "start": "864079",
    "end": "869440"
  },
  {
    "text": "fixed upstream and so we really only needed to upgrade run c here to resolve the issue",
    "start": "869440",
    "end": "875120"
  },
  {
    "text": "and the way run c resolved the problem is by emulating the changes that would be",
    "start": "875120",
    "end": "880240"
  },
  {
    "text": "taken on update and checking them against what is in place and only reflecting the differences between the desired",
    "start": "880240",
    "end": "887519"
  },
  {
    "text": "state and the current state so this was",
    "start": "887519",
    "end": "892720"
  },
  {
    "text": "an interesting problem we relearned how the flow of configuration from through",
    "start": "892720",
    "end": "899040"
  },
  {
    "text": "the runtime from the cubelet down all the way down to the container and we learned about uh c group device",
    "start": "899040",
    "end": "904240"
  },
  {
    "text": "updates as well and possibly a little bit about the cpu set um all this uh investigation goes to",
    "start": "904240",
    "end": "911600"
  },
  {
    "text": "benjamin pino who delved into this a little while back thank you very much to him",
    "start": "911600",
    "end": "918959"
  },
  {
    "text": "so key takeaways it's really good to know the the container runtime flow and",
    "start": "920320",
    "end": "926240"
  },
  {
    "text": "understand it and how the different components interact it's also important to realize that here",
    "start": "926240",
    "end": "932240"
  },
  {
    "text": "we had several parameters influencing the overall behavior",
    "start": "932240",
    "end": "937920"
  },
  {
    "text": "for instance we had pods with guaranteed qos integer cpu requests and a cpu manager configuration for static",
    "start": "937920",
    "end": "944399"
  },
  {
    "text": "if any of those three parameters had been different we would not have encountered this issue",
    "start": "944399",
    "end": "951360"
  },
  {
    "start": "952000",
    "end": "1444000"
  },
  {
    "text": "and so now i'll uh let lauren dive into an even deeper investigation with the uh",
    "start": "952320",
    "end": "959680"
  },
  {
    "text": "network uh layer of kubernetes we're we're not going to talk about uh",
    "start": "959680",
    "end": "967279"
  },
  {
    "text": "how well ips can become zombies so",
    "start": "967279",
    "end": "972880"
  },
  {
    "text": "this problem started with containers remaining in status container creating for for a long time and the error message",
    "start": "972880",
    "end": "979279"
  },
  {
    "text": "was fun in our sense of fun which is well very weird so here the queue is telling",
    "start": "979279",
    "end": "987440"
  },
  {
    "text": "us that it can't create the butt sandbox so it can create networking in the bud because it's failing to add an ip",
    "start": "987440",
    "end": "993680"
  },
  {
    "text": "address because this address is already in use which is weird because",
    "start": "993680",
    "end": "998800"
  },
  {
    "text": "it never happened before and suddenly it's starting to open so address already in use let's see if",
    "start": "998800",
    "end": "1005519"
  },
  {
    "text": "this address is banned on the host it's not can i add it to an interface i",
    "start": "1005519",
    "end": "1010560"
  },
  {
    "text": "definitely can so nothing obvious there so",
    "start": "1010560",
    "end": "1017040"
  },
  {
    "text": "of course it could also be an ip address from a container so let's look at network",
    "start": "1017040",
    "end": "1023199"
  },
  {
    "text": "name spaces and look at ip addresses in them none of the network name spaces",
    "start": "1023199",
    "end": "1029520"
  },
  {
    "text": "have disappeared address so we've done the obvious and now we",
    "start": "1029520",
    "end": "1035438"
  },
  {
    "text": "need to take a step back to exactly understand what the components",
    "start": "1035439",
    "end": "1040880"
  },
  {
    "text": "what components are involved in this so let's look at our cni plugin on this cluster",
    "start": "1041360",
    "end": "1046400"
  },
  {
    "text": "so we use the livescenic plugin which is uh basically allowing us to attach ip",
    "start": "1046400",
    "end": "1052640"
  },
  {
    "text": "addresses from the underlying cloud provider aws directly to pods and not have an overlay",
    "start": "1052640",
    "end": "1059280"
  },
  {
    "text": "so it starts with an ipam plugin responsible for the aws api call and then there is an",
    "start": "1059280",
    "end": "1066640"
  },
  {
    "text": "ipvlan plugin that is creating the main pod interface and giving it an ip there's",
    "start": "1066640",
    "end": "1073919"
  },
  {
    "text": "another interface that is only used for communication with the host and it doesn't really matter here",
    "start": "1073919",
    "end": "1078960"
  },
  {
    "text": "what is interesting is what's happening with ipplan so now that we know that on this host",
    "start": "1078960",
    "end": "1084480"
  },
  {
    "text": "we're using ipvlan let's say we can create a new ipvland device with this opening ip",
    "start": "1084480",
    "end": "1090960"
  },
  {
    "text": "and as you can see on the top of the slide if we try we actually can't and and we get a very",
    "start": "1090960",
    "end": "1097440"
  },
  {
    "text": "clear error message saying that this ip address is already already assigned sorry to an",
    "start": "1097440",
    "end": "1102799"
  },
  {
    "text": "ipv land device something i want to mention here is that the message is very clear",
    "start": "1102799",
    "end": "1108400"
  },
  {
    "text": "but it doesn't make it to the cubelet because of the netflix library used by the plugin which only use um",
    "start": "1108400",
    "end": "1116480"
  },
  {
    "text": "the error code right and not the error message so we only have address news and we don't have the full",
    "start": "1116480",
    "end": "1121600"
  },
  {
    "text": "detailed message which would have been helpful right",
    "start": "1121600",
    "end": "1126080"
  },
  {
    "text": "so now we need to understand uh what else why are you seeing a plugin is using this ip",
    "start": "1127679",
    "end": "1134880"
  },
  {
    "text": "if we look at the registry of ips used by the plugin this ip is known which means it has been used and",
    "start": "1135039",
    "end": "1141440"
  },
  {
    "text": "assigned to a pod in the past but it's been released but somehow it's been released but not completely",
    "start": "1141440",
    "end": "1148480"
  },
  {
    "text": "right because the ip address is still bound to the appeal and device",
    "start": "1148480",
    "end": "1153840"
  },
  {
    "text": "so we looked at how ipvline handles deletion in this plugin and as you can see here there's code to",
    "start": "1153840",
    "end": "1160559"
  },
  {
    "text": "delete the interface but we never run it because we use chaining and in the case of chaining",
    "start": "1160559",
    "end": "1166160"
  },
  {
    "text": "the deletion call is is never done so we never delete the device however uh",
    "start": "1166160",
    "end": "1173360"
  },
  {
    "text": "it works usually because the runtime uh at the end of the deletion of a pod",
    "start": "1173360",
    "end": "1179840"
  },
  {
    "text": "will actually delete the network namespace which would which will take care of garbage collecting everything",
    "start": "1179840",
    "end": "1186799"
  },
  {
    "text": "so what we know so far the ipa has been a drain has been located to a pod the pod was deleted and",
    "start": "1187039",
    "end": "1192960"
  },
  {
    "text": "vip was marked free the equivalent interface was not deleted",
    "start": "1192960",
    "end": "1198320"
  },
  {
    "text": "when the runtime called network deletion and so we can reuse the ip",
    "start": "1198320",
    "end": "1204080"
  },
  {
    "text": "what's important about network namespace deletion is when you delete the namespace",
    "start": "1204080",
    "end": "1209200"
  },
  {
    "text": "uh the qnl will tell you okay i can i can delete it but it won't actually delete the",
    "start": "1209200",
    "end": "1215200"
  },
  {
    "text": "namespace until every single process running it is is is done running right but what's",
    "start": "1215200",
    "end": "1222960"
  },
  {
    "text": "important here is everything is fine from cni and from the runtime right",
    "start": "1222960",
    "end": "1228240"
  },
  {
    "text": "but the kernel hasn't finished deleting the namespace now that we have this piece of",
    "start": "1228240",
    "end": "1234720"
  },
  {
    "text": "information can we can we reproduce so what we did is we took a part we exact into its network namespace from",
    "start": "1234720",
    "end": "1241440"
  },
  {
    "text": "the commands and as you can see here we can see here that the um network namespace",
    "start": "1241440",
    "end": "1249200"
  },
  {
    "text": "of this process is actually the one of the bug right if we delete this namespace everything",
    "start": "1249200",
    "end": "1255440"
  },
  {
    "text": "works the cni marks the ips released however we can't reuse it and add it to an end",
    "start": "1255440",
    "end": "1262559"
  },
  {
    "text": "device and what's interesting is if we enter the network namespace of the process we",
    "start": "1262559",
    "end": "1268480"
  },
  {
    "text": "created we can still see the iqlan interface so we have a very good idea what's happening here",
    "start": "1268480",
    "end": "1277840"
  },
  {
    "text": "so um let's see if we can find the process holding this network",
    "start": "1278000",
    "end": "1283520"
  },
  {
    "text": "namespace so what we did here is we looked at all the processes and the network namespaces and we entered them",
    "start": "1283520",
    "end": "1291120"
  },
  {
    "text": "and we looked for vip for our process we were able to find it however we were not able to find it for",
    "start": "1291120",
    "end": "1297600"
  },
  {
    "text": "the iphone that was already in use right so that was we have an idea but we can't",
    "start": "1297600",
    "end": "1302720"
  },
  {
    "text": "actually trace the problem back to an actual process so we took a step back because we had a",
    "start": "1302720",
    "end": "1309039"
  },
  {
    "text": "good idea what was happening but we were not able to pinpoint exactly what was the concrete",
    "start": "1309039",
    "end": "1315039"
  },
  {
    "text": "we had a few additional data points we knew that the problem started with a node and slowly propagated to others and that",
    "start": "1315039",
    "end": "1321760"
  },
  {
    "text": "regularly the problem was completely disappearing and restarting again",
    "start": "1321760",
    "end": "1327520"
  },
  {
    "text": "so what are we looking for we're looking at the process we're looking for a process that is running on all nodes that is",
    "start": "1327520",
    "end": "1334640"
  },
  {
    "text": "regularly either stopped restarted or redeployed what kind of process is that maybe a",
    "start": "1334640",
    "end": "1341120"
  },
  {
    "text": "demon set right so we look at demon sets and we have a lot of very few demon sets",
    "start": "1341120",
    "end": "1347520"
  },
  {
    "text": "but there's one that has a lot of that does a lot of things and it's data log agents so we",
    "start": "1347520",
    "end": "1354559"
  },
  {
    "text": "actually were able to trace back the fact that when the deadlock agent was redeployed the error was fixed so we reached out to",
    "start": "1354559",
    "end": "1362480"
  },
  {
    "text": "the team and they told us well we actually have a feature that is allowing us to instrument",
    "start": "1362480",
    "end": "1368000"
  },
  {
    "text": "the contract and nuts inside network and space of containers and so we looked into it with them and",
    "start": "1368000",
    "end": "1374159"
  },
  {
    "text": "it turned out this new feature was using a long-lived connection on a netting circuit to get this",
    "start": "1374159",
    "end": "1379600"
  },
  {
    "text": "information and of course this was holding the network namespaces and to fix that",
    "start": "1379600",
    "end": "1385440"
  },
  {
    "text": "we just had to use short-lived connections in terms of long-lived one so the network namespace could be released by the kernel",
    "start": "1385440",
    "end": "1393840"
  },
  {
    "text": "key takeaways uh interaction between all these components are complex",
    "start": "1394880",
    "end": "1399919"
  },
  {
    "text": "it's also very difficult in the kernel to understand what's happening at the networking level because a lot of information can't",
    "start": "1399919",
    "end": "1407520"
  },
  {
    "text": "be found so you need to to guess a lot and also i said that we were lucky because",
    "start": "1407520",
    "end": "1413360"
  },
  {
    "text": "we code this in staging and also this error was made very visible by the cni issue but",
    "start": "1413360",
    "end": "1421919"
  },
  {
    "text": "if we hadn't seen it in the sinai issue we would have leaked network namespace with unknown consequences right and",
    "start": "1421919",
    "end": "1431039"
  },
  {
    "text": "that's it for this network debugging problem i mean i love networking but i think the story",
    "start": "1431039",
    "end": "1436240"
  },
  {
    "text": "that eric is going to share next uh which is even deeper because of the channels is",
    "start": "1436240",
    "end": "1441440"
  },
  {
    "text": "even more interesting yeah thanks so who ate my capabilities",
    "start": "1441440",
    "end": "1448720"
  },
  {
    "start": "1444000",
    "end": "1876000"
  },
  {
    "text": "this is the the story that gave its title to to the presentation",
    "start": "1448720",
    "end": "1454640"
  },
  {
    "text": "okay a word about uh lingus capabilities capabilities are a feature of the linux",
    "start": "1456159",
    "end": "1462880"
  },
  {
    "text": "kernel that allow us to give privileges to certain processes",
    "start": "1462880",
    "end": "1467919"
  },
  {
    "text": "that are not normally privileges that don't run as root for instance so in the example that i'm showing here",
    "start": "1467919",
    "end": "1475600"
  },
  {
    "text": "we have the capture pro program that just displays the capabilities that it has and decodes",
    "start": "1475600",
    "end": "1481440"
  },
  {
    "text": "them for us which initially doesn't have any and to which i then assign a capability",
    "start": "1481440",
    "end": "1488000"
  },
  {
    "text": "through the extended attributes of the file on disk and then when i run so here the capnet",
    "start": "1488000",
    "end": "1493919"
  },
  {
    "text": "bind service which means i this process would be allowed to bind ports under 1024 which is",
    "start": "1493919",
    "end": "1499200"
  },
  {
    "text": "privileged normally reserved for root and um then once it has these capabilities",
    "start": "1499200",
    "end": "1504880"
  },
  {
    "text": "on the file and extended attributes if i run the program i see that i have this privilege now",
    "start": "1504880",
    "end": "1511200"
  },
  {
    "text": "so we wanted to use this for the api server the cube api server which",
    "start": "1511200",
    "end": "1518159"
  },
  {
    "text": "we run as a pod in certain circumstances and so quite simply in our docker file we set",
    "start": "1518159",
    "end": "1525600"
  },
  {
    "text": "the capability on the file and then when we run the prod the strange thing is that it doesn't work we are unable to bind",
    "start": "1525600",
    "end": "1532960"
  },
  {
    "text": "port 443 we get a permission denied error just as if we didn't have the privilege",
    "start": "1532960",
    "end": "1538720"
  },
  {
    "text": "so if we look at the actual process characteristics we see that indeed it does not have the",
    "start": "1540240",
    "end": "1546320"
  },
  {
    "text": "privilege it doesn't have the capability the permitted capabilities the effective capabilities of the process",
    "start": "1546320",
    "end": "1552480"
  },
  {
    "text": "are all zero there all the bits are cleared um even though we should have 400 uh",
    "start": "1552480",
    "end": "1559600"
  },
  {
    "text": "where the the value of the cabinet main service uh a bit and yet if we look at the",
    "start": "1559600",
    "end": "1566080"
  },
  {
    "text": "capabilities on the file system they're there it's what we built in the dockerfile so",
    "start": "1566080",
    "end": "1571200"
  },
  {
    "text": "again what is happening if we think about the usual suspects um",
    "start": "1571200",
    "end": "1578240"
  },
  {
    "text": "there's the no new privileges flag we check and that isn't set",
    "start": "1578240",
    "end": "1583440"
  },
  {
    "text": "we're not using no set user id file systems our kernels are booted with file",
    "start": "1583440",
    "end": "1589120"
  },
  {
    "text": "capabilities allowed um and yet one interesting observation",
    "start": "1589120",
    "end": "1594159"
  },
  {
    "text": "here is that since the capabilities are all clear",
    "start": "1594159",
    "end": "1599279"
  },
  {
    "text": "it means and yet the program it ran was was launched by the kernel it does",
    "start": "1599279",
    "end": "1604559"
  },
  {
    "text": "mean that the file capabilities literally don't apply according to the kernel",
    "start": "1604559",
    "end": "1610320"
  },
  {
    "text": "and another thing we build the image on our laptops and interestingly it works",
    "start": "1610320",
    "end": "1617120"
  },
  {
    "text": "so that prompts us to look at the layers of the image that we build and here we realize",
    "start": "1619200",
    "end": "1626159"
  },
  {
    "text": "that the extended attributes of the file are actually different in the laptop",
    "start": "1626159",
    "end": "1632320"
  },
  {
    "text": "case we see that the capabilities are encoded using a version 2 and in the comp by",
    "start": "1632320",
    "end": "1640080"
  },
  {
    "text": "the the image built up by our continuous integration has a version three and we also see that in the version",
    "start": "1640080",
    "end": "1645600"
  },
  {
    "text": "three there's more information",
    "start": "1645600",
    "end": "1649520"
  },
  {
    "text": "so what are these attributes exactly in fact now we realize that there's actually an",
    "start": "1652480",
    "end": "1658799"
  },
  {
    "text": "extra option on the get cap command which we didn't notice initially which causes the extra information the",
    "start": "1658799",
    "end": "1666240"
  },
  {
    "text": "version 3 capabilities information to be displayed and it's in fact a user id the root id",
    "start": "1666240",
    "end": "1671679"
  },
  {
    "text": "so what exactly is this root id well in our ci's docker configuration we",
    "start": "1671679",
    "end": "1677600"
  },
  {
    "text": "actually run it with user name spaces usernamespaces is a great feature uh",
    "start": "1677600",
    "end": "1683760"
  },
  {
    "text": "very applicable in this particular context where we want to run untrusted code and so what usernamespaces do",
    "start": "1683760",
    "end": "1692159"
  },
  {
    "text": "is that they actually remap user ids and group ids and in this situation the kernel needs",
    "start": "1692159",
    "end": "1699600"
  },
  {
    "text": "to ensure from a security perspective that if a program run in one username",
    "start": "1699600",
    "end": "1705440"
  },
  {
    "text": "space grants a certain set of capabilities to a file",
    "start": "1705440",
    "end": "1711120"
  },
  {
    "text": "you can't simply take that file and run it in another username space and acquire those same privileges",
    "start": "1711120",
    "end": "1716559"
  },
  {
    "text": "that would be an immediate security issue so what the kernel does is that it persists the root user id",
    "start": "1716559",
    "end": "1723279"
  },
  {
    "text": "of the user namespace in the extended file attribute",
    "start": "1723279",
    "end": "1728559"
  },
  {
    "text": "so that it can ensure that if it's run in another username space unless the user ids map",
    "start": "1728559",
    "end": "1733600"
  },
  {
    "text": "match sorry they will not be applied now as it happens this is irrelevant for",
    "start": "1733600",
    "end": "1739440"
  },
  {
    "text": "container image builds because clearly we want to build images in one place and run them anywhere else that we that we wish so we",
    "start": "1739440",
    "end": "1745520"
  },
  {
    "text": "really don't want a v3 capability here we just want",
    "start": "1745520",
    "end": "1751279"
  },
  {
    "text": "the v2 without the root id and and in this case the fact that um docker mobi",
    "start": "1751279",
    "end": "1758799"
  },
  {
    "text": "build actually persists this um this information this user id is is an",
    "start": "1758799",
    "end": "1765039"
  },
  {
    "text": "error and so we developed a fix and the pr was merged upstream and we should have to fix",
    "start": "1765039",
    "end": "1771679"
  },
  {
    "text": "in docker 21. so key takeaways here well read the",
    "start": "1771679",
    "end": "1779200"
  },
  {
    "text": "manuals the capabilities manual page is really excellent contains a lot of detail a lot of",
    "start": "1779200",
    "end": "1784320"
  },
  {
    "text": "explanations and allows you to reason about what is happening with respect to capabilities in great detail also read the get capman page",
    "start": "1784320",
    "end": "1791840"
  },
  {
    "text": "this is the one that we missed initially where we didn't see the dash n option which would have been really useful",
    "start": "1791840",
    "end": "1797679"
  },
  {
    "text": "and finally reproducible build yes by all means but to within bugs so there we have it",
    "start": "1797679",
    "end": "1806399"
  },
  {
    "text": "and just to conclude then um kubernetes does an awful lot of heavy",
    "start": "1806399",
    "end": "1811600"
  },
  {
    "text": "lifting for us um and all the associated uh software around it the container",
    "start": "1811600",
    "end": "1817600"
  },
  {
    "text": "runtimes the container network interfaces um however the bigger your organization",
    "start": "1817600",
    "end": "1823520"
  },
  {
    "text": "grows the more strange issues you are going to encounter and these will challenge you to go deep in your understanding",
    "start": "1823520",
    "end": "1829520"
  },
  {
    "text": "of the of kubernetes and the overall ecosystem but it is extremely interesting to do so",
    "start": "1829520",
    "end": "1836080"
  },
  {
    "text": "we have maybe a strange definition of fun but we definitely like digging into these kinds of issues",
    "start": "1836080",
    "end": "1841600"
  },
  {
    "text": "we think the debugging also teaches us an awful lot about the the platform and that overall it allows",
    "start": "1841600",
    "end": "1847679"
  },
  {
    "text": "us to give our users a better experience because we're able to help them better because we know the platform well",
    "start": "1847679",
    "end": "1854240"
  },
  {
    "text": "and it's not easy definitely but it is feasible in particular because here we are",
    "start": "1854240",
    "end": "1859360"
  },
  {
    "text": "dealing with open source and so we're able to dig through absolutely everything but it is definitely worth it and we",
    "start": "1859360",
    "end": "1865519"
  },
  {
    "text": "find it's very rewarding so thank you for listening watching and",
    "start": "1865519",
    "end": "1871519"
  },
  {
    "text": "we hope to see you one day in person again thank you very much",
    "start": "1871519",
    "end": "1878240"
  }
]