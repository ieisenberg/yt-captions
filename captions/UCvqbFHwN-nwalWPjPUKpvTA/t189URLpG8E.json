[
  {
    "text": "alright let's get started hello and welcome to scheduling deep-dive this is",
    "start": "50",
    "end": "6420"
  },
  {
    "text": "the last talk of the conference and I'm surprised that you're still here but thank you for attending my name is Bobby",
    "start": "6420",
    "end": "12929"
  },
  {
    "text": "salamat I am one of the sig leads of sig scheduling the other sig is Klaus who is",
    "start": "12929",
    "end": "19199"
  },
  {
    "text": "here with us so today I'm going to talk",
    "start": "19199",
    "end": "25500"
  },
  {
    "text": "about the scheduling and schedule and recent developments in the city in the scheduler but before starting how many",
    "start": "25500",
    "end": "33450"
  },
  {
    "text": "of you by show of hands think that the scheduler is boring",
    "start": "33450",
    "end": "39649"
  },
  {
    "text": "nobody even you guys you guys are at Google you should believe that scheduler",
    "start": "40340",
    "end": "46440"
  },
  {
    "text": "is boring in that case it means that we haven't done our job well so so I Google",
    "start": "46440",
    "end": "52110"
  },
  {
    "text": "we believe that infrastructure should be boring basically you should not think about it you should not see it and if",
    "start": "52110",
    "end": "59219"
  },
  {
    "text": "it's really the case that you think that scheduler is not boring then means that we haven't done our job well but all",
    "start": "59219",
    "end": "66479"
  },
  {
    "text": "right how many of you think the schedule is good kubernetes the scheduler is good does whatever you want to do alright so",
    "start": "66479",
    "end": "74369"
  },
  {
    "text": "you're a self-contradicting Bunch but ok alright so disclaimer we are going to go",
    "start": "74369",
    "end": "80580"
  },
  {
    "text": "fast because sick deep dives are meant to be more like Q&A so I'm gonna go kind",
    "start": "80580",
    "end": "89909"
  },
  {
    "text": "of fast over over what scheduler is and what it does and recent development scheduler and then given ample time for",
    "start": "89909",
    "end": "97229"
  },
  {
    "text": "Q&A although or not that many people here in the room so at least I will give you a little bit more time to enjoy",
    "start": "97229",
    "end": "102659"
  },
  {
    "text": "viewing the city and seeing the things around here so what is the scheduler kubernetes the scheduler is this",
    "start": "102659",
    "end": "108990"
  },
  {
    "text": "component that usually runs on the master node it doesn't have to but it what we normally we run it on a master",
    "start": "108990",
    "end": "114030"
  },
  {
    "text": "node and is responsible for placing pods on nodes it does not concern itself with",
    "start": "114030",
    "end": "120299"
  },
  {
    "text": "the lifecycle management of parts what it means is that it doesn't create parts for you it does not care about",
    "start": "120299",
    "end": "126479"
  },
  {
    "text": "terminating parts basically there's normally terminate parts only a particular scenario where for",
    "start": "126479",
    "end": "132750"
  },
  {
    "text": "scheduling it doesn't find enough resources in a cluster or nodes in a cluster in that case it performs",
    "start": "132750",
    "end": "137970"
  },
  {
    "text": "preemption that's the only case that it terminates what but otherwise it does not concern itself with lifecycle of",
    "start": "137970",
    "end": "144600"
  },
  {
    "text": "pods another thing is that the scheduler does not do any temporal decisions",
    "start": "144600",
    "end": "151110"
  },
  {
    "text": "basically does not tell any nodes how many seconds or minutes it should run a",
    "start": "151110",
    "end": "156570"
  },
  {
    "text": "pod it just places pods on those nodes so for for that reason the scheduler might be a little bit of a confusing",
    "start": "156570",
    "end": "162510"
  },
  {
    "text": "name but it's probably better than Placer so we go we do scheduler it",
    "start": "162510",
    "end": "170820"
  },
  {
    "text": "supports a whole slew of features some of them are here for example it supports",
    "start": "170820",
    "end": "177180"
  },
  {
    "text": "checking my slides are online is I need to take pictures you can just download the PDF from schedule that makes your",
    "start": "177180",
    "end": "183330"
  },
  {
    "text": "life a little easier so it for example checks node resources it does someone do",
    "start": "183330",
    "end": "190440"
  },
  {
    "text": "stuff like spelling pods among nodes or checks affinity anti affinity for nodes",
    "start": "190440",
    "end": "198060"
  },
  {
    "text": "and also sorry for pods and affinity towards notes like notes selector and",
    "start": "198060",
    "end": "203250"
  },
  {
    "text": "stuff like that supports the Saints and toleration these are this is just a list you can take a look there or lots of",
    "start": "203250",
    "end": "208890"
  },
  {
    "text": "documentation on kubernetes that IO about each one of these features you might already be familiar with some of",
    "start": "208890",
    "end": "215190"
  },
  {
    "text": "those I'm not gonna go into any of those in depth but if feel free to ask questions about them towards the end so",
    "start": "215190",
    "end": "222870"
  },
  {
    "text": "what are recent developments in the scheduler one of the important things that we've done is improving performance",
    "start": "222870",
    "end": "229380"
  },
  {
    "text": "of the scheduler and this is a big effort to make the scheduler more scalable for larger clusters so we have",
    "start": "229380",
    "end": "236310"
  },
  {
    "text": "been able to achieve a hundred parts per second now in 4,000 node clusters 5,000",
    "start": "236310",
    "end": "241770"
  },
  {
    "text": "no it was like a goal that we set for ourselves to reach to and get to a",
    "start": "241770",
    "end": "247050"
  },
  {
    "text": "reasonable performance for some of these clusters with higher churn so if your",
    "start": "247050",
    "end": "253320"
  },
  {
    "text": "cluster is a smaller it's very likely that kubernetes the scheduler can achieve a lot more than 100 parts per",
    "start": "253320",
    "end": "258750"
  },
  {
    "text": "second so this is a good news for those who run large clusters and this improvement",
    "start": "258750",
    "end": "266980"
  },
  {
    "text": "most of these improvements are already released they are available in 114 and",
    "start": "266980",
    "end": "272950"
  },
  {
    "text": "even older versions maybe they are not like all 3x maybe 2x is available in",
    "start": "272950",
    "end": "278800"
  },
  {
    "text": "like 113 and 112 if you're using those and if you are using like 110 and older we recommend to upgrade to a little bit",
    "start": "278800",
    "end": "286060"
  },
  {
    "text": "newer version of kubernetes if you need performance another important performance optimisation that we've made",
    "start": "286060",
    "end": "292420"
  },
  {
    "text": "is to interpret affinity an anti affinity with Ritchie we have received a",
    "start": "292420",
    "end": "297540"
  },
  {
    "text": "lot of improvement here so inter pod affinity Anand - it was super slow like",
    "start": "297540",
    "end": "302680"
  },
  {
    "text": "a thousand times slower than many other features of the scheduler so if you used affinity and anti affinity you couldn't",
    "start": "302680",
    "end": "309400"
  },
  {
    "text": "probably in fact we had written this explicitly on kubernetes that IO that if",
    "start": "309400",
    "end": "314410"
  },
  {
    "text": "you have any cluster larger than a couple hundred nodes or so do not use an affinity on or anti affinity because",
    "start": "314410",
    "end": "320050"
  },
  {
    "text": "your pods are not going to get a scheduled so a single part was taking like 600 milliseconds close to a second",
    "start": "320050",
    "end": "326320"
  },
  {
    "text": "to get scheduled if you use this in slightly larger clusters but now we have",
    "start": "326320",
    "end": "331420"
  },
  {
    "text": "had like close like 200 X performance improvement so we are within like 4 or 5",
    "start": "331420",
    "end": "337090"
  },
  {
    "text": "X or maybe like 3x slower than other no other features of kubernetes so this has",
    "start": "337090",
    "end": "344800"
  },
  {
    "text": "been a huge improvement most of these are available I believe in 114 and there is a one more improvement",
    "start": "344800",
    "end": "354190"
  },
  {
    "text": "which is another like 1/2 X ish in 1 115",
    "start": "354190",
    "end": "359500"
  },
  {
    "text": "which is the upcoming release so if you use 114 you will enjoy most of these improvements and another important thing",
    "start": "359500",
    "end": "367390"
  },
  {
    "text": "is that we have graduated pod priority and preemption to staple in 114 so now",
    "start": "367390",
    "end": "373990"
  },
  {
    "text": "you can use this feature with ease of mind we are not going to advocate it soon or change it in an backward",
    "start": "373990",
    "end": "380170"
  },
  {
    "text": "incompatible way now what are the plan features so an important plan feature is",
    "start": "380170",
    "end": "387250"
  },
  {
    "text": "even par spreading but why do we need this feature we have already had anti affinity right so anti affinity",
    "start": "387250",
    "end": "394580"
  },
  {
    "text": "was a way to say I don't want my part or multiple of my parts to be scheduled",
    "start": "394580",
    "end": "399920"
  },
  {
    "text": "together on the same failure domain a failure domain for example is a zone or in or a node or could be a any group of",
    "start": "399920",
    "end": "406040"
  },
  {
    "text": "part that you can arbitrarily create by placing the same label on those parts so",
    "start": "406040",
    "end": "411650"
  },
  {
    "text": "you could say that I do not want my parts to be placed together on the same failure domain but what was the problem",
    "start": "411650",
    "end": "419180"
  },
  {
    "text": "with anti affinity the problem was that hard part anti affinity basically anti-fan it has a hard and a soft",
    "start": "419180",
    "end": "426110"
  },
  {
    "text": "basically rule hard means that do not schedule if you cannot satisfy this and",
    "start": "426110",
    "end": "431680"
  },
  {
    "text": "soft means try to try to basically honor",
    "start": "431680",
    "end": "436730"
  },
  {
    "text": "this request but if you cannot go ahead and schedule the problem with soft is",
    "start": "436730",
    "end": "441860"
  },
  {
    "text": "that if you use it with pod autoscaler potolsky does not care about these preferences so when it wants to",
    "start": "441860",
    "end": "448370"
  },
  {
    "text": "you guys can correct me if I'm wrong but then it very basically wants to scale",
    "start": "448370",
    "end": "454730"
  },
  {
    "text": "down clusters and it sees that oh you only have preference it goes ahead and",
    "start": "454730",
    "end": "459830"
  },
  {
    "text": "scales down right so what happens in those cases is that you have preference so your paws will be scheduled on the",
    "start": "459830",
    "end": "466850"
  },
  {
    "text": "same notes for example if you want it to spread it'll just put schedule remove some of those parts from the nose then",
    "start": "466850",
    "end": "473780"
  },
  {
    "text": "remove the note from the cluster this is done by the autoscaler and then schedule goes and picks that part puts it on one",
    "start": "473780",
    "end": "480020"
  },
  {
    "text": "of the existing notes so some people didn't want to use soft for that reason so they wanted to go with hard pattern",
    "start": "480020",
    "end": "485600"
  },
  {
    "text": "tie affinity but as you can see here if for example you have four pods these",
    "start": "485600",
    "end": "491330"
  },
  {
    "text": "blue circle things are our pods and if you wanted to place them on two zones",
    "start": "491330",
    "end": "497990"
  },
  {
    "text": "let's say two of them could not get them scheduled because you know you said an anti affinity in a zone towards other",
    "start": "497990",
    "end": "506360"
  },
  {
    "text": "pods or other similar parts at least so two of them would get a schedule and the",
    "start": "506360",
    "end": "511430"
  },
  {
    "text": "other two wouldn't so part of schedule even part spreading basically allows you",
    "start": "511430",
    "end": "517070"
  },
  {
    "text": "to say that I don't care about how many parts you want to put on the same zone",
    "start": "517070",
    "end": "525089"
  },
  {
    "text": "as long as or an F fitness in the same failure domain I just want you to spread",
    "start": "525089",
    "end": "530100"
  },
  {
    "text": "them evenly among whatever failure domain that I have for example I have four four pods and I have two zones",
    "start": "530100",
    "end": "536689"
  },
  {
    "text": "schedule automatically knows that it has to put like two pods per zone we also",
    "start": "536689",
    "end": "542579"
  },
  {
    "text": "support as a part of this we also support specifying imbalance between",
    "start": "542579",
    "end": "547949"
  },
  {
    "text": "these failure domains for example you say I want to tolerate two three four",
    "start": "547949",
    "end": "554269"
  },
  {
    "text": "imbalance of pots so basically what it means so for example two imbalance means that if a zone has two parts the other",
    "start": "554269",
    "end": "560639"
  },
  {
    "text": "one has four parts okay what I don't want zone to have two parts if the other zone",
    "start": "560639",
    "end": "565800"
  },
  {
    "text": "has like five pots only two parts of imbalance is fine so it supports all of",
    "start": "565800",
    "end": "574110"
  },
  {
    "text": "that this is this is planned for 115 but there is a chance that it may not make",
    "start": "574110",
    "end": "581490"
  },
  {
    "text": "it to 115 because we are very close to the code freeze and some of the PRS are not married yet the other feature and",
    "start": "581490",
    "end": "589019"
  },
  {
    "text": "that we are we are actually working on his pod schedule scheduling framework so",
    "start": "589019",
    "end": "594480"
  },
  {
    "text": "the scheduling framework is an attempt to make this scheduling architecture a",
    "start": "594480",
    "end": "600569"
  },
  {
    "text": "pluggable one currently the scheduler is a monolithic piece of code any changes",
    "start": "600569",
    "end": "606149"
  },
  {
    "text": "that we make basically goes to the guts of the schedule to the core of the scheduler mostly and with the addition",
    "start": "606149",
    "end": "613889"
  },
  {
    "text": "of new more complex features we felt like the scheduler is becoming more and more complex and at times it's hard to",
    "start": "613889",
    "end": "621329"
  },
  {
    "text": "maintain so we are introducing this new architecture where all these features will become plugins for this scheduler",
    "start": "621329",
    "end": "626850"
  },
  {
    "text": "it has two benefits for us it makes maintaining the scheduler easier for you guys or some of you guys it makes it",
    "start": "626850",
    "end": "633990"
  },
  {
    "text": "easier to customize the scheduler so you can build plug-ins for this scheduler and makes these plugins can be merged",
    "start": "633990",
    "end": "640170"
  },
  {
    "text": "with upstream easily and then also supports a sophisticated conflict that",
    "start": "640170",
    "end": "647040"
  },
  {
    "text": "you can basically pick and choose all these plugins that you want to enable or disable and configure them another",
    "start": "647040",
    "end": "655319"
  },
  {
    "text": "effort is gang scheduling gang scheduling is basically the idea that you have a series of pods",
    "start": "655319",
    "end": "660599"
  },
  {
    "text": "or a group of pods you want to say either you schedule all of these parts or do not escape do any of them this is",
    "start": "660599",
    "end": "667349"
  },
  {
    "text": "actually a feature that is oftentimes required with HPC workloads or batch processing many times you have like a",
    "start": "667349",
    "end": "673999"
  },
  {
    "text": "master or masters that need a bunch of workers if these masters are scheduled",
    "start": "673999",
    "end": "680009"
  },
  {
    "text": "and no worker can be scheduled those masters will just sit there wasting your resources and are not making any",
    "start": "680009",
    "end": "685859"
  },
  {
    "text": "progress similarly if for example workers are only scheduled you they won't do any job",
    "start": "685859",
    "end": "691259"
  },
  {
    "text": "so basically you can specify a group of parts that they should all be scheduled",
    "start": "691259",
    "end": "696299"
  },
  {
    "text": "together and if some of them cannot be scheduled then don't schedule any of these parts so this is also being worked",
    "start": "696299",
    "end": "702539"
  },
  {
    "text": "on it's available in head incubator project called Hugh patch if you have been in in",
    "start": "702539",
    "end": "709889"
  },
  {
    "text": "our contributor summit on on Monday you might have already heard of it there are",
    "start": "709889",
    "end": "717599"
  },
  {
    "text": "also some of these longer-term projects that we have in mind we know that kubernetes the scheduler is not great for running batch workloads",
    "start": "717599",
    "end": "724559"
  },
  {
    "text": "there are a lot of other batch schedulers like new yarn or slur more similar schedulers that have been around",
    "start": "724559",
    "end": "731009"
  },
  {
    "text": "in the industry they do a lot better job for like lifecycle management of patch",
    "start": "731009",
    "end": "736079"
  },
  {
    "text": "workloads or like yank scheduling stuff like this we we know that our kubernetes",
    "start": "736079",
    "end": "742199"
  },
  {
    "text": "scheduler is not really great for those kind of workers kubernetes from the beginning has been mostly designed for",
    "start": "742199",
    "end": "748229"
  },
  {
    "text": "running you know micro services as opposed to batch jobs but we're working",
    "start": "748229",
    "end": "753689"
  },
  {
    "text": "on this there are a number of these projects that we would like to add to make kubernetes scheduler better for",
    "start": "753689",
    "end": "760349"
  },
  {
    "text": "batch workloads and with the help of gangue with the help of the scheduling framework we hope that we can deliver",
    "start": "760349",
    "end": "766709"
  },
  {
    "text": "those soon another incubator project that we have is the scheduler so the",
    "start": "766709",
    "end": "773039"
  },
  {
    "text": "idea here is that basically when this scheduler kubernetes the scheduler only",
    "start": "773039",
    "end": "781139"
  },
  {
    "text": "cares about the state of cluster at the time that it schedules a new part afterwards it doesn't care so for",
    "start": "781139",
    "end": "788999"
  },
  {
    "text": "example if you set anti affinity or affinity affinity is a better example actually so",
    "start": "788999",
    "end": "794700"
  },
  {
    "text": "let's say you put affinity on some of your parts to put two parts for example",
    "start": "794700",
    "end": "800280"
  },
  {
    "text": "together on it on the same note but suddenly one of the parts for example terminates and disappears from that note",
    "start": "800280",
    "end": "807990"
  },
  {
    "text": "kubernetes the scheduler does not care at all that this part this affinity rule is no longer satisfied one of the part",
    "start": "807990",
    "end": "814470"
  },
  {
    "text": "is gone this one of them has remained so it doesn't care these scheduler is an attempt to reapply",
    "start": "814470",
    "end": "821820"
  },
  {
    "text": "some of these scheduling constraints at runtime and for the parts that are already running it supports a number of",
    "start": "821820",
    "end": "829020"
  },
  {
    "text": "features already for example it's supposed to be balancing node resources distributes pods of collections when the",
    "start": "829020",
    "end": "837030"
  },
  {
    "text": "number of nodes in the cluster change applies into a pod affinity an anti",
    "start": "837030",
    "end": "842220"
  },
  {
    "text": "affinity applies no definitely correct me if I'm missing anything wrong and",
    "start": "842220",
    "end": "849090"
  },
  {
    "text": "it's as I said it's enough it's available in an incubator project that's",
    "start": "849090",
    "end": "854670"
  },
  {
    "text": "all okay I did a relatively good job by finishing everything in almost 15",
    "start": "854670",
    "end": "860910"
  },
  {
    "text": "minutes so if you have questions or comments we are here to hopefully answer",
    "start": "860910",
    "end": "869280"
  },
  {
    "text": "that some of those and if you want to find us you can go to our home page for",
    "start": "869280",
    "end": "875280"
  },
  {
    "text": "kubernetes scheduler or cg scheduling or our slack channel you can subscribe to",
    "start": "875280",
    "end": "881670"
  },
  {
    "text": "our mailing list or just send us messages or mention us on github any",
    "start": "881670",
    "end": "890430"
  },
  {
    "text": "questions yes let me give you the microphone",
    "start": "890430",
    "end": "894320"
  },
  {
    "text": "I'm not sure if this is on but yeah",
    "start": "897950",
    "end": "903350"
  },
  {
    "text": "thanks it was a question about the even pod spreading so how common well the",
    "start": "905270",
    "end": "912570"
  },
  {
    "text": "term was where you say you know the set number out with imbalance for example",
    "start": "912570",
    "end": "919020"
  },
  {
    "text": "yeah yeah so let's say you want that to be zero so you want exactly the same number on each node zone what's the",
    "start": "919020",
    "end": "925860"
  },
  {
    "text": "behavior and if say no fails and so that imbalance comes does this scheduler then automatically reschedule",
    "start": "925860",
    "end": "932720"
  },
  {
    "text": "pod to be created or so so basically again our kubernetes scheduler does not",
    "start": "932720",
    "end": "940230"
  },
  {
    "text": "care about the run time it only cares at the time of the scheduling so right so one of your for example you have you",
    "start": "940230",
    "end": "947070"
  },
  {
    "text": "have two zones which are perfectly balanced but suddenly one of the pods from one zone dies and goes away right",
    "start": "947070",
    "end": "953040"
  },
  {
    "text": "so schedule is not gonna do anything unless at the same part for example what",
    "start": "953040",
    "end": "958470"
  },
  {
    "text": "not the same exact but if the parties for example belong belongs to a collection let's say a replica said then",
    "start": "958470",
    "end": "964110"
  },
  {
    "text": "the replica said controller is likely to create a rep replacement for that part so when that replacement comes up then",
    "start": "964110",
    "end": "970500"
  },
  {
    "text": "the scheduler puts it on that zone which is missing a pod with with so many",
    "start": "970500",
    "end": "978810"
  },
  {
    "text": "conditions that the scheduler needs to take into account when scheduling a pod",
    "start": "978810",
    "end": "984290"
  },
  {
    "text": "can you maybe say a few words how do you test and assert that the pod is all",
    "start": "984290",
    "end": "991650"
  },
  {
    "text": "things considered is being scheduled in the right place like what is the tech testing framework right so generally",
    "start": "991650",
    "end": "1001340"
  },
  {
    "text": "scheduler or many other components of kubernetes have three main types of",
    "start": "1001340",
    "end": "1007490"
  },
  {
    "text": "tests one is unit test which tests like actual functions of you know smaller",
    "start": "1007490",
    "end": "1012680"
  },
  {
    "text": "units of code which are the usual functions of the code that's one one",
    "start": "1012680",
    "end": "1018560"
  },
  {
    "text": "thing that we do but it's really not enough for testing everything right so most of the tests of schedule",
    "start": "1018560",
    "end": "1024290"
  },
  {
    "text": "integration tests as we call them in integration tests kubernetes generations as we have a framework for integration",
    "start": "1024290",
    "end": "1030589"
  },
  {
    "text": "where it brings up an API server and scheduler and all in a single process basically it runs a net CD on the side",
    "start": "1030589",
    "end": "1036740"
  },
  {
    "text": "it brings up all of those and then we have written a number of tests - for example check a lot of these features",
    "start": "1036740",
    "end": "1042949"
  },
  {
    "text": "each one of these features have usually at least one sometimes many more tests",
    "start": "1042949",
    "end": "1048260"
  },
  {
    "text": "to check various scenarios and also we have end-to-end test which runs in a",
    "start": "1048260",
    "end": "1053990"
  },
  {
    "text": "real cluster these are usually the tests that we really want to make sure that end-to-end admission and scheduling and",
    "start": "1053990",
    "end": "1062779"
  },
  {
    "text": "everything works and nodes actually run these workloads so we have these all",
    "start": "1062779",
    "end": "1068270"
  },
  {
    "text": "these three tests to make sure that scheduler is reasonably reliable have",
    "start": "1068270",
    "end": "1078409"
  },
  {
    "text": "you noticed any particular they were not",
    "start": "1078409",
    "end": "1109580"
  },
  {
    "text": "too bad actually because we've built a library that you know and of they're all",
    "start": "1109580",
    "end": "1114590"
  },
  {
    "text": "functions that we can pretty much lower you can kind of like create some a lot",
    "start": "1114590",
    "end": "1121250"
  },
  {
    "text": "of these scenarios relatively easily in our test environments yeah on the dish",
    "start": "1121250",
    "end": "1129590"
  },
  {
    "text": "Angela I know there's a future sort of thing but the the previous talk was",
    "start": "1129590",
    "end": "1135710"
  },
  {
    "text": "about the cluster autoscaler and the concept of scaling in so removing a node from the cluster see then the scheduler",
    "start": "1135710",
    "end": "1143090"
  },
  {
    "text": "needing to kind of bin pack their pods how often does that will that they said",
    "start": "1143090",
    "end": "1149779"
  },
  {
    "text": "you know to run is it something that gets triggered or is it kind of a try to go in tasks you so I think it's up to",
    "start": "1149779",
    "end": "1156289"
  },
  {
    "text": "you but Ravi do you do you know the answer to this question how frequently the scheduler",
    "start": "1156289",
    "end": "1164590"
  },
  {
    "text": "cron job and then set the schedule to whatever you want it to be so it's",
    "start": "1168910",
    "end": "1174320"
  },
  {
    "text": "technically just a power and then create a job around it with appropriate service account that should work fine thank you",
    "start": "1174320",
    "end": "1183700"
  },
  {
    "text": "do you have any plans to introduce some kind of dependencies between pots don't",
    "start": "1185740",
    "end": "1191390"
  },
  {
    "text": "schedule a before B yeah so actually if I don't know if you noticed or not as a part of supporting batch workloads this",
    "start": "1191390",
    "end": "1198230"
  },
  {
    "text": "is actually a very common thing for batch processing right so a lot of batch processing pipelines are really like",
    "start": "1198230",
    "end": "1203720"
  },
  {
    "text": "pipelines so they have stages one stage should finish before the next stage can you start yes we are aware of that",
    "start": "1203720",
    "end": "1209780"
  },
  {
    "text": "problem and we are working on supporting dependencies clause could probably you know I as far as I know we have we have",
    "start": "1209780",
    "end": "1217730"
  },
  {
    "text": "this plan to add like dependency management and dependency to basically",
    "start": "1217730",
    "end": "1223010"
  },
  {
    "text": "batches right so we have that in mind I don't I don't know if we have any particular a silly deadline to deliver",
    "start": "1223010",
    "end": "1230780"
  },
  {
    "text": "this feature we don't try it you cannot really make any promises at this point",
    "start": "1230780",
    "end": "1235910"
  },
  {
    "text": "but it's sink away you still focus on some some policy has meant to the batch",
    "start": "1235910",
    "end": "1242720"
  },
  {
    "text": "workload and we do have some pipeline for the job with dependency and the task",
    "start": "1242720",
    "end": "1248000"
  },
  {
    "text": "with it dependency for this for this account especially for the genomic workload I'm not sure what kind of kind",
    "start": "1248000",
    "end": "1255920"
  },
  {
    "text": "of you know for genomic they need to do some day to clarify DNA DNA test or",
    "start": "1255920",
    "end": "1263600"
  },
  {
    "text": "something like that yeah but I we don't have generate with how timeline",
    "start": "1263600",
    "end": "1269540"
  },
  {
    "text": "for this part but they do you know our pipeline our first day is to try to some basic concept such as draw queue we",
    "start": "1269540",
    "end": "1277430"
  },
  {
    "text": "don't have fun and finalize such kind of concept right now right I think this in",
    "start": "1277430",
    "end": "1284090"
  },
  {
    "text": "your pipeline if there are some special Department we can we can",
    "start": "1284090",
    "end": "1289850"
  },
  {
    "text": "have some easy one have discussed in our meeting yeah one of them one of the things is that we really try to listen",
    "start": "1289850",
    "end": "1296900"
  },
  {
    "text": "to feedback from our users and prioritize based on the number of feedback that we receive from our users",
    "start": "1296900",
    "end": "1303260"
  },
  {
    "text": "and customers if you have a real need please say something you know create an",
    "start": "1303260",
    "end": "1308300"
  },
  {
    "text": "issue as as loud as possible you know usually we listen to the louder customers more so don't be shy",
    "start": "1308300",
    "end": "1316880"
  },
  {
    "text": "yeah create an issue go on it's like oh my workers are not running I have like a cluster of odd thousand no isn't yeah I",
    "start": "1316880",
    "end": "1323210"
  },
  {
    "text": "would bring a million dollars to DK team and we immediately schedule your work of",
    "start": "1323210",
    "end": "1332090"
  },
  {
    "text": "course I was joking but it could happen in practice too sometimes I have a",
    "start": "1332090",
    "end": "1341240"
  },
  {
    "text": "question about failure modes for even pot spreading so let's assume that I have free failure domains and I don't",
    "start": "1341240",
    "end": "1347750"
  },
  {
    "text": "know 100 pots so they will be spread like 30 something on each and one of the",
    "start": "1347750",
    "end": "1353560"
  },
  {
    "text": "failure domains fails so a zone is gone so now probably my deployment or replica",
    "start": "1353560",
    "end": "1359540"
  },
  {
    "text": "set or whatever control the pots will add to 30 pots and what would scheduler",
    "start": "1359540",
    "end": "1364670"
  },
  {
    "text": "do would it schedule it 50/50 on the remaining two or will they become pending or is it in my control that's a",
    "start": "1364670",
    "end": "1372200"
  },
  {
    "text": "good question so I mean I don't know what you mean by in your control but basically scheduler will try to spread",
    "start": "1372200",
    "end": "1380330"
  },
  {
    "text": "all of those among the remaining failure domains for example remaining nodes or remaining zones whatever it is and try",
    "start": "1380330",
    "end": "1386630"
  },
  {
    "text": "tries to rebalance all those existing ones so that if you have for example let's say you have thirteen let's say",
    "start": "1386630",
    "end": "1393020"
  },
  {
    "text": "you have sixty nodes and you have one pot on each and suddenly 30 of them disappear for example right so then you",
    "start": "1393020",
    "end": "1400760"
  },
  {
    "text": "have like 30 pending pots after like controllers create another 30 pots for you so the scheduler looks at looks at",
    "start": "1400760",
    "end": "1408650"
  },
  {
    "text": "these nodes like 30 nodes are available after the planning part and I need to spread evenly we'll put one pot or no",
    "start": "1408650",
    "end": "1415700"
  },
  {
    "text": "and basically for an extra pot or not any other questions",
    "start": "1415700",
    "end": "1423230"
  },
  {
    "text": "you mentioned at the start that scheduling should be boring and that because no one hope yes no one raised",
    "start": "1435080",
    "end": "1442490"
  },
  {
    "text": "their hand so my question would be is the reason it's not boring for us it is",
    "start": "1442490",
    "end": "1447950"
  },
  {
    "text": "the default kind of spread the load and we see a lot of workloads where we have small pots we buy a small amount of",
    "start": "1447950",
    "end": "1456110"
  },
  {
    "text": "resources and what's requiring the full note and then we very quickly see that we can can't schedule the large",
    "start": "1456110",
    "end": "1463910"
  },
  {
    "text": "workloads with scheduling yeah so we have to dig it into the predicate and understand what they have to put there",
    "start": "1463910",
    "end": "1469730"
  },
  {
    "text": "I'm not sure if this is has a solution so that people don't have to deal with",
    "start": "1469730",
    "end": "1474770"
  },
  {
    "text": "this it is actually a problem you know basically your problem oftentimes is",
    "start": "1474770",
    "end": "1480170"
  },
  {
    "text": "referred to as fragmentation you know in like academic terms so basically it",
    "start": "1480170",
    "end": "1485450"
  },
  {
    "text": "means that you have a bunch of nodes each one of them has a small amount of",
    "start": "1485450",
    "end": "1491270"
  },
  {
    "text": "free resources but each one of them has the three sources on each node is is",
    "start": "1491270",
    "end": "1497600"
  },
  {
    "text": "smaller than your largest parts right so this is something that in many cases for",
    "start": "1497600",
    "end": "1505010"
  },
  {
    "text": "example my background is in like cluster management i was working on borg borg is",
    "start": "1505010",
    "end": "1511070"
  },
  {
    "text": "like the cluster management at Google uses so in bored we have something similar 2d scheduler that we were",
    "start": "1511070",
    "end": "1516950"
  },
  {
    "text": "talking about in work terminology it's called three scheduler but anyway it's",
    "start": "1516950",
    "end": "1522590"
  },
  {
    "text": "the same thing so in those cases usually reschedule or D schedule kicks in you",
    "start": "1522590",
    "end": "1528680"
  },
  {
    "text": "know finds out that for example there are pending parts and all I see fragmentation and kicks in remove some",
    "start": "1528680",
    "end": "1534890"
  },
  {
    "text": "of the parts so that's one thing that can happen by D schedule another possibility in with kubernetes that we",
    "start": "1534890",
    "end": "1543620"
  },
  {
    "text": "have today if this may not apply to everybody but if you have different",
    "start": "1543620",
    "end": "1549980"
  },
  {
    "text": "priorities for these parts and you know your larger parts happen to have higher priority then then preemption will take",
    "start": "1549980",
    "end": "1557390"
  },
  {
    "text": "care of that for you so it will basically remove some of those smaller parts and chances are that those smaller",
    "start": "1557390",
    "end": "1564860"
  },
  {
    "text": "parts fit on other nodes which have like smaller amount of resources available and it",
    "start": "1564860",
    "end": "1570150"
  },
  {
    "text": "will schedule your larger pods on the freedoms alright any other questions can",
    "start": "1570150",
    "end": "1582390"
  },
  {
    "text": "you pass this is scale a problem or like",
    "start": "1582390",
    "end": "1589800"
  },
  {
    "text": "are they in the working like improving with truth but since it oh so say it's an again it's a scale a problem for Java",
    "start": "1589800",
    "end": "1595710"
  },
  {
    "text": "are we working on like increasing the performance for it I don't know if you",
    "start": "1595710",
    "end": "1600929"
  },
  {
    "text": "were here earlier in the talk but we said that we've improved performance of the scheduler by like three X so this is",
    "start": "1600929",
    "end": "1609210"
  },
  {
    "text": "this is relatively recent base if it has happened in the past six months so it's available in the more recent versions of",
    "start": "1609210",
    "end": "1615690"
  },
  {
    "text": "this scheduler but actually I was just before this talk I was talking to one of",
    "start": "1615690",
    "end": "1621360"
  },
  {
    "text": "our contributors who works at Alibaba Alibaba claims that they have been able",
    "start": "1621360",
    "end": "1626520"
  },
  {
    "text": "to run clusters kubernetes clusters with tens of thousands of parts sometimes",
    "start": "1626520",
    "end": "1631740"
  },
  {
    "text": "they apparently run like clusters with 30,000 calls or so and what they have",
    "start": "1631740",
    "end": "1637020"
  },
  {
    "text": "found is that the scheduler is oftentimes actually is not the bottleneck bottleneck is mostly on an",
    "start": "1637020",
    "end": "1642690"
  },
  {
    "text": "API server and HCD so they have had some patches to remove some of those bottlenecks and actually",
    "start": "1642690",
    "end": "1648630"
  },
  {
    "text": "that's how they achieve to reach to these like tens of thousands of pots per cluster but scalability is a complex",
    "start": "1648630",
    "end": "1656160"
  },
  {
    "text": "problem right so it's if you've been to any of these scalability talks folks from security and those who work on the",
    "start": "1656160",
    "end": "1663300"
  },
  {
    "text": "scalability can tell you more information but it's a complex problem it's not just about scheduler by itself",
    "start": "1663300",
    "end": "1670200"
  },
  {
    "text": "or about like the number of nodes in the cluster by it's it's a very multi-dimensional problem if you stretch",
    "start": "1670200",
    "end": "1679110"
  },
  {
    "text": "it from one side you have to compromise on some other sides basically for example if you want to run clusters of",
    "start": "1679110",
    "end": "1685470"
  },
  {
    "text": "many nodes probably you cannot also have many parts per node so you have to",
    "start": "1685470",
    "end": "1690720"
  },
  {
    "text": "basically tone down on some of those or maybe you cannot have like many attached volumes for all of those nodes but",
    "start": "1690720",
    "end": "1697650"
  },
  {
    "text": "overall what we have seen so far is the scheduled at this at this point of time",
    "start": "1697650",
    "end": "1703320"
  },
  {
    "text": "that we have all these improvements is not a major bottleneck in most of the clusters unless you have like particular",
    "start": "1703320",
    "end": "1710850"
  },
  {
    "text": "needs that for example I have a 12,000 north cluster and I wanna be able to",
    "start": "1710850",
    "end": "1717720"
  },
  {
    "text": "schedule a thousand parts per second yes then schedule is a problem but even if the scheduler was able to do a thousand",
    "start": "1717720",
    "end": "1724530"
  },
  {
    "text": "parts per second I would say still you run into another problem on the API server side because API server has to",
    "start": "1724530",
    "end": "1731190"
  },
  {
    "text": "update these parts at like a rate of a thousand parts per second an API server",
    "start": "1731190",
    "end": "1736440"
  },
  {
    "text": "wouldn't be able to do it so yeah there is more than just one problem to solve",
    "start": "1736440",
    "end": "1741750"
  },
  {
    "text": "here thank you",
    "start": "1741750",
    "end": "1747590"
  },
  {
    "text": "any other questions yes yeah so many questions security really even at the",
    "start": "1747590",
    "end": "1755280"
  },
  {
    "text": "start that the scheduler normally runs on the masters and obviously that's the default when what use cases do you see",
    "start": "1755280",
    "end": "1761310"
  },
  {
    "text": "where that's not okay I mean scheduler schedules not have to run on the master",
    "start": "1761310",
    "end": "1767220"
  },
  {
    "text": "node in fact some of our customers and users run custom schedulers on their regular nodes in the cluster as long as",
    "start": "1767220",
    "end": "1774300"
  },
  {
    "text": "it can communicate with the API server it can run on any node in the cluster no",
    "start": "1774300",
    "end": "1783030"
  },
  {
    "text": "no there's no specific use case and if you have some concerns about like",
    "start": "1783030",
    "end": "1788370"
  },
  {
    "text": "availability for example you know one like multiple schedules for some reason you can put some of these schedules on",
    "start": "1788370",
    "end": "1794760"
  },
  {
    "text": "other nodes in the cluster so any other questions I guess this is",
    "start": "1794760",
    "end": "1804590"
  },
  {
    "text": "officially the end of the conference for you guys I hope you enjoyed the conference and hopefully see some of you",
    "start": "1804590",
    "end": "1810740"
  },
  {
    "text": "folks in the future thank you very much [Applause] [Music]",
    "start": "1810740",
    "end": "1817750"
  }
]