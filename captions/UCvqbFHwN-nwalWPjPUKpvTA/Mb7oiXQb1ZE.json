[
  {
    "text": "welcome to the rook deep-dive session i'm travis nielsen one of the maintainer",
    "start": "210",
    "end": "5220"
  },
  {
    "text": "Jean the rook project I've been with the project since its I was originally created a couple years ago we open",
    "start": "5220",
    "end": "12300"
  },
  {
    "text": "sourced it at coop con Seattle when it was a little bit smaller it's amazing to see how much it's grown and the project",
    "start": "12300",
    "end": "19350"
  },
  {
    "text": "is grown and it's great to be a part of it earlier this year I joined the Red Hat team got to work with the the Ceph",
    "start": "19350",
    "end": "26160"
  },
  {
    "text": "team so all right so what is rook so to get an idea of who's in the audience so",
    "start": "26160",
    "end": "32790"
  },
  {
    "text": "who all was able to make it to the intro to rook a couple of days ago so a lot of",
    "start": "32790",
    "end": "38280"
  },
  {
    "text": "you that's good and Jared also he's here he gave it and who who has actually run",
    "start": "38280",
    "end": "44910"
  },
  {
    "text": "rook before in test or other cluster all right we got a lot of users great good",
    "start": "44910",
    "end": "50940"
  },
  {
    "text": "to see all of you here and who's running in production okay yes we do have a good",
    "start": "50940",
    "end": "55949"
  },
  {
    "text": "collection of production or near production users tomorrow okay perfect",
    "start": "55949",
    "end": "64280"
  },
  {
    "text": "those are the stories I love to hear that it's you know that it's working for you that the production is ready for it",
    "start": "64280",
    "end": "69930"
  },
  {
    "text": "and you know that you know earlier this year well September we graduated to be",
    "start": "69930",
    "end": "75509"
  },
  {
    "text": "an CN CF incubation project thanks to all of you the community and knowing",
    "start": "75509",
    "end": "82560"
  },
  {
    "text": "that it's working in production so what so what is rook as I've talked to people",
    "start": "82560",
    "end": "87689"
  },
  {
    "text": "in the booth the last few days I've kind of refined how I'm talking about it and sort of the one-liner I came up with is",
    "start": "87689",
    "end": "94680"
  },
  {
    "text": "so it's open source it's and it's a storage couldn't control plane and it's",
    "start": "94680",
    "end": "100470"
  },
  {
    "text": "for kubernetes so buy storage control plane what that means is it provides operators and CR DS to orchestrate or",
    "start": "100470",
    "end": "109020"
  },
  {
    "text": "otherwise configure and manage storage backends so we'll get into more what",
    "start": "109020",
    "end": "115560"
  },
  {
    "text": "that means but basically think about we don't provide the implementation for the storage we bring the storage platforms",
    "start": "115560",
    "end": "122759"
  },
  {
    "text": "to kubernetes in a kubernetes native way so I'll go briefly over this that you've",
    "start": "122759",
    "end": "128819"
  },
  {
    "text": "already heard in the intro section but so an operator like you heard in the keynote this morning the operator applies",
    "start": "128819",
    "end": "136520"
  },
  {
    "text": "desired state into your cluster so you say here's my desired state the operator",
    "start": "136520",
    "end": "142380"
  },
  {
    "text": "goes and makes it happen and the way you do that is with the CR DS let's see our DS or your standard",
    "start": "142380",
    "end": "148500"
  },
  {
    "text": "kubernetes the ml file it looks like any other kubernetes type and but you know",
    "start": "148500",
    "end": "156000"
  },
  {
    "text": "ruk defines those CR teas that are specific to the storage providers and so",
    "start": "156000",
    "end": "163950"
  },
  {
    "text": "in addition to that it look really is more than operators and and CR DS so a",
    "start": "163950",
    "end": "170220"
  },
  {
    "text": "framework and this framework is it's still and it's in its youth but we're",
    "start": "170220",
    "end": "175350"
  },
  {
    "text": "evolving it to make it even easier for storage providers to join the kubernetes",
    "start": "175350",
    "end": "180530"
  },
  {
    "text": "today's native landscape so testing efforts anyway all these things there's",
    "start": "180530",
    "end": "187680"
  },
  {
    "text": "a lot of very specific needs for each storage technology and among those you",
    "start": "187680",
    "end": "193920"
  },
  {
    "text": "know automated deployment bootstrapping configuration upgrading migration and every storage platform needs to do these",
    "start": "193920",
    "end": "200820"
  },
  {
    "text": "if you're going to trust them with your data so what are the storage providers",
    "start": "200820",
    "end": "206340"
  },
  {
    "text": "in Rooke today so we we have kind of what I'm thinking of as two categories of storage providers we have those that",
    "start": "206340",
    "end": "212910"
  },
  {
    "text": "provide the data platform itself you know block object and file storage so SEF Mineo NFS and they exempted Jeff s",
    "start": "212910",
    "end": "220799"
  },
  {
    "text": "fall into those that category the databases then are cockroach DB and",
    "start": "220799",
    "end": "227130"
  },
  {
    "text": "Cassandra which which might build on the other data platforms so if you want to",
    "start": "227130",
    "end": "234150"
  },
  {
    "text": "learn more about storage providers and adding adding them to rook Jared has a presentation this afternoon at 4:30",
    "start": "234150",
    "end": "239870"
  },
  {
    "text": "come come join that he has new code running I hear that I haven't even seen yet and then right after this session we",
    "start": "239870",
    "end": "247950"
  },
  {
    "text": "have a meet the maintainer x' at this main CN CF area in the in the booth hall",
    "start": "247950",
    "end": "255739"
  },
  {
    "text": "okay so for the deep dive what I wanted to really spend the time on is SEF being",
    "start": "255739",
    "end": "262680"
  },
  {
    "text": "so Steph was the first storage provider that that rook really set out to or get straight only recently we been",
    "start": "262680",
    "end": "269670"
  },
  {
    "text": "adding the others and creating this framework so SEF so who's familiar with",
    "start": "269670",
    "end": "275010"
  },
  {
    "text": "SEF a little bit or more okay a good good portion that crowd great I don't",
    "start": "275010",
    "end": "280680"
  },
  {
    "text": "need to explain the details of it though Steph has a number of demons they have to be orchestrated they need to be",
    "start": "280680",
    "end": "287370"
  },
  {
    "text": "running in and they need to be they need to be always running so that your data is available and safe so the the general",
    "start": "287370",
    "end": "295050"
  },
  {
    "text": "orchestration approach we took for rook was the operator it's going it's it's",
    "start": "295050",
    "end": "302520"
  },
  {
    "text": "very familiar with what's F needs to do so Steph needs to you know creates",
    "start": "302520",
    "end": "307890"
  },
  {
    "text": "ethics keys and generate it we need to create crush map we need to do other things to you know with placement groups",
    "start": "307890",
    "end": "313200"
  },
  {
    "text": "and if you've run rook but not SEF hopefully you haven't had to hear about any of those terms because they're you",
    "start": "313200",
    "end": "320370"
  },
  {
    "text": "know they're difficult to manage at the end of the day so rook takes care of those those for you the operator then",
    "start": "320370",
    "end": "327410"
  },
  {
    "text": "after talks to staff or Wallace talking to SEF it's creating kubernetes resources like deployments to manage",
    "start": "327410",
    "end": "334110"
  },
  {
    "text": "life cycle of each SEF demon you know doing things like creating Anette containers to generate the SEF kampf and",
    "start": "334110",
    "end": "342000"
  },
  {
    "text": "then running the SEF daemons directly with with a version of SEF that that you",
    "start": "342000",
    "end": "348330"
  },
  {
    "text": "want to run and I'll talk more about that versioning here soon and then the",
    "start": "348330",
    "end": "354240"
  },
  {
    "text": "last thing that the operator does is it really manages the health of the system if your SEF mons",
    "start": "354240",
    "end": "360240"
  },
  {
    "text": "go down kubernetes doesn't really know if you know if they're really down it",
    "start": "360240",
    "end": "366210"
  },
  {
    "text": "only knows if the pod stops so the operator does extra work to say hey are your SEF mons in Coram and if they're",
    "start": "366210",
    "end": "373860"
  },
  {
    "text": "not it'll take action to start up a new deployment and get a new man running okay so that's the approach start you",
    "start": "373860",
    "end": "381720"
  },
  {
    "text": "know configure your storage in a very storage provider specific way and keep",
    "start": "381720",
    "end": "387720"
  },
  {
    "text": "it healthy so what does a CRD look like so in our release our dot nine that we",
    "start": "387720",
    "end": "394680"
  },
  {
    "text": "just had a few days ago we declared the SEF CR DS as v1 so we're excited about",
    "start": "394680",
    "end": "399990"
  },
  {
    "text": "that that means that users are trusting it",
    "start": "399990",
    "end": "405330"
  },
  {
    "text": "while it was in beta in production even and we're yeah we're just excited that",
    "start": "405330",
    "end": "410790"
  },
  {
    "text": "that that's going to keep rolling out and that means you know they'll always be backward compatible upgradeable and",
    "start": "410790",
    "end": "416490"
  },
  {
    "text": "and we believe you know that that you can do what you need to with these well",
    "start": "416490",
    "end": "421920"
  },
  {
    "text": "be adding new features that doesn't mean they're locked in place that will be backward compatible always so just to",
    "start": "421920",
    "end": "430800"
  },
  {
    "text": "look at a few of these properties here at the CRT dated or host path here on",
    "start": "430800",
    "end": "436560"
  },
  {
    "text": "the right so what this is is some of the SEF demons require persistence its",
    "start": "436560",
    "end": "441630"
  },
  {
    "text": "storage of course if you're if your node reboots if it just goes down momentarily",
    "start": "441630",
    "end": "446730"
  },
  {
    "text": "and comes back you need to make sure that data is persisted so the host path is where we store you know basic",
    "start": "446730",
    "end": "455190"
  },
  {
    "text": "configuration and persistence of the metadata it can hold data as well but",
    "start": "455190",
    "end": "462630"
  },
  {
    "text": "the data is generally held on your raw block devices mounted on on your local",
    "start": "462630",
    "end": "468330"
  },
  {
    "text": "nodes the Ceph version let me skip over that one that I've got another slide for",
    "start": "468330",
    "end": "473670"
  },
  {
    "text": "that settings for dashboard do you want the Ceph dashboard enabled or not and",
    "start": "473670",
    "end": "480020"
  },
  {
    "text": "network yeah what kind of network are you running in do you need host network access or are you running on a virtual",
    "start": "480020",
    "end": "488100"
  },
  {
    "text": "network or with the kubernetes CNA so",
    "start": "488100",
    "end": "494430"
  },
  {
    "text": "the the versioning so in the the latest release one of the big changes we made",
    "start": "494430",
    "end": "499980"
  },
  {
    "text": "is that rook is no longer tied to a specific version of SEF so rook was tied",
    "start": "499980",
    "end": "506280"
  },
  {
    "text": "to Stef 12.2 which was luminous before this release but now you can say I want",
    "start": "506280",
    "end": "513750"
  },
  {
    "text": "to run rook V dot 9 o die 9 but I want to run SEF luminous or SEF mimic or SEF",
    "start": "513750",
    "end": "519719"
  },
  {
    "text": "Nautilus and so the version of SEF is independent of what rook is running this",
    "start": "519720",
    "end": "525270"
  },
  {
    "text": "is very important because people care a lot about their data plane they need to know it's safe and they need the admins",
    "start": "525270",
    "end": "532410"
  },
  {
    "text": "need to decide when they want up creating their data plane so that's the purpose of that separation so you can",
    "start": "532410",
    "end": "539370"
  },
  {
    "text": "control when to upgrade the data plane and yes you can up to upgrade",
    "start": "539370",
    "end": "545850"
  },
  {
    "text": "independently from rook and the support lifetime really then depends on the Ceph project not on the rook project for a",
    "start": "545850",
    "end": "552540"
  },
  {
    "text": "specific version of stuff the the images",
    "start": "552540",
    "end": "557550"
  },
  {
    "text": "then that you can run force F they're just in the Ceph docker hub repo and you",
    "start": "557550",
    "end": "564570"
  },
  {
    "text": "can go to the link there and see what what all tags are available alright so",
    "start": "564570",
    "end": "572490"
  },
  {
    "text": "what are the differences it's the operator actually behaves differently between each of these versions of stuff",
    "start": "572490",
    "end": "577740"
  },
  {
    "text": "so aluminous and just these are the main examples right now and over time this",
    "start": "577740",
    "end": "583260"
  },
  {
    "text": "difference these differences could grow in implementation but so right now with luminous the operator will start up the",
    "start": "583260",
    "end": "591660"
  },
  {
    "text": "SEF dashboard running on port and just on port 80 it's it's a read-only",
    "start": "591660",
    "end": "597300"
  },
  {
    "text": "dashboard it doesn't need anything special or less special in mimic then",
    "start": "597300",
    "end": "602370"
  },
  {
    "text": "now it's a configurable more feature-rich can dashboard in SEF it needs to run in",
    "start": "602370",
    "end": "608790"
  },
  {
    "text": "HTTP so rook will generate a self-signed cert on your behalf and generate an",
    "start": "608790",
    "end": "614070"
  },
  {
    "text": "admin password and store it somewhere safe for you so that you can have a working dashboard right when you launch",
    "start": "614070",
    "end": "621620"
  },
  {
    "text": "and then in Nautilus which is still pre-release it's you know it's planned",
    "start": "621620",
    "end": "628230"
  },
  {
    "text": "to be released maybe the next couple of months rook will allow you to run it we just",
    "start": "628230",
    "end": "634260"
  },
  {
    "text": "need this flag to say allow unsupported true and then we'll do things like",
    "start": "634260",
    "end": "640190"
  },
  {
    "text": "enable the new Orchestrator manager modules which ya have have a plug-in",
    "start": "640190",
    "end": "648060"
  },
  {
    "text": "model and work with the dashboard anyway we don't have time to go into that so a",
    "start": "648060",
    "end": "654930"
  },
  {
    "text": "no dot nine the upgrades of rook are mostly automated now it's it's a lot",
    "start": "654930",
    "end": "662310"
  },
  {
    "text": "easier than ODOT eight who's who in the past has upgraded a rook release",
    "start": "662310",
    "end": "667950"
  },
  {
    "text": "a few of you probably okay there's we have a big long guide a lot of manual steps those manual steps are much",
    "start": "667950",
    "end": "674820"
  },
  {
    "text": "shorter now there's really just a couple of manual steps and I'll I'll get into a",
    "start": "674820",
    "end": "680460"
  },
  {
    "text": "demo of what the upgrades look like now so it's it's really about the operator",
    "start": "680460",
    "end": "687530"
  },
  {
    "text": "managing that complexity to you because or for you because if you have to manage",
    "start": "687530",
    "end": "692550"
  },
  {
    "text": "any anything manually it's it's bound to fail and then your date is at risk so",
    "start": "692550",
    "end": "699690"
  },
  {
    "text": "what it looked like to upgrade rook so you tell with this command at the bottom",
    "start": "699690",
    "end": "704700"
  },
  {
    "text": "of the screen here you can tell rook what version you want to run it's really just the image of the rook operator",
    "start": "704700",
    "end": "711420"
  },
  {
    "text": "that's running so you say coop cuddle set image on this kubernetes deployment",
    "start": "711420",
    "end": "719010"
  },
  {
    "text": "and here's the new image version so rook slash staff vo9 is what you would use to",
    "start": "719010",
    "end": "724670"
  },
  {
    "text": "upgrade from ODOT eight currently so",
    "start": "724670",
    "end": "731280"
  },
  {
    "text": "this to be clear so it updates the rook operator and the related orchestration pieces but it does not update the data",
    "start": "731280",
    "end": "738660"
  },
  {
    "text": "path so after the upgrade you'll still be running luminous just like you were",
    "start": "738660",
    "end": "744170"
  },
  {
    "text": "running before or not on a date and okay",
    "start": "744170",
    "end": "749550"
  },
  {
    "text": "so the Ceph upgrade what does that look like this will this version is specified",
    "start": "749550",
    "end": "756600"
  },
  {
    "text": "in the CR D and you can tell it the image of you want to run like we already saw and I'll I'll go into a demo of this",
    "start": "756600",
    "end": "764820"
  },
  {
    "text": "now but in the future as the complexity of the upgrade might might increase or",
    "start": "764820",
    "end": "771510"
  },
  {
    "text": "maybe it won't the operator will manage whatever steps need to happen if the",
    "start": "771510",
    "end": "777330"
  },
  {
    "text": "data plan needs to be upgraded or data migration we don't currently have any need for a data migration between these",
    "start": "777330",
    "end": "783180"
  },
  {
    "text": "versions of Ceph but it's possible in the future depending on what changes you're made all right so what do I want",
    "start": "783180",
    "end": "790050"
  },
  {
    "text": "to do with the demo now is first of all I'm going to configure luminous and then update that from luminous to mimic and",
    "start": "790050",
    "end": "797580"
  },
  {
    "text": "we'll watch what how the operator deals with that so let's see if I can find my console",
    "start": "797580",
    "end": "807250"
  },
  {
    "text": "all right hopefully that's big enough to see so the first thing I want to do is bigger",
    "start": "807250",
    "end": "815029"
  },
  {
    "text": "okay I really should get a better",
    "start": "815029",
    "end": "821540"
  },
  {
    "text": "terminal here this is all right over here I shouldn't be doing window",
    "start": "821540",
    "end": "833960"
  },
  {
    "text": "management during a demo all right so Oh to do a demo efficiently I thought",
    "start": "833960",
    "end": "841370"
  },
  {
    "text": "I better keep using my same aliases I always use so let me just show you those real quick so you're not totally",
    "start": "841370",
    "end": "846500"
  },
  {
    "text": "confused about what I'm doing so here my alias is Kay for group cuddle KN to do",
    "start": "846500",
    "end": "854180"
  },
  {
    "text": "it in the rook namespace and etc a",
    "start": "854180",
    "end": "859310"
  },
  {
    "text": "command to look at the operator log all right so though that's what I'm running",
    "start": "859310",
    "end": "864760"
  },
  {
    "text": "all right so let's start the cop the operator first assignment I have my V o.9 manifests here so I'm gonna say",
    "start": "864760",
    "end": "872089"
  },
  {
    "text": "create the operator and before I hit go on that I wanted to show down here what",
    "start": "872089",
    "end": "879440"
  },
  {
    "text": "resources are running so no pods are running you up for the operator so after I hit go on that it's going to create a",
    "start": "879440",
    "end": "887690"
  },
  {
    "text": "namespace for the operator and then it's going to start start the operator deployment we should see a pod any",
    "start": "887690",
    "end": "894320"
  },
  {
    "text": "moment here after you operator itself starts it starts up a couple of other",
    "start": "894320",
    "end": "900220"
  },
  {
    "text": "demons that need to run on each node there's the agent which manages mounting",
    "start": "900220",
    "end": "908000"
  },
  {
    "text": "the storage from your PB sees with our flex driver ORS and in the future that",
    "start": "908000",
    "end": "914839"
  },
  {
    "text": "will be moving to CSI the discovery pod this one discovers hey what what storage",
    "start": "914839",
    "end": "921980"
  },
  {
    "text": "devices do I have on each node in the system make sure I discover it and then",
    "start": "921980",
    "end": "927050"
  },
  {
    "text": "the operator you can reconcile what it's going to configure on each node okay so",
    "start": "927050",
    "end": "932360"
  },
  {
    "text": "the operators running nothing new here from the intro so let me run the cluster and",
    "start": "932360",
    "end": "939670"
  },
  {
    "text": "I'm going to show the other namespace so we have two namespaces ones for the operator and ones for the cluster so",
    "start": "939670",
    "end": "948589"
  },
  {
    "text": "this is starting up the SEF cluster the first thing it does is it detects what version of Stef you want so it should",
    "start": "948589",
    "end": "955670"
  },
  {
    "text": "have detected luminous and it's going to start creating sechs Kies store them in",
    "start": "955670",
    "end": "962569"
  },
  {
    "text": "kubernetes secrets and any moment now we should see the first Mon pod there we go",
    "start": "962569",
    "end": "967579"
  },
  {
    "text": "so each Mon is going to go through you see it's in the init stage so it's going",
    "start": "967579",
    "end": "973250"
  },
  {
    "text": "through these init containers as part of that pod definition that rook generated and then finally you know each Mon",
    "start": "973250",
    "end": "981380"
  },
  {
    "text": "started one at a time make sure it's in Coram before you go to the next one so after it gets to the",
    "start": "981380",
    "end": "988730"
  },
  {
    "text": "Mons then it will start up the ACEF manager which is where the dashboard and",
    "start": "988730",
    "end": "994130"
  },
  {
    "text": "the Prometheus modules are running and then it'll get to the OS DS so with the",
    "start": "994130",
    "end": "1000190"
  },
  {
    "text": "OS DS we get this prepare pod you see here at the bottom so this one it goes",
    "start": "1000190",
    "end": "1008800"
  },
  {
    "text": "out to the node and it says ok you told me to configure these devices on this",
    "start": "1008800",
    "end": "1014440"
  },
  {
    "text": "node and for my configuration I just told it you know configure all devices just whatever you find consume so it",
    "start": "1014440",
    "end": "1021899"
  },
  {
    "text": "what it should be doing is it found three devices SDA SDB SDC I think they are and then it",
    "start": "1021899",
    "end": "1029798"
  },
  {
    "text": "tells F hey configure each of these devices with OS DS and after those OS",
    "start": "1029799",
    "end": "1036069"
  },
  {
    "text": "T's are configured it goes back to the operator and says ok I'm done now go ahead and start a pod for each one of",
    "start": "1036069",
    "end": "1042548"
  },
  {
    "text": "these devices and now we see those being created so that each OSD can run",
    "start": "1042549",
    "end": "1050020"
  },
  {
    "text": "independently and if one pod fails it doesn't affect other OSDs running on on",
    "start": "1050020",
    "end": "1056350"
  },
  {
    "text": "the same node alright so we have the basic cluster up and running and to show",
    "start": "1056350",
    "end": "1063549"
  },
  {
    "text": "I guess we're on Luminess what I want to do is is bring up the dashboard",
    "start": "1063549",
    "end": "1069310"
  },
  {
    "text": "so - to expose the dashboard on my on my",
    "start": "1069310",
    "end": "1075010"
  },
  {
    "text": "Mac host I've just got to open up a node pour it into the mini cubm so let me I",
    "start": "1075010",
    "end": "1081190"
  },
  {
    "text": "need to create a dashboard on HTTP so since Luminess is on HTTP now let me get",
    "start": "1081190",
    "end": "1088630"
  },
  {
    "text": "the port that that's on and the mini Q by P and then I'll go to my browser okay",
    "start": "1088630",
    "end": "1098950"
  },
  {
    "text": "so the here's the IP and the port the dashboard is running on is 31 374 whoops",
    "start": "1098950",
    "end": "1109170"
  },
  {
    "text": "that's from a previous run-through okay so we're on this IP with 31 374 same",
    "start": "1109170",
    "end": "1118390"
  },
  {
    "text": "port okay so here is the the Ceph dashboard running on luminous",
    "start": "1118390",
    "end": "1124080"
  },
  {
    "text": "so it's you know read-only we can't do a lot with it all right it's running so",
    "start": "1124080",
    "end": "1130360"
  },
  {
    "text": "now let's go ahead and update to mimic I don't like this dashboard I want the latest and greatest dashboard so now I",
    "start": "1130360",
    "end": "1137590"
  },
  {
    "text": "will go into here and so in the rook",
    "start": "1137590",
    "end": "1144280"
  },
  {
    "text": "namespace I'm going to edit the Ceph",
    "start": "1144280",
    "end": "1150130"
  },
  {
    "text": "cluster rook SEF so the SEF cluster",
    "start": "1150130",
    "end": "1157240"
  },
  {
    "text": "Ciardi called rooks F so it opens me up in this little editor and we see the",
    "start": "1157240",
    "end": "1163990"
  },
  {
    "text": "image so this is the image currently specified and I'm going to change this to be 13 just to make it simple I'll use",
    "start": "1163990",
    "end": "1171910"
  },
  {
    "text": "the top-level tag so as soon as I save",
    "start": "1171910",
    "end": "1177040"
  },
  {
    "text": "this CRD the operator is going to wake up and it's going to say oh you want luminous now I mean sorry you had",
    "start": "1177040",
    "end": "1183520"
  },
  {
    "text": "luminous now you want mimic I'm going to go through and update each of your demons that are running and any",
    "start": "1183520",
    "end": "1190570"
  },
  {
    "text": "additional configuration that's that's needed for that version for example running the mimic dashboard as a secure",
    "start": "1190570",
    "end": "1199450"
  },
  {
    "text": "site now so let me save this and we'll see what starts happening on the bottom with those pods",
    "start": "1199450",
    "end": "1206400"
  },
  {
    "text": "okay so immediately see the a pod that says I'm going to detect the diversion it's already done it detected mimic and",
    "start": "1206400",
    "end": "1215530"
  },
  {
    "text": "now it's going through the Mons the moms are always first to get updated one mile",
    "start": "1215530",
    "end": "1220870"
  },
  {
    "text": "at a time updated to mimic so while that's going I'm going to look at what's",
    "start": "1220870",
    "end": "1228370"
  },
  {
    "text": "happened with with them on so describing them on if I look at the image that's",
    "start": "1228370",
    "end": "1236380"
  },
  {
    "text": "running so now this Mon that I they picked to describe the it's pod for it's",
    "start": "1236380",
    "end": "1242200"
  },
  {
    "text": "running 3:13 okay so the Mons have all been restarted now and it's confirming",
    "start": "1242200",
    "end": "1249130"
  },
  {
    "text": "okay do I have quorum and it looks like it does because it continued now with the manager pod that restarted and what",
    "start": "1249130",
    "end": "1257590"
  },
  {
    "text": "it's doing now is configuring the dashboard with the with the self-signed",
    "start": "1257590",
    "end": "1263200"
  },
  {
    "text": "certificate and then after does that it will proceed to the OS DS and and",
    "start": "1263200",
    "end": "1269550"
  },
  {
    "text": "restart the OSD emic okay so the OS DS",
    "start": "1269550",
    "end": "1276070"
  },
  {
    "text": "will be restarted one by one to reduce any disruption from restarting services",
    "start": "1276070",
    "end": "1284309"
  },
  {
    "text": "okay so now to make sure we have mimic running now what I want to do is start",
    "start": "1284309",
    "end": "1289480"
  },
  {
    "text": "at the dashboard see it's on HTTPS now so I have a separate service I need to create so my laptop can look at it in",
    "start": "1289480",
    "end": "1295990"
  },
  {
    "text": "the browser so dashboard HTTPS alright",
    "start": "1295990",
    "end": "1302470"
  },
  {
    "text": "and let me look at what which can get service so now HTTPS if I tried this try",
    "start": "1302470",
    "end": "1310600"
  },
  {
    "text": "this old port now it's the dashboard isn't running there anymore but now I",
    "start": "1310600",
    "end": "1315880"
  },
  {
    "text": "need to look at this secure port okay so if i refresh I can't get the the",
    "start": "1315880",
    "end": "1324130"
  },
  {
    "text": "dashboard on HTTP anymore it's just not there so let me put HTTPS with the",
    "start": "1324130",
    "end": "1329950"
  },
  {
    "text": "Newport and okay so it's a self-signed certificate so we get this big red",
    "start": "1329950",
    "end": "1335740"
  },
  {
    "text": "warning don't connect to the site well I'm going to anyway for the demo thank you very",
    "start": "1335740",
    "end": "1340990"
  },
  {
    "text": "much okay and here's the mimic dashboard rook generated an an admin password so",
    "start": "1340990",
    "end": "1350620"
  },
  {
    "text": "it's kind of small so this when it generated the password it put it in a",
    "start": "1350620",
    "end": "1356020"
  },
  {
    "text": "kubernetes secret and i need to go get that secret so i can log in so our dashboard help topic it's you know",
    "start": "1356020",
    "end": "1362590"
  },
  {
    "text": "getting a secret out isn't exactly something that's easy to type so let me copy it from our documentation you know",
    "start": "1362590",
    "end": "1368140"
  },
  {
    "text": "get the secret with the dashboard password and decode it from be 64 okay",
    "start": "1368140",
    "end": "1376540"
  },
  {
    "text": "so that's a big long ugly command and we get this password out of it all right if",
    "start": "1376540",
    "end": "1384460"
  },
  {
    "text": "i go back and login now we have the mimic dashboard alright it worked",
    "start": "1384460",
    "end": "1393030"
  },
  {
    "text": "so now we have the cluster is upgraded we have three monitors running all the",
    "start": "1393030",
    "end": "1400480"
  },
  {
    "text": "OSDs are running and you can go and do things like configure pools and block",
    "start": "1400480",
    "end": "1407350"
  },
  {
    "text": "devices file systems from this UI now and you know with each release of Ceph",
    "start": "1407350",
    "end": "1413680"
  },
  {
    "text": "this you know this dashboard gets more feature-rich okay so now one more thing i want to",
    "start": "1413680",
    "end": "1423490"
  },
  {
    "text": "make sure i show a couple more things we have a the toolbox if I could you're",
    "start": "1423490",
    "end": "1431560"
  },
  {
    "text": "right I'll create the toolbox and so what the toolbox pod is just a",
    "start": "1431560",
    "end": "1437710"
  },
  {
    "text": "convenient place to run SEF commands so I'm gonna connect it to this and just",
    "start": "1437710",
    "end": "1443410"
  },
  {
    "text": "show you how simple it is to look at this F status so connect to this we're",
    "start": "1443410",
    "end": "1449350"
  },
  {
    "text": "inside the toolbox now which knows how to talk to SEF so I say hey Steph give me the status and well we already saw in",
    "start": "1449350",
    "end": "1457390"
  },
  {
    "text": "the dashboard we had the Mons and everything running and I've got an error",
    "start": "1457390",
    "end": "1462880"
  },
  {
    "text": "I need to investigate after this but the cluster is all running all right",
    "start": "1462880",
    "end": "1468610"
  },
  {
    "text": "great so so what happens with persistence now on the host so remember",
    "start": "1468610",
    "end": "1476019"
  },
  {
    "text": "how we put the persisted configuration in barley Brook so let me just connect",
    "start": "1476019",
    "end": "1482710"
  },
  {
    "text": "so I'm going to SSH into my mini qbm and just show you basically what's you know",
    "start": "1482710",
    "end": "1490120"
  },
  {
    "text": "what's inside the host path okay so if I go into Varela Brook I see a directory",
    "start": "1490120",
    "end": "1497950"
  },
  {
    "text": "for each of the mons now I only have one note here which isn't a terribly",
    "start": "1497950",
    "end": "1503080"
  },
  {
    "text": "interesting cluster so they're all on the same node you know in any production",
    "start": "1503080",
    "end": "1508269"
  },
  {
    "text": "environment the mons would need to be on different nodes and the OS DS yeah",
    "start": "1508269",
    "end": "1515679"
  },
  {
    "text": "whatever devices you have there so the what's in the Mon directory is not",
    "start": "1515679",
    "end": "1521559"
  },
  {
    "text": "terribly interesting but you see there's a data directory which persists the",
    "start": "1521559",
    "end": "1527799"
  },
  {
    "text": "important Mon data and same thing for OS DS so these are a little more",
    "start": "1527799",
    "end": "1536860"
  },
  {
    "text": "interesting where the osts are what are what's holding your data behind the scenes and",
    "start": "1536860",
    "end": "1542679"
  },
  {
    "text": "I've configured blue store on top of the device so it created partitions for for",
    "start": "1542679",
    "end": "1549399"
  },
  {
    "text": "the data so the block has a symlink over to that partition for the data the boost or database and the blue store",
    "start": "1549399",
    "end": "1555880"
  },
  {
    "text": "right ahead log and other basic configure so that's in a nutshell where",
    "start": "1555880",
    "end": "1562260"
  },
  {
    "text": "this where your date is where your data is stored all right so let me go back to",
    "start": "1562260",
    "end": "1568210"
  },
  {
    "text": "the slides now and move on so there's a lot of options",
    "start": "1568210",
    "end": "1573490"
  },
  {
    "text": "in the in the CR D that we we're running out of time on already talked about but",
    "start": "1573490",
    "end": "1579899"
  },
  {
    "text": "Mons at the end of the day you need to have an odd number of them and you need",
    "start": "1579899",
    "end": "1587139"
  },
  {
    "text": "to control are they on separate nodes so so usually 3 mons as a default it's",
    "start": "1587139",
    "end": "1592210"
  },
  {
    "text": "tolerant of a single failure place them on unique nodes majority is",
    "start": "1592210",
    "end": "1597760"
  },
  {
    "text": "always necessary for them we've let skip over this we've already",
    "start": "1597760",
    "end": "1604159"
  },
  {
    "text": "talked about their basic orchestration and how we need to maintain quorum one",
    "start": "1604159",
    "end": "1611870"
  },
  {
    "text": "thing I want to do is okay if a man dies let's see how it's how the operator",
    "start": "1611870",
    "end": "1618110"
  },
  {
    "text": "recovers that so if I go back over here I'm just gonna delete one of them on",
    "start": "1618110",
    "end": "1624110"
  },
  {
    "text": "deployments it's okay delete deploy rooks F Mon Hey",
    "start": "1624110",
    "end": "1632200"
  },
  {
    "text": "alright so you're gonna see that pod go away now and at this point the the Mon is going",
    "start": "1632200",
    "end": "1640309"
  },
  {
    "text": "to be out of quorum really quickly the operator you know does its health checks periodically I turned the the frequency",
    "start": "1640309",
    "end": "1647539"
  },
  {
    "text": "down for the demo so within the next 15 and 30 seconds we should see the operator noticing the the Mon is gone",
    "start": "1647539",
    "end": "1655129"
  },
  {
    "text": "and bring up a new one so if I look at the operator log we probably see that",
    "start": "1655129",
    "end": "1662029"
  },
  {
    "text": "yep okay is not in quorum and the it's going to wait a little bit is this just",
    "start": "1662029",
    "end": "1667429"
  },
  {
    "text": "a network glitch is this something that I need to recover by default operator we'll wait wait five minutes before it",
    "start": "1667429",
    "end": "1674269"
  },
  {
    "text": "starts up anew and but yeah we should see really soon another one starting now",
    "start": "1674269",
    "end": "1680179"
  },
  {
    "text": "and and here it is so decided yep it's dead it starts up Mon D and now now we",
    "start": "1680179",
    "end": "1689299"
  },
  {
    "text": "should see quorum again and if I went back to the dashboard I should be able to see the new Mons so okay it's still",
    "start": "1689299",
    "end": "1702019"
  },
  {
    "text": "starting up the dashboard isn't aware of it yet",
    "start": "1702019",
    "end": "1707409"
  },
  {
    "text": "okay so it's coming it should refresh itself momentarily all right let me come",
    "start": "1708879",
    "end": "1718309"
  },
  {
    "text": "back to this in just a minute and make sure that that's exceeded but the new Mon came up the old one was taken out of",
    "start": "1718309",
    "end": "1723950"
  },
  {
    "text": "out of the quorum so OS DS there there",
    "start": "1723950",
    "end": "1729080"
  },
  {
    "text": "are a lot of ways to configure your storage devices how do I tell work to go consume",
    "start": "1729080",
    "end": "1734730"
  },
  {
    "text": "you know my my device is on my heterogeneous environment everybody has very specific needs for",
    "start": "1734730",
    "end": "1740490"
  },
  {
    "text": "how they run it but overall you know what Brooke does it schedules a job on",
    "start": "1740490",
    "end": "1746280"
  },
  {
    "text": "each node and then creates a deployment for each OSD when new new nodes and",
    "start": "1746280",
    "end": "1752880"
  },
  {
    "text": "devices are added so the clustered rook can also can also add those to the",
    "start": "1752880",
    "end": "1758370"
  },
  {
    "text": "cluster although currently it does require an operator restart to pick those up usually all right there are",
    "start": "1758370",
    "end": "1766710"
  },
  {
    "text": "different modes for selecting OSDs so the first mode that I was using is",
    "start": "1766710",
    "end": "1772620"
  },
  {
    "text": "automatic selection where on the right here I just said use all nodes use all",
    "start": "1772620",
    "end": "1778799"
  },
  {
    "text": "devices just let go find everything and configure it for me that's easy but I",
    "start": "1778799",
    "end": "1787169"
  },
  {
    "text": "might not want work to do everything so the next option is well the kind of the",
    "start": "1787169",
    "end": "1792419"
  },
  {
    "text": "other end of the spectrum be as prescriptive as you want to be so don't use all devices or nodes set those",
    "start": "1792419",
    "end": "1799410"
  },
  {
    "text": "defaults now you can go set you know which nodes and which devices by a name",
    "start": "1799410",
    "end": "1805559"
  },
  {
    "text": "do you want to configure and work will only go configure OSDs on those devices",
    "start": "1805559",
    "end": "1811250"
  },
  {
    "text": "now that could get a little tedious specifying all those and a big cluster",
    "start": "1811250",
    "end": "1816650"
  },
  {
    "text": "would be very difficult so next mode is filtering so if you filter the devices",
    "start": "1816650",
    "end": "1824030"
  },
  {
    "text": "you can use a regular expression here with the device filters so any right now",
    "start": "1824030",
    "end": "1829890"
  },
  {
    "text": "it'll say all nodes use a device filter and and go go set those those weren't",
    "start": "1829890",
    "end": "1839880"
  },
  {
    "text": "very communities native so that occur Bernays native way to do a similar thing is to use node labels so you can set a",
    "start": "1839880",
    "end": "1847110"
  },
  {
    "text": "label on a node so in this example we've got role equals storage node and then in",
    "start": "1847110",
    "end": "1854280"
  },
  {
    "text": "this CRD you can use this placement then that says hey for OSDs or only place OSDs on on nodes that have",
    "start": "1854280",
    "end": "1864450"
  },
  {
    "text": "this label that says role equals storage node it's a bit of a big yeah Mille snippet",
    "start": "1864450",
    "end": "1871529"
  },
  {
    "text": "but that's that will allow you to place the services therefore OS DS and that",
    "start": "1871529",
    "end": "1877320"
  },
  {
    "text": "same filter or with note affinity can be used for all the demons the Mons and OS",
    "start": "1877320",
    "end": "1883679"
  },
  {
    "text": "DS rgw and all those that's yeah more",
    "start": "1883679",
    "end": "1889590"
  },
  {
    "text": "Cooper native alright so the next option we've got is sort of a performance",
    "start": "1889590",
    "end": "1894809"
  },
  {
    "text": "optimisation so some people have like they've got a high performance they've",
    "start": "1894809",
    "end": "1899850"
  },
  {
    "text": "got SSDs or nvm ease just one or two of those on a node and then a whole bunch of spinning discs so the way you can",
    "start": "1899850",
    "end": "1907289"
  },
  {
    "text": "configure that is you tell okay my my metadata device whoops let me go back my",
    "start": "1907289",
    "end": "1912600"
  },
  {
    "text": "metadata device is this nvme oh one and then you know pick up all the other",
    "start": "1912600",
    "end": "1919830"
  },
  {
    "text": "devices like SD star and and put the data on those but yeah this gives me a",
    "start": "1919830",
    "end": "1927649"
  },
  {
    "text": "higher performance cluster with Ceph put the blue store right head logon database",
    "start": "1927649",
    "end": "1933539"
  },
  {
    "text": "on one high-performance device put the put the data somewhere else on the",
    "start": "1933539",
    "end": "1939389"
  },
  {
    "text": "spinning of disks and one more option is if you really have high performance",
    "start": "1939389",
    "end": "1946139"
  },
  {
    "text": "clusters you've got all nvme devices which would pretty pretty nice I want one of those someday you know envy means",
    "start": "1946139",
    "end": "1954809"
  },
  {
    "text": "are so fast that they you know you can actually put multiple OSDs on the same",
    "start": "1954809",
    "end": "1959869"
  },
  {
    "text": "on the same nvme device so there's this new setting OSDs per device to put say",
    "start": "1959869",
    "end": "1967859"
  },
  {
    "text": "five on the same one so you can have your high performance data okay so",
    "start": "1967859",
    "end": "1978840"
  },
  {
    "text": "another new feature we've got is our buddy mirroring that was we just exposed",
    "start": "1978840",
    "end": "1983850"
  },
  {
    "text": "well it's been around for a while and stuff but ODOT 9 exposes this so let me",
    "start": "1983850",
    "end": "1989879"
  },
  {
    "text": "go back to the demo and we're to go nope",
    "start": "1989879",
    "end": "1997049"
  },
  {
    "text": "not there okay doesn't want to show up all right",
    "start": "1997049",
    "end": "2002580"
  },
  {
    "text": "here we go so if I edit the llamo again so edit the SEF cluster SEF so by",
    "start": "2002580",
    "end": "2012690"
  },
  {
    "text": "default we don't configure our BG marrying but here it is in here so our buddy marrying how many workers do you want three I heard of three okay so it's",
    "start": "2012690",
    "end": "2028200"
  },
  {
    "text": "done so momentarily we should see some new RBD mirroring pods show up on the",
    "start": "2028200",
    "end": "2035580"
  },
  {
    "text": "bottom half it to actually go marry or anything are be marrying is about replicating your data to another cluster",
    "start": "2035580",
    "end": "2042359"
  },
  {
    "text": "it does require more configuration in the SEF at the cellar so you under the tool box and you tell Seth which which",
    "start": "2042359",
    "end": "2051030"
  },
  {
    "text": "images you want to want to orchestrate and alright any moment I'm waiting for",
    "start": "2051030",
    "end": "2059490"
  },
  {
    "text": "them to show up at the bottom there and now we can look at the operator log to",
    "start": "2059490",
    "end": "2065760"
  },
  {
    "text": "see what it's doing and it's anyway it's",
    "start": "2065760",
    "end": "2072089"
  },
  {
    "text": "it's thinking for a little bit alright but it always stays in a reconciliation",
    "start": "2072089",
    "end": "2078179"
  },
  {
    "text": "loop it will come back and and make sure it happens sometimes it just takes a minute or two",
    "start": "2078179",
    "end": "2085158"
  },
  {
    "text": "alright I think we're getting close to out of time so rgw you know this is what the CRT",
    "start": "2086780",
    "end": "2093690"
  },
  {
    "text": "looks like you say create me an object store use replication or erasure coding",
    "start": "2093690",
    "end": "2098750"
  },
  {
    "text": "to make that happen set that fast",
    "start": "2098750",
    "end": "2103770"
  },
  {
    "text": "same sort of thing there's a CRT you create it and and the operator makes it",
    "start": "2103770",
    "end": "2109920"
  },
  {
    "text": "happen I wish we had more time the agent",
    "start": "2109920",
    "end": "2115520"
  },
  {
    "text": "I'll just keep over this so how to get involved you know go to our github now",
    "start": "2115520",
    "end": "2122280"
  },
  {
    "text": "we've got a slack with an active community love to hear your questions make sure your issues are getting fixed",
    "start": "2122280",
    "end": "2127309"
  },
  {
    "text": "always welcoming new contributions can",
    "start": "2127309",
    "end": "2132720"
  },
  {
    "text": "meetings are every other Tuesday I look forward to having in those calls and I",
    "start": "2132720",
    "end": "2138980"
  },
  {
    "text": "think that's it on it and it disappeared okay",
    "start": "2138980",
    "end": "2144220"
  },
  {
    "text": "[Applause]",
    "start": "2144220",
    "end": "2150520"
  }
]