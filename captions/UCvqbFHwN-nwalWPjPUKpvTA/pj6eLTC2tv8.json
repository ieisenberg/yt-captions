[
  {
    "text": "okay so welcome to my talk about uh Autos scaling up atafa my name is yob",
    "start": "240",
    "end": "5640"
  },
  {
    "text": "Schultz I work as an engineer at redhead and I'm also a maintainer of the stry",
    "start": "5640",
    "end": "11320"
  },
  {
    "text": "project and uh occasionally I'm also Apachi Kafka",
    "start": "11320",
    "end": "17039"
  },
  {
    "text": "contributor if you don't know what strumsy then stry is a cncf incubating project which focus on running Apachi",
    "start": "17039",
    "end": "24840"
  },
  {
    "text": "count kubernetes so we provide uh operators for running all the different",
    "start": "24840",
    "end": "30000"
  },
  {
    "text": "server components of uh of apachi Kafka but we also have operators for managing",
    "start": "30000",
    "end": "35120"
  },
  {
    "text": "things such as users topics connectors and we have some other smaller tools to",
    "start": "35120",
    "end": "41280"
  },
  {
    "text": "make it easier to use uh Kafka on on Cube and uh yeah maybe on the beginning",
    "start": "41280",
    "end": "48440"
  },
  {
    "text": "uh we can touch on why Autos scaling Apachi Kafka Brokers might be interesting why it might matter well",
    "start": "48440",
    "end": "56920"
  },
  {
    "text": "gafka is quite often a pretty big workload which is taking quite a lot of resources so yeah if we manage to",
    "start": "56920",
    "end": "64518"
  },
  {
    "text": "autoscale it in some effective and efficient way maybe we can save some costs maybe we can save some energy be a",
    "start": "64519",
    "end": "71520"
  },
  {
    "text": "bit more green uh or something like that so so that's why we uh looked into it",
    "start": "71520",
    "end": "79920"
  },
  {
    "text": "and uh maybe bit untraditionally let's start a talk with uh the first part of a",
    "start": "79920",
    "end": "85159"
  },
  {
    "text": "of a demo so here in my",
    "start": "85159",
    "end": "91920"
  },
  {
    "text": "kubernetes cluster I have already a Kafka cluster deployed so what we can see here is that uh right now I have",
    "start": "93079",
    "end": "100720"
  },
  {
    "text": "here three Brokers 10 11 and 12 uh remember that uh for later uh I'm not",
    "start": "100720",
    "end": "106560"
  },
  {
    "text": "using zookeeper anymore I'm using this craft thing which is new in Kafka so I have these three controllers uh as well",
    "start": "106560",
    "end": "113320"
  },
  {
    "text": "to manage the quarum and so on and then I have these additional tools which streams the use cruise control is for",
    "start": "113320",
    "end": "120159"
  },
  {
    "text": "cluster rebalancing it's a it's a tool from LinkedIn then uh this entity",
    "start": "120159",
    "end": "126079"
  },
  {
    "text": "operator that's what I mentioned for managing users and uh and topics and this scafa exporter is just for some",
    "start": "126079",
    "end": "132239"
  },
  {
    "text": "additional metrics and then uh yeah this is the main stream the operator which is actually running this uh this cluster",
    "start": "132239",
    "end": "139319"
  },
  {
    "text": "and uh so what I'm going to do right now on the beginning is",
    "start": "139319",
    "end": "144879"
  },
  {
    "text": "uh I'm going to deploy my load so just create some",
    "start": "144879",
    "end": "150760"
  },
  {
    "text": "kubernetes jobs which will create bunch of pots and uh yeah they will start uh",
    "start": "150760",
    "end": "156640"
  },
  {
    "text": "producing a load of messages and consuming the load of messages and uh yeah hopefully later when we get back to",
    "start": "156640",
    "end": "162959"
  },
  {
    "text": "the demo we should see that some autoscaling happened and uh yeah if not",
    "start": "162959",
    "end": "168000"
  },
  {
    "text": "then uh yeah something didn't work so in the meantime while it's hopefully AOS scaling we can get back to",
    "start": "168000",
    "end": "174640"
  },
  {
    "text": "the to the slide uh and uh we can talk a bit more about the difference between",
    "start": "174640",
    "end": "182000"
  },
  {
    "text": "scalability and elasticity because uh quite often when you mention Kafka then",
    "start": "182000",
    "end": "187840"
  },
  {
    "text": "for a lot of people who work with it scalability is the first thing which comes to people's mind so why do we need",
    "start": "187840",
    "end": "195159"
  },
  {
    "text": "to talk about how to Autos scale it if it's so scalable but there's a difference between scalability and",
    "start": "195159",
    "end": "201000"
  },
  {
    "text": "elasticity right and the the scalability in which Kafka really excels is this kind of thing where you start maybe with",
    "start": "201000",
    "end": "208000"
  },
  {
    "text": "the three brokers in your cluster when you start your project or company you usually start with free for the",
    "start": "208000",
    "end": "214120"
  },
  {
    "text": "availability and reliability but then you get the first customers you may be on board new Services New users so yeah",
    "start": "214120",
    "end": "221439"
  },
  {
    "text": "you can grow the cluster and uh if you are still doing well if you are becoming the new",
    "start": "221439",
    "end": "228000"
  },
  {
    "text": "LinkedIn Facebook uh whatever then you can grow even more and more and more and",
    "start": "228000",
    "end": "234040"
  },
  {
    "text": "actually it's Kafka can really scale well and having tens of Brokers or even hundreds it's not necessarily something",
    "start": "234040",
    "end": "241400"
  },
  {
    "text": "what's uh what's impossible but when we talk about elasticity we talk a bit more about kind",
    "start": "241400",
    "end": "248040"
  },
  {
    "text": "of this uh reaction to the immediate demand so some advertisement somewhere",
    "start": "248040",
    "end": "253120"
  },
  {
    "text": "showed or someone wrote about your project on on Reddit or Hecker news and suddenly for the few minutes you start",
    "start": "253120",
    "end": "259680"
  },
  {
    "text": "getting in whole bunch of new users uh so you need to scale the cluster up but then after a few minutes you are the old",
    "start": "259680",
    "end": "266080"
  },
  {
    "text": "news you know how it goes these days so yeah you need to scale down again because you don't need really the the",
    "start": "266080",
    "end": "272320"
  },
  {
    "text": "performance so so that's more the the elasticity and that's usually where the",
    "start": "272320",
    "end": "277680"
  },
  {
    "text": "autoscaling fits a bit more to kind of react to the immediate demand uh which",
    "start": "277680",
    "end": "283479"
  },
  {
    "text": "you might have from the applications using something like Kafka and",
    "start": "283479",
    "end": "290039"
  },
  {
    "text": "uh to be honest the scalability part the kind of more midterm long-term scalability to grow with your company",
    "start": "290039",
    "end": "296840"
  },
  {
    "text": "with your project Kafka does that very well the elasticity part part that's not",
    "start": "296840",
    "end": "302720"
  },
  {
    "text": "so so simple and so easy and uh yeah so that's what we try to improve in in stry",
    "start": "302720",
    "end": "309600"
  },
  {
    "text": "and yeah pretty much this stock is the journey which we took to to get",
    "start": "309600",
    "end": "315759"
  },
  {
    "text": "there so the first thing if you want to do some Autos scaling on kubernetes then",
    "start": "315759",
    "end": "322680"
  },
  {
    "text": "yeah the first thing what you need to do is you need to have your scale sub resource now if you use deployment or",
    "start": "322680",
    "end": "328840"
  },
  {
    "text": "state full set has that kind of built-in from kubernetes but if you are operator like a stmy and you have the custom",
    "start": "328840",
    "end": "335000"
  },
  {
    "text": "resources it's something what you need to have supported in the custom resource and in your operator and it's the scale",
    "start": "335000",
    "end": "342560"
  },
  {
    "text": "sub resource which basically allows kubernetes to kind of scale the resource without really understanding the uh",
    "start": "342560",
    "end": "350240"
  },
  {
    "text": "internal structure of the custom resource so thanks to that you can do something like Cube cuddle scale uh",
    "start": "350240",
    "end": "357440"
  },
  {
    "text": "Kafka not pool Brokers uh to five replicas and kubernetes will know",
    "start": "357440",
    "end": "362919"
  },
  {
    "text": "exactly what to do all to the Kafka not poool resource is something stsy came up",
    "start": "362919",
    "end": "368000"
  },
  {
    "text": "with and it has nothing to do with uh with kubernetes itself and uh this is",
    "start": "368000",
    "end": "374120"
  },
  {
    "text": "also where you plug in the horizontal poort autoscaler which is the kind of basic kubernetes resource which you can",
    "start": "374120",
    "end": "380919"
  },
  {
    "text": "use to autoscale Things based on some Metric get them uh up increase the",
    "start": "380919",
    "end": "386479"
  },
  {
    "text": "number of replicas or down to decrease the number of replicas so this was the first thing which we uh",
    "start": "386479",
    "end": "393039"
  },
  {
    "text": "which we did in strey and uh yeah it got us uh one step forward so it got us to",
    "start": "393039",
    "end": "400400"
  },
  {
    "text": "something like uh this so we can now scale the cluster but if we start with",
    "start": "400400",
    "end": "406360"
  },
  {
    "text": "something like free node Kafka cluster which has some partition replicas assigned to the free noes then uh yeah",
    "start": "406360",
    "end": "412880"
  },
  {
    "text": "when the scaling happens new note is added to the cluster but that's it it's empty",
    "start": "412880",
    "end": "420280"
  },
  {
    "text": "and that's because in Kafka all the partition replicas they are assigned to one of the Brokers and if you just add",
    "start": "420280",
    "end": "426919"
  },
  {
    "text": "more Brokers to the cluster that doesn't necessarily mean that uh they will move around right so you edit the note",
    "start": "426919",
    "end": "434479"
  },
  {
    "text": "sometimes that's fine for this kind of long-term scaling that's fine because you will maybe on board new Services new",
    "start": "434479",
    "end": "441039"
  },
  {
    "text": "applications add new topics and this will be created on the new node so over the time they will fill in but for the",
    "start": "441039",
    "end": "447160"
  },
  {
    "text": "Autos scaling this doesn't really help similarly what happens when you scale",
    "start": "447160",
    "end": "453599"
  },
  {
    "text": "down the cluster so let's now imagine we have a for note cluster which doesn't have the for note empty but it's already",
    "start": "453599",
    "end": "460000"
  },
  {
    "text": "in use and it's uh yeah pretty underutilized so let's scale it down to three Brokers well it's not that simple",
    "start": "460000",
    "end": "467240"
  },
  {
    "text": "right because the broker number four which we would maybe try to scale down it holds the data and if you would just",
    "start": "467240",
    "end": "473000"
  },
  {
    "text": "delete the pot then we would lose the data right now uh maybe it would just",
    "start": "473000",
    "end": "478879"
  },
  {
    "text": "break the a ability of your cluster maybe it would actually cause some data loss which for some data which wouldn't",
    "start": "478879",
    "end": "485240"
  },
  {
    "text": "be replicated on the other nodes but you can't do it that",
    "start": "485240",
    "end": "490560"
  },
  {
    "text": "simply so that's why in stry we created something what we called Auto",
    "start": "490560",
    "end": "495639"
  },
  {
    "text": "rebalancing and it's a feature of the strumsy operator which uh underhood uses",
    "start": "495639",
    "end": "500840"
  },
  {
    "text": "the cruise control tool and basically when you do some scaling and if you enable this Auto rebalancing because",
    "start": "500840",
    "end": "507400"
  },
  {
    "text": "it's it's opt in right now then streams will automatically use cruise control and Trigger rebalance to shift the data",
    "start": "507400",
    "end": "514719"
  },
  {
    "text": "around within your your cka cluster and uh similarly at scal down before it",
    "start": "514719",
    "end": "521560"
  },
  {
    "text": "actually removes the Noe it will first start the rebalance to kind of remove the data from the node which will be",
    "start": "521560",
    "end": "527160"
  },
  {
    "text": "scaled down and only once it's really empty it will be actually deleted so did we improve that do we",
    "start": "527160",
    "end": "535120"
  },
  {
    "text": "have now a proper autoscaling well kind of so so what happens now with the",
    "start": "535120",
    "end": "540760"
  },
  {
    "text": "enabled Auto rebalancing is that uh yeah when you start with the free node and the auto scaling happens then",
    "start": "540760",
    "end": "548519"
  },
  {
    "text": "the new node is added to the cluster and then the rebalancing happens and it shifts some of the data to the new node",
    "start": "548519",
    "end": "556160"
  },
  {
    "text": "and uh yeah that looks great right we have now new uh new node which has some",
    "start": "556160",
    "end": "561320"
  },
  {
    "text": "data the cluster is nicely balanced over so that looks like a success similarly",
    "start": "561320",
    "end": "568720"
  },
  {
    "text": "when we would now scale down then basically what stmy would do is uh it",
    "start": "568720",
    "end": "573760"
  },
  {
    "text": "would first move the data to one of the remaining noes and only then it would actually scale down the the empty Noe",
    "start": "573760",
    "end": "581480"
  },
  {
    "text": "and remove it so that that again looks uh pretty good now the question is",
    "start": "581480",
    "end": "588200"
  },
  {
    "text": "what's really happening there under the surface of this uh now the reality might",
    "start": "588200",
    "end": "594160"
  },
  {
    "text": "be that when you start with the three Brokers each of them has for example four terabytes of data stored in the",
    "start": "594160",
    "end": "601279"
  },
  {
    "text": "Brokers now there are surely many Kafka clusters which have a lot less data stored in uh in each broker but there",
    "start": "601279",
    "end": "608760"
  },
  {
    "text": "are also many clusters which have way more than four terabyt stored in each broker so yeah it's kind of an example",
    "start": "608760",
    "end": "614600"
  },
  {
    "text": "but it's not completely unrealistic now when we add the new broker to the cluster it starts with an",
    "start": "614600",
    "end": "622160"
  },
  {
    "text": "empty storage so it has zero and then the rebalance happens and it kind of",
    "start": "622160",
    "end": "627680"
  },
  {
    "text": "roughly equals it out so now instead of three Brokers with four terabytes and",
    "start": "627680",
    "end": "633000"
  },
  {
    "text": "one empty we have four Brokers with three terabytes each and what it really",
    "start": "633000",
    "end": "638320"
  },
  {
    "text": "means is that we basically had to shift three terabytes of data around from the",
    "start": "638320",
    "end": "643440"
  },
  {
    "text": "old nodes to the new node and uh it works but obviously moving three",
    "start": "643440",
    "end": "652160"
  },
  {
    "text": "terabytes of data around means uh you need some network resources because you need to shift this over Network you need",
    "start": "652160",
    "end": "659320"
  },
  {
    "text": "need some storage IO because you need to read it from the diss then on the new note you need to write it to the disk uh",
    "start": "659320",
    "end": "666880"
  },
  {
    "text": "you need some CPU because uh we have encryption surely and all these things around and",
    "start": "666880",
    "end": "673880"
  },
  {
    "text": "uh most of all you also need time because moving three terabytes of data is not something what happens uh within",
    "start": "673880",
    "end": "680519"
  },
  {
    "text": "few seconds as the animation on the slide happen right and the same would happen if you if you scale it down so uh",
    "start": "680519",
    "end": "689399"
  },
  {
    "text": "yeah this is not perfect and uh yeah it works but it's kind of let's",
    "start": "689399",
    "end": "696320"
  },
  {
    "text": "try to improve it and the thing which comes to help here is uh something called theed storage so it's relatively",
    "start": "696320",
    "end": "703560"
  },
  {
    "text": "new feature in uh in Kafka but you might know it from other applications and uh",
    "start": "703560",
    "end": "711040"
  },
  {
    "text": "as well and it basically means that kind of you use different tiers different levels of storage in your Kafka cluster",
    "start": "711040",
    "end": "718240"
  },
  {
    "text": "and some of the data which are maybe not that important not that often accessed uh uh not that fresh they will be",
    "start": "718240",
    "end": "726120"
  },
  {
    "text": "uploaded into the other tier of the storage and locally you would keep mainly the data which are important",
    "start": "726120",
    "end": "731959"
  },
  {
    "text": "which are often accessed again and again or which are just received from the",
    "start": "731959",
    "end": "737120"
  },
  {
    "text": "producers and typically the the the other tier of the storage which you add would be something like uh Amazon awss",
    "start": "737120",
    "end": "744160"
  },
  {
    "text": "free storage or other types of object storage or some NFS storage which kind of of uh they typically are cheaper than",
    "start": "744160",
    "end": "752920"
  },
  {
    "text": "the block storage used directly by the Brokers they uh typically allow you for",
    "start": "752920",
    "end": "758519"
  },
  {
    "text": "much higher capacity than the block storage but also in most cases kind of",
    "start": "758519",
    "end": "764399"
  },
  {
    "text": "the the downside of them is that they often have bigger latency and uh and",
    "start": "764399",
    "end": "769440"
  },
  {
    "text": "things like that so uh it's kind of a trade-off but if we use the thired",
    "start": "769440",
    "end": "774959"
  },
  {
    "text": "storage to have less data on the brokers then we need to shift less data",
    "start": "774959",
    "end": "780800"
  },
  {
    "text": "when we do the rebalancing right so now if we would apply the teered",
    "start": "780800",
    "end": "788240"
  },
  {
    "text": "storage to the cluster which we had before now suddenly we don't need to have four terabytes of data on each of",
    "start": "788240",
    "end": "793639"
  },
  {
    "text": "the Brokers maybe we have only 400 gigabyt of the data on each of the Brokers and then the rest is somewhere",
    "start": "793639",
    "end": "799839"
  },
  {
    "text": "in the cloud and Amazon as free storage so what happens now when we scale the",
    "start": "799839",
    "end": "805600"
  },
  {
    "text": "cluster is we add the new node the new node is again edit with empty storage we",
    "start": "805600",
    "end": "810959"
  },
  {
    "text": "do the rebalance and now after the rebalance each of the nodes is only 300 GB of the storage and yeah it might be",
    "start": "810959",
    "end": "819079"
  },
  {
    "text": "still a lot but it's 10 times less than uh we had before right so it will cost",
    "start": "819079",
    "end": "824440"
  },
  {
    "text": "you 10 times less resources on the networking on the storage but uh also uh",
    "start": "824440",
    "end": "830199"
  },
  {
    "text": "roughly 10 times less uh less time so yeah that's kind of the the next piece",
    "start": "830199",
    "end": "835639"
  },
  {
    "text": "which allows us to improve how quickly the cluster scales and uh how quickly it",
    "start": "835639",
    "end": "841880"
  },
  {
    "text": "can kind of happen so let's try to get back to the to the console and see if something",
    "start": "841880",
    "end": "850360"
  },
  {
    "text": "happened and what I can see here is uh that",
    "start": "850360",
    "end": "855839"
  },
  {
    "text": "here it created the broker number 13 and here it created the N broker",
    "start": "855839",
    "end": "861839"
  },
  {
    "text": "number 14 so if you remember from the beginning we had Brokers 10 11 and 12",
    "start": "861839",
    "end": "867800"
  },
  {
    "text": "three of them so now we we have 13 and 14 we have five of them and then it also needs to restart the the cruise control",
    "start": "867800",
    "end": "874800"
  },
  {
    "text": "because cruise control needs to understand what's the cluster configuration how many nodes does it have uh uh what's the storage so right",
    "start": "874800",
    "end": "882800"
  },
  {
    "text": "now kind of the way to update the configuration is by Rolling the cruise control P so that's why you see it there",
    "start": "882800",
    "end": "889040"
  },
  {
    "text": "as well but if you go somewhere else then this is basically",
    "start": "889040",
    "end": "895320"
  },
  {
    "text": "looking for the horizontal pot Auto scaler",
    "start": "895320",
    "end": "900360"
  },
  {
    "text": "and this is where we started right there was pretty much no load on the cluster",
    "start": "900360",
    "end": "907639"
  },
  {
    "text": "this was the first part of the demo where I fired up the load and started uh increasing",
    "start": "907639",
    "end": "913880"
  },
  {
    "text": "it and then uh yeah this is where the horizontal Port Auto scaler saw that it needs to",
    "start": "913880",
    "end": "921079"
  },
  {
    "text": "scale up the Kafka cluster and it kind of first move it to four nodes then move it to five five noes and that's the 13",
    "start": "921079",
    "end": "927880"
  },
  {
    "text": "and 14 which we saw being edit and then when I go here so this is",
    "start": "927880",
    "end": "934120"
  },
  {
    "text": "the auto rebalancing which was triggered by streamy so here on the beginning I",
    "start": "934120",
    "end": "939360"
  },
  {
    "text": "have kind of a template which just tells streamy and cruise control what are the",
    "start": "939360",
    "end": "944399"
  },
  {
    "text": "default rules with which it should calculate the how the data will be now spread across the cluster and then this",
    "start": "944399",
    "end": "951120"
  },
  {
    "text": "uh this add Brokers uh rebalance is basically what happened after the after",
    "start": "951120",
    "end": "957040"
  },
  {
    "text": "the scale up when it started shifting the data around uh in the cluster so",
    "start": "957040",
    "end": "963040"
  },
  {
    "text": "it's now ready which means that it completed so let me check",
    "start": "963040",
    "end": "969720"
  },
  {
    "text": "uh let me try to EXA into one of the pods and list the",
    "start": "969720",
    "end": "975680"
  },
  {
    "text": "topics it's a demo so I don't have it completely secured so that's why I don't see any certificates and users and so on",
    "start": "975680",
    "end": "982319"
  },
  {
    "text": "but uh if I go here for example to the lowest topic which is used by the by the",
    "start": "982319",
    "end": "988639"
  },
  {
    "text": "job generating the load you can see so this one is still on the original nodes 10 11 12 this one is already on the Node",
    "start": "988639",
    "end": "997160"
  },
  {
    "text": "14 so one replica was moved this is 13",
    "start": "997160",
    "end": "1002240"
  },
  {
    "text": "13 14 13 so you can see how the different partition replicas of the",
    "start": "1002240",
    "end": "1007519"
  },
  {
    "text": "Kafka cluster are now not only on the old Brokers but are now using the new Brokers as well so yeah nice uh I'm glad",
    "start": "1007519",
    "end": "1016399"
  },
  {
    "text": "the demo worked uh and we saw the autoscaling uh and uh now let me show",
    "start": "1016399",
    "end": "1024720"
  },
  {
    "text": "you how I actually deployed it uh and later I will share the link to the to the GitHub where I have the yaml files",
    "start": "1024720",
    "end": "1031678"
  },
  {
    "text": "but basically this is the stry these are the strey custom resources which I use to",
    "start": "1031679",
    "end": "1038199"
  },
  {
    "text": "deploy this this cluster so this is just the normal cka custom resource of",
    "start": "1038199",
    "end": "1044319"
  },
  {
    "text": "streamy I specify here a custom image which I want to use because the thired storage plugins are currently not",
    "start": "1044319",
    "end": "1050600"
  },
  {
    "text": "shipped in the default streams image so this is a custom image where I just addit the the plugin for it but",
    "start": "1050600",
    "end": "1057440"
  },
  {
    "text": "otherwise it's uh it's just using the last trims release 044 and uh then here in this section I",
    "start": "1057440",
    "end": "1066640"
  },
  {
    "text": "actually configur a theed storage so it's using the the theed storage plugin from uh from Ivan uh and in my case I'm",
    "start": "1066640",
    "end": "1074640"
  },
  {
    "text": "using the NFS storage as the theed storage so yeah I just configure kind of where the volume is is mounted and I",
    "start": "1074640",
    "end": "1081039"
  },
  {
    "text": "tell it to use the file system storage plugin and",
    "start": "1081039",
    "end": "1086880"
  },
  {
    "text": "uh then this is another important part so this is the configuration of AO rebalancing where I enable this feature",
    "start": "1086880",
    "end": "1094480"
  },
  {
    "text": "in streamy so I can enable it separately for adding Brokers when scaling up or",
    "start": "1094480",
    "end": "1100640"
  },
  {
    "text": "for removing Brokers for scaling down and I can also specify this template which says uh what are the the algorithm",
    "start": "1100640",
    "end": "1108960"
  },
  {
    "text": "I want to use to kind of rebalance the data uh which is optional if you want to",
    "start": "1108960",
    "end": "1115400"
  },
  {
    "text": "use the default you don't need to specified and then here have the Kafka node pool so this is the one for the",
    "start": "1115400",
    "end": "1121240"
  },
  {
    "text": "craft controllers we are not scaling those you don't need to Autos scale those normally you don't uh don't really",
    "start": "1121240",
    "end": "1128080"
  },
  {
    "text": "touch these they are just free nodes but this is the note pool for the kafa Brokers and this is what we are actually",
    "start": "1128080",
    "end": "1134960"
  },
  {
    "text": "autoscaling in the demo so these are the actual broker uh and uh the important part here is",
    "start": "1134960",
    "end": "1143400"
  },
  {
    "text": "that uh yeah here I kind of Mount additional NFS volume which is used as the as the teered storage so that it's",
    "start": "1143400",
    "end": "1150000"
  },
  {
    "text": "available in the in the Brokers and can be used to offload the data and then",
    "start": "1150000",
    "end": "1155080"
  },
  {
    "text": "finally this is just the the cfar balance which I used as a template so I just give it the rules that it should",
    "start": "1155080",
    "end": "1161039"
  },
  {
    "text": "try to kind of optimize for all of these distributions of uh replica leaders dis",
    "start": "1161039",
    "end": "1167080"
  },
  {
    "text": "usage CPU usage and and uh and so on so that's how I deploy the Kafka",
    "start": "1167080",
    "end": "1173880"
  },
  {
    "text": "cluster uh this is just the topic I used for the for the low test so what's",
    "start": "1173880",
    "end": "1180120"
  },
  {
    "text": "important is it has 100 partition it has replication Factor fre and then of course I had to enable the the storage",
    "start": "1180120",
    "end": "1187600"
  },
  {
    "text": "for this topic and then uh last but not least this is the",
    "start": "1187600",
    "end": "1194400"
  },
  {
    "text": "horizontal po Auto scale Realo so I basically tell you that I want to scale",
    "start": "1194400",
    "end": "1199760"
  },
  {
    "text": "this uh Kafka node pool named broker uh in this demo I want to scale it between",
    "start": "1199760",
    "end": "1204960"
  },
  {
    "text": "three and five replicas and uh really for demo purposes I'm just using the CPU",
    "start": "1204960",
    "end": "1210240"
  },
  {
    "text": "utilization As the metric to scale it on and uh I configure it to Target uh 90%",
    "start": "1210240",
    "end": "1217320"
  },
  {
    "text": "average uh utilization and uh then I have to configure some kind of uh",
    "start": "1217320",
    "end": "1223159"
  },
  {
    "text": "stabilization windows and kind of timings and uh I will get uh in a minute",
    "start": "1223159",
    "end": "1228360"
  },
  {
    "text": "to more why that's uh important so that's kind of the the demo",
    "start": "1228360",
    "end": "1234799"
  },
  {
    "text": "uh just for the sake of fun let's uh stop the load",
    "start": "1234799",
    "end": "1240640"
  },
  {
    "text": "now and maybe later after the talk uh on the hallway we should be able to see that it actually scaled down",
    "start": "1240640",
    "end": "1249158"
  },
  {
    "text": "again so back to the demo uh this is where you can find the the yls uh which",
    "start": "1250679",
    "end": "1257720"
  },
  {
    "text": "I used there on it just redirects to to GitHub the GitHub URL is too long uh the",
    "start": "1257720",
    "end": "1263320"
  },
  {
    "text": "slides are also on the on the schedule so you can find it there uh as",
    "start": "1263320",
    "end": "1271039"
  },
  {
    "text": "well and uh so it the Autos scaling worked really but",
    "start": "1271039",
    "end": "1276480"
  },
  {
    "text": "uh between the first part of the demo when I started the load and when I actually checked the result it took",
    "start": "1276480",
    "end": "1282360"
  },
  {
    "text": "roughly 10 minutes and uh it normally doesn't need the full 10 minutes but uh",
    "start": "1282360",
    "end": "1288120"
  },
  {
    "text": "yeah in in the demo you can expect that uh it roughly takes uh several minutes",
    "start": "1288120",
    "end": "1293760"
  },
  {
    "text": "uh after the scaling happens to finish the rebalance and so w now this might differ for every user it depends how big",
    "start": "1293760",
    "end": "1301080"
  },
  {
    "text": "your cluster is how many data does it have how much performance so now in the demo maybe you saw it in the in the",
    "start": "1301080",
    "end": "1307520"
  },
  {
    "text": "files uh I for example had pretty small cluster with just like 2 gigs of RAM for",
    "start": "1307520",
    "end": "1312679"
  },
  {
    "text": "note and little bit of CPU uh I think the storage which was there it was",
    "start": "1312679",
    "end": "1317799"
  },
  {
    "text": "roughly 100 gabt of storage per broker including the thired storage so so it",
    "start": "1317799",
    "end": "1322919"
  },
  {
    "text": "wasn't the biggest cluster it wasn't the biggest load if you would do it uh with",
    "start": "1322919",
    "end": "1328080"
  },
  {
    "text": "a more better sized cluster you would typically have more data so it would take a bit longer but you would have",
    "start": "1328080",
    "end": "1334440"
  },
  {
    "text": "more resources there so that would take it again faster so with the bigger cluster it was roughly the same but",
    "start": "1334440",
    "end": "1342240"
  },
  {
    "text": "uh uh yeah it might differ depending on your cluster but yeah there are still",
    "start": "1342240",
    "end": "1348080"
  },
  {
    "text": "some challenges which make this not perfect the the time is one of them so if it takes several minutes to scale up",
    "start": "1348080",
    "end": "1355240"
  },
  {
    "text": "it means that uh yeah maybe you should not wait for when you have millions of",
    "start": "1355240",
    "end": "1360279"
  },
  {
    "text": "users forming your application because they saw it somewhere uh maybe it will",
    "start": "1360279",
    "end": "1366000"
  },
  {
    "text": "need some time but one thing to keep in mind is that the scaling is expensive",
    "start": "1366000",
    "end": "1372279"
  },
  {
    "text": "right so what actually happens when you scale up or scale down is that the",
    "start": "1372279",
    "end": "1377400"
  },
  {
    "text": "rebalance starts Bo moving around the data and that costs you the resources so that's why it's important to properly",
    "start": "1377400",
    "end": "1383840"
  },
  {
    "text": "configure these timings of the horizontal Port Auto scaler because for",
    "start": "1383840",
    "end": "1389440"
  },
  {
    "text": "example if the horizontal Port Auto scaler would see that the cluster is underutilized and would scale it down",
    "start": "1389440",
    "end": "1394919"
  },
  {
    "text": "then the first thing which happens is that uh the load goes immediately up because it starts moving the data around",
    "start": "1394919",
    "end": "1402080"
  },
  {
    "text": "so you want to avoid that the horizontal po Auto scaler now immediately says oh wait load is up again let's scale it up",
    "start": "1402080",
    "end": "1408000"
  },
  {
    "text": "and kind of plays this this game up and down right so that's why it's important to kind of have these stabilization",
    "start": "1408000",
    "end": "1414600"
  },
  {
    "text": "windows and so on configured for the proper time which it takes to do the scaling to avoid this kind of uh going",
    "start": "1414600",
    "end": "1422960"
  },
  {
    "text": "up and down and uh similarly if you would do the auto scaling then it's good",
    "start": "1422960",
    "end": "1428360"
  },
  {
    "text": "to kind of do the auto scaling when you still have some performance space there and not wait when the cluster is already",
    "start": "1428360",
    "end": "1435480"
  },
  {
    "text": "under heavy load because you will actually need some additional load to shift the data around and get the use of",
    "start": "1435480",
    "end": "1441279"
  },
  {
    "text": "the new node so so that's something what quite uh important uh other challenge is that uh",
    "start": "1441279",
    "end": "1449440"
  },
  {
    "text": "yes starting the new pod that's fast what takes several minutes is moving the data around and that's not easy to to",
    "start": "1449440",
    "end": "1456480"
  },
  {
    "text": "address obviously one of the solutions would be to have uh uh separation of the",
    "start": "1456480",
    "end": "1462159"
  },
  {
    "text": "storage and the compute nodes to basically use steered storage for everything right because then you",
    "start": "1462159",
    "end": "1467399"
  },
  {
    "text": "actually don't have any local data to move and you can just Autos scale uh and there are messaging systems or streaming",
    "start": "1467399",
    "end": "1474200"
  },
  {
    "text": "platforms which which do this but uh the thing is using only the thired storage",
    "start": "1474200",
    "end": "1479919"
  },
  {
    "text": "using only something like S3 object storage uh it works in some use cases",
    "start": "1479919",
    "end": "1485640"
  },
  {
    "text": "but it doesn't work in many other use cases so it's yeah it might help someone but it's not the general solution for",
    "start": "1485640",
    "end": "1491640"
  },
  {
    "text": "everything and in fact in the demo I quite heavily relied on the teered storage but the teered storage itself is",
    "start": "1491640",
    "end": "1497919"
  },
  {
    "text": "something what yeah might work for you but might not work for you if your cluster is full of some data you use for",
    "start": "1497919",
    "end": "1503640"
  },
  {
    "text": "some model training or model validation and the data are kind of consumed again and again and again and again and you",
    "start": "1503640",
    "end": "1510480"
  },
  {
    "text": "store it somewhere in Amazon as free it suddenly might not be that cheap anymore because you actually pay for the for the",
    "start": "1510480",
    "end": "1516600"
  },
  {
    "text": "requests and uh and for the data so uh yeah it's important uh how much data you",
    "start": "1516600",
    "end": "1522720"
  },
  {
    "text": "have and how well the theer storage really uh fits uh your",
    "start": "1522720",
    "end": "1528960"
  },
  {
    "text": "your use cases uh another thing to consider is what's the right metric to do the",
    "start": "1528960",
    "end": "1535279"
  },
  {
    "text": "scaling on now for the Simplicity I use the use the CPU utilization but Kafka",
    "start": "1535279",
    "end": "1541960"
  },
  {
    "text": "has various different metrics for example which cover utilization of different uh thread pools it's using",
    "start": "1541960",
    "end": "1548159"
  },
  {
    "text": "internally and so on so yeah I didn't really go into the detail and to be honest I'm more on the strumsy side of",
    "start": "1548159",
    "end": "1554720"
  },
  {
    "text": "things than on the Kafka side of things so uh uh maybe some of these metrics",
    "start": "1554720",
    "end": "1560320"
  },
  {
    "text": "would kind of work better in different uh configurations in different clusters",
    "start": "1560320",
    "end": "1566440"
  },
  {
    "text": "and uh one can obviously think whether uh we should have some Kafka a auto",
    "start": "1566440",
    "end": "1573000"
  },
  {
    "text": "scaler like for the Kafka consumers uh KDA has support for autoscaling them",
    "start": "1573000",
    "end": "1578240"
  },
  {
    "text": "where it kind of takes into account how is your topic configured how many partitions does it have how many",
    "start": "1578240",
    "end": "1583760"
  },
  {
    "text": "parallel consumers you can actually have and obviously right now here for out scaling we don't have anything like that",
    "start": "1583760",
    "end": "1590360"
  },
  {
    "text": "and if you would have the free Brokers and all your load would be concentrated into free partitions then uh yeah adding",
    "start": "1590360",
    "end": "1597440"
  },
  {
    "text": "fourth or fifth node doesn't really help with it if you have the load like I had it spread over 100 partitions then it's",
    "start": "1597440",
    "end": "1603880"
  },
  {
    "text": "kind of easy to spread it over the new noes uh and uh the final kind of issue",
    "start": "1603880",
    "end": "1611080"
  },
  {
    "text": "to consider is uh whether your cluster actually has free capacity for uh for",
    "start": "1611080",
    "end": "1617320"
  },
  {
    "text": "the Autos scaling right uh Kafka is often pretty big workload it's often 64",
    "start": "1617320",
    "end": "1623960"
  },
  {
    "text": "100 gigs of RAM 10 CPUs per broker and uh uh It's Not Unusual that when you",
    "start": "1623960",
    "end": "1631520"
  },
  {
    "text": "actually try to scale it you might not have capacity in your cluster to actually start this pot on now if you",
    "start": "1631520",
    "end": "1638200"
  },
  {
    "text": "run on premise you simply might need to have the servers there if you run in some Cloud environments then yeah the",
    "start": "1638200",
    "end": "1645360"
  },
  {
    "text": "cluster Auto scalers for kubernetes might help with this something like Carpenter for example but what really",
    "start": "1645360",
    "end": "1652120"
  },
  {
    "text": "happens when you use that right so the horizontal pot autoscaler scales the Kafka cluster streum sees the the scale",
    "start": "1652120",
    "end": "1659799"
  },
  {
    "text": "up it creates the new pot the new po doesn't have anywhere to run so it gets into the pending mode the cluster",
    "start": "1659799",
    "end": "1666039"
  },
  {
    "text": "autoscaler will see that there's pending pot so it will get the new host provisioned it will have to get it ready",
    "start": "1666039",
    "end": "1672039"
  },
  {
    "text": "then once it's ready the pot can actually start there uh and the rebalance can happen once once it's",
    "start": "1672039",
    "end": "1678440"
  },
  {
    "text": "running so it actually works it's not like it's a blocker but the scaling up",
    "start": "1678440",
    "end": "1684640"
  },
  {
    "text": "which was already quite long because of moving the data is now even longer because maybe you need to wait for the",
    "start": "1684640",
    "end": "1689919"
  },
  {
    "text": "new worker node to be provisioned for the new Brokers to to run",
    "start": "1689919",
    "end": "1695080"
  },
  {
    "text": "on uh that said I don't really want to end on a on a kind of negative note with",
    "start": "1695080",
    "end": "1700200"
  },
  {
    "text": "a problem so I think uh uh it was quite useful thing which we",
    "start": "1700200",
    "end": "1706000"
  },
  {
    "text": "did you can really scale the the Kafka cluster uh you just need to keep in mind",
    "start": "1706000",
    "end": "1711279"
  },
  {
    "text": "to scale early and uh it's probably still a bit more suitable for this kind",
    "start": "1711279",
    "end": "1716600"
  },
  {
    "text": "of mid or long-term scaling to kind of uh for example uh scale down your",
    "start": "1716600",
    "end": "1722399"
  },
  {
    "text": "cluster for weekends or for nights if uh kind of your company or your applications follow this this pattern of",
    "start": "1722399",
    "end": "1728120"
  },
  {
    "text": "working hours uh and uh I think the thing to consider is also that sizing",
    "start": "1728120",
    "end": "1734640"
  },
  {
    "text": "Kafka is pretty hard and from my experience there are plenty of very very",
    "start": "1734640",
    "end": "1739720"
  },
  {
    "text": "oversized Kafka clusters where someone simply didn't want you to take the risk that the cluster would be too small and",
    "start": "1739720",
    "end": "1746279"
  },
  {
    "text": "uh yeah maybe that's the uh thing where the Autos scaling can help as well because it might give you a bit more",
    "start": "1746279",
    "end": "1753200"
  },
  {
    "text": "confidence to start with a smaller cluster and have it grow rather than over provision",
    "start": "1753200",
    "end": "1759440"
  },
  {
    "text": "everything and uh yeah now if you would be interested to autoscale your kfka",
    "start": "1759440",
    "end": "1764799"
  },
  {
    "text": "cluster then uh definitely let us know and help us kind of continue this journey find us the right metrics",
    "start": "1764799",
    "end": "1771480"
  },
  {
    "text": "improve the things even more so yeah you can let us know on all the different strey",
    "start": "1771480",
    "end": "1777559"
  },
  {
    "text": "channels uh if you are interested more in streams in general then tomorrow we will have a maintainer track talk uh",
    "start": "1777559",
    "end": "1785480"
  },
  {
    "text": "where yeah we will cover more the general things uh and otherwise we are always looking for new contributors so",
    "start": "1785480",
    "end": "1792679"
  },
  {
    "text": "yeah if you would be interested uh then uh yeah you can definitely join us and",
    "start": "1792679",
    "end": "1799039"
  },
  {
    "text": "that's it and uh I think we should have few minutes for questions if there are",
    "start": "1799039",
    "end": "1805300"
  },
  {
    "text": "[Applause]",
    "start": "1805300",
    "end": "1811920"
  },
  {
    "text": "any uh I think you should go to the to the mic so that it's on the on the record",
    "start": "1812480",
    "end": "1819640"
  },
  {
    "text": "my question is uh when the topic rebalance happens between the the newly added node will the consumers see any",
    "start": "1828480",
    "end": "1836240"
  },
  {
    "text": "like experience and is errors in or slowness when that topics gets moved over to the new node yeah so yeah the",
    "start": "1836240",
    "end": "1844080"
  },
  {
    "text": "consumers so it depends which partition you are consuming but if the if the",
    "start": "1844080",
    "end": "1849919"
  },
  {
    "text": "leader for your partition repca will be one of those things which are moved as part of the rebalance the consumers will",
    "start": "1849919",
    "end": "1856480"
  },
  {
    "text": "will see it now normally they should be able to just kind of find the new leader",
    "start": "1856480",
    "end": "1862559"
  },
  {
    "text": "and just continue where they left but uh yeah it's a good point it's definitely",
    "start": "1862559",
    "end": "1868000"
  },
  {
    "text": "kind of A disruption which happens on the on the consumers as well as on the producers when the Autos scaling uh is",
    "start": "1868000",
    "end": "1875120"
  },
  {
    "text": "happening okay thank you yeah kind of same thing as him that so uh did you see any the data ordering",
    "start": "1875120",
    "end": "1882919"
  },
  {
    "text": "issue so if the auto scale increased to Auto scale and the data moves from 1 plus to another is the ordering of the",
    "start": "1882919",
    "end": "1890559"
  },
  {
    "text": "data maintained or when AO scaled down and the consumer is not you know picked",
    "start": "1890559",
    "end": "1896960"
  },
  {
    "text": "up anything yet and the data moved back right The Ordering of the data in the partitions is maintained so so what what",
    "start": "1896960",
    "end": "1905360"
  },
  {
    "text": "happens is that the Autos scaling doesn't change the number of partitions or anything like that it basically just takes the partition and moves it to the",
    "start": "1905360",
    "end": "1912240"
  },
  {
    "text": "to the new nde and the way it's done it's basically first creates first keeps the original replicas creates a new one",
    "start": "1912240",
    "end": "1919799"
  },
  {
    "text": "waits for the data to be copied and then basically kind of switchs over and deletes one of the old replicas but the",
    "start": "1919799",
    "end": "1925320"
  },
  {
    "text": "ordering uh should be fine thank you any other",
    "start": "1925320",
    "end": "1932399"
  },
  {
    "text": "questions if not then uh yeah thanks for joining and hopefully it was useful",
    "start": "1932399",
    "end": "1939559"
  }
]