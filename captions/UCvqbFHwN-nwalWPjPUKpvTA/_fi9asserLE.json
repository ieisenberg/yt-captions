[
  {
    "text": "so my name is Kevin kluz I'm from Nvidia and my colleague Alexei fomenko here from Intel we're going to be giving a",
    "start": "0",
    "end": "5700"
  },
  {
    "text": "joint talk on device plugins 2.0 how to build a driver for dynamic resource",
    "start": "5700",
    "end": "10740"
  },
  {
    "text": "allocation otherwise known as dra so what exactly is dynamic resource allocation",
    "start": "10740",
    "end": "16560"
  },
  {
    "text": "well it's a new way of requesting access to resources available in kubernetes 126 and Beyond",
    "start": "16560",
    "end": "22080"
  },
  {
    "text": "it provides an alternative to the count based interface of for example asking for nvidia.com gpu2 and using this",
    "start": "22080",
    "end": "30240"
  },
  {
    "text": "alternative interface it puts full control of the API to request resources in the hands of third-party party",
    "start": "30240",
    "end": "36360"
  },
  {
    "text": "developers so if you you know if you have simple devices you can continue to use the existing device plugin interface",
    "start": "36360",
    "end": "42180"
  },
  {
    "text": "but for more complex devices this new mechanism exists to give you a much more powerful interface",
    "start": "42180",
    "end": "48059"
  },
  {
    "text": "one can think of dynamic resource allocation as sort of a generalization of the persistent volume API for all",
    "start": "48059",
    "end": "54000"
  },
  {
    "text": "types of resources not just volumes and the key Concepts that you want to have in mind when you're thinking about Dynamic resource allocation is that of",
    "start": "54000",
    "end": "60600"
  },
  {
    "text": "the resource class and its Associated class parameters which help you define the API for resource classes as well as",
    "start": "60600",
    "end": "66659"
  },
  {
    "text": "resource claims and resource claim templates and their Associated claim parameters which I'll go into a little",
    "start": "66659",
    "end": "71700"
  },
  {
    "text": "bit more detail as we go through the talk so before I talk about those I want to just go through a really quick example",
    "start": "71700",
    "end": "77520"
  },
  {
    "text": "and show you you know demonstrate kind of how you would move from a user's perspective of requesting access to a",
    "start": "77520",
    "end": "83640"
  },
  {
    "text": "device via the traditional device plugin API and what that same request would look like under dra so our traditional",
    "start": "83640",
    "end": "89759"
  },
  {
    "text": "device plugin if you were to ask for a single GPU under your limits section of your resources spec in your in your pod",
    "start": "89759",
    "end": "97380"
  },
  {
    "text": "spec you could ask for something like nvidia.com GPU one and you would get access to that GPU at runtime",
    "start": "97380",
    "end": "103680"
  },
  {
    "text": "under Dynamic resource allocation that a similar",
    "start": "103680",
    "end": "108920"
  },
  {
    "text": "similar allocation would be would be done using what you see here on the right so the things to note here are",
    "start": "108920",
    "end": "115079"
  },
  {
    "text": "that I have this object this separate object for my pod called a resource claim template inside that resource claim template I",
    "start": "115079",
    "end": "121799"
  },
  {
    "text": "give reference to a specific resource class in this case the resource class name is called Nvidia gpu.nvidia.com and",
    "start": "121799",
    "end": "128759"
  },
  {
    "text": "this is something that's associated with the driver that I'm going to you know talk through today how you can develop one of these but this name gets",
    "start": "128759",
    "end": "134879"
  },
  {
    "text": "associated with your driver is installed by the cluster admin and is kind of analogous to the resource type that you",
    "start": "134879",
    "end": "141060"
  },
  {
    "text": "have from the device plugins of nvidia.com GPU on the left once you have this resource claim",
    "start": "141060",
    "end": "147000"
  },
  {
    "text": "template in place you can now in your pod spec there's a new section called resource claims where you can refer back",
    "start": "147000",
    "end": "152520"
  },
  {
    "text": "to the name of that resource claim template create a local name for your various containers within your pod to",
    "start": "152520",
    "end": "157980"
  },
  {
    "text": "reference that and then put that under underneath a new claims section in your in your resources spec for your container once you've done",
    "start": "157980",
    "end": "164700"
  },
  {
    "text": "that the driver under the hood will you know kick in to allocate a GPU for your for your container and when your",
    "start": "164700",
    "end": "170580"
  },
  {
    "text": "container comes up you'll have access to that GPU if you then expanded this to where on the existing device plugin API",
    "start": "170580",
    "end": "176640"
  },
  {
    "text": "you were to request two gpus on the right that would basically be referring to this claim template multiple times",
    "start": "176640",
    "end": "182280"
  },
  {
    "text": "having different local names for those for the gpus that you want to access and",
    "start": "182280",
    "end": "187860"
  },
  {
    "text": "then you know plugging those into the actual claims section of your container to get access to the gpus that are",
    "start": "187860",
    "end": "194819"
  },
  {
    "text": "represented by that so with that kind of simple example in",
    "start": "194819",
    "end": "200220"
  },
  {
    "text": "place obviously that's not you know it's a it's much more verbose than you have with the existing device plugin API so",
    "start": "200220",
    "end": "205440"
  },
  {
    "text": "that you know the obvious question is why would I want to do things this way and I hope by the end of this talk you'll be convinced that this Dynamic",
    "start": "205440",
    "end": "211680"
  },
  {
    "text": "resource allocation way of getting access to these types of resources is much more powerful um so yeah so you know the first concept",
    "start": "211680",
    "end": "218040"
  },
  {
    "text": "that I talked about is this notion of a resource class and a resource class basically Associates a named resource",
    "start": "218040",
    "end": "223200"
  },
  {
    "text": "with its corresponding resource drivers so if you remember from the last Slide the the resource class that I defined",
    "start": "223200",
    "end": "228959"
  },
  {
    "text": "was called gpu.nvidia.com and the driver that I would install on my on my machine to to to to be able to allocate resource",
    "start": "228959",
    "end": "236640"
  },
  {
    "text": "classes of type GPU and video.com in this case would have the name gpu.resource.nvidia.com",
    "start": "236640",
    "end": "243120"
  },
  {
    "text": "um along with these resource classes though which is something different than what you can do with with persistent volumes API is you can associate a",
    "start": "243540",
    "end": "249900"
  },
  {
    "text": "optional class parameters object which has a completely custom API that's up to you as the resource driver developer to",
    "start": "249900",
    "end": "256199"
  },
  {
    "text": "to Define what this looks like and so in the case of Nvidia gpus this is actually a real claim class parameters object",
    "start": "256199",
    "end": "261720"
  },
  {
    "text": "that we have for the driver that we built for gpus you can set up something like saying that any gpus that are",
    "start": "261720",
    "end": "267840"
  },
  {
    "text": "allocated by this resource class cannot be shared so if you want to make sure that you have exclusive access to the",
    "start": "267840",
    "end": "272940"
  },
  {
    "text": "GPU versus multiple people you know referencing this resource type and you don't want them to share access to it if",
    "start": "272940",
    "end": "279540"
  },
  {
    "text": "you if the admin decides to install this class parameters object along with his resource class it'll limit that sharing",
    "start": "279540",
    "end": "286080"
  },
  {
    "text": "capability now moving on to Resource claims this is kind of the the user side analogous",
    "start": "286080",
    "end": "291180"
  },
  {
    "text": "thing to the resource class where the resource claim represents the actual resource allocation to be made by a",
    "start": "291180",
    "end": "296400"
  },
  {
    "text": "resource driver as defined by the by the end user right they create these objects they refer to the resource classes that",
    "start": "296400",
    "end": "302699"
  },
  {
    "text": "they want to allocate resources for and then when they're referenced inside the Pod these resources get injected into it",
    "start": "302699",
    "end": "308940"
  },
  {
    "text": "at runtime or the main difference between a resource claim template and a resource claim is the resource claim templates create a new resource claim on",
    "start": "308940",
    "end": "316080"
  },
  {
    "text": "the Fly each time that they are referenced and so you know from the example I had before the the end results of this in the case of gpus is that you",
    "start": "316080",
    "end": "322560"
  },
  {
    "text": "get a unique GPU for each reference to one of these resource claim templates on the flip side if you have a resource",
    "start": "322560",
    "end": "328680"
  },
  {
    "text": "claim that's not a template it always refers to the exact same object anytime you refer to it which basically enables",
    "start": "328680",
    "end": "335400"
  },
  {
    "text": "you to have shared access to a GPU for each reference um so just as with resource classes",
    "start": "335400",
    "end": "342660"
  },
  {
    "text": "resource Claims can include an optional set of claim parameters with whatever custom API you've decided to Define for",
    "start": "342660",
    "end": "348479"
  },
  {
    "text": "your resource type so for NVIDIA gpus one of the claim parameters objects that",
    "start": "348479",
    "end": "353580"
  },
  {
    "text": "we've created is called GPU claim parameters where the example that I'm showing here is basically letting you say okay whenever someone asks for uh or",
    "start": "353580",
    "end": "360900"
  },
  {
    "text": "when I'm when I create a claim that wants to access a GPU um in this example I can say okay that",
    "start": "360900",
    "end": "367199"
  },
  {
    "text": "GPU has to either be of the product family T4 or a V100 with less than or",
    "start": "367199",
    "end": "373440"
  },
  {
    "text": "equal to 16 gigabytes of memory on it right so it lets you kind of selectively dive in and more precisely ask for the",
    "start": "373440",
    "end": "379740"
  },
  {
    "text": "type of GPU that you that you want to get access to Additionally you can specify you know extended parameters",
    "start": "379740",
    "end": "385800"
  },
  {
    "text": "such as what Strat sharing strategy you might want to enable for the resource once it's been granted to you so Nvidia",
    "start": "385800",
    "end": "392220"
  },
  {
    "text": "gpus have a couple different sharing strategies you can use one is time slicing one is MPS which allows you to",
    "start": "392220",
    "end": "397740"
  },
  {
    "text": "you know further subject the memory that you have amongst different clients that are sharing access to the GPU and you",
    "start": "397740",
    "end": "403500"
  },
  {
    "text": "can specify all of this as part of this custom claim parameters object that you know we've defined as part of our API",
    "start": "403500",
    "end": "409620"
  },
  {
    "text": "for accessing gpus with dra um so assuming yeah that you had one of these uh claims created with the name",
    "start": "409620",
    "end": "416520"
  },
  {
    "text": "share GPU I can reference that in my pod spec and then multiple containers within that pod",
    "start": "416520",
    "end": "421800"
  },
  {
    "text": "can reference that exact same uh claim and get shared access to that underlying GPU and it's not limited to within a pod",
    "start": "421800",
    "end": "428340"
  },
  {
    "text": "you can do the same thing across pod as long as you've created this this resource claim as a global object you",
    "start": "428340",
    "end": "433620"
  },
  {
    "text": "reference it now uh different containers from different pods can access that same GPU now one thing I don't have on the",
    "start": "433620",
    "end": "439919"
  },
  {
    "text": "slides but it's probably worth mentioning is that we do resource claims are isolated to a",
    "start": "439919",
    "end": "446160"
  },
  {
    "text": "specific namespace so you can't share these gpus across different namespaces once a resource claim exists in one",
    "start": "446160",
    "end": "452280"
  },
  {
    "text": "namespace the pods that are running have to be in that same name space in order to access the resource claims that are",
    "start": "452280",
    "end": "458099"
  },
  {
    "text": "doing created as part of it as a security measure okay so kind of walking through those",
    "start": "458099",
    "end": "463440"
  },
  {
    "text": "simple examples the goal of this talk is really to teach you how to write your own dra resource driver in order to enable similar features on your own",
    "start": "463440",
    "end": "469800"
  },
  {
    "text": "custom resources right it's it's great that we've you know taken the initiative built this initial driver for for NVIDIA",
    "start": "469800",
    "end": "475500"
  },
  {
    "text": "gpus but I really want to you know enable you know third-party Developers for whatever devices that you guys might",
    "start": "475500",
    "end": "480660"
  },
  {
    "text": "want to make available to be able to do something similar um yeah so with that the outline of the",
    "start": "480660",
    "end": "486780"
  },
  {
    "text": "rest of the talk is that I'm basically going to walk through the anatomy of one what one of these dra resource drivers looks like and how they work under the",
    "start": "486780",
    "end": "492960"
  },
  {
    "text": "hood I'm then going to walk through the process of what it takes to actually allocate a resource",
    "start": "492960",
    "end": "498120"
  },
  {
    "text": "um using dra what what happens behind the scenes once you create a resource claim and uh the resource that you're",
    "start": "498120",
    "end": "504539"
  },
  {
    "text": "asking for finally gets injected into your container and you have access to it and I'm going to walk through the process of some helper libraries that we",
    "start": "504539",
    "end": "511080"
  },
  {
    "text": "have to teach you how to build one of these resource drivers and what functions and methods you need to",
    "start": "511080",
    "end": "516240"
  },
  {
    "text": "implement in order to um make your resources available in a similar way then I'm going to hand it",
    "start": "516240",
    "end": "521880"
  },
  {
    "text": "over to Alexa who's going to talk about some of the new and upcoming features that um that we're building in relation to",
    "start": "521880",
    "end": "528120"
  },
  {
    "text": "dra and end with a demo both on Nvidia gpus and Intel gpus to show the flexibility of this across",
    "start": "528120",
    "end": "535080"
  },
  {
    "text": "different resource types okay so what is one of these dra resource drivers look like well at its",
    "start": "535080",
    "end": "541620"
  },
  {
    "text": "core it basically consists of two separate but coordinating components you have a centralized controller that's",
    "start": "541620",
    "end": "546779"
  },
  {
    "text": "running somewhere in your cluster with high availability and then you have a node local coolant plugin that's running",
    "start": "546779",
    "end": "551820"
  },
  {
    "text": "it as a Daemon set on the nodes that where the resources themselves actually need to be advertised and eventually",
    "start": "551820",
    "end": "556860"
  },
  {
    "text": "prepared for use um the centralized controller its job is basically to coordinate with the",
    "start": "556860",
    "end": "562080"
  },
  {
    "text": "kubernetes scheduler to decide which nodes an incoming resource claim can actually be serviced on once it's made",
    "start": "562080",
    "end": "567779"
  },
  {
    "text": "that decision it performs the actual resource claim allocation after the schedulers picked a node where that you",
    "start": "567779",
    "end": "573360"
  },
  {
    "text": "know resource or that pod should actually land to get access to that resource and then you know once everything's done it will perform the",
    "start": "573360",
    "end": "579720"
  },
  {
    "text": "deallocation of the resource claim once it gets deleted at some point in the future",
    "start": "579720",
    "end": "584760"
  },
  {
    "text": "on the other side the no local Kuba plug-in its job is to basically advertise any node local state that the",
    "start": "584760",
    "end": "591180"
  },
  {
    "text": "centralized controller will need in order to make any allocation decisions at runtime also once those allocations have been",
    "start": "591180",
    "end": "598200"
  },
  {
    "text": "made it will then perform any node local operations that are required as part of preparing or unpreparing the resource",
    "start": "598200",
    "end": "603720"
  },
  {
    "text": "claim and we'll go through a couple of examples later on of what what this might entail rather than just you know",
    "start": "603720",
    "end": "608760"
  },
  {
    "text": "device not might not just be already ready to go you might have to you know set up some parameters on it depending",
    "start": "608760",
    "end": "614940"
  },
  {
    "text": "on what the actual resource claim coming in looks like and then once it's done that its job is",
    "start": "614940",
    "end": "620339"
  },
  {
    "text": "to pass the devices associated with that prepared resource claim to the kublet so they can eventually forward them on to",
    "start": "620339",
    "end": "625740"
  },
  {
    "text": "the underlying container runtime and make those resources available to the running container",
    "start": "625740",
    "end": "631620"
  },
  {
    "text": "so I talked about you know these two pieces obviously need to be able to communicate in some manner right centers",
    "start": "631620",
    "end": "637380"
  },
  {
    "text": "controller makes allocation decisions Google plugin tells it what resources are available so they can make those allocation decisions and so there's you",
    "start": "637380",
    "end": "644220"
  },
  {
    "text": "know there's a couple different ways that this communication Could Happen um for the purposes of this talk I'm focusing mostly on on this first one",
    "start": "644220",
    "end": "650880"
  },
  {
    "text": "which is kind of a single all-purpose per node crd which can exist in your cluster where the Kubla plugin will",
    "start": "650880",
    "end": "656820"
  },
  {
    "text": "advertise all available resources that it has the centralized controller will track any resources that it's allocated",
    "start": "656820",
    "end": "663720"
  },
  {
    "text": "in the same crd and then the Kubler plugin will also track any resources It",
    "start": "663720",
    "end": "668760"
  },
  {
    "text": "prepares inside the crd so there's from one crd you can get the full view of everything that's associated with this",
    "start": "668760",
    "end": "674279"
  },
  {
    "text": "driver in terms of what's available how things have been allocated and what's actually you know been set up on the",
    "start": "674279",
    "end": "680160"
  },
  {
    "text": "Node for for someone to get access to alternatively you don't have to do it this way you could have some sort of",
    "start": "680160",
    "end": "685620"
  },
  {
    "text": "split purpose communication where you know the Kubla plugin might still advertise its available resources for a",
    "start": "685620",
    "end": "690720"
  },
  {
    "text": "crd via Sear D so the centralized controller has an easy way to to to access these but then the centralized",
    "start": "690720",
    "end": "698100"
  },
  {
    "text": "controller might actually track all this its allocated resources down through this field that we have in the resource",
    "start": "698100",
    "end": "704640"
  },
  {
    "text": "claim itself called a resource handle it might not need to be something that's stored in the crd accessible via the API",
    "start": "704640",
    "end": "710100"
  },
  {
    "text": "server um for for for making things a bit more efficient and then if the code at the",
    "start": "710100",
    "end": "715620"
  },
  {
    "text": "Kubla level instead of using a crd where you might have you know constant conflicts when you're writing back and",
    "start": "715620",
    "end": "720660"
  },
  {
    "text": "forth to it across these different components instead you can just checkpoint the state on a in a file on the Node local file system so long as",
    "start": "720660",
    "end": "727019"
  },
  {
    "text": "the Kubler plugin has a way to to track this over time um but like I said we're going to focus",
    "start": "727019",
    "end": "732240"
  },
  {
    "text": "at least for the purpose of this talk on the single all-purpose one just because it's easier to talk about if it's all in one place",
    "start": "732240",
    "end": "738480"
  },
  {
    "text": "um and what this might actually look like under the hood is that the Kubler plug-in when it first comes online it'll advertise some set of allocatable",
    "start": "738480",
    "end": "744839"
  },
  {
    "text": "devices that the controller at some point in the future could allocate when the centralized controller is",
    "start": "744839",
    "end": "750240"
  },
  {
    "text": "triggered to do an allocation you could pick one of those devices write some information back to the crd about What",
    "start": "750240",
    "end": "756360"
  },
  {
    "text": "GPU has been allocated to a specific claim and then once the Kuba plugin kicks in and prepares this claim for use it can",
    "start": "756360",
    "end": "763560"
  },
  {
    "text": "also write this back to the crd saying okay you allocated that and now I have prepared it for use before it gets",
    "start": "763560",
    "end": "768720"
  },
  {
    "text": "passed off to the container that gets started later on so what does this process actually look",
    "start": "768720",
    "end": "775380"
  },
  {
    "text": "like so how do we actually allocate a resource using one of these drivers well there's two modes of operation for this",
    "start": "775380",
    "end": "780480"
  },
  {
    "text": "one is called immediate and one is called delayed or wait for uh first consumer where the main difference",
    "start": "780480",
    "end": "786180"
  },
  {
    "text": "between them is that with immediate allocation and this is something you can specify in your in your resource claim",
    "start": "786180",
    "end": "791820"
  },
  {
    "text": "as you create it as an object in the cluster and with immediate allocation",
    "start": "791820",
    "end": "797760"
  },
  {
    "text": "as soon as you create this resource claim that's going to trigger your your resource driver to allocate it on some",
    "start": "797760",
    "end": "804000"
  },
  {
    "text": "node somewhere in the cluster independent of what pods might actually try and come along later to access it",
    "start": "804000",
    "end": "809040"
  },
  {
    "text": "and the project that do reference that claim will end up being restricted to the nodes where those allocations have been made so it's a bit more restrictive",
    "start": "809040",
    "end": "815220"
  },
  {
    "text": "in the sense that you don't know where these pods what are the resource constraints they might have so doing an immediate allocation is really a means",
    "start": "815220",
    "end": "821700"
  },
  {
    "text": "to say I know that anytime some pod references that I want it to be on the specific node whereas the delayed",
    "start": "821700",
    "end": "827760"
  },
  {
    "text": "allocation delays the allocation of the resource claim until the first pod that references it is being scheduled and",
    "start": "827760",
    "end": "833339"
  },
  {
    "text": "there's analogous things in the persistent volumes API so for those of you that are familiar with that this",
    "start": "833339",
    "end": "838620"
  },
  {
    "text": "might not seem too foreign and when you do the delayed style allocation resource",
    "start": "838620",
    "end": "844320"
  },
  {
    "text": "availability will be considered as part of that overall pod scheduling decision for the first one that that accesses it",
    "start": "844320",
    "end": "850920"
  },
  {
    "text": "and we're going to focus because it's the more complicated one we're going to focus on the delayed one for as I walk through the process of allocating a",
    "start": "850920",
    "end": "856980"
  },
  {
    "text": "resource using a dra driver um so assuming that the sysadmin has come along he's deployed his dra",
    "start": "856980",
    "end": "862980"
  },
  {
    "text": "resource driver um got a centralized controller set up demon sets running with the kublet",
    "start": "862980",
    "end": "868500"
  },
  {
    "text": "plugins he's also installed some resource class that um you know enables someone to come",
    "start": "868500",
    "end": "874019"
  },
  {
    "text": "along and create a resource claim that references a specific resource type that that driver knows how to service",
    "start": "874019",
    "end": "879800"
  },
  {
    "text": "a user can then come along create one of these claims create a pot that references that resource class create a",
    "start": "879800",
    "end": "886800"
  },
  {
    "text": "pod that then references one of those resource claims which is then going to trigger the kubernetes scheduler to actually see that pod and start the",
    "start": "886800",
    "end": "893040"
  },
  {
    "text": "scheduling process right when it does that it's going to generate a list of potential nodes where it",
    "start": "893040",
    "end": "899519"
  },
  {
    "text": "thinks that you know where it where it knows that it could potentially put that pod independent of the the resource",
    "start": "899519",
    "end": "905699"
  },
  {
    "text": "driver giving it in for any information at this point about you know where this resource itself can actually be allocated and when it does that it",
    "start": "905699",
    "end": "912660"
  },
  {
    "text": "creates a new API server object called a pod scheduling context which the centralized controller is able to pick",
    "start": "912660",
    "end": "919019"
  },
  {
    "text": "up and see this list of generated potential nodes and help modify that and you know trim down to the point where an",
    "start": "919019",
    "end": "925440"
  },
  {
    "text": "actual scheduling decision can be made so once a centralized controller picks up this list of generated potential",
    "start": "925440",
    "end": "930720"
  },
  {
    "text": "nodes it will narrow down that list of nodes to the ones where it knows it can allocate the resource associated with",
    "start": "930720",
    "end": "936060"
  },
  {
    "text": "this resource claim it'll write that narrow down list back to the Pod scheduling context who will then be",
    "start": "936060",
    "end": "941579"
  },
  {
    "text": "picked up by the kubernetes scheduler to help you know figure out which node we should actually allocate this uh this or",
    "start": "941579",
    "end": "947820"
  },
  {
    "text": "schedule this pod on and this process could end up completing repeating over and over again until the",
    "start": "947820",
    "end": "953820"
  },
  {
    "text": "actual uh node has been found where this resource is able to be to be allocated but once that is found the kubernetes",
    "start": "953820",
    "end": "961079"
  },
  {
    "text": "schedule will select a node write that back to the Pod scheduling context which will be picked up by the centralized",
    "start": "961079",
    "end": "966660"
  },
  {
    "text": "controller who then goes through the process of actually allocating the claim knowing that this is the node where that",
    "start": "966660",
    "end": "972300"
  },
  {
    "text": "resource needs to be made available it writes that allocation back to the resource claim object",
    "start": "972300",
    "end": "977579"
  },
  {
    "text": "the thing gets picked up by the kubernetes scheduler who does the Pod scheduling writes the node name back to",
    "start": "977579",
    "end": "983399"
  },
  {
    "text": "the Pod where that um where that pod has been scheduled which then through the normal processes",
    "start": "983399",
    "end": "988500"
  },
  {
    "text": "that already exist will be picked up by The kublet Who then will call into the um the coupler plug-in associated with",
    "start": "988500",
    "end": "994800"
  },
  {
    "text": "your driver passing all the claim information associated with that resource claim to it who will then",
    "start": "994800",
    "end": "999959"
  },
  {
    "text": "generate a list of what we call CDI devices which you can then pass on to the container runtime will then start",
    "start": "999959",
    "end": "1006380"
  },
  {
    "text": "your container with access to the resources that your driver has allocated so it's kind of there's a lot of back",
    "start": "1006380",
    "end": "1012440"
  },
  {
    "text": "and forth going on but most of this is hidden from you and we as I mentioned we have helper libraries that abstract a",
    "start": "1012440",
    "end": "1017779"
  },
  {
    "text": "lot of this out so you don't have to worry about all this coordination back and forth across all these components",
    "start": "1017779",
    "end": "1023300"
  },
  {
    "text": "um now you know I said we're going to focus on delayed allocation but it's worth looking just really quickly what immediate allocation would look like so",
    "start": "1023300",
    "end": "1029240"
  },
  {
    "text": "assuming you had a user that came along created a resource claim referencing a resource class as I mentioned before the minute you create this claim that's",
    "start": "1029240",
    "end": "1035660"
  },
  {
    "text": "going to trigger the centralized controller to make this allocation on some node in the cluster so that at some",
    "start": "1035660",
    "end": "1040819"
  },
  {
    "text": "point in the future when the when the user comes along creates a pod that's going to get picked up by the",
    "start": "1040819",
    "end": "1045918"
  },
  {
    "text": "kubernetes scheduler now he's going to use the information about where that resource claim has been allocated to",
    "start": "1045919",
    "end": "1050960"
  },
  {
    "text": "make his scheduling decision to trigger that whole process on the right again Okay so",
    "start": "1050960",
    "end": "1057860"
  },
  {
    "text": "that in a nutshell it's a long complicated process in many ways but you know",
    "start": "1057860",
    "end": "1063140"
  },
  {
    "text": "the rest of this talk is now dedicated to knowing that that background information about how all this happens how do you actually build one of these",
    "start": "1063140",
    "end": "1069620"
  },
  {
    "text": "resource drivers yourself right and so this slide here if you if you take nothing else away from this talk and you",
    "start": "1069620",
    "end": "1075440"
  },
  {
    "text": "guys want to build a resource driver this is the slide to remember because we've created an example resource driver",
    "start": "1075440",
    "end": "1081200"
  },
  {
    "text": "that we've you know put as a repo underneath the kubernetes Sig organization on GitHub that has an",
    "start": "1081200",
    "end": "1088940"
  },
  {
    "text": "example of all of this it provides a fully functional Dre resource driver on a set of mock gpus it wraps everything",
    "start": "1088940",
    "end": "1094880"
  },
  {
    "text": "in a Helm chart so you can easily deploy it it provides scripts for bringing up a kind cluster to test all this in a",
    "start": "1094880",
    "end": "1100880"
  },
  {
    "text": "multi-node setup and it runs on Mac and Linux without requiring any specialized Hardware right so you can clone this",
    "start": "1100880",
    "end": "1106640"
  },
  {
    "text": "repo run a couple scripts see the demo running and then dig into the details yourself to kind of Tinker around with",
    "start": "1106640",
    "end": "1112760"
  },
  {
    "text": "it and figure out how everything works um and the readme itself includes a demo with four example deployments that you",
    "start": "1112760",
    "end": "1118880"
  },
  {
    "text": "can see here that are mostly revolve around how do you enable some of these sharing capabilities given that you have",
    "start": "1118880",
    "end": "1124880"
  },
  {
    "text": "resource claims at your disposal now um and yeah like I said we encourage you to Fork this project and and play around",
    "start": "1124880",
    "end": "1132260"
  },
  {
    "text": "with it yourself um but in a nutshell what it takes to actually build your own dra resource",
    "start": "1132260",
    "end": "1138140"
  },
  {
    "text": "driver is you want to decide on a name for your driver in our case in the example so far I've been talking about you know gpu.nvidia.com but in your case",
    "start": "1138140",
    "end": "1145580"
  },
  {
    "text": "you would come up with some name for the driver for the resource type that you're trying to advertise you would then need to decide on a communication strategy",
    "start": "1145580",
    "end": "1151520"
  },
  {
    "text": "whether you're going to use a single purpose crd like I've been showing up until now or some combination of the",
    "start": "1151520",
    "end": "1156799"
  },
  {
    "text": "split purpose communication which was presented earlier as well",
    "start": "1156799",
    "end": "1162200"
  },
  {
    "text": "um they're going to need to Define some types to represent your allocatable resources your allocated resources and",
    "start": "1162200",
    "end": "1167539"
  },
  {
    "text": "your prepared resources so they can be tracked on these three different levels you also then need to find some types to",
    "start": "1167539",
    "end": "1173299"
  },
  {
    "text": "represent your class parameters you want for your resources and any claim parameters that you want to define the API for accessing your resources you",
    "start": "1173299",
    "end": "1180679"
  },
  {
    "text": "should prepare at least one default resource class for distribution with your resource drivers so in the case of gpus we have that one resource class",
    "start": "1180679",
    "end": "1186860"
  },
  {
    "text": "called gpu.nvidia.com that you then access through the claim parameters object which can help you dig in to get",
    "start": "1186860",
    "end": "1193460"
  },
  {
    "text": "a GPU type that you actually want there's been boilerplate code you can pull into your project to register a",
    "start": "1193460",
    "end": "1199039"
  },
  {
    "text": "controller with the scheduler there's boilerplate code you can pull in to register your plugin with the kubelet",
    "start": "1199039",
    "end": "1205160"
  },
  {
    "text": "and then the fun part you sit down and you write the business Logic for your controller and your kublet plugin and that's what",
    "start": "1205160",
    "end": "1211340"
  },
  {
    "text": "I'm going to focus on today because this is you know where um the particulars of your specific driver come in",
    "start": "1211340",
    "end": "1217700"
  },
  {
    "text": "um that where you're going to need to you know implement the logic for for how you make your resources available and we",
    "start": "1217700",
    "end": "1223039"
  },
  {
    "text": "provide a couple of helper libraries to make this a little bit easier so the first one is the controller helper library and its job is to kind of",
    "start": "1223039",
    "end": "1229460"
  },
  {
    "text": "abstract this whole communication between the scheduler and this coordination back and forth to how to to",
    "start": "1229460",
    "end": "1234860"
  },
  {
    "text": "where you have to you know figure out what node you want your allocation to land on that all happens behind the",
    "start": "1234860",
    "end": "1241220"
  },
  {
    "text": "scenes and we provide a driver interface with five functions that you need to implement and as long as you implement",
    "start": "1241220",
    "end": "1246620"
  },
  {
    "text": "these five functions behind the scenes the rest of the library code will make sure that",
    "start": "1246620",
    "end": "1252559"
  },
  {
    "text": "um yeah all of all of that talking to the scheduler and making sure that you",
    "start": "1252559",
    "end": "1257960"
  },
  {
    "text": "um uh you know eventually do the allocation at the proper point in the in",
    "start": "1257960",
    "end": "1263360"
  },
  {
    "text": "the life cycle of the claim is abstracted behind all of these function calls and so I'm just going to walk through each of these one by one so",
    "start": "1263360",
    "end": "1270559"
  },
  {
    "text": "um the first set are the get class parameters and get claim parameters functions and these are the easy ones to",
    "start": "1270559",
    "end": "1276320"
  },
  {
    "text": "implement many ways once you've defined your class parameters and your claim parameter object types the only thing that you that these functions do is",
    "start": "1276320",
    "end": "1283220"
  },
  {
    "text": "basically give you a an API into um or a hook into a point where you can",
    "start": "1283220",
    "end": "1289580"
  },
  {
    "text": "say okay I see that for the specific class or claim there's a claim parameters reference specified inside it",
    "start": "1289580",
    "end": "1297140"
  },
  {
    "text": "and this gives me the opportunity to pull that reference the actual object associated with that reference down from",
    "start": "1297140",
    "end": "1302480"
  },
  {
    "text": "the API server and then return it through this interface object that's the return type from these functions and",
    "start": "1302480",
    "end": "1307820"
  },
  {
    "text": "what that basically does is it makes these objects now available in all of the other calls without you having to read pull that from the API server every",
    "start": "1307820",
    "end": "1315020"
  },
  {
    "text": "time um the second one is the unsuitable nodes function this is this function is",
    "start": "1315020",
    "end": "1322039"
  },
  {
    "text": "the one that kind of sits behind the scenes during this whole loop with the scheduler back and forth about deciding",
    "start": "1322039",
    "end": "1327860"
  },
  {
    "text": "which node you actually want a allocation to be or pod to be scheduled on to to satisfy the allocation that",
    "start": "1327860",
    "end": "1334159"
  },
  {
    "text": "you're trying to make um and what you basically do in the body of this function is that you need to Loop through the potential nodes that",
    "start": "1334159",
    "end": "1340039"
  },
  {
    "text": "you're passed in search of available resources on those nodes and then write back this narrowed down list of nodes",
    "start": "1340039",
    "end": "1345980"
  },
  {
    "text": "where the resources are unavailable into this claim allocation struct that gets passed to you right and so you can",
    "start": "1345980",
    "end": "1351140"
  },
  {
    "text": "imagine that you know there's a whole bunch of nodes there's a whole bunch of claims associated with the Pod you're trying to launch and you just need to",
    "start": "1351140",
    "end": "1356659"
  },
  {
    "text": "write that logic to figure out where these resources can actually be made available for that pod",
    "start": "1356659",
    "end": "1362440"
  },
  {
    "text": "the next call is allocate so this is the call that gets invoked as part of the",
    "start": "1362440",
    "end": "1369559"
  },
  {
    "text": "the the you know the process that you see on the bottom here with delayed on the left and immediate on the right so",
    "start": "1369559",
    "end": "1375679"
  },
  {
    "text": "once the scheduler is actually selected a node and the centralized controller is triggered to make an allocation this is",
    "start": "1375679",
    "end": "1381620"
  },
  {
    "text": "the function that you implement to to define the logic for how that allocation actually happens right and you end up",
    "start": "1381620",
    "end": "1387440"
  },
  {
    "text": "writing that information back to the the crd if you're using the centralized",
    "start": "1387440",
    "end": "1392679"
  },
  {
    "text": "crd approach for communication um and yeah associated with that is the",
    "start": "1392679",
    "end": "1398840"
  },
  {
    "text": "the return type here of of an allocation result which has a field inside of it called a resource handle which is just",
    "start": "1398840",
    "end": "1405500"
  },
  {
    "text": "some some set of opaque data attached to the claim that can be passed back to the kublet for arbitrary interpretation",
    "start": "1405500",
    "end": "1410720"
  },
  {
    "text": "right so you can think of this as basically just a a long string that uh",
    "start": "1410720",
    "end": "1416840"
  },
  {
    "text": "that is that kubernetes doesn't know anything about and it's just a way for your controller to communicate with your Kubler plugin and he can interpret this",
    "start": "1416840",
    "end": "1423559"
  },
  {
    "text": "this data however however he wants um and then obviously the last one is is the deallocate call so once the",
    "start": "1423559",
    "end": "1429500"
  },
  {
    "text": "deallocate what if the claim itself gets deleted deallocate will be triggered and you can clean up anything that you've",
    "start": "1429500",
    "end": "1434840"
  },
  {
    "text": "done from your from your allocate call um and that was all on the controller",
    "start": "1434840",
    "end": "1440780"
  },
  {
    "text": "side so now on the on the Google plug-in side there's basically two um two components that you need to be aware of",
    "start": "1440780",
    "end": "1446780"
  },
  {
    "text": "one is a helper Library which lets you set up registration and actually get your Kubler plugin talking to the kublet",
    "start": "1446780",
    "end": "1453020"
  },
  {
    "text": "uh to begin with and then there is the Kubla plugin API which defines two calls",
    "start": "1453020",
    "end": "1458419"
  },
  {
    "text": "node prepare resource and node unprepared resource and uh under the hood the you know",
    "start": "1458419",
    "end": "1465200"
  },
  {
    "text": "they're they're fairly straightforward when you get a node prepare resource request call coming in you have all the",
    "start": "1465200",
    "end": "1470960"
  },
  {
    "text": "information associated with the claim that you're supposed to prepare the resource for and you do that you take",
    "start": "1470960",
    "end": "1476659"
  },
  {
    "text": "that information and at the end of the day you need to pass back a set of CDI devices and you know I keep throwing",
    "start": "1476659",
    "end": "1482659"
  },
  {
    "text": "this term around CDI devices Without Really defining it it's a bit out of scope to to talk through what CDI devices are but it's a cncs sponsored",
    "start": "1482659",
    "end": "1490159"
  },
  {
    "text": "project for standardizing on how devices can be made available to containers and",
    "start": "1490159",
    "end": "1497780"
  },
  {
    "text": "we leverage that in this VRE project and it becomes kind of the foundational piece for how you actually make these",
    "start": "1497780",
    "end": "1502880"
  },
  {
    "text": "resources available at the end of the day and I encourage you to look up CDI container device interface to learn more",
    "start": "1502880",
    "end": "1509600"
  },
  {
    "text": "about this but it also is built into this example driver that I have so if you just go into the code and look at it",
    "start": "1509600",
    "end": "1515080"
  },
  {
    "text": "it should be pretty self-explanatory how it works from there",
    "start": "1515080",
    "end": "1520580"
  },
  {
    "text": "um yeah and then on the last call unprepared resource request you get a similar",
    "start": "1520580",
    "end": "1525679"
  },
  {
    "text": "um struck pass to you with the claim information that you can use to undo uh any preparation that you've made",
    "start": "1525679",
    "end": "1532640"
  },
  {
    "text": "um via the prepared call yeah and with that I'm going to hand it over to Alexa who's going to talk about",
    "start": "1532640",
    "end": "1538400"
  },
  {
    "text": "some of the new and upcoming features with uh with with with the GRE resource",
    "start": "1538400",
    "end": "1543500"
  },
  {
    "text": "drivers in general",
    "start": "1543500",
    "end": "1546039"
  },
  {
    "text": "thanks Kevin Kyle so what do you have just heard",
    "start": "1550400",
    "end": "1555620"
  },
  {
    "text": "was available since the last release 126 and in the release that we have received",
    "start": "1555620",
    "end": "1562279"
  },
  {
    "text": "last week 127 we have a new Improvement of the dynamic resource",
    "start": "1562279",
    "end": "1567919"
  },
  {
    "text": "allocation in particular it is possible Now to create a custom resource con resource drivers and controllers to",
    "start": "1567919",
    "end": "1576020"
  },
  {
    "text": "allocate several kinds of devices or maybe similar kinds of devices but as at",
    "start": "1576020",
    "end": "1581240"
  },
  {
    "text": "the same time in this case the communications or",
    "start": "1581240",
    "end": "1586580"
  },
  {
    "text": "bookkeeping what resources and how many resources were allocated already have to",
    "start": "1586580",
    "end": "1591860"
  },
  {
    "text": "be done somehow publicly so that both of the controllers that allocate the",
    "start": "1591860",
    "end": "1597140"
  },
  {
    "text": "resources can access this data",
    "start": "1597140",
    "end": "1601360"
  },
  {
    "text": "consider that the native or original resource Driver allocates part of the",
    "start": "1602840",
    "end": "1608120"
  },
  {
    "text": "resources then the custom resource controller has to know about that decision and other way around if the",
    "start": "1608120",
    "end": "1614120"
  },
  {
    "text": "custom controller that is a supplement supplements in the original resource driver controller makes a decision and",
    "start": "1614120",
    "end": "1620059"
  },
  {
    "text": "allocates a bunch of Hardware then the original controller has to know about the decision so that there is no",
    "start": "1620059",
    "end": "1625520"
  },
  {
    "text": "overbooking or a false decision and there are at least two use case for",
    "start": "1625520",
    "end": "1633620"
  },
  {
    "text": "these scenarios one is when you have different kinds of Hardware that somehow",
    "start": "1633620",
    "end": "1639380"
  },
  {
    "text": "have to be for example aligned with the locality",
    "start": "1639380",
    "end": "1645260"
  },
  {
    "text": "the normal locality or other way but then uh the controller can consider these",
    "start": "1645260",
    "end": "1653179"
  },
  {
    "text": "resources of different time for example a network interface and the GPU adapter and make a decision that these are the",
    "start": "1653179",
    "end": "1660500"
  },
  {
    "text": "hardware that have to be allocated and this information will be later on passed",
    "start": "1660500",
    "end": "1666200"
  },
  {
    "text": "to different cubelet plugins another use case scenario is when you have a similar kind of devices and if the workload",
    "start": "1666200",
    "end": "1674419"
  },
  {
    "text": "doesn't care what kind for what vendor of the hardware it gets it's a vendor",
    "start": "1674419",
    "end": "1680000"
  },
  {
    "text": "agnostic it can deal with any kind of Hardware then there can be a custom controller that can manage all of the",
    "start": "1680000",
    "end": "1688159"
  },
  {
    "text": "vendors devices in the cluster in this way so in this case the allocation would",
    "start": "1688159",
    "end": "1695299"
  },
  {
    "text": "have uh going would have been going with the different resource class so naturally since we have a custom",
    "start": "1695299",
    "end": "1701240"
  },
  {
    "text": "resource driver then we have a resource class that guides that the resource claim should be forwarded to this new",
    "start": "1701240",
    "end": "1709179"
  },
  {
    "text": "controller custom controller for the allocation and when doing the allocation",
    "start": "1709179",
    "end": "1715659"
  },
  {
    "text": "the customer resource driver controller will make several resource handle",
    "start": "1715659",
    "end": "1721220"
  },
  {
    "text": "entries in the allocation result that is written to the resource claim and each",
    "start": "1721220",
    "end": "1726860"
  },
  {
    "text": "of these Resort handle resource handle entries will contain a driver name that",
    "start": "1726860",
    "end": "1733340"
  },
  {
    "text": "will be used by the cubelet to ask the cubelet plugin to prepare the actual",
    "start": "1733340",
    "end": "1738740"
  },
  {
    "text": "device when the port is going to the node",
    "start": "1738740",
    "end": "1742778"
  },
  {
    "text": "now another feature that will be coming hopefully in 128 is",
    "start": "1746779",
    "end": "1752720"
  },
  {
    "text": "allowing several resource claims that will be used by the same pod and are",
    "start": "1752720",
    "end": "1759260"
  },
  {
    "text": "targeted to the same resource driver all of them will be able to be allocated at",
    "start": "1759260",
    "end": "1765320"
  },
  {
    "text": "the same time so at the moment the unsuitable notes call the one that",
    "start": "1765320",
    "end": "1771200"
  },
  {
    "text": "narrows down the amount of for nodes the main scheduler can use to pick up where on which node the port should be",
    "start": "1771200",
    "end": "1778340"
  },
  {
    "text": "scheduled on this call receives all of the resource claims of particular resource driver at once so the resource",
    "start": "1778340",
    "end": "1786260"
  },
  {
    "text": "driver that you're building can consider the whole amount of the resources that",
    "start": "1786260",
    "end": "1791600"
  },
  {
    "text": "will be used by the Pod but when the allocate call comes to the resource",
    "start": "1791600",
    "end": "1797299"
  },
  {
    "text": "driver the resource claims are passed at the moment one by one",
    "start": "1797299",
    "end": "1803000"
  },
  {
    "text": "so that's a different kind of consideration of resources by the resource driver and that can bring you",
    "start": "1803000",
    "end": "1809299"
  },
  {
    "text": "to small problems but look nasty problems",
    "start": "1809299",
    "end": "1814940"
  },
  {
    "text": "for example if you have two ports that were deployed simultaneously and they",
    "start": "1814940",
    "end": "1821899"
  },
  {
    "text": "have multiple resource claims unsuitable notes will report to the main",
    "start": "1821899",
    "end": "1828200"
  },
  {
    "text": "scheduler that the node a is suitable for both of these boards but when the",
    "start": "1828200",
    "end": "1834559"
  },
  {
    "text": "actual allocate call sequence will start for every single resource claim at some",
    "start": "1834559",
    "end": "1839899"
  },
  {
    "text": "point there might be a resource of depletion and the scheduling will have",
    "start": "1839899",
    "end": "1846260"
  },
  {
    "text": "to start all over again so the natural solution for this position for this is to make the",
    "start": "1846260",
    "end": "1853460"
  },
  {
    "text": "allocate call to look the same as the unsuitable not call so we planning to pass the whole bunch",
    "start": "1853460",
    "end": "1861380"
  },
  {
    "text": "of resource claims for particular pod so that the allocation call will have a",
    "start": "1861380",
    "end": "1867500"
  },
  {
    "text": "chance to see the whole amount of resources that need to be allocated for a particular pod",
    "start": "1867500",
    "end": "1874059"
  },
  {
    "text": "and the same problem applies for the cubelet plugin especially for the hardware is capable of only handling one",
    "start": "1874820",
    "end": "1883279"
  },
  {
    "text": "entry of the configuration then for example virtual functions on some of",
    "start": "1883279",
    "end": "1888740"
  },
  {
    "text": "the asteroid capable Hardware the ones several virtual functions were provisioned then the subsequent VFS can",
    "start": "1888740",
    "end": "1897860"
  },
  {
    "text": "only be provisioned when the previously created ones are dismantled so if your",
    "start": "1897860",
    "end": "1903200"
  },
  {
    "text": "pod has requested two Standalone virtual functions then the on-prep no",
    "start": "1903200",
    "end": "1911720"
  },
  {
    "text": "unsuitable nodes call will report that okay this GPU on this node simply be suitable and",
    "start": "1911720",
    "end": "1919760"
  },
  {
    "text": "then allocation and the preparation will actually face the situation that the first one has a chance to be created in",
    "start": "1919760",
    "end": "1927799"
  },
  {
    "text": "the SRI capable device but the other one can not anymore be created",
    "start": "1927799",
    "end": "1934520"
  },
  {
    "text": "because of the hardware limitations so similar way",
    "start": "1934520",
    "end": "1939919"
  },
  {
    "text": "if we pass all of the resource claims at the same time to not prepare resource",
    "start": "1939919",
    "end": "1945080"
  },
  {
    "text": "and node unprepared Source then it's much easier to bookkeep and handle",
    "start": "1945080",
    "end": "1952399"
  },
  {
    "text": "Resources by The Resort driver so this is what it is now and",
    "start": "1952399",
    "end": "1959779"
  },
  {
    "text": "this is how we think it will gonna be in 128 so this is work on going",
    "start": "1959779",
    "end": "1967179"
  },
  {
    "text": "and yes thank you I think we have three minutes for questions",
    "start": "1968779",
    "end": "1976720"
  },
  {
    "text": "yeah so the sorry the question was is the centralized controller a scheduler plug-in",
    "start": "1990799",
    "end": "1996260"
  },
  {
    "text": "um no it's not it's its own it's its own entity I mean you could think of it as a",
    "start": "1996260",
    "end": "2001960"
  },
  {
    "text": "scheduled plugin but it's not part of the scheduler plug-in framework that exists in kubernetes it exists as part",
    "start": "2001960",
    "end": "2007720"
  },
  {
    "text": "of the dra framework that knows how to communicate with the scheduler via this pod scheduling context object that I",
    "start": "2007720",
    "end": "2014260"
  },
  {
    "text": "talked about in the slides earlier",
    "start": "2014260",
    "end": "2017460"
  },
  {
    "text": "there there is a scheduler plug-in but it's associated with the dra framework in general it's not something that you",
    "start": "2024399",
    "end": "2030340"
  },
  {
    "text": "as the resource driver developer need to worry about implementing anything for",
    "start": "2030340",
    "end": "2035740"
  },
  {
    "text": "yeah um yeah and we had a couple of demos that we can show people that are",
    "start": "2035740",
    "end": "2041200"
  },
  {
    "text": "interested after this talk but we we ran out of time so we're not going to be able to to show them but if you're interested please come talk to us after",
    "start": "2041200",
    "end": "2046659"
  },
  {
    "text": "this and we can uh go through those with you um are there any other questions",
    "start": "2046659",
    "end": "2052540"
  },
  {
    "text": "uh yep",
    "start": "2052540",
    "end": "2055260"
  },
  {
    "text": "um so if we uh if I have like AI Asic chips uh is it possible to do a topology",
    "start": "2059679",
    "end": "2065440"
  },
  {
    "text": "aware of scheduling for example now with the Dre support or something we are looking at",
    "start": "2065440",
    "end": "2071138"
  },
  {
    "text": "in the future yeah so um currently we haven't thought",
    "start": "2071139",
    "end": "2078940"
  },
  {
    "text": "in detail about you know exactly how we're going to enable the whole topology aware or you know solve the topology",
    "start": "2078940",
    "end": "2085540"
  },
  {
    "text": "aware problem our kind of default answer at the moment is that if you want to do some sort of alignment you build a",
    "start": "2085540",
    "end": "2092260"
  },
  {
    "text": "custom controller that knows how to do the alignment for all the different resources that you know you need to be aligned right and because we have this",
    "start": "2092260",
    "end": "2099760"
  },
  {
    "text": "level of indirection where you can have you know the default drivers for all of your types and then you just write your custom controller without having to",
    "start": "2099760",
    "end": "2105880"
  },
  {
    "text": "rewrite the Kubler plug-in logic it enables you to do this for your custom environments where you know exactly",
    "start": "2105880",
    "end": "2111760"
  },
  {
    "text": "which resources you want to align and can just write that allocation logic rather than all of the prepare and unprepared logic as well",
    "start": "2111760",
    "end": "2118599"
  },
  {
    "text": "okay thank you in addition there is uh another",
    "start": "2118599",
    "end": "2125380"
  },
  {
    "text": "technology just was just released called node resource interface that can help you align uh what you want but it's",
    "start": "2125380",
    "end": "2134380"
  },
  {
    "text": "outside of the dra framework and it's uh outside of the resource driver so it's",
    "start": "2134380",
    "end": "2139420"
  },
  {
    "text": "something that resource driver can Leverage but that's another couple of API calls",
    "start": "2139420",
    "end": "2146200"
  },
  {
    "text": "and uh strictly speaking it's a best effort",
    "start": "2146200",
    "end": "2151599"
  },
  {
    "text": "so if you're interested in this the main author and maintainer of dra framework",
    "start": "2151599",
    "end": "2156940"
  },
  {
    "text": "Patrick is going to be the Intel Booth B13 and we have also people there who involved with the not resource",
    "start": "2156940",
    "end": "2163900"
  },
  {
    "text": "management and the NRI come talk to us",
    "start": "2163900",
    "end": "2168240"
  },
  {
    "text": "one question do you have some recipe called how this can be it can work",
    "start": "2169800",
    "end": "2176619"
  },
  {
    "text": "together with some Legacy device plugins uh what do you mean by work what do you",
    "start": "2176619",
    "end": "2182920"
  },
  {
    "text": "mean by work together so at least our plan for NVIDIA gpus um with our device plugin is that we're",
    "start": "2182920",
    "end": "2189220"
  },
  {
    "text": "going to allow you to run a node by node basis decide whether you want to deploy the existing device Plugin or the dra",
    "start": "2189220",
    "end": "2197079"
  },
  {
    "text": "Kubler plugin basically um so you know there won't be a mix and match of being able to allocate",
    "start": "2197079",
    "end": "2203400"
  },
  {
    "text": "via both methods on on each node but at least throughout your cluster you can have some dra enabled nodes some nodes",
    "start": "2203400",
    "end": "2210700"
  },
  {
    "text": "that are enabled by the existing device plugin so that end users can slowly migrate their applications to using the",
    "start": "2210700",
    "end": "2217000"
  },
  {
    "text": "the dra style of asking for resources and there's technical reasons why we",
    "start": "2217000",
    "end": "2222099"
  },
  {
    "text": "can't really do them together on the same node mostly because the device the device manager within the",
    "start": "2222099",
    "end": "2230020"
  },
  {
    "text": "kublet it's the one that does device allocation by the device manager or the the existing device plug-in framework",
    "start": "2230020",
    "end": "2236320"
  },
  {
    "text": "the the plugins themselves don't do allocation they just do advertisement of resources and so we can't coordinate",
    "start": "2236320",
    "end": "2241780"
  },
  {
    "text": "that allocation between our resource driver and dra and the existing device plugin allocation",
    "start": "2241780",
    "end": "2247720"
  },
  {
    "text": "because we'd have to basically change the Google to do that okay",
    "start": "2247720",
    "end": "2252900"
  },
  {
    "text": "but otherwise it's not exclusive it's not like if you're using um dra the dynamic Source allocation the rest of",
    "start": "2254560",
    "end": "2261160"
  },
  {
    "text": "the devices of different kind cannot be used in the cluster they can live inside the same cluster and some of the",
    "start": "2261160",
    "end": "2267400"
  },
  {
    "text": "hardware can be managed by the device plugins and some of them can be as long as they don't conflict",
    "start": "2267400",
    "end": "2273780"
  },
  {
    "text": "so we need to patch our kubernetes to at least one 28 to use Dynamic resource",
    "start": "2274200",
    "end": "2280960"
  },
  {
    "text": "allocation with Nvidia gpus or can we use it before with the new mechanism",
    "start": "2280960",
    "end": "2286599"
  },
  {
    "text": "yeah I mean officially you shouldn't be using it until dra reaches beta which is going to be at least another couple of",
    "start": "2286599",
    "end": "2292660"
  },
  {
    "text": "releases but if you want to start prototyping and using it now 127 is enough",
    "start": "2292660",
    "end": "2298000"
  },
  {
    "text": "so you can you know use 127 there's some feature flags that you'll need to enable",
    "start": "2298000",
    "end": "2303480"
  },
  {
    "text": "when you start up your 127 cluster because dra isn't on by default as an alpha feature but then you should be",
    "start": "2303480",
    "end": "2309880"
  },
  {
    "text": "able to follow the instructions on our Nvidia diary resource driver for gpus link that's here in order to start",
    "start": "2309880",
    "end": "2315339"
  },
  {
    "text": "playing around with it thank you I think this will this will be the last question",
    "start": "2315339",
    "end": "2322440"
  },
  {
    "text": "can this be used in conjunction with the cni to for example allocate a VF to a",
    "start": "2323380",
    "end": "2328599"
  },
  {
    "text": "pod from a dpu yeah so we have a um a separate project",
    "start": "2328599",
    "end": "2334300"
  },
  {
    "text": "going on in inside Nvidia for um doing exactly what you just said so it's a separate resource driver to our",
    "start": "2334300",
    "end": "2340480"
  },
  {
    "text": "GPU resource driver but it's allocating VFS for RDMA on dpus",
    "start": "2340480",
    "end": "2346859"
  },
  {
    "text": "yep thank you everyone [Applause]",
    "start": "2349180",
    "end": "2356819"
  }
]