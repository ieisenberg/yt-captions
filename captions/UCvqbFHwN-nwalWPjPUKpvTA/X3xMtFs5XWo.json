[
  {
    "text": "so today we'll be running some spark",
    "start": "0",
    "end": "2700"
  },
  {
    "text": "jobs on kubernetes with Argo workflows",
    "start": "2700",
    "end": "5180"
  },
  {
    "text": "what we'll cover here we'll be learning",
    "start": "5180",
    "end": "7500"
  },
  {
    "text": "about the scaling and stability",
    "start": "7500",
    "end": "9179"
  },
  {
    "text": "advantages of running spark jobs on",
    "start": "9179",
    "end": "10980"
  },
  {
    "text": "kubernetes versus traditionally running",
    "start": "10980",
    "end": "13380"
  },
  {
    "text": "them on yarn we'll learn how to use Argo",
    "start": "13380",
    "end": "15540"
  },
  {
    "text": "workflows to deploy and automate spark",
    "start": "15540",
    "end": "17640"
  },
  {
    "text": "jobs on kubernetes and not just run one",
    "start": "17640",
    "end": "20460"
  },
  {
    "text": "job but run multiple spark jobs",
    "start": "20460",
    "end": "22760"
  },
  {
    "text": "successively or in parallel",
    "start": "22760",
    "end": "25439"
  },
  {
    "text": "and once we're running our job on",
    "start": "25439",
    "end": "28439"
  },
  {
    "text": "kubernetes we will also learn what else",
    "start": "28439",
    "end": "30840"
  },
  {
    "text": "is possible to do",
    "start": "30840",
    "end": "32279"
  },
  {
    "text": "so a little bit about us I'm Kalyn",
    "start": "32279",
    "end": "34500"
  },
  {
    "text": "co-founder and CEO of pipekit who you",
    "start": "34500",
    "end": "36600"
  },
  {
    "text": "probably just heard about but at pipekit",
    "start": "36600",
    "end": "38700"
  },
  {
    "text": "we use Argo workflows to scale data",
    "start": "38700",
    "end": "40680"
  },
  {
    "text": "pipelines for Enterprises we help them",
    "start": "40680",
    "end": "43440"
  },
  {
    "text": "with a SAS control plane that sets up",
    "start": "43440",
    "end": "45719"
  },
  {
    "text": "Argo workflows and gets them running on",
    "start": "45719",
    "end": "48420"
  },
  {
    "text": "production using kubernetes to build",
    "start": "48420",
    "end": "51059"
  },
  {
    "text": "faster and have more reliable data",
    "start": "51059",
    "end": "53340"
  },
  {
    "text": "pipelines without having to know",
    "start": "53340",
    "end": "54660"
  },
  {
    "text": "everything about Argo I'm a contributor",
    "start": "54660",
    "end": "57000"
  },
  {
    "text": "on the Argo workflows project and I was",
    "start": "57000",
    "end": "60360"
  },
  {
    "text": "supposed to give this talk today with my",
    "start": "60360",
    "end": "61860"
  },
  {
    "text": "talented teammate Darko but",
    "start": "61860",
    "end": "63120"
  },
  {
    "text": "unfortunately he couldn't make it from",
    "start": "63120",
    "end": "64619"
  },
  {
    "text": "Serbia so just shout out to Darko for",
    "start": "64619",
    "end": "67200"
  },
  {
    "text": "the contributions on this talk",
    "start": "67200",
    "end": "69960"
  },
  {
    "text": "let's get into background of the tools",
    "start": "69960",
    "end": "72000"
  },
  {
    "text": "that we're using so first Apache spark",
    "start": "72000",
    "end": "74580"
  },
  {
    "text": "so it's a really popular Big Data",
    "start": "74580",
    "end": "77640"
  },
  {
    "text": "compute framework that's open source so",
    "start": "77640",
    "end": "79860"
  },
  {
    "text": "in addition to being super performant",
    "start": "79860",
    "end": "81479"
  },
  {
    "text": "for batch data jobs it's actually a",
    "start": "81479",
    "end": "83700"
  },
  {
    "text": "really versatile tool so that's why it's",
    "start": "83700",
    "end": "85380"
  },
  {
    "text": "maintained its popularity over time even",
    "start": "85380",
    "end": "87780"
  },
  {
    "text": "with upstarts like dask and Ray coming",
    "start": "87780",
    "end": "90060"
  },
  {
    "text": "onto the scene that are really even",
    "start": "90060",
    "end": "91619"
  },
  {
    "text": "better at doing batch jobs but spark is",
    "start": "91619",
    "end": "95460"
  },
  {
    "text": "possible because it's versatile you can",
    "start": "95460",
    "end": "97200"
  },
  {
    "text": "adapt use the framework to also run",
    "start": "97200",
    "end": "100140"
  },
  {
    "text": "streaming jobs alongside your batch jobs",
    "start": "100140",
    "end": "102200"
  },
  {
    "text": "has a handy machine learning library",
    "start": "102200",
    "end": "104840"
  },
  {
    "text": "creatively named ml lib so you can",
    "start": "104840",
    "end": "107579"
  },
  {
    "text": "transition right from ingestion and data",
    "start": "107579",
    "end": "110939"
  },
  {
    "text": "processing into machine learning and do",
    "start": "110939",
    "end": "113159"
  },
  {
    "text": "that with a bunch of common languages so",
    "start": "113159",
    "end": "115920"
  },
  {
    "text": "we work with a lot of companies that use",
    "start": "115920",
    "end": "117540"
  },
  {
    "text": "spark alongside all these other",
    "start": "117540",
    "end": "119460"
  },
  {
    "text": "Frameworks and do it on kubernetes so we",
    "start": "119460",
    "end": "122460"
  },
  {
    "text": "want to talk about a bit of the",
    "start": "122460",
    "end": "123659"
  },
  {
    "text": "advantages today because more and more",
    "start": "123659",
    "end": "127140"
  },
  {
    "text": "spark jobs are moving away from yarn",
    "start": "127140",
    "end": "129420"
  },
  {
    "text": "there there's a bunch of issues that we",
    "start": "129420",
    "end": "131580"
  },
  {
    "text": "run into when we Deploy on yarn first is",
    "start": "131580",
    "end": "134760"
  },
  {
    "text": "requiring Global installs so the risk of",
    "start": "134760",
    "end": "137400"
  },
  {
    "text": "sharing libraries between your spark",
    "start": "137400",
    "end": "138840"
  },
  {
    "text": "jobs on one cluster is is really hairy",
    "start": "138840",
    "end": "141660"
  },
  {
    "text": "to deal with and often you want to use a",
    "start": "141660",
    "end": "143580"
  },
  {
    "text": "different spark version for different",
    "start": "143580",
    "end": "145319"
  },
  {
    "text": "apps that you're building and not have",
    "start": "145319",
    "end": "146580"
  },
  {
    "text": "to go back in time and refactor Spark",
    "start": "146580",
    "end": "148980"
  },
  {
    "text": "jobs as versions upgrade but this isn't",
    "start": "148980",
    "end": "151620"
  },
  {
    "text": "really possible with yarn so that's a",
    "start": "151620",
    "end": "153959"
  },
  {
    "text": "big advantage of adopting kubernetes of",
    "start": "153959",
    "end": "157080"
  },
  {
    "text": "course Docker using Docker natively is",
    "start": "157080",
    "end": "160140"
  },
  {
    "text": "another big advantage of moving",
    "start": "160140",
    "end": "161519"
  },
  {
    "text": "kubernetes on yarn you have to do a lot",
    "start": "161519",
    "end": "164220"
  },
  {
    "text": "of leg work to be able to containerize",
    "start": "164220",
    "end": "166140"
  },
  {
    "text": "your applications and take advantage of",
    "start": "166140",
    "end": "168540"
  },
  {
    "text": "the faster Dev experience and the",
    "start": "168540",
    "end": "170760"
  },
  {
    "text": "improved dependency management that",
    "start": "170760",
    "end": "173220"
  },
  {
    "text": "Docker offers",
    "start": "173220",
    "end": "175200"
  },
  {
    "text": "again there's there's a resource benefit",
    "start": "175200",
    "end": "177720"
  },
  {
    "text": "as well so instead of running a jvm on",
    "start": "177720",
    "end": "179879"
  },
  {
    "text": "each node you can take advantage of",
    "start": "179879",
    "end": "181739"
  },
  {
    "text": "kubernetes PODS and then Auto scaling is",
    "start": "181739",
    "end": "184440"
  },
  {
    "text": "the big final advantage that we really",
    "start": "184440",
    "end": "186180"
  },
  {
    "text": "want to take advantage of so like how",
    "start": "186180",
    "end": "187920"
  },
  {
    "text": "can you use different CPU or GPU",
    "start": "187920",
    "end": "190379"
  },
  {
    "text": "thresholds on the same cluster and how",
    "start": "190379",
    "end": "192959"
  },
  {
    "text": "can you adapt to spiky data jobs",
    "start": "192959",
    "end": "194959"
  },
  {
    "text": "depending on the workload you need",
    "start": "194959",
    "end": "198180"
  },
  {
    "text": "but we often ask just why use kubernetes",
    "start": "198180",
    "end": "201360"
  },
  {
    "text": "for data scientists data science and to",
    "start": "201360",
    "end": "204180"
  },
  {
    "text": "quickly touch on this for data teams who",
    "start": "204180",
    "end": "206879"
  },
  {
    "text": "are newer to kubernetes often when you",
    "start": "206879",
    "end": "209040"
  },
  {
    "text": "use containers as you're building blocks",
    "start": "209040",
    "end": "210780"
  },
  {
    "text": "in the pipeline you get reproducibility",
    "start": "210780",
    "end": "213180"
  },
  {
    "text": "of the pipeline so you can share parts",
    "start": "213180",
    "end": "215159"
  },
  {
    "text": "or the full Pipeline with different",
    "start": "215159",
    "end": "216959"
  },
  {
    "text": "people on your team and you get improved",
    "start": "216959",
    "end": "219480"
  },
  {
    "text": "reliability given better dependency",
    "start": "219480",
    "end": "221940"
  },
  {
    "text": "management",
    "start": "221940",
    "end": "223220"
  },
  {
    "text": "kubernetes also gives you a declarative",
    "start": "223220",
    "end": "225299"
  },
  {
    "text": "approach to pipelines so this is much",
    "start": "225299",
    "end": "228360"
  },
  {
    "text": "more advantageous than so that you can",
    "start": "228360",
    "end": "231720"
  },
  {
    "text": "essentially Define what outputs you want",
    "start": "231720",
    "end": "234120"
  },
  {
    "text": "and let kubernetes handle the auto",
    "start": "234120",
    "end": "235680"
  },
  {
    "text": "scaling and so that's where vertical",
    "start": "235680",
    "end": "237420"
  },
  {
    "text": "Auto scaling comes into play kubernetes",
    "start": "237420",
    "end": "240239"
  },
  {
    "text": "is a great complement to the horizontal",
    "start": "240239",
    "end": "242220"
  },
  {
    "text": "scaling that a framework like spark",
    "start": "242220",
    "end": "244140"
  },
  {
    "text": "offers you but now introduces the",
    "start": "244140",
    "end": "246360"
  },
  {
    "text": "ability to also vertically scale",
    "start": "246360",
    "end": "247739"
  },
  {
    "text": "depending on the compute that you want",
    "start": "247739",
    "end": "249959"
  },
  {
    "text": "and finally we'll talk a little bit",
    "start": "249959",
    "end": "251640"
  },
  {
    "text": "later about how the cloud native",
    "start": "251640",
    "end": "253200"
  },
  {
    "text": "ecosystem can come into play to bring",
    "start": "253200",
    "end": "255780"
  },
  {
    "text": "your data pipeline fully to production",
    "start": "255780",
    "end": "258620"
  },
  {
    "text": "we've talked a lot about Argo workflows",
    "start": "258620",
    "end": "260880"
  },
  {
    "text": "today so we'll be using Argo workflows",
    "start": "260880",
    "end": "262860"
  },
  {
    "text": "and the big benefit here is it's",
    "start": "262860",
    "end": "265020"
  },
  {
    "text": "generalizable so beyond just running",
    "start": "265020",
    "end": "266820"
  },
  {
    "text": "your batch jobs you can now use Argo",
    "start": "266820",
    "end": "268979"
  },
  {
    "text": "workflows alongside spark to then",
    "start": "268979",
    "end": "272040"
  },
  {
    "text": "trigger ml training jobs trigger a model",
    "start": "272040",
    "end": "274320"
  },
  {
    "text": "serve model surveying or deployment and",
    "start": "274320",
    "end": "277560"
  },
  {
    "text": "ultimately we're using this as like the",
    "start": "277560",
    "end": "279240"
  },
  {
    "text": "workflow engine the orchestrator to",
    "start": "279240",
    "end": "281699"
  },
  {
    "text": "automate many many smart jobs and",
    "start": "281699",
    "end": "283680"
  },
  {
    "text": "instead of having to run these manually",
    "start": "283680",
    "end": "286560"
  },
  {
    "text": "taking a look at what the solution looks",
    "start": "286560",
    "end": "288540"
  },
  {
    "text": "like so on the cluster here we'll be",
    "start": "288540",
    "end": "290880"
  },
  {
    "text": "deploying Argo workflows and the user",
    "start": "290880",
    "end": "293100"
  },
  {
    "text": "would submit a workflow to Argo in the",
    "start": "293100",
    "end": "295380"
  },
  {
    "text": "Argo namespace Argo then talks to the",
    "start": "295380",
    "end": "297960"
  },
  {
    "text": "kubernetes API to schedule that workflow",
    "start": "297960",
    "end": "300060"
  },
  {
    "text": "which is going to be a series of spark",
    "start": "300060",
    "end": "302400"
  },
  {
    "text": "jobs",
    "start": "302400",
    "end": "303440"
  },
  {
    "text": "kubernetes starts the workflow and then",
    "start": "303440",
    "end": "305820"
  },
  {
    "text": "the workflow starts to step through and",
    "start": "305820",
    "end": "308639"
  },
  {
    "text": "submit spark jobs for the kubernetes",
    "start": "308639",
    "end": "310580"
  },
  {
    "text": "scheduler to then spin up the spark",
    "start": "310580",
    "end": "313560"
  },
  {
    "text": "driver pods and then spin out the",
    "start": "313560",
    "end": "316320"
  },
  {
    "text": "executor pods for the driver to then",
    "start": "316320",
    "end": "318300"
  },
  {
    "text": "assign tasks to and the big Advantage",
    "start": "318300",
    "end": "320940"
  },
  {
    "text": "here is number one workflows Argo",
    "start": "320940",
    "end": "323580"
  },
  {
    "text": "workflows is managing the state of these",
    "start": "323580",
    "end": "325199"
  },
  {
    "text": "jobs so that's how we can then begin to",
    "start": "325199",
    "end": "327360"
  },
  {
    "text": "string jobs together into a longer",
    "start": "327360",
    "end": "331199"
  },
  {
    "text": "Pipeline and the second big Advantage is",
    "start": "331199",
    "end": "334080"
  },
  {
    "text": "kubernetes is handling this the spinning",
    "start": "334080",
    "end": "336180"
  },
  {
    "text": "up and down of all the executor pods so",
    "start": "336180",
    "end": "338820"
  },
  {
    "text": "as pods are no longer needed kubernetes",
    "start": "338820",
    "end": "341520"
  },
  {
    "text": "will spin those down and save you",
    "start": "341520",
    "end": "344340"
  },
  {
    "text": "resources and then once the driver's",
    "start": "344340",
    "end": "345960"
  },
  {
    "text": "done it spins that down as well so",
    "start": "345960",
    "end": "347580"
  },
  {
    "text": "you're not holding any sort of resources",
    "start": "347580",
    "end": "349919"
  },
  {
    "text": "idle",
    "start": "349919",
    "end": "351840"
  },
  {
    "text": "so yeah all in all we're really seeing",
    "start": "351840",
    "end": "355259"
  },
  {
    "text": "the these the the problems we saw",
    "start": "355259",
    "end": "357660"
  },
  {
    "text": "initially solved by moving container",
    "start": "357660",
    "end": "360180"
  },
  {
    "text": "native so we're now running different",
    "start": "360180",
    "end": "362039"
  },
  {
    "text": "spark versions and different jobs on the",
    "start": "362039",
    "end": "364020"
  },
  {
    "text": "same cluster we can manage dependencies",
    "start": "364020",
    "end": "366900"
  },
  {
    "text": "more easily and we can stop downloading",
    "start": "366900",
    "end": "369720"
  },
  {
    "text": "those dependencies at runtime the other",
    "start": "369720",
    "end": "372539"
  },
  {
    "text": "big thing that we get is now lower",
    "start": "372539",
    "end": "374160"
  },
  {
    "text": "resource requirements for running our",
    "start": "374160",
    "end": "375660"
  },
  {
    "text": "jobs and auto scaling vertically",
    "start": "375660",
    "end": "379680"
  },
  {
    "text": "but wait there's of course more that we",
    "start": "379680",
    "end": "382020"
  },
  {
    "text": "get so the big next the the first thing",
    "start": "382020",
    "end": "384780"
  },
  {
    "text": "that we want to call out here is it's",
    "start": "384780",
    "end": "386639"
  },
  {
    "text": "duplicable so this architecture can be",
    "start": "386639",
    "end": "388800"
  },
  {
    "text": "used if you have a team that wants to",
    "start": "388800",
    "end": "390600"
  },
  {
    "text": "move Beyond spark",
    "start": "390600",
    "end": "392100"
  },
  {
    "text": "um start adopting or adopting task array",
    "start": "392100",
    "end": "394620"
  },
  {
    "text": "you don't have to completely set up a",
    "start": "394620",
    "end": "396600"
  },
  {
    "text": "new cluster you can spin up a similar",
    "start": "396600",
    "end": "398520"
  },
  {
    "text": "architecture on the same cluster run",
    "start": "398520",
    "end": "400440"
  },
  {
    "text": "some jobs with spark some with task and",
    "start": "400440",
    "end": "402539"
  },
  {
    "text": "Ray and all and use our workflows to",
    "start": "402539",
    "end": "405000"
  },
  {
    "text": "orchestrate all of that",
    "start": "405000",
    "end": "406380"
  },
  {
    "text": "you can extend it so now you can bring",
    "start": "406380",
    "end": "408120"
  },
  {
    "text": "in other data processing or machine",
    "start": "408120",
    "end": "410039"
  },
  {
    "text": "learning tooling into the same cluster",
    "start": "410039",
    "end": "412020"
  },
  {
    "text": "which we'll talk about in a minute and",
    "start": "412020",
    "end": "413940"
  },
  {
    "text": "you get a bunch of other benefits by",
    "start": "413940",
    "end": "415800"
  },
  {
    "text": "being Cloud native you know you're",
    "start": "415800",
    "end": "417960"
  },
  {
    "text": "agnostic and you can start to pull in",
    "start": "417960",
    "end": "420060"
  },
  {
    "text": "other tools like Prometheus for your",
    "start": "420060",
    "end": "421740"
  },
  {
    "text": "logging and metrics",
    "start": "421740",
    "end": "423840"
  },
  {
    "text": "so quickly I'm going to jump into a demo",
    "start": "423840",
    "end": "426180"
  },
  {
    "text": "and like how we get this done",
    "start": "426180",
    "end": "429000"
  },
  {
    "text": "let's see",
    "start": "429000",
    "end": "431600"
  },
  {
    "text": "so what we're going to be running here",
    "start": "434100",
    "end": "435660"
  },
  {
    "text": "is a couple of basic spark jobs so we",
    "start": "435660",
    "end": "437940"
  },
  {
    "text": "have a bike share data set that we're",
    "start": "437940",
    "end": "439500"
  },
  {
    "text": "pulling off a kaggle we're just doing a",
    "start": "439500",
    "end": "442020"
  },
  {
    "text": "simple map reduce so we're going to",
    "start": "442020",
    "end": "444120"
  },
  {
    "text": "count our bikes by bike type and total",
    "start": "444120",
    "end": "446580"
  },
  {
    "text": "up their um",
    "start": "446580",
    "end": "448620"
  },
  {
    "text": "their the amount of hours we've rode",
    "start": "448620",
    "end": "453380"
  },
  {
    "text": "and then what we're going to do is",
    "start": "454319",
    "end": "455880"
  },
  {
    "text": "create an augural workflow that is a dag",
    "start": "455880",
    "end": "458759"
  },
  {
    "text": "that runs these two spark jobs in",
    "start": "458759",
    "end": "461400"
  },
  {
    "text": "parallel so we're going to run the bike",
    "start": "461400",
    "end": "463199"
  },
  {
    "text": "type count job and then we're going to",
    "start": "463199",
    "end": "465060"
  },
  {
    "text": "run the ride length job in parallel so",
    "start": "465060",
    "end": "468180"
  },
  {
    "text": "what we do is we create an Argo workflow",
    "start": "468180",
    "end": "470400"
  },
  {
    "text": "and we start to define the dag steps so",
    "start": "470400",
    "end": "473220"
  },
  {
    "text": "first down here we're going to define",
    "start": "473220",
    "end": "474500"
  },
  {
    "text": "the bike type step",
    "start": "474500",
    "end": "476819"
  },
  {
    "text": "we're going to define the",
    "start": "476819",
    "end": "480240"
  },
  {
    "text": "um this the success and failure",
    "start": "480240",
    "end": "482280"
  },
  {
    "text": "conditions which again is Handy for when",
    "start": "482280",
    "end": "484020"
  },
  {
    "text": "we want to",
    "start": "484020",
    "end": "485880"
  },
  {
    "text": "um we want to string together",
    "start": "485880",
    "end": "489199"
  },
  {
    "text": "tasks depending on how a step completes",
    "start": "489199",
    "end": "493199"
  },
  {
    "text": "or fails we then deter we then designate",
    "start": "493199",
    "end": "497340"
  },
  {
    "text": "that this is going to be using the spark",
    "start": "497340",
    "end": "499080"
  },
  {
    "text": "operator",
    "start": "499080",
    "end": "501560"
  },
  {
    "text": "we vents designate the language so we're",
    "start": "502039",
    "end": "505919"
  },
  {
    "text": "using Scala for this job",
    "start": "505919",
    "end": "507780"
  },
  {
    "text": "sorry the videos",
    "start": "507780",
    "end": "509940"
  },
  {
    "text": "blowing up on me here we're going to set",
    "start": "509940",
    "end": "512640"
  },
  {
    "text": "the mode to Cluster which we have to do",
    "start": "512640",
    "end": "514020"
  },
  {
    "text": "for running spark on kubernetes",
    "start": "514020",
    "end": "517620"
  },
  {
    "text": "we also deter we",
    "start": "517620",
    "end": "520140"
  },
  {
    "text": "um put in our Docker image here and we",
    "start": "520140",
    "end": "522899"
  },
  {
    "text": "set our main class which is the Scala",
    "start": "522899",
    "end": "525360"
  },
  {
    "text": "file that we're that we defined our job",
    "start": "525360",
    "end": "527519"
  },
  {
    "text": "in",
    "start": "527519",
    "end": "529680"
  },
  {
    "text": "and then we set the jar location and the",
    "start": "529680",
    "end": "532320"
  },
  {
    "text": "spark version so this is where you can",
    "start": "532320",
    "end": "534000"
  },
  {
    "text": "change you know within the same workflow",
    "start": "534000",
    "end": "535800"
  },
  {
    "text": "you can have one step that runs version",
    "start": "535800",
    "end": "538440"
  },
  {
    "text": "3.1 one step that's running version two",
    "start": "538440",
    "end": "543240"
  },
  {
    "text": "and then down at the bottom here we",
    "start": "543240",
    "end": "545279"
  },
  {
    "text": "start to Define our resource",
    "start": "545279",
    "end": "546600"
  },
  {
    "text": "requirements so first for the driver",
    "start": "546600",
    "end": "549060"
  },
  {
    "text": "um which we've set up here to run",
    "start": "549060",
    "end": "550320"
  },
  {
    "text": "locally and then next for the executors",
    "start": "550320",
    "end": "554040"
  },
  {
    "text": "and of course we can scale these up if",
    "start": "554040",
    "end": "556080"
  },
  {
    "text": "we want if we're going to move to our",
    "start": "556080",
    "end": "558360"
  },
  {
    "text": "AWS account and move off a local but for",
    "start": "558360",
    "end": "561060"
  },
  {
    "text": "now we're just going to run two",
    "start": "561060",
    "end": "562920"
  },
  {
    "text": "instances here which would mean that",
    "start": "562920",
    "end": "565740"
  },
  {
    "text": "we're going to spin up two kubernetes",
    "start": "565740",
    "end": "567000"
  },
  {
    "text": "pods to execute each spark job",
    "start": "567000",
    "end": "570980"
  },
  {
    "text": "let me just skip so now what we've done",
    "start": "572880",
    "end": "576060"
  },
  {
    "text": "let's see",
    "start": "576060",
    "end": "577800"
  },
  {
    "text": "go",
    "start": "577800",
    "end": "579660"
  },
  {
    "text": "we're going to take our workflow put it",
    "start": "579660",
    "end": "583140"
  },
  {
    "text": "into Argo and run it",
    "start": "583140",
    "end": "586519"
  },
  {
    "text": "and now we can see our dag that Matt's",
    "start": "588600",
    "end": "590700"
  },
  {
    "text": "back to our workflow file",
    "start": "590700",
    "end": "592920"
  },
  {
    "text": "so we have our bike type count and our",
    "start": "592920",
    "end": "594899"
  },
  {
    "text": "bike length count",
    "start": "594899",
    "end": "597120"
  },
  {
    "text": "and this is going to run and now what we",
    "start": "597120",
    "end": "598860"
  },
  {
    "text": "can do is we can go into Kube CTL and",
    "start": "598860",
    "end": "600720"
  },
  {
    "text": "monitor it so we see that we have our",
    "start": "600720",
    "end": "602339"
  },
  {
    "text": "driver pods two driver pods and two",
    "start": "602339",
    "end": "604260"
  },
  {
    "text": "executor pods for each driver",
    "start": "604260",
    "end": "607680"
  },
  {
    "text": "once they complete I'm going to just",
    "start": "607680",
    "end": "609660"
  },
  {
    "text": "grab the logs here",
    "start": "609660",
    "end": "611459"
  },
  {
    "text": "and we can see that we get our output of",
    "start": "611459",
    "end": "614820"
  },
  {
    "text": "um our count for byte type",
    "start": "614820",
    "end": "617519"
  },
  {
    "text": "and then we'll also grab the logs for",
    "start": "617519",
    "end": "620279"
  },
  {
    "text": "our ride length by bike type",
    "start": "620279",
    "end": "624140"
  },
  {
    "text": "and since this is a demo we're just",
    "start": "627360",
    "end": "629100"
  },
  {
    "text": "using the print statements there but",
    "start": "629100",
    "end": "631680"
  },
  {
    "text": "that's an example of how we can just run",
    "start": "631680",
    "end": "633300"
  },
  {
    "text": "two spark jobs in parallel on kubernetes",
    "start": "633300",
    "end": "635519"
  },
  {
    "text": "with Argo now we're going to shift into",
    "start": "635519",
    "end": "637860"
  },
  {
    "text": "how we can actually parameterize that",
    "start": "637860",
    "end": "640440"
  },
  {
    "text": "workflow file so that more people on our",
    "start": "640440",
    "end": "642660"
  },
  {
    "text": "team can run workflows on their own by",
    "start": "642660",
    "end": "644700"
  },
  {
    "text": "just simply passing in arguments so for",
    "start": "644700",
    "end": "648060"
  },
  {
    "text": "instance this is an example of a",
    "start": "648060",
    "end": "650339"
  },
  {
    "text": "workflow that we're actually going to be",
    "start": "650339",
    "end": "651839"
  },
  {
    "text": "using the workflow template ref",
    "start": "651839",
    "end": "654180"
  },
  {
    "text": "to call in a spark workflow template and",
    "start": "654180",
    "end": "658100"
  },
  {
    "text": "instead of writing all the yaml that we",
    "start": "658100",
    "end": "660240"
  },
  {
    "text": "did before we just simply pass in",
    "start": "660240",
    "end": "663480"
  },
  {
    "text": "template parameters that we've defined",
    "start": "663480",
    "end": "665459"
  },
  {
    "text": "in the workflow template so as a data",
    "start": "665459",
    "end": "669300"
  },
  {
    "text": "scientist we can just Define what our",
    "start": "669300",
    "end": "671579"
  },
  {
    "text": "Scala job is going to be",
    "start": "671579",
    "end": "674240"
  },
  {
    "text": "the file the image and where we want to",
    "start": "674240",
    "end": "678420"
  },
  {
    "text": "run it and what resources we need to run",
    "start": "678420",
    "end": "680160"
  },
  {
    "text": "it",
    "start": "680160",
    "end": "682339"
  },
  {
    "text": "and so what this looks like on on the",
    "start": "685680",
    "end": "690680"
  },
  {
    "text": "workflow template side is a bunch of",
    "start": "690680",
    "end": "693600"
  },
  {
    "text": "input parameters",
    "start": "693600",
    "end": "696500"
  },
  {
    "text": "um that map back to our original",
    "start": "696779",
    "end": "698220"
  },
  {
    "text": "workflow",
    "start": "698220",
    "end": "699380"
  },
  {
    "text": "but instead we set some defaults and we",
    "start": "699380",
    "end": "703200"
  },
  {
    "text": "allow now any user to adapt this",
    "start": "703200",
    "end": "706260"
  },
  {
    "text": "template depending on the spark job they",
    "start": "706260",
    "end": "707880"
  },
  {
    "text": "want to run",
    "start": "707880",
    "end": "710360"
  },
  {
    "text": "so just jumping back into",
    "start": "710820",
    "end": "713279"
  },
  {
    "text": "um",
    "start": "713279",
    "end": "714180"
  },
  {
    "text": "next steps so now that we're running",
    "start": "714180",
    "end": "716220"
  },
  {
    "text": "spark on kubernetes we can next team",
    "start": "716220",
    "end": "719399"
  },
  {
    "text": "enable Chrome workflows so if you hop",
    "start": "719399",
    "end": "721740"
  },
  {
    "text": "into the GitHub repo that we have online",
    "start": "721740",
    "end": "723720"
  },
  {
    "text": "we have an example of that that you can",
    "start": "723720",
    "end": "725459"
  },
  {
    "text": "check out as well we can start to Define",
    "start": "725459",
    "end": "727440"
  },
  {
    "text": "different node pools so if you want to",
    "start": "727440",
    "end": "729240"
  },
  {
    "text": "scale up gpus for certain jobs or not",
    "start": "729240",
    "end": "731820"
  },
  {
    "text": "with a few lines of code you define",
    "start": "731820",
    "end": "733920"
  },
  {
    "text": "those nodes and select them and with",
    "start": "733920",
    "end": "736140"
  },
  {
    "text": "Argo workflows have the logic to then",
    "start": "736140",
    "end": "738720"
  },
  {
    "text": "run certain spark jobs on on more",
    "start": "738720",
    "end": "741480"
  },
  {
    "text": "compute or or less compute depending on",
    "start": "741480",
    "end": "743579"
  },
  {
    "text": "your needs start to use cluster Auto",
    "start": "743579",
    "end": "745620"
  },
  {
    "text": "scaling and then extend the pipeline",
    "start": "745620",
    "end": "747660"
  },
  {
    "text": "with more capabilities an example of",
    "start": "747660",
    "end": "749760"
  },
  {
    "text": "that is this architecture where we're",
    "start": "749760",
    "end": "752399"
  },
  {
    "text": "deploying Argo workflows but we're also",
    "start": "752399",
    "end": "755339"
  },
  {
    "text": "then deploying a rest API that can feed",
    "start": "755339",
    "end": "757920"
  },
  {
    "text": "events into through Kafka and into",
    "start": "757920",
    "end": "759779"
  },
  {
    "text": "Spark's structured streaming app we can",
    "start": "759779",
    "end": "762720"
  },
  {
    "text": "ingest that alongside running some batch",
    "start": "762720",
    "end": "764760"
  },
  {
    "text": "jobs that we're pulling from a database",
    "start": "764760",
    "end": "767220"
  },
  {
    "text": "and we can use Argo and Argo events to",
    "start": "767220",
    "end": "769620"
  },
  {
    "text": "trigger these spark jobs",
    "start": "769620",
    "end": "772339"
  },
  {
    "text": "simultaneously and then we can",
    "start": "772339",
    "end": "774720"
  },
  {
    "text": "now that we know the state of these jobs",
    "start": "774720",
    "end": "776639"
  },
  {
    "text": "we can trigger ml model training hyper",
    "start": "776639",
    "end": "779339"
  },
  {
    "text": "hyper parameter tuning and even model",
    "start": "779339",
    "end": "781260"
  },
  {
    "text": "deployment as the pipeline progresses so",
    "start": "781260",
    "end": "784320"
  },
  {
    "text": "you get an all-encompassing way to",
    "start": "784320",
    "end": "786600"
  },
  {
    "text": "automate your ml Pipeline on kubernetes",
    "start": "786600",
    "end": "790139"
  },
  {
    "text": "with Argo workflows",
    "start": "790139",
    "end": "792360"
  },
  {
    "text": "as far as resources and next steps feel",
    "start": "792360",
    "end": "794880"
  },
  {
    "text": "free to check out our repo where we have",
    "start": "794880",
    "end": "796920"
  },
  {
    "text": "all the code in the demo hit us up on",
    "start": "796920",
    "end": "799800"
  },
  {
    "text": "Twitter if you're curious about chatting",
    "start": "799800",
    "end": "802560"
  },
  {
    "text": "about this any further and make sure to",
    "start": "802560",
    "end": "804360"
  },
  {
    "text": "check out a couple extra resources if",
    "start": "804360",
    "end": "806880"
  },
  {
    "text": "you end up diving in thank you",
    "start": "806880",
    "end": "810060"
  },
  {
    "text": "foreign",
    "start": "810060",
    "end": "811020"
  },
  {
    "text": "[Applause]",
    "start": "811020",
    "end": "814029"
  }
]