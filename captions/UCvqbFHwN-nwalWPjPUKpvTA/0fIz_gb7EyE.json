[
  {
    "text": "hello everyone um really happy to to be here today um I'm Ricardo uh today we'll",
    "start": "399",
    "end": "7399"
  },
  {
    "text": "we'll be talking about training and optimization of our Transformers and we'll talk a bit about the the platforms",
    "start": "7399",
    "end": "14639"
  },
  {
    "text": "we have at CERN for this kind of workloads and also uh a real physics use",
    "start": "14639",
    "end": "19760"
  },
  {
    "text": "case for once with a real physicist as well so I don't have to pretend I know what I'm talking about today uh so so hi",
    "start": "19760",
    "end": "27439"
  },
  {
    "text": "everyone my name is Max and I I'm supposed to be the uh real physicist but unfortunately my talk is mostly going to",
    "start": "27439",
    "end": "33600"
  },
  {
    "text": "be about uh how to apply machine learning for a very specific you use case in our experiment yeah and I'm",
    "start": "33600",
    "end": "39520"
  },
  {
    "text": "Ricardo I lead the platforms infrastructure team at CERN I'm also a member of the technical oversight",
    "start": "39520",
    "end": "44920"
  },
  {
    "text": "committee at at the cncf and the recently formed end user uh technical Advisory Board as well so I'll start by",
    "start": "44920",
    "end": "53559"
  },
  {
    "text": "giving a very brief overview of what CERN is uh it's the European Organization for particle physics uh",
    "start": "53559",
    "end": "59600"
  },
  {
    "text": "it's been uh there for several decades and the our Flagship project is the",
    "start": "59600",
    "end": "65280"
  },
  {
    "text": "large hydron collider which is a very large particle accelerator that has 27",
    "start": "65280",
    "end": "71159"
  },
  {
    "text": "km in perimeter and it's uh 100 m on the ground uh we accelerate protons to very",
    "start": "71159",
    "end": "77600"
  },
  {
    "text": "close to the speed of light and we make them Collide at specific points where we buildt this experiments but because the",
    "start": "77600",
    "end": "83360"
  },
  {
    "text": "video is better than a thousand words I'll try to play this very quickly so",
    "start": "83360",
    "end": "88600"
  },
  {
    "text": "you can see we accelerate two beams of protons on this very large ring we have these four experiments where we make",
    "start": "88600",
    "end": "95360"
  },
  {
    "text": "these collisions and what I want to highlight here is that these collisions and these machines act as like very",
    "start": "95360",
    "end": "101399"
  },
  {
    "text": "large cameras and we produce a lot of data so I'll stop here uh because this",
    "start": "101399",
    "end": "106759"
  },
  {
    "text": "is where it gets interesting is that we are generating pedabytes of data per second and traditionally to handle this",
    "start": "106759",
    "end": "112600"
  },
  {
    "text": "on the nanc we've been building Custom Electronics that will filter this data",
    "start": "112600",
    "end": "118399"
  },
  {
    "text": "uh most of this data the very very large amount of of data we are uh generating",
    "start": "118399",
    "end": "123960"
  },
  {
    "text": "in using Custom Electronics first and then the very few percentage that is left then we use very large Computing",
    "start": "123960",
    "end": "130800"
  },
  {
    "text": "Farms which traditionally are also CPU based U and then from here we finally",
    "start": "130800",
    "end": "136720"
  },
  {
    "text": "get to an amount of data that we can store in our data centers and then reprocess and give the physicists for",
    "start": "136720",
    "end": "144319"
  },
  {
    "text": "analysis so this is kind of important to put some context into this talk uh",
    "start": "144319",
    "end": "150239"
  },
  {
    "text": "because uh it explains uh a transition that is happening which is all this custom Hardware all this Custom",
    "start": "150239",
    "end": "156640"
  },
  {
    "text": "Electronics we got used to develop are now possible to be replaced with gpus",
    "start": "156640",
    "end": "161720"
  },
  {
    "text": "and other accelerators and machine learning algorithms and this is a huge simplification of our infrastructure so",
    "start": "161720",
    "end": "167480"
  },
  {
    "text": "max will talk little bit about one of the experiments um so as I mentioned I'm",
    "start": "167480",
    "end": "172519"
  },
  {
    "text": "part of the atlas experiments and you have a nice uh photo showing you the scale of the of the detector with a",
    "start": "172519",
    "end": "178200"
  },
  {
    "text": "human uh just so that more understandable uh so the point of the atlas experiment is to study Physics at",
    "start": "178200",
    "end": "185120"
  },
  {
    "text": "the fundamental scale and at the fundamental scales physics is described by particles which are um fundamental",
    "start": "185120",
    "end": "191440"
  },
  {
    "text": "Elementary elements that you can't subdivide as far as we you know so far and they all listed here into the",
    "start": "191440",
    "end": "198040"
  },
  {
    "text": "circles showing you the different particles and you have particles that make matter the outer ring and then you have particles that make the",
    "start": "198040",
    "end": "203840"
  },
  {
    "text": "interactions themselves uh the inner rings and as you can see there's one at the center it's the brute hung hibos or",
    "start": "203840",
    "end": "211080"
  },
  {
    "text": "more commonly called just the hibos or the goddamn it particle so the point of the l n Atlas was to discover this",
    "start": "211080",
    "end": "218080"
  },
  {
    "text": "particle which was done in 2012 but we still have to measure a lot of the properties and the user case I'm going",
    "start": "218080",
    "end": "223599"
  },
  {
    "text": "to show is an advanced use of AI so that we can measure some rare decays of it and this is obviously a huge",
    "start": "223599",
    "end": "229640"
  },
  {
    "text": "collaboration given just the size of the detector the size of the data set to see through so we are about 6,000 members uh",
    "start": "229640",
    "end": "236239"
  },
  {
    "text": "scattered across 160 Institute and more than 40 countries now and uh so I'm going to give you a user",
    "start": "236239",
    "end": "241640"
  },
  {
    "text": "Case by the end of the the talk yeah so now we'll break kind of the talk into two parts I'll start by talking about",
    "start": "241640",
    "end": "247879"
  },
  {
    "text": "infrastructure and focus on the journey we had to get here and also the challenges we are facing and then maxam",
    "start": "247879",
    "end": "254480"
  },
  {
    "text": "will talk about the physics use case at the end so if we look at what we've been building at CERN and I we've given many",
    "start": "254480",
    "end": "261120"
  },
  {
    "text": "many talks over the years uh at this conference and others uh we had a long",
    "start": "261120",
    "end": "266680"
  },
  {
    "text": "journey to get to where we are uh we started looking at the cloud native and kubernetes back uh in 2016 where swarm",
    "start": "266680",
    "end": "274199"
  },
  {
    "text": "was also a possibility being considered we even added mesus at some point and we eventually got to um uh Production",
    "start": "274199",
    "end": "282120"
  },
  {
    "text": "service and then all the work we've been doing since then is about integration with our internal systems so things like",
    "start": "282120",
    "end": "288400"
  },
  {
    "text": "storage and our SE systems and other storage systems we have in house and",
    "start": "288400",
    "end": "293800"
  },
  {
    "text": "then usability and uh ease of deployment we get up secret management things like flux and Argo",
    "start": "293800",
    "end": "300520"
  },
  {
    "text": "uh then allowing erogeneous clusters with node groups Auto scaling Auto healing load balancers and then a very",
    "start": "300520",
    "end": "307759"
  },
  {
    "text": "strong fa focus on dissemination uh internally mostly to get people used to",
    "start": "307759",
    "end": "313039"
  },
  {
    "text": "handle these kind of applications and also security uh which is um something we put a lot of focus on and finally uh",
    "start": "313039",
    "end": "321240"
  },
  {
    "text": "one key key aspect is ensuring uh business continuity in disaster recovery and we've been building on all the",
    "start": "321240",
    "end": "327360"
  },
  {
    "text": "features that have been offered by by our tools so this has been a long journey but it",
    "start": "327360",
    "end": "332840"
  },
  {
    "text": "got us to to quite far and uh you can see here we use uh these tools but a lot",
    "start": "332840",
    "end": "338160"
  },
  {
    "text": "more uh but from from the top you can see the webinars we organize internally and you see two different kinds of",
    "start": "338160",
    "end": "344639"
  },
  {
    "text": "things we are offering you can see the services so here at the top you have how",
    "start": "344639",
    "end": "350160"
  },
  {
    "text": "CERN runs critical application service on kubernetes and this is really the critical systems on the campus uh the",
    "start": "350160",
    "end": "356240"
  },
  {
    "text": "everyday uh Services we need to support the campus with 10,000 people uh and",
    "start": "356240",
    "end": "361720"
  },
  {
    "text": "then we also see the other side which is more the physics analysis and you can see here analysis reproducibility with",
    "start": "361720",
    "end": "367400"
  },
  {
    "text": "reanon kubernetes this is where uh the physics actually happens and then the base computing power we need uh Rio here",
    "start": "367400",
    "end": "375039"
  },
  {
    "text": "moving rucho to production in kubernetes rucho is the system that is responsible for in the atlas experiment moving data",
    "start": "375039",
    "end": "382680"
  },
  {
    "text": "around and these are pretty large uh uh requirements so if we look at Ru for",
    "start": "382680",
    "end": "388319"
  },
  {
    "text": "example I have two two plots uh on the top there you can see we are any random",
    "start": "388319",
    "end": "394319"
  },
  {
    "text": "w atern we are moving over 50 paby of data around and you can see transfering",
    "start": "394319",
    "end": "400120"
  },
  {
    "text": "of 7even paby of of data as well and then on the capacity you can see here an",
    "start": "400120",
    "end": "405880"
  },
  {
    "text": "experiment that was done by the atlas uh experiment using public clouds where they Tred to scale out to the to in this",
    "start": "405880",
    "end": "412639"
  },
  {
    "text": "case gcp and they managed to run for several days with over 880,000 cores of uh spot",
    "start": "412639",
    "end": "418960"
  },
  {
    "text": "instances so so we have a lot of um experience we've built all this infrastructure that people got used to",
    "start": "418960",
    "end": "425639"
  },
  {
    "text": "so when we start looking at machine learning in AI it's only obvious to try to build on it and we started looking",
    "start": "425639",
    "end": "431879"
  },
  {
    "text": "and couf flow was was a good option uh it builds on all the principles we we had uh uh in house already it kind of",
    "start": "431879",
    "end": "440080"
  },
  {
    "text": "doesn't build anything new it get it gets a lot of components working together in a way that uh can help the",
    "start": "440080",
    "end": "446400"
  },
  {
    "text": "end users and it's uh it's been uh quite quite successful in answering all our needs which is things from data",
    "start": "446400",
    "end": "453039"
  },
  {
    "text": "preparation to some sort of fast iteration using notebooks or other means then scaling out distribut training and",
    "start": "453039",
    "end": "459879"
  },
  {
    "text": "Hyper parameter optimization and then model storage and serving I won't go into details on this but you have the QR",
    "start": "459879",
    "end": "466120"
  },
  {
    "text": "code of previous talks that we gave in this area so what I will focus is uh we",
    "start": "466120",
    "end": "472360"
  },
  {
    "text": "have this infrastructure uh what are the challenges that we have today and these are challenges that are both on the",
    "start": "472360",
    "end": "478680"
  },
  {
    "text": "stack but also Al on the usage of of the of the resources so the first the first",
    "start": "478680",
    "end": "484840"
  },
  {
    "text": "uh challenge we have and I don't know how many people are running their on premises infrastructure or just relying",
    "start": "484840",
    "end": "491120"
  },
  {
    "text": "on on external uh Cloud providers but if you're running your infrastructure at least for us this has been a huge",
    "start": "491120",
    "end": "497720"
  },
  {
    "text": "challenge the pattern of usage of of this Hardware is very different from",
    "start": "497720",
    "end": "503360"
  },
  {
    "text": "what we call our traditional CPU workloads uh the needs for power and for cooling increase dram atically if we",
    "start": "503360",
    "end": "510240"
  },
  {
    "text": "look at this plot here we can see that the needs for power in the current Generations but especially if we look at",
    "start": "510240",
    "end": "517320"
  },
  {
    "text": "the future Generations that are coming uh kind of limit a lot the density we",
    "start": "517320",
    "end": "522479"
  },
  {
    "text": "can have and at the same time uh people are you are requesting this this kind of",
    "start": "522479",
    "end": "528160"
  },
  {
    "text": "uh density uh not only the density in the single node but they are also asking for things that were traditionally only",
    "start": "528160",
    "end": "535000"
  },
  {
    "text": "needed by HPC uh centers and supercomputers things like f fast network interconnects infin band and",
    "start": "535000",
    "end": "541920"
  },
  {
    "text": "friends all of this comes with a huge request for power and cooling so if we",
    "start": "541920",
    "end": "547120"
  },
  {
    "text": "look at that diagram there we can see if we have four gpus per node with the",
    "start": "547120",
    "end": "552160"
  },
  {
    "text": "interconnects internally we already putting quite a lot of uh Demand on the",
    "start": "552160",
    "end": "557399"
  },
  {
    "text": "power uh and cooling required for each rack if we start talking about fast interconnects between the nodes then",
    "start": "557399",
    "end": "563880"
  },
  {
    "text": "things get even more complicated the second one is Hardware Evolution and this doesn't seem like",
    "start": "563880",
    "end": "570200"
  },
  {
    "text": "it's uh uh calming down uh we got used to uh sort of stable increase in terms",
    "start": "570200",
    "end": "576000"
  },
  {
    "text": "of uh um evolution of Hardware around CPUs uh but suddenly we we started",
    "start": "576000",
    "end": "582160"
  },
  {
    "text": "having uh gpus coming into the scene and the the rate of uh increase is much",
    "start": "582160",
    "end": "588160"
  },
  {
    "text": "higher so we can see here the predicted uh the next Generations for NVIDIA uh",
    "start": "588160",
    "end": "594160"
  },
  {
    "text": "and we see what they are optimizing for so they are clearly targeting uh things like llms which are not necessarily the",
    "start": "594160",
    "end": "600519"
  },
  {
    "text": "main use cases we have internally for machine learning but what this means is that people are following the trend when",
    "start": "600519",
    "end": "606720"
  },
  {
    "text": "technology allows you to do something you start building new use cases and this is what we are seeing in house the",
    "start": "606720",
    "end": "612279"
  },
  {
    "text": "fact that you can have such large models uh in in a GPU and such power in a single GPU means that people are",
    "start": "612279",
    "end": "618600"
  },
  {
    "text": "considering what I was showing at the beginning of having this Custom Electronics for for the very fast uh uh",
    "start": "618600",
    "end": "624800"
  },
  {
    "text": "filtering in the detectors this can now be replaced potentially by uh more",
    "start": "624800",
    "end": "630040"
  },
  {
    "text": "commodity Hardware uh with gpus and other accelerators so these use cases are coming at the same time people will",
    "start": "630040",
    "end": "637120"
  },
  {
    "text": "want to have the new fancy uh gpus from our side because they're extremely",
    "start": "637120",
    "end": "642600"
  },
  {
    "text": "expensive we want to make them last longer so we already pushing the lifetime of this kind of Hardware from 5",
    "start": "642600",
    "end": "648040"
  },
  {
    "text": "years which is our standard to 8 years while people want to have a much faster",
    "start": "648040",
    "end": "653800"
  },
  {
    "text": "turnaround because this is what the public Cloud providers are giving them and then what this means is that we",
    "start": "653800",
    "end": "661360"
  },
  {
    "text": "have to make the best of our internal infrastructure but we also want to uh offer the the more advanced use cases",
    "start": "661360",
    "end": "669040"
  },
  {
    "text": "the hardware they need so uh what this means is that we probably or we are going hybrid clearly and this is to fit",
    "start": "669040",
    "end": "677639"
  },
  {
    "text": "both uh the needs but also the costs for this specific use cases and the very",
    "start": "677639",
    "end": "682720"
  },
  {
    "text": "large liery times we are facing today and we could say this will be a hype that disappears but we already saw this",
    "start": "682720",
    "end": "688760"
  },
  {
    "text": "with Bitcoin Co and now we have another hype maybe there will be a new a new one so it doesn't seem like this kind of uh",
    "start": "688760",
    "end": "695160"
  },
  {
    "text": "infrastructure is going away uh going hybrid brings a lot of new challenges and luckily we have some quite some",
    "start": "695160",
    "end": "702560"
  },
  {
    "text": "experience with this uh the the challenges uh the needs the new needs and requirements for network and storage",
    "start": "702560",
    "end": "709839"
  },
  {
    "text": "uh over the last 20 years we built something that you can see there on the top right which is the great Computing infrastructure for the LHC these are 200",
    "start": "709839",
    "end": "717720"
  },
  {
    "text": "different centers around the world world that we connected with very high uh throughput links so we know how to",
    "start": "717720",
    "end": "723680"
  },
  {
    "text": "distribute the data we know how to do the workload scheduling to put the workloads where the data is or move the",
    "start": "723680",
    "end": "729399"
  },
  {
    "text": "data when appropriate if we look at the cloud native stack though there are things that are missing we don't have The",
    "start": "729399",
    "end": "735839"
  },
  {
    "text": "Primitives yet completely there for advanced scheduling things like cues",
    "start": "735839",
    "end": "741199"
  },
  {
    "text": "things like co- scheduling and we have we don't have an easy way to handle multicluster the management of",
    "start": "741199",
    "end": "747480"
  },
  {
    "text": "multicluster is there the scheduling across multicluster is not there so this is something I find extremely",
    "start": "747480",
    "end": "753440"
  },
  {
    "text": "interesting to focus on right now projects like q and the the new features they've been adding around multiq are",
    "start": "753440",
    "end": "760120"
  },
  {
    "text": "really interesting and if you look at other projects like volcano or Armada I can see that they will start building on",
    "start": "760120",
    "end": "766760"
  },
  {
    "text": "on on this Bas layer as well what is really interesting here is that if we",
    "start": "766760",
    "end": "771839"
  },
  {
    "text": "solve this problem and we are focusing on this mostly because of uh geni uh we",
    "start": "771839",
    "end": "777760"
  },
  {
    "text": "are actually going to solve uh a lot of problems for HPC centers that have been using traditional tools like slurm for",
    "start": "777760",
    "end": "784920"
  },
  {
    "text": "scheduling and suddenly they start being able maybe to offer a pure Cloud native",
    "start": "784920",
    "end": "790440"
  },
  {
    "text": "uh API to their users this is a huge simplification and for us this would mean that we can use the same API on",
    "start": "790440",
    "end": "797040"
  },
  {
    "text": "premises public Cloud providers and uh targeting HPC centers as well so this is",
    "start": "797040",
    "end": "802480"
  },
  {
    "text": "a something that will be very interesting to follow in the next year or so and the last one I have in this this",
    "start": "802480",
    "end": "809279"
  },
  {
    "text": "is came while discussing with Maxon for this talk is that we probably need to start focusing on the python for our C++",
    "start": "809279",
    "end": "816720"
  },
  {
    "text": "infrastructure if you have anything ever to do with high energy physics you probably heard of the root framework",
    "start": "816720",
    "end": "823079"
  },
  {
    "text": "which is a data analysis framework developed atern it's written in C++ and there's a lot of people that are",
    "start": "823079",
    "end": "829800"
  },
  {
    "text": "expert in C++ that love it and wouldn't change C++ for anything else but there's a lot of other people that do not want",
    "start": "829800",
    "end": "836160"
  },
  {
    "text": "to handle the complexity of C++ so there's a layer called pyroot that kind of simplifies the the the life of a data",
    "start": "836160",
    "end": "844720"
  },
  {
    "text": "uh data physics analysis um physicist uh quite quite a",
    "start": "844720",
    "end": "850240"
  },
  {
    "text": "lot so when we start looking at our our our uh stack we we we see constantly",
    "start": "850240",
    "end": "857639"
  },
  {
    "text": "when we offer uh tools like coup flow that people say okay my job is pending I have no idea what that means I don't",
    "start": "857639",
    "end": "864560"
  },
  {
    "text": "know how to debug it I run the cctl command that is in the documentation but I really don't know what I'm doing so",
    "start": "864560",
    "end": "870720"
  },
  {
    "text": "maybe the abstraction is an abstraction is required to kind of bridge this Gap uh",
    "start": "870720",
    "end": "877320"
  },
  {
    "text": "and then also for HPC there's this uh this lack of knowledge to answer HPC questions when",
    "start": "877320",
    "end": "883880"
  },
  {
    "text": "you're running very large uh workloads on a kind of batch-like way there are",
    "start": "883880",
    "end": "889680"
  },
  {
    "text": "things that you want answered uh to to be able to do your job properly things like how long is my workload going to",
    "start": "889680",
    "end": "896360"
  },
  {
    "text": "take to complete or when will re resources be available for my workload to run Cloud native stack wasn't",
    "start": "896360",
    "end": "903639"
  },
  {
    "text": "necessarily designed for this kind of workloads batch workloads it was more for the Ser Service uh oriented type of",
    "start": "903639",
    "end": "910040"
  },
  {
    "text": "workloads uh and maybe we need to like take a step back and see what Primitives we're missing to be able to answer these",
    "start": "910040",
    "end": "916240"
  },
  {
    "text": "kind of questions and with this I pass to Maxon to continue with a real use",
    "start": "916240",
    "end": "923120"
  },
  {
    "text": "case all right um so yeah sorry so go to the use case so as I mentioned like one",
    "start": "923120",
    "end": "929639"
  },
  {
    "text": "of the things we're really interested in Atlas is to find hi B and you have a nice visualization of such an event in",
    "start": "929639",
    "end": "936000"
  },
  {
    "text": "Atlas this is a real event in which you can see one fundamental particle called immune uh that is emanated from the main",
    "start": "936000",
    "end": "942399"
  },
  {
    "text": "event and also you can distinguish these two bluish cone which are more visible on this side which is a h bosion",
    "start": "942399",
    "end": "949079"
  },
  {
    "text": "decaying to two types of quarks called bees and we want to be able to identify these quarks in our detector you can see",
    "start": "949079",
    "end": "956000"
  },
  {
    "text": "that this leaves a very complicated signature definitely impossible to analyze humanly so we do need data",
    "start": "956000",
    "end": "962519"
  },
  {
    "text": "analysis techniques and what we want to be able to do more specifically is classification of these B quarks versus",
    "start": "962519",
    "end": "968639"
  },
  {
    "text": "other types of quarks which are season light primarily so and of course we want to do that because we want to study the",
    "start": "968639",
    "end": "974480"
  },
  {
    "text": "higs because in the end this is a physics experiment and we want to make conclusions on the theory the best way to find these uh B quarks B jets is to",
    "start": "974480",
    "end": "982800"
  },
  {
    "text": "use machine learning for the because they give the best accuracy obviously and obviously to get the best accuracy",
    "start": "982800",
    "end": "987880"
  },
  {
    "text": "you need the best performing machine learning model so you need to optimize your hyper parameters which is very costly and this talk is this part of the",
    "start": "987880",
    "end": "995079"
  },
  {
    "text": "talk really is about this it's about a framework to improve the hyper pment optimization using Cube flow and a",
    "start": "995079",
    "end": "1001000"
  },
  {
    "text": "technique from the literature so on our side we've been not only focusing on the high performances",
    "start": "1001000",
    "end": "1007360"
  },
  {
    "text": "of course we've also been try othero architecture and I'm just showing this plot so that you have a bit of a an idea",
    "start": "1007360",
    "end": "1013199"
  },
  {
    "text": "of the history of of the project so uh we're doing B versus C versus L classification which every time they qus",
    "start": "1013199",
    "end": "1021440"
  },
  {
    "text": "and you can see how the model's performance evolves through the year so this is at the Feb efficiency",
    "start": "1021440",
    "end": "1026678"
  },
  {
    "text": "identification efficiency it's showing you in green the C rejection and in blue",
    "start": "1026679",
    "end": "1032280"
  },
  {
    "text": "the light rejection where the rejection is the inverse of the misclassification efficiency and you can see that through",
    "start": "1032280",
    "end": "1038520"
  },
  {
    "text": "the years adopting more advanced machine learning uh from like a deep NE Network to including a recur Network to using a",
    "start": "1038520",
    "end": "1046760"
  },
  {
    "text": "deep set base architecture using graph attention and finally using Gen 2 which is a",
    "start": "1046760",
    "end": "1052400"
  },
  {
    "text": "Transformer based model I'm going to mostly talk about uh we managed to have this really nice Improvement in um in",
    "start": "1052400",
    "end": "1058960"
  },
  {
    "text": "performance and this is very important for us because these are very difficult and rare events to find so we literally",
    "start": "1058960",
    "end": "1065200"
  },
  {
    "text": "sifting through uh to find like the needle in the Hast stack even though the H tack is much bigger and all of these",
    "start": "1065200",
    "end": "1071679"
  },
  {
    "text": "models are trained in the similar way they use combined type of inputs which combines different types of physics",
    "start": "1071679",
    "end": "1077520"
  },
  {
    "text": "inputs so that they can output the probability for each for jet to be of",
    "start": "1077520",
    "end": "1082640"
  },
  {
    "text": "each quark and then we use this we build discriminant based on this code that later on uh an analysis can use to uh",
    "start": "1082640",
    "end": "1089840"
  },
  {
    "text": "you know do the data analysis they all train on Monte simulated data but they calibrated on real data to account for",
    "start": "1089840",
    "end": "1097000"
  },
  {
    "text": "some mismodeling effects there and uh so the model I'm going to focus on is the",
    "start": "1097000",
    "end": "1102159"
  },
  {
    "text": "gene2 model which is a Transformer based uh model which we could describe as a multimodal multitask uh model",
    "start": "1102159",
    "end": "1109280"
  },
  {
    "text": "it's multimodel in the physics sense that it combines different types of physics input uh which for us is kind of",
    "start": "1109280",
    "end": "1117000"
  },
  {
    "text": "nice because it combines very things we use historically to treat in very different ways we used to have one",
    "start": "1117000",
    "end": "1122760"
  },
  {
    "text": "network that would deal with one type of information and another Network that would build use another one and the nice",
    "start": "1122760",
    "end": "1128520"
  },
  {
    "text": "thing with this single architecture that is combining the different type of input uh it's much simpler to maintain and",
    "start": "1128520",
    "end": "1135240"
  },
  {
    "text": "upgrade and uh do studies such as hyper mization or when we change the entire software stack to reconstruct several uh",
    "start": "1135240",
    "end": "1142440"
  },
  {
    "text": "sub elements and it gives us this Simplicity with the state of theart performance so really uh the usual Trend",
    "start": "1142440",
    "end": "1149799"
  },
  {
    "text": "in I think in machine learning to have one large huge Network able to do several stuff at once and really able to",
    "start": "1149799",
    "end": "1156200"
  },
  {
    "text": "do several stuff at once because it's multitask and you can see this from the fact it's not just predicting uh the",
    "start": "1156200",
    "end": "1161799"
  },
  {
    "text": "flavor so the class it's also predicting um other physics uh physically relevant",
    "start": "1161799",
    "end": "1168840"
  },
  {
    "text": "uh information that we know from expert knowledge so it's a way to put expert knowledge into the design and it's",
    "start": "1168840",
    "end": "1174600"
  },
  {
    "text": "working really well with the only is quite resource intensive for us to train",
    "start": "1174600",
    "end": "1179679"
  },
  {
    "text": "it takes us roughly two to three a100 gpus an entire week of training uh to do",
    "start": "1179679",
    "end": "1185000"
  },
  {
    "text": "wantful training due to the size of the data set mostly and the problem is we are not a large tech company and most",
    "start": "1185000",
    "end": "1192880"
  },
  {
    "text": "people in our collaboration do not have access to a high performance cluster with the sufficient amount of GPU to be",
    "start": "1192880",
    "end": "1199000"
  },
  {
    "text": "able to contribute to this sort of project it also makes it prohibited large for us to do hyper parameter",
    "start": "1199000",
    "end": "1205000"
  },
  {
    "text": "optimization uh just given the scale of one training and the fact that you have to iterate um so really this talk is about",
    "start": "1205000",
    "end": "1211919"
  },
  {
    "text": "you know how we could democratize this and obviously this is the c conference",
    "start": "1211919",
    "end": "1217280"
  },
  {
    "text": "we thought that this would be like a nice way to do this for several reasons so first the nice reason is that this uh",
    "start": "1217280",
    "end": "1224080"
  },
  {
    "text": "um you know container orchestration engine characteristic of cow uh is quite SU able with our framework we we always",
    "start": "1224080",
    "end": "1231919"
  },
  {
    "text": "uh keep our code on gnap repository and it's quite already well uh adopted to to",
    "start": "1231919",
    "end": "1239000"
  },
  {
    "text": "make them build build buildable into executable images um so using um the CI um The",
    "start": "1239000",
    "end": "1246760"
  },
  {
    "text": "Continuous integration tools of a gitlab so for us it's very easy to integrate this with uh this KP framework in this",
    "start": "1246760",
    "end": "1253960"
  },
  {
    "text": "case because it's about optimization uh that is provided by C uh in this um this",
    "start": "1253960",
    "end": "1260440"
  },
  {
    "text": "website here which is This Server that's installed withlow and on our side more specifically so we just have our code on",
    "start": "1260440",
    "end": "1266799"
  },
  {
    "text": "the gitlab which is executable and we have our data accessible for S3 like data storage that we can mount uh more",
    "start": "1266799",
    "end": "1273880"
  },
  {
    "text": "locally so that when we train uh the the speed of loading the input output is is",
    "start": "1273880",
    "end": "1279039"
  },
  {
    "text": "higher and um one thing we really like with this approach is that we can",
    "start": "1279039",
    "end": "1284279"
  },
  {
    "text": "actually use you guys work which means that we get for example access to machine learning Andre which we normally",
    "start": "1284279",
    "end": "1291039"
  },
  {
    "text": "not use uh so it really gives us uh easily access to a lot of development in",
    "start": "1291039",
    "end": "1296279"
  },
  {
    "text": "this side and I just want to highlight like what we think are the key points that really makes it interesting for us",
    "start": "1296279",
    "end": "1303159"
  },
  {
    "text": "uh so first of all it's this open source and active community that really continuously develop these tools which means that we don't have to do it",
    "start": "1303159",
    "end": "1309039"
  },
  {
    "text": "ourselves which is not our expertise uh on on the atlas size more but exper is",
    "start": "1309039",
    "end": "1314559"
  },
  {
    "text": "in on the understand side of course uh for us it's really viewed as a multiplatform and very flexible",
    "start": "1314559",
    "end": "1320919"
  },
  {
    "text": "framework meaning that we don't really we're not really so dependent on one uh server we could really move if we need",
    "start": "1320919",
    "end": "1327919"
  },
  {
    "text": "to to a private Cloud uh provider uh if we need to massively scale suddenly and",
    "start": "1327919",
    "end": "1333720"
  },
  {
    "text": "you know for a single one of project uh so really it gives us an optimized resource usage also it means that we",
    "start": "1333720",
    "end": "1340279"
  },
  {
    "text": "could more easily share Hardware resource with the other experiment because at the moment the thing is quite",
    "start": "1340279",
    "end": "1345720"
  },
  {
    "text": "fragmented and uh we do not work with other experiment exper to share Hardware uh which is not ideal because we would",
    "start": "1345720",
    "end": "1351919"
  },
  {
    "text": "like to have huge GPU clusters and uh it would be easier if we were to share them uh with the other experiments and again",
    "start": "1351919",
    "end": "1358799"
  },
  {
    "text": "one strong point for us is that it's accessible to everyone so it really democratize access to machine learning",
    "start": "1358799",
    "end": "1364320"
  },
  {
    "text": "heavy projects in our experiment and has the nice side benefits of having this",
    "start": "1364320",
    "end": "1369559"
  },
  {
    "text": "cool visual visualization when you do hyper optimization ktip which you you're probably familiar",
    "start": "1369559",
    "end": "1375440"
  },
  {
    "text": "with all right but this is not enough of course because um it just it's a nice framework to run something but it",
    "start": "1375440",
    "end": "1381760"
  },
  {
    "text": "doesn't mean that the thing you're running is simpler uh you can use all stepping from a hyper P optimization but",
    "start": "1381760",
    "end": "1387480"
  },
  {
    "text": "we still need to reduce the computational complexity so that we able to do hyper Pate optimization of our",
    "start": "1387480",
    "end": "1393200"
  },
  {
    "text": "Transformer and this is where the talk slightly changes Direction and goes into a technique because there is a technique",
    "start": "1393200",
    "end": "1399080"
  },
  {
    "text": "in the literature to be able to do the hyperparameter optimization at a lower cost for some of the hyper parameters",
    "start": "1399080",
    "end": "1405640"
  },
  {
    "text": "and this has to do with the parameterization of the network and here you have like a classical deep",
    "start": "1405640",
    "end": "1410799"
  },
  {
    "text": "neural network and what I mean by parameterization really is the way you initialize the weights so the way you",
    "start": "1410799",
    "end": "1416120"
  },
  {
    "text": "sample them from a random uh distribution function in this case Goan but really doesn't matter so much what",
    "start": "1416120",
    "end": "1422640"
  },
  {
    "text": "really matters is that the variance is inversely proportional to the size of the input of the layer and the learning",
    "start": "1422640",
    "end": "1429120"
  },
  {
    "text": "rate classically for all of the weights in the network are the same um ether",
    "start": "1429120",
    "end": "1434360"
  },
  {
    "text": "like the mous learning race so this would be a standard parameterization a paramet",
    "start": "1434360",
    "end": "1440039"
  },
  {
    "text": "ization and one way to actually uh move into a nicer partiz for the project I'm",
    "start": "1440039",
    "end": "1446919"
  },
  {
    "text": "going to mention is to adopt this Maxim maximal update parametrization from the literature and I'm just highlighting the",
    "start": "1446919",
    "end": "1453760"
  },
  {
    "text": "key difference with the standard one the key difference being that the output layer viance is scaled down by the input",
    "start": "1453760",
    "end": "1460120"
  },
  {
    "text": "dimension of this layer and the learning rates of the Hidden and the output layers after the scaled on to um and",
    "start": "1460120",
    "end": "1467600"
  },
  {
    "text": "this is um this actually makes sense in a way like if you think of a network with a specific width the output layer",
    "start": "1467600",
    "end": "1474039"
  },
  {
    "text": "would be like the more opaque layer as if you were to think of a cross-section of the ocean the surface is most of the",
    "start": "1474039",
    "end": "1479600"
  },
  {
    "text": "Sun but the bottom really doesn't see so much of the Sun and here the sun would be the the loss function and uh yeah the",
    "start": "1479600",
    "end": "1485960"
  },
  {
    "text": "bottom would be the input layer so this has the effect of making the output side",
    "start": "1485960",
    "end": "1491200"
  },
  {
    "text": "of the network a bit more transparent so that the learning goes into depth and um",
    "start": "1491200",
    "end": "1497159"
  },
  {
    "text": "this is actually uh proven and I'm going to go into this later on but I'm just going to give you like the two key",
    "start": "1497159",
    "end": "1503360"
  },
  {
    "text": "conclusions from this maximum update primeti is it has this effect that uh",
    "start": "1503360",
    "end": "1508399"
  },
  {
    "text": "the updates of the activations become roughly independent of the size of the",
    "start": "1508399",
    "end": "1513760"
  },
  {
    "text": "width of the layer so just to clarify the width is the number of units per layer so the really the transversal",
    "start": "1513760",
    "end": "1521360"
  },
  {
    "text": "Dimension not the depth and also it has this nice provable uh theoretical",
    "start": "1521360",
    "end": "1526840"
  },
  {
    "text": "property that it's the maximal update one in the sense that it's as big as an update you could get for each",
    "start": "1526840",
    "end": "1532679"
  },
  {
    "text": "layer without leading to instabilities and uh there's a way to",
    "start": "1532679",
    "end": "1537720"
  },
  {
    "text": "see this which is quite cool and come from this paper on on the left uh which is to take the simplest Network you can",
    "start": "1537720",
    "end": "1543880"
  },
  {
    "text": "think of which is one uh input X pass through two layers an input layer U with",
    "start": "1543880",
    "end": "1550679"
  },
  {
    "text": "n Dimension and an output layer V with n Dimension 2 so here n would be the width",
    "start": "1550679",
    "end": "1556279"
  },
  {
    "text": "on the left you have the spks and on the right you have the M case and you can see like some of the key difference",
    "start": "1556279",
    "end": "1561840"
  },
  {
    "text": "differences at initialization are the way the they sampled and also the way the output",
    "start": "1561840",
    "end": "1567559"
  },
  {
    "text": "layer gets updated after one gradient step and if you look at what this layer",
    "start": "1567559",
    "end": "1572960"
  },
  {
    "text": "what what this network computes the f ofx after one gradient update on the SP side you have this ncy term feta UT",
    "start": "1572960",
    "end": "1580440"
  },
  {
    "text": "transpose U which by the L large numbers uh we'll scale not with the input X but",
    "start": "1580440",
    "end": "1586159"
  },
  {
    "text": "with the dimension of the layer n um so this means that this would not be with Independence in a way while on the",
    "start": "1586159",
    "end": "1593320"
  },
  {
    "text": "right side thanks to the modification to the rules of the parametrization you see that this terms is correctly scaled on",
    "start": "1593320",
    "end": "1599919"
  },
  {
    "text": "by n so that the scaling is with the input instead of with uh the dimension",
    "start": "1599919",
    "end": "1605399"
  },
  {
    "text": "and this can be Vis this can be actually viewed in a network so this is showing you a gen2 Lo architecture trained for",
    "start": "1605399",
    "end": "1612880"
  },
  {
    "text": "shown at three different time steps the initialization time step and one and two training steps the first row is SP and",
    "start": "1612880",
    "end": "1620720"
  },
  {
    "text": "the second row is M and every time each plot is showing you for different dimensions um the sum of the absolute",
    "start": "1620720",
    "end": "1627840"
  },
  {
    "text": "value of the preactivation weights of the different layers and each layer is its own curve and you can see that the",
    "start": "1627840",
    "end": "1634000"
  },
  {
    "text": "on on the SP side things quickly get unstable after a few um steps of uh of",
    "start": "1634000",
    "end": "1640600"
  },
  {
    "text": "learning for very large width and on the M side however things stay stable so",
    "start": "1640600",
    "end": "1646000"
  },
  {
    "text": "really it scales nicely across width and uh it's the maximum update in the sense that it's stay",
    "start": "1646000",
    "end": "1652760"
  },
  {
    "text": "stable and um this actually leads to a few properties that are quite nice and relevant to hyper pamet optimization the",
    "start": "1652760",
    "end": "1660159"
  },
  {
    "text": "first one is your trainings are stable from the learning rate point of view so that's actually already quite nice but",
    "start": "1660159",
    "end": "1666600"
  },
  {
    "text": "also a wider model is always going to be more performant than a less wide one and",
    "start": "1666600",
    "end": "1672080"
  },
  {
    "text": "that is on the training gloss you may over over fit but this is a side I'm not going to touch it so wi always better so",
    "start": "1672080",
    "end": "1679519"
  },
  {
    "text": "it also makes you know architecture search easier you just take a model that is as wide as you can afford and finally",
    "start": "1679519",
    "end": "1687120"
  },
  {
    "text": "and this is the key Point here is that you get similar performance I key across different width which means that for the",
    "start": "1687120",
    "end": "1693600"
  },
  {
    "text": "M side on the left scanning for one hyperparameter here which is the maximum value of the learning rate cul and",
    "start": "1693600",
    "end": "1700039"
  },
  {
    "text": "showing the validation loss on the on the xais you can see that they share the same best hyper parameter so that's",
    "start": "1700039",
    "end": "1706519"
  },
  {
    "text": "quite nice that means that if if you want to be efficient at this you scan oh sorry I should have said so there's",
    "start": "1706519",
    "end": "1712039"
  },
  {
    "text": "three curves and the yellow one is a Transformer with bending width of width 64 and the red one is width 128 while",
    "start": "1712039",
    "end": "1720720"
  },
  {
    "text": "the purple one is 256 so the thing that's nice with the fact that they all share the same best Hy parameter is that you could use the",
    "start": "1720720",
    "end": "1727159"
  },
  {
    "text": "small one which is much easier to train to find the best Hy parameter for the full width one while on the SP side you",
    "start": "1727159",
    "end": "1733840"
  },
  {
    "text": "have no search guarantee first even though in this case it seems to roughly work but also you get this really",
    "start": "1733840",
    "end": "1739559"
  },
  {
    "text": "unstable behavior when you get a learning rate that's too high which is again due to the fact uh the standard",
    "start": "1739559",
    "end": "1744880"
  },
  {
    "text": "parameterization models are not resistant to uh the to the width",
    "start": "1744880",
    "end": "1751000"
  },
  {
    "text": "scaling so this is quite nice actually and it leads to a algorith to do the",
    "start": "1751000",
    "end": "1756600"
  },
  {
    "text": "hyperparameter optimization that's come from this paper which is called mut transfer where you just basically do the",
    "start": "1756600",
    "end": "1762240"
  },
  {
    "text": "hyper parment optimization on the small model and you directly transfer that to the full model so that you don't you",
    "start": "1762240",
    "end": "1767760"
  },
  {
    "text": "have to you can avoid like the complexity of training that several times and we've tried this in Atlas and",
    "start": "1767760",
    "end": "1773960"
  },
  {
    "text": "uh what we've tried is because we have a we quite compute limited again uh we try to actually optimize two parameters the",
    "start": "1773960",
    "end": "1780240"
  },
  {
    "text": "maximal uh learning rate in the initial value of the learning rate scheder and this is shown here with the same types",
    "start": "1780240",
    "end": "1786159"
  },
  {
    "text": "of plot as the previous slide just different plots show you different uh initial value of the learning rate so",
    "start": "1786159",
    "end": "1791720"
  },
  {
    "text": "it's very efficient for us so actually so this argument makes it very efficient because if you just focus on the M side",
    "start": "1791720",
    "end": "1797559"
  },
  {
    "text": "which is the B bottom row you could have directly F the best one from the small model and it really avoids like a the",
    "start": "1797559",
    "end": "1804279"
  },
  {
    "text": "very costly uh large model training also it makes the model in general stable and",
    "start": "1804279",
    "end": "1809440"
  },
  {
    "text": "also it makes the model in general more performant than the equivalent sp sp side thanks to this maximal update and",
    "start": "1809440",
    "end": "1815799"
  },
  {
    "text": "this transparency uh Behavior so we think it's actually quite a nice framework to combine with",
    "start": "1815799",
    "end": "1821600"
  },
  {
    "text": "goodflow uh so from the ca side what we really expect is this you know natural multiplatform execution so that as a",
    "start": "1821600",
    "end": "1828200"
  },
  {
    "text": "user you don't really have to think uh where you're going to run um it's quite portable and flexible so if you know how",
    "start": "1828200",
    "end": "1834279"
  },
  {
    "text": "to use it on the on the sun one you can very easily move to a private Cloud one you just it's very nice you don't have",
    "start": "1834279",
    "end": "1840679"
  },
  {
    "text": "to think about slum and all of this stuff because you people do that and uh",
    "start": "1840679",
    "end": "1846440"
  },
  {
    "text": "to another extent it means that we have an improved resource usage means we can more easily share I know the results",
    "start": "1846440",
    "end": "1852080"
  },
  {
    "text": "with the other experiments we can also use resource that out there not really use uh from you know State cloud",
    "start": "1852080",
    "end": "1858639"
  },
  {
    "text": "or National Laboratories and stuff like that uh on the subject of the user case",
    "start": "1858639",
    "end": "1864039"
  },
  {
    "text": "here it really improves the hyper parameter search thanks to this uh you know Auto Machine learning algorithm so stopping and all of that so it also",
    "start": "1864039",
    "end": "1870720"
  },
  {
    "text": "exposes us to some of the development being happening in the machine Learning Community uh also it has this nice thing",
    "start": "1870720",
    "end": "1877880"
  },
  {
    "text": "that you know you can inspect and visualize it also makes it globally simpler to just install things and to uh",
    "start": "1877880",
    "end": "1883880"
  },
  {
    "text": "serve the model later on and importantly for us it's accessible to everyone in our collaboration so it really",
    "start": "1883880",
    "end": "1890039"
  },
  {
    "text": "democratizes access to this very interesting project for us and finally on the we want to combine this with M",
    "start": "1890039",
    "end": "1896840"
  },
  {
    "text": "which is um the this technique which means that the water model will always be more performing and makes it possible",
    "start": "1896840",
    "end": "1903880"
  },
  {
    "text": "for us to use M transfer which is very useful for example just to give you some numbers the full edding width model 256",
    "start": "1903880",
    "end": "1911720"
  },
  {
    "text": "has 2.3 million parameters and it takes 140 minutes for one epex on two gpus due to the size of the data set on the small",
    "start": "1911720",
    "end": "1918600"
  },
  {
    "text": "width one which has a 10 further parameter we can actually do an epic in 20 minutes with one GPU which means that",
    "start": "1918600",
    "end": "1924679"
  },
  {
    "text": "equal compute we can do four tests of the hyper parameters on the small model for one test of the large one which",
    "start": "1924679",
    "end": "1931000"
  },
  {
    "text": "means we get a far better coverage and I just want to conclude with the like reminding that this is quite important",
    "start": "1931000",
    "end": "1937240"
  },
  {
    "text": "like the sort of performance gains we have from the best hyper parameter is equivalent to the sort of performance",
    "start": "1937240",
    "end": "1942679"
  },
  {
    "text": "game we have from adopting way more complicated architecture changes of physics infus change",
    "start": "1942679",
    "end": "1948519"
  },
  {
    "text": "and this is shown here with a very elaborate Rock curves that's typical of the field where you have the bjet",
    "start": "1948519",
    "end": "1953799"
  },
  {
    "text": "efficiency on the x-axis and the y- axis are showing you the rejection for lights and cjets so we you really want to be",
    "start": "1953799",
    "end": "1960679"
  },
  {
    "text": "higher there and I showing you this for the best model in green and the worst model in purple from the scan on the",
    "start": "1960679",
    "end": "1966519"
  },
  {
    "text": "right and you can see that in Aras where we interested you roughly have 20 to 30%",
    "start": "1966519",
    "end": "1971799"
  },
  {
    "text": "gain really from just this one parameter change so really something important for us and uh I think that that's pretty",
    "start": "1971799",
    "end": "1978320"
  },
  {
    "text": "much it for us and thank you for your [Applause]",
    "start": "1978320",
    "end": "1988169"
  },
  {
    "text": "attention see again there's time for one question so oh",
    "start": "1990639",
    "end": "1995679"
  },
  {
    "text": "yeah there's a couple but uh I think we only have time for one so I don't know",
    "start": "1995679",
    "end": "2000919"
  },
  {
    "text": "do I",
    "start": "2000919",
    "end": "2003480"
  },
  {
    "text": "get thank you for the talk uh hi this is abishek from IBM research and I have two",
    "start": "2006760",
    "end": "2013120"
  },
  {
    "text": "questions both based on kubernetes so one is how do you manage custom Hardware",
    "start": "2013120",
    "end": "2018799"
  },
  {
    "text": "with kubernetes and I think you mentioned for quite some time that you use sharing techniques so how does",
    "start": "2018799",
    "end": "2025120"
  },
  {
    "text": "kubernetes help you in sharing your custom Hardware or gpus today yeah okay",
    "start": "2025120",
    "end": "2030480"
  },
  {
    "text": "so that's a very quick question so if you look at uh previous talks we've given uh I think last year at ccon uh",
    "start": "2030480",
    "end": "2037880"
  },
  {
    "text": "Amsterdam we went a bit more in detail so um we've been we wrote If you go to",
    "start": "2037880",
    "end": "2044320"
  },
  {
    "text": "kubernetes dod. ser. you will find a series of blog posts about sharing gpus",
    "start": "2044320",
    "end": "2051040"
  },
  {
    "text": "and we tried and benchmarked all sorts of possibilities so we tried anything",
    "start": "2051040",
    "end": "2056358"
  },
  {
    "text": "from using uh pure sharing of the gpus with no uh knowledge between the",
    "start": "2056359",
    "end": "2061878"
  },
  {
    "text": "workloads and the issues with that we also documented quite well uh how to use Nvidia Mig uh and the integration with",
    "start": "2061879",
    "end": "2068919"
  },
  {
    "text": "the GPU operator from Nvidia uh and we've been trying also NPS uh for for",
    "start": "2068919",
    "end": "2075679"
  },
  {
    "text": "for a possibility of sharing as well so basically all our clusters have these",
    "start": "2075679",
    "end": "2081200"
  },
  {
    "text": "capabilities of uh of partitioning in either physical or logical way all the",
    "start": "2081200",
    "end": "2087480"
  },
  {
    "text": "GPU resources and this is what we expose to the coplow users we you might give get a full GPU a fraction of a GPU or a",
    "start": "2087480",
    "end": "2096158"
  },
  {
    "text": "totally shared GPU depending on what you're asking for and the availability is the main thing uh if we ask for a",
    "start": "2096159",
    "end": "2102800"
  },
  {
    "text": "full GPU full a100 or h100 uh we don't have that many so you won't get them",
    "start": "2102800",
    "end": "2108480"
  },
  {
    "text": "very fast and people fall back to the second best uh I think there was another part of the question but I don't",
    "start": "2108480",
    "end": "2115760"
  },
  {
    "text": "remember how do we manage custom Hardware so Mo in most cases for now these are",
    "start": "2115760",
    "end": "2121200"
  },
  {
    "text": "fpgas uh and these are just attached to the nodes and we just pass through in the same way we do for gpus uh so all",
    "start": "2121200",
    "end": "2128480"
  },
  {
    "text": "the GPU access we are giving we are not virtualizing we're just doing PCI pass through uh in some cases partitioning it",
    "start": "2128480",
    "end": "2135680"
  },
  {
    "text": "using kubernetes but for the notes themselves they they are just doing PCI pass through thank",
    "start": "2135680",
    "end": "2141720"
  },
  {
    "text": "you hey a really cool talk madri from milal um wondering if you had to pick",
    "start": "2141720",
    "end": "2147520"
  },
  {
    "text": "one key problem in the multicluster scheduling space that you alluded to what would that",
    "start": "2147520",
    "end": "2154000"
  },
  {
    "text": "be uh I think I think the scheduling is",
    "start": "2154000",
    "end": "2160680"
  },
  {
    "text": "is like one the main the main issue is that there is no notion in the scheduler",
    "start": "2160680",
    "end": "2166599"
  },
  {
    "text": "of multicluster there are some some efforts to add this to the to the stack",
    "start": "2166599",
    "end": "2173400"
  },
  {
    "text": "uh but there's there's little the scheduler can do right now because it doesn't know about the availability so we tried in the past to kind of go",
    "start": "2173400",
    "end": "2180960"
  },
  {
    "text": "around this so we tried something a few years ago where we implemented a virtual kuet uh that would basically be an",
    "start": "2180960",
    "end": "2188560"
  },
  {
    "text": "abstraction of a full kubernetes cluster behind so that we could advertise to the scheduler the resources from many",
    "start": "2188560",
    "end": "2194800"
  },
  {
    "text": "clusters and this was something that was picked up uh by some people uh but it",
    "start": "2194800",
    "end": "2200319"
  },
  {
    "text": "has drawbacks as well so it it is not a perfect situation I I would say the ideal thing would be for the scheduler",
    "start": "2200319",
    "end": "2207319"
  },
  {
    "text": "somehow to be able to incorporate the resource availability of multiple clusters uh I don't know exactly how we",
    "start": "2207319",
    "end": "2214640"
  },
  {
    "text": "can do this because so availability based scheduling is number one I think so yeah I think that's it",
    "start": "2214640",
    "end": "2221640"
  },
  {
    "text": "yep okay thank you very much and if you have more questions just [Applause]",
    "start": "2222880",
    "end": "2230460"
  },
  {
    "text": "reaches",
    "start": "2237280",
    "end": "2240280"
  }
]