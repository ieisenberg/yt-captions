[
  {
    "text": "thank you all for sticking around last day conference um appreciate you all showing up so",
    "start": "0",
    "end": "5520"
  },
  {
    "text": "we're going to talk about fast image polls using ipfs and opportunistic caching",
    "start": "5520",
    "end": "10559"
  },
  {
    "text": "and my name is Chris I work at gitpod and with me on this stage should be",
    "start": "10559",
    "end": "16080"
  },
  {
    "text": "Alejandro who is actually in a tab in a Google meet in this video browser right now so we'll pull them in later for the",
    "start": "16080",
    "end": "23100"
  },
  {
    "text": "Q a session so we both work at gitpod and what we do",
    "start": "23100",
    "end": "29039"
  },
  {
    "text": "is we turn the hours and days that it takes to set up a deaf environment into seconds",
    "start": "29039",
    "end": "34260"
  },
  {
    "text": "and the reason I'm bringing that up is part of this deaf environment is a Docker image that you can choose or that",
    "start": "34260",
    "end": "41219"
  },
  {
    "text": "we can build for you and so just to show a quick",
    "start": "41219",
    "end": "46320"
  },
  {
    "text": "slightly sped up demo I was just on GitHub clicked the button and then it's preparing the workspace",
    "start": "46320",
    "end": "52739"
  },
  {
    "text": "pulling the image right and this is the face this is exactly the thing that we're talking about initializing content and then you",
    "start": "52739",
    "end": "59280"
  },
  {
    "text": "get ready to go def environment in this case vs code can be other IDs",
    "start": "59280",
    "end": "65479"
  },
  {
    "text": "and the other I the fact that it can be other Ides will also become relevant later in the talk",
    "start": "65580",
    "end": "72200"
  },
  {
    "text": "so that's what gitpod does and this is the context in which we're talking so we pull a lot of different images",
    "start": "72659",
    "end": "78420"
  },
  {
    "text": "so over the any sort of period of seven days we'll pull more than ten thousand",
    "start": "78420",
    "end": "83580"
  },
  {
    "text": "distinct images measured by their manifest not necessarily so they might",
    "start": "83580",
    "end": "88619"
  },
  {
    "text": "have some layer overlap but it's um ten thousand distinct images and we",
    "start": "88619",
    "end": "94080"
  },
  {
    "text": "cache more than a terabyte of um of image layers and these images vary greatly in size anywhere between 200",
    "start": "94080",
    "end": "101880"
  },
  {
    "text": "megabytes to several tens of gigabytes so it's and we have very little control",
    "start": "101880",
    "end": "108240"
  },
  {
    "text": "over what images our users will bring so we have to be very Thrifty to make that",
    "start": "108240",
    "end": "113700"
  },
  {
    "text": "fast so just to tell you a bit about where",
    "start": "113700",
    "end": "118740"
  },
  {
    "text": "we're coming from from a Time perspective and where we've been heading so about 12 months ago when we embarked on a let's get this faster Journey",
    "start": "118740",
    "end": "126299"
  },
  {
    "text": "P95 workspace startup times was more than 10 minutes",
    "start": "126299",
    "end": "131520"
  },
  {
    "text": "and our Prometheus histogram just you know stopped at 10 minutes within like beyond that it doesn't matter",
    "start": "131520",
    "end": "138180"
  },
  {
    "text": "um you're going to assume the system is broken anyways so more than 10 minutes and today we've",
    "start": "138180",
    "end": "145379"
  },
  {
    "text": "brought this down to 120. that is still a very long time to be waiting in front of a computer and until you come work so",
    "start": "145379",
    "end": "153360"
  },
  {
    "text": "we'll keep on working bringing that down but this is by far the largest reduction that we've been able to achieve and a",
    "start": "153360",
    "end": "160560"
  },
  {
    "text": "good part of this is because of the caching mechanisms that we'll discuss in this in this talk",
    "start": "160560",
    "end": "166500"
  },
  {
    "text": "the p50 startup time more than halved as well so we're coming from about 24",
    "start": "166500",
    "end": "171720"
  },
  {
    "text": "seconds down to about 10 seconds today",
    "start": "171720",
    "end": "176120"
  },
  {
    "text": "so what have we tried well first of all we tried nothing the Baseline so gipport workspaces are",
    "start": "177599",
    "end": "184680"
  },
  {
    "text": "essentially kubernetes pods and then some and the Baseline was to just use the",
    "start": "184680",
    "end": "190800"
  },
  {
    "text": "kubernetes mechanisms that are out there so we would rely on the layer reuse",
    "start": "190800",
    "end": "195900"
  },
  {
    "text": "essentially that would that would happen within the nodes and that is sort of the 12 month ago",
    "start": "195900",
    "end": "202500"
  },
  {
    "text": "situation that you just saw me refer to then we try to pre-pool images so what",
    "start": "202500",
    "end": "209159"
  },
  {
    "text": "we would do is we would put a Daemon set in place that would pull well-known images so while we do pull a large",
    "start": "209159",
    "end": "216300"
  },
  {
    "text": "number of different images that is not a uniform distribution it is very very",
    "start": "216300",
    "end": "222360"
  },
  {
    "text": "very spiky so there are like five to ten images that are used a lot and then there are some that I used very seldomly",
    "start": "222360",
    "end": "229920"
  },
  {
    "text": "and so we try to pre-poll those five to ten images that we know we basically",
    "start": "229920",
    "end": "235080"
  },
  {
    "text": "looked at what was used last week and assumed that that was going to be used next week and we would pre-pull that the",
    "start": "235080",
    "end": "240900"
  },
  {
    "text": "problem with this approach is that especially during rush hour so in the",
    "start": "240900",
    "end": "245940"
  },
  {
    "text": "morning when a lot of people fire up their deaf environments will have scale up of nodes too and then the demon set",
    "start": "245940",
    "end": "253200"
  },
  {
    "text": "will come in there are going to be a lot of image pulls on that note happening already so not only is the demon set",
    "start": "253200",
    "end": "258600"
  },
  {
    "text": "ineffective if it even exuberates the problem because it's going to do pulls on top of the pulse that uses actually",
    "start": "258600",
    "end": "264960"
  },
  {
    "text": "are waiting for The Next Step was to pre-bake that into VM images so we run on on k3s on gcp",
    "start": "264960",
    "end": "273419"
  },
  {
    "text": "with VM images so we just pre-baked those into the VM image so that once the VM came up it",
    "start": "273419",
    "end": "280680"
  },
  {
    "text": "wouldn't need to pull those commonly used images that also worked it worked considerably",
    "start": "280680",
    "end": "286800"
  },
  {
    "text": "better than pre-pulling specifically because pulling the VM image is faster",
    "start": "286800",
    "end": "292199"
  },
  {
    "text": "than pulling a lot of individual tiny top comparatively tiny top files",
    "start": "292199",
    "end": "298740"
  },
  {
    "text": "the downside of that is that we would need we would churn those VM images very often like in order for this to be",
    "start": "298740",
    "end": "304740"
  },
  {
    "text": "effective we would need to produce new VM images very very often they would loot like this would lose its",
    "start": "304740",
    "end": "309840"
  },
  {
    "text": "Effectiveness in about three days um and so that just wasn't a viable path",
    "start": "309840",
    "end": "315840"
  },
  {
    "text": "forward also it helped a lot on the p50 it did not help a lot on the P95 right",
    "start": "315840",
    "end": "321479"
  },
  {
    "text": "the P95 is caused by people bringing images that are not commonly used but large",
    "start": "321479",
    "end": "326699"
  },
  {
    "text": "and this would specifically help with images that are commonly used because we just take like the 10 or 15 most",
    "start": "326699",
    "end": "332820"
  },
  {
    "text": "commonly used images and we pre-baked those so we'll help a lot with p50 not with P95",
    "start": "332820",
    "end": "338820"
  },
  {
    "text": "and all of this then led to looking into other means of speeding up the the image",
    "start": "338820",
    "end": "344220"
  },
  {
    "text": "pull times and so we're not alone in this endeavor there is a lot of community effort into",
    "start": "344220",
    "end": "350280"
  },
  {
    "text": "that goes into speeding up image pull times and this is like by no means a complete list and",
    "start": "350280",
    "end": "356639"
  },
  {
    "text": "it's also not an authoritative taxonomy of ways to speed up image pull times",
    "start": "356639",
    "end": "361800"
  },
  {
    "text": "um it is sort of the way we've we've been looking at it and one is sort of distributed file systems where you try",
    "start": "361800",
    "end": "367080"
  },
  {
    "text": "and distribute the um the layers essentially right that's what's what's expensive at the end",
    "start": "367080",
    "end": "372539"
  },
  {
    "text": "across the different nodes so you don't have to pull them from a registry that is potentially far away and slow to",
    "start": "372539",
    "end": "378300"
  },
  {
    "text": "reach then there is a lot of work that goes into lazy pulling most notably stargaze",
    "start": "378300",
    "end": "386220"
  },
  {
    "text": "um who really kicked a lot of this off um there's slacker there's recently",
    "start": "386220",
    "end": "391380"
  },
  {
    "text": "nidas and then also Amazon presenting the seekable oci project all very very",
    "start": "391380",
    "end": "399060"
  },
  {
    "text": "exciting work and some of the concepts that you'll see presented in the coming",
    "start": "399060",
    "end": "404699"
  },
  {
    "text": "slides are based on some of this work and then there's also peer-to-peer distribution mechanisms notably",
    "start": "404699",
    "end": "411720"
  },
  {
    "text": "dragonfly um where the lines in this taxonomy blur a bit also towards distributed file",
    "start": "411720",
    "end": "418860"
  },
  {
    "text": "systems in getpad we have a component that we",
    "start": "418860",
    "end": "425160"
  },
  {
    "text": "call registry facade and this will be Central to how we implemented this caching so let's talk about that for a",
    "start": "425160",
    "end": "431880"
  },
  {
    "text": "second earlier I mentioned that you can choose different Ides that you can run within",
    "start": "431880",
    "end": "437280"
  },
  {
    "text": "your workspace right it doesn't just have to be vs code it can be jetbrains products for example and so we require a",
    "start": "437280",
    "end": "444300"
  },
  {
    "text": "bunch of layers to sit on top of what a user configured",
    "start": "444300",
    "end": "449460"
  },
  {
    "text": "and it's the IDE layers and then there's also some gitpod specific tooling that we require within the same file system",
    "start": "449460",
    "end": "455520"
  },
  {
    "text": "of your of your workspace and we could just build out those combinations right so whenever we update",
    "start": "455520",
    "end": "462479"
  },
  {
    "text": "our own images or whenever there are ID images we could just multiply that out with the images that users have",
    "start": "462479",
    "end": "467880"
  },
  {
    "text": "configured this would lead to a to an explosion quite literally of images that we need",
    "start": "467880",
    "end": "474300"
  },
  {
    "text": "to take care of it would also not provide a very good user experience because whenever we",
    "start": "474300",
    "end": "481139"
  },
  {
    "text": "update one of our images and we'll deploy anywhere between 1 to 15 to 20 times a day you know you'd have to",
    "start": "481139",
    "end": "487860"
  },
  {
    "text": "rebuild this every single time so instead",
    "start": "487860",
    "end": "494039"
  },
  {
    "text": "a registry facade is what workspace pods actually pull from so the image that is",
    "start": "494039",
    "end": "501060"
  },
  {
    "text": "used will not be directly pulled from some remote registry but instead it will point to an instance of registry facade",
    "start": "501060",
    "end": "507000"
  },
  {
    "text": "usually running on the same node and then registry facade will dynamically assemble the Manifest",
    "start": "507000",
    "end": "514520"
  },
  {
    "text": "to be pulled as part of this Dynamic assembly and",
    "start": "514520",
    "end": "521700"
  },
  {
    "text": "this is where ipfs comes in we can add caching mechanisms to speed up the",
    "start": "521700",
    "end": "527399"
  },
  {
    "text": "subsequent pool of the individual blobs and also that manifest generation like",
    "start": "527399",
    "end": "534240"
  },
  {
    "text": "the stacking this production of a manifest itself takes time so we want to catch that as well",
    "start": "534240",
    "end": "541760"
  },
  {
    "text": "so I'm broadly going to assume you all know what ipfs is",
    "start": "542820",
    "end": "547860"
  },
  {
    "text": "um I by no means an expert on this so I will not claim to be broadly speaking ipfs is a peer-to-peer",
    "start": "547860",
    "end": "555720"
  },
  {
    "text": "based distributed file system and for the context of this talk that's good",
    "start": "555720",
    "end": "561180"
  },
  {
    "text": "enough a description there is much much more that goes into it um and I want to be aware of that of",
    "start": "561180",
    "end": "567839"
  },
  {
    "text": "that depth without needing to go into it",
    "start": "567839",
    "end": "571880"
  },
  {
    "text": "so what we did then is we embarked on the Journey of trying to speed up image poles and what",
    "start": "572880",
    "end": "579180"
  },
  {
    "text": "we're going to show here is what that was specifically done around April April to May earlier this year",
    "start": "579180",
    "end": "585660"
  },
  {
    "text": "so some of the statements about other projects that we'll make might no longer be true",
    "start": "585660",
    "end": "592260"
  },
  {
    "text": "and I'd if any of the maintainers of those projects are in the room I'd very much welcome them to speak up in in the",
    "start": "592260",
    "end": "599040"
  },
  {
    "text": "Q a session and bring corrections to um to what I'm saying",
    "start": "599040",
    "end": "605480"
  },
  {
    "text": "so the very very first thing that we did is we looked into nerd control or not cuddle ipfs registry",
    "start": "605760",
    "end": "612839"
  },
  {
    "text": "and what node cuttle ipfs registry does is it is a registry very much like our",
    "start": "612839",
    "end": "618480"
  },
  {
    "text": "own registry facade that you can run for example on the same node and that you can pull from",
    "start": "618480",
    "end": "624240"
  },
  {
    "text": "and the way you pull from it is you address the image using an ipfs Content",
    "start": "624240",
    "end": "630060"
  },
  {
    "text": "ID using an ipfsc ID so instead of doing",
    "start": "630060",
    "end": "635160"
  },
  {
    "text": "uh I don't know GCR or something something you would do a local host 50",
    "start": "635160",
    "end": "640980"
  },
  {
    "text": "50 if that's what you're not control ipfs registry runs on slash",
    "start": "640980",
    "end": "646500"
  },
  {
    "text": "unwieldy Content ID and then it would that would point to a manifest and it would pull from there",
    "start": "646500",
    "end": "653839"
  },
  {
    "text": "so in kubernetes deployment you know you would then use something like that as an",
    "start": "653940",
    "end": "659100"
  },
  {
    "text": "image this got us sort of got our feet wet with ipfs it was not the solution",
    "start": "659100",
    "end": "667380"
  },
  {
    "text": "um to our problem first off you need to know the the CID the content ID and the way you would usually do that is you",
    "start": "667380",
    "end": "673320"
  },
  {
    "text": "would use nerd control ipfs push and you would push into that registry and that would give you the CID that you could",
    "start": "673320",
    "end": "679019"
  },
  {
    "text": "then later use to reference and at the time there was no content",
    "start": "679019",
    "end": "685380"
  },
  {
    "text": "distribution across nodes because the example did not incorporate ipfs cluster",
    "start": "685380",
    "end": "690839"
  },
  {
    "text": "and this has since been fixed so thank you very much to to the maintainers um who added that to to the examples",
    "start": "690839",
    "end": "699680"
  },
  {
    "text": "so this got a feed weight with ipfs and it showed us that there is a way how we could distribute that content using all",
    "start": "701940",
    "end": "708540"
  },
  {
    "text": "those blobs using ipfs so we started introducing this with registry facade and the first attempt",
    "start": "708540",
    "end": "714420"
  },
  {
    "text": "was to use stargaze the Stargate snapshotter and so the Stargate snapshotter has a",
    "start": "714420",
    "end": "720120"
  },
  {
    "text": "feature where if the oci descriptor of a blob contains an ipfs URL it will pull",
    "start": "720120",
    "end": "727260"
  },
  {
    "text": "from that right something that say your your regular overlay snapshot I could not do",
    "start": "727260",
    "end": "733500"
  },
  {
    "text": "and so the idea was that we would use registry facade and if a request comes",
    "start": "733500",
    "end": "738660"
  },
  {
    "text": "in we'll get the Shah of The Blob out of redis to translate that into the content",
    "start": "738660",
    "end": "744839"
  },
  {
    "text": "ID of of ipfs and if that blob was not present",
    "start": "744839",
    "end": "752399"
  },
  {
    "text": "then we would have registry facade take that blob put it into ipfs stream it as",
    "start": "752399",
    "end": "758399"
  },
  {
    "text": "it was being downloaded by the consumer and then set that Shah hash to the CID to the resulting content ID and this is",
    "start": "758399",
    "end": "765480"
  },
  {
    "text": "the opportunistic caching part right we will basically start caching a blob the moment it is used",
    "start": "765480",
    "end": "771540"
  },
  {
    "text": "if we haven't cached it before with that translated CID then we could",
    "start": "771540",
    "end": "777480"
  },
  {
    "text": "make the corresponding entry in the oci descriptor and have the Stargate snapshotter pull that from within ipfs",
    "start": "777480",
    "end": "787819"
  },
  {
    "text": "at the time there also was no content distribution across nodes and there was some limitations",
    "start": "787920",
    "end": "794519"
  },
  {
    "text": "um that meant that we could not use ipfs cluster and also pulling non-stargaze images",
    "start": "794519",
    "end": "800220"
  },
  {
    "text": "failed so we would we would have needed to stargazify all images in order to use",
    "start": "800220",
    "end": "808320"
  },
  {
    "text": "this all right and that is something that we could have done and it's quite interesting to look at the lazy pulling",
    "start": "808320",
    "end": "814920"
  },
  {
    "text": "aspect of that but it is not the path that we wanted to go down also it would have required cons",
    "start": "814920",
    "end": "822300"
  },
  {
    "text": "a considerable amount of excess compute just to to do that given the amount of images that we need to handle",
    "start": "822300",
    "end": "830060"
  },
  {
    "text": "The Next Step then was to bring that one step closer and",
    "start": "831899",
    "end": "838200"
  },
  {
    "text": "basically replace the Stargate snapshot with registry facade itself",
    "start": "838200",
    "end": "844620"
  },
  {
    "text": "and so the way that would look like is if that get request comes in for the Manifest we would check if the",
    "start": "844620",
    "end": "852300"
  },
  {
    "text": "individual blobs already exist much like before if they didn't we would then upload them",
    "start": "852300",
    "end": "858959"
  },
  {
    "text": "into the ipfs cluster the key difference now is that we also added ipfs cluster",
    "start": "858959",
    "end": "866700"
  },
  {
    "text": "and so what that does is we have a bootstrap node that exists as a",
    "start": "866700",
    "end": "871980"
  },
  {
    "text": "deployment within the cluster and then on each individual workspace node that",
    "start": "871980",
    "end": "877260"
  },
  {
    "text": "also the workspace is run on we have registry for start running and we have ipfs node running",
    "start": "877260",
    "end": "882300"
  },
  {
    "text": "all right so that all of this is already node local ideally and ipfs distributes the individual blob content to those",
    "start": "882300",
    "end": "888540"
  },
  {
    "text": "workspace nodes where that content will be needed so that we don't have to pull across nodes a lot",
    "start": "888540",
    "end": "894540"
  },
  {
    "text": "that was the idea and this actually really helped improve",
    "start": "894540",
    "end": "900060"
  },
  {
    "text": "image pull times however the intersone traffic that that caused",
    "start": "900060",
    "end": "905760"
  },
  {
    "text": "was very expensive so we would span anywhere from",
    "start": "905760",
    "end": "910980"
  },
  {
    "text": "like up to 50 to 100 nodes on a given day from close to five up to 100 and",
    "start": "910980",
    "end": "917100"
  },
  {
    "text": "then back down again so there would be a lot of content redistribution happening within this",
    "start": "917100",
    "end": "922440"
  },
  {
    "text": "ipfs cluster and there's no awareness at this point of availability zones so we would be",
    "start": "922440",
    "end": "928620"
  },
  {
    "text": "paying inter-region traffic for this sort of distribution of of content that ipfs was doing",
    "start": "928620",
    "end": "935820"
  },
  {
    "text": "on top of that the ipfs nodes lift on the workspace",
    "start": "935820",
    "end": "940920"
  },
  {
    "text": "nodes right where the actual workloads happen and so they scale up and down considerably throughout the day and as",
    "start": "940920",
    "end": "947459"
  },
  {
    "text": "they scale down we're going to lose content that we previously had cached and so our cache Miss rates would go up",
    "start": "947459",
    "end": "953820"
  },
  {
    "text": "as our cluster would scale down and the next day we'd essentially start from zero because we would have lost most of",
    "start": "953820",
    "end": "960060"
  },
  {
    "text": "what was cached the day before because those nodes had scaled down",
    "start": "960060",
    "end": "965600"
  },
  {
    "text": "so the fourth iteration and this is what we operate today makes this whole thing region aware so",
    "start": "966839",
    "end": "972839"
  },
  {
    "text": "instead of running the ipfs node and cluster proxy on the same nodes as the",
    "start": "972839",
    "end": "979079"
  },
  {
    "text": "work the workloads we run them in a separate setup and at this point we we operate in in",
    "start": "979079",
    "end": "986940"
  },
  {
    "text": "three availability zones so we quite literally have three ipfs nodes running",
    "start": "986940",
    "end": "992940"
  },
  {
    "text": "which farscale currently works um they come together again using a",
    "start": "992940",
    "end": "998759"
  },
  {
    "text": "bootstrap node and we use topology aware hens on the ipfs",
    "start": "998759",
    "end": "1005180"
  },
  {
    "text": "cluster service to Route the request that a registry facade would make to the node that is nearest to it",
    "start": "1005180",
    "end": "1012199"
  },
  {
    "text": "and so we're still paying this into Zone traffic for the redistribution of",
    "start": "1012199",
    "end": "1018380"
  },
  {
    "text": "content within ipfs but it is much fewer nodes and those nodes nowhere nearly",
    "start": "1018380",
    "end": "1023839"
  },
  {
    "text": "scale up and down as much as they used to and so a lot of that traffic cost problem is gone",
    "start": "1023839",
    "end": "1029058"
  },
  {
    "text": "and then because we use topology aware hints to talk from the individual",
    "start": "1029059",
    "end": "1035000"
  },
  {
    "text": "registry facade instances to the ipfs cluster nodes also that traffic cost is",
    "start": "1035000",
    "end": "1041058"
  },
  {
    "text": "considerably reduced",
    "start": "1041059",
    "end": "1044380"
  },
  {
    "text": "so how does that look like in production what what does it actually do and when we look at the the download",
    "start": "1046339",
    "end": "1053179"
  },
  {
    "text": "speeds you'll see the graph on top the green line is the download that's happening",
    "start": "1053179",
    "end": "1059059"
  },
  {
    "text": "from from ipfs this is is cumulative so this is not",
    "start": "1059059",
    "end": "1064220"
  },
  {
    "text": "just one node and you'll notice the the yellow line in the top graph if you can I took that out",
    "start": "1064220",
    "end": "1072500"
  },
  {
    "text": "and put that in the lower graph so this is the the exact same graphs it's just one doesn't have the green line and what",
    "start": "1072500",
    "end": "1077539"
  },
  {
    "text": "you'll notice is that step change this order of magnitude change and download speeds",
    "start": "1077539",
    "end": "1083320"
  },
  {
    "text": "the other thing to look at is to download request rate I.E what are we actually pulling from you know is",
    "start": "1084380",
    "end": "1090080"
  },
  {
    "text": "registry facade proxying to an upstream registry or is it polling using its own",
    "start": "1090080",
    "end": "1095240"
  },
  {
    "text": "ipfs cache and what you can see here is for one",
    "start": "1095240",
    "end": "1100820"
  },
  {
    "text": "sort of the this peaking behavior and that corresponds to the to the scale and to the load that the system sees",
    "start": "1100820",
    "end": "1107240"
  },
  {
    "text": "throughout the day and then again the yellow line is proxying to an upstream registry and the",
    "start": "1107240",
    "end": "1114799"
  },
  {
    "text": "green line is pulling from ipfs and I zoomed in here what you can see is that",
    "start": "1114799",
    "end": "1120520"
  },
  {
    "text": "there are some cache misses essentially which is when we proxy right when we have those spikes on the yellow line but",
    "start": "1120520",
    "end": "1127580"
  },
  {
    "text": "the vast majority of it nowadays comes through ipfs so to the tune of 90",
    "start": "1127580",
    "end": "1132679"
  },
  {
    "text": "percent so just give you a rough idea on the",
    "start": "1132679",
    "end": "1138980"
  },
  {
    "text": "traffic um pulling from from these ipfs nodes is to the tune of 2.3 terabytes a day as",
    "start": "1138980",
    "end": "1145580"
  },
  {
    "text": "compared to proxying up to to Upstream Registries which is you know about 100 gigabytes a day so again an order of",
    "start": "1145580",
    "end": "1153679"
  },
  {
    "text": "magnitude change in um in where we would pull from",
    "start": "1153679",
    "end": "1159760"
  },
  {
    "text": "so in conclusion we gained considerable startup time improvements specifically through the reduction of bandwidth",
    "start": "1160340",
    "end": "1166820"
  },
  {
    "text": "requirements towards Upstream Registries and by bringing that data much closer to where it's needed in a very",
    "start": "1166820",
    "end": "1173780"
  },
  {
    "text": "opportunistic fashion so we do not actively pre-warm caches and we just rely on what users pull and we will add",
    "start": "1173780",
    "end": "1180440"
  },
  {
    "text": "that to the cache assuming that it will be needed it will be needed again there is a time-based eviction mechanism",
    "start": "1180440",
    "end": "1186380"
  },
  {
    "text": "that I have not spoken about um essentially if a blob is not pulled for a given amount of time it is just",
    "start": "1186380",
    "end": "1192740"
  },
  {
    "text": "going to be thrown out of the cache again because otherwise we'd be spending too much money on disk space essentially",
    "start": "1192740",
    "end": "1201220"
  },
  {
    "text": "a lot of this work is heavily inspired by what the community does so we stand on the shoulders of giants and I want to",
    "start": "1201320",
    "end": "1207679"
  },
  {
    "text": "recognize that I'm very grateful for all the work that the community is putting into this topic",
    "start": "1207679",
    "end": "1213799"
  },
  {
    "text": "and then the idea that presented in here although reasonably specific to the architectural choices we made Within",
    "start": "1213799",
    "end": "1219559"
  },
  {
    "text": "getpod are transferable so the idea of adding a pull through proxy and you could quite",
    "start": "1219559",
    "end": "1225860"
  },
  {
    "text": "literally make that an HTTP proxy that that you add to your container runtime config or you could use something",
    "start": "1225860",
    "end": "1231740"
  },
  {
    "text": "similar to how a nerd controls registry mechanism works and add caching",
    "start": "1231740",
    "end": "1238280"
  },
  {
    "text": "mechanisms tuned to your own setup in a very similar fashion and with that thank you all for your",
    "start": "1238280",
    "end": "1244880"
  },
  {
    "text": "attention and happy to take questions [Applause]",
    "start": "1244880",
    "end": "1254240"
  },
  {
    "text": "for the questions I will put my co-speaker on the screen and unmute",
    "start": "1254240",
    "end": "1259760"
  },
  {
    "text": "let's hope that that audio thing is working you all say hi to Alejandro who unfortunately cannot see you but um",
    "start": "1259760",
    "end": "1268179"
  },
  {
    "text": "so as someone who uses a cluster that isn't at this scale is there still benefit to looking into this ipfs style",
    "start": "1286340",
    "end": "1293360"
  },
  {
    "text": "or pulling or is it just something you'd use when you're dealing with multi-terabyte",
    "start": "1293360",
    "end": "1298900"
  },
  {
    "text": "so I think the the main benefit you'll see the main benefit not So Much from the amount of",
    "start": "1300140",
    "end": "1305600"
  },
  {
    "text": "data really but rather the variance of data I want to say so if you keep pulling the same image over and over",
    "start": "1305600",
    "end": "1311720"
  },
  {
    "text": "again the layer reuse that happens on the Node will likely be sufficient if it",
    "start": "1311720",
    "end": "1317480"
  },
  {
    "text": "is a large variety of images that you need to pull over which you have no control then something like this will be",
    "start": "1317480",
    "end": "1324080"
  },
  {
    "text": "useful the moment you can reasonably predict what it is you're going to pull",
    "start": "1324080",
    "end": "1329500"
  },
  {
    "text": "chances are you'll find more efficient ways of pre-warming caches",
    "start": "1329500",
    "end": "1336158"
  },
  {
    "text": "hi uh great talk by the way thank you so one of the aspects you mentioned that uh",
    "start": "1337820",
    "end": "1344120"
  },
  {
    "text": "this helped with was reducing Network bandwidths in a way or reducing load on the actual registry because you're",
    "start": "1344120",
    "end": "1349220"
  },
  {
    "text": "mostly pulling from ipfs did it have a latency impact for cases",
    "start": "1349220",
    "end": "1355159"
  },
  {
    "text": "where you missed the cash for example because then you essentially have two hops",
    "start": "1355159",
    "end": "1360740"
  },
  {
    "text": "I'm not 100 sure I fully understood the question let me try and answer it and then you tell me if I did sure",
    "start": "1360740",
    "end": "1366980"
  },
  {
    "text": "um what I understood is what happens in case of cash misses and what's the cost of that yeah pretty much okay",
    "start": "1366980",
    "end": "1374059"
  },
  {
    "text": "the cost of a cash Miss is essentially pulling from the Upstream registry it is",
    "start": "1374059",
    "end": "1379280"
  },
  {
    "text": "very cheap because we will detect the cache Miss like we assume that what's in redis and what's in ipfs is on parity",
    "start": "1379280",
    "end": "1387020"
  },
  {
    "text": "and so the check if something is in cash is essentially a redis get which is a",
    "start": "1387020",
    "end": "1393200"
  },
  {
    "text": "very very fast operation and if it's not there then all we'll do is we'll just pull from the Upstream",
    "start": "1393200",
    "end": "1398600"
  },
  {
    "text": "registry and add it to the cache in the process",
    "start": "1398600",
    "end": "1403240"
  },
  {
    "text": "um so I saw the you kind of have written your own",
    "start": "1404900",
    "end": "1410900"
  },
  {
    "text": "um shim in order to either go to your cache or go to the source",
    "start": "1410900",
    "end": "1417679"
  },
  {
    "text": "um is there any any consideration of like layering that down to like into container D or into cro so everybody can",
    "start": "1417679",
    "end": "1426919"
  },
  {
    "text": "kind of use this without needing a software shim yeah it's a great question so we",
    "start": "1426919",
    "end": "1433640"
  },
  {
    "text": "ourselves had not considered doing that predominantly because of the deployment modes that we see at goodpod where sort",
    "start": "1433640",
    "end": "1440179"
  },
  {
    "text": "of Upstream adoption of something like containerd is very slow um so we have not actively engaged in",
    "start": "1440179",
    "end": "1446659"
  },
  {
    "text": "this however if there are um containerdy maintainers out there who would be interested in collaborating something",
    "start": "1446659",
    "end": "1452120"
  },
  {
    "text": "like that that'd be awesome yeah hi",
    "start": "1452120",
    "end": "1457880"
  },
  {
    "text": "um maybe I'm missing a basic concept what is the unit of downloaders are you",
    "start": "1457880",
    "end": "1463640"
  },
  {
    "text": "um expanding the layer into ipfs or are you pulling down the layer from ipfs",
    "start": "1463640",
    "end": "1471380"
  },
  {
    "text": "both really so if it's not present in ipfs we will take that tar file and just",
    "start": "1471380",
    "end": "1478039"
  },
  {
    "text": "push it into ipfs so the next time we need it we can pull it from there the layer of the files I'm sorry the",
    "start": "1478039",
    "end": "1485299"
  },
  {
    "text": "layer so we'll take the tar file as is we don't extract that in anything so so the part when the Pod wants to read a",
    "start": "1485299",
    "end": "1491780"
  },
  {
    "text": "file that's in a layer they're getting there's you're not getting you're not providing a file you're providing the",
    "start": "1491780",
    "end": "1498320"
  },
  {
    "text": "overlay and then that gets dealt with that's right so there is no lazy loading um that we that we add here got it if",
    "start": "1498320",
    "end": "1504980"
  },
  {
    "text": "you wanted something like that and your images are all stargazed so to speak uh",
    "start": "1504980",
    "end": "1510500"
  },
  {
    "text": "or also with SQL oci you could uh you could do like those two things are orthogonal right you could combine the",
    "start": "1510500",
    "end": "1516260"
  },
  {
    "text": "two okay um we just opted on two cool thank you",
    "start": "1516260",
    "end": "1522220"
  },
  {
    "text": "first first of all thanks for great talk to ask if any of the",
    "start": "1526100",
    "end": "1534020"
  },
  {
    "text": "files that you're using are they are you guys open source is",
    "start": "1534020",
    "end": "1539240"
  },
  {
    "text": "is that available or just the top so let me rephrase the question just to",
    "start": "1539240",
    "end": "1546020"
  },
  {
    "text": "make sure I understood correctly the question is essentially is what we just showed open source right so it is the all the code of what",
    "start": "1546020",
    "end": "1554900"
  },
  {
    "text": "I just presented can be found in in our repo um github.com IO so pod",
    "start": "1554900",
    "end": "1563059"
  },
  {
    "text": "and they're in the registry facade component you'll you'll find the code that I spoke of",
    "start": "1563059",
    "end": "1569740"
  },
  {
    "text": "hivl license how does this compare with running a",
    "start": "1570500",
    "end": "1575659"
  },
  {
    "text": "registry like in your organization and having like Upstream replication rules",
    "start": "1575659",
    "end": "1580760"
  },
  {
    "text": "and maybe potentially depending on S3 like that's how we do it we have Harbor like yeah I'm kind of curious",
    "start": "1580760",
    "end": "1588020"
  },
  {
    "text": "yeah that's a great question so we also pull from like the Upstream",
    "start": "1588020",
    "end": "1593120"
  },
  {
    "text": "registry that we would pull in a cache Miss case is also one that we control it's essentially Google artifact",
    "start": "1593120",
    "end": "1599900"
  },
  {
    "text": "registry and we have still found that by caching",
    "start": "1599900",
    "end": "1605840"
  },
  {
    "text": "this very close to the node we do see considerable speed improvements so",
    "start": "1605840",
    "end": "1612440"
  },
  {
    "text": "I would vager and this is you know speculation at this point that the most",
    "start": "1612440",
    "end": "1618320"
  },
  {
    "text": "the most benefit we see here is from the locality rather than the replication itself",
    "start": "1618320",
    "end": "1624320"
  },
  {
    "text": "right so if you could build your registry in such a way that it would bring the data closer to the",
    "start": "1624320",
    "end": "1630260"
  },
  {
    "text": "nodes I.E increased the bandwidth of of those poles then you would probably see",
    "start": "1630260",
    "end": "1635900"
  },
  {
    "text": "comparable results um so I yeah I think that the key mechanism here is exactly increasing our bandwidth",
    "start": "1635900",
    "end": "1642620"
  },
  {
    "text": "so whatever means you can do that we'll probably do similar things",
    "start": "1642620",
    "end": "1647679"
  },
  {
    "text": "yeah so the idea of caching helps when the same images are pulled multiple times on a given note for newly",
    "start": "1648260",
    "end": "1655039"
  },
  {
    "text": "provisioned notes have you look into using network SI storage to you know",
    "start": "1655039",
    "end": "1660679"
  },
  {
    "text": "host all the images on the volume and I know it doesn't work with the NFS like overlayfs doesn't work with NFS but have",
    "start": "1660679",
    "end": "1666740"
  },
  {
    "text": "you export options like that uh we have not explicitly",
    "start": "1666740",
    "end": "1672980"
  },
  {
    "text": "um the main reason is because we did not want to dive too much into how the how",
    "start": "1672980",
    "end": "1678679"
  },
  {
    "text": "the nodes themselves operate um again we're coming back to get part deployment models and uh sort of the",
    "start": "1678679",
    "end": "1685520"
  },
  {
    "text": "degree of control that we have over the infrastructure that we run on so we did not",
    "start": "1685520",
    "end": "1691580"
  },
  {
    "text": "um want to sort of assume for example that we can meddle with content of these content store or something to that tune",
    "start": "1691580",
    "end": "1698679"
  },
  {
    "text": "I had a question about authentication since you're sort of sitting in front of the registry and potentially serving",
    "start": "1699559",
    "end": "1705559"
  },
  {
    "text": "images how are you doing any off when the the sort of the authentication will be at the registry which is kind of Beyond you",
    "start": "1705559",
    "end": "1713559"
  },
  {
    "text": "so the um there is a policy on what registry",
    "start": "1713900",
    "end": "1719900"
  },
  {
    "text": "facade can pull what it's allowed to pull in the end the authentication like",
    "start": "1719900",
    "end": "1725720"
  },
  {
    "text": "registry facade itself is just authenticated against a single external registry and that's where it will always",
    "start": "1725720",
    "end": "1732260"
  },
  {
    "text": "pull from so it's not sort of authentication transparent if that makes sense",
    "start": "1732260",
    "end": "1737299"
  },
  {
    "text": "um which is really an artifact of how registry facade is used within the context of gitpod",
    "start": "1737299",
    "end": "1742700"
  },
  {
    "text": "I'm certain that this context could be expanded further so I do remember when Docker introduced",
    "start": "1742700",
    "end": "1749659"
  },
  {
    "text": "um cost on on manifest pools you know we saw a bunch of pull through proxies",
    "start": "1749659",
    "end": "1754820"
  },
  {
    "text": "popping up um notably from Alex Ellis for example who wrote one and he basically built a",
    "start": "1754820",
    "end": "1761299"
  },
  {
    "text": "transparent authentication pass-through which you would need we just opted not to to build this out of the",
    "start": "1761299",
    "end": "1768039"
  },
  {
    "text": "architectural setup but conceptually that would well be possible",
    "start": "1768039",
    "end": "1774460"
  },
  {
    "text": "awesome one more question there",
    "start": "1784220",
    "end": "1788500"
  },
  {
    "text": "yeah pretty cool thanks for introducing this um I do you benefit from the ipfs",
    "start": "1793460",
    "end": "1800020"
  },
  {
    "text": "peer-to-peer uh distribution like if one of your nodes contacts the ipfs in",
    "start": "1800020",
    "end": "1808520"
  },
  {
    "text": "its AZ and it's not there uh does it pull from the other nodes if",
    "start": "1808520",
    "end": "1815360"
  },
  {
    "text": "it's in another AC or does it go to the Upstream like is that automatic or did you have to write a lot of stuff for it",
    "start": "1815360",
    "end": "1822500"
  },
  {
    "text": "that is a great question which I actually cannot answer I would like to defer that to Alejandro I'll repeat it for Alejandro the question is what if",
    "start": "1822500",
    "end": "1829460"
  },
  {
    "text": "there is a cache Miss within an availability Zone will that node talk to like that ipfs node talk to another ipfs",
    "start": "1829460",
    "end": "1836779"
  },
  {
    "text": "node or is it just a cache Miss in the end at the end it's just uh",
    "start": "1836779",
    "end": "1843980"
  },
  {
    "text": "we are we don't Implement any reference in",
    "start": "1843980",
    "end": "1848919"
  },
  {
    "text": "the not sure that came through so I'll try and repeat um it's just a cache Miss",
    "start": "1850880",
    "end": "1857260"
  },
  {
    "text": "on the same lines uh you have three different bootstrap nodes right for each availability Zone",
    "start": "1871520",
    "end": "1878299"
  },
  {
    "text": "can you talk more about the reliability of the ipfs system itself like what if",
    "start": "1878299",
    "end": "1884600"
  },
  {
    "text": "those nodes goes down Who who how is that being monitored maintained and all",
    "start": "1884600",
    "end": "1889700"
  },
  {
    "text": "those things so I did not understand that from an audio perspective",
    "start": "1889700",
    "end": "1895220"
  },
  {
    "text": "the bootstrap nodes that you have right so there is one for each availability Zone where the nodes actually contact",
    "start": "1895220",
    "end": "1901039"
  },
  {
    "text": "the ipfs nodes what is the reliability of that ipfs node what if it goes down who maintains",
    "start": "1901039",
    "end": "1908899"
  },
  {
    "text": "that and how would you take care of that yeah so to rephrase the question to make sure",
    "start": "1908899",
    "end": "1916039"
  },
  {
    "text": "I understood correctly is what if an availability Zone goes down what's the impact of of something like that and its",
    "start": "1916039",
    "end": "1921559"
  },
  {
    "text": "likelihood so the impact of it would be a loss of",
    "start": "1921559",
    "end": "1926779"
  },
  {
    "text": "caching in that particular region um which would be a degradation of",
    "start": "1926779",
    "end": "1932539"
  },
  {
    "text": "service and we would treat it as such at this point we have not seen that so",
    "start": "1932539",
    "end": "1940640"
  },
  {
    "text": "the notes are they cycle right they cycle regularly and so we'll just have",
    "start": "1940640",
    "end": "1946159"
  },
  {
    "text": "it reinitialized with the other three nodes so if it goes down for some time there just isn't caching in that region",
    "start": "1946159",
    "end": "1952880"
  },
  {
    "text": "that's available and then when a new one comes up it will use that ipfs bootstrap node to to make itself part of the ipfs",
    "start": "1952880",
    "end": "1960500"
  },
  {
    "text": "cluster and get the data replicated from the other nodes",
    "start": "1960500",
    "end": "1965380"
  },
  {
    "text": "hi just a quick question about the dynamic dynamically assembling the images that you're you're discussing a",
    "start": "1974899",
    "end": "1981020"
  },
  {
    "text": "bit a bit earlier in the talk um that was blowing my mind a little bit not dynamically creating manifests out",
    "start": "1981020",
    "end": "1986960"
  },
  {
    "text": "of different layers of images um so my question is if you're dynamically assembling manifests each",
    "start": "1986960",
    "end": "1993140"
  },
  {
    "text": "time because some of your your your git spot your get pod specific or your IDE layers are changing does that mean in",
    "start": "1993140",
    "end": "2000340"
  },
  {
    "text": "the end um you know somebody with manifests deploying to the cluster uh that they",
    "start": "2000340",
    "end": "2006700"
  },
  {
    "text": "are they able to use pull by digest or is pull by digest kind of out of the window because you don't know what",
    "start": "2006700",
    "end": "2012159"
  },
  {
    "text": "manifest you're really getting how does that work that's right um identifying that by digest goes out",
    "start": "2012159",
    "end": "2017919"
  },
  {
    "text": "of the window because we don't know um what the content of that digest will be so you basically have to pull by",
    "start": "2017919",
    "end": "2024580"
  },
  {
    "text": "um by tag and in fact we do use the the tag itself to decide on the content of",
    "start": "2024580",
    "end": "2031120"
  },
  {
    "text": "that particular manifest",
    "start": "2031120",
    "end": "2034440"
  },
  {
    "text": "all right we're also smack on time again thank you all for showing on this Friday uh Friday morning and thank you",
    "start": "2041559",
    "end": "2048339"
  },
  {
    "text": "for your attention",
    "start": "2048339",
    "end": "2050760"
  }
]