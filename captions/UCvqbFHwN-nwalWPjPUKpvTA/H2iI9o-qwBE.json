[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "i'm william dennis and thank you for coming to my session on building a nodeless kubernetes platform",
    "start": "80",
    "end": "6240"
  },
  {
    "text": "first a little background about me i'm a product manager in google cloud where i work on kubernetes engine",
    "start": "6240",
    "end": "12480"
  },
  {
    "text": "in 2019 i co-founded gke autopilot and that's what i currently work on this is a new mode of operation for gke",
    "start": "12480",
    "end": "19920"
  },
  {
    "text": "which i'll be talking a bit about today i'm a very big supporter of the kubernetes open source project and in",
    "start": "19920",
    "end": "25680"
  },
  {
    "text": "2017 i also co-founded the certified kubernetes conformance program which is still used today to ensure portability",
    "start": "25680",
    "end": "31760"
  },
  {
    "text": "between vendors distributions of kubernetes i'm also writing a book published by manning called kubernetes",
    "start": "31760",
    "end": "37840"
  },
  {
    "text": "for developers a little bit about this session i hope it's going to be useful for all of you",
    "start": "37840",
    "end": "44000"
  },
  {
    "start": "40000",
    "end": "73000"
  },
  {
    "text": "um as you build or build on kubernetes platforms of your own in this session i plan to give you a",
    "start": "44000",
    "end": "49600"
  },
  {
    "text": "behind-the-scenes look of the creation of gke autopilot which is a fully managed",
    "start": "49600",
    "end": "55600"
  },
  {
    "text": "platform for kubernetes by my team i'll be giving throughout the presentation some arguments for and against uh the idea of",
    "start": "55600",
    "end": "63280"
  },
  {
    "text": "having nodes in a so-called nodeless platform and at the end i'll talk a little bit",
    "start": "63280",
    "end": "68640"
  },
  {
    "text": "about some of the future possibilities that can be enabled by this design firstly let me do a quick audience poll",
    "start": "68640",
    "end": "76799"
  },
  {
    "start": "73000",
    "end": "184000"
  },
  {
    "text": "so i'd like to know raise your hand who believes that it's possible to have a noble kubernetes",
    "start": "76799",
    "end": "81840"
  },
  {
    "text": "where there is still technically a node object show hands got a couple",
    "start": "81840",
    "end": "87280"
  },
  {
    "text": "not too many who doesn't everyone else yeah and third option who believes i was just",
    "start": "87280",
    "end": "93759"
  },
  {
    "text": "adding a bit of controversy to my kubecon talk to you know spruce up the marketing a bit all right",
    "start": "93759",
    "end": "99680"
  },
  {
    "text": "um well the truth probably lies somewhere in the middle of of of those three um but let's let's dig in",
    "start": "99680",
    "end": "107119"
  },
  {
    "text": "so what does you know as we look at building a fully managed kubernetes",
    "start": "107119",
    "end": "113200"
  },
  {
    "text": "platform i think it's worth taking a step back and looking at a traditional kubernetes platform and i'll be using gke as the example here so a traditional",
    "start": "113200",
    "end": "120560"
  },
  {
    "text": "kubernetes platform basically consists of two apis that you need to interact with as the developer the first one",
    "start": "120560",
    "end": "127119"
  },
  {
    "text": "above is the kubernetes api that's what you're here for right that's what you're a kubecon for that's what you use",
    "start": "127119",
    "end": "132879"
  },
  {
    "text": "kubernetes for you want to describe your you know stateless app in a deployment",
    "start": "132879",
    "end": "138000"
  },
  {
    "text": "you want to maybe represent a reddest database or a marie db database",
    "start": "138000",
    "end": "143840"
  },
  {
    "text": "as a stateful set you want to create jobs for your workloads etc that's that's what you're here for and then you",
    "start": "143840",
    "end": "149200"
  },
  {
    "text": "have this other api under the line which is the platform api whether it's gke or one of the others",
    "start": "149200",
    "end": "155760"
  },
  {
    "text": "and with that api you have to configure the cluster in order to serve those",
    "start": "155760",
    "end": "161519"
  },
  {
    "text": "kubernetes objects you might have to create nodes of a certain size or certain capabilities",
    "start": "161519",
    "end": "167920"
  },
  {
    "text": "i believe that a an ideal fully managed kubernetes platform would just be the kubernetes",
    "start": "167920",
    "end": "173599"
  },
  {
    "text": "api where you interact everything at that level using you know cube ctl using",
    "start": "173599",
    "end": "178959"
  },
  {
    "text": "yaml files basically um so",
    "start": "178959",
    "end": "184879"
  },
  {
    "start": "184000",
    "end": "240000"
  },
  {
    "text": "what does it what does it mean to build a fully managed kubernetes platform well when i was looking at this problem in 2018 i wrote a position paper and i had",
    "start": "184879",
    "end": "192000"
  },
  {
    "text": "three headings let me just quickly share them here the first one was that a nodeless or a fully managed kubernetes",
    "start": "192000",
    "end": "197280"
  },
  {
    "text": "platform should still be kubernetes secondly the containers should be out of utilize um",
    "start": "197280",
    "end": "203360"
  },
  {
    "text": "unused reserved capacity in other words allow for bursting people come to kubernetes one of the",
    "start": "203360",
    "end": "209440"
  },
  {
    "text": "reasons they come to kubernetes is to be able to pull their resources and and potentially burst uh when when needed",
    "start": "209440",
    "end": "215519"
  },
  {
    "text": "and the third thing is i wanted to make sure that we priced this in such a way that it supported continued usage uh i didn't like the idea of creating",
    "start": "215519",
    "end": "222000"
  },
  {
    "text": "like a kind of a toy version or something that you wouldn't want to run 100 for 100 percent of your workloads",
    "start": "222000",
    "end": "227040"
  },
  {
    "text": "100 of the time um and on node visibility i had a suggestion just to make them visible um",
    "start": "227040",
    "end": "233360"
  },
  {
    "text": "while still hiding certain bits like maybe the vm i'll be digging into that in a bit",
    "start": "233360",
    "end": "240519"
  },
  {
    "start": "240000",
    "end": "332000"
  },
  {
    "text": "so back to that first point though the first point being that nodeless kubernetes or fully",
    "start": "240640",
    "end": "247439"
  },
  {
    "text": "managed kubernetes should still be kubernetes why is that important so when we approach this problem of",
    "start": "247439",
    "end": "253680"
  },
  {
    "text": "let's build a simpler to operate as simpler to use kubernetes a lot of people came to me",
    "start": "253680",
    "end": "259040"
  },
  {
    "text": "and said well william kubernetes is hard there's there's a lot you have to learn to get started there's a lot you need to do just to deploy an",
    "start": "259040",
    "end": "265280"
  },
  {
    "text": "app maybe at the same time we should simplify that and create a cut down or a",
    "start": "265280",
    "end": "270320"
  },
  {
    "text": "simplified experience so you can deploy things more easily i think that would would be a mistake",
    "start": "270320",
    "end": "277199"
  },
  {
    "text": "and the reason that would be a mistake is that it misses the point of why people were choosing kubernetes and why",
    "start": "277199",
    "end": "282560"
  },
  {
    "text": "kubernetes is so successful and i believe the reason is that it's it's a",
    "start": "282560",
    "end": "288080"
  },
  {
    "text": "orchestration layer designed for professionals these are professionals like they might be running a massive you know website",
    "start": "288080",
    "end": "295759"
  },
  {
    "text": "for a fortune 500 company uh for example and they're a professional right they they",
    "start": "295759",
    "end": "302320"
  },
  {
    "text": "need they need power they need flexibility you know one day someone might approach them and say hey i need to run a reddit database in this or i",
    "start": "302320",
    "end": "308720"
  },
  {
    "text": "have some like legacy state for workload that you have to you have to run kubernetes can handle that so",
    "start": "308720",
    "end": "315280"
  },
  {
    "text": "um i believe that the the power behind kubernetes is is is that flexibility and",
    "start": "315280",
    "end": "320639"
  },
  {
    "text": "the scalability so simplicity at the kubernetes layer was it was a non-goal only simplicity of",
    "start": "320639",
    "end": "327840"
  },
  {
    "text": "operating this cluster uh is the goal",
    "start": "327840",
    "end": "332320"
  },
  {
    "start": "332000",
    "end": "500000"
  },
  {
    "text": "now when the team looked at this we had a couple of different operational models to consider",
    "start": "333600",
    "end": "340000"
  },
  {
    "text": "where uh when thinking about like how to actually run pods how to run uh pods on this",
    "start": "340000",
    "end": "346320"
  },
  {
    "text": "fully managed product um and maybe i'll take a pause there and just say that what this product does",
    "start": "346320",
    "end": "352479"
  },
  {
    "text": "basically what our what our aim was is that you just create a kubernetes workload and you don't have to configure",
    "start": "352479",
    "end": "358639"
  },
  {
    "text": "nodes you don't have to manage anything else will provision all the infrastructure for you so with that in",
    "start": "358639",
    "end": "364000"
  },
  {
    "text": "mind the team looked at a number of different models for actually handling that compute",
    "start": "364000",
    "end": "370479"
  },
  {
    "text": "the first one was actually to use borg so i don't know how many people are familiar with borg it's it's a container",
    "start": "370479",
    "end": "376319"
  },
  {
    "text": "orchestrator developed by google and used internally and was the subject of an academic paper which you can find",
    "start": "376319",
    "end": "381440"
  },
  {
    "text": "online and it actually formed the inspiration for kubernetes itself many years ago",
    "start": "381440",
    "end": "387199"
  },
  {
    "text": "so one of the options for running your pod containers for us would have been just to run them",
    "start": "387199",
    "end": "392639"
  },
  {
    "text": "as so-called tasks or borg one of the advantages of that system is",
    "start": "392639",
    "end": "398080"
  },
  {
    "text": "that it's it's massively multi-tenant and we would have like very rapid scaling",
    "start": "398080",
    "end": "403280"
  },
  {
    "text": "and many other benefits the second option was to reuse more of",
    "start": "403280",
    "end": "408560"
  },
  {
    "text": "gke and run each pod in its own vm the benefit for that is that by being a",
    "start": "408560",
    "end": "414880"
  },
  {
    "text": "lot closer to gke it would be more compatible with with various different infrastructure",
    "start": "414880",
    "end": "420160"
  },
  {
    "text": "the downside of that approach though would be that you would limit the pod sizes that we could offer in this platform to just a range of of uh vms",
    "start": "420160",
    "end": "428400"
  },
  {
    "text": "that we have you wouldn't be able to do something like a you know 5.25",
    "start": "428400",
    "end": "434400"
  },
  {
    "text": "cpu pod on this system because we don't have a vm of that of that size",
    "start": "434400",
    "end": "439840"
  },
  {
    "text": "the final option was just be like gke the existing product that we're looking at just be like gke standard",
    "start": "439840",
    "end": "446960"
  },
  {
    "text": "run run multiple pods per node and yeah basically don't change it too much",
    "start": "446960",
    "end": "453440"
  },
  {
    "text": "the benefit of that when we looked at it was that it would also then support other kubernetes constructs that actually rely on nodes such as daemon",
    "start": "453440",
    "end": "460319"
  },
  {
    "text": "sets pod affinity notification things like that",
    "start": "460319",
    "end": "466479"
  },
  {
    "text": "so in the end the team chose to make it just like gke um one of the other nice things about this",
    "start": "466479",
    "end": "472479"
  },
  {
    "text": "is that we can offer a we were able to offer like a wide range of pod sizes scaling from a quarter of a",
    "start": "472479",
    "end": "478479"
  },
  {
    "text": "core all the way to 28 cores in quad core increments so we'd be able to support something like a 17.75",
    "start": "478479",
    "end": "485919"
  },
  {
    "text": "you know vcpu pod without having to scale it up to one to a predetermined size",
    "start": "485919",
    "end": "491360"
  },
  {
    "text": "it also provides for maximum compatibility so that was kind of one of the first foundational decisions we made",
    "start": "491360",
    "end": "497520"
  },
  {
    "text": "on the on the path of building this product the next question was around visibility",
    "start": "497520",
    "end": "505039"
  },
  {
    "start": "500000",
    "end": "636000"
  },
  {
    "text": "if we're trying to create a fully managed kubernetes platform one that's so called nodeless like what does it",
    "start": "505039",
    "end": "510960"
  },
  {
    "text": "mean when you go cube ctl get nodes uh should it list all the nodes that we actually have there",
    "start": "510960",
    "end": "516399"
  },
  {
    "text": "should we you know just group that together in one and just return you know autopilot node and just sort of hide all that detail",
    "start": "516399",
    "end": "523760"
  },
  {
    "text": "in the end we went with just list the nodes be transparent show the user what's actually happening under",
    "start": "523760",
    "end": "529200"
  },
  {
    "text": "the hood even if they most of the time don't have to care about it second second decision we looked at was",
    "start": "529200",
    "end": "536320"
  },
  {
    "text": "what about the actual like inner workings of those nodes should we be transparent about what",
    "start": "536320",
    "end": "542399"
  },
  {
    "text": "what shape those nodes are should we tell you if it's like an intel or an amd or this this type of machine or that type",
    "start": "542399",
    "end": "548399"
  },
  {
    "text": "of machine should we disclose you know how much of the allocatable table is it has been used um",
    "start": "548399",
    "end": "555120"
  },
  {
    "text": "this was actually the subject of a lot of debate in the team uh some people were like no we should hide it right because you shouldn't have to care about",
    "start": "555120",
    "end": "561279"
  },
  {
    "text": "this so why should you know about it um but in the end we actually went with full transparency",
    "start": "561279",
    "end": "566880"
  },
  {
    "text": "the reason is that we want you to be a we we want you to trust us that we're going to do the right thing with your pods when you schedule them in in this",
    "start": "566880",
    "end": "573519"
  },
  {
    "text": "platform but we're going to let you verify what we did as well so if you want to poke under the covers and and",
    "start": "573519",
    "end": "578560"
  },
  {
    "text": "see exactly how things landed it's all there fully transparent i don't think we hired we literally don't hide a single",
    "start": "578560",
    "end": "583920"
  },
  {
    "text": "field compared to the gkd standard product",
    "start": "583920",
    "end": "589040"
  },
  {
    "text": "the other question was can users access the vm object separately outside of kubernetes um",
    "start": "589040",
    "end": "596959"
  },
  {
    "text": "so at this point it's probably worth mentioning actually i showed a diagram before that developers used two apis to",
    "start": "596959",
    "end": "602320"
  },
  {
    "text": "interact with this system it turns out there's actually a third one right at the bottom right which is uh the vm api",
    "start": "602320",
    "end": "608000"
  },
  {
    "text": "the reason i didn't show that before is typically people using kubernetes don't have to care about the vm api it's 100 managed",
    "start": "608000",
    "end": "615120"
  },
  {
    "text": "for you but it happens to be there and you can interact with it like for example you can ssh into a node",
    "start": "615120",
    "end": "620720"
  },
  {
    "text": "so going back one of the one of the problems we had there was um you know should we allow access to that",
    "start": "620720",
    "end": "626560"
  },
  {
    "text": "object uh decision was no in this case we will completely eliminate that api since",
    "start": "626560",
    "end": "632480"
  },
  {
    "text": "it's not necessary at all for the for the developer so those were some of the key design",
    "start": "632480",
    "end": "637839"
  },
  {
    "start": "636000",
    "end": "749000"
  },
  {
    "text": "decisions going into this now i'm going to cover a little bit about how we actually implemented this",
    "start": "637839",
    "end": "647000"
  },
  {
    "text": "okay so we built this product using um",
    "start": "648640",
    "end": "654320"
  },
  {
    "text": "using various components that already existed in gke the way gke works um",
    "start": "654320",
    "end": "660480"
  },
  {
    "text": "and and the platform that we were dealing with is that uh nodes of the same configuration are",
    "start": "660480",
    "end": "665760"
  },
  {
    "text": "grouped into a semantic grouping called a node pool uh so for example up on screen there i have",
    "start": "665760",
    "end": "671360"
  },
  {
    "text": "a node pool with eight uh vcpu cores and uh 16 gigabytes of memory and there's",
    "start": "671360",
    "end": "676800"
  },
  {
    "text": "another one with four and four um and so what happens is",
    "start": "676800",
    "end": "681920"
  },
  {
    "text": "if there's existing space in one of those nodes the pod will just get placed but what about when there isn't",
    "start": "681920",
    "end": "688680"
  },
  {
    "text": "okay so let's say we have uh two pending pods here one that can fit on one of these",
    "start": "689839",
    "end": "695040"
  },
  {
    "text": "existing uh no no pools and one that can't so for the ones that can fit",
    "start": "695040",
    "end": "702760"
  },
  {
    "text": "there's a component called the cluster autoscaler which will actually look at existing node pools and extend that node",
    "start": "703200",
    "end": "709760"
  },
  {
    "text": "pool to include to add an extra node that can that can handle that pod and therefore",
    "start": "709760",
    "end": "715120"
  },
  {
    "text": "that pod will be scheduled in the case of the pod that didn't fit in any of the existing node pull",
    "start": "715120",
    "end": "720160"
  },
  {
    "text": "definitions we have a we used a separate component that exists in gke called the node auto provisioner",
    "start": "720160",
    "end": "726240"
  },
  {
    "text": "and that component is capable of creating a new node pool definition in this case one larger with 16 cpu 32",
    "start": "726240",
    "end": "732480"
  },
  {
    "text": "gigabytes of memory in order for that pod to fit but the final step is actually that cluster autoscaler is then responsible",
    "start": "732480",
    "end": "739120"
  },
  {
    "text": "for actually creating a node in that node pool to run that pod so under the hood that is what is happening that is",
    "start": "739120",
    "end": "745680"
  },
  {
    "text": "how we built it and how this system works and and how it",
    "start": "745680",
    "end": "752800"
  },
  {
    "text": "actually actuates on the on the user input is because we've pretty much eliminated the",
    "start": "752800",
    "end": "759360"
  },
  {
    "text": "actual like node api and the no pull api so there's no way for users to specify those things we derive everything from",
    "start": "759360",
    "end": "765600"
  },
  {
    "text": "the pod spec so one of the simple examples is the uh the resources needed so things like the cpu and the memory we",
    "start": "765600",
    "end": "773920"
  },
  {
    "text": "derive that from the resource request of the pod another example would be node features",
    "start": "773920",
    "end": "780560"
  },
  {
    "text": "and this is an interesting one in the past if you wanted to have a bunch of nodes with different features",
    "start": "780560",
    "end": "786160"
  },
  {
    "text": "like for example you wanted to use spot compute you would typically go ahead and create a node pull that was a spot node pull",
    "start": "786160",
    "end": "793360"
  },
  {
    "text": "and then you would target that spot notable with your pods with autopilot we flipped the script",
    "start": "793360",
    "end": "799519"
  },
  {
    "text": "there and you actually specify the requirement just in the pod spec and",
    "start": "799519",
    "end": "805120"
  },
  {
    "text": "we will actuate on that and provision a node that can then handle that pod spec",
    "start": "805120",
    "end": "811440"
  },
  {
    "text": "which i think is actually really nice design because it means that all of the configuration of the hardware",
    "start": "811440",
    "end": "816480"
  },
  {
    "text": "essentially right of like hardware properties like this should be a spot node and in the future potentially other",
    "start": "816480",
    "end": "821600"
  },
  {
    "text": "things like you know this pod needs a gpu all this is done at a pod level at a workload level",
    "start": "821600",
    "end": "827440"
  },
  {
    "text": "right where all the rest of your configuration is you don't have to kind of do this multi-pass where you",
    "start": "827440",
    "end": "833120"
  },
  {
    "text": "design your pods and you write those specs and you have to go and figure out the nodes that can run them",
    "start": "833120",
    "end": "839680"
  },
  {
    "start": "839000",
    "end": "958000"
  },
  {
    "text": "there are a couple of other additional components that we used i won't go into too much detail here but uh we use a component called release channels to",
    "start": "839920",
    "end": "846160"
  },
  {
    "text": "keep them updated and no to auto repair to to replace unhealthy nodes",
    "start": "846160",
    "end": "851600"
  },
  {
    "text": "okay so that's how we provision the resources to manage the pods another aspect of the implementation of",
    "start": "851600",
    "end": "858399"
  },
  {
    "text": "this platform was an admission webhook so we had to achieve two things here one is",
    "start": "858399",
    "end": "865040"
  },
  {
    "text": "that i mentioned that we have a we built this system to have a very wide range of resources for the pods but",
    "start": "865040",
    "end": "871040"
  },
  {
    "text": "there were still some limits you need to the pod cpu needs to be between a quarter of a core and 28 cores",
    "start": "871040",
    "end": "878240"
  },
  {
    "text": "and there's a ratio of cpu to memory so what we do is we use a mutating web to look at the pod requests",
    "start": "878240",
    "end": "885600"
  },
  {
    "text": "to ensure everything is within the range and if it's outside of the acceptable value we will actually just mutate the",
    "start": "885600",
    "end": "890959"
  },
  {
    "text": "pod and fix it for you when we do that we emit a warning if you're using cubectl and we also write",
    "start": "890959",
    "end": "897120"
  },
  {
    "text": "an annotation into the pod spec that basically logs what we changed so that you can kind of audit that",
    "start": "897120",
    "end": "903680"
  },
  {
    "text": "the second part is we have a validating admission controller and the validating emission controller is designed to",
    "start": "903680",
    "end": "909199"
  },
  {
    "text": "enforce policies that prevent users from running admin level workloads on the nodes",
    "start": "909199",
    "end": "916000"
  },
  {
    "text": "now why is that needed the reason we need to restrict admin level workloads like basically",
    "start": "916000",
    "end": "921839"
  },
  {
    "text": "preventing root access is that we want to offer a fully managed platform where google sre team are",
    "start": "921839",
    "end": "928240"
  },
  {
    "text": "basically responsible for running these nodes what that means is we can't really offer you the users direct access to those",
    "start": "928240",
    "end": "934880"
  },
  {
    "text": "nodes at a root level because then people can potentially go in modify the kernel change bits and",
    "start": "934880",
    "end": "941440"
  },
  {
    "text": "pieces and and we essentially just lose the confidence to actually manage that thing for you because we don't know what's",
    "start": "941440",
    "end": "947519"
  },
  {
    "text": "happened to the node so it's important for us that all the nodes kind of look the same or at least you know have very well known properties so we have an",
    "start": "947519",
    "end": "954880"
  },
  {
    "text": "emission controller there to enforce those policies what did we pick as our list of enforced",
    "start": "954880",
    "end": "960880"
  },
  {
    "start": "958000",
    "end": "1087000"
  },
  {
    "text": "policies this is the list some of the simple ones are and i already kind of mentioned was limiting",
    "start": "960880",
    "end": "966399"
  },
  {
    "text": "privileged pods so this is a pod with the security context of privilege equals true",
    "start": "966399",
    "end": "971680"
  },
  {
    "text": "this basically gives you you know almost like root access on the node so we we reject that",
    "start": "971680",
    "end": "977920"
  },
  {
    "text": "we also reject some linux capabilities like sysadmin interestingly though many uh linux",
    "start": "977920",
    "end": "983519"
  },
  {
    "text": "capabilities are actually still offered in this product so uh p-trace was actually requested by one of our security partners they wanted to be able",
    "start": "983519",
    "end": "989839"
  },
  {
    "text": "to use p-trace to inspect uh running running processes we looked at that we felt like it was actually quite fun to",
    "start": "989839",
    "end": "995360"
  },
  {
    "text": "offer so we we added that into the list that you can actually use um",
    "start": "995360",
    "end": "1000480"
  },
  {
    "text": "other things we clamp down on uh stuff that like directly relates to the node so you know the goal here is to",
    "start": "1000480",
    "end": "1007040"
  },
  {
    "text": "build a nodeless product so we don't really want people using host support and you know running a container saying",
    "start": "1007040",
    "end": "1012480"
  },
  {
    "text": "port 80 on the host because then if you try and schedule another container also with port 80 we can't co-locate that on",
    "start": "1012480",
    "end": "1018240"
  },
  {
    "text": "the same node it kind of breaks our bin packing model and uh impacts the platform so we had to limit that",
    "start": "1018240",
    "end": "1024480"
  },
  {
    "text": "limited host networking uh which is also fairly fairly highly privileged uh host path where we've also restricted",
    "start": "1024480",
    "end": "1030720"
  },
  {
    "text": "although you can have uh you can mount var logs in in read-only mode which means that you can actually as a user",
    "start": "1030720",
    "end": "1036640"
  },
  {
    "text": "just have like a damon set like scraping logs that's totally fine as far as node affinity keys are",
    "start": "1036640",
    "end": "1042160"
  },
  {
    "text": "concerned we we restrict hostname because again we don't want users thinking about nodes we don't want people targeting specific",
    "start": "1042160",
    "end": "1048960"
  },
  {
    "text": "nodes but we allow many other node affinity keys like the zonal topology or regional",
    "start": "1048960",
    "end": "1054080"
  },
  {
    "text": "topology things like that um one thing we didn't restrict was the ability to run a container",
    "start": "1054080",
    "end": "1060320"
  },
  {
    "text": "as the root user you can yourself restrict that using the the open source pod security mission um",
    "start": "1060320",
    "end": "1066559"
  },
  {
    "text": "we didn't actually need to restrict that because the uh the security boundary of this product is still the vm",
    "start": "1066559",
    "end": "1072320"
  },
  {
    "text": "it's not actually a multi-tenant uh system at all the vms are still 100 percent your vms",
    "start": "1072320",
    "end": "1077760"
  },
  {
    "text": "your nodes so we didn't feel the need to actually restrict this and if we did restrict this from a usability perspective like",
    "start": "1077760",
    "end": "1083280"
  },
  {
    "text": "half of all docker images wouldn't run so that'd be a problem too",
    "start": "1083280",
    "end": "1087679"
  },
  {
    "start": "1087000",
    "end": "1247000"
  },
  {
    "text": "so so far as i've been talking about this implementation one of the interesting things is you could have actually done every",
    "start": "1088320",
    "end": "1094799"
  },
  {
    "text": "single thing i've described yourself you can use the cluster auto scaler you can use the node order provisioner component",
    "start": "1094799",
    "end": "1100559"
  },
  {
    "text": "you can write your own mutating web hook admission controller you can do all that you can literally build exactly what i",
    "start": "1100559",
    "end": "1106000"
  },
  {
    "text": "just described yourselves today on you know gk standard so if that's the case like why do we even need this other",
    "start": "1106000",
    "end": "1112000"
  },
  {
    "text": "product um aside from the fact that it's a bit challenging creating these mutating web hooks and so on well",
    "start": "1112000",
    "end": "1118720"
  },
  {
    "text": "obviously one of the benefits of us doing it is that it's all pre-configured in a nice package but that alone is probably not enough",
    "start": "1118720",
    "end": "1124320"
  },
  {
    "text": "there's a couple of things that we add with this product that is sort of hard to do yourselves and and that is uh the",
    "start": "1124320",
    "end": "1130559"
  },
  {
    "text": "billing model is different it's it's request-based so we we charge based on the pod request rather than the nodes",
    "start": "1130559",
    "end": "1137039"
  },
  {
    "text": "obviously that's not something that you can change as a user probably the biggest selling point is",
    "start": "1137039",
    "end": "1142400"
  },
  {
    "text": "that by creating a fully managed platform with nodes in a known condition for the",
    "start": "1142400",
    "end": "1148240"
  },
  {
    "text": "first time we were actually able to add node sre onto this product so",
    "start": "1148240",
    "end": "1154400"
  },
  {
    "text": "how the traditional gke kubernetes platform works is the nodes were kind of a shared responsibility model",
    "start": "1154400",
    "end": "1161440"
  },
  {
    "text": "google would take a lot of responsibility for it but the users would also be responsible and that was because people could go in with root",
    "start": "1161440",
    "end": "1167679"
  },
  {
    "text": "access and change them and it was sort of hard for us to know what had changed so with this product since since we've",
    "start": "1167679",
    "end": "1173840"
  },
  {
    "text": "eliminated that we can actually for the first time offer sre essentially that's kind of the the bargain that you have when you use this",
    "start": "1173840",
    "end": "1180160"
  },
  {
    "text": "product if you're willing to give up that little bit of control about not being able to modify the node which i",
    "start": "1180160",
    "end": "1185679"
  },
  {
    "text": "think hopefully for most workloads is totally fine to give up you don't you know if you're running mariadb you",
    "start": "1185679",
    "end": "1190720"
  },
  {
    "text": "shouldn't shouldn't have root access um what you get in return is a more fully managed system with with us being the",
    "start": "1190720",
    "end": "1197440"
  },
  {
    "text": "sre the other thing is that we as i mentioned we eliminated",
    "start": "1197440",
    "end": "1202480"
  },
  {
    "text": "completely the the vm api so there is no visibility on these vms the vms actually",
    "start": "1202480",
    "end": "1208240"
  },
  {
    "text": "still there they're exactly the same they have a different prefix um so if you if you look at if you look at the",
    "start": "1208240",
    "end": "1214400"
  },
  {
    "text": "cube ctl get node output you might notice that the autopilot node has the prefix gk3 instead of gke",
    "start": "1214400",
    "end": "1222240"
  },
  {
    "text": "the node the the virtual machines are actually still there they're still there in the product but the the api is",
    "start": "1222240",
    "end": "1227520"
  },
  {
    "text": "removed um this has the benefit for users that particularly security conscious ones",
    "start": "1227520",
    "end": "1233360"
  },
  {
    "text": "they don't have to worry about things like ssh because there is no ssh into these nodes so it's like more lockdown that's another advantage of the product",
    "start": "1233360",
    "end": "1240159"
  },
  {
    "text": "okay so i covered the design and the implementation of how we built it",
    "start": "1240159",
    "end": "1247200"
  },
  {
    "start": "1247000",
    "end": "1321000"
  },
  {
    "text": "let's look a little bit now at the the result um where did we land what what does this",
    "start": "1247200",
    "end": "1252480"
  },
  {
    "text": "look like so at the beginning i showed this diagram of uh",
    "start": "1252480",
    "end": "1258960"
  },
  {
    "text": "the the two apis essentially users have to interact with in order to get kubernetes in order to use kubernetes",
    "start": "1258960",
    "end": "1264880"
  },
  {
    "text": "and even this diagram with that extra vm api at the bottom which you typically don't have to use but it's there and you",
    "start": "1264880",
    "end": "1271440"
  },
  {
    "text": "might have to care that it's there so with autopilot what we're able to achieve is we essentially shrank the",
    "start": "1271440",
    "end": "1278000"
  },
  {
    "text": "entire gke api surface area down to one command which is create you just create it you connect it to",
    "start": "1278000",
    "end": "1284960"
  },
  {
    "text": "cube ctl from that point on you are 100 just interacting",
    "start": "1284960",
    "end": "1290960"
  },
  {
    "text": "using the kubernetes api so it's really a kind of a i like to think of it as like a very pure you know kubernetes",
    "start": "1290960",
    "end": "1296720"
  },
  {
    "text": "platform the api you're using on this platform is just the kubernetes api there's there's no node pool api you",
    "start": "1296720",
    "end": "1302640"
  },
  {
    "text": "don't have to configure scaling auto scaling anything like that you just interact with it through kubernetes",
    "start": "1302640",
    "end": "1308880"
  },
  {
    "text": "from a ui standpoint we you know obviously the ui representation of that of that api is also pretty nice there's",
    "start": "1308880",
    "end": "1314640"
  },
  {
    "text": "just three fields and you can create a basically a production grade cluster and one of those fields is an arbitrary and name",
    "start": "1314640",
    "end": "1320799"
  },
  {
    "text": "okay now for the for the meat of this talk i guess um",
    "start": "1320799",
    "end": "1327760"
  },
  {
    "start": "1321000",
    "end": "1627000"
  },
  {
    "text": "what are the benefits realized from all this from this design of you know building this fully managed platform",
    "start": "1327760",
    "end": "1334000"
  },
  {
    "text": "which actually looks a lot like the traditional gke right it has it has you know pretty much the same nodes under",
    "start": "1334000",
    "end": "1339280"
  },
  {
    "text": "the hood the same multiple pods per node a lot of the same things um other than the fact that that you",
    "start": "1339280",
    "end": "1345600"
  },
  {
    "text": "know potentially made it a little bit faster for us to build like what's the benefit to the user what's the benefit of this design um",
    "start": "1345600",
    "end": "1352559"
  },
  {
    "text": "and and i hope this is relevant to all of you particularly if you're also building your own kubernetes platforms maybe maybe this might be of interest as well",
    "start": "1352559",
    "end": "1359919"
  },
  {
    "text": "i think the first benefit is that it enables really granular pod sizes because we're bin packing pods onto",
    "start": "1359919",
    "end": "1366000"
  },
  {
    "text": "machines we don't have to shoehorn the pods into vm sizes so",
    "start": "1366000",
    "end": "1372400"
  },
  {
    "text": "i did cover that earlier but you know you can basically create like a you know 21.25",
    "start": "1372400",
    "end": "1377520"
  },
  {
    "text": "core pod and and and just slot it in there we'll run that just fine and then you know you can add a quarter of a core",
    "start": "1377520",
    "end": "1382640"
  },
  {
    "text": "another one called whatever whatever you want to do it's it's uh very very flexible and i think that kind of",
    "start": "1382640",
    "end": "1388000"
  },
  {
    "text": "matches what users want um other nice things about keeping the the",
    "start": "1388000",
    "end": "1393039"
  },
  {
    "text": "node object as a or the kind of the node scheduling concept in this design is",
    "start": "1393039",
    "end": "1398880"
  },
  {
    "text": "that things like pod affinity and andy affinity continue to work these are important concepts that come from the",
    "start": "1398880",
    "end": "1404480"
  },
  {
    "text": "kubernetes api right and if you remember back to my um original proposal i really",
    "start": "1404480",
    "end": "1409600"
  },
  {
    "text": "wanted this to look like and be a fully capable kubernetes platform",
    "start": "1409600",
    "end": "1415440"
  },
  {
    "text": "well it's not fully capable if you eliminate things like pod affinity and infinity",
    "start": "1415440",
    "end": "1420799"
  },
  {
    "text": "pod affinity might be used for example if you have like a front end pond and a back end pod and you want to say i want",
    "start": "1420799",
    "end": "1426400"
  },
  {
    "text": "these pods to always be together on the node well it's hard to do that if you don't have nodes in a",
    "start": "1426400",
    "end": "1432559"
  },
  {
    "text": "fully nervous system you can't do that this actually works quite fine with our design we will ensure that that",
    "start": "1432559",
    "end": "1438080"
  },
  {
    "text": "constraint is satisfied similar story with node affinity",
    "start": "1438080",
    "end": "1443200"
  },
  {
    "text": "so we you know we offer zonal affinity so if you have a zonal resource and you want your pods in that same zone you can use that",
    "start": "1443200",
    "end": "1450159"
  },
  {
    "text": "one one slight i guess drawback of this system is occasionally you might want to separate workloads in in a one pod per",
    "start": "1450159",
    "end": "1456320"
  },
  {
    "text": "node system you don't have to separate them because they're always separated but for that you can actually just use",
    "start": "1456320",
    "end": "1461520"
  },
  {
    "text": "the kubernetes constructs of tolerations and node selection so again",
    "start": "1461520",
    "end": "1466720"
  },
  {
    "text": "the kubernetes api already has the you know the language the syntax to describe these things and and we can honor that",
    "start": "1466720",
    "end": "1472880"
  },
  {
    "text": "in this managed product uh that still has notes pod spread topologies work",
    "start": "1472880",
    "end": "1479039"
  },
  {
    "text": "and the last one here is daemon sets daemon sets is often under um often",
    "start": "1479039",
    "end": "1484159"
  },
  {
    "text": "overlooked i think when it comes to fully managed platforms but they're actually really important",
    "start": "1484159",
    "end": "1489360"
  },
  {
    "text": "daemon sets don't make much sense if you only have one pod per node because the whole point is you want to run an agent",
    "start": "1489360",
    "end": "1494400"
  },
  {
    "text": "on the node the good thing is with this with this proposal with this design you can still",
    "start": "1494400",
    "end": "1500159"
  },
  {
    "text": "have a name instead because there are still nodes with one catch and the catch is that daemon sets are",
    "start": "1500159",
    "end": "1506320"
  },
  {
    "text": "typically used to actually modify the node so how does that how does that work in this system we have on one hand it",
    "start": "1506320",
    "end": "1512159"
  },
  {
    "text": "has nodes so you can you know theoretically have a daemon set but on the other hand we limited uh some of the",
    "start": "1512159",
    "end": "1518080"
  },
  {
    "text": "administrative functionality which means some of the use cases of that data set no longer apply",
    "start": "1518080",
    "end": "1523760"
  },
  {
    "text": "the compromise we reached here is that we looked at well-known products and solutions uh",
    "start": "1523760",
    "end": "1529520"
  },
  {
    "text": "out there in the community out there you know commercially available and we decided to allow list specific",
    "start": "1529520",
    "end": "1536000"
  },
  {
    "text": "solutions so they could continue to work on autopilot so many uh security logging and monitoring solutions that are",
    "start": "1536000",
    "end": "1542159"
  },
  {
    "text": "commercially available like all the people you see out there on the on the booth floor most of those solutions still work and",
    "start": "1542159",
    "end": "1547840"
  },
  {
    "text": "if they don't work they can probably come and chat to me and we can make it work the reason we could do that is as i said",
    "start": "1547840",
    "end": "1553440"
  },
  {
    "text": "earlier the security boundary for this product remains the vm so even though by allowing listing",
    "start": "1553440",
    "end": "1558880"
  },
  {
    "text": "certain partner workloads to have elevated privileges uh the security boundary remains the same it's still a",
    "start": "1558880",
    "end": "1564320"
  },
  {
    "text": "vm this is not a multi-tenant product so we didn't have that concern the main concern when we were building this from",
    "start": "1564320",
    "end": "1569600"
  },
  {
    "text": "from like a design perspective was we had to make sure those nodes are supportable we didn't we didn't we didn't just want to like open the flood",
    "start": "1569600",
    "end": "1575039"
  },
  {
    "text": "gates and let all kind of node modifications happen but if it's a known set um and you know professionally",
    "start": "1575039",
    "end": "1581760"
  },
  {
    "text": "supported software we felt like that was safe enough to offer from a supportability point of view",
    "start": "1581760",
    "end": "1587039"
  },
  {
    "text": "so it still allows damages and i think that's important because you know if you have a workload that you're",
    "start": "1587039",
    "end": "1592799"
  },
  {
    "text": "running on-prem or you're running you know on another cloud provider or on gk",
    "start": "1592799",
    "end": "1597840"
  },
  {
    "text": "standard and you want to migrate it around you probably have in fact most of our customers have at",
    "start": "1597840",
    "end": "1603279"
  },
  {
    "text": "least one you know damage set workload they just want to run if we didn't offer this essentially you",
    "start": "1603279",
    "end": "1608480"
  },
  {
    "text": "would have to go in and add that whether it's security or logging or whatever you would have to add that functionality to",
    "start": "1608480",
    "end": "1613760"
  },
  {
    "text": "every single pod which i think is a really really big burden on developers so that is actually one of the really",
    "start": "1613760",
    "end": "1619600"
  },
  {
    "text": "overlooked benefits um i i think of of still supporting nodes and still offering a multi-node a multi-pod per",
    "start": "1619600",
    "end": "1626400"
  },
  {
    "text": "node system other benefits and and this mostly kind of helps us as we you know work on the",
    "start": "1626400",
    "end": "1632559"
  },
  {
    "start": "1627000",
    "end": "1766000"
  },
  {
    "text": "product further but um other benefits of this design are the fact that the",
    "start": "1632559",
    "end": "1638399"
  },
  {
    "text": "infrastructure features uh it's very similar to gke so a lot of things just work um",
    "start": "1638399",
    "end": "1644000"
  },
  {
    "text": "one really good example is stateful set so right out of the gate on on day one we were able to support persistent",
    "start": "1644000",
    "end": "1649279"
  },
  {
    "text": "volume claims using block storage resources which means you can run marie db redis any other state for workload",
    "start": "1649279",
    "end": "1654880"
  },
  {
    "text": "just works because it's the same infrastructure we didn't have to reinvent the wheel when we look at adding additional",
    "start": "1654880",
    "end": "1660720"
  },
  {
    "text": "features things like you know new machine types maybe uh if you're if you",
    "start": "1660720",
    "end": "1665840"
  },
  {
    "text": "if you follow the the google cloud naming pattern you know like n2 or c2",
    "start": "1665840",
    "end": "1671120"
  },
  {
    "text": "things like the the highly optimized compute machines stuff like that is actually going to be a lot easier for us to add because we chose this model and",
    "start": "1671120",
    "end": "1678720"
  },
  {
    "text": "because it's basically sharing that same infrastructure hardware like gpu local ssd again because these are just regular",
    "start": "1678720",
    "end": "1685039"
  },
  {
    "text": "vms under the hood it's a lot simpler for the team to add support for those features",
    "start": "1685039",
    "end": "1692480"
  },
  {
    "text": "and finally it's possible to offer burstable class pods so i mentioned this right the very beginning as one of my",
    "start": "1692640",
    "end": "1698720"
  },
  {
    "text": "three kind of objectives and that was the ability to utilize unused capacity in the class to bipods",
    "start": "1698720",
    "end": "1705600"
  },
  {
    "text": "why do i think this is important the reason i think this is important is that if you have like three or four pods all",
    "start": "1705600",
    "end": "1711360"
  },
  {
    "text": "running on a node it's likely that at some point some of them are completely idle let's say it's a web serving",
    "start": "1711360",
    "end": "1716880"
  },
  {
    "text": "application you may have you know three pods that are completely idle now if a request comes in to one of",
    "start": "1716880",
    "end": "1723120"
  },
  {
    "text": "those pods you ideally don't want to constrain it to just the resources that that pod requested now certainly that",
    "start": "1723120",
    "end": "1728799"
  },
  {
    "text": "pod needs to be able to handle that request within whatever slo you have with the resources it requested but",
    "start": "1728799",
    "end": "1734080"
  },
  {
    "text": "wouldn't it be nice if opportunistically it could just burst use the capacity you're already paying for on the node um",
    "start": "1734080",
    "end": "1740080"
  },
  {
    "text": "and serve that request a little bit faster delight the user a little bit more i feel like that's a really important",
    "start": "1740080",
    "end": "1746240"
  },
  {
    "text": "feature and it is possible with this design it's really only possible with this design",
    "start": "1746240",
    "end": "1751520"
  },
  {
    "text": "because if you don't have multiple pods on the node if you don't have this concept of the node then you can't really pull that capacity for bursting",
    "start": "1751520",
    "end": "1758080"
  },
  {
    "text": "we don't actually offer this yet in in the product that we built but it's something that we're looking at and i'd like to share a quick kind of",
    "start": "1758080",
    "end": "1764720"
  },
  {
    "text": "design of how this might work so if you look at guaranteed class pods",
    "start": "1764720",
    "end": "1769919"
  },
  {
    "start": "1766000",
    "end": "1865000"
  },
  {
    "text": "today the cpu requests equal the limits which means there's no bursting",
    "start": "1769919",
    "end": "1776880"
  },
  {
    "text": "traditionally in kubernetes you could you could set a much higher limit than the request and thus create a",
    "start": "1776880",
    "end": "1783520"
  },
  {
    "text": "burstable pod that can scale up when there's extra resources the problem with the design that we came up with is",
    "start": "1783520",
    "end": "1790320"
  },
  {
    "text": "take this example of three pods that totally have requested five and a half",
    "start": "1790320",
    "end": "1795360"
  },
  {
    "text": "cores and that are running on an eight core machine the problem is in a traditional kubernetes model those",
    "start": "1795360",
    "end": "1801440"
  },
  {
    "text": "pods would be able to use up to eight cores which could be a bit of a problem for us it might allow for some gaming of the system where you could sort of try",
    "start": "1801440",
    "end": "1807919"
  },
  {
    "text": "and convince autopilot to create like an eight core node run a run a one core workload on it you know try and prevent",
    "start": "1807919",
    "end": "1813600"
  },
  {
    "text": "other pods getting on there in with you know using some technique um which is potentially possible and thus kind of",
    "start": "1813600",
    "end": "1820559"
  },
  {
    "text": "you know get like 8x your resources that would be a problem for us so the ideal design would be well what",
    "start": "1820559",
    "end": "1826320"
  },
  {
    "text": "if the user could burst within the paid for capacity on the node at any given time so if we clamp the bursting in this",
    "start": "1826320",
    "end": "1832320"
  },
  {
    "text": "case to five and a half cores then we would be giving you you know the capacity that you're paying for which",
    "start": "1832320",
    "end": "1837360"
  },
  {
    "text": "would be ideal only problem is it turns out that's a bit hard to do with the way uh linux uh",
    "start": "1837360",
    "end": "1842640"
  },
  {
    "text": "the completely fair scheduler works in linux today it's a little bit hard to do that so one idea we have",
    "start": "1842640",
    "end": "1848960"
  },
  {
    "text": "they're actively looking at is to potentially um kind of round up to the nearest integer number of cores by just turning off the",
    "start": "1848960",
    "end": "1856080"
  },
  {
    "text": "the unused cores and allowing full bursting within um in this case six cores",
    "start": "1856080",
    "end": "1862080"
  },
  {
    "text": "that's just an idea we have um stay tuned for that what about the downsides i did i did",
    "start": "1862080",
    "end": "1867360"
  },
  {
    "start": "1865000",
    "end": "1964000"
  },
  {
    "text": "mention that there would be pros and cons a couple of the downsides i believe of the design that we came up with here",
    "start": "1867360",
    "end": "1873600"
  },
  {
    "text": "is that there is a potential for allocatable inefficiency in this system right it's possible if the user is",
    "start": "1873600",
    "end": "1879840"
  },
  {
    "text": "creating and deleting a lot of pods of all different shapes and sizes that you could end up with uh very much",
    "start": "1879840",
    "end": "1885120"
  },
  {
    "text": "underutilized nodes and that requires you know the team to build additional features like defrag to kind of correct",
    "start": "1885120",
    "end": "1891200"
  },
  {
    "text": "that so that's a bit of extra work that we that we that we end up taking it doesn't affect the user the user",
    "start": "1891200",
    "end": "1897120"
  },
  {
    "text": "doesn't care about that really it's kind of more just a problem for the infrastructure platform",
    "start": "1897120",
    "end": "1902480"
  },
  {
    "text": "to solve the other potential issue obviously by you know running two containers two pods",
    "start": "1902480",
    "end": "1908159"
  },
  {
    "text": "on on the one node you can potentially have resource contention um although we do have a way to solve that",
    "start": "1908159",
    "end": "1914159"
  },
  {
    "text": "for users they can still separate their workloads when needed um and a couple of downsides of using a",
    "start": "1914159",
    "end": "1919679"
  },
  {
    "text": "multi-single tenor platform like we do as opposed to a fully multi-tenant platform is that it's harder for us to",
    "start": "1919679",
    "end": "1926000"
  },
  {
    "text": "add hot standby capacity so one of the really nice benefits of a multi-tenant system is you typically have you know a",
    "start": "1926000",
    "end": "1931679"
  },
  {
    "text": "massive resource pool shared by everyone you can just individuals can scale up and down very quickly we don't have that ability uh",
    "start": "1931679",
    "end": "1937679"
  },
  {
    "text": "with with this design so i just want that is one drawback uh adding a new pod uh if it needs a new node can take",
    "start": "1937679",
    "end": "1943279"
  },
  {
    "text": "between 60 and 80 seconds and there's a little bit of greater operational complexity on the platform side i think if any time you operate a",
    "start": "1943279",
    "end": "1949519"
  },
  {
    "text": "multi-single tenant system there's a little bit extra ops complexity but again most of these downsides by the way are",
    "start": "1949519",
    "end": "1955279"
  },
  {
    "text": "kind of just a burden on us i i think you know just makes a couple of different",
    "start": "1955279",
    "end": "1961039"
  },
  {
    "text": "problems a little bit harder to solve but hopefully it's solvable okay so",
    "start": "1961039",
    "end": "1966880"
  },
  {
    "start": "1964000",
    "end": "2020000"
  },
  {
    "text": "in summary um and you know this is the the takeaway here i guess that i'd like to try and convince you i",
    "start": "1966880",
    "end": "1972960"
  },
  {
    "text": "hope i did convince you of this is this nodeless and does that even matter um well",
    "start": "1972960",
    "end": "1979279"
  },
  {
    "text": "you know when i started this project back in 2018 novus was kind of synonymous with fully managed kubernetes",
    "start": "1979279",
    "end": "1984720"
  },
  {
    "text": "and i guess maybe the point i'm trying to make here is that it's not a good i synonym",
    "start": "1984720",
    "end": "1990159"
  },
  {
    "text": "that this design is is operationally nodeless it's nervous in the sense that you don't care about the nodes you don't",
    "start": "1990159",
    "end": "1995600"
  },
  {
    "text": "have to think about them but there is but i do believe there is a benefit as i've hopefully outlined of",
    "start": "1995600",
    "end": "2001279"
  },
  {
    "text": "having nodes existing as a scheduling concept when needed and when relevant",
    "start": "2001279",
    "end": "2006880"
  },
  {
    "text": "so that was our journey i hope that was useful i hope i hope uh yeah i hope it's of interest to to learn",
    "start": "2006880",
    "end": "2012880"
  },
  {
    "text": "how the team went about and built this thing maybe maybe it can inspire you as you're building your own products and services",
    "start": "2012880",
    "end": "2018640"
  },
  {
    "text": "with kubernetes and with that i'd love to take any questions we have a couple of minutes uh",
    "start": "2018640",
    "end": "2024000"
  },
  {
    "start": "2020000",
    "end": "2371000"
  },
  {
    "text": "also happy to continue the debate on twitter over beer or however you want to do it",
    "start": "2024000",
    "end": "2029760"
  },
  {
    "text": "there are a couple of microphones if anyone has a question and it might be a romeo mic as well",
    "start": "2029760",
    "end": "2036240"
  },
  {
    "text": "thank you",
    "start": "2036240",
    "end": "2039480"
  },
  {
    "text": "uh can we have the mike uh go live please",
    "start": "2046960",
    "end": "2051838"
  },
  {
    "text": "yep testing okay lovely hi i'm the moderator i'll be doing questions online",
    "start": "2052240",
    "end": "2058079"
  },
  {
    "text": "if there are any but there aren't any at this point but just wanted to let you know that we have about a minute till our schedule in time but we'll do a",
    "start": "2058079",
    "end": "2064560"
  },
  {
    "text": "couple of questions we can go just a few minutes over okay sounds great please",
    "start": "2064560",
    "end": "2069679"
  },
  {
    "text": "take it away well i hope you can hear me well the company we were i work with is",
    "start": "2069679",
    "end": "2075200"
  },
  {
    "text": "subject to a lot of audits and they require us to split nodes in different subnets",
    "start": "2075200",
    "end": "2081520"
  },
  {
    "text": "in order to you know segregate them on a network level and also they require us to install",
    "start": "2081520",
    "end": "2086560"
  },
  {
    "text": "certain software and binaries on these nodes basically we have to be in control of the machine image that we use to spin",
    "start": "2086560",
    "end": "2091919"
  },
  {
    "text": "up the nodes and i wanted to ask if this is capable if you are capable of doing this yeah i'm just both of these things",
    "start": "2091919",
    "end": "2098320"
  },
  {
    "text": "actually i i don't know if the subnet separation thing is possible that's something i'd have to probably take",
    "start": "2098320",
    "end": "2103359"
  },
  {
    "text": "offline and do a deep dive with you with the with the security components that you're running i would like to think that it is possible we do work with a",
    "start": "2103359",
    "end": "2109920"
  },
  {
    "text": "lot of security partners to make sure that their solutions work so unless it's like a homegrown home built container i",
    "start": "2109920",
    "end": "2116079"
  },
  {
    "text": "would say the answer is probably yes it either works or can be made to work with this system the other thing by the way is as you go",
    "start": "2116079",
    "end": "2122000"
  },
  {
    "text": "to these auditors hopefully you can uh position this and we would help you right position this as a more secure",
    "start": "2122000",
    "end": "2128960"
  },
  {
    "text": "platform to begin with right because things like ssh are completely eliminated from the nodes as well so yeah um i don't know about that because",
    "start": "2128960",
    "end": "2135119"
  },
  {
    "text": "they were pretty reluctant of allowing us to go to the cloud in the first place right so i'm not really sure how you",
    "start": "2135119",
    "end": "2141520"
  },
  {
    "text": "know cool they will be with that but yeah uh then the other question that spikes up is um",
    "start": "2141520",
    "end": "2147760"
  },
  {
    "text": "is uh our rdav for example softwares um you know enabled through daemon sets",
    "start": "2147760",
    "end": "2154960"
  },
  {
    "text": "or is that the way you do it which software sorry um for example antivirus softwares",
    "start": "2154960",
    "end": "2160960"
  },
  {
    "text": "right yes yes you would install that you would install that with a daemon set all right um provided like if the daemon set",
    "start": "2160960",
    "end": "2166400"
  },
  {
    "text": "is using privilege which it probably is as a virus scanner yeah um it wouldn't it would need to be specifically allowed listed by my team but we have we've",
    "start": "2166400",
    "end": "2173040"
  },
  {
    "text": "already done about i think about eight or nine solutions and then there's room to grow that so yes it would be through",
    "start": "2173040",
    "end": "2178400"
  },
  {
    "text": "a data set all right thank you",
    "start": "2178400",
    "end": "2181838"
  },
  {
    "text": "and apart all the good things of benefits of the dki develop we are active users of happy",
    "start": "2185359",
    "end": "2192560"
  },
  {
    "text": "uses of the ukee autopilot but and the restrictions we come up with the with not being able to deploy things like",
    "start": "2192560",
    "end": "2199599"
  },
  {
    "text": "secret install and this kind of drivers do you have any plan to overcome those issues that",
    "start": "2199599",
    "end": "2205520"
  },
  {
    "text": "implicitly this kind of designs imposes because yeah we want to use bolt for",
    "start": "2205520",
    "end": "2211200"
  },
  {
    "text": "example for secrets and we weren't able to do that by using standard csi drivers",
    "start": "2211200",
    "end": "2216960"
  },
  {
    "text": "instead of creating direct connections to those bulk clusters and so on okay",
    "start": "2216960",
    "end": "2223599"
  },
  {
    "text": "i believe vault might actually work now because i i know i've tested it and i think i got it working unless you're",
    "start": "2223599",
    "end": "2228960"
  },
  {
    "text": "using a different configuration your own let's say the google cloud platform github repository has one",
    "start": "2228960",
    "end": "2235040"
  },
  {
    "text": "secret stud and there is a limitation of host path and privilege to to okay",
    "start": "2235040",
    "end": "2240880"
  },
  {
    "text": "actively being used so yeah yeah i think like when it come when it comes to these like restrictions basically we have to",
    "start": "2240880",
    "end": "2246960"
  },
  {
    "text": "we have to either if it's a kind of a partner like a well-known container a well-known",
    "start": "2246960",
    "end": "2252079"
  },
  {
    "text": "workload we can we can take we can potentially allow list it the other option we have is if it's like a technique we have to enable we",
    "start": "2252079",
    "end": "2258160"
  },
  {
    "text": "essentially have to kind of productize it so if you need a driver it might be the case where we just have to offer the driver as a feature right where you can",
    "start": "2258160",
    "end": "2264720"
  },
  {
    "text": "just turn on that driver so um we should we should connect and continue this conversation but but i i do believe",
    "start": "2264720",
    "end": "2271920"
  },
  {
    "text": "vault should actually work i know when when we first launched the product we didn't offer mutating web hook",
    "start": "2271920",
    "end": "2277119"
  },
  {
    "text": "support which broke like half of the you know the kind of types of workloads like",
    "start": "2277119",
    "end": "2282240"
  },
  {
    "text": "vault and things like that which need to mutate workloads we did add that in recently uh so that was a restriction",
    "start": "2282240",
    "end": "2287280"
  },
  {
    "text": "that we never really designed to be in the product it was kind of like collateral damage okay so we fixed that",
    "start": "2287280",
    "end": "2292480"
  },
  {
    "text": "so yeah i do believe that bulk should actually work in particular but we should follow we'll drop you directly",
    "start": "2292480",
    "end": "2299040"
  },
  {
    "text": "all right thank you cheers all right last question i think then we might have to wrap yeah hi uh quick one yeah i saw a couple",
    "start": "2299040",
    "end": "2306240"
  },
  {
    "text": "of times the restriction on cpu increment being",
    "start": "2306240",
    "end": "2311599"
  },
  {
    "text": "is that on purpose and if so why good question um",
    "start": "2311599",
    "end": "2316640"
  },
  {
    "text": "it's it's it's a decision i think we should actually revisit uh the",
    "start": "2316640",
    "end": "2322160"
  },
  {
    "text": "you know the plan was to start a little bit conservative i guess because it's much easier to relax these restrictions over time and",
    "start": "2322160",
    "end": "2328240"
  },
  {
    "text": "and i i believe that the theory was uh and like i was part of the decision and i'm still trying to remember kind of",
    "start": "2328240",
    "end": "2333760"
  },
  {
    "text": "like like why did we actually do that i think it was just so that when we packed the pods on the on on the nodes that are",
    "start": "2333760",
    "end": "2339760"
  },
  {
    "text": "kind of if if they're known kind of sizes like you know the little tetris kind of diagram you know we can kind of",
    "start": "2339760",
    "end": "2344960"
  },
  {
    "text": "slide it in a little bit better in hindsight i'm not convinced it's actually needed to be honest so",
    "start": "2344960",
    "end": "2350880"
  },
  {
    "text": "yeah we might we might take another look at that and yeah i mean would you like to just have like just anything",
    "start": "2350880",
    "end": "2356800"
  },
  {
    "text": "like between we probably need a minimum but uh yeah all right that was a thumbs up for the",
    "start": "2356800",
    "end": "2363280"
  },
  {
    "text": "for the people online great well great questions thanks a lot like i said let's keep the conversation",
    "start": "2363280",
    "end": "2369040"
  },
  {
    "text": "going too thank you",
    "start": "2369040",
    "end": "2373079"
  }
]