[
  {
    "start": "0",
    "end": "78000"
  },
  {
    "text": "you go thank you so much for taking the",
    "start": "30",
    "end": "10019"
  },
  {
    "text": "time to attend this talk it's benefits of no local DNS cache I know it's the",
    "start": "10019",
    "end": "16949"
  },
  {
    "text": "last session for the day so we'll try to keep it short and fun and make sure",
    "start": "16949",
    "end": "22680"
  },
  {
    "text": "you're all ready for the evening party awesome like like it was just introduced",
    "start": "22680",
    "end": "28890"
  },
  {
    "text": "and pavitra Ramesh I've been working on kubernetes for over a year now mostly in",
    "start": "28890",
    "end": "34410"
  },
  {
    "text": "the DNS space plate and Before we jump into the content I'd like to get a sense of how many of you",
    "start": "34410",
    "end": "40950"
  },
  {
    "text": "here have already used no local DNS cache awesome ok thank you thank you for",
    "start": "40950",
    "end": "47579"
  },
  {
    "text": "trying it out and how many of you have had to debug DNS latency in your",
    "start": "47579",
    "end": "53160"
  },
  {
    "text": "clusters ok I expected it to be a superset but Wow ok that was more than I",
    "start": "53160",
    "end": "58320"
  },
  {
    "text": "expected cool not cool that you had a debug but good to know that is a common problem",
    "start": "58320",
    "end": "64338"
  },
  {
    "text": "and finally how many of you are wondering if you're in the wrong talk I'm just kidding awesome",
    "start": "64339",
    "end": "70049"
  },
  {
    "text": "thank you for not raising your hand so everybody here wants to be in this talk that's great so on that note let's jump",
    "start": "70049",
    "end": "75840"
  },
  {
    "text": "in the agenda for today is the following we're gonna start by giving a short",
    "start": "75840",
    "end": "81600"
  },
  {
    "start": "78000",
    "end": "129000"
  },
  {
    "text": "intro to what no local DNS cache is and why we needed to answer the why we are",
    "start": "81600",
    "end": "86880"
  },
  {
    "text": "going to quickly look at what the current DNS therapist in kubernetes and how local fits in then we'll talk about",
    "start": "86880",
    "end": "94560"
  },
  {
    "text": "what new metrics are available to you when you enable this feature then we have a very interesting section that",
    "start": "94560",
    "end": "100860"
  },
  {
    "text": "Blake will be covering which goes over what what real-life improvements in an",
    "start": "100860",
    "end": "106500"
  },
  {
    "text": "actual production cluster heart after enabling the feature then we",
    "start": "106500",
    "end": "111540"
  },
  {
    "text": "talk about ok how can i deploy this in my cluster ok in any of your clusters",
    "start": "111540",
    "end": "116579"
  },
  {
    "text": "that is and then future work what's coming and finally we will have time for",
    "start": "116579",
    "end": "121649"
  },
  {
    "text": "questions as you can see it's a packed agenda but we will keep it brief and",
    "start": "121649",
    "end": "126780"
  },
  {
    "text": "hopefully fun ok let's jump right what is no local DNS cache",
    "start": "126780",
    "end": "133260"
  },
  {
    "start": "129000",
    "end": "153000"
  },
  {
    "text": "an add-on that runs a DNS caching agent on every node so there's one instance",
    "start": "133260",
    "end": "138840"
  },
  {
    "text": "running on every node as a daemon set and all pods that run on a specific node",
    "start": "138840",
    "end": "144450"
  },
  {
    "text": "talk to their local instance for all the DNS needs so that's the short definition",
    "start": "144450",
    "end": "149849"
  },
  {
    "text": "of what that feature is and why do we need this for this I added a screenshot of some of",
    "start": "149849",
    "end": "157260"
  },
  {
    "start": "153000",
    "end": "180000"
  },
  {
    "text": "the github issues tagged yourself if you participated in one of these these are",
    "start": "157260",
    "end": "162750"
  },
  {
    "text": "issues asking for a feature similar to this or it's discussing DNS latency and",
    "start": "162750",
    "end": "169799"
  },
  {
    "text": "the conversation converges to a solution like what we built these are just some of them there will be more and many of",
    "start": "169799",
    "end": "176010"
  },
  {
    "text": "them closed now since we got this feature out but why are DNS latency",
    "start": "176010",
    "end": "183209"
  },
  {
    "start": "180000",
    "end": "279000"
  },
  {
    "text": "issues showing up in kubernetes clusters these are just some of the reasons I put out there not all clusters are the same",
    "start": "183209",
    "end": "189480"
  },
  {
    "text": "so if you have experience it could be one of these it could be a different reason it could be a mix of some of",
    "start": "189480",
    "end": "194489"
  },
  {
    "text": "these and even these reasons are not independent they are intertwined so the",
    "start": "194489",
    "end": "200430"
  },
  {
    "text": "first one I'd like to talk about is too many queries all at once so too many",
    "start": "200430",
    "end": "206130"
  },
  {
    "text": "parallel queries and by v4 and v6 I actually mean a and quad a records so",
    "start": "206130",
    "end": "212489"
  },
  {
    "text": "why does this happen think about it if a cluster or a client part is trying to look up foo.com the",
    "start": "212489",
    "end": "218819"
  },
  {
    "text": "client is trying to issue only one request maybe to request a record and a quarter a record but this is actually",
    "start": "218819",
    "end": "225239"
  },
  {
    "text": "translated into several more requests and you can see all the search path suffixes that get had it in that red box",
    "start": "225239",
    "end": "231840"
  },
  {
    "text": "there so all these suffixes are added to make it five queries of type a and five",
    "start": "231840",
    "end": "239280"
  },
  {
    "text": "queries of quality records so you can get ten or even more queries for every",
    "start": "239280",
    "end": "245160"
  },
  {
    "text": "actual query that a pod requests and this can increase the chance of",
    "start": "245160",
    "end": "251940"
  },
  {
    "text": "netfilter race condition and I'll link this here there's a very nice blog post that explains why this happens when",
    "start": "251940",
    "end": "258599"
  },
  {
    "text": "there is a network address translation so you can take a look I say it is pretty hard to pinpoint that",
    "start": "258599",
    "end": "265669"
  },
  {
    "text": "this is what you're running into if there is a packet drop the metrics to track if this is happening aren't super",
    "start": "265669",
    "end": "272389"
  },
  {
    "text": "clear but it might be one of the reasons but parallel queries are problematic even otherwise here are some of the",
    "start": "272389",
    "end": "279800"
  },
  {
    "start": "279000",
    "end": "379000"
  },
  {
    "text": "other reasons too many DNS queries overwhelming connection tracking tables and parallel queries can make that",
    "start": "279800",
    "end": "286039"
  },
  {
    "text": "problem worse too for every connection that that happens on your Lord whether",
    "start": "286039",
    "end": "292189"
  },
  {
    "text": "it's Andres translated or not there is a tracking going on so each of these connections occupy an entry in your",
    "start": "292189",
    "end": "298400"
  },
  {
    "text": "connection tracking table which has a limit so if you have more connections opening up then there is space on the",
    "start": "298400",
    "end": "304789"
  },
  {
    "text": "table those are gonna get dropped and packet drops are not great not good at all especially for UDP what are the",
    "start": "304789",
    "end": "311569"
  },
  {
    "text": "other reasons again more parallel queries means DNS mask concurrent connections limit this is less of an",
    "start": "311569",
    "end": "318289"
  },
  {
    "text": "issue now that core DNS has become the default but if you're running cube DNS",
    "start": "318289",
    "end": "324110"
  },
  {
    "text": "you might still run into this the default is 150 connections concurrently to a single DNS mask instance there",
    "start": "324110",
    "end": "331909"
  },
  {
    "text": "could be cloud provider limits that allow you to do only a certain number of queries from a node and if you're doing",
    "start": "331909",
    "end": "339349"
  },
  {
    "text": "more than those those might be dropped as well causing the same latency problem and finally this is actually a reason",
    "start": "339349",
    "end": "347240"
  },
  {
    "text": "rather than a separate point since UDP is a an unreliable protocol if packets",
    "start": "347240",
    "end": "352550"
  },
  {
    "text": "are dropped somewhere the client doesn't know whether the packet is really dropped or coming back really slow so",
    "start": "352550",
    "end": "357770"
  },
  {
    "text": "it's gonna give the benefit of doubt and wait the entire time out so that's by default five seconds so these are some",
    "start": "357770",
    "end": "365569"
  },
  {
    "text": "of the reasons that we might be seeing latency DNS latency in kubernetes clusters now even without node local",
    "start": "365569",
    "end": "373189"
  },
  {
    "text": "cache there are a bunch of solutions or workarounds that you can that can help in this case in these cases and let's",
    "start": "373189",
    "end": "380150"
  },
  {
    "start": "379000",
    "end": "496000"
  },
  {
    "text": "look at those the first one is using an option called single request reopen what",
    "start": "380150",
    "end": "387080"
  },
  {
    "text": "this option does is it tells the resolver to issue sequential queries rather than all at once and by doing",
    "start": "387080",
    "end": "395240"
  },
  {
    "text": "this you're ready the number of parallel queries do note that not all DNS resolver libraries",
    "start": "395240",
    "end": "401300"
  },
  {
    "text": "support this option the muscle implementation that Alpine uses doesn't support this specifically but that this",
    "start": "401300",
    "end": "409340"
  },
  {
    "text": "is one way to reduce the parallelism in the queries then there are other options that reduce the number of queries itself",
    "start": "409340",
    "end": "415220"
  },
  {
    "text": "reducing your n dots in dots is another DNS option that you can set to a lower",
    "start": "415220",
    "end": "421460"
  },
  {
    "text": "value this basically disables the search path expansion that we talked about earlier so it says if I have certain",
    "start": "421460",
    "end": "428840"
  },
  {
    "text": "number of dots in my query already don't even bother trying to expand it into ten queries just give me what I asked for",
    "start": "428840",
    "end": "435800"
  },
  {
    "text": "just look look up what I asked for running cube DNS DNS maskers daemon sets",
    "start": "435800",
    "end": "441229"
  },
  {
    "text": "to spread the load that's another solution you can try again these are some more workarounds at the",
    "start": "441229",
    "end": "447710"
  },
  {
    "text": "time and modifying DNS mash parameters to increase the number of connection limit and then using a more reliable",
    "start": "447710",
    "end": "455539"
  },
  {
    "text": "protocol using TCP for your DNS connections with the use vc option again I haven't had too much luck with this",
    "start": "455539",
    "end": "462620"
  },
  {
    "text": "being consistent so again it might work depending on what client images you use and another one I want to point out it's",
    "start": "462620",
    "end": "470330"
  },
  {
    "text": "relatively new I know how to path plug in in core DNS this basically still",
    "start": "470330",
    "end": "475729"
  },
  {
    "text": "gives you the search path expansion feature but it moves that logic to the server so the client still requests only",
    "start": "475729",
    "end": "482779"
  },
  {
    "text": "a small only a single or two queries but all the search expansion happens in the server so you reduce the parallelism",
    "start": "482779",
    "end": "489169"
  },
  {
    "text": "again so these are some of the workaround so let's see how local DNS",
    "start": "489169",
    "end": "494330"
  },
  {
    "text": "fits in here and here I have a diagram of how the workflow would look like and where the",
    "start": "494330",
    "end": "501110"
  },
  {
    "start": "496000",
    "end": "624000"
  },
  {
    "text": "cache fits in I can mention you have the gray box which is a node and we have a caching agent that runs one per load so",
    "start": "501110",
    "end": "508310"
  },
  {
    "text": "yeah within the blue box here that's the local DNS cache it gets its own",
    "start": "508310",
    "end": "513349"
  },
  {
    "text": "interface and I know it's exclusive IP address a link local IP that it listens",
    "start": "513349",
    "end": "518719"
  },
  {
    "text": "on and requests coming to that IP will be handle by the local cash when you enable this",
    "start": "518719",
    "end": "525029"
  },
  {
    "text": "add-on you also the cubelet flag to change the cluster DNS is automatically",
    "start": "525029",
    "end": "530639"
  },
  {
    "text": "done for you so that's the flag that tells cubelet to write the name server in the pods so it will now write this",
    "start": "530639",
    "end": "538439"
  },
  {
    "text": "link local IP in the pods resolve account so pods will talk to the node local",
    "start": "538439",
    "end": "543449"
  },
  {
    "text": "cache rather than cube DNS service now let's see what happens when the client",
    "start": "543449",
    "end": "549689"
  },
  {
    "text": "tries to look up some some hostname if you look at the lines in green it can",
    "start": "549689",
    "end": "556110"
  },
  {
    "text": "send a request via TCP or UDP UDP is by default it hits the local cache and if",
    "start": "556110",
    "end": "561569"
  },
  {
    "text": "there is a cache hit the response goes back to the client quad directly and I",
    "start": "561569",
    "end": "566759"
  },
  {
    "text": "want to point out that connections between the local cache and the client pods are not tracked meaning they do not",
    "start": "566759",
    "end": "572850"
  },
  {
    "text": "occupy entries in the connection tracking table so you bypass the net filter layer for these connections so if",
    "start": "572850",
    "end": "579899"
  },
  {
    "text": "you have a cache hit you don't have to touch the network address translation code at all called paths at all however",
    "start": "579899",
    "end": "586410"
  },
  {
    "text": "if there is a cache miss which is the arrow in the bottom you will go to no",
    "start": "586410",
    "end": "591809"
  },
  {
    "text": "local cache store but no local cache we will talk to the chip DNS service IP to",
    "start": "591809",
    "end": "597899"
  },
  {
    "text": "get the authoritative answer back and this does require address translation but it uses TCP rather than UDP which is",
    "start": "597899",
    "end": "605790"
  },
  {
    "text": "the more reliable protocol and we also reuse connections so you're not going to",
    "start": "605790",
    "end": "611069"
  },
  {
    "text": "create one connection entry for every request that the pod made that wasn't in the cache so this is how local fitting",
    "start": "611069",
    "end": "620660"
  },
  {
    "text": "let's see how this improves the problems we listed earlier so now the too many",
    "start": "620660",
    "end": "628230"
  },
  {
    "text": "queries problem is fixed to many parallel queries problem is fixed",
    "start": "628230",
    "end": "633540"
  },
  {
    "text": "because you're not doing connection tracking so the the tools queries are not filling up the tracking table they",
    "start": "633540",
    "end": "639990"
  },
  {
    "text": "are also being cached so they are not going out to cube DNS all the time there is no net filter",
    "start": "639990",
    "end": "645970"
  },
  {
    "text": "address translation code so the anything could potentially cause there is this out of the picture nearness mask",
    "start": "645970",
    "end": "653649"
  },
  {
    "text": "concurrent connections limit same thing reduce number of queries external queries go directly to the node resolver",
    "start": "653649",
    "end": "659589"
  },
  {
    "text": "they don't even have to go to class ready NS and piggybacking on that same point since external queries go directly",
    "start": "659589",
    "end": "666370"
  },
  {
    "text": "to the node resolver from that node you're able to better utilize any node specific limits there might be for DNS",
    "start": "666370",
    "end": "673480"
  },
  {
    "text": "queries and finally we talked about UDP so we are upgrading the connections to",
    "start": "673480",
    "end": "679720"
  },
  {
    "text": "TCP so that's taken care of as well these are some of the performance",
    "start": "679720",
    "end": "685750"
  },
  {
    "text": "benefits you get by enabling node local DNS cache but that's not all you get",
    "start": "685750",
    "end": "691360"
  },
  {
    "text": "extra visibility and metrics as well no local DNS uses core DNS as a cache so it",
    "start": "691360",
    "end": "699550"
  },
  {
    "start": "693000",
    "end": "768000"
  },
  {
    "text": "uses a cache plug-in the forward plug-in and few other plugins from Co DNS so all",
    "start": "699550",
    "end": "705310"
  },
  {
    "text": "the metrics that Cody and as plugins export are available and the best part is they're available on a per node basis",
    "start": "705310",
    "end": "711459"
  },
  {
    "text": "so you can get insight into what your DNS request patterns look like on a",
    "start": "711459",
    "end": "717339"
  },
  {
    "text": "personal basis and maybe you can pinpoint that to certain pods on your node as well so you get that extra",
    "start": "717339",
    "end": "724240"
  },
  {
    "text": "granular inside here are just some of the stats I put out there this is just",
    "start": "724240",
    "end": "730089"
  },
  {
    "text": "the tip of the iceberg there are several more in the cache plugin you see cache hits misses in the forward plug-in which",
    "start": "730089",
    "end": "736389"
  },
  {
    "text": "sends the request to the upstream you can see what the request counter is what their latency distribution looks like",
    "start": "736389",
    "end": "742709"
  },
  {
    "text": "you can also see this on a per zone basis so you get separate stats for your",
    "start": "742709",
    "end": "747940"
  },
  {
    "text": "cluster domain you get separate stats for external queries which is nice to see the distribution of latency",
    "start": "747940",
    "end": "753970"
  },
  {
    "text": "particularly okay hopefully this all sounds good in theory but does it",
    "start": "753970",
    "end": "761110"
  },
  {
    "text": "actually work and to cover that I would like to invite Blake to talk more so a",
    "start": "761110",
    "end": "770560"
  },
  {
    "text": "post mates we have been running kubernetes for a little over two and a half years and we've hit all of the",
    "start": "770560",
    "end": "778120"
  },
  {
    "text": "that were listed around DNS we initially tried to mitigate this problem by",
    "start": "778120",
    "end": "784660"
  },
  {
    "text": "deploying cute DNS as a daemon set it works fine it does require modifying",
    "start": "784660",
    "end": "790990"
  },
  {
    "text": "Kubla tarts to point the local node at its at the resolver but the problem with",
    "start": "790990",
    "end": "798310"
  },
  {
    "text": "it is if you're running just cube DNS as a daemon set all of those daemon set pods are watching all pods via the API",
    "start": "798310",
    "end": "805960"
  },
  {
    "text": "server and over time it becomes pretty resource intensive so we wanted a little",
    "start": "805960",
    "end": "812470"
  },
  {
    "text": "more streamlined solution to this and we were very excited when I found out about",
    "start": "812470",
    "end": "818440"
  },
  {
    "text": "the development for this project so we started testing it right away",
    "start": "818440",
    "end": "823860"
  },
  {
    "text": "just some stress testing scenarios that we did we most of them target a single",
    "start": "824620",
    "end": "830710"
  },
  {
    "text": "cluster domain service the community's default service so that we eliminate any",
    "start": "830710",
    "end": "837120"
  },
  {
    "text": "you know additional variables in the system uses confirm DNS which is written",
    "start": "837120",
    "end": "844300"
  },
  {
    "text": "by Justin Santa Barbara the if you don't know he's instigated a BOS and does cops",
    "start": "844300",
    "end": "853390"
  },
  {
    "text": "as them he's the main maintainer on cops he he was nice enough to provide us this",
    "start": "853390",
    "end": "859690"
  },
  {
    "text": "code and we did a whole bunch of different scenarios testing this so his",
    "start": "859690",
    "end": "864820"
  },
  {
    "text": "each of the confirmed eNOS pods send about 200 queries per second 100 of a",
    "start": "864820",
    "end": "871540"
  },
  {
    "text": "records 100 of quad-a records and so most of our tests deployed about 240",
    "start": "871540",
    "end": "877450"
  },
  {
    "text": "pods on fairly small clusters just to get a controlled environment to stress",
    "start": "877450",
    "end": "883209"
  },
  {
    "text": "these out we did this on various providers and it was all pretty similar on all of them so this shows how easy it",
    "start": "883209",
    "end": "892150"
  },
  {
    "text": "is to trigger the contract overflow especially on smaller node types like a default decaying instance type or",
    "start": "892150",
    "end": "899140"
  },
  {
    "text": "something any anything on any managed provider usually they're quite small to begin with",
    "start": "899140",
    "end": "905660"
  },
  {
    "text": "and the difference with no vocal is pretty dramatic you can see all of these are five-second timeouts I'm filtering",
    "start": "905660",
    "end": "911510"
  },
  {
    "text": "for anything above 100 milliseconds and there are a lot of five-second timeouts",
    "start": "911510",
    "end": "919180"
  },
  {
    "text": "this is the same cluster with no political cash deployed you can see there's a few minor outliers but by far",
    "start": "919180",
    "end": "927320"
  },
  {
    "text": "most most queries are under 5 milliseconds this is a similar test this",
    "start": "927320",
    "end": "935960"
  },
  {
    "text": "one used a range of DNS names that triggered search paths sir sorry search",
    "start": "935960",
    "end": "943820"
  },
  {
    "text": "path expansion and a bunch of external queries this results in hitting the contract",
    "start": "943820",
    "end": "951710"
  },
  {
    "text": "limit very quickly because of the explosion of queries from all those different types and you can see on the",
    "start": "951710",
    "end": "959150"
  },
  {
    "text": "right side there after we deployed the local DNS it's really improved latency",
    "start": "959150",
    "end": "966610"
  },
  {
    "text": "this is a similar test just a different view of the distribution of latency and",
    "start": "966610",
    "end": "973700"
  },
  {
    "text": "you can see none there are no queries over 16 milliseconds and 60 milliseconds",
    "start": "973700",
    "end": "979010"
  },
  {
    "text": "is an extreme outlier it's also important to note that on much larger",
    "start": "979010",
    "end": "985550"
  },
  {
    "text": "nodes you probably won't hit contract but we do still see a lot of five-second",
    "start": "985550",
    "end": "992120"
  },
  {
    "text": "timeouts from either in that filter race conditions need to be packet loss there's a range of possible problems",
    "start": "992120",
    "end": "1000820"
  },
  {
    "text": "here and we sidestep almost all them using this so we deployed with local",
    "start": "1000820",
    "end": "1011230"
  },
  {
    "text": "cache it was great we solved almost all of the problems with the cluster queries",
    "start": "1011230",
    "end": "1018090"
  },
  {
    "text": "we did notice however that external queries were a little bit erratic",
    "start": "1018090",
    "end": "1023680"
  },
  {
    "text": "a ton of cash misses because the queries were not happening happening quickly",
    "start": "1023680",
    "end": "1028720"
  },
  {
    "text": "enough or just a range of external queries so they weren't getting a lot of",
    "start": "1028720",
    "end": "1034808"
  },
  {
    "text": "cash hits we made use of the create the prefetch plugin for Cortinas which is",
    "start": "1034809",
    "end": "1042779"
  },
  {
    "text": "quite effective at reducing the overall latency there you can see the dramatic change once we enabled that option and",
    "start": "1042779",
    "end": "1050950"
  },
  {
    "text": "if you have questions about that I can answer that after Wow this shows our",
    "start": "1050950",
    "end": "1056950"
  },
  {
    "text": "production query the production cluster we have around 50,000 queries per second",
    "start": "1056950",
    "end": "1063279"
  },
  {
    "text": "at peak and the nodes are really large",
    "start": "1063279",
    "end": "1068679"
  },
  {
    "text": "there I believe 64 core instances and we have",
    "start": "1068679",
    "end": "1075130"
  },
  {
    "text": "some very large workloads running on them with extremely high throughput the",
    "start": "1075130",
    "end": "1080380"
  },
  {
    "text": "core DNS is the cluster DNS service and you can see that all of the latency there is sub millisecond which is really",
    "start": "1080380",
    "end": "1087789"
  },
  {
    "text": "a huge improvement over our previous set up and you can only see the top four",
    "start": "1087789",
    "end": "1094000"
  },
  {
    "text": "notes there but there's I believe around 120 nodes in that cluster all very large",
    "start": "1094000",
    "end": "1101830"
  },
  {
    "text": "nodes and a back to Pavitra okay thank",
    "start": "1101830",
    "end": "1109149"
  },
  {
    "start": "1104000",
    "end": "1199000"
  },
  {
    "text": "you so much Nick I want to point out that Blake was one of the first users of our feature and he's given great",
    "start": "1109149",
    "end": "1116440"
  },
  {
    "text": "feedback from the time we released it and I'm so glad we could present it at a",
    "start": "1116440",
    "end": "1122080"
  },
  {
    "text": "wider audience today so yeah hopefully now you're excited or interested in trying this feature out in your clusters",
    "start": "1122080",
    "end": "1128770"
  },
  {
    "text": "so how can you run it so here are some of the steps you can follow to run it if you are using the e2e scripts to bring",
    "start": "1128770",
    "end": "1135460"
  },
  {
    "text": "up your cluster you can use the command up there in the first option there",
    "start": "1135460",
    "end": "1141010"
  },
  {
    "text": "it works for 1.13 and above clusters you can run it that way but on any environment on any cluster you can use",
    "start": "1141010",
    "end": "1148000"
  },
  {
    "text": "the Yama the links here to do just a cube cuddle apply that lower link is the newer yama",
    "start": "1148000",
    "end": "1153830"
  },
  {
    "text": "and you can you can try the one but you wouldn't need to do the cluster dns",
    "start": "1153830",
    "end": "1159919"
  },
  {
    "text": "change its a flag to cubelet which you will need to modify so that pods",
    "start": "1159919",
    "end": "1165169"
  },
  {
    "text": "automatically start using your local cache but if you want to keep the",
    "start": "1165169",
    "end": "1171289"
  },
  {
    "text": "current setup and cannot change cluster DNS flag for any reason you can still use it on an existing cluster and just",
    "start": "1171289",
    "end": "1178700"
  },
  {
    "text": "in Santa Barbara I and wrote some scripts that you can use to make this happen so do check out the link and the",
    "start": "1178700",
    "end": "1185629"
  },
  {
    "text": "pull requests there or you can reach out to us if you have more questions on how to run this this is all of the ways you",
    "start": "1185629",
    "end": "1191659"
  },
  {
    "text": "can try out the feature and it would be great if you can provide your feedback after you try to so now the only section",
    "start": "1191659",
    "end": "1198620"
  },
  {
    "text": "we have is what's coming in the future we release the feature in alpha in 1.13",
    "start": "1198620",
    "end": "1204259"
  },
  {
    "start": "1199000",
    "end": "1277000"
  },
  {
    "text": "so we are graduating it to beta in 115 and yeah we did like I mentioned we got",
    "start": "1204259",
    "end": "1210559"
  },
  {
    "text": "great feedback from some of our users so it would be great to get more and the",
    "start": "1210559",
    "end": "1215870"
  },
  {
    "text": "two items that we are working on or proposals are in progress are each a and",
    "start": "1215870",
    "end": "1221480"
  },
  {
    "text": "auto path so the one question that I got after folks tried out the feature is",
    "start": "1221480",
    "end": "1227750"
  },
  {
    "text": "like hey what about high availability for my DNS now no local DNS introduces a",
    "start": "1227750",
    "end": "1232759"
  },
  {
    "text": "single point of failure what if it's down what do I do so I don't want to start by saying that",
    "start": "1232759",
    "end": "1238309"
  },
  {
    "text": "this is a common problem for any node specific agent I take Kuebler take your",
    "start": "1238309",
    "end": "1243529"
  },
  {
    "text": "proxy it's one per node with the same H a or non H a story so to some extent you",
    "start": "1243529",
    "end": "1251000"
  },
  {
    "text": "could you consider your node as you know failure domain so anything fails its contained within that domain building",
    "start": "1251000",
    "end": "1257000"
  },
  {
    "text": "that node however I understand there can be scenarios where DNS downtime cannot",
    "start": "1257000",
    "end": "1263690"
  },
  {
    "text": "be tolerated and for those we have a few options one of them is to run to daemon sets instead of one and use the same IP",
    "start": "1263690",
    "end": "1271519"
  },
  {
    "text": "address so whichever one is up can answer queries we have another option",
    "start": "1271519",
    "end": "1277039"
  },
  {
    "text": "that I would go into some detail about and this is what this will let you keep the exit",
    "start": "1277039",
    "end": "1282169"
  },
  {
    "text": "seeing single diamond set approach but we can do some iptables drinks to",
    "start": "1282169",
    "end": "1287799"
  },
  {
    "text": "provide a chain so let's take a quick look on how this works I have the link",
    "start": "1287799",
    "end": "1293720"
  },
  {
    "text": "to the cap here which has a full detail I'm gonna go through it a little bit fast so the picture is still similar to",
    "start": "1293720",
    "end": "1302239"
  },
  {
    "text": "what we had seen before you still have the node you have the per node cache but if you notice it listens on the link",
    "start": "1302239",
    "end": "1307940"
  },
  {
    "text": "local IP it listens on an additional service IP and not just any service IP but it's the cube DNS service IP so what",
    "start": "1307940",
    "end": "1316129"
  },
  {
    "text": "this lets you do is the client pods can talk to node local cache without",
    "start": "1316129",
    "end": "1321230"
  },
  {
    "text": "changing anything on their side so you don't have to change their cluster DNS it's already pointing to the dart and so",
    "start": "1321230",
    "end": "1327200"
  },
  {
    "text": "they hit the cluster do they hit the node local and we will install custom IP tables rules so that if no local DNS is",
    "start": "1327200",
    "end": "1334340"
  },
  {
    "text": "up and running queries will go to no nor local if it is not then those custom",
    "start": "1334340",
    "end": "1340129"
  },
  {
    "text": "rules will be removed which will result in the normal behavior which is the service IP and then going to the cube",
    "start": "1340129",
    "end": "1345950"
  },
  {
    "text": "DNS pods behind so how do we decide that so the only other thing is then what",
    "start": "1345950",
    "end": "1351590"
  },
  {
    "text": "happens when no local needs to talk to cube DNS on a cache miss what IP does it use so we create a second service with",
    "start": "1351590",
    "end": "1359509"
  },
  {
    "text": "the same selectors has cube DNS service so it is backed by the same cube DNS",
    "start": "1359509",
    "end": "1365090"
  },
  {
    "text": "pods this is this new service IP is what no local will talk to when it needs to",
    "start": "1365090",
    "end": "1371029"
  },
  {
    "text": "fetch entries for a cache miss so the one question here who decides when to",
    "start": "1371029",
    "end": "1377629"
  },
  {
    "text": "add the custom rules to route tune or local when to remove them and send it to cube DNS that part is a little bit of a",
    "start": "1377629",
    "end": "1385009"
  },
  {
    "text": "TBD but you can put this logic in the same part as a sidecar container or you",
    "start": "1385009",
    "end": "1391909"
  },
  {
    "text": "can put it in any other demon set that is running host networking that's already doing a bunch of iptables",
    "start": "1391909",
    "end": "1397789"
  },
  {
    "text": "manipulation maybe but that would essentially have to ping the link local IP to see if no local is alive and if it",
    "start": "1397789",
    "end": "1405919"
  },
  {
    "text": "is keep the rules if it is not remove the rules so the packets go to the cube",
    "start": "1405919",
    "end": "1411710"
  },
  {
    "text": "DNS service which is the natural fallback we want anyway one caveat is that this",
    "start": "1411710",
    "end": "1417210"
  },
  {
    "text": "approach wouldn't work for IP vs mode of cube proxy because of the way Q the IP",
    "start": "1417210",
    "end": "1423510"
  },
  {
    "text": "vs called it creates its own interface and binds the IP so we cannot reuse that",
    "start": "1423510",
    "end": "1428670"
  },
  {
    "text": "trick here but this is certainly an option you can look into if H a is a concern and I have the link there for",
    "start": "1428670",
    "end": "1435750"
  },
  {
    "text": "the more details they that it's captured in the Kappa the one other feature we",
    "start": "1435750",
    "end": "1443850"
  },
  {
    "text": "are working on is auto path and I want to take a few minutes to talk about this we discussed what out of path is it is",
    "start": "1443850",
    "end": "1452340"
  },
  {
    "start": "1445000",
    "end": "1527000"
  },
  {
    "text": "the idea of moving the search path expansion logic from the client to the",
    "start": "1452340",
    "end": "1457800"
  },
  {
    "text": "server so why do we need Auto path or why we need these search paths is because you want to be able to discover",
    "start": "1457800",
    "end": "1464970"
  },
  {
    "text": "other services using their short name say you created a my new service in the default namespace you would like to",
    "start": "1464970",
    "end": "1472020"
  },
  {
    "text": "resolve it by just saying my new service instead of my new service default or",
    "start": "1472020",
    "end": "1477480"
  },
  {
    "text": "service or cluster or local you want to use the short names or you want to use named or namespace to find these service",
    "start": "1477480",
    "end": "1483960"
  },
  {
    "text": "IP addresses so that's what the search paths let you do so if you remove it",
    "start": "1483960",
    "end": "1489330"
  },
  {
    "text": "you're losing that ability so with Auto path you get to keep that ability but you get to reduce the number of parallel",
    "start": "1489330",
    "end": "1495480"
  },
  {
    "text": "queries on the client side as well so yeah I went through most of the benefits",
    "start": "1495480",
    "end": "1500850"
  },
  {
    "text": "here the other benefits of this proposal are that you can extend it to any domain",
    "start": "1500850",
    "end": "1506460"
  },
  {
    "text": "that has search paths even outside of kubernetes it can be applied anywhere",
    "start": "1506460",
    "end": "1512340"
  },
  {
    "text": "and also I want to point out that core DNS auto path plug-in already supports",
    "start": "1512340",
    "end": "1517680"
  },
  {
    "text": "this today but the proposal we are Pro we are putting forth is less",
    "start": "1517680",
    "end": "1522690"
  },
  {
    "text": "resource-intensive and the details you can find there but I will go through quickly what the new workflow will look",
    "start": "1522690",
    "end": "1529380"
  },
  {
    "start": "1527000",
    "end": "1570000"
  },
  {
    "text": "like with this let's say we introduce a new mod and all these names are TBD it's",
    "start": "1529380",
    "end": "1534930"
  },
  {
    "text": "in progress so but we will introduce a new mod say cluster first with Auto path and in this",
    "start": "1534930",
    "end": "1540510"
  },
  {
    "text": "mode cubelet will create a single custom search paths instead of the fibers for whatever search parts it",
    "start": "1540510",
    "end": "1547710"
  },
  {
    "text": "creates it will be just a single one and that will have the namespace and the",
    "start": "1547710",
    "end": "1552809"
  },
  {
    "text": "suffix encoder in it because depending on the part you're running on your search path is going to be different so",
    "start": "1552809",
    "end": "1558629"
  },
  {
    "text": "this is the search path that will be in the pods result or column now what",
    "start": "1558629",
    "end": "1563820"
  },
  {
    "text": "happens when the pod looks up an IP or looks up a domain name let us take a",
    "start": "1563820",
    "end": "1569609"
  },
  {
    "text": "look now when a client pod looks up a hostname say foo.com the search bar",
    "start": "1569609",
    "end": "1575129"
  },
  {
    "start": "1570000",
    "end": "1688000"
  },
  {
    "text": "expansion kicks in and that's the domain that is going to be sent to no local no",
    "start": "1575129",
    "end": "1580859"
  },
  {
    "text": "local cache we'll have the ability to recognize this custom pattern and extract namespace and cluster suffix",
    "start": "1580859",
    "end": "1587879"
  },
  {
    "text": "from it and once it has both these variables it knows to construct what does our new list of search paths need",
    "start": "1587879",
    "end": "1593999"
  },
  {
    "text": "to be and once it constructs this it can put it as a needy and a zero option it's",
    "start": "1593999",
    "end": "1599609"
  },
  {
    "text": "an extended dns value in the in the request packet key value so we are going",
    "start": "1599609",
    "end": "1605249"
  },
  {
    "text": "to introduce a new option number and we're going to put the list of search paths as the value and if the cluster",
    "start": "1605249",
    "end": "1611249"
  },
  {
    "text": "dns knows to recognize this key value option it can look at the search paths",
    "start": "1611249",
    "end": "1617190"
  },
  {
    "text": "and fan out into multiple queries so that's that's what the last green box is",
    "start": "1617190",
    "end": "1622379"
  },
  {
    "text": "doing there it gets the cluster all of the search path from the node local in",
    "start": "1622379",
    "end": "1628109"
  },
  {
    "text": "the packet just needs to look at it append all of them send out the queries return anything that had a non-empty",
    "start": "1628109",
    "end": "1635580"
  },
  {
    "text": "answer so that is how this would work and you don't need to watch any",
    "start": "1635580",
    "end": "1641460"
  },
  {
    "text": "resources in order to figure out what your such paths need to be and you've moved the search path logic outside of",
    "start": "1641460",
    "end": "1647249"
  },
  {
    "text": "the client so this is one other thing that's in progress like I said feel free",
    "start": "1647249",
    "end": "1652649"
  },
  {
    "text": "to comment on the cab feedback would be great I believe this is the end of what",
    "start": "1652649",
    "end": "1659399"
  },
  {
    "text": "we had here so I hope this was useful and got you excited to try it out on",
    "start": "1659399",
    "end": "1664469"
  },
  {
    "text": "your cluster and I have the test that we talked about I have it running on my cluster if you want to take a look after",
    "start": "1664469",
    "end": "1670440"
  },
  {
    "text": "I'll be around for questions we'll both be around for questions but I have the set up also",
    "start": "1670440",
    "end": "1676179"
  },
  {
    "text": "you wanna take a look now we have time for some questions [Applause] [Music]",
    "start": "1676179",
    "end": "1683990"
  },
  {
    "text": "thank you for the presentation my question is regarding the details for",
    "start": "1687980",
    "end": "1693299"
  },
  {
    "start": "1688000",
    "end": "2078000"
  },
  {
    "text": "the DNS cache and if that is quite long then we could get there is a risk that",
    "start": "1693299",
    "end": "1699030"
  },
  {
    "text": "the application could get like an old version for the DNS and that would impact some timeouts on the request what",
    "start": "1699030",
    "end": "1708480"
  },
  {
    "text": "do you think how do you think we could go around that are you talking about timeouts for an X domain exam of your",
    "start": "1708480",
    "end": "1717270"
  },
  {
    "text": "application like what what call the local DNS cache it would get like an old",
    "start": "1717270",
    "end": "1723120"
  },
  {
    "text": "version for another hat right if it tries to reach it it's it's not there",
    "start": "1723120",
    "end": "1729419"
  },
  {
    "text": "after for example the company this new deployment or something okay yeah yeah",
    "start": "1729419",
    "end": "1735090"
  },
  {
    "text": "which so we do set the timeout to a low value for for the negative domains so",
    "start": "1735090",
    "end": "1741419"
  },
  {
    "text": "that if it's a case that if a client looked it up and we cashed it as no no",
    "start": "1741419",
    "end": "1746850"
  },
  {
    "text": "host available but the entry actually came up you don't want to run into the case where that's not reflected so for",
    "start": "1746850",
    "end": "1752910"
  },
  {
    "text": "negative answers we do set that value to a low number but I'm not sure if that's answering the question kind of like we",
    "start": "1752910",
    "end": "1762720"
  },
  {
    "text": "said it like yeah we said you'd do a low number for a scenario which might be something out of what you described that",
    "start": "1762720",
    "end": "1770490"
  },
  {
    "text": "is part of the Yammer we set it to a low value hello first thank you very much I",
    "start": "1770490",
    "end": "1779340"
  },
  {
    "text": "still keep really bad memories of was long weeks trying to debug DNS latency in my clusters and it's really nice to",
    "start": "1779340",
    "end": "1786030"
  },
  {
    "text": "see a technical implementation of a solution for that so thank you the second thing is a question we we do have",
    "start": "1786030",
    "end": "1793410"
  },
  {
    "text": "a lot of latency sensitive applications in our clusters and one of the challenge we see is also the I think you haven't I",
    "start": "1793410",
    "end": "1800730"
  },
  {
    "text": "think you have an answer for this but one of the challenge we see is that DNS",
    "start": "1800730",
    "end": "1805740"
  },
  {
    "text": "resolution for most HTTP libraries is on the critical path and the TTL for",
    "start": "1805740",
    "end": "1812070"
  },
  {
    "text": "community services is five seconds so every five seconds you need to actually make the full DNS the cache will expire",
    "start": "1812070",
    "end": "1817980"
  },
  {
    "text": "it will be a cache miss and you need to do the DNS resolution and please take some time and you do",
    "start": "1817980",
    "end": "1823480"
  },
  {
    "text": "this every five seconds for an IP but barely changes is there a way to do some",
    "start": "1823480",
    "end": "1828640"
  },
  {
    "text": "prefetching behind the scene or something like this you mentioned prefetch before so I'm really hoping",
    "start": "1828640",
    "end": "1834520"
  },
  {
    "text": "about the solution so yeah that's my question okay do you want to present",
    "start": "1834520",
    "end": "1842070"
  },
  {
    "text": "so yeah the prefetching will work for any of the you can set it up for clustered domains also but by default it",
    "start": "1842070",
    "end": "1849760"
  },
  {
    "text": "is at least the way I showed it we set it up only for external queries because",
    "start": "1849760",
    "end": "1855190"
  },
  {
    "text": "that's where it metals matters the most for us but you could configure it any way you'd like with you you can tweak",
    "start": "1855190",
    "end": "1862000"
  },
  {
    "text": "the settings we have it set quite liberally so that we always get the most popular queries pre fetched you can have",
    "start": "1862000",
    "end": "1869500"
  },
  {
    "text": "it a little less aggressive and have it so that your applications have kind of a",
    "start": "1869500",
    "end": "1876120"
  },
  {
    "text": "specific configuration there's also the",
    "start": "1876120",
    "end": "1881860"
  },
  {
    "text": "the cache settings and coordinates are quite flexible you can set different cache for different subdomains if you",
    "start": "1881860",
    "end": "1887650"
  },
  {
    "text": "wanted to break it out that way and then you could also have different default",
    "start": "1887650",
    "end": "1895390"
  },
  {
    "text": "cache timeouts for for those different subdomains",
    "start": "1895390",
    "end": "1899880"
  },
  {
    "text": "hello and thank you again for the in very nice talk about the subject my",
    "start": "1906149",
    "end": "1911950"
  },
  {
    "text": "question is about Auto pass current of limitation always requires that you have",
    "start": "1911950",
    "end": "1918700"
  },
  {
    "text": "both verification enabled it sounds like your new approach may not have to do",
    "start": "1918700",
    "end": "1924370"
  },
  {
    "text": "with that anymore because and it is related with a footprint of coordinates in the system so can you maybe tell",
    "start": "1924370",
    "end": "1931000"
  },
  {
    "text": "myself it's more about it how basically only need to be enabling from now on",
    "start": "1931000",
    "end": "1937648"
  },
  {
    "text": "yeah I'm not I'll ask the question again because I'm not sure I got it I'll ask",
    "start": "1939960",
    "end": "1946000"
  },
  {
    "text": "you can tell me if I got it right are you asking how the new Auto path",
    "start": "1946000",
    "end": "1951669"
  },
  {
    "text": "compares to what the existing one in Karina's is yes especially regarding footprint of core DNS pods meaning that",
    "start": "1951669",
    "end": "1959320"
  },
  {
    "text": "the verification the pod verified setting is always memory no it's hogging",
    "start": "1959320",
    "end": "1965649"
  },
  {
    "text": "memory and seems that now there can't be a way out of it because of how you implement the other path into the local",
    "start": "1965649",
    "end": "1973620"
  },
  {
    "text": "right so the auto path as it works today in Korea Nestle yes there needs to be a",
    "start": "1973620",
    "end": "1979929"
  },
  {
    "text": "watch that's happening in order to map the pods IP to the pods namespace so it",
    "start": "1979929",
    "end": "1985059"
  },
  {
    "text": "has to watch the pod resource in order to do this but with the current approach we don't need to do that because we've",
    "start": "1985059",
    "end": "1991179"
  },
  {
    "text": "pushed it to the client to encode this information and sell it out so again I",
    "start": "1991179",
    "end": "1997659"
  },
  {
    "text": "think it's hard to pinpoint how much of the memory usage is through auto path unless you've had some benchmarking but",
    "start": "1997659",
    "end": "2003929"
  },
  {
    "text": "that Biff that you might be seeing shouldn't be there with this other approach because the only thing would be",
    "start": "2003929",
    "end": "2010019"
  },
  {
    "text": "that it'll still fan out into multiple queries and collect the answers but there shouldn't be something that's",
    "start": "2010019",
    "end": "2016139"
  },
  {
    "text": "growing as there are more endpoints in your service for instance thank you hello regarding the contract issue in",
    "start": "2016139",
    "end": "2023789"
  },
  {
    "text": "the kernel is there any issue that this be solved that point some point in the future it's just too hard to fix running",
    "start": "2023789",
    "end": "2031679"
  },
  {
    "text": "into the limit like too many entries there is a race condition in the colonel",
    "start": "2031679",
    "end": "2037460"
  },
  {
    "text": "there any hope that this can be fixed the link to the blog and yeah I don't",
    "start": "2037460",
    "end": "2043340"
  },
  {
    "text": "know as much as the authors of the blog but they did submit three patches I",
    "start": "2043340",
    "end": "2049398"
  },
  {
    "text": "believe to the colonel and two of them have been accepted in different kernel versions so fixes are definitely on the",
    "start": "2049399",
    "end": "2056030"
  },
  {
    "text": "way for that I did talk to them at the booth yesterday so affixes are",
    "start": "2056030",
    "end": "2061669"
  },
  {
    "text": "definitely under way for that thank you all right",
    "start": "2061669",
    "end": "2068570"
  },
  {
    "text": "okay thank you so much and enjoy the party everyone please",
    "start": "2068570",
    "end": "2073810"
  }
]