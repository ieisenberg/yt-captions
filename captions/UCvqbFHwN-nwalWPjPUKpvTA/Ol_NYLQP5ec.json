[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "so hi everyone just to make sure you're in the right place you're here for do as I say not as I do",
    "start": "30",
    "end": "5609"
  },
  {
    "text": "how we build our cute clusters and why you shouldn't I am Nikki oh I to",
    "start": "5609",
    "end": "12599"
  },
  {
    "text": "introduce myself I have been a sysadmin for 20 years now I've been on a lysines",
    "start": "12599",
    "end": "18029"
  },
  {
    "text": "cube team or then our last in for five years and on a licensed cube team since it started three years ago and yeah",
    "start": "18029",
    "end": "26250"
  },
  {
    "text": "we've been we've built a whole bunch of stuff in that time so it's some important things as we get started the",
    "start": "26250",
    "end": "32820"
  },
  {
    "text": "name of our team which will be relevant is the kubernetes infrastructure technology team you may notice there's a",
    "start": "32820",
    "end": "39120"
  },
  {
    "text": "redundant T there it solely exists so that we can make Nightrider pants for",
    "start": "39120",
    "end": "44370"
  },
  {
    "text": "all very pose and stuff yeah I firmly believe that whimsy is important in your",
    "start": "44370",
    "end": "51480"
  },
  {
    "text": "name is games so what I'd like to you to walk away with today firstly what we've",
    "start": "51480",
    "end": "58440"
  },
  {
    "start": "54000",
    "end": "73000"
  },
  {
    "text": "been trying for we're doing all around cube clusters what we actually did how",
    "start": "58440",
    "end": "66240"
  },
  {
    "text": "it went and lastly and the meat of the thing why you shouldn't do the same as us",
    "start": "66240",
    "end": "72439"
  },
  {
    "text": "okay so what we were trying for here is",
    "start": "72439",
    "end": "77520"
  },
  {
    "start": "73000",
    "end": "166000"
  },
  {
    "text": "what we were trying for from the start we wanted to build a set of clusters that invention could run 95 percent or",
    "start": "77520",
    "end": "84390"
  },
  {
    "text": "more of compute inside of us in so you know the numbers the numbers not a hundred percent but it's pretty close",
    "start": "84390",
    "end": "90240"
  },
  {
    "text": "and so that's the key thing here we wanted to make make the whole thing fit",
    "start": "90240",
    "end": "95430"
  },
  {
    "text": "as many use cases as possible inside a Lyceum why did we build our own firstly",
    "start": "95430",
    "end": "101960"
  },
  {
    "text": "the tools weren't ready three years ago the you was just the stand me up a",
    "start": "101960",
    "end": "108689"
  },
  {
    "text": "cluster tooling was very incomplete to build our clusters we literally started from kubernetes the hard way and you",
    "start": "108689",
    "end": "116759"
  },
  {
    "text": "know built from there so our clusters are you know it's close to artisinal as",
    "start": "116759",
    "end": "122610"
  },
  {
    "text": "you can get I would say secondly compliance we knew we needed a",
    "start": "122610",
    "end": "129179"
  },
  {
    "text": "compliance regime we know it's not that long ago that we up you'd know the company we've got sauce controls the",
    "start": "129179",
    "end": "134940"
  },
  {
    "text": "same as everybody we wanted to make sure that we could meet Alison's internal compliance regime which has a couple of",
    "start": "134940",
    "end": "140220"
  },
  {
    "text": "nifty things but we'll talk about is that that is for another day lastly if",
    "start": "140220",
    "end": "145710"
  },
  {
    "text": "just everything was still pretty new at the time and so we wanted to make sure that we knew the system as well as",
    "start": "145710",
    "end": "150810"
  },
  {
    "text": "possible and that was why we started for all that was why we started from basically cube the hard way was just to",
    "start": "150810",
    "end": "157050"
  },
  {
    "text": "make sure that we knew this whole platform you know every which way upside",
    "start": "157050",
    "end": "163170"
  },
  {
    "text": "down back to front so what we actually did well myself and one other guy were",
    "start": "163170",
    "end": "171270"
  },
  {
    "start": "166000",
    "end": "300000"
  },
  {
    "text": "the ones who actually did the initial design and so I started from my rule",
    "start": "171270",
    "end": "177180"
  },
  {
    "text": "zero design out the biggest problems you know about so you can find all the new and interesting ones later the this",
    "start": "177180",
    "end": "183510"
  },
  {
    "text": "certainly applied we picked we picked some problems that we knew would be a",
    "start": "183510",
    "end": "188820"
  },
  {
    "text": "problem based on our experience running the on-demand platform a lot of our team came from running the lysine on-demand",
    "start": "188820",
    "end": "195570"
  },
  {
    "text": "platform which used to run hosted confluence at hosted JIRA it was a",
    "start": "195570",
    "end": "200700"
  },
  {
    "text": "pretty big platform that used an early form of containerization in openvz and",
    "start": "200700",
    "end": "206370"
  },
  {
    "text": "so we had experience at running quite a big fleet of stuff and we wanted to make",
    "start": "206370",
    "end": "211650"
  },
  {
    "text": "sure that we took as many of the learnings that we had from that as possible and built them into this new platform so the problems we decided to",
    "start": "211650",
    "end": "217709"
  },
  {
    "text": "solve were firstly to manage blast radius this is the number one thing that we had learned running a big platform",
    "start": "217709",
    "end": "223410"
  },
  {
    "text": "earlier that you really needed to make sure that when you make changes you can make changes to some subset of your",
    "start": "223410",
    "end": "230459"
  },
  {
    "text": "stuff if you're if you're managing a full stack like we are then the having",
    "start": "230459",
    "end": "235739"
  },
  {
    "text": "that subset be manageable is super important and it lives and it frees you up keeping it orthogonal frees you up",
    "start": "235739",
    "end": "240780"
  },
  {
    "text": "from actually you know stepping on each other's toes when you're working together so what we actually did was",
    "start": "240780",
    "end": "246299"
  },
  {
    "text": "build a layer cake sort of you know based on your classic OSI model style things but that keeps strong isolation",
    "start": "246299",
    "end": "253170"
  },
  {
    "text": "between the between the layers with a you know a very clearly defined boundary so that then you know if I'm working on",
    "start": "253170",
    "end": "259470"
  },
  {
    "text": "the lowest layer someone else can work on the next layer up and someone else can work on the top layer we wanted to",
    "start": "259470",
    "end": "265289"
  },
  {
    "text": "really manage dependencies we knew that eventually 95% of computing",
    "start": "265289",
    "end": "270510"
  },
  {
    "text": "would be running on us so it was really important to us that we didn't have anything we didn't depend on anything run by a",
    "start": "270510",
    "end": "276300"
  },
  {
    "text": "lassie and that would end up running on us because cyclic dependencies are really hard they give you really",
    "start": "276300",
    "end": "281400"
  },
  {
    "text": "difficult bootstrap problems so we tried our best to keep them cut out the whole",
    "start": "281400",
    "end": "287010"
  },
  {
    "text": "time lastly cattle no pets we want to we embraced immutable infrastructure as",
    "start": "287010",
    "end": "292979"
  },
  {
    "text": "much as we could this I mean don't want to preamp too much but this has been a real saving",
    "start": "292979",
    "end": "298440"
  },
  {
    "text": "grace for us so the layer cake so the",
    "start": "298440",
    "end": "303570"
  },
  {
    "start": "300000",
    "end": "791000"
  },
  {
    "text": "first layer we called the flag that is a kit that is a Knight Rider acronym in",
    "start": "303570",
    "end": "309030"
  },
  {
    "text": "Knight Rider it was the foundation for law and government who funded the whole cool truck and everything so the",
    "start": "309030",
    "end": "315860"
  },
  {
    "text": "important part there is that that's the base AWS config so it has like VP sees",
    "start": "315860",
    "end": "321350"
  },
  {
    "text": "the relationships to vgw security groups stuff that we need to be there to build",
    "start": "321350",
    "end": "327600"
  },
  {
    "text": "the next layer but that we don't anticipate changing very often you know changing this changing this bottom layer",
    "start": "327600",
    "end": "332910"
  },
  {
    "text": "is a bit of a big deal it's a bit riskier than the other ones because you know it affects everything all the way",
    "start": "332910",
    "end": "337950"
  },
  {
    "text": "up the stack next up was the car one if you go watch and I rode I shouldn't need",
    "start": "337950",
    "end": "342990"
  },
  {
    "text": "to explain that one to you so this one has all the compute the control plane",
    "start": "342990",
    "end": "348030"
  },
  {
    "text": "the eightydaze basically all of the ec2 instances and associated shenanigans is",
    "start": "348030",
    "end": "354990"
  },
  {
    "text": "all in this layer the important defining part of this layer is that it stands up",
    "start": "354990",
    "end": "361830"
  },
  {
    "text": "an API server in point nothing else that API server is a completely unconsidered vanilla api server because we have all",
    "start": "361830",
    "end": "370440"
  },
  {
    "text": "the are back things turned on that means that when you first stand up one of these clusters this API server can do",
    "start": "370440",
    "end": "377010"
  },
  {
    "text": "nothing not as if you can't even get nodes because the nodes haven't even checked in yet because they're not allowed to because the arbok hasn't been",
    "start": "377010",
    "end": "383250"
  },
  {
    "text": "done that is the responsibility of the Goliath layer 'life was the truck so the",
    "start": "383250",
    "end": "391639"
  },
  {
    "text": "this contains all config that actually runs inside cube so that includes our",
    "start": "391639",
    "end": "396960"
  },
  {
    "text": "back the PSP's the initial cube network config all of the sort of a basic stuff that",
    "start": "396960",
    "end": "403410"
  },
  {
    "text": "paints a cluster and makes it a cluster that we are prepared to manage is in here plus all of the load all of the",
    "start": "403410",
    "end": "410010"
  },
  {
    "text": "system services that we provide out to our users because again the point of this platform is to be you know a",
    "start": "410010",
    "end": "415470"
  },
  {
    "text": "platform that other Atlassian users can use to deal what they're going to do and so we're trying to be as opinionated as",
    "start": "415470",
    "end": "421980"
  },
  {
    "text": "we can up to our demarcation point which is this which is this Goliath layer and",
    "start": "421980",
    "end": "427340"
  },
  {
    "text": "on top of that we try to be relatively unup Union ated and let people do they think they can come talk to us and say",
    "start": "427340",
    "end": "433200"
  },
  {
    "text": "hey you did this stuff all the time what do you think but we're not going to say you know no don't do that with some",
    "start": "433200",
    "end": "440250"
  },
  {
    "text": "exceptions ok so let's talk about cattle markets so the controllers and the nodes",
    "start": "440250",
    "end": "448430"
  },
  {
    "text": "we are an AWS shop so we the controllers",
    "start": "448430",
    "end": "453750"
  },
  {
    "text": "and the nodes of both created in OS cheese the controllers are cycled automatically when we update the launch",
    "start": "453750",
    "end": "459900"
  },
  {
    "text": "config by the ASG and terraform sorry I should have mentioned that the first two",
    "start": "459900",
    "end": "465330"
  },
  {
    "text": "layers a terraform the top layer Goliath is ansible rig so this one yeah so this",
    "start": "465330",
    "end": "472050"
  },
  {
    "text": "one when we say need to upgrade the way that we upgrade the control plane is we",
    "start": "472050",
    "end": "477360"
  },
  {
    "text": "update the cube version inside the terraform and push there from that's it",
    "start": "477360",
    "end": "483919"
  },
  {
    "text": "terraform handles are creating a new ASG mate they'll be health checks confirm",
    "start": "483919",
    "end": "489270"
  },
  {
    "text": "that the API servers have come up correctly and then it deposed is the old one it destroys it so for us it's a",
    "start": "489270",
    "end": "496080"
  },
  {
    "text": "relatively atomic operation to do a cube upgrade that's been really good when we have things like the recent severe meant",
    "start": "496080",
    "end": "502260"
  },
  {
    "text": "that we needed to deploy changes pretty quickly it's really straightforward to us to do the other thing that's good is",
    "start": "502260",
    "end": "507600"
  },
  {
    "text": "that the other nodes there cycle by the otoscope by an auto scaler",
    "start": "507600",
    "end": "512940"
  },
  {
    "text": "so if we want to upgrade their version we just changed their launch config and",
    "start": "512940",
    "end": "518010"
  },
  {
    "text": "then the next time that they die and come back and you get new nodes then those new nodes are the right version so",
    "start": "518010",
    "end": "524039"
  },
  {
    "text": "there's never we do because of the cattle no pets thing we do is zero in-place upgrades zero touching the",
    "start": "524039",
    "end": "529530"
  },
  {
    "text": "instance touching yes this is 100% any pattern the only reason you go on to the instance at all is to figure out some curly",
    "start": "529530",
    "end": "536879"
  },
  {
    "text": "problem that you don't have in your logs or or some Dockers gone weird or something like that and that has really",
    "start": "536879",
    "end": "543420"
  },
  {
    "text": "really saved us so much time and heartache in the long run it is not",
    "start": "543420",
    "end": "548759"
  },
  {
    "text": "funny a CD servers now in terms of cattle not pets I like to say they like",
    "start": "548759",
    "end": "554040"
  },
  {
    "text": "milk cows you know the name off you try and help them get better if they get",
    "start": "554040",
    "end": "559170"
  },
  {
    "text": "sick but there's a very definite time period after which you say look I'm sorry you know I got to buy a new milk",
    "start": "559170",
    "end": "566040"
  },
  {
    "text": "cow you are right the and so the yeah because the point there is you do got to",
    "start": "566040",
    "end": "572819"
  },
  {
    "text": "be a little bit careful with the air CDs because they are the source of truth but trying to keep them immutable is really",
    "start": "572819",
    "end": "581910"
  },
  {
    "text": "really helpful because it means it because they're the sort of truth you're never going to accidentally shoot yourself in the foot and so you again",
    "start": "581910",
    "end": "590009"
  },
  {
    "text": "this is this has actually been one of the hardest parts for us to get right in our setup it is really tricky to run a CD nodes in",
    "start": "590009",
    "end": "597389"
  },
  {
    "text": "an immutable fashion we have got some rules about it that I'm sorry I don't have too much time to come in go into",
    "start": "597389",
    "end": "602819"
  },
  {
    "text": "right now but grab me afterwards and I will be happy to run you through it",
    "start": "602819",
    "end": "608129"
  },
  {
    "text": "so yeah rebuilding originally our plan was that any significant cluster change",
    "start": "608129",
    "end": "613709"
  },
  {
    "text": "we will burn it down to the ground and rebuild it from scratch and so the tooling is designed to allow us to do",
    "start": "613709",
    "end": "618990"
  },
  {
    "text": "that in about 30 minutes so basically there's a build that we can do that is a",
    "start": "618990",
    "end": "625139"
  },
  {
    "text": "destroy build that will destroy the at the car level that will destroy all the compute and then we do a fresh deploy",
    "start": "625139",
    "end": "631589"
  },
  {
    "text": "and we'll get all the compute back that means however all the stuff that was in that CD is gone so as we've started to",
    "start": "631589",
    "end": "639420"
  },
  {
    "text": "take on loads that were less stateless and more stateful and start nave started",
    "start": "639420",
    "end": "644879"
  },
  {
    "text": "to persist data and NCD that they don't want to lose we needed to it's harder to",
    "start": "644879",
    "end": "650069"
  },
  {
    "text": "do that now right we've got to do a bit more shenanigans about a CD restores it is possible though so secrets here this",
    "start": "650069",
    "end": "659759"
  },
  {
    "text": "is talking about managing dependencies for these secrets I'm not talking about cube secrets I'm talking about the",
    "start": "659759",
    "end": "665069"
  },
  {
    "text": "initial secrets that you need to actually stand up the cubes so I would like things like the see a search for",
    "start": "665069",
    "end": "672240"
  },
  {
    "text": "the cluster the new this the search for the serving search for NCD because the",
    "start": "672240",
    "end": "678360"
  },
  {
    "text": "client search that you use to do a TD authentication because you better be doing entity authentication at this",
    "start": "678360",
    "end": "684509"
  },
  {
    "text": "point if you are not you could possibly be in for a very bad time the it is off",
    "start": "684509",
    "end": "690720"
  },
  {
    "text": "by default as well so you know things to remember and look at that so yeah we saw",
    "start": "690720",
    "end": "696870"
  },
  {
    "text": "the secrets in private s3 buckets there are only accessible to the nodes and obviously to ask this admins and then",
    "start": "696870",
    "end": "702779"
  },
  {
    "text": "there's a there's a net job that runs at startup on the nodes and it pulls all the secrets put them in the right place",
    "start": "702779",
    "end": "707939"
  },
  {
    "text": "and then it is a system to you that has that everything else is has a",
    "start": "707939",
    "end": "714629"
  },
  {
    "text": "requirement has that as a requirement so that yeah nothing happens until it's finished",
    "start": "714629",
    "end": "720470"
  },
  {
    "text": "similarly we use sorry a CR for image",
    "start": "720470",
    "end": "725850"
  },
  {
    "text": "storage we don't currently run our own private registry Atlassian does run in a",
    "start": "725850",
    "end": "731730"
  },
  {
    "text": "private registry but we don't we don't use it with the intent being that as I",
    "start": "731730",
    "end": "738089"
  },
  {
    "text": "said before there's a very good chance that that registry will end up running on top of us and we didn't want to be stuck in that situation at you know four",
    "start": "738089",
    "end": "745649"
  },
  {
    "text": "o'clock in the morning that all the doctor registry is down because the doctor registry is down I can't boot new",
    "start": "745649",
    "end": "750750"
  },
  {
    "text": "nodes because you know they need to pull the hypercube images from the docker",
    "start": "750750",
    "end": "755790"
  },
  {
    "text": "registry so what we do is we have some jobs that pull down specific versions of",
    "start": "755790",
    "end": "761459"
  },
  {
    "text": "the documents we're going to use it and push them into a CR hire my to-do list",
    "start": "761459",
    "end": "766500"
  },
  {
    "text": "when we get when I get back is to mess with that build a little bit and put all of the image and vulnerability scanning",
    "start": "766500",
    "end": "773399"
  },
  {
    "text": "stuff into that pipeline because I think that is a bit of a hole that a lot of people have right now and as we've seen",
    "start": "773399",
    "end": "779069"
  },
  {
    "text": "by the node thing and a bunch of other similar attacks that you know that's that supply chain integrity is a really",
    "start": "779069",
    "end": "786720"
  },
  {
    "text": "really important security hole that everybody needs to plug so how it went",
    "start": "786720",
    "end": "793819"
  },
  {
    "start": "791000",
    "end": "1124000"
  },
  {
    "text": "well I cost us go pretty well our biggest cluster size so far is about",
    "start": "793819",
    "end": "799620"
  },
  {
    "text": "14,000 three CPUs and 50 terabytes of RAM and we currently have about twenty most of",
    "start": "799620",
    "end": "805170"
  },
  {
    "text": "them are not usually that big that's the biggest one that runs a whole bunch of the elastin internal see I you know it's",
    "start": "805170",
    "end": "812459"
  },
  {
    "text": "pretty common that that will you know have two and a half thousand builds dropped onto it in a minute or two and",
    "start": "812459",
    "end": "819420"
  },
  {
    "text": "so it will need to scale really quickly to make sure that people don't have to wait too long for their builds because I",
    "start": "819420",
    "end": "824700"
  },
  {
    "text": "don't know about you who else is dis admins but if there's one thing that will make your devs come and poke you",
    "start": "824700",
    "end": "830430"
  },
  {
    "text": "real hard it's when their builds don't start for like 10 or 15 minutes with a medallion reason and so we've spent a",
    "start": "830430",
    "end": "837600"
  },
  {
    "text": "lot of effort to make sure that that time is as short as possible as I said",
    "start": "837600",
    "end": "843060"
  },
  {
    "text": "mainly batch for now most of our workload is batch jobs ci builds from",
    "start": "843060",
    "end": "849240"
  },
  {
    "text": "one internal customer or another at in",
    "start": "849240",
    "end": "854520"
  },
  {
    "text": "October our average was 177 K pods a day which is a little bit more around about",
    "start": "854520",
    "end": "860670"
  },
  {
    "text": "2 per second so it's a pretty it's a pretty respectable amount of pods I",
    "start": "860670",
    "end": "865860"
  },
  {
    "text": "think overall the and what we find is that the internal workloads are super",
    "start": "865860",
    "end": "873060"
  },
  {
    "text": "bursty as I said a lot of the time it'll be at 5 a.m. Sydney time will often get two and a half to three thousand builds",
    "start": "873060",
    "end": "878970"
  },
  {
    "text": "shared you'll double need you know a couple hundred nodes added very quickly so that's one of the reasons why we",
    "start": "878970",
    "end": "885959"
  },
  {
    "text": "built escalator which is the batch optimized ortus cluster autoscaler that",
    "start": "885959",
    "end": "891540"
  },
  {
    "text": "all the order scales nodes for us that's a github home slash or less in flash",
    "start": "891540",
    "end": "897690"
  },
  {
    "text": "escalator if you want to check it out it is definitely built for jobs that run to",
    "start": "897690",
    "end": "903209"
  },
  {
    "text": "completion though so it's completely not useful for state bliss server workloads",
    "start": "903209",
    "end": "909300"
  },
  {
    "text": "that you know are intended to stick around for a long time but if you have jobs that are going to run for some period of time and finish it is a very",
    "start": "909300",
    "end": "916079"
  },
  {
    "text": "responsive elastic on a scale that will scale your up and scale back down all things I want to learn for now though",
    "start": "916079",
    "end": "921930"
  },
  {
    "text": "our service workloads are coming we you we have some people building a whole big",
    "start": "921930",
    "end": "930300"
  },
  {
    "text": "teams building internal pairs some of them here today I the and so basically the point of that",
    "start": "930300",
    "end": "937180"
  },
  {
    "text": "internal payers is that developers aren't is to have developers not need to care about how their stuff gets deployed",
    "start": "937180",
    "end": "943210"
  },
  {
    "text": "they'll just say I want a docker image a Postgres database and I want it to be",
    "start": "943210",
    "end": "949810"
  },
  {
    "text": "public you know I wanted to exposed on the internet please in a really simple declarative file and then there is a",
    "start": "949810",
    "end": "956170"
  },
  {
    "text": "whole bunch of magic that the poster team are building that will then translate all of that through a variety",
    "start": "956170",
    "end": "961720"
  },
  {
    "text": "of translations into actual cube things why aren't all up for you generate your Postgres database make sure that the",
    "start": "961720",
    "end": "967660"
  },
  {
    "text": "poster has connection strings and you and details are all wired up to your pod for you so that if you're a developer",
    "start": "967660",
    "end": "974290"
  },
  {
    "text": "the experience of using the the platform is as close as possible to the",
    "start": "974290",
    "end": "980200"
  },
  {
    "text": "experience of running stuff on your laptop however some stuff has nothing great",
    "start": "980200",
    "end": "986310"
  },
  {
    "text": "managing at today is hard really hard the we've had heaps of problems with",
    "start": "986310",
    "end": "992440"
  },
  {
    "text": "running at TD yeah consensus problems with raft io problem running out I Oh problems can",
    "start": "992440",
    "end": "998440"
  },
  {
    "text": "cause a election storms that you know are not a good time if you ever get if",
    "start": "998440",
    "end": "1003990"
  },
  {
    "text": "you ever hit the 2.1 gig limit on the size of your database then your cluster will flip into read-only mode and you",
    "start": "1003990",
    "end": "1010830"
  },
  {
    "text": "will have a pretty bad day the coverings from that is a bit tricky and there's",
    "start": "1010830",
    "end": "1016080"
  },
  {
    "text": "you know we've got extensive run books now on how to do it so yeah it is really hard I'll get to what I think you should",
    "start": "1016080",
    "end": "1024089"
  },
  {
    "text": "do about that later the Goliath layer needed a lot of work the instable is really good for early",
    "start": "1024089",
    "end": "1031140"
  },
  {
    "text": "prototyping but the deployment model we originally use just in scale what one of our guys had to do was to paralyze the",
    "start": "1031140",
    "end": "1040470"
  },
  {
    "text": "build state to build jobs in bamboo the CI tall we use obviously since it sails",
    "start": "1040470",
    "end": "1046040"
  },
  {
    "text": "and when we once we paralyze the build jobs you know we admin's that now we",
    "start": "1046040",
    "end": "1051930"
  },
  {
    "text": "wait 20 minutes rebuild not 20 minutes for each cluster times the number of clusters we're currently at 20 clusters",
    "start": "1051930",
    "end": "1057750"
  },
  {
    "text": "and adding more so that was a really important step for us not to be spending three or four hours a day waiting for",
    "start": "1057750",
    "end": "1063360"
  },
  {
    "text": "built but that is mostly fixed now yeah",
    "start": "1063360",
    "end": "1069669"
  },
  {
    "text": "security is a challenge like I said there it's it's not really tied to design it's gonna happen no matter what",
    "start": "1069669",
    "end": "1075159"
  },
  {
    "text": "you do but it is really worth noting security is really hard the other the",
    "start": "1075159",
    "end": "1080500"
  },
  {
    "text": "other main workload Bachelor clothes that we use is bitbucket pipelines epochal pipelines is bit buckets CI CD",
    "start": "1080500",
    "end": "1087549"
  },
  {
    "text": "in the cloud for you I like to say to people as the people who run the platform it is a arbitrary remote code",
    "start": "1087549",
    "end": "1094360"
  },
  {
    "text": "execution as a service so as you can imagine the security profile of that is pretty nightmarish so we've had a whole",
    "start": "1094360",
    "end": "1103299"
  },
  {
    "text": "bunch problems with that that we've had to fix yeah and it can be pretty scary",
    "start": "1103299",
    "end": "1108970"
  },
  {
    "text": "and lots of kubernetes security things can be pretty scary there are great",
    "start": "1108970",
    "end": "1113980"
  },
  {
    "text": "options available for you in Cuba noddies to tune all this stuff but like a bunch of people have said in the",
    "start": "1113980",
    "end": "1120129"
  },
  {
    "text": "keynotes and other places that defaults are not very secure yet so tips sorry so",
    "start": "1120129",
    "end": "1130169"
  },
  {
    "start": "1124000",
    "end": "1506000"
  },
  {
    "text": "here's just some general notes and stuff on things that I learned from our design that really really worked firstly manage",
    "start": "1130169",
    "end": "1137230"
  },
  {
    "text": "the dependencies be careful about those dependency cycles if you are going to have dependency cycles you need to make",
    "start": "1137230",
    "end": "1142840"
  },
  {
    "text": "sure that you are knowingly choosing them and that",
    "start": "1142840",
    "end": "1147809"
  },
  {
    "text": "sorry you need to be careful that you are choosing knowingly to have",
    "start": "1149570",
    "end": "1155940"
  },
  {
    "text": "dependency cycles and then you know how to bootstrap yourself out of them if everything is down we chose to not have",
    "start": "1155940",
    "end": "1162510"
  },
  {
    "text": "to worry about that by making sure we didn't have dependency cycles it's okay to have dependency cycles you just have to know that they're cattle not pets the",
    "start": "1162510",
    "end": "1171990"
  },
  {
    "text": "immutable infrastructure practices will make your life easier Abel it means that if you need to upgrade if you need to rotate it's just",
    "start": "1171990",
    "end": "1179190"
  },
  {
    "text": "it's a matter of just throwing things away and it also means that you're troubleshooting time on any one issue",
    "start": "1179190",
    "end": "1184640"
  },
  {
    "text": "for a single event is also bounded because the sorry if if things are going",
    "start": "1184640",
    "end": "1195720"
  },
  {
    "text": "wrong and you know you don't know what's going on and it's the first time you've seen something you only spend an errand",
    "start": "1195720",
    "end": "1201030"
  },
  {
    "text": "or two trying to figure out what's going on if you can't figure it out you say aliens or cosmic rays or something I",
    "start": "1201030",
    "end": "1207150"
  },
  {
    "text": "don't know and you only blow out the instance and then if it comes back two or three times then you know you have a",
    "start": "1207150",
    "end": "1213480"
  },
  {
    "text": "real problem but the one time when you're running things at scale you know 0.01 percent things happen all the time",
    "start": "1213480",
    "end": "1220320"
  },
  {
    "text": "and sometimes they are not worth investigating and so it lets you make be careful and choosy about what you",
    "start": "1220320",
    "end": "1228570"
  },
  {
    "text": "actually spend your time investigating use layers when you're doing your design",
    "start": "1228570",
    "end": "1234150"
  },
  {
    "text": "if you if when you're building anything you know composability and careful",
    "start": "1234150",
    "end": "1239910"
  },
  {
    "text": "interface design between sections of whatever you're building will meet is guaranteed to increase your velocity because you can pick the thing that",
    "start": "1239910",
    "end": "1246690"
  },
  {
    "text": "you're working on and know that you're only working on that in terms of infrastructure the natural way to do that is to use layers where you build",
    "start": "1246690",
    "end": "1253110"
  },
  {
    "text": "upper layers because it's much harder to break infrastructure apart into separate modules I mean you should do that anyway",
    "start": "1253110",
    "end": "1259350"
  },
  {
    "text": "inside the code as much as you can but in terms of like overall deployment it's really hard to sort of say you know I'm",
    "start": "1259350",
    "end": "1266040"
  },
  {
    "text": "gonna deploy the EDD servers separately to the rest of the staff because there's so much so many type couplings between",
    "start": "1266040",
    "end": "1272520"
  },
  {
    "text": "them and so it's better in general to go like as a layer cake type structure I",
    "start": "1272520",
    "end": "1277890"
  },
  {
    "text": "think lastly you should percent get involved I you know I do",
    "start": "1277890",
    "end": "1285750"
  },
  {
    "text": "I'm a sysadmin not you historically not even a poll that has changed since I started using cube but the you i've got",
    "start": "1285750",
    "end": "1295260"
  },
  {
    "text": "involved in the kubernetes community literally just a shop to meetings and be like hey i use this thing and i have",
    "start": "1295260",
    "end": "1301080"
  },
  {
    "text": "found that you know as an end user people are desperate to hear what I have",
    "start": "1301080",
    "end": "1306840"
  },
  {
    "text": "to say about things and so I would strongly recommend even if you are just a clock cluster operator I guarantee",
    "start": "1306840",
    "end": "1312930"
  },
  {
    "text": "that any seek call you dial into if you say I'm a cluster operator and here's",
    "start": "1312930",
    "end": "1317940"
  },
  {
    "text": "some problems or some concerns or some feedback that I have everybody will be extremely happy to hear it so here's the",
    "start": "1317940",
    "end": "1327020"
  },
  {
    "text": "you know here's juicy bit right I'm sure you all came here to see why I think you shouldn't they could be summarized for",
    "start": "1327020",
    "end": "1333960"
  },
  {
    "text": "this year's boring tools Manesh cube is pretty great now I'm",
    "start": "1333960",
    "end": "1340860"
  },
  {
    "text": "sorry the you know when we started there was no such thing as managed cube really",
    "start": "1340860",
    "end": "1347000"
  },
  {
    "text": "there was very early gke there wasn't even google kubernetes engine back then",
    "start": "1347000",
    "end": "1354290"
  },
  {
    "text": "but now you know between the three major club providers and everyone else you",
    "start": "1354290",
    "end": "1361260"
  },
  {
    "text": "know the manage cube clusters are 100% usable especially if you're just starting out now if you're just starting",
    "start": "1361260",
    "end": "1366930"
  },
  {
    "text": "out now there's really no reason for you not to use one of them at whichever manage cloud provider you're closest to or to you know whichever your cloud",
    "start": "1366930",
    "end": "1374760"
  },
  {
    "text": "provider of choice you know if there is a managed cube solution for you and you're just starting out it is almost",
    "start": "1374760",
    "end": "1380100"
  },
  {
    "text": "certainly going to be better for you to get started with a managed cube solution and then think about doing your own if",
    "start": "1380100",
    "end": "1386640"
  },
  {
    "text": "you find you need to yeah I mean the the managed platforms don't claim to be all",
    "start": "1386640",
    "end": "1392370"
  },
  {
    "text": "things to all people and they're not you know there is definitely stuff that you just cannot do I mean for us we've",
    "start": "1392370",
    "end": "1398460"
  },
  {
    "text": "twiddled every API server knob you can twiddle and so for us we can't go back",
    "start": "1398460",
    "end": "1404460"
  },
  {
    "text": "to not being able to twiddle those things you know you get addicted to the twiddling once you once you start",
    "start": "1404460",
    "end": "1409740"
  },
  {
    "text": "actually doing it um you don't want to run a TD that's the other reason to use a menu one is you",
    "start": "1409740",
    "end": "1416910"
  },
  {
    "text": "really don't want to have to run a TD if you have to running your own NCD is like running your own database thirty years",
    "start": "1416910",
    "end": "1422910"
  },
  {
    "text": "ago there is not you know the software is great definitely the social is excellent it is",
    "start": "1422910",
    "end": "1428070"
  },
  {
    "text": "well built it runs really well but even the documentation for the software there are pages and pages and pages of caveats",
    "start": "1428070",
    "end": "1434730"
  },
  {
    "text": "things you've got to be aware of if you haven't if you don't know that they're there they will shoot they will catch",
    "start": "1434730",
    "end": "1441000"
  },
  {
    "text": "you and so that's why I mean by it's like a database 30 years ago there's no sort of you know using years and years",
    "start": "1441000",
    "end": "1448050"
  },
  {
    "text": "of best practices and here's the things you've got to watch out for and all those sort of you know industry",
    "start": "1448050",
    "end": "1453540"
  },
  {
    "text": "knowledge that is just sitting around waiting to be used if you do decide that",
    "start": "1453540",
    "end": "1460470"
  },
  {
    "text": "you need to use your existing tooling then yeah cops cubicle",
    "start": "1460470",
    "end": "1466110"
  },
  {
    "text": "cube ATM they're all really good the you know use one of those as opposed to",
    "start": "1466110",
    "end": "1472410"
  },
  {
    "text": "doing what we did and literally started for kubernetes the hard way is you know Kelsey's not joking it is the hard way you know and from the problem for us now",
    "start": "1472410",
    "end": "1480780"
  },
  {
    "text": "is that we have got this whole tool train working and it's you as good as",
    "start": "1480780",
    "end": "1487770"
  },
  {
    "text": "you buy em for us for our specific use case and now there's a huge ing√©nue effort involved in actually swapping",
    "start": "1487770",
    "end": "1494100"
  },
  {
    "text": "over to everything else so again it's kind of a bit like being addicted to it because we can't give it up so yeah",
    "start": "1494100",
    "end": "1500550"
  },
  {
    "text": "that's does the reason why I think you really should use manage things where",
    "start": "1500550",
    "end": "1505590"
  },
  {
    "text": "you can thanks I've got time for questions [Applause]",
    "start": "1505590",
    "end": "1515340"
  },
  {
    "start": "1506000",
    "end": "1566000"
  },
  {
    "text": "yeah yeah so the question was about",
    "start": "1515340",
    "end": "1526769"
  },
  {
    "text": "managed services and where I feel the future is um yes for us absolutely um we",
    "start": "1526769",
    "end": "1532279"
  },
  {
    "text": "we reevaluate obviously for us it's eks because we are mainly we're enabled a WI",
    "start": "1532279",
    "end": "1538980"
  },
  {
    "text": "shop are we reevaluate BK s every six months or so to see where it's at we've",
    "start": "1538980",
    "end": "1544590"
  },
  {
    "text": "talked with Hawking to the e KS team about the exact features we want and that sort of thing I would strongly",
    "start": "1544590",
    "end": "1550470"
  },
  {
    "text": "recommend that ya if you do run your own thing yeah keep talking to the cloud providers if you're running anything at",
    "start": "1550470",
    "end": "1556710"
  },
  {
    "text": "any scale I guarantee you the Thames and then mediums and stuff we'll be very happy to talk to you about where you",
    "start": "1556710",
    "end": "1563399"
  },
  {
    "text": "feel their product the product is lacking yeah yep I don't have much",
    "start": "1563399",
    "end": "1576539"
  },
  {
    "text": "experience I started the question is about TD if we run zookeeper already is there TD harder than that I don't have",
    "start": "1576539",
    "end": "1582870"
  },
  {
    "text": "much experience with the zookeeper personally but yeah all consensus algorithm just kind of similar you know",
    "start": "1582870",
    "end": "1588120"
  },
  {
    "text": "like if you're used to running a consistent algorithm and you have a good sense of like how many nodes you need and all those sort of things it's",
    "start": "1588120",
    "end": "1594419"
  },
  {
    "text": "probably gonna be easier for you than it would be for other people that said it C D does have a bunch of you know of its",
    "start": "1594419",
    "end": "1600750"
  },
  {
    "text": "own stuff that sort of make your life a bit harder okay sorry I think at the",
    "start": "1600750",
    "end": "1607590"
  },
  {
    "text": "back here you first",
    "start": "1607590",
    "end": "1610340"
  },
  {
    "start": "1609000",
    "end": "1727000"
  },
  {
    "text": "yeah sure um so the question is about Costas eyes and look sounds like we have lots of smaller clusters and was that a",
    "start": "1627280",
    "end": "1635060"
  },
  {
    "text": "decision during the design so yes and no during the design the original intent was actually to have very small clusters",
    "start": "1635060",
    "end": "1642980"
  },
  {
    "text": "to have no class to span and AZ boundary in OWS so they would all be single quite",
    "start": "1642980",
    "end": "1648230"
  },
  {
    "text": "single AZ clusters it would the intent that different AZ had problems that went away you know you treated the clusters",
    "start": "1648230",
    "end": "1654860"
  },
  {
    "text": "as as though they were nodes you know super cluster the intent was to use a original Federation to do that",
    "start": "1654860",
    "end": "1660290"
  },
  {
    "text": "we found that original federation just it didn't move fast enough for us and we had you know greater reliability",
    "start": "1660290",
    "end": "1665990"
  },
  {
    "text": "requirements then the solution could deliver and so what we ended up doing was changing our clusters design so that it spanned three AZ's all the time so",
    "start": "1665990",
    "end": "1673280"
  },
  {
    "text": "that your main three young main important control plane parts all had a node in haz and that's how we sort of",
    "start": "1673280",
    "end": "1679370"
  },
  {
    "text": "met our availability requirements the cluster size we have built them to scale",
    "start": "1679370",
    "end": "1684950"
  },
  {
    "text": "I think the number of nodes we can have is about four thousands in each cluster so the thing that one of the things that",
    "start": "1684950",
    "end": "1692900"
  },
  {
    "text": "we've found is one of the primary determinants of the scale of your that your clusters can get to is actually",
    "start": "1692900",
    "end": "1698270"
  },
  {
    "text": "your IP allocation strategy you know because if you if you're running an SDN",
    "start": "1698270",
    "end": "1703580"
  },
  {
    "text": "each node needs to have a subnet carved out for it the size of your note of your pool of",
    "start": "1703580",
    "end": "1709390"
  },
  {
    "text": "for your SD n divided by the number of nodes you want to lay out is how many pods you can have running on each node",
    "start": "1709390",
    "end": "1714950"
  },
  {
    "text": "so there's a real sort of tight coupling between those three numbers as to",
    "start": "1714950",
    "end": "1720800"
  },
  {
    "text": "exactly how much you can scale sorry oh they were then into your couldn't yeah thanks I think I'll go back to front of",
    "start": "1720800",
    "end": "1726770"
  },
  {
    "text": "those okay ah right yeah so if we were running",
    "start": "1726770",
    "end": "1735640"
  },
  {
    "start": "1727000",
    "end": "1776000"
  },
  {
    "text": "containers before uncle II we did Amazon asked us to run release yes um so the containers that we were running before were not containers in the modern sense",
    "start": "1735640",
    "end": "1743650"
  },
  {
    "text": "of the word they were they used the same C group container functionality but they",
    "start": "1743650",
    "end": "1749919"
  },
  {
    "text": "were openvz containers where um it's kind of a bit more like a truth you know then a container with an image and so it",
    "start": "1749919",
    "end": "1757660"
  },
  {
    "text": "just it wasn't any of the existing containerization things wouldn't have been a good fit and basically when we",
    "start": "1757660",
    "end": "1764440"
  },
  {
    "text": "migrated away from that data center platform part of the process of migrating away was actually modernizing the apps so that they worked in in a way",
    "start": "1764440",
    "end": "1770679"
  },
  {
    "text": "that was more compatible with modern containerization as opposed to this sort of very early precursor to containerization so at the time um we",
    "start": "1770679",
    "end": "1786929"
  },
  {
    "start": "1776000",
    "end": "1799000"
  },
  {
    "text": "yeah basically we were looking at cube for some of the other advantages you get",
    "start": "1786929",
    "end": "1792190"
  },
  {
    "text": "from cuba as opposed to justice yes so that's why at the time we didn't really look at it yeah sorry I think that going front and",
    "start": "1792190",
    "end": "1798460"
  },
  {
    "text": "then they come back",
    "start": "1798460",
    "end": "1800909"
  },
  {
    "start": "1799000",
    "end": "1890000"
  },
  {
    "text": "yes your so the question is about our the number of individual clusters and",
    "start": "1809350",
    "end": "1814400"
  },
  {
    "text": "what the use case we have for each cluster so the way that we designate so",
    "start": "1814400",
    "end": "1820400"
  },
  {
    "text": "when I talked about the bottom layer the flag um the way that we break up it",
    "start": "1820400",
    "end": "1825650"
  },
  {
    "text": "divided the clusters is via a three tuple of information there's the internal cost customer name an internal",
    "start": "1825650",
    "end": "1832670"
  },
  {
    "text": "customer environment and the AWS region and so the intent being that we could have you know we have the say tobacco",
    "start": "1832670",
    "end": "1840320"
  },
  {
    "text": "placards clusters we can have a big pocket pipelines customer that has a dev environment in u.s. East one and a dev",
    "start": "1840320",
    "end": "1847340"
  },
  {
    "text": "environment in u.s. West - and a pride environment in u.s. this one on a protein or a man in u.s. was - and then",
    "start": "1847340",
    "end": "1852470"
  },
  {
    "text": "we can have a different customer the payers customer that also have their own set of all the levels that they want in",
    "start": "1852470",
    "end": "1858410"
  },
  {
    "text": "all the ridges that they want and so that's one of the reasons we've ended up with a proliferation of things is that we've got a few internal customers now",
    "start": "1858410",
    "end": "1864380"
  },
  {
    "text": "and they all want different regions or different levels or different environment names it's not actually that",
    "start": "1864380",
    "end": "1869570"
  },
  {
    "text": "each of them is very big it's that these are administrative sort of distinctions",
    "start": "1869570",
    "end": "1875780"
  },
  {
    "text": "between the clusters and it the good part about that is that it lets you do you know it makes the clusters more the",
    "start": "1875780",
    "end": "1883490"
  },
  {
    "text": "security boundary easy to understand so I think there was one person over there yep sorry and then",
    "start": "1883490",
    "end": "1891610"
  },
  {
    "start": "1890000",
    "end": "1949000"
  },
  {
    "text": "ah sure I'm yes so the question is about exactly how we're bringing up the clusters um see the the the flag layer",
    "start": "1897510",
    "end": "1905669"
  },
  {
    "text": "just as I said does the initial VPC creation and stuff that's with terraform and then we're terraform in in a car",
    "start": "1905669",
    "end": "1911730"
  },
  {
    "text": "layer um it literally just blacked out or look all of the compute and the good",
    "start": "1911730",
    "end": "1917400"
  },
  {
    "text": "part about terraform is that it deduces a lot of the dependencies based on how you use outputs from tariffs or modules",
    "start": "1917400",
    "end": "1924450"
  },
  {
    "text": "and so we've set up dependencies and stuff so there's a CDS come up first API",
    "start": "1924450",
    "end": "1930360"
  },
  {
    "text": "servers come up after that so we use controller nodes which run the API server the controller manager and the",
    "start": "1930360",
    "end": "1936210"
  },
  {
    "text": "scheduler and then we have separated city nodes that handle the actual secret storing and then there are multiple",
    "start": "1936210",
    "end": "1943200"
  },
  {
    "text": "pools of worker nodes that can scale to different sizes sorry does that answer",
    "start": "1943200",
    "end": "1948690"
  },
  {
    "text": "your question",
    "start": "1948690",
    "end": "1950840"
  },
  {
    "start": "1949000",
    "end": "2013000"
  },
  {
    "text": "so for those so the question is out of the layers in the cake which ones does our team manage and which ones do we",
    "start": "1958280",
    "end": "1963530"
  },
  {
    "text": "delegate out of him those are the three layers that we that we actually manage we are uncalled-for all three of those",
    "start": "1963530",
    "end": "1969520"
  },
  {
    "text": "the delegation to other teams is usually about what they build after Goliath runs",
    "start": "1969520",
    "end": "1976790"
  },
  {
    "text": "and so part of what we do with Goliath is say if someone says hey I'd like a no space in a cluster then we will create",
    "start": "1976790",
    "end": "1984530"
  },
  {
    "text": "we will create the namespace for them and ensure that the you that super basic",
    "start": "1984530",
    "end": "1989690"
  },
  {
    "text": "stuff is wired up and make sure that they have a namespace admin service",
    "start": "1989690",
    "end": "1995810"
  },
  {
    "text": "account and then after that it's on them what they do with it we sort of say hey it's up to you but come and come and",
    "start": "1995810",
    "end": "2002470"
  },
  {
    "text": "talk to us if you just say what you want to do yes sorry",
    "start": "2002470",
    "end": "2007500"
  },
  {
    "text": "yep",
    "start": "2011850",
    "end": "2014480"
  },
  {
    "start": "2013000",
    "end": "2075000"
  },
  {
    "text": "sure sure so the question is about you know are we concerned about turning over",
    "start": "2030759",
    "end": "2036610"
  },
  {
    "text": "are we concerned about turning over control to other people the the in turn",
    "start": "2036610",
    "end": "2041899"
  },
  {
    "text": "is actually that we will never turn over control of those things to other people when other people when other customers are using the platform we are still on",
    "start": "2041899",
    "end": "2048829"
  },
  {
    "text": "the hook for the operational platform so our SLA s are all about availability",
    "start": "2048829",
    "end": "2054648"
  },
  {
    "text": "of the API servers and whatever metric the customer cares about so in the case of the CI the metric is actually time to",
    "start": "2054649",
    "end": "2061849"
  },
  {
    "text": "schedule for pods in the case of sort of stateless long-running service customers",
    "start": "2061849",
    "end": "2067280"
  },
  {
    "text": "their metric is like how the uptime of the services that are running on the platform does that answer your question",
    "start": "2067280",
    "end": "2072618"
  },
  {
    "text": "yes cool so I think why not the back day",
    "start": "2072619",
    "end": "2076929"
  },
  {
    "start": "2075000",
    "end": "2133000"
  },
  {
    "text": "so the question is about have we run into any entity limits on the number of objects I would say no the limits that",
    "start": "2084869",
    "end": "2091618"
  },
  {
    "text": "we have run to have been the size limits and I say that as someone who had a cluster accidentally put have 80,000",
    "start": "2091619",
    "end": "2097829"
  },
  {
    "text": "namespaces the good news is kubernetes does work when you have 80,000 namespaces it CD does not yeah 80,000",
    "start": "2097829",
    "end": "2105809"
  },
  {
    "text": "namespaces 100% will take up your 2.1 gig at about 50,000 namespaces your ETD",
    "start": "2105809",
    "end": "2111119"
  },
  {
    "text": "will start slowing down and you'll see witness and then eventually when your attorney fills up you will have a bad",
    "start": "2111119",
    "end": "2116249"
  },
  {
    "text": "day and you will have to if you're lucky flip traffic to a another cluster while",
    "start": "2116249",
    "end": "2122069"
  },
  {
    "text": "you sort out their problem which is what we actually managed to do I was very glad that we talked that customer into having a failover clustering ready to go",
    "start": "2122069",
    "end": "2129619"
  },
  {
    "text": "yes sorry was there someone over there no okay",
    "start": "2129619",
    "end": "2135349"
  },
  {
    "start": "2133000",
    "end": "2159000"
  },
  {
    "text": "do we retire clusters we don't retire clusters very often unless we determine",
    "start": "2138569",
    "end": "2143700"
  },
  {
    "text": "it's no longer a need for them that said we found that one of the dead clusters we stood up originally recently that no",
    "start": "2143700",
    "end": "2149969"
  },
  {
    "text": "one was using it so we have retired that one but that's the only one that we've actually retired since we started standing them up yeah most of the time",
    "start": "2149969",
    "end": "2156180"
  },
  {
    "text": "people want to use them sorry",
    "start": "2156180",
    "end": "2161869"
  },
  {
    "start": "2159000",
    "end": "2240000"
  },
  {
    "text": "yes oh right so the question is about failover and are we running clusters active-passive so yes what we try and do",
    "start": "2164200",
    "end": "2172040"
  },
  {
    "text": "is make it so the customer gets to decide exactly how they're using the clusters as far as we're concerned they",
    "start": "2172040",
    "end": "2177440"
  },
  {
    "text": "are active active active everything always got to be up you know we're in our internal sort of priority",
    "start": "2177440",
    "end": "2182480"
  },
  {
    "text": "classification we are at e0 at service you know we're four or five nines and reliability depending on which customer",
    "start": "2182480",
    "end": "2188390"
  },
  {
    "text": "you're talking to so but the intent being that the customers need to have a",
    "start": "2188390",
    "end": "2194120"
  },
  {
    "text": "way to be able to flip to another cluster if they if they need to I'm the way that we actually do that is we have",
    "start": "2194120",
    "end": "2199280"
  },
  {
    "text": "cluster Registry running and with",
    "start": "2199280",
    "end": "2204380"
  },
  {
    "text": "there's ways that we can poke that closet registry to say hey for this customer you should direct your traffic over there only some customers are",
    "start": "2204380",
    "end": "2210950"
  },
  {
    "text": "started using it because it's still pretty new but we're hoping that that will be the way that we can sort of guide traffic without people needing to",
    "start": "2210950",
    "end": "2217010"
  },
  {
    "text": "be woken up even ideally no no so people",
    "start": "2217010",
    "end": "2222350"
  },
  {
    "text": "only schedule most customers that we have now we'll only schedule load in one cluster and so there is a problem it's literally just we ask them to you know",
    "start": "2222350",
    "end": "2229670"
  },
  {
    "text": "currently we have to ask them to change their to linked up to flip to another one we want to move to cluster registry so that we can do that without you know",
    "start": "2229670",
    "end": "2236210"
  },
  {
    "text": "we'll obviously call them and tell them but you know we don't need them to make changes yes yeah and that's what we",
    "start": "2236210",
    "end": "2247010"
  },
  {
    "start": "2240000",
    "end": "2261000"
  },
  {
    "text": "found I mean generally if you're having a bad enough problem that you'd even consider failing over the you know",
    "start": "2247010",
    "end": "2252260"
  },
  {
    "text": "people would prefer to have some workload working then be too concerned about the stuff that is broken already well we we had found I think there was",
    "start": "2252260",
    "end": "2259310"
  },
  {
    "text": "one other person behind there and then your next min",
    "start": "2259310",
    "end": "2262960"
  },
  {
    "start": "2261000",
    "end": "2343000"
  },
  {
    "text": "okay so the question is about what are the requirements we have that that we can't run on managed service yet most of",
    "start": "2267420",
    "end": "2274300"
  },
  {
    "text": "them are around messing with the API server turning on experimental features",
    "start": "2274300",
    "end": "2279360"
  },
  {
    "text": "for a while we needed to add a bunch of settings about Web books and things like",
    "start": "2279360",
    "end": "2287170"
  },
  {
    "text": "that most of that stuff is defaulted on now so some of that has gone away but one of",
    "start": "2287170",
    "end": "2293590"
  },
  {
    "text": "the things about having app as having people developing app has on us is a lot of the time thanks for this guys they",
    "start": "2293590",
    "end": "2300700"
  },
  {
    "text": "want the newest hottest bits of kubernetes or they've submitted patches upstream to kubernetes and they want the you know they want their patches as soon",
    "start": "2300700",
    "end": "2307300"
  },
  {
    "text": "as possible so we've got a keep rolling forward yep",
    "start": "2307300",
    "end": "2314279"
  },
  {
    "text": "no I know that is a specific sort of contract to the platform is that you",
    "start": "2322260",
    "end": "2328330"
  },
  {
    "text": "know if you if you store stuff in the cluster it's up to you to make sure that it ends up in both places since you're up to your pipeline cool I think we're",
    "start": "2328330",
    "end": "2335320"
  },
  {
    "text": "out of time I am happy to take some more questions just I'll I'll be just outside",
    "start": "2335320",
    "end": "2340690"
  },
  {
    "text": "once I packed up",
    "start": "2340690",
    "end": "2343619"
  }
]