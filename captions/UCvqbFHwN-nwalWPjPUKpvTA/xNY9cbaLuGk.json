[
  {
    "start": "0",
    "end": "34000"
  },
  {
    "text": "hello everyone my name is kevin cluse and i'm a principal software engineer at nvidia today i'm going to be talking about the",
    "start": "1599",
    "end": "7759"
  },
  {
    "text": "support i built for multi-instance gpus in containers and kubernetes multi-instance gpus or mig for short is",
    "start": "7759",
    "end": "15759"
  },
  {
    "text": "a new hardware feature of the latest nvidia ampere gpus so everything i talk about today is specific to this type of gpu",
    "start": "15759",
    "end": "23279"
  },
  {
    "text": "before i begin i'd like to remind you that i am available for chat during this talk so as questions come up please feel free",
    "start": "23279",
    "end": "29279"
  },
  {
    "text": "to drop them on slack and i'll make sure to answer them as best i can",
    "start": "29279",
    "end": "34160"
  },
  {
    "start": "34000",
    "end": "34000"
  },
  {
    "text": "i imagine if you're at this talk you probably already understand the benefits of supporting gpus and kubernetes so i'm",
    "start": "35280",
    "end": "40640"
  },
  {
    "text": "only going to spend a few minutes highlighting some of their more important use cases",
    "start": "40640",
    "end": "45760"
  },
  {
    "text": "first and foremost supporting gpus in kubernetes lets us scale up ai training and",
    "start": "45760",
    "end": "51120"
  },
  {
    "text": "inference jobs to a cluster of gpu machines using the same underlying infrastructure we use to deploy all of our cpu-based",
    "start": "51120",
    "end": "58000"
  },
  {
    "text": "workloads such as web servers databases streaming data servers etc using the tools we've built as a",
    "start": "58000",
    "end": "66000"
  },
  {
    "text": "community users are able to take a kubernetes pod spec specify some number of gpus",
    "start": "66000",
    "end": "74080"
  },
  {
    "text": "and direct their application to a particular class of gpus that can meet their workload demands",
    "start": "74080",
    "end": "80640"
  },
  {
    "text": "a typical gpu cluster then looks something like this where you have a bunch of servers each hosting some",
    "start": "81200",
    "end": "86799"
  },
  {
    "text": "number of possibly heterogeneous gpus all being managed under a single kubernetes instance",
    "start": "86799",
    "end": "94400"
  },
  {
    "text": "where training jobs tend to require multiple more powerful gpus",
    "start": "94400",
    "end": "99759"
  },
  {
    "text": "and inference or development jobs tend to only require a single instance of a less powerful gpu",
    "start": "99759",
    "end": "107679"
  },
  {
    "text": "but what if you don't have the luxury of having such a diverse mix of gpus in your cluster",
    "start": "108000",
    "end": "113360"
  },
  {
    "text": "or as is often the case you have a small cluster with only the most powerful gpus you can get your hands on",
    "start": "113360",
    "end": "118960"
  },
  {
    "text": "and want to be able to efficiently use them for all types of workloads",
    "start": "118960",
    "end": "124000"
  },
  {
    "text": "and even if you do have a bunch of nodes at your disposal wouldn't it be better to have your cluster full of the most powerful gpus possible so that you have",
    "start": "124000",
    "end": "130479"
  },
  {
    "text": "them when you need them but then have a way of sharing an individual gpu for jobs that only",
    "start": "130479",
    "end": "135680"
  },
  {
    "text": "require a fraction of its full power well this is where multi-instance gpus",
    "start": "135680",
    "end": "142400"
  },
  {
    "start": "140000",
    "end": "140000"
  },
  {
    "text": "come in it's hardware support for taking a full gpu and dividing it into several smaller",
    "start": "142400",
    "end": "149920"
  },
  {
    "text": "what we call gpu instances each of which have their own dedicated set of memory and compute resources",
    "start": "149920",
    "end": "157200"
  },
  {
    "text": "additionally each gpu instance gets a dedicated percentage of the overall memory bandwidth",
    "start": "157200",
    "end": "162560"
  },
  {
    "text": "and any faults that occur are isolated to a single instance without disrupting the others",
    "start": "162560",
    "end": "168319"
  },
  {
    "text": "from the perspective of the software consuming it each of these gpu instances really looks like its own individual gpu",
    "start": "168319",
    "end": "174879"
  },
  {
    "text": "and can be treated as such by the end user all gpu instances run in parallel with",
    "start": "174879",
    "end": "180879"
  },
  {
    "text": "predictable throughput and latency providing improved quality of service over previous gpu sharing solutions",
    "start": "180879",
    "end": "187280"
  },
  {
    "text": "such as mps or cuda streams it allows you to get the right sized gpu",
    "start": "187280",
    "end": "193280"
  },
  {
    "text": "for your job in cases where you don't need the full power of an entire gpu",
    "start": "193280",
    "end": "198800"
  },
  {
    "text": "and is already supported in a diverse set of deployment scenarios including bare metal v-gpus stand-alone",
    "start": "198800",
    "end": "205680"
  },
  {
    "text": "containers and kubernetes which we're focusing on",
    "start": "205680",
    "end": "210840"
  },
  {
    "text": "today so with that brief introduction this is",
    "start": "210840",
    "end": "216239"
  },
  {
    "start": "212000",
    "end": "212000"
  },
  {
    "text": "the outline for the rest of the talk i'm going to start by going into a bit more detail about what multi-instance",
    "start": "216239",
    "end": "222480"
  },
  {
    "text": "gpus are and how they can be used i'll then give a brief overview of how full gpus are supported in",
    "start": "222480",
    "end": "228959"
  },
  {
    "text": "containers and kubernetes today followed by the details of how we have added make support to them",
    "start": "228959",
    "end": "234720"
  },
  {
    "text": "after that i'll talk a bit about the system level interface for mig which has made all of this possible followed by a discussion on best",
    "start": "234720",
    "end": "240959"
  },
  {
    "text": "practices and tooling available for provisioning a set of make devices across a cluster of machines",
    "start": "240959",
    "end": "246720"
  },
  {
    "text": "finally i'll summarize everything i've talked about here today and end with a demo",
    "start": "246720",
    "end": "255760"
  },
  {
    "start": "255000",
    "end": "255000"
  },
  {
    "text": "okay so as i said before mig is a way of taking a full gpu and dividing it into a smaller set of",
    "start": "255760",
    "end": "261759"
  },
  {
    "text": "what we call gpu instances each of which has some amount of guaranteed memory and compute resources",
    "start": "261759",
    "end": "267360"
  },
  {
    "text": "available to it but what are some typical use cases for multi-instance gpus",
    "start": "267360",
    "end": "273520"
  },
  {
    "text": "well in general mig is useful anytime you have an application that doesn't require the full power of an entire gpu",
    "start": "273520",
    "end": "280240"
  },
  {
    "text": "but does want similar isolation guarantees provided by full gpus",
    "start": "280240",
    "end": "285680"
  },
  {
    "text": "this might be a single user who wants to run multiple inference jobs or some sort of experimentation he is doing",
    "start": "285680",
    "end": "292880"
  },
  {
    "text": "or it might be several trusted users from the same organization each running model exploration in a",
    "start": "292880",
    "end": "298000"
  },
  {
    "text": "jupiter notebook on their own dedicated gpu instance",
    "start": "298000",
    "end": "303120"
  },
  {
    "text": "and given the hardware isolation that multi-instance gpus provide you really can treat them as separate",
    "start": "303280",
    "end": "308479"
  },
  {
    "text": "gpus allowing untrusted users to run on their own dedicated instances",
    "start": "308479",
    "end": "313520"
  },
  {
    "text": "as would be the case when running on a managed cloud service such as google cloud ai or amazon",
    "start": "313520",
    "end": "318960"
  },
  {
    "text": "sagemaker but can we really think of a gpu instance as a replacement for a full gpu",
    "start": "318960",
    "end": "327600"
  },
  {
    "text": "well in the case of running inference on the burt natural language processing model a single gpu instance constituting 1 7",
    "start": "327600",
    "end": "334479"
  },
  {
    "text": "of a full a100 gpu was able to perform on par with a full v100 gpu running the same",
    "start": "334479",
    "end": "340800"
  },
  {
    "text": "application likewise running seven of these gpu",
    "start": "340800",
    "end": "345840"
  },
  {
    "text": "instances in parallel gave us roughly seven times the overall throughput of a single gpu instance",
    "start": "345840",
    "end": "351520"
  },
  {
    "text": "showcasing the linear scalability you would expect from seven distinct gpus",
    "start": "351520",
    "end": "357759"
  },
  {
    "text": "so why seven this number keeps cropping up and it's not clear exactly where it's coming from",
    "start": "359919",
    "end": "366400"
  },
  {
    "text": "similarly what exactly goes into creating these various gpu instances and how do you get access to them well",
    "start": "366400",
    "end": "372960"
  },
  {
    "text": "the best way to visualize this is with a diagram like the one you see here this shows how the 40 gigabyte a100 card",
    "start": "372960",
    "end": "380720"
  },
  {
    "text": "can be broken down into a set of eight five gigabyte memory chunks and seven",
    "start": "380720",
    "end": "386479"
  },
  {
    "text": "what we call compute slices it's important to note that there's also",
    "start": "386479",
    "end": "391840"
  },
  {
    "text": "a variant of this for the 80 gigabyte a100 card which has 10 gigabyte memory slices instead of five",
    "start": "391840",
    "end": "398720"
  },
  {
    "text": "as well as the newly announced a30 which has four six gigabyte memory slices and four compute slices for the purpose",
    "start": "398720",
    "end": "406880"
  },
  {
    "text": "of this talk i'm going to stick with the 40 gigabyte a100 variant but everything i talk about is",
    "start": "406880",
    "end": "412080"
  },
  {
    "text": "applicable to all of these cards so coming back to this question of y7",
    "start": "412080",
    "end": "418720"
  },
  {
    "text": "ideally we would have included eight compute slices to match the eight memory slices we have available",
    "start": "418720",
    "end": "424880"
  },
  {
    "text": "however the yields in the actual silicon make it so we can only reliably get seven compute slices instead of eight",
    "start": "424880",
    "end": "431199"
  },
  {
    "text": "so we are stuck with a somewhat awkward combination in any case to create a gpu instance all",
    "start": "431199",
    "end": "438400"
  },
  {
    "text": "we need to do is combine some number of compute slices with some number of memory slices and",
    "start": "438400",
    "end": "443599"
  },
  {
    "text": "merge them together in the case seen here we are merging a single memory slice",
    "start": "443599",
    "end": "449440"
  },
  {
    "text": "with a single compute slice and that name you see in the middle there that",
    "start": "449440",
    "end": "454599"
  },
  {
    "text": "1g.5gb that's an example of our canonical naming convention for this combination",
    "start": "454599",
    "end": "460720"
  },
  {
    "text": "anyone familiar with meg has likely seen names like this already in the output of nvidia smi",
    "start": "460720",
    "end": "467520"
  },
  {
    "text": "now jumping to a slightly bigger gpu instance and digging inside of it we see that we",
    "start": "468160",
    "end": "474319"
  },
  {
    "text": "can actually perform a second level of partitioning which subdivides a gpu instance into a",
    "start": "474319",
    "end": "479520"
  },
  {
    "text": "set of what we call compute instances all of which share access to the memory of the wrapping gpu",
    "start": "479520",
    "end": "485199"
  },
  {
    "text": "instance but have their own dedicated compute resources and when we do this you'll notice that",
    "start": "485199",
    "end": "491840"
  },
  {
    "text": "we tack on an extra parameter to the front of the canonical naming convention to denote this",
    "start": "491840",
    "end": "498159"
  },
  {
    "text": "once we've done this we now have all the pieces required to form what's called a mig device",
    "start": "498879",
    "end": "504319"
  },
  {
    "text": "which is the actual entity recognized by cuda on top of which workloads are able to run",
    "start": "504319",
    "end": "510560"
  },
  {
    "text": "it is represented as a three tuple of the top level gpu where the mig device has been created",
    "start": "510560",
    "end": "515919"
  },
  {
    "text": "its gpu instance and its compute instance this definition will become important",
    "start": "515919",
    "end": "521518"
  },
  {
    "text": "later on when i start to talk about how make support is made available to containers and kubernetes",
    "start": "521519",
    "end": "527440"
  },
  {
    "text": "so anyway jumping back to our compute instance example the compute instance may be created to consume the first compute slice",
    "start": "527440",
    "end": "534720"
  },
  {
    "text": "or the second or the third or the fourth or it might take up two",
    "start": "534720",
    "end": "541600"
  },
  {
    "text": "compute slices or three or all four",
    "start": "541600",
    "end": "547279"
  },
  {
    "text": "and in this special case where a single compute instance consumes all of the compute slices of its wrapping gpu",
    "start": "547279",
    "end": "552640"
  },
  {
    "text": "instance we talk about the gpu instance equaling the compute instance",
    "start": "552640",
    "end": "558240"
  },
  {
    "text": "and drop the compute instance portion from the canonical naming scheme going back to the more simplified name",
    "start": "558240",
    "end": "564640"
  },
  {
    "text": "shown here dropping this prefix is unambiguous because these names are meant to refer",
    "start": "564640",
    "end": "570959"
  },
  {
    "text": "to the actual mig devices they represent and not any individual gpu instance or compute instance",
    "start": "570959",
    "end": "578399"
  },
  {
    "text": "and in the context of kubernetes these are actually the only type of mig devices we currently support",
    "start": "578399",
    "end": "584320"
  },
  {
    "text": "due to the fact that all compute instances share access to the gpu memory of the wrapping gpu instance",
    "start": "584320",
    "end": "590080"
  },
  {
    "text": "subdividing into multiple compute instances does not fit well into the isolation guarantees one normally expects from the kubernetes resource",
    "start": "590080",
    "end": "596560"
  },
  {
    "text": "model as such for the remainder of this talk i'll only be referring to make devices",
    "start": "596560",
    "end": "601600"
  },
  {
    "text": "of this type we may expand on this support in the future as things like pod level resources start to take shape",
    "start": "601600",
    "end": "608560"
  },
  {
    "text": "but for now this is what we have and i'll go into the details of how all of this works a little bit later on",
    "start": "608560",
    "end": "615839"
  },
  {
    "text": "now with all this said we can't just arbitrarily create mig devices with any combination of memory slices and compute",
    "start": "616480",
    "end": "622560"
  },
  {
    "text": "slices for example this one isn't allowed and neither is this one",
    "start": "622560",
    "end": "630560"
  },
  {
    "text": "in fact there's a distinct set of mig devices which actually can be created you can have one of these or one of",
    "start": "630640",
    "end": "638720"
  },
  {
    "text": "these or two of these or three of these",
    "start": "638720",
    "end": "644800"
  },
  {
    "text": "or seven of these you can also have some combination like this",
    "start": "644800",
    "end": "650560"
  },
  {
    "text": "or like this or like this but you can't have something like this",
    "start": "650560",
    "end": "658240"
  },
  {
    "text": "or like this in general this diagram represents the physical",
    "start": "658240",
    "end": "663360"
  },
  {
    "text": "layout of how these make devices are created on the actual hardware so finding a valid combination consists",
    "start": "663360",
    "end": "668640"
  },
  {
    "text": "of walking from left to right and adding devices into the configuration such that no two devices",
    "start": "668640",
    "end": "674079"
  },
  {
    "text": "overlap vertically when applied to an actual node",
    "start": "674079",
    "end": "679839"
  },
  {
    "text": "configurations like the ones you see here are most common that is striping a single device type",
    "start": "679839",
    "end": "685839"
  },
  {
    "text": "across all of the gpus on a machine turning an 8 gpu dgx a100 for example",
    "start": "685839",
    "end": "692000"
  },
  {
    "text": "into either 56 1g 5gb devices",
    "start": "692000",
    "end": "697200"
  },
  {
    "text": "24 2g 10gb devices or 16 3g 20 gb devices",
    "start": "697200",
    "end": "706399"
  },
  {
    "text": "however it's not uncommon to also see a configuration like this where a single node has a good mix of",
    "start": "706399",
    "end": "711760"
  },
  {
    "text": "different device types or even this where some gpus are not",
    "start": "711760",
    "end": "717120"
  },
  {
    "text": "configured for mig at all and some are it really just depends on your particular cluster configuration",
    "start": "717120",
    "end": "723600"
  },
  {
    "text": "and how many nodes you have at your disposal to decide which configuration is best for you",
    "start": "723600",
    "end": "731839"
  },
  {
    "start": "730000",
    "end": "730000"
  },
  {
    "text": "okay now that we understand megabit better i'm going to switch gears for a minute and give a brief overview of how",
    "start": "731839",
    "end": "737040"
  },
  {
    "text": "we support full gpus in kubernetes today since general support for gpus in",
    "start": "737040",
    "end": "742079"
  },
  {
    "text": "containers is a prerequisite for kubernetes i'll first start there i'll then jump",
    "start": "742079",
    "end": "747200"
  },
  {
    "text": "back to explain how we've added mig support to containers and kubernetes later on",
    "start": "747200",
    "end": "752480"
  },
  {
    "text": "well the first thing to note is that unlike most containerized applications nvidia gpu enabled containers require",
    "start": "752480",
    "end": "758800"
  },
  {
    "text": "extra runtime support in order to guarantee that they run on machines with different nvidia gpu driver versions installed",
    "start": "758800",
    "end": "766320"
  },
  {
    "text": "if the set of nvidia driver libraries in user space don't match the exact version of the nvidia kernel module running on",
    "start": "766320",
    "end": "772320"
  },
  {
    "text": "the host then applications linking to those libraries will fail to run",
    "start": "772320",
    "end": "777600"
  },
  {
    "text": "for example the container shown here will run just fine on a host with the v1 kernel module installed",
    "start": "777600",
    "end": "784399"
  },
  {
    "text": "but fail on a host with the v2 kernel module installed",
    "start": "784399",
    "end": "789440"
  },
  {
    "text": "to solve for this we provide a package called the nvidia container toolkit which takes care to ensure that",
    "start": "789760",
    "end": "795279"
  },
  {
    "text": "compatible nvidia driver libraries are injected into a container at runtime as well as give an application access to",
    "start": "795279",
    "end": "801519"
  },
  {
    "text": "any required gpu hardware many of you have likely interacted with",
    "start": "801519",
    "end": "807360"
  },
  {
    "text": "the nvidia container toolkit through a docker command like the one shown here this basically says launch a docker",
    "start": "807360",
    "end": "813920"
  },
  {
    "text": "container with gpus 0 and 1 injected into it and run nvidia smi over them",
    "start": "813920",
    "end": "820639"
  },
  {
    "text": "to make this possible this command hooks into a component called nvidia docker which is just one small piece of the",
    "start": "821279",
    "end": "827120"
  },
  {
    "text": "overall nvidia container toolkit that i've been talking about the toolkit itself consists of a stack",
    "start": "827120",
    "end": "832560"
  },
  {
    "text": "of components allowing gpus to be used by many different container runtimes",
    "start": "832560",
    "end": "837839"
  },
  {
    "text": "different runtimes hook into different layers of this stack depending on their integration points",
    "start": "837839",
    "end": "843680"
  },
  {
    "text": "for example container d hooks in here cryo hooks in here and lexi",
    "start": "843680",
    "end": "851360"
  },
  {
    "text": "hooks in here and of all of these components this bottom one is the most important",
    "start": "851360",
    "end": "857760"
  },
  {
    "text": "because it does all of the heavy lifting for injecting gpu support into a container",
    "start": "857760",
    "end": "863440"
  },
  {
    "text": "as such the majority of the code we added for mix support and containers lives here",
    "start": "863440",
    "end": "871040"
  },
  {
    "start": "870000",
    "end": "870000"
  },
  {
    "text": "in the context of kubernetes one needs to ensure that the container runtime in use is configured to work with the nvidia",
    "start": "871040",
    "end": "876800"
  },
  {
    "text": "container toolkit under the hood for example docker can be configured like this",
    "start": "876800",
    "end": "884399"
  },
  {
    "text": "container d can be configured like this and cryo can be configured like this",
    "start": "884399",
    "end": "893040"
  },
  {
    "text": "and once you have that set up a component called the nvidia kh device plugin can be installed",
    "start": "893519",
    "end": "898560"
  },
  {
    "text": "to allow gpu resources to be requested as shown here when invoked this plugin sets things up",
    "start": "898560",
    "end": "905120"
  },
  {
    "text": "so that the nvidia container toolkit will be triggered under the hood to inject any necessary libraries and gpu hardware into a container at runtime",
    "start": "905120",
    "end": "913839"
  },
  {
    "text": "additionally a component called gpu feature discovery can be installed to apply labels to a node with the",
    "start": "914720",
    "end": "920160"
  },
  {
    "text": "various properties of the gpus installed on that node the user can then define a node selector",
    "start": "920160",
    "end": "926320"
  },
  {
    "text": "to direct the pod to a node with a specific type of gpu installed on it in the example here we are directing the",
    "start": "926320",
    "end": "932800"
  },
  {
    "text": "pod to a node with a100 gpus running cuda 11 and the nvidia r450",
    "start": "932800",
    "end": "938839"
  },
  {
    "text": "driver so what does this look like in the mig world then",
    "start": "938839",
    "end": "944720"
  },
  {
    "start": "940000",
    "end": "940000"
  },
  {
    "text": "well if this is how we inject full gpus into a container then this is how it has been extended to",
    "start": "944720",
    "end": "950160"
  },
  {
    "text": "support mig specifically there's a new colon syntax",
    "start": "950160",
    "end": "955279"
  },
  {
    "text": "that allows you to specify both the index of the top level gpu or the make device exists followed by the index of the specific",
    "start": "955279",
    "end": "961759"
  },
  {
    "text": "mic device within that gpu of course you can always specify the full uuid of the mig device you would",
    "start": "961759",
    "end": "968720"
  },
  {
    "text": "like to inject as well likewise in kubernetes if this is how",
    "start": "968720",
    "end": "974160"
  },
  {
    "text": "you request gpus in a pod spec then requesting access to a mic device can be done similarly",
    "start": "974160",
    "end": "981040"
  },
  {
    "text": "instead of asking for a resource type nvidia.com gpu you instead ask for a resource type",
    "start": "981040",
    "end": "986959"
  },
  {
    "text": "of nvidia.com slash make dash whatever make device type you want",
    "start": "986959",
    "end": "992000"
  },
  {
    "text": "in this case 1g.5gb in order for make devices to be",
    "start": "992000",
    "end": "998079"
  },
  {
    "text": "advertised like this the kh device plugin must be configured with what's called the mixed mig strategy it's so named",
    "start": "998079",
    "end": "1006320"
  },
  {
    "text": "because a mixture of different mig devices and full gpus can all be advertised from the same underlying node with this",
    "start": "1006320",
    "end": "1011680"
  },
  {
    "text": "strategy turned on this is in contrast to what we call the single strategy which allows one to",
    "start": "1011680",
    "end": "1018639"
  },
  {
    "text": "continue using the existing nvidia.com gpu resource type but make sure that labels are defined",
    "start": "1018639",
    "end": "1025199"
  },
  {
    "text": "such that a user is able to get to the underlying make device type that they want it's so named because in this setup a",
    "start": "1025199",
    "end": "1032160"
  },
  {
    "text": "node is only able to advertise a single mig device type across all of its gpus rather than",
    "start": "1032160",
    "end": "1037520"
  },
  {
    "text": "allowing a mix and match of different types and that's really all there is to it",
    "start": "1037520",
    "end": "1043199"
  },
  {
    "text": "from the end user's perspective so long as you've installed these versions of the components seen here you should have everything you need to",
    "start": "1043199",
    "end": "1049280"
  },
  {
    "text": "enable make support in containers and kubernetes",
    "start": "1049280",
    "end": "1053840"
  },
  {
    "start": "1054000",
    "end": "1054000"
  },
  {
    "text": "so how does all this work under the hood what is the nvidia container toolkit actually doing to give a container",
    "start": "1055600",
    "end": "1061280"
  },
  {
    "text": "access to a mic device well stepping back for a second and looking what it does for full gpus",
    "start": "1061280",
    "end": "1067760"
  },
  {
    "text": "it first injects all of the device nodes seen here at the top and then selectively injects device",
    "start": "1067760",
    "end": "1072960"
  },
  {
    "text": "nodes from the bottom depending on which gpus the container should actually have access to",
    "start": "1072960",
    "end": "1079440"
  },
  {
    "text": "it then limits the view of gpu devices under proc driver nvidia gpus to something like what you see here on",
    "start": "1079760",
    "end": "1085919"
  },
  {
    "text": "the right where each entry in this folder corresponds to the pcie bus id of a full",
    "start": "1085919",
    "end": "1092240"
  },
  {
    "text": "gpu whose device node has just been injected when moving to the mig world you still",
    "start": "1092240",
    "end": "1099120"
  },
  {
    "text": "need to inject the set of device nodes seen at the top but instead of selectively injecting a single device node from the bottom",
    "start": "1099120",
    "end": "1106320"
  },
  {
    "text": "you now need to inject three device nodes one for each component of the three tuple representing the mig device",
    "start": "1106320",
    "end": "1114799"
  },
  {
    "text": "but what are these funky names for the device nodes representing the gpu and compute instance why aren't they just named for the",
    "start": "1115200",
    "end": "1120640"
  },
  {
    "text": "components they represent well it boils down to a new abstraction",
    "start": "1120640",
    "end": "1125679"
  },
  {
    "text": "provided by the nvidia kernel driver called nvidia capabilities",
    "start": "1125679",
    "end": "1130799"
  },
  {
    "text": "whenever a gpu instance or compute instance is created a set of capability files are created",
    "start": "1130799",
    "end": "1136080"
  },
  {
    "text": "under slash proc which point to the set of device nodes under slash dev which will give a user access to them",
    "start": "1136080",
    "end": "1143520"
  },
  {
    "text": "so taking this example that represents the access files for a mig device on gpu 0.",
    "start": "1143600",
    "end": "1149600"
  },
  {
    "text": "if we cat the access file for the gpu instance we see that it contains a reference to the minor number of a device node",
    "start": "1149600",
    "end": "1158080"
  },
  {
    "text": "likewise if we cut the access file of the compute instance we see something similar",
    "start": "1158160",
    "end": "1165200"
  },
  {
    "text": "and since these devices are all part of this new nvidia capabilities framework we can find the devices they actually",
    "start": "1165200",
    "end": "1171039"
  },
  {
    "text": "represent under the slash dev nvidia caps folder with the admittedly non-obvious names shown here",
    "start": "1171039",
    "end": "1179840"
  },
  {
    "text": "now similar to how we limited the view of slash proc driver nvidia for full gpus we also limit the view of this folder",
    "start": "1180480",
    "end": "1186480"
  },
  {
    "text": "for mig devices specifically to only see the access files for the gpu and compute instances",
    "start": "1186480",
    "end": "1192320"
  },
  {
    "text": "it has access to under proc driver nvidia capabilities",
    "start": "1192320",
    "end": "1198080"
  },
  {
    "start": "1199000",
    "end": "1199000"
  },
  {
    "text": "okay now we know how to give a container access to a mic device but how do we actually create these mig devices in the first place",
    "start": "1199360",
    "end": "1206720"
  },
  {
    "text": "what software is needed to take a node and configure a bunch of mic devices on it so that the nvidia container toolkit",
    "start": "1206720",
    "end": "1212640"
  },
  {
    "text": "k8's device plug-in and gpu feature discovery components can make use of them",
    "start": "1212640",
    "end": "1218639"
  },
  {
    "text": "well unfortunately it's not quite as straightforward as one might hope there are actually two distinct",
    "start": "1218720",
    "end": "1224000"
  },
  {
    "text": "workflows you need to consider when configuring a gpu for use with mig enabling mig mode on a gpu in the first",
    "start": "1224000",
    "end": "1230240"
  },
  {
    "text": "place and then taking a mig-enabled gpu and configuring it with a set of mic devices",
    "start": "1230240",
    "end": "1236880"
  },
  {
    "text": "the commands seen here can be used to perform both of these steps in the case of creating 7 1g 5gb devices across all gpus on a",
    "start": "1236880",
    "end": "1244400"
  },
  {
    "text": "node but what steps are required to enable mig mode on the gpu well unfortunately",
    "start": "1244400",
    "end": "1251440"
  },
  {
    "text": "it's not as simple as just running the command i showed on the previous slide under the hood a mig mode switch",
    "start": "1251440",
    "end": "1257039"
  },
  {
    "text": "actually requires a full gpu reset which not only requires that all gpu workloads are complete",
    "start": "1257039",
    "end": "1263280"
  },
  {
    "text": "but also that all support clients are disconnected from the underlying nvidia driver including components like the kx device",
    "start": "1263280",
    "end": "1269760"
  },
  {
    "text": "plugin and gpu feature discovery as anyone that's struggled through this process in the past knows",
    "start": "1269760",
    "end": "1275679"
  },
  {
    "text": "enumerating this set of clients and reconnecting them after a reset is a real pain",
    "start": "1275679",
    "end": "1280799"
  },
  {
    "text": "moreover when running inside a vm like you would on ec2 or google compute engine a full node reboot is actually required",
    "start": "1280799",
    "end": "1287520"
  },
  {
    "text": "in order to carry out this reset operation as such this operation is considered",
    "start": "1287520",
    "end": "1293600"
  },
  {
    "text": "very heavy weight and should only be carried out very infrequently",
    "start": "1293600",
    "end": "1299039"
  },
  {
    "text": "in contrast configuring the actual set of mig devices on a mig-enabled gpu is actually pretty dynamic",
    "start": "1300000",
    "end": "1306240"
  },
  {
    "text": "it does come with its challenges though first the order in which make devices are created on a gpu matters",
    "start": "1306240",
    "end": "1313360"
  },
  {
    "text": "if you remember the diagram i showed you at the beginning of this talk depending on the order in which you create your mig devices",
    "start": "1313360",
    "end": "1319280"
  },
  {
    "text": "you may end up with fragmentation and not be able to create a device that technically should fit on the gpu",
    "start": "1319280",
    "end": "1324720"
  },
  {
    "text": "but there are not enough adjacent slices available to accommodate it additionally you will need to restart",
    "start": "1324720",
    "end": "1331520"
  },
  {
    "text": "any components that cache previous mic device state after reconfiguration has taken place both the kh device",
    "start": "1331520",
    "end": "1337919"
  },
  {
    "text": "plugin and gpu feature discovery currently qualify for this and finally these last two challenges",
    "start": "1337919",
    "end": "1344960"
  },
  {
    "text": "shown here are the ones most complained about when talking with users of make mig configurations do not persist across",
    "start": "1344960",
    "end": "1351840"
  },
  {
    "text": "a node reboot and managing make device state across a cluster of machines is not well",
    "start": "1351840",
    "end": "1358720"
  },
  {
    "text": "supported to address each of these challenges we've created a tool called the nvidia mig partition editor",
    "start": "1358840",
    "end": "1365120"
  },
  {
    "start": "1360000",
    "end": "1360000"
  },
  {
    "text": "or make parted for short using this tool one can declaratively",
    "start": "1365120",
    "end": "1371440"
  },
  {
    "text": "define all of the possible mic configurations they may want to apply to different nodes around their cluster",
    "start": "1371440",
    "end": "1378080"
  },
  {
    "text": "they can then make this file available to all of their nodes and then run this simple nvidia",
    "start": "1378080",
    "end": "1384000"
  },
  {
    "text": "mig-parted command to apply one of the configs from this file to the node",
    "start": "1384000",
    "end": "1389840"
  },
  {
    "text": "in addition to this base partitioning tool we also provide a systemd service wrapper around it which can",
    "start": "1390720",
    "end": "1395919"
  },
  {
    "text": "do three things first it can persist mid configurations across node reboots",
    "start": "1395919",
    "end": "1401600"
  },
  {
    "text": "it can apply mig mode changes without the nvidia driver being loaded and it can automatically handle the",
    "start": "1401600",
    "end": "1407440"
  },
  {
    "text": "start and stop of any gpu clients across a configuration",
    "start": "1407440",
    "end": "1412559"
  },
  {
    "text": "and we are planning on releasing a kubernetes service wrapper soon it provides similar functionality to the",
    "start": "1413520",
    "end": "1418640"
  },
  {
    "text": "system dwrapper but can be run directly on kubernetes instead of systemd",
    "start": "1418640",
    "end": "1424400"
  },
  {
    "text": "it will then become one of the core components of the nvidia gpu operator which i didn't talk about today but is",
    "start": "1424400",
    "end": "1430400"
  },
  {
    "text": "one of the key components in nvidia's arsenal of gpu support on kubernetes",
    "start": "1430400",
    "end": "1436159"
  },
  {
    "text": "and with that i'm now going to show a demo of all of these pieces in action",
    "start": "1438240",
    "end": "1443360"
  },
  {
    "text": "i'll first show a kubernetes setup capable of advertising eight full gpus on a dgx a100 box and run a pod to",
    "start": "1443360",
    "end": "1450960"
  },
  {
    "text": "consume one of these gpus i'll then reconfigure the box to advertise 56",
    "start": "1450960",
    "end": "1456840"
  },
  {
    "text": "1g.5 gb devices using the mixed mig strategy and run a pod to consume one of these",
    "start": "1456840",
    "end": "1462720"
  },
  {
    "text": "mic devices finally i'll switch things over to the single mig strategy",
    "start": "1462720",
    "end": "1468000"
  },
  {
    "text": "and run things again",
    "start": "1468000",
    "end": "1473840"
  },
  {
    "text": "okay so this demo is going to be run on a dgx a100 server with eight 40 gigabyte a100 gpus",
    "start": "1476159",
    "end": "1483679"
  },
  {
    "text": "installed to start all of these gpus currently have migmo disabled",
    "start": "1483679",
    "end": "1489520"
  },
  {
    "text": "which i can show you using the export sub command of the nvidia mig parted tool as shown here",
    "start": "1489520",
    "end": "1496480"
  },
  {
    "text": "next i'll run a command on the host called nvidia smi which will print out a summary of all of",
    "start": "1497120",
    "end": "1502159"
  },
  {
    "text": "the gpus installed on the node here we see all eight a100 gpus being",
    "start": "1502159",
    "end": "1508320"
  },
  {
    "text": "listed if we recall during the talk i mentioned",
    "start": "1508320",
    "end": "1513520"
  },
  {
    "text": "that the nvidia container toolkit was a prerequisite to running with gpu support on kubernetes",
    "start": "1513520",
    "end": "1518960"
  },
  {
    "text": "this next command just prints out the versions of the nvidia container tool components i have installed on this machine",
    "start": "1518960",
    "end": "1525919"
  },
  {
    "text": "the versions shown here are consistent with the minimum versions i showed in the talk for running with mag support",
    "start": "1525919",
    "end": "1532720"
  },
  {
    "text": "i also mentioned that the runtime in use by kubernetes needs to be configured for use with the nvidia container toolkit",
    "start": "1533679",
    "end": "1540880"
  },
  {
    "text": "well for this demo our kubernetes setup is running with docker and this command just verifies this",
    "start": "1540880",
    "end": "1546000"
  },
  {
    "text": "configuration as you can see here the default runtime for docker is set to nvidia",
    "start": "1546000",
    "end": "1554080"
  },
  {
    "text": "this next command just shows that we aren't currently running any nvidia components in our kubernetes deployment",
    "start": "1555120",
    "end": "1562000"
  },
  {
    "text": "it also shows that we are running kubernetes in a single node setup with all of its components running on this single node",
    "start": "1562320",
    "end": "1571360"
  },
  {
    "text": "next i'll show the set of allocatable resources that are currently available in the cluster",
    "start": "1571360",
    "end": "1577679"
  },
  {
    "text": "as you can see there are some cpus some memory some disk space etc you may also notice",
    "start": "1578000",
    "end": "1584640"
  },
  {
    "text": "though that there are in fact a few nvidia resources listed here that are all set to zero",
    "start": "1584640",
    "end": "1590720"
  },
  {
    "text": "this is just an artifact of the fact that i've run previous versions of the nvidia cage device plug-in on this node",
    "start": "1590720",
    "end": "1596000"
  },
  {
    "text": "and the scheduler hasn't cleaned up any state for them yet the fact that they're all zero though",
    "start": "1596000",
    "end": "1601120"
  },
  {
    "text": "shows that none of these resources are currently being advertised",
    "start": "1601120",
    "end": "1605760"
  },
  {
    "text": "next i'll run a set of helm commands to set up the appropriate repos to install the latest case device plugin and gpu",
    "start": "1606640",
    "end": "1612559"
  },
  {
    "text": "feature discovery components",
    "start": "1612559",
    "end": "1615840"
  },
  {
    "text": "and then i'll install these components",
    "start": "1617840",
    "end": "1621200"
  },
  {
    "text": "themselves and then print out what has been installed from helm's perspective",
    "start": "1624840",
    "end": "1630960"
  },
  {
    "text": "now that i've done that i'll show these pods running on kubernetes itself",
    "start": "1632799",
    "end": "1640080"
  },
  {
    "text": "i'll then show that the set of allocatable gpus has been updated to eight",
    "start": "1640080",
    "end": "1645278"
  },
  {
    "text": "and that there now exists a set of labels describing the properties of the a100 gpus on this node",
    "start": "1646720",
    "end": "1654080"
  },
  {
    "text": "the most notable things here being the gpu product label and the number of gpus available",
    "start": "1654080",
    "end": "1660960"
  },
  {
    "text": "and with that i'll actually run a pod to consume a gpu and print it out by nvidia smi",
    "start": "1663120",
    "end": "1670158"
  },
  {
    "text": "and here it is",
    "start": "1671039",
    "end": "1673759"
  },
  {
    "text": "now if you remember the first command i ran in this demo was nvidia mig parted export i'm just going to run that again to",
    "start": "1676080",
    "end": "1682080"
  },
  {
    "text": "remind you what its output looked like with mig disabled on all the gpus",
    "start": "1682080",
    "end": "1687440"
  },
  {
    "text": "now i'm going to show you the contents of a mig parted configuration file where the main thing to note here is the",
    "start": "1688480",
    "end": "1695120"
  },
  {
    "text": "all 1g 5gb configuration that i have highlighted this",
    "start": "1695120",
    "end": "1701039"
  },
  {
    "text": "configuration says to take all gpus on the node and divide them into seven 1g.5gb",
    "start": "1701039",
    "end": "1707039"
  },
  {
    "text": "devices each and when i then run this next command",
    "start": "1707039",
    "end": "1712320"
  },
  {
    "text": "this new configuration will be applied note however that i am running with the",
    "start": "1712320",
    "end": "1718559"
  },
  {
    "text": "time command because this command has to disconnect all drivers as well as perform a gpu reset to switch each gpu",
    "start": "1718559",
    "end": "1724960"
  },
  {
    "text": "into make mode this command actually takes about five minutes to complete and i won't make you all wait but i'm",
    "start": "1724960",
    "end": "1731760"
  },
  {
    "text": "including the time command here so we can see how much time is actually passed once we come back",
    "start": "1731760",
    "end": "1739840"
  },
  {
    "text": "okay so as you can see the command took about five minutes to complete and if i then run nvidia mig parted",
    "start": "1740880",
    "end": "1746960"
  },
  {
    "text": "export on it we can see that the configuration has indeed been applied successfully",
    "start": "1746960",
    "end": "1753840"
  },
  {
    "text": "and with that in place i can re-run my commands to print out the set of allocatable resources",
    "start": "1755120",
    "end": "1760240"
  },
  {
    "text": "and the set of node labels that are applied where we now see 56 1g 5gb devices",
    "start": "1760240",
    "end": "1768080"
  },
  {
    "text": "instead of 8 full gpus as well as a bunch of other labels showing the properties of these mig devices",
    "start": "1768080",
    "end": "1776158"
  },
  {
    "text": "i can then run a pod to consume this mig device as shown here except that in this case i'm going to do",
    "start": "1777440",
    "end": "1783440"
  },
  {
    "text": "more than just print nvidia smi i'm also going to list out the set of device nodes and the limited view of",
    "start": "1783440",
    "end": "1789440"
  },
  {
    "text": "proc driver nvidia capabilities inside the container",
    "start": "1789440",
    "end": "1794960"
  },
  {
    "text": "as you can see we have access to exactly one mig device and all three device nodes representing",
    "start": "1794960",
    "end": "1800559"
  },
  {
    "text": "that mig device are present we also only see access files for the gpu instance and compute instance of our",
    "start": "1800559",
    "end": "1807279"
  },
  {
    "text": "particular device and no others",
    "start": "1807279",
    "end": "1811440"
  },
  {
    "text": "now the final thing i want to show you is this working with the single mig strategy instead of the mixed strategy",
    "start": "1813919",
    "end": "1819520"
  },
  {
    "text": "if you recall this strategy allows us to overload the nvidia.com gpu resource type for mig devices and",
    "start": "1819520",
    "end": "1826240"
  },
  {
    "text": "assumes a user will use a label to direct a pod at the proper device type",
    "start": "1826240",
    "end": "1831600"
  },
  {
    "text": "so i first upgrade the plugin to this strategy followed by upgrading gpu feature",
    "start": "1831600",
    "end": "1837679"
  },
  {
    "text": "discovery to this strategy and then i print out the set of",
    "start": "1837679",
    "end": "1842880"
  },
  {
    "text": "allocatable resources and node labels again",
    "start": "1842880",
    "end": "1846960"
  },
  {
    "text": "where we now see that there are 56 nvidia.com gpu resource types instead of eight",
    "start": "1848000",
    "end": "1855120"
  },
  {
    "text": "likewise our labels have all been updated to put the mig properties directly on the gpu resource",
    "start": "1855760",
    "end": "1861039"
  },
  {
    "text": "type rather than in the individual named mig device where the product label now encodes",
    "start": "1861039",
    "end": "1868080"
  },
  {
    "text": "which mig device this resource corresponds to so user can set their node selector appropriately",
    "start": "1868080",
    "end": "1875679"
  },
  {
    "text": "if i then go back and run a pod against this setup you see that i am once again requesting the nvidia.com",
    "start": "1876399",
    "end": "1882159"
  },
  {
    "text": "gpu resource type but then inspecting the things you would expect to see when injecting a mig device",
    "start": "1882159",
    "end": "1889600"
  },
  {
    "text": "as you can see everything is present as expected nvidia smi shows the correct make device",
    "start": "1890159",
    "end": "1896720"
  },
  {
    "text": "and all of the device nodes and proc files are present",
    "start": "1896720",
    "end": "1901600"
  },
  {
    "start": "1902000",
    "end": "1902000"
  },
  {
    "text": "and with that that brings us to the end of my presentation i'll leave this final slide up while i",
    "start": "1903760",
    "end": "1909200"
  },
  {
    "text": "continue to answer any questions you have on slack thank you",
    "start": "1909200",
    "end": "1916320"
  }
]