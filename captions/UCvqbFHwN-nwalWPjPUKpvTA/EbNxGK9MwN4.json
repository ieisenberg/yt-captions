[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "I'm gonna go again head and get started in custards quite a bit of stuff to go over so thanks everyone for joining me on the",
    "start": "0",
    "end": "7560"
  },
  {
    "text": "early session on the second day my name is Aaron Levie I'm a software engineer at core OS and one of the",
    "start": "7560",
    "end": "15420"
  },
  {
    "text": "things I want to talk about today is self hosted kubernetes so when I say self hosted kubernetes kind of show of",
    "start": "15420",
    "end": "21660"
  },
  {
    "text": "hands has anyone heard this phrase does it mean anything awesome sweet",
    "start": "21660",
    "end": "27170"
  },
  {
    "text": "so just to start off a quick explanation of you know what exactly is so postal",
    "start": "27170",
    "end": "32340"
  },
  {
    "text": "kubernetes when I say this what I what I mean is that you know communities is responsible",
    "start": "32340",
    "end": "38640"
  },
  {
    "start": "33000",
    "end": "347000"
  },
  {
    "text": "for managing its own core components and that those core components are actually",
    "start": "38640",
    "end": "43739"
  },
  {
    "text": "deployed as native API objects so that means things like the API server the",
    "start": "43739",
    "end": "48870"
  },
  {
    "text": "scheduler the controller manager these are modeled as deployments and daemon sets and secrets and services so if we",
    "start": "48870",
    "end": "55980"
  },
  {
    "text": "were to actually look at a self-hosted control plane this is what it would look",
    "start": "55980",
    "end": "61530"
  },
  {
    "text": "like if we said coop control get deployments get daemon sets get secrets we can see that for deployments we have",
    "start": "61530",
    "end": "67590"
  },
  {
    "text": "our controller manager and our scheduler and we're running a couple copies for daemon sets we have the API server",
    "start": "67590",
    "end": "73530"
  },
  {
    "text": "and the proxy and then we also distribute some of the assets that these components need as secrets so for the",
    "start": "73530",
    "end": "79770"
  },
  {
    "text": "API server that would be things like the TOS assets the controller manager that might be things like the service account",
    "start": "79770",
    "end": "85500"
  },
  {
    "text": "keys so that's just like really brief like",
    "start": "85500",
    "end": "90750"
  },
  {
    "text": "well actually one one other point that I wanted to make when I say it's self hosted communities and it's responsible",
    "start": "90750",
    "end": "97590"
  },
  {
    "text": "for managing its own components what we're thinking of as a flat cluster so this isn't like a control plane in",
    "start": "97590",
    "end": "103680"
  },
  {
    "text": "kubernetes than managing multiple other kubernetes inside of it this is actually the control plane of kubernetes managing",
    "start": "103680",
    "end": "109920"
  },
  {
    "text": "itself in a single flat cluster so if we have a self-hosted cluster or",
    "start": "109920",
    "end": "116369"
  },
  {
    "text": "what's important about this why do we want this one of the reasons is that you know we've already you know drunk the",
    "start": "116369",
    "end": "123380"
  },
  {
    "text": "container kool-aid like we all believe in container izing our applications we think that that's the right way forward",
    "start": "123380",
    "end": "129060"
  },
  {
    "text": "so why don't we do the same thing for kubernetes it's you know there's many ways that we can actually deploy the",
    "start": "129060",
    "end": "134939"
  },
  {
    "text": "kubernetes components we could run them as binaries but if we already kind of believe in this this method of running",
    "start": "134939",
    "end": "142019"
  },
  {
    "text": "our applications why not do the same for kubernetes and then two other really big reasons and I'll go into more detail are",
    "start": "142019",
    "end": "148290"
  },
  {
    "text": "that by self-hosting kubernetes we can vastly simplify simplify how we",
    "start": "148290",
    "end": "153420"
  },
  {
    "text": "bootstrap our nodes and then also greatly simplify how we actually manage the lifecycle of the cluster",
    "start": "153420",
    "end": "160219"
  },
  {
    "text": "so going over how we actually can simplify the the node bootstrap",
    "start": "160219",
    "end": "165530"
  },
  {
    "text": "one of the main things is that when we're saying that all of the components in kubernetes are going to be run as as",
    "start": "165530",
    "end": "170909"
  },
  {
    "text": "communities objects we're essentially saying that we're going to be stacking them on top of the Kubla and removing that necessity of configuring them on",
    "start": "170909",
    "end": "178019"
  },
  {
    "text": "the host so what we get the point that we get to is that our on host requirements essentially become we have",
    "start": "178019",
    "end": "183540"
  },
  {
    "text": "a couplet we have a container run time and we have a coop config with enough information of how to contact an API",
    "start": "183540",
    "end": "188790"
  },
  {
    "text": "server and that that's essentially what you need to configure for a node and so no matter how you're actually",
    "start": "188790",
    "end": "194250"
  },
  {
    "text": "configuring them those are essentially the three things that you need and this is true for all nodes including master",
    "start": "194250",
    "end": "199620"
  },
  {
    "text": "nodes so for a core OS node what this might look like in a very simplified way is",
    "start": "199620",
    "end": "205760"
  },
  {
    "text": "like the other cloud config the two things that we actually need to set up",
    "start": "205760",
    "end": "211169"
  },
  {
    "text": "are a Kubla service and the coop config you know you would actually want to add the flags that you wanted for your",
    "start": "211169",
    "end": "217560"
  },
  {
    "text": "equivalent there but core OS itself ships with both docker and rocket as part of part of core OS Linux so that's",
    "start": "217560",
    "end": "224430"
  },
  {
    "text": "already available for you and this becomes your generic every node config you're just throwing compute out your",
    "start": "224430",
    "end": "230819"
  },
  {
    "text": "cluster without really having to configure that much on the host but ultimately this isn't just a core OS",
    "start": "230819",
    "end": "237989"
  },
  {
    "text": "specific thing when we're abstracting away all of our on host requirements",
    "start": "237989",
    "end": "243750"
  },
  {
    "text": "what that means is that for any distribution we get these same benefits because we're layering it on top of",
    "start": "243750",
    "end": "249060"
  },
  {
    "text": "kubernetes and the process again just becomes you know we need to install the couplet we need to install it container",
    "start": "249060",
    "end": "255750"
  },
  {
    "text": "run time we should have a coop config and then we start the couplet now this is compute that is added to our cluster",
    "start": "255750",
    "end": "263350"
  },
  {
    "text": "so I also said this should be all nodes and not just master nodes so if you know what makes a master node",
    "start": "263350",
    "end": "271310"
  },
  {
    "text": "is is certain components are running on that that host and that's something that",
    "start": "271310",
    "end": "276530"
  },
  {
    "text": "we get for free from kubernetes because we're saying well these components are pieces of software that managing inside",
    "start": "276530",
    "end": "282740"
  },
  {
    "text": "of kubernetes so we can give you breeze the information that needs to know how to schedule those components so we could",
    "start": "282740",
    "end": "288560"
  },
  {
    "text": "do something as simple as adding a particular label selector to something",
    "start": "288560",
    "end": "293780"
  },
  {
    "text": "that we deploy so we can say master equals true we could add this via to control just labeling a particular node",
    "start": "293780",
    "end": "299690"
  },
  {
    "text": "or we could precede this information on the couplet as a flag so that when the node registers it already knows that it",
    "start": "299690",
    "end": "306440"
  },
  {
    "text": "should have this label so if we actually go back to this slide we can see here",
    "start": "306440",
    "end": "311840"
  },
  {
    "text": "for the API server we have master equals true as the node selector and we see we only have one copy of it running but",
    "start": "311840",
    "end": "318229"
  },
  {
    "text": "right below it there's the coop proxy where we have three copies running and no node selector so this is a three node",
    "start": "318229",
    "end": "323660"
  },
  {
    "text": "cluster where we've told the API server that we only want it running on a particular",
    "start": "323660",
    "end": "329060"
  },
  {
    "text": "node and we might want this maybe because of network address ability or at CD address ability or something of that",
    "start": "329060",
    "end": "335180"
  },
  {
    "text": "sort but rather than a special case in our nodes themselves we add this into",
    "start": "335180",
    "end": "340370"
  },
  {
    "text": "the scheduling mechanics of kubernetes because there's self-hosted components it will figure out where it should run",
    "start": "340370",
    "end": "345530"
  },
  {
    "text": "for us so ultimately what this comes back to is that you know we get a simple and",
    "start": "345530",
    "end": "354020"
  },
  {
    "start": "347000",
    "end": "512000"
  },
  {
    "text": "universal on host configuration so this is like every node doesn't matter what it is we're giving the same you know",
    "start": "354020",
    "end": "360560"
  },
  {
    "text": "very simple installation and then however you want to add compute so if",
    "start": "360560",
    "end": "366289"
  },
  {
    "text": "you're in a if you're in a cloud environment you can use you know maybe it's auto scaling groups or maybe it's",
    "start": "366289",
    "end": "372500"
  },
  {
    "text": "the the cloud environments GUI or maybe it's CLI tools but you're essentially just launching very simple",
    "start": "372500",
    "end": "378099"
  },
  {
    "text": "infrastructure and throwing it at your kubernetes cluster to add it in and then everything else we let kubernetes manage",
    "start": "378099",
    "end": "384380"
  },
  {
    "text": "so once we've thrown that compute out the cluster we start stacking our applications including kubernetes itself",
    "start": "384380",
    "end": "389660"
  },
  {
    "text": "on top of that so one of the other major things and",
    "start": "389660",
    "end": "394669"
  },
  {
    "text": "something that I find super cool about self-hosting kubernetes is",
    "start": "394669",
    "end": "399780"
  },
  {
    "text": "being able to really simplify how we actually manage our clusters",
    "start": "399780",
    "end": "405510"
  },
  {
    "text": "and where this kind of come from is that you know kubernetes is really really good at",
    "start": "405510",
    "end": "412180"
  },
  {
    "text": "managing software life cycles like if we have our application and you want to do",
    "start": "412180",
    "end": "417220"
  },
  {
    "text": "a rolling update of that application across our cluster this is something that's already built into kubernetes you know you have a deployment you add a new",
    "start": "417220",
    "end": "423820"
  },
  {
    "text": "deployment and it manages the rowing update in between if you know what we end up doing instead",
    "start": "423820",
    "end": "431080"
  },
  {
    "text": "is we start writing external tools to kind of do these same things except just for kubernetes well we kind of end up",
    "start": "431080",
    "end": "437830"
  },
  {
    "text": "with is like less portable and more fragile solutions and I'm personally guilty of this probably like three times",
    "start": "437830",
    "end": "443470"
  },
  {
    "text": "over I've written external tools to manage kubernetes and at the end of the day Karina's is always going to be",
    "start": "443470",
    "end": "449800"
  },
  {
    "text": "better than at this stuff and any tool that I'm gonna end up writing and so we should try instead to kind of build on",
    "start": "449800",
    "end": "456970"
  },
  {
    "text": "the shoulders of kubernetes take advantage of all of these strengths that it has about managing our actual application life cycles and then have it",
    "start": "456970",
    "end": "463510"
  },
  {
    "text": "manage itself as well and so when I say can building on the shoulders of kubernetes what I mean is",
    "start": "463510",
    "end": "470740"
  },
  {
    "text": "that like for example upgrading your cluster rather than building this complex doing external to kubernetes",
    "start": "470740",
    "end": "476740"
  },
  {
    "text": "itself to then go and upgrade the components and know things about rolling these updates and how your nodes are",
    "start": "476740",
    "end": "481860"
  },
  {
    "text": "managed which ones are masters which ones are not ultimately we should be able to get to a point where these are",
    "start": "481860",
    "end": "488020"
  },
  {
    "text": "like the four commands in a manual upgrade process we say coup control apply our new manifests and what's",
    "start": "488020",
    "end": "494260"
  },
  {
    "text": "happening underneath is that you know kubernetes is doing these rolling updates of these updated deployments and",
    "start": "494260",
    "end": "499690"
  },
  {
    "text": "these updated objects for us and our interface to it is just the kubernetes api and I have a little note down here",
    "start": "499690",
    "end": "506260"
  },
  {
    "text": "that says we're not quite to this point of running these four commands upgrade a self-hosted cluster but we're actually",
    "start": "506260",
    "end": "511480"
  },
  {
    "text": "really close so I actually want to show that real quick",
    "start": "511480",
    "end": "517289"
  },
  {
    "text": "to make this smaller for you that's oh ok for people",
    "start": "518280",
    "end": "524880"
  },
  {
    "text": "let's see so like we saw in one of the slides",
    "start": "525390",
    "end": "534250"
  },
  {
    "text": "you know we have our deployments so we have a controller manager we have a scheduler if we were to look at our",
    "start": "535570",
    "end": "540830"
  },
  {
    "text": "daemon sets we have the API server in the proxy if",
    "start": "540830",
    "end": "546529"
  },
  {
    "text": "we were to look at the version so my local coop control client is 1 4",
    "start": "546529",
    "end": "554180"
  },
  {
    "text": "or 5 but my actual server is 1 4 4 and so that's what we're going to be upgrading today and we're gonna upgrade",
    "start": "554180",
    "end": "560660"
  },
  {
    "text": "the control plane using nothing but coop control because there's a self-hosted cluster and it's actually fairly straightforward so",
    "start": "560660",
    "end": "566200"
  },
  {
    "text": "[Music]",
    "start": "566200",
    "end": "569289"
  },
  {
    "text": "tell me if this gets too small okay I can see it though okay so the first",
    "start": "578560",
    "end": "583960"
  },
  {
    "text": "thing that we want to do is well we have one API server one scheduler one controller manager and what a good",
    "start": "583960",
    "end": "589000"
  },
  {
    "text": "operator would do is if we're going to do a rolling update of this cluster we shouldn't rely on Singleton's of these",
    "start": "589000",
    "end": "595660"
  },
  {
    "text": "components we should actually have a highly available control plane so while we're doing the upgrade it actually",
    "start": "595660",
    "end": "600940"
  },
  {
    "text": "keeps working so the first thing we should probably do is let's label a node as a master",
    "start": "600940",
    "end": "607769"
  },
  {
    "text": "so we have three nodes I'm going to add one as a master",
    "start": "609750",
    "end": "615390"
  },
  {
    "text": "so what we should see now is that it just launched another API server and",
    "start": "619920",
    "end": "626830"
  },
  {
    "text": "this is actually something I need to look into what this match note selector is doing because it actually thinks that it didn't match for some reason but it",
    "start": "626830",
    "end": "634300"
  },
  {
    "text": "will in a second",
    "start": "634300",
    "end": "637230"
  },
  {
    "text": "okay so now we see the container creating so we've told it you know we want a second master node and so now we",
    "start": "645750",
    "end": "651839"
  },
  {
    "text": "have two API servers running so we've just made the API server H a now we need to do the same for the controller",
    "start": "651839",
    "end": "657480"
  },
  {
    "text": "manager and the scheduler so",
    "start": "657480",
    "end": "661160"
  },
  {
    "text": "and we should see that jump up",
    "start": "671449",
    "end": "676639"
  },
  {
    "text": "see you down there and then the same for the controller manager",
    "start": "678679",
    "end": "684559"
  },
  {
    "text": "alright so we just made a single singleton master into an H a master",
    "start": "689380",
    "end": "695830"
  },
  {
    "text": "control plane just using coop CTL so now let's actually roll this update so what we're going to change first is",
    "start": "695830",
    "end": "702610"
  },
  {
    "text": "the API server",
    "start": "702610",
    "end": "705360"
  },
  {
    "text": "so I'm just gonna change the image field and until I want to use one four five instead of 144 now that note that I had",
    "start": "712970",
    "end": "720110"
  },
  {
    "text": "before that said you know we're not quite to that point where we can just apply these new manifests and have it",
    "start": "720110",
    "end": "726140"
  },
  {
    "text": "have it upgraded this is is kind of what I'm talking about so deployments are sorry Damon says don't support rolling",
    "start": "726140",
    "end": "732230"
  },
  {
    "text": "updates natively in kubernetes yet there's actually an open PR about adding",
    "start": "732230",
    "end": "737240"
  },
  {
    "text": "this functionality so this is this is one of the pieces where I'm gonna save this manifest and the daemon set has",
    "start": "737240",
    "end": "743210"
  },
  {
    "text": "been updated but it's not going to roll these pods for us so I actually have to go through and now I'm going to tell the",
    "start": "743210",
    "end": "750230"
  },
  {
    "text": "API server that I want to roll the pods now down below I just have a watch command running so this is just every",
    "start": "750230",
    "end": "756110"
  },
  {
    "text": "two seconds that's hitting the API server and I'm gonna do what a rolling update underneath would actually do which is",
    "start": "756110",
    "end": "762830"
  },
  {
    "text": "you update the daemon set and then you kill one of the pods the daemon set will see well I actually should be running",
    "start": "762830",
    "end": "768260"
  },
  {
    "text": "one on one of those pods on that node so I'm gonna start a new one with the new with the new manifest",
    "start": "768260",
    "end": "775570"
  },
  {
    "text": "okay so now I happen to have hit the API server that I just terminated but the",
    "start": "779890",
    "end": "785540"
  },
  {
    "text": "next request hit the other API server so we can see that the one that I just",
    "start": "785540",
    "end": "790910"
  },
  {
    "text": "killed is now is now terminating there's actually a pretty long grace period on that so that'll take a second for it to",
    "start": "790910",
    "end": "796520"
  },
  {
    "text": "determinate but I'll get ready with this other one so we're gonna do the same thing on this other API server",
    "start": "796520",
    "end": "803920"
  },
  {
    "text": "we want to wait for this one to come back because we're good operators and we don't want to take the whole control plan down at once",
    "start": "808230",
    "end": "814800"
  },
  {
    "text": "alright so we have that that new API server we can see by the the suffix here",
    "start": "814800",
    "end": "819850"
  },
  {
    "text": "that the new pod has launched and we can also see that it's only nine seconds old so let's go ahead and delete the other",
    "start": "819850",
    "end": "825550"
  },
  {
    "text": "one sometimes if you if you delete the pod that you're contacting you get that",
    "start": "825550",
    "end": "832780"
  },
  {
    "text": "error in the file so we'll see see the API server now also terminate",
    "start": "832780",
    "end": "839020"
  },
  {
    "text": "well line up the next command cuz again the grace periods gonna take a second but this is actually the more interesting one so when you update",
    "start": "839020",
    "end": "844270"
  },
  {
    "text": "deployments it actually does it for you and so it's it's super fast when it happens",
    "start": "844270",
    "end": "850500"
  },
  {
    "text": "okay so both of our API servers have enrolled I'm going to create the controller manager now I",
    "start": "860350",
    "end": "867910"
  },
  {
    "text": "want that to be running four or five all right so when I save this what's gonna",
    "start": "869110",
    "end": "874639"
  },
  {
    "text": "happen is the new manifest is going to go in place and the deployment controller in kubernetes is going to see",
    "start": "874639",
    "end": "879740"
  },
  {
    "text": "that now it needs to do a rolling update of those components and we have two of them running and it also right now has a",
    "start": "879740",
    "end": "886819"
  },
  {
    "text": "I think by default it converts to one extra copy so what we'll probably see is like four copies of a controller manager",
    "start": "886819",
    "end": "893600"
  },
  {
    "text": "we it are it already did it like it's that fast so it's doing the rolling",
    "start": "893600",
    "end": "898790"
  },
  {
    "text": "update of those pods underneath us and keeping one running while it's doing the rolling update but you can see they're",
    "start": "898790",
    "end": "904190"
  },
  {
    "text": "both about 11 seconds old so that one's done let's do the scheduler",
    "start": "904190",
    "end": "909790"
  },
  {
    "text": "okay",
    "start": "918400",
    "end": "921180"
  },
  {
    "text": "and this is probably gonna happen pretty quick again yeah so that's already done",
    "start": "927050",
    "end": "933370"
  },
  {
    "text": "so we just we created an H a control plane from a single node and then we",
    "start": "933370",
    "end": "938570"
  },
  {
    "text": "upgraded the control plane and we didn't use anything except for control so and",
    "start": "938570",
    "end": "943580"
  },
  {
    "text": "this is just me as a person doing this manually like this wouldn't be that difficult to script right something else",
    "start": "943580",
    "end": "949850"
  },
  {
    "text": "that is more intelligent about this of greatest single component have some kind of exit criteria of saying yeah that",
    "start": "949850",
    "end": "955070"
  },
  {
    "text": "component is successfully upgraded I can move on to the next one so",
    "start": "955070",
    "end": "962080"
  },
  {
    "start": "959000",
    "end": "1023000"
  },
  {
    "text": "coming back to you know some of the ways that we simplify the lifecycle management why we",
    "start": "962080",
    "end": "968810"
  },
  {
    "text": "want to do this one of the main reasons is the any improvements that we see upstream directly translate into",
    "start": "968810",
    "end": "974930"
  },
  {
    "text": "improvements in managing kubernetes itself so Damon said rolling updates are",
    "start": "974930",
    "end": "980090"
  },
  {
    "text": "great for our applications and as soon as that's an upstream we can use that for kubernetes and I don't have to go",
    "start": "980090",
    "end": "985550"
  },
  {
    "text": "and roll those API server pods anymore and then also another big point is that",
    "start": "985550",
    "end": "990820"
  },
  {
    "text": "you know your expertise in managing your software in kubernetes then also directly translates into your expertise",
    "start": "990820",
    "end": "997370"
  },
  {
    "text": "and managing kubernetes itself rather than having these two kind of concepts in your team of well we have this whole",
    "start": "997370",
    "end": "1003400"
  },
  {
    "text": "set of tools that maybe some people understand about how to manage kubernetes and deploy communities then",
    "start": "1003400",
    "end": "1008560"
  },
  {
    "text": "we have people that are familiar with how to manage their applications within kubernetes this is just all cross over",
    "start": "1008560",
    "end": "1014200"
  },
  {
    "text": "and it all makes sense in the same exact way the way that you would introspect and debug your applications is the way",
    "start": "1014200",
    "end": "1019960"
  },
  {
    "text": "that you would introspect and debug your actual kubernetes installation itself so I kind of want to go over a little",
    "start": "1019960",
    "end": "1026410"
  },
  {
    "start": "1023000",
    "end": "1058000"
  },
  {
    "text": "bit about how it works and so some some pieces I want to touch on are you know how we launch a self-hosted cluster some",
    "start": "1026410",
    "end": "1034329"
  },
  {
    "text": "of the self hosted components and which ones are self hosted and which ones are not and disaster recovery to your",
    "start": "1034330",
    "end": "1039790"
  },
  {
    "text": "questions right now in this cluster it's not self",
    "start": "1039790",
    "end": "1046510"
  },
  {
    "text": "hosted but I'm actually gonna so the question was how do you run a CD right now so in that cluster that I was just",
    "start": "1046510",
    "end": "1052060"
  },
  {
    "text": "on it is not so fast but I'll come back to that and we'll talk about kind of next steps there as well",
    "start": "1052060",
    "end": "1059460"
  },
  {
    "start": "1058000",
    "end": "1117000"
  },
  {
    "text": "so in terms of launching a cellphone closer this is kind of an interesting",
    "start": "1059480",
    "end": "1066570"
  },
  {
    "text": "thing because you need it's a kind of a chicken and egg situation where you're asking the control plane that's telling",
    "start": "1066570",
    "end": "1073080"
  },
  {
    "text": "you that you need to be running a control plane but if there's no control plane there to tell you that you you're",
    "start": "1073080",
    "end": "1078299"
  },
  {
    "text": "not running anything and so what we needed to do is is we need to build a tool that can act as this temporary",
    "start": "1078299",
    "end": "1084149"
  },
  {
    "text": "control plane just long enough that a self-hosted control plane can be launched and then that initial control",
    "start": "1084149",
    "end": "1090659"
  },
  {
    "text": "plane can disappear so we have a tool called boot cube it's actually a community's incubator project but",
    "start": "1090659",
    "end": "1097769"
  },
  {
    "text": "essentially it's just a binary that starts up acts like a control plane replaces itself and then you don't need",
    "start": "1097769",
    "end": "1103919"
  },
  {
    "text": "it anymore and there should only be run on the very first node of your cluster and you don't need it after that point",
    "start": "1103919",
    "end": "1109110"
  },
  {
    "text": "because after that point it's just kind of self assembling of a self-hosted cluster add more nodes to it have work",
    "start": "1109110",
    "end": "1114480"
  },
  {
    "text": "scheduled to them but fukube doesn't actually come into it so how Bill coop works so similar the",
    "start": "1114480",
    "end": "1122039"
  },
  {
    "text": "the question that was just asked in this case let's consider that at CD is network addressable it could be on the",
    "start": "1122039",
    "end": "1129510"
  },
  {
    "text": "same host it could be a remote cluster it could be of any size but for the sake of keeping this relatively simple let's",
    "start": "1129510",
    "end": "1136350"
  },
  {
    "text": "assume that CD is is network addressable somewhere and that we have a couplet that's available on a node that we can",
    "start": "1136350",
    "end": "1141720"
  },
  {
    "text": "start up so the first thing we do is we start boot",
    "start": "1141720",
    "end": "1146880"
  },
  {
    "text": "tube and buku is just a single binary that contains the community's control plane inside of it so you start via Kubb",
    "start": "1146880",
    "end": "1152519"
  },
  {
    "text": "runs an API server runs a scheduler runs a controller manager so once that starts",
    "start": "1152519",
    "end": "1158190"
  },
  {
    "text": "you have told buku we're at CD lives as a network address and then now you",
    "start": "1158190",
    "end": "1163289"
  },
  {
    "text": "essentially have a fully fledged control plan you have your data store your API server is pointing at it and then you",
    "start": "1163289",
    "end": "1169080"
  },
  {
    "text": "have the the controller components pointing at that API server so the couplet can now connect to the API",
    "start": "1169080",
    "end": "1176669"
  },
  {
    "text": "server and let's say on port 443 it's just sitting there spinning waiting for an API server to show up and it does",
    "start": "1176669",
    "end": "1183260"
  },
  {
    "text": "so at that point once we have the full control plane inside of buku",
    "start": "1183260",
    "end": "1188600"
  },
  {
    "text": "we issue a command it says you know create the deployments the daemon sets the secrets the services that make up",
    "start": "1188600",
    "end": "1195360"
  },
  {
    "text": "our self-hosted control plan the thing that we want to replace ourselves with so the couplet gets scheduled those pods",
    "start": "1195360",
    "end": "1202409"
  },
  {
    "text": "because it's registered as as a component that can be scheduled and now we have a replacement control plane but",
    "start": "1202409",
    "end": "1208919"
  },
  {
    "text": "this replacement control plane isn't really doing anything right now the API server can't bind on four four three because it's already in use so it's just",
    "start": "1208919",
    "end": "1215580"
  },
  {
    "text": "kind of sitting there at this point but buku knows that it's it's launched a replacement and it's actually checking",
    "start": "1215580",
    "end": "1221370"
  },
  {
    "text": "and seeing this season ok those pods have started I can go away now so we killed boot cube and what happens here",
    "start": "1221370",
    "end": "1227880"
  },
  {
    "text": "is that from the coup boots perspective the only thing that changes is that it's seen a network blip on it can't connect",
    "start": "1227880",
    "end": "1234630"
  },
  {
    "text": "to 443 anymore so it was connecting to 443 boot coop dies it can't anymore this",
    "start": "1234630",
    "end": "1240299"
  },
  {
    "text": "other control plane starts up and so the API server can now bind on 443 all the components are running together it's",
    "start": "1240299",
    "end": "1246330"
  },
  {
    "text": "still referencing the same data store oops and then the Kubik can now connect",
    "start": "1246330",
    "end": "1252240"
  },
  {
    "text": "to the API server on 443 so for the mokuba's perspective it's just seen a network blip and we've actually pivoted",
    "start": "1252240",
    "end": "1258090"
  },
  {
    "text": "from boot cube to a self-hosted cluster and as far as our data is concerned we're actually coordinating coordinating",
    "start": "1258090",
    "end": "1264269"
  },
  {
    "text": "around at CD so the Kubla sees oh I should be running these three pods I am running these three pods all as well",
    "start": "1264269",
    "end": "1269760"
  },
  {
    "text": "with the world so that's like a very brief general overview of like how boo cube itself",
    "start": "1269760",
    "end": "1275850"
  },
  {
    "start": "1271000",
    "end": "1641000"
  },
  {
    "text": "works question",
    "start": "1275850",
    "end": "1279408"
  },
  {
    "text": "so the question was the version that gets launched the self-hosted cluster",
    "start": "1286740",
    "end": "1291880"
  },
  {
    "text": "does that need to be the same version as we've coop itself and right now it",
    "start": "1291880",
    "end": "1297190"
  },
  {
    "text": "should be close and the reason for that is that boot cube actually compiles in most of the communities control plan",
    "start": "1297190",
    "end": "1303370"
  },
  {
    "text": "like you saw and so if we're ven during in kubernetes and it's like you know one for something but you want to launch a",
    "start": "1303370",
    "end": "1309190"
  },
  {
    "text": "one three cluster that version skew may not actually be viable so one of the ways for that we can do is rather than",
    "start": "1309190",
    "end": "1316299"
  },
  {
    "text": "comparing those components into buku we instead just use static manifests",
    "start": "1316299",
    "end": "1321600"
  },
  {
    "text": "and so all boot coupe would be doing is doing a shuffle of static manifests temporarily that could be of any version",
    "start": "1321600",
    "end": "1328419"
  },
  {
    "text": "as long as you can read those files so that's something we've talked about changing but for now you should probably",
    "start": "1328419",
    "end": "1333730"
  },
  {
    "text": "launch a cluster that's within the same minor version it probably wouldn't be anything wrong if you didn't but yeah",
    "start": "1333730",
    "end": "1339309"
  },
  {
    "text": "and then do you have a question",
    "start": "1339309",
    "end": "1342929"
  },
  {
    "text": "right yeah so essentially what it doesn't the what we're doing is we we need a temporary starting point and that",
    "start": "1344639",
    "end": "1351460"
  },
  {
    "text": "temporary starting point could be binaries it could be static files it could be an all compiled in binary like",
    "start": "1351460",
    "end": "1357250"
  },
  {
    "text": "we just need a temporary control plane after this point we can throw it out so it doesn't even need to be that resilient it just needs to be there for",
    "start": "1357250",
    "end": "1363220"
  },
  {
    "text": "a short period of time any question",
    "start": "1363220",
    "end": "1367260"
  },
  {
    "text": "so the question was is there some kind of like upgrade contract like how the",
    "start": "1378070",
    "end": "1383690"
  },
  {
    "text": "ordering of versions that kind of thing and there is so there's actually you",
    "start": "1383690",
    "end": "1388789"
  },
  {
    "text": "should read the the upstream documents on this because there's it's even between there's certain supposedly",
    "start": "1388789",
    "end": "1394610"
  },
  {
    "text": "supported version skews but it's it's kind of undefined at this point right now essentially you want your control",
    "start": "1394610",
    "end": "1400789"
  },
  {
    "text": "plane to be a higher or equal to all of your other components so you don't want to be upgrading your couplets before you",
    "start": "1400789",
    "end": "1406879"
  },
  {
    "text": "upgrade your your API servers this has actually bitten me in the past so there are some concerns in that and so when I",
    "start": "1406879",
    "end": "1412309"
  },
  {
    "text": "went through and I said API server first and then the rest of the control green components that's kind of a very simple",
    "start": "1412309",
    "end": "1418009"
  },
  {
    "text": "process but in the future what kind of version skew is actually supported in terms of you know maybe you have you",
    "start": "1418009",
    "end": "1425119"
  },
  {
    "text": "know if I'm jumping from some minor and patch version up to a new minor version",
    "start": "1425119",
    "end": "1431480"
  },
  {
    "text": "is that supported how many of those and I think right now it's saying that there should be within three is the upstream",
    "start": "1431480",
    "end": "1437239"
  },
  {
    "text": "like sort of three minor versions but I don't know if that's in actually proved out I'm not sure how much actual version",
    "start": "1437239",
    "end": "1443480"
  },
  {
    "text": "SKU testing we're doing right now but these are all like things that we should start considering yes",
    "start": "1443480",
    "end": "1449710"
  },
  {
    "text": "yeah I mean so the same kind of thing where we're you know you consider these",
    "start": "1485250",
    "end": "1490270"
  },
  {
    "text": "operators as kind of smart tools that know how to manage the lifecycle of a particular component there's no reason",
    "start": "1490270",
    "end": "1495280"
  },
  {
    "text": "that if our contract is the kubernetes api that we couldn't build an operator that is aware of kubernetes and these",
    "start": "1495280",
    "end": "1501010"
  },
  {
    "text": "kinds of restrictions and how it should and should not be upgraded or you know the order that didn't need so current or",
    "start": "1501010",
    "end": "1506440"
  },
  {
    "text": "like the exit criteria for or upgrade to the scheduler now how do we actually exercise that to a point that we feel",
    "start": "1506440",
    "end": "1512830"
  },
  {
    "text": "good about that it's not just running but it's actually functioning normally so yeah I definitely think that there's room there",
    "start": "1512830",
    "end": "1518790"
  },
  {
    "text": "so the couplet so I took out this slide because I'm probably gonna be pretty",
    "start": "1518790",
    "end": "1524680"
  },
  {
    "text": "pressed to time on this one but so you can self host the couplet as well but ultimately even if your self hosting the",
    "start": "1524680",
    "end": "1531100"
  },
  {
    "text": "cupola there's an on host coup but there's a turtle at the bottom so four core OS what we've done is we execute",
    "start": "1531100",
    "end": "1537970"
  },
  {
    "text": "the couplet the on host pubic whether you're self hosting the couplet or not the turtle at the bottom we run by a",
    "start": "1537970",
    "end": "1544990"
  },
  {
    "text": "system D and we also execute it inside of a unconstrained rocket pod and so",
    "start": "1544990",
    "end": "1551620"
  },
  {
    "text": "essentially we're just pulling down a cupola container and so a new version is saying pull down a new Kubla container",
    "start": "1551620",
    "end": "1557020"
  },
  {
    "text": "and that's expressed through an environment variable so we have a daemon",
    "start": "1557020",
    "end": "1562150"
  },
  {
    "text": "set that we call the node agent which is just sitting there watching annotations on the node object and if it sees that",
    "start": "1562150",
    "end": "1568630"
  },
  {
    "text": "we say desired version of the the couplet is a new version then it will pull it will doesn't actually do any of",
    "start": "1568630",
    "end": "1575320"
  },
  {
    "text": "this it just sets the environment variable on the service file and then it tells system D to restart the cupola and",
    "start": "1575320",
    "end": "1581140"
  },
  {
    "text": "what ends up happening is that service restarts it pulls down the new image it starts the couplet so even that it's you",
    "start": "1581140",
    "end": "1587740"
  },
  {
    "text": "know still on the host it's the one last component on the host and it's not truly self hosted we are still pushing those",
    "start": "1587740",
    "end": "1593410"
  },
  {
    "text": "updates via the API via annotations on the node object and the new agents itself is actually pretty",
    "start": "1593410",
    "end": "1599470"
  },
  {
    "text": "straightforward like it's set an environment variable and then restart a",
    "start": "1599470",
    "end": "1604480"
  },
  {
    "text": "system d-unit but let's say you're not on core OS that could be you know DNF install or apt-get install a new version",
    "start": "1604480",
    "end": "1610600"
  },
  {
    "text": "of the Kubler that's what the node agent does but there's other interesting things we should start doing there as",
    "start": "1610600",
    "end": "1616750"
  },
  {
    "text": "well where you could you know you're going to be upgrading the kula so it's actually drain the node first because maybe this has changed some kind of",
    "start": "1616750",
    "end": "1623020"
  },
  {
    "text": "interaction with how it's going to launch and manage manage the pause all",
    "start": "1623020",
    "end": "1628150"
  },
  {
    "text": "right so let's see one of the things I wanted to go over is like when you say self host and",
    "start": "1628150",
    "end": "1633370"
  },
  {
    "text": "kubernetes there's a potentially a spectrum of what that actually encompasses so you could be self",
    "start": "1633370",
    "end": "1639010"
  },
  {
    "text": "listening to google it you may not be you may just suppose some of these components and not others so when I talk",
    "start": "1639010",
    "end": "1645190"
  },
  {
    "start": "1641000",
    "end": "1717000"
  },
  {
    "text": "about it generally what I'm thinking of is the control plane and the proxy some of these other pieces are a little bit",
    "start": "1645190",
    "end": "1650559"
  },
  {
    "text": "more experimental so if you were to self host the couplet there's still some things to be worked out there but",
    "start": "1650559",
    "end": "1655780"
  },
  {
    "text": "essentially the API server we run as a daemon set because that makes Network addressability much easier because it's",
    "start": "1655780",
    "end": "1661480"
  },
  {
    "text": "going to potentially be on a fixed set of nodes this could be a deployment in some cases and there's some some",
    "start": "1661480",
    "end": "1667630"
  },
  {
    "text": "upstream work around improving API server discovery so that as the API server moves around your cluster if it",
    "start": "1667630",
    "end": "1673390"
  },
  {
    "text": "is a deployment and it's just running as a pod anywhere that we can still discover where it's actually living and then the controller manager and",
    "start": "1673390",
    "end": "1679540"
  },
  {
    "text": "scheduler running as deployments this works really well pretty straightforward to proxy is a perfect fit for the daemon",
    "start": "1679540",
    "end": "1685990"
  },
  {
    "text": "set we want this pretty much on every node and then we set our TLS assets and service account information as secrets",
    "start": "1685990",
    "end": "1693460"
  },
  {
    "text": "so if the controller manager dies on a particular node and moves to another it's not up to us to be like well you",
    "start": "1693460",
    "end": "1699309"
  },
  {
    "text": "need these service account keys to also be on that host as well for it to function properly as that pod moves that",
    "start": "1699309",
    "end": "1705370"
  },
  {
    "text": "secret is going to be available to it as well so when I say self-hosted kubernetes like this is I feel like",
    "start": "1705370",
    "end": "1711270"
  },
  {
    "text": "completely sane and stable and reasonable and then we can kind span from here into into new components so",
    "start": "1711270",
    "end": "1718820"
  },
  {
    "start": "1717000",
    "end": "1847000"
  },
  {
    "text": "one of those is self closing the pod network so deploying the pod networking tools",
    "start": "1718820",
    "end": "1724710"
  },
  {
    "text": "actually as a daemon set this has been kind of a difficulty in a lot of",
    "start": "1724710",
    "end": "1730049"
  },
  {
    "text": "installations because it means you need to get when you're deploying a pod networking tool you need to install it",
    "start": "1730049",
    "end": "1735450"
  },
  {
    "text": "correctly on the host you need to interact with docker correctly you need it to interact with kubernetes correctly so there's a lot of knobs there that you",
    "start": "1735450",
    "end": "1743220"
  },
  {
    "text": "know kind of can make this a difficult thing so what we can do now actually is that we can coordinate around the",
    "start": "1743220",
    "end": "1749100"
  },
  {
    "text": "container network interface configuration the cni configuration so what the couplet will do is it's looking",
    "start": "1749100",
    "end": "1755039"
  },
  {
    "text": "for this configuration file and if it doesn't exist then it thinks and you've told it that you're using CNI then it",
    "start": "1755039",
    "end": "1760830"
  },
  {
    "text": "knows I can only start pods that use hosts networking so they don't need to know about the pod Network so that's",
    "start": "1760830",
    "end": "1766379"
  },
  {
    "text": "fine I can start them anything that needs the pod Network I can't start yet because I don't know how to configure it",
    "start": "1766379",
    "end": "1772289"
  },
  {
    "text": "so what this means is that rather than us having a pre configure the pod network on every one of our hosts and",
    "start": "1772289",
    "end": "1777450"
  },
  {
    "text": "complicating our on host configuration that we're still at this very simple on",
    "start": "1777450",
    "end": "1782460"
  },
  {
    "text": "hosts configuration and then we will tell it how to configure the pod network via you know native kubernetes objects",
    "start": "1782460",
    "end": "1788250"
  },
  {
    "text": "and so what happens is you could say you know coop control create the daemon set that can be tuned to your pod networking",
    "start": "1788250",
    "end": "1794639"
  },
  {
    "text": "tool that tool will drop the cni configuration now the couplet we'll see ok there's there's dni configuration and",
    "start": "1794639",
    "end": "1801870"
  },
  {
    "text": "the the pod network tool uses host networking so it gets started before before that configuration is dropped and",
    "start": "1801870",
    "end": "1808950"
  },
  {
    "text": "then every other pod from that point forward can start running",
    "start": "1808950",
    "end": "1813378"
  },
  {
    "text": "yeah so the comment was people are already using this this works today like this is",
    "start": "1815059",
    "end": "1822480"
  },
  {
    "text": "like a super easy way of deploying pod networking",
    "start": "1822480",
    "end": "1827509"
  },
  {
    "text": "they're just yeah networking can there's always oh it's gonna be some caveats there but for the most part this",
    "start": "1831100",
    "end": "1838220"
  },
  {
    "text": "is a pretty stable really like simple way of actually deploying the networking for your cluster",
    "start": "1838220",
    "end": "1845889"
  },
  {
    "start": "1847000",
    "end": "1891000"
  },
  {
    "text": "so Sophos Ledet CD so if you haven't seen it yet take a",
    "start": "1847120",
    "end": "1853160"
  },
  {
    "text": "look at the blog post about operators and this is a piece of software that is meant to manage",
    "start": "1853160",
    "end": "1860260"
  },
  {
    "text": "another application inside of kubernetes when you kind of need some more",
    "start": "1860260",
    "end": "1865880"
  },
  {
    "text": "intelligence from I mean like kind of managing its lifecycle so in terms of sed this operator can be responsible for",
    "start": "1865880",
    "end": "1872480"
  },
  {
    "text": "expanding and contracting your cluster for backing it up these kinds of things and also upgrading at CD itself so we're",
    "start": "1872480",
    "end": "1881420"
  },
  {
    "text": "actually integrating into boot Combe we're in the process of that so that we're able to self host at CD as well",
    "start": "1881420",
    "end": "1887020"
  },
  {
    "text": "and so I want to talk a little bit about how that process would work so similar picture to what we had before",
    "start": "1887020",
    "end": "1894020"
  },
  {
    "text": "is that CD now we're not considering as this external component that we're managing and it's just network",
    "start": "1894020",
    "end": "1899330"
  },
  {
    "text": "addressable we are adding as a piece of boot cube as well so we now have a full",
    "start": "1899330",
    "end": "1904700"
  },
  {
    "text": "control plane inside of boot coop where we have the main communities components and then we have our data store as well",
    "start": "1904700",
    "end": "1910250"
  },
  {
    "text": "and so the couplet is contacting that and when boot cube chests are launched the replacement",
    "start": "1910250",
    "end": "1916900"
  },
  {
    "text": "self-hosted control plane we have an extra piece here that we've deployed the SED operator so what this we're going to",
    "start": "1916900",
    "end": "1923990"
  },
  {
    "text": "do with is we're going to say okay let's see the operator we want to start a self-hosted at CD cluster but we wanted",
    "start": "1923990",
    "end": "1929840"
  },
  {
    "text": "to seed with this previous data store we want it to actually migrate that data into this new cluster and so when we",
    "start": "1929840",
    "end": "1937400"
  },
  {
    "text": "told it to seed it what it's going to do is it's gonna say okay well let's expand this and so we need to do an ad member",
    "start": "1937400",
    "end": "1942560"
  },
  {
    "text": "operation we're going to start a pod and we're going to add a number so now these are replicating data between them and",
    "start": "1942560",
    "end": "1947630"
  },
  {
    "text": "what boot coop is doing is once the data is replicated it knows it",
    "start": "1947630",
    "end": "1953420"
  },
  {
    "text": "can die away and the NCD operator needs to just say remove members so we've gone from a single Etsy denote our seed node",
    "start": "1953420",
    "end": "1960080"
  },
  {
    "text": "and then we've expanded it to a two node cluster and then we've dropped it back down to a one node cluster and at this",
    "start": "1960080",
    "end": "1965779"
  },
  {
    "text": "point buku can die and then now we have a fully self hosted control plane including at CD and the same way that",
    "start": "1965779",
    "end": "1971720"
  },
  {
    "text": "you saw before where I said you know coops ETL scale and I wanted to actually expand this into multiple nodes we can",
    "start": "1971720",
    "end": "1977539"
  },
  {
    "text": "do the same thing by the EDD operator so now that we've migrated into a fully self hosted control plane we tell the",
    "start": "1977539",
    "end": "1984049"
  },
  {
    "text": "NCD operator well let's make this a three node cluster now and it'll actually schedule new pods and then add",
    "start": "1984049",
    "end": "1989149"
  },
  {
    "text": "those as members so this is being integrated now it's not quite there yet but this is our life structural plan at",
    "start": "1989149",
    "end": "1995450"
  },
  {
    "text": "a high level okay so this is all awesome stuff and it's amazing but we should talk a little",
    "start": "1995450",
    "end": "2002590"
  },
  {
    "start": "1996000",
    "end": "2039000"
  },
  {
    "text": "bit about disaster recovery in in some failure situations so",
    "start": "2002590",
    "end": "2008760"
  },
  {
    "text": "situations I want to go over node failure in h8 appointments partial loss",
    "start": "2008760",
    "end": "2014139"
  },
  {
    "text": "of control plane components power cycling the entire control plane or just permanent loss of the control plane",
    "start": "2014139",
    "end": "2020639"
  },
  {
    "text": "so in the case of node failure and h8 appointments we get this for free kubernetes will deal with this for us",
    "start": "2020639",
    "end": "2027490"
  },
  {
    "text": "you know and no dies it had control thing components on it they will be scheduled elsewhere you know we've we've",
    "start": "2027490",
    "end": "2033880"
  },
  {
    "text": "deployed multiple copies of them so if one or a couple of copies dies we should still be fine",
    "start": "2033880",
    "end": "2040740"
  },
  {
    "start": "2039000",
    "end": "2073000"
  },
  {
    "text": "partial loss of the control plan so let's say we lose all schedulers or all controller managers or both",
    "start": "2040769",
    "end": "2047609"
  },
  {
    "text": "we're essentially in a situation where we need a scheduler to schedule the scheduler their schedulers that makes",
    "start": "2047609",
    "end": "2054490"
  },
  {
    "text": "any sense but this actually can be recovered still just using cubes ETL commands and the recovery method is",
    "start": "2054490",
    "end": "2061148"
  },
  {
    "text": "scheduling a single copy pod onto a particular node and saying on this node you should run a scheduler temporarily",
    "start": "2061149",
    "end": "2067419"
  },
  {
    "text": "so that we can schedule the schedulers and I don't feel like I can say that with a straight face without showing it",
    "start": "2067419",
    "end": "2074460"
  },
  {
    "start": "2073000",
    "end": "2357000"
  },
  {
    "text": "so let's do this so one it makes it a little bit difficult like I actually have to try to to kill this",
    "start": "2074460",
    "end": "2081070"
  },
  {
    "text": "[Music]",
    "start": "2081070",
    "end": "2084209"
  },
  {
    "text": "so it's scale this down so we don't have",
    "start": "2091000",
    "end": "2096620"
  },
  {
    "text": "multiple and then let's try to lead it",
    "start": "2096620",
    "end": "2102610"
  },
  {
    "text": "now see when I say I actually have to try is that sometimes it's faster and it will just recover from it immediately",
    "start": "2114580",
    "end": "2120640"
  },
  {
    "text": "let's try and delete it again okay so",
    "start": "2120640",
    "end": "2127080"
  },
  {
    "text": "I mean this is okay so this is this is enough of a demo like if we have more",
    "start": "2127080",
    "end": "2132970"
  },
  {
    "text": "time I would delete the controller manager too but what we see here is that we did we did the scheduler but the",
    "start": "2132970",
    "end": "2138700"
  },
  {
    "text": "controller manager is actually what takes a deployment and then creates new pods from it so the the controller",
    "start": "2138700",
    "end": "2144070"
  },
  {
    "text": "mannequin for a deployment ways so the controller manager is saying well I should be running one copy of this and",
    "start": "2144070",
    "end": "2150550"
  },
  {
    "text": "we're not anymore because we just deleted it so I'm gonna create a new pod but it's an unscheduled pod and that's",
    "start": "2150550",
    "end": "2155920"
  },
  {
    "text": "why we see the coop scheduler is pending so the controller manager has created that pod entry it just has nowhere to",
    "start": "2155920",
    "end": "2162130"
  },
  {
    "text": "run because there's no scheduler to schedule scheduler so the way that we recover this is that",
    "start": "2162130",
    "end": "2169240"
  },
  {
    "text": "we want to just extract the pod spec out of that deployment and then just tell it",
    "start": "2169240",
    "end": "2174970"
  },
  {
    "text": "go run on a node and then it will start scheduling everything and it'll it'll be happy again but let's do",
    "start": "2174970",
    "end": "2181650"
  },
  {
    "text": "let's run some other pods just to make this even more apparent",
    "start": "2181650",
    "end": "2187260"
  },
  {
    "text": "so let's say let's run a deployment of Engine X so what we should see is the same thing we get pending controller",
    "start": "2191860",
    "end": "2199190"
  },
  {
    "text": "manager is broken down the deployment into pods and they're not they don't have anywhere to go",
    "start": "2199190",
    "end": "2205720"
  },
  {
    "text": "so I'm gonna get the deployment for this scheduler I'm gonna output it to ya mall",
    "start": "2209650",
    "end": "2214840"
  },
  {
    "text": "if I can spill",
    "start": "2214840",
    "end": "2218470"
  },
  {
    "text": "let's see okay so this is the piece that I care about",
    "start": "2226690",
    "end": "2232750"
  },
  {
    "text": "this is just the pod spec portion of the deployment so we're going to and because",
    "start": "2232750",
    "end": "2240380"
  },
  {
    "text": "I'm lazy I have the pod header here",
    "start": "2240380",
    "end": "2244390"
  },
  {
    "text": "so this is just the main top-level object for a pod and then",
    "start": "2249790",
    "end": "2257960"
  },
  {
    "text": "I'm gonna paste in what I just copied let's call it cover scheduler and what",
    "start": "2257960",
    "end": "2266330"
  },
  {
    "text": "we need to do is we need to give it a particular node to go on so we want to bypass the scheduler because we don't",
    "start": "2266330",
    "end": "2272780"
  },
  {
    "text": "have one and this is going to to be our recovery um so",
    "start": "2272780",
    "end": "2277930"
  },
  {
    "text": "let's get a node let's put it on this one",
    "start": "2277930",
    "end": "2282790"
  },
  {
    "text": "okay so this is now a scheduled pod that we can just force into the cluster",
    "start": "2283390",
    "end": "2289420"
  },
  {
    "text": "and okay so what we should see when I run",
    "start": "2290230",
    "end": "2297140"
  },
  {
    "text": "this is it's going to schedule everything again including these including the the scheduler itself and",
    "start": "2297140",
    "end": "2303460"
  },
  {
    "text": "our engine X pods",
    "start": "2303460",
    "end": "2307119"
  },
  {
    "text": "they reel so start a scheduler and it's like okay these things need to be running",
    "start": "2323579",
    "end": "2329369"
  },
  {
    "text": "somewhere let's find somewhere for them to run and one of those things is the actual real scheduler that is managed by",
    "start": "2329369",
    "end": "2335999"
  },
  {
    "text": "a higher-level object so we no longer need this pod anymore so let's just delete it and so we've recovered from",
    "start": "2335999",
    "end": "2341880"
  },
  {
    "text": "like a complete you know partial control plane failure and the same exact thing works for controller managers given an",
    "start": "2341880",
    "end": "2348809"
  },
  {
    "text": "API server and Etsy D to interact with you can recover these states using coops ETL",
    "start": "2348809",
    "end": "2355519"
  },
  {
    "start": "2357000",
    "end": "2532000"
  },
  {
    "text": "okay so the next thing that I wanted to go over is power cycling the entire control",
    "start": "2357739",
    "end": "2364049"
  },
  {
    "text": "plane so this doesn't mean like node failure or anything this just means like power",
    "start": "2364049",
    "end": "2369450"
  },
  {
    "text": "off power back up how do you actually recover from that state and so this is back to that chicken and egg issue where",
    "start": "2369450",
    "end": "2375719"
  },
  {
    "text": "in some cases you know if you have a control plane to talk to even if you like power cycled most of your nodes if",
    "start": "2375719",
    "end": "2382410"
  },
  {
    "text": "you still had an API server to talk to that's enough for it to tell you hey go run these other components but if we",
    "start": "2382410",
    "end": "2388140"
  },
  {
    "text": "have no API server to talk to anymore we're innocent we're back in that chicken in an egg state so for the case",
    "start": "2388140",
    "end": "2394170"
  },
  {
    "text": "where we're not actually losing any data this is just a power cycle this is solved by something called checkpointing",
    "start": "2394170",
    "end": "2399630"
  },
  {
    "text": "which is essentially you know we know the state that we're running locally on a node and we want to persist that state",
    "start": "2399630",
    "end": "2405479"
  },
  {
    "text": "in some way so in the absence of an API server I should continue running whatever I was running and this is true",
    "start": "2405479",
    "end": "2411420"
  },
  {
    "text": "today just normally like your coolant is running if the API server goes down for whatever reason it's going to continue",
    "start": "2411420",
    "end": "2417029"
  },
  {
    "text": "running what it was and if your pods have a restart policy of always and those pods die it's still going to",
    "start": "2417029",
    "end": "2422309"
  },
  {
    "text": "restart them even in the absence of an API server so what we need is that same kind of recovery but in in a power cycle",
    "start": "2422309",
    "end": "2428699"
  },
  {
    "text": "sense so we've solved this initially essentially using what we're calling user space checkpointing so we run an",
    "start": "2428699",
    "end": "2435059"
  },
  {
    "text": "actual pod that is responsible for saying in this case the API server you know",
    "start": "2435059",
    "end": "2441989"
  },
  {
    "text": "critical component it's saying I was running the API server and if I'm no longer running the API server anymore",
    "start": "2441989",
    "end": "2447630"
  },
  {
    "text": "then I need to make sure that it starts back up and it does this in a very similar way to if any of you remember",
    "start": "2447630",
    "end": "2453449"
  },
  {
    "text": "the old pod master before we had master elected and for the controller manager in the scheduler it essentially just",
    "start": "2453449",
    "end": "2459180"
  },
  {
    "text": "shifts around files on disk so it will store the checkpoint in a cold location and then if it sees that it's not",
    "start": "2459180",
    "end": "2465270"
  },
  {
    "text": "running anymore it will move back into the static pod location so then it will be started up by the couplet so I have",
    "start": "2465270",
    "end": "2470700"
  },
  {
    "text": "to reboot what it would do is it would say well I was running a API server and I have this cold copy of it so I should",
    "start": "2470700",
    "end": "2476670"
  },
  {
    "text": "move that over into the static pod location kuba starts it up eventually it",
    "start": "2476670",
    "end": "2481980"
  },
  {
    "text": "sees well the actual API server has come down just like we saw with that recovery pond we can now move that back out into",
    "start": "2481980",
    "end": "2488220"
  },
  {
    "text": "cold storage ideally this is something that we can eventually get into the coolant itself",
    "start": "2488220",
    "end": "2493940"
  },
  {
    "text": "the actual issue is that jumped over a little bit but for 89 so we're in the 30",
    "start": "2493940",
    "end": "2499440"
  },
  {
    "text": "thousands of issues now this is something that kind of known about and wanted for a while and the actual title",
    "start": "2499440",
    "end": "2504870"
  },
  {
    "text": "of the issue is kuba pod checkpointing or something so it is a fair amount of work and it it",
    "start": "2504870",
    "end": "2512160"
  },
  {
    "text": "can be complex so it's just kind of figuring out like how do we start to build this kind of functionality and it's starting that discussion because",
    "start": "2512160",
    "end": "2517770"
  },
  {
    "text": "it's really useful we can get around it with the user space stuff but it's a little bit more fragile than if the kulit already knows its state and it's",
    "start": "2517770",
    "end": "2524430"
  },
  {
    "text": "able to checkpoint that speed so this is the case of just power cycling and",
    "start": "2524430",
    "end": "2529460"
  },
  {
    "text": "not actually losing any data so we figured to go over a permanent",
    "start": "2529460",
    "end": "2534570"
  },
  {
    "start": "2532000",
    "end": "2590000"
  },
  {
    "text": "loss of the control plane so this is a similar situation to the initial node",
    "start": "2534570",
    "end": "2540300"
  },
  {
    "text": "bootstrap we're back in a place where you know we're in that chicken and egg scenario it's slightly different in that",
    "start": "2540300",
    "end": "2545970"
  },
  {
    "text": "we either have existing MCD state or we have an MCD backup in this case ideally you are backing up your data store and",
    "start": "2545970",
    "end": "2551640"
  },
  {
    "text": "CD is not magic it's pretty close to it but you should still be doing backups so again what we essentially need to do at",
    "start": "2551640",
    "end": "2558210"
  },
  {
    "text": "this stage is we need to start a temporary replacement API server we just need to get enough so that we can",
    "start": "2558210",
    "end": "2563490"
  },
  {
    "text": "contact the API server and this could be a binary static pods we could create a new tool that does simple recovery we",
    "start": "2563490",
    "end": "2569910"
  },
  {
    "text": "could make boot Kubb operate in slightly different form where it just starts an API server",
    "start": "2569910",
    "end": "2575000"
  },
  {
    "text": "but once we have at CDE and an API then as you've seen we can recover the rest",
    "start": "2575000",
    "end": "2580800"
  },
  {
    "text": "of the state just using coop CTL so there's a lot of options here and even in the case of like everything goes",
    "start": "2580800",
    "end": "2586020"
  },
  {
    "text": "wrong that stand up process is in isn't actually that crazy so I just want to quickly touch on some",
    "start": "2586020",
    "end": "2593360"
  },
  {
    "start": "2590000",
    "end": "2660000"
  },
  {
    "text": "of the things that you know there's still work to be done here and some of the high-level things are things like",
    "start": "2593360",
    "end": "2599180"
  },
  {
    "text": "the daemon set rolling update and there's an open pull request about adding this functionality pod checkpointing which I just talked about",
    "start": "2599180",
    "end": "2605240"
  },
  {
    "text": "or something so posting at CD this is a work in progress right now and then I think that",
    "start": "2605240",
    "end": "2610430"
  },
  {
    "text": "you know as I showed I I can recover these pods using cube CTL and copying",
    "start": "2610430",
    "end": "2615590"
  },
  {
    "text": "out the pod spec but that's a fragile process to explain to someone I go get the deployment and take this one section",
    "start": "2615590",
    "end": "2621590"
  },
  {
    "text": "of the spec and paste it into a pod so we could build simple tooling that just does that for us you know we could just",
    "start": "2621590",
    "end": "2627140"
  },
  {
    "text": "have a command that says recover deployment target node equals in it asks for that deployment it extracts the the",
    "start": "2627140",
    "end": "2633680"
  },
  {
    "text": "pod spec and it just assigns it to a node we can have you know simple you know complete failure case tooling where",
    "start": "2633680",
    "end": "2640460"
  },
  {
    "text": "it knows how to take a net CD backup and then start a temporary API server and then stand up the rest of your control",
    "start": "2640460",
    "end": "2646370"
  },
  {
    "text": "plane after that but these are the kinds of things we start to need to build through so that's all I have for you",
    "start": "2646370",
    "end": "2652370"
  },
  {
    "text": "guys thank you very much [Applause]",
    "start": "2652370",
    "end": "2662439"
  }
]