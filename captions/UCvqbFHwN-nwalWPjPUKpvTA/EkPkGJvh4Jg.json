[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "good morning No welcome to the second talk within our Prometheus Trek it's a",
    "start": "179",
    "end": "7620"
  },
  {
    "text": "deep dive some of you attend at the intro welcome to those you'll learn something new as well thank you",
    "start": "7620",
    "end": "15379"
  },
  {
    "text": "so I'm Richard I did a lot of things I'm currently building a data center I'm",
    "start": "15379",
    "end": "21000"
  },
  {
    "text": "mrs. team member founder of open metrics and this is Frederick hey so yeah I I",
    "start": "21000",
    "end": "26730"
  },
  {
    "text": "work at Red Hat and I basically work on everything Prometheus and kubernetes and",
    "start": "26730",
    "end": "33030"
  },
  {
    "text": "everything that connects the two projects basically so yeah let's get started",
    "start": "33030",
    "end": "39000"
  },
  {
    "start": "38000",
    "end": "70000"
  },
  {
    "text": "show of hands like how many people are have heard of Prometheus hopefully",
    "start": "39000",
    "end": "44160"
  },
  {
    "text": "that's everyone in this room okay who is considering using Prometheus okay that's",
    "start": "44160",
    "end": "51570"
  },
  {
    "text": "less will convince you sorry okay well",
    "start": "51570",
    "end": "57719"
  },
  {
    "text": "that's even better okay maybe who is not quite using it in production but like trying it out okay",
    "start": "57719",
    "end": "64559"
  },
  {
    "text": "that's good and who's using it in production that's a lot nice okay so",
    "start": "64559",
    "end": "72060"
  },
  {
    "start": "70000",
    "end": "173000"
  },
  {
    "text": "yeah Prometheus is a monitoring system that was inspired by Google Sparkman it",
    "start": "72060",
    "end": "77580"
  },
  {
    "text": "was founded by a couple of ex Googlers who like knew how things worked at",
    "start": "77580",
    "end": "82770"
  },
  {
    "text": "Google and they're like well it wouldn't it be nice if we had something like this open source and it really came out of a need",
    "start": "82770",
    "end": "90170"
  },
  {
    "text": "by themselves they were working at SoundCloud at the time so yeah and it's",
    "start": "90170",
    "end": "96229"
  },
  {
    "text": "Prometheus is a time-series database what that or there are other monitoring",
    "start": "96229",
    "end": "102689"
  },
  {
    "text": "systems out there there are not necessarily time series data bases but Prometheus is and you can see that by",
    "start": "102689",
    "end": "110670"
  },
  {
    "text": "the super super simple data model which is just every time stamp is a UN 64 and",
    "start": "110670",
    "end": "116520"
  },
  {
    "text": "every every corresponding value is just a float 64 and that's quite literally",
    "start": "116520",
    "end": "122640"
  },
  {
    "text": "what we write to disk yeah and the way that all of this works is that",
    "start": "122640",
    "end": "128899"
  },
  {
    "text": "Prometheus target just has an HTTP server that X closes metrics on some path and then",
    "start": "128899",
    "end": "136380"
  },
  {
    "text": "Prometheus comes around and collects all of these metrics so Prometheus is a pull",
    "start": "136380",
    "end": "141720"
  },
  {
    "text": "based system but what's really important to understand with Prometheus is we're really just about metrics",
    "start": "141720",
    "end": "148710"
  },
  {
    "text": "it's not about logging it's not about tracing and that's deliberate we don't",
    "start": "148710",
    "end": "155040"
  },
  {
    "text": "what we acknowledge that those topics are also important but it's not what",
    "start": "155040",
    "end": "160890"
  },
  {
    "text": "we're fixing or not what we're trying to solve and Prometheus itself just has a",
    "start": "160890",
    "end": "166110"
  },
  {
    "text": "query browser basically where you can do exploratory viewing querying and if",
    "start": "166110",
    "end": "171240"
  },
  {
    "text": "you'd want to do dashboarding you do that with graph Anna and so I guess a lot of people here are already using",
    "start": "171240",
    "end": "177600"
  },
  {
    "start": "173000",
    "end": "199000"
  },
  {
    "text": "Prometheus so I don't really need to sell you on it it's great at scaled super well it has a great query language",
    "start": "177600",
    "end": "184500"
  },
  {
    "text": "to explore all of this data and I think what most people are appreciate most",
    "start": "184500",
    "end": "191160"
  },
  {
    "text": "about prometheus is that it's so simple to operate and it's just reliable and",
    "start": "191160",
    "end": "197459"
  },
  {
    "text": "super super efficient so yeah some flashy words it's super nicely",
    "start": "197459",
    "end": "207209"
  },
  {
    "start": "199000",
    "end": "249000"
  },
  {
    "text": "integrated with all this cloud technology so with kubernetes for example we have a kubernetes service",
    "start": "207209",
    "end": "212640"
  },
  {
    "text": "discovery so when pods come and disappear Prometheus keeps up with that",
    "start": "212640",
    "end": "218520"
  },
  {
    "text": "and dynamically reconfigures itself yeah but in in itself Prometheus is a very",
    "start": "218520",
    "end": "227310"
  },
  {
    "text": "monolithic application which means everything is like it's all one binary",
    "start": "227310",
    "end": "232380"
  },
  {
    "text": "and it does all of what I've just said within that one binary which kind of",
    "start": "232380",
    "end": "239760"
  },
  {
    "text": "sounds weird but it's actually what makes it so reliable and so simple to run because it's just this one statically linked binary that you put on",
    "start": "239760",
    "end": "246540"
  },
  {
    "text": "your server and it just works yeah and the reason why we're doing this is just",
    "start": "246540",
    "end": "253170"
  },
  {
    "text": "we want to be we want to build reliable and resilient systems yeah so that was",
    "start": "253170",
    "end": "263070"
  },
  {
    "start": "259000",
    "end": "329000"
  },
  {
    "text": "kind of the intro and now we want to deep dive a bit into what has happened lately in prometheus",
    "start": "263070",
    "end": "269520"
  },
  {
    "text": "world so in Prometheus 2.0 which was pretty much exactly released a year ago",
    "start": "269520",
    "end": "276510"
  },
  {
    "text": "like a year and a month ago and for that release the Prometheus team rewrote the",
    "start": "276510",
    "end": "282810"
  },
  {
    "text": "entire storage back-end so previously Prometheus was based on level DB and basically every single time series rose",
    "start": "282810",
    "end": "290520"
  },
  {
    "text": "a single file on disk and samples would",
    "start": "290520",
    "end": "295770"
  },
  {
    "text": "just be appended onto those piles which is great but it turns out when you have",
    "start": "295770",
    "end": "301289"
  },
  {
    "text": "a lot of time time series okay yeah",
    "start": "301289",
    "end": "309600"
  },
  {
    "text": "so we rewrote the backend and we introduced better new staleness handling",
    "start": "309600",
    "end": "315410"
  },
  {
    "text": "and the remote read and write API is got",
    "start": "315410",
    "end": "320729"
  },
  {
    "text": "a little bit more stable but there still being we reworked a bit to be actually",
    "start": "320729",
    "end": "325800"
  },
  {
    "text": "stable yeah so in Prometheus one point X",
    "start": "325800",
    "end": "331530"
  },
  {
    "start": "329000",
    "end": "388000"
  },
  {
    "text": "as I was saying we had one file per time series and that's nice and easy to",
    "start": "331530",
    "end": "337320"
  },
  {
    "text": "implement and it works fairly well but when we are in super dynamic environments like with kubernetes that",
    "start": "337320",
    "end": "343740"
  },
  {
    "text": "becomes a problem because if we have hundreds and thousands of targets that each exposed hundreds of thousands of",
    "start": "343740",
    "end": "350970"
  },
  {
    "text": "metrics that easily becomes a couple of million files on disk which is not",
    "start": "350970",
    "end": "356910"
  },
  {
    "text": "particularly well handled by a lot of file systems and it's not efficient so",
    "start": "356910",
    "end": "362449"
  },
  {
    "text": "because of all that we yeah so what I",
    "start": "362449",
    "end": "372960"
  },
  {
    "text": "just described is turn when x used new",
    "start": "372960",
    "end": "378479"
  },
  {
    "text": "time series come all the time and get stopped so we have lots of them that are",
    "start": "378479",
    "end": "383880"
  },
  {
    "text": "short-lived yeah so if we look at that",
    "start": "383880",
    "end": "390120"
  },
  {
    "start": "388000",
    "end": "416000"
  },
  {
    "text": "on like a visualization we we can see",
    "start": "390120",
    "end": "395490"
  },
  {
    "text": "that this is not very well like distributed anymore right with in a very static environment we would",
    "start": "395490",
    "end": "401720"
  },
  {
    "text": "just have continuous lines of dots where we can pretty where we don't have to",
    "start": "401720",
    "end": "409340"
  },
  {
    "text": "create new files all the time and we wouldn't be creating millions all the",
    "start": "409340",
    "end": "414830"
  },
  {
    "text": "time and but what's really what really makes the value of Prometheus is",
    "start": "414830",
    "end": "420889"
  },
  {
    "start": "416000",
    "end": "435000"
  },
  {
    "text": "querying it right so not only do we need to open all of these files we also not",
    "start": "420889",
    "end": "428780"
  },
  {
    "text": "only do we write to them we ought to need also need to read them to be able to do all this great querying that",
    "start": "428780",
    "end": "434060"
  },
  {
    "text": "Prometheus allows us to do and how we change this in Prometheus 2.0 is that",
    "start": "434060",
    "end": "439699"
  },
  {
    "start": "435000",
    "end": "464000"
  },
  {
    "text": "instead of having one file per time series we actually have time sliced",
    "start": "439699",
    "end": "445960"
  },
  {
    "text": "databases that are individually fully",
    "start": "445960",
    "end": "451610"
  },
  {
    "text": "functional databases and they're just slices of two hours of data and there",
    "start": "451610",
    "end": "456680"
  },
  {
    "text": "are just a handful of files per database basically and yeah as I said this was",
    "start": "456680",
    "end": "462199"
  },
  {
    "text": "completely rewritten from scratch and in",
    "start": "462199",
    "end": "467449"
  },
  {
    "start": "464000",
    "end": "515000"
  },
  {
    "text": "order to ensure that all the things that we were intending to solve are going to",
    "start": "467449",
    "end": "474349"
  },
  {
    "text": "be solved we had relatively complex test setup where we were ingesting a lot of",
    "start": "474349",
    "end": "480110"
  },
  {
    "text": "metrics per second with a lot of churn so we had a G we have a GAE cluster",
    "start": "480110",
    "end": "486800"
  },
  {
    "text": "we're just every 5 minutes we scale deployment up to like 500 targets and",
    "start": "486800",
    "end": "494330"
  },
  {
    "text": "then again down to 300 targets up to 500 targets and so on and what that does is",
    "start": "494330",
    "end": "500539"
  },
  {
    "text": "it just creates lots of targets which means lots of churn and super short lift",
    "start": "500539",
    "end": "505880"
  },
  {
    "text": "time series so that that puts a lot of stress on Prometheus and with that kind of setup we were able to ensure that we",
    "start": "505880",
    "end": "514099"
  },
  {
    "text": "actually are meeting our goals and and we did we have super significant",
    "start": "514099",
    "end": "520990"
  },
  {
    "start": "515000",
    "end": "573000"
  },
  {
    "text": "resource consumption improvement I mean these these numbers kind of vary right",
    "start": "520990",
    "end": "529279"
  },
  {
    "text": "these were some of the best results that we've seen we also have some we've also seen some setups where",
    "start": "529279",
    "end": "536329"
  },
  {
    "text": "resource consumption is actually the same as in one point X but generally speaking resource consumption is way way",
    "start": "536329",
    "end": "543589"
  },
  {
    "text": "way way better and I think the reduction of undersize is pretty general because",
    "start": "543589",
    "end": "550550"
  },
  {
    "text": "the all the metadata of millions of",
    "start": "550550",
    "end": "555589"
  },
  {
    "text": "files is actually pretty expensive on disk when it becomes millions of files",
    "start": "555589",
    "end": "562069"
  },
  {
    "text": "but now we just have these handful of files per to our slot and that means we",
    "start": "562069",
    "end": "569589"
  },
  {
    "text": "never get into that kind of scale so we",
    "start": "569589",
    "end": "579759"
  },
  {
    "start": "573000",
    "end": "686000"
  },
  {
    "text": "in Prometheus 2.0 we released all of this and it we had really great reception of this so what's next so now",
    "start": "579759",
    "end": "588259"
  },
  {
    "text": "we're we're working on making the remote read and write API is much better",
    "start": "588259",
    "end": "593779"
  },
  {
    "text": "we've already improved them a lot but there's also some more ongoing work that",
    "start": "593779",
    "end": "598850"
  },
  {
    "text": "Ritchie is going to elaborate on and lots of people actually started",
    "start": "598850",
    "end": "605259"
  },
  {
    "text": "implementing these so if you have an existing time series database that you",
    "start": "605259",
    "end": "610399"
  },
  {
    "text": "are comfortably scaling you can totally use Prometheus as like an ingesting",
    "start": "610399",
    "end": "616339"
  },
  {
    "text": "front-end and just send off these samples to your existing time series database and to complete all of this",
    "start": "616339",
    "end": "625149"
  },
  {
    "text": "work remote write currently is actually it was basically when we initially",
    "start": "625149",
    "end": "632059"
  },
  {
    "text": "implemented it we just did the simplest solution to the problem which is",
    "start": "632059",
    "end": "637839"
  },
  {
    "text": "whenever Prometheus scrapes data from a target it was just right of that those",
    "start": "637839",
    "end": "644420"
  },
  {
    "text": "samples to disk and immediately send it off to the remote storage which is great",
    "start": "644420",
    "end": "649550"
  },
  {
    "text": "but it turns out that we thought that we had to buffer these things in memory and",
    "start": "649550",
    "end": "654980"
  },
  {
    "text": "if you have a large buffer then this kind of thing can explode and if your remote read back-end goes down and you",
    "start": "654980",
    "end": "661069"
  },
  {
    "text": "just always have ever-growing memory usage and but it turns out we actually",
    "start": "661069",
    "end": "666679"
  },
  {
    "text": "already have an on disk before to use which is the right a headlock so that's what we're in",
    "start": "666679",
    "end": "673880"
  },
  {
    "text": "commenting right now and it's probably going to be released in Prometheus 2.7",
    "start": "673880",
    "end": "679370"
  },
  {
    "text": "which should be released the end at the end of January beginning of February but",
    "start": "679370",
    "end": "688790"
  },
  {
    "start": "686000",
    "end": "764000"
  },
  {
    "text": "within the like 2.0 2 to 2 timeframe we",
    "start": "688790",
    "end": "694250"
  },
  {
    "text": "also had a security audit sponsored by the CN CF done by the Cure 53 people and",
    "start": "694250",
    "end": "702519"
  },
  {
    "text": "basically we knew or we kind of",
    "start": "702519",
    "end": "708010"
  },
  {
    "text": "anticipated the result which is we don't didn't ever care about security features",
    "start": "708010",
    "end": "715130"
  },
  {
    "text": "in Prometheus so there's no not really any security flaws but this is also like",
    "start": "715130",
    "end": "724610"
  },
  {
    "text": "at the same time we were also thinking of introducing security features so at",
    "start": "724610",
    "end": "731300"
  },
  {
    "text": "the actually decided - yes sorry this is what it was was what I was getting - so",
    "start": "731300",
    "end": "737240"
  },
  {
    "text": "at the dev summit in Munich in August we actually decided that Prometheus will",
    "start": "737240",
    "end": "742699"
  },
  {
    "text": "have native support for TLS going forward but there is just a couple of",
    "start": "742699",
    "end": "747769"
  },
  {
    "text": "things that we need to figure out like escalation processes and stuff like that which we because we didn't have security",
    "start": "747769",
    "end": "753860"
  },
  {
    "text": "features we didn't have this set up and we're going to work with the Cure 53 folks to make sure that all of this",
    "start": "753860",
    "end": "760970"
  },
  {
    "text": "process is also well thought out so we",
    "start": "760970",
    "end": "768589"
  },
  {
    "start": "764000",
    "end": "856000"
  },
  {
    "text": "did the Prometheus 2.0 release and that release in itself was super stable because we trialed it for months and we",
    "start": "768589",
    "end": "776269"
  },
  {
    "text": "had so many pre releases but then every single release after that until like",
    "start": "776269",
    "end": "783110"
  },
  {
    "text": "Prometheus two point three point two was like really unstable and we realized",
    "start": "783110",
    "end": "789860"
  },
  {
    "text": "this and we took that as feedback and decided and also in the dev summit that",
    "start": "789860",
    "end": "795920"
  },
  {
    "text": "instead of just doing a release whenever we are going to do a release we are now",
    "start": "795920",
    "end": "800980"
  },
  {
    "text": "going to have set cadence of releases which means we have we have a defined",
    "start": "800980",
    "end": "810870"
  },
  {
    "text": "period where there's going to be implementations implementation work",
    "start": "810870",
    "end": "816040"
  },
  {
    "text": "going on and so that reduces the surface of potential errors but yeah Ritchie is",
    "start": "816040",
    "end": "824350"
  },
  {
    "text": "going to talk a bit more about that but we did apply that and then two point",
    "start": "824350",
    "end": "831010"
  },
  {
    "text": "three point two was actually the true first stable release in the two point ex",
    "start": "831010",
    "end": "837089"
  },
  {
    "text": "release train because in 2.0 duper no itself was very stable but it had some",
    "start": "837089",
    "end": "842500"
  },
  {
    "text": "semantic errors so yeah if you're super concerned run to point three point two I",
    "start": "842500",
    "end": "850630"
  },
  {
    "text": "think that's what we're running in production as well yeah test okay so",
    "start": "850630",
    "end": "857260"
  },
  {
    "text": "let's try it like this yes it actually works so as you can see we put a more or less arbitrary gap between the two point",
    "start": "857260",
    "end": "865540"
  },
  {
    "text": "three and two two point four and this is precisely for that reason course for us",
    "start": "865540",
    "end": "871149"
  },
  {
    "text": "this two point reader too was more or less of a watershed moment we had a moratorium where no new features were",
    "start": "871149",
    "end": "877540"
  },
  {
    "text": "being added and people were screaming and shouting because they wanted to add features but they weren't allowed because we had a moratorium because we",
    "start": "877540",
    "end": "883899"
  },
  {
    "text": "tried it differently before and that just didn't work so to us this was",
    "start": "883899",
    "end": "889000"
  },
  {
    "text": "really like a large moment in in the history of the project course we really",
    "start": "889000",
    "end": "895269"
  },
  {
    "text": "changed how we worked for for quite some time and after that we actually adapted",
    "start": "895269",
    "end": "900699"
  },
  {
    "start": "900000",
    "end": "964000"
  },
  {
    "text": "changes during the dev summit where we committed to doing it the same way so if",
    "start": "900699",
    "end": "905800"
  },
  {
    "text": "we ever enter this phase of instability once more we will have a moratorium no",
    "start": "905800",
    "end": "910899"
  },
  {
    "text": "one wants to have moratorium so hopefully people will avoid it so what",
    "start": "910899",
    "end": "916180"
  },
  {
    "text": "did we say did we desire to do pretty simple every six week we mark a new release candidate and we just work on",
    "start": "916180",
    "end": "923620"
  },
  {
    "text": "this release candidate it's being improved improved improved when we don't have any any error reports anymore on",
    "start": "923620",
    "end": "928980"
  },
  {
    "text": "form the people running canneries and such then we basically release totally this thing",
    "start": "928980",
    "end": "934660"
  },
  {
    "text": "from this release process after six weeks from the last RC we cut the next RC so all those considerations about hey",
    "start": "934660",
    "end": "941680"
  },
  {
    "text": "we if we push and shove in this a little bit then the release after next we'll have them a little bit earlier go away",
    "start": "941680",
    "end": "947769"
  },
  {
    "text": "because you have this fixed our C cycle anyway and you can even plan for okay I want to have my holidays during that",
    "start": "947769",
    "end": "954069"
  },
  {
    "text": "time so I'm not going to be the release maintainer for this one so this makes it a lot simpler on a lot of levels that we",
    "start": "954069",
    "end": "960430"
  },
  {
    "text": "that we have done this yeah so then we",
    "start": "960430",
    "end": "966730"
  },
  {
    "start": "964000",
    "end": "992000"
  },
  {
    "text": "had quite some optimal optimizations in between 2.4 and 2.5 I think",
    "start": "966730",
    "end": "971829"
  },
  {
    "text": "very Brian did tons of work of prompt QL and for some functions he actually had a",
    "start": "971829",
    "end": "976990"
  },
  {
    "text": "5 times reduction in in in time and a ton of reductions in garbage collection",
    "start": "976990",
    "end": "982029"
  },
  {
    "text": "which you don't always notice it but when you run into into those cases you",
    "start": "982029",
    "end": "987310"
  },
  {
    "text": "really notice is just a lot quicker than you used to be in in those cases then we",
    "start": "987310",
    "end": "993610"
  },
  {
    "start": "992000",
    "end": "1038000"
  },
  {
    "text": "have long term storage so it's it's one of the last things within premises",
    "start": "993610",
    "end": "999130"
  },
  {
    "text": "proper which have not been really tackled which is even more of an issue",
    "start": "999130",
    "end": "1004920"
  },
  {
    "text": "because if you're running promises proper we actually suggest you just run more permissive servers you need to",
    "start": "1004920",
    "end": "1010889"
  },
  {
    "text": "scale run more you need to scream or run even more maybe toss on some Federation or such but basically run more but all",
    "start": "1010889",
    "end": "1017310"
  },
  {
    "text": "these are islands of data so if that one instance goes down that data it goes",
    "start": "1017310",
    "end": "1022709"
  },
  {
    "text": "down with it which is probably not the best thing to have also we don't currently do backfill so you can't even",
    "start": "1022709",
    "end": "1029819"
  },
  {
    "text": "copy this data over into other promises instances we are working on backfill it'll happen soon but still as of right",
    "start": "1029819",
    "end": "1036449"
  },
  {
    "text": "now you don't have backflow so how did",
    "start": "1036449",
    "end": "1041548"
  },
  {
    "text": "we tackle this within the last few release cycles first and foremost we now",
    "start": "1041549",
    "end": "1047760"
  },
  {
    "text": "support snapshotting so if your system is able to snapshot the system you can already make proper backups of your data",
    "start": "1047760",
    "end": "1054030"
  },
  {
    "text": "and this remote readwrite integration allows you to just push your data to someone else obviously you're not",
    "start": "1054030",
    "end": "1060570"
  },
  {
    "text": "actually solving the long-term problem you're just shifting it to someplace else but it's hopefully another place",
    "start": "1060570",
    "end": "1066059"
  },
  {
    "text": "where you have better better tooling for actually solving that issue there's tons and tons of projects which",
    "start": "1066059",
    "end": "1073350"
  },
  {
    "text": "which adapt to this I think we have like 15 ish or something yeah I I think it's",
    "start": "1073350",
    "end": "1079350"
  },
  {
    "text": "roughly 15 projects which which are working with our remote read and write API so you have a ton of options to push",
    "start": "1079350",
    "end": "1087809"
  },
  {
    "text": "your data to cortex which comes out of out of premises basically antennas would",
    "start": "1087809",
    "end": "1093419"
  },
  {
    "text": "be a few of them also you have influx you have tons of them of course as you",
    "start": "1093419",
    "end": "1099809"
  },
  {
    "text": "as we saw earlier that's the next thing we now have blocks we don't have this one file which goes on forever more or",
    "start": "1099809",
    "end": "1107070"
  },
  {
    "text": "maybe not we have blocks which are clearly delineated and after those two hours by default you start a new block",
    "start": "1107070",
    "end": "1114000"
  },
  {
    "text": "and this looks awfully like like something which you could put an object search which is what cyanosis we're",
    "start": "1114000",
    "end": "1120990"
  },
  {
    "text": "basically also people from premises team fork the code base and started putting",
    "start": "1120990",
    "end": "1126269"
  },
  {
    "text": "the storage into object store which is a nice way to scale cortex I should probably mention it's doing the same",
    "start": "1126269",
    "end": "1131850"
  },
  {
    "text": "thing but on the index level everybody can scale horizontally we are also",
    "start": "1131850",
    "end": "1136919"
  },
  {
    "text": "looking at merging those two back together as we have around a head lock you can also send this over the wire",
    "start": "1136919",
    "end": "1143399"
  },
  {
    "text": "along with with your reader remote read/write API so all of a sudden gaps",
    "start": "1143399",
    "end": "1148830"
  },
  {
    "text": "in network connectivity and such you can fill those at least to some extent and I",
    "start": "1148830",
    "end": "1155519"
  },
  {
    "text": "should put 15 for next version anyway we are not endorsing any version we might",
    "start": "1155519",
    "end": "1160980"
  },
  {
    "text": "have preferences amongst ourselves but those are personal preferences we deliberately decided not to mate to the",
    "start": "1160980",
    "end": "1167549"
  },
  {
    "text": "PD king maker and say okay this one thing is what you should be using this is not how we operate and not how we",
    "start": "1167549",
    "end": "1175110"
  },
  {
    "text": "want to operate we might at some point actually suggest that people start using something specific maybe for a specific",
    "start": "1175110",
    "end": "1181710"
  },
  {
    "text": "use case but at the moment we dead liberally do not make this recommendation if anyone is claiming",
    "start": "1181710",
    "end": "1187980"
  },
  {
    "text": "that their product has the stamp of approval they are probably lying to you just saying then we have lots and lots",
    "start": "1187980",
    "end": "1197010"
  },
  {
    "start": "1193000",
    "end": "1257000"
  },
  {
    "text": "of testing improvement as recently especially in the last in a 2.5 we have actual unit tests for",
    "start": "1197010",
    "end": "1205290"
  },
  {
    "text": "alerts which is great course you can test that your that your alerts actually work when you have certain types of data",
    "start": "1205290",
    "end": "1211200"
  },
  {
    "text": "or certain kinds of data in your Bay in your back-end and we want to get away",
    "start": "1211200",
    "end": "1217020"
  },
  {
    "text": "from this only us doing testing for our own stuff we recently also for example",
    "start": "1217020",
    "end": "1223530"
  },
  {
    "text": "enabled in our CI pipeline that we are doing load tests against those commits so we can see if we have any performance",
    "start": "1223530",
    "end": "1229830"
  },
  {
    "text": "regressions all these kind of things so that's all obviously also something which we are trying to do but this",
    "start": "1229830",
    "end": "1235830"
  },
  {
    "text": "doesn't go far enough this is just our stuff and we want to enable premises users to test all their stuff as well to",
    "start": "1235830",
    "end": "1244110"
  },
  {
    "text": "make their deployment testable and this is something we are we are actively working on to just improve and improve",
    "start": "1244110",
    "end": "1250230"
  },
  {
    "text": "and prove but we are also going a little bit beyond just that what what I just",
    "start": "1250230",
    "end": "1256980"
  },
  {
    "text": "talked about something which is not really firmly in the realm it might",
    "start": "1256980",
    "end": "1262980"
  },
  {
    "start": "1257000",
    "end": "1271000"
  },
  {
    "text": "happen 2.8 whatever ASA databases is probably a concept I don't really have",
    "start": "1262980",
    "end": "1268830"
  },
  {
    "text": "to talk about we are missing one of them its isolation the thing how we want to",
    "start": "1268830",
    "end": "1274500"
  },
  {
    "start": "1271000",
    "end": "1339000"
  },
  {
    "text": "solve isolation or how we the isolation is relatively simple and highly complex at the same time we have a ma non",
    "start": "1274500",
    "end": "1281040"
  },
  {
    "text": "monotonic counter it's just up up up up up every white gets one of those IDs and",
    "start": "1281040",
    "end": "1289350"
  },
  {
    "text": "basically as long as long as this ID has not yet been committed it will not show",
    "start": "1289350",
    "end": "1294960"
  },
  {
    "text": "up in any of the queries and to make it more efficient basically as soon as you know this one has been saved properly",
    "start": "1294960",
    "end": "1302520"
  },
  {
    "text": "all older IDs automatically also become become marked as as saved and persisted",
    "start": "1302520",
    "end": "1309870"
  },
  {
    "text": "and so they can be returning theories and such or used for calculations or what-have-you and basically if if we",
    "start": "1309870",
    "end": "1318360"
  },
  {
    "text": "don't persist something fully or if it's great fails we just toss away that data and at some point we just go beyond that",
    "start": "1318360",
    "end": "1324390"
  },
  {
    "text": "with our counter and then we yeah everything's fine if we crash or",
    "start": "1324390",
    "end": "1329520"
  },
  {
    "text": "anything we just toss away the data once again and the counter keeps us as from doing anything wrongly and just",
    "start": "1329520",
    "end": "1336809"
  },
  {
    "text": "starts at Zurich and it goes up so but there is even more we like to say well",
    "start": "1336809",
    "end": "1344640"
  },
  {
    "start": "1339000",
    "end": "1394000"
  },
  {
    "text": "especially I like to say that we really want to change how the world does monitoring and we are being serious",
    "start": "1344640",
    "end": "1350880"
  },
  {
    "text": "about this one one of the single most powerful things which we have within",
    "start": "1350880",
    "end": "1356730"
  },
  {
    "text": "parameters are labels of course you're doing I see people nodding that's good it's just so insanely more powerful and",
    "start": "1356730",
    "end": "1364789"
  },
  {
    "text": "the thing about labels is they are encoding our exposition format so anyone",
    "start": "1364789",
    "end": "1370500"
  },
  {
    "text": "who is using our exposition format and this is quite a few projects in the meantime already support to some extent",
    "start": "1370500",
    "end": "1377549"
  },
  {
    "text": "at least labels as opposed to hierarchy datasets which is great still some",
    "start": "1377549",
    "end": "1383159"
  },
  {
    "text": "companies some projects have issues with supporting something which they see as competing which I can understand on one",
    "start": "1383159",
    "end": "1391350"
  },
  {
    "text": "hand on the other and maybe not as much but whatever let's just fix it so we are spending our to premises",
    "start": "1391350",
    "end": "1397919"
  },
  {
    "start": "1394000",
    "end": "1475000"
  },
  {
    "text": "exposition form it into its own thing it's called open metrics we had a",
    "start": "1397919",
    "end": "1403710"
  },
  {
    "text": "kickoff or just last year at Google London are working on writing an RFC",
    "start": "1403710",
    "end": "1408809"
  },
  {
    "text": "that's actually me but I've been time locked for the last few months promises and the Python client library already",
    "start": "1408809",
    "end": "1415649"
  },
  {
    "text": "support open metrics so if you use the most current version of those two they will auto negotiate using open metrics",
    "start": "1415649",
    "end": "1421409"
  },
  {
    "text": "in between themselves without you noticing we have stuff in there at that",
    "start": "1421409",
    "end": "1426779"
  },
  {
    "text": "repo I'm also I already uploaded on slide so you don't have to take pictures it's fine",
    "start": "1426779",
    "end": "1433230"
  },
  {
    "text": "but we want to go also just those metrics at least within the context not with promises but with this whole",
    "start": "1433230",
    "end": "1439250"
  },
  {
    "text": "observability space and story so what open metrics allows you to do is",
    "start": "1439250",
    "end": "1445740"
  },
  {
    "text": "allows you to attach exemplars to histogram packets so you can for example say my trace is in this in this",
    "start": "1445740",
    "end": "1452970"
  },
  {
    "text": "licensees bucket and you can just jump from that really bad latency directly into a trace which is really really",
    "start": "1452970",
    "end": "1458730"
  },
  {
    "text": "powerful to just go into stuff and if you look at for example open sensors when SEC driver those will support this",
    "start": "1458730",
    "end": "1464850"
  },
  {
    "text": "and for me sis will probably never ever persist ex-employer information just as carly's which is totally fine as long as",
    "start": "1464850",
    "end": "1471670"
  },
  {
    "text": "they keep speaking the same language to talk to each other",
    "start": "1471670",
    "end": "1476520"
  },
  {
    "text": "we are also trying to go more beyond us open tracing is interested cloud events",
    "start": "1476679",
    "end": "1481720"
  },
  {
    "text": "is interested we have this observability site trick actually already during this",
    "start": "1481720",
    "end": "1487600"
  },
  {
    "text": "conference and the goal is to have this one standard which is really well formulated in well-engineered",
    "start": "1487600",
    "end": "1494230"
  },
  {
    "text": "and they all support labels so you can correlate your data quite easily with each other and just basically like this",
    "start": "1494230",
    "end": "1501880"
  },
  {
    "text": "change how the stuff is done there is a little bit of interest it's easily three",
    "start": "1501880",
    "end": "1507100"
  },
  {
    "text": "times that but I stopped updating once I ran out of space and so yes we do want",
    "start": "1507100",
    "end": "1514480"
  },
  {
    "start": "1511000",
    "end": "1572000"
  },
  {
    "text": "to change the road we will always and this is a hard promise core promises promises probably will always be simple",
    "start": "1514480",
    "end": "1521170"
  },
  {
    "text": "and resilient to operate it will most likely always be a single binary we are working on other stuff as well but this",
    "start": "1521170",
    "end": "1527530"
  },
  {
    "text": "is one of the promises of promises proper the most important thing we have",
    "start": "1527530",
    "end": "1533590"
  },
  {
    "text": "with infirmities and the one thing which we take most care that it will always work is going from your raw data to your",
    "start": "1533590",
    "end": "1541240"
  },
  {
    "text": "alerts to alert you that some things on fire everything else is nice but it's secondary that is the single most thing",
    "start": "1541240",
    "end": "1548320"
  },
  {
    "text": "we are doing within the project yeah we are also talking to hardware vendors",
    "start": "1548320",
    "end": "1553690"
  },
  {
    "text": "that is going slowly but not too bad and we are always trying to unless you're",
    "start": "1553690",
    "end": "1559360"
  },
  {
    "text": "like a large search company or something always support whatever scale you might",
    "start": "1559360",
    "end": "1565120"
  },
  {
    "text": "have tomorrow already today and more than that just so you can grow and do this then just toss more metrics at us",
    "start": "1565120",
    "end": "1572220"
  },
  {
    "start": "1572000",
    "end": "1609000"
  },
  {
    "text": "yeah here are a few relevant talks which you would probably like to see again you",
    "start": "1572220",
    "end": "1577660"
  },
  {
    "text": "don't have to take pictures and I hope you have lots of questions",
    "start": "1577660",
    "end": "1582540"
  },
  {
    "text": "I hate sorry for my voice is there a",
    "start": "1584039",
    "end": "1601830"
  },
  {
    "text": "maximum of metrics that prophetess is able to retrieve simultaneously so",
    "start": "1601830",
    "end": "1609779"
  },
  {
    "start": "1609000",
    "end": "1671000"
  },
  {
    "text": "prometheus is at least starting with the 2.0 release is largely just resource",
    "start": "1609779",
    "end": "1615570"
  },
  {
    "text": "bound so whatever can go over the wire it can do like it's not limited in some",
    "start": "1615570",
    "end": "1622320"
  },
  {
    "text": "technical sense it's just whatever the hardware allows used to do prometheus is gonna do I mean there are still like",
    "start": "1622320",
    "end": "1629099"
  },
  {
    "text": "optimizations and tweaks that we can do but it's like generally it scales with the hardware there's one thing where we",
    "start": "1629099",
    "end": "1637379"
  },
  {
    "text": "limit to eleven thousand data points in one query but you can change that it's just a safety mechanism but it's it's a",
    "start": "1637379",
    "end": "1643859"
  },
  {
    "text": "config flag so you can just disable to change it more questions",
    "start": "1643859",
    "end": "1652669"
  },
  {
    "text": "yeah can you touch a word about scaling primitives with multiple coercion for",
    "start": "1657409",
    "end": "1663539"
  },
  {
    "text": "the important you have a lot of targets and we have you have multiple go routines and how do you use not even of",
    "start": "1663539",
    "end": "1670350"
  },
  {
    "text": "course to scale so as of as of today a good number would be to say twenty Kay a",
    "start": "1670350",
    "end": "1676200"
  },
  {
    "start": "1671000",
    "end": "1733000"
  },
  {
    "text": "twenty no sorry two hundred thousand samples per second and core that's something you can consistently ingest",
    "start": "1676200",
    "end": "1683549"
  },
  {
    "text": "with with Prometheus so as long as you're staying under this limit per core",
    "start": "1683549",
    "end": "1688860"
  },
  {
    "text": "you are most likely fine if you want to go to absolute extremes it's always a",
    "start": "1688860",
    "end": "1693929"
  },
  {
    "text": "good idea to do your own testing and try and break things and once you broke it please make a blog post and tell us how",
    "start": "1693929",
    "end": "1699570"
  },
  {
    "text": "you broke it and when it broke but yeah this is a good number to start with",
    "start": "1699570",
    "end": "1704899"
  },
  {
    "text": "yeah I always keep in mind that this is like the most expensive thing and",
    "start": "1704899",
    "end": "1710159"
  },
  {
    "text": "prometheus still is to create a new time series so all this resource consumption",
    "start": "1710159",
    "end": "1716309"
  },
  {
    "text": "is very much based on how much turn you're going to have we optimize turn crop return a lot in 2.0 but it's still",
    "start": "1716309",
    "end": "1723539"
  },
  {
    "text": "basically the most expensive operation to do do you recommend for the local",
    "start": "1723539",
    "end": "1735330"
  },
  {
    "start": "1733000",
    "end": "1822000"
  },
  {
    "text": "storage to use something do you recommend for a local storage with something like EFS or NFS instead of",
    "start": "1735330",
    "end": "1744029"
  },
  {
    "text": "having multiple isolation event or Bechtold no no no any particular reasons",
    "start": "1744029",
    "end": "1749399"
  },
  {
    "text": "why not it'll break or it probably won't break but it will be more pain than it",
    "start": "1749399",
    "end": "1756330"
  },
  {
    "text": "should be you're introducing stuff complexity latency at a level where you don't want",
    "start": "1756330",
    "end": "1761669"
  },
  {
    "text": "to have it just don't so Prometheus also heavily relies on a map",
    "start": "1761669",
    "end": "1767100"
  },
  {
    "text": "to work correctly and over a network file system that's difficult to actually get right so we've seen a number of",
    "start": "1767100",
    "end": "1774269"
  },
  {
    "text": "issues with NFS already which is why we officially say that we don't support it where if you give us an issue and we can",
    "start": "1774269",
    "end": "1782190"
  },
  {
    "text": "reproduce it we're probably going to try and fix it but I for example don't run",
    "start": "1782190",
    "end": "1787980"
  },
  {
    "text": "any of my stuff on network disks because of this yeah",
    "start": "1787980",
    "end": "1796240"
  },
  {
    "text": "is there any plan to support replication in prometheus across nodes in the data",
    "start": "1801610",
    "end": "1808669"
  },
  {
    "text": "replication what do you mean don't get a question as an as in two nodes sharing",
    "start": "1808669",
    "end": "1816440"
  },
  {
    "text": "the data just like a my sequel database or was chris gloucester you can already",
    "start": "1816440",
    "end": "1822590"
  },
  {
    "text": "get the data out of communities basically you always run prompt queries no matter if you want to just have the",
    "start": "1822590",
    "end": "1828440"
  },
  {
    "text": "most current data if you want to have a graph if you want to put it into graph on ax if you want to have an alert a little girl recording rule or something",
    "start": "1828440",
    "end": "1835639"
  },
  {
    "text": "for federation it's always basically the same it's always prompt QL and then the background you always return jason",
    "start": "1835639",
    "end": "1842210"
  },
  {
    "text": "already so you just basically make a request again C API and you get Jason",
    "start": "1842210",
    "end": "1847460"
  },
  {
    "text": "back so I think you were basically talking about making Prometheus a distributed system and whereas not",
    "start": "1847460",
    "end": "1854480"
  },
  {
    "text": "planning to do that that this is it's like really the the core thing about prometheus is it needs to be super",
    "start": "1854480",
    "end": "1860809"
  },
  {
    "text": "resilient and if as soon as it like relies on network for just querying for",
    "start": "1860809",
    "end": "1866090"
  },
  {
    "text": "example it's not gonna work anymore",
    "start": "1866090",
    "end": "1869768"
  },
  {
    "text": "could you talk more about how remote write works and if we don't really want",
    "start": "1872679",
    "end": "1878960"
  },
  {
    "text": "to keep any metrics locally and we have a good solution on the side where we can also read from there how would you",
    "start": "1878960",
    "end": "1886309"
  },
  {
    "text": "recommend configuring it you need another end point where you just write the data to and you configure without",
    "start": "1886309",
    "end": "1893299"
  },
  {
    "text": "local disk that works it's a config flag actually you can do no local disk in",
    "start": "1893299",
    "end": "1899000"
  },
  {
    "text": "yeah okay but as we were saying like we're implementing the basically",
    "start": "1899000",
    "end": "1907299"
  },
  {
    "text": "replicating the right ahead log so you basically only need the most recent two",
    "start": "1907299",
    "end": "1913250"
  },
  {
    "text": "hours of data is what basically is a persistent buffer on disk and everything",
    "start": "1913250",
    "end": "1918799"
  },
  {
    "text": "else you can just you can just set your attention to two hours and it will never keep the processor chunks on to two",
    "start": "1918799",
    "end": "1927350"
  },
  {
    "text": "hours is a must yeah from 2.0 onwards",
    "start": "1927350",
    "end": "1933100"
  },
  {
    "text": "the OPM I think suspense is you know I",
    "start": "1933350",
    "end": "1939679"
  },
  {
    "text": "have we have we thought about removing the remote read and remote right models",
    "start": "1944780",
    "end": "1952460"
  },
  {
    "text": "and API functionality from Prometheus core so that it's easier for like",
    "start": "1952460",
    "end": "1959370"
  },
  {
    "text": "external applications to consume those models and libraries I don't know it may",
    "start": "1959370",
    "end": "1971880"
  },
  {
    "text": "be undermines and the question but I would removing help others do you mean",
    "start": "1971880",
    "end": "1981470"
  },
  {
    "text": "oh okay so basically what you're saying is have we thought about like making ven",
    "start": "1987020",
    "end": "1993320"
  },
  {
    "text": "during the relevant aspects better yeah",
    "start": "1993320",
    "end": "1999020"
  },
  {
    "text": "okay that's interesting that's good feedback we we haven't heard heard of this before basically it the question was about how",
    "start": "1999020",
    "end": "2006429"
  },
  {
    "text": "can we reduce the surface of stuff that needs to be been dirt when people want",
    "start": "2006429",
    "end": "2012550"
  },
  {
    "text": "to build these integrations no I don't think we've actually considered that but that's great feedback so if we see one",
    "start": "2012550",
    "end": "2024790"
  },
  {
    "text": "promises instance consume much memory so just a JIRA data ingesting and did your",
    "start": "2024790",
    "end": "2031720"
  },
  {
    "text": "data compaction and there is querying can you suggest which part might be the you know the most part to consume memory",
    "start": "2031720",
    "end": "2037960"
  },
  {
    "text": "how we can troubleshoot that yeah that's",
    "start": "2037960",
    "end": "2044320"
  },
  {
    "text": "probably one of the hardest thing like particularly because querying is can use",
    "start": "2044320",
    "end": "2050020"
  },
  {
    "text": "a lot of memory because basically what what you're doing is you're loading all of these samples from disk and then",
    "start": "2050020",
    "end": "2055600"
  },
  {
    "text": "doing computation on it right so at some point in time the actual raw data was in memory and so yeah that does can take a",
    "start": "2055600",
    "end": "2063100"
  },
  {
    "text": "lot of resources to troubleshoot that I",
    "start": "2063100",
    "end": "2069490"
  },
  {
    "text": "would just recommend like taking profiles and sending them to us we actually have the prom tool you can run",
    "start": "2069490",
    "end": "2078040"
  },
  {
    "text": "prom tool against the Prometheus server it's like prom prom tool debug and it'll",
    "start": "2078040",
    "end": "2083648"
  },
  {
    "text": "get all the relevant profiles and you can just like open an issue and we can",
    "start": "2083649",
    "end": "2088929"
  },
  {
    "text": "look into the data",
    "start": "2088929",
    "end": "2092158"
  },
  {
    "start": "2099000",
    "end": "2139000"
  },
  {
    "text": "can you please explain more about the snapshotting backups and recovery and where they can take it to toward or how",
    "start": "2100060",
    "end": "2108130"
  },
  {
    "text": "to access them you just snapshot your VMO whatever and the format you have on",
    "start": "2108130",
    "end": "2114430"
  },
  {
    "text": "disk is safe when you snap shot when you snapshot your your block device and that's basically it",
    "start": "2114430",
    "end": "2121620"
  },
  {
    "text": "yes it's still the same source so if you were using LVM for example you could just create a snapshot and and it'll be",
    "start": "2121620",
    "end": "2128820"
  },
  {
    "text": "consistent and that wasn't the case with one not exit it often worked but there",
    "start": "2128820",
    "end": "2133840"
  },
  {
    "text": "was no hard guarantee now basically diversed what can happen is you lose two hours of data yeah so",
    "start": "2133840",
    "end": "2140400"
  },
  {
    "start": "2139000",
    "end": "2201000"
  },
  {
    "text": "like in terms of the database terms that Richie had on the slide earlier Prometheus one point X wasn't it didn't",
    "start": "2140400",
    "end": "2149170"
  },
  {
    "text": "have the atomicity characteristic which means like we can always guarantee that",
    "start": "2149170",
    "end": "2156700"
  },
  {
    "text": "whatever is on disk is actually consistent yeah so when Prometheus",
    "start": "2156700",
    "end": "2164110"
  },
  {
    "text": "crashes or something we can actually properly recover from that and that has the same characteristic for snapshotting",
    "start": "2164110",
    "end": "2172720"
  },
  {
    "text": "and there was a follow-up up here which was good so there is also a functionality for for actual backups and",
    "start": "2172720",
    "end": "2180610"
  },
  {
    "text": "Prometheus which basically because Prometheus that the chunks are immutable",
    "start": "2180610",
    "end": "2185650"
  },
  {
    "text": "so all that it really is it's just it does a hard link on the on the databases",
    "start": "2185650",
    "end": "2193150"
  },
  {
    "text": "and you can do like back that up to some external storage",
    "start": "2193150",
    "end": "2199140"
  },
  {
    "start": "2201000",
    "end": "2229000"
  },
  {
    "text": "so yet labels are absolutely awesome can you help me understand how as you add",
    "start": "2201720",
    "end": "2206730"
  },
  {
    "text": "more labels to a time series how that will impact impact performance at some point it'll start impacting performance",
    "start": "2206730",
    "end": "2213320"
  },
  {
    "text": "usually though when you say more what do you mean like you 5 10 10 100 right now",
    "start": "2213320",
    "end": "2221820"
  },
  {
    "text": "we probably have around like 30 labels on our time so much so or I guess in",
    "start": "2221820",
    "end": "2228270"
  },
  {
    "text": "what way is it too much just from the conceptual point of view just from the",
    "start": "2228270",
    "end": "2233640"
  },
  {
    "start": "2229000",
    "end": "2260000"
  },
  {
    "text": "sniff test I would assume that you would be able to to push some of those labels into info metrics and then just group",
    "start": "2233640",
    "end": "2239550"
  },
  {
    "text": "this information into the other metrics when you need them 30 is why I don't",
    "start": "2239550",
    "end": "2246180"
  },
  {
    "text": "think I ever saw someone using 15 properly and so 30 it just it's too much",
    "start": "2246180",
    "end": "2251730"
  },
  {
    "text": "unless you're doing really really extreme stuff but you should look into info metrics probably thank you thing to",
    "start": "2251730",
    "end": "2257640"
  },
  {
    "text": "do I think the general kind of like",
    "start": "2257640",
    "end": "2265740"
  },
  {
    "start": "2260000",
    "end": "2286000"
  },
  {
    "text": "guidance on labeling is you want to have only that information in labels that",
    "start": "2265740",
    "end": "2270900"
  },
  {
    "text": "actually uniquely identifies the time series so if you do actually actually have 30 labels that only in that",
    "start": "2270900",
    "end": "2278310"
  },
  {
    "text": "combination uniquely identify the time series maybe it's fine but probably not",
    "start": "2278310",
    "end": "2283940"
  },
  {
    "text": "yeah so in the remote reads the labels weren't exported well so I",
    "start": "2283940",
    "end": "2291359"
  },
  {
    "start": "2286000",
    "end": "2318000"
  },
  {
    "text": "was running into an issue where Prometheus we had a PMF Kono's monitoring framework and we will we are",
    "start": "2291359",
    "end": "2297570"
  },
  {
    "text": "you're running Prometheus with all of our node exporters everything else and we just wanted a remote reads from their",
    "start": "2297570",
    "end": "2302820"
  },
  {
    "text": "tool to ours so that we would does you'd be the data source but what we found out is that the graph founded dashboards wouldn't bring",
    "start": "2302820",
    "end": "2309150"
  },
  {
    "text": "the labels into the dropdowns because they weren't actually exposed to other",
    "start": "2309150",
    "end": "2314640"
  },
  {
    "text": "remote read endpoints did you use the most current version and",
    "start": "2314640",
    "end": "2320540"
  },
  {
    "start": "2318000",
    "end": "2382000"
  },
  {
    "text": "if yes could you thought about it should probably not happen it sounds weird it",
    "start": "2320540",
    "end": "2326930"
  },
  {
    "text": "sounds like a bug okay okay this is the",
    "start": "2326930",
    "end": "2334310"
  },
  {
    "text": "boring part about security you mentioned as a plan to add a physician and HTTPS and all this stuff",
    "start": "2334310",
    "end": "2340820"
  },
  {
    "text": "it's only on Prometheus core or also the exporters would support that the support",
    "start": "2340820",
    "end": "2346100"
  },
  {
    "text": "is as well so this is supposed to be like a comprehensive security story so previously what we said was like if you",
    "start": "2346100",
    "end": "2353300"
  },
  {
    "text": "want to have exporters be authenticated then you have to put a proxy in front of it or something and so yeah we do want",
    "start": "2353300",
    "end": "2362180"
  },
  {
    "text": "to build it into node exporter into I don't know although exporters that we have in the Prometheus org yeah actually",
    "start": "2362180",
    "end": "2370430"
  },
  {
    "text": "we're going to start with node exporter and basically extract all the common",
    "start": "2370430",
    "end": "2375710"
  },
  {
    "text": "things then out of North Port or into the common repo and then share that across the repositories it goes into the",
    "start": "2375710",
    "end": "2384050"
  },
  {
    "start": "2382000",
    "end": "2391000"
  },
  {
    "text": "libraries basically so it will permeate through the ecosystem yeah if you're",
    "start": "2384050",
    "end": "2392330"
  },
  {
    "start": "2391000",
    "end": "2453000"
  },
  {
    "text": "using our libraries then yes I think maybe we take last question and then of course yep",
    "start": "2392330",
    "end": "2399160"
  },
  {
    "text": "can you elaborate a bit on info matrix if I understood you correctly what is",
    "start": "2399160",
    "end": "2404300"
  },
  {
    "text": "this because I was just trying to google it and I failed basically it's underscore info whatever",
    "start": "2404300",
    "end": "2409850"
  },
  {
    "text": "underscore info and then it just the value is always one and you put labels",
    "start": "2409850",
    "end": "2415520"
  },
  {
    "text": "which you want to correlate into other stuff and then you have something which identifies between this info metric and",
    "start": "2415520",
    "end": "2421850"
  },
  {
    "text": "the other metrics but it's probably best to take this downstage course if anyone",
    "start": "2421850",
    "end": "2428690"
  },
  {
    "text": "wants open metrics ribbons for their badges I've I printed plenty",
    "start": "2428690",
    "end": "2433880"
  },
  {
    "text": "they are also rainbow so it's also predicted statement yeah maybe I can",
    "start": "2433880",
    "end": "2439880"
  },
  {
    "text": "also answer answer that question with a with a concrete example so in the coop state metrics exporter for kubernetes",
    "start": "2439880",
    "end": "2447170"
  },
  {
    "text": "sorry okay okay we're away weirdo",
    "start": "2447170",
    "end": "2452199"
  }
]