[
  {
    "start": "0",
    "end": "75000"
  },
  {
    "text": "so thank you all for coming my name is Tim I work at Google on kubernetes so today's talk as we got ready for cube",
    "start": "30",
    "end": "8639"
  },
  {
    "text": "con I thought I would like to you know do a talk because I like to hear myself speak and I thought well like what I",
    "start": "8639",
    "end": "15179"
  },
  {
    "text": "want to talk about so I went through sort of the history of some of the interactions we've had with customers through Google container engine and",
    "start": "15179",
    "end": "20490"
  },
  {
    "text": "through kubernetes open source and I and I picked up on a theme of things that people were asking that I thought they",
    "start": "20490",
    "end": "26160"
  },
  {
    "text": "were asking the wrong questions I was gonna title this this talk you're asking the wrong questions but I thought it was a little too negative so and then in",
    "start": "26160",
    "end": "33840"
  },
  {
    "text": "preparing for this I thought there's actually some background that I need to bring people up to speed on before I can actually answer these questions",
    "start": "33840",
    "end": "39420"
  },
  {
    "text": "correctly so I have 73 slides in 40 minutes there's no way I'm gonna get to cover it",
    "start": "39420",
    "end": "45989"
  },
  {
    "text": "all so I'm gonna rush through some of it I'm gonna try to give you a little bit of a background on scheduling and the way we think about resources in Google",
    "start": "45989",
    "end": "52469"
  },
  {
    "text": "and in kubernetes and and how are you go through that if you have questions please ask questions right away if it's",
    "start": "52469",
    "end": "58649"
  },
  {
    "text": "if it's relevant we'll try to answer it in line if not we'll save it for the end so I generally hate these like Who am I",
    "start": "58649",
    "end": "65700"
  },
  {
    "text": "slides I put this up just so you can see the physical me and the github me if you",
    "start": "65700",
    "end": "70860"
  },
  {
    "text": "get on github and you see that guy that that's me so fair warning some of what we're gonna",
    "start": "70860",
    "end": "77159"
  },
  {
    "start": "75000",
    "end": "92000"
  },
  {
    "text": "do today is aspirational actually most of it is aspirational I'm gonna talk a lot about what we've done in Bourg",
    "start": "77159",
    "end": "82490"
  },
  {
    "text": "inside Google and what we want to do in kubernetes what we should do in communities not all of this is done not",
    "start": "82490",
    "end": "87869"
  },
  {
    "text": "all of this has even started there's an opportunity for people to help so starting in I posit that kubernetes",
    "start": "87869",
    "end": "96360"
  },
  {
    "start": "92000",
    "end": "178000"
  },
  {
    "text": "is fundamentally underneath everything is about managing resources resources what I mean by resources",
    "start": "96360",
    "end": "103619"
  },
  {
    "text": "resources are those things that are in your computer that you use to run your jobs there's CPU and its memory",
    "start": "103619",
    "end": "108780"
  },
  {
    "text": "everybody's familiar with these right but it's also disk space ok we sort of get that one it's also a disk time right",
    "start": "108780",
    "end": "115680"
  },
  {
    "text": "it's also disk spindles right ok I get it I get discs let's talk about network",
    "start": "115680",
    "end": "121439"
  },
  {
    "text": "bandwidth let's talk about ports on the machine let's talk about cache let's",
    "start": "121439",
    "end": "126659"
  },
  {
    "text": "talk about memory bandwidth IP addresses is a very interesting problem in kubernetes and in containers in general",
    "start": "126659",
    "end": "133370"
  },
  {
    "text": "storage attachments we talk about things like process IDs GPUs power right Google",
    "start": "133370",
    "end": "138950"
  },
  {
    "text": "ghosts as far as we manage power as a contained resource and we charge back for it remember I said aspirational",
    "start": "138950",
    "end": "145819"
  },
  {
    "text": "right we're not there yet and then there's this whole other category of arbitrary opaque things that I don't",
    "start": "145819",
    "end": "152930"
  },
  {
    "text": "know about that you probably have in some of your hardware that you want to schedule jobs against but you don't want",
    "start": "152930",
    "end": "157970"
  },
  {
    "text": "to patch kubernetes to understand that it's really special to you maybe it's GPS receivers or maybe it's a special",
    "start": "157970",
    "end": "164480"
  },
  {
    "text": "type of storage device or maybe it's just some some fact about the machine and you want to be able to schedule to",
    "start": "164480",
    "end": "170209"
  },
  {
    "text": "these things so we have to introduce this this class of sort of opaque resources now we've got a 14 dimensional",
    "start": "170209",
    "end": "177049"
  },
  {
    "text": "problem so as I want to build a chi mental model of kubernetes first so nodes these are",
    "start": "177049",
    "end": "183560"
  },
  {
    "start": "178000",
    "end": "197000"
  },
  {
    "text": "us our word for machines once upon a time called minions nodes produce capacity they offer it into the cluster",
    "start": "183560",
    "end": "189409"
  },
  {
    "text": "right here's a little snippet of the amol that node object would produce I have capacity of 4 CPUs and I have 32",
    "start": "189409",
    "end": "196099"
  },
  {
    "text": "gigs of memory pods that's our icon for pod get used to it pods consume resources they say I need",
    "start": "196099",
    "end": "204280"
  },
  {
    "text": "CPU I need memory and then the scheduler is going to map those things together",
    "start": "204280",
    "end": "209450"
  },
  {
    "text": "it's going to build a binding between a pod and a node and that pot is then going to run on that node simple stuff right so",
    "start": "209450",
    "end": "215739"
  },
  {
    "text": "let's build out the mental model of how do we represent resources",
    "start": "215739",
    "end": "221500"
  },
  {
    "start": "220000",
    "end": "237000"
  },
  {
    "text": "it's attractive to think about resources in a sort of a multi-dimensional box I'm",
    "start": "221500",
    "end": "226549"
  },
  {
    "text": "only gonna focus on CPU and memory today because I can't do 14 at once so it's",
    "start": "226549",
    "end": "231889"
  },
  {
    "text": "it's attractive to think about memory and and CPU as a two dimensional box right and you're gonna pack stuff into",
    "start": "231889",
    "end": "237680"
  },
  {
    "start": "237000",
    "end": "245000"
  },
  {
    "text": "the box so I've got my blue job and it fits into the box awesome cool I can continue to pack stuff into my box until",
    "start": "237680",
    "end": "244099"
  },
  {
    "text": "my box is full right no this is not the right way to think about resources when in this case I've",
    "start": "244099",
    "end": "250730"
  },
  {
    "start": "245000",
    "end": "258000"
  },
  {
    "text": "double used everything when you think about resources that blue thing that we schedule first it casts a shadow in",
    "start": "250730",
    "end": "255949"
  },
  {
    "text": "every dimension so I now have only this much available this is a really weird",
    "start": "255949",
    "end": "261229"
  },
  {
    "start": "258000",
    "end": "266000"
  },
  {
    "text": "way to think about resources and to represent them it's really attractive sorry",
    "start": "261229",
    "end": "266580"
  },
  {
    "start": "266000",
    "end": "290000"
  },
  {
    "text": "so we have a set of mike says it's a vector not a box it is in fact a set of parallel vectors this is how we think",
    "start": "266580",
    "end": "273940"
  },
  {
    "text": "about resources this is how I'm gonna represent them when I talk about them in this in this way this is I think much",
    "start": "273940",
    "end": "280330"
  },
  {
    "text": "clearer to to visualize it's just not as is immediately obvious to you so now I",
    "start": "280330",
    "end": "286510"
  },
  {
    "text": "want to talk about scheduling scheduling is sort of the heart of what qu Brandis does so imagine we have our hypothetical",
    "start": "286510",
    "end": "292320"
  },
  {
    "start": "290000",
    "end": "354000"
  },
  {
    "text": "kubernetes cluster we have two nodes a and B they equal CPU and equal ram they have ten units of cpu and ten units of",
    "start": "292320",
    "end": "299860"
  },
  {
    "text": "ram okay so I've got a blue job and this blue job comes in it says I need six units of CPU and four units of ram no",
    "start": "299860",
    "end": "306790"
  },
  {
    "text": "problem the scheduler can stick it on their first prop on the first node first fit now the green job comes along and",
    "start": "306790",
    "end": "312580"
  },
  {
    "text": "says okay I want to I want to three units of CPU and four units of RAM we say great actually we don't do first of",
    "start": "312580",
    "end": "318880"
  },
  {
    "text": "it we sort of do best fit so it fits on this node awesome everything's being spread across the cluster now if one",
    "start": "318880",
    "end": "325120"
  },
  {
    "text": "node fails at least you don't bring out all of your jobs with you I come along with my yellow job and it tries to fit",
    "start": "325120",
    "end": "332200"
  },
  {
    "text": "on node a that doesn't work so that no day is removed from the the pool of eligible nodes but it fits fine on node",
    "start": "332200",
    "end": "337870"
  },
  {
    "text": "B cool we've still got some leftover resources let's keep scheduling so we bring along this red pod and the red pod",
    "start": "337870",
    "end": "343930"
  },
  {
    "text": "we stick it on no Dame that doesn't work so we remove node a from the eligibility pool we try to fit it on the node B it",
    "start": "343930",
    "end": "349810"
  },
  {
    "text": "doesn't fit on node B either node B is removed from the eligibility pool so oh damn my pod just went pending",
    "start": "349810",
    "end": "357729"
  },
  {
    "start": "354000",
    "end": "490000"
  },
  {
    "text": "right and you'll see this in the cube cuddle state for a pod that can't be scheduled pending if you look at the",
    "start": "357729",
    "end": "363669"
  },
  {
    "text": "events you'll see no available in sufficient resources to run this this is a bad place to be because you look at",
    "start": "363669",
    "end": "369640"
  },
  {
    "text": "this cluster as a person you like both clearly there's enough resources in this cluster if you look at your aggregate",
    "start": "369640",
    "end": "374710"
  },
  {
    "text": "dashboard there's more than enough resources to fit in this cluster what we need is an optimizing reschedule er this",
    "start": "374710",
    "end": "380979"
  },
  {
    "text": "is that aspirational part right I need something to come along and say hey you know what I can VIN pack this",
    "start": "380979",
    "end": "386800"
  },
  {
    "text": "better I can move the green up to the first node and then I can actually fit the red on the second node that would be",
    "start": "386800",
    "end": "392950"
  },
  {
    "text": "really cool right patches welcome",
    "start": "392950",
    "end": "397650"
  },
  {
    "text": "the problem with all this is we know how to do it we know what to do we just don't have enough hands on enough",
    "start": "398120",
    "end": "403860"
  },
  {
    "text": "keyboards to actually achieve it all in the timeframe that we want to achieve so we back of the envelope that if we just",
    "start": "403860",
    "end": "411060"
  },
  {
    "text": "do all the things we know we know how to do and we want to do already we've got about eight hundred man years of work left so it was our question",
    "start": "411060",
    "end": "420289"
  },
  {
    "text": "that's true",
    "start": "435800",
    "end": "439039"
  },
  {
    "text": "so for the sake of the the recording the question is that there are some",
    "start": "444920",
    "end": "450210"
  },
  {
    "text": "resources that exist that are not particular to anyone know that exists sort of in the abstract in the in the",
    "start": "450210",
    "end": "455520"
  },
  {
    "text": "network as addresses and and those sorts of things yes and I can talk at length about how",
    "start": "455520",
    "end": "462030"
  },
  {
    "text": "Google manages say aggregate network bandwidth or top-of-rack bandwidth I do",
    "start": "462030",
    "end": "467160"
  },
  {
    "text": "not have enough time today to talk about that",
    "start": "467160",
    "end": "471919"
  },
  {
    "text": "yes I do not pretend to have a full list of the resources here I'm actually really gonna go a little bit myopic",
    "start": "473030",
    "end": "480000"
  },
  {
    "text": "today and we're gonna just really focus on an unknown we're gonna talk a little bit about cluster at the end but really we're talking about node eccentric",
    "start": "480000",
    "end": "486060"
  },
  {
    "text": "resources you're right that there's a whole other category of stuff so now that we've rescheduled this this",
    "start": "486060",
    "end": "492990"
  },
  {
    "start": "490000",
    "end": "550000"
  },
  {
    "text": "red-pawed we noticed that the node B has all ten units of CPU consumed but not",
    "start": "492990",
    "end": "498180"
  },
  {
    "text": "all ten units of memory consumed we call this stranded this is resources that are just they're stuck they're isolated you",
    "start": "498180",
    "end": "504120"
  },
  {
    "text": "can't use them in the current state of things it is a real problem there's not a really good answer to it except that",
    "start": "504120",
    "end": "510990"
  },
  {
    "text": "you need to be aware of this as you think about your jobs this especially applies when you get to these bigger clusters right we're in aggregate you",
    "start": "510990",
    "end": "517680"
  },
  {
    "text": "start to see sort of a ratio of CPU to memory emerge that makes sense and there's the machines that you build google has a sort of a golden ratio of",
    "start": "517680",
    "end": "524430"
  },
  {
    "text": "how build we bit how big we build our standard machines and we expect jobs to sort of fit within spitting distance of",
    "start": "524430",
    "end": "530610"
  },
  {
    "text": "that golden ratio and if they're not they actually get charged more because they're going to strand resources right",
    "start": "530610",
    "end": "535740"
  },
  {
    "text": "you don't necessarily pay for what you use you pay for what you prevent other people from using right so in this case",
    "start": "535740",
    "end": "541649"
  },
  {
    "text": "we might charge back to each of those the red and the yellow jobs some portion of the stranded resources because",
    "start": "541649",
    "end": "547709"
  },
  {
    "text": "they're they're out of ratio right so now I get to the fun part so I think",
    "start": "547709",
    "end": "554189"
  },
  {
    "start": "550000",
    "end": "563000"
  },
  {
    "text": "a lot of people are just still looking at kubernetes in there they're coming at it from the history of what they've done before and they're asking the questions",
    "start": "554189",
    "end": "560309"
  },
  {
    "text": "that they know to ask but I think they're asking the wrong questions so these are real questions that I have",
    "start": "560309",
    "end": "565889"
  },
  {
    "start": "563000",
    "end": "579000"
  },
  {
    "text": "shortened for forfeiting on a slide but there are real questions have come from users how do how do I make sure that my",
    "start": "565889",
    "end": "572040"
  },
  {
    "text": "compute intensive jobs my cpu-bound jobs don't get scheduled on my database machine my database machine is special",
    "start": "572040",
    "end": "578040"
  },
  {
    "text": "right why would I want to use multiple replicas I just want to use all the",
    "start": "578040",
    "end": "583889"
  },
  {
    "start": "579000",
    "end": "591000"
  },
  {
    "text": "memory right this I don't understand how scaling works let me use it all please can you add in all parameter to",
    "start": "583889",
    "end": "589920"
  },
  {
    "text": "communities and borg has got the same question",
    "start": "589920",
    "end": "595189"
  },
  {
    "start": "591000",
    "end": "610000"
  },
  {
    "text": "how do I save some machines for important work like this is important",
    "start": "595189",
    "end": "600269"
  },
  {
    "text": "stuff and the other rest is batch it's MapReduce it's whatever I don't want those things to interfere with each other so how do i partition my cluster",
    "start": "600269",
    "end": "605519"
  },
  {
    "text": "this way so I can't obviously present questions without rewriting them so how",
    "start": "605519",
    "end": "611730"
  },
  {
    "start": "610000",
    "end": "638000"
  },
  {
    "text": "do I make sure that my compute you like the pictures those old machines sexy new",
    "start": "611730",
    "end": "616759"
  },
  {
    "text": "how do I make sure that my compute can't hurt my database what you should be talking about is oh sorry the the first",
    "start": "616759",
    "end": "623399"
  },
  {
    "text": "question of how do I make sure my computer doesn't hurt my databases how do I make sure that they can't hurt it huh it's not how do I make sure they",
    "start": "623399",
    "end": "630179"
  },
  {
    "text": "land on different machines it's how do they how do I make sure that they can't actually impact each other you shouldn't care about which machines they land on",
    "start": "630179",
    "end": "636629"
  },
  {
    "text": "we call this isolation how do I know how much memory and how much CPU my job needs this is about",
    "start": "636629",
    "end": "644639"
  },
  {
    "text": "sizing and how do I pack more machines more work on to less machines this is",
    "start": "644639",
    "end": "651179"
  },
  {
    "text": "about utilization so I'm gonna jump in each of these three topics and I'm just going to pontificate",
    "start": "651179",
    "end": "657509"
  },
  {
    "start": "653000",
    "end": "777000"
  },
  {
    "text": "until I run out of time so isolation right this is fundamentally",
    "start": "657509",
    "end": "662950"
  },
  {
    "text": "about preventing applications from hurting each other this is the the the cornerstone of building a system like",
    "start": "662950",
    "end": "669010"
  },
  {
    "text": "kubernetes you have to have isolation and it has to be reliable you have to make sure that if you bought some",
    "start": "669010",
    "end": "675580"
  },
  {
    "text": "resources if you bought four gigs of memory that you're gonna get that four gigs of memory if you don't you're going to do bad things you're gonna",
    "start": "675580",
    "end": "681490"
  },
  {
    "text": "over-provision you're going to not trust the system you're gonna be less efficient we want to drive efficiency up",
    "start": "681490",
    "end": "687870"
  },
  {
    "text": "so kubernetes and docker already isolates CPU and memory and they do a reasonable job of it using the the",
    "start": "687870",
    "end": "694060"
  },
  {
    "text": "kernel subsystems that exist today we don't even try to handle things like memory bandwidth or disk time or cache",
    "start": "694060",
    "end": "700990"
  },
  {
    "text": "or bandwidth these are these are much harder problems and they just haven't really been solved very well in Linux",
    "start": "700990",
    "end": "706930"
  },
  {
    "text": "yet Google has a bunch of patches that we're gonna continue to try to push upstream that will make these things",
    "start": "706930",
    "end": "712390"
  },
  {
    "text": "better we need to work with docker to define the api's to manage these sorts of things on machines the key here is",
    "start": "712390",
    "end": "720370"
  },
  {
    "text": "that when you get to the end game at the very extremes of the operation of the system it has to be stable and it has to",
    "start": "720370",
    "end": "725950"
  },
  {
    "text": "be predictable and I will take predictable over performant every day of the week so",
    "start": "725950",
    "end": "731640"
  },
  {
    "text": "you know we have a sort of running joke that every summer at Google we get a full on stress test of our isolation",
    "start": "731640",
    "end": "738070"
  },
  {
    "text": "systems when the interns arrive and because hey MapReduce it runs fine to 500 replicas I wonder 5,000 works no",
    "start": "738070",
    "end": "745060"
  },
  {
    "text": "problem I wonder 50,000 works no problem I wonder 500,000 works mm sometimes that",
    "start": "745060",
    "end": "751150"
  },
  {
    "text": "falls over and every time it does we get a p0 bug against the Borg team and the Borg team scrambles like crazy people to",
    "start": "751150",
    "end": "757450"
  },
  {
    "text": "figure out what the heck just happened and why were they able to sort of penetrate through the isolation and there's always something going on right",
    "start": "757450",
    "end": "763930"
  },
  {
    "text": "and we've built layer upon layer of isolation techniques that again I could",
    "start": "763930",
    "end": "769030"
  },
  {
    "text": "talk for days on that helped to prevent the the well-meaning but poorly informed",
    "start": "769030",
    "end": "774850"
  },
  {
    "text": "intern from destroying clusters so when does isolation when does it kick",
    "start": "774850",
    "end": "780340"
  },
  {
    "start": "777000",
    "end": "810000"
  },
  {
    "text": "in when does it really matter anybody ever written an infinite loop not I mean not me but maybe you did",
    "start": "780340",
    "end": "787080"
  },
  {
    "text": "and we have written a memory leak again totally not me",
    "start": "787080",
    "end": "792579"
  },
  {
    "text": "Discogs right sometimes you just run away with the disk and you need to forget to clean up oops",
    "start": "792579",
    "end": "797779"
  },
  {
    "text": "fork bombs I almost got kicked out of school for that and and the most nefarious of them all is cache thrashing",
    "start": "797779",
    "end": "803809"
  },
  {
    "text": "right because it's actually really not visible to you when your cache thrashing you're hurting everybody else",
    "start": "803809",
    "end": "810069"
  },
  {
    "start": "810000",
    "end": "905000"
  },
  {
    "text": "so how do we how do we defend against these things infinite loops I can I can",
    "start": "810069",
    "end": "815539"
  },
  {
    "text": "limit the amount of CPU that you use so Linux kernel gives us things like shares and quota which are two different ways of managing CPU for memory leaks I can",
    "start": "815539",
    "end": "823669"
  },
  {
    "text": "give you memory limits so that when you run out of memory you die not everybody else anybody ever run afoul of the Linux",
    "start": "823669",
    "end": "829910"
  },
  {
    "text": "out of memory killer right it's like firing a shotgun into a crowd right somebody gonna die I don't know who it",
    "start": "829910",
    "end": "835970"
  },
  {
    "text": "is usually it's the guy who uses the most memory which is not necessarily the same",
    "start": "835970",
    "end": "842329"
  },
  {
    "text": "as the guy who's actually causing the problem right so with the advent of cgroups this is getting a little bit",
    "start": "842329",
    "end": "847399"
  },
  {
    "text": "more rigid google's put a ton of work into making this very deterministic not all those patches have been accepted",
    "start": "847399",
    "end": "852799"
  },
  {
    "text": "upstream yet when it comes to discogs we've had disk quota forever and it actually works pretty well",
    "start": "852799",
    "end": "858639"
  },
  {
    "text": "comes to fork bombs there's finally process a process C group in Linux so we can actually tell you you were allowed",
    "start": "858639",
    "end": "865009"
  },
  {
    "text": "to have 10,000 pigs and not more right 10,000 should be enough for anybody and",
    "start": "865009",
    "end": "871279"
  },
  {
    "text": "when it comes to cache thrashing again the most nefarious you can do things like say I've detected you're a bad",
    "start": "871279",
    "end": "876470"
  },
  {
    "text": "actor and I keep picking on you because right in front of me you're a bad actor and I'm gonna put you and all the other",
    "start": "876470",
    "end": "882230"
  },
  {
    "text": "bad actors or all the other low priority jobs on a single last level cache thrash all you want you can't hurt everybody",
    "start": "882230",
    "end": "888109"
  },
  {
    "text": "else right now Intel has brought out new stuff with the cache allocation technology which is thrashing in the",
    "start": "888109",
    "end": "894799"
  },
  {
    "text": "kernel trying to figure out how to patch it in but when you apply things like that you can actually get really awesome",
    "start": "894799",
    "end": "900049"
  },
  {
    "text": "isolation where it doesn't matter how hard you thrash the cache everybody else is gonna be fine",
    "start": "900049",
    "end": "905769"
  },
  {
    "start": "905000",
    "end": "921000"
  },
  {
    "text": "now the state of these things infinite loops and memory leaks CPU and memory",
    "start": "905769",
    "end": "911059"
  },
  {
    "text": "those are pretty well understood at this point disk quota stuff is in progress in current kubernetes and the other stuff",
    "start": "911059",
    "end": "918259"
  },
  {
    "text": "at the moment nobody is working on patches welcome proposals first please",
    "start": "918259",
    "end": "925100"
  },
  {
    "start": "921000",
    "end": "1007000"
  },
  {
    "text": "so I want to introduce a quick resource taxonomy because I'm gonna use these words so I want to explain them first",
    "start": "925100",
    "end": "931450"
  },
  {
    "text": "there's two fundamental classes of resources compressible resources compressible resources have no state and",
    "start": "931450",
    "end": "939020"
  },
  {
    "text": "they're the main characteristics they can be taken away very quickly without actually hurting the application except",
    "start": "939020",
    "end": "944240"
  },
  {
    "text": "just performance right these are things that tend to be measured in time so like you measure CPU allocations people think",
    "start": "944240",
    "end": "952040"
  },
  {
    "text": "of it and how many cores do you get what you're really getting is how many core seconds per second right so these are things that tend to be",
    "start": "952040",
    "end": "959090"
  },
  {
    "text": "measured in time so if your application is using too much CPU I can take away CPU from you and your application",
    "start": "959090",
    "end": "964970"
  },
  {
    "text": "doesn't get wrong it just gets slow right the counterpoint to that is is non",
    "start": "964970",
    "end": "970820"
  },
  {
    "text": "compressible resources they tend to have state they tend to be very slow to be taken away and sometimes they just fail",
    "start": "970820",
    "end": "977990"
  },
  {
    "text": "to be taken away the classic example here is memory you",
    "start": "977990",
    "end": "986690"
  },
  {
    "text": "use up a whole bunch of memory and I need to take it back from you how do I do that I can't just take it away you",
    "start": "986690",
    "end": "992090"
  },
  {
    "text": "actually have stuff in there maybe have pointers that are cross-referencing it right maybe they're dirty pages I can do",
    "start": "992090",
    "end": "997550"
  },
  {
    "text": "things like write you back to disk I can do things like throw out all your clean pages and force you to fault them back",
    "start": "997550",
    "end": "1004060"
  },
  {
    "text": "in or I can just straight out and kill you if it takes too long so let's then",
    "start": "1004060",
    "end": "1010420"
  },
  {
    "start": "1007000",
    "end": "1075000"
  },
  {
    "text": "talk about how we apply limits to these things so kubernetes has a two-tiered concept of what you're asking for with",
    "start": "1010420",
    "end": "1017680"
  },
  {
    "text": "respect to resources the first tier is a request the request is what we use in the scheduler this is your guaranteed",
    "start": "1017680",
    "end": "1023680"
  },
  {
    "text": "amount if you ask for four gigs you're gonna get four gigs we're gonna do everything we can to make sure that you have four gigs available to you should",
    "start": "1023680",
    "end": "1030339"
  },
  {
    "text": "you need four gigs we do not over commit at this level at",
    "start": "1030339",
    "end": "1035980"
  },
  {
    "text": "EDD requests the second line is limits limit says I might use more than my",
    "start": "1035980",
    "end": "1041860"
  },
  {
    "text": "request if it's available to me and I'm okay with it being at a lower quality of",
    "start": "1041860",
    "end": "1046990"
  },
  {
    "text": "service right so it's sort of opportunistic CPU if there's extra CPU on the machine I'll use it to complete",
    "start": "1046990",
    "end": "1052180"
  },
  {
    "text": "my thing faster but if it's not there I'll be fine right this gets really interesting when you start talking about",
    "start": "1052180",
    "end": "1057550"
  },
  {
    "text": "non compressible read sources and being over request but under limit right if I'm using more memory than I'm",
    "start": "1057550",
    "end": "1063280"
  },
  {
    "text": "technically allowed to use and somebody else needs it you have to reclaim it so",
    "start": "1063280",
    "end": "1071610"
  },
  {
    "text": "this is what we used to define our quality of service bands so quality of service guaranteed it says I'm not going",
    "start": "1071610",
    "end": "1078070"
  },
  {
    "start": "1075000",
    "end": "1137000"
  },
  {
    "text": "to use more than I've requested set request equals limit we're happy you should always get what you asked for in",
    "start": "1078070",
    "end": "1084160"
  },
  {
    "text": "that case in a fully packed machine if everybody is running full tilt they will all get their CPU request right that is",
    "start": "1084160",
    "end": "1091419"
  },
  {
    "text": "the way things are supposed to work if you've asked for a request and an optional limit above it you're called",
    "start": "1091419",
    "end": "1097990"
  },
  {
    "text": "first of all and you have a slightly lower quality of service and as long as you're within that band if you hit the",
    "start": "1097990",
    "end": "1104110"
  },
  {
    "text": "limit you're dead or you'll get throttled so we use this again we use the secret mechanisms like shares in",
    "start": "1104110",
    "end": "1110980"
  },
  {
    "text": "quota to implement CPU limits and requests and we use boom and at some",
    "start": "1110980",
    "end": "1116710"
  },
  {
    "text": "point we will include user space evictions where we'll actually run ahead of your process a little bit and do a predictive models as what we do on board",
    "start": "1116710",
    "end": "1122440"
  },
  {
    "text": "we predict that given your current trajectory that you're going to go over a limit so we're going to send you a warning and if you continue to go above",
    "start": "1122440",
    "end": "1129010"
  },
  {
    "text": "then we're gonna lower your protection in the kernel until we're sure that when the kernel pulls the trigger on that shotgun it's you that gets killed",
    "start": "1129010",
    "end": "1137250"
  },
  {
    "start": "1137000",
    "end": "1245000"
  },
  {
    "text": "so again the the the behavior at the limits is really what's interesting so and it depends on the resources so when",
    "start": "1139080",
    "end": "1145929"
  },
  {
    "text": "you run out of CPU if you've set your limit you just stop getting scheduled right so this is a it's a really",
    "start": "1145929",
    "end": "1151870"
  },
  {
    "text": "interesting problem that's not exactly obvious imagine you're running on a 32 core machine you're writing and go",
    "start": "1151870",
    "end": "1157750"
  },
  {
    "text": "because why wouldn't you and go as a highly concurrent language it encourages concurrency so you've written your your",
    "start": "1157750",
    "end": "1165490"
  },
  {
    "text": "sieve your prime number sieve to be highly parallel and you're gonna use up",
    "start": "1165490",
    "end": "1170980"
  },
  {
    "text": "as many courses you can use you've been allocated eight cores you bought eight",
    "start": "1170980",
    "end": "1176650"
  },
  {
    "text": "cores for this job you're running at a 32 core machine cool in the first scheduling pass you can",
    "start": "1176650",
    "end": "1182980"
  },
  {
    "text": "actually have 30 to go routines scheduled all at once they will all consume a quarter of a CPU second and",
    "start": "1182980",
    "end": "1189309"
  },
  {
    "text": "then you are stuck for the remainder of that scheduling window if it's as if it's a one-second window you get nothing",
    "start": "1189309",
    "end": "1196760"
  },
  {
    "text": "because you've used all eight of your CPU seconds in that quarter of a second right this is something that people need",
    "start": "1196760",
    "end": "1202460"
  },
  {
    "text": "to think about as they build applications and as they work with systems like kubernetes all the other CPU s/s were given to somebody else",
    "start": "1202460",
    "end": "1210610"
  },
  {
    "text": "when it comes to things like memory we talked about reclaim we can talk about soft limits when the kernel starts",
    "start": "1210610",
    "end": "1216530"
  },
  {
    "text": "actively trying to reclaim pages from your process and we have a time limit",
    "start": "1216530",
    "end": "1221690"
  },
  {
    "text": "like there's only so long I'm willing to wait for you to free up pages before I end up killing you because somebody else is waiting for that memory",
    "start": "1221690",
    "end": "1228850"
  },
  {
    "text": "in this case we talked about correctness and sort of optimal miss we need to",
    "start": "1228850",
    "end": "1234590"
  },
  {
    "text": "focus on being correct first an optimal second it's easy to forget that and try to optimize things I will take",
    "start": "1234590",
    "end": "1240710"
  },
  {
    "text": "predictable and I will take correct over optimal every every day of the week so",
    "start": "1240710",
    "end": "1245770"
  },
  {
    "start": "1245000",
    "end": "1255000"
  },
  {
    "text": "what happens now when you end up with coupled resources right and we try to back up a truck",
    "start": "1245770",
    "end": "1251410"
  },
  {
    "text": "it's pretty complicated because it's coupled in an interesting way right so in a great example of coupled resources",
    "start": "1251410",
    "end": "1257870"
  },
  {
    "start": "1255000",
    "end": "1373000"
  },
  {
    "text": "is memory my good old friend memory if I try to allocate memory and it fails",
    "start": "1257870",
    "end": "1263630"
  },
  {
    "text": "right but I'm below my Rome I request so I'm supposed to be guaranteed what does the system going to do it's gonna run",
    "start": "1263630",
    "end": "1270080"
  },
  {
    "text": "around the Machine trying to find somebody who's over there request somebody who has clean pages that they",
    "start": "1270080",
    "end": "1275090"
  },
  {
    "text": "can release first thing like code pages you can fault that back in next time you need it right dump those pages out give",
    "start": "1275090",
    "end": "1281300"
  },
  {
    "text": "them to the first guy if I can't do that and first of all that consumes CPU so now I've got a coupled resource here",
    "start": "1281300",
    "end": "1287050"
  },
  {
    "text": "then if I can't find that then I'm gonna take some of your dirty pages I'm gonna write your dirty pages back to disk right and that consumes disk time and",
    "start": "1287050",
    "end": "1294830"
  },
  {
    "text": "CPU and if that isn't sufficient and I have to go and repeat this on another container this takes time right every",
    "start": "1294830",
    "end": "1301640"
  },
  {
    "text": "time you you fire up Firefox or something and you hear the disk launch it's cuz it's thrashing dumping pages",
    "start": "1301640",
    "end": "1308570"
  },
  {
    "text": "back to disk maybe this doesn't matter anymore now we all have SSDs we don't hear anything I'm old",
    "start": "1308570",
    "end": "1315130"
  },
  {
    "text": "so if that's not enough you do it on another container and you do another container until you've found enough",
    "start": "1315130",
    "end": "1321800"
  },
  {
    "text": "memory to give back to this other guy and how long should we allow this to take should it if it is a minute",
    "start": "1321800",
    "end": "1327130"
  },
  {
    "text": "acceptable probably not like if you're writing a program you call malloc and it takes a minute to return you're probably",
    "start": "1327130",
    "end": "1332920"
  },
  {
    "text": "not gonna be real happy right so we have some heuristics that are built around how long this thing should take and it",
    "start": "1332920",
    "end": "1338980"
  },
  {
    "text": "you know it's order milliseconds before we end up saying screw it and I'm killing somebody and that's like so the ultimate big hammer right in reality we",
    "start": "1338980",
    "end": "1346900"
  },
  {
    "text": "should be constantly doing this we should be constant pressure on the kernel to free up unused pages but",
    "start": "1346900",
    "end": "1352210"
  },
  {
    "text": "that's a two-edged sword because you're going to use those pages we don't actually know which pages you do and don't need so one of the things we did",
    "start": "1352210",
    "end": "1358720"
  },
  {
    "text": "inside Google is we built a like a two minute hot list and we say look these are all the pages you've used in the",
    "start": "1358720",
    "end": "1364690"
  },
  {
    "text": "last two minutes and we're gonna try to protect those and everything else fair game right we still run into this",
    "start": "1364690",
    "end": "1370930"
  },
  {
    "text": "problem and we still end up killing people so what happens now if you don't specify",
    "start": "1370930",
    "end": "1376540"
  },
  {
    "start": "1373000",
    "end": "1418000"
  },
  {
    "text": "how much resources you need right just leave it blank it's all Yambol so it's all it's all default able if you don't",
    "start": "1376540",
    "end": "1382870"
  },
  {
    "text": "specify you get 0 what you get is is best-effort isolation no promises to you you might get defaulted values depending",
    "start": "1382870",
    "end": "1390040"
  },
  {
    "text": "on how your clusters been configured or how your namespace has been configured we have thing called limit range which will help default your values for these",
    "start": "1390040",
    "end": "1396010"
  },
  {
    "text": "things you might just get killed randomly you might get CPU starved and not scheduled for five minutes or you",
    "start": "1396010",
    "end": "1403570"
  },
  {
    "text": "might just get no isolation all I all bets are off if you don't specify so if you're running like a MapReduce or",
    "start": "1403570",
    "end": "1410110"
  },
  {
    "text": "something maybe that's okay with you if you're running a production serving job you need to think about what resources",
    "start": "1410110",
    "end": "1416830"
  },
  {
    "text": "you need it's it's it's a really difficult problem to think about how",
    "start": "1416830",
    "end": "1422020"
  },
  {
    "start": "1418000",
    "end": "1423000"
  },
  {
    "text": "much resources you need so how many replicas do I need what is my what is my scaling factor what do i scale on I'm",
    "start": "1422020",
    "end": "1429070"
  },
  {
    "start": "1423000",
    "end": "1499000"
  },
  {
    "text": "having a interactions with customers on this all the time where they think they can sort of wave a magic wand and the",
    "start": "1429070",
    "end": "1435310"
  },
  {
    "text": "auto scalars will just handle stuff right and the auto scalars have very concrete signals on what they use to scale for you and you have to understand",
    "start": "1435310",
    "end": "1442630"
  },
  {
    "text": "to be able to characterize your application or you're just gonna get creamed when the traffic shows up how many replicas is an easy question",
    "start": "1442630",
    "end": "1448900"
  },
  {
    "text": "how much CPU and memory does my job need right these things are changing all the time there's no such thing as how much",
    "start": "1448900",
    "end": "1454780"
  },
  {
    "text": "CPU are you using it's how much CPU are you using over a period X of time right",
    "start": "1454780",
    "end": "1459850"
  },
  {
    "text": "and now you're back into calculus so you have to figure sort of how do i how do i",
    "start": "1459850",
    "end": "1466240"
  },
  {
    "text": "characterize these things for my applications do i provision for the worst case like okay suppose Michael Jackson dies or comes back from the dead",
    "start": "1466240",
    "end": "1472680"
  },
  {
    "text": "what is what's gonna happen to my application all right that's a spike about this high but my average usage is",
    "start": "1472680",
    "end": "1477850"
  },
  {
    "text": "actually down here if you provision for that it's massively wasteful right well what if I provision for the average case",
    "start": "1477850",
    "end": "1483400"
  },
  {
    "text": "down here and then Michael Jackson comes back from the dead well my shits gonna fall over sorry my my jobs are gonna fall over I'm",
    "start": "1483400",
    "end": "1491500"
  },
  {
    "text": "gonna get killed I'm gonna be CPU starved I'm gonna have a bad time right the clear answer of course is write a",
    "start": "1491500",
    "end": "1497350"
  },
  {
    "text": "benchmark right it's easy no benchmarks are incredibly hard right even inside",
    "start": "1497350",
    "end": "1503200"
  },
  {
    "start": "1499000",
    "end": "1506000"
  },
  {
    "text": "Google almost nobody writes benchmarks that work properly accurate benchmarks are damn near",
    "start": "1503200",
    "end": "1509980"
  },
  {
    "start": "1506000",
    "end": "1522000"
  },
  {
    "text": "impossible very few teams even inside Google have the bandwidth to write an accurate benchmark that models a complicated",
    "start": "1509980",
    "end": "1516490"
  },
  {
    "text": "system that lets them predict with any accuracy what they're going to need over time and so so let's talk real quickly",
    "start": "1516490",
    "end": "1524920"
  },
  {
    "start": "1522000",
    "end": "1565000"
  },
  {
    "text": "then about the horizontal scaling so we've got auto scaling it's easy to reason about horizontal scaling how do",
    "start": "1524920",
    "end": "1531340"
  },
  {
    "text": "you adapt to load this way you just add more copies of your job right to go back to the user question of you know what do",
    "start": "1531340",
    "end": "1537640"
  },
  {
    "text": "I just wanna use all the memory but once you think about resources once you draw a box around your application it makes",
    "start": "1537640",
    "end": "1543130"
  },
  {
    "text": "perfect sense then to just take multiple copies of that thing and stamp it out and say well I only need one gig of",
    "start": "1543130",
    "end": "1548530"
  },
  {
    "text": "memory in the average case but now sometimes I need 10 gigs of memory so I",
    "start": "1548530",
    "end": "1553570"
  },
  {
    "text": "can scale it on a granule of one gig at a time and I will get better efficiency right you ever do the calculus where you",
    "start": "1553570",
    "end": "1559720"
  },
  {
    "text": "draw the area under the curve and you get the little boxes the smaller the boxes the more accurate your estimation is right it's the same basic idea so",
    "start": "1559720",
    "end": "1567340"
  },
  {
    "start": "1565000",
    "end": "1595000"
  },
  {
    "text": "then it starts to make sense we'll all understand why I would have multiple replicas but it's not always applicable",
    "start": "1567340",
    "end": "1573310"
  },
  {
    "text": "sometimes there are just things that don't scale horizontally right a classic example that we're experiencing right",
    "start": "1573310",
    "end": "1578680"
  },
  {
    "text": "now with kubernetes is DNS DNS scales with the size of the cluster it's not",
    "start": "1578680",
    "end": "1584200"
  },
  {
    "text": "actually anything that DNS itself is nothing inherent to the traffic on DNS it has a map in memory of all the things",
    "start": "1584200",
    "end": "1591070"
  },
  {
    "text": "in the cluster the bigger the cluster is the more memory you need for DNS shoot so what can we do about this resource",
    "start": "1591070",
    "end": "1598570"
  },
  {
    "start": "1595000",
    "end": "1717000"
  },
  {
    "text": "needs change over time we need an autopilot I need a way to say these are",
    "start": "1598570",
    "end": "1603940"
  },
  {
    "text": "the coordinates of my destination please get me there right I should put a picture of a self-driving car",
    "start": "1603940",
    "end": "1610230"
  },
  {
    "text": "we need to collect stats on what you're using now we need to be able to build a model of what we think you're going to",
    "start": "1610720",
    "end": "1617590"
  },
  {
    "text": "need and we need to be able to predict and we need to be able to react this is where bursts of all actually comes in like it's really cool if you say well I",
    "start": "1617590",
    "end": "1624009"
  },
  {
    "text": "predicted wrong so you went over the baseline limit into the optional space I can then react and say I see that you",
    "start": "1624009",
    "end": "1630460"
  },
  {
    "text": "need more let me react by raising the limit for you now you're back into safe territory this can be used to manage pods and",
    "start": "1630460",
    "end": "1637509"
  },
  {
    "text": "deployments and jobs we can race to stay ahead of the spikes it's still going to be correct in all cases because worst",
    "start": "1637509",
    "end": "1643149"
  },
  {
    "text": "cases you get owned best cases it works fine for you so we actually we have this",
    "start": "1643149",
    "end": "1648460"
  },
  {
    "text": "in Borg it's called autopilot and the vast majority of Borg users something like",
    "start": "1648460",
    "end": "1654070"
  },
  {
    "text": "2/3 of board users use autopilot they put their job into Borg and they say I don't want to know how much resources I",
    "start": "1654070",
    "end": "1659710"
  },
  {
    "text": "have here's that here's an upper-bound don't like build me infinite money and here's my credit card go nuts",
    "start": "1659710",
    "end": "1666990"
  },
  {
    "text": "benchmarks are really hard like in the end it turned out to be more efficient and more effective to write an autopilot",
    "start": "1666990",
    "end": "1673480"
  },
  {
    "text": "system than it was to actually get people to write benchmarks so conveniently the kubernetes api is purpose-built for this use case we've",
    "start": "1673480",
    "end": "1680889"
  },
  {
    "text": "thought about this from the very beginning how do we enable us to build a system this is why pods are unique",
    "start": "1680889",
    "end": "1686200"
  },
  {
    "text": "entities you can actually manage individual pods on individual resources and they can be different from each",
    "start": "1686200",
    "end": "1691419"
  },
  {
    "text": "other or you can manage deployments and you can make them unique across the entire deployment but we need a vertical pod autoscaler as",
    "start": "1691419",
    "end": "1699039"
  },
  {
    "text": "if that wasn't a horizontal pod wasn't a mouthful enough we need a vertical pod autoscaler remember that aspirational",
    "start": "1699039",
    "end": "1705820"
  },
  {
    "text": "part patches welcome so now I want to talk about utilization",
    "start": "1705820",
    "end": "1714090"
  },
  {
    "text": "this is the part I think most people are most familiar with resources they cost you money right big",
    "start": "1714179",
    "end": "1720580"
  },
  {
    "start": "1717000",
    "end": "1742000"
  },
  {
    "text": "memory machines cost big money wasted resources is wasted money so you want to use as much of your",
    "start": "1720580",
    "end": "1726470"
  },
  {
    "text": "capacity as useful but selling it to one of your users is not the same as using it right you can show your CIO or CTO",
    "start": "1726470",
    "end": "1733880"
  },
  {
    "text": "that I've got a hundred percent bookings on my cluster but utilization is still",
    "start": "1733880",
    "end": "1739220"
  },
  {
    "text": "four percent you're still not going to be happy right so real quick show of hands who in this room has utilization",
    "start": "1739220",
    "end": "1747260"
  },
  {
    "start": "1742000",
    "end": "1766000"
  },
  {
    "text": "above say five percent on average just five percent come on hands up alright",
    "start": "1747260",
    "end": "1754370"
  },
  {
    "text": "that's that's pretty good actually how about over ten percent I'm impressed over twenty yeah there we",
    "start": "1754370",
    "end": "1761720"
  },
  {
    "text": "go well I can't say what Google has but I'll say it's more than that so how can",
    "start": "1761720",
    "end": "1768830"
  },
  {
    "start": "1766000",
    "end": "1784000"
  },
  {
    "text": "we do better at this first of all I so I put isolation at the beginning of this list because isolation is the most",
    "start": "1768830",
    "end": "1774560"
  },
  {
    "text": "important part isolation yeah utilization demands isolation if you want to push the limits you have to be",
    "start": "1774560",
    "end": "1780260"
  },
  {
    "text": "safe when you get to the edges you need a fence around that cliff people are inherently cautious even if",
    "start": "1780260",
    "end": "1787340"
  },
  {
    "start": "1784000",
    "end": "1812000"
  },
  {
    "text": "you tell them there's a 20 foot tall fence that somebody else paid for at the edge of the cliff they're still gonna be",
    "start": "1787340",
    "end": "1793910"
  },
  {
    "text": "careful about it and they're still gonna provision for their 90th percentile or their 99th percentile case right it's",
    "start": "1793910",
    "end": "1799040"
  },
  {
    "text": "just inherent in in the way we're built so vertical pod auto-scaling and strong",
    "start": "1799040",
    "end": "1805310"
  },
  {
    "text": "isolation would give us a lot of confidence to work more tightly but we need to do more kernel work to make this",
    "start": "1805310",
    "end": "1810410"
  },
  {
    "text": "even more reliable so let then take away some other lessons from the Borg system that things we've",
    "start": "1810410",
    "end": "1818000"
  },
  {
    "start": "1812000",
    "end": "1964000"
  },
  {
    "text": "built in the past that have worked reasonably well for us we need priority we need something that says this job",
    "start": "1818000",
    "end": "1823700"
  },
  {
    "text": "more important than that job right and to borrow from Animal Farm if",
    "start": "1823700",
    "end": "1828950"
  },
  {
    "text": "everybody's equal then nobody's equal right we need to be able to say the low",
    "start": "1828950",
    "end": "1835670"
  },
  {
    "text": "priority jobs in the case of an emergency they get paused or they get killed right",
    "start": "1835670",
    "end": "1841300"
  },
  {
    "text": "we need to be able to build this into the scheduling system so if this high-protein Lee needs its memory I can",
    "start": "1841300",
    "end": "1847370"
  },
  {
    "text": "look around the machine and say low priority job out of here hyper a job gets your stuff right and when we've",
    "start": "1847370",
    "end": "1853340"
  },
  {
    "text": "built it inside Google is we have a chargeback model and the high priority jobs they get charged prime rate and the",
    "start": "1853340",
    "end": "1859589"
  },
  {
    "text": "low priority job is get charged next to nothing and most people look at that and they say you know the kill rate for the",
    "start": "1859589",
    "end": "1866369"
  },
  {
    "text": "low party jobs in practice is pretty good I'm just gonna use that because my bill is 1/10 as much right and this",
    "start": "1866369",
    "end": "1872039"
  },
  {
    "text": "search and the Gmail and the ads they buy the the prime USDA grade a beef because they need it",
    "start": "1872039",
    "end": "1879229"
  },
  {
    "text": "now once you've got this this distinction between sort of tiers of work you can actually over commit your",
    "start": "1879229",
    "end": "1884549"
  },
  {
    "text": "systems remember I said we don't over commit totally lied we don't over commit at a given tier",
    "start": "1884549",
    "end": "1890940"
  },
  {
    "text": "given quality of service if you have low priority work if you you you're hyper to",
    "start": "1890940",
    "end": "1895950"
  },
  {
    "text": "job and you've bought 8 gigs of memory and you're currently using three and a half I've got four and a half gigs and",
    "start": "1895950",
    "end": "1901409"
  },
  {
    "text": "memory that I can resell to somebody else right if you pay a tenth as much as he did for that other four gigs of",
    "start": "1901409",
    "end": "1907320"
  },
  {
    "text": "memory I can now drive my utilization way up knowing that I put all those safeguards in place that should he need",
    "start": "1907320",
    "end": "1913709"
  },
  {
    "text": "his memory you're out of there you're ok with that because you got a lower SLA for the lower price you're ok with that",
    "start": "1913709",
    "end": "1918959"
  },
  {
    "text": "because I gave you a higher SLA for the higher price late a memory allocation might not be a ten nanoseconds might be",
    "start": "1918959",
    "end": "1925919"
  },
  {
    "text": "one millisecond but on average you're gonna get really solid performance question",
    "start": "1925919",
    "end": "1933320"
  },
  {
    "text": "we do by fiddling with things like soft limits and hard limits within the cgroups",
    "start": "1940219",
    "end": "1946009"
  },
  {
    "text": "I'm not gonna run it I'm not gonna have enough time to talk about sort of the Google theory of layering of containers",
    "start": "1946009",
    "end": "1951869"
  },
  {
    "text": "I'm happy to talk at length about it maybe it's another talk but yes we go through a lot of sort of",
    "start": "1951869",
    "end": "1958469"
  },
  {
    "text": "soft real-time manipulation of the underlying resources to make sure that the higher party jobs are protected more",
    "start": "1958469",
    "end": "1964759"
  },
  {
    "start": "1964000",
    "end": "2033000"
  },
  {
    "text": "so now we build this overcommit model I just I just ruin my slide we use a lot of stats in a lot of",
    "start": "1964759",
    "end": "1971700"
  },
  {
    "text": "history to figure out what's safe and then we essentially have a knob that you can turn that says how aggressive do you",
    "start": "1971700",
    "end": "1977129"
  },
  {
    "text": "want to be with your over commitment how much utilization do you want the more utilization you get the more churn you're gonna get in your cluster sort of",
    "start": "1977129",
    "end": "1983190"
  },
  {
    "text": "the more volatile the market becomes and then we just let the priority system and",
    "start": "1983190",
    "end": "1988229"
  },
  {
    "text": "the reschedule or deal with the fallout right things are gonna get killed and moved in shuffled around and that's fine",
    "start": "1988229",
    "end": "1993330"
  },
  {
    "text": "we've already built the system for this right in practice it works really really well I'll tell you one of the hardest",
    "start": "1993330",
    "end": "1999150"
  },
  {
    "text": "things I did too when I came to Google I came in as a sort of junior ish software",
    "start": "1999150",
    "end": "2004490"
  },
  {
    "text": "engineer where in my mind things either work or they don't right and it's a really attractive model but it's totally",
    "start": "2004490",
    "end": "2010160"
  },
  {
    "text": "wrong in reality when you start talking about running clusters of thousands of machines you talk about the percentage",
    "start": "2010160",
    "end": "2015920"
  },
  {
    "text": "of times that things work and you talk about distributions and percentiles and this I can't guarantee that this will",
    "start": "2015920",
    "end": "2022160"
  },
  {
    "text": "work right most of the time in practice it works great right and we set the knob",
    "start": "2022160",
    "end": "2027800"
  },
  {
    "text": "at a place where we're happy with the trade-off between the volatility and the utilization",
    "start": "2027800",
    "end": "2033250"
  },
  {
    "start": "2033000",
    "end": "2054000"
  },
  {
    "text": "so there's this this very attractive nuisance which is I want to pack my cluster as full as I can get and this",
    "start": "2033250",
    "end": "2039830"
  },
  {
    "text": "comes out of real customer experiences right customer had an outage where they had packed their cluster to ninety-nine",
    "start": "2039830",
    "end": "2045080"
  },
  {
    "text": "point something percent containers on every machine and one of their machines died and it happened to",
    "start": "2045080",
    "end": "2052550"
  },
  {
    "text": "be the machine that was running their DNS server oh damn",
    "start": "2052550",
    "end": "2057760"
  },
  {
    "start": "2054000",
    "end": "2077000"
  },
  {
    "text": "everything went sideways their DNS couldn't get rescheduled because it went pending remember that pending slide the",
    "start": "2057760",
    "end": "2063950"
  },
  {
    "text": "cluster went went south because everything depends on DNS at this point and and they were really unhappy and",
    "start": "2063950",
    "end": "2069649"
  },
  {
    "text": "they called us and they yelled at us so you have to you have to think about this",
    "start": "2069650",
    "end": "2075889"
  },
  {
    "text": "right nodes fail the reality check nodes fail all the time nodes get upgraded I mean upgrades in",
    "start": "2075890",
    "end": "2082190"
  },
  {
    "start": "2077000",
    "end": "2153000"
  },
  {
    "text": "general involve taking some amount of unavailability on some percentage of your cluster and you have to be able to",
    "start": "2082190",
    "end": "2088760"
  },
  {
    "text": "handle this think about a tube of toothpaste as my analogy here if you've got a brand new full tube of toothpaste",
    "start": "2088760",
    "end": "2093889"
  },
  {
    "text": "right you grab ahold of it you squeeze it as hard as you can and nothing happens right except when you get the",
    "start": "2093890",
    "end": "2098930"
  },
  {
    "text": "pop top and it blows the top and it squirts all over your counter and and your wife gets mad at you but",
    "start": "2098930",
    "end": "2104980"
  },
  {
    "text": "if you if you squeeze out some of that toothpaste and you leave a little bit of room in that toothpaste and you squeeze",
    "start": "2104980",
    "end": "2110960"
  },
  {
    "text": "it all your toothpaste shuffles over here and you can then fix the nodes over at this end of the toothpaste to mix",
    "start": "2110960",
    "end": "2116750"
  },
  {
    "text": "analogies and you squeeze the toothpaste back and you can fix all the nodes over at this side of the toothpaste tube and",
    "start": "2116750",
    "end": "2121790"
  },
  {
    "text": "everybody's happy this shows up in your utilization though right do you leave 5% of your cluster 1% of your cluster",
    "start": "2121790",
    "end": "2127940"
  },
  {
    "text": "you've got to decide right and it comes down to an organizational thing of how much waste are you willing to have in",
    "start": "2127940",
    "end": "2135319"
  },
  {
    "text": "your cluster for the safety right Google has numbers but they're specific the way we do our machine repairs and the way we",
    "start": "2135319",
    "end": "2140869"
  },
  {
    "text": "do our scheduling in these sorts of things the main point here is please do not think about your cluster as being",
    "start": "2140869",
    "end": "2146990"
  },
  {
    "text": "good at a hundred percent because it's not right you are standing on a time bomb if you set your cluster up 200",
    "start": "2146990",
    "end": "2152390"
  },
  {
    "text": "percent so I just got totally out the 5-minute window so I'm gonna wrap it up Hey look",
    "start": "2152390",
    "end": "2158329"
  },
  {
    "start": "2153000",
    "end": "2159000"
  },
  {
    "text": "at that I hit 70 slides reminder some of this was aspirational a",
    "start": "2158329",
    "end": "2163369"
  },
  {
    "start": "2159000",
    "end": "2178000"
  },
  {
    "text": "lot of what I'm talking about here is not implemented yet but this is where we want to go with kubernetes and I really",
    "start": "2163369",
    "end": "2168470"
  },
  {
    "text": "I thought it was important to bring it out and talk about these things because I think people who are having a rough",
    "start": "2168470",
    "end": "2174170"
  },
  {
    "text": "time using these new cloud native techniques are running afoul of some of these assumptions we still have a long",
    "start": "2174170",
    "end": "2180259"
  },
  {
    "start": "2178000",
    "end": "2186000"
  },
  {
    "text": "way to go we have a lot of work to do 800 men years maybe just became 1,600",
    "start": "2180259",
    "end": "2186069"
  },
  {
    "start": "2186000",
    "end": "2239000"
  },
  {
    "text": "patches are welcome kubernetes is an open project it's an open community I love to wrap with this slide because",
    "start": "2186069",
    "end": "2192170"
  },
  {
    "text": "this is it like the reason that communities is is successful as the",
    "start": "2192170",
    "end": "2197329"
  },
  {
    "text": "people who are here in this room the people who use it that people who contribute to it this is a chance for you to come and say what you need out of",
    "start": "2197329",
    "end": "2203750"
  },
  {
    "text": "the system so with that I'll turn it over to questions",
    "start": "2203750",
    "end": "2208900"
  },
  {
    "text": "perfect do you see priority being a separate",
    "start": "2211390",
    "end": "2218509"
  },
  {
    "text": "thing from the existing QoS tiers end is the are all members of the powerset",
    "start": "2218509",
    "end": "2224410"
  },
  {
    "text": "valid or how's it gonna work I don't know I'm actually not this is not my",
    "start": "2224410",
    "end": "2231140"
  },
  {
    "text": "area of expertise when it comes to scheduling and David in the back there we need something around priority we",
    "start": "2231140",
    "end": "2236779"
  },
  {
    "text": "already have actually a sort of a very binary form of priority that we've implemented just to protect critical",
    "start": "2236779",
    "end": "2242450"
  },
  {
    "start": "2239000",
    "end": "2589000"
  },
  {
    "text": "cluster services like DNS as this is a very real concern so we have a binary",
    "start": "2242450",
    "end": "2247789"
  },
  {
    "text": "flag this is a critical thing and the critical things will be able to sort of take over and preempt other things it is",
    "start": "2247789",
    "end": "2255109"
  },
  {
    "text": "not a generally useable user facing API yet so we need to work through that and figure that out part of what we bring to",
    "start": "2255109",
    "end": "2261710"
  },
  {
    "text": "communities is the lessons from Borg and some of those lessons are how not to do things and so I don't think we want to",
    "start": "2261710",
    "end": "2267980"
  },
  {
    "text": "build something as complicated as we built the Borg for priority but we need something here you see you had mentioned some things",
    "start": "2267980",
    "end": "2274940"
  },
  {
    "text": "about applications and designing them to be aware of some of these constraints and things are there any guidelines for",
    "start": "2274940",
    "end": "2280670"
  },
  {
    "text": "designing applications to work in kubernetes or is that kind of an open area right now are there questions for",
    "start": "2280670",
    "end": "2286340"
  },
  {
    "text": "designing applications we try not to I don't want to just to",
    "start": "2286340",
    "end": "2291740"
  },
  {
    "text": "prescribe how people should write applications right I don't want people to write applications for kubernetes I",
    "start": "2291740",
    "end": "2296780"
  },
  {
    "text": "think that would be a bad thing for the ecosystem overall but I think the problems that we're talking about here",
    "start": "2296780",
    "end": "2302120"
  },
  {
    "text": "are not unique to kubernetes they're they're incumbent in any sort of scheduling system whether that's meso",
    "start": "2302120",
    "end": "2307670"
  },
  {
    "text": "source warm or kubernetes are nomads or something else I mean you have the same problem with VMs right at some level you",
    "start": "2307670",
    "end": "2313460"
  },
  {
    "text": "have to right-size your VMs though that's a lot less granular generally and so you have a lot less choices and so",
    "start": "2313460",
    "end": "2319280"
  },
  {
    "text": "you pick something that's sort of the next largest unit than what you think you're gonna need people people do think about it in that",
    "start": "2319280",
    "end": "2326300"
  },
  {
    "text": "sense we're giving people a much more granular way to think about things and I think that we need to solve that in",
    "start": "2326300",
    "end": "2332240"
  },
  {
    "text": "general I don't think it's specific to kubernetes and no I don't have any hard guidelines for you",
    "start": "2332240",
    "end": "2338200"
  },
  {
    "text": "question just yell I'll repeat it [Music]",
    "start": "2338200",
    "end": "2345280"
  },
  {
    "text": "is the scheduler going to take into account real-time resource consumption in Borg we call that reservation which",
    "start": "2345730",
    "end": "2353030"
  },
  {
    "text": "is a sort of modeled recent usage and prediction and we use that to inform how",
    "start": "2353030",
    "end": "2358670"
  },
  {
    "text": "much we overcommit also I don't know that we have any immediate plans to do",
    "start": "2358670",
    "end": "2364160"
  },
  {
    "text": "the exact same thing but certainly the topic comes up and I think that given the success of it within board we'll see",
    "start": "2364160",
    "end": "2369890"
  },
  {
    "text": "something like that",
    "start": "2369890",
    "end": "2372640"
  },
  {
    "text": "so the question is are any of these aspirational features in Borg most of them are most of what I'm talking about",
    "start": "2380760",
    "end": "2385780"
  },
  {
    "text": "here comes directly from what we've done in Borg some of it is simplified and some of it",
    "start": "2385780",
    "end": "2391000"
  },
  {
    "text": "is tweaked a little bit would we consider porting it porting is a bad choice of words",
    "start": "2391000",
    "end": "2397450"
  },
  {
    "text": "because Borg is 300 million lines of C++ and kubernetes is not and so there's",
    "start": "2397450",
    "end": "2404590"
  },
  {
    "text": "really no port there's a reinvent and yes so I've already got I've got people",
    "start": "2404590",
    "end": "2410080"
  },
  {
    "text": "inside Google who are interested in working on some of these things if I can shake them loose from their current projects",
    "start": "2410080",
    "end": "2415770"
  },
  {
    "text": "but I don't want the craze cannot continue to be a Google project Google Google centric project right and we've",
    "start": "2415770",
    "end": "2422020"
  },
  {
    "text": "already haven't if you saw the keynote yesterday Ken's slide like where Google is already less than 50 percent of the",
    "start": "2422020",
    "end": "2427270"
  },
  {
    "text": "net contributions to kubernetes which is amazing so it needs to be a community driven",
    "start": "2427270",
    "end": "2432400"
  },
  {
    "text": "effort the things that we've done inside Google do not always apply outside of Google we have very interesting",
    "start": "2432400",
    "end": "2438310"
  },
  {
    "text": "constraints and actually degrees of freedom that people outside Google don't have I can call up all the developers in",
    "start": "2438310",
    "end": "2443800"
  },
  {
    "text": "Google and say thou shalt link this library at this version or you shall not run and I can't so much do that with",
    "start": "2443800",
    "end": "2450100"
  },
  {
    "text": "like Apache I got two minutes left questions",
    "start": "2450100",
    "end": "2455430"
  },
  {
    "text": "using node pools to partition a cluster so node pools is a container engine centric construct but the idea is that",
    "start": "2462540",
    "end": "2469900"
  },
  {
    "text": "you have a set of different templates for different sizes of nodes meaning that you end up with a heterogeneous",
    "start": "2469900",
    "end": "2475780"
  },
  {
    "text": "cluster heterogeneous cluster is a fact of life I mean we deal with it in board because we have eight generations of",
    "start": "2475780",
    "end": "2482230"
  },
  {
    "text": "hardware that we support in a given cluster it's an interesting way to",
    "start": "2482230",
    "end": "2487290"
  },
  {
    "text": "fit the cluster to your problem space I think I I don't like it as a way of",
    "start": "2487290",
    "end": "2494080"
  },
  {
    "text": "partitioning capabilities I like to think of the cluster as sort of a homogenous scheduling zone where the",
    "start": "2494080",
    "end": "2500890"
  },
  {
    "text": "actual resources offered by each machine might be different so if you have a 32",
    "start": "2500890",
    "end": "2505960"
  },
  {
    "text": "core machine and you have a four core mission you have some room we should actually save the 32 core machine for",
    "start": "2505960",
    "end": "2511390"
  },
  {
    "text": "when that 32 core request comes along and not burn all a bunch of four core of jobs on those 32 core machines I think",
    "start": "2511390",
    "end": "2518380"
  },
  {
    "text": "that we have work to do sort of in that in that vein but I don't I don't want to encourage",
    "start": "2518380",
    "end": "2525099"
  },
  {
    "text": "people to say well this is my data specific pool and this is my compute specific pool because I think that",
    "start": "2525099",
    "end": "2531160"
  },
  {
    "text": "you're going to end up with lower utilization that said far be it for me to say who's right or wrong and using this if it works for you then it's right",
    "start": "2531160",
    "end": "2537540"
  },
  {
    "text": "but I think that the model that we want to push people towards is more unified higher can I just I you may have",
    "start": "2537540",
    "end": "2546369"
  },
  {
    "text": "mentioned this already I came in late but for people who are interested in these topics there's a scheduling sig sig scheduling and also a sig",
    "start": "2546369",
    "end": "2552940"
  },
  {
    "text": "auto-scaling so those are kind of the two homes for the topics that Tim talked",
    "start": "2552940",
    "end": "2558760"
  },
  {
    "text": "about for people who are interested so the funny thing is this is not actually my area of expertise at all but because",
    "start": "2558760",
    "end": "2565869"
  },
  {
    "text": "I ended up talking to a lot of customers and and I think it's interesting I",
    "start": "2565869",
    "end": "2570880"
  },
  {
    "text": "thought it would be a good topic to talk about today David who just asked the or",
    "start": "2570880",
    "end": "2576099"
  },
  {
    "text": "just pointed that out is is actually a scheduling expert I just pretend to be one and so I think we'd be happy to take",
    "start": "2576099",
    "end": "2583030"
  },
  {
    "text": "more questions and we kicked out of the room because there's another talk but we'll do the usual thing of hanging out",
    "start": "2583030",
    "end": "2588400"
  },
  {
    "text": "outside and talking",
    "start": "2588400",
    "end": "2591180"
  }
]