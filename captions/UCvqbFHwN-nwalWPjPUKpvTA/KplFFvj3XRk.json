[
  {
    "text": "[Music] let's chat hi everyone my name is Robert Agarwal I'm a software engineer at Google I work",
    "start": "460",
    "end": "6899"
  },
  {
    "text": "in the Cuba native Engine team which is part of Google cloud and today I'm going to talk about using GPUs with kubernetes",
    "start": "6899",
    "end": "14030"
  },
  {
    "text": "before we begin a quick show of hands how many of you are already using GPUs",
    "start": "14030",
    "end": "19230"
  },
  {
    "text": "on Cuban IDs awesome and how many of you are using GPUs that are not NVIDIA GPUs",
    "start": "19230",
    "end": "28130"
  },
  {
    "text": "this may be a one single person okay yeah okay",
    "start": "28130",
    "end": "34079"
  },
  {
    "text": "so in this talk I'm not going to talk about why to use GPUs or when to use",
    "start": "34079",
    "end": "39450"
  },
  {
    "text": "GPUs I'm only going to focus on how to use GPUs and communities I'll start by",
    "start": "39450",
    "end": "45539"
  },
  {
    "text": "talking about what makes it hard to use GPUs from containers I'll go into a bit",
    "start": "45539",
    "end": "51870"
  },
  {
    "text": "of history about the GPU support in communities and then I'll talk about",
    "start": "51870",
    "end": "57030"
  },
  {
    "text": "what you need to do as a user and as an operator to get cheap use working on communities and finally I'll end with",
    "start": "57030",
    "end": "64400"
  },
  {
    "text": "describing what's missing and where the support is going next okay so containers",
    "start": "64400",
    "end": "73380"
  },
  {
    "text": "and GPUs other things which make containers great and haven't made them so popular is that you can package your",
    "start": "73380",
    "end": "80250"
  },
  {
    "text": "application and all of its dependencies in a container image and then you don't",
    "start": "80250",
    "end": "85650"
  },
  {
    "text": "have to worry about missing dependencies or conflicts with other applications you",
    "start": "85650",
    "end": "91710"
  },
  {
    "text": "can run that container image anywhere however this property breaks down when",
    "start": "91710",
    "end": "97680"
  },
  {
    "text": "one of your application dependencies is a kernel module containers use the host",
    "start": "97680",
    "end": "103380"
  },
  {
    "text": "kernel so if one of your application dependencies is a kernel module then the",
    "start": "103380",
    "end": "108570"
  },
  {
    "text": "host on which your application container is supposed to run needs to have that kernel module installed and applications",
    "start": "108570",
    "end": "116490"
  },
  {
    "text": "using nvidia gpus are examples of such applications using nvidia gpus require",
    "start": "116490",
    "end": "123950"
  },
  {
    "text": "the nvidia kernel module to be installed on the host using nvidia gpus also",
    "start": "123950",
    "end": "131280"
  },
  {
    "text": "require the press of Nvidia's shared user level libraries",
    "start": "131280",
    "end": "137290"
  },
  {
    "text": "to be accessible from inside the container these libraries are used to",
    "start": "137290",
    "end": "142999"
  },
  {
    "text": "communicate with the kernel module and therefore the GP devices so you need the",
    "start": "142999",
    "end": "149239"
  },
  {
    "text": "libraries you need the kernel module on the host and you need the libraries to be accessible from with within inside",
    "start": "149239",
    "end": "155780"
  },
  {
    "text": "the container container so maybe you can package the libraries in the container",
    "start": "155780",
    "end": "162680"
  },
  {
    "text": "image and assume that the kernel module will be present on the host accept the",
    "start": "162680",
    "end": "170540"
  },
  {
    "text": "version of the Nvidia shared libraries needs to be the same as the version of",
    "start": "170540",
    "end": "176389"
  },
  {
    "text": "the Nvidia kernel module so if you package these libraries in your container image then your container",
    "start": "176389",
    "end": "183319"
  },
  {
    "text": "image is no longer portable because it depends on the version of the kernel module on the host so if you package it",
    "start": "183319",
    "end": "190340"
  },
  {
    "text": "for a particular version it will only run on hosts which have the kernel module of that version right",
    "start": "190340",
    "end": "196549"
  },
  {
    "text": "so this is not good you don't want to make your container images non portable because that defeats one of the main",
    "start": "196549",
    "end": "202939"
  },
  {
    "text": "advantages of containers so in communities the first thing we decided",
    "start": "202939",
    "end": "209959"
  },
  {
    "text": "to do to support GPUs was very simple we decided to let the user deal with with",
    "start": "209959",
    "end": "215810"
  },
  {
    "text": "these dependencies all communities would do is that it would assume that the",
    "start": "215810",
    "end": "221299"
  },
  {
    "text": "Nvidia driver is already present on the host and then it would see the GP",
    "start": "221299",
    "end": "226849"
  },
  {
    "text": "devices present and host and expose them as scheduled level alpha dot communities",
    "start": "226849",
    "end": "232040"
  },
  {
    "text": "retire slash NVIDIA GPU resources and when a container asks for this resource",
    "start": "232040",
    "end": "238489"
  },
  {
    "text": "in its resource requirement then communities would add these devices to the container",
    "start": "238489",
    "end": "244389"
  },
  {
    "text": "so this is all it would do it would keep track of the GP devices on the host and attach them to the container when",
    "start": "244389",
    "end": "251299"
  },
  {
    "text": "requested so now you have devices",
    "start": "251299",
    "end": "256430"
  },
  {
    "text": "attached to the container how do you access the device from inside the container so either side before to",
    "start": "256430",
    "end": "263389"
  },
  {
    "text": "access the device you need those user level libraries but also if have those user level libraries than you",
    "start": "263389",
    "end": "270200"
  },
  {
    "text": "container image is not portable so we've recommended the users to install both",
    "start": "270200",
    "end": "276080"
  },
  {
    "text": "the kernel module and the shared libraries on the host and then use host",
    "start": "276080",
    "end": "281870"
  },
  {
    "text": "path volumes to inject these libraries inside the container so install with",
    "start": "281870",
    "end": "287750"
  },
  {
    "text": "them and the hosts create a hotspot valium inject that volume into the container so this is how it looked like",
    "start": "287750",
    "end": "296350"
  },
  {
    "text": "in blue you see that this container is this part of this container is",
    "start": "296350",
    "end": "301729"
  },
  {
    "text": "requesting two GPUs and this is the same place where you'd add CPU and memory at",
    "start": "301729",
    "end": "307010"
  },
  {
    "text": "the top right and red color you see that we are creating a host path volume",
    "start": "307010",
    "end": "312590"
  },
  {
    "text": "called Nvidia libraries and the path there is the path and the host where",
    "start": "312590",
    "end": "318080"
  },
  {
    "text": "these shared libraries are present and at the bottom in red you see that we are",
    "start": "318080",
    "end": "323570"
  },
  {
    "text": "adding this host path volume called Nvidia libraries inside the container at the mount path and this path in the",
    "start": "323570",
    "end": "331910"
  },
  {
    "text": "container needs to be a path which is in the shared library search path so after",
    "start": "331910",
    "end": "338390"
  },
  {
    "text": "you do this your container image will be portable but and things will work but",
    "start": "338390",
    "end": "344600"
  },
  {
    "text": "this this is still terrible because your u container images portable but your pod",
    "start": "344600",
    "end": "349880"
  },
  {
    "text": "spec is no longer portable your pod spec contains some for specific stuff and we",
    "start": "349880",
    "end": "355669"
  },
  {
    "text": "don't want the pod spec to be not portable so there were some other things",
    "start": "355669",
    "end": "363169"
  },
  {
    "text": "which were undesirable about this approach so this this support for NVIDIA",
    "start": "363169",
    "end": "369440"
  },
  {
    "text": "GPUs was in tree it was part of Cuban Indies core code base and that's not",
    "start": "369440",
    "end": "376580"
  },
  {
    "text": "ideal because what happens when other vendors want support for their devices",
    "start": "376580",
    "end": "382700"
  },
  {
    "text": "in communities right we cannot continue expanding the community's codebase to",
    "start": "382700",
    "end": "388370"
  },
  {
    "text": "support all these things right we cannot add an MD GPUs Intel GPU Xilinx FPGA and",
    "start": "388370",
    "end": "393530"
  },
  {
    "text": "so on we at the same time we also cannot play favorites to a particular vendor because communities are the open source",
    "start": "393530",
    "end": "399770"
  },
  {
    "text": "software and like supposed to work well with everything as a result of these things we deprecated",
    "start": "399770",
    "end": "407060"
  },
  {
    "text": "the entry support in 1.10 and in 1.11 we",
    "start": "407060",
    "end": "412670"
  },
  {
    "text": "are completely removing the entry support so this resource will stop working in 1.11 completely to replace",
    "start": "412670",
    "end": "420470"
  },
  {
    "text": "this entry support we added device plugins in humanity's device plugins are",
    "start": "420470",
    "end": "427160"
  },
  {
    "text": "aware to support generic devices in communities it's an extension mechanism",
    "start": "427160",
    "end": "432770"
  },
  {
    "text": "which allows the vendor specific code to remain outside the core community street",
    "start": "432770",
    "end": "438560"
  },
  {
    "text": "but still allows these devices to be available natively through the",
    "start": "438560",
    "end": "443900"
  },
  {
    "text": "communities API and they also enable the",
    "start": "443900",
    "end": "449450"
  },
  {
    "text": "pot spec to be portable the pots if you use device plugins the pod spec will not contain any host specific parts which we",
    "start": "449450",
    "end": "457250"
  },
  {
    "text": "had in the previous part spec so this is how it looks like now in in blue you see",
    "start": "457250",
    "end": "465370"
  },
  {
    "text": "nvidia GPU so all you need to do now is add the resource exposed by the device",
    "start": "465370",
    "end": "472670"
  },
  {
    "text": "plugin in your resource requirements for nvidia gpus that's Nvidia dot-com / GPU",
    "start": "472670",
    "end": "477830"
  },
  {
    "text": "and once you do that everything will work for you you'll notice that there's",
    "start": "477830",
    "end": "482870"
  },
  {
    "text": "no specific part and this in this pot spec right but now like there is the",
    "start": "482870",
    "end": "490880"
  },
  {
    "text": "magic like there's no hood specific part in the pot spec how is the container getting access to the shared libraries",
    "start": "490880",
    "end": "497300"
  },
  {
    "text": "present on the host and the magic is in the device plugin api's device plug-in",
    "start": "497300",
    "end": "504080"
  },
  {
    "text": "API is allows the device plug-in to set environment variables inside the",
    "start": "504080",
    "end": "509150"
  },
  {
    "text": "container or mount volumes inside the container so for example the cluster",
    "start": "509150",
    "end": "514400"
  },
  {
    "text": "administrator could configure the device plugin to point to the part on the host",
    "start": "514400",
    "end": "520099"
  },
  {
    "text": "where the shared libraries are present and when the device plugins sees a pod requesting GPUs it will add that volume",
    "start": "520100",
    "end": "528820"
  },
  {
    "text": "to to the container so that the container can access this right so with",
    "start": "528820",
    "end": "534230"
  },
  {
    "text": "this you have your container image which is portable because you don't have user level shared libraries in the content in",
    "start": "534230",
    "end": "541820"
  },
  {
    "text": "the container image and your port spec is also portable because you and your",
    "start": "541820",
    "end": "553900"
  },
  {
    "text": "pod spec is also portable because you don't have any host specific stuff in",
    "start": "553900",
    "end": "559940"
  },
  {
    "text": "the Pacific all the host specific stuff is part of the device plugin which is",
    "start": "559940",
    "end": "566900"
  },
  {
    "text": "which is configured by the cluster administrator who know about the host so",
    "start": "566900",
    "end": "573290"
  },
  {
    "text": "device begins were introduced as an alpha feature in 1.8 they went beta in",
    "start": "573290",
    "end": "579100"
  },
  {
    "text": "1.10 and if you using GPUs the Kuban it is you should start using device plugins",
    "start": "579100",
    "end": "585700"
  },
  {
    "text": "so just to recap what do you need to do as a user at the user you should build",
    "start": "585700",
    "end": "592310"
  },
  {
    "text": "your images without the user level shared Nvidia libraries like Lib Nvidia and l dot s o and so on they should not",
    "start": "592310",
    "end": "599540"
  },
  {
    "text": "be part of your container image however the container image should still contain the cuda toolkit so the cuda toolkit",
    "start": "599540",
    "end": "607870"
  },
  {
    "text": "does have some dependence on the host driver version so for example each cuda",
    "start": "607870",
    "end": "613460"
  },
  {
    "text": "version requires a minimum driver version but this dependence is not as",
    "start": "613460",
    "end": "618650"
  },
  {
    "text": "onerous as the one with shared libraries where you have to match with the exact",
    "start": "618650",
    "end": "623810"
  },
  {
    "text": "kernel version so you build your container image with the cuda toolkit",
    "start": "623810",
    "end": "629090"
  },
  {
    "text": "but without the user level shared libraries and then all you do is request nvidia calm slash GPU in as part of your",
    "start": "629090",
    "end": "637100"
  },
  {
    "text": "resource requirements and if the cluster administrator has set up the cluster correctly then everything should work",
    "start": "637100",
    "end": "644000"
  },
  {
    "text": "for you at the user this may be one more",
    "start": "644000",
    "end": "649670"
  },
  {
    "text": "thing you may have to do at the user so let's say your cluster has multiple",
    "start": "649670",
    "end": "654890"
  },
  {
    "text": "types of GPU nodes so for example some GPU nodes have Tesla P 100 and some GPU",
    "start": "654890",
    "end": "661340"
  },
  {
    "text": "nodes have Tesla V 100 so there is a and",
    "start": "661340",
    "end": "666590"
  },
  {
    "text": "you want your application a particular type of GPU so you you want to say that I want my application to run",
    "start": "666590",
    "end": "673610"
  },
  {
    "text": "on nodes which have V 100 so this",
    "start": "673610",
    "end": "678890"
  },
  {
    "text": "unfortunately no native way of doing this targeting in Cuban it is",
    "start": "678890",
    "end": "684970"
  },
  {
    "text": "communities does not natively understand different types of GPUs it only understands that there is you're",
    "start": "684970",
    "end": "691040"
  },
  {
    "text": "requesting a GPU doesn't understand that you're requesting of V 100 GPS etc and",
    "start": "691040",
    "end": "696440"
  },
  {
    "text": "the workaround what you can do is you can ask your cluster administrator to label GP nodes with the type of GPUs",
    "start": "696440",
    "end": "703640"
  },
  {
    "text": "that a present on the net node so you can ask your operator to label the P 100",
    "start": "703640",
    "end": "709100"
  },
  {
    "text": "nodes so P 100 V owner or even read and so on so this is for example what we do",
    "start": "709100",
    "end": "714680"
  },
  {
    "text": "in GK where you can say that I want this part to run on a node which has Tesla",
    "start": "714680",
    "end": "721100"
  },
  {
    "text": "keys and then this part will schedule on a node which has just like eighties okay",
    "start": "721100",
    "end": "726980"
  },
  {
    "text": "so we learned about what we need to do as a user what do you need to do as a",
    "start": "726980",
    "end": "733790"
  },
  {
    "text": "cluster operator to create a functioning GPU cluster so first of all you need to",
    "start": "733790",
    "end": "741590"
  },
  {
    "text": "have notes for GPUs these things are very expensive and as I said before if",
    "start": "741590",
    "end": "747650"
  },
  {
    "text": "you have multiple types of GPU nodes you you should label them so that your users",
    "start": "747650",
    "end": "753500"
  },
  {
    "text": "can target particular types of GPUs then",
    "start": "753500",
    "end": "759800"
  },
  {
    "text": "and this is the one of the most important things need to install the Nvidia driver and I can talk about just",
    "start": "759800",
    "end": "766760"
  },
  {
    "text": "this point alone for a long time unfortunately but I will just keep it",
    "start": "766760",
    "end": "773030"
  },
  {
    "text": "short here so few things which we need to keep in mind about this parts of the",
    "start": "773030",
    "end": "778340"
  },
  {
    "text": "Nvidia driver are closed source and Linux is GPL license so keep that in mind and the other thing you would want",
    "start": "778340",
    "end": "785960"
  },
  {
    "text": "to do is you'd want to keep up with the driver version required by the latest",
    "start": "785960",
    "end": "791150"
  },
  {
    "text": "cuda release so as I said before CUDA toolkit has some dependence on the",
    "start": "791150",
    "end": "796970"
  },
  {
    "text": "driver version so you'd want that if CUDA releases a new version",
    "start": "796970",
    "end": "802070"
  },
  {
    "text": "that you update to a driver which is required by that version and if you like",
    "start": "802070",
    "end": "807080"
  },
  {
    "text": "because what would happen eventually is that your users who start using newer CUDA versions and if you have not",
    "start": "807080",
    "end": "813500"
  },
  {
    "text": "updated the drivers on your nodes those containers will stop running because good a 9.2 requires I don't know 395 got",
    "start": "813500",
    "end": "822200"
  },
  {
    "text": "something driver which your nodes to under in GK we use a daemon set to",
    "start": "822200",
    "end": "828080"
  },
  {
    "text": "install the drivers you could you can look at that and like folk that and do your own thing you'll also need to",
    "start": "828080",
    "end": "836420"
  },
  {
    "text": "install the device plugins there is a device plug-in for NVIDIA GPUs available",
    "start": "836420",
    "end": "842330"
  },
  {
    "text": "from Nvidia there's one available from Google you can choose whichever one fits",
    "start": "842330",
    "end": "849050"
  },
  {
    "text": "your needs I am hopeful that in the future we will converge them so that you get the best of both worlds but right",
    "start": "849050",
    "end": "856130"
  },
  {
    "text": "now there are two different device plugins and you can use whichever one you want okay so now you have a cluster",
    "start": "856130",
    "end": "863330"
  },
  {
    "text": "with GPUs running what else you may want to do other things you may want to do is you want to set a resource quotas per",
    "start": "863330",
    "end": "870620"
  },
  {
    "text": "namespace you want to say that my marketing namespace or my production namespace can only use so many GPUs so",
    "start": "870620",
    "end": "879050"
  },
  {
    "text": "we added support for doing that in version one point 10 and this is how it",
    "start": "879050",
    "end": "884300"
  },
  {
    "text": "looks like in purple I think you have what this is saying that whenever this",
    "start": "884300",
    "end": "891110"
  },
  {
    "text": "object is created in a particular namespace the total number of GPUs that can be used by different parts or single",
    "start": "891110",
    "end": "898610"
  },
  {
    "text": "part in that namespace is four so you can you can set limits like that per namespace using resource coders we also",
    "start": "898610",
    "end": "909230"
  },
  {
    "text": "have so one of the other things which you users move on to you after you have a GPU lesser is for GPU metrics for",
    "start": "909230",
    "end": "916460"
  },
  {
    "text": "their workloads so communities already supports a couple of important metrics",
    "start": "916460",
    "end": "922490"
  },
  {
    "text": "which users care about for their workloads so these are memory usage and",
    "start": "922490",
    "end": "929620"
  },
  {
    "text": "duty GPU duty cycles they are collected",
    "start": "929620",
    "end": "934910"
  },
  {
    "text": "by a C advisor which monitoring agent which is built into the cubelet and they collected using env ml",
    "start": "934910",
    "end": "942470"
  },
  {
    "text": "which is a closed source nvidia as management library which can be used to collect GP metrics and these metrics are",
    "start": "942470",
    "end": "949820"
  },
  {
    "text": "can be accessed using SI Advisors Prometheus endpoint or hipster or Google",
    "start": "949820",
    "end": "955700"
  },
  {
    "text": "stack driver and they were added in 1.9 so you can already use them this is one",
    "start": "955700",
    "end": "964940"
  },
  {
    "text": "other very interesting thing which a lot of you should do if you are not doing it",
    "start": "964940",
    "end": "970400"
  },
  {
    "text": "already is that you you want to because GPUs are expensive you want to create",
    "start": "970400",
    "end": "975920"
  },
  {
    "text": "dedicated nodes for GPU workloads and why why do you want to do this right",
    "start": "975920",
    "end": "981830"
  },
  {
    "text": "like so you want to prevent parts that are not requesting GPUs from getting",
    "start": "981830",
    "end": "986990"
  },
  {
    "text": "scheduled on nodes which have GPS this helps to make sure that when a pod which",
    "start": "986990",
    "end": "994460"
  },
  {
    "text": "is actually requesting GPU arrives there's some place to schedule it because if that GPU node is already busy",
    "start": "994460",
    "end": "1001450"
  },
  {
    "text": "running on GPU workloads then you will get into a situation where your GPUs are",
    "start": "1001450",
    "end": "1007540"
  },
  {
    "text": "idle and your pods requesting GPUs are pending and you don't want to be in that",
    "start": "1007540",
    "end": "1012670"
  },
  {
    "text": "situation because they're just wasting money another thing this helps with is",
    "start": "1012670",
    "end": "1018040"
  },
  {
    "text": "to aggressively downscale nodes if you are in a cloud environment so if you",
    "start": "1018040",
    "end": "1023530"
  },
  {
    "text": "let's say you don't have any GPU workloads at the moment your machine learning scientists are away on vacation or whatever and if you prevent parts",
    "start": "1023530",
    "end": "1031990"
  },
  {
    "text": "that are not requesting GPUs from scheduling on GP nodes then those GPU nodes will be empty when there is no GP",
    "start": "1031990",
    "end": "1038079"
  },
  {
    "text": "workload and the cluster autoscaler can scale those nodes down and save you costs and how do you do this the way to",
    "start": "1038079",
    "end": "1048490"
  },
  {
    "text": "do this in Kuban it is is to add panes to node so what you do is you would",
    "start": "1048490",
    "end": "1054100"
  },
  {
    "text": "paint the GPU nodes with some paint and then no pod will schedule on them until",
    "start": "1054100",
    "end": "1060970"
  },
  {
    "text": "it has a corresponding Toleration so you would tall you'd add those Toleration to",
    "start": "1060970",
    "end": "1067150"
  },
  {
    "text": "the pods requesting GPUs and so what will happen is now you have notes with GPUs that",
    "start": "1067150",
    "end": "1072610"
  },
  {
    "text": "have pains someone submit support that does not request GPU it doesn't have that toleration it doesn't get scheduled",
    "start": "1072610",
    "end": "1079000"
  },
  {
    "text": "on a GPU node someone some it support with GPUs adds that toleration so it gets scheduled on that nodes right in",
    "start": "1079000",
    "end": "1087270"
  },
  {
    "text": "1.9 we added an admission controller called extended resource toleration so",
    "start": "1087270",
    "end": "1094390"
  },
  {
    "text": "this extended this controller recommends that you add a particular tain to your",
    "start": "1094390",
    "end": "1102760"
  },
  {
    "text": "GP nodes so it tells you what the key of that tin should be and if you do that",
    "start": "1102760",
    "end": "1108240"
  },
  {
    "text": "what this control what this admission controller will do is that it will automatically add a toleration to your",
    "start": "1108240",
    "end": "1116320"
  },
  {
    "text": "GP pots and so like so what you do is you add the taint suggested by this",
    "start": "1116320",
    "end": "1124270"
  },
  {
    "text": "admission controller to your nodes this admission controller will automatically add toleration for that taint in parts",
    "start": "1124270",
    "end": "1131830"
  },
  {
    "text": "that are requesting GPUs and so those pots will get automatically scheduled right so you would have like your users",
    "start": "1131830",
    "end": "1138010"
  },
  {
    "text": "would not have to worry about that I need to add toleration to my part spec that will happen automatically for them",
    "start": "1138010",
    "end": "1143980"
  },
  {
    "text": "so the pot specs will remain they will not know that there is a dedicated GPU node pool or something they'll just",
    "start": "1143980",
    "end": "1150160"
  },
  {
    "text": "submit the GP workloads only the workloads which request GPUs will schedule on those nodes and nothing else",
    "start": "1150160",
    "end": "1156040"
  },
  {
    "text": "is getting on those nodes okay so we we talked about what is already present in",
    "start": "1156040",
    "end": "1163120"
  },
  {
    "text": "communities for GPUs what's missing so there is no support for GPU in mini cube",
    "start": "1163120",
    "end": "1170980"
  },
  {
    "text": "a lot of people have expressed interest in this a lot of people have asked about it but the development of this is not",
    "start": "1170980",
    "end": "1177820"
  },
  {
    "text": "done yet and this is a great area where people can contribute there's also no",
    "start": "1177820",
    "end": "1184540"
  },
  {
    "text": "fine-grained quota control now what do I mean by that I mentioned that you can",
    "start": "1184540",
    "end": "1190090"
  },
  {
    "text": "use resource quotas to cap the number of GPUs that can be used in a particular namespace but because humanities does",
    "start": "1190090",
    "end": "1198070"
  },
  {
    "text": "not natively understand the types of GPUs it only in the sense whether something is a GP or not",
    "start": "1198070",
    "end": "1204100"
  },
  {
    "text": "we cannot say that in my names in this namespace I want to only allow 4k 80",
    "start": "1204100",
    "end": "1209409"
  },
  {
    "text": "GPUs or 4 P 100 GPS you can only say that I want to only allow for GPUs right",
    "start": "1209409",
    "end": "1214929"
  },
  {
    "text": "so there's no fine-grained quota control and there is some design discussion going on on how to support this and as I",
    "start": "1214929",
    "end": "1222999"
  },
  {
    "text": "said there's some user level metrics already present in the in communities",
    "start": "1222999",
    "end": "1228489"
  },
  {
    "text": "for GPUs but more metrics can be added specifically a lot of metrics which are required by cluster administrators for",
    "start": "1228489",
    "end": "1235989"
  },
  {
    "text": "example they may want GPU temperature or power usage etc they are not present at",
    "start": "1235989",
    "end": "1241960"
  },
  {
    "text": "the moment and we are working like we're designing how to add these metrics",
    "start": "1241960",
    "end": "1247269"
  },
  {
    "text": "without polluting the cumulative score and like having an extensible way of",
    "start": "1247269",
    "end": "1252700"
  },
  {
    "text": "adding these metrics there's also no support for GPU sharing so when a GPU is",
    "start": "1252700",
    "end": "1260559"
  },
  {
    "text": "attached to a part or a container it's exclusively used by that container no",
    "start": "1260559",
    "end": "1267009"
  },
  {
    "text": "other container can use that GPU at the same time also you cannot request half a",
    "start": "1267009",
    "end": "1274419"
  },
  {
    "text": "GPU like you get the whole GPU or you don't you get nothing so this no GPU sharing its exclusive",
    "start": "1274419",
    "end": "1282460"
  },
  {
    "text": "usage right now and we are thinking about how to do this and like there's just thoughts right now I think",
    "start": "1282460",
    "end": "1291009"
  },
  {
    "text": "Kuban is also not aware of GPU topology so if for example you get scheduled in a",
    "start": "1291009",
    "end": "1299109"
  },
  {
    "text": "way that the topology is wrong you may not get the performance you want with",
    "start": "1299109",
    "end": "1304119"
  },
  {
    "text": "your workloads so that's this is another thing which we are actively thinking about but there's no concrete design or",
    "start": "1304119",
    "end": "1311109"
  },
  {
    "text": "anything yet finally the auto scaling support in communities for GPUs is not",
    "start": "1311109",
    "end": "1318519"
  },
  {
    "text": "ideal so there is auto scaling support you can auto scale based on the number of GPUs and if their pods pending you",
    "start": "1318519",
    "end": "1325509"
  },
  {
    "text": "can the autoscaler will know these GP pods are pending so I need to add norsu GPUs but it's not ideal and the reason",
    "start": "1325509",
    "end": "1332739"
  },
  {
    "text": "for this is again the same community is aware of GPUs it's not aware of GPU types right so pod is pending it",
    "start": "1332739",
    "end": "1340509"
  },
  {
    "text": "requires two GPUs what type of GPU note should I add should i add a KT node or should I add a P 100 node and there's",
    "start": "1340509",
    "end": "1347919"
  },
  {
    "text": "there's some hacks in the autoscaler which work with us but like that's not an ideal scenario and we are again",
    "start": "1347919",
    "end": "1354549"
  },
  {
    "text": "working towards fixing this situation okay so I have a shameless plug now if",
    "start": "1354549",
    "end": "1361059"
  },
  {
    "text": "you don't want to do all of this you can run these two commands in the in GK and",
    "start": "1361059",
    "end": "1366399"
  },
  {
    "text": "you will have a GPU cluster working the first command creates the GPU cluster and the second command installs the",
    "start": "1366399",
    "end": "1372999"
  },
  {
    "text": "driver yeah you still have to install the driver any questions yes",
    "start": "1372999",
    "end": "1384179"
  },
  {
    "text": "so the question is what if there are multiple types of GPUs on the same note",
    "start": "1400110",
    "end": "1407549"
  },
  {
    "text": "yeah so we have not heard a lot of use cases and I am not sure if there are any",
    "start": "1407549",
    "end": "1414039"
  },
  {
    "text": "plans to immediately support this I'm hopeful that the design we come for",
    "start": "1414039",
    "end": "1419679"
  },
  {
    "text": "where we make GPUs or we make humanities or device plugins aware of GPU types may",
    "start": "1419679",
    "end": "1427000"
  },
  {
    "text": "handle that your skills but like that's not like that's not a very common use case people usually have heterogeneous",
    "start": "1427000",
    "end": "1433090"
  },
  {
    "text": "clusters heterogeneous nodes are not that common so yeah so the question is",
    "start": "1433090",
    "end": "1462960"
  },
  {
    "text": "has there been cases where newer versions of CUDA don't work or older versions of CUDA don't work on newer",
    "start": "1462960",
    "end": "1468970"
  },
  {
    "text": "versions of driver I have not seen them like thankfully CUDA versions have been",
    "start": "1468970",
    "end": "1474760"
  },
  {
    "text": "backward compatible like the drivers have been backward compatible with the old CUDA versions so that has been very",
    "start": "1474760",
    "end": "1480190"
  },
  {
    "text": "nice yes",
    "start": "1480190",
    "end": "1484259"
  },
  {
    "text": "yeah yeah so the question is what about",
    "start": "1497040",
    "end": "1509020"
  },
  {
    "text": "power metrics for workloads yeah so we are we're working on making sure that we",
    "start": "1509020",
    "end": "1516520"
  },
  {
    "text": "can add more metrics or we can working",
    "start": "1516520",
    "end": "1521830"
  },
  {
    "text": "on making sure that we can add a way to extensively add more metrics to",
    "start": "1521830",
    "end": "1527670"
  },
  {
    "text": "humanity's workloads but there's that's I think that will happen maybe in 1.3",
    "start": "1527670",
    "end": "1535720"
  },
  {
    "text": "like we are hopeful for design in 1.12 and then some implementation 1.3 yeah so",
    "start": "1535720",
    "end": "1553000"
  },
  {
    "text": "the question is is there a recommendation for a base container image to build your containers on we",
    "start": "1553000",
    "end": "1558010"
  },
  {
    "text": "recommend Nvidia's CUDA images which are available on docker hub and on get good lab I think yeah so they are they work",
    "start": "1558010",
    "end": "1565960"
  },
  {
    "text": "really well I think back",
    "start": "1565960",
    "end": "1570600"
  },
  {
    "text": "yeah the question is how does the Nvidia runtime environment fit into all this",
    "start": "1591029",
    "end": "1596440"
  },
  {
    "text": "so for NVIDIA container runtime you still need the driver installed I think",
    "start": "1596440",
    "end": "1602380"
  },
  {
    "text": "there is some talks about how to integrate both of them together but right now you still need the driver",
    "start": "1602380",
    "end": "1608590"
  },
  {
    "text": "installed for NVIDIA container runtime to work and you so so as I mentioned",
    "start": "1608590",
    "end": "1614590"
  },
  {
    "text": "there are two device plugins right now the nvidia has device plugin uses nvidia container runtime underneath to make",
    "start": "1614590",
    "end": "1620860"
  },
  {
    "text": "your make the shared libraries available to the container the google one does not",
    "start": "1620860",
    "end": "1627059"
  },
  {
    "text": "doesn't do that yet but like yeah so that's how it feels like it it's a way of getting these shared libraries",
    "start": "1627059",
    "end": "1635830"
  },
  {
    "text": "available to the container it does some other things as well but is the question",
    "start": "1635830",
    "end": "1656169"
  },
  {
    "text": "that you whether you can use GPUs without end in NVIDIA container and time",
    "start": "1656169",
    "end": "1664020"
  },
  {
    "text": "I see I see so the question is what is",
    "start": "1671190",
    "end": "1676770"
  },
  {
    "text": "the relationship between the darker diamond nvidia container n time and the driver right so so the darker demon",
    "start": "1676770",
    "end": "1683790"
  },
  {
    "text": "allows you to have multiple container n times right by default you use run C",
    "start": "1683790",
    "end": "1689150"
  },
  {
    "text": "Nvidia container runtime is run C plus app restart hook which adds some things",
    "start": "1689150",
    "end": "1695550"
  },
  {
    "text": "which make your shared libraries available and so on so the doctor demon is like you communities does not",
    "start": "1695550",
    "end": "1702510"
  },
  {
    "text": "actually require darker anymore right there is a CRI API which which can talk to cryo which can talk to multiple",
    "start": "1702510",
    "end": "1708300"
  },
  {
    "text": "container n times so docker is basically talking to the container in time underneath to start the container and",
    "start": "1708300",
    "end": "1714480"
  },
  {
    "text": "media container runtime is just one of those runtimes and it's very close to",
    "start": "1714480",
    "end": "1719970"
  },
  {
    "text": "run C which is the default runtime in awe in docker and in cryo and in communities and the driver is totally",
    "start": "1719970",
    "end": "1726300"
  },
  {
    "text": "separate driver is basically a collection of the kernel module the",
    "start": "1726300",
    "end": "1732660"
  },
  {
    "text": "shared libraries and some debug utilities so you need to install the",
    "start": "1732660",
    "end": "1738810"
  },
  {
    "text": "driver which will install these things then video content container runtime after that will make these shared",
    "start": "1738810",
    "end": "1745680"
  },
  {
    "text": "libraries available in the container and docker is just one way to use the Nvidia",
    "start": "1745680",
    "end": "1750900"
  },
  {
    "text": "container runtime you can use communities directly and you can use cryo as well yeah",
    "start": "1750900",
    "end": "1758540"
  },
  {
    "text": "oh so the question is what are the like",
    "start": "1763190",
    "end": "1770210"
  },
  {
    "text": "the estimated time then these the things which are missing arrive it's really hard to say",
    "start": "1770210",
    "end": "1776800"
  },
  {
    "text": "it's like software estimation right but like so there's some so the monitoring",
    "start": "1776800",
    "end": "1784040"
  },
  {
    "text": "thing is one of the top priorities of the resource management workgroup which",
    "start": "1784040",
    "end": "1789080"
  },
  {
    "text": "works on device plugins the GPU sharing and GPU topology is in very early design",
    "start": "1789080",
    "end": "1797510"
  },
  {
    "text": "phase and then mini cube is like we're",
    "start": "1797510",
    "end": "1802880"
  },
  {
    "text": "just looking for someone to add mini Q support like it should so many cube support is you can add it on Linux you",
    "start": "1802880",
    "end": "1809060"
  },
  {
    "text": "can add it on Mac the Linux one should be fairly easy because mini cube on",
    "start": "1809060",
    "end": "1814190"
  },
  {
    "text": "Linux can run with a mode called driver equal to none not the Nvidia driver but the VM driver and that should be fairly",
    "start": "1814190",
    "end": "1821240"
  },
  {
    "text": "easy but no one has just gotten around to do it on Mac I'm not sure I think be a hard thing on Mac or Windows maybe on",
    "start": "1821240",
    "end": "1827600"
  },
  {
    "text": "Windows it'd be easy and why else was there I forgot but yeah like this all of",
    "start": "1827600",
    "end": "1836000"
  },
  {
    "text": "them are yeah so the other things were making the autoscaler better and making finally encoder control that's also in",
    "start": "1836000",
    "end": "1842690"
  },
  {
    "text": "the design phase so hopefully in one point well you'll start seeing some sorts of like some it is some design",
    "start": "1842690",
    "end": "1849770"
  },
  {
    "text": "invent well for all these things and hopefully later you'll have some alphas and betas",
    "start": "1849770",
    "end": "1856210"
  },
  {
    "text": "yes so people have used device plugins I think to implement F like provide an",
    "start": "1861220",
    "end": "1868010"
  },
  {
    "text": "FPGA device but I am NOT very familiar with that like there there are examples out there and so there is a you should",
    "start": "1868010",
    "end": "1874940"
  },
  {
    "text": "look up resource management workgroup where we talk about these things and like you'll have you can talk to more",
    "start": "1874940",
    "end": "1881390"
  },
  {
    "text": "people there and they'll have more sites about this okay all right thanks a lot",
    "start": "1881390",
    "end": "1888830"
  },
  {
    "text": "everyone [Applause]",
    "start": "1888830",
    "end": "1894550"
  }
]