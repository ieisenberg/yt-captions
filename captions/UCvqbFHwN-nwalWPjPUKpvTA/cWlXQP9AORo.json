[
  {
    "text": "a lot of content to get through I want to make sure this time so my name is Noah Eisen I'm an engineer on the G RPC",
    "start": "60",
    "end": "7020"
  },
  {
    "text": "team I've been working there for about two and a half years now with the focus on C++ and for the past year and a half",
    "start": "7020",
    "end": "14370"
  },
  {
    "text": "I've been focusing on performance so this talk is on GC performance we're",
    "start": "14370",
    "end": "19680"
  },
  {
    "text": "going to talk about tuning both libraries like the internals of the GBC library and also tuning applications",
    "start": "19680",
    "end": "26039"
  },
  {
    "text": "that use G RPC so before we get further into this I want to talk about intended",
    "start": "26039",
    "end": "31410"
  },
  {
    "text": "audience it's really meant for anyone who has an interest in performance the examples are going to come from GBC",
    "start": "31410",
    "end": "37320"
  },
  {
    "text": "library and geography applications but all of the concepts are very high level",
    "start": "37320",
    "end": "42379"
  },
  {
    "text": "and so they should be interesting to anyone who has worked with performance or who cares about performance so I want",
    "start": "42379",
    "end": "51030"
  },
  {
    "text": "to talk and also on the slide I don't have it in text but I kinda want to talk about like goals of this presentation",
    "start": "51030",
    "end": "56300"
  },
  {
    "text": "like definitely some general not to be specific ideas about performance best",
    "start": "56300",
    "end": "63270"
  },
  {
    "text": "practices is something I want people to walk away with and then also near the end I kind of want this to become a way",
    "start": "63270",
    "end": "68400"
  },
  {
    "text": "that people are more comfortable starting conversations about like maybe specifically geography performance like",
    "start": "68400",
    "end": "73590"
  },
  {
    "text": "how to open a github issue and wedding kind of information to put in there that would make it more actionable from our",
    "start": "73590",
    "end": "79110"
  },
  {
    "text": "end okay so I'll start with the jerbs the overview in case people aren't as",
    "start": "79110",
    "end": "85590"
  },
  {
    "text": "familiar with G RPC I want to do a quick show of hands how many people have heard of G RPC okay cool that's what I figured",
    "start": "85590",
    "end": "92729"
  },
  {
    "text": "how many people have actually written Jer BC code and among that who has",
    "start": "92729",
    "end": "97799"
  },
  {
    "text": "worked with the Java stack okay how about the go stack and then how about",
    "start": "97799",
    "end": "104189"
  },
  {
    "text": "the C++ or rap language stack okay cool cool okay so I'll do an overview of",
    "start": "104189",
    "end": "112560"
  },
  {
    "text": "your PC and then I'm going to talk about kind of like the crux of this presentation which is tooling benchmarks",
    "start": "112560",
    "end": "120270"
  },
  {
    "text": "and data I'm going to keep coming back to those three word you're gonna hear them again and again and again applied",
    "start": "120270",
    "end": "125969"
  },
  {
    "text": "throughout the talk in different ways so then we're gonna go through some examples in the GRP c library itself",
    "start": "125969",
    "end": "133930"
  },
  {
    "text": "and then near the end we'll talk about govt applications and again apply those",
    "start": "133930",
    "end": "139300"
  },
  {
    "text": "same concepts so starting with the GOP overview Google has this rich history of",
    "start": "139300",
    "end": "147760"
  },
  {
    "text": "taking the awesome technology internally and making it available to the open",
    "start": "147760",
    "end": "154239"
  },
  {
    "text": "source so at Google internally there's a container management system called Borg that was open sources kubernetes I",
    "start": "154239",
    "end": "161319"
  },
  {
    "text": "assume most people are familiar with that because we're at coop con similarly the internal build system at",
    "start": "161319",
    "end": "166840"
  },
  {
    "text": "Google is called blaze that was open source as basil recently tensorflow the machine learning library exists",
    "start": "166840",
    "end": "173140"
  },
  {
    "text": "internally at Google and it was also made available to the open source community and then lastly the internal",
    "start": "173140",
    "end": "180370"
  },
  {
    "text": "RPC solution for Google was stubby but Google is replacing it with GRDC which",
    "start": "180370",
    "end": "187989"
  },
  {
    "text": "is built in the open source and currently migrating internal use cases to use GBC more and more so that's kind",
    "start": "187989",
    "end": "194980"
  },
  {
    "text": "of like the origins in history of Djerba see in case you aren't familiar g-o-p C stands for G RPC remote procedure call",
    "start": "194980",
    "end": "201760"
  },
  {
    "text": "it's a recursive definition I promise Oh G every C is a high-performance open",
    "start": "201760",
    "end": "208480"
  },
  {
    "text": "source standards-based general-purpose polyglot feature-rich RPC framework each of those bullet",
    "start": "208480",
    "end": "214900"
  },
  {
    "text": "points could deserve an hour-long talk by itself I've taken the high",
    "start": "214900",
    "end": "220989"
  },
  {
    "text": "performance one and put it in bold because that's what we're going to talk about today and it's actually developed",
    "start": "220989",
    "end": "226900"
  },
  {
    "text": "there's a big teen of Google that is working on it and some open source contributors we'd love them work but there are definitely some and it is",
    "start": "226900",
    "end": "233349"
  },
  {
    "text": "production ready today so that's the brief overview of G RPC a generic stack",
    "start": "233349",
    "end": "239230"
  },
  {
    "text": "for G or PC looks kind of like this there at the top and the green you have a user application that's the code that",
    "start": "239230",
    "end": "245169"
  },
  {
    "text": "is actually calling into the GRDC api's so we don't own that the first entry",
    "start": "245169",
    "end": "250569"
  },
  {
    "text": "point into our library is a layer of generated code and that is API specific so you feed some generating process a",
    "start": "250569",
    "end": "257799"
  },
  {
    "text": "proto file which has your service definitions and it will spin up that generated code layer the generated code",
    "start": "257799",
    "end": "264700"
  },
  {
    "text": "is then going to hook into the core library that's where main functionality and main features are implemented and then some are lower down",
    "start": "264700",
    "end": "271710"
  },
  {
    "text": "at the bottom is a transport and this is a library or set of system calls that",
    "start": "271710",
    "end": "276960"
  },
  {
    "text": "knows how to put bytes onto a wire and this is going to look different from stack to stack so I'm gonna briefly run",
    "start": "276960",
    "end": "282120"
  },
  {
    "text": "through each of our language 2x because we have three separate language stacks um first off go I just wanted to call it",
    "start": "282120",
    "end": "288420"
  },
  {
    "text": "some differences in the bottom they in some ways depend on the XNA HTTP to library this is mostly for h2 framing",
    "start": "288420",
    "end": "296900"
  },
  {
    "text": "and then on the bottom they will be calling to TCP Java stack also the main",
    "start": "296900",
    "end": "304080"
  },
  {
    "text": "differences to call out are in the transport you know the bottom Java depends on two libraries Medi and ok",
    "start": "304080",
    "end": "310140"
  },
  {
    "text": "HTTP okay HTTP is used for mobile clients Java also has a completely G RPC",
    "start": "310140",
    "end": "316950"
  },
  {
    "text": "own in process transport to work in a case where you have gypsy in the same",
    "start": "316950",
    "end": "323760"
  },
  {
    "text": "process the C++ deck looks a little bit different so starting from the bottom",
    "start": "323760",
    "end": "330450"
  },
  {
    "text": "and going up at the bottom is also a TCP but the C library has implemented its",
    "start": "330450",
    "end": "335730"
  },
  {
    "text": "own HTTP implementation that's called C HTTP that allows us to write an h2",
    "start": "335730",
    "end": "341340"
  },
  {
    "text": "implementation that uses your PC data structures and we can more heavily optimize it because it's more of our own code we can move faster above that",
    "start": "341340",
    "end": "349380"
  },
  {
    "text": "there's the GOP C core again that's the bulk of the functionality on top of the core is a thin sea surface and the sea",
    "start": "349380",
    "end": "355890"
  },
  {
    "text": "surface is what supports are wrapped languages so above that you can see Python Ruby there's also a node",
    "start": "355890",
    "end": "361020"
  },
  {
    "text": "Objective C PHP anything that wasn't mentioned before is supported by our sea surface and then that kind of allows the",
    "start": "361020",
    "end": "369660"
  },
  {
    "text": "rap language is to share most of the functionality and we implement it in core and they just get that feature for free and then above that is of course",
    "start": "369660",
    "end": "376590"
  },
  {
    "text": "layer generated code ok that's it for the overview I want to talk about the key points as I mentioned tooling",
    "start": "376590",
    "end": "383190"
  },
  {
    "text": "benchmarks and data these the points we're gonna keep coming back to and if there's three words that you remember walking out of this it should be tooling",
    "start": "383190",
    "end": "389520"
  },
  {
    "text": "benchmarks and data and you'll hear them a lot so starting with tooling in order",
    "start": "389520",
    "end": "394590"
  },
  {
    "text": "to optimize code you need to know where to look I have confused Jackie Chan asking where am i microseconds going",
    "start": "394590",
    "end": "400860"
  },
  {
    "text": "you don't know you cannot possibly start to perform optimizations and so in my",
    "start": "400860",
    "end": "406650"
  },
  {
    "text": "mind tooling kind of narrows the problem scope so tooling narrows I know that's kind of cryptic we'll get back to what",
    "start": "406650",
    "end": "412439"
  },
  {
    "text": "that what I exactly mean by that and the last thing I want to call out before I dive deeper into tooling is that it's important that there is no",
    "start": "412439",
    "end": "419789"
  },
  {
    "text": "there's no perfect tool there's no one tool that's going to show all the hotspots optimize all the places where",
    "start": "419789",
    "end": "425189"
  },
  {
    "text": "your code is slow there's no one tool to rule them all or something which is why it's important to use many tools and",
    "start": "425189",
    "end": "431669"
  },
  {
    "text": "we'll talk about that so diving into a few important kinds of tooling that have been very important important to GRDC I",
    "start": "431669",
    "end": "437789"
  },
  {
    "text": "want to start with latency traces so just know they're like show of hands",
    "start": "437789",
    "end": "443340"
  },
  {
    "text": "kind of thing who here has used a tracing system this could be like distributed tracing or just in process",
    "start": "443340",
    "end": "448919"
  },
  {
    "text": "okay cool so experience yeah so then this might look very familiar tracing involves adding annotations directly to",
    "start": "448919",
    "end": "454740"
  },
  {
    "text": "the code gr PCC has our own timer system that does tracing so as you can see",
    "start": "454740",
    "end": "460680"
  },
  {
    "text": "that's actually what it looks like GPR timer scope and you give it a string value and what that'll do it'll log a",
    "start": "460680",
    "end": "467219"
  },
  {
    "text": "timestamp and that string and it'll allow for some post-processing so at the",
    "start": "467219",
    "end": "472409"
  },
  {
    "text": "simplest form this could actually just be writing to a log file and then have a post processor that reads in the timestamps it could get a little more",
    "start": "472409",
    "end": "479400"
  },
  {
    "text": "fancy these annotations could instead write some in-memory data structure if you want less overhead but anything",
    "start": "479400",
    "end": "486930"
  },
  {
    "text": "latency tracing maintains ordering and gives you kind of a this happens and this happens and this happens view of",
    "start": "486930",
    "end": "493110"
  },
  {
    "text": "your code and that's a great way to show hotspots on the left I have actually a",
    "start": "493110",
    "end": "498210"
  },
  {
    "text": "screenshot of some internal visualization tool and as you can see with an arrow this is just capturing",
    "start": "498210",
    "end": "503520"
  },
  {
    "text": "kind of all the things that happen before we actually go to the send message syscall and i've linked to our gob c implementation of these timers in",
    "start": "503520",
    "end": "510419"
  },
  {
    "text": "our github repo if anyone's curious but",
    "start": "510419",
    "end": "515490"
  },
  {
    "text": "as I mentioned there's no perfect tool and tracing has its strengths and it has",
    "start": "515490",
    "end": "521729"
  },
  {
    "text": "its weaknesses and it's important to understand those to use it more most effectively so for tracing one of the",
    "start": "521729",
    "end": "528329"
  },
  {
    "text": "biggest weaknesses is that the tracing system itself can add a lot of overhead they tend to be more of a heavyweight tool so",
    "start": "528329",
    "end": "533980"
  },
  {
    "text": "trusting benchmark numbers that come when tracing enabled might not be a great idea sometimes what we'll do is we'll you",
    "start": "533980",
    "end": "540430"
  },
  {
    "text": "will enable tracing to get a graph like that but then to get actual absolute numbers disabled to tracing also tracing",
    "start": "540430",
    "end": "549160"
  },
  {
    "text": "works very well in low thread environments multiple threads are happening at once tracing can get a",
    "start": "549160",
    "end": "554170"
  },
  {
    "text": "little Messier this comes from a very single threaded benchmark we have so",
    "start": "554170",
    "end": "559570"
  },
  {
    "text": "tracing in itself is not enough another tool that geo PC has focused on CPU",
    "start": "559570",
    "end": "565360"
  },
  {
    "text": "profiling again kind of show of hands like who here is familiar with profiling or is like looked at charts like this",
    "start": "565360",
    "end": "571060"
  },
  {
    "text": "okay I figured this would be a little more popular so this is the flame graph",
    "start": "571060",
    "end": "576100"
  },
  {
    "text": "again taking of a real GBC benchmark again you can kind of see that send message up in the top which is taking a",
    "start": "576100",
    "end": "583300"
  },
  {
    "text": "large chunk of time which is good if we could make all of you know the more time they've spent in the actual assist call the better the less overhead that's",
    "start": "583300",
    "end": "589060"
  },
  {
    "text": "happening in the JBC library profiling like any tool will have its strengths",
    "start": "589060",
    "end": "595779"
  },
  {
    "text": "and have its weaknesses so like a strength of profiling would be you don't actually need to go and annotate your",
    "start": "595779",
    "end": "601510"
  },
  {
    "text": "code profiling kind of works for free out of the box the kernel is actually",
    "start": "601510",
    "end": "606699"
  },
  {
    "text": "you know asking each core 400 times a second or some frequency what function",
    "start": "606699",
    "end": "612040"
  },
  {
    "text": "is running on you right now it'll value at that data and generate this chart so",
    "start": "612040",
    "end": "617290"
  },
  {
    "text": "if a function is hot it will show up in an in a profile which is great because it can kind of be the fastest way forward and fastest tool to get started",
    "start": "617290",
    "end": "623829"
  },
  {
    "text": "on and then it also tends to be pretty low overhead depending on like with what frequency it's profiling but again every",
    "start": "623829",
    "end": "631389"
  },
  {
    "text": "tool has weaknesses if the thread is sleeping it's not running on a snot running out for they won't be profiled",
    "start": "631389",
    "end": "637810"
  },
  {
    "text": "so CPU profiling not really a strong tool to show contention problems because in that case you'll have threads waiting",
    "start": "637810",
    "end": "644199"
  },
  {
    "text": "but they won't actually be showing that up here so lastly I also wanted to call",
    "start": "644199",
    "end": "650620"
  },
  {
    "text": "out some other tools that are worth mentioning lock contention measuring tools just mentioned hard to do lock",
    "start": "650620",
    "end": "657279"
  },
  {
    "text": "contention with profiling so Val gram you trace things like that jus bc has also had very good luck with custom",
    "start": "657279",
    "end": "663939"
  },
  {
    "text": "counter code so we've implemented some infrastructure in our library that will actually in a",
    "start": "663939",
    "end": "670090"
  },
  {
    "text": "certain kind of build hook into the allocation system the atomic system and run counters and then log that",
    "start": "670090",
    "end": "676720"
  },
  {
    "text": "information so we can actually have benchmarks that start tracking allocation counts and we can see if",
    "start": "676720",
    "end": "681850"
  },
  {
    "text": "there's change and that's really great and kind of reducing any of those always gonna be a good idea and then generally",
    "start": "681850",
    "end": "689200"
  },
  {
    "text": "other kernel tools can always be useful like perfect chorus and s trace for looking at system calls and polka holes",
    "start": "689200",
    "end": "695560"
  },
  {
    "text": "and new one we're kind of exploring with that shows you more efficient C++ truck packing so that that's a new one but the",
    "start": "695560",
    "end": "701650"
  },
  {
    "text": "bottom line about tooling is that it's you need to obtain an arsenal of tools and then that's not enough because that",
    "start": "701650",
    "end": "708760"
  },
  {
    "text": "Arsenal needs to continuously grow and people should be experimenting with new tools trying to figure out what's cool",
    "start": "708760",
    "end": "714160"
  },
  {
    "text": "how can we look at our stack in another way and then use tools together where one is weak use another one or when a",
    "start": "714160",
    "end": "720340"
  },
  {
    "text": "strong focus on its strengths and that really gives like a performance team a great sense of where the time is being",
    "start": "720340",
    "end": "727120"
  },
  {
    "text": "spent where the microseconds are going so that's tooling let's move on to",
    "start": "727120",
    "end": "733300"
  },
  {
    "text": "benchmarks so in order to optimize you need to know how to measure so now I",
    "start": "733300",
    "end": "739120"
  },
  {
    "text": "have confused Fry asking is that really an optimization I'm not sure like I think it was faster but I mean if you",
    "start": "739120",
    "end": "744820"
  },
  {
    "text": "can't prove it it's a little bit harder to say so benchmarks then we're tooling",
    "start": "744820",
    "end": "752560"
  },
  {
    "text": "narrow scope I kind of see it as benchmarks widen scope of impact and again I'll get back to that and make",
    "start": "752560",
    "end": "757840"
  },
  {
    "text": "that a little more clear some other diagrams like tooling benchmarks come",
    "start": "757840",
    "end": "763900"
  },
  {
    "text": "with many different kinds so micro benchmarks raise your hand if you have",
    "start": "763900",
    "end": "771370"
  },
  {
    "text": "written a micro benchmark in any language with okay fewer so microphones",
    "start": "771370",
    "end": "779080"
  },
  {
    "text": "folks were awesome they're great for narrow scope optimizations so this is a real snippet of from GBC code base it's",
    "start": "779080",
    "end": "784840"
  },
  {
    "text": "benchmarking air creation geo PCC core has a struct that represents air and",
    "start": "784840",
    "end": "790240"
  },
  {
    "text": "status and in this tight loop it's going to create it and then delete it and that's gonna run something like five",
    "start": "790240",
    "end": "796630"
  },
  {
    "text": "five point five million times I think yeah 5.5 million times until the",
    "start": "796630",
    "end": "801670"
  },
  {
    "text": "benchmarking system gets a good sense of how long that operation took and it will output data so it's telling us that",
    "start": "801670",
    "end": "807480"
  },
  {
    "text": "creating and deleting an error is about 120 mana seconds and micro benchmarks",
    "start": "807480",
    "end": "813550"
  },
  {
    "text": "are awesome in a very narrow specific use case they can show changes to small sub components of your code but this is",
    "start": "813550",
    "end": "823420"
  },
  {
    "text": "again a point that I'm going to circle back on and back on all of these kinds of tools have their strengths and have their weaknesses and so micro benchmarks",
    "start": "823420",
    "end": "830379"
  },
  {
    "text": "have the weakness of being narrow in scope when in a program would it ever be realistic to create 5.5 million errors",
    "start": "830379",
    "end": "837490"
  },
  {
    "text": "and then delete them immediately never so it's gonna show pretty unrealistic cache behavior behavior you're not gonna",
    "start": "837490",
    "end": "843040"
  },
  {
    "text": "see multi-threaded scenarios working well so micro benchmarks are good for showing directional changes of sub",
    "start": "843040",
    "end": "849069"
  },
  {
    "text": "components as far as looking at absolute numbers from micro benchmarks probably not as useful but to show you know to",
    "start": "849069",
    "end": "857800"
  },
  {
    "text": "show an optimization for something some type module they're great so another",
    "start": "857800",
    "end": "864670"
  },
  {
    "text": "kind of mesh mark that we've focused on is synthetic benchmarks and what I mean",
    "start": "864670",
    "end": "870429"
  },
  {
    "text": "by synthetic benchmarks is anything that is written above the API so this is a",
    "start": "870429",
    "end": "876610"
  },
  {
    "text": "program and for gr pcs use case this is a program that uses the JIRA PC library does some amount of work and then",
    "start": "876610",
    "end": "882459"
  },
  {
    "text": "outputs some data about that work so I've thrown up an example of GR pcs",
    "start": "882459",
    "end": "890069"
  },
  {
    "text": "synthetic benchmarking system which is highly configurable it involves a driver",
    "start": "890069",
    "end": "895569"
  },
  {
    "text": "sending out configuration files to a client and server and then the client and server will talk to each other with",
    "start": "895569",
    "end": "901689"
  },
  {
    "text": "some configuration of messages and then output data about the latency and the RPC counts and then for our example our",
    "start": "901689",
    "end": "911529"
  },
  {
    "text": "configuration file looks like that on the right it is very configurable you can configure the number of servers the",
    "start": "911529",
    "end": "918069"
  },
  {
    "text": "number of server threads number of clients request response size and that",
    "start": "918069",
    "end": "925240"
  },
  {
    "text": "is very useful for us to kind of experiment with a wide array of RPC topographies and",
    "start": "925240",
    "end": "930720"
  },
  {
    "text": "maybe we have a problem when there's very large messages with only one client in one server maybe it's when there's",
    "start": "930720",
    "end": "936300"
  },
  {
    "text": "five clients and five servers and uneven messages small requests large response it's completely configurable and this",
    "start": "936300",
    "end": "942090"
  },
  {
    "text": "has been awesome but I want to make a point about this that this is just an example of our synthetic benchmarking system it's not the using synthetic",
    "start": "942090",
    "end": "949290"
  },
  {
    "text": "benchmarking system so another example of a synthetic benchmark might be if you were to write a server and a client and",
    "start": "949290",
    "end": "956550"
  },
  {
    "text": "the client does some fixed amount of work like a thousand are pcs and you just use time like the system called",
    "start": "956550",
    "end": "962070"
  },
  {
    "text": "time on it to see how long that takes that's also an example of a synthetic benchmark it's anything written above the API that's doing work for the sole",
    "start": "962070",
    "end": "969720"
  },
  {
    "text": "purpose of benchmarking for the sole purpose of timing it or getting counts and I want to talk about one more",
    "start": "969720",
    "end": "977340"
  },
  {
    "text": "benchmark type and that's application benchmarks and this is kind of a subtle differentiation so I want to talk about",
    "start": "977340",
    "end": "983520"
  },
  {
    "text": "this because benchmarking applicable application benchmarks are also synthetic benchmarks but the difference",
    "start": "983520",
    "end": "989370"
  },
  {
    "text": "in my mind is that these are written with another team's API so this is gonna exercise your stack in a new way it's",
    "start": "989370",
    "end": "996240"
  },
  {
    "text": "actually gonna exercise your stack in the exact way that this other team exercises your stack so to make this more clear the example is tensor flows",
    "start": "996240",
    "end": "1003910"
  },
  {
    "text": "RPC bench also yeah so I know this only applies to libraries because like if",
    "start": "1003910",
    "end": "1009890"
  },
  {
    "text": "you're not a library no and probably be using your API so this asterisk there but the example that",
    "start": "1009890",
    "end": "1015530"
  },
  {
    "text": "I chose is tension flows RPC bench this is a benchmark written via tensor flow api's but it's written in a way that",
    "start": "1015530",
    "end": "1022460"
  },
  {
    "text": "it's it's very RPC intensive so I kind of like drew up a little bit of the configuration it has 30 workers one of",
    "start": "1022460",
    "end": "1029360"
  },
  {
    "text": "them starts it sends an RPC to all the other ones they all talk to the other ones in one full mesh step and then all",
    "start": "1029360",
    "end": "1035449"
  },
  {
    "text": "the results come back to the first worker and all of the actual touchflo",
    "start": "1035449",
    "end": "1041720"
  },
  {
    "text": "work doing that it's doing is pretty lightweight this this stresses the RPC system heavily and so they created this",
    "start": "1041720",
    "end": "1047449"
  },
  {
    "text": "benchmark and we we found it and this was so useful to us because it exercised",
    "start": "1047449",
    "end": "1052760"
  },
  {
    "text": "our stack in a new way with more of a realistic RPC typography for tensorflow so that's that's application benchmarks",
    "start": "1052760",
    "end": "1061120"
  },
  {
    "text": "lastly data so we talked about tooling we've talked about benchmarks I want to talk about data and this kind of comes a",
    "start": "1061300",
    "end": "1069560"
  },
  {
    "text": "little bit of a backstory so there was a time in the JBC performance effort where we had a lot of engineer's working on it",
    "start": "1069560",
    "end": "1075170"
  },
  {
    "text": "bringing numbers to the table with various benchmarks various environments and it was a lot of motion but it was",
    "start": "1075170",
    "end": "1082820"
  },
  {
    "text": "kind of hard to equate that motion with actual progress and so this is kind of a",
    "start": "1082820",
    "end": "1088580"
  },
  {
    "text": "point about the performance team needing kind of a shared language needing a lingua franca with which to talk about optimizations and I mean this both in",
    "start": "1088580",
    "end": "1095870"
  },
  {
    "text": "terms of tooling and benchmarks so like being familiar with the same sets of tools being used to looking at the same diagrams and also kind of knowing which",
    "start": "1095870",
    "end": "1103190"
  },
  {
    "text": "benchmarks we focus on what the average numbers might look like from certain benchmarks once everyone is kind of",
    "start": "1103190",
    "end": "1108800"
  },
  {
    "text": "standardized on that talking about performance optimizations becomes much cleaner and much clearer the the clip",
    "start": "1108800",
    "end": "1116630"
  },
  {
    "text": "art I have here I heard was kind of opaque that's the Tower of Babel but kind of so you could find that if like all your engineers are speaking",
    "start": "1116630",
    "end": "1121790"
  },
  {
    "text": "different languages about like different numbers and different metrics it's impossible to make progress so that's",
    "start": "1121790",
    "end": "1127340"
  },
  {
    "text": "the data point 18 bits so another one of",
    "start": "1127340",
    "end": "1133520"
  },
  {
    "text": "my crappy diagrams this is back to tooling narrows the scope of the problem until it becomes manageable to solve and",
    "start": "1133520",
    "end": "1139550"
  },
  {
    "text": "then benchmarking widens the scope to demonstrate the impacts and all this together gets packaged up as data as a",
    "start": "1139550",
    "end": "1145970"
  },
  {
    "text": "way to like discuss and talk about and show optimizations and I'll come back to this in a case study which will",
    "start": "1145970",
    "end": "1151100"
  },
  {
    "text": "hopefully make it more clear ok so that is we're kind of the high level general",
    "start": "1151100",
    "end": "1156710"
  },
  {
    "text": "library application agnostic points don't worry you'll hear them again they're coming back they will be in this",
    "start": "1156710",
    "end": "1162350"
  },
  {
    "text": "section in the next section as well but those were this kind of the crux of this talk so now we're gonna dive into the GBC library talk about some specific",
    "start": "1162350",
    "end": "1169610"
  },
  {
    "text": "examples from there and before I even",
    "start": "1169610",
    "end": "1175700"
  },
  {
    "text": "get into the text of this slide I kind of want to make a point of these these three principles tooling benchmarks and",
    "start": "1175700",
    "end": "1181010"
  },
  {
    "text": "data are awesome to have and it would be so great if when you started the project your tooling and benchmarks were in a",
    "start": "1181010",
    "end": "1187850"
  },
  {
    "text": "perfect place and they were they were amazing but realistically that's not how it happens things need to get built in to get built fast",
    "start": "1187850",
    "end": "1194120"
  },
  {
    "text": "and realistically your benchmarking system will come up a little bit later than the actual core functionality GBC",
    "start": "1194120",
    "end": "1200870"
  },
  {
    "text": "found this to be the case and there are some things that you know we're known to be slow we went and we fixed them we got",
    "start": "1200870",
    "end": "1207050"
  },
  {
    "text": "all the low-hanging fruit off the table and then it came time to say okay well where is the next big spot for",
    "start": "1207050",
    "end": "1212420"
  },
  {
    "text": "optimization and there wasn't really a clear answer but at the same time there's three kind of important facts I",
    "start": "1212420",
    "end": "1218000"
  },
  {
    "text": "want to call out that's features can cause small regressions and these small regressions",
    "start": "1218000",
    "end": "1223850"
  },
  {
    "text": "might be below your margin of detection and I have that italicized because I want to talk about what that",
    "start": "1223850",
    "end": "1229460"
  },
  {
    "text": "means below margin of detection if you have no benchmarking system that can detect regressions then by definition",
    "start": "1229460",
    "end": "1235880"
  },
  {
    "text": "every regression is below the margin of detection or if you're benchmarking system is in place but it doesn't have",
    "start": "1235880",
    "end": "1241670"
  },
  {
    "text": "coverage maybe you have rich benchmarks on insecure scenarios but then a",
    "start": "1241670",
    "end": "1247670"
  },
  {
    "text": "regression slips into a secure pathway like that's missed or if the benchmark is noisy and it can't catch regressions",
    "start": "1247670",
    "end": "1254570"
  },
  {
    "text": "then that's another way that these Russians can like slip into the codebase regardless the result is consistent slow",
    "start": "1254570",
    "end": "1260840"
  },
  {
    "text": "degradation and performance and so at this point kind of injury history we asked ourselves the question of how do",
    "start": "1260840",
    "end": "1267710"
  },
  {
    "text": "we reverse this process the answer of course was like better benchmarks better",
    "start": "1267710",
    "end": "1274730"
  },
  {
    "text": "tooling more data so we focused on a new benchmark we called it the minimal RPC",
    "start": "1274730",
    "end": "1279850"
  },
  {
    "text": "this was a specific scenario of our synthetic system with that highly configurable system so we configured it",
    "start": "1279850",
    "end": "1286970"
  },
  {
    "text": "to ping pong back and forth one byte payloads very small we did no security",
    "start": "1286970",
    "end": "1292250"
  },
  {
    "text": "we did no stamps or tracing and then we started focusing on the median latency between two computers running in the",
    "start": "1292250",
    "end": "1297890"
  },
  {
    "text": "same rack and we knew this number was about 70 microseconds and everyone the team knew this number and we could start",
    "start": "1297890",
    "end": "1304070"
  },
  {
    "text": "iterating on this number and demonstrating change using this scenario and looking at data coming from this scenario at the same time we reached out",
    "start": "1304070",
    "end": "1311150"
  },
  {
    "text": "and we found new tools and we also this is a little bit I haven't mentioned before but we had to do an effort to",
    "start": "1311150",
    "end": "1317330"
  },
  {
    "text": "reduce our noise and this was just a case of throwing more resources that the problem annoys the scenario was running",
    "start": "1317330",
    "end": "1323270"
  },
  {
    "text": "a two minute benchmark we ran that two minute benchmark for 30 times then took the median so throw some",
    "start": "1323270",
    "end": "1329080"
  },
  {
    "text": "resources at it get the noise down and then you have a minor granularity with which to measure changers so I'm gonna",
    "start": "1329080",
    "end": "1336130"
  },
  {
    "text": "run through a quick case study this was from a latency trace this is the visualization tool I showed earlier we",
    "start": "1336130",
    "end": "1343930"
  },
  {
    "text": "would kind of look at chunks of time together and ask ourselves is anything surprising or taking more time than we think it should so in this case we",
    "start": "1343930",
    "end": "1351220"
  },
  {
    "text": "looked at this Apple filter it was just doing an access control check on an outbound RPC and it was taking about a",
    "start": "1351220",
    "end": "1356890"
  },
  {
    "text": "microsecond you can say from here so that seemed a little bit sketchy the next step of course is to go to another",
    "start": "1356890",
    "end": "1363220"
  },
  {
    "text": "tool so this is a flame graph kind of NER it in I'm gonna zoom it in one",
    "start": "1363220",
    "end": "1368290"
  },
  {
    "text": "further so take a second to look at that top layer and like see if you can see",
    "start": "1368290",
    "end": "1374230"
  },
  {
    "text": "any things that look kind of out of place there's there's like three really good answers here actually there's if",
    "start": "1374230",
    "end": "1380080"
  },
  {
    "text": "there's a lot of things going on so I'm gonna pause her like two more seconds so I'm not gonna pause have to keep going",
    "start": "1380080",
    "end": "1385600"
  },
  {
    "text": "okay firstly there's do it as though there's a flag green going on and it's",
    "start": "1385600",
    "end": "1391000"
  },
  {
    "text": "not just any flag read it's it's actually a very long flag great it's taking a long time and this is all",
    "start": "1391000",
    "end": "1396910"
  },
  {
    "text": "happening in the the per RPC hot path so this is kind of sketchy because flag reads probably aren't going to change",
    "start": "1396910",
    "end": "1402060"
  },
  {
    "text": "over the life of the by an area this is something that could be cached somewhere other good answers would be all the",
    "start": "1402060",
    "end": "1408400"
  },
  {
    "text": "string manipulation stuff going on over here again probably not needed for something to like happen every time the",
    "start": "1408400",
    "end": "1415720"
  },
  {
    "text": "RPC goes through and there's a mutex and that one I mean maybe could be replaced for the tommix but that's a little bit",
    "start": "1415720",
    "end": "1422650"
  },
  {
    "text": "but mainly was the flag read this was a quick quick fix we saw this data we went to the code we",
    "start": "1422650",
    "end": "1428020"
  },
  {
    "text": "saw the problem cache the flag reduce the overhead so this was a good a good story for that and so now I want to take",
    "start": "1428020",
    "end": "1435190"
  },
  {
    "text": "that crappy diagram again with some examples and I think it'll be more clear but we started with a very wide problem",
    "start": "1435190",
    "end": "1441690"
  },
  {
    "text": "optimize gr PC makes your PC faster that is so wide in scope that it's almost impossible to give to an engineer and",
    "start": "1441690",
    "end": "1448150"
  },
  {
    "text": "they'll do it so then we use the tooling and we got that a little bit more narrow in scope so then it became okay optimize",
    "start": "1448150",
    "end": "1454840"
  },
  {
    "text": "just the Apple filter like figure out why it's slow and optimize it and then again with some more tooling that was narrative and",
    "start": "1454840",
    "end": "1460820"
  },
  {
    "text": "so then the problems book became remove the per RPC flag grade in the Apple",
    "start": "1460820",
    "end": "1466519"
  },
  {
    "text": "filter and that is a very actionable like problem set to give to an engineer who can ask it on that in a week or so",
    "start": "1466519",
    "end": "1473710"
  },
  {
    "text": "at that point once the optimization is in place it comes time to show the",
    "start": "1473710",
    "end": "1478730"
  },
  {
    "text": "impact and so this can start with micro benchmarks which are narrow and say hey",
    "start": "1478730",
    "end": "1484129"
  },
  {
    "text": "this one operation of this one filter was at this number now it's at this number that's great and then the next",
    "start": "1484129",
    "end": "1490279"
  },
  {
    "text": "question is okay to this impact our synthetic benchmarks should this impact an actual scenario using gypsy API and",
    "start": "1490279",
    "end": "1495470"
  },
  {
    "text": "then to get even wider to this impact applications using geodesy and all that",
    "start": "1495470",
    "end": "1500480"
  },
  {
    "text": "comes together package with the data and the code change and that that becomes a very clear consistent story for progressing and making optimizations to",
    "start": "1500480",
    "end": "1507409"
  },
  {
    "text": "the code base so the last point I want to make in this section on breaking down",
    "start": "1507409",
    "end": "1514399"
  },
  {
    "text": "the layer in a section on tuning libraries is breaking down the layers and so this means never thinking that",
    "start": "1514399",
    "end": "1522200"
  },
  {
    "text": "you've hit like the bottom point of what you can do when you hit the bottom layer of your API so the Java team has made a",
    "start": "1522200",
    "end": "1529159"
  },
  {
    "text": "lot of contributions to Nettie and okay HTTP because they said atop that library overhead in that library will be a it's",
    "start": "1529159",
    "end": "1535999"
  },
  {
    "text": "attributed to the RPC layer and if there's things that can be faster between them that's great similarly go",
    "start": "1535999",
    "end": "1541490"
  },
  {
    "text": "has made some contributions to X Net HTTP library and in Google the C team",
    "start": "1541490",
    "end": "1547190"
  },
  {
    "text": "has worked extensively with Google's tcp team to tune some of the tcp options to make it more more optimized for the RPC",
    "start": "1547190",
    "end": "1553909"
  },
  {
    "text": "use case so you can always go deeper you can always go lower well maybe not always but you can",
    "start": "1553909",
    "end": "1559549"
  },
  {
    "text": "definitely go lower probably that's going deeper in the stack you can also rise up and go higher in the stack",
    "start": "1559549",
    "end": "1565100"
  },
  {
    "text": "that's gonna be kind of the next part of this talk is tuning things more in userland from the library perspective so",
    "start": "1565100",
    "end": "1574999"
  },
  {
    "text": "tuning gypsy applications tuning applications that use g RPC and",
    "start": "1574999",
    "end": "1580269"
  },
  {
    "text": "this part was a little impossible to keep language agnostic so I have kind of a slide I'm gonna run through that has",
    "start": "1580269",
    "end": "1585909"
  },
  {
    "text": "some language by language call outs but to start with like the obvious thing",
    "start": "1585909",
    "end": "1590990"
  },
  {
    "text": "which is always good to start with the obvious all language facts are going to benefit from reducing allocations reducing your copies using",
    "start": "1590990",
    "end": "1598250"
  },
  {
    "text": "syscalls reducing contention how to figure out how to reduce those answer is",
    "start": "1598250",
    "end": "1603620"
  },
  {
    "text": "probably tooling in benchmarks bruising allocations is going to be especially important for the go stack and the Java",
    "start": "1603620",
    "end": "1609800"
  },
  {
    "text": "stack because by nature of being garbage collected there gonna be hurt way more by excessive allocations so specifically",
    "start": "1609800",
    "end": "1619610"
  },
  {
    "text": "I'm just gonna run through each language stack but the Java stack using the async API tuning thread pools Java stack has",
    "start": "1619610",
    "end": "1627580"
  },
  {
    "text": "some default thread pools default unbounded thread pools your use case",
    "start": "1627580",
    "end": "1633010"
  },
  {
    "text": "might make it more optimal to use like a fork/join pool or some fixed size pool",
    "start": "1633010",
    "end": "1639940"
  },
  {
    "text": "also tuning many direct memory that your java library gives hooks to actually",
    "start": "1639940",
    "end": "1645020"
  },
  {
    "text": "reach down into Nettie and toon some of things there and the last point is maybe consider using net EE pool which is",
    "start": "1645020",
    "end": "1651140"
  },
  {
    "text": "supported by gr PC or or KQ not the out-of-the-box solution is uses ni√±o and",
    "start": "1651140",
    "end": "1656840"
  },
  {
    "text": "these are all things like quick things to try things that I you know aren't going to be always the right solution or",
    "start": "1656840",
    "end": "1663770"
  },
  {
    "text": "else they would be the default but if you have good benchmarks in place trying them and then experimenting is a good",
    "start": "1663770",
    "end": "1669740"
  },
  {
    "text": "idea for the C++ stack this above all else use the async API which is way more",
    "start": "1669740",
    "end": "1677480"
  },
  {
    "text": "highly tuned than our sink API I don't have it in the text because it's pretty experimental but there is a new callback",
    "start": "1677480",
    "end": "1684230"
  },
  {
    "text": "based API coming to C++ it sits in the code base now it needs more tuning but",
    "start": "1684230",
    "end": "1689480"
  },
  {
    "text": "that's like what kind of like what's next look out for we've got a lot of feedback that our async API is not that",
    "start": "1689480",
    "end": "1696140"
  },
  {
    "text": "easy to use and so this is kind of an effort to fix that and make it easier to write high-performance li+ plus G or C",
    "start": "1696140",
    "end": "1702380"
  },
  {
    "text": "applications for suppose plus tuning the threading model and tuning the number of completion cues is always gonna be",
    "start": "1702380",
    "end": "1709700"
  },
  {
    "text": "important like based on how you're using the library so how many threads do you have driving the completion queues",
    "start": "1709700",
    "end": "1715760"
  },
  {
    "text": "calling c.q next and how many actually queues you have yourself and then how",
    "start": "1715760",
    "end": "1721520"
  },
  {
    "text": "many as 10 are pcs can be in flight that's also gonna be specifically application but a number that can have a",
    "start": "1721520",
    "end": "1726890"
  },
  {
    "text": "lot of impact on for the go stack parallelizing with more go routines while at the same time not",
    "start": "1726890",
    "end": "1734119"
  },
  {
    "text": "creating unnecessary goroutines is going to be important doing the read/write buffer size and",
    "start": "1734119",
    "end": "1740109"
  },
  {
    "text": "also number of outstanding our PCs and",
    "start": "1740109",
    "end": "1745269"
  },
  {
    "text": "of course throw tooling benchmarks and data at these problems going to run",
    "start": "1745269",
    "end": "1754009"
  },
  {
    "text": "through a case study from the application side now so I distributed tensor flow needs a network layer this",
    "start": "1754009",
    "end": "1759440"
  },
  {
    "text": "involves many tensor flow processes running on different machines in the open source they use G RPC at some point",
    "start": "1759440",
    "end": "1766700"
  },
  {
    "text": "it became a goal and a project to improve attention flow over G or PC performance this was kind of a cross",
    "start": "1766700",
    "end": "1772580"
  },
  {
    "text": "team effort between our team and their team so the first step was to learn some",
    "start": "1772580",
    "end": "1778820"
  },
  {
    "text": "of Thames flows tooling this is some of the times flow logging system which is in their open source repo and this is a",
    "start": "1778820",
    "end": "1784249"
  },
  {
    "text": "visualization of that tooling this is showing actual communication between two GPUs that are going to the network the",
    "start": "1784249",
    "end": "1791179"
  },
  {
    "text": "purple rectangles show our pcs that are happening and we can see their lengths and we can see that they're kind of stacking up on each other and being very",
    "start": "1791179",
    "end": "1798409"
  },
  {
    "text": "uneven and this is like not really doesn't good you kind of want to see consistent performance across program",
    "start": "1798409",
    "end": "1803840"
  },
  {
    "text": "execution so with new tooling under our belt the next step was to go through",
    "start": "1803840",
    "end": "1810950"
  },
  {
    "text": "some benchmarks I already mentioned RPC bench from our perspective from govt",
    "start": "1810950",
    "end": "1816379"
  },
  {
    "text": "seed libraries perspective RPC bench was an application benchmark because it was using the tential API is not gypsies ApS",
    "start": "1816379",
    "end": "1823039"
  },
  {
    "text": "from tetra flows team RPC bench was a synthetic benchmark because this is a program that they've written with their",
    "start": "1823039",
    "end": "1828859"
  },
  {
    "text": "own api is for the purpose of benchmarking so like what one team's application benchmark is another team's",
    "start": "1828859",
    "end": "1834590"
  },
  {
    "text": "synthetic benchmark is so we use that but then we also went even higher and we",
    "start": "1834590",
    "end": "1839840"
  },
  {
    "text": "we use kind of what from tensor flows team would be an application layer benchmark we use benchmarks that people",
    "start": "1839840",
    "end": "1846529"
  },
  {
    "text": "had written using the tension flow api's themselves so this was real attention flow training tasks and I've kind of",
    "start": "1846529",
    "end": "1853309"
  },
  {
    "text": "drawn out what the typography looks like they are they they have n workers maybe 300 maybe 500",
    "start": "1853309",
    "end": "1859160"
  },
  {
    "text": "and then some probably odd number of parameter servers and this would look familiar if anyone's familiar with the",
    "start": "1859160",
    "end": "1864770"
  },
  {
    "text": "distributed tension flow model but this is different than our percy bench and this is good because it exercises things",
    "start": "1864770",
    "end": "1869960"
  },
  {
    "text": "in different ways the parameter servers are going to be way more bottlenecks and that was the case in certain cases that",
    "start": "1869960",
    "end": "1875240"
  },
  {
    "text": "that the parameter servers were kind of the slow point for this execution and so",
    "start": "1875240",
    "end": "1880970"
  },
  {
    "text": "that needed some better threading to support kind of like overloaded parameter servers and also putting",
    "start": "1880970",
    "end": "1887060"
  },
  {
    "text": "serialization in a thread pool instead of the the thread that was doing do you ever see network work was very I've linked to those changes if you're",
    "start": "1887060",
    "end": "1892850"
  },
  {
    "text": "curious but again this is just about using new benchmarks that exercise the stack and new in different ways in more",
    "start": "1892850",
    "end": "1899450"
  },
  {
    "text": "realistic ways is always gonna be a good thing one of the last points I want to",
    "start": "1899450",
    "end": "1906950"
  },
  {
    "text": "make breaking up the layers again from the application side you can break down the layers and go lower by digging into",
    "start": "1906950",
    "end": "1913100"
  },
  {
    "text": "the G RPC library we're always happy for contributions that improve our performance and by contributions I don't",
    "start": "1913100",
    "end": "1921020"
  },
  {
    "text": "just mean code contributions this could also come in the form of very",
    "start": "1921020",
    "end": "1926270"
  },
  {
    "text": "informative bug reports but with those focus on tooling and benchmarks you know",
    "start": "1926270",
    "end": "1932330"
  },
  {
    "text": "a bug report that mentions GRC performance not being up to par but it's",
    "start": "1932330",
    "end": "1937640"
  },
  {
    "text": "not actionable is not super helpful but one that comes with here's some tooling here's some data here's some diagrams we",
    "start": "1937640",
    "end": "1943310"
  },
  {
    "text": "found here's an exact benchmark that you can run on your machine then it become something that like we can engage with",
    "start": "1943310",
    "end": "1949130"
  },
  {
    "text": "and and it's actionable and so yeah we love contributions from open source",
    "start": "1949130",
    "end": "1955780"
  },
  {
    "text": "that's all I have again my name is no isin links to Juba Zoo code and to",
    "start": "1955780",
    "end": "1962870"
  },
  {
    "text": "personal stuff for their gypsy is hiring if you were interested at all in any language stacks come talk to me I have",
    "start": "1962870",
    "end": "1969560"
  },
  {
    "text": "an email for you and that's it open it up two minutes really for questions",
    "start": "1969560",
    "end": "1977290"
  },
  {
    "text": "[Applause]",
    "start": "1983040",
    "end": "1986210"
  },
  {
    "text": "hmm definitely so the question was have",
    "start": "1988820",
    "end": "1998970"
  },
  {
    "text": "we ever run into a case where observing the program caused a degradation in performance absolutely that was that was",
    "start": "1998970",
    "end": "2005480"
  },
  {
    "text": "something that bit us with like some tracing tools which like as I mentioned can have higher overhead so some of",
    "start": "2005480",
    "end": "2010549"
  },
  {
    "text": "these yeah some of these toolings if they're enabled the actual performance",
    "start": "2010549",
    "end": "2015770"
  },
  {
    "text": "numbers get way out of whack because there may be logging or they're writing some in memory data structure doing",
    "start": "2015770",
    "end": "2020870"
  },
  {
    "text": "something slow yeah definitely and that's something like if it try to understand if a tooling has that",
    "start": "2020870",
    "end": "2026570"
  },
  {
    "text": "weakness and then just understand it and then it can still be useful mm-hmm",
    "start": "2026570",
    "end": "2037419"
  },
  {
    "text": "yes",
    "start": "2041450",
    "end": "2044450"
  },
  {
    "text": "yeah that's a good that's a good question that's gonna I so caveat I'm",
    "start": "2051609",
    "end": "2057289"
  },
  {
    "text": "not a go expert more of a C++ sack person but I think that comes in using",
    "start": "2057289",
    "end": "2063049"
  },
  {
    "text": "goroutines to paralyze more work so maybe using a separate go routine to do the serialization whereas separate and",
    "start": "2063049",
    "end": "2069230"
  },
  {
    "text": "then a different one to do the interaction with the GBC API can be something you look into but yes that is",
    "start": "2069230",
    "end": "2079490"
  },
  {
    "text": "I'm not totally sure having not had too much familiarity with go",
    "start": "2079490",
    "end": "2084730"
  },
  {
    "text": "mm-hmm",
    "start": "2094169",
    "end": "2097169"
  },
  {
    "text": "so spot-check we're here but I think the answer is not yet",
    "start": "2113490",
    "end": "2120130"
  },
  {
    "text": "right now the traffic does go through a translation step I think eventually the",
    "start": "2120130",
    "end": "2125560"
  },
  {
    "text": "goal of like I mean this was one of the goals of buildings you ever see in the open-source and using it internally was for that communication to happen with",
    "start": "2125560",
    "end": "2131770"
  },
  {
    "text": "less translation I don't know right now",
    "start": "2131770",
    "end": "2136930"
  },
  {
    "text": "if there's any like like direct communication gypsy client outside of Google going to inside but that is",
    "start": "2136930",
    "end": "2143020"
  },
  {
    "text": "definitely like a goal of the use case like that yeah Oh many translation steps",
    "start": "2143020",
    "end": "2148470"
  },
  {
    "text": "yeah so this I've got read some repeats and questions but this is asking about struck packing yes this is looking into",
    "start": "2156900",
    "end": "2163380"
  },
  {
    "text": "kind of like the internal struck the core uses like to represent a transport which are these huge huge trucks and",
    "start": "2163380",
    "end": "2169109"
  },
  {
    "text": "both doing better packing and also making sure that kind of like the hot the hot items in that struct occur at",
    "start": "2169109",
    "end": "2176160"
  },
  {
    "text": "the beginning of the cache line where it's going to be easier and that also things that will be touched together up",
    "start": "2176160",
    "end": "2181470"
  },
  {
    "text": "here in the same cache line so like you know if we know that these four variables are used for compression make",
    "start": "2181470",
    "end": "2187950"
  },
  {
    "text": "sure that they're right next to each other so that gets pulled in at the same time yeah",
    "start": "2187950",
    "end": "2193700"
  },
  {
    "text": "that is a great question I have no idea I'm not sure",
    "start": "2198820",
    "end": "2203849"
  },
  {
    "text": "nothing that I know that's like a pre-existing solution for but just I mean yeah anything that is hitting a hitting a",
    "start": "2210970",
    "end": "2218750"
  },
  {
    "text": "server in different way yeah anything configurable",
    "start": "2218750",
    "end": "2223900"
  },
  {
    "text": "um so for the synthetic oh right so sub-channels the question was very I should keep",
    "start": "2233110",
    "end": "2239230"
  },
  {
    "text": "remembering to repeat the question the question was about the benchmarks are they doing multiple sub channels or one sub channel or top-level channel for",
    "start": "2239230",
    "end": "2246760"
  },
  {
    "text": "everything I've talked about there's nothing that involved actual load balancing so like we never involved the",
    "start": "2246760",
    "end": "2253090"
  },
  {
    "text": "load balancing pathways of round-robin II over sub channels they were all doing one client channel equals one connection",
    "start": "2253090",
    "end": "2261220"
  },
  {
    "text": "underlying there are some cases where we're using multiple client channels which each have their own connection",
    "start": "2261220",
    "end": "2266800"
  },
  {
    "text": "over them but no that's a great point and that's kind of that is a miss coverage area right now for GOP C is",
    "start": "2266800",
    "end": "2271840"
  },
  {
    "text": "that those hot paths of if a given client channel has 100 sub channels load",
    "start": "2271840",
    "end": "2279850"
  },
  {
    "text": "we don't have many existing performance benchmarks over that",
    "start": "2279850",
    "end": "2284730"
  },
  {
    "text": "all right I think that's also about it for a time so thank you",
    "start": "2296260",
    "end": "2302890"
  },
  {
    "text": "[Applause]",
    "start": "2302890",
    "end": "2304960"
  }
]