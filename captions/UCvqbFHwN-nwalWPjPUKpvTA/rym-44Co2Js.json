[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "schedule on time so welcome to this session and we will give some",
    "start": "149",
    "end": "5670"
  },
  {
    "text": "introduction about how the schedule works in the juban alias i'm wave one",
    "start": "5670",
    "end": "11610"
  },
  {
    "text": "and from IBM as a freaking year and this is hey everyone my name is Ravi I work",
    "start": "11610",
    "end": "17400"
  },
  {
    "text": "at software engineer at Red Hat I focus on scheduling aspects of Kona is an open shirt so so first we had talked about",
    "start": "17400",
    "end": "26220"
  },
  {
    "start": "25000",
    "end": "25000"
  },
  {
    "text": "how scheduler works in the whole cabin Elliott picture and there's a ticker",
    "start": "26220",
    "end": "31380"
  },
  {
    "text": "very typical yes of flow so user create a deployment and the Department I",
    "start": "31380",
    "end": "37829"
  },
  {
    "text": "request the girls to a guess over API server does some basic authentication of a registration check and if that's good",
    "start": "37829",
    "end": "44520"
  },
  {
    "text": "it will position the pay object the deployment if you have two to the backend storage 83 then the Java API",
    "start": "44520",
    "end": "52860"
  },
  {
    "text": "server is down the after that controller manager as well as many other components",
    "start": "52860",
    "end": "58850"
  },
  {
    "text": "watch the API objects they are interesting for example catering manager",
    "start": "58850",
    "end": "64080"
  },
  {
    "text": "are interesting that diploma and Rebecca said so it gets an event there than your",
    "start": "64080",
    "end": "70860"
  },
  {
    "text": "diploma is created but it doesn't have any associate the Rivka set so it",
    "start": "70860",
    "end": "76890"
  },
  {
    "text": "creates one and once the Rivka cells gets created beside controller notice",
    "start": "76890",
    "end": "84869"
  },
  {
    "text": "the desired State for example for this department is record have that number",
    "start": "84869",
    "end": "91290"
  },
  {
    "text": "equals three but there's no pal yet so rep except controller pretty pass for",
    "start": "91290",
    "end": "96840"
  },
  {
    "text": "them then the surpass doesn't have one field set which is the no name because",
    "start": "96840",
    "end": "103649"
  },
  {
    "text": "this not job of container manager then it just creates three paths with the no",
    "start": "103649",
    "end": "111000"
  },
  {
    "text": "suspect s blank has their that by this death a tremendous jobs down then there",
    "start": "111000",
    "end": "117420"
  },
  {
    "text": "were the three paths in a system we now to the name no-name cell then it's the",
    "start": "117420",
    "end": "123479"
  },
  {
    "text": "scheduler going onto the stage it watched some the path as well as many",
    "start": "123479",
    "end": "128640"
  },
  {
    "text": "other related resources because it's neat only needs to know the overall view",
    "start": "128640",
    "end": "134970"
  },
  {
    "text": "of the whole clusters so that it can make the good decision to put the power",
    "start": "134970",
    "end": "140190"
  },
  {
    "text": "on to which now so if watch the part which doesn't have the no name set then does some internal",
    "start": "140190",
    "end": "147840"
  },
  {
    "text": "magic about I will goes detail into that piece so it has some internal magic",
    "start": "147840",
    "end": "154760"
  },
  {
    "text": "across the castle check the cluster which now is the best fit for the",
    "start": "154760",
    "end": "160620"
  },
  {
    "text": "incoming note sorry for the incoming pod then it will create a API request to set",
    "start": "160620",
    "end": "167489"
  },
  {
    "text": "the known name to that perfect note and this face we call the point then Podesta",
    "start": "167489",
    "end": "175230"
  },
  {
    "text": "the double of schedule is done then after that couplet we also watch on the",
    "start": "175230",
    "end": "181620"
  },
  {
    "text": "past which has the known name set to exactly is own know",
    "start": "181620",
    "end": "187680"
  },
  {
    "text": "so it only cares about which knows which part will then turn itself right then it",
    "start": "187680",
    "end": "193530"
  },
  {
    "text": "will invoke the underlying contain a runtime either Creole LED or darker to",
    "start": "193530",
    "end": "202019"
  },
  {
    "text": "spin up the real containers dang it we also do some",
    "start": "202019",
    "end": "207810"
  },
  {
    "text": "internal we can say internal group to watch the part to report the past that",
    "start": "207810",
    "end": "214049"
  },
  {
    "text": "has regulated API server so this is the basically the whole picture of what",
    "start": "214049",
    "end": "220340"
  },
  {
    "text": "happens underneath one user create a deployment then because we are scheduled",
    "start": "220340",
    "end": "226459"
  },
  {
    "text": "introductions so we focus on this piece let's take a look what's inside this",
    "start": "226459",
    "end": "231569"
  },
  {
    "text": "black box so internally basic there are",
    "start": "231569",
    "end": "237659"
  },
  {
    "start": "234000",
    "end": "234000"
  },
  {
    "text": "four big components inside schedule the first wise Informer well if you are",
    "start": "237659",
    "end": "244709"
  },
  {
    "text": "reading CDE or co2 controllers you may be familiar with this so this is the component that watch o all by page",
    "start": "244709",
    "end": "253110"
  },
  {
    "text": "objects you are interested and the ones API object your users interested get",
    "start": "253110",
    "end": "259079"
  },
  {
    "text": "updated a little aquarium you can vacation thank you do your thing so here",
    "start": "259079",
    "end": "264699"
  },
  {
    "text": "in schedule with these two things why is to update its internal cache so",
    "start": "264699",
    "end": "272530"
  },
  {
    "text": "that means we don't query the APS over",
    "start": "272530",
    "end": "277870"
  },
  {
    "text": "every time we need to decision instead we rely on this internal cache because",
    "start": "277870",
    "end": "283000"
  },
  {
    "text": "some because you cannot query the 80s over so many times because scheduling needs high volumes in your do you need",
    "start": "283000",
    "end": "290379"
  },
  {
    "text": "to do a lot of scheduling by it's running the second thing it does is",
    "start": "290379",
    "end": "295509"
  },
  {
    "text": "build some specific data structure if you are writing C audio controller you",
    "start": "295509",
    "end": "301150"
  },
  {
    "text": "may be just use the tools like what cue that the kind girl provides right but",
    "start": "301150",
    "end": "309400"
  },
  {
    "text": "schedule works a little different because you need to make the scheduling",
    "start": "309400",
    "end": "314710"
  },
  {
    "text": "decision very efficiently so it has to rely on some specific data structure so",
    "start": "314710",
    "end": "321009"
  },
  {
    "text": "make efficient decision so this is the first part informers second policy that is a cue so basically",
    "start": "321009",
    "end": "329889"
  },
  {
    "text": "you can think of Li it's just one cue by internally it has some sub kills like I could kill back off cue and schedule",
    "start": "329889",
    "end": "335650"
  },
  {
    "text": "kill so though and we had a feature",
    "start": "335650",
    "end": "341020"
  },
  {
    "text": "called preemption or priority JD 114 and before that we're just making a very",
    "start": "341020",
    "end": "347289"
  },
  {
    "text": "simple queue which is the FIFO queue for seniors first but after we introduced the priority we had to manage which part",
    "start": "347289",
    "end": "355509"
  },
  {
    "text": "has the higher priority right so internal you have the priority queue as",
    "start": "355509",
    "end": "361089"
  },
  {
    "text": "well as we need to ensure the fairness is not the case that the higher priority",
    "start": "361089",
    "end": "369069"
  },
  {
    "text": "public always can be reach high we need to sort of back off them to make them",
    "start": "369069",
    "end": "374979"
  },
  {
    "text": "sleep a little bit so that the lower part has the chances to be scheduled as",
    "start": "374979",
    "end": "380710"
  },
  {
    "text": "well so internally the kills job is maintaining the own schedule and scheduling path properly sorted in the",
    "start": "380710",
    "end": "389949"
  },
  {
    "text": "in the in the internal data structure then it also works",
    "start": "389949",
    "end": "395139"
  },
  {
    "text": "as the pub dispatcher every times pops up odd fries in internal priority queue",
    "start": "395139",
    "end": "403210"
  },
  {
    "text": "and the consumer is the main scheduling girl routine so once this gets apart the",
    "start": "403210",
    "end": "411069"
  },
  {
    "text": "main scheduler thing will do its internal algorithm and as well as logic",
    "start": "411069",
    "end": "417520"
  },
  {
    "text": "to decide whether there is some room for",
    "start": "417520",
    "end": "423189"
  },
  {
    "text": "the path to be laying down or there's no room then so to decision there is the a",
    "start": "423189",
    "end": "430120"
  },
  {
    "text": "and B here either put head back to the queue or post a pending request to the",
    "start": "430120",
    "end": "436330"
  },
  {
    "text": "API server which is a set the no name to the exactly no name and the first",
    "start": "436330",
    "end": "442539"
  },
  {
    "text": "component here is called bending here it's a very simple just bill routine but we do it in another goal which means ISM",
    "start": "442539",
    "end": "450909"
  },
  {
    "text": "another thread we did this is because we don't want the bonding API requests",
    "start": "450909",
    "end": "456789"
  },
  {
    "text": "blocking the main process because sometimes the bending takes time we don't want to that HTTP communication",
    "start": "456789",
    "end": "464080"
  },
  {
    "text": "pixel brought the main authority and by this step the bending phase we have some",
    "start": "464080",
    "end": "473169"
  },
  {
    "text": "internally called optimistic concurrency that means by this step in ApS over the bond maybe",
    "start": "473169",
    "end": "481389"
  },
  {
    "text": "hasn't finished yet but internally we assume the path has been assigned to the node so we update the internal cash and",
    "start": "481389",
    "end": "490649"
  },
  {
    "text": "but don't panic if it's felt the band pair failed we can get notification we",
    "start": "490649",
    "end": "496449"
  },
  {
    "text": "then invalidate the cache so this is the basic internal flow of the schedule so",
    "start": "496449",
    "end": "503740"
  },
  {
    "text": "next we are focused on the internal algorithm and the logic of how the main",
    "start": "503740",
    "end": "510789"
  },
  {
    "text": "schedule team gets apart and how it decides which the best of nerve is so",
    "start": "510789",
    "end": "518018"
  },
  {
    "start": "518000",
    "end": "518000"
  },
  {
    "text": "basically there are two phase in in that box one is called fear predicates also known",
    "start": "518019",
    "end": "523779"
  },
  {
    "text": "as filtering so they say you had to step it goes through some Devon predicates",
    "start": "523779",
    "end": "531920"
  },
  {
    "text": "the predicates you're sure it comes from the pot a pot API spec like how many",
    "start": "531920",
    "end": "537680"
  },
  {
    "text": "resources you are and what's the pot affinity is etc etc so internally we map",
    "start": "537680",
    "end": "543589"
  },
  {
    "text": "those 8's back to a different practice so eat so we go through every note and",
    "start": "543589",
    "end": "551110"
  },
  {
    "text": "get a filter result that passes all the predicates but if Nano is qualified then we will go",
    "start": "551110",
    "end": "559910"
  },
  {
    "text": "to a second try which is called preemption two for the higher priority",
    "start": "559910",
    "end": "565519"
  },
  {
    "text": "part it has a chance to prevent lower pre-empting priority pass to make room",
    "start": "565519",
    "end": "571760"
  },
  {
    "text": "for it so the little difference here is that if it enters the preemption phase",
    "start": "571760",
    "end": "579910"
  },
  {
    "text": "the result is that it only sets a speck of nominating note to the target node we want to eat",
    "start": "579910",
    "end": "587930"
  },
  {
    "text": "for landau and the preempt path then goes to next scheduled cycle because we",
    "start": "587930",
    "end": "594529"
  },
  {
    "text": "don't want to eat to be stay full without we wanted to stay less so that",
    "start": "594529",
    "end": "601819"
  },
  {
    "text": "it can better be managed in our internal logic so this is the phase one and phase",
    "start": "601819",
    "end": "608209"
  },
  {
    "text": "two is called priority that means we do have some nails which can fit for the",
    "start": "608209",
    "end": "614870"
  },
  {
    "start": "609000",
    "end": "609000"
  },
  {
    "text": "nail so the first tool called priority also known as scoring is that for each filter no we have internally have some",
    "start": "614870",
    "end": "624310"
  },
  {
    "text": "priorities and we give score plus its",
    "start": "624310",
    "end": "629899"
  },
  {
    "text": "weight to each node on each priority then we sum up the score and finally the",
    "start": "629899",
    "end": "637760"
  },
  {
    "text": "node with the highest score will be chosen then we go to the next phase which the bending phase can give some",
    "start": "637760",
    "end": "645740"
  },
  {
    "text": "example for example there is a priority called lists require says requested",
    "start": "645740",
    "end": "652130"
  },
  {
    "text": "which means we once the part comes in and there are three nails available right now we want to choose",
    "start": "652130",
    "end": "659269"
  },
  {
    "text": "the know that we les you're cute ization so we want to balance the workers but of",
    "start": "659269",
    "end": "664939"
  },
  {
    "text": "course you can choose the most requested to make the resource more bin packed onto the nails you are right those",
    "start": "664939",
    "end": "672980"
  },
  {
    "text": "different scenarios can use different strategy okay this is basic the",
    "start": "672980",
    "end": "680139"
  },
  {
    "text": "scheduling flow in the very high level so funding I don't need to mention penny",
    "start": "680139",
    "end": "687019"
  },
  {
    "text": "penny just send a request to ApS over okay and the next wheel if you do some",
    "start": "687019",
    "end": "692540"
  },
  {
    "start": "691000",
    "end": "691000"
  },
  {
    "text": "recent developments in the fall schedule so why is that the priority and",
    "start": "692540",
    "end": "698120"
  },
  {
    "text": "preemption has been g18 140 so that it's a default behavior that high priority",
    "start": "698120",
    "end": "704779"
  },
  {
    "text": "car high priority part can priam they're low priority low priority pass a second",
    "start": "704779",
    "end": "712160"
  },
  {
    "text": "wine is called we saw Skoda using scope selector so we sort coated a price to",
    "start": "712160",
    "end": "719120"
  },
  {
    "text": "the namespace and you can specify that the resource coder is apply to some",
    "start": "719120",
    "end": "726620"
  },
  {
    "text": "specific priority class this is what scopes like to come soon and along with",
    "start": "726620",
    "end": "732259"
  },
  {
    "text": "the GA we also remove the restriction that the system critical project cars",
    "start": "732259",
    "end": "738470"
  },
  {
    "text": "can only be applied to the cube system right now you can apply to any namespace you want the other going features I just",
    "start": "738470",
    "end": "747259"
  },
  {
    "text": "list some major features here why is the scheduling framework so as I know",
    "start": "747259",
    "end": "753189"
  },
  {
    "text": "extensibility is a very important party the whole kubernetes community and",
    "start": "753189",
    "end": "758439"
  },
  {
    "text": "schedule is also no exceptional and in the before we do have some experiments",
    "start": "758439",
    "end": "765350"
  },
  {
    "text": "mechanism but this time we want to move a step further so that's why we bring in",
    "start": "765350",
    "end": "772939"
  },
  {
    "text": "the schedule for work so basically we does some code refactoring to basically",
    "start": "772939",
    "end": "778189"
  },
  {
    "text": "expose every face inside schedule outside so that you can build your own",
    "start": "778189",
    "end": "785300"
  },
  {
    "text": "plugins and the combined which the deferred schedule to make it your own customized schedule",
    "start": "785300",
    "end": "791379"
  },
  {
    "text": "job needs so I so Abdullah will give more detail introduction to tomorrow's",
    "start": "791379",
    "end": "797229"
  },
  {
    "text": "dip-dye so right now the this feature has been offered in 115 and 116 we",
    "start": "797229",
    "end": "804249"
  },
  {
    "text": "completed it's like we are eating our own doctor we completed the code",
    "start": "804249",
    "end": "809919"
  },
  {
    "text": "migration from the all-star tour this framework plugging step the second one",
    "start": "809919",
    "end": "816099"
  },
  {
    "text": "is called even past breath in yesterday's keynote the wiki also",
    "start": "816099",
    "end": "821409"
  },
  {
    "text": "mentioned that this is one of the features the community Oscar for a long time they want to sort of control how",
    "start": "821409",
    "end": "829449"
  },
  {
    "text": "the path is spread across your cluster on different topology domains and after",
    "start": "829449",
    "end": "838059"
  },
  {
    "text": "that also will give some detail examples and third wise called its extended",
    "start": "838059",
    "end": "847139"
  },
  {
    "text": "priority on the on the requested to capita capacity ratio the motivation is",
    "start": "847139",
    "end": "853449"
  },
  {
    "text": "that for extended resources we don't provided the option see whether you want",
    "start": "853449",
    "end": "863319"
  },
  {
    "text": "to be impact behavior or the even part",
    "start": "863319",
    "end": "868659"
  },
  {
    "text": "even spread behavior for example for some GPU if there are two GPUs whether",
    "start": "868659",
    "end": "875229"
  },
  {
    "text": "you want the behaviors to use one as much as possible or just use them as",
    "start": "875229",
    "end": "880569"
  },
  {
    "text": "average as possible so this feature gives you the option you can provide your inputs okay so this is specially",
    "start": "880569",
    "end": "889659"
  },
  {
    "text": "the recent development of a handle to Ravi",
    "start": "889659",
    "end": "894899"
  },
  {
    "text": "thanks we so I think this slide should have been the starting slide but I want",
    "start": "897760",
    "end": "904519"
  },
  {
    "text": "to set the tone to next part of the presentation where I will give some updates on on the the projects that fall",
    "start": "904519",
    "end": "910160"
  },
  {
    "start": "907000",
    "end": "907000"
  },
  {
    "text": "under six scheduling purview so like to talk about the design rationale of scheduler like when we have started",
    "start": "910160",
    "end": "916070"
  },
  {
    "text": "designing what are the goals that we had in our mind the first one foremost thing is scheduler is not responsible for",
    "start": "916070",
    "end": "922699"
  },
  {
    "text": "managing the entire lifecycle of a pod so at a high level when the pod has been",
    "start": "922699",
    "end": "929720"
  },
  {
    "text": "bound to I know the binding stage is done schedulers job is is done it does not care what happens when the pod is in",
    "start": "929720",
    "end": "936589"
  },
  {
    "text": "running state so this is something that we have made as a choice consciously and",
    "start": "936589",
    "end": "941990"
  },
  {
    "text": "there are some side effects of that we'll talk about that in a bit and how the projects that we that we are going",
    "start": "941990",
    "end": "948410"
  },
  {
    "text": "to see are going to handle that so the second important thing that we have",
    "start": "948410",
    "end": "956180"
  },
  {
    "text": "thought about is the minimum scheduling unit is always going to be a part it's",
    "start": "956180",
    "end": "961310"
  },
  {
    "text": "not going to be a group of pods or job or some other entities so it's always the part that we are interested in in",
    "start": "961310",
    "end": "967430"
  },
  {
    "text": "scheduling and we scheduled one part at a time we also make sure that it's best",
    "start": "967430",
    "end": "973940"
  },
  {
    "text": "effort than we are choosing a node for a particular part we could have chosen a first fit approach like greedily",
    "start": "973940",
    "end": "980300"
  },
  {
    "text": "in the predicate stage itself we could have exited in the scheduler saying that we found a node that satisfies the resource requests for the part but we",
    "start": "980300",
    "end": "987470"
  },
  {
    "text": "did not do it we do not we wanted the scheduler to provide a best fit for the node that's why the pod is actually",
    "start": "987470",
    "end": "994100"
  },
  {
    "text": "going through the second stage which is the prioritization of the nodes from the list of nodes that were filtered in the",
    "start": "994100",
    "end": "1000459"
  },
  {
    "text": "first stage so that's that's something that we have made as a as a design",
    "start": "1000459",
    "end": "1006010"
  },
  {
    "text": "choice consciously and we also wanted to make it configurable like the end user",
    "start": "1006010",
    "end": "1012970"
  },
  {
    "text": "should be or the cluster admin should be able to pick and choose whatever the priorities or the predicates that he or",
    "start": "1012970",
    "end": "1018519"
  },
  {
    "text": "she wants for the cluster this is something that we that Kuban it is as a",
    "start": "1018519",
    "end": "1023829"
  },
  {
    "text": "whole as we explained is interested in in going towards and scheduler is no exception to that the other thing that",
    "start": "1023829",
    "end": "1030490"
  },
  {
    "text": "is of different for scheduler is making scheduler pluggable so previously we",
    "start": "1030490",
    "end": "1039640"
  },
  {
    "text": "used to have or even now we have scheduler extender which runs at various stages like you can choose scheduled",
    "start": "1039640",
    "end": "1046390"
  },
  {
    "text": "scheduler extender to be run at the predicates level or after the priorities level or after binding so we have gone",
    "start": "1046390",
    "end": "1055210"
  },
  {
    "text": "ahead and taken it to the next level where we have broken it down broken has broken the scheduling cycle down to",
    "start": "1055210",
    "end": "1061630"
  },
  {
    "text": "multiple stages like pre-filter post filter filter and then prioritizing",
    "start": "1061630",
    "end": "1067030"
  },
  {
    "text": "stage where we are ranking and and all that so for every phase we are going to",
    "start": "1067030",
    "end": "1072520"
  },
  {
    "text": "have extensions going forward and this is what Wei was explaining when he was",
    "start": "1072520",
    "end": "1077860"
  },
  {
    "text": "mentioning that we are eating our own dog food converting the existing predicates and priorities to this new",
    "start": "1077860",
    "end": "1084370"
  },
  {
    "text": "scheduling framework Abdullah is going to talk about that in in the deep dive tomorrow and the other important feature",
    "start": "1084370",
    "end": "1092049"
  },
  {
    "text": "is multiple schedulers like like as of now in Cuban it is you can specify",
    "start": "1092049",
    "end": "1099640"
  },
  {
    "text": "within the pod spec which scheduler you would like to you would like your pot to be scheduled by so that's something that",
    "start": "1099640",
    "end": "1106860"
  },
  {
    "text": "we have thought through and we thought that it would be good if we can make",
    "start": "1106860",
    "end": "1112570"
  },
  {
    "text": "sure this extensibility is available to the user so as at all if you go back to",
    "start": "1112570",
    "end": "1120460"
  },
  {
    "text": "the previous slide the first three points and the design rationale like scheduler is responsible for managing lifecycle the minimum scheduling unit is",
    "start": "1120460",
    "end": "1127030"
  },
  {
    "text": "pod and scheduled one part at a time there are some side effects of that the first thing is since scheduling is at",
    "start": "1127030",
    "end": "1136120"
  },
  {
    "start": "1134000",
    "end": "1134000"
  },
  {
    "text": "the Java schedule it has done once a pod has been bound to a node at runtime like",
    "start": "1136120",
    "end": "1142179"
  },
  {
    "text": "say when the pod is running there is a very good chance that a new label has been applied to the part or attaint has",
    "start": "1142179",
    "end": "1148179"
  },
  {
    "text": "been applied to a node or a Toleration has been removed or applied to the pod so in those cases the scheduler won't",
    "start": "1148179",
    "end": "1155860"
  },
  {
    "text": "reschedule the parts then they are in running state so we are need of an entity which can do that this is where d",
    "start": "1155860",
    "end": "1162309"
  },
  {
    "text": "scheduler comes into picture the other use case 4d scheduler is",
    "start": "1162309",
    "end": "1167429"
  },
  {
    "text": "ensuring that a cluster is evenly balanced so you can configure D",
    "start": "1167429",
    "end": "1172649"
  },
  {
    "text": "scheduler with some limits or the threshold saying that this is the lower threshold and this is the target",
    "start": "1172649",
    "end": "1178139"
  },
  {
    "text": "threshold like for example you can tell that I would like 30% and 70% as the",
    "start": "1178139",
    "end": "1183929"
  },
  {
    "text": "range of the utilization for every node in the cluster and when the scheduler is run with that particular strategy enable",
    "start": "1183929",
    "end": "1190950"
  },
  {
    "text": "it ensures that all the nodes in the cluster are falling within that range so",
    "start": "1190950",
    "end": "1196169"
  },
  {
    "text": "this helps in solving the fragmentation issues in the cluster so this is another use case for D scheduler how does this",
    "start": "1196169",
    "end": "1203309"
  },
  {
    "text": "work so D scheduler at a high level is is an a victor meaning it just evicts",
    "start": "1203309",
    "end": "1209759"
  },
  {
    "text": "the parts that are not conforming to the scheduling decisions anymore and when",
    "start": "1209759",
    "end": "1215220"
  },
  {
    "text": "the pod gets evicted as you know there are higher level controllers that are responsible for spinning up or",
    "start": "1215220",
    "end": "1221789"
  },
  {
    "text": "recreating those spots and when out a new pod gets recreated it goes through the scheduling cycle again but no where",
    "start": "1221789",
    "end": "1227879"
  },
  {
    "text": "D scheduler influences the scheduling cycle by saying by adding a label to the",
    "start": "1227879",
    "end": "1233190"
  },
  {
    "text": "part that is evicting or by adding the taint to a node or or something like that so this scheduler is just an effector",
    "start": "1233190",
    "end": "1242029"
  },
  {
    "start": "1242000",
    "end": "1242000"
  },
  {
    "text": "some of the strategies that D scheduler has include remove duplicate parts this",
    "start": "1242029",
    "end": "1249480"
  },
  {
    "text": "is this kind of helpful if you have multiple replicas of a replica set or deployment running on the same node it",
    "start": "1249480",
    "end": "1256859"
  },
  {
    "text": "makes sure that there is only one copy of that collection running on the node and evicts the rest of them and the lone",
    "start": "1256859",
    "end": "1264299"
  },
  {
    "text": "odd utilization is the same example that I was talking about earlier where you can configure the lower thresholds and",
    "start": "1264299",
    "end": "1270149"
  },
  {
    "text": "the target thresholds and ensure that all the nodes are falling within the range within a cluster remove pods while",
    "start": "1270149",
    "end": "1277049"
  },
  {
    "text": "eating inter pod anti affinity and node affinity both of them are similar to what I was explaining earlier where we",
    "start": "1277049",
    "end": "1283619"
  },
  {
    "text": "have you have applied the pods sorry the labels to the pods or remote panes from",
    "start": "1283619",
    "end": "1289559"
  },
  {
    "text": "the nodes or added panes from the node so these are the strategies the D scheduler supports we are also working",
    "start": "1289559",
    "end": "1295080"
  },
  {
    "text": "on other strategies like when we are evicting we will ensure",
    "start": "1295080",
    "end": "1300690"
  },
  {
    "start": "1298000",
    "end": "1298000"
  },
  {
    "text": "that certain types of pots are not evicted for example critical ports static ports the diamond set parts and",
    "start": "1300690",
    "end": "1307350"
  },
  {
    "text": "parts with local storage we also ensure that the ture steers are respected when",
    "start": "1307350",
    "end": "1314250"
  },
  {
    "text": "we are evicting the pots like for example when we are considering a node",
    "start": "1314250",
    "end": "1319890"
  },
  {
    "text": "we would ensure that first the best effort pots are evicted then burstable and then guaranteed and if you notice",
    "start": "1319890",
    "end": "1327540"
  },
  {
    "text": "that a parts pdb power disruption budget would get violated when we are awaiting we would not go and if those kind of",
    "start": "1327540",
    "end": "1334710"
  },
  {
    "text": "parts so that's that's what the scheduler does now I will talk a bit",
    "start": "1334710",
    "end": "1340650"
  },
  {
    "start": "1337000",
    "end": "1337000"
  },
  {
    "text": "about Q batch so if you remember I was mentioning about three different aspects",
    "start": "1340650",
    "end": "1346260"
  },
  {
    "text": "of scheduler 1sv schedule one part at a time the basic scheduling entity is spot",
    "start": "1346260",
    "end": "1352260"
  },
  {
    "text": "so we schedule one part at a time but there are scenarios where you would like",
    "start": "1352260",
    "end": "1358200"
  },
  {
    "text": "to schedule five parts or ten parts at a time so the default scheduler cannot handle those scenarios currently that's",
    "start": "1358200",
    "end": "1364740"
  },
  {
    "text": "where Q batch comes into picture it ensures that say if you if you tell that I would like all these five parts",
    "start": "1364740",
    "end": "1371130"
  },
  {
    "text": "to be scheduled you can create a custom resource definition which tells that this is a pod group and we need ten pots",
    "start": "1371130",
    "end": "1377640"
  },
  {
    "text": "to be running and all of them would be scheduled at the same time this is kind of helpful for machine learning jobs and",
    "start": "1377640",
    "end": "1385170"
  },
  {
    "text": "there are like projects like tensorflow apache spark and all those projects that are using cube batch this also falls",
    "start": "1385170",
    "end": "1391500"
  },
  {
    "text": "under six scheduling purview some of the features of cube batch are like Co",
    "start": "1391500",
    "end": "1397680"
  },
  {
    "start": "1396000",
    "end": "1396000"
  },
  {
    "text": "scheduling fair scheduling predicates tasks priam preemption and then task",
    "start": "1397680",
    "end": "1403080"
  },
  {
    "text": "priority so the thing that is kind of",
    "start": "1403080",
    "end": "1408240"
  },
  {
    "text": "important for most of the machine learning jobs is Co scheduling it previously used to be called gang",
    "start": "1408240",
    "end": "1413640"
  },
  {
    "text": "scheduling we have changed it changed its name to Co scheduling and this is where Q batch helps in the next project",
    "start": "1413640",
    "end": "1422550"
  },
  {
    "text": "that also falls under six scheduling purview is Poseidon Poseidon is",
    "start": "1422550",
    "end": "1431140"
  },
  {
    "text": "a network flow based scheduler the main advantage of using Poseidon when",
    "start": "1431140",
    "end": "1436690"
  },
  {
    "text": "compared to default scheduler is again it comes back to back scheduling it",
    "start": "1436690",
    "end": "1442270"
  },
  {
    "text": "inherently is a batch scheduler and this takes the basket leading to a step further like instead of taking ten parts",
    "start": "1442270",
    "end": "1449950"
  },
  {
    "text": "or five parts at a time it continuously reevaluate s-- and shuffles the pods or",
    "start": "1449950",
    "end": "1456220"
  },
  {
    "text": "all the parts within the cluster to ensure that all of them are optimally placed so one of the things that this",
    "start": "1456220",
    "end": "1465220"
  },
  {
    "text": "helps in is again coming back or going back to what I explained earlier batch processing we are also in need of an",
    "start": "1465220",
    "end": "1471190"
  },
  {
    "text": "entity that watches the cluster and then say if a label has been applied or if a",
    "start": "1471190",
    "end": "1476230"
  },
  {
    "text": "label has been removed or retained has been applied all those kind of things are not taken into account by de schedule sorry scheduler and we need",
    "start": "1476230",
    "end": "1482980"
  },
  {
    "text": "external entity like D scheduler to help us with those scenarios now with",
    "start": "1482980",
    "end": "1488530"
  },
  {
    "text": "Poseidon since it does a continuous reevaluation and rescheduling we would not need that entity like D scheduler if",
    "start": "1488530",
    "end": "1495669"
  },
  {
    "text": "we are using Poseidon some of the features that Poseidon has are very much",
    "start": "1495669",
    "end": "1502179"
  },
  {
    "start": "1500000",
    "end": "1500000"
  },
  {
    "text": "similar to default scheduler like node affinity anti affinity teens and",
    "start": "1502179",
    "end": "1507820"
  },
  {
    "text": "toleration and Co scheduling but it's not feature-rich enough at this point like it's not on par with default",
    "start": "1507820",
    "end": "1514390"
  },
  {
    "text": "scheduler to replace it so this is all of the projects that are falling in",
    "start": "1514390",
    "end": "1521679"
  },
  {
    "text": "scheduling per view there are like other projects like cluster capacity analysis and other projects they are also under",
    "start": "1521679",
    "end": "1529510"
  },
  {
    "text": "six scheduling per view so one of the things that people frequently ask us",
    "start": "1529510",
    "end": "1535929"
  },
  {
    "text": "about is how do those schedule let's work and the answer is we do not know yet this is something that we are",
    "start": "1535929",
    "end": "1542049"
  },
  {
    "text": "working on like how can we ensure that we can bring in batch processing capabilities from cube batch or Poseidon",
    "start": "1542049",
    "end": "1547210"
  },
  {
    "text": "using the new scheduling framework that we have written like can we use those extinction points to call cube batch or",
    "start": "1547210",
    "end": "1553120"
  },
  {
    "text": "get the code from cube batch to ensure that both batch processing and default",
    "start": "1553120",
    "end": "1559150"
  },
  {
    "text": "scheduler the schedulers that are capable of providing bad scheduling and the default scheduler can coexist",
    "start": "1559150",
    "end": "1565070"
  },
  {
    "text": "there's something that we are working on so if you have any questions or suggestions please feel free to contact",
    "start": "1565070",
    "end": "1571880"
  },
  {
    "text": "us that is the end of our presentation if you have any questions please feel",
    "start": "1571880",
    "end": "1577039"
  },
  {
    "text": "free to ask sure",
    "start": "1577039",
    "end": "1582398"
  },
  {
    "text": "they would elect answer so it's not only",
    "start": "1608179",
    "end": "1614250"
  },
  {
    "text": "a scheduling order also the wrong time order even if scheduling is schedule schedule",
    "start": "1614250",
    "end": "1619500"
  },
  {
    "text": "them on today so I think it's sort of",
    "start": "1619500",
    "end": "1626270"
  },
  {
    "text": "more my personal making it's more good fit for your specified order into a path",
    "start": "1626270",
    "end": "1632429"
  },
  {
    "text": "rather than different paths because in scheduling is perspective it's just",
    "start": "1632429",
    "end": "1638160"
  },
  {
    "text": "schedule Papapa Papapa and once the three paths you mentioned cascade road Dan schedules job is down",
    "start": "1638160",
    "end": "1644220"
  },
  {
    "text": "by in run time is also the case is a cat and the red and random ha to be executed",
    "start": "1644220",
    "end": "1651929"
  },
  {
    "text": "right so yeah just to add to what we way",
    "start": "1651929",
    "end": "1657150"
  },
  {
    "text": "was feeling like it's it's mostly at the cubelet level eggs if you look at the cubelet preemption logic that we have",
    "start": "1657150",
    "end": "1663030"
  },
  {
    "text": "currently it takes into account the priority of the pots but when it is once",
    "start": "1663030",
    "end": "1668760"
  },
  {
    "text": "the pods are in running state or one support lands on to the north during admission we do not have this concept of",
    "start": "1668760",
    "end": "1674790"
  },
  {
    "text": "taking priority into consideration and to be said that does not fall under schedulers realm as such right because",
    "start": "1674790",
    "end": "1682410"
  },
  {
    "text": "we have told that this part needs to be scheduled first when compared to other part and then it's the cubelets job to",
    "start": "1682410",
    "end": "1687960"
  },
  {
    "text": "identify like if there are five parts that needs to be admitted it needs to find out which part is of higher",
    "start": "1687960",
    "end": "1694500"
  },
  {
    "text": "priority usually even if there is another part that has like come up first",
    "start": "1694500",
    "end": "1699960"
  },
  {
    "text": "and there is no resource for higher priority part cubelet thus preemption or it it's called qubit preemption I",
    "start": "1699960",
    "end": "1706919"
  },
  {
    "text": "believe but there is a feature which ensures that eventually we may reach a stage where this pod would would come",
    "start": "1706919",
    "end": "1713070"
  },
  {
    "text": "into running when there are no resources does that answer your question",
    "start": "1713070",
    "end": "1718600"
  },
  {
    "text": "okay",
    "start": "1718600",
    "end": "1721600"
  },
  {
    "text": "between yeah so when we started out we always",
    "start": "1725360",
    "end": "1732270"
  },
  {
    "text": "wanted to schedule one part at a time but it may not be the case like for example you may need to have or you may",
    "start": "1732270",
    "end": "1739440"
  },
  {
    "text": "have a scenario where you you have a very big data set and you would like to process it parallely like for example if",
    "start": "1739440",
    "end": "1746549"
  },
  {
    "text": "you have a training data set you would like to process it parallely and you want to ensure that all the parts have",
    "start": "1746549",
    "end": "1752520"
  },
  {
    "text": "come to running straight while the processing is happening before we move",
    "start": "1752520",
    "end": "1759090"
  },
  {
    "text": "on to the actual job execution you would like to ensure that the processing has",
    "start": "1759090",
    "end": "1764789"
  },
  {
    "text": "happened and all the parts that do the processing are coming to running State like as a group instead of one part and",
    "start": "1764789",
    "end": "1771809"
  },
  {
    "text": "then second part and then the third part so this is where batch processing would help in like you can take ten parts all",
    "start": "1771809",
    "end": "1778529"
  },
  {
    "text": "of them would come to running state or none of them would come to running state that is one example which is called gang",
    "start": "1778529",
    "end": "1783570"
  },
  {
    "text": "scheduling but batch is like taking a group of parts and then scheduling all of them together to add something is",
    "start": "1783570",
    "end": "1792899"
  },
  {
    "text": "that the four scheduler is not a controller or resource manager so it",
    "start": "1792899",
    "end": "1800220"
  },
  {
    "text": "doesn't reset possible for spinner past or destroy a pass so comparing some",
    "start": "1800220",
    "end": "1808010"
  },
  {
    "text": "other scheduler so if we want to bring the best capacity into schedule one one",
    "start": "1808010",
    "end": "1816179"
  },
  {
    "text": "one option is to call implement equivalence cache so that we know once",
    "start": "1816179",
    "end": "1822000"
  },
  {
    "text": "we did the scheduling calculation for the first part we can do reuse the",
    "start": "1822000",
    "end": "1827399"
  },
  {
    "text": "result of father for other part so that can make it efficient this is one option but we try kervins cache but it does",
    "start": "1827399",
    "end": "1834210"
  },
  {
    "text": "work as performant as we supposed to be a second option is that the schedule",
    "start": "1834210",
    "end": "1841350"
  },
  {
    "text": "also work has the resource manager but it's not a case for the for schedule so we want to see if other ways we can do",
    "start": "1841350",
    "end": "1848700"
  },
  {
    "text": "this so that the scheduler can schedule all them in one time rather than scheduling one by one",
    "start": "1848700",
    "end": "1855010"
  },
  {
    "text": "yeah for sure for sure of you express",
    "start": "1855010",
    "end": "1870480"
  },
  {
    "text": "its personal purpose I'm sorry",
    "start": "1870480",
    "end": "1877730"
  },
  {
    "text": "so some some problems in the badge area",
    "start": "1894620",
    "end": "1899670"
  },
  {
    "text": "a knob proper model in the different schedule",
    "start": "1899670",
    "end": "1904950"
  },
  {
    "text": "so some some some problem I actually not only a scheduling problem for example",
    "start": "1904950",
    "end": "1910590"
  },
  {
    "text": "fair share or some resource sharing there might be more need to control that",
    "start": "1910590",
    "end": "1917040"
  },
  {
    "text": "the admission time right for example my need some additional control prism if",
    "start": "1917040",
    "end": "1926250"
  },
  {
    "text": "you want to make his church so that you need to make sure u n stand different",
    "start": "1926250",
    "end": "1931560"
  },
  {
    "text": "either namespace or different what kind of pub and part the relationship right but that is why we",
    "start": "1931560",
    "end": "1940590"
  },
  {
    "text": "need some impose some extra capabilities instead of only focus on the power pod",
    "start": "1940590",
    "end": "1948300"
  },
  {
    "text": "cycle so we are trying to see whether the scheduling for work can happen us on",
    "start": "1948300",
    "end": "1954870"
  },
  {
    "text": "that and this is otherwise you have to discuss stuff from scratch to Bill to",
    "start": "1954870",
    "end": "1959940"
  },
  {
    "text": "another schedule only for their specific concert of what you need so that's what",
    "start": "1959940",
    "end": "1968070"
  },
  {
    "text": "patches a complex problem live Jay",
    "start": "1968070",
    "end": "1972500"
  },
  {
    "text": "yeah go ahead",
    "start": "1976180",
    "end": "1979080"
  },
  {
    "text": "yeah the current throughput that is documented is 100 pots per second that's",
    "start": "1998290",
    "end": "2003990"
  },
  {
    "text": "the throughput or the SLA that we support we are trying to provide or work",
    "start": "2003990",
    "end": "2011040"
  },
  {
    "text": "on the improvements like in 114 or 115 I believe like we have improved the",
    "start": "2011040",
    "end": "2017610"
  },
  {
    "text": "performance of POD anti affinity by 20x as of now we don't see that scheduler is the bottleneck in the entire lifecycle",
    "start": "2017610",
    "end": "2025830"
  },
  {
    "text": "of pot but if we think or if it reaches stage where scheduler is actually causing those kind of problems we would",
    "start": "2025830",
    "end": "2032760"
  },
  {
    "text": "consider working on that the basic",
    "start": "2032760",
    "end": "2038130"
  },
  {
    "text": "answer is we are continuously improving the performance so very for very basic",
    "start": "2038130",
    "end": "2043320"
  },
  {
    "text": "workflow we have a which we have already reached 100 per second of 5000 real GCE",
    "start": "2043320",
    "end": "2053090"
  },
  {
    "text": "clusters and in terms of some complex or",
    "start": "2053090",
    "end": "2058710"
  },
  {
    "text": "close especially the pot affinity part anti affinity now it's invertible to to",
    "start": "2058710",
    "end": "2067138"
  },
  {
    "text": "do some heavy calculation because you for example for para affinity which is",
    "start": "2067139",
    "end": "2072210"
  },
  {
    "text": "check where the past can be coexist right so that it's not only check in the",
    "start": "2072210",
    "end": "2077550"
  },
  {
    "text": "past on the same on this node but also check across the cluster so it's invertible slower so you must to be",
    "start": "2077550",
    "end": "2085230"
  },
  {
    "text": "assured that but we are continually improving the data structure using",
    "start": "2085230",
    "end": "2090600"
  },
  {
    "text": "proper benchmark to to improve on that",
    "start": "2090600",
    "end": "2096440"
  },
  {
    "text": "using multiple schedule you me isn't using multiple schedulers or multiple",
    "start": "2103109",
    "end": "2108910"
  },
  {
    "text": "parts in a single scheduling cycle no",
    "start": "2108910",
    "end": "2115450"
  },
  {
    "text": "that has no authority that's that's not in but we are trying to have like we are",
    "start": "2115450",
    "end": "2124660"
  },
  {
    "text": "trying to see if we can fit in other schedulers into default scheduler like using the extinction mechanism that we",
    "start": "2124660",
    "end": "2130569"
  },
  {
    "text": "are talking about but we haven't decided like what the approach will be yet it's something that we are going to work on",
    "start": "2130569",
    "end": "2135999"
  },
  {
    "text": "and 118 yeah I think we just have like",
    "start": "2135999",
    "end": "2143259"
  },
  {
    "text": "last question perhaps No",
    "start": "2143259",
    "end": "2146970"
  },
  {
    "text": "that's not going to change at all the apex rendered would be my guess I can give a more detailed explanation yes",
    "start": "2159180",
    "end": "2167350"
  },
  {
    "text": "discounts answer is we we will keep them in paranoia yes so we want with their support D",
    "start": "2167350",
    "end": "2177360"
  },
  {
    "text": "sophisticated uh so basically does",
    "start": "2177360",
    "end": "2183580"
  },
  {
    "text": "accept our extender works is that you have this data external process which is",
    "start": "2183580",
    "end": "2189310"
  },
  {
    "text": "the ATP server so that we expose several stage for you for the first schedule to",
    "start": "2189310",
    "end": "2195910"
  },
  {
    "text": "interact with your extender the extent is more webhook but that means the",
    "start": "2195910",
    "end": "2204360"
  },
  {
    "text": "communication and the Michoud the measuring well takes a lot of resources especially if you have a fat 5,000 nose",
    "start": "2204360",
    "end": "2212310"
  },
  {
    "text": "that has a lot of you know they'd have to transfer and maturity mushroom so",
    "start": "2212310",
    "end": "2217840"
  },
  {
    "text": "there is the limitation of the scheduling but for the framework we're",
    "start": "2217840",
    "end": "2223150"
  },
  {
    "text": "doing another approach well we are still exposing this point this different face",
    "start": "2223150",
    "end": "2231220"
  },
  {
    "text": "but is that we want you to implement the plugins and we provide a very simple",
    "start": "2231220",
    "end": "2239460"
  },
  {
    "text": "interface for your vendor so your vendor given at his slash packages schedules",
    "start": "2239460",
    "end": "2246010"
  },
  {
    "text": "baba and you can now compile your binary so all the communication works in the",
    "start": "2246010",
    "end": "2252550"
  },
  {
    "text": "process so no need to communicate with external things",
    "start": "2252550",
    "end": "2258180"
  },
  {
    "text": "[Music] not that different thing that differently yeah schedule for the",
    "start": "2260890",
    "end": "2269250"
  },
  {
    "text": "extender no extender we are still yeah HTTP extended we are not going to touch",
    "start": "2269250",
    "end": "2275220"
  },
  {
    "text": "upon that but if people are interested I mean we have like some proposals around",
    "start": "2275220",
    "end": "2281760"
  },
  {
    "text": "or people have created github issues around like adding gr PC support and all that well you can here it is interest",
    "start": "2281760",
    "end": "2288270"
  },
  {
    "text": "rate by even using G obviously the cost us they exist so it's I was a stupid",
    "start": "2288270",
    "end": "2294330"
  },
  {
    "text": "before some small Custer and the you will be a very quick implantation onto",
    "start": "2294330",
    "end": "2299550"
  },
  {
    "text": "your requirement before very large cluster you have complicated every comment then try with us it's a far yeah",
    "start": "2299550",
    "end": "2312960"
  },
  {
    "text": "we our plans in next race we will use some real examples instead of color",
    "start": "2312960",
    "end": "2320250"
  },
  {
    "text": "world to give standard the presentation to know how to use that because it's not",
    "start": "2320250",
    "end": "2326790"
  },
  {
    "text": "very easy comparing two extender we can just you start to expand the framework",
    "start": "2326790",
    "end": "2332370"
  },
  {
    "text": "we we must use something good compelling example to describe is okay oh maybe we",
    "start": "2332370",
    "end": "2342150"
  },
  {
    "text": "are wrong auditory and I will believe in the corner we can still answer decision mysteries alright thank you thank you",
    "start": "2342150",
    "end": "2349210"
  },
  {
    "text": "[Applause]",
    "start": "2349210",
    "end": "2352380"
  }
]