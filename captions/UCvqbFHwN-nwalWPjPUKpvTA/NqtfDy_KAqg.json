[
  {
    "text": "gang thanks for coming today uh we've got a packed agenda I think we're clocked in at like 34 30. so we're going",
    "start": "480",
    "end": "6960"
  },
  {
    "text": "to start right at the pen uh we can't take any questions to get through all of it but I tell you what we will stay as",
    "start": "6960",
    "end": "13920"
  },
  {
    "text": "long as it takes to answer all your questions um so don't think that we're fleeing the",
    "start": "13920",
    "end": "19320"
  },
  {
    "text": "scene or anything like that I promise we'll be with you um and I think we're about two minutes",
    "start": "19320",
    "end": "24420"
  },
  {
    "text": "out thank you very much for coming in if you guys in the back it's not like the Blue Man Group we won't spread anything",
    "start": "24420",
    "end": "29460"
  },
  {
    "text": "on here or anything so feel free plenty of places up front thanks again",
    "start": "29460",
    "end": "34880"
  },
  {
    "text": "foreign",
    "start": "36059",
    "end": "38780"
  },
  {
    "text": "take a picture for me yeah",
    "start": "47820",
    "end": "51920"
  },
  {
    "text": "oh I turned my turn my phone off so",
    "start": "54899",
    "end": "62538"
  },
  {
    "text": "let's begin drink",
    "start": "63719",
    "end": "68479"
  },
  {
    "text": "called the kubecon police on you all right",
    "start": "73860",
    "end": "78740"
  },
  {
    "text": "all right that's good",
    "start": "80520",
    "end": "83539"
  },
  {
    "text": "to go I'm good sorry ladies and gentlemen my name's Shane",
    "start": "88200",
    "end": "95100"
  },
  {
    "text": "this is my good friend will and this whole week at kubecon you're going to",
    "start": "95100",
    "end": "100200"
  },
  {
    "text": "hear amazing success story after amazing success story delivered by some of the most brilliant",
    "start": "100200",
    "end": "106920"
  },
  {
    "text": "people in our industry so will and I we're going to give you a break from all that right now we're going to tell you",
    "start": "106920",
    "end": "113460"
  },
  {
    "text": "about a misadventure we had running a large-scale kubernetes cluster and the reason why we think you're going to be",
    "start": "113460",
    "end": "119220"
  },
  {
    "text": "interested in this is We Believe that many of you in this room right now are on this misadventure with us it's",
    "start": "119220",
    "end": "126240"
  },
  {
    "text": "just no one told you our misadventure begins we're all great",
    "start": "126240",
    "end": "132360"
  },
  {
    "text": "misadventure begins a misunderstanding you see we took the things that we knew to be true in kubernetes",
    "start": "132360",
    "end": "139140"
  },
  {
    "text": "and misapplied all those Concepts to things that were actually governed by the Linux performance rules and it turns",
    "start": "139140",
    "end": "145319"
  },
  {
    "text": "out that's kind of a big mistake to make but it's an easy one to make you see we",
    "start": "145319",
    "end": "151440"
  },
  {
    "text": "were thinking in cores because that's how kubernetes thinks about stuff right there is a node object",
    "start": "151440",
    "end": "158520"
  },
  {
    "text": "with the number of CPUs on it and every time you schedule something it decrements that object it's definitely a",
    "start": "158520",
    "end": "165720"
  },
  {
    "text": "thing however Linux has an abstraction layer and it",
    "start": "165720",
    "end": "173099"
  },
  {
    "text": "thinks in time not cores now I get it I'm probably sounding like the crazy guy on the kubernetes subway",
    "start": "173099",
    "end": "179459"
  },
  {
    "text": "right the chords are a lot so let me explain that for just a second okay so I'm going to use the least controversial",
    "start": "179459",
    "end": "187860"
  },
  {
    "text": "feature in all of kubernetes to explain this kubernetes limits now I know I know",
    "start": "187860",
    "end": "194940"
  },
  {
    "text": "right there's there's a little take how you feel about kubernetes limits and put that aside for me for just a second",
    "start": "194940",
    "end": "200159"
  },
  {
    "text": "because it is the best way to understand how kubernetes thinks in time",
    "start": "200159",
    "end": "205379"
  },
  {
    "text": "and how dangerous it can be to think in cores just as a quick review",
    "start": "205379",
    "end": "211739"
  },
  {
    "text": "there is metadata attached to a container a specific container for how much CPU it can take in a given period",
    "start": "211739",
    "end": "219300"
  },
  {
    "text": "of time that period of time happens to be 100 milliseconds just follow that number",
    "start": "219300",
    "end": "225420"
  },
  {
    "text": "away for me real quick we'll need that in just a few slides and every period we reset this quota",
    "start": "225420",
    "end": "234000"
  },
  {
    "text": "of limits pretty straightforward right but what I didn't understand is when I",
    "start": "234000",
    "end": "240840"
  },
  {
    "text": "was configuring cores what was really going on behind the scenes is it was converting into time",
    "start": "240840",
    "end": "246959"
  },
  {
    "text": "let's explain how that works really quickly see in my mind I was configuring One Core and that's broken up into 1 000",
    "start": "246959",
    "end": "254760"
  },
  {
    "text": "little chunks called Millis CPU right and if I configure 10 Milli CPU",
    "start": "254760",
    "end": "262500"
  },
  {
    "text": "which is the minimum value what I'm really saying here is whatever value that I configure divided by that base",
    "start": "262500",
    "end": "270120"
  },
  {
    "text": "value of one CPU or 1 000 Milli cores there's a percentage",
    "start": "270120",
    "end": "275520"
  },
  {
    "text": "but a percentage of what it's one percent of this 100 millisecond",
    "start": "275520",
    "end": "281400"
  },
  {
    "text": "period in time or one millisecond which is why that's the smallest value",
    "start": "281400",
    "end": "287400"
  },
  {
    "text": "now how is that different so someone would come up to me and like Shane my application needs half a core sounds",
    "start": "287400",
    "end": "293220"
  },
  {
    "text": "pretty reasonable sure but what's happening when I convert that into time you see that is 50 of that 100",
    "start": "293220",
    "end": "300960"
  },
  {
    "text": "millisecond period or 50 milliseconds in time how is 50 milliseconds in time different",
    "start": "300960",
    "end": "309120"
  },
  {
    "text": "from half a core well you see I didn't think to have the",
    "start": "309120",
    "end": "314940"
  },
  {
    "text": "good sense to ask them anything about this app and it turns out with our fictitious app we have four threads",
    "start": "314940",
    "end": "321900"
  },
  {
    "text": "running on four cores and if I add up the total bill the total",
    "start": "321900",
    "end": "327720"
  },
  {
    "text": "amount of CPU utilization in this 100 milliseconds of time that's passing in",
    "start": "327720",
    "end": "334680"
  },
  {
    "text": "the real world I get 400 milliseconds that I've just",
    "start": "334680",
    "end": "340139"
  },
  {
    "text": "said no that was nowhere near the 50 that I thought when I'm thinking a time it sounds reasonable but let's take a",
    "start": "340139",
    "end": "346020"
  },
  {
    "text": "step back right if if uh can anyone tell you the right way to set your limits if",
    "start": "346020",
    "end": "352620"
  },
  {
    "text": "they didn't know this information how many threads you're running how you're gonna oh how many parallel processes",
    "start": "352620",
    "end": "358259"
  },
  {
    "text": "actually one person can it turns out Prometheus can",
    "start": "358259",
    "end": "363660"
  },
  {
    "text": "so by using the familiar C advisor metric container CPU usage seconds total now this is in seconds so we're going to",
    "start": "363660",
    "end": "370259"
  },
  {
    "text": "do a little prom Foo right prom ql and convert that into periods right we're",
    "start": "370259",
    "end": "376380"
  },
  {
    "text": "going to save all that I promise I'll share that with you at the end but when we measure that in time what's the value",
    "start": "376380",
    "end": "383600"
  },
  {
    "text": "exactly the amount that we needed to set it for to keep it from throttling did I need a Ouija board right a dream catcher",
    "start": "383600",
    "end": "391080"
  },
  {
    "text": "and five million blogs to set it no I just needed like a five second chart and Prometheus to get this all down was it",
    "start": "391080",
    "end": "396539"
  },
  {
    "text": "really that hard it's not when you're thinking in time okay Shane is it really that easy all",
    "start": "396539",
    "end": "402840"
  },
  {
    "text": "right I left a few things out so let's cover that our application we decided to do the number of threads static but we",
    "start": "402840",
    "end": "408240"
  },
  {
    "text": "had an option to have that go Dynamic with user load what would happen",
    "start": "408240",
    "end": "414360"
  },
  {
    "text": "as the load goes up would the value be the same if the thread count was dynamic in fact it would not be whoops",
    "start": "414360",
    "end": "421800"
  },
  {
    "text": "thinking in terms of static can be highly dangerous more on that in just a second",
    "start": "421800",
    "end": "428520"
  },
  {
    "text": "but how does this affect things in the real world well you see I've got four",
    "start": "428520",
    "end": "433919"
  },
  {
    "text": "cores that I'm running in test right and so I'm running like a runtime maybe like golang or something that says",
    "start": "433919",
    "end": "439620"
  },
  {
    "text": "oh cool you gave me four cores well I'm gonna run four OS threads and I'll run",
    "start": "439620",
    "end": "444780"
  },
  {
    "text": "my go routines on that but what happens when I go to production and I give it an eight core box it's",
    "start": "444780",
    "end": "451199"
  },
  {
    "text": "going to go oh you gave me eight cores great is the number for performance going to measure out to be the same if I have eight OS threads in fact it's not",
    "start": "451199",
    "end": "458400"
  },
  {
    "text": "whoops whoops so a few things that we have to think about when we do this sort of",
    "start": "458400",
    "end": "463440"
  },
  {
    "text": "thing cool so you get this wrong like I got it",
    "start": "463440",
    "end": "468539"
  },
  {
    "text": "wrong I used to start to do these percentage of periods and all this type of stuff and I wasn't thinking in time",
    "start": "468539",
    "end": "474660"
  },
  {
    "text": "so I'd go to Will all judgy and stuff right I'd be like will you're 99 throttled on your",
    "start": "474660",
    "end": "481380"
  },
  {
    "text": "application how could you and he'd be like oh oh dude dude what do we set it for",
    "start": "481380",
    "end": "486780"
  },
  {
    "text": "and I'd be like I don't know right and so uh because you",
    "start": "486780",
    "end": "491819"
  },
  {
    "text": "know it's just some random percentage up there and he'd look at me all disappointed kind of like you're looking at me right now sir and and I'd be like",
    "start": "491819",
    "end": "499500"
  },
  {
    "text": "man there's got to be a better way to do this right well you see I'd go back and I'd do my",
    "start": "499500",
    "end": "505680"
  },
  {
    "text": "engineering right because I'm an engineer I got this right we'll double it right he's running one we'll run two course because that sounds good to my",
    "start": "505680",
    "end": "511740"
  },
  {
    "text": "ear oh you've done it don't you judge me and so I do two cores now right that sounds",
    "start": "511740",
    "end": "518459"
  },
  {
    "text": "like a lot but when we think in time immediately we see the problem so just converting between these two pretty",
    "start": "518459",
    "end": "523740"
  },
  {
    "text": "simple just drop the zero right right going back and forth and what is it 200 milliseconds time what did I just",
    "start": "523740",
    "end": "530040"
  },
  {
    "text": "do I just have this application performance sorry buddy right and in that 100",
    "start": "530040",
    "end": "536820"
  },
  {
    "text": "milliseconds of time if I didn't have another application that needed to run guess what",
    "start": "536820",
    "end": "542640"
  },
  {
    "text": "goes out as Idle answer I got 50 utilization but I got a",
    "start": "542640",
    "end": "547680"
  },
  {
    "text": "huge performance problem that would be a monstrous problem to try to figure out unless we were thinking in time",
    "start": "547680",
    "end": "554459"
  },
  {
    "text": "you see luckily I have the CPU seconds throttle total and if I just convert that to",
    "start": "554459",
    "end": "560459"
  },
  {
    "text": "periods and again I'll share this with you lo and behold what number is it the exact number I need to raise it by",
    "start": "560459",
    "end": "568080"
  },
  {
    "text": "right I had 200 I needed to add an extra 200 to get it to 400 the exact value I",
    "start": "568080",
    "end": "573120"
  },
  {
    "text": "needed to stop the throttles stuff's actually quite simple it's just two little charts I need to look at for",
    "start": "573120",
    "end": "578760"
  },
  {
    "text": "five seconds and I didn't need all this stuff that's been going on for years right",
    "start": "578760",
    "end": "584959"
  },
  {
    "text": "but I know exactly what you're thinking right now you're thinking shame",
    "start": "585000",
    "end": "590339"
  },
  {
    "text": "you didn't even cover per CPU slice allocation on the global container for the metadata of the container what kind",
    "start": "590339",
    "end": "596880"
  },
  {
    "text": "of amateur night are you running up there man that's a little harsh but I got you fam",
    "start": "596880",
    "end": "603720"
  },
  {
    "text": "don't worry I wrote a whole blog post going into the brain damage details of all this now I know the the Grand Master",
    "start": "603720",
    "end": "611100"
  },
  {
    "text": "of kubernetes himself Tim Hawkins tried to get us not to shoot ourselves in the foot years ago but there's actually some",
    "start": "611100",
    "end": "617339"
  },
  {
    "text": "cool stuff coming out you see in a new version of the Linux kernel that's coming uh there's rollover and burst",
    "start": "617339",
    "end": "623700"
  },
  {
    "text": "values for the stuff so limits might be cool again but more importantly you're in a multi-tenant talk right now and us as",
    "start": "623700",
    "end": "629399"
  },
  {
    "text": "multi-tenant operators we don't get to control what people do in our containers we need a safety net and if you need a",
    "start": "629399",
    "end": "634980"
  },
  {
    "text": "safety net this stuff's actually not too bad if you think in time",
    "start": "634980",
    "end": "640019"
  },
  {
    "text": "now someone if not all of you are going to send me this oh but you can do all this stuff with requests it turns out",
    "start": "640019",
    "end": "645779"
  },
  {
    "text": "that's actually a more fascinating topic and might not work quite like you think",
    "start": "645779",
    "end": "651420"
  },
  {
    "text": "to discuss that let's go explore that Universe for just a bit",
    "start": "651420",
    "end": "656640"
  },
  {
    "text": "the gentleman on this name is Edwin Hubble and two hours from where I live in Los Angeles back in the",
    "start": "656640",
    "end": "662399"
  },
  {
    "text": "1920s Edwin Hubble provided direct evidence that the Universe was expanding",
    "start": "662399",
    "end": "667740"
  },
  {
    "text": "at a faster and faster rate now some mathematicians at the time were",
    "start": "667740",
    "end": "672779"
  },
  {
    "text": "pretty bummed because their whole life's work was on the universe was static right overnight just completely bad",
    "start": "672779",
    "end": "678600"
  },
  {
    "text": "this makes me feel a lot better because all my math and all my performance calculations on Linux were wrong because",
    "start": "678600",
    "end": "683940"
  },
  {
    "text": "I thought the Linux Universe was static now I don't want to brag",
    "start": "683940",
    "end": "689519"
  },
  {
    "text": "but when I was in the United States Marine Corps I was so good at basic math they gave me a special box of crayons to",
    "start": "689519",
    "end": "695399"
  },
  {
    "text": "do hard things like ratios right so when it came to take my cka course I was feeling",
    "start": "695399",
    "end": "700560"
  },
  {
    "text": "pretty confident I'm like I got this now tell me if you learned the same thing right so one request equals 10 24 shares",
    "start": "700560",
    "end": "707700"
  },
  {
    "text": "and if I got double those amount of requests I get double the amount of shares and",
    "start": "707700",
    "end": "713100"
  },
  {
    "text": "just like shares at a company whoever gets the most shares gets a greater percentage of the company right so in my",
    "start": "713100",
    "end": "719880"
  },
  {
    "text": "mind at work like this this application got 25 percent of",
    "start": "719880",
    "end": "724980"
  },
  {
    "text": "shares this application got 50 percent of shares except none of that is true",
    "start": "724980",
    "end": "730860"
  },
  {
    "text": "I'm read my hero uh Brendan Gregg right who makes me feel like I probably should be working at",
    "start": "730860",
    "end": "736620"
  },
  {
    "text": "Best Buy and not doing this but he says Hey crayon eater guess what",
    "start": "736620",
    "end": "742019"
  },
  {
    "text": "it's only the busy shares that count wait what so you mean if this container is Idle it",
    "start": "742019",
    "end": "749459"
  },
  {
    "text": "doesn't count in the performance calculation it's just these two that completely changes my performance ratio",
    "start": "749459",
    "end": "755640"
  },
  {
    "text": "on this node oh my goodness but wait a minute wait a minute I have hundreds",
    "start": "755640",
    "end": "761640"
  },
  {
    "text": "of PODS forget containers I got startup containers and I'd rather try to figure out the heat death of the universe than",
    "start": "761640",
    "end": "767160"
  },
  {
    "text": "try to figure out like what container is going to be busy at any given time if I could do that kind of math I would have gotten to ml right",
    "start": "767160",
    "end": "774959"
  },
  {
    "text": "and would it be the same if I could calculate it on a node would it be on all thousand nodes no it wouldn't oh",
    "start": "774959",
    "end": "782880"
  },
  {
    "text": "I'm getting a little anxious now right but is this practical you see when I was",
    "start": "782880",
    "end": "789060"
  },
  {
    "text": "thinking in core as a developer come up to me and like can I have a core sure buddy here you go right but what was",
    "start": "789060",
    "end": "795300"
  },
  {
    "text": "actually happening let's say just to stretch a point we were we were working in stage would all the containers be busy in",
    "start": "795300",
    "end": "802139"
  },
  {
    "text": "stage like they are prod no so I do some performance crushing you all do performance regression on your apps",
    "start": "802139",
    "end": "807300"
  },
  {
    "text": "right I know I know so we do a little performance regression what happens it takes up that",
    "start": "807300",
    "end": "812459"
  },
  {
    "text": "entire note because nothing else is going on right if I was thinking in cores I'd be convinced I had another core but I'm actually taking up the",
    "start": "812459",
    "end": "818579"
  },
  {
    "text": "whole box whoops what happens when I move to production would I get the same performance for",
    "start": "818579",
    "end": "823860"
  },
  {
    "text": "fall indeed I wouldn't I'd be like what's going on and would that be the same across all 1000 boxes that we're",
    "start": "823860",
    "end": "829079"
  },
  {
    "text": "running and I think no oh my goodness okay so now I'm just disillusioned right I'm",
    "start": "829079",
    "end": "837120"
  },
  {
    "text": "screaming at the lens guys why does this have to be so brain damaging right and it turns out it was just my mental model",
    "start": "837120",
    "end": "842279"
  },
  {
    "text": "was all wrong you see once these tasks or threads get put on a particular CPU",
    "start": "842279",
    "end": "849060"
  },
  {
    "text": "right really what we're talking about this weighted share system is who gets",
    "start": "849060",
    "end": "854279"
  },
  {
    "text": "to run next on that particular CPU oh well that kind of makes sense now",
    "start": "854279",
    "end": "860279"
  },
  {
    "text": "because if this task doesn't need to run on CPU does it need to be in that share calculation of who gets to run on next",
    "start": "860279",
    "end": "867959"
  },
  {
    "text": "oh thank goodness now I understand I just misunderstood all of kubernetes no big deal right oh",
    "start": "867959",
    "end": "874139"
  },
  {
    "text": "goodness okay to really understand this though we'll",
    "start": "874139",
    "end": "879180"
  },
  {
    "text": "have to do the completely Fair scheduler and gang we have 30 seconds per slide so I need a",
    "start": "879180",
    "end": "884699"
  },
  {
    "text": "little overconfidence for me in the crowd right now we're going to learn the completely Fair scheduler in exactly 30",
    "start": "884699",
    "end": "890160"
  },
  {
    "text": "seconds are you feeling it this morning are you ready yeah that's what I'm talking about all right a little delusional overconfidence right all",
    "start": "890160",
    "end": "896699"
  },
  {
    "text": "right gang will time us go sweet all right gang this task is running 100 milliseconds on",
    "start": "896699",
    "end": "904380"
  },
  {
    "text": "this CPU it is hogging up all the CPU time meanwhile this poor task is waiting",
    "start": "904380",
    "end": "911220"
  },
  {
    "text": "patiently for that CPU it has only had 10 milliseconds of run time on the CPU",
    "start": "911220",
    "end": "917760"
  },
  {
    "text": "that's not fair Linux fare scheduler to the rescue it sees this and it says wait a minute",
    "start": "917760",
    "end": "923519"
  },
  {
    "text": "this task has the lowest run time let's put that on CPU",
    "start": "923519",
    "end": "928560"
  },
  {
    "text": "once these two a tasks have the same amount of time or their fair share of",
    "start": "928560",
    "end": "934079"
  },
  {
    "text": "the CPU time bam that's it ladies and gentlemen completely fair schedule of time",
    "start": "934079",
    "end": "940160"
  },
  {
    "text": "39. oh come see my presentation at overconfidence tomorrow I'm going to",
    "start": "940160",
    "end": "945240"
  },
  {
    "text": "kill it it'll be much better I promise all right all right all right so why did I take you through",
    "start": "945240",
    "end": "951060"
  },
  {
    "text": "that okay what is a request at the end of the day a little hand wavy here but bear with me",
    "start": "951060",
    "end": "957660"
  },
  {
    "text": "all right I got my CPU Hawk is back run time 100 milliseconds in time",
    "start": "957660",
    "end": "963240"
  },
  {
    "text": "what I'm actually saying here with request is one is divide this runtime by the number of shares this is an advanced",
    "start": "963240",
    "end": "970199"
  },
  {
    "text": "crayon math thing with me but when you divide a number evidently it makes it smaller right and so when you have the",
    "start": "970199",
    "end": "976560"
  },
  {
    "text": "shortest run time as you guys are already experts at now right the shortest time is the highest priority right cool and if I was to increase that",
    "start": "976560",
    "end": "984839"
  },
  {
    "text": "value it's even shorter oh my goodness we're messing with time again",
    "start": "984839",
    "end": "990839"
  },
  {
    "text": "wow okay all right so let's think about this for a minute but am I the Master and Commander of everything that's going",
    "start": "990839",
    "end": "997620"
  },
  {
    "text": "in the node or is there maybe something else that might be in play we call that the witness right okay okay here we go",
    "start": "997620",
    "end": "1004459"
  },
  {
    "text": "all right there was something I didn't tell you in all this and that was that that 10 millisecond task was waiting on",
    "start": "1004459",
    "end": "1011600"
  },
  {
    "text": "IO and so what was happening is it didn't even need to run on CPU now if I was",
    "start": "1011600",
    "end": "1017660"
  },
  {
    "text": "running a real-time scheduler what would happen this CPU hog would get interrupted at every so often right hey",
    "start": "1017660",
    "end": "1024860"
  },
  {
    "text": "you know do I need to run something no no and I would waste all those CPU Cycles we don't want that that's where",
    "start": "1024860",
    "end": "1030140"
  },
  {
    "text": "CFS shines right so I want to let the CPU Hog Run as long as it wants and when",
    "start": "1030140",
    "end": "1035600"
  },
  {
    "text": "this packet comes back I set a flag saying I need to run and I want it to run it as fast as possible and you know",
    "start": "1035600",
    "end": "1041240"
  },
  {
    "text": "what happens from here right lowest time wins right it goes in but it's an i o",
    "start": "1041240",
    "end": "1046760"
  },
  {
    "text": "task so I want you to think about this it's probably going to run on CPU for very small and then go back I O task by",
    "start": "1046760",
    "end": "1054500"
  },
  {
    "text": "default are probably going to be the smallest time what starts happening when I do aggressive requests and all this",
    "start": "1054500",
    "end": "1060320"
  },
  {
    "text": "type of stuff on something that's already pretty aggressive right oh wait there's",
    "start": "1060320",
    "end": "1065600"
  },
  {
    "text": "a lot going on here evidently cool so how did I get this fictitious",
    "start": "1065600",
    "end": "1071480"
  },
  {
    "text": "version of the universe right well it turns out you know I did CPU stress and all the",
    "start": "1071480",
    "end": "1078380"
  },
  {
    "text": "containers at the same time and we know if all containers are busy that doesn't happen in the real world right",
    "start": "1078380",
    "end": "1083720"
  },
  {
    "text": "I ran the note at 100 busy hopefully that's not happened in the real world right I had a fixed number of threads",
    "start": "1083720",
    "end": "1089419"
  },
  {
    "text": "when they could be dynamic and I had no I O based things so I got this like fictitious version of the universe and",
    "start": "1089419",
    "end": "1095960"
  },
  {
    "text": "how that worked okay so what would be this new mental model that I would have to pin like what",
    "start": "1095960",
    "end": "1102320"
  },
  {
    "text": "would that look like and in my head it looks something like this so I have two containers we'll call them task groups each with four threads or",
    "start": "1102320",
    "end": "1109880"
  },
  {
    "text": "tasks in them there's a million places we can go with this but I'm going to focus on this concept of saturation you",
    "start": "1109880",
    "end": "1116480"
  },
  {
    "text": "see this is what my node really looks like and on this CPU I have one task it's all so the caches are all warm it's",
    "start": "1116480",
    "end": "1123500"
  },
  {
    "text": "at its Peak Performance however this one where I have two tasks going on it's",
    "start": "1123500",
    "end": "1129080"
  },
  {
    "text": "going to take double the amount of time right to get that same resource now what",
    "start": "1129080",
    "end": "1134539"
  },
  {
    "text": "I didn't know is if the CPU wasn't 100 man I was packing that note as dense as a neutron star right what's the big deal",
    "start": "1134539",
    "end": "1141380"
  },
  {
    "text": "well it turns out maybe that wasn't the best but to understand that I want you to do",
    "start": "1141380",
    "end": "1146900"
  },
  {
    "text": "a thought experiment with me real quick gang let's just pretend right that these two CPUs are 100 utilized I'm going to",
    "start": "1146900",
    "end": "1155000"
  },
  {
    "text": "stretch a point to make a point here right but this task",
    "start": "1155000",
    "end": "1160899"
  },
  {
    "text": "is actually perfect right there it's it's 100 utilized but that's fine right",
    "start": "1161240",
    "end": "1167120"
  },
  {
    "text": "because it's perfectly optimized but you let me bin pack your notes so on this side oh my goodness right I've got",
    "start": "1167120",
    "end": "1174140"
  },
  {
    "text": "so many tasks contexts switching I'm not getting any real work done",
    "start": "1174140",
    "end": "1179600"
  },
  {
    "text": "whoops right so it turns out would I see this saturation metric that I've",
    "start": "1179600",
    "end": "1185780"
  },
  {
    "text": "saturated this with utilization in fact I wouldn't but I would argue Genghis multi-tenant people that need to",
    "start": "1185780",
    "end": "1192260"
  },
  {
    "text": "do these things super dense that's the more important metric out of the two",
    "start": "1192260",
    "end": "1199000"
  },
  {
    "text": "now I'm a little crestfallen I'm not going to win the Nobel Prize for any of this it looks like uh there has been a",
    "start": "1199940",
    "end": "1206179"
  },
  {
    "text": "family of node metrics called pressure style information that tell you all of",
    "start": "1206179",
    "end": "1211280"
  },
  {
    "text": "this wonderful data right we just didn't know to turn it on right and what we did the the findings were pretty crazy so",
    "start": "1211280",
    "end": "1217880"
  },
  {
    "text": "sure okay there was like you know 50 percent of threads waiting and all that type of stuff maybe we should use go Max",
    "start": "1217880",
    "end": "1223039"
  },
  {
    "text": "procs or something to limit the thread counts and stuff like that but interestingly enough that wasn't the",
    "start": "1223039",
    "end": "1228380"
  },
  {
    "text": "most interesting thing that we found we found that we were stalled on memory",
    "start": "1228380",
    "end": "1234080"
  },
  {
    "text": "up to 10 we were thrashing the memories on the box they were saturated and the most fascinating thing was IO stalled",
    "start": "1234080",
    "end": "1241940"
  },
  {
    "text": "means every single thread on the box was stalled up to 35 of the time whoops",
    "start": "1241940",
    "end": "1251660"
  },
  {
    "text": "because we weren't even looking at this sort of thing there is all kinds of amazing work uh by",
    "start": "1251660",
    "end": "1258320"
  },
  {
    "text": "uh guys like Chris down that's did like c groups version two in the meta Facebook guys um that I really recommend that you",
    "start": "1258320",
    "end": "1264799"
  },
  {
    "text": "watch but the point is I was super excited about all this and I was thinking who do I know",
    "start": "1264799",
    "end": "1271340"
  },
  {
    "text": "that runs a large scale multi-tenant cluster my buddy will",
    "start": "1271340",
    "end": "1276100"
  },
  {
    "text": "so I pick up the phone and I'm like hey Will guess what",
    "start": "1276919",
    "end": "1282380"
  },
  {
    "text": "I don't know anything about kubernetes and he's like oh I know man I know why don't you tell me what's going on",
    "start": "1282380",
    "end": "1288200"
  },
  {
    "text": "and I was like uh jabbering on like a monkey and I'm like yeah all right all",
    "start": "1288200",
    "end": "1293960"
  },
  {
    "text": "right Shane click hey babe I think uh I think Shane hit",
    "start": "1293960",
    "end": "1299179"
  },
  {
    "text": "his head bungee jumping again but in the morning",
    "start": "1299179",
    "end": "1304340"
  },
  {
    "text": "started thinking about some of these decisions that we'd made at Aquia in a little bit of a Different Light",
    "start": "1304340",
    "end": "1311299"
  },
  {
    "text": "see we were scaling our customer workloads on CPU utilization as a percent of requests",
    "start": "1311299",
    "end": "1318140"
  },
  {
    "text": "and I mean that's a kubernetes default right it practically comes out of the box so could it really bit us that bad",
    "start": "1318140",
    "end": "1326360"
  },
  {
    "text": "well it turns out it could this is one customer namespace",
    "start": "1326360",
    "end": "1331760"
  },
  {
    "text": "over a period of like a week and you can see we're running hundreds of nodes an",
    "start": "1331760",
    "end": "1337520"
  },
  {
    "text": "hour thousands a day that it turns out after I looked into it",
    "start": "1337520",
    "end": "1342860"
  },
  {
    "text": "a little bit we did not need their load did not look like this right so we have",
    "start": "1342860",
    "end": "1348140"
  },
  {
    "text": "all this churn but why well",
    "start": "1348140",
    "end": "1353480"
  },
  {
    "text": "what is percent of requests I never really thought about it before but when Shane tells us all this about",
    "start": "1353480",
    "end": "1359600"
  },
  {
    "text": "requests I mean he's talking about these weights right and like the Linux Fair scheduler and stuff like that",
    "start": "1359600",
    "end": "1366380"
  },
  {
    "text": "he's not talking about like using a certain percent of your weight right what does that even mean",
    "start": "1366380",
    "end": "1372500"
  },
  {
    "text": "well it turns out in this context right percent of request is simply the actual",
    "start": "1372500",
    "end": "1378140"
  },
  {
    "text": "CPU usage per second on average and remember Linux thinks in time",
    "start": "1378140",
    "end": "1383720"
  },
  {
    "text": "right so this is a this is the amount of time that something spent on the CPU",
    "start": "1383720",
    "end": "1389600"
  },
  {
    "text": "well not something actually to be specific it's all the processes and all the threads in that container added up",
    "start": "1389600",
    "end": "1396500"
  },
  {
    "text": "on average per second and we just divide that by the request so if something ran for 500 milliseconds",
    "start": "1396500",
    "end": "1404480"
  },
  {
    "text": "per second on average on the CPU and it requested a thousand Milli cores",
    "start": "1404480",
    "end": "1409880"
  },
  {
    "text": "we just take those units we throw them out the window and we do 500 divided by a thousand and we get 50 of requests",
    "start": "1409880",
    "end": "1417620"
  },
  {
    "text": "right simple enough well with that in mind let's revisit an",
    "start": "1417620",
    "end": "1423080"
  },
  {
    "text": "example that Shane gave us a little earlier let's take two containers",
    "start": "1423080",
    "end": "1428960"
  },
  {
    "text": "all right we're going to run them as hot as they can they're just going to eat up as much CPU as possible",
    "start": "1428960",
    "end": "1435080"
  },
  {
    "text": "the first container we're going to give one core right in its request and the second we're going to get three",
    "start": "1435080",
    "end": "1441799"
  },
  {
    "text": "it shouldn't surprise us at this point that on a four core node that first container is going to get one CPU second",
    "start": "1441799",
    "end": "1448700"
  },
  {
    "text": "on average and the second one is going to get three and when we do that division out with Shane's special box of crayons we do one",
    "start": "1448700",
    "end": "1456140"
  },
  {
    "text": "second divided by one core and we get a hundred percent simple enough right I mean this is kind",
    "start": "1456140",
    "end": "1461539"
  },
  {
    "text": "of intuitively how we would expect all this stuff to work but what happens",
    "start": "1461539",
    "end": "1467600"
  },
  {
    "text": "if we change those requests around a little bit see Shane tells us it's just the ratio",
    "start": "1467600",
    "end": "1474679"
  },
  {
    "text": "of these requests that matter right so as far as Linux is concerned whether",
    "start": "1474679",
    "end": "1479780"
  },
  {
    "text": "it's one core and three cores or a hundred millocores and 300 Milli cores it is actually honest to goodness as far",
    "start": "1479780",
    "end": "1487340"
  },
  {
    "text": "as Linux is concerned the same exact situation",
    "start": "1487340",
    "end": "1492639"
  },
  {
    "text": "right so we get the same performance one second and three seconds but when we do that math with our kubernetes brains",
    "start": "1493520",
    "end": "1499280"
  },
  {
    "text": "right we take one second divided by a hundred millocores and we get a thousand",
    "start": "1499280",
    "end": "1504799"
  },
  {
    "text": "percent of requests that's a little misleading right I mean it's wildly different numbers 100 and a",
    "start": "1504799",
    "end": "1511940"
  },
  {
    "text": "thousand percent that both describe the exact same situation as far as Linux sees it",
    "start": "1511940",
    "end": "1519639"
  },
  {
    "text": "but I wasn't quite convinced at this point right I mean that's interesting and odd maybe you know not how I",
    "start": "1521240",
    "end": "1528020"
  },
  {
    "text": "expected that it would work but Shane's asking me to basically redo my entire scaling metric for all you know",
    "start": "1528020",
    "end": "1534260"
  },
  {
    "text": "five thousand six thousand of our of our customers so uh you know I needed to dive a little deeper on this",
    "start": "1534260",
    "end": "1541279"
  },
  {
    "text": "see what we're doing really when we're calculating this percent request and we're thinking this way we're comparing",
    "start": "1541279",
    "end": "1546860"
  },
  {
    "text": "what's essentially a performance metric right how how long something's running with a number that frankly we're just",
    "start": "1546860",
    "end": "1552919"
  },
  {
    "text": "kind of using to figure out how many pods we want to run on that node right how densely we want to pack that node",
    "start": "1552919",
    "end": "1558620"
  },
  {
    "text": "which as Shane talks about is maybe a little bit more complicated and we're doing that comparison between",
    "start": "1558620",
    "end": "1564799"
  },
  {
    "text": "these two concepts with really no regard to how Linux actually views that number",
    "start": "1564799",
    "end": "1570740"
  },
  {
    "text": "and sometimes don't get me wrong sometimes that comparison is a perfectly valid one to make it gives us a lot of",
    "start": "1570740",
    "end": "1576500"
  },
  {
    "text": "information but by no means does it give us a complete and accurate view of",
    "start": "1576500",
    "end": "1581720"
  },
  {
    "text": "what's actually going on within the system let's look at a more extreme example of",
    "start": "1581720",
    "end": "1587179"
  },
  {
    "text": "this let's say we have just one container and every time he gets a request like a web request or something",
    "start": "1587179",
    "end": "1592460"
  },
  {
    "text": "it has some work to churn through and that work takes about eight seconds of CPU time",
    "start": "1592460",
    "end": "1598520"
  },
  {
    "text": "but let's say we weren't thinking in time right let's say okay well that's a pretty big container right it's doing a lot of work I'll give it four cores",
    "start": "1598520",
    "end": "1605900"
  },
  {
    "text": "right four chords has to be enough right it ran on a four core box before four cores is plenty",
    "start": "1605900",
    "end": "1611480"
  },
  {
    "text": "well what happens when we try to run that on kubernetes we send that up to the API server and it says great you",
    "start": "1611480",
    "end": "1619100"
  },
  {
    "text": "want four cores I'll give you four cores here you go but let's say it schedules us on an eight core node",
    "start": "1619100",
    "end": "1625760"
  },
  {
    "text": "well when that container comes up on Linux and there's nothing else running on the box it can really spread out right over all",
    "start": "1625760",
    "end": "1633380"
  },
  {
    "text": "eight cores which means it can get its eight seconds of work done in just one second of real",
    "start": "1633380",
    "end": "1641360"
  },
  {
    "text": "time which is wonderful right I mean we're getting a lot of work done as fast as we can but",
    "start": "1641360",
    "end": "1647240"
  },
  {
    "text": "when we look at this when we look at our metrics we're going to see eight seconds divided by four cores",
    "start": "1647240",
    "end": "1653000"
  },
  {
    "text": "200 percent of requests oh no we say right 200 of requests we",
    "start": "1653000",
    "end": "1658640"
  },
  {
    "text": "must be overloaded we must be putting too much work on this pod so what do we do as kubernetes aficionados when",
    "start": "1658640",
    "end": "1665900"
  },
  {
    "text": "something's overloaded we scale up right we add another pod so let's say we do",
    "start": "1665900",
    "end": "1672740"
  },
  {
    "text": "let's say we just set that you know dash dash scale replicas two done",
    "start": "1672740",
    "end": "1678520"
  },
  {
    "text": "next pod comes up kubernetes helps us out right doesn't want to waste space puts us on the same node gives us",
    "start": "1678520",
    "end": "1685039"
  },
  {
    "text": "another four chords but now when that container comes up it's not a free range anymore right",
    "start": "1685039",
    "end": "1691640"
  },
  {
    "text": "Linux has to constrain both of those workloads equally because they have the same request to 50 of the time",
    "start": "1691640",
    "end": "1699980"
  },
  {
    "text": "so now in each one second of real time they only get four seconds on the CPU each",
    "start": "1699980",
    "end": "1706700"
  },
  {
    "text": "but when we look at our percent of requests we're going to say okay four seconds over four cores hundred percent of requests",
    "start": "1706700",
    "end": "1714679"
  },
  {
    "text": "right hooray we solved the problem we're not overloaded anymore but is this is this a good thing did we",
    "start": "1714679",
    "end": "1720740"
  },
  {
    "text": "help I mean we still have the same eight seconds of work right that didn't change",
    "start": "1720740",
    "end": "1725900"
  },
  {
    "text": "but in the first case when we have one container it's getting it done in one second and by adding that second container",
    "start": "1725900",
    "end": "1731419"
  },
  {
    "text": "we're doubling the amount of time it takes to get that work done but if we just look at this percent of",
    "start": "1731419",
    "end": "1736460"
  },
  {
    "text": "requests which is all the HPA is doing right we could be misleading ourselves pretty",
    "start": "1736460",
    "end": "1743659"
  },
  {
    "text": "easily to think that we're actually helping that we're actually doing something good right",
    "start": "1743659",
    "end": "1749779"
  },
  {
    "text": "now okay fine requests maybe maybe it's a little misleading right maybe it's a",
    "start": "1749779",
    "end": "1755659"
  },
  {
    "text": "little not the whole picture right but maybe we should be asking ourselves",
    "start": "1755659",
    "end": "1760760"
  },
  {
    "text": "a little bit of uh you know we should back it up a little bit maybe we should have some more fundamental question",
    "start": "1760760",
    "end": "1767500"
  },
  {
    "text": "what makes us think that CPU is the right metric to scale our workloads on",
    "start": "1767720",
    "end": "1773899"
  },
  {
    "text": "at all well let's take these two web applications let's say they're both saturated they're",
    "start": "1773899",
    "end": "1780559"
  },
  {
    "text": "right they're both taking as many web requests as they possibly can",
    "start": "1780559",
    "end": "1786380"
  },
  {
    "text": "the one on the left though right every web request is getting it's just doing some data processing right number",
    "start": "1786380",
    "end": "1791419"
  },
  {
    "text": "crunching something like that that all shows up on the CPU wonderful",
    "start": "1791419",
    "end": "1796580"
  },
  {
    "text": "but more typical web applications right they don't necessarily do that let's say we're waiting on a database or maybe",
    "start": "1796580",
    "end": "1802580"
  },
  {
    "text": "we're calling out to some external thing it doesn't really matter none of that shows up as CPU utilization",
    "start": "1802580",
    "end": "1809899"
  },
  {
    "text": "so we have two wildly different CPU utilizations and the same saturation",
    "start": "1809899",
    "end": "1815360"
  },
  {
    "text": "it turns out that CPU utilization right it just tells us what the app is doing",
    "start": "1815360",
    "end": "1821020"
  },
  {
    "text": "namely whether it's spending its you know free time on the CPU",
    "start": "1821020",
    "end": "1826340"
  },
  {
    "text": "or whether it's doing something else well fine okay at this point",
    "start": "1826340",
    "end": "1832399"
  },
  {
    "text": "you know I give Shane a call back and I say Shane I don't say this enough",
    "start": "1832399",
    "end": "1838880"
  },
  {
    "text": "but you were right ah I don't like for Center requests anymore right it's it's not it doesn't I can't",
    "start": "1838880",
    "end": "1845480"
  },
  {
    "text": "trust it uh CPU you know it doesn't even work for my application what do I do what do I scale on how do I solve this",
    "start": "1845480",
    "end": "1852440"
  },
  {
    "text": "problem right and he looks at me with that confident glitt in his eyes that 39 second confident glint in his eyes and",
    "start": "1852440",
    "end": "1860740"
  },
  {
    "text": "he says",
    "start": "1860740",
    "end": "1864039"
  },
  {
    "text": "right which is what all the blogs say right requests per second scale your applications are in quest per second because of that specific CPU problem but",
    "start": "1865880",
    "end": "1872360"
  },
  {
    "text": "it turns out requests per second has kind of exactly the same problem",
    "start": "1872360",
    "end": "1878059"
  },
  {
    "text": "see we run a lot of different applications at Aquia right some let's say they're really Simple Sites maybe they're serving static content or some",
    "start": "1878059",
    "end": "1885440"
  },
  {
    "text": "right not a complicated request to fulfill so that can take a lot of requests per",
    "start": "1885440",
    "end": "1891020"
  },
  {
    "text": "second but the more complicated sites right spending a lot of time in the database maybe they have some crazy back-end",
    "start": "1891020",
    "end": "1897919"
  },
  {
    "text": "proprietary stuff or or let's be honest maybe they're just not the most optimized in the world right they can't",
    "start": "1897919",
    "end": "1904880"
  },
  {
    "text": "take as many requests per second no judgment um",
    "start": "1904880",
    "end": "1909919"
  },
  {
    "text": "requests per second just like CPU right it tells us what the application is doing namely how fast it's processing",
    "start": "1909919",
    "end": "1916279"
  },
  {
    "text": "requests but it doesn't give us that context to tell us how many more it can",
    "start": "1916279",
    "end": "1921320"
  },
  {
    "text": "do before we actually need another replica before before it's going to fall over",
    "start": "1921320",
    "end": "1927440"
  },
  {
    "text": "so we really sat back and we thought about it we said okay what do we want to scale on",
    "start": "1927440",
    "end": "1932539"
  },
  {
    "text": "and we decided that any metric that's worth scaling on needs to at the very least it needs to be highly correlated",
    "start": "1932539",
    "end": "1938779"
  },
  {
    "text": "with what's actually going on inside the application right at the very least we",
    "start": "1938779",
    "end": "1943880"
  },
  {
    "text": "wanted something smooth right we didn't want these big spikes up and down and wasting all that time scheduling all",
    "start": "1943880",
    "end": "1949340"
  },
  {
    "text": "those pods and lastly we wanted an early enough signal that we could actually respond to it right so not something",
    "start": "1949340",
    "end": "1955940"
  },
  {
    "text": "like latency which sure is very highly correlated right but if you're already",
    "start": "1955940",
    "end": "1960980"
  },
  {
    "text": "having high latency then you've already failed right you're scaling too late",
    "start": "1960980",
    "end": "1966500"
  },
  {
    "text": "see every application has this kind of curve as you apply load when the loads",
    "start": "1966500",
    "end": "1972440"
  },
  {
    "text": "light right your application can handle it no problem but maybe it's not as utilized as it otherwise could be and if",
    "start": "1972440",
    "end": "1979640"
  },
  {
    "text": "you scale at that point right you end up just kind of scaling empty right we're not taking as much uh as a good",
    "start": "1979640",
    "end": "1985940"
  },
  {
    "text": "advantage of the resources where we have available to us but if you push that load too far right you end up",
    "start": "1985940",
    "end": "1992360"
  },
  {
    "text": "introducing latency or drops or errors or crashes or ooms or whatever right",
    "start": "1992360",
    "end": "1997580"
  },
  {
    "text": "like you had well hopefully not items but you end up with issues in your application right",
    "start": "1997580",
    "end": "2003460"
  },
  {
    "text": "what we were looking for is a metric that told us where that inflection point was right right before we start",
    "start": "2003460",
    "end": "2008740"
  },
  {
    "text": "introducing issues and we wanted to set our HPA to a little bit before that",
    "start": "2008740",
    "end": "2014140"
  },
  {
    "text": "right the idea being that as we scaled up and down we would be holding the majority of our replicas in that",
    "start": "2014140",
    "end": "2022620"
  },
  {
    "text": "optimized Zone right now for us",
    "start": "2022620",
    "end": "2029080"
  },
  {
    "text": "that ended up being essentially the number of threads that were active at any one time",
    "start": "2029080",
    "end": "2034240"
  },
  {
    "text": "right it didn't tell us what those threads were doing and it didn't need to it just told us hey",
    "start": "2034240",
    "end": "2040120"
  },
  {
    "text": "they're busy they can't take another request right and when you have a fixed number of threads like Shane was saying",
    "start": "2040120",
    "end": "2046000"
  },
  {
    "text": "you know kind of how much of that you can take until you need another replica now for you this metric is very likely",
    "start": "2046000",
    "end": "2053200"
  },
  {
    "text": "to be something very different right depending on the application and the language and all that",
    "start": "2053200",
    "end": "2059020"
  },
  {
    "text": "but the point is the process we went through to find that metric it forced us to learn a lot about our",
    "start": "2059020",
    "end": "2065560"
  },
  {
    "text": "application about what happened when we apply load to it and what it looked like",
    "start": "2065560",
    "end": "2070898"
  },
  {
    "text": "when it was about to fall over right and I'm happy to report",
    "start": "2070899",
    "end": "2076358"
  },
  {
    "text": "we started rolling this out to production and we drastically reduced the churn",
    "start": "2076359",
    "end": "2081820"
  },
  {
    "text": "right turns out when you're scaling on the proper metric",
    "start": "2081820",
    "end": "2087220"
  },
  {
    "text": "maybe you don't need to be running 73 000 buds every single day",
    "start": "2087220",
    "end": "2092260"
  },
  {
    "text": "right now in our customers scale it's because they actually need to right because they",
    "start": "2092260",
    "end": "2098200"
  },
  {
    "text": "they need the threads which makes us really happy because it greatly simplifies our cluster setup",
    "start": "2098200",
    "end": "2104619"
  },
  {
    "text": "right we don't have these crazy churn and all the problems that come with it and that's a whole nother talk",
    "start": "2104619",
    "end": "2111220"
  },
  {
    "text": "um and it makes our customers happy right because a lot of customers aren't particularly CPU heavy right sometimes",
    "start": "2111220",
    "end": "2117160"
  },
  {
    "text": "they wait a long time in the database and now they scale just as well as those well better everybody scales better but",
    "start": "2117160",
    "end": "2123099"
  },
  {
    "text": "just as well as the people that used High CPU originally",
    "start": "2123099",
    "end": "2128280"
  },
  {
    "text": "there we go gang our time together is that and then but uh two things before you go",
    "start": "2128280",
    "end": "2136540"
  },
  {
    "text": "you see we got caught up that what we needed was webassembly based injected Envoy BPF",
    "start": "2136540",
    "end": "2143079"
  },
  {
    "text": "and it was the fundamentals that we had wrong the whole time and when we focused on those fundamentals that's when we got",
    "start": "2143079",
    "end": "2148540"
  },
  {
    "text": "the biggest gain and we thought someone was going to come on a silver platter with this best",
    "start": "2148540",
    "end": "2154060"
  },
  {
    "text": "practice this thing and it was always wrong because we had to measure what was going on to really understand what the",
    "start": "2154060",
    "end": "2159700"
  },
  {
    "text": "best practices were for our cluster and it turns out that Prometheus is the",
    "start": "2159700",
    "end": "2165460"
  },
  {
    "text": "way to do that but it turns out everything we knew about all that stuff was wrong too and that's an even more fascinating universe but that's a tale",
    "start": "2165460",
    "end": "2172119"
  },
  {
    "text": "for another day on a personal note gang uh I just want",
    "start": "2172119",
    "end": "2177160"
  },
  {
    "text": "to give you a heartfelt I got rejected for kubecon three times in a row and this is like my last big thing and so",
    "start": "2177160",
    "end": "2183040"
  },
  {
    "text": "the fact that it's like standing room only and 700 of you guys showed up I can't tell you what that means to us this is a big labor of love we spent",
    "start": "2183040",
    "end": "2189339"
  },
  {
    "text": "three months on it um we'd love to get your feedback if uh you dig this kind of stuff or you rather",
    "start": "2189339",
    "end": "2194500"
  },
  {
    "text": "hear from like adults on this sort of thing in the future so if you fill that out I'd be grateful gang we will stay as",
    "start": "2194500",
    "end": "2200740"
  },
  {
    "text": "long as it takes to answer all your questions we sorry we couldn't take them live uh have a wonderful rest of your",
    "start": "2200740",
    "end": "2205900"
  },
  {
    "text": "kubecon [Applause]",
    "start": "2205900",
    "end": "2210199"
  }
]