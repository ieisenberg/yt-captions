[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "welcome to sig auto scaling um this talk is going to be both a",
    "start": "80",
    "end": "6399"
  },
  {
    "text": "quick introduction to kubernetes auto scaling and a little bit of a deep dive we're going to cover some best practices",
    "start": "6399",
    "end": "14160"
  },
  {
    "text": "for running kubernetes all the scaling in production uh my name is joseph burnett i'm here with guy templeton",
    "start": "14160",
    "end": "20480"
  },
  {
    "text": "who is the co-lead of segato scaling um so he'll take over in the second half and run you through some",
    "start": "20480",
    "end": "27359"
  },
  {
    "text": "some practical tips so this is the agenda i'm also going to",
    "start": "27359",
    "end": "32480"
  },
  {
    "text": "talk on a couple um upcoming features so um you can get a",
    "start": "32480",
    "end": "37680"
  },
  {
    "text": "preview for what's coming in the future we'll save about 10 minutes for questions at the end",
    "start": "37680",
    "end": "43760"
  },
  {
    "text": "okay so kubernetes auto scaling like the rest of",
    "start": "43760",
    "end": "49520"
  },
  {
    "start": "45000",
    "end": "490000"
  },
  {
    "text": "kubernetes is sort of divided into two levels the workload which is the",
    "start": "49520",
    "end": "54960"
  },
  {
    "text": "pods and the infrastructure which are the nodes",
    "start": "54960",
    "end": "60559"
  },
  {
    "text": "and the workloads can scale out or in you can make more or fewer of",
    "start": "60559",
    "end": "66640"
  },
  {
    "text": "the same kind of pod and they can also scale up and down which is",
    "start": "66640",
    "end": "72000"
  },
  {
    "text": "making the resource requests larger or smaller and the corresponding auto scalers are",
    "start": "72000",
    "end": "78000"
  },
  {
    "text": "the hpa and the vpa the horizontal product scaler and the vertical plot out of scalar",
    "start": "78000",
    "end": "83600"
  },
  {
    "text": "so the same concepts of vertical and horizontal apply to cluster out of scaling as well cluster",
    "start": "83600",
    "end": "90240"
  },
  {
    "text": "autoscaler scales a node pool in and out by making",
    "start": "90240",
    "end": "95280"
  },
  {
    "text": "more and fewer nodes and it's also capable of scaling vertically by selecting different node",
    "start": "95280",
    "end": "100320"
  },
  {
    "text": "pools so if you have node pools of different sizes in your cluster it can make intelligent decisions about",
    "start": "100320",
    "end": "106240"
  },
  {
    "text": "which one to use but to kind of make it real concrete for you",
    "start": "106240",
    "end": "111840"
  },
  {
    "text": "i'm going to walk through a practical example so here we have a happy little cluster",
    "start": "111840",
    "end": "118799"
  },
  {
    "text": "you have two workloads deployed here there's the blue pods and there's the purple pod and they're",
    "start": "118799",
    "end": "124640"
  },
  {
    "text": "deployed on three notes now both of these workloads are auto-scaled let's say that the",
    "start": "124640",
    "end": "132080"
  },
  {
    "text": "blue and purple workloads are auto-scaled horizontally on cpu utilization and vertically on",
    "start": "132080",
    "end": "138400"
  },
  {
    "text": "memory and the cluster is auto-scaled as well so when there are new nodes",
    "start": "138400",
    "end": "144160"
  },
  {
    "text": "new nodes will be created when new nodes are needed so suppose that blue service gets some",
    "start": "144160",
    "end": "151680"
  },
  {
    "text": "more traffic and the hpa will notice that the average utilization is slightly",
    "start": "151680",
    "end": "158720"
  },
  {
    "text": "higher than the target it will figure out how many pods it needs to create",
    "start": "158720",
    "end": "163920"
  },
  {
    "text": "in order to bring that back down to the target value and it will cause those to be created",
    "start": "163920",
    "end": "170480"
  },
  {
    "text": "now the hpa doesn't actually make the pods here what it does is it is it looks it looks at the scale",
    "start": "170480",
    "end": "178239"
  },
  {
    "text": "target ref so it takes a scale sub resource it looks at the pods that are selected by the label gets",
    "start": "178239",
    "end": "185599"
  },
  {
    "text": "the current scale and when it has a recommendation when it thinks we need more it just",
    "start": "185599",
    "end": "190720"
  },
  {
    "text": "updates that scale and if this is for example pointed at a deployment that deployment will turn around and",
    "start": "190720",
    "end": "198159"
  },
  {
    "text": "adjust the size of the replica set and the replica set will create the pots and then the scheduler will see these",
    "start": "198159",
    "end": "204640"
  },
  {
    "text": "pods and it will schedule them onto a node and the node will see the pods and the pods will be",
    "start": "204640",
    "end": "210480"
  },
  {
    "text": "started so the auto scale is actually sort of just a controller around this one scale",
    "start": "210480",
    "end": "217760"
  },
  {
    "text": "sub resource knob but ultimately the cluster creates two more nodes or",
    "start": "217760",
    "end": "224239"
  },
  {
    "text": "two more pods because the auto scaler thinks that we need more so here they are and they get scheduled onto this node",
    "start": "224239",
    "end": "230080"
  },
  {
    "text": "because there's there's room for it so then uh maybe a little more traffic comes in",
    "start": "230080",
    "end": "235599"
  },
  {
    "text": "for the purple service well same thing the uh the hpa will notice that we're a little above target",
    "start": "235599",
    "end": "242560"
  },
  {
    "text": "it'll decide we need two replicas instead of one and it will adjust the deployment and a",
    "start": "242560",
    "end": "248799"
  },
  {
    "text": "pod will get created but as you can see there's not a node for this pod",
    "start": "248799",
    "end": "254239"
  },
  {
    "text": "the scheduler is unable to place it anywhere and this is when cluster auto scaler",
    "start": "254239",
    "end": "260400"
  },
  {
    "text": "kicks in so cluster auto scaler responds to unscheduled pods not like the underlying resource metrics",
    "start": "260400",
    "end": "269120"
  },
  {
    "text": "so cluster autoscaler will say oh i clearly need to scale up the cluster and what it does is",
    "start": "269120",
    "end": "276400"
  },
  {
    "text": "it runs a simulation and it says if i was able to if i create another node",
    "start": "276400",
    "end": "283120"
  },
  {
    "text": "will this pod be able to be scheduled and if the answer is yes then that's the thing to do and it can do this across multiple node",
    "start": "283120",
    "end": "289840"
  },
  {
    "text": "pools and it can do it you know and more than one node at a time so it actually has a",
    "start": "289840",
    "end": "295520"
  },
  {
    "text": "little simulation of the scheduler because it's ultimately up to the scheduler where these pots go so the cluster auto scaler says yes we",
    "start": "295520",
    "end": "302720"
  },
  {
    "text": "need another node this sounds good it creates the new node scheduler puts the",
    "start": "302720",
    "end": "308080"
  },
  {
    "text": "pod on that node and then the purple service has scaled out so",
    "start": "308080",
    "end": "314990"
  },
  {
    "text": "[Music] over time vpa which is paying attention to the actual",
    "start": "314990",
    "end": "321759"
  },
  {
    "text": "resource usage as compared to the request may notice that the purple service is asking for way",
    "start": "321759",
    "end": "328560"
  },
  {
    "text": "more memory than it needs so it's going to draw a little histogram and figure out like a conservative place",
    "start": "328560",
    "end": "334400"
  },
  {
    "text": "to draw the line and it's going to recommend a smaller pod size so the recommender makes this change and",
    "start": "334400",
    "end": "340639"
  },
  {
    "text": "then um there's an updated process that comes through",
    "start": "340639",
    "end": "345840"
  },
  {
    "text": "and we'll carefully delete the pods one by one and cause them to be recreated so the",
    "start": "345840",
    "end": "350960"
  },
  {
    "text": "updater is going to delete this pod and a new new pod will be recreated because the replica site will always try",
    "start": "350960",
    "end": "357039"
  },
  {
    "text": "to replace the pods and it will be recreated with a new smaller size",
    "start": "357039",
    "end": "363039"
  },
  {
    "text": "so this is this is how vpa actually causes pods to change their",
    "start": "363199",
    "end": "369120"
  },
  {
    "text": "size and you may notice that it's actually a little bit disruptive because you actually have to delete pods",
    "start": "369120",
    "end": "376319"
  },
  {
    "text": "a really important part about using auto scaling effectively is being able to control and manage that",
    "start": "376319",
    "end": "383199"
  },
  {
    "text": "disruption and so guy is going to tell you in a little bit how you can do that point to some tools",
    "start": "383199",
    "end": "390000"
  },
  {
    "text": "and best practices for managing disruption um [Music]",
    "start": "390000",
    "end": "395759"
  },
  {
    "text": "then vpa updater is going to delete the next pod and it will get recreated as well and",
    "start": "395759",
    "end": "402800"
  },
  {
    "text": "you'll notice that actually there's an empty node here now and cluster autoscaler will notice this too",
    "start": "402800",
    "end": "409360"
  },
  {
    "text": "it's able to delete that node and scale the cluster in so you can see the cluster auto scaler",
    "start": "409360",
    "end": "415280"
  },
  {
    "text": "goes out and in usually these nodes are what you pay money for so it's good that it can scale",
    "start": "415280",
    "end": "421599"
  },
  {
    "text": "down because that's what's actually going to cost that's what's going to save you money",
    "start": "421599",
    "end": "427199"
  },
  {
    "text": "um now suppose that maybe at the end of the day the blue service",
    "start": "427199",
    "end": "433280"
  },
  {
    "text": "loses some of its traffic and just this hpa decides to naturally scale down because utilization is really low",
    "start": "433280",
    "end": "440560"
  },
  {
    "text": "some more pods will be deleted and they're not going to get rescheduled because they just don't need them anymore",
    "start": "440560",
    "end": "446080"
  },
  {
    "text": "so you may be in a situation where um you could actually fit more on the nodes",
    "start": "446080",
    "end": "452800"
  },
  {
    "text": "that you have and cluster auto scaler will notice this and it will go and it will delete",
    "start": "452800",
    "end": "458479"
  },
  {
    "text": "a pod so that it can be rescheduled onto the existing nodes in effect it",
    "start": "458479",
    "end": "464960"
  },
  {
    "text": "will do some defragmentation for you so again it can free up nodes uh",
    "start": "464960",
    "end": "471840"
  },
  {
    "text": "and scale your cluster in so this is kind of a quick view of the way that vertical and horizontal auto",
    "start": "471840",
    "end": "478400"
  },
  {
    "text": "scaling go in and out the cluster goes out and back in again",
    "start": "478400",
    "end": "484800"
  },
  {
    "text": "and yeah that's that's the summary of the problem space so this is the end state of your cluster",
    "start": "485120",
    "end": "493199"
  },
  {
    "start": "490000",
    "end": "900000"
  },
  {
    "text": "and uh i wanted to mention a couple upcoming features so i talked a little",
    "start": "493199",
    "end": "499280"
  },
  {
    "text": "bit about cpu utilization or memory utilization and when you use hpa or vpa you point",
    "start": "499280",
    "end": "507199"
  },
  {
    "text": "at a target um you point one of these resource metrics and you say i'd like you to shoot for this now",
    "start": "507199",
    "end": "513680"
  },
  {
    "text": "there can be more than one container inside of the pod and uh an upcoming feature for hpa",
    "start": "513680",
    "end": "521680"
  },
  {
    "text": "allows you to set individual targets for each container for example if you",
    "start": "521680",
    "end": "527360"
  },
  {
    "text": "have a side car that it is likes to use a lot of cpu you don't necessarily need to scale on",
    "start": "527360",
    "end": "534160"
  },
  {
    "text": "that sidecar and it may overwhelm your calculations or if you have a sidecar that has a very generous resource",
    "start": "534160",
    "end": "539600"
  },
  {
    "text": "request and it's mostly idle it may water down your your metrics and",
    "start": "539600",
    "end": "544800"
  },
  {
    "text": "so take a look at this cap here the it's already gone through api review and it's",
    "start": "544800",
    "end": "549839"
  },
  {
    "text": "just going to land very soon and it's a nice way to focus the hp on",
    "start": "549839",
    "end": "556160"
  },
  {
    "text": "specifically which containers are most important to you in your auto scaling",
    "start": "556160",
    "end": "561600"
  },
  {
    "text": "and the second thing that we're working on in the cigar scaling group is graduating the v2 beta 2",
    "start": "561600",
    "end": "568720"
  },
  {
    "text": "api to stable it's been around for quite a few years and it definitely needs to get graduated",
    "start": "568720",
    "end": "576640"
  },
  {
    "text": "the all of the ability to scale on multiple metrics and custom metrics are a part of the v2",
    "start": "576640",
    "end": "584560"
  },
  {
    "text": "api so they've been around for a while so we're going to work on making sure taking all the boxes and making sure we",
    "start": "584560",
    "end": "590320"
  },
  {
    "text": "can get it to a stable state so that's a little bit about auto scaling i'm going to turn it over to guy",
    "start": "590320",
    "end": "597600"
  },
  {
    "text": "now who will walk you through some practical tips about running kubernetes auto scaling",
    "start": "597600",
    "end": "603839"
  },
  {
    "text": "and production okay thanks um so just for a bit of context",
    "start": "603839",
    "end": "611200"
  },
  {
    "text": "here i'm a senior software engineer at skyscanner and so a lot of what we do",
    "start": "611200",
    "end": "616320"
  },
  {
    "text": "is running workloads that are involved in scrape either getting prices from um",
    "start": "616320",
    "end": "623440"
  },
  {
    "text": "online travel agents or airlines um and then doing filtering and that sort of thing",
    "start": "623440",
    "end": "628720"
  },
  {
    "text": "this will this will come into play in a couple of the examples i get for things that we should consider",
    "start": "628720",
    "end": "634720"
  },
  {
    "text": "um so i think us a lot of people uh watching will know um a lot of the",
    "start": "634720",
    "end": "641040"
  },
  {
    "text": "hardest problems in computing on technical problems they're people problems and this is one of those spaces where",
    "start": "641040",
    "end": "646560"
  },
  {
    "text": "the complexity of all the options and all the the tunable parameters and knobs that",
    "start": "646560",
    "end": "653200"
  },
  {
    "text": "kubernetes gives developers and cluster admins um can overwhelm developers especially when",
    "start": "653200",
    "end": "659040"
  },
  {
    "text": "they're first getting started and i think a lot of the time developers",
    "start": "659040",
    "end": "664240"
  },
  {
    "text": "can not know exactly what what options they need to consider when looking to scale workloads safely and perform",
    "start": "664240",
    "end": "673040"
  },
  {
    "text": "in a performant way so i'm going to go over a few of these options that i",
    "start": "673040",
    "end": "679200"
  },
  {
    "text": "think developers should at least be aware of and potentially if you're cluster admins and providing your",
    "start": "679200",
    "end": "685440"
  },
  {
    "text": "developers same defaults to work from and potentially override if your",
    "start": "685440",
    "end": "690800"
  },
  {
    "text": "developers need to or want to um so the first one is lifecycle hooks so",
    "start": "690800",
    "end": "699519"
  },
  {
    "text": "in our in skyscanner's case um we occasionally have some very long running requests for that's to",
    "start": "699519",
    "end": "705680"
  },
  {
    "text": "fetch prices or uh reach out to partners who will uh make bookings with um and in that",
    "start": "705680",
    "end": "711600"
  },
  {
    "text": "case we don't want to drop an in-flight connection especially if it's making a booking and so instead what we want to do is set",
    "start": "711600",
    "end": "718880"
  },
  {
    "text": "up our pods such that when they're terminated by scaling action wherever possible",
    "start": "718880",
    "end": "724639"
  },
  {
    "text": "um they gracefully terminate and finish the work that they're currently doing before shutting down and so this is where um",
    "start": "724639",
    "end": "732320"
  },
  {
    "text": "pre-stock lifecycle hooks can come in so if if you define a pre-stop lifecycle hook which can either",
    "start": "732320",
    "end": "738240"
  },
  {
    "text": "be an hp request or an exec so a command into the pod to call a binary for instance and",
    "start": "738240",
    "end": "745440"
  },
  {
    "text": "the the cubelet when terminating that pod will uh execute that lifecycle at the same",
    "start": "745440",
    "end": "752959"
  },
  {
    "text": "time sending in a sig term so you can either set up your application to handle the sick term gracefully or",
    "start": "752959",
    "end": "758000"
  },
  {
    "text": "instead you can make use of say an api endpoint that can be hit by the the lifecycle hook use that to gracefully",
    "start": "758000",
    "end": "765920"
  },
  {
    "text": "finish any work and refuse new work potentially and then allow the cubelet",
    "start": "765920",
    "end": "771040"
  },
  {
    "text": "to shut it down and so these pre-stock hooks they're blocking and will block up to a grace period",
    "start": "771040",
    "end": "777519"
  },
  {
    "text": "so if you need more than the default 30 seconds to gracefully terminate you'll need to set this up in your podspec",
    "start": "777519",
    "end": "783200"
  },
  {
    "text": "but by default it'll give you 30 seconds for that command to complete and you can also make uh use of what are",
    "start": "783200",
    "end": "789360"
  },
  {
    "text": "called pulse start hooks um and again these can be hp or exec central pods",
    "start": "789360",
    "end": "794880"
  },
  {
    "text": "they're not guaranteed to run before the container's entry point but if you need",
    "start": "794880",
    "end": "800639"
  },
  {
    "text": "to do some sort of environment dependent setup work you can make use of those",
    "start": "800639",
    "end": "806160"
  },
  {
    "text": "and the next thing is loudness and readiness probes so these are these are fairly key",
    "start": "806160",
    "end": "812800"
  },
  {
    "text": "if your workloads for instance on startup need to do some sort of uh runtime compilation or something",
    "start": "812800",
    "end": "819120"
  },
  {
    "text": "similar to um ensure that you're giving your users the best possible experience",
    "start": "819120",
    "end": "824800"
  },
  {
    "text": "and so in this case you can use license probes to mark your products alive and healthy so the cube that won't",
    "start": "824800",
    "end": "831519"
  },
  {
    "text": "restart the pod however you can use readiness probes to state that the pod is currently not",
    "start": "831519",
    "end": "837680"
  },
  {
    "text": "ready to receive um traffic and that can be particularly useful uh in combination",
    "start": "837680",
    "end": "844320"
  },
  {
    "text": "with phk so the hpa um when a pod is not marked ready so if it's doing some cpu intensive",
    "start": "844320",
    "end": "850639"
  },
  {
    "text": "uh work to begin with the hpa will not um consider those that pods metrics",
    "start": "850639",
    "end": "857920"
  },
  {
    "text": "until it is marked ready so that that ensures that the hpa is not seeing new pods come up use a lot of cpu",
    "start": "857920",
    "end": "864800"
  },
  {
    "text": "and then over going oh i need to over scale again and the next thing is pod annotation so",
    "start": "864800",
    "end": "872160"
  },
  {
    "text": "in the case that you might have say a machine learning workload where if you interrupt that pod um via scaling",
    "start": "872160",
    "end": "879120"
  },
  {
    "text": "or the cluster autoscalers trying to bin pack uh you might end up with the workload",
    "start": "879120",
    "end": "884639"
  },
  {
    "text": "interrupted and effectively losing work or data and in this case you can mark",
    "start": "884639",
    "end": "889760"
  },
  {
    "text": "pods with this cluster auto scaler uh safe to evict annotation and if you mark",
    "start": "889760",
    "end": "895440"
  },
  {
    "text": "that as safe to vect equals false effectively when the cluster auto scaler is considering bin packing",
    "start": "895440",
    "end": "901199"
  },
  {
    "start": "900000",
    "end": "1140000"
  },
  {
    "text": "it will notice that that node has a pod on it which is not safe to event and therefore not consider it as a candidate",
    "start": "901199",
    "end": "907519"
  },
  {
    "text": "for scale down and so theoretically you can spin up workloads which are marked as",
    "start": "907519",
    "end": "912959"
  },
  {
    "text": "safe to evict equals false and then either update that annotation or remove it to safety vector equals true",
    "start": "912959",
    "end": "919600"
  },
  {
    "text": "at the point where they are then safe to evict again and to ensure that your workloads are not",
    "start": "919600",
    "end": "925440"
  },
  {
    "text": "interrupted by the cluster auto scaler it's important to note that this doesn't prevent any interruption so",
    "start": "925440",
    "end": "931920"
  },
  {
    "text": "hardware failure etc or manual deletion of nodes will still impact that pod but the cluster autoscaler won't",
    "start": "931920",
    "end": "939040"
  },
  {
    "text": "interrupt it um and vice versa you can also use that",
    "start": "939040",
    "end": "944160"
  },
  {
    "text": "annotation set true to override some other behaviors which generally the",
    "start": "944160",
    "end": "949600"
  },
  {
    "text": "cluster autoscaler will mark pods as not safe to vet and so things like if it's if a pods using local",
    "start": "949600",
    "end": "956320"
  },
  {
    "text": "storage the cluster auto scaler by default will consider that pod as being unsafe to vect because it doesn't know what data",
    "start": "956320",
    "end": "963360"
  },
  {
    "text": "it's writing to the local disk it doesn't want to cause data loss therefore it's uh it will",
    "start": "963360",
    "end": "969600"
  },
  {
    "text": "not try and move that pod around however if you can tolerate the loss of that data then you can mark it if um yourself",
    "start": "969600",
    "end": "977519"
  },
  {
    "text": "as safe to evict in the cluster auto scale would take that into consideration and so pod disruption budgets are",
    "start": "977519",
    "end": "985040"
  },
  {
    "text": "probably one of the most important thing so um going back to joseph's example of the vpa",
    "start": "985040",
    "end": "991199"
  },
  {
    "text": "where it was recreating pods um due to changing the uh resources given",
    "start": "991199",
    "end": "997600"
  },
  {
    "text": "to them the cholesterol scaler and vpa both respect the pod disruption budget so this this",
    "start": "997600",
    "end": "1004000"
  },
  {
    "text": "is a really granular way of service owners being able to define how much disruption their",
    "start": "1004000",
    "end": "1009680"
  },
  {
    "text": "their workload can tolerate so if a given service can tolerate say 20 of its pods at any one time being moved around",
    "start": "1009680",
    "end": "1016560"
  },
  {
    "text": "and being unavailable um without harming the user experience",
    "start": "1016560",
    "end": "1021839"
  },
  {
    "text": "the the pod disruption budget is the way of defining that and then the uh claustrophobic skill and",
    "start": "1021839",
    "end": "1028079"
  },
  {
    "text": "vertical pod auto scaler will take that disruption into account and therefore never",
    "start": "1028079",
    "end": "1034079"
  },
  {
    "text": "uh perform a disruptive action which would cause that budget to be exceeded um it's important",
    "start": "1034079",
    "end": "1040959"
  },
  {
    "text": "to note that this is this is the way to define this so that other components",
    "start": "1040959",
    "end": "1046480"
  },
  {
    "text": "respect how services want to be moved around deployment rollout strategies which are",
    "start": "1046480",
    "end": "1052559"
  },
  {
    "text": "a sort of more course uh setting that people may be more familiar with",
    "start": "1052559",
    "end": "1057760"
  },
  {
    "text": "those aren't respected by these tools because they're that's not what they're set up to look",
    "start": "1057760",
    "end": "1064160"
  },
  {
    "text": "at and finally for this slide uh pod priorities so this this allows you to decide which",
    "start": "1064160",
    "end": "1071120"
  },
  {
    "text": "workloads are um or should be prioritized when your cluster currently doesn't have enough capacity",
    "start": "1071120",
    "end": "1076799"
  },
  {
    "text": "for all pods so if we think back to joseph's example again where one of the services",
    "start": "1076799",
    "end": "1082320"
  },
  {
    "text": "was scaled up and there was not enough room in the cluster for all of those pods",
    "start": "1082320",
    "end": "1087679"
  },
  {
    "text": "um if instead one of those workloads was the workload that had been scaled up was",
    "start": "1087679",
    "end": "1093280"
  },
  {
    "text": "of higher priority than the than some of the existing pods uh some of those existing pods if they",
    "start": "1093280",
    "end": "1100240"
  },
  {
    "text": "could have been evicted and higher priority pods being scheduled those would have been evicted so you can",
    "start": "1100240",
    "end": "1106720"
  },
  {
    "text": "effectively say if if there's not currently enough room on the cluster i want to prioritize this",
    "start": "1106720",
    "end": "1112320"
  },
  {
    "text": "user impacting workload and i'm okay with this um background task workload which i'd",
    "start": "1112320",
    "end": "1119760"
  },
  {
    "text": "marked as lower priority being evicted for a period of time the cluster autoscaler would still then",
    "start": "1119760",
    "end": "1124960"
  },
  {
    "text": "um kick in once those lower priority pods were unscheduleable and scale up the cluster",
    "start": "1124960",
    "end": "1130400"
  },
  {
    "text": "so that they could be scheduled again but it allows you to get the higher",
    "start": "1130400",
    "end": "1135520"
  },
  {
    "text": "priority workload scheduled faster",
    "start": "1135520",
    "end": "1139200"
  },
  {
    "start": "1140000",
    "end": "1365000"
  },
  {
    "text": "so horizontal product scaling and so there's a few things here especially around making sure that you",
    "start": "1143760",
    "end": "1150880"
  },
  {
    "text": "you optimize your scaling with respect to cost so in potentially you could have a",
    "start": "1150880",
    "end": "1157039"
  },
  {
    "text": "service owner misconfigure their horizontal product to scaling rules maybe a metric that's that's never quite uh",
    "start": "1157039",
    "end": "1165600"
  },
  {
    "text": "reached a target utilization um which can lead to runaway scaling one",
    "start": "1165600",
    "end": "1170640"
  },
  {
    "text": "one way of handling this is name space level resource quotas so this allows you to effectively",
    "start": "1170640",
    "end": "1176320"
  },
  {
    "text": "um say in a number of dimensions so pods cpu and memory what is the maximum uh resources this",
    "start": "1176320",
    "end": "1184480"
  },
  {
    "text": "namespace can ever make use of so never let this never let pods in this namespace uh take",
    "start": "1184480",
    "end": "1191440"
  },
  {
    "text": "up more than 100 cpu cores say and that that will um the horizontal",
    "start": "1191440",
    "end": "1196880"
  },
  {
    "text": "scale will still work it will try and increase the desired uh capacity of whatever its target is",
    "start": "1196880",
    "end": "1202559"
  },
  {
    "text": "but the resource quota will then prevent new pods which would cause that resource",
    "start": "1202559",
    "end": "1208080"
  },
  {
    "text": "quota to be exceeded um from being created so it's really important for",
    "start": "1208080",
    "end": "1213440"
  },
  {
    "text": "preventing any runaway scaling potentially costing you hundreds or even thousands of uh uh",
    "start": "1213440",
    "end": "1221039"
  },
  {
    "text": "we then have uh so metrics sources for non-resource metric based auto scaling so this has been covered in great detail by",
    "start": "1221039",
    "end": "1228240"
  },
  {
    "text": "uh previous psychotic scaling cube contacts but you can scale as well as on resource metrics on",
    "start": "1228240",
    "end": "1235200"
  },
  {
    "text": "custom or external metrics so the in this case a lot of cases um there will be a better metric",
    "start": "1235200",
    "end": "1242559"
  },
  {
    "text": "than say cpu or memory utilization for scaling on so you could that could be number of hp",
    "start": "1242559",
    "end": "1248000"
  },
  {
    "text": "requests in flight or something similar and that's that's a way of optimizing the scaling",
    "start": "1248000",
    "end": "1255120"
  },
  {
    "text": "for far more than cpu for instance um we've also got pod affinities so if",
    "start": "1255120",
    "end": "1261360"
  },
  {
    "text": "you want to spread your pods out or schedule a workload together and for best performance",
    "start": "1261360",
    "end": "1269440"
  },
  {
    "text": "then potentially you need to set up pod affinities so you can either make these two nodes",
    "start": "1269440",
    "end": "1275679"
  },
  {
    "text": "or pod based um so that you can try and spread spread and pods out to maximize",
    "start": "1275679",
    "end": "1283120"
  },
  {
    "text": "fault tolerance and there are the pod affinities however are quite coarse and there's a new",
    "start": "1283120",
    "end": "1290000"
  },
  {
    "text": "concept coming in and that is slightly better for these um as joseph already mentioned beware of",
    "start": "1290000",
    "end": "1296960"
  },
  {
    "text": "the current behavior on uh resource metrics is the fact that they're across the entire pod",
    "start": "1296960",
    "end": "1302559"
  },
  {
    "text": "and it's particularly uh an issue if for instance you're injecting side cars like you've got a service mesh or",
    "start": "1302559",
    "end": "1309360"
  },
  {
    "text": "you're injecting login containers or something like that and it's particularly if you've got",
    "start": "1309360",
    "end": "1314720"
  },
  {
    "text": "small containers and you're running say uh istio you're injecting an sdom boyside car",
    "start": "1314720",
    "end": "1320159"
  },
  {
    "text": "if that's doing a lot of work you can end up causing unintended scale up particularly if your developers aren't aware of these um",
    "start": "1320159",
    "end": "1326880"
  },
  {
    "text": "and finally the the more fine-grained version of pod affinities topology",
    "start": "1326880",
    "end": "1333440"
  },
  {
    "text": "spread constraints and so this is only in beta as of 118 so um",
    "start": "1333440",
    "end": "1338559"
  },
  {
    "text": "i'm guessing a lot of people won't have had a chance to play around with it however it's implemented as a scheduling",
    "start": "1338559",
    "end": "1344400"
  },
  {
    "text": "plugin so it allows you to define either hard or soft scheduling constraints to scheduling pods over different failure zones",
    "start": "1344400",
    "end": "1350640"
  },
  {
    "text": "whether that's a host or a rack or an availability zone in the cloud and it",
    "start": "1350640",
    "end": "1356320"
  },
  {
    "text": "provides more flexibility than pod affinities and anti-affinities and allows you to try and balance your",
    "start": "1356320",
    "end": "1361840"
  },
  {
    "text": "um pods as best you can over as many um fault zones as you can",
    "start": "1361840",
    "end": "1368960"
  },
  {
    "start": "1365000",
    "end": "1415000"
  },
  {
    "text": "and so for an example of this and you can see this graph here every color represents a different",
    "start": "1368960",
    "end": "1376480"
  },
  {
    "text": "instance type um and this is one service in sky scanner and you can see despite the fact that",
    "start": "1376480",
    "end": "1382640"
  },
  {
    "text": "skyscanner currently support over 20 instance types in our clusters",
    "start": "1382640",
    "end": "1388159"
  },
  {
    "text": "uh over 50 of this services uh pods are scheduled on just two",
    "start": "1388159",
    "end": "1393600"
  },
  {
    "text": "instance types increasing our potential impact of a loss of one or both of those instance",
    "start": "1393600",
    "end": "1399760"
  },
  {
    "text": "types due to spot outage or something similar and this is without using uh",
    "start": "1399760",
    "end": "1405840"
  },
  {
    "text": "the spread constraints um we'd really like to make use of them to obviously limit behavior like this going",
    "start": "1405840",
    "end": "1414840"
  },
  {
    "text": "forward uh vertical auto scaling so memory based",
    "start": "1414840",
    "end": "1420159"
  },
  {
    "start": "1415000",
    "end": "1537000"
  },
  {
    "text": "vertical auto scaling isn't suitable for every language um i'm being a bit facetious here in that jvm",
    "start": "1420159",
    "end": "1425679"
  },
  {
    "text": "languages can now be tuned through jvm parameters etc to",
    "start": "1425679",
    "end": "1431520"
  },
  {
    "text": "give kubernetes a far better view into what the actual memory usage of a pod is um however it takes a lot of expertise",
    "start": "1431520",
    "end": "1437600"
  },
  {
    "text": "so it's not it's not simple um i i would suggest it's probably not uh",
    "start": "1437600",
    "end": "1443200"
  },
  {
    "text": "worth heading down that rabbit hole unless you really know what you're doing in terms of like tuning jbm heat parameters etc um",
    "start": "1443200",
    "end": "1450720"
  },
  {
    "text": "but it can be done most of the time i would suggest it's probably not worth doing though um you can also try combining",
    "start": "1450720",
    "end": "1458159"
  },
  {
    "text": "horizontal and vertical quadratic scaling on the same metrics say so if you tried say horizontal uh",
    "start": "1458159",
    "end": "1464320"
  },
  {
    "text": "horizontal and vertical polar auto scaling on memory on the same workload uh this will",
    "start": "1464320",
    "end": "1470400"
  },
  {
    "text": "generally lead to unintended behavior the two will end up fighting with each other and",
    "start": "1470400",
    "end": "1476159"
  },
  {
    "text": "generally one or both will end up not understand understanding what has gone on",
    "start": "1476159",
    "end": "1482000"
  },
  {
    "text": "due to resources changing underneath it and so generally you can you can combine",
    "start": "1482000",
    "end": "1488080"
  },
  {
    "text": "horizontal and vertical auto scaling on different metrics but don't don't try doing both of them on the same metric at the",
    "start": "1488080",
    "end": "1494400"
  },
  {
    "text": "same time and finally ensure resource policy is",
    "start": "1494400",
    "end": "1499760"
  },
  {
    "text": "the same and so this is this is taking up the same role as the resource quarters we're taking with the",
    "start": "1499760",
    "end": "1505679"
  },
  {
    "text": "cluster auto skate uh with the horizontal paradox again sorry and so this is preventing runaway",
    "start": "1505679",
    "end": "1511200"
  },
  {
    "text": "scaling by saying this is the maximum size i ever want a certain container within",
    "start": "1511200",
    "end": "1517440"
  },
  {
    "text": "this target to scale to so you can tell make sure that the vertical auto",
    "start": "1517440",
    "end": "1523440"
  },
  {
    "text": "scale never gets into a situation where it's runaway scaling up and a given container or a given pod to the",
    "start": "1523440",
    "end": "1530799"
  },
  {
    "text": "point where it's you're getting ever bigger nodes and costing yourself money again",
    "start": "1530799",
    "end": "1537039"
  },
  {
    "text": "uh so cluster of finally cluster auto scaling and so the the number one tip i would",
    "start": "1538880",
    "end": "1544480"
  },
  {
    "text": "have is to prioritize node startup time so this um not only has the benefit of your cluster",
    "start": "1544480",
    "end": "1551120"
  },
  {
    "text": "scaling up faster pods being able to be scheduled from unscheduleable faster and it also allows you to tune a number",
    "start": "1551120",
    "end": "1557360"
  },
  {
    "text": "of parameters from their defaults on the cluster autoscaler so you're more tolerant to the cluster autoscaler realizing that",
    "start": "1557360",
    "end": "1565679"
  },
  {
    "text": "nodes have developed a fault whether that's due to cubelet dying or a hardware failure or something",
    "start": "1565679",
    "end": "1572000"
  },
  {
    "text": "similar also beware that occasionally the",
    "start": "1572000",
    "end": "1577279"
  },
  {
    "text": "simulation uh that joseph mentioned inside the cluster autoscaler so when it",
    "start": "1577279",
    "end": "1582400"
  },
  {
    "text": "goes if i bring up a new node of this node group what will it look like what will its resources be",
    "start": "1582400",
    "end": "1587679"
  },
  {
    "text": "and occasionally that doesn't always match reality uh you can occasionally see uh instances in the cloud where for",
    "start": "1587679",
    "end": "1594720"
  },
  {
    "text": "instance the memory varies slightly from node to node and potentially the cluster autoscaler will",
    "start": "1594720",
    "end": "1600799"
  },
  {
    "text": "think that it can schedule a new pod on a new node in a given node group but that doesn't actually",
    "start": "1600799",
    "end": "1607360"
  },
  {
    "text": "match when the new node is brought up and finally and should always ensure",
    "start": "1607360",
    "end": "1612720"
  },
  {
    "text": "this estimate you have enough resources on the nodes so you can set up um how much",
    "start": "1612720",
    "end": "1618720"
  },
  {
    "text": "resources are reserved for the cubelet in the system to make sure that they are operating",
    "start": "1618720",
    "end": "1624240"
  },
  {
    "text": "however if for instance you're bringing up really big nodes um and you're scheduling a load of pods on them that",
    "start": "1624240",
    "end": "1629679"
  },
  {
    "text": "are trying to do a load of cpu work at startup time you could if you've not set these high enough you can end up either starving",
    "start": "1629679",
    "end": "1636320"
  },
  {
    "text": "the system or the cubelet uh result in effectively uh killing off the cubelet because of",
    "start": "1636320",
    "end": "1642880"
  },
  {
    "text": "resource starvation at which point all the pods that have just been scheduled and done cpu work on that node become unready",
    "start": "1642880",
    "end": "1649279"
  },
  {
    "text": "uh get potentially are killed try to be scheduled onto newport new nodes",
    "start": "1649279",
    "end": "1654480"
  },
  {
    "text": "end up in a cycle of doom and giving you a very bad afternoon as slowly you've",
    "start": "1654480",
    "end": "1659840"
  },
  {
    "text": "killed more and more of the cluster due to resource starvation",
    "start": "1659840",
    "end": "1665120"
  },
  {
    "start": "1665000",
    "end": "1693000"
  },
  {
    "text": "that was pretty much it in terms of uh sort of best practice tips um we've got a couple of links um to",
    "start": "1666240",
    "end": "1673679"
  },
  {
    "text": "previous auto scaling talk so the kubecon eu earlier in the year there's some good",
    "start": "1673679",
    "end": "1679200"
  },
  {
    "text": "stuff there on custom metrics and other bits and we've",
    "start": "1679200",
    "end": "1684559"
  },
  {
    "text": "got a couple of links as well to some nice blog posts about best practices for autoscaling",
    "start": "1684559",
    "end": "1692399"
  },
  {
    "text": "thank you very much and any questions",
    "start": "1692399",
    "end": "1697840"
  },
  {
    "start": "1693000",
    "end": "2053000"
  },
  {
    "text": "hello uh joseph brennan here i'm here for some live q a and we've gotten",
    "start": "1703919",
    "end": "1710960"
  },
  {
    "text": "quite a few good questions already in the chat and i tried to answer as many as we could me",
    "start": "1710960",
    "end": "1716159"
  },
  {
    "text": "and guy have both been going through there and answering questions but i'll take a few now um",
    "start": "1716159",
    "end": "1724480"
  },
  {
    "text": "let's see here um [Music]",
    "start": "1724480",
    "end": "1731130"
  },
  {
    "text": "i think this one's answered sorry just give me a moment",
    "start": "1732240",
    "end": "1739840"
  },
  {
    "text": "so there was a question here um we would like to scale based on custom metrics",
    "start": "1740799",
    "end": "1747919"
  },
  {
    "text": "our team needs to be able to scale based on the kafka leg as mentioned several times right now you",
    "start": "1748799",
    "end": "1754559"
  },
  {
    "text": "have to overcome it we have to over provision our coffee",
    "start": "1754559",
    "end": "1767840"
  },
  {
    "text": "thank you",
    "start": "1778640",
    "end": "1781840"
  },
  {
    "text": "then the resource metrics and so you need to stall an adapter in",
    "start": "1787520",
    "end": "1794080"
  },
  {
    "text": "your cluster which will be able to fetch those metrics",
    "start": "1794080",
    "end": "1800399"
  },
  {
    "text": "in for example in gke you can install stackdriver adapter and so so wherever that kafka leg is you",
    "start": "1800399",
    "end": "1807919"
  },
  {
    "text": "can install an adapter to get to it and set it up as an external metric you can",
    "start": "1807919",
    "end": "1813039"
  },
  {
    "text": "target an average value and just say for whatever the lag is we would like",
    "start": "1813039",
    "end": "1818240"
  },
  {
    "text": "you know this many pods for this period of this number of undelivered messages",
    "start": "1818240",
    "end": "1823679"
  },
  {
    "text": "or you know take a look at those external metric docs",
    "start": "1823679",
    "end": "1832399"
  },
  {
    "text": "and that should be something that can help it's definitely possible to do and if you have questions about it stop",
    "start": "1832399",
    "end": "1837919"
  },
  {
    "text": "by the slack channel auto scaling selection on the kubernetes",
    "start": "1837919",
    "end": "1844640"
  },
  {
    "text": "workspace okay let me go on to another question here hopefully that answered",
    "start": "1844960",
    "end": "1850880"
  },
  {
    "text": "that um so uh carl asked a question if a target",
    "start": "1850880",
    "end": "1859440"
  },
  {
    "text": "raw value is set the raw metric values are used directly the controller then takes the",
    "start": "1859440",
    "end": "1865919"
  },
  {
    "text": "mean of utilization of the raw value could you please elaborate",
    "start": "1865919",
    "end": "1871600"
  },
  {
    "text": "so it sounds like carl's talking about external metrics and maybe i'll take a moment to clarify",
    "start": "1871600",
    "end": "1878799"
  },
  {
    "text": "something that i think is kind of confusing about the api um",
    "start": "1878799",
    "end": "1885120"
  },
  {
    "text": "when you specify a target for external metrics you can say value",
    "start": "1885120",
    "end": "1891919"
  },
  {
    "text": "or average value and these two behave differently when you say i would like to",
    "start": "1891919",
    "end": "1899600"
  },
  {
    "text": "target this value for this external metric the hpa by default every 15 seconds",
    "start": "1899600",
    "end": "1906799"
  },
  {
    "text": "we'll take a look at where the metric is now and where you want to be and it will it will scale out or scale",
    "start": "1906799",
    "end": "1913519"
  },
  {
    "text": "in to try to achieve that and it will keep doing that so if the metric doesn't change it'll",
    "start": "1913519",
    "end": "1920080"
  },
  {
    "text": "keep scaling up up up up or down down down whichever way you need to go in a feedback loop",
    "start": "1920080",
    "end": "1926159"
  },
  {
    "text": "however if you set average value not value but average value then instead",
    "start": "1926159",
    "end": "1932320"
  },
  {
    "text": "it actually reconciles directly to the metric without regard to how many pods there are currently",
    "start": "1932320",
    "end": "1938880"
  },
  {
    "text": "so if for example you say i would like an average value of five pods",
    "start": "1938880",
    "end": "1945919"
  },
  {
    "text": "your five undelivered messages per pod so you're targeting an average value of five on an undelivered message q",
    "start": "1945919",
    "end": "1954640"
  },
  {
    "text": "then you won't get more pods if the value doesn't change you'll just keep reconciling the same number",
    "start": "1954640",
    "end": "1962559"
  },
  {
    "text": "so i hope that answers that question it's a little bit subtle but",
    "start": "1962559",
    "end": "1967200"
  },
  {
    "text": "it could be clarified by like maybe looking at the examples so if you're reconciling",
    "start": "1967919",
    "end": "1975279"
  },
  {
    "text": "undelivered messages average value is the one that you want okay um guy were there any that you",
    "start": "1975279",
    "end": "1983600"
  },
  {
    "text": "wanted to pick out and answer um i'm just just having a quick look",
    "start": "1983600",
    "end": "1989360"
  },
  {
    "text": "um i think there was there was one uh about whether there were recommendations",
    "start": "1989360",
    "end": "1994960"
  },
  {
    "text": "about adding cholesterol to scale or annotations to pods um the way um i've",
    "start": "1994960",
    "end": "2001360"
  },
  {
    "text": "achieved this in the past is giving the um the pods in the deployment themselves and the service account and allowing",
    "start": "2001360",
    "end": "2008880"
  },
  {
    "text": "that service account to um add and remove annotations to those",
    "start": "2008880",
    "end": "2014000"
  },
  {
    "text": "those pods so effectively they're only allowed to modify themselves and that way they can implement that",
    "start": "2014000",
    "end": "2021679"
  },
  {
    "text": "um but i think that's pretty much us out of time for answering questions",
    "start": "2021679",
    "end": "2027120"
  },
  {
    "text": "on the call um however we're going to continue trying to answer any questions people",
    "start": "2027120",
    "end": "2032159"
  },
  {
    "text": "may have had or we haven't managed to get to on um flat channel um to dash kubecon",
    "start": "2032159",
    "end": "2039279"
  },
  {
    "text": "maintainer so and if you've got any questions that we didn't answer feel free to post them over there and we'll",
    "start": "2039279",
    "end": "2045519"
  },
  {
    "text": "try and get to them",
    "start": "2045519",
    "end": "2048240"
  },
  {
    "text": "thank you very much",
    "start": "2051679",
    "end": "2055358"
  }
]