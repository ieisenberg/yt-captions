[
  {
    "start": "0",
    "end": "62000"
  },
  {
    "text": "this talk is called getting the most out of kubernetes and really it's a story",
    "start": "0",
    "end": "5549"
  },
  {
    "text": "about recent quests and basically the",
    "start": "5549",
    "end": "11160"
  },
  {
    "text": "journey that I had to figure out some of these concepts a couple a couple years ago so just a brief introduction what",
    "start": "11160",
    "end": "21330"
  },
  {
    "text": "I'm hoping that you get on out of this talk is an understanding of requests and limits and I want to define a process",
    "start": "21330",
    "end": "29580"
  },
  {
    "text": "that we can all use to set requests in limits I also want to give you some",
    "start": "29580",
    "end": "34680"
  },
  {
    "text": "tools that you can use to help you along the way with your journey as well so my name is",
    "start": "34680",
    "end": "40590"
  },
  {
    "text": "Harrison and I'm a staff software engineer at site I've only been there for a couple weeks",
    "start": "40590",
    "end": "45809"
  },
  {
    "text": "I just switched over from buffer I was there for a couple years before that and",
    "start": "45809",
    "end": "51469"
  },
  {
    "text": "see I've been either an operator or an end user of kubernetes as a developer",
    "start": "51469",
    "end": "57660"
  },
  {
    "text": "for the past close to three years now so",
    "start": "57660",
    "end": "62789"
  },
  {
    "start": "62000",
    "end": "234000"
  },
  {
    "text": "let's start off with a story that comes from buffer but first started off as a",
    "start": "62789",
    "end": "69600"
  },
  {
    "text": "monolith written in PHP and there was a service or this endpoint that was",
    "start": "69600",
    "end": "75720"
  },
  {
    "text": "receiving a lot more traffic compared to the rest of the application and basically what this thing did was if you",
    "start": "75720",
    "end": "82799"
  },
  {
    "text": "don't know buffer is a social media management platform and basically anytime that somebody sends a tweet or",
    "start": "82799",
    "end": "89610"
  },
  {
    "text": "or a Facebook post or any other kind of post that contains a link we take that link and then we increment a counter",
    "start": "89610",
    "end": "95790"
  },
  {
    "text": "somewhere and then that count is used in people's blogs so they they can show",
    "start": "95790",
    "end": "100890"
  },
  {
    "text": "that this is a popular blog post on buffer they can gauge interest so we",
    "start": "100890",
    "end": "108420"
  },
  {
    "text": "settled on a pretty simple design and this is one of the first real applications that we had put in production and on the kubernetes and it",
    "start": "108420",
    "end": "116579"
  },
  {
    "text": "was basically just a node node back end with DynamoDB database",
    "start": "116579",
    "end": "122110"
  },
  {
    "text": "so when we needed more than one replica so we just set it to 4 and we deployed",
    "start": "122110",
    "end": "127870"
  },
  {
    "text": "the service on the kubernetes and then we manually verified with curl to make sure that things were operational and",
    "start": "127870",
    "end": "134220"
  },
  {
    "text": "had some metrics hooked up so we could see what was going on so we shifted one",
    "start": "134220",
    "end": "140380"
  },
  {
    "text": "percent of the traffic from our monolith over to the service on kubernetes and things looking pretty good so then we",
    "start": "140380",
    "end": "150310"
  },
  {
    "text": "scaled up to ten percent of the traffic and shifted some more the traffic and we could see resource utilization metrics",
    "start": "150310",
    "end": "158410"
  },
  {
    "text": "going up approximately what you'd expect about 10x so we moved on to the next",
    "start": "158410",
    "end": "164530"
  },
  {
    "text": "phase roll it out to 50 percent and this is when things started to get interesting because well basically went",
    "start": "164530",
    "end": "172720"
  },
  {
    "text": "into a crash loop and so first thing I did very new to kubernetes I just scaled",
    "start": "172720",
    "end": "178810"
  },
  {
    "text": "it up to 20 pods it's like yeah it's crew natives we can scale it up it'll be fine it wasn't fine and it helped a",
    "start": "178810",
    "end": "186430"
  },
  {
    "text": "service state up but the pods did they just kept dying so we scaled the service",
    "start": "186430",
    "end": "193030"
  },
  {
    "text": "back down to 0% and shifted the traffic back over to a monolith so we dug deeper",
    "start": "193030",
    "end": "201160"
  },
  {
    "text": "and basically I had copied a deployment from another service it was another node",
    "start": "201160",
    "end": "206530"
  },
  {
    "text": "service and it was something that we were just messing with just just a little prototype to kick the tires on",
    "start": "206530",
    "end": "211840"
  },
  {
    "text": "kubernetes and this particular deployment it contains some resource limits that were",
    "start": "211840",
    "end": "217630"
  },
  {
    "text": "set on it and when we used coop control describe we could see that we're getting",
    "start": "217630",
    "end": "222820"
  },
  {
    "text": "o and killed so the pod was running out of memory because we hadn't allocated enough resources so kind of set off this",
    "start": "222820",
    "end": "229390"
  },
  {
    "text": "whole journey of figuring out how do we actually set these things so it's",
    "start": "229390",
    "end": "236290"
  },
  {
    "start": "234000",
    "end": "345000"
  },
  {
    "text": "important to understand the difference between limits and requests because these are kind of your your building blocks here so thinking about limits",
    "start": "236290",
    "end": "244350"
  },
  {
    "text": "these are the upper limit on container resources now if you don't set any limit",
    "start": "244350",
    "end": "249760"
  },
  {
    "text": "at all the containers can be can run completely unbounded and they can take up all of the resources on it",
    "start": "249760",
    "end": "256680"
  },
  {
    "text": "so if the limits crossed now there's two types of things that can happen here",
    "start": "256680",
    "end": "263160"
  },
  {
    "text": "there's compressible so CPU it's basically going to compress the amount of CPU that can be consumed and also",
    "start": "263160",
    "end": "271590"
  },
  {
    "text": "memory is a little bit different when you cross this threshold the couplets gonna come come through and then it's",
    "start": "271590",
    "end": "277229"
  },
  {
    "text": "going to it's going to restart the pod now the other building block is requests",
    "start": "277229",
    "end": "283789"
  },
  {
    "text": "so this is kind of a hint to the scheduler that this particular workload",
    "start": "283789",
    "end": "290639"
  },
  {
    "text": "needs a certain amount of resources and if there's capacity available and it",
    "start": "290639",
    "end": "296610"
  },
  {
    "text": "gets scheduled that amount of resources should always be available for that pod unless it needs to be evicted for some",
    "start": "296610",
    "end": "302910"
  },
  {
    "text": "reason by the scheduler now for a period of time let's say you just set requests",
    "start": "302910",
    "end": "309750"
  },
  {
    "text": "and there's a capacity available it's",
    "start": "309750",
    "end": "314820"
  },
  {
    "text": "going to your your container is going to be able to use more resources for a period of time so you get this kind of bursty behavior that you can program",
    "start": "314820",
    "end": "321870"
  },
  {
    "text": "into it and if you don't set requests but you set a limit it's basically going",
    "start": "321870",
    "end": "328380"
  },
  {
    "text": "to implicitly set the request to the women so I'm going to break that down",
    "start": "328380",
    "end": "333630"
  },
  {
    "text": "here into a few different quality of services so just to name them off",
    "start": "333630",
    "end": "338940"
  },
  {
    "text": "you've got guaranteed yep first of all and best-effort so guaranteed is when",
    "start": "338940",
    "end": "348060"
  },
  {
    "start": "345000",
    "end": "506000"
  },
  {
    "text": "you set your limits to the request or maybe adjust set the limits here these are the highest priority pods so these",
    "start": "348060",
    "end": "354750"
  },
  {
    "text": "are going to be the last thing that gets killed by the scheduler or that the scheduler decides to kill and the next",
    "start": "354750",
    "end": "363389"
  },
  {
    "text": "one is the burst of all quality of service and this is where you set limits",
    "start": "363389",
    "end": "369510"
  },
  {
    "text": "that are higher than your requests so let's say in this example here you set",
    "start": "369510",
    "end": "375570"
  },
  {
    "text": "your request to 100m which is roughly 1/10 of a core and CPU and then you set",
    "start": "375570",
    "end": "382979"
  },
  {
    "text": "the limit to 200m which is roughly 2 to of a court the scheduler is going to",
    "start": "382979",
    "end": "390120"
  },
  {
    "text": "allocate 100m CPU to your container and for a period of time if there are",
    "start": "390120",
    "end": "397590"
  },
  {
    "text": "resources available on that node it can burst up to twice the capacity if it's",
    "start": "397590",
    "end": "403440"
  },
  {
    "text": "available now and in the case here if it does burst up to that 200m that's it's",
    "start": "403440",
    "end": "408930"
  },
  {
    "text": "going to be compressed in CPU is compressible go back here last one is",
    "start": "408930",
    "end": "419160"
  },
  {
    "text": "the lowest priority so this is these are best effort pods now these can use up",
    "start": "419160",
    "end": "425820"
  },
  {
    "text": "any amount of free resources so they're kind of liquid in that way if there's resources available on a node and it",
    "start": "425820",
    "end": "432360"
  },
  {
    "text": "gets scheduled on a note it's going to it has the ability to kind of fill in the gaps and take up the rest of the",
    "start": "432360",
    "end": "437370"
  },
  {
    "text": "space or maybe you just want it to be able to take up all the resources on a node if you schedule things that way if",
    "start": "437370",
    "end": "445020"
  },
  {
    "text": "you have highly interruptible processes this is where this can be useful maybe",
    "start": "445020",
    "end": "450330"
  },
  {
    "text": "you do some garbage collect chinna or some optimization that you don't need to be running all the time or maybe you",
    "start": "450330",
    "end": "456630"
  },
  {
    "text": "want to run off-peak hours these these are great things to schedule this way",
    "start": "456630",
    "end": "461699"
  },
  {
    "text": "with best effort quality of service so how do we actually go about setting these things so it's important to talk",
    "start": "461699",
    "end": "469460"
  },
  {
    "text": "about and understand what optimal means here and with any optimization problem there's it's usually a balancing act",
    "start": "469460",
    "end": "475789"
  },
  {
    "text": "between a couple different variables when when thinking about what you're",
    "start": "475789",
    "end": "481460"
  },
  {
    "text": "balancing you're looking at so each pod has enough resources to complete the",
    "start": "481460",
    "end": "486710"
  },
  {
    "text": "task so whatever task that happens to be pod has enough room to run now on the on",
    "start": "486710",
    "end": "493939"
  },
  {
    "text": "the other side of that all of the nodes can run the maximum number of pods it's your kind of trading off between here so",
    "start": "493939",
    "end": "500749"
  },
  {
    "text": "pods just you can't just allocate tons of resources to a pod because you know you'll end up being wasteful so there's",
    "start": "500749",
    "end": "507650"
  },
  {
    "text": "a couple of different ways you kind of put these group these into buckets there's a couple ways if you get it wrong and then kind of the optimal is",
    "start": "507650",
    "end": "514099"
  },
  {
    "text": "something you strive for but under allocation just thinking about limits",
    "start": "514099",
    "end": "519979"
  },
  {
    "text": "here where the the couplets going to intervene if something is is acting out",
    "start": "519979",
    "end": "525260"
  },
  {
    "text": "of place or unexpectedly if you cross the memory threshold the couplets going",
    "start": "525260",
    "end": "530690"
  },
  {
    "text": "to kill the pod so these things are a little bit easier to see and we're gonna",
    "start": "530690",
    "end": "536600"
  },
  {
    "text": "use this property when we do some when we get into the demo here so we can have more visible effects of these things so",
    "start": "536600",
    "end": "544130"
  },
  {
    "text": "the other way that you can get this wrong is you can over allocate resources to your to your pods and basically this",
    "start": "544130",
    "end": "552110"
  },
  {
    "text": "means that you give so much resources to the pod that it can never consume it maybe something goes wrong but under",
    "start": "552110",
    "end": "558949"
  },
  {
    "text": "typical conditions it's not going to consume that amount of resources this is a little bit trickier to catch because",
    "start": "558949",
    "end": "565490"
  },
  {
    "start": "562000",
    "end": "648000"
  },
  {
    "text": "the effects aren't immediately obvious and it really becomes a problem when you",
    "start": "565490",
    "end": "571850"
  },
  {
    "text": "scale up replicas so let's say you over allocate just a few megabytes of memory to a pod but then you hit this point",
    "start": "571850",
    "end": "579769"
  },
  {
    "text": "where you're scaling it up thousands and thousands of pods you're wasting that little sliver multiplied by a thousand so basically",
    "start": "579769",
    "end": "587449"
  },
  {
    "text": "you'll be wasting resources and and wasting capacity and waste",
    "start": "587449",
    "end": "593949"
  },
  {
    "text": "so in a simple example let's say we have a pod that can only ever use one-third",
    "start": "594120",
    "end": "601570"
  },
  {
    "text": "of the capacity on a node but we allocate half of the resources on the node to it we're missing out on a on the",
    "start": "601570",
    "end": "609250"
  },
  {
    "text": "extra pod that we could be running and imagine thousands of these same exact",
    "start": "609250",
    "end": "615460"
  },
  {
    "text": "scenario is happening you're gonna be wasting basically wasting a lot of capacity so optimal is something that",
    "start": "615460",
    "end": "624580"
  },
  {
    "text": "you want to strive towards in practice you're probably gonna over allocate and",
    "start": "624580",
    "end": "630460"
  },
  {
    "text": "you probably want to Leon over allocation a little bit anyways because you want just a little extra capacity that's kind of up to the application",
    "start": "630460",
    "end": "638200"
  },
  {
    "text": "that you're building and the domain but it's something that you strive towards it shouldn't be it's not a hundred",
    "start": "638200",
    "end": "644200"
  },
  {
    "text": "percent thing maybe you get 90% optimal okay so let's switch gears a little bit",
    "start": "644200",
    "end": "650680"
  },
  {
    "start": "648000",
    "end": "778000"
  },
  {
    "text": "let's talk about how kubernetes does monitoring of these pods so you don't",
    "start": "650680",
    "end": "657310"
  },
  {
    "text": "have to remember everything here but I'm kind of gonna walk through each one of these pieces but it's important to note",
    "start": "657310",
    "end": "663010"
  },
  {
    "text": "that there's multiple nodes each node has a couplet and see advisor and some",
    "start": "663010",
    "end": "671770"
  },
  {
    "text": "of the nodes are gonna have aggregation metrics aggregation here it's called heaps to get a little bit more a share",
    "start": "671770",
    "end": "678730"
  },
  {
    "text": "some news on that and a little bit but basically there's this thing that's getting all the metrics from all of the",
    "start": "678730",
    "end": "685390"
  },
  {
    "text": "nodes and providing and basically a surface to view all the containers but",
    "start": "685390",
    "end": "693430"
  },
  {
    "text": "breaking down how this works see advisor it connects to the docker",
    "start": "693430",
    "end": "698440"
  },
  {
    "text": "socket if you're if you're using that on all of the nodes and what it's doing is",
    "start": "698440",
    "end": "703570"
  },
  {
    "text": "pulling metrics at an interval depending on how its configured it's usually",
    "start": "703570",
    "end": "709210"
  },
  {
    "text": "somewhere between every 10 and 15 seconds and then see advisor provides an",
    "start": "709210",
    "end": "714850"
  },
  {
    "text": "interface or an API for other services like the couplet to pull those metrics",
    "start": "714850",
    "end": "720310"
  },
  {
    "text": "and it collects things like CPU memory file system usage some stuff on network",
    "start": "720310",
    "end": "727389"
  },
  {
    "text": "used utilization as well so then the couplet is going to use information from",
    "start": "727389",
    "end": "733990"
  },
  {
    "text": "C advisor to make decisions on what to do so in the case where you use too many",
    "start": "733990",
    "end": "741129"
  },
  {
    "text": "resources maybe you use too much memory the couplet will get that information",
    "start": "741129",
    "end": "746889"
  },
  {
    "text": "from C advisor and then it will actually kill the pod directly so on top of this",
    "start": "746889",
    "end": "753129"
  },
  {
    "text": "kind of the final layer its heap ster here but what it's doing is collecting",
    "start": "753129",
    "end": "759189"
  },
  {
    "text": "all of the metrics from all the couplets and it's aggregating them together and",
    "start": "759189",
    "end": "765720"
  },
  {
    "text": "usually what it does is it pushes it off to like a storage back-end and there's a",
    "start": "765720",
    "end": "771610"
  },
  {
    "text": "number of different ones of these and kind of depends on your cloud provider",
    "start": "771610",
    "end": "777149"
  },
  {
    "text": "so just a PSA on heap stir if you're using it it's currently in deprecation",
    "start": "777149",
    "end": "784269"
  },
  {
    "start": "778000",
    "end": "860000"
  },
  {
    "text": "it's been deprecated on 111 and it's going to be completely removed on 113",
    "start": "784269",
    "end": "789540"
  },
  {
    "text": "the suggestion is to migrate to the metric server and something like",
    "start": "789540",
    "end": "794589"
  },
  {
    "text": "Prometheus what the link here and the slides are available on my Twitter",
    "start": "794589",
    "end": "799600"
  },
  {
    "text": "handle so you can take a look and check this out but there's a lot more",
    "start": "799600",
    "end": "805750"
  },
  {
    "text": "information there okay so let's go back to setting limits",
    "start": "805750",
    "end": "810950"
  },
  {
    "text": "and requests so this is the define a process part basically the goal is to",
    "start": "810950",
    "end": "817760"
  },
  {
    "text": "understand what one of your pods can handle and when we're trying to",
    "start": "817760",
    "end": "823370"
  },
  {
    "text": "understand what that is we're gonna set the limits and the reason what we're gonna use limits is because we want to",
    "start": "823370",
    "end": "828860"
  },
  {
    "text": "see the immediate effects we want to see the CPU being being throttled we want to",
    "start": "828860",
    "end": "837170"
  },
  {
    "text": "see the container get restarted because across the memory utilization so start",
    "start": "837170",
    "end": "842900"
  },
  {
    "text": "with the conservative set of limits so we can see those effects and then we'll",
    "start": "842900",
    "end": "848060"
  },
  {
    "text": "run a test and then observe what happened and then we're gonna change",
    "start": "848060",
    "end": "853250"
  },
  {
    "text": "just one thing at a time and we're gonna look at what what happened after we changed each one of those pieces so to",
    "start": "853250",
    "end": "862250"
  },
  {
    "start": "860000",
    "end": "930000"
  },
  {
    "text": "do this we're going to employ a couple different testing strategies so the first one is where you start from",
    "start": "862250",
    "end": "868280"
  },
  {
    "text": "basically zero traffic and you increment your traffic up to the point where you",
    "start": "868280",
    "end": "874430"
  },
  {
    "text": "cause the container to restart or you hit a breaking point or maybe maybe the",
    "start": "874430",
    "end": "880130"
  },
  {
    "text": "test completes but you keep adding traffic until until something changes",
    "start": "880130",
    "end": "887110"
  },
  {
    "text": "now after you kind of find that breaking point and you've set like the general",
    "start": "887110",
    "end": "892910"
  },
  {
    "text": "maybe the macro level tweaks then you want to run that service under load",
    "start": "892910",
    "end": "900560"
  },
  {
    "text": "maybe let's say 90% of that breaking point for an extended period of time and what you're doing is looking for things",
    "start": "900560",
    "end": "907370"
  },
  {
    "text": "like well sudden jumps or maybe like variants and you're in your response",
    "start": "907370",
    "end": "913970"
  },
  {
    "text": "times because there's things like memory leaks that could occur there's things",
    "start": "913970",
    "end": "920150"
  },
  {
    "text": "like queue overflows that could occur you're kind of gonna catch those things",
    "start": "920150",
    "end": "925340"
  },
  {
    "text": "with this type of this other type of testing so you need both together",
    "start": "925340",
    "end": "930450"
  },
  {
    "start": "930000",
    "end": "1153000"
  },
  {
    "text": "all right so let's do a demo I'm gonna do this with etsy D a relatively recent",
    "start": "930450",
    "end": "936760"
  },
  {
    "text": "version of it and just to show that you can do this with just about any kind of application",
    "start": "936760",
    "end": "943380"
  },
  {
    "text": "so that looks good I'm using a tool called loader IO and that is allowing me to",
    "start": "947000",
    "end": "956420"
  },
  {
    "text": "apply load to the fcd instance and let's",
    "start": "956420",
    "end": "961970"
  },
  {
    "text": "see the way that I had to hook this up so",
    "start": "961970",
    "end": "968770"
  },
  {
    "text": "just so you know there's no shenanigans going on and funny nginx is sitting in",
    "start": "968770",
    "end": "974980"
  },
  {
    "text": "front of that CD and the reason why I'm doing that is so I can hook up the loader i/o token that way loader i/o can",
    "start": "974980",
    "end": "982600"
  },
  {
    "text": "apply these tests because loader doesn't want to be denial of servicing anybody",
    "start": "982600",
    "end": "987840"
  },
  {
    "text": "so I'm gonna get the pods just to show you what's running and I've got three",
    "start": "987840",
    "end": "994300"
  },
  {
    "text": "things running right now I've got my Etsy d-pod I've got something called coop scope and I'll hook that up in a",
    "start": "994300",
    "end": "999610"
  },
  {
    "text": "second here and then also nginx so I'm",
    "start": "999610",
    "end": "1006630"
  },
  {
    "text": "gonna attach to coops go",
    "start": "1006630",
    "end": "1010700"
  },
  {
    "text": "and what this is doing is this is this is hooking up directly to the doctor a socket on the node and it's displaying",
    "start": "1011910",
    "end": "1018000"
  },
  {
    "text": "the metrics at about one second interval which is a lot faster than you'll get if you're trying to get these metrics from",
    "start": "1018000",
    "end": "1024540"
  },
  {
    "text": "something like Prometheus because it's talking directly to the socket so what",
    "start": "1024540",
    "end": "1032520"
  },
  {
    "text": "you can see here is I'm I'm looking at the Etsy d-pod directly so when I'm running hoops when I'm running these",
    "start": "1032520",
    "end": "1039390"
  },
  {
    "text": "tests we can see what's going on all",
    "start": "1039390",
    "end": "1045089"
  },
  {
    "text": "right so I'm going to apply a load to this and make sure you watch this Kubb",
    "start": "1045089",
    "end": "1052650"
  },
  {
    "text": "scope channel here",
    "start": "1052650",
    "end": "1056900"
  },
  {
    "text": "so I'm doing a ramp up test and what you'll see is that CPU went up that",
    "start": "1058560",
    "end": "1065970"
  },
  {
    "text": "looks like the container died yes this is what we wanted the test is still",
    "start": "1065970",
    "end": "1071160"
  },
  {
    "text": "trying to run basically but we'll see here okay tests completed and you'll see",
    "start": "1071160",
    "end": "1078120"
  },
  {
    "text": "not two things basically that that second row of Kubb scope was the",
    "start": "1078120",
    "end": "1083460"
  },
  {
    "text": "container restarting so it's hooked back up it restarted so we know something went wrong there let's take a look so",
    "start": "1083460",
    "end": "1093150"
  },
  {
    "text": "I'm gonna describe the pod",
    "start": "1093150",
    "end": "1097100"
  },
  {
    "text": "and if I walk up",
    "start": "1098860",
    "end": "1103408"
  },
  {
    "text": "what's that there we go yep Auto memory killed so we",
    "start": "1107120",
    "end": "1112929"
  },
  {
    "text": "ran out of memory they're kind of expected so what's bump up the memory resources",
    "start": "1112929",
    "end": "1120669"
  },
  {
    "text": "on that",
    "start": "1120669",
    "end": "1123000"
  },
  {
    "text": "so I'm just going to 10x this",
    "start": "1133340",
    "end": "1137378"
  },
  {
    "text": "and what we're gonna see is that pods gonna restart there we go",
    "start": "1139809",
    "end": "1146219"
  },
  {
    "text": "Oh bug they gotta restart look at you know all right so I'm gonna kick the test off again and this time I'm",
    "start": "1147380",
    "end": "1153950"
  },
  {
    "text": "expecting the pod to stay up it's gonna be a little slow because it's still bounded by you can see here that it's",
    "start": "1153950",
    "end": "1164090"
  },
  {
    "text": "still bounded by memory so we've hit a bottleneck or sorry CPU we've hit the bottle and I can CPU and coop scope",
    "start": "1164090",
    "end": "1170210"
  },
  {
    "text": "makes that pretty obvious because we're basically being throttled at about 2.1",
    "start": "1170210",
    "end": "1177020"
  },
  {
    "text": "percent approximately at 2.5% so I'm gonna let this test finish so we",
    "start": "1177020",
    "end": "1183230"
  },
  {
    "text": "can look at the average when we're done because then we can see we're gonna make",
    "start": "1183230",
    "end": "1188360"
  },
  {
    "text": "another adjustment after this based off of that test so we just changed the memory now we've given the pod enough",
    "start": "1188360",
    "end": "1195560"
  },
  {
    "text": "memory to run and if you look at the memory percent we're actually probably over allocated here because we're we're",
    "start": "1195560",
    "end": "1205130"
  },
  {
    "text": "not even getting close to the amount utilization at this at this traffic that we're applying to it so a couple more",
    "start": "1205130",
    "end": "1212030"
  },
  {
    "text": "seconds",
    "start": "1212030",
    "end": "1214570"
  },
  {
    "text": "okay so that test is completed you'll see the CPU drop down and the average",
    "start": "1217230",
    "end": "1224309"
  },
  {
    "text": "it's a little hard to see but it's around one that's about it's around one second so if if in fact the CPU is the",
    "start": "1224309",
    "end": "1230850"
  },
  {
    "text": "bottleneck here if we give it more resources we should expect to see that this that the average response time",
    "start": "1230850",
    "end": "1237539"
  },
  {
    "text": "should go down and if you want to get really detailed you can also look at the",
    "start": "1237539",
    "end": "1243659"
  },
  {
    "text": "requests per second so we're about 96 requests per second loader just provides that for you which",
    "start": "1243659",
    "end": "1250169"
  },
  {
    "text": "is nice so",
    "start": "1250169",
    "end": "1253759"
  },
  {
    "text": "I'm gonna go and edit the deployment again for ED CD",
    "start": "1256270",
    "end": "1263040"
  },
  {
    "text": "and let's say I go crazy let's let's do something that's definitely not gonna",
    "start": "1265789",
    "end": "1271220"
  },
  {
    "text": "work let's try to give it 500m so I've got a",
    "start": "1271220",
    "end": "1276950"
  },
  {
    "text": "really small cluster and I want to demonstrate what happens when you try to give something too much resources you",
    "start": "1276950",
    "end": "1283249"
  },
  {
    "text": "see that cube scope there's no pods running so if I get I get pods I'm gonna",
    "start": "1283249",
    "end": "1289070"
  },
  {
    "text": "see something that's in this pending status and if I describe the pod for ED",
    "start": "1289070",
    "end": "1296330"
  },
  {
    "text": "CD we're gonna see that warning or the it couldn't it couldn't scale because or",
    "start": "1296330",
    "end": "1303440"
  },
  {
    "text": "couldn't schedule it because 0-1 nodes are available I don't have enough CPU to run this thing basically so what's that",
    "start": "1303440",
    "end": "1309440"
  },
  {
    "text": "it again",
    "start": "1309440",
    "end": "1311919"
  },
  {
    "text": "so we're at about 20 M before right now we're at about our 25 now we're at about",
    "start": "1320960",
    "end": "1326360"
  },
  {
    "text": "50 so we should expect to see not not quite double the performance in this",
    "start": "1326360",
    "end": "1332360"
  },
  {
    "text": "case kind of depends on what you're running so I'm gonna run this test again",
    "start": "1332360",
    "end": "1340550"
  },
  {
    "text": "before our average was about one second I've got a bug in Cusco so you'll see",
    "start": "1340550",
    "end": "1349280"
  },
  {
    "text": "now we're still bottlenecked on CPU but we're worried about double",
    "start": "1349280",
    "end": "1356170"
  },
  {
    "text": "what we're using about double the resources and just kind of preliminary",
    "start": "1356170",
    "end": "1364150"
  },
  {
    "text": "results yeah bridge is still creeping up but we're a lot better than one second at",
    "start": "1364150",
    "end": "1370490"
  },
  {
    "text": "this point we're probably about 600 milliseconds so I'd keep going on with",
    "start": "1370490",
    "end": "1377810"
  },
  {
    "text": "this process until I kind of find this point where I it really depends on what",
    "start": "1377810",
    "end": "1385580"
  },
  {
    "text": "you're running but you could probably give this a half a core and in handle a",
    "start": "1385580",
    "end": "1390860"
  },
  {
    "text": "lot of traffic so if you have any information about the capacity that you're going to be handling in the case",
    "start": "1390860",
    "end": "1398330"
  },
  {
    "text": "where we had a monolith and that we could look at the number of requests we",
    "start": "1398330",
    "end": "1404360"
  },
  {
    "text": "knew we had to handle that amount so any constraints that you can get here gonna be useful you can see here yeah we're",
    "start": "1404360",
    "end": "1412670"
  },
  {
    "text": "running a lot better we're about 550 milliseconds before we were about a second so definitely CPU is the",
    "start": "1412670",
    "end": "1420410"
  },
  {
    "text": "bottleneck in this case so after I kind of found a sweet spot with these with",
    "start": "1420410",
    "end": "1426500"
  },
  {
    "text": "this with the ramp up test I would want to run a duration test and just kind of",
    "start": "1426500",
    "end": "1432710"
  },
  {
    "text": "generally thinking about that it's probably at least 10 minutes as a good rule of thumb the longer the better",
    "start": "1432710",
    "end": "1438940"
  },
  {
    "text": "especially if you don't have any traffic that you can throw at it that way you",
    "start": "1438940",
    "end": "1443990"
  },
  {
    "text": "have a have some confidence that it's going to work when you get to production nice and we go back to the slides here",
    "start": "1443990",
    "end": "1450859"
  },
  {
    "text": "so another important thing is while you're doing this like we saw some",
    "start": "1450859",
    "end": "1456049"
  },
  {
    "text": "different failure modes but keep a log take some notes while you're doing this because these things are going to end up",
    "start": "1456049",
    "end": "1462919"
  },
  {
    "text": "in your run books these are gonna be stuff that other people are going to use",
    "start": "1462919",
    "end": "1468350"
  },
  {
    "text": "and ask you questions like my service is getting a bunch of 500s and at the cpu's",
    "start": "1468350",
    "end": "1474129"
  },
  {
    "text": "at the max like what what's going on here some of the observed failure modes",
    "start": "1474129",
    "end": "1480889"
  },
  {
    "text": "for a number of different service so memory slowly increasing one that we",
    "start": "1480889",
    "end": "1486739"
  },
  {
    "text": "just saw here was the CPU is pegged at 100% of what the resources that were allocated to it you can also see service",
    "start": "1486739",
    "end": "1494509"
  },
  {
    "text": "riders five hundreds high response times another interesting one is variance in",
    "start": "1494509",
    "end": "1501109"
  },
  {
    "text": "response times that's usually some sort of queuing mechanism is in place and",
    "start": "1501109",
    "end": "1506629"
  },
  {
    "text": "also just flat-out dropping requests so some of the stuff that we learned is",
    "start": "1506629",
    "end": "1514009"
  },
  {
    "start": "1510000",
    "end": "1550000"
  },
  {
    "text": "that what it means to be production ready I I think you need to know how",
    "start": "1514009",
    "end": "1520639"
  },
  {
    "text": "your service is going to break before you can put it in production because it's a when when you're having real live",
    "start": "1520639",
    "end": "1528710"
  },
  {
    "text": "customers using your products especially ones that where people are dependent on them it's it's it's good to understand",
    "start": "1528710",
    "end": "1535279"
  },
  {
    "text": "how they break so you can plan ahead and when they do break you can act rather",
    "start": "1535279",
    "end": "1540379"
  },
  {
    "text": "than have to do this triage stuff like in the cluster while things are breaking",
    "start": "1540379",
    "end": "1546739"
  },
  {
    "text": "so you can kind of do some of this before and really it's about increasing",
    "start": "1546739",
    "end": "1552710"
  },
  {
    "text": "predictability so if you know if you can predict how something is going to break you can make better decisions on what to",
    "start": "1552710",
    "end": "1560659"
  },
  {
    "text": "do about it so some tools that you can use after you",
    "start": "1560659",
    "end": "1565880"
  },
  {
    "text": "understand the breaking point so one of them is and there's a fantastic talk that's gonna be later I think today or",
    "start": "1565880",
    "end": "1572600"
  },
  {
    "text": "tomorrow on the horizontal pod autoscaler but basically what this thing",
    "start": "1572600",
    "end": "1577970"
  },
  {
    "text": "does is it changes the replica count based on a metric so maybe once you hit",
    "start": "1577970",
    "end": "1583940"
  },
  {
    "text": "90 percent of the CPU utilization and you start scaling up replicas until it",
    "start": "1583940",
    "end": "1591110"
  },
  {
    "text": "has enough capacity to handle the load you can hook up all kinds of metrics to",
    "start": "1591110",
    "end": "1597680"
  },
  {
    "text": "this you cannot get up to Prometheus as your adapter stackdriver just to name a few",
    "start": "1597680",
    "end": "1603290"
  },
  {
    "text": "it's very well supported this thing's this thing's pretty solid you can hook",
    "start": "1603290",
    "end": "1608720"
  },
  {
    "text": "up multiple metrics a lot of people are running communities on AWS and have",
    "start": "1608720",
    "end": "1614510"
  },
  {
    "text": "worker queues implemented you can scale up using external metrics from sqs so",
    "start": "1614510",
    "end": "1621440"
  },
  {
    "text": "you can look at the number of messages that are in flight so another one this",
    "start": "1621440",
    "end": "1628790"
  },
  {
    "start": "1626000",
    "end": "1683000"
  },
  {
    "text": "is the vertical pod or a scalar and this one is it's an alpha feature so use with",
    "start": "1628790",
    "end": "1634310"
  },
  {
    "text": "caution this one it changes the resources in place and currently for it",
    "start": "1634310",
    "end": "1640790"
  },
  {
    "text": "to do that it needs to restart the pod so it's gonna change kind of do what we did but in an automated way but it's",
    "start": "1640790",
    "end": "1648050"
  },
  {
    "text": "going to change the resources and then restart the pod so just looking ahead a",
    "start": "1648050",
    "end": "1655280"
  },
  {
    "text": "little bit kind of tools for developers right now there's really great tooling",
    "start": "1655280",
    "end": "1662780"
  },
  {
    "text": "for aggregate metrics we have Prometheus we have things like data dog so we can",
    "start": "1662780",
    "end": "1668570"
  },
  {
    "text": "see what's going on at the cluster level but there's still a gap for high resolution tools so you can look at",
    "start": "1668570",
    "end": "1675080"
  },
  {
    "text": "maybe just a group of deployments maybe a few pods or kind of what we did here we were just looking at one container so",
    "start": "1675080",
    "end": "1684310"
  },
  {
    "start": "1683000",
    "end": "2190000"
  },
  {
    "text": "this is why I started this project coupe scope because these things didn't exist",
    "start": "1684310",
    "end": "1690800"
  },
  {
    "text": "and I'd love help building this so if anybody is interested in this sort of",
    "start": "1690800",
    "end": "1697050"
  },
  {
    "text": "it's all it's it's MIT license and free forever I want people to be able to use",
    "start": "1697050",
    "end": "1703170"
  },
  {
    "text": "this alright so thanks everybody and I want to open up for a few minutes of",
    "start": "1703170",
    "end": "1709560"
  },
  {
    "text": "questions let's see we got a little bit of time thank you all right yes",
    "start": "1709560",
    "end": "1719659"
  },
  {
    "text": "like self boosted instead of using an external service",
    "start": "1721340",
    "end": "1726190"
  },
  {
    "text": "so the question was are there any basically open source tools that we",
    "start": "1726770",
    "end": "1733760"
  },
  {
    "text": "could use to apply load testing within the cluster I haven't used it for a",
    "start": "1733760",
    "end": "1740120"
  },
  {
    "text": "while but I believe locust is is one that's pretty popular depending on the",
    "start": "1740120",
    "end": "1746270"
  },
  {
    "text": "amount of load that you need to apply there's a bunch of like NPM installable",
    "start": "1746270",
    "end": "1751309"
  },
  {
    "text": "like like this one here I can I can send",
    "start": "1751309",
    "end": "1758360"
  },
  {
    "text": "you the package for that but that one that one will allow you to go up to about a thousand requests a second which is pretty okay you're welcome yes",
    "start": "1758360",
    "end": "1769690"
  },
  {
    "text": "so the question was do any suggestions for testing services that have upstream",
    "start": "1774630",
    "end": "1780840"
  },
  {
    "text": "dependencies that's where things get a little more complex if you can deploy it",
    "start": "1780840",
    "end": "1790620"
  },
  {
    "text": "and its upstream dependencies may be in a separate namespace maybe like a dev a",
    "start": "1790620",
    "end": "1795840"
  },
  {
    "text": "dev workspace it's good to have the actual service like of those upstream dependencies because you want to make",
    "start": "1795840",
    "end": "1804210"
  },
  {
    "text": "sure that those things are ok too but try to isolate as much of it as you can maybe you stub something out to to maybe",
    "start": "1804210",
    "end": "1813270"
  },
  {
    "text": "give a a dummy request or something for the upstream services but yeah isolation is good and in that case you probably",
    "start": "1813270",
    "end": "1819840"
  },
  {
    "text": "don't want to hit your production service because it's got the upstream bits yes right here",
    "start": "1819840",
    "end": "1828830"
  },
  {
    "text": "it's so the the question was when I scaled up",
    "start": "1835090",
    "end": "1842929"
  },
  {
    "text": "from I kind of had this service that was failing and then I immediately scaled up",
    "start": "1842929",
    "end": "1849489"
  },
  {
    "text": "why didn't that work in that case it didn't work because even even at that",
    "start": "1849489",
    "end": "1857989"
  },
  {
    "text": "level there weren't there still wasn't enough resources like it was just too",
    "start": "1857989",
    "end": "1863029"
  },
  {
    "text": "constrained that's a good question yes",
    "start": "1863029",
    "end": "1868429"
  },
  {
    "text": "right here",
    "start": "1868429",
    "end": "1870700"
  },
  {
    "text": "could you say it again okay so that the",
    "start": "1876560",
    "end": "1882900"
  },
  {
    "text": "question was once you find the breaking point with limits how do you set the request so you know where the things",
    "start": "1882900",
    "end": "1890130"
  },
  {
    "text": "gonna break I think at that point you want your your standard load should fit",
    "start": "1890130",
    "end": "1899190"
  },
  {
    "text": "inside of a request and then maybe like after this kind of takes a little bit of",
    "start": "1899190",
    "end": "1905640"
  },
  {
    "text": "context on the service that you're building but if you know it's kind of got a bursty behavior you'll want to you'll want to set the limit higher than",
    "start": "1905640",
    "end": "1912600"
  },
  {
    "text": "that so it can use that capacity especially with memory it's it's I don't",
    "start": "1912600",
    "end": "1918750"
  },
  {
    "text": "want to say it's less important with CPU because it's compressible but yet kind of it kind of depends on what you're",
    "start": "1918750",
    "end": "1924300"
  },
  {
    "text": "building if you wanted to what's that",
    "start": "1924300",
    "end": "1930020"
  },
  {
    "text": "yes yeah and the demo they were the same yes",
    "start": "1930470",
    "end": "1936740"
  },
  {
    "text": "so the question was do I know of any tools that'll set those manually so the SEPA basically changed those limits",
    "start": "1945400",
    "end": "1952570"
  },
  {
    "text": "manually I don't know of any tools that exist and if that existed I'd love to use it they'd be cool yes",
    "start": "1952570",
    "end": "1962880"
  },
  {
    "text": "I'm sorry I'm having trouble hearing you huh",
    "start": "1967450",
    "end": "1974940"
  },
  {
    "text": "so the question was I have let's I had two pods one of them use it one of them",
    "start": "1988700",
    "end": "1997249"
  },
  {
    "text": "is guaranteed quality of service and it you it's guaranteed 500 and then another",
    "start": "1997249",
    "end": "2004119"
  },
  {
    "text": "pod that has a burst of all but has been scheduled",
    "start": "2004119",
    "end": "2010950"
  },
  {
    "text": "I would have to play with that I don't know I don't know the answer to that question yes",
    "start": "2023419",
    "end": "2033249"
  },
  {
    "text": "okay so the question was there's vertical autoscaler and then there's the",
    "start": "2051690",
    "end": "2057960"
  },
  {
    "text": "horizontal autoscaler do it is there anybody is there anybody who's using a",
    "start": "2057960",
    "end": "2064408"
  },
  {
    "text": "combination of those things to solve some scaling issues is that correct I'm",
    "start": "2064409",
    "end": "2070858"
  },
  {
    "text": "not sure if anybody's combining them in a way that's yeah I haven't seen a",
    "start": "2070859",
    "end": "2077309"
  },
  {
    "text": "really good use case for that I'm in the wild but yeah there's a lot of people here who might be doing that if any but",
    "start": "2077309",
    "end": "2084118"
  },
  {
    "text": "show of hands does anybody actually using both vertical and right there yes",
    "start": "2084119",
    "end": "2089309"
  },
  {
    "text": "you should talk to that guy I'm gonna a",
    "start": "2089309",
    "end": "2099740"
  },
  {
    "text": "one more question last question",
    "start": "2099740",
    "end": "2103609"
  },
  {
    "text": "so the question is using so guaranteed",
    "start": "2130240",
    "end": "2135900"
  },
  {
    "text": "versus burstable quality of service and like how does that how do you get the",
    "start": "2135900",
    "end": "2142720"
  },
  {
    "text": "most out of that with when you take into the consideration the scheduler is that",
    "start": "2142720",
    "end": "2148720"
  },
  {
    "text": "is that what you're asking",
    "start": "2148720",
    "end": "2152040"
  },
  {
    "text": "I think that's a longer conversation you want to do like hallway track okay",
    "start": "2179730",
    "end": "2187369"
  },
  {
    "text": "thanks everybody [Applause]",
    "start": "2187369",
    "end": "2191960"
  }
]