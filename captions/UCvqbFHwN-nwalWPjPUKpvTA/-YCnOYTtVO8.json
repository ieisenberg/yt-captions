[
  {
    "text": "hi you're here for um navigation failures in uh ports with",
    "start": "760",
    "end": "6680"
  },
  {
    "text": "devices we are my name is Serge kelif I'm chair of signode and one of the",
    "start": "6680",
    "end": "11880"
  },
  {
    "text": "approvers I work for gke and sometimes on on call for node and many of these no",
    "start": "11880",
    "end": "18600"
  },
  {
    "text": "these days uh have accelerators so every time something failed I will know about",
    "start": "18600",
    "end": "25160"
  },
  {
    "text": "it and I will dig into that and oh it's a device failed we need to deal something uh with that somehow and um I",
    "start": "25160",
    "end": "32880"
  },
  {
    "text": "by no means I'm professional in accelerators I don't know much about them but I don't know I I know enough to",
    "start": "32880",
    "end": "40440"
  },
  {
    "text": "how they affect kubernetes and Ral uh Hey folks I'm Ral I'm a software",
    "start": "40440",
    "end": "45640"
  },
  {
    "text": "engineer at redart I'm one of the tech leads and co-chairs of Sig node and our goal today is to figure out how we can",
    "start": "45640",
    "end": "53239"
  },
  {
    "text": "better surface device uh issues from pods and uh better support AIML",
    "start": "53239",
    "end": "59039"
  },
  {
    "text": "workloads so motivation like modern workloads use",
    "start": "59039",
    "end": "66240"
  },
  {
    "text": "a lot of new devices Beyond just CPU memory and network right you have your accelerator cards you have specialized",
    "start": "66240",
    "end": "72880"
  },
  {
    "text": "uh network devices and so on so AI ml training can run for weeks to months so",
    "start": "72880",
    "end": "80320"
  },
  {
    "text": "uh any device failures could drastically slow you down and it can be frustrating and we want uh kubernetes to be the",
    "start": "80320",
    "end": "86880"
  },
  {
    "text": "platform that can be used uh for all these use cases es so visibility into",
    "start": "86880",
    "end": "92079"
  },
  {
    "text": "this device failures and automatic remediation uh is the goal that we want to work",
    "start": "92079",
    "end": "98079"
  },
  {
    "text": "towards so uh if folks have seen uh the Llama paper they site that uh during",
    "start": "98079",
    "end": "106439"
  },
  {
    "text": "training Lama 78% of the interruptions were due to hardware issues and uh 59%",
    "start": "106439",
    "end": "113960"
  },
  {
    "text": "were because of GPU issues so this is like some motivation on why we need to",
    "start": "113960",
    "end": "121719"
  },
  {
    "text": "this so you just saw numbers in red right so number in red is how many time",
    "start": "121719",
    "end": "127320"
  },
  {
    "text": "device failed and you remember about kubernetes kubernetes is I mean in kubernetes you",
    "start": "127320",
    "end": "133360"
  },
  {
    "text": "provision not you barely can change the size of it in CPU and memory and if you",
    "start": "133360",
    "end": "138640"
  },
  {
    "text": "have a device you either have it or you don't have it that was a pattern for a very long time and with this pattern",
    "start": "138640",
    "end": "145319"
  },
  {
    "text": "like we survived for many years and now with AIML going uh being dominant",
    "start": "145319",
    "end": "151720"
  },
  {
    "text": "uh workload that we want to serve it's a change of Paradigm and why would people",
    "start": "151720",
    "end": "158680"
  },
  {
    "text": "use kubernetes for this change of Paradigm why people still running on kubernetes if it's not designed for IML",
    "start": "158680",
    "end": "165159"
  },
  {
    "text": "like maybe we need to switch to something else uh this question we heard a lot uh especially past year and I",
    "start": "165159",
    "end": "172319"
  },
  {
    "text": "think many people already settled on that but I just want to reiterate on that why this talk is even happening why",
    "start": "172319",
    "end": "178640"
  },
  {
    "text": "we talking about e IML and kubernetes and not talking about let's build something new some new Frameworks that",
    "start": "178640",
    "end": "184440"
  },
  {
    "text": "will know about device fell that we will know about specifics of this workloads so",
    "start": "184440",
    "end": "191080"
  },
  {
    "text": "containers became a standard of publishing applications so people know how to do containers how to version",
    "start": "191080",
    "end": "197159"
  },
  {
    "text": "containers how to layer containers how to secure containers and this infrastructure is not going anywhere and",
    "start": "197159",
    "end": "202680"
  },
  {
    "text": "kubernetes is a good way to serve containers plus kubernetes has years of security reliability uh investment that",
    "start": "202680",
    "end": "210720"
  },
  {
    "text": "needs to be marched by any new framework that can come up that's why kubernetes is still very valuable and every",
    "start": "210720",
    "end": "216879"
  },
  {
    "text": "framework that we want to compete with kubernetes for AIML we need to go through the same years of maturing plus",
    "start": "216879",
    "end": "224920"
  },
  {
    "text": "Implement many many features similar to kubernetes like Auto scaler and uh stuff like that so even though kubernetes is",
    "start": "224920",
    "end": "232959"
  },
  {
    "text": "not ideal for IM IML today uh we believe we can make it great forl and many",
    "start": "232959",
    "end": "238000"
  },
  {
    "text": "people already settled on running IML workload on",
    "start": "238000",
    "end": "243079"
  },
  {
    "text": "kubernetes so let's talk about whatl workloads are I'm trying to oversimplify",
    "start": "243159",
    "end": "249760"
  },
  {
    "text": "here like I know that like so many details and you can run this in this mode or that in this mode so if you look",
    "start": "249760",
    "end": "257600"
  },
  {
    "text": "at types of workloads we typically observe it's a either training workload",
    "start": "257600",
    "end": "263320"
  },
  {
    "text": "uh training workload is uh often running on many many machines uh typically takes",
    "start": "263320",
    "end": "269120"
  },
  {
    "text": "whole machine all gpus on this machines because it's just more efficient and it typically runs as a gang like as many",
    "start": "269120",
    "end": "275400"
  },
  {
    "text": "many ports running together scheduled together working to completion of a step and then next step executes and then",
    "start": "275400",
    "end": "281759"
  },
  {
    "text": "working to completion next step and next step executes and typically if one port is lagging behind whole step is uh uh",
    "start": "281759",
    "end": "289400"
  },
  {
    "text": "needs to be redone from beginning uh inference workload is",
    "start": "289400",
    "end": "295080"
  },
  {
    "text": "different uh typically I mean inference can be any size uh we see inference very",
    "start": "295080",
    "end": "300639"
  },
  {
    "text": "small inference very big it can spawn across multiple nodes or it can take part partial um GPU so there are",
    "start": "300639",
    "end": "308360"
  },
  {
    "text": "different uh inference workflows but what is different about inference it's uh typically I know a BGE inference but",
    "start": "308360",
    "end": "314759"
  },
  {
    "text": "typically it's continuously running typically it's restart policy always and uh it uh also requires heavy file with",
    "start": "314759",
    "end": "323840"
  },
  {
    "text": "weights to serve the model so you need to download it somehow either from Image store or from a storage but uh it's a",
    "start": "323840",
    "end": "330960"
  },
  {
    "text": "very specific for serving that weights are very",
    "start": "330960",
    "end": "336440"
  },
  {
    "text": "heavy and looking at this workloads you can see how assumptions that we",
    "start": "336960",
    "end": "342639"
  },
  {
    "text": "previously made in kubernetes for typical workload are changing so you in the past you can just",
    "start": "342639",
    "end": "350600"
  },
  {
    "text": "give better CPU or bigger CPU and everything works just fine just faster same comes with u failure modes",
    "start": "350600",
    "end": "360520"
  },
  {
    "text": "like if something failed you just recreate it no matter where you recreate you can recreate on this note on that",
    "start": "360520",
    "end": "365759"
  },
  {
    "text": "note it doesn't really matter nobody car it uh with the IML you care a lot because every reallocation Recreation is",
    "start": "365759",
    "end": "372080"
  },
  {
    "text": "a heavy operation and you spending valuable cycles of uh your very",
    "start": "372080",
    "end": "377400"
  },
  {
    "text": "expensive Hardware standing uh not doing anything also um uh another assumption",
    "start": "377400",
    "end": "385599"
  },
  {
    "text": "was any node uh will work like as I said previously you can just assume that uh",
    "start": "385599",
    "end": "391000"
  },
  {
    "text": "you can schedu on any note many workload needs to run on specific devices specific device class and you sometimes",
    "start": "391000",
    "end": "398120"
  },
  {
    "text": "it's even not only device class but also which device connected to which other device on other node it adds a lot of",
    "start": "398120",
    "end": "404560"
  },
  {
    "text": "complexity how we schedule things and uh it brings more challenges uh that typically weren't a problem before and",
    "start": "404560",
    "end": "412160"
  },
  {
    "text": "then uh you can uh read other things but uh uh what I want to point here is that",
    "start": "412160",
    "end": "417560"
  },
  {
    "text": "uh every port becomes become very expensive and cherished entity so you",
    "start": "417560",
    "end": "424319"
  },
  {
    "text": "you you want to uh approach the port as a path you need to cherish it you need to keep it alive you need to uh make",
    "start": "424319",
    "end": "430759"
  },
  {
    "text": "sure it's not interrupted and then uh with uh and with when in the past it was",
    "start": "430759",
    "end": "436280"
  },
  {
    "text": "a kettle so you can sced that anywhere it will work no matter",
    "start": "436280",
    "end": "442120"
  },
  {
    "text": "what so with this I want to say that um when we see a ml working on um",
    "start": "443440",
    "end": "452319"
  },
  {
    "text": "kubernetes we know that existing failure model Works uh for many cases of kubernetes is still failure we still",
    "start": "452319",
    "end": "459680"
  },
  {
    "text": "know how to deal with the failers but uh one kubernetes has barely knows about",
    "start": "459680",
    "end": "466360"
  },
  {
    "text": "devices and device health and second every like how the way we handle uh",
    "start": "466360",
    "end": "471960"
  },
  {
    "text": "failures in kubernetes is very expensive for AML workload and now we will go into",
    "start": "471960",
    "end": "477440"
  },
  {
    "text": "specific failure modes and we will discuss how this failer mode uh operates and how what you can do with this failer",
    "start": "477440",
    "end": "485560"
  },
  {
    "text": "mode okay so first is my favorite failer mode is kubernetes infra it's like when",
    "start": "486159",
    "end": "491960"
  },
  {
    "text": "kubernetes infra is not up to your satisfaction and this is uh the mode I",
    "start": "491960",
    "end": "497159"
  },
  {
    "text": "can do the best uh uh I can do the most about because it's like it's a kubernetes I do kubernetes I can fix it",
    "start": "497159",
    "end": "503879"
  },
  {
    "text": "up so let's see how a typical AML workload being scheduled with devices so",
    "start": "503879",
    "end": "510080"
  },
  {
    "text": "if have if you have a device you need to uh tell kubernetes that you have this device and kuet has no knowledge about",
    "start": "510080",
    "end": "517159"
  },
  {
    "text": "this device so we have a pluggable model and I'm listing here device plug-in model I don't list Dr because with Dr we",
    "start": "517159",
    "end": "524839"
  },
  {
    "text": "uh some problems will be solved there are new problems but uh this is what we have today with Device plug-in device",
    "start": "524839",
    "end": "531120"
  },
  {
    "text": "plugin gives information about uh devices to kuet uh via watch um uh",
    "start": "531120",
    "end": "536959"
  },
  {
    "text": "interface and this watch interface can say I have have that many devices and all of them healthy and then when one",
    "start": "536959",
    "end": "542440"
  },
  {
    "text": "goes unhealthy it will Market as unhealthy uh it also uh the same interface is used to for KU to ask to",
    "start": "542440",
    "end": "549560"
  },
  {
    "text": "allocate resources for Port um and then device plugin will allocate it and uh",
    "start": "549560",
    "end": "555519"
  },
  {
    "text": "tell how exactly uh map this device to container and then kuet also reports not",
    "start": "555519",
    "end": "562360"
  },
  {
    "text": "status at how many devices it knows about the um API server in terms of",
    "start": "562360",
    "end": "567760"
  },
  {
    "text": "allocatable and capacity as well us whenever um it is a user Port it go",
    "start": "567760",
    "end": "574040"
  },
  {
    "text": "through this process of allocating it and making sure that there are devices for the sport to",
    "start": "574040",
    "end": "579240"
  },
  {
    "text": "execute so you see like so many components and there is like iteration",
    "start": "579240",
    "end": "585120"
  },
  {
    "text": "like how everything registers and how everything works involves many many steps like first device plugin reports",
    "start": "585120",
    "end": "592040"
  },
  {
    "text": "about itself KU reports about devices to API server API server knows about this",
    "start": "592040",
    "end": "597120"
  },
  {
    "text": "devices schedu sport what comes to the note and kuet will allocate and then if",
    "start": "597120",
    "end": "602640"
  },
  {
    "text": "it allocated successfully it will run the sport if you ever did a analysis of uh",
    "start": "602640",
    "end": "609680"
  },
  {
    "text": "reliability of components uh you know like uh you draw a schema and then on every arrow it will ask you what if this",
    "start": "609680",
    "end": "615839"
  },
  {
    "text": "error fails so imagine uh this uh fail",
    "start": "615839",
    "end": "621000"
  },
  {
    "text": "mode and we see a lot of that like I in in all the on call that I made and all",
    "start": "621000",
    "end": "627000"
  },
  {
    "text": "the customer issues I've been looking at there is a lot of situations like that so imagine many customers for some",
    "start": "627000",
    "end": "633399"
  },
  {
    "text": "reason will need to restart kuet so they have some custom configurations you need to apply before they run a port so they",
    "start": "633399",
    "end": "640360"
  },
  {
    "text": "restart the kuet and it's benign operation typically but in case of device plugin while kualt is restarting",
    "start": "640360",
    "end": "646680"
  },
  {
    "text": "it's forgetting about devices so there is a period of time when there is no information about devices so uh you have",
    "start": "646680",
    "end": "652839"
  },
  {
    "text": "a new Port you cannot schedule it because there is no devices KU just doesn't know yet about it yes it's",
    "start": "652839",
    "end": "658040"
  },
  {
    "text": "couple seconds of uh while it doesn't know about it but two seconds is a lot of time when you're thinking about race",
    "start": "658040",
    "end": "664320"
  },
  {
    "text": "conditions and how fast everything works then device plug-in device plugin",
    "start": "664320",
    "end": "669880"
  },
  {
    "text": "can be updated like in if you update a device plugin we don't support overlapping updates so you need to tore",
    "start": "669880",
    "end": "676639"
  },
  {
    "text": "down one device plugin create a new device plugin it will connect to kuet and report about this devices but you",
    "start": "676639",
    "end": "682519"
  },
  {
    "text": "have a period of time when there is no devices kuet again doesn't know about the devices it cannot schedule anything",
    "start": "682519",
    "end": "687760"
  },
  {
    "text": "it cannot allocate uh and furthermore there are situations when device plugin",
    "start": "687760",
    "end": "692959"
  },
  {
    "text": "is not updated normally but it was evicted for some reason so you have too many things running on this note and",
    "start": "692959",
    "end": "699160"
  },
  {
    "text": "then uh KU just decided that device plugin is the least important port and",
    "start": "699160",
    "end": "704720"
  },
  {
    "text": "it will just kick it out um yeah uh we have mechanism to prevent that but those mechanism doesn't work all the time and",
    "start": "704720",
    "end": "711880"
  },
  {
    "text": "then there are other situations like uh um user ports user ports comes in all",
    "start": "711880",
    "end": "718200"
  },
  {
    "text": "flavors and people people do code mistakes and this mistakes comes in",
    "start": "718200",
    "end": "723800"
  },
  {
    "text": "shapes of like I I Define the long graceful termination and now when I",
    "start": "723800",
    "end": "729200"
  },
  {
    "text": "uning Port it uh some somehow stuck in termination and like good is it graceful",
    "start": "729200",
    "end": "735199"
  },
  {
    "text": "termination and uh kuet knows about it but what if it's termination with like unattaching some volume and this",
    "start": "735199",
    "end": "742120"
  },
  {
    "text": "termination go outside of kuet knowledge and then it's stuck with owning this",
    "start": "742120",
    "end": "747360"
  },
  {
    "text": "device and then there is like a situation and you cannot schedule New Port while Old Port is still in this like limbo state so there are lots of",
    "start": "747360",
    "end": "755120"
  },
  {
    "text": "different race conditions and uh situations like that and uh the best practices we can recommend today without",
    "start": "755120",
    "end": "761360"
  },
  {
    "text": "making improvements in kuet that we plan to do um is just make sure that you",
    "start": "761360",
    "end": "766600"
  },
  {
    "text": "configured kuet in ahead of time don't try to configure it while you schedule your uh device PL uh device related",
    "start": "766600",
    "end": "773399"
  },
  {
    "text": "ports uh you also need to monitor device plugin Health yes many clouds provide",
    "start": "773399",
    "end": "778839"
  },
  {
    "text": "this fun fun ality for you but you also like if you're running it yourself or even if you have a problem and you don't",
    "start": "778839",
    "end": "784079"
  },
  {
    "text": "know what to look just look at device plugin maybe it's it was evicted and then uh try not to run too many things",
    "start": "784079",
    "end": "790920"
  },
  {
    "text": "on a node yes you have wasted CPU on this node but don't use it all otherwise something will be evicted and it will go",
    "start": "790920",
    "end": "798120"
  },
  {
    "text": "unstable and you wasting valuable Hardware time and then uh yeah this is one",
    "start": "798120",
    "end": "804199"
  },
  {
    "text": "example from logs from one of the uh support cases when device plugin uh that we had had some issue and it was failing",
    "start": "804199",
    "end": "811360"
  },
  {
    "text": "to detect one of devices and it was crashing and we end up with deleting this note creating another note deleting",
    "start": "811360",
    "end": "817800"
  },
  {
    "text": "not again creating it again so we get into very bad situation so please avoid the situation and monitor your device",
    "start": "817800",
    "end": "825199"
  },
  {
    "text": "plugin another infra related thing but it's not like kubernetes infra it's more",
    "start": "825199",
    "end": "830560"
  },
  {
    "text": "vendor infra is a situation with uh device version mismatch so now we have a",
    "start": "830560",
    "end": "837079"
  },
  {
    "text": "device and this device needs to have some plugins installed you install plugins for the device uh and then Port",
    "start": "837079",
    "end": "844440"
  },
  {
    "text": "that you install also need to be compatible with this plugin so now you not only looking at which device class",
    "start": "844440",
    "end": "850720"
  },
  {
    "text": "you want but also which drivers are installed in a node that you will be running on and it's like typically it's",
    "start": "850720",
    "end": "856480"
  },
  {
    "text": "not a problem typically there are like range of compatibility is large but we see problems now and when uh now and",
    "start": "856480",
    "end": "863440"
  },
  {
    "text": "then that uh this mis much happens and with more Technologies like nickel for",
    "start": "863440",
    "end": "868680"
  },
  {
    "text": "um like running Network more efficiently you have more drivers now so you need to",
    "start": "868680",
    "end": "873759"
  },
  {
    "text": "coordinate between two drivers three drivers and then versioning becomes a little bit hectic and uh uh you need to",
    "start": "873759",
    "end": "880399"
  },
  {
    "text": "make sure that you have some processes in place to make sure that uh you don't get into this troubles and this process",
    "start": "880399",
    "end": "886959"
  },
  {
    "text": "may be like uh monitor driver installation uh sometimes it fails uh",
    "start": "886959",
    "end": "892800"
  },
  {
    "text": "make sure that you have some upgrade plan and this upgrade plan now not only include when you upgrade nodes but also",
    "start": "892800",
    "end": "899240"
  },
  {
    "text": "includes how you upgrade notes and then upgrade ports to match the version of the driver and then um have some canar",
    "start": "899240",
    "end": "906079"
  },
  {
    "text": "environment it's always a good idea it's not always possible with heavy devices because they are expensive and they are",
    "start": "906079",
    "end": "912480"
  },
  {
    "text": "in shortage but please try to think how you can be creative and uh uh test it",
    "start": "912480",
    "end": "918120"
  },
  {
    "text": "before you go into full mode on full scale in production and with that I want to pass",
    "start": "918120",
    "end": "923839"
  },
  {
    "text": "to as device F to moral uh all right so",
    "start": "923839",
    "end": "929000"
  },
  {
    "text": "what are some ways we work around the issues and what we can do",
    "start": "929000",
    "end": "934480"
  },
  {
    "text": "right now before we go into Solutions so you can have a health controller right so if your device plug in uh the",
    "start": "934480",
    "end": "941680"
  },
  {
    "text": "controller can check whether allocate table is equal to the capacity or not if",
    "start": "941680",
    "end": "946839"
  },
  {
    "text": "a device failed then the allocate table will be less than capacity and your controller can then wait for a grace",
    "start": "946839",
    "end": "953079"
  },
  {
    "text": "period and then take the node down but that's not really great right because we don't know the root cause uh of the",
    "start": "953079",
    "end": "959440"
  },
  {
    "text": "issue with the device it's not aware of the workload it's possible that the device that failed was not even in use",
    "start": "959440",
    "end": "966759"
  },
  {
    "text": "uh and sometimes this process is too low is too slow because you're waiting for some time and then a node may be part of",
    "start": "966759",
    "end": "973160"
  },
  {
    "text": "a slice and you cannot just simply delete it and then you end up with a lot of Uh custom configuration to handle",
    "start": "973160",
    "end": "981040"
  },
  {
    "text": "this so another way uh so what can you do at the Pod level right if you if you",
    "start": "981040",
    "end": "986680"
  },
  {
    "text": "don't want to take away the node entirely so your pod can Define special error codes and then uh have a pod",
    "start": "986680",
    "end": "993680"
  },
  {
    "text": "failure policy for example if the Pod can detect uh device uh failed then it",
    "start": "993680",
    "end": "999040"
  },
  {
    "text": "can have a special error code for that and then it can retry or if it it's an",
    "start": "999040",
    "end": "1004880"
  },
  {
    "text": "application error then it can just fail now the problem with this is it'll only",
    "start": "1004880",
    "end": "1009920"
  },
  {
    "text": "work with jobs with restart policy equal to never and we are not really surfacing",
    "start": "1009920",
    "end": "1015319"
  },
  {
    "text": "the issue that the device failed which we'll talk about in the future Solutions",
    "start": "1015319",
    "end": "1022800"
  },
  {
    "text": "section uh yet another solution is uh you can have a pod Watcher uh this pod",
    "start": "1023640",
    "end": "1029798"
  },
  {
    "text": "can keep uh watching your pods and if if a pod is failing it's in a crash loop",
    "start": "1029799",
    "end": "1036678"
  },
  {
    "text": "back off then it can get deleted by the controller but with this you don't know",
    "start": "1036679",
    "end": "1043480"
  },
  {
    "text": "what actually happened whether there was a device failure or there was an application failure causing the part to",
    "start": "1043480",
    "end": "1049400"
  },
  {
    "text": "uh go into this",
    "start": "1049400",
    "end": "1052200"
  },
  {
    "text": "mode so uh what if there's an application failure right so since AI ml",
    "start": "1056400",
    "end": "1061960"
  },
  {
    "text": "pods are uh heavy to start they need device allocation and so on it may make",
    "start": "1061960",
    "end": "1067200"
  },
  {
    "text": "sense to just reschedule them or start them in place instead of trying to start a new pod entirely and also remember",
    "start": "1067200",
    "end": "1073720"
  },
  {
    "text": "like these pods like for example take leader work ass set it has a collection",
    "start": "1073720",
    "end": "1079600"
  },
  {
    "text": "of PODS that need to work together so you're not just worried about one pod but you're worried about like restarting",
    "start": "1079600",
    "end": "1085080"
  },
  {
    "text": "a bunch of PODS together which is going to be expensive and timec",
    "start": "1085080",
    "end": "1090000"
  },
  {
    "text": "consuming so yet another class of uh issue is device degradation so this is",
    "start": "1091760",
    "end": "1097360"
  },
  {
    "text": "even worse this is the case where the device is working but it's not",
    "start": "1097360",
    "end": "1102600"
  },
  {
    "text": "performing uh the way it's supposed to perform it's it's working slow or it's it has some edge cases so how do you",
    "start": "1102600",
    "end": "1109760"
  },
  {
    "text": "even detect that so maybe you can have some metrics you can have some performance measurements and then if",
    "start": "1109760",
    "end": "1116559"
  },
  {
    "text": "you're if your workload is not progressing at the rate it's expected to uh your custom controller can act on",
    "start": "1116559",
    "end": "1125600"
  },
  {
    "text": "it so what do we do in terms of monitoring today so on the cuet we have",
    "start": "1126880",
    "end": "1132480"
  },
  {
    "text": "the P resources API it's a node local cuet grpc API uh so you can have a",
    "start": "1132480",
    "end": "1139840"
  },
  {
    "text": "privileged demon set running on each node connecting to the cubet and G gathering information through this API",
    "start": "1139840",
    "end": "1146480"
  },
  {
    "text": "so it through this API you can get device assignment information you can know the status of the devices and so on",
    "start": "1146480",
    "end": "1154480"
  },
  {
    "text": "so this is used by monitoring agents such as nvidia's dcgm it's also used by malus however the downside is that this",
    "start": "1154480",
    "end": "1162600"
  },
  {
    "text": "is like a backend way to surface information about devices right and you also need a privileged demon set ideally",
    "start": "1162600",
    "end": "1169240"
  },
  {
    "text": "this information should be surfaced up to a pod so now Serge will talk about uh what",
    "start": "1169240",
    "end": "1176320"
  },
  {
    "text": "we are trying to do to address these issues thank you munal yeah so we just",
    "start": "1176320",
    "end": "1182159"
  },
  {
    "text": "walk through many DUI solution do it yourself like you have this kind of othera do this you",
    "start": "1182159",
    "end": "1190760"
  },
  {
    "text": "kind this kind of other do that and every time it's a very custom you need to know what your workload is doing how",
    "start": "1190760",
    "end": "1196320"
  },
  {
    "text": "it is doing and uh what kind of reaction you want to apply to that and what we",
    "start": "1196320",
    "end": "1203400"
  },
  {
    "text": "discovered over time that every time we talk to customers and saying what can we",
    "start": "1203400",
    "end": "1208480"
  },
  {
    "text": "do in kubernetes to help you and often we see we hear very conflicting messages",
    "start": "1208480",
    "end": "1216000"
  },
  {
    "text": "like some people will say oh when device fails immediately like SC like remove my port and other other people say no you",
    "start": "1216000",
    "end": "1223400"
  },
  {
    "text": "need to do it very slowly you need to give us a lot of graceful degradation time and maybe not even like touch it",
    "start": "1223400",
    "end": "1230159"
  },
  {
    "text": "for a while um so looking at all of that we said that we want to take like slow",
    "start": "1230159",
    "end": "1236840"
  },
  {
    "text": "approach to that we cannot just start introducing breaking changes because many many do do it to yourself Solutions",
    "start": "1236840",
    "end": "1243039"
  },
  {
    "text": "already working they already being implemented and if you will break some backward compatibility and start",
    "start": "1243039",
    "end": "1249080"
  },
  {
    "text": "implementing behavior that wasn't implemented before we break more deployments that we can save so that's",
    "start": "1249080",
    "end": "1257080"
  },
  {
    "text": "why we want to do it slowly we want to do it step by step and mostly we prioritize new extensibility points and",
    "start": "1257080",
    "end": "1263840"
  },
  {
    "text": "new information uh exposure rather than Behavior change of",
    "start": "1263840",
    "end": "1269760"
  },
  {
    "text": "kuet so first failure mode as we discussed is uh kubernetes infraa uh",
    "start": "1271000",
    "end": "1276600"
  },
  {
    "text": "changes uh breakages and as I said it's my favorite one because we can do a lot of about it we know how kubernetes works",
    "start": "1276600",
    "end": "1284760"
  },
  {
    "text": "we know how to make it more reliable less race conditioning uh that's why we Implement uh we invested here a lot and",
    "start": "1284760",
    "end": "1292600"
  },
  {
    "text": "we started with small steps like if my endpoint is not working on kuet let's",
    "start": "1292600",
    "end": "1298400"
  },
  {
    "text": "restart kuet I mean it's the easiest we can do uh but then in future we have a lot of plans how to uh minimize number",
    "start": "1298400",
    "end": "1305520"
  },
  {
    "text": "of uh race conditions um how to minimize number of down times one of the biggest",
    "start": "1305520",
    "end": "1310880"
  },
  {
    "text": "requests right now is how do we deploy new version of device Plug-In or Dr",
    "start": "1310880",
    "end": "1316240"
  },
  {
    "text": "without removing the old one so they can swap immediately that's a very nice",
    "start": "1316240",
    "end": "1322640"
  },
  {
    "text": "feature to have we don't have it yet and I hope that we can implement it soon uh and we have a active uh issue about that",
    "start": "1322640",
    "end": "1329880"
  },
  {
    "text": "other things is uh um I just restarted kuet and I want to readmit all my ports",
    "start": "1329880",
    "end": "1336320"
  },
  {
    "text": "how I make sure that device plugin has time to register all um all the devices before I start readmitting the ports and",
    "start": "1336320",
    "end": "1343720"
  },
  {
    "text": "for that we also have an issue and we have a very nice and elegant uh solution that may not even require a cap so it's",
    "start": "1343720",
    "end": "1350960"
  },
  {
    "text": "uh some retries that we can add on P admission that are benign enough for any",
    "start": "1350960",
    "end": "1356039"
  },
  {
    "text": "other work CL but help drastically for device plugin scenarios and we have uh",
    "start": "1356039",
    "end": "1361919"
  },
  {
    "text": "some other ideas how to improve this uh model so we're basically looking at diagrams that we had before and trying",
    "start": "1361919",
    "end": "1367760"
  },
  {
    "text": "to optimize every single step there every single arrow and make sure that if this AR error file what do we do is this",
    "start": "1367760",
    "end": "1374840"
  },
  {
    "text": "error file what do we do um and by the way in Dr we started with like uh testing of",
    "start": "1374840",
    "end": "1383520"
  },
  {
    "text": "uh fail conditions in device plugin we didn't even have fail condition tests now we have fail condition test for",
    "start": "1383520",
    "end": "1388799"
  },
  {
    "text": "device plug-in uh we extending it and trying uh to add more features",
    "start": "1388799",
    "end": "1394400"
  },
  {
    "text": "there now uh device fail uh failer",
    "start": "1394400",
    "end": "1399559"
  },
  {
    "text": "mode as moral pointed out when device failed there are multiple many many",
    "start": "1399559",
    "end": "1404760"
  },
  {
    "text": "scenarios like first scenario he mentioned is let's just nuke the no like we know that it has to have eight",
    "start": "1404760",
    "end": "1410600"
  },
  {
    "text": "devices it doesn't have eight devices we don't need it um other scenarios is other scenario is custom Watcher that",
    "start": "1410600",
    "end": "1417960"
  },
  {
    "text": "will uh remove all the crush loop back offen uh in inference uh ports and",
    "start": "1417960",
    "end": "1425279"
  },
  {
    "text": "another scenario is uh uh how do we do Port uh uh Port failer policy so it",
    "start": "1425279",
    "end": "1432159"
  },
  {
    "text": "knows about device F so all this we want to implement as a extensibility points and it's something that customers can",
    "start": "1432159",
    "end": "1438480"
  },
  {
    "text": "configure because as I said it's generic solution is very hard here uh so a step",
    "start": "1438480",
    "end": "1444440"
  },
  {
    "text": "zero is like immediate step we taken is trying to expose device failure information in Port status so now when",
    "start": "1444440",
    "end": "1450440"
  },
  {
    "text": "you port failed you at least can look at the Port status and see oh it's failed because of device failure now I know",
    "start": "1450440",
    "end": "1456279"
  },
  {
    "text": "like I don't need to c special error codes I don't need to do any special uh handling of the situation I I just I",
    "start": "1456279",
    "end": "1462559"
  },
  {
    "text": "just look at the Port status and I know um and then this extensibility point can be used by multiple controllers it still",
    "start": "1462559",
    "end": "1470399"
  },
  {
    "text": "will be do it yourself controllers but at least they have information that it's easy to obtain and it's",
    "start": "1470399",
    "end": "1476440"
  },
  {
    "text": "reliable and then next steps maybe uh we thinking how to integrate this information in the P failure policy",
    "start": "1476440",
    "end": "1483360"
  },
  {
    "text": "maybe we have a special policy for device failur maybe we'll have something like that then uh very big thing we want",
    "start": "1483360",
    "end": "1490640"
  },
  {
    "text": "to do is uh to think how we can start unscheduling ports when something failed",
    "start": "1490640",
    "end": "1495799"
  },
  {
    "text": "uh and cannot be recovered so if device failed we know it wouldn't be recovered we see it Crush loop back open maybe we",
    "start": "1495799",
    "end": "1503080"
  },
  {
    "text": "need to start unscheduling it kubernetes as I said never assumed that device can fail uh it just doesn't know about this",
    "start": "1503080",
    "end": "1510279"
  },
  {
    "text": "concept it is it either has a device or doesn't have a device so it will be brand new concept and unscheduling is",
    "start": "1510279",
    "end": "1516200"
  },
  {
    "text": "something unheard of uh and we will start exploring this",
    "start": "1516200",
    "end": "1521158"
  },
  {
    "text": "area now uh for container code failed um this a uh very interesting failure mode",
    "start": "1521600",
    "end": "1530000"
  },
  {
    "text": "when uh we already know how to handle it it's a cot failure like I mean kubernetes is all about making",
    "start": "1530000",
    "end": "1536200"
  },
  {
    "text": "applications reliable and like if it failed we know how to restart it and as munal said that the problem is that is",
    "start": "1536200",
    "end": "1542679"
  },
  {
    "text": "not that we don't know how to handle it it just be handling it very expensive way uh right now if you run a training",
    "start": "1542679",
    "end": "1549480"
  },
  {
    "text": "job and this job is like 512 PS and one of the port failed yes you have a port",
    "start": "1549480",
    "end": "1555919"
  },
  {
    "text": "failer policy this port failer policy goal so way to controller of this job and this controller of job say okay I",
    "start": "1555919",
    "end": "1562000"
  },
  {
    "text": "have one port failed I need to Nuke all the ports so it deletes every single one of them and then recreate every single",
    "start": "1562000",
    "end": "1568039"
  },
  {
    "text": "one of them guess what while it's doing it first devices are not doing anything",
    "start": "1568039",
    "end": "1573360"
  },
  {
    "text": "and second something may race with this uh job so some inference will start working on one of the nodes and suddenly",
    "start": "1573360",
    "end": "1579799"
  },
  {
    "text": "you cannot even schedule your node there so you know need to deal with that situation now you need to make sure that you pause everything on kubernetes like",
    "start": "1579799",
    "end": "1587200"
  },
  {
    "text": "uh like big pause on server and then you schedule your job so it's it becomes",
    "start": "1587200",
    "end": "1592480"
  },
  {
    "text": "very uh expensive and complicated so what we want to do in uh this cap is coming soon I hope we will start uh",
    "start": "1592480",
    "end": "1598679"
  },
  {
    "text": "discussing and maybe even implementing it in 133 is in place recycle of ports",
    "start": "1598679",
    "end": "1603760"
  },
  {
    "text": "so we'll have API and likely it will be local for kuet that will tell that this",
    "start": "1603760",
    "end": "1609159"
  },
  {
    "text": "poort needs to be recycled it will be restarted in place no real allocation no nothing it just like old container being",
    "start": "1609159",
    "end": "1617039"
  },
  {
    "text": "deleted new container created in place uh and it will be very fast and very inexpensive and it uh another",
    "start": "1617039",
    "end": "1623640"
  },
  {
    "text": "sensibility point that customers will need to code for but at least they don't need to um hack around kubernetes",
    "start": "1623640",
    "end": "1632760"
  },
  {
    "text": "patterns finally for device degradation this is hardest one like right now the",
    "start": "1632760",
    "end": "1638200"
  },
  {
    "text": "solution uh as M now mentioned is like we we need to pause all the soft all the",
    "start": "1638200",
    "end": "1645000"
  },
  {
    "text": "uh workload test every device for Benchmark and then see oh this is outlier clearly we need to delete this",
    "start": "1645000",
    "end": "1651279"
  },
  {
    "text": "node and recreate it um it doesn't work very well for typical jobs you don't",
    "start": "1651279",
    "end": "1656799"
  },
  {
    "text": "want to run Benchmark on all the devices before or after every job and another",
    "start": "1656799",
    "end": "1662440"
  },
  {
    "text": "solution R now mentioned is like measuring job execution if it's running longer then we will um uh recreate like",
    "start": "1662440",
    "end": "1671279"
  },
  {
    "text": "we will notify that like this job running longer than other job so we need to uh look at this device so this also",
    "start": "1671279",
    "end": "1678200"
  },
  {
    "text": "doesn't work very well and uh for that we really want to get more insights from",
    "start": "1678200",
    "end": "1683880"
  },
  {
    "text": "uh device vendors to understand what's happening with this device and uh does it work up to the",
    "start": "1683880",
    "end": "1691039"
  },
  {
    "text": "standard and with that I want to thank you for all for coming to the to our talk if you have any questions please uh",
    "start": "1691240",
    "end": "1698360"
  },
  {
    "text": "go ahead [Applause]",
    "start": "1698360",
    "end": "1708890"
  },
  {
    "text": "uh could you could you provide a little bit more details on the use case for uh in place container restart without pod",
    "start": "1710880",
    "end": "1717720"
  },
  {
    "text": "resling because if uh if there is a code issue which is partic which is specific",
    "start": "1717720",
    "end": "1723760"
  },
  {
    "text": "for let's say a bad Shard of the data restarting in place will just keep",
    "start": "1723760",
    "end": "1729080"
  },
  {
    "text": "looping over and over again without termination what what what particular use case is addressed by in place",
    "start": "1729080",
    "end": "1734679"
  },
  {
    "text": "container restart is like device remapping or thank",
    "start": "1734679",
    "end": "1740398"
  },
  {
    "text": "you so um specific issue targeted here is uh imagine you have um so what we",
    "start": "1740799",
    "end": "1749840"
  },
  {
    "text": "observed as do it-yourself solution from some of very big customers is they want",
    "start": "1749840",
    "end": "1755279"
  },
  {
    "text": "to restart a job and they have a custom implementation of their training job this implementation either wraps the job",
    "start": "1755279",
    "end": "1762640"
  },
  {
    "text": "itself into um um some archist strator and job itself and then it watch for",
    "start": "1762640",
    "end": "1769679"
  },
  {
    "text": "total job status execution status and if we see if uh one of the sports failed",
    "start": "1769679",
    "end": "1776159"
  },
  {
    "text": "execution status of a job will be cancelled and it will react on this execution status and recycle the worker",
    "start": "1776159",
    "end": "1782000"
  },
  {
    "text": "job and then it will recycle all of them except one that failed and one that failed will be recreated in uh uh in a",
    "start": "1782000",
    "end": "1789039"
  },
  {
    "text": "place that ideally you have extra node that you can uh put it on if you don't have extra note yeah you need to Nuke",
    "start": "1789039",
    "end": "1796600"
  },
  {
    "text": "everything right thank you you so you mentioned the the concept of",
    "start": "1796600",
    "end": "1803200"
  },
  {
    "text": "unscheduling as possibly an idea that could be native in kubernetes to handle",
    "start": "1803200",
    "end": "1809039"
  },
  {
    "text": "issues with devices uh let's say for a second that that was something that existed today and you ran into a",
    "start": "1809039",
    "end": "1814399"
  },
  {
    "text": "situation where a device failed on a node and it was reported as failed and kubernetes started doing unscheduling on",
    "start": "1814399",
    "end": "1820919"
  },
  {
    "text": "it uh what would you expect to happen next after we unscheduled the",
    "start": "1820919",
    "end": "1826720"
  },
  {
    "text": "workloads it so I think after it gets unscheduled it goes back to the Schuler and it finds another device U healthy",
    "start": "1826720",
    "end": "1834440"
  },
  {
    "text": "device to schedule it with yeah so we we hope uh we want to uh",
    "start": "1834440",
    "end": "1840200"
  },
  {
    "text": "use it for mostly restart policy always ports so right now restart policy always",
    "start": "1840200",
    "end": "1845640"
  },
  {
    "text": "says that um assume that you can always like Hardware is always good it's just logical problem somewhere so we keep",
    "start": "1845640",
    "end": "1852399"
  },
  {
    "text": "restarting you and eventually you'll uh work out of this problem with unrecovered device failers we want to",
    "start": "1852399",
    "end": "1859360"
  },
  {
    "text": "say like it's impossible like I mean we can keep restarting you but you will never recover so go away and like your",
    "start": "1859360",
    "end": "1865440"
  },
  {
    "text": "controller will deal with that that's kind of idea okay so like essentially that we we know that that that node that",
    "start": "1865440",
    "end": "1872240"
  },
  {
    "text": "device is no longer working we we get the workloads off of it we hope that okay maybe we'll find a new place for it",
    "start": "1872240",
    "end": "1878880"
  },
  {
    "text": "there's possibly another device and maybe we have some elastic hardware and we move it and then you know at some",
    "start": "1878880",
    "end": "1884440"
  },
  {
    "text": "point we fix that Hardware yeah this problema we typically see on uh smaller inference workload like uh it can find a",
    "start": "1884440",
    "end": "1891919"
  },
  {
    "text": "place like I mean it's very bad situation when like you have a node and you you have like de healthy devices",
    "start": "1891919",
    "end": "1898039"
  },
  {
    "text": "next to it but it's still assigned to the same device because kubernetes has no um mechanism to unsing can reallocate",
    "start": "1898039",
    "end": "1905080"
  },
  {
    "text": "into different device okay um maybe another question um so let's say if you",
    "start": "1905080",
    "end": "1911679"
  },
  {
    "text": "start seeing lots of device failures starting to to pop up and they're the ones that are possibly recoverable but",
    "start": "1911679",
    "end": "1919279"
  },
  {
    "text": "um that recovery step is like a a reboot on the Noe or an spr spr on on the GPU",
    "start": "1919279",
    "end": "1925240"
  },
  {
    "text": "or something like that and you needed to safely schedule these things right",
    "start": "1925240",
    "end": "1932279"
  },
  {
    "text": "like you didn't want um to affect any of the workloads on the node right cuz we could be possibly scheduling new",
    "start": "1932279",
    "end": "1937600"
  },
  {
    "text": "workloads or something else is there and we don't want to reboot that node right so um is there anything that like we",
    "start": "1937600",
    "end": "1944080"
  },
  {
    "text": "could do scheduling wise to say like okay this node is has a problem like we we sort of see",
    "start": "1944080",
    "end": "1951519"
  },
  {
    "text": "it as a repeat offender we should probably start steering things away from it or even if we start seeing patterns",
    "start": "1951519",
    "end": "1958039"
  },
  {
    "text": "you know we start detecting these device failures should we start scheduling moving things around saying that okay",
    "start": "1958039",
    "end": "1963639"
  },
  {
    "text": "this node has high potential of failure we move away from it or um or even say",
    "start": "1963639",
    "end": "1969200"
  },
  {
    "text": "like okay we probably should do you know some work to spr reboot this not in the future so we should really start moving",
    "start": "1969200",
    "end": "1976159"
  },
  {
    "text": "our devices away scheduling away from or or scheduling our workloads away from it slowly so I I didn't get all of it but",
    "start": "1976159",
    "end": "1984000"
  },
  {
    "text": "like if if we have this uh status where we are surfacing the device issue then",
    "start": "1984000",
    "end": "1990200"
  },
  {
    "text": "the Schuler will no longer consider that device is available right your Dr driver",
    "start": "1990200",
    "end": "1996039"
  },
  {
    "text": "will be no longer be advertising that device so if all the devices on that node are unhealthy then nothing will get",
    "start": "1996039",
    "end": "2003039"
  },
  {
    "text": "scheduled back there automatically right let me try uh so what I so once",
    "start": "2003039",
    "end": "2009200"
  },
  {
    "text": "there was like customer and they had a port that did some GPU operations but",
    "start": "2009200",
    "end": "2015080"
  },
  {
    "text": "this GPU operation somehow broke the GPU every time so it's like they schedule they run for 5 minutes and they like",
    "start": "2015080",
    "end": "2021320"
  },
  {
    "text": "broke this device and then go to another one and then broke this device and then go into another one broke this device so",
    "start": "2021320",
    "end": "2027000"
  },
  {
    "text": "this is a pattern we probably can detect it but uh if this this port will keep",
    "start": "2027000",
    "end": "2033760"
  },
  {
    "text": "going to the same node we can think that it's a node broken but in fact that like P is like incompatible in some way or it",
    "start": "2033760",
    "end": "2041120"
  },
  {
    "text": "you go to different nodes we can assume that P broken but this assumptions start being complicated like you cannot really",
    "start": "2041120",
    "end": "2048200"
  },
  {
    "text": "tell like yes you can say like this this not is very faulty so maybe you can",
    "start": "2048200",
    "end": "2053800"
  },
  {
    "text": "assume that but you you you don't know like maybe it's a port that's why I think what that's why this like small um",
    "start": "2053800",
    "end": "2061960"
  },
  {
    "text": "steps approach is workable here we will provide many sensibility points and try",
    "start": "2061960",
    "end": "2067200"
  },
  {
    "text": "to cook uh different solutions for different customers and then whatever sticks with the most of them will be uh",
    "start": "2067200",
    "end": "2074560"
  },
  {
    "text": "integrated into API Upstream I think that would be the approach yeah yeah I I agree like taking small steps and then",
    "start": "2074560",
    "end": "2080960"
  },
  {
    "text": "people can experiment with it and then come back oh okay this is another class of issues that can be fixed in the core",
    "start": "2080960",
    "end": "2086398"
  },
  {
    "text": "versus something you're doing outside yeah that makes sense I get like what I'm trying to say is like you have these",
    "start": "2086399",
    "end": "2091800"
  },
  {
    "text": "xids you have these failures it's it's sort of like their information like you're were saying with monitoring and",
    "start": "2091800",
    "end": "2097000"
  },
  {
    "text": "they can help us make decisions and like me there's so many things that could go wrong with our accelerators we switch",
    "start": "2097000",
    "end": "2102280"
  },
  {
    "text": "device drivers we upgrade the device drivers stuff can happen you know not just pods but like things can occur and",
    "start": "2102280",
    "end": "2108560"
  },
  {
    "text": "so almost like assuming that that can go wrong and and possibly making decisions as to okay taking the knowledge of okay",
    "start": "2108560",
    "end": "2115640"
  },
  {
    "text": "this device is probably having a problem let's schedule away from it and do something about",
    "start": "2115640",
    "end": "2120720"
  },
  {
    "text": "it yeah I think it's a good feedback thank you and need more feedback so if",
    "start": "2120720",
    "end": "2125760"
  },
  {
    "text": "you have any uh feedback on S ility points we Mission or some device failers we failer mode we didn't cover please",
    "start": "2125760",
    "end": "2133040"
  },
  {
    "text": "come to sign note and help us thank you [Applause]",
    "start": "2133040",
    "end": "2142139"
  }
]