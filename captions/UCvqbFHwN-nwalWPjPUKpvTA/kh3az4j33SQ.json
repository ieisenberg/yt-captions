[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "hi my name is Chip zoller I'm a technical product manager at nermata the creators of caverno and I'm also a",
    "start": "3780",
    "end": "10500"
  },
  {
    "text": "coverno maintainer today in this webinar I want to talk a little bit about automation as policy for platform teams",
    "start": "10500",
    "end": "17640"
  },
  {
    "text": "with caverno so a brief agenda here first of all",
    "start": "17640",
    "end": "25019"
  },
  {
    "start": "22000",
    "end": "22000"
  },
  {
    "text": "let's cover an overview of caverno very quickly we'll talk about what is it for",
    "start": "25019",
    "end": "30180"
  },
  {
    "text": "those that may not have heard of the project what does it do and then use cases for policy management of which",
    "start": "30180",
    "end": "36480"
  },
  {
    "text": "caverna has many and look at caverno across the life cycle and then we'll actually dive into some of these",
    "start": "36480",
    "end": "42480"
  },
  {
    "text": "specific use cases for platform automation teams by focusing on four such use cases first we'll look at",
    "start": "42480",
    "end": "49200"
  },
  {
    "text": "copying and syncing of config Maps we'll look at refreshing environment variables and pods we will clean up their pods",
    "start": "49200",
    "end": "56940"
  },
  {
    "text": "that might be left in a cluster and we'll also show how to scale deployments to zero based upon an event",
    "start": "56940",
    "end": "64640"
  },
  {
    "start": "64000",
    "end": "64000"
  },
  {
    "text": "so for a brief overview of caberno caverna was an admission controller that's purpose built for kubernetes it",
    "start": "64739",
    "end": "70860"
  },
  {
    "text": "is not a general purpose admission controller as a result of it being purpose-built for kubernetes all of the",
    "start": "70860",
    "end": "76920"
  },
  {
    "text": "policies and resources are written as standard yaml which means there's no programming language required either",
    "start": "76920",
    "end": "82439"
  },
  {
    "text": "knowledge of a programming language or use of a programming language anywhere in the process of implementing or",
    "start": "82439",
    "end": "88860"
  },
  {
    "text": "reasoning about a policy caverno is also the most popular by stars policy engine for kubernetes and",
    "start": "88860",
    "end": "96119"
  },
  {
    "text": "it boasts many capabilities several of which aren't found in any other policy engine for example validation which is",
    "start": "96119",
    "end": "102780"
  },
  {
    "text": "where most policy engines begin and end is your quintessential yes or no response here's a resource here's a",
    "start": "102780",
    "end": "109380"
  },
  {
    "text": "policy which matches it should it be allowed into the cluster yes or no that's validation governo does that very",
    "start": "109380",
    "end": "115680"
  },
  {
    "text": "simply and very easily mutation is the ability to change a resource API server",
    "start": "115680",
    "end": "121740"
  },
  {
    "text": "sends a resource to an admission controller and the expectation is that",
    "start": "121740",
    "end": "126960"
  },
  {
    "text": "it's either going to be modified and sent back or it's going to be allowed to persist as is Governor has very rich and",
    "start": "126960",
    "end": "133260"
  },
  {
    "text": "mature mutation capability that's been there basically out of the gate generation is a capability that's",
    "start": "133260",
    "end": "139140"
  },
  {
    "text": "endemic only to caverno which is an ability for caverto to create all new resources in the cluster based upon a",
    "start": "139140",
    "end": "146640"
  },
  {
    "text": "policy that you define caberno can also verify images both",
    "start": "146640",
    "end": "152400"
  },
  {
    "text": "image signatures and attestations on oci images in a registry this is great for",
    "start": "152400",
    "end": "158520"
  },
  {
    "text": "things like software supply chain security as you can have caverno validate these things before they're",
    "start": "158520",
    "end": "165000"
  },
  {
    "text": "allowed to run in a cluster and also and this is a new feature as of Cabernet 1.9",
    "start": "165000",
    "end": "170700"
  },
  {
    "text": "which was recently released cleanup policies caberno now has the ability to go into",
    "start": "170700",
    "end": "175739"
  },
  {
    "text": "the cluster and remove resources on a scheduled basis based upon a very",
    "start": "175739",
    "end": "180840"
  },
  {
    "text": "granular definition that you install by creating a policy in the cluster",
    "start": "180840",
    "end": "188239"
  },
  {
    "start": "188000",
    "end": "188000"
  },
  {
    "text": "so some use cases for caverno which are many in stretch from the command line",
    "start": "189480",
    "end": "195000"
  },
  {
    "text": "which caberno has as a separate CLI utility which can be useful as the ICD pipeline all the way through in the",
    "start": "195000",
    "end": "201420"
  },
  {
    "text": "cluster several of these are in many different categories so for example security this is your pod security",
    "start": "201420",
    "end": "209099"
  },
  {
    "text": "making sure that pods do not run as rude making sure that they don't run privileged things like pod security",
    "start": "209099",
    "end": "215940"
  },
  {
    "text": "standards caberno can it very well and very easily enforce granular rbac being",
    "start": "215940",
    "end": "222060"
  },
  {
    "text": "able to do things like making sure that certain users cannot delete resources",
    "start": "222060",
    "end": "228360"
  },
  {
    "text": "maybe with a certain label being able to use labels and and Define labels on",
    "start": "228360",
    "end": "236879"
  },
  {
    "text": "different types of workloads to make sure that they're properly identified in the operations category which we'll",
    "start": "236879",
    "end": "244140"
  },
  {
    "text": "look at Several of these today this is where we're kind of focused on even things like secure self-service",
    "start": "244140",
    "end": "250680"
  },
  {
    "text": "provisioning of clusters caberno can really handle a lot of this when it comes to Cluster registration",
    "start": "250680",
    "end": "256560"
  },
  {
    "text": "label and annotation management making sure that labels and annotations are there for a variety of purposes making",
    "start": "256560",
    "end": "263040"
  },
  {
    "text": "sure that names follow a given convention maybe that's a regex or something that's more simple custom CA",
    "start": "263040",
    "end": "268860"
  },
  {
    "text": "management which we'll look at a little bit today and even things like time-bound policies which we won't be",
    "start": "268860",
    "end": "274320"
  },
  {
    "text": "covering today but we'll probably do in a future session the ability for policies to be activated and deactivated",
    "start": "274320",
    "end": "280320"
  },
  {
    "text": "based upon a schedule and from a financial Ops perspective which is becoming more and more",
    "start": "280320",
    "end": "286440"
  },
  {
    "text": "prevalent making sure that the decisions that you make are driven by financial reasons things like having quotas making",
    "start": "286440",
    "end": "293759"
  },
  {
    "text": "sure that requests and limits are there making sure that labels are there for a cost purpose scaling qos management and",
    "start": "293759",
    "end": "302699"
  },
  {
    "text": "more can all be done and driven through coverino policies and all written the",
    "start": "302699",
    "end": "307740"
  },
  {
    "text": "standard yaml without having to go to a custom programming language",
    "start": "307740",
    "end": "314300"
  },
  {
    "start": "314000",
    "end": "314000"
  },
  {
    "text": "so if you look at this across the cloud-native life cycle there's really something through all stages so in the",
    "start": "314340",
    "end": "321960"
  },
  {
    "text": "commit process you can use the caperno CLI to validate that the changes in your",
    "start": "321960",
    "end": "327960"
  },
  {
    "text": "manifest that you're checking in which may ultimately be be deployed in a git Ops tool are valid and correct long",
    "start": "327960",
    "end": "335100"
  },
  {
    "text": "before they ever hit a cluster you can sign your images in those workflows and",
    "start": "335100",
    "end": "341400"
  },
  {
    "text": "then have caverno validate them again before they ever hit a cluster and then in the deployment and running",
    "start": "341400",
    "end": "348000"
  },
  {
    "text": "phase obviously this is where an admission controller like caberno shines but being able to validate these things",
    "start": "348000",
    "end": "353220"
  },
  {
    "text": "added at time of admission but also caberno has a background scan capability",
    "start": "353220",
    "end": "358320"
  },
  {
    "text": "where it will periodically scan the resources in the cluster and",
    "start": "358320",
    "end": "363419"
  },
  {
    "text": "generate a policy report if any compliance has been deviated based upon",
    "start": "363419",
    "end": "369300"
  },
  {
    "text": "what's been installed on a policy so a lot of capabilities that it has throughout this Cloud native life cycle",
    "start": "369300",
    "end": "375800"
  },
  {
    "text": "and today we'll be looking at our platform use cases which really fall",
    "start": "375800",
    "end": "381360"
  },
  {
    "text": "sort of at the end of the spectrum but this really is prevalent across all of",
    "start": "381360",
    "end": "386460"
  },
  {
    "text": "those phases so the first one let's look at here is copying and syncing of config Maps so",
    "start": "386460",
    "end": "393360"
  },
  {
    "start": "388000",
    "end": "388000"
  },
  {
    "text": "here's the problem config Maps like pods are namespace resources for a pod to use a config map",
    "start": "393360",
    "end": "400380"
  },
  {
    "text": "it must be co-located in the same namespace the challenge typically is",
    "start": "400380",
    "end": "405780"
  },
  {
    "text": "you have a lot of namespaces and you may need to use one config map across a",
    "start": "405780",
    "end": "411660"
  },
  {
    "text": "bunch of different namespaces now you could Define that config map either",
    "start": "411660",
    "end": "416699"
  },
  {
    "text": "multiple times in your git Ops tool of choice or you could use other forms of",
    "start": "416699",
    "end": "422520"
  },
  {
    "text": "automation or imperative declaration to ensure that you remember all of your",
    "start": "422520",
    "end": "428460"
  },
  {
    "text": "namespaces and lay that down but what caverno can do for us here with its",
    "start": "428460",
    "end": "433560"
  },
  {
    "text": "generation capability is allow you to define a config map in a central namespace perhaps it's called platform",
    "start": "433560",
    "end": "440639"
  },
  {
    "text": "as we'll show in this demo and be able to drive that config map to both",
    "start": "440639",
    "end": "445979"
  },
  {
    "text": "existing and new namespaces but in addition to that making sure that those",
    "start": "445979",
    "end": "451259"
  },
  {
    "text": "config Maps can all be kept in sync this is great for platform teams because now",
    "start": "451259",
    "end": "456300"
  },
  {
    "text": "it allows you to manage resources in just a central namespace and Define the",
    "start": "456300",
    "end": "461580"
  },
  {
    "text": "behavior that you want from an automation perspective as policy as code",
    "start": "461580",
    "end": "466979"
  },
  {
    "text": "that could be stored and deployed alongside all of your other resources and so what we'll show here is that same",
    "start": "466979",
    "end": "475560"
  },
  {
    "text": "type of Paradigm we'll have a namespaceful platform which we expect to",
    "start": "475560",
    "end": "480660"
  },
  {
    "text": "Define many resources that we need to consume across the cluster and one of these will be a config map and we want",
    "start": "480660",
    "end": "487319"
  },
  {
    "text": "caverno once we install a policy to lay that config map down across existing",
    "start": "487319",
    "end": "492960"
  },
  {
    "text": "namespaces in the cluster and you can see in this diagram that that's represented by a very creative namespace",
    "start": "492960",
    "end": "500340"
  },
  {
    "text": "name called existing but also new namespaces that we expect to create after that point in time we want those",
    "start": "500340",
    "end": "506639"
  },
  {
    "text": "to get the config map as well so in addition to a Brownfield this is also useful in a totally Greenfield",
    "start": "506639",
    "end": "513120"
  },
  {
    "text": "environment and a combination of both which is probably what you're going to fall into",
    "start": "513120",
    "end": "519060"
  },
  {
    "text": "so let's flip over and see this in action so I've got a standard config map here and it's called org CA",
    "start": "519060",
    "end": "526800"
  },
  {
    "text": "and this config map as the key denotes is a CA certificate now this is a",
    "start": "526800",
    "end": "532380"
  },
  {
    "text": "certificate that's from my lab environment where I have an Enterprise root certificate Authority and you may",
    "start": "532380",
    "end": "538440"
  },
  {
    "text": "be doing something very similar where you might have a certificate that",
    "start": "538440",
    "end": "544279"
  },
  {
    "text": "represents trust across your Enterprise environment and you need that certificate to be consumed by a bunch of",
    "start": "544279",
    "end": "551040"
  },
  {
    "text": "different pods and a bunch of different locations maybe even across clusters although sure you could Define that and",
    "start": "551040",
    "end": "557459"
  },
  {
    "text": "perhaps you are doing that to Sir up to a certain extent by building it into your container images maybe you need to",
    "start": "557459",
    "end": "564480"
  },
  {
    "text": "decouple that for one reason or another in config maps are commonly used for storing certificates so that's what",
    "start": "564480",
    "end": "569580"
  },
  {
    "text": "we'll do here and we're going to put this certificate in a namespace called platform",
    "start": "569580",
    "end": "575399"
  },
  {
    "text": "and now what we want is to just be able to manage this CA this config map in our",
    "start": "575399",
    "end": "581160"
  },
  {
    "text": "Central namespace and have everything else be deferred to coverno as a policy",
    "start": "581160",
    "end": "586279"
  },
  {
    "text": "so in our policy and this is a standard coverno policy and we'll just walk",
    "start": "586279",
    "end": "592260"
  },
  {
    "text": "through it very quickly converter has the ability to write a policy that applies across the entire",
    "start": "592260",
    "end": "597300"
  },
  {
    "text": "cluster very simply and in this case this is a caverno cluster policy which means it's doing just that it's applying",
    "start": "597300",
    "end": "603660"
  },
  {
    "text": "across the entire cluster and we are going to ask caverno to generate a",
    "start": "603660",
    "end": "609480"
  },
  {
    "text": "resource for us and this is the generate type of rule and we're going to generate for existing namespaces and we're",
    "start": "609480",
    "end": "616980"
  },
  {
    "text": "mapping we're matching on namespaces now once we match on any namespace we want",
    "start": "616980",
    "end": "623339"
  },
  {
    "text": "caberno to generate this resource for us it's going to be a config map also named",
    "start": "623339",
    "end": "628560"
  },
  {
    "text": "org CA and the namespace is going to be whatever namespace it matches on and it's going to clone from an existing",
    "start": "628560",
    "end": "635700"
  },
  {
    "text": "resource that's out there another variant of this might be rather than defining an existing an existing",
    "start": "635700",
    "end": "642420"
  },
  {
    "text": "resource that's out in your cluster like we're doing here this could be defined in line in the policy with a what's",
    "start": "642420",
    "end": "649140"
  },
  {
    "text": "called Data declaration now I'm not showing that here but that is another variant where if you didn't want to",
    "start": "649140",
    "end": "654660"
  },
  {
    "text": "Define this in a platform namespace you could Define everything in the policy",
    "start": "654660",
    "end": "659760"
  },
  {
    "text": "and caverna would work the same way but we're also telling it to synchronize which means that should any changes",
    "start": "659760",
    "end": "666300"
  },
  {
    "text": "happen to the source resource that's here then caverna will respond and",
    "start": "666300",
    "end": "671459"
  },
  {
    "text": "synchronize those changes down to every place that it has generated that config map so what we want to have happen is",
    "start": "671459",
    "end": "677880"
  },
  {
    "text": "when we create this policy existing namespaces because we're matching all namespaces get this config map instantly",
    "start": "677880",
    "end": "685320"
  },
  {
    "text": "and should any new namespaces be created after this point in time those new",
    "start": "685320",
    "end": "690779"
  },
  {
    "text": "namespaces will also get that so let's just try this out first thing we need to do is create the config map so that's",
    "start": "690779",
    "end": "697200"
  },
  {
    "text": "what I'll do and we've created the config map in the",
    "start": "697200",
    "end": "702839"
  },
  {
    "text": "platform namespace and now we will create this cluster policy",
    "start": "702839",
    "end": "709279"
  },
  {
    "text": "and we have created the cluster policy so now what I expect to see and I'll show my namespaces and I've got quite a",
    "start": "711180",
    "end": "717300"
  },
  {
    "text": "few of them here but let's go into the existing namespace and let's see if we got to config map",
    "start": "717300",
    "end": "725000"
  },
  {
    "text": "and you can see here we did get a config map and if we were to inspect it we would find that it's identical to the",
    "start": "726839",
    "end": "733380"
  },
  {
    "text": "one that's in the platform namespace now that's great but the last part that's missing here is we need to be able to",
    "start": "733380",
    "end": "739500"
  },
  {
    "text": "create new namespaces because this is a production cluster and we're going to continue to operate this and have",
    "start": "739500",
    "end": "744839"
  },
  {
    "text": "coverino fire and manage that certificate that config mat for us so let's create a new namespace",
    "start": "744839",
    "end": "751820"
  },
  {
    "text": "all right so we just created a namespace called new and we will get config maps in this",
    "start": "752040",
    "end": "757620"
  },
  {
    "text": "namespace and we should see that caberno has detected that new namespace and has responded by cloning this new this",
    "start": "757620",
    "end": "765120"
  },
  {
    "text": "config map into this new namespace and indeed we see that here we see the org",
    "start": "765120",
    "end": "770639"
  },
  {
    "text": "CA has been generated into this new namespace",
    "start": "770639",
    "end": "775639"
  },
  {
    "text": "so let's go ahead and clean up here all right so that's the copying and",
    "start": "775860",
    "end": "782399"
  },
  {
    "text": "syncing of config Maps so we'll click back over now this works with any",
    "start": "782399",
    "end": "787800"
  },
  {
    "text": "resource it doesn't necessarily have to be with a config map but commonly what we see that platform teams can really",
    "start": "787800",
    "end": "794579"
  },
  {
    "text": "take advantage of are things like config maps and secrets and other resources like that which are names based",
    "start": "794579",
    "end": "801060"
  },
  {
    "text": "resources that should be present in other namespaces across the cluster and by the way even though I didn't do it",
    "start": "801060",
    "end": "807660"
  },
  {
    "text": "here we could certainly narrow the scope down and say perhaps only namespaces",
    "start": "807660",
    "end": "812700"
  },
  {
    "text": "with a specific label should get this or only namespaces that had these other",
    "start": "812700",
    "end": "818399"
  },
  {
    "text": "criteria there's a lot of flexibility there but you kind of get the point so that's the first use case copying and",
    "start": "818399",
    "end": "823980"
  },
  {
    "text": "syncing a config Maps let's move on here let's talk about the problem of",
    "start": "823980",
    "end": "829800"
  },
  {
    "start": "826000",
    "end": "826000"
  },
  {
    "text": "refreshing environment variables and pods so here's the problem in kubernetes you have a pod that consumes something",
    "start": "829800",
    "end": "836760"
  },
  {
    "text": "like a config map or a secret in an environment variable and later you need to update whatever",
    "start": "836760",
    "end": "844620"
  },
  {
    "text": "that source is could be a config map could be a secret doesn't matter in this diagram obviously we're showing a secret",
    "start": "844620",
    "end": "850200"
  },
  {
    "text": "now normally if you do that and you're consuming a secret as an environment",
    "start": "850200",
    "end": "855600"
  },
  {
    "text": "variable in a pod after you update that secret the pods have no knowledge of the update that you just made there's no API",
    "start": "855600",
    "end": "863160"
  },
  {
    "text": "that goes and refreshes that if you did this in a volume that would be another story but very commonly these things",
    "start": "863160",
    "end": "870060"
  },
  {
    "text": "need to be consumed in an environment variable yet changing that source does not affect the",
    "start": "870060",
    "end": "877079"
  },
  {
    "text": "downstream pods they don't know anything about it this is where platform teams really can use caverno to make their",
    "start": "877079",
    "end": "883800"
  },
  {
    "text": "lives Easier by installing automation that's defined as policy without having",
    "start": "883800",
    "end": "890399"
  },
  {
    "text": "to write any code and maybe even eliminate some other tools in the processes in the process maybe some of",
    "start": "890399",
    "end": "896459"
  },
  {
    "text": "those being either bash scripts or maybe it's even handwork that's done",
    "start": "896459",
    "end": "901860"
  },
  {
    "text": "so any case what we want to see here is we have a secret that's being consumed in a deployment and obviously that",
    "start": "901860",
    "end": "907980"
  },
  {
    "text": "deployment is spawning pods now I'm not showing your replica set here we understand that that is an intermediary controller but that the deployment is",
    "start": "907980",
    "end": "915060"
  },
  {
    "text": "responsible for pods and those pods are getting a secret now caverno is going to watch that specific",
    "start": "915060",
    "end": "923220"
  },
  {
    "text": "secret for any changes that may occur if there is a change that's detected",
    "start": "923220",
    "end": "929160"
  },
  {
    "text": "perhaps by a user by process it makes no difference caverna is going to see that and in this case it's going to be able",
    "start": "929160",
    "end": "935880"
  },
  {
    "text": "to respond by finding the deployments that consume that secret as an environment variable and it's going to",
    "start": "935880",
    "end": "942300"
  },
  {
    "text": "annotate the deployment within the Pod template area and the effect that this is going to have is it's going to create",
    "start": "942300",
    "end": "948779"
  },
  {
    "text": "a new rollout which means that new pods are going to get spawned and as a result of those new pods getting spawned they",
    "start": "948779",
    "end": "955680"
  },
  {
    "text": "will be able to pick up the changes that were made in the environment variable from the secret and as that happens",
    "start": "955680",
    "end": "964560"
  },
  {
    "text": "it's going to once the new pod is up and running it's going to tear down the old pods so the new pods with which were",
    "start": "964560",
    "end": "971040"
  },
  {
    "text": "left will have the new value from the modifications made to the secret",
    "start": "971040",
    "end": "976380"
  },
  {
    "text": "so let's flip over and show that let's go into",
    "start": "976380",
    "end": "983300"
  },
  {
    "text": "our second one here now the first thing that we need to do is we need to Grant coverno some additional privileges",
    "start": "985380",
    "end": "991560"
  },
  {
    "text": "caverno is very security conscious and follows the principle of least privilege",
    "start": "991560",
    "end": "999060"
  },
  {
    "text": "one of the things that we need to be able to do here is to be able to modify or update deployments because we need to",
    "start": "999060",
    "end": "1005240"
  },
  {
    "text": "be able to annotate them makes this very easy because it uses role aggregation so rather than going",
    "start": "1005240",
    "end": "1011180"
  },
  {
    "text": "and changing cluster roles which may be a pain because if you're deploying coverno as many folks are with a git Ops",
    "start": "1011180",
    "end": "1018079"
  },
  {
    "text": "tool that involves making changes in git which may not necessarily be desirable",
    "start": "1018079",
    "end": "1023980"
  },
  {
    "text": "at least changing the existing cluster roles we can introduce a new cluster role here that has the necessary labels",
    "start": "1023980",
    "end": "1030678"
  },
  {
    "text": "which aggregate to the cluster role that's responsible and can get picked up",
    "start": "1030679",
    "end": "1035720"
  },
  {
    "text": "by the coverno service account so in this case I'll just create an additional cluster role that will get aggregated",
    "start": "1035720",
    "end": "1040938"
  },
  {
    "text": "and it will grant additional privileges that allows coverno to update deployments",
    "start": "1040939",
    "end": "1047319"
  },
  {
    "text": "so I've created that and now let's take a look at",
    "start": "1048980",
    "end": "1054320"
  },
  {
    "text": "our original secret so I'm going to create a secret here and this is an API",
    "start": "1054320",
    "end": "1060320"
  },
  {
    "text": "token you can think of it as and here's the value that has that's in the clear up here so 0628 is the value encoded as",
    "start": "1060320",
    "end": "1068660"
  },
  {
    "text": "base64 and this is going to be called Blue secret and it's going to go into our existing namespace now I'm going to",
    "start": "1068660",
    "end": "1075260"
  },
  {
    "text": "label this with coverno.io watch equals true now this could be any label that",
    "start": "1075260",
    "end": "1080720"
  },
  {
    "text": "you want in fact you don't necessarily need a label but for this demo we want to make this more dynamic in nature",
    "start": "1080720",
    "end": "1087020"
  },
  {
    "text": "rather than focusing on a specific Secret by name so this will allow coverno to be able to watch it a little",
    "start": "1087020",
    "end": "1092900"
  },
  {
    "text": "bit more easy without having to define or declare a specific resource so in any case we're going to create this Secret",
    "start": "1092900",
    "end": "1100580"
  },
  {
    "text": "as the first step and now that we've created blue secret",
    "start": "1100580",
    "end": "1106700"
  },
  {
    "text": "we're going to create a deployment and now this deployment as you might have",
    "start": "1106700",
    "end": "1113900"
  },
  {
    "text": "guessed is going to consume in an environment variable that token and so",
    "start": "1113900",
    "end": "1119600"
  },
  {
    "text": "it's going to consume it in an environment variable name token and it's going to fetch it from that blue secret",
    "start": "1119600",
    "end": "1125360"
  },
  {
    "text": "that we just created in the key called token and this is just a standard busy box pod it's going to sit out there and",
    "start": "1125360",
    "end": "1131240"
  },
  {
    "text": "sleep so that we can we could just want to make sure that the environment variable is consumed properly so we'll",
    "start": "1131240",
    "end": "1136880"
  },
  {
    "text": "go ahead and deploy this so we've created the blue busy box",
    "start": "1136880",
    "end": "1143600"
  },
  {
    "text": "deployment let's go ahead and get pods for this",
    "start": "1143600",
    "end": "1148640"
  },
  {
    "text": "and let's just check and make sure that the environment variable that it got is",
    "start": "1148640",
    "end": "1155600"
  },
  {
    "text": "as expected so you can see here it's picking up our",
    "start": "1155600",
    "end": "1161480"
  },
  {
    "text": "token environment variable and we see the value 0628 is what I just showed a moment ago and also our other endpoint",
    "start": "1161480",
    "end": "1168380"
  },
  {
    "text": "environment variable so that's all well and good now we want to get caverno in the picture because this is where it can",
    "start": "1168380",
    "end": "1174440"
  },
  {
    "text": "really help us in our jobs so this is what the policy is going to look like again this is a cluster policy which",
    "start": "1174440",
    "end": "1180620"
  },
  {
    "text": "means that Cabernet is going to consider this across the entire cluster and we're telling it to watch on secrets",
    "start": "1180620",
    "end": "1187100"
  },
  {
    "text": "that have this coprino.io watch label and again this could be any label that",
    "start": "1187100",
    "end": "1192919"
  },
  {
    "text": "you want if you didn't want to have a label you could certainly watch by name but we want to watch by a label to make",
    "start": "1192919",
    "end": "1199460"
  },
  {
    "text": "this a little bit more dynamic in nature because we may have multiple secrets that are consumed across multiple",
    "start": "1199460",
    "end": "1205039"
  },
  {
    "text": "deployments in multiple namespaces so we want to not tie us down we're going to watch specifically for",
    "start": "1205039",
    "end": "1211940"
  },
  {
    "text": "updates we're not interested in creations of Secrets we're interested in when a secret gets updated because that's when caberno needs to snap into",
    "start": "1211940",
    "end": "1218480"
  },
  {
    "text": "action one of the abilities that caberno has several admission controllers can do",
    "start": "1218480",
    "end": "1223640"
  },
  {
    "text": "things like mutation but what caverno can additionally do is mutate existing resources and that's what we're defining",
    "start": "1223640",
    "end": "1230539"
  },
  {
    "text": "here in the Target section we're saying any existing deployments we're interested in in any namespace",
    "start": "1230539",
    "end": "1238160"
  },
  {
    "text": "and now the magic is actually happening here where caverno is going to check",
    "start": "1238160",
    "end": "1244340"
  },
  {
    "text": "that the name of the secret is mounted or consumed by this deployment and if",
    "start": "1244340",
    "end": "1250700"
  },
  {
    "text": "it's the same one that's consumed by it and that's what this tag does here then it is going to write an annotation and",
    "start": "1250700",
    "end": "1256940"
  },
  {
    "text": "you notice that this is in the Pod template area it's going to write an annotation that I've just called",
    "start": "1256940",
    "end": "1262120"
  },
  {
    "text": "corp.org random with an eight character length random string caberno has the",
    "start": "1262120",
    "end": "1270020"
  },
  {
    "text": "ability to use a system called James path and within the James path system that caverno consumes there are many",
    "start": "1270020",
    "end": "1276500"
  },
  {
    "text": "filters that that we have written and provided specifically for caverno's use that aren't found in Upstream James path",
    "start": "1276500",
    "end": "1282559"
  },
  {
    "text": "and one of these allows you to very simply generate a random string based upon a composition of your design and so",
    "start": "1282559",
    "end": "1288380"
  },
  {
    "text": "you can see here with this regex I'm just saying give me a string that's eight characters long composed of",
    "start": "1288380",
    "end": "1293900"
  },
  {
    "text": "numbers and lowercase letters and it can be done just as simple as that and we're going to put the value of that in this",
    "start": "1293900",
    "end": "1300919"
  },
  {
    "text": "field called corp.org random so this is an annotation and the effect that this is going to have is it's going to cause",
    "start": "1300919",
    "end": "1308179"
  },
  {
    "text": "the deployment controller to see that change and understand that the actual",
    "start": "1308179",
    "end": "1313820"
  },
  {
    "text": "State now has diverged from the desired State and in response it's going to create a new rollout which is going to",
    "start": "1313820",
    "end": "1320360"
  },
  {
    "text": "give us new pods and those new pods should be able to fetch this updated secret so let's go ahead and create this",
    "start": "1320360",
    "end": "1327980"
  },
  {
    "text": "cluster policy in our cluster",
    "start": "1327980",
    "end": "1333200"
  },
  {
    "text": "and now that we've done that now we want to change the secret so we've already got",
    "start": "1333200",
    "end": "1339440"
  },
  {
    "text": "pods that are out there running now we need to rotate our API token and you can see above here I've already generated",
    "start": "1339440",
    "end": "1346940"
  },
  {
    "text": "the new base64 for this and the new value of our API token is going to end in 5 echo2 so what we expect to have",
    "start": "1346940",
    "end": "1354679"
  },
  {
    "text": "happen and this is the same representation uh as the original secret it's just we're modifying the value here",
    "start": "1354679",
    "end": "1360520"
  },
  {
    "text": "converter should be able to watch this and since it has the same label if we're",
    "start": "1360520",
    "end": "1365600"
  },
  {
    "text": "just going to see it and it's going to find matching deployments that are in that namespace and it's going to perform that",
    "start": "1365600",
    "end": "1372260"
  },
  {
    "text": "annotation that I just mentioned a moment ago so let's actually see what happens here",
    "start": "1372260",
    "end": "1378580"
  },
  {
    "text": "and now that we've done that let's go back to our existing namespace",
    "start": "1381440",
    "end": "1388000"
  },
  {
    "text": "and do a watch on pods and okay as we see here we've got a new pod that has a",
    "start": "1388000",
    "end": "1394220"
  },
  {
    "text": "four seconds ago is being spawned and this one is going into a terminating state so this is the new rollout that's",
    "start": "1394220",
    "end": "1400159"
  },
  {
    "text": "taken place and it's going to tear down the old one so we should be able to get",
    "start": "1400159",
    "end": "1405919"
  },
  {
    "text": "the environment variables in this pod and hopefully with luck we will see that",
    "start": "1405919",
    "end": "1413140"
  },
  {
    "text": "scriveno has done its job and the value of the new token environment variable",
    "start": "1413140",
    "end": "1418340"
  },
  {
    "text": "has been updated to reflect the changes",
    "start": "1418340",
    "end": "1422620"
  },
  {
    "text": "and as you can see here in fact that has occurred the new value of the token",
    "start": "1428059",
    "end": "1433760"
  },
  {
    "text": "environment variable is zero five Echo two zero five Echo two that corresponds",
    "start": "1433760",
    "end": "1438799"
  },
  {
    "text": "to our new value so you can see in this case there's some really Nifty capabilities that you can use as a",
    "start": "1438799",
    "end": "1445820"
  },
  {
    "text": "clustered operator or if you're in a platform team already this can really save you time and help uh alleviate some",
    "start": "1445820",
    "end": "1453980"
  },
  {
    "text": "of the challenges that you might be faced with today or if that's not a",
    "start": "1453980",
    "end": "1459320"
  },
  {
    "text": "challenge this can give you some new ability that you didn't have today in any case this is an illustration of",
    "start": "1459320",
    "end": "1465740"
  },
  {
    "text": "caberno's mutation capability but its ability to mutate existing",
    "start": "1465740",
    "end": "1470960"
  },
  {
    "text": "resources not just it resources that come in on the admission chain",
    "start": "1470960",
    "end": "1476620"
  },
  {
    "text": "okay so let's clean up here and we will move on to the next one",
    "start": "1476720",
    "end": "1483820"
  },
  {
    "text": "all right so that's refreshing environment variables and pods hopefully you can kind of see this is a hey that's",
    "start": "1484460",
    "end": "1489919"
  },
  {
    "text": "kind of a cool moment the next one here cleaning up bear pods this is a new",
    "start": "1489919",
    "end": "1497059"
  },
  {
    "start": "1492000",
    "end": "1492000"
  },
  {
    "text": "capability that we released in covering a 1.9 which gives caverno the ability to",
    "start": "1497059",
    "end": "1503659"
  },
  {
    "text": "delete resources clean them up based upon another governo policy that you install so caverna has long had the",
    "start": "1503659",
    "end": "1510679"
  },
  {
    "text": "ability to validate butate and even generate as we saw in the first use case but what we heard was there's still gaps",
    "start": "1510679",
    "end": "1519620"
  },
  {
    "text": "that need to be addressed when it comes to a lot of these especially platform and automation use cases that something",
    "start": "1519620",
    "end": "1526820"
  },
  {
    "text": "like being able to move remove resources would nicely complement so we came up with this ability for it to remove",
    "start": "1526820",
    "end": "1532700"
  },
  {
    "text": "resources based upon a new cluster cleanup policy or a cleanup policy so",
    "start": "1532700",
    "end": "1537740"
  },
  {
    "text": "here's a challenge then this use case that this solves very often when you're operating a",
    "start": "1537740",
    "end": "1543559"
  },
  {
    "text": "cluster we all run into problems during the course of operation no matter how much you automate no matter what you're",
    "start": "1543559",
    "end": "1549919"
  },
  {
    "text": "doing in git Ops there are always cases where a human needs to get involved and jump into a cluster and do some",
    "start": "1549919",
    "end": "1556400"
  },
  {
    "text": "troubleshooting now this could be doing things like Ping check name resolution",
    "start": "1556400",
    "end": "1561520"
  },
  {
    "text": "curling to another pod just to make sure that either the network is good or you have services that are up and running",
    "start": "1561520",
    "end": "1567679"
  },
  {
    "text": "but as commonly happens we tend to forget about some of these things once",
    "start": "1567679",
    "end": "1573260"
  },
  {
    "text": "the job is done we put down our tool tools and we go home so bear pods are",
    "start": "1573260",
    "end": "1579020"
  },
  {
    "text": "oftentimes used for this type of break glass or troubleshooting scenario and",
    "start": "1579020",
    "end": "1584179"
  },
  {
    "text": "the word bear pod refers to a pod that's not owned by a high level a higher level controller like a deployment",
    "start": "1584179",
    "end": "1591080"
  },
  {
    "text": "a bear pod is oftentimes created imperatively using something like a cube control create or cube control run",
    "start": "1591080",
    "end": "1597740"
  },
  {
    "text": "command and once those pods have done their job and users and operators have",
    "start": "1597740",
    "end": "1603020"
  },
  {
    "text": "execed into them or done whatever they need to do they might still be running out there and in cases where you might",
    "start": "1603020",
    "end": "1610220"
  },
  {
    "text": "be running kubernetes in a public Cloud environment this can incur additional costs because you multiply this by",
    "start": "1610220",
    "end": "1616340"
  },
  {
    "text": "multiple teams in multiple namespaces it's not uncommon to see maybe many of",
    "start": "1616340",
    "end": "1622220"
  },
  {
    "text": "these pods that are running out there and that could become fairly cumbersome and introduce a lot of clutter so what",
    "start": "1622220",
    "end": "1628700"
  },
  {
    "text": "we could do is use caverno to help us solve this by scouring the cluster and",
    "start": "1628700",
    "end": "1633799"
  },
  {
    "text": "finding these bare pods and if they exist deleting them and it can do this on a scheduled basis rather than just",
    "start": "1633799",
    "end": "1640700"
  },
  {
    "text": "running an imperative command one time so that's what we'll show here we've got a bunch of different name spaces with",
    "start": "1640700",
    "end": "1646640"
  },
  {
    "text": "these bear pods and we're going to create a new cluster",
    "start": "1646640",
    "end": "1651740"
  },
  {
    "text": "cleanup policy which will look across the cluster and find all of these bear pods across these",
    "start": "1651740",
    "end": "1657320"
  },
  {
    "text": "namespaces and it will remove these for us so let's flip over and show this",
    "start": "1657320",
    "end": "1664179"
  },
  {
    "text": "so similar to what I talked about in the second demo we need to Grant coverno a",
    "start": "1667159",
    "end": "1673640"
  },
  {
    "text": "little bit more privileges here specifically we need the ability for it to remove pods and it's necessary for us",
    "start": "1673640",
    "end": "1680240"
  },
  {
    "text": "to list and delete these and you'll notice again here we're not having to modify one of the main or the main",
    "start": "1680240",
    "end": "1687020"
  },
  {
    "text": "cholesterol we have role aggregation that's enabled you can create a simple cluster role like this and as long as",
    "start": "1687020",
    "end": "1693320"
  },
  {
    "text": "the labels are installed it will get aggregated to the base cluster role and caverna will be able to do its job so",
    "start": "1693320",
    "end": "1700700"
  },
  {
    "text": "let's first give governor of these privileges we've done that",
    "start": "1700700",
    "end": "1706700"
  },
  {
    "text": "and now let's create some bear pods so you've noticed here I like to use BusyBox as a very simple uh container to",
    "start": "1706700",
    "end": "1714380"
  },
  {
    "text": "illustrate a number of things and we've got a number of busy box pod that are just going to go into another sleep State across a bunch of different",
    "start": "1714380",
    "end": "1720980"
  },
  {
    "text": "namespaces so our ing namespace our platform namespace and our existing",
    "start": "1720980",
    "end": "1726620"
  },
  {
    "text": "namespace we're just going to simulate some bear pods now you've noticed here these are just your standard bear pods",
    "start": "1726620",
    "end": "1732260"
  },
  {
    "text": "they're not owned by a deployment and so let's go ahead and create these",
    "start": "1732260",
    "end": "1738279"
  },
  {
    "text": "and we have some bear pods that are now out there running now let's take a look at the new cluster cleanup policy that",
    "start": "1738799",
    "end": "1746120"
  },
  {
    "text": "comes with caverno 1.9 so in this cluster cleanup policy this is a new custom resource you notice the previous",
    "start": "1746120",
    "end": "1753200"
  },
  {
    "text": "ones were cluster policies well we've introduced a cluster cleanup policy which is similar to other caverno",
    "start": "1753200",
    "end": "1759440"
  },
  {
    "text": "policies accepted is only specific to the cleanup ability and what we're doing here is matching on",
    "start": "1759440",
    "end": "1767000"
  },
  {
    "text": "all pods and you can if you're not familiar with caverno you can see kind of a common theme here that the way that",
    "start": "1767000",
    "end": "1773600"
  },
  {
    "text": "we declare policy is all standard yaml using constructs and patterns with which you're likely already there you're",
    "start": "1773600",
    "end": "1780140"
  },
  {
    "text": "familiar on account of having to do these probably on a daily basis for more than just policy for things like pods",
    "start": "1780140",
    "end": "1786980"
  },
  {
    "text": "certificates even ordering whole clusters you can do with simple paradigms like this so we want to be",
    "start": "1786980",
    "end": "1793940"
  },
  {
    "text": "able to use those same constructs and policy and so we're just matching on pods and also we're going to look into",
    "start": "1793940",
    "end": "1801080"
  },
  {
    "text": "those pods and now what we're doing here is we're looking at the owner references key and the owner references object",
    "start": "1801080",
    "end": "1807799"
  },
  {
    "text": "that's in a pod is where a pod would declare another ownership so for example",
    "start": "1807799",
    "end": "1813620"
  },
  {
    "text": "if you had a deployment that was spawning pods those pods would have an owner reference back to a replica set",
    "start": "1813620",
    "end": "1819380"
  },
  {
    "text": "for example and these could be owned by something else or even multiple potentially but we're we're specifically",
    "start": "1819380",
    "end": "1824480"
  },
  {
    "text": "looking for pods and Target refers to all of the existing pods that are found out there not new pods that are coming",
    "start": "1824480",
    "end": "1830720"
  },
  {
    "text": "in it's only looking at the existing ones it's returning all the ones that have",
    "start": "1830720",
    "end": "1835820"
  },
  {
    "text": "empty owner references because those are the only ones that we're interested in and then our schedule here which is a",
    "start": "1835820",
    "end": "1841279"
  },
  {
    "text": "common cron format we're going to run this every minute now I'm only doing this for the purposes of",
    "start": "1841279",
    "end": "1846980"
  },
  {
    "text": "this demo this is probably not something that you want to do in a live environment but in the interest of time",
    "start": "1846980",
    "end": "1852919"
  },
  {
    "text": "I will go ahead and create this and what we expect to have happen is from the moment that I create this it'll start",
    "start": "1852919",
    "end": "1858799"
  },
  {
    "text": "its countdown when the schedule elapses covering is going to kick in it's going to look across the entire cluster four",
    "start": "1858799",
    "end": "1865580"
  },
  {
    "text": "pods it's going to crack each one open and evaluate this and it's going to make sure that it does not have an owner",
    "start": "1865580",
    "end": "1871700"
  },
  {
    "text": "reference if it does not have an owner reference it's going to gather them all together and then immediately delete",
    "start": "1871700",
    "end": "1876860"
  },
  {
    "text": "them so let's create this and see if it actually does that so I have created our clean naked pods",
    "start": "1876860",
    "end": "1884899"
  },
  {
    "text": "cluster cleanup policy and so let's go and look at these bear pods and I've",
    "start": "1884899",
    "end": "1892039"
  },
  {
    "text": "assigned a label to them for easier tracking so let's get pods across all",
    "start": "1892039",
    "end": "1898039"
  },
  {
    "text": "the namespaces that have run busy box associated with them and we will watch",
    "start": "1898039",
    "end": "1903679"
  },
  {
    "text": "them and since these are all in a running State these have done their job admirably they're no longer needed yet",
    "start": "1903679",
    "end": "1910940"
  },
  {
    "text": "they're still sitting out there and running so once the schedule has elapsed",
    "start": "1910940",
    "end": "1916039"
  },
  {
    "text": "proveno should be able to find just these pods but not any other pods because these are the only ones that are",
    "start": "1916039",
    "end": "1921620"
  },
  {
    "text": "bare and it should call out to the cleanup controller and have it remove them so what we expect to see is which",
    "start": "1921620",
    "end": "1928159"
  },
  {
    "text": "is what we're just now seeing is these pods go into terminating State because Governor has ordered their deletion and",
    "start": "1928159",
    "end": "1934940"
  },
  {
    "text": "once this termination State finishes these pods will be removed from the cluster and if we check again then",
    "start": "1934940",
    "end": "1942380"
  },
  {
    "text": "they're still in terminating state but in just a moment here once those processes exit",
    "start": "1942380",
    "end": "1949659"
  },
  {
    "text": "these pods should be removed from the cluster",
    "start": "1949659",
    "end": "1955299"
  },
  {
    "text": "and so we'll trust that that'll happen here and there you go so pods there are bear",
    "start": "1956360",
    "end": "1963500"
  },
  {
    "text": "pods were just removed but the rest of the pods that are owned by controllers they are still present",
    "start": "1963500",
    "end": "1971299"
  },
  {
    "text": "so let's clean up here and let's flip back over for the last",
    "start": "1971299",
    "end": "1976940"
  },
  {
    "text": "use case so anyway as you can see with this use case this can be super useful",
    "start": "1976940",
    "end": "1983120"
  },
  {
    "text": "for cluster admins and platform teams because it can remove a lot of craft",
    "start": "1983120",
    "end": "1988760"
  },
  {
    "text": "from your cluster that you may not be interested in and operators and",
    "start": "1988760",
    "end": "1994720"
  },
  {
    "text": "financial teams typically like this because this oftentimes has a real cost",
    "start": "1994720",
    "end": "2000519"
  },
  {
    "text": "savings that associ that's associated with it now I'm only doing a a I only showed a",
    "start": "2000519",
    "end": "2006880"
  },
  {
    "text": "very simplistic use case here with cleaning up pods but you can imagine this could be any resource and even multiple resources across your cluster",
    "start": "2006880",
    "end": "2013779"
  },
  {
    "text": "with some very complex conditions that you use in order to reduce the the",
    "start": "2013779",
    "end": "2020080"
  },
  {
    "text": "number of matches down to only the ones that you care about so hopefully this was a a an enlightening use case on how",
    "start": "2020080",
    "end": "2027940"
  },
  {
    "text": "you can maybe operationalize these types of cleanup policies so let's look at the last use case here",
    "start": "2027940",
    "end": "2034840"
  },
  {
    "start": "2032000",
    "end": "2032000"
  },
  {
    "text": "last platform use case here is scale deployments to zero now this isn't really about things like event Driven",
    "start": "2034840",
    "end": "2042279"
  },
  {
    "text": "Auto scaling this is more about from an operator's perspective you getting",
    "start": "2042279",
    "end": "2047860"
  },
  {
    "text": "automated help that you need to be able to know what to do next or just to",
    "start": "2047860",
    "end": "2053080"
  },
  {
    "text": "assist in some of your day-to-day jobs so here's a common problem you have a deployment that's managing pods and",
    "start": "2053080",
    "end": "2059378"
  },
  {
    "text": "something happens in those pods continually restart they go into crash loop back off and as you probably know",
    "start": "2059379",
    "end": "2065320"
  },
  {
    "text": "caverna is going to continue to try and restart those on a periodic basis very",
    "start": "2065320",
    "end": "2071919"
  },
  {
    "text": "often though depending on the problem that happens no amount of restart could",
    "start": "2071919",
    "end": "2077378"
  },
  {
    "text": "be potentially useful of course that depends on the situation but but in some certain circumstances",
    "start": "2077379",
    "end": "2084339"
  },
  {
    "text": "they're just going to endlessly come in to be scheduled go into a running State",
    "start": "2084339",
    "end": "2089378"
  },
  {
    "text": "maybe immediately or at some point in the future crash and then that cycle is",
    "start": "2089379",
    "end": "2094540"
  },
  {
    "text": "just going to repeat Ad nauseam what we can do is help use caverno to help us",
    "start": "2094540",
    "end": "2101020"
  },
  {
    "text": "identify those and if that situation is happening on a threshold which may be",
    "start": "2101020",
    "end": "2106720"
  },
  {
    "text": "too much we can have coverto scale that deployment down to zero and also tell us about it in some way marking that we",
    "start": "2106720",
    "end": "2113800"
  },
  {
    "text": "need to do some additional troubleshooting but the reason why this is especially helpful is that it doesn't create that additional pod churn and if",
    "start": "2113800",
    "end": "2121240"
  },
  {
    "text": "this is happening across multiple deployments in a cluster that pod churn could be fairly significant so if this",
    "start": "2121240",
    "end": "2127420"
  },
  {
    "text": "is happening if this restart process is happening too much caverno can observe that",
    "start": "2127420",
    "end": "2132820"
  },
  {
    "text": "and that's exactly what we're going to show here so we've got a deployment that is spawning one or multiple pods and",
    "start": "2132820",
    "end": "2140560"
  },
  {
    "text": "something happens in that pod there's a problem and it's restarting and in this case I'm just going to set the threshold",
    "start": "2140560",
    "end": "2146920"
  },
  {
    "text": "the three you can imagine this could be anything but the Pod if it restarts any more than three times we know that",
    "start": "2146920",
    "end": "2153760"
  },
  {
    "text": "something's gone wrong with it further restarts aren't going to help it we need coverno to help us out here so that we",
    "start": "2153760",
    "end": "2159700"
  },
  {
    "text": "can take action on a later date so caverno is going to be deployed",
    "start": "2159700",
    "end": "2165880"
  },
  {
    "text": "and it's going to observe that event happening and when that happens it is going to scale that deployment down to",
    "start": "2165880",
    "end": "2172720"
  },
  {
    "text": "zero and also annotate it so that we know from a platform perspective and we",
    "start": "2172720",
    "end": "2178599"
  },
  {
    "text": "might pick this up in our monitoring tools hey this is now with zero replicas it's because Something's Happened let's",
    "start": "2178599",
    "end": "2184180"
  },
  {
    "text": "take a look at it so let's flip over and do that let's take a look at this now again we",
    "start": "2184180",
    "end": "2191680"
  },
  {
    "text": "need to Grant Governor some of those additional privilege that I mentioned because just like in the Second Use case",
    "start": "2191680",
    "end": "2200440"
  },
  {
    "text": "we need to update deployments so since I've removed those previous resources I",
    "start": "2200440",
    "end": "2205599"
  },
  {
    "text": "need to put that cluster role back in place and again same thing with role aggregation that's what we're doing here",
    "start": "2205599",
    "end": "2211180"
  },
  {
    "text": "so we've given it those permissions and now I'm going to create a deployment and back to our Ye Old busy box this time",
    "start": "2211180",
    "end": "2218140"
  },
  {
    "text": "it's going to sleep for 10 seconds so after that sleep cycle is done it's going to cause the main process to exit",
    "start": "2218140",
    "end": "2224920"
  },
  {
    "text": "which is in turn going to have kubernetes try and restart it so it's just going to sit out there and",
    "start": "2224920",
    "end": "2231460"
  },
  {
    "text": "run and we're calling it distressed busy box it's going to sleep and then after 10 seconds it's going to restart and so",
    "start": "2231460",
    "end": "2239140"
  },
  {
    "text": "here's our governor policy now this is a little bit more involved so I won't walk through all of this but this is just an",
    "start": "2239140",
    "end": "2244960"
  },
  {
    "text": "illustration of some of the power that's capable of in caberno and as you can see",
    "start": "2244960",
    "end": "2250119"
  },
  {
    "text": "there's no programming language that's involved here if you're familiar with things like variables and as we showed",
    "start": "2250119",
    "end": "2256839"
  },
  {
    "text": "with previous policies some of the existing constructs and patterns with which you're likely using today then you",
    "start": "2256839",
    "end": "2264400"
  },
  {
    "text": "can probably parse this so we'll just a couple things to to point out here Governor has the ability to look at sub",
    "start": "2264400",
    "end": "2270339"
  },
  {
    "text": "resources so we are looking at the status sub resource in a pod we are checking for updates to it and here's",
    "start": "2270339",
    "end": "2277480"
  },
  {
    "text": "where we're going to Define our update count we want to see any restarts anything that's greater than two is",
    "start": "2277480",
    "end": "2282940"
  },
  {
    "text": "going to trigger this policy now a pod is going to Define an owner as a replica",
    "start": "2282940",
    "end": "2288940"
  },
  {
    "text": "set in the case of a deployment and we need to be able to identify the deployment so",
    "start": "2288940",
    "end": "2294579"
  },
  {
    "text": "we're going to use what are called context variables in order to ask the API server to build",
    "start": "2294579",
    "end": "2300640"
  },
  {
    "text": "that chain back up to the parent so we're going to find the replica set name and then we're going to ask the API",
    "start": "2300640",
    "end": "2306160"
  },
  {
    "text": "server hey give me the deployment that corresponds to that replica set name",
    "start": "2306160",
    "end": "2311800"
  },
  {
    "text": "that'll give us the deployment and then from there and that started deployment we will be able to once again mutate",
    "start": "2311800",
    "end": "2318700"
  },
  {
    "text": "existing deployments not new deployments because we already have a deployment that's that's out there in running we",
    "start": "2318700",
    "end": "2323800"
  },
  {
    "text": "will mutate any existing deployment that is met by this and we are going to put",
    "start": "2323800",
    "end": "2329440"
  },
  {
    "text": "the replica count to zero and we're going to write our sre.corp.org",
    "start": "2329440",
    "end": "2334619"
  },
  {
    "text": "troubleshooting needed annotation set to True which will allow us to pick this up in a monitoring tool or some sort of",
    "start": "2334619",
    "end": "2340839"
  },
  {
    "text": "reporting tool that we might be using to identify hey it's not just set to zero",
    "start": "2340839",
    "end": "2347140"
  },
  {
    "text": "replicas because something like keita for example was in the cluster and it scaled it to zero because there wasn't anything to do it's because there is an",
    "start": "2347140",
    "end": "2353980"
  },
  {
    "text": "actual problem and somebody really needs to look into this so let's go ahead and create this cluster policy",
    "start": "2353980",
    "end": "2361020"
  },
  {
    "text": "and now that that's in place let's create the deployment which is going to sit out there and",
    "start": "2362020",
    "end": "2370180"
  },
  {
    "text": "run a busy box container in a deployment and it's going to sleep",
    "start": "2370180",
    "end": "2376119"
  },
  {
    "text": "and therefore restart every 10 minutes every 10 seconds so what we expect to",
    "start": "2376119",
    "end": "2381579"
  },
  {
    "text": "have happen and I'm just going to watch the deployment here we're going to watch the deployment",
    "start": "2381579",
    "end": "2387700"
  },
  {
    "text": "we've currently got one replica and now what we expect to have happen is",
    "start": "2387700",
    "end": "2394420"
  },
  {
    "text": "this deployment count getting set to zero and that's going to indicate that",
    "start": "2394420",
    "end": "2400300"
  },
  {
    "text": "caverno has observed the three pod restarts and it has backtracked found the replica",
    "start": "2400300",
    "end": "2408160"
  },
  {
    "text": "set which is defined as part of the Pod and then looked up the deployment that corresponds to that replica set",
    "start": "2408160",
    "end": "2414220"
  },
  {
    "text": "scaled the replicas by setting that field to zero and also added this",
    "start": "2414220",
    "end": "2419619"
  },
  {
    "text": "annotation to it so let's just watch for a moment here more as you can see the oscillation",
    "start": "2419619",
    "end": "2427900"
  },
  {
    "text": "between one of one and zero of one represents those pod restarts when our",
    "start": "2427900",
    "end": "2433180"
  },
  {
    "text": "sleep period of 10 seconds has ended and so therefore uh cubelet's going to attempt to restart that and give us",
    "start": "2433180",
    "end": "2439119"
  },
  {
    "text": "another sleep for 10 seconds but it's just going to do this over and over again and on the next one once this",
    "start": "2439119",
    "end": "2444520"
  },
  {
    "text": "occurs it's going to observe that and as you can see that's just happened here it's now set the the number of replicas",
    "start": "2444520",
    "end": "2453339"
  },
  {
    "text": "to zero so there should not be any pods that remain from this because it scaled",
    "start": "2453339",
    "end": "2458859"
  },
  {
    "text": "it to zero let's just check it could be in the process of terminating them and it's not",
    "start": "2458859",
    "end": "2464020"
  },
  {
    "text": "it scaled them to zero now let's just check one last time let's take a look at",
    "start": "2464020",
    "end": "2470200"
  },
  {
    "text": "the distress busy box deployment and see that we actually got what we expected",
    "start": "2470200",
    "end": "2479700"
  },
  {
    "text": "and so let's scroll up here and as we can see we got replica count",
    "start": "2480520",
    "end": "2485980"
  },
  {
    "text": "of zero and we got our annotation informing us that somebody needs to get",
    "start": "2485980",
    "end": "2491740"
  },
  {
    "text": "in here and do some troubleshooting all of this happened in an automated fashion",
    "start": "2491740",
    "end": "2497140"
  },
  {
    "text": "that you define as policy without having any code that's defined in there and",
    "start": "2497140",
    "end": "2502839"
  },
  {
    "text": "caverna was able to take care of this so with that",
    "start": "2502839",
    "end": "2508540"
  },
  {
    "text": "that is the end of the demo and the end of this recording I hope this has been",
    "start": "2508540",
    "end": "2515680"
  },
  {
    "text": "useful and how you can use caverno to help you in your platform engineering",
    "start": "2515680",
    "end": "2522220"
  },
  {
    "text": "jobs and save some trouble and also maybe make your lives a little bit",
    "start": "2522220",
    "end": "2528640"
  },
  {
    "text": "easier maybe eliminate some tools and all of these capabilities can be combined in a lot of really interesting",
    "start": "2528640",
    "end": "2535900"
  },
  {
    "text": "ways so that you can get even larger more complex use cases out of this thanks for attending and please hit me",
    "start": "2535900",
    "end": "2542980"
  },
  {
    "text": "up if I can help you out thanks very much",
    "start": "2542980",
    "end": "2547200"
  }
]