[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "so H hello everyone my name is Zano Kaiser I'm from Nvidia I'm with the cloud native team um my current main",
    "start": "320",
    "end": "8440"
  },
  {
    "text": "responsibilities are working on Kata and confidential containers and um in this",
    "start": "8440",
    "end": "15080"
  },
  {
    "text": "talk I I just want to give you also some history about sandbox environments um",
    "start": "15080",
    "end": "20240"
  },
  {
    "text": "what we've done in the Kata space to enable our use cases um and how we can",
    "start": "20240",
    "end": "26560"
  },
  {
    "text": "apply all those features that we added uh to Reg llms but also to any AI ml",
    "start": "26560",
    "end": "33800"
  },
  {
    "text": "pipeline um agenda is talking about how we came to confidential Computing um why",
    "start": "33800",
    "end": "41239"
  },
  {
    "start": "35000",
    "end": "83000"
  },
  {
    "text": "we have chosen Kata as the main driver for enabling sandbox environments with",
    "start": "41239",
    "end": "47079"
  },
  {
    "text": "the GPU um a little bit explanation about our GPU enablement stack because",
    "start": "47079",
    "end": "53680"
  },
  {
    "text": "it's uh important for our lift and shift strategy where we said we don't want to have any code modification to run our",
    "start": "53680",
    "end": "61039"
  },
  {
    "text": "GPU workloads on Kata or on confidential container um we added also a virtualization reference architecture to",
    "start": "61039",
    "end": "67439"
  },
  {
    "text": "support Advanced use cases like GPU direct GDs or RDMA in virtualized sandbox",
    "start": "67439",
    "end": "73159"
  },
  {
    "text": "environments um a small stop on confidential containers and then I'm going to talk a little bit about",
    "start": "73159",
    "end": "79600"
  },
  {
    "text": "confidential Rec llms um but let me first set a stage why",
    "start": "79600",
    "end": "86720"
  },
  {
    "start": "83000",
    "end": "157000"
  },
  {
    "text": "why are we even doing this right um if you're looking at a container that's just a process with namespace separation",
    "start": "86720",
    "end": "92840"
  },
  {
    "text": "and and cgroup Resource Management um containers share the host kernel meaning",
    "start": "92840",
    "end": "98399"
  },
  {
    "text": "if I have a container breakout it can take over the complete note it can take over the complete cluster um it's just a",
    "start": "98399",
    "end": "104759"
  },
  {
    "text": "modern way of packaging and sharing and deploying uh applications it's a user space abstraction only and um the red",
    "start": "104759",
    "end": "112719"
  },
  {
    "text": "part shows what we are most concerned about it's the weak security boundary",
    "start": "112719",
    "end": "118000"
  },
  {
    "text": "between a container runtime and the host operating system we have things like",
    "start": "118000",
    "end": "123360"
  },
  {
    "text": "selinux and armor uh to fortify it um but we don't have any H isolation and we",
    "start": "123360",
    "end": "129280"
  },
  {
    "text": "are highly dependent on on the Kernel and what we are mostly worried about is",
    "start": "129280",
    "end": "134440"
  },
  {
    "text": "that host changes may break our container stack because we are um deploying drivers and other components",
    "start": "134440",
    "end": "140760"
  },
  {
    "text": "that we need to enable the GPU um as I said before container escapes take over a hold note um and we need to trust the",
    "start": "140760",
    "end": "147760"
  },
  {
    "text": "container imges um and the manifests that we are pulling in into into our",
    "start": "147760",
    "end": "154319"
  },
  {
    "start": "157000",
    "end": "321000"
  },
  {
    "text": "cluster in the past there were some techniques that were people are looking",
    "start": "157720",
    "end": "164120"
  },
  {
    "text": "into to create sandbox environments um way back there were unic kernels unic kernels is a way to package uh parts of",
    "start": "164120",
    "end": "172159"
  },
  {
    "text": "the kernel and parts of your user space into one binary without a memory protection unit so you have fast po",
    "start": "172159",
    "end": "179040"
  },
  {
    "text": "times small etic surfaces and and a really nice latency but nobody wants to",
    "start": "179040",
    "end": "184720"
  },
  {
    "text": "recompile uh their applications and package it and uh link uh various um",
    "start": "184720",
    "end": "190760"
  },
  {
    "text": "libraries to it it just didn't um was easy enough to create unic kernels",
    "start": "190760",
    "end": "197760"
  },
  {
    "text": "from existing um applications um ABM IBM nabla was took this idea of unic kernels",
    "start": "197760",
    "end": "205440"
  },
  {
    "text": "and run it as a as a process essentially what they're were doing is reducing um",
    "start": "205440",
    "end": "211519"
  },
  {
    "text": "reducing the amount of CIS calls that the container runtime is issuing to the post operating system",
    "start": "211519",
    "end": "220599"
  },
  {
    "text": "um they added hyper calls for privileged operations and they even added a complete oci compatible um runtime so",
    "start": "220599",
    "end": "227680"
  },
  {
    "text": "you could run containers with it and on kubernetes um another sandbox environment which people are aware of is",
    "start": "227680",
    "end": "234879"
  },
  {
    "text": "gvisor it's essentially a user space kernel implementing all CIS calls um in",
    "start": "234879",
    "end": "240439"
  },
  {
    "text": "user space and they as far as I know they reimplemented like 70% of CIS calls",
    "start": "240439",
    "end": "246799"
  },
  {
    "text": "but gisa has no device model so you cannot run device drivers you cannot run uh easily a GPU inside of of",
    "start": "246799",
    "end": "253159"
  },
  {
    "text": "gvisor um there's also cubert cubert is a VM in a pot and the PO is mainly used",
    "start": "253159",
    "end": "259560"
  },
  {
    "text": "as the deployment vehicle mostly used for legacy VM applications um where you",
    "start": "259560",
    "end": "264960"
  },
  {
    "text": "are still interacting with the VM um AWS",
    "start": "264960",
    "end": "270080"
  },
  {
    "text": "took a step and uh created firecracker um micro virtual machine uh to run",
    "start": "270080",
    "end": "275759"
  },
  {
    "text": "sandbox in a sandbox environment it's a minimal operating system but they don't have any emulation for devices so GPU",
    "start": "275759",
    "end": "282560"
  },
  {
    "text": "are currently not working with with firecracker um and lastly Kata",
    "start": "282560",
    "end": "288080"
  },
  {
    "text": "containers um this is where we invested",
    "start": "288080",
    "end": "293520"
  },
  {
    "text": "a lot because um I will go on the reasons why and how and what we're doing in the Kata space it's essentially also",
    "start": "293520",
    "end": "300440"
  },
  {
    "text": "a micro and light virtual machines that we are leveraging uh to run sandbox environments and especially GPU uh GPU",
    "start": "300440",
    "end": "307520"
  },
  {
    "text": "workloads um there's a nice talk on Friday by our friends in the community about running unic kernels in kubernetes",
    "start": "307520",
    "end": "314360"
  },
  {
    "text": "so check it out um especially in in the serverless Computing in the K native",
    "start": "314360",
    "end": "321120"
  },
  {
    "start": "321000",
    "end": "390000"
  },
  {
    "text": "space okay what's Kata so Kata essentially a container in in a VM uh",
    "start": "321520",
    "end": "326880"
  },
  {
    "text": "Kata supports a broad spectrum of hyp visors be U Acorn cloud hypervisor and",
    "start": "326880",
    "end": "332120"
  },
  {
    "text": "currently we are working on with the Upstream community on adding a rust micr vmm which is called Dragon Ball um it",
    "start": "332120",
    "end": "338160"
  },
  {
    "text": "seamlessly plugs into any orchestration platforms like kubernetes and container run times um container and workloads are",
    "start": "338160",
    "end": "346160"
  },
  {
    "text": "now kernel and user space independent meaning host kernel updates cannot easily break break the stack we can run",
    "start": "346160",
    "end": "353080"
  },
  {
    "text": "untrusted code in a container uh virtualization is a second line of defense um the AO runtime is mainly",
    "start": "353080",
    "end": "360639"
  },
  {
    "text": "responsible for Life cycling of the VM and we have an inner runtime which is an oci",
    "start": "360639",
    "end": "366000"
  },
  {
    "text": "compliant um run time it adheres to cni CSI CRI so it's completely transparent",
    "start": "366000",
    "end": "373080"
  },
  {
    "text": "running a container or cat container in kubernetes um all the functionality that",
    "start": "373080",
    "end": "378400"
  },
  {
    "text": "kubernetes is providing you cata will just pick it up be it storage be it networking or what the CR is going to do",
    "start": "378400",
    "end": "386360"
  },
  {
    "text": "you not only we are interested in fortifying",
    "start": "388560",
    "end": "393800"
  },
  {
    "start": "390000",
    "end": "559000"
  },
  {
    "text": "like the isolation of the container runtime to the host operating system but one other thing that we are really",
    "start": "393800",
    "end": "399599"
  },
  {
    "text": "interested is also to fortify the isolation between applications that are running on the cluster right and there",
    "start": "399599",
    "end": "405880"
  },
  {
    "text": "are several um features or projects that are trying",
    "start": "405880",
    "end": "412520"
  },
  {
    "text": "to enable or fortify the installation one is homomorphic encryption which essentially enables computation on",
    "start": "412520",
    "end": "418440"
  },
  {
    "text": "encrypted data without decrypting uh decrypt without decryption we have secure multi party computation AKA",
    "start": "418440",
    "end": "425680"
  },
  {
    "text": "Federate learning meaning you can allow parties to jointly compute uh a function",
    "start": "425680",
    "end": "430960"
  },
  {
    "text": "over the inputs without uh without revealing their inputs to other parties",
    "start": "430960",
    "end": "437000"
  },
  {
    "text": "um and the third big function that was or features that were added uh to",
    "start": "437000",
    "end": "443759"
  },
  {
    "text": "Hardware vendors are crossed execution environments um for the past years",
    "start": "443759",
    "end": "450120"
  },
  {
    "text": "we have very good solutions for uh protecting data add rest meaning we have",
    "start": "450120",
    "end": "455160"
  },
  {
    "text": "encrypted databases we have good Solutions of data and Transit meaning um encryption on the network be it IPC or",
    "start": "455160",
    "end": "462800"
  },
  {
    "text": "TLs but as soon as you decrypt your database and running on a host it's completely vulnerable because you don't",
    "start": "462800",
    "end": "468960"
  },
  {
    "text": "have any encryption on the Note uh and this is where confidential Computing comes into place where those trust",
    "start": "468960",
    "end": "474919"
  },
  {
    "text": "execution environments are providing a environment um to run your workload in a",
    "start": "474919",
    "end": "481199"
  },
  {
    "text": "VM which is completely encrypted not only the memory is encrypted um the hypervisor has no access to your",
    "start": "481199",
    "end": "487000"
  },
  {
    "text": "register State because the register state would expose like the frame pointer and St pointer so hypervisor could deduce what what you're doing in a",
    "start": "487000",
    "end": "493639"
  },
  {
    "text": "VM uh also interrupts are obfuscated uh so that essentially the hypervisor does",
    "start": "493639",
    "end": "499280"
  },
  {
    "text": "not have access to any parts of the VM so if we are breaking out of of a",
    "start": "499280",
    "end": "504759"
  },
  {
    "text": "container um we still have like VM as a second line of defense but if a attacker",
    "start": "504759",
    "end": "511199"
  },
  {
    "text": "also is able to escape out of the VM he has no access to the other VMS he can",
    "start": "511199",
    "end": "518279"
  },
  {
    "text": "still do dyal s attacks on the VM you know shutting them down but the confidential data inside of the VMS is",
    "start": "518279",
    "end": "524080"
  },
  {
    "text": "still uh still protected um just a small",
    "start": "524080",
    "end": "529279"
  },
  {
    "text": "history uh trust ex is nothing completely new already in 2004 um we had",
    "start": "529279",
    "end": "536480"
  },
  {
    "text": "first PR execution environments uh all the major CPU vendors uh have trusted acution",
    "start": "536480",
    "end": "542760"
  },
  {
    "text": "environments right now if you have a mobile phone you running a trusted acution environment for vpay Apple pay",
    "start": "542760",
    "end": "549279"
  },
  {
    "text": "Google pay they're all running in some security enclaves and all major architectures are",
    "start": "549279",
    "end": "555240"
  },
  {
    "text": "providing uh trust execution environments so let me just talk a",
    "start": "555240",
    "end": "561200"
  },
  {
    "start": "559000",
    "end": "750000"
  },
  {
    "text": "little bit why and how and what we're doing with",
    "start": "561200",
    "end": "566240"
  },
  {
    "text": "Kata as I mentioned before the why why why we chosen it containers are now",
    "start": "566240",
    "end": "572279"
  },
  {
    "text": "kernel and user space independent uh host changes do not affect us very much um container breakouts cannot compromise",
    "start": "572279",
    "end": "579279"
  },
  {
    "text": "the whole note or the complete cluster because we are still running in a VM um we can seamlessly plug in into existing",
    "start": "579279",
    "end": "586600"
  },
  {
    "text": "orchestration platforms like kubernetes and other container run times we have full oci runtime image support and we",
    "start": "586600",
    "end": "593160"
  },
  {
    "text": "can run containers without modification",
    "start": "593160",
    "end": "598120"
  },
  {
    "text": "um the other point is we can run untrusted cod in a container um because we have",
    "start": "598320",
    "end": "604040"
  },
  {
    "text": "virtualization as a second line of defense and Kata supports a wide range of trusted execution environments like",
    "start": "604040",
    "end": "609720"
  },
  {
    "text": "Intel TDX AMD SMP uh arm CCA and uh s390",
    "start": "609720",
    "end": "615800"
  },
  {
    "text": "U secure enclaves uh we are very active in the",
    "start": "615800",
    "end": "622839"
  },
  {
    "text": "Upstream Community we are working with many companies attending at architecture committee meetings of Kata and the",
    "start": "622839",
    "end": "628959"
  },
  {
    "text": "confid IAL containers um we are providing the reference architecture for",
    "start": "628959",
    "end": "634760"
  },
  {
    "text": "uh virtualized environments using accelerators um we are trying to reuse",
    "start": "634760",
    "end": "640200"
  },
  {
    "text": "um all the parts of the cloud native stack that we have uh you may heard about the talks about CDI Dr uh NFD the",
    "start": "640200",
    "end": "648120"
  },
  {
    "text": "GPU operator all those parts we are using in Kata and trying to integrate as",
    "start": "648120",
    "end": "653360"
  },
  {
    "text": "much as possible because uh eat your own dog food right we we already have established a good enablement picture",
    "start": "653360",
    "end": "660240"
  },
  {
    "text": "with the GPU on a bare metal and we just following uh this path to enable it in Kata as well um so we are extending the",
    "start": "660240",
    "end": "666880"
  },
  {
    "text": "cloud needed St for any sandbox environment be it cubert Kata firecracker or any other sandbox",
    "start": "666880",
    "end": "674399"
  },
  {
    "text": "environment so what we've done in Kata we enabled uh GPU Nick pass through or",
    "start": "674399",
    "end": "679480"
  },
  {
    "text": "in general vfio pass through um and also VFS um we are extending Kata PCI",
    "start": "679480",
    "end": "685160"
  },
  {
    "text": "implementation to support host topology replication or side channels to provide meta information I will talk about this a little later on",
    "start": "685160",
    "end": "691880"
  },
  {
    "text": "the virtualization reference architecture um the use case is really GPU direct RDMA and GDs in virtualized",
    "start": "691880",
    "end": "697320"
  },
  {
    "text": "environments uh enhance NFD which is no feature Discovery to expose features to the cluster so that you can schedule um",
    "start": "697320",
    "end": "704959"
  },
  {
    "text": "those confidential or Kata containers on the on the right note uh we are also creating various run times to support",
    "start": "704959",
    "end": "710600"
  },
  {
    "text": "all of the use cases the nice thing is uh for each pot you can Define how the PCI topology is going to look like so",
    "start": "710600",
    "end": "716839"
  },
  {
    "text": "you can run one pot with GPU direct RDMA you can run the other part with GPU direct GDs or you have uh virtual gpus",
    "start": "716839",
    "end": "724720"
  },
  {
    "text": "uh or you have a complete GPU pass through so you can by setting",
    "start": "724720",
    "end": "729760"
  },
  {
    "text": "configurations or runtime classes on your pod yam spec you can decide what PCI implementation or PCI Express",
    "start": "729760",
    "end": "737040"
  },
  {
    "text": "topology you want to run in your pot um so we are also adding some new features like inter VM communication vtpm and as",
    "start": "737040",
    "end": "744800"
  },
  {
    "text": "I said the end goal is really to run GPU direct R GDs in in Kata containers",
    "start": "744800",
    "end": "751160"
  },
  {
    "start": "750000",
    "end": "938000"
  },
  {
    "text": "so I just want to show you a brief overview of the GPU container ement stack on bare metal and uh what we've",
    "start": "752079",
    "end": "759720"
  },
  {
    "text": "done uh and how we integrated in Kata because this is important for the lift and shift characteristics uh that we",
    "start": "759720",
    "end": "766639"
  },
  {
    "text": "when we said all premises we don't want to do any code modification we just want the the container running uh inside of",
    "start": "766639",
    "end": "772839"
  },
  {
    "text": "Kata the same as we running on bare metal um SE is pretty easy so there are",
    "start": "772839",
    "end": "778600"
  },
  {
    "text": "features like CDI that we are using to modify the GPO container to bind uh the needed files into the Container",
    "start": "778600",
    "end": "786079"
  },
  {
    "text": "um because we need to make sure that user space and kernel space are in syncs um this enablement stack works with all",
    "start": "786079",
    "end": "792880"
  },
  {
    "text": "major runtime speed Docker container D cryo um there are other features like cgroup V2 that we need to add a BPF",
    "start": "792880",
    "end": "799120"
  },
  {
    "text": "program to enable devices so all those dirty details should be really hidden uh from the user and it should be a",
    "start": "799120",
    "end": "805760"
  },
  {
    "text": "seamless integration uh there was just a talk about about um how to manage device",
    "start": "805760",
    "end": "811160"
  },
  {
    "text": "drivers by one of our colleagues in the cloud native team and the other talk is tomorrow about how the GPU operator",
    "start": "811160",
    "end": "816959"
  },
  {
    "text": "works and how to um life cycle uh gpus in a cluster so we have the bare metal",
    "start": "816959",
    "end": "823680"
  },
  {
    "text": "bare metal enablement uh as I said before GPU operator is for the kubernetes enablement there's also on Friday",
    "start": "823680",
    "end": "830800"
  },
  {
    "text": "another talk about how you can use operator patents to manage uh hardare life cycles in a cluster so I I'm not",
    "start": "830800",
    "end": "837240"
  },
  {
    "text": "going to too much into detail but uh the point is we are using all of our proven",
    "start": "837240",
    "end": "843480"
  },
  {
    "text": "and working stack that's running um for production in many years and we didn't",
    "start": "843480",
    "end": "849079"
  },
  {
    "text": "want to reinvent the wheel and we want to reuse what we have and the goal is really to run GPU containers unmodified",
    "start": "849079",
    "end": "855839"
  },
  {
    "text": "uh user should have the very same experience no matter what the underlying uh enablement mechanism is so we are to",
    "start": "855839",
    "end": "863279"
  },
  {
    "text": "using it this bare metal enablement stack and just putting it in the VM",
    "start": "863279",
    "end": "868639"
  },
  {
    "text": "since T Kata agent running inside is an oci compliant it will support all the things that you are used that you're",
    "start": "868639",
    "end": "874920"
  },
  {
    "text": "used to on a bare metal enablement meaning you take your Cuda container run it on bare metal you can run it one to",
    "start": "874920",
    "end": "881240"
  },
  {
    "text": "one on your Kata container or in your confidential container um one thing we need to do is",
    "start": "881240",
    "end": "888160"
  },
  {
    "text": "of course we need to provide all the guest operating artifacts like the kernel uh fimber and uh guest FS images",
    "start": "888160",
    "end": "896240"
  },
  {
    "text": "and configurations but the main point is really uh no code modification just run",
    "start": "896240",
    "end": "901600"
  },
  {
    "text": "it as you would on your bare metal system and for the Kata use case we enabled a GPU PF pass through VF pass",
    "start": "901600",
    "end": "908800"
  },
  {
    "text": "through meaning all the virtualized stuff like vgpu time slid vgpu MCB uh are working and our current use case to",
    "start": "908800",
    "end": "915480"
  },
  {
    "text": "enable is GPU direct RDMA inside of a virtualized environment how do you choose between",
    "start": "915480",
    "end": "922360"
  },
  {
    "text": "those configurations it's easy than setting a runtime class on your PO yaml uh you set your runtime class vgpu",
    "start": "922360",
    "end": "929639"
  },
  {
    "text": "runtime class GPU CA uh it's just a matter of changing the runtime class to enable any of those use",
    "start": "929639",
    "end": "937839"
  },
  {
    "start": "938000",
    "end": "1263000"
  },
  {
    "text": "cases let me just go a little bit about the virtualization reference architecture uh what I said before that",
    "start": "938800",
    "end": "944519"
  },
  {
    "text": "we have a PCI topology per part um this is a brief overview of the use cases that we want to enable there are a lot",
    "start": "944519",
    "end": "950399"
  },
  {
    "text": "of combinations of PFS VFS and mix SES uh we are adding the Nick into the mix",
    "start": "950399",
    "end": "956040"
  },
  {
    "text": "uh and the main point is the driver St will disable peer to-peer communication",
    "start": "956040",
    "end": "961360"
  },
  {
    "text": "if the P PCI Express topology is not suitable uh there are various factors like I mmu ACS ATS PCI root Port switch",
    "start": "961360",
    "end": "968920"
  },
  {
    "text": "Sports all the factors that can influence peer-to-peer uh capability we have Hardware constraints we are Numa VM",
    "start": "968920",
    "end": "975959"
  },
  {
    "text": "CPU sockets essentially are two Motors aandi that can be reused for any virtualized environment",
    "start": "975959",
    "end": "984560"
  },
  {
    "text": "um So based on this host PCI topology uh we are not those are running on the both",
    "start": "984839",
    "end": "994160"
  },
  {
    "text": "uh on the same Numa so we excluding Numa for here now so it's on on one Numa note we have a PCI switch with a one nox and",
    "start": "994160",
    "end": "1001000"
  },
  {
    "text": "a GPU um and most of the csps that are you",
    "start": "1001000",
    "end": "1007399"
  },
  {
    "text": "providing a VM you will get a flat H key so you don't know which GPU can talk to which um melanox snic you lost your PCI",
    "start": "1007399",
    "end": "1015360"
  },
  {
    "text": "topology uh information um usually",
    "start": "1015360",
    "end": "1020399"
  },
  {
    "text": "the VM will get a side Channel such as a file like topologist XML uh to set for",
    "start": "1020399",
    "end": "1028280"
  },
  {
    "text": "example nickel peer-to-peer levels uh if you're running on Infinity band or higher level libraries like ucx can read",
    "start": "1028280",
    "end": "1034600"
  },
  {
    "text": "this but there's a problem on on lower level libraries like I on the infin band RDMA uh libraries UB verbs or or GDs",
    "start": "1034600",
    "end": "1042480"
  },
  {
    "text": "they don't know nothing about that",
    "start": "1042480",
    "end": "1046480"
  },
  {
    "text": "um yeah and recently we also added cold plug support uh into Kata usually all",
    "start": "1047840",
    "end": "1053799"
  },
  {
    "text": "the stuff is hot plugged uh but how do",
    "start": "1053799",
    "end": "1059320"
  },
  {
    "text": "we let me see yeah how do we provide additional uh information transparently in a cloud native way uh which is tied",
    "start": "1060559",
    "end": "1068039"
  },
  {
    "text": "not tied to the podt but rather to the hardware used um the GPU driver stch can read a",
    "start": "1068039",
    "end": "1075880"
  },
  {
    "text": "specific PCI Express virual P2P approval capability uh which needs to be set by by the user",
    "start": "1075880",
    "end": "1083080"
  },
  {
    "text": "uh to tell which melanox snik and which Nvidia GPU are creating a group that are",
    "start": "1083080",
    "end": "1088760"
  },
  {
    "text": "capable of doing P2P or which two gpus can do p P2P based on my host topology",
    "start": "1088760",
    "end": "1095280"
  },
  {
    "text": "and uh you may heard CDI which is the container device interface that we using",
    "start": "1095280",
    "end": "1100799"
  },
  {
    "text": "is a DSL to provide additional meta information to any container run time",
    "start": "1100799",
    "end": "1106240"
  },
  {
    "text": "and that's what we leveraged here so we can say okay this PCI Express um device belongs to",
    "start": "1106240",
    "end": "1113640"
  },
  {
    "text": "click ID zero and this Nick with this PCI address belongs also to click ID zero and this is going to be pick up by",
    "start": "1113640",
    "end": "1120840"
  },
  {
    "text": "the C runtime and will properly configure the qm or any other hypervisor",
    "start": "1120840",
    "end": "1125919"
  },
  {
    "text": "lying around which has a PC Express topology uh implementation to enable uh",
    "start": "1125919",
    "end": "1131200"
  },
  {
    "text": "peer to-peer between uh two devices that are capable",
    "start": "1131200",
    "end": "1136639"
  },
  {
    "text": "of the other mode that we can use is host topology replication so we are not",
    "start": "1136960",
    "end": "1142520"
  },
  {
    "text": "replicating the complete host we are only using the main parts that we need like the two PCI switches where we know",
    "start": "1142520",
    "end": "1148880"
  },
  {
    "text": "okay there's a melonic and and a Nvidia GPU uh we can easily replicate those and",
    "start": "1148880",
    "end": "1154200"
  },
  {
    "text": "create in VM the very same architecture where it's easier for the driver St to deduce which which devices can talk",
    "start": "1154200",
    "end": "1161640"
  },
  {
    "text": "easily together so this is on the host and this is then in the virtual environment the GP F driver and the Nick",
    "start": "1161640",
    "end": "1169720"
  },
  {
    "text": "drivers can easily deduce the topology and enable peer to-peer far more easily",
    "start": "1169720",
    "end": "1176039"
  },
  {
    "text": "uh than having this side channel to add more meta information as explained",
    "start": "1176039",
    "end": "1184080"
  },
  {
    "text": "earlier so of course we have also some um hypervisor limitations I am not going",
    "start": "1184080",
    "end": "1190400"
  },
  {
    "text": "to much of the detail but this is mainly based on on Kimu uh you cannot attach uh",
    "start": "1190400",
    "end": "1196159"
  },
  {
    "text": "indefinite number of gpus and nicks to the um to the to the VM you need to make",
    "start": "1196159",
    "end": "1202720"
  },
  {
    "text": "sure that you are uh attaching only what you need um another feature that we",
    "start": "1202720",
    "end": "1208440"
  },
  {
    "text": "added if you do not care to which PCI if it's if you don't care where your device",
    "start": "1208440",
    "end": "1215799"
  },
  {
    "text": "is going to be attached like does it need to be a PCI root pod if not you can still attach it to the PCI Express PCI",
    "start": "1215799",
    "end": "1221840"
  },
  {
    "text": "Bridge meaning you can say okay my gpus are important I need to have them on a highspeed PCI Express link uh but for",
    "start": "1221840",
    "end": "1229360"
  },
  {
    "text": "the melanox snakes you can just attach them to the uh to the PCI PCI bridge if",
    "start": "1229360",
    "end": "1235000"
  },
  {
    "text": "if it's needed and the constraints on the horse are telling so um again with CDI oops with CDI we can",
    "start": "1235000",
    "end": "1243000"
  },
  {
    "text": "um tell Kata or the hypervisor how and where uh to attach those devices so that",
    "start": "1243000",
    "end": "1251799"
  },
  {
    "text": "we can enable easily P2P or GPU direct RDMA or GDs so so we have the we have the conf",
    "start": "1251799",
    "end": "1258559"
  },
  {
    "text": "confidential gpus we we have the runtime we we have the virtualization uh one one",
    "start": "1258559",
    "end": "1264400"
  },
  {
    "start": "1263000",
    "end": "1342000"
  },
  {
    "text": "piece missing is the confidential GPU [Music]",
    "start": "1264400",
    "end": "1270159"
  },
  {
    "text": "um the confidential GPU didn't happen like from one day to the other it's a",
    "start": "1270159",
    "end": "1275880"
  },
  {
    "text": "longer St where we are adding more and more features um to the GPU um started",
    "start": "1275880",
    "end": "1281640"
  },
  {
    "text": "with f artification encrypted firmware uh measured boot secure boot uh On th",
    "start": "1281640",
    "end": "1286760"
  },
  {
    "text": "route of trust and and um couple of days ago U we also announced the Blackwell",
    "start": "1286760",
    "end": "1294000"
  },
  {
    "text": "architecture which is the first accelerator which supports TP IDE TP ID is a new standard in the PCI Gen 6",
    "start": "1294000",
    "end": "1301679"
  },
  {
    "text": "standard um all the encryption is done on the PCI Express Bus the attestation",
    "start": "1301679",
    "end": "1307799"
  },
  {
    "text": "is done on the PCI Express bus so you get full performance with the Blackwell",
    "start": "1307799",
    "end": "1313120"
  },
  {
    "text": "architecture on any on any workloads the h100 was using bounce",
    "start": "1313120",
    "end": "1318600"
  },
  {
    "text": "buffers to exchange information between the CPU and the GPU so if your workload",
    "start": "1318600",
    "end": "1324200"
  },
  {
    "text": "has a lot of CPU to GPU communication you may get get some performance segregation uh because we are limited by",
    "start": "1324200",
    "end": "1330520"
  },
  {
    "text": "the capability of the CPU to encrypt data to the GPU the GPU can can encrypt",
    "start": "1330520",
    "end": "1335679"
  },
  {
    "text": "at Full Line rate so it's only the CPU to GPU communication but with black Bel this is all",
    "start": "1335679",
    "end": "1342240"
  },
  {
    "start": "1342000",
    "end": "1590000"
  },
  {
    "text": "gone all those features now we have we have the PCI topology in the VM we have",
    "start": "1345320",
    "end": "1352200"
  },
  {
    "text": "the confidential GPU uh we have cart at the runtime which all leads us to now to confidential containers and again the",
    "start": "1352200",
    "end": "1358880"
  },
  {
    "text": "premise that we also made for confidential containers we don't want to do any mo code modification here as well",
    "start": "1358880",
    "end": "1364000"
  },
  {
    "text": "um and we're using the very same enablement stack here as well uh nothing changes we are still reusing our",
    "start": "1364000",
    "end": "1370520"
  },
  {
    "text": "container enablement stack inside of it um again it's hypervisor independent and we are supplying also the confidential",
    "start": "1370520",
    "end": "1376840"
  },
  {
    "text": "parts for for the GPU artifacts here as",
    "start": "1376840",
    "end": "1381360"
  },
  {
    "text": "well um one important part of if you're running a confidential environment is",
    "start": "1381960",
    "end": "1388440"
  },
  {
    "text": "that you want to make sure that your components are trustworthy meaning you are expecting that your kernel that's",
    "start": "1388440",
    "end": "1396240"
  },
  {
    "text": "running inside of the guest uh the firmware uh the guest image uh the",
    "start": "1396240",
    "end": "1402520"
  },
  {
    "text": "memory are all in a specific state that you're expecting so essentially what",
    "start": "1402520",
    "end": "1407640"
  },
  {
    "text": "you're doing during the station you're measuring your artifacts and comparing to some um reference value that you're",
    "start": "1407640",
    "end": "1415760"
  },
  {
    "text": "expecting and you as a workload owner can then decide what you're going to do with this attestation report meaning um",
    "start": "1415760",
    "end": "1423200"
  },
  {
    "text": "one thing after you deployed um your components you may want to release",
    "start": "1423200",
    "end": "1428919"
  },
  {
    "text": "secrets into your VM uh and you only want to do that if you your attestation",
    "start": "1428919",
    "end": "1434600"
  },
  {
    "text": "succeeds um Nvidia and all the m major",
    "start": "1434600",
    "end": "1439720"
  },
  {
    "text": "um provider of confidential all following the R's architecture so it's an ietf standard uh there are more",
    "start": "1439720",
    "end": "1446159"
  },
  {
    "text": "things like how do you provide reference values and uh and other um standards",
    "start": "1446159",
    "end": "1454120"
  },
  {
    "text": "that can be read by the R's working group but essentially uh the the workflow is very same you set up a",
    "start": "1454120",
    "end": "1460840"
  },
  {
    "text": "confidential environment you get measurements of all your components you send it out to some remote entity which",
    "start": "1460840",
    "end": "1466159"
  },
  {
    "text": "compares the measured value against the reference value and you as work own can decide okay I want to release my secrets",
    "start": "1466159",
    "end": "1473520"
  },
  {
    "text": "into the VM and with the secret in the VM you can for example decrypt your",
    "start": "1473520",
    "end": "1479000"
  },
  {
    "text": "storage decrypt your encrypted container or any other things that you you might",
    "start": "1479000",
    "end": "1484039"
  },
  {
    "text": "want to do in in the VM um yeah this is just an overview of um how secq ke",
    "start": "1484039",
    "end": "1489960"
  },
  {
    "text": "release uh is working in confidential containers but um I just explained the",
    "start": "1489960",
    "end": "1496960"
  },
  {
    "text": "the simple version of it do the attestation if it all it's okay release your keys into the uh confidential VM",
    "start": "1496960",
    "end": "1505000"
  },
  {
    "text": "and then decrypt whatever you",
    "start": "1505000",
    "end": "1508520"
  },
  {
    "text": "need deployment with um of confidential containers again with the GPU operator",
    "start": "1510360",
    "end": "1517159"
  },
  {
    "text": "um on the left hand side is the stack for the traditional container that you all know um we can configure the GPU",
    "start": "1517159",
    "end": "1525360"
  },
  {
    "text": "operator to deploy a confidential container with GPU pass through or you can",
    "start": "1525360",
    "end": "1530480"
  },
  {
    "text": "configure a cata container uh with let's say with GPU virtualized GPU uh on the same",
    "start": "1530480",
    "end": "1538480"
  },
  {
    "text": "cluster so you can tell on which note you want to run let's say Kata on which note you want to run confidential",
    "start": "1538480",
    "end": "1544679"
  },
  {
    "text": "container or on which note you want to run a traditional container this is all on top of the confidential containers",
    "start": "1544679",
    "end": "1550399"
  },
  {
    "text": "operator which which provides as um the CPU artifacts and the GPU operator will",
    "start": "1550399",
    "end": "1556679"
  },
  {
    "text": "provide you g PR effects and I said before it's just a",
    "start": "1556679",
    "end": "1563399"
  },
  {
    "text": "matter of changing your rine class in a podspec uh what you want to run in the middle it's kataku GPU S&P so it's an",
    "start": "1563399",
    "end": "1571279"
  },
  {
    "text": "AMD system or you set TDX and you run on a TDX system or you set CCA and you're running on Arm System um it's it's the",
    "start": "1571279",
    "end": "1578919"
  },
  {
    "text": "characteristic what we want to achieved one of what we want to achieve the lift and shift characteristics use your",
    "start": "1578919",
    "end": "1585279"
  },
  {
    "text": "workload change the random class and decide how you want to run",
    "start": "1585279",
    "end": "1590919"
  },
  {
    "start": "1590000",
    "end": "1827000"
  },
  {
    "text": "it okay now now we have confidential environments we have a confidential GPU",
    "start": "1591240",
    "end": "1596720"
  },
  {
    "text": "um what can we do with it right um Rec I took Rec Gams as an example um",
    "start": "1596720",
    "end": "1605080"
  },
  {
    "text": "and I'm listing here the potential threats this is coming from ovest the",
    "start": "1605080",
    "end": "1610360"
  },
  {
    "text": "top 10 threats for llms um and the potential",
    "start": "1610360",
    "end": "1615679"
  },
  {
    "text": "mitigation strategies that can be uh mitigated",
    "start": "1615679",
    "end": "1621440"
  },
  {
    "text": "limited or eliminated with confidential Computing um we cannot eliminate all",
    "start": "1621440",
    "end": "1629240"
  },
  {
    "text": "threats with confidential Computing um for example if we have model over Reliance that's nothing we can do with",
    "start": "1629240",
    "end": "1635720"
  },
  {
    "text": "confidential Computing if your model is hallucinating we cannot do nothing with confidential Computing that's your part",
    "start": "1635720",
    "end": "1641240"
  },
  {
    "text": "to do um but where resources are",
    "start": "1641240",
    "end": "1646559"
  },
  {
    "text": "exhausted where breakouts are possible like um running insecure plugins uh that many",
    "start": "1646559",
    "end": "1654919"
  },
  {
    "text": "llms have um or denial of service things",
    "start": "1654919",
    "end": "1659960"
  },
  {
    "text": "um this is where virtualization or confidential compute can help you for prompt injection um this is more a topic",
    "start": "1659960",
    "end": "1667240"
  },
  {
    "text": "on how your API can you know validate sanitize um or check what the user is",
    "start": "1667240",
    "end": "1674919"
  },
  {
    "text": "doing um with confidential computing we can limit the text surface or we can",
    "start": "1674919",
    "end": "1681559"
  },
  {
    "text": "enable secure execution of plugins so if we are looking at a very simple reg LM Pipeline",
    "start": "1681559",
    "end": "1690039"
  },
  {
    "text": "with a frontend API server a model server Vector DB the question would come",
    "start": "1690039",
    "end": "1695720"
  },
  {
    "text": "up which of those parts do you want to run uh inside a confidential container",
    "start": "1695720",
    "end": "1702679"
  },
  {
    "text": "um if you remember what we talked at the beginning that I said that each",
    "start": "1702679",
    "end": "1707720"
  },
  {
    "text": "container a breakout can take over a complete note I would say just run every of your containers in a confidential",
    "start": "1707720",
    "end": "1713880"
  },
  {
    "text": "environment because if for example aeka in the web front end breaks out he has no access to anything on the on the",
    "start": "1713880",
    "end": "1721640"
  },
  {
    "text": "containers that are running the API server for men in the middle attex or on the vector DB uh Vector",
    "start": "1721640",
    "end": "1728760"
  },
  {
    "text": "DB it's an abstract representation of your trained data but still there are",
    "start": "1728760",
    "end": "1735320"
  },
  {
    "text": "some some ways to extract confidential data um and again",
    "start": "1735320",
    "end": "1742240"
  },
  {
    "text": "on with the secure key releas secure key release and the attestation um you can",
    "start": "1742240",
    "end": "1749279"
  },
  {
    "text": "make sure that you only release your data sources into your vector DB if atation or your uh confidential VM is in",
    "start": "1749279",
    "end": "1756679"
  },
  {
    "text": "a state that you're expecting it the same for for the models that you're running in your model server you want to",
    "start": "1756679",
    "end": "1761760"
  },
  {
    "text": "only deploy your model your confidential model that you are trained only to the model server if the confidential R is in",
    "start": "1761760",
    "end": "1770039"
  },
  {
    "text": "a state that you're expecting",
    "start": "1770039",
    "end": "1773360"
  },
  {
    "text": "it some closing remarks um not even recm is a special example but but",
    "start": "1777760",
    "end": "1784720"
  },
  {
    "text": "looking at uh all AI ml pipelines and personas there's always one Persona who",
    "start": "1784720",
    "end": "1790080"
  },
  {
    "text": "wants to protect data there's always some stage in the a AIML pipeline that you want to protect um which can be",
    "start": "1790080",
    "end": "1799240"
  },
  {
    "text": "partially mitigated eliminated or or that where confidential containers can",
    "start": "1799240",
    "end": "1806279"
  },
  {
    "text": "help uh to mitigate uh attacks but nothing",
    "start": "1806279",
    "end": "1811919"
  },
  {
    "text": "will protect your data if you're running a random shell script from the Internet",
    "start": "1811919",
    "end": "1817120"
  },
  {
    "text": "or a random model inside of your confidential environment and this thing is doing a reverse shell to Suiter and",
    "start": "1817120",
    "end": "1823480"
  },
  {
    "text": "he's leaking your data and this would be end of my session",
    "start": "1823480",
    "end": "1830760"
  },
  {
    "start": "1827000",
    "end": "1918000"
  },
  {
    "text": "any",
    "start": "1830760",
    "end": "1833080"
  },
  {
    "text": "questions I thank you for your uh presentation I have a question would",
    "start": "1847760",
    "end": "1853000"
  },
  {
    "text": "would it doesn't make sense to like uh do that on smaller sized GPU like for",
    "start": "1853000",
    "end": "1860200"
  },
  {
    "text": "example the Nvidia Jetson so like smaller Bo boards where it's not",
    "start": "1860200",
    "end": "1866559"
  },
  {
    "text": "necessarily A GPU but it's maybe a nasc on the Jon board I'm not sure if it's a",
    "start": "1866559",
    "end": "1872679"
  },
  {
    "text": "same sort of GPU than the a100 for example um only the h100 and the",
    "start": "1872679",
    "end": "1879519"
  },
  {
    "text": "Blackwell architecture have the hardware architecture to support confidential",
    "start": "1879519",
    "end": "1884559"
  },
  {
    "text": "Computing if you have virtualization on your platform enabled uh you can run GPU pass through of your",
    "start": "1884559",
    "end": "1890360"
  },
  {
    "text": "Jetson devices and use Kata containers but confidential Computing is only available on Hopper and",
    "start": "1890360",
    "end": "1895720"
  },
  {
    "text": "blackw agreed thank you hi uh thanks a lot for the",
    "start": "1895720",
    "end": "1901080"
  },
  {
    "text": "presentation a more generic question so how about the P peer-to-peer communication for the NV links the GPS",
    "start": "1901080",
    "end": "1907320"
  },
  {
    "text": "with NV links does it going to work or no um I I cannot say anything about that",
    "start": "1907320",
    "end": "1915120"
  },
  {
    "text": "fair enough okay thank you",
    "start": "1915120",
    "end": "1919840"
  }
]