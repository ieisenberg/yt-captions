[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "so",
    "start": "480",
    "end": "1360"
  },
  {
    "text": "hi everybody uh welcome to our talk",
    "start": "1360",
    "end": "2960"
  },
  {
    "text": "about clusterless architectures uh this",
    "start": "2960",
    "end": "4880"
  },
  {
    "text": "is a new design that's coming out of the",
    "start": "4880",
    "end": "6160"
  },
  {
    "text": "virtual cluster space and it allows you",
    "start": "6160",
    "end": "8240"
  },
  {
    "text": "to utilize clusters of clusters uh",
    "start": "8240",
    "end": "11280"
  },
  {
    "text": "seamlessly to run workloads wherever",
    "start": "11280",
    "end": "13519"
  },
  {
    "text": "wherever you need to uh faye and i uh",
    "start": "13519",
    "end": "16000"
  },
  {
    "text": "we'll be talking to you about this and",
    "start": "16000",
    "end": "17600"
  },
  {
    "text": "we're both the maintainers for the",
    "start": "17600",
    "end": "19119"
  },
  {
    "text": "project along with some other folks um",
    "start": "19119",
    "end": "21840"
  },
  {
    "text": "faye works at alibaba and i work at",
    "start": "21840",
    "end": "23439"
  },
  {
    "text": "apple",
    "start": "23439",
    "end": "25199"
  },
  {
    "start": "25000",
    "end": "25000"
  },
  {
    "text": "so",
    "start": "25199",
    "end": "26560"
  },
  {
    "text": "uh",
    "start": "26560",
    "end": "27760"
  },
  {
    "text": "why we're talking about this and why",
    "start": "27760",
    "end": "29199"
  },
  {
    "text": "this is important so one of the things",
    "start": "29199",
    "end": "30880"
  },
  {
    "text": "that as you start to use kubernetes you",
    "start": "30880",
    "end": "33040"
  },
  {
    "text": "start to realize that when you started",
    "start": "33040",
    "end": "35120"
  },
  {
    "text": "you initially started with at least a",
    "start": "35120",
    "end": "36880"
  },
  {
    "text": "single cluster and",
    "start": "36880",
    "end": "39280"
  },
  {
    "text": "you started to build out your",
    "start": "39280",
    "end": "40239"
  },
  {
    "text": "architectures and you and you deployed",
    "start": "40239",
    "end": "41760"
  },
  {
    "text": "workloads into that single environment",
    "start": "41760",
    "end": "43600"
  },
  {
    "text": "you started to see some problems this is",
    "start": "43600",
    "end": "45200"
  },
  {
    "text": "something that",
    "start": "45200",
    "end": "46239"
  },
  {
    "text": "i feel like everybody experiences in",
    "start": "46239",
    "end": "48000"
  },
  {
    "text": "their kubernetes journey",
    "start": "48000",
    "end": "50239"
  },
  {
    "text": "and so",
    "start": "50239",
    "end": "51440"
  },
  {
    "text": "where folks usually go is they turn to a",
    "start": "51440",
    "end": "54000"
  },
  {
    "text": "multi-cluster management style",
    "start": "54000",
    "end": "56480"
  },
  {
    "text": "and the whole idea behind this is it",
    "start": "56480",
    "end": "58239"
  },
  {
    "text": "allows you to actually scale out your",
    "start": "58239",
    "end": "59760"
  },
  {
    "text": "architecture so uh taking from a single",
    "start": "59760",
    "end": "62239"
  },
  {
    "text": "cluster which has scale targets uh that",
    "start": "62239",
    "end": "64239"
  },
  {
    "text": "are defined by sig scalability um and",
    "start": "64239",
    "end": "67439"
  },
  {
    "text": "moving that into how do we make it so",
    "start": "67439",
    "end": "69439"
  },
  {
    "text": "that we could run you know 4x the type",
    "start": "69439",
    "end": "71760"
  },
  {
    "text": "of capacity that we currently have and",
    "start": "71760",
    "end": "73439"
  },
  {
    "text": "and being able to do that and scale your",
    "start": "73439",
    "end": "75280"
  },
  {
    "text": "workloads horizontally gets somewhat",
    "start": "75280",
    "end": "77119"
  },
  {
    "text": "difficult",
    "start": "77119",
    "end": "78320"
  },
  {
    "text": "uh on top of that uh customers really",
    "start": "78320",
    "end": "80560"
  },
  {
    "text": "want to start to look at things like how",
    "start": "80560",
    "end": "82560"
  },
  {
    "text": "to how to deploy and run workloads in",
    "start": "82560",
    "end": "85360"
  },
  {
    "text": "specific regions and areas where their",
    "start": "85360",
    "end": "87759"
  },
  {
    "text": "data might be located um if you're",
    "start": "87759",
    "end": "89439"
  },
  {
    "text": "thinking like bachelor workloads where",
    "start": "89439",
    "end": "90799"
  },
  {
    "text": "you're processing a lot of data you",
    "start": "90799",
    "end": "92159"
  },
  {
    "text": "don't want to run that from completely",
    "start": "92159",
    "end": "94079"
  },
  {
    "text": "opposite ends of uh the country for",
    "start": "94079",
    "end": "96320"
  },
  {
    "text": "example uh in this environment you also",
    "start": "96320",
    "end": "98560"
  },
  {
    "text": "get to a place where you're really",
    "start": "98560",
    "end": "99840"
  },
  {
    "text": "taking you're bringing in a lot more",
    "start": "99840",
    "end": "101759"
  },
  {
    "text": "compute resources and everything gets",
    "start": "101759",
    "end": "103360"
  },
  {
    "text": "more difficult to actually manage and",
    "start": "103360",
    "end": "105360"
  },
  {
    "text": "aggregate across those so what you end",
    "start": "105360",
    "end": "107280"
  },
  {
    "text": "up doing is start to look at that",
    "start": "107280",
    "end": "108640"
  },
  {
    "text": "multi-cluster space and then bring in",
    "start": "108640",
    "end": "110320"
  },
  {
    "text": "things like uh",
    "start": "110320",
    "end": "112000"
  },
  {
    "text": "aha strategies how would you actually",
    "start": "112000",
    "end": "114720"
  },
  {
    "text": "increase the availability of your",
    "start": "114720",
    "end": "116479"
  },
  {
    "text": "workloads how do you set up regions that",
    "start": "116479",
    "end": "118320"
  },
  {
    "text": "you can actually fail over into uh when",
    "start": "118320",
    "end": "120560"
  },
  {
    "text": "something goes wrong uh",
    "start": "120560",
    "end": "122560"
  },
  {
    "text": "i imagine everybody here has experienced",
    "start": "122560",
    "end": "125280"
  },
  {
    "text": "issues with etcd having key space",
    "start": "125280",
    "end": "127840"
  },
  {
    "text": "problems and you get a corrupted key and",
    "start": "127840",
    "end": "129759"
  },
  {
    "text": "you end up with an entire cluster that",
    "start": "129759",
    "end": "131200"
  },
  {
    "text": "just doesn't function anymore and how do",
    "start": "131200",
    "end": "132879"
  },
  {
    "text": "you actually recover for those things on",
    "start": "132879",
    "end": "134879"
  },
  {
    "text": "top of this we get into a new world of",
    "start": "134879",
    "end": "137599"
  },
  {
    "text": "how do you actually build out the",
    "start": "137599",
    "end": "139200"
  },
  {
    "text": "security compliance when you have now",
    "start": "139200",
    "end": "141520"
  },
  {
    "text": "distributed workloads across different",
    "start": "141520",
    "end": "144000"
  },
  {
    "text": "environments",
    "start": "144000",
    "end": "145200"
  },
  {
    "text": "as well as the autonomy of those",
    "start": "145200",
    "end": "146879"
  },
  {
    "text": "workloads going forward",
    "start": "146879",
    "end": "148800"
  },
  {
    "text": "now on top of that there's an",
    "start": "148800",
    "end": "150239"
  },
  {
    "text": "ever-growing space that's really",
    "start": "150239",
    "end": "151440"
  },
  {
    "text": "important here to acknowledge which is",
    "start": "151440",
    "end": "153200"
  },
  {
    "text": "the hybrid and multi-cloud space so",
    "start": "153200",
    "end": "156160"
  },
  {
    "text": "workloads that might have been deployed",
    "start": "156160",
    "end": "157920"
  },
  {
    "text": "into on-premise data centers are now",
    "start": "157920",
    "end": "159920"
  },
  {
    "text": "trying to expand in to get the",
    "start": "159920",
    "end": "161599"
  },
  {
    "text": "flexibility that you get out of the",
    "start": "161599",
    "end": "163599"
  },
  {
    "text": "multi-cluster world",
    "start": "163599",
    "end": "165440"
  },
  {
    "text": "and this really brings in a the new",
    "start": "165440",
    "end": "167519"
  },
  {
    "text": "aspects and what faye and i work on most",
    "start": "167519",
    "end": "169599"
  },
  {
    "text": "closely is the multi-tenancy space",
    "start": "169599",
    "end": "173760"
  },
  {
    "start": "174000",
    "end": "174000"
  },
  {
    "text": "and so what this actually brings on",
    "start": "174400",
    "end": "175760"
  },
  {
    "text": "brings on a lot of challenges",
    "start": "175760",
    "end": "177519"
  },
  {
    "text": "as you can see there's at least five",
    "start": "177519",
    "end": "178959"
  },
  {
    "text": "bullets here that that",
    "start": "178959",
    "end": "180400"
  },
  {
    "text": "underneath them have a lot of components",
    "start": "180400",
    "end": "182720"
  },
  {
    "text": "uh that go into them but there's a lot",
    "start": "182720",
    "end": "184400"
  },
  {
    "text": "of tools already in this space that",
    "start": "184400",
    "end": "185680"
  },
  {
    "text": "folks have been working on so things",
    "start": "185680",
    "end": "187280"
  },
  {
    "text": "like lifecycle management how do you",
    "start": "187280",
    "end": "188720"
  },
  {
    "text": "actually go and create new clusters are",
    "start": "188720",
    "end": "190560"
  },
  {
    "text": "you using public cloud tools like eks",
    "start": "190560",
    "end": "193040"
  },
  {
    "text": "and gke and ak ack",
    "start": "193040",
    "end": "195360"
  },
  {
    "text": "to actually deploy those things are you",
    "start": "195360",
    "end": "196800"
  },
  {
    "text": "using open source tools like uh what",
    "start": "196800",
    "end": "198879"
  },
  {
    "text": "faye and i work on with the cappy space",
    "start": "198879",
    "end": "200720"
  },
  {
    "text": "or are using open container management",
    "start": "200720",
    "end": "203200"
  },
  {
    "text": "are you using any of the off-the-shelf",
    "start": "203200",
    "end": "204560"
  },
  {
    "text": "vendor tools um going in going on top of",
    "start": "204560",
    "end": "207360"
  },
  {
    "text": "that there's then the governance of",
    "start": "207360",
    "end": "209120"
  },
  {
    "text": "those so now that you have distributed",
    "start": "209120",
    "end": "211120"
  },
  {
    "text": "workloads across many clusters how do",
    "start": "211120",
    "end": "212959"
  },
  {
    "text": "you do centralized management how do you",
    "start": "212959",
    "end": "214720"
  },
  {
    "text": "make sure that the security policies are",
    "start": "214720",
    "end": "216640"
  },
  {
    "text": "set up and they abide by what your",
    "start": "216640",
    "end": "219120"
  },
  {
    "text": "security teams within your companies",
    "start": "219120",
    "end": "221120"
  },
  {
    "text": "actually need",
    "start": "221120",
    "end": "222319"
  },
  {
    "text": "and really making sure that they're",
    "start": "222319",
    "end": "224080"
  },
  {
    "text": "they're customizable but they give their",
    "start": "224080",
    "end": "226400"
  },
  {
    "text": "they customizable but they don't expose",
    "start": "226400",
    "end": "228640"
  },
  {
    "text": "you to anything uh too far",
    "start": "228640",
    "end": "231040"
  },
  {
    "text": "on top of that you have to look into a a",
    "start": "231040",
    "end": "233760"
  },
  {
    "text": "vast space here of monitoring and",
    "start": "233760",
    "end": "235760"
  },
  {
    "text": "tracing there are many tools that we",
    "start": "235760",
    "end": "237920"
  },
  {
    "text": "have to work with nowadays dashboards to",
    "start": "237920",
    "end": "240159"
  },
  {
    "text": "visualize across those clusters",
    "start": "240159",
    "end": "242239"
  },
  {
    "text": "where the data is actually going to live",
    "start": "242239",
    "end": "244000"
  },
  {
    "text": "and how that functions",
    "start": "244000",
    "end": "245840"
  },
  {
    "text": "now what we're really going to be",
    "start": "245840",
    "end": "247200"
  },
  {
    "text": "talking about in this in this talk is",
    "start": "247200",
    "end": "249280"
  },
  {
    "text": "the fourth and fifth sections and i and",
    "start": "249280",
    "end": "251840"
  },
  {
    "text": "because there are already tools",
    "start": "251840",
    "end": "253200"
  },
  {
    "text": "expanding across those other those other",
    "start": "253200",
    "end": "256000"
  },
  {
    "text": "challenges we really wanted to look into",
    "start": "256000",
    "end": "259199"
  },
  {
    "text": "how to make this how to abstract this",
    "start": "259199",
    "end": "261519"
  },
  {
    "text": "and how to make it so that you can have",
    "start": "261519",
    "end": "263120"
  },
  {
    "text": "better workload management but still",
    "start": "263120",
    "end": "265759"
  },
  {
    "text": "in essence give you the same sort of",
    "start": "265759",
    "end": "267360"
  },
  {
    "text": "experience we have now the tools that",
    "start": "267360",
    "end": "269120"
  },
  {
    "text": "are currently existing and that folks",
    "start": "269120",
    "end": "271040"
  },
  {
    "text": "have been using things like cube fed and",
    "start": "271040",
    "end": "272880"
  },
  {
    "text": "argo cd",
    "start": "272880",
    "end": "274479"
  },
  {
    "text": "they bring on that same level of",
    "start": "274479",
    "end": "275919"
  },
  {
    "text": "abstraction",
    "start": "275919",
    "end": "277120"
  },
  {
    "text": "but they kind of given they give you",
    "start": "277120",
    "end": "278639"
  },
  {
    "text": "some trade-offs in terms of what you're",
    "start": "278639",
    "end": "280080"
  },
  {
    "text": "actually going to be doing at a granular",
    "start": "280080",
    "end": "282000"
  },
  {
    "text": "level",
    "start": "282000",
    "end": "283199"
  },
  {
    "text": "and then the last thing is scheduling",
    "start": "283199",
    "end": "285840"
  },
  {
    "text": "now if you have multiple clusters you",
    "start": "285840",
    "end": "288000"
  },
  {
    "text": "need to figure out how you're going to",
    "start": "288000",
    "end": "289360"
  },
  {
    "text": "actually schedule those workloads across",
    "start": "289360",
    "end": "291280"
  },
  {
    "text": "them is that something manual where",
    "start": "291280",
    "end": "293360"
  },
  {
    "text": "everybody knows about every single",
    "start": "293360",
    "end": "294720"
  },
  {
    "text": "cluster that is potentially",
    "start": "294720",
    "end": "296560"
  },
  {
    "text": "accessible to them and they go and",
    "start": "296560",
    "end": "298320"
  },
  {
    "text": "manually decide i want to deploy to this",
    "start": "298320",
    "end": "300639"
  },
  {
    "text": "cluster in this region because of this",
    "start": "300639",
    "end": "302240"
  },
  {
    "text": "specific thing uh or",
    "start": "302240",
    "end": "304960"
  },
  {
    "text": "is there an automated system that kind",
    "start": "304960",
    "end": "306400"
  },
  {
    "text": "of does those does that work for you",
    "start": "306400",
    "end": "308880"
  },
  {
    "text": "thinking about things like",
    "start": "308880",
    "end": "310720"
  },
  {
    "text": "the get ops world and trying to deploy",
    "start": "310720",
    "end": "313039"
  },
  {
    "text": "to some cluster based on some decision",
    "start": "313039",
    "end": "315440"
  },
  {
    "text": "in a in a declarative script",
    "start": "315440",
    "end": "318800"
  },
  {
    "start": "319000",
    "end": "319000"
  },
  {
    "text": "so enter in clusterless",
    "start": "319360",
    "end": "321680"
  },
  {
    "text": "now in a multi-cluster environment uh",
    "start": "321680",
    "end": "324080"
  },
  {
    "text": "the idea here with clusterless is that",
    "start": "324080",
    "end": "325680"
  },
  {
    "text": "we want to achieve really the single",
    "start": "325680",
    "end": "328240"
  },
  {
    "text": "cluster user experience so everything",
    "start": "328240",
    "end": "330479"
  },
  {
    "text": "that everybody knows and loves about",
    "start": "330479",
    "end": "332400"
  },
  {
    "text": "kubernetes today being able to take that",
    "start": "332400",
    "end": "334479"
  },
  {
    "text": "and say what does this look like across",
    "start": "334479",
    "end": "336479"
  },
  {
    "text": "many clusters",
    "start": "336479",
    "end": "337840"
  },
  {
    "text": "without really introducing new apis and",
    "start": "337840",
    "end": "340080"
  },
  {
    "text": "making it so that every single",
    "start": "340080",
    "end": "341680"
  },
  {
    "text": "off-the-shelf tool that you currently",
    "start": "341680",
    "end": "343280"
  },
  {
    "text": "use could potentially be deployed into",
    "start": "343280",
    "end": "345120"
  },
  {
    "text": "that same exact experience you deploy",
    "start": "345120",
    "end": "347120"
  },
  {
    "text": "some operator to do maybe prometheus how",
    "start": "347120",
    "end": "349280"
  },
  {
    "text": "do you make it so that user using the",
    "start": "349280",
    "end": "351360"
  },
  {
    "text": "single cluster experience can deploy",
    "start": "351360",
    "end": "353280"
  },
  {
    "text": "that operator and have it naturally",
    "start": "353280",
    "end": "355199"
  },
  {
    "text": "translate into a into a multi-cluster",
    "start": "355199",
    "end": "357440"
  },
  {
    "text": "environment",
    "start": "357440",
    "end": "358560"
  },
  {
    "text": "now there's some caveats to what we've",
    "start": "358560",
    "end": "360720"
  },
  {
    "text": "designed so far and where it is in its",
    "start": "360720",
    "end": "362560"
  },
  {
    "text": "current landscape escape",
    "start": "362560",
    "end": "364960"
  },
  {
    "text": "it doesn't really work as well for for",
    "start": "364960",
    "end": "367440"
  },
  {
    "text": "redundancy for availability of workloads",
    "start": "367440",
    "end": "369840"
  },
  {
    "text": "um we it it also doesn't uh suffice",
    "start": "369840",
    "end": "373199"
  },
  {
    "text": "exactly today for uh heterogeneous",
    "start": "373199",
    "end": "375759"
  },
  {
    "text": "hardware where you have different slas",
    "start": "375759",
    "end": "378160"
  },
  {
    "text": "and um",
    "start": "378160",
    "end": "380160"
  },
  {
    "text": "different types of models for the actual",
    "start": "380160",
    "end": "381680"
  },
  {
    "text": "hardware that you're running on but it's",
    "start": "381680",
    "end": "383600"
  },
  {
    "text": "really great for when you just have a",
    "start": "383600",
    "end": "386000"
  },
  {
    "text": "lot of the exact same worker cluster and",
    "start": "386000",
    "end": "388479"
  },
  {
    "text": "you're just trying to run more of what",
    "start": "388479",
    "end": "390400"
  },
  {
    "text": "you already have in that single cluster",
    "start": "390400",
    "end": "392800"
  },
  {
    "text": "and realistically it's trying to trying",
    "start": "392800",
    "end": "395360"
  },
  {
    "text": "to in essence minimize the integration",
    "start": "395360",
    "end": "397280"
  },
  {
    "text": "cost by truly treating this as just",
    "start": "397280",
    "end": "399840"
  },
  {
    "text": "another off-the-shelf kubernetes",
    "start": "399840",
    "end": "401440"
  },
  {
    "text": "deployment",
    "start": "401440",
    "end": "403919"
  },
  {
    "text": "and so this is where we are we're going",
    "start": "404560",
    "end": "406240"
  },
  {
    "text": "to be talking about the design of how",
    "start": "406240",
    "end": "407360"
  },
  {
    "text": "this all how cluster list functions for",
    "start": "407360",
    "end": "409360"
  },
  {
    "text": "us and then we'll go into the",
    "start": "409360",
    "end": "410319"
  },
  {
    "text": "architecture uh the implementation and",
    "start": "410319",
    "end": "412319"
  },
  {
    "text": "some of and at the end we'll summarize",
    "start": "412319",
    "end": "414160"
  },
  {
    "text": "kind of what we've all talked about",
    "start": "414160",
    "end": "416720"
  },
  {
    "start": "416000",
    "end": "416000"
  },
  {
    "text": "so cluster abstraction what's really",
    "start": "416720",
    "end": "418880"
  },
  {
    "text": "important here and what where we started",
    "start": "418880",
    "end": "420639"
  },
  {
    "text": "with the project is basically looking at",
    "start": "420639",
    "end": "423520"
  },
  {
    "text": "how to take a single cluster or multiple",
    "start": "423520",
    "end": "426000"
  },
  {
    "text": "clusters and abstract them into that",
    "start": "426000",
    "end": "427840"
  },
  {
    "text": "into that single cluster space and what",
    "start": "427840",
    "end": "430479"
  },
  {
    "text": "we wanted to do is really introduce no",
    "start": "430479",
    "end": "432479"
  },
  {
    "text": "new",
    "start": "432479",
    "end": "433360"
  },
  {
    "text": "workload clusters uh new workload",
    "start": "433360",
    "end": "435759"
  },
  {
    "text": "controllers",
    "start": "435759",
    "end": "436960"
  },
  {
    "text": "into those worker clusters so taking a",
    "start": "436960",
    "end": "439599"
  },
  {
    "text": "look at the lowest level abstraction",
    "start": "439599",
    "end": "441120"
  },
  {
    "text": "that's in kubernetes things like pods",
    "start": "441120",
    "end": "443280"
  },
  {
    "text": "and how would you actually spread that",
    "start": "443280",
    "end": "444880"
  },
  {
    "text": "across",
    "start": "444880",
    "end": "445919"
  },
  {
    "text": "across those clusters and this is",
    "start": "445919",
    "end": "447599"
  },
  {
    "text": "something that we already are seeing um",
    "start": "447599",
    "end": "449919"
  },
  {
    "text": "other open source tools doing so tensile",
    "start": "449919",
    "end": "452240"
  },
  {
    "text": "cube and",
    "start": "452240",
    "end": "453680"
  },
  {
    "text": "likio are basically doing this already",
    "start": "453680",
    "end": "456240"
  },
  {
    "text": "but they do this using tools like",
    "start": "456240",
    "end": "457840"
  },
  {
    "text": "virtual cubelet",
    "start": "457840",
    "end": "459199"
  },
  {
    "text": "as their abstraction so they can they",
    "start": "459199",
    "end": "460800"
  },
  {
    "text": "can run a virtual cubelet which then",
    "start": "460800",
    "end": "462560"
  },
  {
    "text": "addresses one or many clusters under the",
    "start": "462560",
    "end": "465120"
  },
  {
    "text": "hood uh for actually deploying those",
    "start": "465120",
    "end": "467120"
  },
  {
    "text": "workloads now in our world uh we started",
    "start": "467120",
    "end": "469919"
  },
  {
    "text": "with with the tool chain that we've been",
    "start": "469919",
    "end": "471280"
  },
  {
    "text": "working on most heavily which is virtual",
    "start": "471280",
    "end": "473280"
  },
  {
    "text": "cluster and trying to bring that same",
    "start": "473280",
    "end": "475199"
  },
  {
    "text": "level of abstraction um to that",
    "start": "475199",
    "end": "477680"
  },
  {
    "text": "environment so it's something that you",
    "start": "477680",
    "end": "479680"
  },
  {
    "text": "already are seeing uh and we're just",
    "start": "479680",
    "end": "481599"
  },
  {
    "text": "extending this into tools that we have",
    "start": "481599",
    "end": "483520"
  },
  {
    "text": "today",
    "start": "483520",
    "end": "485840"
  },
  {
    "start": "487000",
    "end": "487000"
  },
  {
    "text": "now to take it aside for a second uh",
    "start": "487919",
    "end": "489759"
  },
  {
    "text": "there's an important there's an",
    "start": "489759",
    "end": "490879"
  },
  {
    "text": "important conversation that we need to",
    "start": "490879",
    "end": "492160"
  },
  {
    "text": "have here about uh the abstraction level",
    "start": "492160",
    "end": "494720"
  },
  {
    "text": "that we're talking about and that's",
    "start": "494720",
    "end": "495919"
  },
  {
    "text": "specific to pods versus workloads so as",
    "start": "495919",
    "end": "498479"
  },
  {
    "text": "you look at the the entire space of this",
    "start": "498479",
    "end": "500400"
  },
  {
    "text": "not just the uh tensile cube and those",
    "start": "500400",
    "end": "502800"
  },
  {
    "text": "kind of workloads uh there's other",
    "start": "502800",
    "end": "505199"
  },
  {
    "text": "things in here there's ocm or yeah ocm",
    "start": "505199",
    "end": "507520"
  },
  {
    "text": "there's argo uh there's carmada a lot of",
    "start": "507520",
    "end": "510000"
  },
  {
    "text": "these tools are doing it at a different",
    "start": "510000",
    "end": "511440"
  },
  {
    "text": "abstraction so they're requiring you to",
    "start": "511440",
    "end": "513200"
  },
  {
    "text": "have a specific",
    "start": "513200",
    "end": "515120"
  },
  {
    "text": "crs or custom resources that you",
    "start": "515120",
    "end": "516880"
  },
  {
    "text": "actually deploy with that define the",
    "start": "516880",
    "end": "519120"
  },
  {
    "text": "entire entire resource that you're",
    "start": "519120",
    "end": "521360"
  },
  {
    "text": "actually deploying um and so you it ends",
    "start": "521360",
    "end": "523839"
  },
  {
    "text": "up in a place where you have to do",
    "start": "523839",
    "end": "525760"
  },
  {
    "text": "very specific policy-based replication",
    "start": "525760",
    "end": "528240"
  },
  {
    "text": "of those objects uh and synchronizing",
    "start": "528240",
    "end": "530320"
  },
  {
    "text": "those workloads objects to the",
    "start": "530320",
    "end": "532320"
  },
  {
    "text": "downstream clusters that deploy them uh",
    "start": "532320",
    "end": "534240"
  },
  {
    "text": "what you can kind of see in there is you",
    "start": "534240",
    "end": "535920"
  },
  {
    "text": "end up having controllers running in",
    "start": "535920",
    "end": "537600"
  },
  {
    "text": "nest in those worker clusters that",
    "start": "537600",
    "end": "540000"
  },
  {
    "text": "operate on those workload objects and",
    "start": "540000",
    "end": "542320"
  },
  {
    "text": "make sure that those are scheduled as",
    "start": "542320",
    "end": "543760"
  },
  {
    "text": "you'd expect",
    "start": "543760",
    "end": "545040"
  },
  {
    "text": "and uh where this actually leads you to",
    "start": "545040",
    "end": "547040"
  },
  {
    "text": "is not having any pod objects in your",
    "start": "547040",
    "end": "549519"
  },
  {
    "text": "user's control plane so i as a user",
    "start": "549519",
    "end": "552080"
  },
  {
    "text": "might be using one of those tools and",
    "start": "552080",
    "end": "553680"
  },
  {
    "text": "deploying hdr to deploy a full workload",
    "start": "553680",
    "end": "556640"
  },
  {
    "text": "which defines everything and it goes to",
    "start": "556640",
    "end": "558480"
  },
  {
    "text": "a specific cluster and gets scheduled",
    "start": "558480",
    "end": "561360"
  },
  {
    "text": "and so there's the difference between",
    "start": "561360",
    "end": "562959"
  },
  {
    "text": "pod and workload granularity there so",
    "start": "562959",
    "end": "565519"
  },
  {
    "text": "things like with pods you get no new",
    "start": "565519",
    "end": "567519"
  },
  {
    "text": "apis you get to use the standard",
    "start": "567519",
    "end": "568880"
  },
  {
    "text": "off-the-shelf kubernetes resources that",
    "start": "568880",
    "end": "570800"
  },
  {
    "text": "we all know and love and have been using",
    "start": "570800",
    "end": "572480"
  },
  {
    "text": "for many years at this standpoint",
    "start": "572480",
    "end": "574880"
  },
  {
    "text": "we're not fragmenting those resources in",
    "start": "574880",
    "end": "576880"
  },
  {
    "text": "any any new ways uh and we and we define",
    "start": "576880",
    "end": "580800"
  },
  {
    "text": "this at a better utilization perspective",
    "start": "580800",
    "end": "583040"
  },
  {
    "text": "because it's singularly focused on the",
    "start": "583040",
    "end": "586320"
  },
  {
    "text": "the pod as that unit of kubernetes to be",
    "start": "586320",
    "end": "589120"
  },
  {
    "text": "scheduled and this kind of breaks down",
    "start": "589120",
    "end": "591839"
  },
  {
    "text": "uh in in one regard which is if you have",
    "start": "591839",
    "end": "595040"
  },
  {
    "text": "if you have a centralized control plane",
    "start": "595040",
    "end": "597440"
  },
  {
    "text": "that is the user's control plane that's",
    "start": "597440",
    "end": "598880"
  },
  {
    "text": "actually creating those objects and if",
    "start": "598880",
    "end": "600880"
  },
  {
    "text": "something goes wrong with that and",
    "start": "600880",
    "end": "602480"
  },
  {
    "text": "that's now disconnected and something",
    "start": "602480",
    "end": "604480"
  },
  {
    "text": "goes wrong with the pods on those lower",
    "start": "604480",
    "end": "605920"
  },
  {
    "text": "level clusters there's really no",
    "start": "605920",
    "end": "607200"
  },
  {
    "text": "orchestration piece that's continuing to",
    "start": "607200",
    "end": "608880"
  },
  {
    "text": "orchestrate them or to continue to keep",
    "start": "608880",
    "end": "611279"
  },
  {
    "text": "them up",
    "start": "611279",
    "end": "612480"
  },
  {
    "text": "now from a workload granularity side of",
    "start": "612480",
    "end": "614240"
  },
  {
    "text": "things it does require you to run those",
    "start": "614240",
    "end": "616800"
  },
  {
    "text": "extra extra controllers that go and",
    "start": "616800",
    "end": "619120"
  },
  {
    "text": "operate on those but it gives you the",
    "start": "619120",
    "end": "620480"
  },
  {
    "text": "benefit of",
    "start": "620480",
    "end": "622079"
  },
  {
    "text": "distributed",
    "start": "622079",
    "end": "623600"
  },
  {
    "text": "orchestration so if something goes wrong",
    "start": "623600",
    "end": "625600"
  },
  {
    "text": "with the top level user control plane",
    "start": "625600",
    "end": "627839"
  },
  {
    "text": "that underlying cr could continue to run",
    "start": "627839",
    "end": "630560"
  },
  {
    "text": "even if something happens if the cluster",
    "start": "630560",
    "end": "632160"
  },
  {
    "text": "has a failure and it comes back up to a",
    "start": "632160",
    "end": "634000"
  },
  {
    "text": "healthy state it can continue to run",
    "start": "634000",
    "end": "635760"
  },
  {
    "text": "those workloads but you get the",
    "start": "635760",
    "end": "637839"
  },
  {
    "text": "negatives of having to run those new",
    "start": "637839",
    "end": "639839"
  },
  {
    "text": "apis",
    "start": "639839",
    "end": "641120"
  },
  {
    "text": "to assist with the actual propagation of",
    "start": "641120",
    "end": "642800"
  },
  {
    "text": "the resources and it's kind of",
    "start": "642800",
    "end": "644959"
  },
  {
    "text": "inconvenient to actually uh debug those",
    "start": "644959",
    "end": "647440"
  },
  {
    "text": "workloads because now you have a",
    "start": "647440",
    "end": "649920"
  },
  {
    "text": "difference between what's in the what's",
    "start": "649920",
    "end": "651680"
  },
  {
    "text": "in the user cluster and what's in the",
    "start": "651680",
    "end": "653680"
  },
  {
    "text": "actual workload cluster that's the",
    "start": "653680",
    "end": "655760"
  },
  {
    "text": "worker cluster that's actually running",
    "start": "655760",
    "end": "657200"
  },
  {
    "text": "those things",
    "start": "657200",
    "end": "660440"
  },
  {
    "start": "660000",
    "end": "660000"
  },
  {
    "text": "so that brings us into scheduling so now",
    "start": "661680",
    "end": "664160"
  },
  {
    "text": "as you kind of can understand where",
    "start": "664160",
    "end": "665680"
  },
  {
    "text": "we're coming from with the architecture",
    "start": "665680",
    "end": "667519"
  },
  {
    "text": "uh you can you can kind of look at the",
    "start": "667519",
    "end": "669040"
  },
  {
    "text": "the many different outlets that we could",
    "start": "669040",
    "end": "670480"
  },
  {
    "text": "have had for for building a solution",
    "start": "670480",
    "end": "672399"
  },
  {
    "text": "like this so there's single tier single",
    "start": "672399",
    "end": "675120"
  },
  {
    "text": "tier scheduling and if you're familiar",
    "start": "675120",
    "end": "676480"
  },
  {
    "text": "with like the mesos world a lot of these",
    "start": "676480",
    "end": "678240"
  },
  {
    "text": "concepts will kind of come into play",
    "start": "678240",
    "end": "680240"
  },
  {
    "text": "basically if you have single tier",
    "start": "680240",
    "end": "682079"
  },
  {
    "text": "scheduling which very much looks like",
    "start": "682079",
    "end": "683760"
  },
  {
    "text": "what the kubernetes scheduler has you",
    "start": "683760",
    "end": "685680"
  },
  {
    "text": "have all of the nodes allocated to a",
    "start": "685680",
    "end": "687760"
  },
  {
    "text": "single scheduler",
    "start": "687760",
    "end": "689360"
  },
  {
    "text": "and it and it will pick and choose where",
    "start": "689360",
    "end": "691120"
  },
  {
    "text": "the workloads are going to run along",
    "start": "691120",
    "end": "692800"
  },
  {
    "text": "those nodes um this is also",
    "start": "692800",
    "end": "696160"
  },
  {
    "text": "this also limits your abilities at the",
    "start": "696160",
    "end": "698000"
  },
  {
    "text": "end of the day a single a single",
    "start": "698000",
    "end": "700399"
  },
  {
    "text": "scheduler is going to have scalability",
    "start": "700399",
    "end": "702160"
  },
  {
    "text": "limits for how many pods it can actually",
    "start": "702160",
    "end": "703839"
  },
  {
    "text": "handle how many nodes it can actually",
    "start": "703839",
    "end": "705360"
  },
  {
    "text": "handle because it's going to have to do",
    "start": "705360",
    "end": "706720"
  },
  {
    "text": "checks against all of those nodes in the",
    "start": "706720",
    "end": "708560"
  },
  {
    "text": "cluster and it's going to have to make",
    "start": "708560",
    "end": "709920"
  },
  {
    "text": "sure that all of those",
    "start": "709920",
    "end": "712000"
  },
  {
    "text": "every component of the environment is up",
    "start": "712000",
    "end": "714160"
  },
  {
    "text": "and ready to accept workloads",
    "start": "714160",
    "end": "716959"
  },
  {
    "text": "now",
    "start": "716959",
    "end": "717839"
  },
  {
    "text": "take a step down we go into two-level",
    "start": "717839",
    "end": "720000"
  },
  {
    "text": "scheduling and so this is things like",
    "start": "720000",
    "end": "721920"
  },
  {
    "text": "tensile cube which is basically",
    "start": "721920",
    "end": "724320"
  },
  {
    "text": "taking at the top level user cluster and",
    "start": "724320",
    "end": "726639"
  },
  {
    "text": "creating a",
    "start": "726639",
    "end": "728160"
  },
  {
    "text": "first tier scheduler again this is very",
    "start": "728160",
    "end": "730160"
  },
  {
    "text": "similar to the mesa architecture where",
    "start": "730160",
    "end": "732000"
  },
  {
    "text": "we're going to we're going to delegate",
    "start": "732000",
    "end": "733360"
  },
  {
    "text": "the tasks along so that each thing can",
    "start": "733360",
    "end": "735040"
  },
  {
    "text": "do it very well so the first tier is",
    "start": "735040",
    "end": "736959"
  },
  {
    "text": "going to basically go through and pick a",
    "start": "736959",
    "end": "739600"
  },
  {
    "text": "in essence a cluster that the pods could",
    "start": "739600",
    "end": "741519"
  },
  {
    "text": "be dispatched to so where in our entire",
    "start": "741519",
    "end": "744480"
  },
  {
    "text": "environment the many clusters that are",
    "start": "744480",
    "end": "746160"
  },
  {
    "text": "all connected to",
    "start": "746160",
    "end": "747680"
  },
  {
    "text": "this single single user facing control",
    "start": "747680",
    "end": "750000"
  },
  {
    "text": "plane where does the workload actually",
    "start": "750000",
    "end": "751600"
  },
  {
    "text": "get scheduled uh and then once that once",
    "start": "751600",
    "end": "753920"
  },
  {
    "text": "it picks that it drops it on to those",
    "start": "753920",
    "end": "755839"
  },
  {
    "text": "lower level worker clusters to go and",
    "start": "755839",
    "end": "757920"
  },
  {
    "text": "decide what node is it going to do so we",
    "start": "757920",
    "end": "760240"
  },
  {
    "text": "kind of distribute we distribute the the",
    "start": "760240",
    "end": "762000"
  },
  {
    "text": "decision making to make it",
    "start": "762000",
    "end": "763680"
  },
  {
    "text": "a faster faster scheduling",
    "start": "763680",
    "end": "765920"
  },
  {
    "text": "but it does introduce some failure modes",
    "start": "765920",
    "end": "767920"
  },
  {
    "text": "that need to be accounted for so if you",
    "start": "767920",
    "end": "770000"
  },
  {
    "text": "go through you can imagine a workload",
    "start": "770000",
    "end": "772240"
  },
  {
    "text": "you deployed a pod into your worker",
    "start": "772240",
    "end": "774079"
  },
  {
    "text": "cluster and that now tried to get",
    "start": "774079",
    "end": "775839"
  },
  {
    "text": "scheduled to a cluster that at the same",
    "start": "775839",
    "end": "778000"
  },
  {
    "text": "exact time had a job that was scaling up",
    "start": "778000",
    "end": "781040"
  },
  {
    "text": "and it ran out of capacity and maybe you",
    "start": "781040",
    "end": "782800"
  },
  {
    "text": "didn't have a cluster auto scaler setup",
    "start": "782800",
    "end": "785279"
  },
  {
    "text": "or maybe those controls aren't something",
    "start": "785279",
    "end": "786639"
  },
  {
    "text": "that you have available to you uh and",
    "start": "786639",
    "end": "788560"
  },
  {
    "text": "now that pod can't actually schedule in",
    "start": "788560",
    "end": "790639"
  },
  {
    "text": "the new cluster so those are kind of the",
    "start": "790639",
    "end": "792560"
  },
  {
    "text": "the downfalls of that type of",
    "start": "792560",
    "end": "794160"
  },
  {
    "text": "architecture",
    "start": "794160",
    "end": "795279"
  },
  {
    "text": "and then the last thing here and this is",
    "start": "795279",
    "end": "796720"
  },
  {
    "text": "the the last decision kind of making",
    "start": "796720",
    "end": "798399"
  },
  {
    "text": "that had to be done is distributing that",
    "start": "798399",
    "end": "800720"
  },
  {
    "text": "scheduling decision to many schedulers",
    "start": "800720",
    "end": "802959"
  },
  {
    "text": "so coordinated schedulers talking to",
    "start": "802959",
    "end": "805040"
  },
  {
    "text": "each other and deciding and doing",
    "start": "805040",
    "end": "807519"
  },
  {
    "text": "resource requests and reservations",
    "start": "807519",
    "end": "810079"
  },
  {
    "text": "against those",
    "start": "810079",
    "end": "811440"
  },
  {
    "text": "sub",
    "start": "811440",
    "end": "812240"
  },
  {
    "text": "schedulers and this is a really a really",
    "start": "812240",
    "end": "814880"
  },
  {
    "text": "interesting architecture because it",
    "start": "814880",
    "end": "816959"
  },
  {
    "text": "pretty much reduces the the chance of",
    "start": "816959",
    "end": "818639"
  },
  {
    "text": "any of those race conditions but uh it",
    "start": "818639",
    "end": "821440"
  },
  {
    "text": "requires a lot of coordination so the",
    "start": "821440",
    "end": "823600"
  },
  {
    "text": "top level scheduler needs to know about",
    "start": "823600",
    "end": "825199"
  },
  {
    "text": "every single potential subscheduler and",
    "start": "825199",
    "end": "827360"
  },
  {
    "text": "they need to be doing requests",
    "start": "827360",
    "end": "829199"
  },
  {
    "text": "against them while the actual pod is",
    "start": "829199",
    "end": "831519"
  },
  {
    "text": "being scheduled which incurs",
    "start": "831519",
    "end": "833600"
  },
  {
    "text": "a significant latency to your actual",
    "start": "833600",
    "end": "836240"
  },
  {
    "text": "to your actual workloads",
    "start": "836240",
    "end": "839360"
  },
  {
    "start": "840000",
    "end": "840000"
  },
  {
    "text": "so enter where we where we mostly work",
    "start": "841360",
    "end": "843839"
  },
  {
    "text": "again which is virtual cluster uh and",
    "start": "843839",
    "end": "845839"
  },
  {
    "text": "trying to basically come in at from the",
    "start": "845839",
    "end": "847440"
  },
  {
    "text": "multi-tenancy side of things if you're",
    "start": "847440",
    "end": "849120"
  },
  {
    "text": "deploying into a multi-cluster world",
    "start": "849120",
    "end": "851360"
  },
  {
    "text": "you're likely to dealing with a lot of",
    "start": "851360",
    "end": "853600"
  },
  {
    "text": "different tenants and trying to make",
    "start": "853600",
    "end": "855040"
  },
  {
    "text": "sure that uh",
    "start": "855040",
    "end": "856800"
  },
  {
    "text": "that all of their workloads get",
    "start": "856800",
    "end": "858639"
  },
  {
    "text": "scheduled into somewhere so problem",
    "start": "858639",
    "end": "860399"
  },
  {
    "text": "being multi-tenancy is a legitimate",
    "start": "860399",
    "end": "861920"
  },
  {
    "text": "concern when utilizing a large amount of",
    "start": "861920",
    "end": "864000"
  },
  {
    "text": "aggregate uh cluster resources we have",
    "start": "864000",
    "end": "866240"
  },
  {
    "text": "tons of cpu and memory now we need to",
    "start": "866240",
    "end": "868160"
  },
  {
    "text": "aggregate them all together and expose",
    "start": "868160",
    "end": "870240"
  },
  {
    "text": "those to a subset of users so they can",
    "start": "870240",
    "end": "872480"
  },
  {
    "text": "actually deploy their workloads onto it",
    "start": "872480",
    "end": "874320"
  },
  {
    "text": "and again since we work with virtual",
    "start": "874320",
    "end": "875680"
  },
  {
    "text": "cluster we kind of started to take a a",
    "start": "875680",
    "end": "879199"
  },
  {
    "text": "lens towards that implementation which",
    "start": "879199",
    "end": "881600"
  },
  {
    "text": "really abstracts out the user control",
    "start": "881600",
    "end": "884720"
  },
  {
    "text": "plane as we called it earlier and we",
    "start": "884720",
    "end": "886720"
  },
  {
    "text": "refer to it as the tenant control planes",
    "start": "886720",
    "end": "888320"
  },
  {
    "text": "in our space and this is really",
    "start": "888320",
    "end": "889519"
  },
  {
    "text": "deploying a",
    "start": "889519",
    "end": "891040"
  },
  {
    "text": "kubernetes api server controller manager",
    "start": "891040",
    "end": "893360"
  },
  {
    "text": "and xcd",
    "start": "893360",
    "end": "894720"
  },
  {
    "text": "completely off the shelf tools",
    "start": "894720",
    "end": "896720"
  },
  {
    "text": "into your cluster and exposing that to",
    "start": "896720",
    "end": "900079"
  },
  {
    "text": "the users of your system so each each",
    "start": "900079",
    "end": "902320"
  },
  {
    "text": "individual tenant however you want to",
    "start": "902320",
    "end": "903839"
  },
  {
    "text": "scope out the architecture they get",
    "start": "903839",
    "end": "905519"
  },
  {
    "text": "access to one of those dedicated control",
    "start": "905519",
    "end": "907519"
  },
  {
    "text": "planes and it kind of it solves the hard",
    "start": "907519",
    "end": "909839"
  },
  {
    "text": "multi-tenancy space by by isolating",
    "start": "909839",
    "end": "912639"
  },
  {
    "text": "customers against that so that you can",
    "start": "912639",
    "end": "914399"
  },
  {
    "text": "now really focus on making it so that",
    "start": "914399",
    "end": "916000"
  },
  {
    "text": "the super cluster is really good at",
    "start": "916000",
    "end": "918160"
  },
  {
    "text": "running workloads and the top level tier",
    "start": "918160",
    "end": "921040"
  },
  {
    "text": "gives the flexibility that everybody",
    "start": "921040",
    "end": "922959"
  },
  {
    "text": "wants out of kubernetes so if somebody",
    "start": "922959",
    "end": "924720"
  },
  {
    "text": "wants to deploy a crd into that cluster",
    "start": "924720",
    "end": "926959"
  },
  {
    "text": "they can do that",
    "start": "926959",
    "end": "928399"
  },
  {
    "text": "and it's not going to affect the the",
    "start": "928399",
    "end": "929839"
  },
  {
    "text": "lower level workloads",
    "start": "929839",
    "end": "933120"
  },
  {
    "text": "and so that originally started with",
    "start": "933120",
    "end": "935120"
  },
  {
    "text": "being scoped specific to one",
    "start": "935120",
    "end": "936959"
  },
  {
    "text": "supercluster",
    "start": "936959",
    "end": "938639"
  },
  {
    "text": "but it kind of became a natural",
    "start": "938639",
    "end": "939759"
  },
  {
    "text": "progression to well can we make this run",
    "start": "939759",
    "end": "942560"
  },
  {
    "text": "on multiple clusters using that same",
    "start": "942560",
    "end": "944480"
  },
  {
    "text": "exact syncing mechanism that we have in",
    "start": "944480",
    "end": "946639"
  },
  {
    "text": "place",
    "start": "946639",
    "end": "948959"
  },
  {
    "start": "950000",
    "end": "950000"
  },
  {
    "text": "so",
    "start": "950160",
    "end": "950880"
  },
  {
    "text": "has actually functioned it functions by",
    "start": "950880",
    "end": "952720"
  },
  {
    "text": "adding another interface in in the",
    "start": "952720",
    "end": "954399"
  },
  {
    "text": "middle uh so we talked about the two",
    "start": "954399",
    "end": "956320"
  },
  {
    "text": "two layer scheduling uh and how that",
    "start": "956320",
    "end": "958639"
  },
  {
    "text": "could function this in essence is",
    "start": "958639",
    "end": "960560"
  },
  {
    "text": "building out a top tier scheduler which",
    "start": "960560",
    "end": "963759"
  },
  {
    "text": "operates off of the resources that we",
    "start": "963759",
    "end": "965920"
  },
  {
    "text": "already in most multi-tenant",
    "start": "965920",
    "end": "967360"
  },
  {
    "text": "environments use today so things like",
    "start": "967360",
    "end": "969440"
  },
  {
    "text": "name spaces and resource quotas so a top",
    "start": "969440",
    "end": "972000"
  },
  {
    "text": "level",
    "start": "972000",
    "end": "973120"
  },
  {
    "text": "a top level scheduler can go through and",
    "start": "973120",
    "end": "975279"
  },
  {
    "text": "look at the amount of resources that",
    "start": "975279",
    "end": "977120"
  },
  {
    "text": "you're requesting for a specific name",
    "start": "977120",
    "end": "979040"
  },
  {
    "text": "space and it can start to make those",
    "start": "979040",
    "end": "980639"
  },
  {
    "text": "first decisions so phase one of this is",
    "start": "980639",
    "end": "982480"
  },
  {
    "text": "going and doing the creation of the",
    "start": "982480",
    "end": "984000"
  },
  {
    "text": "namespace and at the same time picking",
    "start": "984000",
    "end": "987120"
  },
  {
    "text": "what cluster it should be deployed into",
    "start": "987120",
    "end": "989279"
  },
  {
    "text": "based on the amount of quota that you've",
    "start": "989279",
    "end": "990959"
  },
  {
    "text": "requested for that single namespace",
    "start": "990959",
    "end": "993920"
  },
  {
    "text": "that can be a very fast decision again",
    "start": "993920",
    "end": "995519"
  },
  {
    "text": "because it's it's just picking based on",
    "start": "995519",
    "end": "997600"
  },
  {
    "text": "capacity and it's not trying to",
    "start": "997600",
    "end": "998880"
  },
  {
    "text": "intermingle uh the amount of uh the the",
    "start": "998880",
    "end": "1001839"
  },
  {
    "text": "pods that are potentially deployed into",
    "start": "1001839",
    "end": "1003360"
  },
  {
    "text": "it or any uh any of the other decisions",
    "start": "1003360",
    "end": "1005120"
  },
  {
    "text": "in there",
    "start": "1005120",
    "end": "1006160"
  },
  {
    "text": "and then once you've picked a cluster",
    "start": "1006160",
    "end": "1008320"
  },
  {
    "text": "for the namespace and the that has",
    "start": "1008320",
    "end": "1010320"
  },
  {
    "text": "capacity to run those workloads it goes",
    "start": "1010320",
    "end": "1012240"
  },
  {
    "text": "on to the pod creation so it",
    "start": "1012240",
    "end": "1014000"
  },
  {
    "text": "in essence uh goes through and",
    "start": "1014000",
    "end": "1016880"
  },
  {
    "text": "will annotate the pod and the name space",
    "start": "1016880",
    "end": "1019519"
  },
  {
    "text": "with the cluster name that the workload",
    "start": "1019519",
    "end": "1021920"
  },
  {
    "text": "is supposed to be deployed into",
    "start": "1021920",
    "end": "1023519"
  },
  {
    "text": "allowing the downstream sinkers to pick",
    "start": "1023519",
    "end": "1026240"
  },
  {
    "text": "up",
    "start": "1026240",
    "end": "1027038"
  },
  {
    "text": "just specifically those workloads and",
    "start": "1027039",
    "end": "1029199"
  },
  {
    "text": "allowing the the workloads to then get",
    "start": "1029199",
    "end": "1030959"
  },
  {
    "text": "scheduled",
    "start": "1030959",
    "end": "1033360"
  },
  {
    "text": "let's look at the architecture of the",
    "start": "1035679",
    "end": "1037438"
  },
  {
    "text": "entire solution for the cost list",
    "start": "1037439",
    "end": "1039678"
  },
  {
    "text": "as chris mentioned before the entire",
    "start": "1039679",
    "end": "1041839"
  },
  {
    "text": "character is the extension of the",
    "start": "1041839",
    "end": "1043280"
  },
  {
    "text": "exiting virtual cluster",
    "start": "1043280",
    "end": "1046558"
  },
  {
    "text": "in virtual cluster we have",
    "start": "1046559",
    "end": "1048480"
  },
  {
    "text": "synchro which synchronizes object",
    "start": "1048480",
    "end": "1050799"
  },
  {
    "text": "between the canon control plane and",
    "start": "1050799",
    "end": "1052400"
  },
  {
    "text": "analyzed super crosstalk as the natural",
    "start": "1052400",
    "end": "1054640"
  },
  {
    "text": "extension you will see that we will",
    "start": "1054640",
    "end": "1056799"
  },
  {
    "text": "assign we will we will create a sinker",
    "start": "1056799",
    "end": "1060000"
  },
  {
    "text": "for each worker clusters and we have a",
    "start": "1060000",
    "end": "1062640"
  },
  {
    "text": "new scheduler which watch for the work",
    "start": "1062640",
    "end": "1065120"
  },
  {
    "text": "across the capacity changing capacity",
    "start": "1065120",
    "end": "1067120"
  },
  {
    "text": "changes and another object will be",
    "start": "1067120",
    "end": "1069360"
  },
  {
    "text": "creating a technical direction",
    "start": "1069360",
    "end": "1070880"
  },
  {
    "text": "again in the vc model the synchro can",
    "start": "1070880",
    "end": "1073919"
  },
  {
    "text": "watch for multiple canon clusters so the",
    "start": "1073919",
    "end": "1076240"
  },
  {
    "text": "same are applied for the scanner as well",
    "start": "1076240",
    "end": "1079120"
  },
  {
    "text": "so",
    "start": "1079120",
    "end": "1079840"
  },
  {
    "text": "overall if you look at the the",
    "start": "1079840",
    "end": "1082640"
  },
  {
    "text": "the new architect uh is that",
    "start": "1082640",
    "end": "1085360"
  },
  {
    "text": "uh we on top of the virtual cluster we",
    "start": "1085360",
    "end": "1088080"
  },
  {
    "text": "have uh modified sinkers and then we",
    "start": "1088080",
    "end": "1090240"
  },
  {
    "text": "have a new uh first level schedulers",
    "start": "1090240",
    "end": "1093280"
  },
  {
    "text": "in practice both the thinker and the",
    "start": "1093280",
    "end": "1095679"
  },
  {
    "text": "scheduler will normally manage by",
    "start": "1095679",
    "end": "1097760"
  },
  {
    "text": "separate meta clusters but for",
    "start": "1097760",
    "end": "1099760"
  },
  {
    "text": "simplicity i'm not going to show that in",
    "start": "1099760",
    "end": "1101600"
  },
  {
    "text": "this uh this figure",
    "start": "1101600",
    "end": "1105200"
  },
  {
    "text": "all right next i'm going to talk about",
    "start": "1106400",
    "end": "1107679"
  },
  {
    "text": "some of the implementation details to",
    "start": "1107679",
    "end": "1109919"
  },
  {
    "text": "realize the uh costliest uh curl height",
    "start": "1109919",
    "end": "1113440"
  },
  {
    "text": "uh as i mentioned before uh for the",
    "start": "1113440",
    "end": "1115760"
  },
  {
    "text": "first thing if we need to enhance the",
    "start": "1115760",
    "end": "1118080"
  },
  {
    "text": "thinker to support selective object",
    "start": "1118080",
    "end": "1120960"
  },
  {
    "text": "synchronization",
    "start": "1120960",
    "end": "1122559"
  },
  {
    "text": "why we want to do that because",
    "start": "1122559",
    "end": "1125679"
  },
  {
    "text": "in theory you can simplify the solution",
    "start": "1125679",
    "end": "1128240"
  },
  {
    "text": "by just copying all the tenant objects",
    "start": "1128240",
    "end": "1130960"
  },
  {
    "text": "with the namespace in all the underlying",
    "start": "1130960",
    "end": "1133360"
  },
  {
    "text": "work clusters but not the part object",
    "start": "1133360",
    "end": "1135840"
  },
  {
    "text": "just to prepare the part will be",
    "start": "1135840",
    "end": "1138240"
  },
  {
    "text": "scheduled to any of the water clusters",
    "start": "1138240",
    "end": "1140320"
  },
  {
    "text": "but this will introduce some unnecessary",
    "start": "1140320",
    "end": "1142320"
  },
  {
    "text": "overheads of storing the objects if the",
    "start": "1142320",
    "end": "1145120"
  },
  {
    "text": "part is not running those clusters they",
    "start": "1145120",
    "end": "1147120"
  },
  {
    "text": "are completely waste",
    "start": "1147120",
    "end": "1148559"
  },
  {
    "text": "so to to resolve that problem we",
    "start": "1148559",
    "end": "1151600"
  },
  {
    "text": "uh enhancing the sinker to support",
    "start": "1151600",
    "end": "1153760"
  },
  {
    "text": "selective civilization and the uh the",
    "start": "1153760",
    "end": "1157600"
  },
  {
    "text": "the",
    "start": "1157600",
    "end": "1159919"
  },
  {
    "text": "the decision is is determined by the",
    "start": "1159919",
    "end": "1162320"
  },
  {
    "text": "placement results which is the which is",
    "start": "1162320",
    "end": "1166160"
  },
  {
    "text": "which is specified by the scheduler so",
    "start": "1166160",
    "end": "1168880"
  },
  {
    "text": "if you look at the right figure in this",
    "start": "1168880",
    "end": "1171039"
  },
  {
    "text": "example uh assuming the namespace a in",
    "start": "1171039",
    "end": "1175520"
  },
  {
    "text": "cannon control plane e1",
    "start": "1175520",
    "end": "1178320"
  },
  {
    "text": "has the quarter and the quarter size",
    "start": "1178320",
    "end": "1180880"
  },
  {
    "text": "config so that we have two slices and",
    "start": "1180880",
    "end": "1183280"
  },
  {
    "text": "the scheduler",
    "start": "1183280",
    "end": "1184559"
  },
  {
    "text": "has decided",
    "start": "1184559",
    "end": "1186000"
  },
  {
    "text": "the the scheduling decision with the",
    "start": "1186000",
    "end": "1188320"
  },
  {
    "text": "placement results we put one slice in c1",
    "start": "1188320",
    "end": "1190960"
  },
  {
    "text": "one size in c2",
    "start": "1190960",
    "end": "1192880"
  },
  {
    "text": "uh on the other hand for the name series",
    "start": "1192880",
    "end": "1196000"
  },
  {
    "text": "b it can be in",
    "start": "1196000",
    "end": "1198000"
  },
  {
    "text": "tandem control and two uh it only has",
    "start": "1198000",
    "end": "1200640"
  },
  {
    "text": "one size and it's scheduled to",
    "start": "1200640",
    "end": "1202799"
  },
  {
    "text": "c1",
    "start": "1202799",
    "end": "1203840"
  },
  {
    "text": "so the sinker from c1 will synchronize",
    "start": "1203840",
    "end": "1206480"
  },
  {
    "text": "the objects from two name spaces but the",
    "start": "1206480",
    "end": "1209280"
  },
  {
    "text": "sinker from for c2 we are just",
    "start": "1209280",
    "end": "1211280"
  },
  {
    "text": "synchronized the name space",
    "start": "1211280",
    "end": "1213120"
  },
  {
    "text": "for one named space which is the name",
    "start": "1213120",
    "end": "1214880"
  },
  {
    "text": "space a in t1",
    "start": "1214880",
    "end": "1217760"
  },
  {
    "text": "again so the sinker rear synchronize all",
    "start": "1217760",
    "end": "1220400"
  },
  {
    "text": "objects except the part",
    "start": "1220400",
    "end": "1222240"
  },
  {
    "text": "to the underlying over clusters and uh",
    "start": "1222240",
    "end": "1224880"
  },
  {
    "text": "we we make sure the part will be synced",
    "start": "1224880",
    "end": "1227440"
  },
  {
    "text": "to only one work clusters which is the",
    "start": "1227440",
    "end": "1229840"
  },
  {
    "text": "target",
    "start": "1229840",
    "end": "1231039"
  },
  {
    "text": "we which is targeting",
    "start": "1231039",
    "end": "1232559"
  },
  {
    "text": "clusters",
    "start": "1232559",
    "end": "1233840"
  },
  {
    "text": "determined by the scheduler",
    "start": "1233840",
    "end": "1237200"
  },
  {
    "text": "uh out of all the implementation the",
    "start": "1237760",
    "end": "1240000"
  },
  {
    "text": "most challenging part is to implement a",
    "start": "1240000",
    "end": "1241840"
  },
  {
    "text": "scheduled cache unlike the traditional",
    "start": "1241840",
    "end": "1244720"
  },
  {
    "text": "kubernetes scheduler you only need to",
    "start": "1244720",
    "end": "1246400"
  },
  {
    "text": "watch for one",
    "start": "1246400",
    "end": "1248400"
  },
  {
    "text": "ap server",
    "start": "1248400",
    "end": "1249760"
  },
  {
    "text": "in this in this for this scheduler you",
    "start": "1249760",
    "end": "1252559"
  },
  {
    "text": "have to watch for the status of multiple",
    "start": "1252559",
    "end": "1255520"
  },
  {
    "text": "uh clutters both in both tenant control",
    "start": "1255520",
    "end": "1258400"
  },
  {
    "text": "plane and mla worker clusters there are",
    "start": "1258400",
    "end": "1260480"
  },
  {
    "text": "a lot of failure points in",
    "start": "1260480",
    "end": "1263440"
  },
  {
    "text": "in this settings for example dependent",
    "start": "1263440",
    "end": "1265120"
  },
  {
    "text": "controller can be offline uh one worker",
    "start": "1265120",
    "end": "1267679"
  },
  {
    "text": "cluster can be offline or one loading",
    "start": "1267679",
    "end": "1270000"
  },
  {
    "text": "one workflow cluster can also be offline",
    "start": "1270000",
    "end": "1272240"
  },
  {
    "text": "so we need to make sure rows while rose",
    "start": "1272240",
    "end": "1275760"
  },
  {
    "text": "fader happens the the",
    "start": "1275760",
    "end": "1278320"
  },
  {
    "text": "uh the scheduled cast is still",
    "start": "1278320",
    "end": "1279840"
  },
  {
    "text": "consistent we put a lot of efforts to",
    "start": "1279840",
    "end": "1281840"
  },
  {
    "text": "make sure uh",
    "start": "1281840",
    "end": "1283520"
  },
  {
    "text": "it happens",
    "start": "1283520",
    "end": "1284720"
  },
  {
    "text": "and another general problem is that uh",
    "start": "1284720",
    "end": "1288240"
  },
  {
    "text": "whether the scheduler should watch for",
    "start": "1288240",
    "end": "1290320"
  },
  {
    "text": "the all the node events coming from",
    "start": "1290320",
    "end": "1292640"
  },
  {
    "text": "underlying water clusters it can be a",
    "start": "1292640",
    "end": "1294880"
  },
  {
    "text": "huge amount of manual traffic if you",
    "start": "1294880",
    "end": "1297120"
  },
  {
    "text": "really want to do that because",
    "start": "1297120",
    "end": "1299039"
  },
  {
    "text": "kubernetes really generate known harvest",
    "start": "1299039",
    "end": "1301840"
  },
  {
    "text": "and will be kind of released",
    "start": "1301840",
    "end": "1304480"
  },
  {
    "text": "no least api calls",
    "start": "1304480",
    "end": "1306559"
  },
  {
    "text": "we to reduce the overhead we choose to",
    "start": "1306559",
    "end": "1309440"
  },
  {
    "text": "periodically scan the clusters to",
    "start": "1309440",
    "end": "1312080"
  },
  {
    "text": "collect the node status",
    "start": "1312080",
    "end": "1314320"
  },
  {
    "text": "to uh and compute the available",
    "start": "1314320",
    "end": "1317280"
  },
  {
    "text": "classic",
    "start": "1317280",
    "end": "1318240"
  },
  {
    "text": "capabilities the downside of doing that",
    "start": "1318240",
    "end": "1320640"
  },
  {
    "text": "is that there are some there is a",
    "start": "1320640",
    "end": "1322400"
  },
  {
    "text": "certain delay if the underlying uh",
    "start": "1322400",
    "end": "1324880"
  },
  {
    "text": "cluster capacity change we the scheduler",
    "start": "1324880",
    "end": "1327760"
  },
  {
    "text": "will be aware of those changes um these",
    "start": "1327760",
    "end": "1331200"
  },
  {
    "text": "with some delays which which can cause",
    "start": "1331200",
    "end": "1333600"
  },
  {
    "text": "some uh raw scheduling decision but we",
    "start": "1333600",
    "end": "1336240"
  },
  {
    "text": "have some uh remediation process can",
    "start": "1336240",
    "end": "1338960"
  },
  {
    "text": "accommodate that effect",
    "start": "1338960",
    "end": "1342480"
  },
  {
    "text": "in terms of the algorithm for the",
    "start": "1342960",
    "end": "1344480"
  },
  {
    "text": "actually scheduling algorithm it is",
    "start": "1344480",
    "end": "1346159"
  },
  {
    "text": "pretty simplified algorithm",
    "start": "1346159",
    "end": "1348640"
  },
  {
    "text": "as of now",
    "start": "1348640",
    "end": "1350320"
  },
  {
    "text": "for namespace quarter quarter slice",
    "start": "1350320",
    "end": "1352640"
  },
  {
    "text": "scheduling we pretty much just do two",
    "start": "1352640",
    "end": "1354720"
  },
  {
    "text": "things first we try to choose the",
    "start": "1354720",
    "end": "1356960"
  },
  {
    "text": "minimum amount of clusters can satisfy",
    "start": "1356960",
    "end": "1359440"
  },
  {
    "text": "all the slice requirements",
    "start": "1359440",
    "end": "1361760"
  },
  {
    "text": "uh the reason is that we want to reduce",
    "start": "1361760",
    "end": "1363760"
  },
  {
    "text": "the the",
    "start": "1363760",
    "end": "1365679"
  },
  {
    "text": "the amount of classes that synchronize",
    "start": "1365679",
    "end": "1368080"
  },
  {
    "text": "the uh non-part objects from those space",
    "start": "1368080",
    "end": "1373120"
  },
  {
    "text": "and and",
    "start": "1373280",
    "end": "1374159"
  },
  {
    "text": "we will just use a simple phosphate",
    "start": "1374159",
    "end": "1375840"
  },
  {
    "text": "algorithm to pick the uh the clutch",
    "start": "1375840",
    "end": "1378159"
  },
  {
    "text": "clusters",
    "start": "1378159",
    "end": "1379280"
  },
  {
    "text": "uh for some phase two part wise schedule",
    "start": "1379280",
    "end": "1382080"
  },
  {
    "text": "uh among candidate clusters the",
    "start": "1382080",
    "end": "1383840"
  },
  {
    "text": "algorithm is really simple we just use a",
    "start": "1383840",
    "end": "1385760"
  },
  {
    "text": "simple first feed",
    "start": "1385760",
    "end": "1387600"
  },
  {
    "text": "or round robin to find the target",
    "start": "1387600",
    "end": "1389120"
  },
  {
    "text": "clusters",
    "start": "1389120",
    "end": "1390799"
  },
  {
    "text": "note that there are large room cleaning",
    "start": "1390799",
    "end": "1392559"
  },
  {
    "text": "costs in the scheduling",
    "start": "1392559",
    "end": "1394559"
  },
  {
    "text": "domain",
    "start": "1394559",
    "end": "1395600"
  },
  {
    "text": "for example we can leverage a lot of",
    "start": "1395600",
    "end": "1397200"
  },
  {
    "text": "from the upstream scheduling",
    "start": "1397200",
    "end": "1399919"
  },
  {
    "text": "schedulers scheduling capabilities such",
    "start": "1399919",
    "end": "1402559"
  },
  {
    "text": "as support affinity and anti-affinity or",
    "start": "1402559",
    "end": "1405280"
  },
  {
    "text": "even spread policy so",
    "start": "1405280",
    "end": "1407760"
  },
  {
    "text": "we've so if we have that",
    "start": "1407760",
    "end": "1410559"
  },
  {
    "text": "implementation online we can",
    "start": "1410559",
    "end": "1413039"
  },
  {
    "text": "pretty much support some",
    "start": "1413039",
    "end": "1414799"
  },
  {
    "text": "scenario that will be currently not",
    "start": "1414799",
    "end": "1417840"
  },
  {
    "text": "that we cannot support for now let's say",
    "start": "1417840",
    "end": "1419919"
  },
  {
    "text": "for the redundancy of living",
    "start": "1419919",
    "end": "1422080"
  },
  {
    "text": "the uh",
    "start": "1422080",
    "end": "1423279"
  },
  {
    "text": "kind of aspect",
    "start": "1423279",
    "end": "1425440"
  },
  {
    "text": "but this area is to be enhanced",
    "start": "1425440",
    "end": "1428080"
  },
  {
    "text": "uh although the arizona are simple but",
    "start": "1428080",
    "end": "1430640"
  },
  {
    "text": "we still can get some insights from the",
    "start": "1430640",
    "end": "1433200"
  },
  {
    "text": "design such that uh the namespace uh",
    "start": "1433200",
    "end": "1435840"
  },
  {
    "text": "quarter slice schedule is not in a pod",
    "start": "1435840",
    "end": "1437600"
  },
  {
    "text": "creation grid path",
    "start": "1437600",
    "end": "1439600"
  },
  {
    "text": "and uh and our scheduling always make",
    "start": "1439600",
    "end": "1442159"
  },
  {
    "text": "sure for part of scaling the number of",
    "start": "1442159",
    "end": "1444480"
  },
  {
    "text": "candidates clusters should be small so",
    "start": "1444480",
    "end": "1447520"
  },
  {
    "text": "this scheduling overheads can be",
    "start": "1447520",
    "end": "1449360"
  },
  {
    "text": "measurable overall by doing this",
    "start": "1449360",
    "end": "1452559"
  },
  {
    "text": "we can roughly achieve can system wise",
    "start": "1452559",
    "end": "1455360"
  },
  {
    "text": "positive throughput with with all the",
    "start": "1455360",
    "end": "1458640"
  },
  {
    "text": "unstability",
    "start": "1458640",
    "end": "1460240"
  },
  {
    "text": "which is the so potentially you can",
    "start": "1460240",
    "end": "1463039"
  },
  {
    "text": "leverage all the schedulers running",
    "start": "1463039",
    "end": "1465440"
  },
  {
    "text": "inside the underlying",
    "start": "1465440",
    "end": "1467039"
  },
  {
    "text": "uh worker clusters to",
    "start": "1467039",
    "end": "1469279"
  },
  {
    "text": "schedule all the paths that",
    "start": "1469279",
    "end": "1472159"
  },
  {
    "text": "are sending to the",
    "start": "1472159",
    "end": "1474240"
  },
  {
    "text": "top level user facing attendance",
    "start": "1474240",
    "end": "1476159"
  },
  {
    "text": "enterprise",
    "start": "1476159",
    "end": "1478640"
  },
  {
    "text": "i want to talk about some other features",
    "start": "1479840",
    "end": "1482000"
  },
  {
    "text": "which is very specific to this",
    "start": "1482000",
    "end": "1483600"
  },
  {
    "text": "multi-class scheduling the first one is",
    "start": "1483600",
    "end": "1486159"
  },
  {
    "text": "rescheduling because",
    "start": "1486159",
    "end": "1487840"
  },
  {
    "text": "uh the the the placed candidate clusters",
    "start": "1487840",
    "end": "1490720"
  },
  {
    "text": "for the name services can be offline",
    "start": "1490720",
    "end": "1492960"
  },
  {
    "text": "and time",
    "start": "1492960",
    "end": "1495279"
  },
  {
    "text": "and",
    "start": "1495279",
    "end": "1496159"
  },
  {
    "text": "unlike the traditional kubernetes they",
    "start": "1496159",
    "end": "1498559"
  },
  {
    "text": "have no computer to edit the path in",
    "start": "1498559",
    "end": "1500320"
  },
  {
    "text": "case of node fader we don't have that",
    "start": "1500320",
    "end": "1502080"
  },
  {
    "text": "controller ready yet for this",
    "start": "1502080",
    "end": "1503600"
  },
  {
    "text": "multi-cluster domain",
    "start": "1503600",
    "end": "1505520"
  },
  {
    "text": "instead for now we are just",
    "start": "1505520",
    "end": "1507679"
  },
  {
    "text": "allowing people to manually revoke the",
    "start": "1507679",
    "end": "1510000"
  },
  {
    "text": "scheduled decision by",
    "start": "1510000",
    "end": "1511840"
  },
  {
    "text": "removing the",
    "start": "1511840",
    "end": "1513200"
  },
  {
    "text": "scale results from the namespace",
    "start": "1513200",
    "end": "1515440"
  },
  {
    "text": "annotation the schedule we are kicking",
    "start": "1515440",
    "end": "1517279"
  },
  {
    "text": "and rescheduling the namespace",
    "start": "1517279",
    "end": "1519679"
  },
  {
    "text": "to find the online cluster to support",
    "start": "1519679",
    "end": "1522240"
  },
  {
    "text": "those workload",
    "start": "1522240",
    "end": "1524320"
  },
  {
    "text": "and in case there are some any stating",
    "start": "1524320",
    "end": "1526880"
  },
  {
    "text": "objects stays in the work clusters the",
    "start": "1526880",
    "end": "1529440"
  },
  {
    "text": "sinker will do the gc",
    "start": "1529440",
    "end": "1531679"
  },
  {
    "text": "another aspect of in terms of the",
    "start": "1531679",
    "end": "1534159"
  },
  {
    "text": "application workload",
    "start": "1534159",
    "end": "1536080"
  },
  {
    "text": "runtime it isn't service support clearly",
    "start": "1536080",
    "end": "1539039"
  },
  {
    "text": "the native kubernetes doesn't support a",
    "start": "1539039",
    "end": "1541039"
  },
  {
    "text": "service across clusters",
    "start": "1541039",
    "end": "1543039"
  },
  {
    "text": "uh but if you choose a load balance load",
    "start": "1543039",
    "end": "1546159"
  },
  {
    "text": "balance type of service and point the",
    "start": "1546159",
    "end": "1548559"
  },
  {
    "text": "traffic to your",
    "start": "1548559",
    "end": "1550000"
  },
  {
    "text": "global",
    "start": "1550000",
    "end": "1551039"
  },
  {
    "text": "load balancer which support much",
    "start": "1551039",
    "end": "1552640"
  },
  {
    "text": "clusters i think the entire networking",
    "start": "1552640",
    "end": "1554720"
  },
  {
    "text": "may still work",
    "start": "1554720",
    "end": "1557440"
  },
  {
    "text": "another thing is that the the current",
    "start": "1557440",
    "end": "1559760"
  },
  {
    "text": "architecture should work for other",
    "start": "1559760",
    "end": "1561679"
  },
  {
    "text": "existing multi-cluster networking",
    "start": "1561679",
    "end": "1563360"
  },
  {
    "text": "solutions such as the cdma cluster mesh",
    "start": "1563360",
    "end": "1566720"
  },
  {
    "text": "i think our scheduler our offering help",
    "start": "1566720",
    "end": "1569919"
  },
  {
    "text": "us release the design support those kind",
    "start": "1569919",
    "end": "1572799"
  },
  {
    "text": "of a solution for that for the",
    "start": "1572799",
    "end": "1575760"
  },
  {
    "text": "network service cross cluster",
    "start": "1575760",
    "end": "1579360"
  },
  {
    "text": "uh then we have uh implemented the",
    "start": "1579840",
    "end": "1582799"
  },
  {
    "text": "features that i mentioned before and it",
    "start": "1582799",
    "end": "1584799"
  },
  {
    "text": "come with a prototype uh due to the time",
    "start": "1584799",
    "end": "1587440"
  },
  {
    "text": "limitation i'm not going to give a full",
    "start": "1587440",
    "end": "1589200"
  },
  {
    "text": "live",
    "start": "1589200",
    "end": "1590000"
  },
  {
    "text": "live demo but you can find the demo in",
    "start": "1590000",
    "end": "1592480"
  },
  {
    "text": "this link essentially in this demo what",
    "start": "1592480",
    "end": "1594880"
  },
  {
    "text": "i do is i",
    "start": "1594880",
    "end": "1596320"
  },
  {
    "text": "create one tenant control plan and with",
    "start": "1596320",
    "end": "1598880"
  },
  {
    "text": "two",
    "start": "1598880",
    "end": "1599919"
  },
  {
    "text": "uh",
    "start": "1599919",
    "end": "1600720"
  },
  {
    "text": "worker clusters uh and i manipulated the",
    "start": "1600720",
    "end": "1604159"
  },
  {
    "text": "quarter as specified in the default name",
    "start": "1604159",
    "end": "1606080"
  },
  {
    "text": "space and the quarter slide size so that",
    "start": "1606080",
    "end": "1609039"
  },
  {
    "text": "we have two slices and the scheduler",
    "start": "1609039",
    "end": "1611679"
  },
  {
    "text": "scheduled each one slice in one worker",
    "start": "1611679",
    "end": "1614400"
  },
  {
    "text": "clusters",
    "start": "1614400",
    "end": "1615679"
  },
  {
    "text": "now i create a deployment with two",
    "start": "1615679",
    "end": "1619840"
  },
  {
    "text": "replicas called gold line",
    "start": "1619840",
    "end": "1622480"
  },
  {
    "text": "and the scheduler rear schedule rows",
    "start": "1622480",
    "end": "1625679"
  },
  {
    "text": "parts in two separate worker clusters",
    "start": "1625679",
    "end": "1628480"
  },
  {
    "text": "and if you look at the screenshot you",
    "start": "1628480",
    "end": "1629919"
  },
  {
    "text": "can see that from the user face control",
    "start": "1629919",
    "end": "1631760"
  },
  {
    "text": "plan you check the the path the usdq",
    "start": "1631760",
    "end": "1634880"
  },
  {
    "text": "process is running and if you check each",
    "start": "1634880",
    "end": "1637760"
  },
  {
    "text": "two",
    "start": "1637760",
    "end": "1639120"
  },
  {
    "text": "across the name name the root one root",
    "start": "1639120",
    "end": "1641360"
  },
  {
    "text": "two only one part is running one worker",
    "start": "1641360",
    "end": "1644320"
  },
  {
    "text": "clusters uh again for more details",
    "start": "1644320",
    "end": "1646559"
  },
  {
    "text": "please go through this link you see a",
    "start": "1646559",
    "end": "1648399"
  },
  {
    "text": "full demo",
    "start": "1648399",
    "end": "1650960"
  },
  {
    "text": "so in summary so the multi cluster",
    "start": "1651120",
    "end": "1652880"
  },
  {
    "text": "management solution becomes popular and",
    "start": "1652880",
    "end": "1655440"
  },
  {
    "text": "they they usually choose different uh",
    "start": "1655440",
    "end": "1658240"
  },
  {
    "text": "workload abstract models and choose",
    "start": "1658240",
    "end": "1660640"
  },
  {
    "text": "different scheduling strategies",
    "start": "1660640",
    "end": "1662720"
  },
  {
    "text": "to",
    "start": "1662720",
    "end": "1663440"
  },
  {
    "text": "schedule those workloads",
    "start": "1663440",
    "end": "1665120"
  },
  {
    "text": "uh and normally they will bring new apis",
    "start": "1665120",
    "end": "1668320"
  },
  {
    "text": "so really integrating those solutions",
    "start": "1668320",
    "end": "1671120"
  },
  {
    "text": "with existing solutions",
    "start": "1671120",
    "end": "1673440"
  },
  {
    "text": "will be non-treat will be a a",
    "start": "1673440",
    "end": "1675840"
  },
  {
    "text": "non-trigger effort",
    "start": "1675840",
    "end": "1677520"
  },
  {
    "text": "and",
    "start": "1677520",
    "end": "1679120"
  },
  {
    "text": "for road solution normally requires",
    "start": "1679120",
    "end": "1680880"
  },
  {
    "text": "manual capacity planning which means the",
    "start": "1680880",
    "end": "1683600"
  },
  {
    "text": "users have to have",
    "start": "1683600",
    "end": "1685679"
  },
  {
    "text": "full knowledge about the resource usage",
    "start": "1685679",
    "end": "1688480"
  },
  {
    "text": "of the underlying worker clusters so",
    "start": "1688480",
    "end": "1690720"
  },
  {
    "text": "that they can make sure",
    "start": "1690720",
    "end": "1692320"
  },
  {
    "text": "they are",
    "start": "1692320",
    "end": "1693360"
  },
  {
    "text": "scheduling",
    "start": "1693360",
    "end": "1695760"
  },
  {
    "text": "their scheduling policy can work well",
    "start": "1695760",
    "end": "1697840"
  },
  {
    "text": "make sure those workloads can still can",
    "start": "1697840",
    "end": "1700399"
  },
  {
    "text": "run in and out worker clusters",
    "start": "1700399",
    "end": "1702880"
  },
  {
    "text": "uh on the other hand in this time we",
    "start": "1702880",
    "end": "1705440"
  },
  {
    "text": "propose clusters uh it is a solution",
    "start": "1705440",
    "end": "1708559"
  },
  {
    "text": "focusing on primarily on the scheduling",
    "start": "1708559",
    "end": "1710480"
  },
  {
    "text": "and multi-tenancy aspects of the",
    "start": "1710480",
    "end": "1712440"
  },
  {
    "text": "multi-cluster management",
    "start": "1712440",
    "end": "1715039"
  },
  {
    "text": "space",
    "start": "1715039",
    "end": "1716640"
  },
  {
    "text": "we implement the class view by extending",
    "start": "1716640",
    "end": "1719039"
  },
  {
    "text": "the virtual cluster framework and we can",
    "start": "1719039",
    "end": "1721200"
  },
  {
    "text": "achieve the scalable scheduling",
    "start": "1721200",
    "end": "1723039"
  },
  {
    "text": "throughput and the entire solution is",
    "start": "1723039",
    "end": "1725279"
  },
  {
    "text": "pretty easy to integrate because we",
    "start": "1725279",
    "end": "1726720"
  },
  {
    "text": "don't introduce any new api commandment",
    "start": "1726720",
    "end": "1729360"
  },
  {
    "text": "to manage the workloads",
    "start": "1729360",
    "end": "1732640"
  },
  {
    "text": "all right this is the end of the",
    "start": "1732640",
    "end": "1734159"
  },
  {
    "text": "plantation i'm happy to take any",
    "start": "1734159",
    "end": "1736080"
  },
  {
    "text": "questions thank you",
    "start": "1736080",
    "end": "1739840"
  }
]