[
  {
    "start": "0",
    "end": "197000"
  },
  {
    "text": "all right welcome to beyond Nagios modern monitoring of bronze ADA applications with Prometheus my name is",
    "start": "30",
    "end": "6359"
  },
  {
    "text": "ben kochi I'm an s3 at SoundCloud on our production engineering team we manage",
    "start": "6359",
    "end": "12179"
  },
  {
    "text": "all of the infrastructure software required to run",
    "start": "12179",
    "end": "17369"
  },
  {
    "text": "the sound cloud platform for all the product teams that make the actual",
    "start": "17369",
    "end": "22460"
  },
  {
    "text": "software for for SoundCloud the history of me I've been in ISP I",
    "start": "22460",
    "end": "29730"
  },
  {
    "text": "Tisa said men s serene changes over the years for about 20",
    "start": "29730",
    "end": "35280"
  },
  {
    "text": "years I got into high performance computing in the early 2000s and then I got into the sra thing at Google in 2005",
    "start": "35280",
    "end": "43890"
  },
  {
    "text": "and the last three years I've been at SoundCloud trying to bring the the s re ideals to",
    "start": "43890",
    "end": "52520"
  },
  {
    "text": "to a smaller startup a quick thing how many people here are",
    "start": "52520",
    "end": "60629"
  },
  {
    "text": "familiar with previous so",
    "start": "60629",
    "end": "64460"
  },
  {
    "text": "and what we're active in the pok√©mon is a software we planted we nicknamed mother",
    "start": "66990",
    "end": "73740"
  },
  {
    "text": "their mother typically some classic bailiff",
    "start": "73740",
    "end": "80299"
  },
  {
    "text": "organ rails application Road TN that side Alexa",
    "start": "80299",
    "end": "86460"
  },
  {
    "text": "teeth and [Applause] that seems to be the popular music now a",
    "start": "86460",
    "end": "93850"
  },
  {
    "text": "breakup smaller services to reduce their",
    "start": "93850",
    "end": "99590"
  },
  {
    "text": "selection",
    "start": "99590",
    "end": "101920"
  },
  {
    "text": "and being intelligent 725 database and very large employer and",
    "start": "106509",
    "end": "116070"
  },
  {
    "text": "we are actively trying to produce kopecks relief teachers moving about",
    "start": "117030",
    "end": "122060"
  },
  {
    "text": "building new services to replace so the real-time intelligence",
    "start": "122060",
    "end": "128360"
  },
  {
    "text": "hi my middle name when I started",
    "start": "128360",
    "end": "134830"
  },
  {
    "text": "and I'm currently occurred but then",
    "start": "138220",
    "end": "145870"
  },
  {
    "text": "we started to say hey wait a minute we should we should probably bring disgrace",
    "start": "145870",
    "end": "151240"
  },
  {
    "text": "to tomorrow so over the last four years",
    "start": "151240",
    "end": "157360"
  },
  {
    "text": "migration from the classic modern",
    "start": "160380",
    "end": "164450"
  },
  {
    "text": "mothers itself has already asked a lot",
    "start": "168870",
    "end": "174080"
  },
  {
    "text": "was New Relic and New Relic is one of these super nice tools for small rails",
    "start": "176650",
    "end": "182959"
  },
  {
    "text": "applications and other types of apps because you just drop in their plugin it",
    "start": "182959",
    "end": "188120"
  },
  {
    "text": "gives you all kinds of detail lots of debug ability",
    "start": "188120",
    "end": "193180"
  },
  {
    "text": "and it's actually quite nice but there's some downsides",
    "start": "193180",
    "end": "199720"
  },
  {
    "start": "197000",
    "end": "336000"
  },
  {
    "text": "the some of the biggest downside is it's really expensive I did a bit of a math",
    "start": "199720",
    "end": "206659"
  },
  {
    "text": "to figure out how we were using my new relic and like",
    "start": "206659",
    "end": "212389"
  },
  {
    "text": "hey why are we only running new relic on a small 5% sample of our servers well it",
    "start": "212389",
    "end": "217970"
  },
  {
    "text": "turns out that that 5% sample was costing my salary every month",
    "start": "217970",
    "end": "223480"
  },
  {
    "text": "so it was it would be economically infeasible for us to actually deploy new",
    "start": "223480",
    "end": "230299"
  },
  {
    "text": "relic to the entire mothership so we only had a small 5% sample of servers",
    "start": "230299",
    "end": "235510"
  },
  {
    "text": "instrumented which was just like wait a minute we're only looking at when I look at New Relic I'm only looking at a 5%",
    "start": "235510",
    "end": "241970"
  },
  {
    "text": "sample of our of our app what if all the other 90% of our servers are down well",
    "start": "241970",
    "end": "247639"
  },
  {
    "text": "we don't know so we needed we needed we needed other things",
    "start": "247639",
    "end": "253060"
  },
  {
    "text": "other problems with New Relic it's data aggregation is by minute and",
    "start": "253060",
    "end": "259070"
  },
  {
    "text": "you know I see this there everything is in New Relic US by minute so it's pretty low resolution so you can't see small",
    "start": "259070",
    "end": "265909"
  },
  {
    "text": "spikes and there was a time when after we added Prometheus that",
    "start": "265909",
    "end": "271460"
  },
  {
    "text": "we discovered oh wait a minute there's a small five-second burst that's hidden in",
    "start": "271460",
    "end": "277070"
  },
  {
    "text": "the noise at the top about the hour every hour and it's hitting from one subsystem and it was actually causing",
    "start": "277070",
    "end": "283940"
  },
  {
    "text": "errors to users because the Android devices were waking up on the top of the hour",
    "start": "283940",
    "end": "290979"
  },
  {
    "text": "right was a yeah I was the top of the hour and they were all hit all Android",
    "start": "290979",
    "end": "296150"
  },
  {
    "text": "devices in the entire planet we're hitting our servers simultaneously or maybe not all but a pretty big chunk of",
    "start": "296150",
    "end": "301789"
  },
  {
    "text": "them and with with New Relic we couldn't see that but with Prometheus we could",
    "start": "301789",
    "end": "307779"
  },
  {
    "text": "the other problem with New Relic was the the user interface is they're using your interface and you only get to see it the",
    "start": "308390",
    "end": "314870"
  },
  {
    "text": "way that they want you to see it so you don't have the flexibility to access the",
    "start": "314870",
    "end": "320300"
  },
  {
    "text": "raw data that New Relic collects which is a pretty big downside when you want",
    "start": "320300",
    "end": "325370"
  },
  {
    "text": "to have a good dashboard view for an on call incident response",
    "start": "325370",
    "end": "331150"
  },
  {
    "text": "so it's great for general browsing but not for incident response",
    "start": "331150",
    "end": "336910"
  },
  {
    "start": "336000",
    "end": "411000"
  },
  {
    "text": "another thing that the mothership had is stats T because the rails model the",
    "start": "336910",
    "end": "342710"
  },
  {
    "text": "rails app is basically single threaded there's no way to get to it other than",
    "start": "342710",
    "end": "348980"
  },
  {
    "text": "it's HTTP interface but that's blocked responding to requests so",
    "start": "348980",
    "end": "355120"
  },
  {
    "text": "early on before I worked on them on this there had already been some work to",
    "start": "355120",
    "end": "361520"
  },
  {
    "text": "instrument the mothership with stats T but there are downsides to this in the original version",
    "start": "361520",
    "end": "368950"
  },
  {
    "text": "all of the rails binaries were sending their metrics over UDP to a single",
    "start": "368950",
    "end": "375860"
  },
  {
    "text": "collector in production and it turned out that 70% of the samples were being",
    "start": "375860",
    "end": "382340"
  },
  {
    "text": "lost but because of UDP drops over the network and one of the one of",
    "start": "382340",
    "end": "388550"
  },
  {
    "text": "the Prometheus developers actually fixed this by adding stats the aggregators on",
    "start": "388550",
    "end": "394910"
  },
  {
    "text": "every node but so now we had and then building forwarders but we were still losing data it wasn't accurate so there",
    "start": "394910",
    "end": "403220"
  },
  {
    "text": "is there's a big downside to the way stats these UDP protocol handles data",
    "start": "403220",
    "end": "409800"
  },
  {
    "text": "[Music] and then stats T we would take all the data from",
    "start": "409800",
    "end": "416900"
  },
  {
    "start": "411000",
    "end": "495000"
  },
  {
    "text": "stats T and send it into graphite we'd also take note level metrics from ganglia plugins and send that into",
    "start": "416900",
    "end": "423560"
  },
  {
    "text": "graphite and that was actually not too bad I got a lot of really good",
    "start": "423560",
    "end": "428830"
  },
  {
    "text": "instrumentation done of mothership by building new views into the graphite",
    "start": "428830",
    "end": "434030"
  },
  {
    "text": "data because we had all this data but nobody had really put the time into",
    "start": "434030",
    "end": "439490"
  },
  {
    "text": "thinking about how to visualize that and I got us about half of the way to a",
    "start": "439490",
    "end": "444740"
  },
  {
    "text": "really nice setup just by using Graphite's data but graphite is not fast enough to",
    "start": "444740",
    "end": "454730"
  },
  {
    "text": "keep up with the amount of servers we have the mothership runs on about",
    "start": "454730",
    "end": "460480"
  },
  {
    "text": "200-250 servers full bare metal hard working using lots of core time",
    "start": "460480",
    "end": "468910"
  },
  {
    "text": "handling tens of thousands of requests per second so graphite couldn't keep up with the",
    "start": "470230",
    "end": "476390"
  },
  {
    "text": "amount of data we wanted to collect to really get good insight into into what was going on plus the data",
    "start": "476390",
    "end": "484070"
  },
  {
    "text": "model is a bit limiting and if if you compare the Prometheus data model to the graphite data model",
    "start": "484070",
    "end": "490280"
  },
  {
    "text": "everybody knows that the labeling is just way better another one that",
    "start": "490280",
    "end": "498040"
  },
  {
    "start": "495000",
    "end": "590000"
  },
  {
    "text": "instruments mothership is that air brake air brake is completely unrelated to",
    "start": "498040",
    "end": "503720"
  },
  {
    "text": "Prometheus for me theist it simply forwards exceptions that so code crashes",
    "start": "503720",
    "end": "510080"
  },
  {
    "text": "within this within mothership it forwards that off to air brake this is an unsolved problem for us because",
    "start": "510080",
    "end": "517090"
  },
  {
    "text": "Prometheus doesn't do this it's not what Prometheus for so it's still actually quite useful to",
    "start": "517090",
    "end": "523099"
  },
  {
    "text": "use air brake for monitoring but I finally some people really like it I",
    "start": "523099",
    "end": "529400"
  },
  {
    "text": "find the user experience as an engineer highly frustrating",
    "start": "529400",
    "end": "534820"
  },
  {
    "text": "the the most hilarious thing is if you get a spike in exceptions on their graph",
    "start": "534820",
    "end": "542260"
  },
  {
    "text": "you can't click on it it's completely useless it won't actually you can't you",
    "start": "542260",
    "end": "547730"
  },
  {
    "text": "can't even select a time range to get that data to find out what happened in that air break window like there's a",
    "start": "547730",
    "end": "554510"
  },
  {
    "text": "spike but what did what is it no I don't know you can and it only shows you by",
    "start": "554510",
    "end": "560390"
  },
  {
    "text": "latest exception which could like the small low frequency exceptions can just",
    "start": "560390",
    "end": "567980"
  },
  {
    "text": "scroll off the high frequency exceptions you don't know you don't actually know what happened and you end up Tate you",
    "start": "567980",
    "end": "573410"
  },
  {
    "text": "end up spending three times the amount of work chasing down was this was this the one that was",
    "start": "573410",
    "end": "579860"
  },
  {
    "text": "in the spike was this the one that was in the spike was this one that was in the spike you just you just don't know",
    "start": "579860",
    "end": "584889"
  },
  {
    "text": "but unfortunately that's not something we can with fixed with prometheus and then to",
    "start": "584889",
    "end": "592790"
  },
  {
    "start": "590000",
    "end": "663000"
  },
  {
    "text": "actually get alerts out of the mothership we have Nagios and Nagios is everybody's favorite thing no it's a",
    "start": "592790",
    "end": "602510"
  },
  {
    "text": "pretty terrible broken data model for alerting it's super slow it's super annoying to",
    "start": "602510",
    "end": "608240"
  },
  {
    "text": "configure but we were able to get some of things like using check HTTP to monitor the",
    "start": "608240",
    "end": "615740"
  },
  {
    "text": "passenger engine ex passengers that would run the Ruby on Rails and then we",
    "start": "615740",
    "end": "621230"
  },
  {
    "text": "would use check mutant to go into our H a we had mutant stats for H a proxy and",
    "start": "621230",
    "end": "627380"
  },
  {
    "text": "that was pretty much it so we could tell that we were sending errors from H a proxy",
    "start": "627380",
    "end": "633519"
  },
  {
    "text": "and we could tell that the passenger was running but that was pretty much it",
    "start": "633519",
    "end": "639850"
  },
  {
    "text": "we you couldn't actually get information about what was broken so you you would",
    "start": "639850",
    "end": "645500"
  },
  {
    "text": "have to go and tail logs and and look at things or go to New Relic and try and",
    "start": "645500",
    "end": "652640"
  },
  {
    "text": "figure out from from from logs and new relic what was actually going on so",
    "start": "652640",
    "end": "658510"
  },
  {
    "text": "prometheus is what we wanted to get the mothership back up and floating",
    "start": "658510",
    "end": "663790"
  },
  {
    "start": "663000",
    "end": "682000"
  },
  {
    "text": "so Prometheus for the people that don't know as a time",
    "start": "663790",
    "end": "669740"
  },
  {
    "text": "series based monitoring system it pulls all the metrics that you it",
    "start": "669740",
    "end": "675290"
  },
  {
    "text": "pulls all the endpoints of the data that you want and aggregates it and allows",
    "start": "675290",
    "end": "680510"
  },
  {
    "text": "you to write alerts and graphs the first thing that we instrumented in",
    "start": "680510",
    "end": "686110"
  },
  {
    "text": "the mothership was we want to instrument the database that runs the mothership so",
    "start": "686110",
    "end": "691610"
  },
  {
    "text": "the my sequel database we wrote the my sequel exporter and this gave us really",
    "start": "691610",
    "end": "696829"
  },
  {
    "text": "really deep insight into what was going on on the database side so that we could improve",
    "start": "696829",
    "end": "702100"
  },
  {
    "text": "performance and latency from the databases perspective",
    "start": "702100",
    "end": "706930"
  },
  {
    "text": "the performance my seagulls performance schema is a hugely hugely valuable",
    "start": "707260",
    "end": "714420"
  },
  {
    "start": "714000",
    "end": "732000"
  },
  {
    "text": "data mine but it's really hard to look at because the only way it presents it",
    "start": "715800",
    "end": "721060"
  },
  {
    "text": "is in SQL so you have to do select States minutes to get it against it and this is really not useful so Prometheus",
    "start": "721060",
    "end": "727839"
  },
  {
    "text": "what collects that data turns it into a time series and now I can do stuff like this",
    "start": "727839",
    "end": "734260"
  },
  {
    "start": "732000",
    "end": "740000"
  },
  {
    "text": "so this is a performance schema example where I can take and look at the",
    "start": "734260",
    "end": "740580"
  },
  {
    "start": "740000",
    "end": "799000"
  },
  {
    "text": "average number of rows examined versus the average number of statements",
    "start": "740580",
    "end": "748750"
  },
  {
    "text": "so now I can get a graph of how many rows per query are required for a",
    "start": "748750",
    "end": "756820"
  },
  {
    "text": "particular query and this tells you an approximate cost for that query and now",
    "start": "756820",
    "end": "763029"
  },
  {
    "text": "I can look at a graph and see oh these are the top 10 select statements that cause me a super expensive I think the",
    "start": "763029",
    "end": "770470"
  },
  {
    "text": "number over there is something and they'd like 400 rows per query are needed to resolve that query and that",
    "start": "770470",
    "end": "775750"
  },
  {
    "text": "could that can be and and there there are periodic problems there's constant problems and there's periodic problems",
    "start": "775750",
    "end": "781779"
  },
  {
    "text": "so we can go and look and see well is there a bad API or a bad",
    "start": "781779",
    "end": "788070"
  },
  {
    "text": "index or is there somebody scraping us we don't we we can now have a target for",
    "start": "788070",
    "end": "794680"
  },
  {
    "text": "investigation based on the tables affected by these the next thing I did is a total hack",
    "start": "794680",
    "end": "803520"
  },
  {
    "start": "799000",
    "end": "836000"
  },
  {
    "text": "a Google engineer wrote this really nice tool called M tail and M tail just tails",
    "start": "803520",
    "end": "810310"
  },
  {
    "text": "your log file parses it with reg X matching and turns those reg X match",
    "start": "810310",
    "end": "815520"
  },
  {
    "text": "matches into counts so now I can count every log line and but with Prometheus I",
    "start": "815520",
    "end": "824200"
  },
  {
    "text": "don't just get the count of log lines I can pull out little bits of data from each log line using the regular",
    "start": "824200",
    "end": "830290"
  },
  {
    "text": "expression pattern-matching and put those into labels so now",
    "start": "830290",
    "end": "835440"
  },
  {
    "text": "with a little bit of tweaking of the rails log I can now say well I have the info well",
    "start": "835440",
    "end": "843550"
  },
  {
    "start": "836000",
    "end": "864000"
  },
  {
    "text": "sometimes it's info sometimes it's worn so now I can have counts of log lines by info worn I can take the",
    "start": "843550",
    "end": "852840"
  },
  {
    "text": "time out of the time stamp there and actually have a",
    "start": "852840",
    "end": "858520"
  },
  {
    "text": "count of time spent so now I can get average Layton sees and then a little bit and plus of course we have the",
    "start": "858520",
    "end": "865029"
  },
  {
    "start": "864000",
    "end": "905000"
  },
  {
    "text": "status code so we know whether we're sending a two hundred or a four oh four or a five hundred or whatever the HTTP",
    "start": "865029",
    "end": "871000"
  },
  {
    "text": "status code is returned by the rails controller and then",
    "start": "871000",
    "end": "876029"
  },
  {
    "text": "with a little bit of tweaking to the rails code we appended the rails controller and",
    "start": "876029",
    "end": "881970"
  },
  {
    "text": "action so basically what's what function within rails was being called and what",
    "start": "881970",
    "end": "888670"
  },
  {
    "text": "mode it was being requested in and so now we have we have latency errors responses by",
    "start": "888670",
    "end": "898930"
  },
  {
    "text": "action by controller so now we have all of these dimensions and we are now we have time series for every single one of",
    "start": "898930",
    "end": "905050"
  },
  {
    "start": "905000",
    "end": "926000"
  },
  {
    "text": "those and we can now take these with this M tale",
    "start": "905050",
    "end": "911880"
  },
  {
    "text": "configurations this is a contrived version of what was actually in our production configuration and we",
    "start": "911880",
    "end": "919210"
  },
  {
    "text": "pull all those things out and now we have all these labeled",
    "start": "919210",
    "end": "923820"
  },
  {
    "start": "926000",
    "end": "957000"
  },
  {
    "text": "values and we can actually start writing alerts and now we can get we can",
    "start": "926370",
    "end": "932560"
  },
  {
    "text": "actually get an alert for a single controller and a single action from",
    "start": "932560",
    "end": "937780"
  },
  {
    "text": "every instance of our rails app and and know immediately what's going wrong",
    "start": "937780",
    "end": "946020"
  },
  {
    "text": "instead of going oh mothership is returning errors I get but I better spend half an hour",
    "start": "946020",
    "end": "952060"
  },
  {
    "text": "investigating now I just get an email that tells me exactly what controllers malfunctioning",
    "start": "952060",
    "end": "958139"
  },
  {
    "start": "957000",
    "end": "1098000"
  },
  {
    "text": "here's a dashboard example of looking into a single specific controller so you can",
    "start": "958680",
    "end": "965320"
  },
  {
    "text": "see the responses by control by action you can see the response codes by coming",
    "start": "965320",
    "end": "973310"
  },
  {
    "text": "from the controller and we know and we have a latency graph down on the bottom here that tells us",
    "start": "973310",
    "end": "979269"
  },
  {
    "text": "the latency by action so I can see that Oh index index actions are nice and",
    "start": "979269",
    "end": "987589"
  },
  {
    "text": "fine there's a little bit of noise in the in the create and there's actually",
    "start": "987589",
    "end": "994399"
  },
  {
    "text": "quite a bit of noise but maybe that's because there's not so many there's some smoothness problems with",
    "start": "994399",
    "end": "1000449"
  },
  {
    "text": "when you're trying to do average latency x' but this this particular controller handles people posting",
    "start": "1000449",
    "end": "1007449"
  },
  {
    "text": "comments on tracks so we get 200 you know at this point in time there's 200 requests per second to post a comma so",
    "start": "1007449",
    "end": "1013899"
  },
  {
    "text": "to look at comments for track and how and maybe a few per second of people",
    "start": "1013899",
    "end": "1019930"
  },
  {
    "text": "posting new comments and then we have a a DIN error and that's actually a",
    "start": "1019930",
    "end": "1028350"
  },
  {
    "text": "different prometheus query that tells us the increase of errors and over the last five minutes and then it's blocked in",
    "start": "1028350",
    "end": "1034600"
  },
  {
    "text": "five minute blocks so you can you can really see if you have a very low error rate you can now see that super low",
    "start": "1034600",
    "end": "1041949"
  },
  {
    "text": "error rate because Prometheus allows that flexibility in query language",
    "start": "1041949",
    "end": "1048428"
  },
  {
    "text": "the downside to em tail here is it's super expensive from a cpu",
    "start": "1048429",
    "end": "1056429"
  },
  {
    "text": "perspective it costs about 10 to 15% of motherships CPU capacity to just tail",
    "start": "1057480",
    "end": "1063970"
  },
  {
    "text": "and process all those logs so for for every one of our every one of our",
    "start": "1063970",
    "end": "1070090"
  },
  {
    "text": "servers if it's my if it's running 10 cores of rails its running one core just flat-out reg xing logs because we're",
    "start": "1070090",
    "end": "1077559"
  },
  {
    "text": "generating tens of megabytes of logs per minute per node so 200 nodes is we're generating",
    "start": "1077559",
    "end": "1085059"
  },
  {
    "text": "gigabytes per you know order order magnitude gigabytes per minute of logs from these things but",
    "start": "1085059",
    "end": "1091600"
  },
  {
    "text": "because it's M tails distributed on every node it's actually pretty scalable",
    "start": "1091600",
    "end": "1098880"
  },
  {
    "start": "1098000",
    "end": "1148000"
  },
  {
    "text": "Stastny actually is a totally great way to get data out of it out of something",
    "start": "1100560",
    "end": "1106230"
  },
  {
    "text": "that is a single-threaded so if you've got PHP",
    "start": "1106230",
    "end": "1111660"
  },
  {
    "text": "code or rails code that's single threaded or some other thing that",
    "start": "1111660",
    "end": "1116840"
  },
  {
    "text": "can't isn't a persistently running app or purse it doesn't have any persistence",
    "start": "1116990",
    "end": "1122840"
  },
  {
    "text": "threads stat the sending out to that local host aggregator is actually pretty",
    "start": "1122840",
    "end": "1129030"
  },
  {
    "text": "nice because we can take that those stats the metrics use the Prometheus stats the exporter and turn those into",
    "start": "1129030",
    "end": "1135500"
  },
  {
    "text": "counts that Prometheus can come and collect and then graph in time series but the trees are a little messy and you",
    "start": "1135500",
    "end": "1143370"
  },
  {
    "text": "have to be very you have to be much more diligent in your code to do this so I",
    "start": "1143370",
    "end": "1149160"
  },
  {
    "text": "had to do a bit of code cleanup to actually make some of these stats D metrics useful so we had some met",
    "start": "1149160",
    "end": "1155850"
  },
  {
    "text": "metrics for memcache usage and there's one piece of code that would count the",
    "start": "1155850",
    "end": "1161010"
  },
  {
    "text": "number of hits and count the number of misses then there's a different piece of code that would count the number of lookups and the number of misses well",
    "start": "1161010",
    "end": "1167930"
  },
  {
    "text": "lookups is the sum of hits and misses so I had that we had to fix that plus",
    "start": "1167930",
    "end": "1175080"
  },
  {
    "text": "there's not enough context in here it's like wait user or user whitelisted what is it where is that coming from so it",
    "start": "1175080",
    "end": "1181350"
  },
  {
    "text": "turns out it was actually cut well one was coming from a rails model and one was coming from a rail service and it's",
    "start": "1181350",
    "end": "1188270"
  },
  {
    "text": "that's that's real specific things but now I started cleaning up all this",
    "start": "1188270",
    "end": "1196430"
  },
  {
    "text": "stats D names and move the the hit to actually count lookups so now",
    "start": "1196430",
    "end": "1203880"
  },
  {
    "text": "we have consistent lookups and mis metrics and then I use the stats T",
    "start": "1203880",
    "end": "1210240"
  },
  {
    "text": "mapping in the stats the exporter to turn that into a key value labels for",
    "start": "1210240",
    "end": "1216300"
  },
  {
    "text": "Prometheus to collect and I don't know if I have any good graphs",
    "start": "1216300",
    "end": "1222120"
  },
  {
    "start": "1221000",
    "end": "1289000"
  },
  {
    "text": "of that one but it it it turned out to be really useful so we can we could actually start alerting in Prometheus on",
    "start": "1222120",
    "end": "1228060"
  },
  {
    "text": "cache miss problems here's the example of a New Relic thing",
    "start": "1228060",
    "end": "1234070"
  },
  {
    "text": "it's actually quite a nice interface but you can't customize it as as much as I'd",
    "start": "1234070",
    "end": "1241990"
  },
  {
    "text": "want and then here's this similar like application overview and it gives me the",
    "start": "1241990",
    "end": "1248320"
  },
  {
    "text": "things that I really actually care about which is which can which top controllers are having errors so I can immediately",
    "start": "1248320",
    "end": "1254890"
  },
  {
    "text": "look at a graph and see which controllers are causing any problems and then I can also looking at latency so",
    "start": "1254890",
    "end": "1262049"
  },
  {
    "text": "whereas New Relic tends to focus on their apdex score the overall throughput",
    "start": "1262049",
    "end": "1267190"
  },
  {
    "text": "of the entire thing and then but it really does have this nice stacked latency graph for different subsystems",
    "start": "1267190",
    "end": "1273730"
  },
  {
    "text": "so that's that's one thing I haven't been able to reproduce in Prometheus yet but I probably could if I actually went",
    "start": "1273730",
    "end": "1280360"
  },
  {
    "text": "into the rails code and and added some stats D metrics on this latency to get",
    "start": "1280360",
    "end": "1286269"
  },
  {
    "text": "that data out so what's coming up in the future for mothership I",
    "start": "1286269",
    "end": "1292679"
  },
  {
    "start": "1292000",
    "end": "1324000"
  },
  {
    "text": "right now we collect latency just average latency but improvements to the M tail code now now",
    "start": "1292679",
    "end": "1299470"
  },
  {
    "text": "allow us to actually add histogram support so we can we can now bucket eyes the latency response for different",
    "start": "1299470",
    "end": "1305649"
  },
  {
    "text": "controllers I want to continue doing the stats D cleanup we want to stop sending we still send",
    "start": "1305649",
    "end": "1312399"
  },
  {
    "text": "the stats T to graphite but we don't nobody looks at it anymore so we just need to turn that off and I finally want",
    "start": "1312399",
    "end": "1318399"
  },
  {
    "text": "to remove that last bit of New Relic oops that's done and just did that yesterday",
    "start": "1318399",
    "end": "1324450"
  },
  {
    "start": "1324000",
    "end": "1356000"
  },
  {
    "text": "all right any questions",
    "start": "1324450",
    "end": "1328950"
  },
  {
    "text": "no so the stats D so that is there a library to swap out stats D",
    "start": "1333420",
    "end": "1339500"
  },
  {
    "text": "well so there's the Prometheus library so if your code is if you can load the",
    "start": "1339500",
    "end": "1344640"
  },
  {
    "text": "Prometheus library in your code you can swap out stats T that way but of course you'll have to it's not the same exact",
    "start": "1344640",
    "end": "1351780"
  },
  {
    "text": "semantics depending on your programming language it's not it's not exactly it's",
    "start": "1351780",
    "end": "1357000"
  },
  {
    "start": "1356000",
    "end": "1405000"
  },
  {
    "text": "it's you would use it the same way you do increments and you do sets so you set",
    "start": "1357000",
    "end": "1363180"
  },
  {
    "text": "a metric or you increment a metric or you or you but it's not it's not exactly this not",
    "start": "1363180",
    "end": "1369000"
  },
  {
    "text": "exactly the same so you would have to add it in or or do a code search and replace",
    "start": "1369000",
    "end": "1374960"
  },
  {
    "text": "but what we were doing is just using stats the out to a",
    "start": "1374960",
    "end": "1380870"
  },
  {
    "text": "node local stats the aggregator",
    "start": "1380870",
    "end": "1385220"
  },
  {
    "text": "[Music]",
    "start": "1395330",
    "end": "1398440"
  },
  {
    "start": "1405000",
    "end": "1513000"
  },
  {
    "text": "yeah so the question is are there are there deeper recipes and other best practices",
    "start": "1405160",
    "end": "1410720"
  },
  {
    "text": "for producing alert able alerting queries",
    "start": "1410720",
    "end": "1417970"
  },
  {
    "text": "yes and no you you touched on a couple of them like the the apdex score",
    "start": "1417970",
    "end": "1424660"
  },
  {
    "text": "no there aren't early on and the Prometheus days we",
    "start": "1425710",
    "end": "1431090"
  },
  {
    "text": "decided to leave that for later because we didn't want people copying and pasting bad ideas",
    "start": "1431090",
    "end": "1437200"
  },
  {
    "text": "so right now it's really think about your SLO s think about your",
    "start": "1437200",
    "end": "1443570"
  },
  {
    "text": "SLA is what it what what things matter to your users and what things matter to",
    "start": "1443570",
    "end": "1450200"
  },
  {
    "text": "your engineers and you and just drop put draw a line in the sand and if it's over",
    "start": "1450200",
    "end": "1457880"
  },
  {
    "text": "that line that's an alert so for example high persistent latency high persistent",
    "start": "1457880",
    "end": "1463490"
  },
  {
    "text": "errors large spikes and errors those those kinds of things that's a lot",
    "start": "1463490",
    "end": "1471080"
  },
  {
    "text": "of that it is actually really well covered in the Google s or ebook is there are lots and lots of things you",
    "start": "1471080",
    "end": "1476930"
  },
  {
    "text": "could alert on but you really only want to pick the things that matter to your users",
    "start": "1476930",
    "end": "1482770"
  },
  {
    "text": "next question you see my is we feel silly but you",
    "start": "1482770",
    "end": "1489910"
  },
  {
    "text": "realize you get the same granularity some reason",
    "start": "1489910",
    "end": "1497580"
  },
  {
    "text": "we haven't turned really do crap though we did just crazy granularity as to like what's happening",
    "start": "1497580",
    "end": "1504380"
  },
  {
    "text": "in rails what's happening - true",
    "start": "1504380",
    "end": "1508360"
  },
  {
    "text": "yeah but the that's yeah so so",
    "start": "1509629",
    "end": "1516679"
  },
  {
    "start": "1513000",
    "end": "1608000"
  },
  {
    "text": "the question is how do you how do you come overcome some",
    "start": "1516679",
    "end": "1522539"
  },
  {
    "text": "of the the extra bits that you get in the New Relic model and the the big one",
    "start": "1522539",
    "end": "1528659"
  },
  {
    "text": "for me that was is like the the sample events stuff in",
    "start": "1528659",
    "end": "1535019"
  },
  {
    "text": "New Relic is actually really useful and",
    "start": "1535019",
    "end": "1539869"
  },
  {
    "text": "what do we do we were kind of just living without it",
    "start": "1540109",
    "end": "1546440"
  },
  {
    "text": "and yeah so yeah so the question is open tracing yes I open tracing is probably",
    "start": "1548179",
    "end": "1556049"
  },
  {
    "text": "the answer there or some some kind of tracing or something like air break",
    "start": "1556049",
    "end": "1562879"
  },
  {
    "text": "but yet having having having the logs is and what were",
    "start": "1562879",
    "end": "1569730"
  },
  {
    "text": "what we're working on right now is just making really easily searchable logs so",
    "start": "1569730",
    "end": "1574950"
  },
  {
    "text": "we try and just up the logging verbosity and put as much information into the logs",
    "start": "1574950",
    "end": "1581519"
  },
  {
    "text": "but only keep logs for a very short amount of time so that we don't overwhelm our log in cluster and that",
    "start": "1581519",
    "end": "1590399"
  },
  {
    "text": "way we get that way we get the tracing in the logs and we're gonna we're building basically a",
    "start": "1590399",
    "end": "1596210"
  },
  {
    "text": "highly searchable logs cluster using Kafka and molasse textures",
    "start": "1596210",
    "end": "1602389"
  },
  {
    "text": "oh my god oh my god how is managing prometheus compared to",
    "start": "1607160",
    "end": "1613070"
  },
  {
    "start": "1608000",
    "end": "1687000"
  },
  {
    "text": "graphite that is entirely why we wrote Prometheus in the first place was it's",
    "start": "1613070",
    "end": "1619370"
  },
  {
    "text": "super easy to run a Prometheus server you give it a config file you start it",
    "start": "1619370",
    "end": "1624830"
  },
  {
    "text": "up you're done that that's it's it's one binary it reads its config file and you",
    "start": "1624830",
    "end": "1633020"
  },
  {
    "text": "just let it do its thing it's it's just hilariously easy to run",
    "start": "1633020",
    "end": "1638120"
  },
  {
    "text": "so we we end up we now in it because it's so hilariously easy to operate prometheus we have 30 of them monitoring",
    "start": "1638120",
    "end": "1645710"
  },
  {
    "text": "different applications so there instead of having one big giant graphite that",
    "start": "1645710",
    "end": "1651770"
  },
  {
    "text": "does everything and it's like nobody wants to touch it nobody upgrades it it just sits there no we just have this",
    "start": "1651770",
    "end": "1659240"
  },
  {
    "text": "Prometheus monitors this app this one does this app this one does this app and if they break well we don't care we just",
    "start": "1659240",
    "end": "1666140"
  },
  {
    "text": "hit a button and a new one pops up so we it's it's so much easier to",
    "start": "1666140",
    "end": "1672080"
  },
  {
    "text": "operate",
    "start": "1672080",
    "end": "1674350"
  },
  {
    "text": "looking calm as this crazy cluster yeah",
    "start": "1678780",
    "end": "1685109"
  },
  {
    "start": "1687000",
    "end": "1737000"
  },
  {
    "text": "yep yeah so the yes you couldn't can automate graphite",
    "start": "1687500",
    "end": "1693710"
  },
  {
    "text": "but it's it was it's still a little bit more work",
    "start": "1693710",
    "end": "1700799"
  },
  {
    "text": "from what I've seen and previous we don't we didn't have like the Prometheus",
    "start": "1700799",
    "end": "1706140"
  },
  {
    "text": "itself didn't have to do much work and it can handle significant amount more data you know the the Prometheus server",
    "start": "1706140",
    "end": "1713520"
  },
  {
    "text": "that's handling the mothership is ingesting I forget I",
    "start": "1713520",
    "end": "1719460"
  },
  {
    "text": "think it was about it's it's it's actually only about 600,000 time series",
    "start": "1719460",
    "end": "1724500"
  },
  {
    "text": "for that server but and it's but its handling about 50,000 samples per second and it's idle",
    "start": "1724500",
    "end": "1732380"
  },
  {
    "text": "yeah",
    "start": "1732380",
    "end": "1735380"
  },
  {
    "start": "1737000",
    "end": "1773000"
  },
  {
    "text": "what is the memory of memory usage for that Prometheus I don't know I'd have to log into the cluster maybe",
    "start": "1737840",
    "end": "1746659"
  },
  {
    "text": "I don't let me let me let me ask",
    "start": "1747470",
    "end": "1752270"
  },
  {
    "text": "Hey yeah there's a little bit of",
    "start": "1771000",
    "end": "1776160"
  },
  {
    "start": "1773000",
    "end": "1833000"
  },
  {
    "text": "configuration for bigger servers but it it's it's pretty trivial and it's all templated in our",
    "start": "1776160",
    "end": "1781400"
  },
  {
    "text": "in our deployment but I",
    "start": "1781400",
    "end": "1787550"
  },
  {
    "text": "find out that should be a relatively easy",
    "start": "1789710",
    "end": "1796530"
  },
  {
    "text": "question to answer",
    "start": "1796530",
    "end": "1799190"
  },
  {
    "text": "this Prometheus server is only using about 17 gigs of RAM for time series and",
    "start": "1802429",
    "end": "1809970"
  },
  {
    "text": "that that's and it's it's",
    "start": "1809970",
    "end": "1814820"
  },
  {
    "text": "yeah we I think we have a I think we have a two or three month retention and it's only four hundred gigs of disk",
    "start": "1815120",
    "end": "1820620"
  },
  {
    "text": "space so it's one",
    "start": "1820620",
    "end": "1824419"
  },
  {
    "text": "say what [Music]",
    "start": "1827730",
    "end": "1832660"
  },
  {
    "start": "1833000",
    "end": "1889000"
  },
  {
    "text": "so this is a difference between Prometheus and graphite Prometheus does not downsample so we have full full time",
    "start": "1833739",
    "end": "1840639"
  },
  {
    "text": "series data for the entire two months retention period on that server",
    "start": "1840639",
    "end": "1847739"
  },
  {
    "text": "so actually let me let me let me tell you exactly",
    "start": "1847739",
    "end": "1853950"
  },
  {
    "text": "this one is storing",
    "start": "1860489",
    "end": "1865409"
  },
  {
    "text": "oh this one is actually storing is that right oh this one is storing one",
    "start": "1867570",
    "end": "1874539"
  },
  {
    "text": "year of data and it's only using four hundred fifty gigabytes of SSD space on this server",
    "start": "1874539",
    "end": "1884039"
  },
  {
    "start": "1889000",
    "end": "1929000"
  },
  {
    "text": "so the question is what we're using for alerting we were using Nagios and now we're using",
    "start": "1890010",
    "end": "1895020"
  },
  {
    "text": "prometheus alerting and",
    "start": "1895020",
    "end": "1898100"
  },
  {
    "text": "but yeah so the performance wise this Prometheus server",
    "start": "1900680",
    "end": "1906960"
  },
  {
    "text": "is losing less than one core and",
    "start": "1906960",
    "end": "1910520"
  },
  {
    "text": "come on Wi-Fi slow",
    "start": "1920630",
    "end": "1925510"
  },
  {
    "start": "1929000",
    "end": "1969000"
  },
  {
    "text": "oh this this for me that we are only only really need about 300,000 time series from from the mothership and",
    "start": "1930250",
    "end": "1938460"
  },
  {
    "text": "we're so we're where this is not a very big Prometheus server compared to say",
    "start": "1938460",
    "end": "1945250"
  },
  {
    "text": "our global my sequel database Prometheus server that's doing a million and a half time series",
    "start": "1945250",
    "end": "1952530"
  },
  {
    "text": "so that's another nice thing as we ice we can isolate with Prometheus we isolate different",
    "start": "1953820",
    "end": "1961390"
  },
  {
    "text": "applications on different servers another question",
    "start": "1961390",
    "end": "1966480"
  },
  {
    "start": "1969000",
    "end": "2029000"
  },
  {
    "text": "that's a good question what are what is the bulk of all that data it is yeah a",
    "start": "1970550",
    "end": "1976760"
  },
  {
    "text": "lot of a lot of time series for for those endpoint metrics because I",
    "start": "1976760",
    "end": "1981980"
  },
  {
    "text": "think there's something like 30 or 40 controllers with however many actions and you multiply that because we have a",
    "start": "1981980",
    "end": "1989210"
  },
  {
    "text": "time series for every single rails server that's running so it's actually quite a lot of",
    "start": "1989210",
    "end": "1994929"
  },
  {
    "text": "granularity but this is nice because I can either get an overall view of errors",
    "start": "1994929",
    "end": "2002160"
  },
  {
    "text": "per controller or I can get an individual servers errors per controller so I can find individual servers that",
    "start": "2002160",
    "end": "2009250"
  },
  {
    "text": "might have a broken network problem or some other some other corrupted file",
    "start": "2009250",
    "end": "2015870"
  },
  {
    "text": "yeah yeah basically so that how do you not",
    "start": "2026180",
    "end": "2033590"
  },
  {
    "text": "annihilate your Prometheus server basically anything that we put on a dashboard",
    "start": "2033590",
    "end": "2039310"
  },
  {
    "text": "eventually becomes a recording rule so the dashboard just pulls from that that pre-recorded data which makes the",
    "start": "2039310",
    "end": "2045860"
  },
  {
    "text": "dashboard super fast but really 600,000 time series seven is not",
    "start": "2045860",
    "end": "2052460"
  },
  {
    "text": "actually that much for Prometheus it's totally an ax normal",
    "start": "2052460",
    "end": "2057639"
  },
  {
    "text": "normal load load level for Prometheus although we do run on we do run our Prometheus on",
    "start": "2057640",
    "end": "2064850"
  },
  {
    "text": "bare metal so we have pretty beefy 32 core servers with 64 gigs of RAM to run",
    "start": "2064850",
    "end": "2071360"
  },
  {
    "text": "that from efya server but even if we didn't have that it's still",
    "start": "2071360",
    "end": "2076720"
  },
  {
    "text": "we kind of we just leave it totally on tuned and it's still only using 17 gigs of ram so if we were to",
    "start": "2076720",
    "end": "2084460"
  },
  {
    "text": "if we were to squeeze that down we probably could cut that resource usage in half it's just we have we have there's no",
    "start": "2084460",
    "end": "2092450"
  },
  {
    "text": "motivation there any other questions",
    "start": "2092450",
    "end": "2098360"
  },
  {
    "text": "all right thanks [Applause]",
    "start": "2098360",
    "end": "2104089"
  }
]