[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "hello hi everyone today I'm gonna talk about a journey so far to try to cognize",
    "start": "0",
    "end": "8429"
  },
  {
    "text": "big data and ml workload at uber as you",
    "start": "8429",
    "end": "15000"
  },
  {
    "text": "know always actually become a pretty fast growing and a big company we",
    "start": "15000",
    "end": "20430"
  },
  {
    "text": "wouldn't IPO this year and the Carosa is pretty big we have like 15 billion trips",
    "start": "20430",
    "end": "28580"
  },
  {
    "text": "so far if you remember last year when we had the talk we are like 10 billion trips so just like one year like we",
    "start": "28580",
    "end": "35730"
  },
  {
    "text": "record like 5 billion trips and we do like 15 million trips per day and we're",
    "start": "35730",
    "end": "41940"
  },
  {
    "text": "a global company different compared to like our competitor we are like about a",
    "start": "41940",
    "end": "47399"
  },
  {
    "text": "global company wearing six continents and the 65 countries and 700 cities we",
    "start": "47399",
    "end": "54629"
  },
  {
    "text": "have like hungry mini active monthly users and we are serving almost close to",
    "start": "54629",
    "end": "60239"
  },
  {
    "text": "4 meeting active drivers so a bigger data in ml is very important for a",
    "start": "60239",
    "end": "66540"
  },
  {
    "start": "63000",
    "end": "63000"
  },
  {
    "text": "company like uber because that's kind of how we can potentially become profitable",
    "start": "66540",
    "end": "72030"
  },
  {
    "text": "in the future so we are like a using big data and MMA in all different areas like",
    "start": "72030",
    "end": "77280"
  },
  {
    "text": "uber heats etas safe German cars and like lots of use case I'm gonna quickly",
    "start": "77280",
    "end": "83070"
  },
  {
    "text": "go through a couple of them for example one example is ETA as you know whenever",
    "start": "83070",
    "end": "88829"
  },
  {
    "text": "you open Google app you can tell you what's the pick pick up ETA what's the arrival ETA so those ETS are very",
    "start": "88829",
    "end": "96060"
  },
  {
    "text": "important not only from customer experience perspective also from like a pricing perspective so we are using",
    "start": "96060",
    "end": "104280"
  },
  {
    "text": "email models to predict the route based ETA arrows and use that to comprehend to",
    "start": "104280",
    "end": "109649"
  },
  {
    "text": "the arrows so that's actually very important for uber business and the next",
    "start": "109649",
    "end": "114689"
  },
  {
    "text": "one is like a fundamental to call of the uber business is how we matching drivers and riders so in that area there lots of",
    "start": "114689",
    "end": "121770"
  },
  {
    "text": "em your algorithms as well next we also have a new business line not regretting you but it's like faster",
    "start": "121770",
    "end": "128489"
  },
  {
    "text": "growing business file which is overheats we use a lot of our models",
    "start": "128489",
    "end": "133620"
  },
  {
    "text": "for ranking of restaurants and the dish delivery at times as well as the",
    "start": "133620",
    "end": "139170"
  },
  {
    "text": "searching ranking so like every time we open the eats homepage there's like more",
    "start": "139170",
    "end": "144239"
  },
  {
    "text": "than Hungry's of mm models killed court to render the page now of course we also",
    "start": "144239",
    "end": "152129"
  },
  {
    "text": "have a self-driving car units as you know like that's a Miyamoto's is fundamental to lots of aspect of the",
    "start": "152129",
    "end": "158819"
  },
  {
    "text": "self-driving car like a perception and also 3d mapping so let me quickly go",
    "start": "158819",
    "end": "166470"
  },
  {
    "start": "164000",
    "end": "164000"
  },
  {
    "text": "through introduce the uber speak data stack on the right side we have a TR the",
    "start": "166470",
    "end": "172680"
  },
  {
    "text": "data like basically we have a hot storage like HDFS and warm HDFS also as",
    "start": "172680",
    "end": "179760"
  },
  {
    "text": "aware of archived and in front of that we have some in memory database and then on the left side we have lots of events",
    "start": "179760",
    "end": "186239"
  },
  {
    "text": "which we from mobile app micro surveys database and so the party events and we",
    "start": "186239",
    "end": "193890"
  },
  {
    "text": "basically use a cop car to feed all those in through our data wakes and then we have a computer fabric today it's a",
    "start": "193890",
    "end": "201329"
  },
  {
    "text": "combination of young plus politic on my sauce if you're interesting about that",
    "start": "201329",
    "end": "206519"
  },
  {
    "text": "you can go back to our talk last year so but our plan is kind of to go to unify",
    "start": "206519",
    "end": "213120"
  },
  {
    "text": "others into become cognates plus palatini lay on top of the computer",
    "start": "213120",
    "end": "218910"
  },
  {
    "text": "fabric we have our data processing engines which we could impose streaming as well as batch processing we're using",
    "start": "218910",
    "end": "225180"
  },
  {
    "text": "Frink spark and intense they on top of",
    "start": "225180",
    "end": "230340"
  },
  {
    "text": "that we have our Curie engines which is like real-time engines like an antenna X as well as a high priest o then on top",
    "start": "230340",
    "end": "239129"
  },
  {
    "text": "of the current engines we have our data analytic tools like Piper which is oobs working of airflow",
    "start": "239129",
    "end": "245910"
  },
  {
    "text": "and the dashboards ad-hoc queries and the PA tours so as you can see compute",
    "start": "245910",
    "end": "251370"
  },
  {
    "text": "fabric is very important to Big Data because that's basically the core of the engine where all the big data is rolling",
    "start": "251370",
    "end": "257299"
  },
  {
    "text": "now let's move on to next one is MA so in America we have",
    "start": "257299",
    "end": "266450"
  },
  {
    "text": "a platform called McCann jela we're specifically trying to platformer eyes",
    "start": "266450",
    "end": "271760"
  },
  {
    "text": "the whole email work follows within OPA including data preparation photo typing",
    "start": "271760",
    "end": "276770"
  },
  {
    "text": "training as well as inference so the idea is like you know every email engineers can just do a couple clicks",
    "start": "276770",
    "end": "283730"
  },
  {
    "text": "and at the end they can specify what's the data sets and what's the models able",
    "start": "283730",
    "end": "289340"
  },
  {
    "text": "to use and then what's the algorithm they're gonna use energy we're gonna train all the models deploy the models everything you know automatic way in",
    "start": "289340",
    "end": "297440"
  },
  {
    "text": "this system actually as you can see the computer is also like a very fundamental to that not only from an etiquette",
    "start": "297440",
    "end": "305150"
  },
  {
    "text": "training perspective but as well as like inference or real-time prediction then",
    "start": "305150",
    "end": "310250"
  },
  {
    "text": "there's also like we have to support different training engines like there's a fellow PI torch XG pose to Spock email",
    "start": "310250",
    "end": "318099"
  },
  {
    "text": "so now the question is white cabinets you know we can say okay you can use",
    "start": "318820",
    "end": "323990"
  },
  {
    "text": "young or missus prosperity and that cause of the needs but why do we would move to companies so there's a first",
    "start": "323990",
    "end": "331510"
  },
  {
    "text": "important thing is there are lots of features and extensions in companies for",
    "start": "331510",
    "end": "336680"
  },
  {
    "text": "mixed the workload so Kublai's way it's designed it was for mixed double cloud right so basically support like a",
    "start": "336680",
    "end": "341810"
  },
  {
    "text": "deployment port stateful set a batch jobs as well as demons it and most",
    "start": "341810",
    "end": "348800"
  },
  {
    "text": "importantly the growing community okay everybody here can see like how",
    "start": "348800",
    "end": "353960"
  },
  {
    "text": "many attendees for cocoa it's growing every year and wide adoption of native",
    "start": "353960",
    "end": "360800"
  },
  {
    "text": "wide wide adoption as well native integrations from open source projects like spark also like cloud native",
    "start": "360800",
    "end": "367370"
  },
  {
    "text": "support and flexible extension models now the question is can we just use companies as these to replace young and",
    "start": "367370",
    "end": "374540"
  },
  {
    "text": "replace my sauce that is not really right there are a couple like a missing",
    "start": "374540",
    "end": "380420"
  },
  {
    "text": "pieces in today's community which is not as popular as a big data platform as a",
    "start": "380420",
    "end": "387650"
  },
  {
    "text": "young today in large-scale production like over there's a cup of gaps like",
    "start": "387650",
    "end": "393830"
  },
  {
    "text": "elastic resource sharing basically companies don't have a young cube concept so you have like names",
    "start": "393830",
    "end": "400110"
  },
  {
    "text": "you have quota but it was aesthetic if you specify and then some quota to my organization if it's an organisation",
    "start": "400110",
    "end": "405659"
  },
  {
    "text": "he's not using a quota you cannot really use that quota for other people now also like the folk and Sterling for",
    "start": "405659",
    "end": "413310"
  },
  {
    "text": "America loads and supporter like a batch and stick a bit smoke a little",
    "start": "413310",
    "end": "418379"
  },
  {
    "text": "collocation so this is not only just like put them in together but also make sure the batch workload is not",
    "start": "418379",
    "end": "424319"
  },
  {
    "text": "interfering the real-time workload or real-time micro-service workload they're",
    "start": "424319",
    "end": "431400"
  },
  {
    "text": "also like we want like if we want order place yeah are they we have to match the high throughput via which is like more",
    "start": "431400",
    "end": "437430"
  },
  {
    "text": "than a 1k per second Porter lunch what we observe today in uber",
    "start": "437430",
    "end": "444050"
  },
  {
    "text": "now there's other like that I make a porter location which is important for because we don't use really use DNS",
    "start": "444050",
    "end": "450569"
  },
  {
    "text": "because that's not gonna schedule like tens unload clusters or more instead we",
    "start": "450569",
    "end": "455639"
  },
  {
    "text": "have something called you open naming service which is very similar to like being nice from book so for those to",
    "start": "455639",
    "end": "464699"
  },
  {
    "text": "walk we actually have to like for every container you have to like a dynamic a look at the ports so before I go to the",
    "start": "464699",
    "end": "472469"
  },
  {
    "start": "470000",
    "end": "470000"
  },
  {
    "text": "more details let me quickly introduce like palliative so we had a talked about a part in last year so it is kind of a",
    "start": "472469",
    "end": "479400"
  },
  {
    "text": "unified resource schedule to then for mixed workload which has integrations on my sauce and also we are building the",
    "start": "479400",
    "end": "486719"
  },
  {
    "text": "integrations with cabinets as well but it's the current way actually that was a not talked in on Tuesday is basically",
    "start": "486719",
    "end": "494849"
  },
  {
    "text": "just have culminates as the plugging through peloton without touching too much of the high-level architecture of",
    "start": "494849",
    "end": "501389"
  },
  {
    "text": "peloton so this has like one limitation which is we still have the build order",
    "start": "501389",
    "end": "507449"
  },
  {
    "text": "to each driver for Politan it's ok for like a micro services because waiting",
    "start": "507449",
    "end": "513448"
  },
  {
    "text": "over we have our own deployment systems which can use part an API and then for big data and the Emirates can be",
    "start": "513449",
    "end": "519659"
  },
  {
    "text": "challenged for example we actually have a spark driver for piloting so that's what we have in production",
    "start": "519659",
    "end": "524940"
  },
  {
    "text": "today but we have to maintain like different versions for every spark we support stuff to point 1 to point 3 to",
    "start": "524940",
    "end": "530610"
  },
  {
    "text": "point 4 it's like huge maintenance cost if we can move like basically if we can",
    "start": "530610",
    "end": "536100"
  },
  {
    "text": "move through supporting native companies the API then that can unlock lots of use case and also can reduce our maintenance",
    "start": "536100",
    "end": "542610"
  },
  {
    "text": "overhead and also with peloton as a",
    "start": "542610",
    "end": "551430"
  },
  {
    "text": "plug-in we can also support an elastic resource sharing and scheduling and high throughput with some performance tunings",
    "start": "551430",
    "end": "557640"
  },
  {
    "text": "of the cognate system so let me quickly introduce a concept called a",
    "start": "557640",
    "end": "564210"
  },
  {
    "start": "560000",
    "end": "560000"
  },
  {
    "text": "hierarchical resource pool so the idea is that it's very similar to youngkyu the idea is that different organizations",
    "start": "564210",
    "end": "571380"
  },
  {
    "text": "we have a resource pool and then you can assign three parameters to each resource",
    "start": "571380",
    "end": "578640"
  },
  {
    "text": "pool why is the reservation why is the limit another is shear and then based on",
    "start": "578640",
    "end": "584580"
  },
  {
    "text": "basically the machine is like the minimum of guarantee of how much resource you can use and limit is what's",
    "start": "584580",
    "end": "590490"
  },
  {
    "text": "the max and shear is like the proportion of extra resource you can use if user",
    "start": "590490",
    "end": "596340"
  },
  {
    "text": "class these idle and then we calculated this entitlement which is somewhere in",
    "start": "596340",
    "end": "601950"
  },
  {
    "text": "between reservation limit so in in this part on company's word we actually",
    "start": "601950",
    "end": "609720"
  },
  {
    "text": "models the issue resourceful as a recovery CRD so basically you can do",
    "start": "609720",
    "end": "614730"
  },
  {
    "text": "coups it here apply a resourceful setting like resource pool Mac replace",
    "start": "614730",
    "end": "619920"
  },
  {
    "text": "and then you get that resource pool which specified you know what's a reservation for CPF for each of the",
    "start": "619920",
    "end": "626370"
  },
  {
    "text": "resource dimension like CPU and memory so this is how like the parking",
    "start": "626370",
    "end": "632100"
  },
  {
    "start": "629000",
    "end": "629000"
  },
  {
    "text": "scheduler is designed the idea is like we basically use all the resource pool",
    "start": "632100",
    "end": "638040"
  },
  {
    "text": "code or the poor scheduler placement engines parameters owns for same license",
    "start": "638040",
    "end": "645150"
  },
  {
    "text": "but instead using may sauce underneath",
    "start": "645150",
    "end": "651780"
  },
  {
    "text": "we actually gonna basically watch the resource pool to populate the resource pool tree and then we have a poor",
    "start": "651780",
    "end": "658170"
  },
  {
    "text": "watcher to watch all the poor creations which like all the armed bandit ports",
    "start": "658170",
    "end": "665070"
  },
  {
    "text": "and it including ah spook you so which actually it's resource-poor has a pending queue and",
    "start": "665070",
    "end": "671290"
  },
  {
    "text": "then in the background we calculate what's the entitlement for each resource pool and they then admitted to them to",
    "start": "671290",
    "end": "677080"
  },
  {
    "text": "already queue so after the in the ready queue we have like a replacement engine to dequeue all those raid a ports and",
    "start": "677080",
    "end": "684130"
  },
  {
    "text": "then bind them to Sulu API server and then it's going to launch the sewer",
    "start": "684130",
    "end": "689290"
  },
  {
    "text": "covenant and as they on the other side we also have a note of what sure which watch all the know the informations and",
    "start": "689290",
    "end": "697030"
  },
  {
    "text": "put them in no cash so let me quickly give an example of elastic resource pool",
    "start": "697030",
    "end": "703420"
  },
  {
    "text": "so for example in this example we have three resource pools each of them has a",
    "start": "703420",
    "end": "710320"
  },
  {
    "text": "reservation of 20 units and they then have a limit of 100 so initially for",
    "start": "710320",
    "end": "716320"
  },
  {
    "start": "715000",
    "end": "715000"
  },
  {
    "text": "example resource pool long only have a demand of 20 or over ten so basically",
    "start": "716320",
    "end": "721330"
  },
  {
    "text": "you know it's there's no other no pod launch through this resource pool and",
    "start": "721330",
    "end": "726850"
  },
  {
    "text": "then we actually gonna use let's resource pool to and resource pool three to use the additional resource so",
    "start": "726850",
    "end": "734290"
  },
  {
    "text": "presumably because the at demand is much higher they're like half the amount of eighty and then each troublesome we are",
    "start": "734290",
    "end": "739480"
  },
  {
    "text": "getting like a forty units of resource now second the next time cycle we could",
    "start": "739480",
    "end": "745690"
  },
  {
    "text": "launch more ports in resource pool Wong so at the document moment and we're",
    "start": "745690",
    "end": "751000"
  },
  {
    "text": "gonna look at her to the environment and is then we're gonna figure out like okay now each resource push the kind of",
    "start": "751000",
    "end": "757089"
  },
  {
    "text": "deserve so these three are locations and then we're going to preamp the back the",
    "start": "757089",
    "end": "762130"
  },
  {
    "text": "the ports in resource pool to n resource for three and gave us a resource to",
    "start": "762130",
    "end": "767500"
  },
  {
    "text": "resource one resource pool one so next we're gonna talk more about spark and",
    "start": "767500",
    "end": "773710"
  },
  {
    "text": "other applications on top of communist party hey guys so now we have",
    "start": "773710",
    "end": "781330"
  },
  {
    "text": "peloton running on top of kubernetes so let's talk about how we're running spark and other frameworks on top of it let's",
    "start": "781330",
    "end": "788830"
  },
  {
    "text": "talk about spark first through we have the spark running differently in different clusters you have some spark",
    "start": "788830",
    "end": "795640"
  },
  {
    "text": "running on yarn some spark running on missiles and some spark running on we started to move",
    "start": "795640",
    "end": "800649"
  },
  {
    "text": "on palette in as well so there are different challenges for each of these for running on y'all currently we don't",
    "start": "800649",
    "end": "807040"
  },
  {
    "text": "have docker support in uber we have it an upstream we haven't used it yet and",
    "start": "807040",
    "end": "812110"
  },
  {
    "text": "we have a lot lack of big containers at septum--ah support also for yarn in uber",
    "start": "812110",
    "end": "817709"
  },
  {
    "text": "we have challenges running on missiles missiles does not have elastically so sharing as we main talked about it and",
    "start": "817709",
    "end": "824410"
  },
  {
    "text": "then the the bigger problem is its power job registered as a framework which is a",
    "start": "824410",
    "end": "829509"
  },
  {
    "text": "scalability bottleneck right so you cannot run two to three hundred spark jobs together and me so so so then",
    "start": "829509",
    "end": "835959"
  },
  {
    "text": "that's the reason we came up with the palette ur so right now spark running on peloton but we have to support all these",
    "start": "835959",
    "end": "842649"
  },
  {
    "text": "drivers which is a bigger cost because you change API is in spark they are changing API is left and right you have",
    "start": "842649",
    "end": "849279"
  },
  {
    "text": "to keep on maintaining it fix bugs for each version because you once you have such a big organization you have",
    "start": "849279",
    "end": "855490"
  },
  {
    "text": "different versions running everywhere every time right so this is a big big cost we are still in production from two",
    "start": "855490",
    "end": "862029"
  },
  {
    "text": "years is part and there are six plus production clusters running spot so that's the reason we thought okay this",
    "start": "862029",
    "end": "868660"
  },
  {
    "text": "is not the scalable so let's do something else and that's the why we wanted to run spark on communities so",
    "start": "868660",
    "end": "875620"
  },
  {
    "text": "the multiple reasons running spark on kubernetes right so his kubernetes is becoming the de facto for AI and m/l",
    "start": "875620",
    "end": "882069"
  },
  {
    "text": "workloads so we wanted to consolidate everything together it's very expensive",
    "start": "882069",
    "end": "888939"
  },
  {
    "text": "to maintain all these custom drivers so we wanted to unify all these spark drivers and its ml drivers and ml",
    "start": "888939",
    "end": "896230"
  },
  {
    "text": "frameworks together into one resource scheduler and that gives us two things one we will not have this cluster of",
    "start": "896230",
    "end": "903160"
  },
  {
    "text": "fragmentation so that means we would not so let's say if one cluster is free and another is busy we can use the workloads",
    "start": "903160",
    "end": "909370"
  },
  {
    "text": "to come in if it is one cluster we can use the resources appropriately and secondly we want we can now prioritize",
    "start": "909370",
    "end": "917259"
  },
  {
    "text": "all these workloads if it's running into one compute platform you don't need to have this global priority you can have",
    "start": "917259",
    "end": "923740"
  },
  {
    "text": "your local priority for each organizations and then that can be prioritized across your workloads so",
    "start": "923740",
    "end": "930220"
  },
  {
    "text": "these are the advantage you want to use for one resource scheduler and definitely we wanted to leverage the",
    "start": "930220",
    "end": "936500"
  },
  {
    "text": "growing community we see there is lot of momentum in kubernetes and on top of",
    "start": "936500",
    "end": "942529"
  },
  {
    "text": "kubernetes all the frameworks right so we wanted to use all of that we can use out of the shelf all these distribute",
    "start": "942529",
    "end": "948710"
  },
  {
    "text": "tensorflow spar flame all these drivers which is already working for flame we",
    "start": "948710",
    "end": "954200"
  },
  {
    "text": "can just use them right we don't need to have the support cost so these are some of the reasons we wanted to go spark on",
    "start": "954200",
    "end": "961250"
  },
  {
    "text": "kubernetes so this is we are running spark on kubernetes right now we have",
    "start": "961250",
    "end": "968330"
  },
  {
    "text": "paladin scheduler so somebody submit a spark summer job to apply server the written scheduler takes that job put it",
    "start": "968330",
    "end": "975529"
  },
  {
    "text": "into a it's a hierarchical resource queue and then we go admitted it based on admission control and then it watches",
    "start": "975529",
    "end": "983420"
  },
  {
    "text": "the nodes and then bind the part then spark driver go and launch that in some",
    "start": "983420",
    "end": "989060"
  },
  {
    "text": "cubelet then queue on the spark driver then then then the normal spark",
    "start": "989060",
    "end": "994420"
  },
  {
    "text": "scheduler runs in spark driver which go and talk to API server get the executors",
    "start": "994420",
    "end": "1000450"
  },
  {
    "text": "then those executors again go to paladin as a scheduler and then go admission",
    "start": "1000450",
    "end": "1006339"
  },
  {
    "text": "control scheduler and all that right and then they get launched into the another cubelet for spark executors and then",
    "start": "1006339",
    "end": "1012250"
  },
  {
    "text": "peloton scheduler can do all kind of preemptions based on the quota management and all that other stuff",
    "start": "1012250",
    "end": "1018580"
  },
  {
    "text": "right so this is how this spark is running on kubernetes right now so we",
    "start": "1018580",
    "end": "1025329"
  },
  {
    "text": "have this pattern scheduler which can do all that so there are certain challenges",
    "start": "1025329",
    "end": "1031540"
  },
  {
    "start": "1029000",
    "end": "1029000"
  },
  {
    "text": "running spark on kubernetes right now and we have solved some of those so lack",
    "start": "1031540",
    "end": "1038650"
  },
  {
    "text": "of this elastic resource sharing right now in kubernetes so which we are complementing with paladin resource",
    "start": "1038650",
    "end": "1045010"
  },
  {
    "text": "scheduler resource pools second there is no support of global we",
    "start": "1045010",
    "end": "1050320"
  },
  {
    "text": "don't want to sow in batch workloads the global priority is very hard you have many organization which is running and",
    "start": "1050320",
    "end": "1056170"
  },
  {
    "text": "you don't have to have a global priority kubernetes right now works on a global priority you need to have all workloads",
    "start": "1056170",
    "end": "1062080"
  },
  {
    "text": "to be prioritized into single scale which is not possible for batch",
    "start": "1062080",
    "end": "1067120"
  },
  {
    "text": "so you probably want to have each organization has their own priorities so",
    "start": "1067120",
    "end": "1072220"
  },
  {
    "text": "this is how we enable it through resource pools currently SPARC does not",
    "start": "1072220",
    "end": "1077679"
  },
  {
    "text": "support the dynamic resource allocation because of there is no external shuffle service and it's park for communities",
    "start": "1077679",
    "end": "1083740"
  },
  {
    "text": "right now so we are solving it through right we are already wrote remote SPARC shuffle service which we talked about a",
    "start": "1083740",
    "end": "1089679"
  },
  {
    "text": "little further and then there is a lotta support of security FS so we are passing",
    "start": "1089679",
    "end": "1095289"
  },
  {
    "text": "Kerberos tokens through security kubernetes secure secrets so this is how",
    "start": "1095289",
    "end": "1101260"
  },
  {
    "text": "we are overcoming all these challenges for SPARC on communities right now so",
    "start": "1101260",
    "end": "1106899"
  },
  {
    "start": "1106000",
    "end": "1106000"
  },
  {
    "text": "let's talk about how this overview how this local Shepherd works so right now",
    "start": "1106899",
    "end": "1112720"
  },
  {
    "text": "what happens is you the SPARC mapper and reducer runs mappers right to their",
    "start": "1112720",
    "end": "1118090"
  },
  {
    "text": "local disks then they generate generally we have this SSD machines we are on for",
    "start": "1118090",
    "end": "1125470"
  },
  {
    "text": "map for compute machines or SSD machines so all these local shuffle or the external shuffle write all the load this",
    "start": "1125470",
    "end": "1131799"
  },
  {
    "text": "data into the local disk this is how this is being laid out on the disk case you have index files and you have data",
    "start": "1131799",
    "end": "1138279"
  },
  {
    "text": "files and local shuffle mapper write to the local shuffle service and then they are being written into these index file",
    "start": "1138279",
    "end": "1144610"
  },
  {
    "text": "and the data files and these each partition are laid out one reducer comes a reducer goes and talk to each local",
    "start": "1144610",
    "end": "1151210"
  },
  {
    "text": "shuffle service finds out which partition it needs to find out it goes and tells the local shuffle service that",
    "start": "1151210",
    "end": "1156549"
  },
  {
    "text": "ok I want these partitions and then it goes is pair merge them and then give it",
    "start": "1156549",
    "end": "1162159"
  },
  {
    "text": "to the iterator for the reducer right this is how your local shuffle works there are certain challenges of having",
    "start": "1162159",
    "end": "1168850"
  },
  {
    "text": "this local shuffle service right now first if you are using assistive",
    "start": "1168850",
    "end": "1174130"
  },
  {
    "start": "1171000",
    "end": "1171000"
  },
  {
    "text": "machines and if you are using lot of writes to the disk your SSD got we are",
    "start": "1174130",
    "end": "1180490"
  },
  {
    "text": "out so we have our data centers which we used to run the SSD for 3 years our dos",
    "start": "1180490",
    "end": "1187179"
  },
  {
    "text": "assist you got word out in 3 six months right so all the disks were bad within 6",
    "start": "1187179",
    "end": "1192190"
  },
  {
    "text": "months because we write so much data or shuffle data on to the disk there is",
    "start": "1192190",
    "end": "1197230"
  },
  {
    "text": "something called DW PD in SSD which is called disk right per day and if you exceed that per day then your",
    "start": "1197230",
    "end": "1205230"
  },
  {
    "text": "life of your SSD goes down and this is what is happening in our data centers too so that's when we thought okay we",
    "start": "1205230",
    "end": "1210929"
  },
  {
    "text": "need to write something which because you can have better SSDs but there is all you those are not good in terms I",
    "start": "1210929",
    "end": "1219029"
  },
  {
    "text": "mean in terms of your economics right so you don't want to put those higher SSDs so second problem is reliability you",
    "start": "1219029",
    "end": "1227269"
  },
  {
    "text": "certain very often you see that your job got killed because somebody else not a",
    "start": "1227269",
    "end": "1233820"
  },
  {
    "text": "lot like wrote lot of data on to your disk so this is like noisy neighbor",
    "start": "1233820",
    "end": "1239669"
  },
  {
    "text": "issue right so people write lot of data and then certain jobs get fair I mean this is like every day we have like",
    "start": "1239669",
    "end": "1245850"
  },
  {
    "text": "three thousand jobs filled in our data center in yarn cluster three thousand job fails because of this issue we don't",
    "start": "1245850",
    "end": "1253559"
  },
  {
    "text": "further kubernetes we don't have dynamic allocation and this colocation also so",
    "start": "1253559",
    "end": "1258690"
  },
  {
    "text": "the we wanted to write unified scheduler so we wanted to co-locate stateless and batch together however we have so much",
    "start": "1258690",
    "end": "1266909"
  },
  {
    "text": "written data written but the bad jobs into the local disk the disk utilization because becomes hundred percent and",
    "start": "1266909",
    "end": "1272789"
  },
  {
    "text": "because of that all the stateless services running on that machine gets unresponsive because you are writing so",
    "start": "1272789",
    "end": "1278669"
  },
  {
    "text": "much data your load average on the machine is very high and because of that you can't even go and do a search on",
    "start": "1278669",
    "end": "1284309"
  },
  {
    "text": "those machines so the the stateless services vary which are very very latency sensitive gets like very much",
    "start": "1284309",
    "end": "1290820"
  },
  {
    "text": "blog in terms of throughput so we can't do colocation on the same machine if you",
    "start": "1290820",
    "end": "1297510"
  },
  {
    "text": "are writing so much data so this is what we are thought maybe this is one of the reason we should write remote shuffle",
    "start": "1297510",
    "end": "1303120"
  },
  {
    "text": "service so this is how we did remote shuffle we have multiple so so we did so",
    "start": "1303120",
    "end": "1311789"
  },
  {
    "text": "there is something called shuffle manager shuffle manager it's a it's a part of your spot today however you can",
    "start": "1311789",
    "end": "1319409"
  },
  {
    "text": "plug in different storage into shuffle manager you can say local you can say remote you can say NFS you can say GFS",
    "start": "1319409",
    "end": "1326340"
  },
  {
    "text": "and you can write your own shuffle manager for that today but if we did these experiments with those",
    "start": "1326340",
    "end": "1332490"
  },
  {
    "text": "managers we wrote as de festival manager we wrote an official manager because we wanted to see instead of writing to",
    "start": "1332490",
    "end": "1338880"
  },
  {
    "text": "local we can write remote but we did that and the experiments were we are like 2x or 3x slower even on NFS even on",
    "start": "1338880",
    "end": "1346980"
  },
  {
    "text": "HDFS right because there is a lot of data which people I mean lot of small",
    "start": "1346980",
    "end": "1351990"
  },
  {
    "text": "files with these hosts or these mapper task light which cannot get so the",
    "start": "1351990",
    "end": "1357090"
  },
  {
    "text": "closing opening file going on network causes so much issues and so much",
    "start": "1357090",
    "end": "1362179"
  },
  {
    "text": "latency is because of that the latency of the whole job becomes 2x or 3x slower",
    "start": "1362179",
    "end": "1367520"
  },
  {
    "text": "so we took a step back and what we did was we changed a little bit paradigm for",
    "start": "1367520",
    "end": "1373230"
  },
  {
    "text": "schmuck shuffle right now what we did was so all the shuffle manager goes and",
    "start": "1373230",
    "end": "1378570"
  },
  {
    "text": "so we have a cluster of shuffle machines all the shuffle manager finds out which",
    "start": "1378570",
    "end": "1384179"
  },
  {
    "text": "partition and can I can write to which shuffle server and then these shuffle",
    "start": "1384179",
    "end": "1390929"
  },
  {
    "text": "partitions all the mappers of same partition will go to one server and the",
    "start": "1390929",
    "end": "1396690"
  },
  {
    "text": "server will write into the local SSD sequentially so because of that reducer",
    "start": "1396690",
    "end": "1403200"
  },
  {
    "text": "will go directly to one server and fetch all the records into one shot it doesn't need to go to each machine and find out",
    "start": "1403200",
    "end": "1410130"
  },
  {
    "text": "where is that it partitions are merge them together and all that so it doesn't need to do all that it's all there",
    "start": "1410130",
    "end": "1415830"
  },
  {
    "text": "go to one server and fetch the file and after doing that and these are all",
    "start": "1415830",
    "end": "1422130"
  },
  {
    "text": "streaming so mappers are streaming to stream or shuffle and reducers are streaming from remote shuffle to the",
    "start": "1422130",
    "end": "1428610"
  },
  {
    "text": "lower after doing that we are actually on power performance with the local",
    "start": "1428610",
    "end": "1434370"
  },
  {
    "text": "shuffle so we are doing remote but the performance is pretty much the same what we used to get from external shuffle so",
    "start": "1434370",
    "end": "1441360"
  },
  {
    "start": "1441000",
    "end": "1441000"
  },
  {
    "text": "remote shuffle we are doing last three months all these yarn and paladin workloads are running on top of it currently thousands",
    "start": "1441360",
    "end": "1450450"
  },
  {
    "text": "of application running job latencies are on par with external shuffle right now and we are actively enhancing that",
    "start": "1450450",
    "end": "1458660"
  },
  {
    "text": "working towards onboarding allspark workloads on to remote shuffle and we",
    "start": "1458660",
    "end": "1464640"
  },
  {
    "text": "are trying to open source so let's talk about GPO and deep",
    "start": "1464640",
    "end": "1471200"
  },
  {
    "text": "learnings right so as Minh said we have lot of use cases which are using GPUs and deep learning use today in our",
    "start": "1471200",
    "end": "1478760"
  },
  {
    "start": "1472000",
    "end": "1472000"
  },
  {
    "text": "computer stack we have like self-driving vehicles we have trip forecasting fraud detection and many many more",
    "start": "1478760",
    "end": "1485059"
  },
  {
    "text": "deep deep learning use cases which are coming up on GPUs there there are",
    "start": "1485059",
    "end": "1491960"
  },
  {
    "text": "certain challenges when running distributed tensorflow right now we don't have elastic GPU",
    "start": "1491960",
    "end": "1499580"
  },
  {
    "start": "1492000",
    "end": "1492000"
  },
  {
    "text": "resource management so that is one of the issues which we are complementing",
    "start": "1499580",
    "end": "1504770"
  },
  {
    "text": "with pollutant we don't have locality and network aware placement tasks the",
    "start": "1504770",
    "end": "1509960"
  },
  {
    "text": "surgeries issue gang scheduling which is a challenge right now we are trying to solve it",
    "start": "1509960",
    "end": "1516230"
  },
  {
    "text": "through peloton and as well as the failure handling if one of the tasks goes down right gang scheduling let's",
    "start": "1516230",
    "end": "1523610"
  },
  {
    "start": "1522000",
    "end": "1522000"
  },
  {
    "text": "talk about a little bit on the gang scheduling see peace a subset of the tasks in job can be skipped specified as",
    "start": "1523610",
    "end": "1529730"
  },
  {
    "text": "a gang right you can say these are the tasks run as a gang so what are the primitives which we need to follow they",
    "start": "1529730",
    "end": "1536090"
  },
  {
    "text": "have to be admitted as a gang scheduled as a gang preempted as a Gangnam killed as again right so these are the",
    "start": "1536090",
    "end": "1542720"
  },
  {
    "text": "primitives which are missing and which peloton have it we have that in production on peloton and mesos and we",
    "start": "1542720",
    "end": "1548840"
  },
  {
    "text": "are trying to do it through pod groups into kubernetes gang task are so we what",
    "start": "1548840",
    "end": "1556820"
  },
  {
    "text": "we do is we we take us again we admit we do the already so sharing all the quota",
    "start": "1556820",
    "end": "1562640"
  },
  {
    "text": "management we admit based on that we do advert as as a gang and we bind or place",
    "start": "1562640",
    "end": "1568760"
  },
  {
    "text": "as a gang and then if you wanted to preempt any of the tasks because of the priorities or cluster is busy and",
    "start": "1568760",
    "end": "1575899"
  },
  {
    "text": "whatnot we do preempt all the gang together similarly if something fails we make",
    "start": "1575899",
    "end": "1582320"
  },
  {
    "text": "sure that the whole gang has been killed right so this is how we added the gang primitives so this is how we run",
    "start": "1582320",
    "end": "1590360"
  },
  {
    "text": "distribute tensorflow right now on kubernetes so Michelangelo is our deep",
    "start": "1590360",
    "end": "1595520"
  },
  {
    "text": "learning service which runs and talk to a PI server and create the part and similarly Pelton takes that Perdue",
    "start": "1595520",
    "end": "1601940"
  },
  {
    "text": "admission control place it and then once it is placed it places through parameter",
    "start": "1601940",
    "end": "1607100"
  },
  {
    "text": "server and this worker executor and every para every this every part runs to",
    "start": "1607100",
    "end": "1613100"
  },
  {
    "text": "containers with the params parameter server container and the Michelangelo container because that's the service",
    "start": "1613100",
    "end": "1618980"
  },
  {
    "text": "which we use and then they go and discover each other through API server right and then they get launched and",
    "start": "1618980",
    "end": "1625970"
  },
  {
    "text": "then run as a gap so this is how we currently run a distributed tensor flow on coop ideas so let's talk about a",
    "start": "1625970",
    "end": "1635270"
  },
  {
    "text": "little bit of workload colocation so why",
    "start": "1635270",
    "end": "1641360"
  },
  {
    "start": "1641000",
    "end": "1641000"
  },
  {
    "text": "we wanted to work load why you want to co-locate workloads this is a bigger problem for efficiency right",
    "start": "1641360",
    "end": "1646970"
  },
  {
    "text": "if we run stateless service separate than the batch service then you you",
    "start": "1646970",
    "end": "1653450"
  },
  {
    "text": "cannot use all the resources how she should be using we are aiming to use 20",
    "start": "1653450",
    "end": "1659299"
  },
  {
    "text": "to 25 percent of the resources if we can work them or to run them together and 20",
    "start": "1659299",
    "end": "1666260"
  },
  {
    "text": "to 25 percent resources is if if you are talking about ubers fleet it's like huge",
    "start": "1666260",
    "end": "1672169"
  },
  {
    "text": "huge amount of money right so and we we can save 20 to 25 percent of money or servers if we go locate them the",
    "start": "1672169",
    "end": "1680539"
  },
  {
    "text": "challenges are if we wanted to co-locate them on the same machine the disk i/o which I just talked about network on the",
    "start": "1680539",
    "end": "1687110"
  },
  {
    "text": "machine CPU caches and the memory over subscriptions right so if you are",
    "start": "1687110",
    "end": "1692179"
  },
  {
    "text": "running everything together then these are the challenges which we which happens today so what we thought is it's",
    "start": "1692179",
    "end": "1698929"
  },
  {
    "text": "hard to solve this these are the problems on to same machine so what we thought of ok but we can do it solve it",
    "start": "1698929",
    "end": "1706159"
  },
  {
    "text": "on the same cluster so we created something called dynamic partition into",
    "start": "1706159",
    "end": "1711440"
  },
  {
    "text": "our cluster which is you have stateless partition and you have batch partition so you have two different partitions you",
    "start": "1711440",
    "end": "1719179"
  },
  {
    "text": "oversubscribe all these physical resources on each partition and then you move machines if needed so as I said you",
    "start": "1719179",
    "end": "1728149"
  },
  {
    "text": "have this cluster where you have stateless partitions you have these batch partitions and all these partitions are running the",
    "start": "1728149",
    "end": "1735620"
  },
  {
    "text": "stateless partitions are running stateless services batch partitions are running bad jobs and then we are",
    "start": "1735620",
    "end": "1741020"
  },
  {
    "text": "measuring the hotness of each partition so you see 40 percent utilization 40 to",
    "start": "1741020",
    "end": "1747740"
  },
  {
    "text": "50 percent and 50 to 60% right so we over subscribe the resources on these",
    "start": "1747740",
    "end": "1752990"
  },
  {
    "text": "each partition this is how you save money you you you pack each services and each more job together and then you",
    "start": "1752990",
    "end": "1760070"
  },
  {
    "text": "might measure if something is getting impacted you move machines based on that",
    "start": "1760070",
    "end": "1766960"
  },
  {
    "text": "so this is how we do right so we measure the partition and we and then the nodes",
    "start": "1767170",
    "end": "1773510"
  },
  {
    "start": "1768000",
    "end": "1768000"
  },
  {
    "text": "which are least loaded we move them into different partition based on the",
    "start": "1773510",
    "end": "1778580"
  },
  {
    "text": "hardness of each partition so if you're much if your partition is like 70 or 60% heart we go move machines from the other",
    "start": "1778580",
    "end": "1786140"
  },
  {
    "text": "partitions to this partition by that we can easy up this this partition and",
    "start": "1786140",
    "end": "1791500"
  },
  {
    "text": "services and other jobs are not getting impacted there is an assumption here we",
    "start": "1791500",
    "end": "1798620"
  },
  {
    "text": "we say services are higher priorities or the Staedtler services which are higher",
    "start": "1798620",
    "end": "1805280"
  },
  {
    "text": "priorities then the some of the bad jobs so this is the underlying assumption which we take in our questions so this",
    "start": "1805280",
    "end": "1813140"
  },
  {
    "start": "1813000",
    "end": "1813000"
  },
  {
    "text": "is how it works we have something called node agent which runs in each node which",
    "start": "1813140",
    "end": "1820010"
  },
  {
    "text": "measures what is the CPU length what is the CPU load and all different CPU matrixes for each node and then send it",
    "start": "1820010",
    "end": "1827929"
  },
  {
    "text": "to this something called node adviser node adviser goes and finds out from all",
    "start": "1827929",
    "end": "1833600"
  },
  {
    "text": "the nodes what are the load and the inference of services on these node to",
    "start": "1833600",
    "end": "1839360"
  },
  {
    "text": "each other our it finds out that and then it gets that information to the penitent scheduler now Paragon scheduler",
    "start": "1839360",
    "end": "1845840"
  },
  {
    "text": "will take decisions based on the if the partition is hot if the node is heart of",
    "start": "1845840",
    "end": "1851330"
  },
  {
    "text": "the services is hot based on these three parameters it takes decision which machines we need to move from which",
    "start": "1851330",
    "end": "1858440"
  },
  {
    "text": "partition by that we don't impact these services which are running or the jobs",
    "start": "1858440",
    "end": "1863960"
  },
  {
    "text": "which are and we want to do it proactively because we don't want because if you do reactive",
    "start": "1863960",
    "end": "1869500"
  },
  {
    "text": "the services is already impacted so we try to measure the whole partition along",
    "start": "1869500",
    "end": "1875150"
  },
  {
    "text": "with the node as well as the service so if we see the partition is GU growing in",
    "start": "1875150",
    "end": "1881390"
  },
  {
    "text": "terms of hotness we move machines to that partition so for that it's it seems",
    "start": "1881390",
    "end": "1889310"
  },
  {
    "text": "easy we have to do a lot of work to do that we did load aware placement load aware placement because we we have to do",
    "start": "1889310",
    "end": "1898310"
  },
  {
    "text": "the load of a placement because we don't want to cause churn into the system because it may happen that one machine",
    "start": "1898310",
    "end": "1903950"
  },
  {
    "text": "is hard and then you move much service from that machine to another machine and that machine is hard so you always plays",
    "start": "1903950",
    "end": "1910070"
  },
  {
    "text": "into that partition based on the load on the machines we have the scorer's we",
    "start": "1910070",
    "end": "1915680"
  },
  {
    "text": "build the scorer's for batch and stateless which can find out what is the right machine at this point which we can",
    "start": "1915680",
    "end": "1921650"
  },
  {
    "text": "move virtual partition we added partition within partitions because",
    "start": "1921650",
    "end": "1927410"
  },
  {
    "text": "there were some bad jobs which are needed which are more important and the",
    "start": "1927410",
    "end": "1932540"
  },
  {
    "text": "break glass right if if there is a spikes into the unusual spikes in your",
    "start": "1932540",
    "end": "1938960"
  },
  {
    "text": "services then we can go break glass and then get all the machines from the batch with the assumption that the batch",
    "start": "1938960",
    "end": "1945890"
  },
  {
    "text": "machines some of the batch workloads are not as important as stateless and give those machines to the stateless clusters",
    "start": "1945890",
    "end": "1952210"
  },
  {
    "text": "so this is how we are implementing this colocation this right now in for its",
    "start": "1952210",
    "end": "1960200"
  },
  {
    "text": "implemented in peloton so it's pretty much orthogonal to communities or missiles",
    "start": "1960200",
    "end": "1966460"
  },
  {
    "text": "so somebody kubernetes is the future for big data and m/l workload based on the",
    "start": "1966670",
    "end": "1972710"
  },
  {
    "text": "adoption so we have done peloton and gate scheduler POC we are already",
    "start": "1972710",
    "end": "1979070"
  },
  {
    "text": "implementing all these bad features peloton on misses is in production stateless in batch and we are thinking",
    "start": "1979070",
    "end": "1986420"
  },
  {
    "text": "how we migrate and looking for code of collaboration to enable kubernetes for",
    "start": "1986420",
    "end": "1991700"
  },
  {
    "text": "big data and emeralds that's all thanks [Applause]",
    "start": "1991700",
    "end": "2002549"
  }
]