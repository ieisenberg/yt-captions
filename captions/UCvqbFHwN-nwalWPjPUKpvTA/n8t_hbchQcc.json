[
  {
    "text": "um so welcome to my talk uh auto scaling kubernetes deployments a mostly",
    "start": "80",
    "end": "5279"
  },
  {
    "text": "practical guide so hopefully this is the one you actually meant to go to",
    "start": "5279",
    "end": "11920"
  },
  {
    "text": "just a little bit about me before we start i'm a software engineer at new relic",
    "start": "12559",
    "end": "17680"
  },
  {
    "text": "i work on the pixi open source project pixi is a cncf sandbox project and it",
    "start": "17680",
    "end": "24720"
  },
  {
    "text": "basically provides observability for your kubernetes cluster and it's super cool so please check it",
    "start": "24720",
    "end": "31760"
  },
  {
    "text": "out i love observability and performance problems and",
    "start": "31760",
    "end": "37360"
  },
  {
    "text": "prior to that and partially because of that i formally worked in the data space and what we kind of see with",
    "start": "37360",
    "end": "42879"
  },
  {
    "text": "observability is a lot of the same problems and observability are data problems so that's kind of the uniting thread",
    "start": "42879",
    "end": "49280"
  },
  {
    "text": "behind kind of some of the stuff i've worked on so today's talk is all about auto",
    "start": "49280",
    "end": "55760"
  },
  {
    "text": "scaling kubernetes deployments so first we'll briefly touch on what is",
    "start": "55760",
    "end": "60879"
  },
  {
    "text": "kubernetes auto scaling and why would you do it then we'll cover more detailed stuff",
    "start": "60879",
    "end": "66080"
  },
  {
    "text": "like what are the different knobs that kubernetes auto scaling provides us with",
    "start": "66080",
    "end": "72080"
  },
  {
    "text": "you have to know the right bottleneck in your application in order to make your auto scaling as performant as possible",
    "start": "72080",
    "end": "77200"
  },
  {
    "text": "so that then we'll cover selecting the right auto scaling metric for your application",
    "start": "77200",
    "end": "82400"
  },
  {
    "text": "and then finally i love to push the boundaries of the technology i work in and uh it turns out you can make a",
    "start": "82400",
    "end": "89439"
  },
  {
    "text": "touring complete auto scaler for kubernetes so we'll be showing that at the end",
    "start": "89439",
    "end": "95200"
  },
  {
    "text": "so let's get started so when you're sizing an application or",
    "start": "96960",
    "end": "103520"
  },
  {
    "text": "kubernetes cluster how do you decide how many nodes there should be how do you decide how many pods should",
    "start": "103520",
    "end": "110000"
  },
  {
    "text": "be in your deployment how do you know how many resources to give a pod",
    "start": "110000",
    "end": "116159"
  },
  {
    "text": "these are not obvious questions or these are obvious questions maybe but they don't necessarily have obvious answers",
    "start": "116159",
    "end": "121280"
  },
  {
    "text": "and the strategies that people take for them vary for example there is the methodology of",
    "start": "121280",
    "end": "126640"
  },
  {
    "text": "doing a completely random guess and hope that it works you copy pasta from some other thing",
    "start": "126640",
    "end": "132000"
  },
  {
    "text": "that i've already deployed on your cluster this is one of the most common ones in my experience",
    "start": "132000",
    "end": "137040"
  },
  {
    "text": "you might be one of those people that is always thinking ahead and you're always proactively iterating you're monitoring",
    "start": "137040",
    "end": "142560"
  },
  {
    "text": "your deployments and seeing oh shoot cpu is a little bit close to the limit",
    "start": "142560",
    "end": "147760"
  },
  {
    "text": "but mostly in practice we reactively iterate we see oh shoot my application's down oh",
    "start": "147760",
    "end": "154000"
  },
  {
    "text": "my gosh the pod it's using all the cpu it's given incident",
    "start": "154000",
    "end": "160879"
  },
  {
    "text": "auto scaling is intended to help solve some of these problems and kubernetes provides really really good support for",
    "start": "162000",
    "end": "167519"
  },
  {
    "text": "auto scaling thing is that the ideal resource allocation completely depends on your",
    "start": "167519",
    "end": "174080"
  },
  {
    "text": "workload and these workloads are spiky and often unpredictable here we have a screenshot of a workload",
    "start": "174080",
    "end": "180560"
  },
  {
    "text": "that we're running you can see that the traffic can be over 10x at one point what it is at a different point",
    "start": "180560",
    "end": "188000"
  },
  {
    "text": "and a lot of times these spikes will bite you when you least expect it it's not something that you can always plan on",
    "start": "188000",
    "end": "194640"
  },
  {
    "text": "if you have too few resources you give a bad user experience you have latency problems and even",
    "start": "194800",
    "end": "200319"
  },
  {
    "text": "outages this is a big no-no but on the other hand you can't just go",
    "start": "200319",
    "end": "205840"
  },
  {
    "text": "allocate everything under the sun either it's really expensive",
    "start": "205840",
    "end": "211879"
  },
  {
    "text": "kubernetes provides three types of auto scaling we'll give a quick overview of two of them and then focus most of our",
    "start": "212720",
    "end": "218319"
  },
  {
    "text": "time on the third first there's the cluster auto scaler this adds and removes nodes to your",
    "start": "218319",
    "end": "224560"
  },
  {
    "text": "cluster based on resource utilization like cpu next there's the vertical pod autoscaler",
    "start": "224560",
    "end": "232239"
  },
  {
    "text": "this adds resources like cpu and memory to your existing replicas or pods",
    "start": "232239",
    "end": "237760"
  },
  {
    "text": "also based on resource utilization finally we have the horizontal pod auto",
    "start": "237760",
    "end": "243280"
  },
  {
    "text": "scaler this is going to be the one that we focus on most in this talk because it's extremely powerful",
    "start": "243280",
    "end": "249680"
  },
  {
    "text": "it adds additional replicas to your deployment etc and it also can look at resource",
    "start": "249680",
    "end": "255519"
  },
  {
    "text": "utilization but the really cool thing is you can actually define your own metrics that you want to scale based off of and",
    "start": "255519",
    "end": "262720"
  },
  {
    "text": "this makes it one of the most powerful features of kubernetes in my opinion",
    "start": "262720",
    "end": "268240"
  },
  {
    "text": "also i know that the font might be i try to blow it up where possible but for the people in the back um the slides are",
    "start": "268240",
    "end": "274800"
  },
  {
    "text": "available on the talk kind of sched thing so if you want to follow along i feel kind of bad if",
    "start": "274800",
    "end": "280960"
  },
  {
    "text": "you can't see it so let's quickly touch on the cluster auto scaler",
    "start": "280960",
    "end": "287199"
  },
  {
    "text": "i just wanted to give some quick tips for each of these scalers so one thing that you have to know with a cluster auto scaler is you have to set",
    "start": "287199",
    "end": "294080"
  },
  {
    "text": "the pod resource requests and limits otherwise the cluster auto scaler can't really know how many nodes to add or",
    "start": "294080",
    "end": "300160"
  },
  {
    "text": "take away you also need to make sure that your resource requests reflect your actual",
    "start": "300160",
    "end": "306000"
  },
  {
    "text": "usage if i have a pod that's only using 10 cpu and i've requested 100 cpu",
    "start": "306000",
    "end": "312479"
  },
  {
    "text": "the auto scaler is going to treat it like a 100 and not like a 10 potentially leading to unnecessary waste and lack of",
    "start": "312479",
    "end": "318560"
  },
  {
    "text": "utilization you also want to make sure you specify pod disruption budgets this one's really",
    "start": "318560",
    "end": "325520"
  },
  {
    "text": "really important for the cluster auto scaler because when the cluster auto scaler adds or removes nodes it's going",
    "start": "325520",
    "end": "330720"
  },
  {
    "text": "to reschedule the pods in your cluster and if you have a high availability system or if you just have a pod that",
    "start": "330720",
    "end": "337919"
  },
  {
    "text": "you need to make sure a different one's running before you move the second one over the pod disruption budget is the",
    "start": "337919",
    "end": "342960"
  },
  {
    "text": "best way to make sure that these workloads can be transitioned to the new node safely",
    "start": "342960",
    "end": "348880"
  },
  {
    "text": "you also need to make sure that you don't do what i had this morning where you try to scale beyond the limit that",
    "start": "350400",
    "end": "355919"
  },
  {
    "text": "your cloud provider is actually giving you you don't want to say oh you can go up to 100 nodes and your cloud provider",
    "start": "355919",
    "end": "362080"
  },
  {
    "text": "you know nerfs you at 10. this is something that's really important and can definitely cause an",
    "start": "362080",
    "end": "367199"
  },
  {
    "text": "incident if you're not careful one thing to note is that the kubernetes",
    "start": "367199",
    "end": "372479"
  },
  {
    "text": "contributors uh in their documentation they note that they've tested this cluster autoscaler for up to a thousand",
    "start": "372479",
    "end": "378319"
  },
  {
    "text": "nodes with 30 pods per node so if you're running a mega workload that's bigger than this you should keep that in mind",
    "start": "378319",
    "end": "384240"
  },
  {
    "text": "when using the auto scaler",
    "start": "384240",
    "end": "387360"
  },
  {
    "text": "next let's look at the vertical pod auto scaler by the way there is this really hilarious tick tock that i saw for",
    "start": "389280",
    "end": "395440"
  },
  {
    "text": "kubernetes that was like showing the difference between the vertical pod auto scaler and the horizontal pod auto scaler in terms of glasses of water so",
    "start": "395440",
    "end": "402560"
  },
  {
    "text": "definitely check that out if you're a tic toc user so with the vertical pod auto scaler",
    "start": "402560",
    "end": "409039"
  },
  {
    "text": "you can still set a resource cap with the vertical plot auto scaler so when it's adding something like cpu you'll",
    "start": "409039",
    "end": "414400"
  },
  {
    "text": "set a cap so it doesn't just grow the cpu indefinitely you may see a container restart or your",
    "start": "414400",
    "end": "420960"
  },
  {
    "text": "pod get rescheduled because it's adding more cpu maybe it needs to be moved to a different node to have that amount of",
    "start": "420960",
    "end": "426240"
  },
  {
    "text": "cpu you want to use the vertical pod auto",
    "start": "426240",
    "end": "431840"
  },
  {
    "text": "scaler in conjunction with the cluster auto scaler because otherwise you can have a situation where you're scaling up",
    "start": "431840",
    "end": "437440"
  },
  {
    "text": "your deployments altogether needing many more resources than before and you're also going to need more nodes to run those resources",
    "start": "437440",
    "end": "445840"
  },
  {
    "text": "and finally you can't auto scale resources like cpu and pod for a given",
    "start": "446960",
    "end": "452240"
  },
  {
    "text": "replica and add replicas with the horizontal potato skiller at the same time for the same metric on the same",
    "start": "452240",
    "end": "458400"
  },
  {
    "text": "application so whether you're gonna make your existing pods bigger or add more pods for a given deployment you're going",
    "start": "458400",
    "end": "465440"
  },
  {
    "text": "to want to pick a lane and choose either the horizontal plot auto scaler or the vertical plot auto scaler",
    "start": "465440",
    "end": "472000"
  },
  {
    "text": "and then finally i don't know the exact specifics but they know in their documentation that this has not been tested on large",
    "start": "473599",
    "end": "479280"
  },
  {
    "text": "clusters so um you know it's something that you'll always want to test on your own application",
    "start": "479280",
    "end": "485599"
  },
  {
    "text": "okay finally we'll talk about my favorite the horizontal pod autoscaler so this provides so much flexibility for",
    "start": "486319",
    "end": "492479"
  },
  {
    "text": "metric selection it's not even funny and every application is its own special snowflake so you can give it its own",
    "start": "492479",
    "end": "498080"
  },
  {
    "text": "corresponding special snowflake metric but you're going to want to check your",
    "start": "498080",
    "end": "503360"
  },
  {
    "text": "service client affinity policies to ensure even load distribution you don't want to scale up your application from",
    "start": "503360",
    "end": "509199"
  },
  {
    "text": "two replicas to 100 only to discover that the first two are still getting all the traffic that would be a complete",
    "start": "509199",
    "end": "514640"
  },
  {
    "text": "waste of resources also you need to make sure you set",
    "start": "514640",
    "end": "519760"
  },
  {
    "text": "resource requests when you're scaling on cpu and memory horizontal positive scaler when you're",
    "start": "519760",
    "end": "524880"
  },
  {
    "text": "scaling on something like cpu looks at the percentage of your request that you're utilizing so if you don't have a",
    "start": "524880",
    "end": "530399"
  },
  {
    "text": "request it's never going to auto scale and finally as with the vertical pod",
    "start": "530399",
    "end": "536480"
  },
  {
    "text": "auto scaler we can't use these two in conjunction for the same workload on the same metric yet but that is something",
    "start": "536480",
    "end": "541680"
  },
  {
    "text": "that the contributors are working on so let's get to it so i have a demo of",
    "start": "541680",
    "end": "549839"
  },
  {
    "text": "horizontal pod auto ceiling on cpu um i heard there's been a little network connectivity so i recorded it which also",
    "start": "549839",
    "end": "556480"
  },
  {
    "text": "allows us to watch it on fast forward and this demo exists on the pixi demos",
    "start": "556480",
    "end": "562000"
  },
  {
    "text": "repo so you can actually do it yourself and try auto scaling on various different metrics",
    "start": "562000",
    "end": "568720"
  },
  {
    "text": "and i'll try to narrate for the people in the back because i know the fonts a little bit small",
    "start": "572080",
    "end": "577839"
  },
  {
    "text": "okay cool so what we have here is i'm actually showing a view in pixi",
    "start": "580320",
    "end": "585440"
  },
  {
    "text": "it's just completely defined by a script that i've written and i'm showing a view of my application before auto scaling is",
    "start": "585440",
    "end": "592000"
  },
  {
    "text": "added so just for the people in the back the upper left quadrant is request per second for my service",
    "start": "592000",
    "end": "598560"
  },
  {
    "text": "the upper right quadrant is the http latency and there's three lines because we have p50",
    "start": "598560",
    "end": "604640"
  },
  {
    "text": "p90 and p99 latencies the bottom left we have the cp usage",
    "start": "604640",
    "end": "610000"
  },
  {
    "text": "bipod and on the bottom right we have the number of pods to the surface since this",
    "start": "610000",
    "end": "615040"
  },
  {
    "text": "is before auto scaling it's stuck at one so we're going to use pixi to observe what's happening in the cluster as we",
    "start": "615040",
    "end": "621600"
  },
  {
    "text": "auto scale up",
    "start": "621600",
    "end": "624240"
  },
  {
    "text": "so we're going to watch this and what we're going to do is use my favorite load generating",
    "start": "627040",
    "end": "633200"
  },
  {
    "text": "application called hey and it basically is going to allow me to",
    "start": "633200",
    "end": "638320"
  },
  {
    "text": "spam a ton of traffic to this endpoint and this endpoint is an expensive endpoint it's very cpu intensive so",
    "start": "638320",
    "end": "644399"
  },
  {
    "text": "that's why we're going to scale on cpu so i'm spinning up 20 concurrent clients",
    "start": "644399",
    "end": "649440"
  },
  {
    "text": "each sending one query per second which is a lot for this service and in the bottom left what we can see",
    "start": "649440",
    "end": "654959"
  },
  {
    "text": "is that one replica is running right now for my echo service",
    "start": "654959",
    "end": "660600"
  },
  {
    "text": "and the beauty of this is we can jump ahead and let the auto scaler do its thing",
    "start": "663360",
    "end": "668399"
  },
  {
    "text": "oh look we have four new replicas it's responded to the load and now we know that this",
    "start": "668399",
    "end": "674079"
  },
  {
    "text": "deployment needs more resources associated with it so we can look in pixi to see the spikes",
    "start": "674079",
    "end": "680560"
  },
  {
    "text": "now the load has ended on the application but we can see the request per second has gone up",
    "start": "680560",
    "end": "685920"
  },
  {
    "text": "the http latency has gone up too because auto scaling hasn't kicked in yet cpu usage has gone up",
    "start": "685920",
    "end": "692480"
  },
  {
    "text": "these latencies are very problematic so we're counting on the auto scaler to quickly adjust so that we can get to",
    "start": "692480",
    "end": "698320"
  },
  {
    "text": "much more user-friendly latency we'll jump forward again and the load",
    "start": "698320",
    "end": "704640"
  },
  {
    "text": "has ended",
    "start": "704640",
    "end": "707200"
  },
  {
    "text": "so what we can see is that it added four pot or added three pods in",
    "start": "712000",
    "end": "717760"
  },
  {
    "text": "response to the latency that we had and it wasn't running for very long but you can see that near the tail end of it the",
    "start": "717760",
    "end": "724160"
  },
  {
    "text": "latency went down a lot so it did its job if this had been a persistent spike then we would have seen much better",
    "start": "724160",
    "end": "730320"
  },
  {
    "text": "latencies as a result of this auto scaling and we'll come back to this view later for other use cases",
    "start": "730320",
    "end": "739000"
  },
  {
    "text": "yeah so this is just a screenshot of what we saw so you might be wondering how is it that",
    "start": "744000",
    "end": "750480"
  },
  {
    "text": "the horizontal pod auto scaler figures out how many replicas it needs to add it's very simple equation it basically",
    "start": "750480",
    "end": "757200"
  },
  {
    "text": "takes the ratio of the current value for your metric that you've defined like cpu",
    "start": "757200",
    "end": "762720"
  },
  {
    "text": "or throughput divided by the desired value for that metric",
    "start": "762720",
    "end": "768800"
  },
  {
    "text": "and then that's what it uses to determine the right number of replicas",
    "start": "769120",
    "end": "775360"
  },
  {
    "text": "so it's a lot more than just the metric so what knobs do we have",
    "start": "776000",
    "end": "781440"
  },
  {
    "text": "we have lots of questions like how do we set the minimum and maximum number of replicas how often do we look for changes in the",
    "start": "781440",
    "end": "788000"
  },
  {
    "text": "metric i might just have a super transient spike that i don't need to auto scale on but transient to me might be different",
    "start": "788000",
    "end": "794160"
  },
  {
    "text": "than transient to you that might be something you want to auto scale on how quickly we want to add or remove",
    "start": "794160",
    "end": "799279"
  },
  {
    "text": "pods and each time we add or remove pods do we want to cap the amount of change that we",
    "start": "799279",
    "end": "804800"
  },
  {
    "text": "allow at any given period so on the right what we have is a chart of my metric and then slightly lagging",
    "start": "804800",
    "end": "810720"
  },
  {
    "text": "that is the number of pots so when you cap the number of pods",
    "start": "810720",
    "end": "816720"
  },
  {
    "text": "you're basically squishing this curve and i like to look at input and output waves because i come from the hardware space",
    "start": "816720",
    "end": "823040"
  },
  {
    "text": "so what we see here is that for the same metric you can put a you can squash down the maximum number of pods and put a",
    "start": "823040",
    "end": "828480"
  },
  {
    "text": "bottom on the limit as well and this is good when you have resource constraints or you want to provide a certain",
    "start": "828480",
    "end": "833920"
  },
  {
    "text": "guarantee that at least a certain number of replicas are running",
    "start": "833920",
    "end": "838480"
  },
  {
    "text": "you also might add a stabilization period which basically says i don't want to make changes too quickly i want to",
    "start": "839360",
    "end": "845519"
  },
  {
    "text": "wait before making change to make sure that it's actually something i need to scale on now looking at this chart you might say",
    "start": "845519",
    "end": "852560"
  },
  {
    "text": "hey but this looks strictly worse than the one on the left there's this period of time here where",
    "start": "852560",
    "end": "857600"
  },
  {
    "text": "we actually don't have enough pods based on the metric that we've defined and might be wondering why would we even do",
    "start": "857600",
    "end": "862880"
  },
  {
    "text": "this the reason is it reduces the pod turn so for example consider a waveform like",
    "start": "862880",
    "end": "869279"
  },
  {
    "text": "this where the metric is going up and down and up and down and up and down if we were to do the naive thing here and just add pods",
    "start": "869279",
    "end": "877680"
  },
  {
    "text": "as soon as we can as many as we can you're going to be churning all of these pots again and again and again it's not",
    "start": "877680",
    "end": "883760"
  },
  {
    "text": "efficient it would be much better to take like almost like a low-pass filter approach to this you don't want your car to go",
    "start": "883760",
    "end": "890800"
  },
  {
    "text": "telling you every single time there's a bump you want your car to provide a smooth experience and in the same way you want your auto scaler to be smooth",
    "start": "890800",
    "end": "896720"
  },
  {
    "text": "as well so if you were to set a stabilization period for this you could actually keep",
    "start": "896720",
    "end": "901920"
  },
  {
    "text": "this same number of pods instead of having it go back and forth this would be better because it would be",
    "start": "901920",
    "end": "907519"
  },
  {
    "text": "ready for the spike to happen again in the near future",
    "start": "907519",
    "end": "912680"
  },
  {
    "text": "it's a similar thing with capping the max pods to add or remove per period once again if you look right here it",
    "start": "914079",
    "end": "919760"
  },
  {
    "text": "would look as if this is a strictly worse outcome than the one on the left because you have a period of time where",
    "start": "919760",
    "end": "925120"
  },
  {
    "text": "you don't actually have enough pause for the metric that you're looking at so this is something that you can just",
    "start": "925120",
    "end": "931600"
  },
  {
    "text": "configure in the auto scale like anything else it's all specified in the ammo",
    "start": "931600",
    "end": "936720"
  },
  {
    "text": "but consider a workload like this where you have a very brief very transient spike in the metric you do not need to",
    "start": "936720",
    "end": "942480"
  },
  {
    "text": "spin up all of these pods just for this temporary spike you probably want to react in a more",
    "start": "942480",
    "end": "948320"
  },
  {
    "text": "limited way until you have more information that this is actually a persistent change in your workload so that would be",
    "start": "948320",
    "end": "955120"
  },
  {
    "text": "a use for capping the max step size both up and down and you might set these differently you might say hey i want to",
    "start": "955120",
    "end": "961839"
  },
  {
    "text": "scale up really quickly but i want to scale down slowly because i have trust issues that the actual spike has gone",
    "start": "961839",
    "end": "967519"
  },
  {
    "text": "away",
    "start": "967519",
    "end": "969839"
  },
  {
    "text": "all right now let's get into some more of the meat selecting an auto scaling metric for your application",
    "start": "972800",
    "end": "979839"
  },
  {
    "text": "so because the horizontal plot auto scaler is super super powerful it gives you",
    "start": "979839",
    "end": "985199"
  },
  {
    "text": "lots of options for these metrics so let's go over what they are the first one is the one that shared",
    "start": "985199",
    "end": "991040"
  },
  {
    "text": "with the cluster auto scaler and the vertical pod auto scaler it's built in",
    "start": "991040",
    "end": "996480"
  },
  {
    "text": "you don't have to do anything special to use it other than write a yaml file basically it says that you can easily",
    "start": "996480",
    "end": "1003279"
  },
  {
    "text": "scale on resources like cpu and memory that kubernetes already has definitions for",
    "start": "1003279",
    "end": "1009199"
  },
  {
    "text": "number two and this is one that we have lots of great demos on and that demos repo i told you about",
    "start": "1009600",
    "end": "1015279"
  },
  {
    "text": "is the custom metrics api now this is a user-defined metric which means that you get to say what you want",
    "start": "1015279",
    "end": "1021120"
  },
  {
    "text": "this metric to be these are metrics about kubernetes resources so you would define a metric",
    "start": "1021120",
    "end": "1027120"
  },
  {
    "text": "for a pod or a service or something like that they have to be associated with a particular resource",
    "start": "1027120",
    "end": "1033918"
  },
  {
    "text": "i've made all kinds of these you can do latency you can do throughput you can do the depth of your queue",
    "start": "1033919",
    "end": "1039678"
  },
  {
    "text": "really you can do anything that you have defined as the critical thing to scale on in your application",
    "start": "1039679",
    "end": "1046480"
  },
  {
    "text": "finally we have external metrics this is another user to find one but the difference here is that these do not",
    "start": "1047520",
    "end": "1053440"
  },
  {
    "text": "associate with a particular kubernetes resource this might be a business metric like the number of people that are using",
    "start": "1053440",
    "end": "1058960"
  },
  {
    "text": "your application right now",
    "start": "1058960",
    "end": "1062000"
  },
  {
    "text": "there are so many possible bottlenecks in your application cpu obviously",
    "start": "1064080",
    "end": "1069440"
  },
  {
    "text": "memory duh network people don't always think about this one as much but i've seen it happen in practice",
    "start": "1069440",
    "end": "1076400"
  },
  {
    "text": "the number of worker threads maybe you only have two worker threads at once and this pod can simply not take more than",
    "start": "1076400",
    "end": "1081840"
  },
  {
    "text": "that it's just the way it is maybe it's using the gpu or something like that",
    "start": "1081840",
    "end": "1086799"
  },
  {
    "text": "the number of outbound connections it might not be the case that your deployment is the bottleneck there might actually be a downstream dependency it",
    "start": "1087039",
    "end": "1093760"
  },
  {
    "text": "would be completely pointless to scale up on something like cpu when the thing isn't even cpu bound",
    "start": "1093760",
    "end": "1100000"
  },
  {
    "text": "q depth as we mentioned this is another big one that we see in practice a lot there's so many more",
    "start": "1100720",
    "end": "1106880"
  },
  {
    "text": "the best metric depends on your workload it's really important to characterize your workload before choosing a metric",
    "start": "1106880",
    "end": "1112720"
  },
  {
    "text": "to auto scale on and that's where using open source tools like pixi can help",
    "start": "1112720",
    "end": "1118640"
  },
  {
    "text": "so let's look at an example application so in the upper left is the same view as",
    "start": "1118799",
    "end": "1125200"
  },
  {
    "text": "the one we showed the video of request per second it's also showing errors per second in green so what we're seeing for this",
    "start": "1125200",
    "end": "1131440"
  },
  {
    "text": "application before auto scaling is the request per second is going up to",
    "start": "1131440",
    "end": "1136480"
  },
  {
    "text": "about 25 when it's hitting load and when that happens we're seeing a",
    "start": "1136480",
    "end": "1141679"
  },
  {
    "text": "five second latency this is probably unacceptable it might be fine for your workload but let's just imagine it's a",
    "start": "1141679",
    "end": "1147440"
  },
  {
    "text": "common case and this is unacceptable spike in latency but the weird thing is we're seeing a",
    "start": "1147440",
    "end": "1153039"
  },
  {
    "text": "low cpu the cpu is hitting eight percent how is the latency so bad",
    "start": "1153039",
    "end": "1159039"
  },
  {
    "text": "that's because this is one of those workloads that actually has a q latency it puts items in a queue and",
    "start": "1159039",
    "end": "1165600"
  },
  {
    "text": "then it processes them at a certain speed the dependency the latency is not actually in the cpu or resources that",
    "start": "1165600",
    "end": "1171919"
  },
  {
    "text": "you would expect but after we auto scale on latency",
    "start": "1171919",
    "end": "1177120"
  },
  {
    "text": "we can see that we've added a certain number of pods it's reached five and this has had a huge impact both on the",
    "start": "1177120",
    "end": "1183840"
  },
  {
    "text": "request per second which is now four x higher and on the latency which starts off at",
    "start": "1183840",
    "end": "1188880"
  },
  {
    "text": "that five second problematic zone but then it drops to two which is a huge improvement",
    "start": "1188880",
    "end": "1193919"
  },
  {
    "text": "and the cpu usage is stable as well so this would be a good example of a case where you really have to think",
    "start": "1193919",
    "end": "1199280"
  },
  {
    "text": "through the metric that you're auto scaling on let's take another example this is a",
    "start": "1199280",
    "end": "1206400"
  },
  {
    "text": "really problematic workload it's maybe a little bit hard to see but over 90 of these requests under load",
    "start": "1206400",
    "end": "1213679"
  },
  {
    "text": "are errors and the latency drops when that starts to happen",
    "start": "1213679",
    "end": "1219280"
  },
  {
    "text": "what's going on for this workload cpu is really high",
    "start": "1219280",
    "end": "1225360"
  },
  {
    "text": "so what's happening in this workload is that there is a certain capacity it has and it starts turning away requests as",
    "start": "1226480",
    "end": "1232480"
  },
  {
    "text": "soon as it reaches that capacity so let's say i can only handle 10 things at once i'm just going to send an error",
    "start": "1232480",
    "end": "1238000"
  },
  {
    "text": "back out and tell you too bad i'm full go away when that happens the latency drops",
    "start": "1238000",
    "end": "1243919"
  },
  {
    "text": "because it's actually really fast to say no go away it's a lot faster to do that than it is to actually do the work of",
    "start": "1243919",
    "end": "1249840"
  },
  {
    "text": "the request so if we were to scale on latency here it would actually do the inverse of what",
    "start": "1249840",
    "end": "1255440"
  },
  {
    "text": "we want we don't want to scale on latency when latency is actually lower in our problematic state",
    "start": "1255440",
    "end": "1262640"
  },
  {
    "text": "so what can we do instead we can actually scale using a custom",
    "start": "1262640",
    "end": "1267679"
  },
  {
    "text": "metric for error rate so this is something that we built out in the pixie demos as well you can actually look at",
    "start": "1267679",
    "end": "1273919"
  },
  {
    "text": "the error rate in your application and provide a custom metric to kubernetes based on that",
    "start": "1273919",
    "end": "1279520"
  },
  {
    "text": "so what we see here is that in the beginning most of the requests are sending errors it's saying i'm full i",
    "start": "1279520",
    "end": "1285120"
  },
  {
    "text": "can't take your capacity but after we auto scale it adds 10 more repo or adds nine more replicas",
    "start": "1285120",
    "end": "1291919"
  },
  {
    "text": "and we see a huge drop in the number of errors we see the latency rise as well but",
    "start": "1291919",
    "end": "1298320"
  },
  {
    "text": "that's expected more of these requests are actually being handled",
    "start": "1298320",
    "end": "1304520"
  },
  {
    "text": "okay so now for something totally deranged",
    "start": "1307679",
    "end": "1312240"
  },
  {
    "text": "so for those of us who are a little bit far away from our cs classes let's review really quickly what a",
    "start": "1312720",
    "end": "1318799"
  },
  {
    "text": "turing machine is and what turn completeness is before we get into it a turing machine is capable it's a",
    "start": "1318799",
    "end": "1325039"
  },
  {
    "text": "theoretical idea and it's capable of any computation given enough time and tape what do i mean by tape a turing machine",
    "start": "1325039",
    "end": "1332720"
  },
  {
    "text": "takes an input program and it does the computation and it writes an output value to a theoretical",
    "start": "1332720",
    "end": "1339120"
  },
  {
    "text": "tape and something is a turing machine when you can do any possible computation on",
    "start": "1339120",
    "end": "1344640"
  },
  {
    "text": "that thing so for example if all i had was the ability to add one that wouldn't be turing complete because",
    "start": "1344640",
    "end": "1350799"
  },
  {
    "text": "that structure instruction is not complex enough to do any arbitrary computation that i might want to do",
    "start": "1350799",
    "end": "1358080"
  },
  {
    "text": "but there is something that is an instruction that would be turned complete on its own",
    "start": "1359520",
    "end": "1364880"
  },
  {
    "text": "this is called sublek actually i'm not sure how it's pronounced because i've only read about it so if that's completely wrong i",
    "start": "1364880",
    "end": "1371280"
  },
  {
    "text": "apologize but anyway sublek is a one instruction set computer and it's",
    "start": "1371280",
    "end": "1376320"
  },
  {
    "text": "sufficient for touring completeness on its own it may be very convoluted but you can actually write any program you want",
    "start": "1376320",
    "end": "1382880"
  },
  {
    "text": "using exclusively sublux what does it do what it does is it says i'm going to",
    "start": "1382880",
    "end": "1388720"
  },
  {
    "text": "compare two values and subtract one from the other",
    "start": "1388720",
    "end": "1395600"
  },
  {
    "text": "if that is less than zero i'm going to jump to my next instruction provided in the command otherwise i'm just going to",
    "start": "1395600",
    "end": "1401760"
  },
  {
    "text": "continue on with my program like normal i'll leave the proof of why this is true and complete to the mathematicians",
    "start": "1401760",
    "end": "1409600"
  },
  {
    "text": "so how are we going to make a horizontal plot autoscaler turing complete what we're going to do is have this",
    "start": "1409840",
    "end": "1416000"
  },
  {
    "text": "horizontal pod auto scaler evaluate a different sublet command every interval",
    "start": "1416000",
    "end": "1421520"
  },
  {
    "text": "that it's queried for its metrics so we'll take a sublec program",
    "start": "1421520",
    "end": "1426640"
  },
  {
    "text": "we'll execute one instruction at a time and then we'll set the number of output",
    "start": "1426640",
    "end": "1431679"
  },
  {
    "text": "plots and that output pods is the result of the computation",
    "start": "1431679",
    "end": "1437440"
  },
  {
    "text": "so you can think of the tape as the time series of the number of pods over time and that's what the output value is",
    "start": "1437440",
    "end": "1445520"
  },
  {
    "text": "how do you get the input program well i had to get a little bit creative",
    "start": "1445520",
    "end": "1451039"
  },
  {
    "text": "this is my input program we'll split on x now we have a series of numbers",
    "start": "1451039",
    "end": "1459039"
  },
  {
    "text": "there's three input arguments per command so this is my result",
    "start": "1459039",
    "end": "1464080"
  },
  {
    "text": "so you can specify a deployment name like this and say i want to auto scale on this deployment and that's how the auto scaler loads in the input program",
    "start": "1464080",
    "end": "1472000"
  },
  {
    "text": "like i said this is totally deranged and just a way of demonstrating the power of the horizontal pod auto scaler",
    "start": "1472000",
    "end": "1479360"
  },
  {
    "text": "how do we set the certain number of output pods what we do is we take this out this equation and turn it on its head",
    "start": "1479919",
    "end": "1486720"
  },
  {
    "text": "the autoscaler looks at the current number of pods and it backwards calculates what the metric value has to be to ensure a certain number of output",
    "start": "1486720",
    "end": "1493520"
  },
  {
    "text": "pods so let's take a look at it and see how it actually works",
    "start": "1493520",
    "end": "1501640"
  },
  {
    "text": "and i'll try to narrate for the people on the back and we're going to put this at 2x",
    "start": "1502000",
    "end": "1509120"
  },
  {
    "text": "because that is the power of a video so this is a completely open source",
    "start": "1509120",
    "end": "1515440"
  },
  {
    "text": "thing you can look at it and try it out yourself so",
    "start": "1515440",
    "end": "1520640"
  },
  {
    "text": "i've created my metrics api server i have a deployment which has a funky",
    "start": "1520640",
    "end": "1526640"
  },
  {
    "text": "name like we saw",
    "start": "1526640",
    "end": "1529840"
  },
  {
    "text": "and what we're going to do is we're going to watch the output number of pods over time and make sure it hits the values that we want you might be",
    "start": "1532720",
    "end": "1538720"
  },
  {
    "text": "wondering what this program does what it does is it prints out high and ascii",
    "start": "1538720",
    "end": "1545840"
  },
  {
    "text": "but it could do anything so basically",
    "start": "1548159",
    "end": "1553200"
  },
  {
    "text": "72 is going to correspond to the ascii value for h capital h",
    "start": "1553200",
    "end": "1559679"
  },
  {
    "text": "you can see next the auto scaler has computed a value of 105. it's happened really fast because i've sped it up",
    "start": "1559679",
    "end": "1565200"
  },
  {
    "text": "so you can see now it's hitting this value up here well it's actually 107 because i add two to every result so we never have a",
    "start": "1565200",
    "end": "1571360"
  },
  {
    "text": "negative result but the point is that we're seeing the number of pods here",
    "start": "1571360",
    "end": "1577120"
  },
  {
    "text": "hit the result of this computation so it started out 72 or 74 once you add two 105 or 107",
    "start": "1577120",
    "end": "1585039"
  },
  {
    "text": "once you add 2 and then the program is going to terminate",
    "start": "1585039",
    "end": "1589759"
  },
  {
    "text": "so there you go this autoscaler outputs high",
    "start": "1591919",
    "end": "1597000"
  },
  {
    "text": "okay great so this is kind of the end of the prepared remarks but i just wanted to give some shout outs to the",
    "start": "1602159",
    "end": "1607840"
  },
  {
    "text": "kubernetes sig instrumentation they've made it so easy to get started with defining your own custom metric for",
    "start": "1607840",
    "end": "1613760"
  },
  {
    "text": "horizontal pod auto scaling so definitely check them out if you want to do that also",
    "start": "1613760",
    "end": "1619440"
  },
  {
    "text": "check out pixi which i use for a lot of these demos and then finally thanks to jana dogen",
    "start": "1619440",
    "end": "1626159"
  },
  {
    "text": "who made this awesome load generator that's it",
    "start": "1626159",
    "end": "1633559"
  },
  {
    "text": "if you have questions raise your hand",
    "start": "1642240",
    "end": "1646240"
  },
  {
    "text": "okay one over there",
    "start": "1649200",
    "end": "1652760"
  },
  {
    "text": "oh i forgot to say also we have swag at the back for the pixie open source project if you want to pick it up",
    "start": "1654880",
    "end": "1661600"
  },
  {
    "text": "hey there thank you for your really great talk um i have a question did you",
    "start": "1661679",
    "end": "1667039"
  },
  {
    "text": "use the vertical autoscaler also on openshift do you have experience with",
    "start": "1667039",
    "end": "1672320"
  },
  {
    "text": "that using auto scaling with openshift yes um so i don't have direct experience",
    "start": "1672320",
    "end": "1677919"
  },
  {
    "text": "using it with openshift but i assume that it would work just fine because it should support",
    "start": "1677919",
    "end": "1684480"
  },
  {
    "text": "you know most kubernetes features and this is a built-in kubernetes feature but if you want to give it a try you can",
    "start": "1684480",
    "end": "1690320"
  },
  {
    "text": "try that pixie demos repo that i mentioned and you can just deploy it on your application and see if it works",
    "start": "1690320",
    "end": "1697840"
  },
  {
    "text": "you have a question you didn't said anything about the",
    "start": "1699200",
    "end": "1705200"
  },
  {
    "text": "limit you said about the request not the limits i know limit is a bad",
    "start": "1705200",
    "end": "1710720"
  },
  {
    "text": "bad usage it's supposed to be deprecated i think but it's supposed to",
    "start": "1710720",
    "end": "1715919"
  },
  {
    "text": "we need to say a few words so people won't use it yeah that's a good point",
    "start": "1715919",
    "end": "1722640"
  },
  {
    "text": "oh that that was more of a statement than a question yeah",
    "start": "1723600",
    "end": "1728559"
  },
  {
    "text": "uh at the back where you're leaving could you keep the noise down some people are still trying to",
    "start": "1730320",
    "end": "1735440"
  },
  {
    "text": "follow along thank you who's got a question",
    "start": "1735440",
    "end": "1740880"
  },
  {
    "text": "no no more oh one yeah you need to hold your hand up high",
    "start": "1743760",
    "end": "1751159"
  },
  {
    "text": "okay so what's going to happen when autoscaler hits a limit of some research",
    "start": "1754399",
    "end": "1759919"
  },
  {
    "text": "on the cluster so are you saying that it's targeted at a",
    "start": "1759919",
    "end": "1765520"
  },
  {
    "text": "value above the limit of the cluster resources or when you're saying it hits the limit that you have set",
    "start": "1765520",
    "end": "1772640"
  },
  {
    "text": "the limit of the cluster resources yeah so what you'll probably see in that scenario is that the pods at least for",
    "start": "1773679",
    "end": "1779360"
  },
  {
    "text": "the horizontal plot auto scaler will be pending and so that's why it's really important",
    "start": "1779360",
    "end": "1785840"
  },
  {
    "text": "to always check your auto scaling policies with the limits that you have especially with your cloud provider",
    "start": "1785840",
    "end": "1791200"
  },
  {
    "text": "because you don't want to get a bunch of pods stuck in a pending state",
    "start": "1791200",
    "end": "1795840"
  },
  {
    "text": "is that okay one more over here",
    "start": "1797360",
    "end": "1802559"
  },
  {
    "text": "hi natalie thanks first um for your great and structured talk um one question and uh lessons learned so using",
    "start": "1807039",
    "end": "1813600"
  },
  {
    "text": "the hpa um with a sidecar proxy like we have one pod ending up having two",
    "start": "1813600",
    "end": "1818720"
  },
  {
    "text": "containers like istio um so the hpa is always using the average of the predefined metrics so i could have istio",
    "start": "1818720",
    "end": "1825760"
  },
  {
    "text": "using 200 of the requested cpu and my application only 50 so i end up with a",
    "start": "1825760",
    "end": "1831840"
  },
  {
    "text": "very messed up state do you have a lessons learned there or like best practices yeah that's a really great",
    "start": "1831840",
    "end": "1837679"
  },
  {
    "text": "point i think that it really depends on what you want to scale on like if you're trying to look at the joint utilization of the entire",
    "start": "1837679",
    "end": "1844480"
  },
  {
    "text": "pot across containers then the built-in behavior can work just fine but in your case if you care a lot more about one of",
    "start": "1844480",
    "end": "1850480"
  },
  {
    "text": "those than the other you might want to create a custom metric for the one that you care about because it's your way of specifying the exact thing to behave",
    "start": "1850480",
    "end": "1857120"
  },
  {
    "text": "with okay i am going to read out oh i think",
    "start": "1857120",
    "end": "1864320"
  },
  {
    "text": "there was one behind you actually i'm still going to read out a question from the online uh a lot of people following",
    "start": "1864320",
    "end": "1870640"
  },
  {
    "text": "along online and some of them have asked questions oh great uh so first one was could you re-share the",
    "start": "1870640",
    "end": "1876399"
  },
  {
    "text": "demo url maybe you could just put that slide back up oh yeah",
    "start": "1876399",
    "end": "1881840"
  },
  {
    "text": "um and the second one was is it better to use a small number of large pods or a",
    "start": "1881840",
    "end": "1887039"
  },
  {
    "text": "large number of small pods that is such a good question and there's not really",
    "start": "1887039",
    "end": "1892799"
  },
  {
    "text": "one size fits all as an answer it's something that really depends on your application so what i would generally say is that if",
    "start": "1892799",
    "end": "1900559"
  },
  {
    "text": "you have a lot of small pods that can be really good for a more stateless application and a more stateful",
    "start": "1900559",
    "end": "1906880"
  },
  {
    "text": "application you might prefer to size the pods bigger but that's just something that i've seen in practice it's not a",
    "start": "1906880",
    "end": "1912000"
  },
  {
    "text": "one size fits all i think that the more that you can build your application to scale out proportionally",
    "start": "1912000",
    "end": "1918799"
  },
  {
    "text": "to pods rather than having one megapod that's more of a pet",
    "start": "1918799",
    "end": "1924080"
  },
  {
    "text": "it's more canonical to do it in more pods rather than just one megapod but there are some cases that",
    "start": "1924080",
    "end": "1931120"
  },
  {
    "text": "you run into where you have a staple application and you really do need a certain amount of heavy resources for",
    "start": "1931120",
    "end": "1936320"
  },
  {
    "text": "that pod thank you next question yeah so you said the resource for hpa is",
    "start": "1936320",
    "end": "1943360"
  },
  {
    "text": "important so is it recommendable to mix together with the cluster autoscaler with hpa yes those",
    "start": "1943360",
    "end": "1951760"
  },
  {
    "text": "two can play together the only ones you have to be careful about are the vertical pod auto scaler with the horizontal pod auto scaler if you're",
    "start": "1951760",
    "end": "1958000"
  },
  {
    "text": "scaling on the same metric hi uh so do you have experiences or",
    "start": "1958000",
    "end": "1964640"
  },
  {
    "text": "strategies for predictive auto scaling as well and not reactive on metrics",
    "start": "1964640",
    "end": "1971279"
  },
  {
    "text": "so what you're trying to say is like as opposed to responding to a change in metric that",
    "start": "1971279",
    "end": "1977120"
  },
  {
    "text": "you've observed you want to actually predict that the metric will rise and then auto scale based on that",
    "start": "1977120",
    "end": "1982799"
  },
  {
    "text": "yeah it's a really interesting question i think that probably the best practice for that today is try to create a custom",
    "start": "1982799",
    "end": "1989840"
  },
  {
    "text": "metric that actually does that prediction and auto scale based on that and that's something that um you know",
    "start": "1989840",
    "end": "1996480"
  },
  {
    "text": "the specifics of how to do that would probably depend on what you're predicting but at the end of the day a prediction a prediction is just another",
    "start": "1996480",
    "end": "2002480"
  },
  {
    "text": "metric hi there sorry if i missed this at the",
    "start": "2002480",
    "end": "2008080"
  },
  {
    "text": "beginning but um how does this differ to using something like say kida to manage the auto scaling of pods because kida",
    "start": "2008080",
    "end": "2015039"
  },
  {
    "text": "supports kind of custom metrics through prometheus so i was just wondering you know what advantages does this have over",
    "start": "2015039",
    "end": "2020960"
  },
  {
    "text": "keita and vice versa yeah for sure so i think that you know what i've covered today is the stuff",
    "start": "2020960",
    "end": "2026559"
  },
  {
    "text": "that's built into kubernetes so it's the stuff that if you're using kubernetes anyone can use",
    "start": "2026559",
    "end": "2031760"
  },
  {
    "text": "there's cloud providers with their own node scalars there's you know you know keita also provides great auto scaling",
    "start": "2031760",
    "end": "2037840"
  },
  {
    "text": "so what i would say is you know if you're just looking to use the kubernetes api you would use these",
    "start": "2037840",
    "end": "2043360"
  },
  {
    "text": "things but if you're already a user of something that provides auto scaling you'd have to compare it against that and see which features",
    "start": "2043360",
    "end": "2049919"
  },
  {
    "text": "meet your use case the best and i realize that that's a non-answer but it's the kind of thing that everyone's",
    "start": "2049919",
    "end": "2054960"
  },
  {
    "text": "situation is very different okay i think this is gonna be the last",
    "start": "2054960",
    "end": "2060079"
  },
  {
    "text": "one",
    "start": "2060079",
    "end": "2062320"
  },
  {
    "text": "hi there uh is it possible to scale using multiple metrics or do you just have to choose one",
    "start": "2065839",
    "end": "2071599"
  },
  {
    "text": "it is possible to scale using multiple metrics and what you can do then is say to the auto scaler how do you want to",
    "start": "2071599",
    "end": "2077358"
  },
  {
    "text": "decide which one wins so you'd have to set a policy and that policy would tell you",
    "start": "2077359",
    "end": "2082480"
  },
  {
    "text": "when the two things disagree which one to defer to okay thank you very much",
    "start": "2082480",
    "end": "2088079"
  },
  {
    "text": "okay i think we're at time thanks everyone",
    "start": "2088079",
    "end": "2093480"
  }
]