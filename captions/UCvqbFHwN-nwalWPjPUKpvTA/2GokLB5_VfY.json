[
  {
    "text": "all right um thank you everyone for coming we didn't expect such a full room",
    "start": "120",
    "end": "6359"
  },
  {
    "text": "um so this session will be uh both an introduction and a deep dive into Thanos",
    "start": "6359",
    "end": "11360"
  },
  {
    "text": "so the first part we're going to do like an overview of Thanos what it is how you",
    "start": "11360",
    "end": "16920"
  },
  {
    "text": "run it and then in the second part we will cover some of the improvements that have been done recently and then we'll",
    "start": "16920",
    "end": "23400"
  },
  {
    "text": "talk about some more exciting stuff some concrete use cases and so on maybe just a quick show of hands how many people",
    "start": "23400",
    "end": "29279"
  },
  {
    "text": "here run Thanos in production already okay that's so for you you might find",
    "start": "29279",
    "end": "36420"
  },
  {
    "text": "the first part of it boring but maybe you'll also learn something new before we get started uh just a quick",
    "start": "36420",
    "end": "43020"
  },
  {
    "text": "round of introductions my name is Philip I'm a production engineer at Shopify I",
    "start": "43020",
    "end": "48180"
  },
  {
    "text": "work in the infrastructure team also part of the towns maintainers team and also in the past I helped maintain",
    "start": "48180",
    "end": "53820"
  },
  {
    "text": "Primitives operator and Cube State metrics and with me today's sauce water",
    "start": "53820",
    "end": "60000"
  },
  {
    "text": "yeah so my name is I'm a software engineer at Red Hat where I work on an internal monitoring",
    "start": "60000",
    "end": "67020"
  },
  {
    "text": "platform I'm also a maintenance Thanos and was previously a g-socumented under the same project I also helped maintain",
    "start": "67020",
    "end": "74220"
  },
  {
    "text": "a couple of cncf adjacent projects like mdocs and observatorium",
    "start": "74220",
    "end": "80360"
  },
  {
    "text": "you can find me as ATM code on Twitter GitHub or pretty much anywhere else",
    "start": "80360",
    "end": "86880"
  },
  {
    "text": "all right um okay so now getting into the good stuff um it's kind of hard to talk about",
    "start": "86880",
    "end": "93000"
  },
  {
    "text": "Thanos without mentioning Prometheus first um Prometheus I expect most of you to be",
    "start": "93000",
    "end": "100020"
  },
  {
    "text": "to have heard at least about it it's a standalone monitoring server or system",
    "start": "100020",
    "end": "105659"
  },
  {
    "text": "that you drop in your environment very close to your applications for me Tuesday scrapes metrics from",
    "start": "105659",
    "end": "112259"
  },
  {
    "text": "applications and then stores them locally so Prometheus doesn't have any external",
    "start": "112259",
    "end": "117299"
  },
  {
    "text": "dependencies which means you can't actually offload metrics data into an external database",
    "start": "117299",
    "end": "123500"
  },
  {
    "text": "and as a result it has to store those metrics on disk it also has like a very",
    "start": "123500",
    "end": "129300"
  },
  {
    "text": "flexible query language which we call pramcal and using that query language we can also write alerts which means we can",
    "start": "129300",
    "end": "136440"
  },
  {
    "text": "have alerts that are constantly executed and they let us know when something goes wrong when if for example a threshold is",
    "start": "136440",
    "end": "143160"
  },
  {
    "text": "violated uh prameters can fire on the Earth and so if we zoom into the Prometheus",
    "start": "143160",
    "end": "149400"
  },
  {
    "text": "design and what it's composed of we see four kind of key modules which are",
    "start": "149400",
    "end": "156300"
  },
  {
    "text": "present in the Prometheus code base and functionality which are very relevant for what Thanos is",
    "start": "156300",
    "end": "162000"
  },
  {
    "text": "um for what we'll be speaking today so we have the rule manager which executes alerting rules we have the query engine",
    "start": "162000",
    "end": "168720"
  },
  {
    "text": "which executes primeql and then we have the time series database which",
    "start": "168720",
    "end": "173879"
  },
  {
    "text": "Prometheus uses to store metrics on disk and it also has something called a compactor which optimizes the layout of",
    "start": "173879",
    "end": "181739"
  },
  {
    "text": "of metrics on disk over time and so if we were to now use Prometheus",
    "start": "181739",
    "end": "188160"
  },
  {
    "text": "to monitor various environments and we would have to deploy like at least one",
    "start": "188160",
    "end": "193500"
  },
  {
    "text": "Prometheus instance per environment and Prometheus cannot handle like a large",
    "start": "193500",
    "end": "198980"
  },
  {
    "text": "multi-environment setup and so this week this can work like Prometheus will take care of each",
    "start": "198980",
    "end": "204840"
  },
  {
    "text": "individual cluster for example but very quickly we see that we won't be able to",
    "start": "204840",
    "end": "211319"
  },
  {
    "text": "get something that we call a global view of the data so we won't be able to query metrics across environments or across",
    "start": "211319",
    "end": "217620"
  },
  {
    "text": "kubernetes clusters or maybe across namespaces we also won't be able to",
    "start": "217620",
    "end": "222659"
  },
  {
    "text": "retain data for a long period of time because of the local storage constraints so disks can be hard to move or move",
    "start": "222659",
    "end": "229200"
  },
  {
    "text": "around or they could be expensive with one editing data for a year for example and so on also the the resolution that",
    "start": "229200",
    "end": "237000"
  },
  {
    "text": "Primitives has it's going to scrape every 30 seconds and if we want to execute a query across say three months",
    "start": "237000",
    "end": "243900"
  },
  {
    "text": "of data or six months of data that resolution is going to be very dense and very high so a query of that duration is",
    "start": "243900",
    "end": "251459"
  },
  {
    "text": "going to be very challenging to execute and so this is where Thanos comes in this is these are the gaps that Thanos",
    "start": "251459",
    "end": "258060"
  },
  {
    "text": "system tries to fill out um it has features such as a global view",
    "start": "258060",
    "end": "264500"
  },
  {
    "text": "a long-term retention it has down sampling built in over time so it can",
    "start": "264500",
    "end": "270419"
  },
  {
    "text": "deduce the resolution of scrape samples um has also some nice multi-tennessy",
    "start": "270419",
    "end": "276600"
  },
  {
    "text": "features and so on so um starting kind of with a global view how tunnel solves this global view",
    "start": "276600",
    "end": "282660"
  },
  {
    "text": "problem uh and um maybe before we kind of explain how it does it it's worth mentioning what a",
    "start": "282660",
    "end": "289919"
  },
  {
    "text": "global what we mean by a global view so if we were to have a set of Prometheus instances and we would",
    "start": "289919",
    "end": "296040"
  },
  {
    "text": "execute a query we really want that query to be executed across the entire fleet so if you want to say give me a",
    "start": "296040",
    "end": "302280"
  },
  {
    "text": "sum over given metric we really want the sum across for all primitive instances",
    "start": "302280",
    "end": "307440"
  },
  {
    "text": "and so the way that we do it is downloads basically takes out this module called promptql from from",
    "start": "307440",
    "end": "313979"
  },
  {
    "text": "Prometheus bundles dot into a standalone service that can be run and scaled",
    "start": "313979",
    "end": "319139"
  },
  {
    "text": "independently and then that's and in addition to this we also Define something that we call the store API and",
    "start": "319139",
    "end": "326400"
  },
  {
    "text": "as you can see here the second RPC that's the series RPC which the queryer",
    "start": "326400",
    "end": "332880"
  },
  {
    "text": "can use to request time series data from any component um and so since Prometheus doesn't",
    "start": "332880",
    "end": "338940"
  },
  {
    "text": "actually understand this API what we do is in the tunnels ecosystem we have",
    "start": "338940",
    "end": "344039"
  },
  {
    "text": "something called the southern sidecar which we deploy next to each Prometheus instance the queryer can then talk to",
    "start": "344039",
    "end": "350759"
  },
  {
    "text": "the sidecar and the sidecar takes data out of Prometheus and by doing this now",
    "start": "350759",
    "end": "356340"
  },
  {
    "text": "the query can connect to multiple Prometheus instances so um we would basically end up if we were",
    "start": "356340",
    "end": "363120"
  },
  {
    "text": "to get a global view we'd end up with something like this where we have multiple primitive systems sidecar for",
    "start": "363120",
    "end": "369840"
  },
  {
    "text": "each Primitives instance and Aquarium that's connected to all of the sidecares and so now the query is responsible for",
    "start": "369840",
    "end": "375419"
  },
  {
    "text": "executing queries and so by extension of this by having",
    "start": "375419",
    "end": "381060"
  },
  {
    "text": "this global view over the data we can now also have Global alerting and Global",
    "start": "381060",
    "end": "386400"
  },
  {
    "text": "rule recordings so for example if we want something like the global error rate across multiple environments or we",
    "start": "386400",
    "end": "392699"
  },
  {
    "text": "have an SLO for the P90 latency we'd be able to do that with something that we call the Thanos ruler so yet again we",
    "start": "392699",
    "end": "400800"
  },
  {
    "text": "pull out the component from Prometheus we take out the rule engine which and we reuse a lot of the code from the from",
    "start": "400800",
    "end": "407580"
  },
  {
    "text": "the rule engine we package that into something that we call a Thanos ruler the ruler is then connected to the",
    "start": "407580",
    "end": "414060"
  },
  {
    "text": "queryer and now through the query it has access to the entire data set",
    "start": "414060",
    "end": "419819"
  },
  {
    "text": "and so in our baby example from before we can now as mentioned we can deploy a",
    "start": "419819",
    "end": "425039"
  },
  {
    "text": "Thanos ruler and the ruler can talk can be connected to the queryer and because",
    "start": "425039",
    "end": "430080"
  },
  {
    "text": "the query has a global view over the whole data set the ruler back extension can execute alerting rules across the",
    "start": "430080",
    "end": "437039"
  },
  {
    "text": "entireties all right we also mentioned that it's kind of challenging to store data on",
    "start": "437039",
    "end": "443880"
  },
  {
    "text": "disk for longer periods of time it's also hard to move disks around and so for this purpose",
    "start": "443880",
    "end": "451500"
  },
  {
    "text": "um the Thanos sidecar can be configured to upload data from Prometheus into",
    "start": "451500",
    "end": "458280"
  },
  {
    "text": "object storage so Prometheus is going to create data on disk every two hours by default and the",
    "start": "458280",
    "end": "466020"
  },
  {
    "text": "sidecar can then upload the data to object storage and then the Thanos store",
    "start": "466020",
    "end": "472440"
  },
  {
    "text": "Gateway can query the data from object storage directly um and so the both the store Gateway and",
    "start": "472440",
    "end": "479340"
  },
  {
    "text": "the sidecar now implement the same API so the queryer can talk to both of them so in our again if we extend our example",
    "start": "479340",
    "end": "486120"
  },
  {
    "text": "from before we would configure both of these sidecars to upload data to for",
    "start": "486120",
    "end": "491460"
  },
  {
    "text": "example an S3 bucket a GCS bucket and we will deploy the store Gateway which has",
    "start": "491460",
    "end": "497099"
  },
  {
    "text": "the same store API like a sidecar and so the query in this case would get the",
    "start": "497099",
    "end": "502259"
  },
  {
    "text": "latest data from Prometheus to the sidecars and would get historical data from the store Gateway",
    "start": "502259",
    "end": "510199"
  },
  {
    "text": "and directly getting to object storage so there's two kind of important things",
    "start": "510199",
    "end": "517260"
  },
  {
    "text": "to note here the store Gateway doesn't actually download data from object storage it downloads a very small parts",
    "start": "517260",
    "end": "524099"
  },
  {
    "text": "of of that particular data set and then it uses the we call this an index header",
    "start": "524099",
    "end": "531120"
  },
  {
    "text": "and then it uses that index scatter to make further requests to object storage",
    "start": "531120",
    "end": "536339"
  },
  {
    "text": "on demand as we query data so we don't actually need all these massive disks",
    "start": "536339",
    "end": "541560"
  },
  {
    "text": "data can stay in object storage and we just need to download very small parts",
    "start": "541560",
    "end": "546600"
  },
  {
    "text": "also both the store and the what we call here the compactor which optimizes this",
    "start": "546600",
    "end": "553080"
  },
  {
    "text": "this um this data over time have a UI so if you were to visualize if you wanted",
    "start": "553080",
    "end": "559140"
  },
  {
    "text": "to get a like a visual representation of what your data looks like you can open the UI and here we see",
    "start": "559140",
    "end": "566640"
  },
  {
    "text": "um you know we have these blocks what we call blocks they all have a Time duration some are two our uh sorry two",
    "start": "566640",
    "end": "573779"
  },
  {
    "text": "day blocks other are seven day blocks and so on so as data comes in the",
    "start": "573779",
    "end": "578880"
  },
  {
    "text": "compactor emerges these blocks and creates bigger blocks out of smaller blocks",
    "start": "578880",
    "end": "584040"
  },
  {
    "text": "also this is a like this is um the compactor",
    "start": "584040",
    "end": "589200"
  },
  {
    "text": "um when it's doing its job we get up to something like this but then you know there are cases where the compactor",
    "start": "589200",
    "end": "595500"
  },
  {
    "text": "would fall behind and we would we might see a situation where we have a bunch of",
    "start": "595500",
    "end": "600779"
  },
  {
    "text": "small blocks in object storage and this is um just keep in mind that it's worth",
    "start": "600779",
    "end": "607260"
  },
  {
    "text": "keeping in mind that having a situation like this is going to increase costs against object storage because we have",
    "start": "607260",
    "end": "612779"
  },
  {
    "text": "to make simply more API calls so monitoring the compactor making sure that it's doing a good job is is going",
    "start": "612779",
    "end": "618420"
  },
  {
    "text": "to be important for both cost control but also making sure that queries perform well",
    "start": "618420",
    "end": "624920"
  },
  {
    "text": "um okay and finally um the last kind of component that's been added to the Thomas ecosystem is",
    "start": "625080",
    "end": "631440"
  },
  {
    "text": "something that we call the Thomas receiver um the reason why it exists is because",
    "start": "631440",
    "end": "637140"
  },
  {
    "text": "the sidecar model might be problematic in certain cases so if we have you know",
    "start": "637140",
    "end": "642420"
  },
  {
    "text": "hundreds of clusters for example we might not be able to open ports to all",
    "start": "642420",
    "end": "648120"
  },
  {
    "text": "of these sidecars or to all of these prometheuses that are running all over the place and so",
    "start": "648120",
    "end": "654360"
  },
  {
    "text": "um we have um certain certain situations where a global query or at the root",
    "start": "654360",
    "end": "660420"
  },
  {
    "text": "simply cannot connect to many many different sidecars and so for this reason we've introduced",
    "start": "660420",
    "end": "666779"
  },
  {
    "text": "something that we call the Thanos receiver which is a kind of component",
    "start": "666779",
    "end": "672720"
  },
  {
    "text": "with to which metrics can be pushed to so it's more of a push-based approach",
    "start": "672720",
    "end": "677880"
  },
  {
    "text": "essentially and so what we have here on the right hand side might be something like Prometheus agents which are fairly",
    "start": "677880",
    "end": "684480"
  },
  {
    "text": "lightweight they they can scrape metrics and remote write them to the receiver component and then the quitter just like",
    "start": "684480",
    "end": "691440"
  },
  {
    "text": "it can query the sidecar can also query the receiver and by doing this we get to instead of a pull-based approach to a",
    "start": "691440",
    "end": "698760"
  },
  {
    "text": "push-based approach finally these kind of approaches are not",
    "start": "698760",
    "end": "704160"
  },
  {
    "text": "mutually exclusive you can use them both at the same time so you can be pulling metrics from certain places and you can",
    "start": "704160",
    "end": "710640"
  },
  {
    "text": "also be pushing metrics from other places and this is kind of a fairly common scenario that people have we also",
    "start": "710640",
    "end": "717180"
  },
  {
    "text": "have a similar set of Shopify where we kind of utilize all of these components in a fairly complex setup",
    "start": "717180",
    "end": "723540"
  },
  {
    "text": "okay so that was the the overview the introduction to Thanos and now we'll",
    "start": "723540",
    "end": "728820"
  },
  {
    "text": "look into some of the recent improvements okay so with that detailed introduction",
    "start": "728820",
    "end": "734760"
  },
  {
    "text": "out of the way let's talk a bit about the several new features improvements",
    "start": "734760",
    "end": "739860"
  },
  {
    "text": "and optimizations that the awesome Thanos Community has been cooking up behind the scenes",
    "start": "739860",
    "end": "746040"
  },
  {
    "text": "um starting with the Tesla store Gateway so as Philip mentioned in the introduction the store API acts as a",
    "start": "746040",
    "end": "753180"
  },
  {
    "text": "sort of glue to fetch data from the various from the various Thanos pack-ins so this kind of store Gateway is one",
    "start": "753180",
    "end": "760140"
  },
  {
    "text": "such backend that basically acts as a caching layer and exposes the grpc store",
    "start": "760140",
    "end": "766079"
  },
  {
    "text": "API for fetching chunks from Prometheus format tstv blocks in any object storage",
    "start": "766079",
    "end": "772019"
  },
  {
    "text": "so we use three distinct caches to make this operation of fetching and querying",
    "start": "772019",
    "end": "777899"
  },
  {
    "text": "data from a historical tsdb much more performant for us firstly we cache the",
    "start": "777899",
    "end": "783899"
  },
  {
    "text": "meta.json file for each dstb block on disk alongside a small portion of the",
    "start": "783899",
    "end": "789540"
  },
  {
    "text": "tsdb index file which we call as index header and the index header is basically",
    "start": "789540",
    "end": "794880"
  },
  {
    "text": "a truncated DStv index file we also apart from that we also maintain an",
    "start": "794880",
    "end": "801000"
  },
  {
    "text": "index cache and a caching bucket to Cache the postings series and chunks from the DStv blocks during the store",
    "start": "801000",
    "end": "808079"
  },
  {
    "text": "API C series RPC calls but recently with a larger and larger Thanos installations",
    "start": "808079",
    "end": "814380"
  },
  {
    "text": "which have hundreds of historical tacv blocks and object storage and also need",
    "start": "814380",
    "end": "820200"
  },
  {
    "text": "higher availability via multiple replicas we have observed that caching",
    "start": "820200",
    "end": "825240"
  },
  {
    "text": "the index Adder files on this becomes problematic and just simply unsustainable slowing Gateway startup",
    "start": "825240",
    "end": "832260"
  },
  {
    "text": "time starts to slow down as the Gateway needs to cache all the index Adder files during startup and the disk fills up",
    "start": "832260",
    "end": "838980"
  },
  {
    "text": "very quickly now you could throw money at the problem and increase the size of the disk but it is expensive and not",
    "start": "838980",
    "end": "845519"
  },
  {
    "text": "very practical when you need to run multiple different replicas fortunately there is now a solution for",
    "start": "845519",
    "end": "851880"
  },
  {
    "text": "this which is to just disable the disable caching the index handles on the",
    "start": "851880",
    "end": "857100"
  },
  {
    "text": "disk completely this and that is the store Gateway will now be stateless and not cache the index headers on disk nor",
    "start": "857100",
    "end": "863700"
  },
  {
    "text": "will it load from it um this still doesn't change the storing gateways internal memory representation",
    "start": "863700",
    "end": "870120"
  },
  {
    "text": "it will just create that on the fly during a particular query instead of referring back to the disk",
    "start": "870120",
    "end": "876540"
  },
  {
    "text": "so that's where this new feature enabled the store Gateway becomes fully stateless and this makes it possible to",
    "start": "876540",
    "end": "882720"
  },
  {
    "text": "run store Gateway over hundreds of tscb blocks in object storage without having to pay for Ultra expensive ssds and we",
    "start": "882720",
    "end": "890760"
  },
  {
    "text": "still cash posting series and chunks as we usually do uh the next Improvement that we'd like",
    "start": "890760",
    "end": "897839"
  },
  {
    "text": "to talk about is the quality of service improvements for Thanos especially for",
    "start": "897839",
    "end": "902880"
  },
  {
    "text": "monitoring as a service use cases where you would be serving multiple different tenants",
    "start": "902880",
    "end": "908699"
  },
  {
    "text": "um so the meaning of quality of service in SAS terms is simply the ability to provide uh different priorities to",
    "start": "908699",
    "end": "915300"
  },
  {
    "text": "different users or data flows and to guarantee a certain level of performance and we want to make sure that a single",
    "start": "915300",
    "end": "922620"
  },
  {
    "text": "tenant or user does not ruin and deteriorate services for other tenants so in the context of Thanos there are",
    "start": "922620",
    "end": "929699"
  },
  {
    "text": "basically two data flows for end users that we want to protect from disruptions which which are the read and the right",
    "start": "929699",
    "end": "937139"
  },
  {
    "text": "parts so let's delve into them individually so now consider a scenario",
    "start": "937139",
    "end": "942300"
  },
  {
    "text": "where the in which the receive component is may be deployed in a hashing configuration with multiple uh different",
    "start": "942300",
    "end": "950100"
  },
  {
    "text": "tenants remote rating metrics to it now maybe you have scaled uh the receive notes to only be able to tolerate around",
    "start": "950100",
    "end": "957360"
  },
  {
    "text": "100K series from each tenant now if a certain tenant starts to misbehave and",
    "start": "957360",
    "end": "963000"
  },
  {
    "text": "send way more metrics than that maybe a million series you would start to face uh disruptions in the form of ohms and",
    "start": "963000",
    "end": "970019"
  },
  {
    "text": "receive hash ring will start to lose stability you then have a couple of different options for mitigating that so",
    "start": "970019",
    "end": "976620"
  },
  {
    "text": "you could use an HP or a vpa setup that can scale your thundering up to a",
    "start": "976620",
    "end": "981899"
  },
  {
    "text": "certain point but if a tenant writes even more even more metrics than that you like clear to man crash loop again",
    "start": "981899",
    "end": "988740"
  },
  {
    "text": "you might want to scale infinitely but again that is not very practical or economical",
    "start": "988740",
    "end": "995699"
  },
  {
    "text": "so the way we want to tackle uh this problem is we are granular per tenant",
    "start": "995699",
    "end": "1001220"
  },
  {
    "text": "remote right limits that allow us to keep our right path completely disruption free by ensuring that we only",
    "start": "1001220",
    "end": "1008060"
  },
  {
    "text": "ingest what we allow it to ingest so with this with these limits every",
    "start": "1008060",
    "end": "1014060"
  },
  {
    "text": "time uh product of encoded remote right request comes in we'll check if it is",
    "start": "1014060",
    "end": "1019820"
  },
  {
    "text": "under our request size under request samples and under request series limits",
    "start": "1019820",
    "end": "1025100"
  },
  {
    "text": "and we also ensure that the tenant the remote right request is coming from is still below a certain number of head or",
    "start": "1025100",
    "end": "1032298"
  },
  {
    "text": "active series after which we allow it to be ingested into the particular tenants",
    "start": "1032299",
    "end": "1038000"
  },
  {
    "text": "tscp the configuration for this limit ensures and you can set Global defaults for the",
    "start": "1038000",
    "end": "1045740"
  },
  {
    "text": "limiting configs and override those values per tenant on every single receive node for the active series limit",
    "start": "1045740",
    "end": "1052880"
  },
  {
    "text": "we directly query any Prometheus compatible meta monitoring endpoint to",
    "start": "1052880",
    "end": "1058220"
  },
  {
    "text": "query the total number of head series for each tenant now coming to the query path the way it",
    "start": "1058220",
    "end": "1065179"
  },
  {
    "text": "works is that a user can fire off a promiscal query to a query which can then call several store apis and fetch",
    "start": "1065179",
    "end": "1072200"
  },
  {
    "text": "data using the series RPC call now if the store API happens to be a store gateway then it also downloads data from",
    "start": "1072200",
    "end": "1079400"
  },
  {
    "text": "object storage to fulfill that series IPC call but in the case of two excessive queries that end up may be",
    "start": "1079400",
    "end": "1086419"
  },
  {
    "text": "selecting too much data or too many samples we end up with a couple of different bottlenecks the queryer can",
    "start": "1086419",
    "end": "1093260"
  },
  {
    "text": "start ohm and crash if it ends up fetching too many samples and having to run from Kill functions on top of them",
    "start": "1093260",
    "end": "1100580"
  },
  {
    "text": "uh to fulfill multiple different queries the store Gateway mind also start to if",
    "start": "1100580",
    "end": "1105620"
  },
  {
    "text": "it ends up having to download a lot of tsdb data from object storage to fulfill a single series RPC call and as with",
    "start": "1105620",
    "end": "1113240"
  },
  {
    "text": "everything you can probably throw money at the problem again and scale up vertically and horizontally but that is",
    "start": "1113240",
    "end": "1120200"
  },
  {
    "text": "not really economical or practical so we ended up introducing limits at the",
    "start": "1120200",
    "end": "1125780"
  },
  {
    "text": "store API level and the story Gateway level that allow for more stability in",
    "start": "1125780",
    "end": "1130880"
  },
  {
    "text": "these parts so with these we can set limits uh on every store API implementation like",
    "start": "1130880",
    "end": "1137480"
  },
  {
    "text": "receive rule or store gateway to limit the number of series and samples returned in a single series RPC call",
    "start": "1137480",
    "end": "1144919"
  },
  {
    "text": "this ensures that you have an upper limit on the total data that can be requested by a single from Kill query",
    "start": "1144919",
    "end": "1152059"
  },
  {
    "text": "and with the already present query concurrency limit you can even size your tunnel Square years accordingly we also",
    "start": "1152059",
    "end": "1159980"
  },
  {
    "text": "limit the downloaded bytes on the store gateway to ensure that fetching historical data from the tsdb does not",
    "start": "1159980",
    "end": "1167120"
  },
  {
    "text": "from the object storage tsdbs does not end up booming in so with these limits",
    "start": "1167120",
    "end": "1172280"
  },
  {
    "text": "in place we end up having a pretty comfortable way of operating a multi-tenant monitoring as a service",
    "start": "1172280",
    "end": "1178640"
  },
  {
    "text": "platform with Thanos and ensuring some level of quality of service",
    "start": "1178640",
    "end": "1184760"
  },
  {
    "text": "now let's also talk about another feature which makes the right path on Thanos substantially stable again before",
    "start": "1184760",
    "end": "1191660"
  },
  {
    "text": "which is consistent hashing on receipts so this was a feature which adds a new",
    "start": "1191660",
    "end": "1198260"
  },
  {
    "text": "hashing mode to the Thanos receive component called katama which enables very stable scale up and scaled down",
    "start": "1198260",
    "end": "1205160"
  },
  {
    "text": "scenarios and it is now our recommended way of running Thanos receive hashing so",
    "start": "1205160",
    "end": "1210860"
  },
  {
    "text": "let us take a look into how this works is configured the same way as our old",
    "start": "1210860",
    "end": "1218299"
  },
  {
    "text": "hash mask based hash ring using hashring.json files on every single receive node to specify the topology",
    "start": "1218299",
    "end": "1226160"
  },
  {
    "text": "and we add a flag on each received to switch to catamar mode so in this mode",
    "start": "1226160",
    "end": "1231500"
  },
  {
    "text": "every receive node is assigned a section to manage within the hash ring and these",
    "start": "1231500",
    "end": "1237020"
  },
  {
    "text": "sections are composed of a range of hashes so when a new remote right request is received by the receive node",
    "start": "1237020",
    "end": "1244280"
  },
  {
    "text": "we iterate through the time series present in that request and hash the labels with the name of the Tenon that",
    "start": "1244280",
    "end": "1251419"
  },
  {
    "text": "the request came from we then batch these and batch and forward these time series to the sections that is to the",
    "start": "1251419",
    "end": "1258380"
  },
  {
    "text": "receive notes that should ingest them so with this we ensure a stable ingest path",
    "start": "1258380",
    "end": "1263720"
  },
  {
    "text": "that actually has consistent hashing and hence an even distribution of data for",
    "start": "1263720",
    "end": "1269000"
  },
  {
    "text": "tenants across your receive nodes internally at Red Hat we also use a",
    "start": "1269000",
    "end": "1274520"
  },
  {
    "text": "kubernetes controller which is open sourced Under The Observatory Mark which",
    "start": "1274520",
    "end": "1279679"
  },
  {
    "text": "monitors stateful set configuration for your receives and the Pod status and simultaneously ensures that the receive",
    "start": "1279679",
    "end": "1286160"
  },
  {
    "text": "nodes have the correct updated hashing.json configs so that their",
    "start": "1286160",
    "end": "1291320"
  },
  {
    "text": "internal representation of the received topology is as close to the real world as possible and this makes operating and",
    "start": "1291320",
    "end": "1298940"
  },
  {
    "text": "scaling such setup so much more automated let's now segue into one of the greatest",
    "start": "1298940",
    "end": "1305539"
  },
  {
    "text": "highlights for the past few months which is the new monkey threaded from ql query",
    "start": "1305539",
    "end": "1310820"
  },
  {
    "text": "engine so to provide you with a little bit of context the promises engine currently is",
    "start": "1310820",
    "end": "1318320"
  },
  {
    "text": "a single threaded function that passes and traverses the query abstract syntax",
    "start": "1318320",
    "end": "1323659"
  },
  {
    "text": "tree recursively and evaluates the query result alongside it all at once before",
    "start": "1323659",
    "end": "1329419"
  },
  {
    "text": "returning a result this is the problem kill engine that we all know and love",
    "start": "1329419",
    "end": "1334580"
  },
  {
    "text": "but due to the way distributed systems like Thanos are set up it becomes difficult to query large sets of data",
    "start": "1334580",
    "end": "1341600"
  },
  {
    "text": "that are distributed across different networks not to mention being limited to",
    "start": "1341600",
    "end": "1349039"
  },
  {
    "text": "a single thread causes issues so with that in mind the Thanos from",
    "start": "1349039",
    "end": "1354320"
  },
  {
    "text": "Kill engine project was started by Philip based on the volcano query engine",
    "start": "1354320",
    "end": "1359419"
  },
  {
    "text": "paper with specifies the architecture for an extensible and parallel queration",
    "start": "1359419",
    "end": "1364820"
  },
  {
    "text": "that can utilize concurrency and multiple cores to its fullest and allow",
    "start": "1364820",
    "end": "1369860"
  },
  {
    "text": "space for several different optimization techniques now the current volcano based from Kill",
    "start": "1369860",
    "end": "1376940"
  },
  {
    "text": "engine works somewhat like this it passes the query into an abstract syntax tree using the same Upstream parser of",
    "start": "1376940",
    "end": "1384559"
  },
  {
    "text": "Prometheus it then tries to optimize the query expression by applying several",
    "start": "1384559",
    "end": "1390200"
  },
  {
    "text": "logical plan optimizers for it and one such logical plan optimizers have been",
    "start": "1390200",
    "end": "1395299"
  },
  {
    "text": "applied the query expression is somewhat simpler than it used to be it traverses the ASD again and constructs a tree of",
    "start": "1395299",
    "end": "1403520"
  },
  {
    "text": "executable operators that is a physical query plan the root of this operator",
    "start": "1403520",
    "end": "1409159"
  },
  {
    "text": "tree can now be executed to fetch the final result of the bromqel query and",
    "start": "1409159",
    "end": "1414320"
  },
  {
    "text": "The Operators themselves can use multiple threads or even run parallel to each other",
    "start": "1414320",
    "end": "1420380"
  },
  {
    "text": "so what do these operators look like well essentially they Implement an interface like this and it has two",
    "start": "1420380",
    "end": "1427760"
  },
  {
    "text": "important methods uh the first of which is series which returns all the series",
    "start": "1427760",
    "end": "1432860"
  },
  {
    "text": "that an operator will ever return in its lifetime and this is useful for parent",
    "start": "1432860",
    "end": "1438380"
  },
  {
    "text": "or Upstream operators to allocate the buffers they would need beforehand the",
    "start": "1438380",
    "end": "1444140"
  },
  {
    "text": "second is the next call which Returns the vectors of samples of all series for",
    "start": "1444140",
    "end": "1449960"
  },
  {
    "text": "a particular execution step so every operator calls next and series on its",
    "start": "1449960",
    "end": "1455240"
  },
  {
    "text": "child operators until there is no more data to be written that is until the query has reached leaves",
    "start": "1455240",
    "end": "1462559"
  },
  {
    "text": "now we mentioned operator so it's a good idea to also mention how the data flows between these operators so as mentioned",
    "start": "1462559",
    "end": "1469580"
  },
  {
    "text": "the operators are arranged in a tree like fashion where every operator calls text and this sort of model allows for",
    "start": "1469580",
    "end": "1476120"
  },
  {
    "text": "samples of an individual time series to flow from each execution step from the",
    "start": "1476120",
    "end": "1481280"
  },
  {
    "text": "left most to the right in this diagram since most prompt expressions are aggregations the samples are reduced as",
    "start": "1481280",
    "end": "1488720"
  },
  {
    "text": "they are pulled in by The Operators from the right and because of this the samples can be decoded and kept in",
    "start": "1488720",
    "end": "1494840"
  },
  {
    "text": "memory in patches aside from this operators are aside from the operators that are a one-to-one mapping with",
    "start": "1494840",
    "end": "1501620"
  },
  {
    "text": "regular from Kill constructs we also have some exchange operators that allow for flow control and concurrency",
    "start": "1501620",
    "end": "1509240"
  },
  {
    "text": "so let's see how that works so we have enabled two types of parallelism between the operators",
    "start": "1509240",
    "end": "1515299"
  },
  {
    "text": "um the first one is inter operator parallelism so as The Operators are independent and rely on a common",
    "start": "1515299",
    "end": "1521539"
  },
  {
    "text": "interface they can be run in parallel so as soon as one operator has processed a",
    "start": "1521539",
    "end": "1526700"
  },
  {
    "text": "bunch of samples they can be pulled in by the next and then the next thing is insha",
    "start": "1526700",
    "end": "1532279"
  },
  {
    "text": "operator parallelism so where parallelism can be added within the individual operator using certain",
    "start": "1532279",
    "end": "1538720"
  },
  {
    "text": "coalesc special operators and they would be indistinguishable from regular ones",
    "start": "1538720",
    "end": "1543799"
  },
  {
    "text": "as they pass on data using the same next call so now that we know how it works to some",
    "start": "1543799",
    "end": "1550340"
  },
  {
    "text": "extent on a much higher level let's talk about benchmarking so we have quite a lot of go benchmarks for this project",
    "start": "1550340",
    "end": "1556460"
  },
  {
    "text": "for benchmarking several different forms of queries both instant and range queries against the original engine and",
    "start": "1556460",
    "end": "1564380"
  },
  {
    "text": "we run this on every commit to main on a cncf sponsored equinex GitHub actions",
    "start": "1564380",
    "end": "1569779"
  },
  {
    "text": "Runner and we publish the results on a central web page for everyone to track",
    "start": "1569779",
    "end": "1575659"
  },
  {
    "text": "and finally as this is an alternative Downstream implementation of Chromecast let's talk a bit about how we maintain",
    "start": "1575659",
    "end": "1583159"
  },
  {
    "text": "compatibility with the original engine so we use a special compatibility engine flow which while uh evaluating query",
    "start": "1583159",
    "end": "1591679"
  },
  {
    "text": "checks if the new promptcal engine supports it and if not we fall back to",
    "start": "1591679",
    "end": "1596720"
  },
  {
    "text": "the regular Prometheus essential so why most functions uh have already been ported over such an approach ensures",
    "start": "1596720",
    "end": "1604100"
  },
  {
    "text": "that even if the Upstream spec changes we would still end up being able to support it and nearly all of our tests",
    "start": "1604100",
    "end": "1611360"
  },
  {
    "text": "in this project test the new engine by comparing results between the upstream and the downstream engines and a setting",
    "start": "1611360",
    "end": "1618320"
  },
  {
    "text": "on matches all right that's that was a lot of",
    "start": "1618320",
    "end": "1624679"
  },
  {
    "text": "information right um the something that we're really excited to",
    "start": "1624679",
    "end": "1631039"
  },
  {
    "text": "um to be working on and that's going to end up in the next maybe several months is what we call distributed execution",
    "start": "1631039",
    "end": "1638080"
  },
  {
    "text": "to explain why it's so important in Thanos we can visualize this like small",
    "start": "1638080",
    "end": "1643880"
  },
  {
    "text": "example where we have so Thanos allows us to stack queriors on top of each other so we can have for example a",
    "start": "1643880",
    "end": "1649700"
  },
  {
    "text": "cluster a cluster B they run Prometheus with sidecars and each cluster can have its own query and then you can have a",
    "start": "1649700",
    "end": "1656600"
  },
  {
    "text": "query at the root that's going that's doing basically the Federation and the global view and so today if you execute something",
    "start": "1656600",
    "end": "1664460"
  },
  {
    "text": "like uh sum or an average or account expression what's going to happen is the root",
    "start": "1664460",
    "end": "1670279"
  },
  {
    "text": "queryer is going to pull data through the queriors on the second level is",
    "start": "1670279",
    "end": "1675440"
  },
  {
    "text": "going to use them as proxies essentially and it's going to pull all of the relevant time series in memory and",
    "start": "1675440",
    "end": "1682100"
  },
  {
    "text": "execute the coordinate memory so this is fairly wasteful because the second level quitters are going to be doing very",
    "start": "1682100",
    "end": "1687799"
  },
  {
    "text": "little work even though they are capable of executing prompt call so this has obviously scalability issues as we",
    "start": "1687799",
    "end": "1694940"
  },
  {
    "text": "extend the environment as we add more clusters the root query becomes a bottleneck",
    "start": "1694940",
    "end": "1700100"
  },
  {
    "text": "so what we are aiming to do with distributed execution is the root queryer is going to decompose a query",
    "start": "1700100",
    "end": "1707600"
  },
  {
    "text": "into what we call subquerors and it's going to push them down as low as possible in the query path so in this",
    "start": "1707600",
    "end": "1714260"
  },
  {
    "text": "case the sum will become a sum over these two partitions and then at the",
    "start": "1714260",
    "end": "1719539"
  },
  {
    "text": "root we'll do the sum of those sums and so by doing this the every query in",
    "start": "1719539",
    "end": "1726080"
  },
  {
    "text": "the query path is going to be utilized to its fullest and queries will be able to run much faster and will be much more",
    "start": "1726080",
    "end": "1733100"
  },
  {
    "text": "scalable essentially also the good thing is that any Primal expression can be",
    "start": "1733100",
    "end": "1738380"
  },
  {
    "text": "kind of decomposed this way so for example account can be a sum over count a group can be a group over groups a top",
    "start": "1738380",
    "end": "1745159"
  },
  {
    "text": "top gear or top case and so on so they're very there's maybe one or two aggregations in prom kill which cannot",
    "start": "1745159",
    "end": "1751340"
  },
  {
    "text": "be done this way so this is again super exciting um but still under development we are",
    "start": "1751340",
    "end": "1757039"
  },
  {
    "text": "still kind of figuring out some edge cases um and finally",
    "start": "1757039",
    "end": "1762860"
  },
  {
    "text": "um there is something called native histograms a very powerful feature",
    "start": "1762860",
    "end": "1768860"
  },
  {
    "text": "that's Landing in in prompt ql and in Upstream Prometheus if you're interested",
    "start": "1768860",
    "end": "1774860"
  },
  {
    "text": "in in how in what Native histograms are how they work uh Ganesh and Bjorn from",
    "start": "1774860",
    "end": "1781220"
  },
  {
    "text": "the Prometheus maintainers team they have excellent talks so make sure to check those out but this is also a",
    "start": "1781220",
    "end": "1787039"
  },
  {
    "text": "feature that we're pulling into Thanos because Thanos heavily borrows from Prometheus um we have an issue that kind of tracks",
    "start": "1787039",
    "end": "1793700"
  },
  {
    "text": "the development of that feature so we expect it to land again in the next maybe several months",
    "start": "1793700",
    "end": "1800539"
  },
  {
    "text": "um all right so then people um also like very often have questions such as you know can tunnel scale to XYZ",
    "start": "1800539",
    "end": "1808220"
  },
  {
    "text": "and always it's like a different metric they have some different constraints or ideas in mind what scalability means",
    "start": "1808220",
    "end": "1816980"
  },
  {
    "text": "um so in order to kind of set some sort of a bound at least a low round of where",
    "start": "1816980",
    "end": "1823460"
  },
  {
    "text": "we know Thanos can perform adequately and I've basically taken a screenshot of",
    "start": "1823460",
    "end": "1828919"
  },
  {
    "text": "one of our internal dashboards that we have at Shopify for in our internal monitoring which shows the",
    "start": "1828919",
    "end": "1836240"
  },
  {
    "text": "um basically what we call the head series or the cardinality of the entire data set across the entire monitoring",
    "start": "1836240",
    "end": "1843740"
  },
  {
    "text": "infrastructure so this is how many time series we have globally within the last",
    "start": "1843740",
    "end": "1849919"
  },
  {
    "text": "two hours that's that's the in the impromptuous parlance that's what hit series means and so we see that",
    "start": "1849919",
    "end": "1857840"
  },
  {
    "text": "um here obviously cardinality virus the the top panel shows the global one but",
    "start": "1857840",
    "end": "1863000"
  },
  {
    "text": "we are basically capable of getting anywhere between 3 billion and 6 billion",
    "start": "1863000",
    "end": "1868159"
  },
  {
    "text": "time series within you know a two hour window which is fairly High also like if we break that down per",
    "start": "1868159",
    "end": "1874460"
  },
  {
    "text": "region we run tunnels in different regions uh the two biggest regions USC senior Central they also spiked about",
    "start": "1874460",
    "end": "1880820"
  },
  {
    "text": "1.5 billion um and also finally like Thanos is not",
    "start": "1880820",
    "end": "1885980"
  },
  {
    "text": "like a done project we're still making constant improvements to it there's still ongoing work and there will be",
    "start": "1885980",
    "end": "1891500"
  },
  {
    "text": "more work in the future um just to kind of illustrate that we had an environment when where the",
    "start": "1891500",
    "end": "1898279"
  },
  {
    "text": "queriors were simply not updated for any reason and by simply updating them from",
    "start": "1898279",
    "end": "1903440"
  },
  {
    "text": "like a six month old version to the latest version we saw approximately a 50 reduction in like both CPU and memory",
    "start": "1903440",
    "end": "1910700"
  },
  {
    "text": "usage and in addition to this we also saw like latency like P90 latency dropped down by 50 just by doing an",
    "start": "1910700",
    "end": "1918020"
  },
  {
    "text": "update um and we were we basically just picked up all of the changes like the small incremental changes that have happened",
    "start": "1918020",
    "end": "1924440"
  },
  {
    "text": "over you know six month period uh also the reason why we have such a stable resource usage here is because",
    "start": "1924440",
    "end": "1931580"
  },
  {
    "text": "these queriors are used to execute um recording and alerting rules so they're constantly executing queries and",
    "start": "1931580",
    "end": "1939500"
  },
  {
    "text": "the load on them is about 16 000 queries per second",
    "start": "1939500",
    "end": "1944679"
  },
  {
    "text": "um all right so now maybe just find out the words about the Thomas community",
    "start": "1944840",
    "end": "1950360"
  },
  {
    "text": "so with the intro and all the exciting new features and the work we've been doing uh to constantly improve the",
    "start": "1950360",
    "end": "1957380"
  },
  {
    "text": "Thanos project I also want to quickly share how you can get involved with the",
    "start": "1957380",
    "end": "1962539"
  },
  {
    "text": "thanks community and how you can get involved with these efforts um so our major project Communications",
    "start": "1962539",
    "end": "1969640"
  },
  {
    "text": "Communications happen over the cncf's Thanos and Thanos Dev slack channels",
    "start": "1969640",
    "end": "1975500"
  },
  {
    "text": "um for code or bugs issues feature requests we usually use GitHub",
    "start": "1975500",
    "end": "1980600"
  },
  {
    "text": "um we discuss on the issues and pis on the relevant repos and you can also",
    "start": "1980600",
    "end": "1986539"
  },
  {
    "text": "raise in GitHub discussions as well we answer bi-weekly Community office",
    "start": "1986539",
    "end": "1992480"
  },
  {
    "text": "hours on Thursdays at around 2 pm UTC over Zoom we maintain a public agenda",
    "start": "1992480",
    "end": "1999500"
  },
  {
    "text": "doc so that you can reference previous discussions and add in your own agenda items and we usually try to be present",
    "start": "1999500",
    "end": "2006640"
  },
  {
    "text": "at kubecon Via talks project meetings booths and this time we even had a",
    "start": "2006640",
    "end": "2012159"
  },
  {
    "text": "Country Fest session uh put on help on board newer open source contributors",
    "start": "2012159",
    "end": "2017919"
  },
  {
    "text": "we also Mentor pretty extensively in the cncf we try to submit projects for the",
    "start": "2017919",
    "end": "2023440"
  },
  {
    "text": "LFX mentorships which happen nearly every quarter as well as for g-sock which happens like once a year so please",
    "start": "2023440",
    "end": "2030039"
  },
  {
    "text": "feel free to apply if you are a student or even if you are a full-time engineer who is looking to explore observability",
    "start": "2030039",
    "end": "2036640"
  },
  {
    "text": "monitoring or the CNC app we're trying to not only do some technical projects but also try to provide some sort of",
    "start": "2036640",
    "end": "2043659"
  },
  {
    "text": "holistic guidance to help them become greater open source Engineers who are awesome to work with",
    "start": "2043659",
    "end": "2050260"
  },
  {
    "text": "and finally thanos's website now has a dedicated space for technical blogs on",
    "start": "2050260",
    "end": "2055839"
  },
  {
    "text": "the topic so we love to hear your stories on how you are using or adopting Thanos and if you have some nice use",
    "start": "2055839",
    "end": "2063099"
  },
  {
    "text": "case for it or even if you are using it as a as a dependency to build something",
    "start": "2063099",
    "end": "2068980"
  },
  {
    "text": "cool um so feel free to share this as this is a nice way to Garner feedback from the",
    "start": "2068980",
    "end": "2074500"
  },
  {
    "text": "community thank you",
    "start": "2074500",
    "end": "2079500"
  },
  {
    "text": "I don't know if we have any time for questions left yeah there's one over there",
    "start": "2084399",
    "end": "2092398"
  },
  {
    "text": "yeah um maybe just the mic thank you for the presentation I've got",
    "start": "2093040",
    "end": "2100060"
  },
  {
    "text": "two questions um so the one the the first one is uh if",
    "start": "2100060",
    "end": "2105400"
  },
  {
    "text": "every prom Prometheus are down can we still use the tunnels with storage only",
    "start": "2105400",
    "end": "2112200"
  },
  {
    "text": "and the second one is what is the order for request do we are we requesting",
    "start": "2112200",
    "end": "2119020"
  },
  {
    "text": "um first of all the storage or are we requesting directly the stanus sidecars",
    "start": "2119020",
    "end": "2124780"
  },
  {
    "text": "uh yeah so if all parameters are down you can still query data from object storage",
    "start": "2124780",
    "end": "2130900"
  },
  {
    "text": "um and we usually request data from the sidecars which then talk to the",
    "start": "2130900",
    "end": "2137320"
  },
  {
    "text": "Prometheus instances so both the sidecar and the parameters are on the same core path",
    "start": "2137320",
    "end": "2142960"
  },
  {
    "text": "um and the data is requested in parallel from the sidecar and from object storage",
    "start": "2142960",
    "end": "2148420"
  },
  {
    "text": "and it's kind of joined together at queer time okay",
    "start": "2148420",
    "end": "2154060"
  },
  {
    "text": "okay thank you welcome",
    "start": "2154060",
    "end": "2157500"
  },
  {
    "text": "um there's a question there I think yeah all the functionality is a new",
    "start": "2159339",
    "end": "2165220"
  },
  {
    "text": "functionalities you mentioned in which versions are they available or will they be available so with the exception of",
    "start": "2165220",
    "end": "2173200"
  },
  {
    "text": "the distributed execution in Native histograms they're available in 0.31 so the latest release yeah yeah",
    "start": "2173200",
    "end": "2179800"
  },
  {
    "text": "okay and you can like always reference the change lock for any specific feature",
    "start": "2179800",
    "end": "2187020"
  },
  {
    "text": "all right thank you thank you all right awesome yeah hi uh I have one question about the",
    "start": "2188440",
    "end": "2197020"
  },
  {
    "text": "RAM usage of Prometheus if I'm converting to Thanos because currently we have a multi-classer set up where we",
    "start": "2197020",
    "end": "2204220"
  },
  {
    "text": "have two uh Prometheus instances and they have very high Ram uses because",
    "start": "2204220",
    "end": "2209260"
  },
  {
    "text": "they monitor our gitlab Runners and we've seen sometimes Prometheus run out",
    "start": "2209260",
    "end": "2214420"
  },
  {
    "text": "of memory and then we have holes in our queries because the data got lost because nobody could could save it and",
    "start": "2214420",
    "end": "2221800"
  },
  {
    "text": "do you know how this would change if we convert to Thanos sidecars and then to",
    "start": "2221800",
    "end": "2227500"
  },
  {
    "text": "the Thanos carrier uh like how do you do you think the performance of Prometheus would claims there so if you start",
    "start": "2227500",
    "end": "2235119"
  },
  {
    "text": "uploading data to object storage which is the easiest way to do it with the sidecars you can reduce the retention in",
    "start": "2235119",
    "end": "2240760"
  },
  {
    "text": "Prometheus um so that should help reduce memory you also won't be executing queries inside",
    "start": "2240760",
    "end": "2247119"
  },
  {
    "text": "Prometheus which also cruises memory and I think with those two changes you",
    "start": "2247119",
    "end": "2253180"
  },
  {
    "text": "should be able to at least bring the memory usage down to a bit yeah thank you and the distributed execution would",
    "start": "2253180",
    "end": "2259599"
  },
  {
    "text": "help I think as well yeah definitely um",
    "start": "2259599",
    "end": "2265800"
  }
]