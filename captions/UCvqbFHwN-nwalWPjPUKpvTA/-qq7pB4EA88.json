[
  {
    "start": "0",
    "end": "150000"
  },
  {
    "text": "hi everyone um i'm alex i'm very happy to be here at kubicon north america",
    "start": "1599",
    "end": "7200"
  },
  {
    "text": "to talk to everyone about the harper project um it's a shame that this is still a virtual conference and we",
    "start": "7200",
    "end": "12559"
  },
  {
    "text": "couldn't come in and share this in person but i hope everyone is doing really well and thank you for tuning in so today my",
    "start": "12559",
    "end": "20080"
  },
  {
    "text": "co-presenter and i will talk a little bit about the harvard project this is a graduated project in",
    "start": "20080",
    "end": "25920"
  },
  {
    "text": "cncf and if this is your first time attending harvard session",
    "start": "25920",
    "end": "31039"
  },
  {
    "text": "uh is think of it as a registry a place to to store and manage your container",
    "start": "31039",
    "end": "36239"
  },
  {
    "text": "images and other cloud native artifacts um possibly you're using helm charts you're using",
    "start": "36239",
    "end": "41440"
  },
  {
    "text": "singularity files or it's something something else but for the purpose of this session",
    "start": "41440",
    "end": "46640"
  },
  {
    "text": "we're going to be focusing mostly on the latest and greatest of the harvard project so focusing on",
    "start": "46640",
    "end": "52640"
  },
  {
    "text": "the latest version 2.1 release as well as some of the things we're working on for upcoming 2.2 if you've never if you're not",
    "start": "52640",
    "end": "60719"
  },
  {
    "text": "familiar with what an image registry does um and have never been exposed to the",
    "start": "60719",
    "end": "66240"
  },
  {
    "text": "basic set of capabilities that harvard provides please check out our website and check out talks at previous google",
    "start": "66240",
    "end": "72960"
  },
  {
    "text": "homes where we've always had an introductory session that you know sort of starts with",
    "start": "72960",
    "end": "78240"
  },
  {
    "text": "what is an image registry um what does it do why do you need it so that can be really helpful a quick",
    "start": "78240",
    "end": "84840"
  },
  {
    "text": "introduction um i'm alex jr i'm a product manager in the cloud native team at",
    "start": "84840",
    "end": "90400"
  },
  {
    "text": "vmware leading the harbor effort so responsible for trying to understand the requirements around the container",
    "start": "90400",
    "end": "96479"
  },
  {
    "text": "registry for responsible driving the road map collecting feedback from from you guys and making sure that",
    "start": "96479",
    "end": "103920"
  },
  {
    "text": "we're building something that people actually use and will actually want to recommend to to their friends",
    "start": "103920",
    "end": "111119"
  },
  {
    "text": "so and today with me is my co-presenter daniel",
    "start": "111119",
    "end": "116399"
  },
  {
    "text": "hey daniel hey guys this is hi guys this is daniel um yeah i will present with alex you um",
    "start": "117280",
    "end": "123520"
  },
  {
    "text": "today i will show you a few cool demos regarding the latest feature we delivered in version 2.1",
    "start": "123520",
    "end": "129920"
  },
  {
    "text": "uh i'm working for vmware as a staff engineer and maintainer of harbor product glad to",
    "start": "129920",
    "end": "137200"
  },
  {
    "text": "meet you thanks daniel um we've also listed two",
    "start": "137200",
    "end": "142239"
  },
  {
    "text": "of our colleagues who helped us with this presentation um so",
    "start": "142239",
    "end": "147520"
  },
  {
    "text": "shout out to stephen and steven zoe as well",
    "start": "147520",
    "end": "151920"
  },
  {
    "start": "150000",
    "end": "224000"
  },
  {
    "text": "so we you know we start with the intro slide what is harper well harvard is a trusted cloud native registry",
    "start": "152720",
    "end": "158800"
  },
  {
    "text": "that can store sign and scan content so it's basically a place to host and",
    "start": "158800",
    "end": "165280"
  },
  {
    "text": "manage your artifacts and you know when we started this project back in 2014",
    "start": "165280",
    "end": "170879"
  },
  {
    "text": "building an on-prem registry leveraging talk distribution we came across some issues right when we",
    "start": "170879",
    "end": "176400"
  },
  {
    "text": "were using docker hub and other public registries so we're",
    "start": "176400",
    "end": "181920"
  },
  {
    "text": "over time we've you know addressed those issues and also added more services and features related to",
    "start": "181920",
    "end": "188159"
  },
  {
    "text": "life cycle management security features like scanning and signing images",
    "start": "188159",
    "end": "193760"
  },
  {
    "text": "and recently we've been focusing on image distribution through p2p and proxy cache and now we're focusing",
    "start": "193760",
    "end": "201200"
  },
  {
    "text": "on high availability delivering a high highly available hardware cluster through our operator",
    "start": "201200",
    "end": "208799"
  },
  {
    "text": "so our mission is to be the best cloud native registry for kubernetes and we started with support for docker",
    "start": "208799",
    "end": "215040"
  },
  {
    "text": "images we expanded the helm charts and with harvard 2.0 we can now support",
    "start": "215040",
    "end": "220319"
  },
  {
    "text": "any oci compatible artifacts",
    "start": "220319",
    "end": "224400"
  },
  {
    "start": "224000",
    "end": "278000"
  },
  {
    "text": "so let's start by looking at the community which is thriving doing really well so we have more than 13k github stars we",
    "start": "225440",
    "end": "233360"
  },
  {
    "text": "have 200 plus committers um 50 plus contributing companies more than 4 000 forks we have 14 maintainers",
    "start": "233360",
    "end": "241120"
  },
  {
    "text": "across five different companies across three different continents in north america and europe",
    "start": "241120",
    "end": "247439"
  },
  {
    "text": "in asia we've also seen a lot of diversity in our contributors as well in terms of",
    "start": "247439",
    "end": "253120"
  },
  {
    "text": "backgrounds and skill sets from developers to devop engineers to system admins",
    "start": "253120",
    "end": "258400"
  },
  {
    "text": "and basically everyone everyone in their and their team has needs around",
    "start": "258400",
    "end": "265199"
  },
  {
    "text": "a registry to to manage their artifacts so we're extremely happy about this and",
    "start": "265199",
    "end": "270880"
  },
  {
    "text": "and you know we want to thank you guys for all the work you've done and thanks again for tuning",
    "start": "270880",
    "end": "282080"
  },
  {
    "start": "278000",
    "end": "436000"
  },
  {
    "text": "and so i want to just quickly jump into this and start with some of the things that we've been working on for the 2.1 release the first",
    "start": "282080",
    "end": "288880"
  },
  {
    "text": "one i want to talk about is called proxy cache which is the ability for harvard to act as a pull through cache",
    "start": "288880",
    "end": "295280"
  },
  {
    "text": "for another remote registry and you know that's usually we call that",
    "start": "295280",
    "end": "300720"
  },
  {
    "text": "the target or the remote registry or the upstream and this is basically situations where you have",
    "start": "300720",
    "end": "307919"
  },
  {
    "text": "a registry that you're trying to hit but you have either limited connectivity or no connectivity um to that target",
    "start": "307919",
    "end": "315680"
  },
  {
    "text": "and this could be because of compliance or because of security issues or limited equest",
    "start": "315680",
    "end": "322840"
  },
  {
    "text": "options and um so docker hub is a really great example",
    "start": "322840",
    "end": "329360"
  },
  {
    "text": "of a client register of a target registry they're trying to hit right docker hub is a registry where docker",
    "start": "329360",
    "end": "336160"
  },
  {
    "text": "clients from all over the world are trying to pull images from but if you're pulling too fast or you're pulling too",
    "start": "336160",
    "end": "342479"
  },
  {
    "text": "frequently then you can trigger the dot the docker hub rate limiter",
    "start": "342479",
    "end": "347759"
  },
  {
    "text": "which can throttle your connection or even get uip band so hardware as a proxy cache is meant to",
    "start": "347759",
    "end": "353440"
  },
  {
    "text": "address this very problem in this case it means that harper will serve as an intermediary to pull the",
    "start": "353440",
    "end": "359440"
  },
  {
    "text": "images and cache them locally for you and then serve it to you to serve it to the darker clients so it's",
    "start": "359440",
    "end": "365680"
  },
  {
    "text": "much faster and it minimizes going over the network unnecessarily and then prevents you from",
    "start": "365680",
    "end": "372479"
  },
  {
    "text": "getting ip banned so the way to",
    "start": "372479",
    "end": "379280"
  },
  {
    "text": "utilize this feature is basically when you're as you would when you're creating a harper project if you're familiar",
    "start": "379520",
    "end": "385759"
  },
  {
    "text": "familiar with that workflow there's an added option to enable it as a proxy project",
    "start": "385759",
    "end": "391840"
  },
  {
    "text": "where you would have to then put in the select the target registry endpoint um that's that's the end point that",
    "start": "391840",
    "end": "398000"
  },
  {
    "text": "you're proxying for and so when you want to docker pull from that remote registry",
    "start": "398000",
    "end": "403759"
  },
  {
    "text": "you will dock or pull from the proxy project instead and every time you do a poll uh it'll kick off a request to to harbor",
    "start": "403759",
    "end": "410080"
  },
  {
    "text": "which will kick off requests to that upstream registry and and do a comparison to see if the image in its",
    "start": "410080",
    "end": "416560"
  },
  {
    "text": "cache is indeed the most up-to-date copy if it is it will just serve the cache to copy and if it isn't it will",
    "start": "416560",
    "end": "423120"
  },
  {
    "text": "repo recache it and then serve it to to the docker clients so you will have",
    "start": "423120",
    "end": "428560"
  },
  {
    "text": "to modify your docker pull command and your pod manifest to hit the proxy project",
    "start": "428560",
    "end": "434840"
  },
  {
    "text": "instead um so the second project or the second",
    "start": "434840",
    "end": "441199"
  },
  {
    "start": "436000",
    "end": "643000"
  },
  {
    "text": "feature i want to highlight is called non-blocking gc it's called non-blocking because prior to 2.1",
    "start": "441199",
    "end": "447039"
  },
  {
    "text": "collection will put the registry in read-only mode so you can still",
    "start": "447039",
    "end": "452240"
  },
  {
    "text": "interact with your existing artifacts in harbor you can still you know use the apis you can still use",
    "start": "452240",
    "end": "458639"
  },
  {
    "text": "the web console run you know certain policies you can",
    "start": "458639",
    "end": "463680"
  },
  {
    "text": "still pull the images but you can't push images and you can't delete them essentially for the entire duration of",
    "start": "463680",
    "end": "469360"
  },
  {
    "text": "that gc execution and so with the 2.1 non-blocking garbage collection",
    "start": "469360",
    "end": "474639"
  },
  {
    "text": "users can push images they can pull images they until the images from harbor while gc is running so you",
    "start": "474639",
    "end": "481120"
  },
  {
    "text": "know it completely runs completely silent in the background with no impact to user operations and you have to worry",
    "start": "481120",
    "end": "487599"
  },
  {
    "text": "about you know data corruption anything like that data corruption is you know it was a",
    "start": "487599",
    "end": "494639"
  },
  {
    "text": "potential concern for us um because of the the way the images are laid out and stored in",
    "start": "494639",
    "end": "501360"
  },
  {
    "text": "distribution um they're essentially um you know this is owing to the the cross-referenceable nature of stock distribution where the",
    "start": "501360",
    "end": "508160"
  },
  {
    "text": "image layers are stored in the most efficient way possible with no redundancy",
    "start": "508160",
    "end": "513200"
  },
  {
    "text": "um they're essentially deduped across all the projects in a single harbor instance so it's really great from a storage",
    "start": "513200",
    "end": "519120"
  },
  {
    "text": "perspective but you know it can be tricky from from whenever you're doing something",
    "start": "519120",
    "end": "524399"
  },
  {
    "text": "destructive like running garbage collection or deleting images and the way we were finally able to achieve this was",
    "start": "524399",
    "end": "531040"
  },
  {
    "text": "we're essentially snapshotting all the image layers that are marked to be deleted and then sort of",
    "start": "531040",
    "end": "536640"
  },
  {
    "text": "continuously tracking and resolving these against any incoming image blobs as part of a push request",
    "start": "536640",
    "end": "544880"
  },
  {
    "text": "while gc is running so it's quite uh an important feature from",
    "start": "544880",
    "end": "550480"
  },
  {
    "text": "an operational standpoint it's very highly requested by large enterprise users that have anywhere from",
    "start": "550480",
    "end": "557920"
  },
  {
    "text": "hundreds of gigabytes to we've seen tens of terabytes of images stored",
    "start": "557920",
    "end": "563200"
  },
  {
    "text": "on each harbor instance and and because you know gc can be quite time consuming depending on",
    "start": "563200",
    "end": "568800"
  },
  {
    "text": "how often you run it if it's placed in read-only mode for an extended amount of time",
    "start": "568800",
    "end": "573920"
  },
  {
    "text": "and on top of that there's no way to tell how long that job will run for or how much disk",
    "start": "573920",
    "end": "580240"
  },
  {
    "text": "space your you will it will be deallocated from each execution you know that can be um",
    "start": "580240",
    "end": "586080"
  },
  {
    "text": "problematic and it can be quite stressful all right so imagine if you're in the software delivery business sort of",
    "start": "586080",
    "end": "591839"
  },
  {
    "text": "software supply chain business where all the images um are landing in your harper instance and",
    "start": "591839",
    "end": "598320"
  },
  {
    "text": "you're the one responsible for distributing these to your customers um and so that's why we've added the",
    "start": "598320",
    "end": "603360"
  },
  {
    "text": "ability to perform a dry run which will give you an estimation of the amount of disk space that will be freed up",
    "start": "603360",
    "end": "610720"
  },
  {
    "text": "and so by making it non-blocking not only is it you know not um incurring a penalty on your user",
    "start": "610720",
    "end": "617839"
  },
  {
    "text": "operations not only is it running in a faster and more efficient manner but you know the time it takes to run gc",
    "start": "617839",
    "end": "625360"
  },
  {
    "text": "is no longer as much of a factor it's no longer as debilitating to your business",
    "start": "625360",
    "end": "630399"
  },
  {
    "text": "if it goes over you know if it goes um runs for for quite some time right",
    "start": "630399",
    "end": "636800"
  },
  {
    "text": "you're you're taking the pressure off the garbage collection aspect of maintaining a registry",
    "start": "636800",
    "end": "644000"
  },
  {
    "text": "um so before i move on to 2.2 um daniel we'll do a demo of the the proxy cache",
    "start": "644560",
    "end": "651360"
  },
  {
    "text": "as well as the online garbage the non-blocking garbage collection sorry so let me stop sharing",
    "start": "651360",
    "end": "659120"
  },
  {
    "text": "let me make sure that's still recording oh okay yeah it is still recording",
    "start": "659120",
    "end": "667040"
  },
  {
    "start": "664000",
    "end": "1186000"
  },
  {
    "text": "yeah so let me get started uh hey guys so uh the first demo i'm gonna show you",
    "start": "667040",
    "end": "672240"
  },
  {
    "text": "is proxy cache i need to use a recorded demo due to the instability of my",
    "start": "672240",
    "end": "678320"
  },
  {
    "text": "network um so let's move on so in the proxy cache to implement it we reuse the",
    "start": "678320",
    "end": "685839"
  },
  {
    "text": "code of replication so the user experience also has some",
    "start": "685839",
    "end": "692240"
  },
  {
    "text": "overlap for you to create a proxycad project first you need to create a registry",
    "start": "692240",
    "end": "698079"
  },
  {
    "text": "endpoint the same way as you are created for a target registry create a target registry for replication",
    "start": "698079",
    "end": "706560"
  },
  {
    "text": "so in this case i just created a docker hub and point with my personal account",
    "start": "706560",
    "end": "713440"
  },
  {
    "text": "and i can create a new project note that in this dialog there is a",
    "start": "715360",
    "end": "724000"
  },
  {
    "text": "new uh switch a proxy cache i can switch it on and select this",
    "start": "724000",
    "end": "729519"
  },
  {
    "text": "endpoint i just created yeah after that this will be the",
    "start": "729519",
    "end": "736000"
  },
  {
    "text": "proxy cache project process images from docker hub",
    "start": "736000",
    "end": "740959"
  },
  {
    "text": "note that this is an empty project no images",
    "start": "741360",
    "end": "745839"
  },
  {
    "text": "and if i issue a docker pool command against this",
    "start": "746959",
    "end": "753120"
  },
  {
    "text": "proxy cache project",
    "start": "753120",
    "end": "758240"
  },
  {
    "text": "here i want to pull uh go harbor slash hardware core images from doppler hub",
    "start": "758240",
    "end": "763279"
  },
  {
    "text": "via my proxy cache project i just append this relative uri",
    "start": "763279",
    "end": "768880"
  },
  {
    "text": "to the uh url of my project uh of the hover instance",
    "start": "768880",
    "end": "776079"
  },
  {
    "text": "like this and if i start pulling",
    "start": "776079",
    "end": "783120"
  },
  {
    "text": "you can see the images is serving directly via this proxy cache",
    "start": "783760",
    "end": "790480"
  },
  {
    "text": "from docker hub okay now at the same time under the hood",
    "start": "790480",
    "end": "798079"
  },
  {
    "text": "there in the background there is a go routine to write the content of this image to",
    "start": "798079",
    "end": "803680"
  },
  {
    "text": "harvest storage and uh because it's asynchronous you need to wait a couple",
    "start": "803680",
    "end": "809920"
  },
  {
    "text": "of seconds when you refresh and you can see that there is a repository hover core",
    "start": "809920",
    "end": "815519"
  },
  {
    "text": "in the image note that once this image is stored in harbor",
    "start": "815519",
    "end": "822399"
  },
  {
    "text": "storage it is the same as the images you pushed to harbor so it has all the information",
    "start": "822399",
    "end": "829440"
  },
  {
    "text": "and you can do all the stuff to this image such as scanning",
    "start": "829440",
    "end": "834959"
  },
  {
    "text": "or adding tabs removing text here i simulate a outage of docker hub",
    "start": "834959",
    "end": "842320"
  },
  {
    "text": "by messing up my dns so that my hardware instance cannot access this",
    "start": "842320",
    "end": "848399"
  },
  {
    "text": "docker hub and uh you can see the test connection the ping doesn't work now",
    "start": "848399",
    "end": "854480"
  },
  {
    "text": "um but if i remove this image locally and pull it again from my",
    "start": "854480",
    "end": "861040"
  },
  {
    "text": "docker client um",
    "start": "861040",
    "end": "866800"
  },
  {
    "text": "harbor will do the best effort to sync with the target registry but in this",
    "start": "867680",
    "end": "873680"
  },
  {
    "text": "case it is not accessible so um however will serve whatever is available locally so",
    "start": "873680",
    "end": "881680"
  },
  {
    "text": "in this case you can still get this image that is cached in harbor",
    "start": "881680",
    "end": "887360"
  },
  {
    "text": "we consider it's really helpful for you to alleviate the problem caused by the read limiting",
    "start": "887360",
    "end": "894480"
  },
  {
    "text": "in the docker hub it's also worth uh mentioning that this proxy",
    "start": "894480",
    "end": "901279"
  },
  {
    "text": "cache project [Music] has all the capabilities uh",
    "start": "901279",
    "end": "909199"
  },
  {
    "text": "except for pushing has all the capabilities of a regular projecting harbor",
    "start": "909199",
    "end": "914320"
  },
  {
    "text": "for example you can use the coda policy to control how much images can be proxied and you",
    "start": "914320",
    "end": "920800"
  },
  {
    "text": "can also set up this uh tag retention policy to remove this",
    "start": "920800",
    "end": "926000"
  },
  {
    "text": "style style image cache so that you can you know serve the store",
    "start": "926000",
    "end": "933680"
  },
  {
    "text": "the new or process images but that's the demo for the proxy cache",
    "start": "933680",
    "end": "940399"
  },
  {
    "text": "uh next is the non-blocking gc non-blocking garbage collection demo",
    "start": "940399",
    "end": "946560"
  },
  {
    "text": "also as alex mentioned in when we implement",
    "start": "946560",
    "end": "951680"
  },
  {
    "text": "this feature uh instead of relying on the docker distributions code we use",
    "start": "951680",
    "end": "959120"
  },
  {
    "text": "this data in hubble's database as a single source of truth to help us decide",
    "start": "959120",
    "end": "965199"
  },
  {
    "text": "what layers to be removed and what uh you know how much space will be freed is",
    "start": "965199",
    "end": "971440"
  },
  {
    "text": "also available so um when you hit the uh dry run",
    "start": "971440",
    "end": "976880"
  },
  {
    "text": "we we just mark the layers that are candidates to be removed",
    "start": "976880",
    "end": "984240"
  },
  {
    "text": "and calculate the size and we can provide a hint for the admin to",
    "start": "984240",
    "end": "990240"
  },
  {
    "text": "you know let them know how much space will be freed if we run the dc right now no this is a very",
    "start": "990240",
    "end": "997279"
  },
  {
    "text": "rough estimation because this is a non-blocking dc so images may be pushed when the gc is",
    "start": "997279",
    "end": "1003279"
  },
  {
    "text": "running you're gonna see now um if i uh you know trigger the gc now",
    "start": "1003279",
    "end": "1012079"
  },
  {
    "text": "you can see the job is scheduled and it's running you can notice that there is no longer",
    "start": "1012079",
    "end": "1019279"
  },
  {
    "text": "the warning bar saying that hover is in read-only mode and",
    "start": "1019279",
    "end": "1024480"
  },
  {
    "text": "at the same time i can do a darker push to my uh hover instance",
    "start": "1024480",
    "end": "1031360"
  },
  {
    "text": "while it's you know being garbage",
    "start": "1031360",
    "end": "1035360"
  },
  {
    "text": "collected yeah it's still running and the pushing",
    "start": "1036839",
    "end": "1042558"
  },
  {
    "text": "is in progress it's an instance on aws so that you know",
    "start": "1042559",
    "end": "1049120"
  },
  {
    "text": "it's a bit slow",
    "start": "1049120",
    "end": "1053840"
  },
  {
    "text": "okay um you can see this guy recreation is also finished um if we uh see the lock uh you can see",
    "start": "1120960",
    "end": "1128880"
  },
  {
    "text": "uh what layers are removed and there is a actual uh you know size of the storage",
    "start": "1128880",
    "end": "1135200"
  },
  {
    "text": "that is free by calculating what layers are removed um this is a",
    "start": "1135200",
    "end": "1141679"
  },
  {
    "text": "you know a big improvement for the admin so that they will not need to worry about you know their",
    "start": "1141679",
    "end": "1148480"
  },
  {
    "text": "developer not being able to push images during gc um next step uh we may",
    "start": "1148480",
    "end": "1156320"
  },
  {
    "text": "improve you know by we are considering to improve the performance of dc by",
    "start": "1156320",
    "end": "1162400"
  },
  {
    "text": "you know doing the deletion in parallel but this is not that an issue because",
    "start": "1162400",
    "end": "1169280"
  },
  {
    "text": "it's non-blocking so um right if the gc runs a little bit longer",
    "start": "1169280",
    "end": "1174799"
  },
  {
    "text": "we consider it's okay yeah that's all that's all my power addicts thanks okay thank you daniel um",
    "start": "1174799",
    "end": "1181919"
  },
  {
    "text": "i'll continue with the 2.2 yep so let me make sure it's still recording yes let me share my",
    "start": "1181919",
    "end": "1190840"
  },
  {
    "start": "1186000",
    "end": "1388000"
  },
  {
    "text": "screen",
    "start": "1190840",
    "end": "1193840"
  },
  {
    "text": "can you see my screen um okay so let's continue",
    "start": "1196320",
    "end": "1203039"
  },
  {
    "text": "um so thanks daniel for that for that demo um switching gears a",
    "start": "1203039",
    "end": "1210159"
  },
  {
    "text": "bit now i'm going to talk about the upcoming 2.2 release and some of the",
    "start": "1210159",
    "end": "1215280"
  },
  {
    "text": "things that are being planned and being worked on um the first one is metrics and",
    "start": "1215280",
    "end": "1221280"
  },
  {
    "text": "observability and this is really just a fancy way of saying we're going to be supporting prometheus so the goal is to monitor",
    "start": "1221280",
    "end": "1228559"
  },
  {
    "text": "services deployed actions performed as part of your harvard deployment um",
    "start": "1228559",
    "end": "1235120"
  },
  {
    "text": "and expose those metrics through an http endpoint for prometheus and so if you have",
    "start": "1235120",
    "end": "1241280"
  },
  {
    "text": "prometheus already set up in your environment if it's already part of your stack",
    "start": "1241280",
    "end": "1246799"
  },
  {
    "text": "you can then scrape whatever data you want off that endpoint i'm going to make it you know highly",
    "start": "1246799",
    "end": "1252720"
  },
  {
    "text": "customizable in terms of the kind of data you can retrieve and you know when you think about key",
    "start": "1252720",
    "end": "1259360"
  },
  {
    "text": "telemetry telemetry can be divided into several layers right at the very bottom",
    "start": "1259360",
    "end": "1264640"
  },
  {
    "text": "you have the platform which is your node level information and you can monitor your resource consumption",
    "start": "1264640",
    "end": "1271440"
  },
  {
    "text": "so cpu ram disk basic event logging health status of the",
    "start": "1271440",
    "end": "1277840"
  },
  {
    "text": "of the the deployment as a whole and then you have the container orchestration layer",
    "start": "1277840",
    "end": "1283919"
  },
  {
    "text": "which in the case of kubernetes will provide monitoring for all the pots deployed in your cluster so",
    "start": "1283919",
    "end": "1290320"
  },
  {
    "text": "here you can see in this diagram we have some of the main pods that make up the hardware cluster",
    "start": "1290320",
    "end": "1295919"
  },
  {
    "text": "there's hardware core there's a hardware registry there's db there's job service but there's you know a bunch of others",
    "start": "1295919",
    "end": "1303520"
  },
  {
    "text": "so the web portal has its own nginx has its own chart museum",
    "start": "1303520",
    "end": "1309039"
  },
  {
    "text": "if you're using trivia or clear image scanners they have their own and then finally we",
    "start": "1309039",
    "end": "1315360"
  },
  {
    "text": "have telemetry at the application layer which is you know basically the relevant registry",
    "start": "1315360",
    "end": "1321360"
  },
  {
    "text": "operations anything that the hardware api provides is a potential candidate here",
    "start": "1321360",
    "end": "1327039"
  },
  {
    "text": "and we can potentially expose aggregated statistics over some period so looking at the number of",
    "start": "1327039",
    "end": "1333280"
  },
  {
    "text": "image poles or number of images pushes image deletions also looking at top",
    "start": "1333280",
    "end": "1338880"
  },
  {
    "text": "images repositories projects being requested possibly looking at you know number of",
    "start": "1338880",
    "end": "1344400"
  },
  {
    "text": "push errors or delete errors or authentication errors and you know any other statistics that",
    "start": "1344400",
    "end": "1350400"
  },
  {
    "text": "could be useful for detecting abnormalities right like users number of users logged into",
    "start": "1350400",
    "end": "1356960"
  },
  {
    "text": "a particular instance number of failed attempts unique ips number of image requests",
    "start": "1356960",
    "end": "1363520"
  },
  {
    "text": "served in some period requests in flight number of replications um so this is still being",
    "start": "1363520",
    "end": "1372159"
  },
  {
    "text": "actively worked on i've linked the pr to the proposal in our committee repo if you have any questions or comments or",
    "start": "1372159",
    "end": "1378799"
  },
  {
    "text": "want to contribute to this work please please make comments on the pr or just reach out to us",
    "start": "1378799",
    "end": "1389840"
  },
  {
    "start": "1388000",
    "end": "1534000"
  },
  {
    "text": "so we've also got a lot of requests to deliver system level robot accounts",
    "start": "1390159",
    "end": "1395840"
  },
  {
    "text": "which is a robot account scoped to the harper instance level so it has access to",
    "start": "1395840",
    "end": "1401120"
  },
  {
    "text": "multiple projects as opposed to the current robot implementation which is scoped to a particular project",
    "start": "1401120",
    "end": "1407679"
  },
  {
    "text": "right if you're familiar with that how that works you're creating the robot account from within",
    "start": "1407679",
    "end": "1413360"
  },
  {
    "text": "the project configuration page right now and you know that's that's also understandable project skill permissions",
    "start": "1413360",
    "end": "1420480"
  },
  {
    "text": "it becomes an issue when an image in one project is expected to extend an image in a",
    "start": "1420480",
    "end": "1427039"
  },
  {
    "text": "different project right which is often the case if you're using images within harper as part of your docker",
    "start": "1427039",
    "end": "1432559"
  },
  {
    "text": "build and you're executing the docker builds in in multiple stages each from",
    "start": "1432559",
    "end": "1438559"
  },
  {
    "text": "instruction would use a different base image and build iteratively so that's exactly what we're going to be",
    "start": "1438559",
    "end": "1445039"
  },
  {
    "text": "delivering in the 2.2 as the system admin persona",
    "start": "1445039",
    "end": "1451200"
  },
  {
    "text": "you will be able to create system level robot accounts in addition to project level robot accounts which are only",
    "start": "1451200",
    "end": "1456400"
  },
  {
    "text": "available to project admins and then you know you can choose the projects",
    "start": "1456400",
    "end": "1462000"
  },
  {
    "text": "that that robot account that particular robot account has permissions to during the creation of their robot",
    "start": "1462000",
    "end": "1469279"
  },
  {
    "text": "account you can also choose the particular set of actions actions that it can perform",
    "start": "1469279",
    "end": "1474720"
  },
  {
    "text": "so like push an image or pulling image deleting images um and much more and then you'll be able",
    "start": "1474720",
    "end": "1481360"
  },
  {
    "text": "to change the set of projects and the set of actions at any given time",
    "start": "1481360",
    "end": "1486400"
  },
  {
    "text": "without having to recreate a robot account with a new token right which is the",
    "start": "1486400",
    "end": "1491440"
  },
  {
    "text": "experience right now so if that token is hard-coded somewhere into your ci",
    "start": "1491440",
    "end": "1496720"
  },
  {
    "text": "or it's being used to construct a pull secret it needs to be manually recycled right now so we're",
    "start": "1496720",
    "end": "1501840"
  },
  {
    "text": "trying to kind of we're trying to remove this kind of friction so that changing the project",
    "start": "1501840",
    "end": "1506960"
  },
  {
    "text": "scope or changing the permissions of that robot will not impact your ci in any way",
    "start": "1506960",
    "end": "1513520"
  },
  {
    "text": "so again this is being actively discussed and being worked on i've linked the pr as well to this",
    "start": "1513520",
    "end": "1519039"
  },
  {
    "text": "proposal in our community repo if you have any questions um comments um",
    "start": "1519039",
    "end": "1524559"
  },
  {
    "text": "please you know share your feelings on the pr if you have already implemented your own solution we'd be",
    "start": "1524559",
    "end": "1529840"
  },
  {
    "text": "really interested in hearing that as well",
    "start": "1529840",
    "end": "1533039"
  },
  {
    "start": "1534000",
    "end": "1763000"
  },
  {
    "text": "and finally if you've been tracking our progress or attended any of our community meetings over the last six to 12 months really",
    "start": "1536320",
    "end": "1544240"
  },
  {
    "text": "you know that we've been working on an operator deployment of harper uh that will you know run it as a kubernetes cluster sort of",
    "start": "1544240",
    "end": "1550559"
  },
  {
    "text": "similar to our help and deployment right now but it has certain advantages when it comes to the day two operations and it's also",
    "start": "1550559",
    "end": "1558400"
  },
  {
    "text": "much more intuitive for performing some of the more heavy duty operations like upgrades redeploys",
    "start": "1558400",
    "end": "1564880"
  },
  {
    "text": "backups and restores and to give a little background this really started with ovh cloud",
    "start": "1564880",
    "end": "1571600"
  },
  {
    "text": "i want to give a shout out to ovh for kickstarting this effort and doing a really great job",
    "start": "1571600",
    "end": "1576720"
  },
  {
    "text": "ovh cloud team built the first version of the harper operator and you can find this as the",
    "start": "1576720",
    "end": "1582640"
  },
  {
    "text": "core operator rebuild under go hardware project on github but it doesn't include",
    "start": "1582640",
    "end": "1588159"
  },
  {
    "text": "certain dependent services like databases redis cache or storage because you know",
    "start": "1588159",
    "end": "1594320"
  },
  {
    "text": "some vendors already have those services running as managed services",
    "start": "1594320",
    "end": "1599440"
  },
  {
    "text": "and they're perhaps already deployed in a much more uh highly distributed or highly",
    "start": "1599440",
    "end": "1605039"
  },
  {
    "text": "available manner and that makes sense right that's that's definitely one use case that we have to we have to cover",
    "start": "1605039",
    "end": "1612480"
  },
  {
    "text": "but you know for the other users that are perhaps not as advanced that's looking for an",
    "start": "1612480",
    "end": "1618000"
  },
  {
    "text": "all-in-one deployment that comes with all the components that harvard needs to run we have built what's called the hardware cluster",
    "start": "1618000",
    "end": "1624159"
  },
  {
    "text": "operator and that will basically deploy harper with all the dependent services like database",
    "start": "1624159",
    "end": "1629520"
  },
  {
    "text": "redis cache storage and deploy these in aja mode so we don't have to deep dive every term",
    "start": "1629520",
    "end": "1636960"
  },
  {
    "text": "every aspect of this diagram but the key takeaway here is that you can see that",
    "start": "1636960",
    "end": "1642240"
  },
  {
    "text": "in this diagram there's a harbor hardware cluster cr at the very",
    "start": "1642240",
    "end": "1647440"
  },
  {
    "text": "top right so there's a there's a hardware cluster controller that's essentially managing all the services needed for the registry",
    "start": "1647440",
    "end": "1654480"
  },
  {
    "text": "and the cluster operator customer resource encapsulates the core operator customer resource that",
    "start": "1654480",
    "end": "1660159"
  },
  {
    "text": "i just talked about as well as the customer resources for all the other individual components",
    "start": "1660159",
    "end": "1667039"
  },
  {
    "text": "and so the harvard cluster is basically watching over everything and",
    "start": "1667039",
    "end": "1673279"
  },
  {
    "text": "continuously reconciling to make sure that what is running is is what is actually specified in the cr",
    "start": "1673279",
    "end": "1679120"
  },
  {
    "text": "spec and so if there any failures or hiccups then it can",
    "start": "1679120",
    "end": "1684320"
  },
  {
    "text": "automatically redeploy those services so now for anyone who's wondering about",
    "start": "1684320",
    "end": "1690159"
  },
  {
    "text": "high availability in the sense of multiple harbors harbor clusters across different data",
    "start": "1690159",
    "end": "1696080"
  },
  {
    "text": "centers perhaps in different availability zones even we we do have a story in the works",
    "start": "1696080",
    "end": "1701279"
  },
  {
    "text": "to address that as well but that's sort of outside the scope of the harvard cluster operator um",
    "start": "1701279",
    "end": "1708240"
  },
  {
    "text": "i know that there's people have been asking about it so i just want to address it and say you know this is something that we are aware of",
    "start": "1708240",
    "end": "1714799"
  },
  {
    "text": "and it's something that we're working towards and so for each of these components you can customize the spec",
    "start": "1714799",
    "end": "1721200"
  },
  {
    "text": "for that crd essentially right you can specify the number of replicas you can prefer something like cache",
    "start": "1721200",
    "end": "1728720"
  },
  {
    "text": "for the redis cache or for the database you can choose to use an external cache or you can declare in",
    "start": "1728720",
    "end": "1734720"
  },
  {
    "text": "class and run that in aj mode so we have a really detailed proposal",
    "start": "1734720",
    "end": "1740720"
  },
  {
    "text": "detailed meeting notes on the progress for the cluster operator um under the go harvard project so",
    "start": "1740720",
    "end": "1747520"
  },
  {
    "text": "please feel free to check that out and share your thoughts you know we're still wanting more feedback on design because",
    "start": "1747520",
    "end": "1753520"
  },
  {
    "text": "we will only have released the very first cluster operator version 1.0",
    "start": "1753520",
    "end": "1758799"
  },
  {
    "text": "by the time this recording is shared",
    "start": "1758799",
    "end": "1762320"
  },
  {
    "text": "so that's everything um we want to share related to the 2.1 as well as the upcoming 2.2 uh",
    "start": "1765120",
    "end": "1771039"
  },
  {
    "text": "just a quick word on collaborating with us and how to get more involved in the community",
    "start": "1771039",
    "end": "1776080"
  },
  {
    "text": "please follow our twitter if you want to be alerted to any of the latest announcements on releases community events collaborations",
    "start": "1776080",
    "end": "1783679"
  },
  {
    "text": "or anything harbor related we're still holding the the bi-weekly community meetings one for u.s time zone",
    "start": "1783679",
    "end": "1790159"
  },
  {
    "text": "and one for apec this is where you know we have some of the future demos",
    "start": "1790159",
    "end": "1795679"
  },
  {
    "text": "like the ones that daniel just did and we highlight some of the latest developments in the project and",
    "start": "1795679",
    "end": "1801760"
  },
  {
    "text": "anyone can come to resume meeting and ask questions or share your experiences around harper",
    "start": "1801760",
    "end": "1807840"
  },
  {
    "text": "you can find the detailed meeting notes in uh or the meeting details in the",
    "start": "1807840",
    "end": "1813679"
  },
  {
    "text": "readme for the harper project on github and finally we do have a demo deployed",
    "start": "1813679",
    "end": "1821039"
  },
  {
    "text": "that anyone can just go on and and give it a try get familiar with the the ui um create a project add users",
    "start": "1821039",
    "end": "1829520"
  },
  {
    "text": "push and pull images to and from that project um so demo dot go harder dot io",
    "start": "1829520",
    "end": "1837840"
  },
  {
    "text": "so that's all i have for today um thanks daniel for sharing the demo um",
    "start": "1838320",
    "end": "1845200"
  },
  {
    "text": "and then you know we'll take any questions for the remaining five to 5-10 minutes",
    "start": "1845200",
    "end": "1850399"
  },
  {
    "text": "so thanks everyone for staying with us so far",
    "start": "1850399",
    "end": "1856960"
  }
]