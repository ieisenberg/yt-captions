[
  {
    "start": "0",
    "end": "51000"
  },
  {
    "text": "hello my name is srivadsan and with me is nagaraj both of us",
    "start": "80",
    "end": "6319"
  },
  {
    "text": "are part of intuit's data platform we are going to present spock intuits implementation of spark",
    "start": "6319",
    "end": "14240"
  },
  {
    "text": "on kubernetes and how that has enabled us to scale our big data processing",
    "start": "14240",
    "end": "22320"
  },
  {
    "text": "to give a brief overview of to give a brief overview of intuit we",
    "start": "24960",
    "end": "31760"
  },
  {
    "text": "are the makers of finance and accounting software such as turbotax quickbooks and",
    "start": "31760",
    "end": "37120"
  },
  {
    "text": "mint our products serve the financial needs of consumers small businesses and those who are",
    "start": "37120",
    "end": "43600"
  },
  {
    "text": "self-employed intuit's mission is to power prosperity around the world with ai driven expert platforms and that",
    "start": "43600",
    "end": "51760"
  },
  {
    "start": "51000",
    "end": "208000"
  },
  {
    "text": "mission has never been more important than now",
    "start": "51760",
    "end": "59440"
  },
  {
    "text": "powering intuits mission are the financial data sets from various intuits products all of",
    "start": "59440",
    "end": "66799"
  },
  {
    "text": "this data is organized in intuits data lake",
    "start": "66799",
    "end": "71840"
  },
  {
    "text": "which has upwards of seven petabytes of data in the lake and growing overall entude's platform",
    "start": "71840",
    "end": "80159"
  },
  {
    "text": "manages over 40 petabytes of data across our products in order to process these data sets we",
    "start": "80159",
    "end": "87600"
  },
  {
    "text": "leverage aws emrs spinning upwards of 7500 emrs",
    "start": "87600",
    "end": "93119"
  },
  {
    "text": "every day these emrs use over 50 000 ec2 instances",
    "start": "93119",
    "end": "99520"
  },
  {
    "text": "purely for data processing purposes in addition to emrs we also use other aws services",
    "start": "99520",
    "end": "106560"
  },
  {
    "text": "such as sagemaker athena s3 and redshift and perform a majority of",
    "start": "106560",
    "end": "112240"
  },
  {
    "text": "data processing using apache spark or apache beam",
    "start": "112240",
    "end": "118240"
  },
  {
    "text": "the data processing infrastructure is crucial to power intuits data and ml platforms",
    "start": "120320",
    "end": "127920"
  },
  {
    "text": "and pipelines this park plus emr ecosystem powers the ingestion of data into the lake",
    "start": "127920",
    "end": "134640"
  },
  {
    "text": "curation into well-defined data sets and exploration for analytical purposes furthermore",
    "start": "134640",
    "end": "141680"
  },
  {
    "text": "this infrastructure also powers ml pipelines from the same data sets processing of features training and",
    "start": "141680",
    "end": "148959"
  },
  {
    "text": "evaluating of ml models and performing inference for predictions in our products",
    "start": "148959",
    "end": "155519"
  },
  {
    "text": "and these are the inferences that power the ai driven expert systems that intuit this building",
    "start": "155519",
    "end": "164800"
  },
  {
    "text": "with such a large data processing footprint it becomes crucial for optimizing the development and",
    "start": "168800",
    "end": "174720"
  },
  {
    "text": "deployment lifecycle of a data processing code in order to enable that we have a paved road built around native",
    "start": "174720",
    "end": "182560"
  },
  {
    "text": "aws services for data processing at the same time intuit has also",
    "start": "182560",
    "end": "188480"
  },
  {
    "text": "invested heavily on a paved road for services development which is based on kubernetes argo cd",
    "start": "188480",
    "end": "194800"
  },
  {
    "text": "argo workflows and other cloud native technologies the services paved road",
    "start": "194800",
    "end": "200400"
  },
  {
    "text": "also encourages git ops and other modern development practices we wanted to",
    "start": "200400",
    "end": "206159"
  },
  {
    "text": "leverage the power of cloud native technologies for data processing",
    "start": "206159",
    "end": "213599"
  },
  {
    "start": "208000",
    "end": "478000"
  },
  {
    "text": "and that's how spark was born",
    "start": "213599",
    "end": "217840"
  },
  {
    "text": "when while we were extensively using emr and yarn we were very interested to integrate",
    "start": "224640",
    "end": "230080"
  },
  {
    "text": "with the existing kubernetes infrastructure in the organization we liked containers over bootstraps",
    "start": "230080",
    "end": "236560"
  },
  {
    "text": "scripts that we were used to in emr world kubernetes as a resource manager as",
    "start": "236560",
    "end": "242319"
  },
  {
    "text": "compared to yarn we also wanted a modern continuous delivery process for data processing jobs let's dig",
    "start": "242319",
    "end": "250080"
  },
  {
    "text": "deep into each one of the above in the subsequent slides",
    "start": "250080",
    "end": "255840"
  },
  {
    "text": "services within intuit are deployed and managed on kubernetes the entity kubernetes service shown some",
    "start": "256320",
    "end": "263120"
  },
  {
    "text": "of the ui elements are shown here allows service developers to manage their",
    "start": "263120",
    "end": "268479"
  },
  {
    "text": "service runtime and deployment since intuit already uses kubernetes at a large scale",
    "start": "268479",
    "end": "275040"
  },
  {
    "text": "for running a majority of our online services here was an opportunity to have a single",
    "start": "275040",
    "end": "281199"
  },
  {
    "text": "resource manager to manage both service and data processing workloads",
    "start": "281199",
    "end": "286400"
  },
  {
    "text": "this would help immensely in the management of infrastructure reducing costs reducing operational",
    "start": "286400",
    "end": "292320"
  },
  {
    "text": "overhead on automation across the organization",
    "start": "292320",
    "end": "297840"
  },
  {
    "text": "containers for data processing jobs are extremely useful because it helps data workers write",
    "start": "300560",
    "end": "306080"
  },
  {
    "text": "their code once and deploy it across environments in a standardized and very predictable way this becomes",
    "start": "306080",
    "end": "313039"
  },
  {
    "text": "extremely important in the cloud where your dev environment and test environment and production environment",
    "start": "313039",
    "end": "318560"
  },
  {
    "text": "are quite different this approach combined with the declarative management",
    "start": "318560",
    "end": "323600"
  },
  {
    "text": "of spark jobs means no more complicated shell scripts to manage as you can see here you can see",
    "start": "323600",
    "end": "331520"
  },
  {
    "text": "the containers that are being deployed declaratively and our earlier systems",
    "start": "331520",
    "end": "338800"
  },
  {
    "text": "where we used to manage these deployments using numerous shell scripts that were",
    "start": "338800",
    "end": "345360"
  },
  {
    "text": "orchestrated by other bootstraps of emrs",
    "start": "345360",
    "end": "352639"
  },
  {
    "text": "the other big aspect is around scheduling so kubernetes is a general purpose scheduler which is",
    "start": "352639",
    "end": "359360"
  },
  {
    "text": "widely used and adopted in the organization whereas yarn is a specific scheduler for managing data processing",
    "start": "359360",
    "end": "365919"
  },
  {
    "text": "jobs which requires a lot of expertise and thorough knowledge earlier",
    "start": "365919",
    "end": "371039"
  },
  {
    "text": "it had taken us a long time to fine tune yarn scheduler for the for its load balancing and",
    "start": "371039",
    "end": "377440"
  },
  {
    "text": "queuing capabilities to achieve maximum efficiency out of it given that it is quite complicated and",
    "start": "377440",
    "end": "385520"
  },
  {
    "text": "it is hard to find experts for who are really good at performance tuning yarn",
    "start": "385520",
    "end": "391199"
  },
  {
    "text": "it becomes difficult to manage large installations based on yarn",
    "start": "391199",
    "end": "397280"
  },
  {
    "text": "so kubernetes was extremely powerful choice for us because we could",
    "start": "397600",
    "end": "402639"
  },
  {
    "text": "use the same scheduling advanced scheduling capabilities of kubernetes to run data processing workloads",
    "start": "402639",
    "end": "410400"
  },
  {
    "text": "the other really important aspect for any software development is the continuous deployment",
    "start": "411680",
    "end": "418160"
  },
  {
    "text": "data processing jobs are no different they need to have really well-defined predictable modern",
    "start": "418160",
    "end": "424960"
  },
  {
    "text": "ci cd pipelines in order to power faster build and deploy cycles this improves the developer productivity",
    "start": "424960",
    "end": "432080"
  },
  {
    "text": "and also improves the quality of production releases spock makes it easier to use the",
    "start": "432080",
    "end": "438479"
  },
  {
    "text": "industry standard continuous delivery tooling for data processing workloads",
    "start": "438479",
    "end": "444639"
  },
  {
    "text": "at intuit we use argo cd as an operator to manage kubernetes resources and",
    "start": "444639",
    "end": "450319"
  },
  {
    "text": "as the continuous delivery platform for data processing applications earlier this was very cumbersome due to",
    "start": "450319",
    "end": "458160"
  },
  {
    "text": "the complicated shell scripts and multiple sets of imperative commands that we had to use in order to deploy",
    "start": "458160",
    "end": "465360"
  },
  {
    "text": "the applications and to monitor status here is a cd workflow taken from a real in production",
    "start": "465360",
    "end": "472479"
  },
  {
    "text": "application where you can see the comment from the developer going all",
    "start": "472479",
    "end": "477759"
  },
  {
    "text": "the way to production in a fully automated continuous delivery manner",
    "start": "477759",
    "end": "484080"
  },
  {
    "start": "478000",
    "end": "601000"
  },
  {
    "text": "to talk about the rest of the how spock was implemented on the internals of spock i",
    "start": "485840",
    "end": "491919"
  },
  {
    "text": "invite nagaraj to continue with this presentation",
    "start": "491919",
    "end": "497520"
  },
  {
    "text": "thanks sriwatson thanks for the introduction so this is a",
    "start": "500160",
    "end": "506960"
  },
  {
    "text": "native kubernetes native workflow that we have here and this is an end-to-end view of",
    "start": "506960",
    "end": "513200"
  },
  {
    "text": "what a data worker interacting with the data processing infrastructure experiences the data worker does data",
    "start": "513200",
    "end": "520560"
  },
  {
    "text": "exploration using jupyter notebooks and once happy with the results",
    "start": "520560",
    "end": "525600"
  },
  {
    "text": "commits start to get then primarily using the githubs deployment model",
    "start": "525600",
    "end": "531200"
  },
  {
    "text": "this code is pushed to the different environments and for that we use argo cd and argo",
    "start": "531200",
    "end": "538240"
  },
  {
    "text": "workflows were required as part of this deployment of",
    "start": "538240",
    "end": "544399"
  },
  {
    "text": "spark operator we also have enabled hull metastore which talks to a",
    "start": "544399",
    "end": "550240"
  },
  {
    "text": "centralized hive data store database",
    "start": "550240",
    "end": "555839"
  },
  {
    "text": "to manage metadata about the data which is stored in s3 which is intuits data lake along with",
    "start": "555839",
    "end": "563760"
  },
  {
    "text": "that we also have the spark history server deployed which manages the history of the spark",
    "start": "563760",
    "end": "569040"
  },
  {
    "text": "processing jobs and driving all of the spark processing",
    "start": "569040",
    "end": "575120"
  },
  {
    "text": "within the kubernetes is obviously the spark operator the spark operator",
    "start": "575120",
    "end": "580480"
  },
  {
    "text": "is one single installation of spark operator towards in the entire cluster and it manages",
    "start": "580480",
    "end": "585680"
  },
  {
    "text": "name space across all the nexus across the cluster",
    "start": "585680",
    "end": "590880"
  },
  {
    "text": "in future we plan to host it outside of the kubernetes cluster so that we can manage",
    "start": "590880",
    "end": "597040"
  },
  {
    "text": "spark jobs across multiple clusters",
    "start": "597040",
    "end": "605600"
  },
  {
    "start": "601000",
    "end": "686000"
  },
  {
    "text": "the deployment model for data processing jobs reuses the capabilities the centralized kubernetes platform in",
    "start": "605600",
    "end": "612000"
  },
  {
    "text": "the organization provides the centralized kubernetes platform in the organization provides",
    "start": "612000",
    "end": "618160"
  },
  {
    "text": "some of the key capabilities such as self-service namespace creation and management",
    "start": "618160",
    "end": "623839"
  },
  {
    "text": "the authentication and authorization into this namespaces the tools for alerting and monitoring of",
    "start": "623839",
    "end": "630320"
  },
  {
    "text": "the resources which are launched within this namespaces spark operator is another deployment in",
    "start": "630320",
    "end": "635839"
  },
  {
    "text": "one of this to manage the namespaces across the cluster",
    "start": "635839",
    "end": "646880"
  },
  {
    "text": "next we will walk you through an use case which has been enabled using the new spark on kubernetes",
    "start": "646880",
    "end": "654000"
  },
  {
    "text": "infrastructure that we have talked about quickbooks is one of the flagship products of intuit",
    "start": "654000",
    "end": "660720"
  },
  {
    "text": "and transaction categorization is one of the key features within quickbooks",
    "start": "660720",
    "end": "666320"
  },
  {
    "text": "this transaction categorization helps users understand their banking transactions better",
    "start": "666320",
    "end": "673680"
  },
  {
    "text": "if categorization is done right this could provide critical insights into the earnings and spending",
    "start": "673839",
    "end": "679519"
  },
  {
    "text": "patterns of the user and that can be a very handy information for the user",
    "start": "679519",
    "end": "686839"
  },
  {
    "start": "686000",
    "end": "717000"
  },
  {
    "text": "and the challenges with categorization of",
    "start": "686839",
    "end": "692720"
  },
  {
    "text": "features is a similar transaction could be of different category",
    "start": "692720",
    "end": "698160"
  },
  {
    "text": "based on the user especially on the person of the user for example otherwise there is an",
    "start": "698160",
    "end": "705120"
  },
  {
    "text": "example of a home improvement transaction a contractor would categorize it as",
    "start": "705120",
    "end": "711200"
  },
  {
    "text": "materials and supplies but a retailer might categorize it as a repays and",
    "start": "711200",
    "end": "716639"
  },
  {
    "text": "maintenance so in summary transaction categorization",
    "start": "716639",
    "end": "725279"
  },
  {
    "start": "717000",
    "end": "816000"
  },
  {
    "text": "becomes a personalization problem to help with this the model needs to be trained to understand the categorization",
    "start": "725279",
    "end": "731360"
  },
  {
    "text": "preferences of the user this requires feature extraction per user based on their transactions",
    "start": "731360",
    "end": "739279"
  },
  {
    "text": "and and their categorization history all this is done as a data processing",
    "start": "739279",
    "end": "746639"
  },
  {
    "text": "job running on spark and this is an example of the the spark job so it",
    "start": "746639",
    "end": "754079"
  },
  {
    "text": "combines transactions and the user's behavior to produce some features",
    "start": "754079",
    "end": "760720"
  },
  {
    "text": "for that user",
    "start": "760720",
    "end": "763439"
  },
  {
    "text": "the previous feature extraction job for transaction categorization uses argo cd",
    "start": "766079",
    "end": "772079"
  },
  {
    "text": "for continuous deployment this follows the gitoff's deployment model all the resources required for the",
    "start": "772079",
    "end": "779360"
  },
  {
    "text": "job such as the spark role the spark history server the actual scheduled spark application",
    "start": "779360",
    "end": "786079"
  },
  {
    "text": "jobs are managed as native kubernetes resources and argo cd make sure to keep them up to",
    "start": "786079",
    "end": "793040"
  },
  {
    "text": "date with what's required or what's what the data scientist wants it to be",
    "start": "793040",
    "end": "801680"
  },
  {
    "text": "this simplifies infrastructure management as the data workers just need to specify",
    "start": "801680",
    "end": "807360"
  },
  {
    "text": "that in a declarative vmware what needs to be done rather than having to deal with shell scripts or imperative commands on",
    "start": "807360",
    "end": "814639"
  },
  {
    "text": "how things needs to be done",
    "start": "814639",
    "end": "817759"
  },
  {
    "start": "816000",
    "end": "878000"
  },
  {
    "text": "we have to build additional capabilities in addition to deploying spark operator within the kubernetes cluster",
    "start": "821760",
    "end": "827760"
  },
  {
    "text": "here are here are some of the learnings having gone through that journey we have to set up an higher metastore",
    "start": "827760",
    "end": "834320"
  },
  {
    "text": "service which connects to the centralized high meta store we have to package",
    "start": "834320",
    "end": "839920"
  },
  {
    "text": "hadoop aws jars which is not part of the standard spark image",
    "start": "839920",
    "end": "845600"
  },
  {
    "text": "and we have to get it working with s3a file protocol as to not interact with the object store",
    "start": "845600",
    "end": "852800"
  },
  {
    "text": "s3 one of the critical challenges that we faced was getting spark",
    "start": "852800",
    "end": "858480"
  },
  {
    "text": "245 and below to work with hive 2 and latest hadoop aws jazz is a",
    "start": "858480",
    "end": "863839"
  },
  {
    "text": "dependency nightmare and that's where the suggestion to move towards",
    "start": "863839",
    "end": "869360"
  },
  {
    "text": "part 3 which has been recently released as most of the issues with working with",
    "start": "869360",
    "end": "874480"
  },
  {
    "text": "hadoop aws and hive has been fixed in that latest release",
    "start": "874480",
    "end": "880240"
  },
  {
    "start": "878000",
    "end": "902000"
  },
  {
    "text": "spa 3 is also well suited for kubernetes as some of the important things like",
    "start": "882320",
    "end": "888079"
  },
  {
    "text": "dynamic allocations and external shuffle service has been added in spark 3",
    "start": "888079",
    "end": "894240"
  },
  {
    "text": "and some of the critical parking bugs or bugs related to parquet has been fixed with the latest upgrade",
    "start": "894240",
    "end": "900959"
  },
  {
    "text": "to spark 3.",
    "start": "900959",
    "end": "904079"
  },
  {
    "start": "902000",
    "end": "966000"
  },
  {
    "text": "below are some of the advantages we have observed running big data processing workloads on kubernetes",
    "start": "907839",
    "end": "914720"
  },
  {
    "text": "the unified management of clusters across different types of workloads say service workloads and data",
    "start": "914720",
    "end": "920800"
  },
  {
    "text": "processing workloads is a huge win container simplify dependency management",
    "start": "920800",
    "end": "927519"
  },
  {
    "text": "and the data processing jobs get also get to use all the tooling built around",
    "start": "927519",
    "end": "933519"
  },
  {
    "text": "the rest of the organization like artifactory ci cd tools etc this also helps reduce cost",
    "start": "933519",
    "end": "941839"
  },
  {
    "text": "as opex and other resources are shared across multiple different kinds of workloads",
    "start": "941839",
    "end": "950079"
  },
  {
    "text": "declarative specification of data processing jobs and infrastructure make it easier for data workers to",
    "start": "950240",
    "end": "956399"
  },
  {
    "text": "specify and manage them especially for ml this is a huge win",
    "start": "956399",
    "end": "963199"
  },
  {
    "text": "as it involves lot of configs in some cases",
    "start": "963199",
    "end": "969839"
  },
  {
    "text": "these are some of the numbers that we have run that we have seen with running spock in production",
    "start": "971759",
    "end": "979199"
  },
  {
    "text": "there is around 30 percent of cost reduction due to better uses better use of cluster resources across",
    "start": "979199",
    "end": "986560"
  },
  {
    "text": "different uh workloads the dev productivity has improved a lot",
    "start": "986560",
    "end": "992160"
  },
  {
    "text": "because of the automated cd deployments in some cases we are seeing it come down",
    "start": "992160",
    "end": "998560"
  },
  {
    "text": "from week to a day from dev to broad cycle",
    "start": "998560",
    "end": "1004880"
  },
  {
    "text": "the operational overhead of maintaining this has reduced especially the security patches and the cluster upgrades",
    "start": "1004880",
    "end": "1011600"
  },
  {
    "text": "since it is done as part of the entire org there is no need for managing them",
    "start": "1011600",
    "end": "1017120"
  },
  {
    "text": "separately as part of data work and performance wise we are seeing",
    "start": "1017120",
    "end": "1024240"
  },
  {
    "text": "porting an application running on the previous stack to on kubernetes",
    "start": "1024240",
    "end": "1029438"
  },
  {
    "text": "we are able to get similar performance i think there are we definitely see possibilities to",
    "start": "1029439",
    "end": "1037038"
  },
  {
    "text": "tune the scheduler which would help us get further benefits in the future",
    "start": "1037039",
    "end": "1044000"
  },
  {
    "start": "1041000",
    "end": "1087000"
  },
  {
    "text": "having built having worked on a spark operator for data processing workloads in future we",
    "start": "1047039",
    "end": "1052799"
  },
  {
    "text": "would like to build a custom ml operator to orchestrate ml workflow and resources through different stages",
    "start": "1052799",
    "end": "1059919"
  },
  {
    "text": "of ml lifecycle such as feature engineering model training and model deployment",
    "start": "1059919",
    "end": "1067679"
  },
  {
    "text": "we are hoping this operator will encapsulate and abstract the nuances of the underlying infrastructure and runtime",
    "start": "1067679",
    "end": "1076160"
  },
  {
    "text": "as we also want to expose the managed a managed platform",
    "start": "1076240",
    "end": "1082240"
  },
  {
    "text": "for self-serve of data processing jobs and their life cycle management",
    "start": "1082240",
    "end": "1088240"
  },
  {
    "start": "1087000",
    "end": "1103000"
  },
  {
    "text": "thanks for listening to us to our talk if you would like to chat",
    "start": "1091360",
    "end": "1096720"
  },
  {
    "text": "about any of the other things please contact us at our emails",
    "start": "1096720",
    "end": "1104160"
  },
  {
    "start": "1103000",
    "end": "1156000"
  },
  {
    "text": "hello please let us know if you have any questions we'll be glad to answer it",
    "start": "1107440",
    "end": "1113039"
  },
  {
    "text": "live",
    "start": "1120840",
    "end": "1123840"
  },
  {
    "start": "1156000",
    "end": "1245000"
  },
  {
    "text": "i can see some questions in spark versus emr can you elaborate how cost reduction was",
    "start": "1157840",
    "end": "1163600"
  },
  {
    "text": "secured um yeah so there was there were multiple aspects to the cost",
    "start": "1163600",
    "end": "1170400"
  },
  {
    "text": "um one was for example emr as a service has a cost of 25",
    "start": "1170400",
    "end": "1176320"
  },
  {
    "text": "additional surcharge on the underlying hardware cost over and above the underlying hardware cost",
    "start": "1176320",
    "end": "1181520"
  },
  {
    "text": "which was straight away we could avoid and also the operational expenditure",
    "start": "1181520",
    "end": "1187039"
  },
  {
    "text": "around maintaining this clusters since within the organization",
    "start": "1187039",
    "end": "1192240"
  },
  {
    "text": "we had operationalized management of these clusters for the rest of the services",
    "start": "1192240",
    "end": "1197840"
  },
  {
    "text": "workload we could build on top of that so for example if there was a critical security patch which",
    "start": "1197840",
    "end": "1203200"
  },
  {
    "text": "had to be applied for our cluster uh in the past we have to do it separately but with uh",
    "start": "1203200",
    "end": "1210559"
  },
  {
    "text": "going to kubernetes as a single uh resource manager",
    "start": "1210559",
    "end": "1215840"
  },
  {
    "text": "uh all of that is taken care of by the rest so there is a shared",
    "start": "1215840",
    "end": "1221440"
  },
  {
    "text": "operational expenditure cost um and also the additional emr cost",
    "start": "1221440",
    "end": "1226880"
  },
  {
    "text": "um but we have we also see opportunities around scheduling um are basically",
    "start": "1226880",
    "end": "1235440"
  },
  {
    "text": "i mean optimizing the scheduler uh and stuff like that which we haven't done it yet but we",
    "start": "1235440",
    "end": "1241919"
  },
  {
    "text": "think will give us more benefits in the future",
    "start": "1241919",
    "end": "1246799"
  },
  {
    "start": "1245000",
    "end": "1295000"
  },
  {
    "text": "uh in addition to what nagrat said the other um reason why we are able to leverage uh",
    "start": "1247039",
    "end": "1254400"
  },
  {
    "text": "cost and we realize cost advantages is because we're able to now join workloads",
    "start": "1254400",
    "end": "1260400"
  },
  {
    "text": "together so we can run spark applications and other ml workloads in the same cluster",
    "start": "1260400",
    "end": "1268400"
  },
  {
    "text": "utilizing the same compute as well so that gives us an overall cost amortization so we don't",
    "start": "1268400",
    "end": "1276080"
  },
  {
    "text": "have like islands of capacity which is what we used to have for with our emr solutions",
    "start": "1276080",
    "end": "1283039"
  },
  {
    "text": "so while 30 is 25 is the minimum cost advantage that we",
    "start": "1283039",
    "end": "1289600"
  },
  {
    "text": "got and we've seen in some cases almost up to 50 percent of cost advantage",
    "start": "1289600",
    "end": "1297440"
  },
  {
    "start": "1295000",
    "end": "1428000"
  },
  {
    "text": "this another question how much effort was it to move from yarn to kubernetes as a scheduler for your spark jobs",
    "start": "1299200",
    "end": "1306159"
  },
  {
    "text": "um from spark perspective i think the effort was not much since spark supports at least the spark",
    "start": "1306159",
    "end": "1312159"
  },
  {
    "text": "3 supports kubernetes as a resource manager i think it was around the rest of the infrastructure for example in our case",
    "start": "1312159",
    "end": "1318880"
  },
  {
    "text": "s3 was our data lake or our sdfs equivalent equivalent and",
    "start": "1318880",
    "end": "1325280"
  },
  {
    "text": "we had to do a lot of work to get it working with the hadoop aws jars",
    "start": "1325280",
    "end": "1331679"
  },
  {
    "text": "because uh when you run on emr uh there is a emr file system which",
    "start": "1331679",
    "end": "1337520"
  },
  {
    "text": "comes with it which is when which is aws proprietary which we cannot use with kubernetes so we have",
    "start": "1337520",
    "end": "1344880"
  },
  {
    "text": "to switch back to the s3a file system which is the open source equivalent of it",
    "start": "1344880",
    "end": "1351440"
  },
  {
    "text": "um the other thing was uh emr is a managed service which provides",
    "start": "1351440",
    "end": "1357200"
  },
  {
    "text": "you things like hive and stuff rest of the stuff hype server and hive service to which",
    "start": "1357200",
    "end": "1365520"
  },
  {
    "text": "since within the organization we use a centralized high metastore for managing the metadata we have to build",
    "start": "1365520",
    "end": "1372240"
  },
  {
    "text": "support for that um so i think these were the two big ones and then um",
    "start": "1372240",
    "end": "1377760"
  },
  {
    "text": "i think a lot of the pain pain has been eased with spark 3 with all this open source different open",
    "start": "1377760",
    "end": "1383520"
  },
  {
    "text": "source components but yeah just lifting and shifting the existing application asses",
    "start": "1383520",
    "end": "1390400"
  },
  {
    "text": "involved only this changes",
    "start": "1390400",
    "end": "1393760"
  },
  {
    "text": "um i think yeah there's a question from stephen which technical part of this",
    "start": "1396400",
    "end": "1402799"
  },
  {
    "text": "migration journey was the most complex and what more challenges did he face",
    "start": "1402799",
    "end": "1409120"
  },
  {
    "text": "i think as i said i think it was mostly around the rest of the infrastructure like s3 high metastore",
    "start": "1409120",
    "end": "1415200"
  },
  {
    "text": "and getting them to work with getting them getting all this big data infrastructure",
    "start": "1415200",
    "end": "1420799"
  },
  {
    "text": "to move on to the kubernetes world was a challenging part for us",
    "start": "1420799",
    "end": "1430960"
  },
  {
    "text": "there's a question in the chat um so all your data access by spark resides in s3",
    "start": "1430960",
    "end": "1436320"
  },
  {
    "text": "now um yes um all the spark data is an s3",
    "start": "1436320",
    "end": "1441919"
  },
  {
    "text": "and we have the s3a file system view which allows spark to access the",
    "start": "1441919",
    "end": "1449360"
  },
  {
    "text": "data from",
    "start": "1449360",
    "end": "1451919"
  },
  {
    "text": "s3",
    "start": "1456840",
    "end": "1459840"
  },
  {
    "start": "1468000",
    "end": "1531000"
  },
  {
    "text": "is to add on to the effort question i think it took us about",
    "start": "1469760",
    "end": "1475919"
  },
  {
    "text": "six months start to end to actually create a fully paved road",
    "start": "1475919",
    "end": "1482480"
  },
  {
    "text": "approach around around spark where we were able to then provision",
    "start": "1482480",
    "end": "1488240"
  },
  {
    "text": "name spaces and provide security controls and and so on um a lot of as nagraj said a lot of the",
    "start": "1488240",
    "end": "1495840"
  },
  {
    "text": "work was around um making spark and kubernetes",
    "start": "1495840",
    "end": "1501120"
  },
  {
    "text": "work with things like name space access and um roles specifications and so on uh",
    "start": "1501120",
    "end": "1507919"
  },
  {
    "text": "spark itself we were able to get the spark application running in a few days but the majority of the",
    "start": "1507919",
    "end": "1515279"
  },
  {
    "text": "effort was spent on operationalizing um the rest of the kubernetes infrastructure",
    "start": "1515279",
    "end": "1529840"
  },
  {
    "start": "1531000",
    "end": "1576000"
  },
  {
    "text": "um question did you experience network bottlenecks with s3",
    "start": "1531919",
    "end": "1538320"
  },
  {
    "text": "aggregate yeah um so i think there were some challenges with uh",
    "start": "1540840",
    "end": "1546480"
  },
  {
    "text": "s3 uh a initially um but uh nothing specific related to",
    "start": "1546480",
    "end": "1553120"
  },
  {
    "text": "bottlenecks obviously the i think the emr there is a lot of work going on in the s3a file system",
    "start": "1553120",
    "end": "1559279"
  },
  {
    "text": "related to uh at least on the commit side uh there are different commit strategies",
    "start": "1559279",
    "end": "1565279"
  },
  {
    "text": "um but yeah uh we did not face much on the bottlenecks on the network bottlenecks",
    "start": "1565279",
    "end": "1573840"
  },
  {
    "start": "1576000",
    "end": "1614000"
  },
  {
    "text": "the question from alex on using the spark operator to deploy spark",
    "start": "1578960",
    "end": "1584000"
  },
  {
    "text": "jobs on kubernetes yes we are using the spark operator from google and we're",
    "start": "1584000",
    "end": "1591200"
  },
  {
    "text": "using that to deploy spark on kubernetes we have a",
    "start": "1591200",
    "end": "1597440"
  },
  {
    "text": "we have a local installation of the spark operator and so intuits kubernetes has its own um",
    "start": "1597440",
    "end": "1605200"
  },
  {
    "text": "controls around namespace security and so on so we have a custom deployment of the spark",
    "start": "1605200",
    "end": "1611279"
  },
  {
    "text": "operator in our cluster there's another question on spark",
    "start": "1611279",
    "end": "1618320"
  },
  {
    "text": "streaming are we running spark streaming jobs such as pad jobs we are running both we have both spark",
    "start": "1618320",
    "end": "1624799"
  },
  {
    "text": "streaming jobs and spark back jobs running on this infrastructure",
    "start": "1624799",
    "end": "1630080"
  },
  {
    "text": "majority are batch there are a few streaming jobs as well",
    "start": "1630080",
    "end": "1635200"
  },
  {
    "text": "in general for streaming intuit users beam apache beam and we have a beam based",
    "start": "1635200",
    "end": "1641840"
  },
  {
    "text": "streaming platform so we try to direct most of the streaming uh use cases on to",
    "start": "1641840",
    "end": "1647600"
  },
  {
    "text": "the beam platform there are quite a few legacy spark",
    "start": "1647600",
    "end": "1652840"
  },
  {
    "text": "streaming applications which are starting to move to this infrastructure",
    "start": "1652840",
    "end": "1669840"
  },
  {
    "start": "1671000",
    "end": "1750000"
  },
  {
    "text": "the question on as a data scientist data analyst what new things should they learn to work in this park",
    "start": "1672320",
    "end": "1678240"
  },
  {
    "text": "environment coming from the aws emr background the",
    "start": "1678240",
    "end": "1683360"
  },
  {
    "text": "good news is we as a platform team we've abstracted out",
    "start": "1683360",
    "end": "1688480"
  },
  {
    "text": "a lot of the underlying spark infrastructure so it does um",
    "start": "1688480",
    "end": "1696799"
  },
  {
    "text": "the expectation from data scientists is that they are um familiar with spark and they",
    "start": "1696799",
    "end": "1703600"
  },
  {
    "text": "write a really good spark code and after they commit the spark code the platform takes",
    "start": "1703600",
    "end": "1709760"
  },
  {
    "text": "over and the platform does the build and deployment in a standardized fashion",
    "start": "1709760",
    "end": "1716399"
  },
  {
    "text": "for them so they um from a data scientist perspective they don't really see",
    "start": "1716399",
    "end": "1721520"
  },
  {
    "text": "the um the underlying kubernetes infrastructure and all the um complexities that go around with it",
    "start": "1721520",
    "end": "1729440"
  },
  {
    "text": "so in addition to just having a spark application we had to build um the ci cd for the application so that",
    "start": "1729440",
    "end": "1737600"
  },
  {
    "text": "we can provide our data scientists and analysts with a interface that was",
    "start": "1737600",
    "end": "1743200"
  },
  {
    "text": "as close to what they already know which is around spark and so that they can be productive with",
    "start": "1743200",
    "end": "1749440"
  },
  {
    "text": "that yeah just to add on to that um i think once one of the big changes one going",
    "start": "1749440",
    "end": "1756640"
  },
  {
    "start": "1750000",
    "end": "1816000"
  },
  {
    "text": "from emr to on kubernetes was the whole container",
    "start": "1756640",
    "end": "1762000"
  },
  {
    "text": "journey uh so which they were very familiar with because that's how they used to deploy",
    "start": "1762000",
    "end": "1767039"
  },
  {
    "text": "their machine learning models so now they could they can do the same thing for",
    "start": "1767039",
    "end": "1772320"
  },
  {
    "text": "their future engineering or data processing jobs um and i think since",
    "start": "1772320",
    "end": "1779360"
  },
  {
    "text": "we were using most of the stuff which was already built in the organization like ci cd pipelines and uh build automation",
    "start": "1779360",
    "end": "1787840"
  },
  {
    "text": "um so it's free it's uh fit seamlessly into that infrastructure and they did not have to",
    "start": "1787840",
    "end": "1794320"
  },
  {
    "text": "do much apart from declaratively specifying in the reamel what needs to be done to run their spark",
    "start": "1794320",
    "end": "1800799"
  },
  {
    "text": "jobs yeah i think there's a question for max beam",
    "start": "1800799",
    "end": "1808159"
  },
  {
    "text": "is also on kubernetes yes beam is also on kubernetes it runs on",
    "start": "1808159",
    "end": "1814159"
  },
  {
    "text": "entirely on the kubernetes infrastructure we have beam running with both samsa",
    "start": "1814159",
    "end": "1821679"
  },
  {
    "text": "and flink as back-ends and we have again built a platform",
    "start": "1821679",
    "end": "1827360"
  },
  {
    "text": "around it so our customers who are our data scientists and analysts they get a very simple interface",
    "start": "1827360",
    "end": "1835600"
  },
  {
    "text": "to write their beam code and the platform takes care of ci cd and deploying it for them",
    "start": "1835600",
    "end": "1843519"
  },
  {
    "start": "1845000",
    "end": "1912000"
  },
  {
    "text": "there's a question from sasha is it possible to get the slides definitely we will publish the slides",
    "start": "1846640",
    "end": "1852320"
  },
  {
    "text": "on slideshare",
    "start": "1852320",
    "end": "1865840"
  },
  {
    "text": "um",
    "start": "1904840",
    "end": "1907840"
  },
  {
    "start": "1912000",
    "end": "1969000"
  },
  {
    "text": "hope this is a question from max on um how we manage the containers for spark",
    "start": "1913760",
    "end": "1918840"
  },
  {
    "text": "so the question is for spark jugs are you preparing container for every job",
    "start": "1918840",
    "end": "1925600"
  },
  {
    "text": "or do we have an universal container which pulls the job jar um it's a little nuance so we have a",
    "start": "1925600",
    "end": "1933360"
  },
  {
    "text": "spark based container which has all the spark related",
    "start": "1933360",
    "end": "1940240"
  },
  {
    "text": "stuff and then what we do is during the build process we layer layer the spark",
    "start": "1940240",
    "end": "1947519"
  },
  {
    "text": "job on top of that base container and we create a",
    "start": "1947519",
    "end": "1952640"
  },
  {
    "text": "new container which is versioned very specifically versioned and with the commit id and that is used",
    "start": "1952640",
    "end": "1960720"
  },
  {
    "text": "to propagate through our various environments like dev tests and production environments",
    "start": "1960720",
    "end": "1969919"
  },
  {
    "start": "1969000",
    "end": "2025000"
  },
  {
    "text": "so the answer is yes there is a container for every job and um",
    "start": "1970159",
    "end": "1976559"
  },
  {
    "text": "this is built this is a the it's a it's a thin layer uh built from the base container of",
    "start": "1976559",
    "end": "1984240"
  },
  {
    "text": "spark there are there is one there is another variant which we are using for",
    "start": "1984240",
    "end": "1989399"
  },
  {
    "text": "featurization uh feature processing in um in the machine learning workflows where",
    "start": "1989399",
    "end": "1997200"
  },
  {
    "text": "we have we interact with the feature store which is uh in s3 and we need to run spark",
    "start": "1997200",
    "end": "2003279"
  },
  {
    "text": "application to get the features from the features too in that case the feature store reader",
    "start": "2003279",
    "end": "2008960"
  },
  {
    "text": "itself is a pre-packaged application and we only pass in configurations but a majority use cases",
    "start": "2008960",
    "end": "2016720"
  },
  {
    "text": "where the jobs will be will be built every commit gets built",
    "start": "2016720",
    "end": "2022640"
  },
  {
    "text": "and gets versioned yeah just to add into that i think",
    "start": "2022640",
    "end": "2030799"
  },
  {
    "start": "2025000",
    "end": "2105000"
  },
  {
    "text": "this was a very pleasant experience also for example we take the open source base spark image",
    "start": "2030799",
    "end": "2037760"
  },
  {
    "text": "and then um this one level of overlay where we add stuff like hadoop aws jars or specific jars",
    "start": "2037760",
    "end": "2045200"
  },
  {
    "text": "required for meta store access and stuff like that so that's one level of overlay",
    "start": "2045200",
    "end": "2050398"
  },
  {
    "text": "um and then that is released and on top of that application teams can take that and just",
    "start": "2050399",
    "end": "2056398"
  },
  {
    "text": "add their stuff on top of that so i think it's uh because of the whole container thing",
    "start": "2056399",
    "end": "2062320"
  },
  {
    "text": "it's uh very easy to manage the dependency also and whenever there's a new version of a",
    "start": "2062320",
    "end": "2067440"
  },
  {
    "text": "spark we just the there's a central platform team or us who are responsible for releasing the",
    "start": "2067440",
    "end": "2073040"
  },
  {
    "text": "base images and then the application teams can take that and build their stuff on top of it",
    "start": "2073040",
    "end": "2089839"
  },
  {
    "start": "2105000",
    "end": "2140000"
  },
  {
    "text": "i want to go back to stephen's question on um what new things they would have to learn",
    "start": "2106000",
    "end": "2111520"
  },
  {
    "text": "to work in the spock environment coming from the emr background one of the things",
    "start": "2111520",
    "end": "2117040"
  },
  {
    "text": "that we have done with this is we've actually reduced the things that a data scientist has to learn because they don't need to",
    "start": "2117040",
    "end": "2123440"
  },
  {
    "text": "learn about how to set up an emr how to manage the infrastructure and write like",
    "start": "2123440",
    "end": "2128640"
  },
  {
    "text": "terraform or cloud formation scripts so there's a lot of things that they don't need to do and instead they just need to focus on their",
    "start": "2128640",
    "end": "2135680"
  },
  {
    "text": "spark code which has been the focus of spock",
    "start": "2135680",
    "end": "2140480"
  },
  {
    "start": "2140000",
    "end": "2181000"
  },
  {
    "text": "okay we just got a notification that we should be wrapping up so um thank you all for",
    "start": "2141760",
    "end": "2150000"
  },
  {
    "text": "attending our talk and these were great questions thank you so much would love to connect",
    "start": "2150000",
    "end": "2155280"
  },
  {
    "text": "and continue to continue the conversation we will publish the slides and um and share it",
    "start": "2155280",
    "end": "2163839"
  },
  {
    "text": "in the appropriate forums so um thank you so much and please do feel",
    "start": "2163839",
    "end": "2169599"
  },
  {
    "text": "free to reach out to us for any questions and any any further conversations on this we'll be glad to",
    "start": "2169599",
    "end": "2177119"
  },
  {
    "text": "collaborate yeah thanks everyone",
    "start": "2177119",
    "end": "2183760"
  }
]