[
  {
    "text": "hello everyone welcome to the session and uh thank you for making it to almost",
    "start": "560",
    "end": "5600"
  },
  {
    "text": "the last session of the conference my name is minaki Kosik and I want to introduce my co-speaker Shiva Krishna",
    "start": "5600",
    "end": "12000"
  },
  {
    "text": "Mara we both work at Nvidia and are responsible for helping customers deploy",
    "start": "12000",
    "end": "17840"
  },
  {
    "text": "and life cycle manage Nvidia inference microservice in short known as Nims on",
    "start": "17840",
    "end": "23920"
  },
  {
    "text": "kubernetes and uh we see customers using variety of deployments from Helm charts",
    "start": "23920",
    "end": "29279"
  },
  {
    "text": "to casers to our open source Nim operator today we are going to share some of the learnings in helping our",
    "start": "29279",
    "end": "35840"
  },
  {
    "text": "customers and uh the agenda for today's uh uh session is as follows we're going",
    "start": "35840",
    "end": "41719"
  },
  {
    "text": "to start with overview and then look at components of inference and fine tuning they're are different from a typical",
    "start": "41719",
    "end": "48520"
  },
  {
    "text": "microservice and then look at some of the emerging Technologies such as AI agents and finally",
    "start": "48520",
    "end": "55039"
  },
  {
    "text": "conclusion so generative AI is giving rise to Sovereign clouds because customer customer wants data",
    "start": "55039",
    "end": "60760"
  },
  {
    "text": "sovereignity security and privacy and uh most of the Enterprise",
    "start": "60760",
    "end": "66600"
  },
  {
    "text": "really want to customize because they want to provide uh business specific answers as seen from this",
    "start": "66600",
    "end": "73479"
  },
  {
    "text": "chatbot so let's take a look at these pipelines so a simplistic inference pipeline is you would have a large",
    "start": "73479",
    "end": "79759"
  },
  {
    "text": "language model front ended with a guard rail uh for a rack pipeline you would uh",
    "start": "79759",
    "end": "84960"
  },
  {
    "text": "have an uh you you would have an Enterprise data and you would use an embedding model to index your data using",
    "start": "84960",
    "end": "90479"
  },
  {
    "text": "a vector database and then in response to the customer query um you would use a llm framework such as a l chain and use",
    "start": "90479",
    "end": "98159"
  },
  {
    "text": "an embedding model to retrieve the relevant chunks and then finally rerank it combine it with a query and provide",
    "start": "98159",
    "end": "105320"
  },
  {
    "text": "the response finally uh for fine-tuning models you would run a fine-tuning job",
    "start": "105320",
    "end": "111280"
  },
  {
    "text": "and then evaluate and if it is good enough use your uh uh L of adapters",
    "start": "111280",
    "end": "116520"
  },
  {
    "text": "generate lot of adapters and fine tune and start serving so when we take a look at these pipelines there are inference",
    "start": "116520",
    "end": "122840"
  },
  {
    "text": "servers such as embedding reranking and large language models and then there are",
    "start": "122840",
    "end": "127880"
  },
  {
    "text": "fine-tuning jobs which is in uh blue and dependencies which are really well-known",
    "start": "127880",
    "end": "133840"
  },
  {
    "text": "Services microservices which we know about which is Vector databases or uh even guard rails and evaluator so we are",
    "start": "133840",
    "end": "141480"
  },
  {
    "text": "going to focus more on inference servers and fine tuning jobs in this",
    "start": "141480",
    "end": "147239"
  },
  {
    "text": "presentation so when we took take a look at current landscape there is awesome work which is going on in open source",
    "start": "147239",
    "end": "153000"
  },
  {
    "text": "Community it uh there are uh there are inference server server platforms as",
    "start": "153000",
    "end": "158160"
  },
  {
    "text": "well as training platforms such as kerve CU flow and also our open- source anim",
    "start": "158160",
    "end": "164040"
  },
  {
    "text": "operator and there are many inference servers for example there is BLM there is Onyx there is NVIDIA Nim our talk is",
    "start": "164040",
    "end": "172000"
  },
  {
    "text": "independent of these inference server platforms and also the inference servers and is applicable to all of",
    "start": "172000",
    "end": "178599"
  },
  {
    "text": "them so so let's look at inference and fine tuning so what's an inference server typically large language models",
    "start": "178599",
    "end": "186319"
  },
  {
    "text": "get very large so for example uh there is a 405b uh llama model and it would",
    "start": "186319",
    "end": "191959"
  },
  {
    "text": "have eight uh it needs around 800 gig of storage so these models are not bundled",
    "start": "191959",
    "end": "198080"
  },
  {
    "text": "into the inference server image so when we when we are talking about inference server we talking about the model as",
    "start": "198080",
    "end": "204560"
  },
  {
    "text": "well as the inference server and also the inference server a typical inference server this is an example of Nvidia Nim",
    "start": "204560",
    "end": "211080"
  },
  {
    "text": "but it's true for any inference server it has three layers one is the API layer",
    "start": "211080",
    "end": "216799"
  },
  {
    "text": "the second is the inference serving layer and third is a runtime layer the job of the inference serving layer and",
    "start": "216799",
    "end": "223400"
  },
  {
    "text": "the runtime layer is to load the model uh model into the GPU memory and start",
    "start": "223400",
    "end": "228480"
  },
  {
    "text": "the inferencing service so um now let's take a look at inference serving pipeline so the first thing is uh we",
    "start": "228480",
    "end": "235040"
  },
  {
    "text": "would download the model into the kubernetes cluster the second would be to allocate sufficient gpus and start",
    "start": "235040",
    "end": "243400"
  },
  {
    "text": "running the inference server and start serving uh Ser serving the customers and",
    "start": "243400",
    "end": "248439"
  },
  {
    "text": "the third would be performing day2 managements such as upgrade F uh",
    "start": "248439",
    "end": "254239"
  },
  {
    "text": "upgrades Auto scale um and observability a fine-tuning pipeline",
    "start": "254239",
    "end": "260680"
  },
  {
    "text": "looks almost similar you download a foundation model into your kubernetes cluster and then schedule your",
    "start": "260680",
    "end": "267680"
  },
  {
    "text": "fine-tuning jobs uh using a some kind of GPU scheduler and then finally it",
    "start": "267680",
    "end": "273600"
  },
  {
    "text": "generates a la model or a fine-tune model so um let's look at each of these",
    "start": "273600",
    "end": "280120"
  },
  {
    "text": "components in more details and uh what are some of the common best practices so",
    "start": "280120",
    "end": "285680"
  },
  {
    "text": "for Model Management there are three things that are important one is that",
    "start": "285680",
    "end": "291000"
  },
  {
    "text": "you want uh the you want the initial inference and the autoscaling time to be quick the second is that the model",
    "start": "291000",
    "end": "298400"
  },
  {
    "text": "should be available across name spaces and across node and third is the security of the model so first uh to uh",
    "start": "298400",
    "end": "306240"
  },
  {
    "text": "to make sure that you have the models uh which you can service fast reduce initial inference time and autoscaling",
    "start": "306240",
    "end": "312759"
  },
  {
    "text": "it is best to pre-cache the models into the kubernetes Clusters what I've seen is that the customers don't like to",
    "start": "312759",
    "end": "318720"
  },
  {
    "text": "directly pull the models from a uh Global container registry all the way into the kubernetes cluster but that",
    "start": "318720",
    "end": "325520"
  },
  {
    "text": "they want to First pull the model into the local registry the reason is they want to support air gap they want to",
    "start": "325520",
    "end": "331600"
  },
  {
    "text": "have uh compliance uh for the models before they push it into their environment uh when pulling the models",
    "start": "331600",
    "end": "338720"
  },
  {
    "text": "across the global and the uh local registry um the the one thing to look at",
    "start": "338720",
    "end": "344319"
  },
  {
    "text": "is the protocols because the different registry support different protocols it could be S3 https hugging phase NGC",
    "start": "344319",
    "end": "351800"
  },
  {
    "text": "protocol or it could be some oci compliant and so the your model poer image the the local registry and the",
    "start": "351800",
    "end": "359080"
  },
  {
    "text": "container Reg registry has to match um when pulling the model and making sure",
    "start": "359080",
    "end": "364560"
  },
  {
    "text": "that it is available uh across Auto scale and initial inference um there are",
    "start": "364560",
    "end": "370800"
  },
  {
    "text": "two options one is to use a distributed storage which is what most of our customers use such as NFS S3 or NV mesh",
    "start": "370800",
    "end": "379280"
  },
  {
    "text": "or uh you can also like kerve uh have uh local uh local PVS but then ensure that",
    "start": "379280",
    "end": "386720"
  },
  {
    "text": "all uh it is replicated across all nodes in which you want to autoscale so the",
    "start": "386720",
    "end": "392080"
  },
  {
    "text": "second is that uh the model should be available across across PVCs and across",
    "start": "392080",
    "end": "398560"
  },
  {
    "text": "uh nodes and uh if you have a distributed storage and a local PV it is very easy that it is available across uh",
    "start": "398560",
    "end": "406400"
  },
  {
    "text": "nodes and across uh across name spaces the third is uh security model security",
    "start": "406400",
    "end": "411720"
  },
  {
    "text": "model is a precious resource um the goal standard would be you encrypt your model and the inference server decrypts the",
    "start": "411720",
    "end": "418440"
  },
  {
    "text": "model but um not most of the models are not that kind of encrypted and uh you",
    "start": "418440",
    "end": "424280"
  },
  {
    "text": "can uh use uh many existing kubernetes tool to uh do some uh initial low-level",
    "start": "424280",
    "end": "430919"
  },
  {
    "text": "stuff so for example um when you're serving the model you can serve on different name spaces so uh that",
    "start": "430919",
    "end": "437759"
  },
  {
    "text": "provides isolation especially uh with PB uh and even if you're using within the",
    "start": "437759",
    "end": "442919"
  },
  {
    "text": "same name space then you can have a different PV PVC pair for the model if",
    "start": "442919",
    "end": "448039"
  },
  {
    "text": "you're using within the PV DC then uh you can create different subpaths so for",
    "start": "448039",
    "end": "453520"
  },
  {
    "text": "example you can have a subpath for a yellow model and subpath for the orange model so in that way um you only Mount",
    "start": "453520",
    "end": "460759"
  },
  {
    "text": "the subpath uh into the uh inference container and you can also have a group",
    "start": "460759",
    "end": "466520"
  },
  {
    "text": "ID uh match between uh when you create a a file system with the PVC as well as",
    "start": "466520",
    "end": "472240"
  },
  {
    "text": "with the inference server uh so that it provides some uh initial level of",
    "start": "472240",
    "end": "477800"
  },
  {
    "text": "security so now now uh let's take a look at scheduling so really the goal uh in",
    "start": "477800",
    "end": "484319"
  },
  {
    "text": "the scheduling is that the model May support many gpus but you may have a preference of a GPU so the goal of the",
    "start": "484319",
    "end": "491319"
  },
  {
    "text": "model uh the goal of uh the scheduling is that the uh you can allocate your preferred GPU into the kubernetes uh for",
    "start": "491319",
    "end": "498680"
  },
  {
    "text": "your model in the kubernetes cluster so the the very simplistic approach is to use a kubernetes scheduler to uh to",
    "start": "498680",
    "end": "506360"
  },
  {
    "text": "achieve this um this this works great except if you have mixed nodes so like",
    "start": "506360",
    "end": "511720"
  },
  {
    "text": "for example here uh in this cluster there is an l4s a100 and h100 and kuber",
    "start": "511720",
    "end": "518719"
  },
  {
    "text": "schedular might might pick a GPU from l4s but uh if you have a 70b model then",
    "start": "518719",
    "end": "525240"
  },
  {
    "text": "it will not fit into l4s so um the one one thing that can be done is that you",
    "start": "525240",
    "end": "531399"
  },
  {
    "text": "can have note selectors so so that you influence a scheduler to go to the specific preferred node and then pick up",
    "start": "531399",
    "end": "538399"
  },
  {
    "text": "the GPU the challenge here is that uh if you have lots of inference servers then",
    "start": "538399",
    "end": "544279"
  },
  {
    "text": "you have to inject these node selectors in every pod and uh with the new Dr you",
    "start": "544279",
    "end": "550440"
  },
  {
    "text": "can H uh you can have a single resource claim template and so you modify at a single place and then it is applicable",
    "start": "550440",
    "end": "557720"
  },
  {
    "text": "across uh your all your INF server so that's uh that's what you can do for a GPU",
    "start": "557720",
    "end": "564120"
  },
  {
    "text": "scheduling next let's take a look at observability and autoscaling so for um",
    "start": "564120",
    "end": "571440"
  },
  {
    "text": "uh and and other dat2 operations like upgrades so upgrades are almost similar to what exists in kubernetes uh existing",
    "start": "571440",
    "end": "579600"
  },
  {
    "text": "microservices uh for example uh you you can do rolling upgrades or you can do Canary upgrades for observability",
    "start": "579600",
    "end": "587040"
  },
  {
    "text": "because the system is getting complex it would be best to have three layers of uh",
    "start": "587040",
    "end": "592839"
  },
  {
    "text": "uh a three or four layers of observability or the level one observability would be like GPU the",
    "start": "592839",
    "end": "598760"
  },
  {
    "text": "level two observability would be observability of the platform like for example kerve or Nim operator and then",
    "start": "598760",
    "end": "605720"
  },
  {
    "text": "the third level of observability would be the inference server itself so for example Nim or VM model that you are",
    "start": "605720",
    "end": "612240"
  },
  {
    "text": "using and uh once you have observability in place uh it is very easy to start uh",
    "start": "612240",
    "end": "618320"
  },
  {
    "text": "using it for autoscaling uh for autoscaling it's best to use the U",
    "start": "618320",
    "end": "623440"
  },
  {
    "text": "inference uh server uh metric because uh small models like embedding and the",
    "start": "623440",
    "end": "628880"
  },
  {
    "text": "ranking model you may have a single model running on uh multiple models",
    "start": "628880",
    "end": "633920"
  },
  {
    "text": "running on a single GPU especially if you have an h100 or an a100 GPU so if",
    "start": "633920",
    "end": "639079"
  },
  {
    "text": "you use the GPU metrics it uh uh it will not reflect really which which of the",
    "start": "639079",
    "end": "645360"
  },
  {
    "text": "inference server model you want to scale so it would be good to use the uh model server metrics and uh I know there's",
    "start": "645360",
    "end": "652240"
  },
  {
    "text": "been many talks on in this uh in this conference um the KV cache is a very a",
    "start": "652240",
    "end": "657480"
  },
  {
    "text": "good metrics to use because it's the Leading Edge metrics which gives uh a state of uh it has not saturated and uh",
    "start": "657480",
    "end": "666120"
  },
  {
    "text": "you want to Auto scale um if you configure autoscaler you won't be able to scale to zero so the preferred uh if",
    "start": "666120",
    "end": "673000"
  },
  {
    "text": "you want to scale to zero then uh have a scaler like ker so with that let me show",
    "start": "673000",
    "end": "678959"
  },
  {
    "text": "a simple um",
    "start": "678959",
    "end": "685120"
  },
  {
    "text": "demo let me just see yeah so I'm showing the demo using Nim operator Nim operator",
    "start": "685279",
    "end": "692399"
  },
  {
    "text": "is a very simple operator it has three crds it has a caching crd it has a",
    "start": "692399",
    "end": "698480"
  },
  {
    "text": "serving crd and a pipeline and so we look at start by looking at how we",
    "start": "698480",
    "end": "704560"
  },
  {
    "text": "pre-cache the models into the kuber 0s cluster so for uh pre-caching the model",
    "start": "704560",
    "end": "710440"
  },
  {
    "text": "uh this is almost similar across kerve across name operator you have a model poer image so um the model po image",
    "start": "710440",
    "end": "719399"
  },
  {
    "text": "supports some of the mular image supports only a single protocol like here or it may support multiple",
    "start": "719399",
    "end": "726079"
  },
  {
    "text": "protocols for example we plan to support NGC S3 https so as long as you have that",
    "start": "726079",
    "end": "732839"
  },
  {
    "text": "and then um you have to attach a PVC you can pre-create or uh you can in our case",
    "start": "732839",
    "end": "739399"
  },
  {
    "text": "we create the PVC optionally create the PVC so now we run the caching job and",
    "start": "739399",
    "end": "745160"
  },
  {
    "text": "this uh caching job pulls the model either from your local registry or from the global registry into the kubernetes",
    "start": "745160",
    "end": "752079"
  },
  {
    "text": "cluster so you can see that here I'm downloading a a model which is also laa",
    "start": "752079",
    "end": "757440"
  },
  {
    "text": "capable which we are going to use in other parts of our demo and now um this",
    "start": "757440",
    "end": "763360"
  },
  {
    "text": "caching has been done and uh the model uh has uh uh now the model is in the PVC",
    "start": "763360",
    "end": "769240"
  },
  {
    "text": "so this is the PVC so next uh after we have pre-cached the model we want to run",
    "start": "769240",
    "end": "774680"
  },
  {
    "text": "this model on a kubernetes uh cluster so U there are the node is already labeled",
    "start": "774680",
    "end": "780440"
  },
  {
    "text": "especially if you're running Nvidia GPU operator with the the uh the um product",
    "start": "780440",
    "end": "785760"
  },
  {
    "text": "ID as well as the GPU ID so um uh I have a uh I just have like four gpus on a",
    "start": "785760",
    "end": "793320"
  },
  {
    "text": "single node cluster so I don't really need to uh add note selectors but uh it",
    "start": "793320",
    "end": "799240"
  },
  {
    "text": "would be good to like add notes selectors if you have a mixed note cluster and uh so here um here I'm uh",
    "start": "799240",
    "end": "807480"
  },
  {
    "text": "this this is a Nim service CR and uh I'm pulling a simple small model an 8B model",
    "start": "807480",
    "end": "814920"
  },
  {
    "text": "and uh attaching the cache uh or the PVC and because we are attaching the cache",
    "start": "814920",
    "end": "820399"
  },
  {
    "text": "it I don't need to specify the limits because we Auto detect the gpus but otherwise you have to specify the limits",
    "start": "820399",
    "end": "826639"
  },
  {
    "text": "and now this is configuring a horizontal pod autoscaler with minimum uh Max of",
    "start": "826639",
    "end": "831920"
  },
  {
    "text": "two and one replica and using KB cache uh when KB cach exceeds 50% to spin",
    "start": "831920",
    "end": "838560"
  },
  {
    "text": "another replica and so now we run the service so now the service is running",
    "start": "838560",
    "end": "844360"
  },
  {
    "text": "and uh after the service is running um you uh yeah so there is the uh you can",
    "start": "844360",
    "end": "850000"
  },
  {
    "text": "see the service is running and now we'll start uh seeing the auto scaling job so",
    "start": "850000",
    "end": "855639"
  },
  {
    "text": "uh here uh uh the basically look at the kubernetes service to pass traffic to",
    "start": "855639",
    "end": "862880"
  },
  {
    "text": "the Nim and uh then see if the horizontal Auto scalar scales uh",
    "start": "862880",
    "end": "869600"
  },
  {
    "text": "as the KB cach exceeds um the KB cach utilization exceeds above",
    "start": "869600",
    "end": "875839"
  },
  {
    "text": "50% so um yeah so uh in uh this is uh the grafana dashboard uh this is the",
    "start": "875839",
    "end": "882600"
  },
  {
    "text": "dashboard from the GPU operator the dcgm metric but as I mentioned uh it is preferable to use the Nim metric or the",
    "start": "882600",
    "end": "889440"
  },
  {
    "text": "inference server metric so we are not going to use the GPU metrix and uh we",
    "start": "889440",
    "end": "894880"
  },
  {
    "text": "are going to use the model server metrics and here right now the K",
    "start": "894880",
    "end": "900399"
  },
  {
    "text": "utilization is very low so we're going to just simply pass traffic and once the",
    "start": "900399",
    "end": "906399"
  },
  {
    "text": "the traffic is passed on the kubernetes service which is a n service and once that passes we are going to uh just uh",
    "start": "906399",
    "end": "915320"
  },
  {
    "text": "add more uh users and add more chck completion commands and we see that the KV metrics has started increasing as we",
    "start": "915320",
    "end": "921920"
  },
  {
    "text": "increase the load and uh that uh changes the horizontal Part auto scaler so",
    "start": "921920",
    "end": "927320"
  },
  {
    "text": "that's uh really My Demo and then uh now next uh Shiva will take over and for the",
    "start": "927320",
    "end": "934120"
  },
  {
    "text": "rest of the presentation so here you can see that uh uh the now two replicas are",
    "start": "934120",
    "end": "940000"
  },
  {
    "text": "running and the horizontal part AOS scaler has been",
    "start": "940000",
    "end": "944160"
  },
  {
    "text": "configured um thank you um um so I'm going to talk about um",
    "start": "948959",
    "end": "955240"
  },
  {
    "text": "multia serving um multia serving is a quite popular um serving technique where",
    "start": "955240",
    "end": "962040"
  },
  {
    "text": "um multiple fine tuned Lowa adapters can be served using a single um serving",
    "start": "962040",
    "end": "967839"
  },
  {
    "text": "instance the key thing is uh multiple adapters will share a a common",
    "start": "967839",
    "end": "973440"
  },
  {
    "text": "Foundation model rather than uh for different use cases and different like different um tasks rather than having to",
    "start": "973440",
    "end": "980800"
  },
  {
    "text": "deploy different Foundation models we can have a single Foundation model and attach it with a different set of Laura",
    "start": "980800",
    "end": "987160"
  },
  {
    "text": "adapters uh for different Tas TKS um the advantage uh is Lowa low",
    "start": "987160",
    "end": "993440"
  },
  {
    "text": "adapters are very easy to build they don't take too much computational they don't need too much computational um",
    "start": "993440",
    "end": "998800"
  },
  {
    "text": "Power to to generate um they are uh built using uh fine tuned using very um",
    "start": "998800",
    "end": "1005680"
  },
  {
    "text": "fractional number of parameters um of the of the foundation model and the original models of the found original",
    "start": "1005680",
    "end": "1012360"
  },
  {
    "text": "weights of the foundation model are untouched uh once we have the the lower adapters uh the key Advantage is is",
    "start": "1012360",
    "end": "1019199"
  },
  {
    "text": "efficient utilization of the GPU because we we are sharing the same Foundation model um but based on the dynamic use",
    "start": "1019199",
    "end": "1026199"
  },
  {
    "text": "cases that the that the customers have you can keep loading and unloading these low adapters from the GPU memory so in",
    "start": "1026199",
    "end": "1032438"
  },
  {
    "text": "this case we have three different low adapters um one lower adapter which is fine- tuned uh for customer support um",
    "start": "1032439",
    "end": "1039678"
  },
  {
    "text": "related data set and also we have another low adapter which is um fine tuned for code generation and third uh",
    "start": "1039679",
    "end": "1046678"
  },
  {
    "text": "lower adapter which is fine tuned for math query uh data math query data set",
    "start": "1046679",
    "end": "1052280"
  },
  {
    "text": "um and when the when all these requests come in they specify adapter ID which is nothing but the Lura adapter ID and all",
    "start": "1052280",
    "end": "1059320"
  },
  {
    "text": "these requests will batch batch together and concurrently processed within a GPU",
    "start": "1059320",
    "end": "1065240"
  },
  {
    "text": "um the advantage is um so we get faster response and also in parallel um it is",
    "start": "1065240",
    "end": "1072480"
  },
  {
    "text": "Computing different tasks at the same time uh in this case if I issue a math",
    "start": "1072480",
    "end": "1078120"
  },
  {
    "text": "query adapter if it is not loaded in the GPU memory um it will be dynamically loaded into the GPU memory um and",
    "start": "1078120",
    "end": "1085600"
  },
  {
    "text": "generate a response for that particular request let's see a quick demo of using",
    "start": "1085600",
    "end": "1091240"
  },
  {
    "text": "low adapters with Nvidia",
    "start": "1091240",
    "end": "1094960"
  },
  {
    "text": "NS in this case um I have um I have uh Nvidia Nvidia Nim operator installed and",
    "start": "1105360",
    "end": "1112480"
  },
  {
    "text": "also I have a a PVC or Nim cach created with a base model so what I'm going to do is um I'm going to launch uh a sample",
    "start": "1112480",
    "end": "1121039"
  },
  {
    "text": "job to pull these low adapters from NGC into the local PVC um NGC have have some",
    "start": "1121039",
    "end": "1127159"
  },
  {
    "text": "pre-built L adapters which are fine tuned for math related queries so I'm going to launch a standalone job to pull",
    "start": "1127159",
    "end": "1132440"
  },
  {
    "text": "those things into into a PVC um so as you you can see um I'm I'm",
    "start": "1132440",
    "end": "1139880"
  },
  {
    "text": "using NGC CLI um container here which is basically pulling a predefined set of",
    "start": "1139880",
    "end": "1145480"
  },
  {
    "text": "low adapters from from NGC into into a local PVC so the path in the PVC is called as",
    "start": "1145480",
    "end": "1151640"
  },
  {
    "text": "a model store and loras that's where I'm downloading uh these PVCs and once the downloading is",
    "start": "1151640",
    "end": "1157960"
  },
  {
    "text": "complete um now we can go ahead and deploy uh a n service instance or a inferencing service",
    "start": "1157960",
    "end": "1165120"
  },
  {
    "text": "instance uh within the name service instance um I specify the same n cach so the same PVC get used uh but I also need",
    "start": "1165520",
    "end": "1173000"
  },
  {
    "text": "to specify uh the source path where these loras are um are staged locally um",
    "start": "1173000",
    "end": "1178240"
  },
  {
    "text": "and also I I have to specify the refresh interval at which uh the ining service can load these adapters if there is no",
    "start": "1178240",
    "end": "1184440"
  },
  {
    "text": "request coming in um so once I have the the Lowa",
    "start": "1184440",
    "end": "1191039"
  },
  {
    "text": "service up and running yeah let's create this new",
    "start": "1191039",
    "end": "1196799"
  },
  {
    "text": "service and once the the name service is up and running we can try to issue some um some",
    "start": "1196799",
    "end": "1203760"
  },
  {
    "text": "queries we can list the models and also we can see the difference uh with queries to the foundation model as well",
    "start": "1203760",
    "end": "1210159"
  },
  {
    "text": "as um to the to a particular load adapter um in the infesting surveys we",
    "start": "1210159",
    "end": "1216520"
  },
  {
    "text": "can see the these adapters are cached",
    "start": "1216520",
    "end": "1221480"
  },
  {
    "text": "um and now I'm going to list all the models that are serviced by this uh Nim",
    "start": "1223840",
    "end": "1229200"
  },
  {
    "text": "service um and we can see that uh now the the Nim container is not only",
    "start": "1229200",
    "end": "1234440"
  },
  {
    "text": "servicing the base model but also it is servicing uh the math low adapter and",
    "start": "1234440",
    "end": "1239640"
  },
  {
    "text": "the squad low adapter that have attached we take this adapter ID and now",
    "start": "1239640",
    "end": "1245440"
  },
  {
    "text": "for each of these adap IDs we can issue inference in queries",
    "start": "1245440",
    "end": "1251280"
  },
  {
    "text": "um so let's take a sample a math query uh where we have a a basic um very",
    "start": "1257600",
    "end": "1263240"
  },
  {
    "text": "simple math query um and issue to a foundation",
    "start": "1263240",
    "end": "1268720"
  },
  {
    "text": "model and once we issue to the foundation model we can see that the response is accurate but it is it is",
    "start": "1277400",
    "end": "1284080"
  },
  {
    "text": "very verbos and also it gives additional reasoning",
    "start": "1284080",
    "end": "1289080"
  },
  {
    "text": "it indeed gets the answer right but it it gives additional reasoning and and quite coros now if you issue the same",
    "start": "1292799",
    "end": "1298720"
  },
  {
    "text": "query um uh using an an an low adapter which is fine-tuned with a math data set",
    "start": "1298720",
    "end": "1305679"
  },
  {
    "text": "uh we can see the answer is quite precise and also it's it's very",
    "start": "1305679",
    "end": "1311120"
  },
  {
    "text": "crisp so I'm using in this case I'm using the the model ID as the the Lowa adapter that we have staged and we can",
    "start": "1311120",
    "end": "1317720"
  },
  {
    "text": "see uh um this for the same",
    "start": "1317720",
    "end": "1321520"
  },
  {
    "text": "query so the answer is very precise um and much more accurate so yeah this",
    "start": "1328440",
    "end": "1333919"
  },
  {
    "text": "showcases that um um Nvidia Nim um easily support this multia serving which",
    "start": "1333919",
    "end": "1340240"
  },
  {
    "text": "is which is very useful in case of you have diers set of customers who are asking um for different use cases you",
    "start": "1340240",
    "end": "1346960"
  },
  {
    "text": "can support um without without having to fine tune without having to um fine tune",
    "start": "1346960",
    "end": "1352840"
  },
  {
    "text": "the the model you can generate uh lower adapters and and dynamically and rapidly deploy these in a kuus",
    "start": "1352840",
    "end": "1361120"
  },
  {
    "text": "cluster",
    "start": "1361840",
    "end": "1364840"
  },
  {
    "text": "that so let's talk about AI pipelines um so llms are not uh deployed um uh",
    "start": "1368000",
    "end": "1375600"
  },
  {
    "text": "independently anymore so most of the time all these models are deployed in some sort of a pipeline for example if",
    "start": "1375600",
    "end": "1381400"
  },
  {
    "text": "you take rag as an example uh for rag we have to have an embedding model um for",
    "start": "1381400",
    "end": "1387039"
  },
  {
    "text": "for injesting and generating vectors uh Vector embeddings for all all user data",
    "start": "1387039",
    "end": "1392320"
  },
  {
    "text": "and also you need to have a reranking model uh for enabling semantic search and all these U these two models also",
    "start": "1392320",
    "end": "1399080"
  },
  {
    "text": "have to be deployed along with the uh the base llm model or a fine tuned llm model in order to deploy all these",
    "start": "1399080",
    "end": "1405880"
  },
  {
    "text": "things together we need to choose tools which are very intuitive and also will help us manage not only deploying but",
    "start": "1405880",
    "end": "1412559"
  },
  {
    "text": "also life cycle day to life cycle management which which includes Auto scaling upgrades so all those things",
    "start": "1412559",
    "end": "1419240"
  },
  {
    "text": "have to be very easily manageable through a single interface and also these um um these",
    "start": "1419240",
    "end": "1427039"
  },
  {
    "text": "tools are connected together using a framework some sort of Frameworks and we also have some third- party dependencies",
    "start": "1427039",
    "end": "1432919"
  },
  {
    "text": "so we need to also have a easy way to connect all these dependencies together using using a common interface",
    "start": "1432919",
    "end": "1440440"
  },
  {
    "text": "um let's look at an example of deploying a sample pipeline rag pipeline using using the Nim",
    "start": "1441679",
    "end": "1447880"
  },
  {
    "text": "operator so in this case I have a Nim operator installed um and I'm going to",
    "start": "1447880",
    "end": "1453120"
  },
  {
    "text": "bring up the three uh models that I talked about embedding model re ranking model and Lama 8B model llama 38b model",
    "start": "1453120",
    "end": "1461480"
  },
  {
    "text": "and I also have um to execute some some some queries I also have a sample",
    "start": "1461480",
    "end": "1466679"
  },
  {
    "text": "playground that we built uh for executing gr queries and also we have um",
    "start": "1466679",
    "end": "1472640"
  },
  {
    "text": "a lang chain um Lang Lang chain instance running uh to orchestrate uh the",
    "start": "1472640",
    "end": "1479080"
  },
  {
    "text": "query so what what we are doing is we are creating a n pipeline instance U which is again another crd that is",
    "start": "1481480",
    "end": "1487480"
  },
  {
    "text": "installed by the Nim operator uh which will allow deployment of multiple Nim Services together um as one instance and",
    "start": "1487480",
    "end": "1494760"
  },
  {
    "text": "these Services also can connect to other dependencies we also have service dependency injected into the pipeline",
    "start": "1494760",
    "end": "1500399"
  },
  {
    "text": "where we automatically configure um with the deployments that we create as soon as we create an N",
    "start": "1500399",
    "end": "1506399"
  },
  {
    "text": "pipeline instance we can see that um the deployments and N Services uh for the",
    "start": "1506399",
    "end": "1511480"
  },
  {
    "text": "corresponding models have been created and all of them are in running State and now we see um the services for",
    "start": "1511480",
    "end": "1520679"
  },
  {
    "text": "the the corresponding deployments have been created as well um it creates the HPA Incas anything that can be",
    "start": "1520679",
    "end": "1526120"
  },
  {
    "text": "configured to a im service can be uh deployed uh",
    "start": "1526120",
    "end": "1531399"
  },
  {
    "text": "here so I'm uploading uh a custom uh data set um into uh into the rag system",
    "start": "1533559",
    "end": "1541120"
  },
  {
    "text": "so we have um Nvidia gb200 uh data set um ined by this rack pipeline so before",
    "start": "1541120",
    "end": "1548159"
  },
  {
    "text": "using the knowledge base we issued a sample query to the r rack pipeline that is running we asked the system how many",
    "start": "1548159",
    "end": "1555000"
  },
  {
    "text": "CPUs and gpus are connected in a gp200 system uh this data is quite recent and",
    "start": "1555000",
    "end": "1560200"
  },
  {
    "text": "llm is not trained on this data and without using a custom um the data sheet that we have uploaded um it completely",
    "start": "1560200",
    "end": "1567279"
  },
  {
    "text": "hallucinates and makes up something some response it says it's using Intel Intel CPUs and also it's using V100 gpus now",
    "start": "1567279",
    "end": "1575240"
  },
  {
    "text": "with the same thing um if you use a knowledge database that we have um",
    "start": "1575240",
    "end": "1580320"
  },
  {
    "text": "ingested using Rag and issue a same query now we see that um",
    "start": "1580320",
    "end": "1588399"
  },
  {
    "text": "it accurately says that gb200 is made up of 36 Nvidia gray CPUs and 72 nvdia",
    "start": "1591080",
    "end": "1597039"
  },
  {
    "text": "black whe gpus and they're connected together using NV",
    "start": "1597039",
    "end": "1601519"
  },
  {
    "text": "link so that's how easy to deploy um rack pipelines uh through through a single",
    "start": "1603320",
    "end": "1609799"
  },
  {
    "text": "interface so what are the next things um that we are exploring um in terms of",
    "start": "1609799",
    "end": "1615520"
  },
  {
    "text": "deploying inferencing Services um so one one thing that uh that comes across is multi node inference or distributed",
    "start": "1615520",
    "end": "1621799"
  },
  {
    "text": "inference um this is a use case where models are quite large and cannot be fit",
    "start": "1621799",
    "end": "1627440"
  },
  {
    "text": "into a single GPU memory so this is where uh we have to use techniques called as a model sharding um where",
    "start": "1627440",
    "end": "1634279"
  },
  {
    "text": "indidual models uh have to be shattered across different gpus and also different",
    "start": "1634279",
    "end": "1639919"
  },
  {
    "text": "uh nodes um in some cases um I'll go through different sh sharding mechanisms um in a second uh",
    "start": "1639919",
    "end": "1647159"
  },
  {
    "text": "but the key use cases of multi node inferencing is again to make sure that we we able to deploy these massive GPU",
    "start": "1647159",
    "end": "1653600"
  },
  {
    "text": "massive models into a GPU cluster and also to be able to manage them as a single entity you should be able to",
    "start": "1653600",
    "end": "1659679"
  },
  {
    "text": "autoscale uh them as a single entity and also load balance um easily across all",
    "start": "1659679",
    "end": "1664880"
  },
  {
    "text": "these uh entrancing services that are running so we have two uh popular uh",
    "start": "1664880",
    "end": "1671399"
  },
  {
    "text": "model sharing uh techniques that are used um one is called as a pipeline",
    "start": "1671399",
    "end": "1676760"
  },
  {
    "text": "parallelism where uh a model is is uh is split into different segments where each",
    "start": "1676760",
    "end": "1683399"
  },
  {
    "text": "segment is consisting of different layers of the model and each of the segment will be loaded into different",
    "start": "1683399",
    "end": "1690399"
  },
  {
    "text": "GPU devices on the left I have three layers of the model loaded into GPU 0",
    "start": "1690399",
    "end": "1696240"
  },
  {
    "text": "and also the remaining three layers of the model are loaded into GPU 1 in this case um uh computations are",
    "start": "1696240",
    "end": "1704559"
  },
  {
    "text": "mostly sequential so the computations have to be complete on on on segment zero and once the computations are are",
    "start": "1704559",
    "end": "1711679"
  },
  {
    "text": "complete from segment zero then they'll be fed into the next U next segment in the",
    "start": "1711679",
    "end": "1717240"
  },
  {
    "text": "pipeline um internally they use for for communication between these segments they use nickel uh for um which is a",
    "start": "1717240",
    "end": "1724760"
  },
  {
    "text": "Nvidia Collective Communications library and they use commands like nickel send",
    "start": "1724760",
    "end": "1730640"
  },
  {
    "text": "um nickel receive uh commands in this case and other popular Shing um",
    "start": "1730640",
    "end": "1736240"
  },
  {
    "text": "technique is also called as tensor parallelism where uh computations within a layer is",
    "start": "1736240",
    "end": "1741919"
  },
  {
    "text": "shattered across two different multiple or two or multiple gpus for this example I'm taking tensil",
    "start": "1741919",
    "end": "1748360"
  },
  {
    "text": "parallelism as two where each layer is shattered across two gpus for example if",
    "start": "1748360",
    "end": "1754519"
  },
  {
    "text": "there is a complex uh dense matrix multiplication computation uh it is going to break break up the",
    "start": "1754519",
    "end": "1760640"
  },
  {
    "text": "multiplication uh computation into a smaller computations and each of those smaller computations are shared across",
    "start": "1760640",
    "end": "1767480"
  },
  {
    "text": "two different uh gpus within the same layer and once the computation the smaller computation is complete it uses",
    "start": "1767480",
    "end": "1774519"
  },
  {
    "text": "nickel um all reduce uh reduce scatter commands uh to again combine uh the",
    "start": "1774519",
    "end": "1781519"
  },
  {
    "text": "result of the computation and send it to the next layer the communication between the layers is again sequential there is a depend sequential dependency between",
    "start": "1781519",
    "end": "1788000"
  },
  {
    "text": "the layers but the smaller subtasks or smaller computations between each layer is done in",
    "start": "1788000",
    "end": "1794000"
  },
  {
    "text": "parallel so for M multi node inferencing we use both both of these techniques to",
    "start": "1794000",
    "end": "1799640"
  },
  {
    "text": "kind of partition U model and also make sure that they can fit into the gpus that you have in the",
    "start": "1799640",
    "end": "1806600"
  },
  {
    "text": "cluster so what what are the key challenges um when when using multi node",
    "start": "1806600",
    "end": "1811720"
  },
  {
    "text": "or distributed inferencing so one thing is all of these uh have to be scheduled",
    "start": "1811720",
    "end": "1817080"
  },
  {
    "text": "as a as a single entity because even if some parts cannot run um we cannot service inferencing requests so all of",
    "start": "1817080",
    "end": "1824480"
  },
  {
    "text": "these have to be scheduled as a gang and and also we need to have efficient windin packing um um to be done in the",
    "start": "1824480",
    "end": "1830919"
  },
  {
    "text": "cluster we need to make sure all the gpus are efficiently used um and and make sure that the the inferencing um",
    "start": "1830919",
    "end": "1837880"
  },
  {
    "text": "all the all the parts of the inferencing service are able to run and from operations perspective uh",
    "start": "1837880",
    "end": "1844799"
  },
  {
    "text": "we should be able to Auto scale as if like a single entity all of the replicas um of of this particular multi",
    "start": "1844799",
    "end": "1851679"
  },
  {
    "text": "distributed uh instance have to be um autoscaled across um different set of nodes",
    "start": "1851679",
    "end": "1858960"
  },
  {
    "text": "and um because we have shattered across different gpus the communication uh the",
    "start": "1858960",
    "end": "1864159"
  },
  {
    "text": "message passing is important across all these gpus and for NVIDIA we use MPI there is also Ray which is used for",
    "start": "1864159",
    "end": "1870320"
  },
  {
    "text": "cross um um cross GPU communication um um but we use",
    "start": "1870320",
    "end": "1877279"
  },
  {
    "text": "MPI um and also for auto scaling uh we need leader aware um sorry for load",
    "start": "1877279",
    "end": "1882760"
  },
  {
    "text": "balancing we need leader aware load balancing um and and for optimizations um one of",
    "start": "1882760",
    "end": "1890639"
  },
  {
    "text": "the key optimizations that we um that we need for multi node multi multi node",
    "start": "1890639",
    "end": "1895919"
  },
  {
    "text": "inferencing is to make sure that whenever we Auto scale whenever these new instances are coming up um in in in",
    "start": "1895919",
    "end": "1902600"
  },
  {
    "text": "different nodes the models are readily available on those nodes because if the models are not available it's going to",
    "start": "1902600",
    "end": "1908000"
  },
  {
    "text": "dynamically pull those models on each of these nodes and that will take uh quite a bit of time so we recommend using some",
    "start": "1908000",
    "end": "1915200"
  },
  {
    "text": "sort of shared storage like minaki was saying we can use NFS SEF um any kind of",
    "start": "1915200",
    "end": "1920360"
  },
  {
    "text": "distributed storage NV mesh um where uh the volume can be easily uh attached to",
    "start": "1920360",
    "end": "1926440"
  },
  {
    "text": "different instances and then model caching models are readily available across all these nodes and also for high",
    "start": "1926440",
    "end": "1933320"
  },
  {
    "text": "throughput we need to have um uh connectivity between all these nodes um like Infinity band Rocky so",
    "start": "1933320",
    "end": "1939559"
  },
  {
    "text": "configuration of this is very important across nodes uh for multide",
    "start": "1939559",
    "end": "1945080"
  },
  {
    "text": "entrancing lastly um I'll talk briefly about e agents um so e",
    "start": "1945760",
    "end": "1951519"
  },
  {
    "text": "agents are becoming quite popular for advanced problem solving um they are the next next V of generative AI um what a",
    "start": "1951519",
    "end": "1959120"
  },
  {
    "text": "agents uh basically do is they take the complex problem and apply reasoning and break the the complex problem into",
    "start": "1959120",
    "end": "1965279"
  },
  {
    "text": "smaller tasks and when they break them into smaller tasks they take advantage",
    "start": "1965279",
    "end": "1970399"
  },
  {
    "text": "of external tools uh to solve those independent tasks uh it can be a weather application",
    "start": "1970399",
    "end": "1976559"
  },
  {
    "text": "it can be a web search um so it will reach out to all these external tools to provide meaningful um responses to the",
    "start": "1976559",
    "end": "1983320"
  },
  {
    "text": "user there's also a short-term memory and longterm memory where it can generate responses which are personal to",
    "start": "1983320",
    "end": "1989960"
  },
  {
    "text": "the user and which are very contextual uh for the for the query in terms of a agents the",
    "start": "1989960",
    "end": "1996480"
  },
  {
    "text": "challenges we have is again in terms of standard there is no standard as such to deploy a agents into kubernetes cluster",
    "start": "1996480",
    "end": "2003600"
  },
  {
    "text": "Nvidia have agentic blueprints um we have multi-turn drag agentic Blueprints and virtual screening um blueprints that",
    "start": "2003600",
    "end": "2010720"
  },
  {
    "text": "have been published um but we are looking to standardize the deployment of uh blueprints or E agent or pipelines",
    "start": "2010720",
    "end": "2017679"
  },
  {
    "text": "for E agents with that um I'll conclude uh by",
    "start": "2017679",
    "end": "2024120"
  },
  {
    "text": "saying that kuber is a great great platform for AI pipelines uh we have very strong uh GPU and storage",
    "start": "2024120",
    "end": "2030639"
  },
  {
    "text": "Integrations in kuties uh now with Dr we can do a lot of advanced GPU scheduling",
    "start": "2030639",
    "end": "2035840"
  },
  {
    "text": "um uh in communities that we looking forward to integrate those kind of changes into into the N operator um and",
    "start": "2035840",
    "end": "2042240"
  },
  {
    "text": "from Storage side we also have like well established CSI drivers in place uh where we can do all sorts of model",
    "start": "2042240",
    "end": "2047679"
  },
  {
    "text": "caching um connect different uh kind of storage backends um into into the",
    "start": "2047679",
    "end": "2053440"
  },
  {
    "text": "nodes and also we have advanced inferencing and uh fine-tuning uh platforms we have Cube flow we have K",
    "start": "2053440",
    "end": "2059760"
  },
  {
    "text": "sub for advanced use cases um that are that are quite easy to",
    "start": "2059760",
    "end": "2064919"
  },
  {
    "text": "use um and and and commun is working on addressing some gaps so we are kind of closely monitoring those um in terms of",
    "start": "2064919",
    "end": "2072280"
  },
  {
    "text": "providing Auto scaling what kind of metrics to use for auto scaling um and also model cache management which like",
    "start": "2072280",
    "end": "2079040"
  },
  {
    "text": "can we use OCA artifacts can we um pull them into like S3 storage or NFS PVC so",
    "start": "2079040",
    "end": "2085398"
  },
  {
    "text": "so we are also closely watching those things and finally llm Gateway um is",
    "start": "2085399",
    "end": "2091000"
  },
  {
    "text": "also becoming quite popular so we're also looking to see how we can integrate this with uh with the service that we",
    "start": "2091000",
    "end": "2096560"
  },
  {
    "text": "have with that um I'll conclude the talk thank you and and yeah please provide",
    "start": "2096560",
    "end": "2102200"
  },
  {
    "text": "the feedback and I'll take any questions [Applause]",
    "start": "2102200",
    "end": "2113239"
  }
]