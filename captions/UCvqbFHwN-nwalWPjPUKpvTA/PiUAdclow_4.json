[
  {
    "text": "hello everyone thanks for joining the session uh today we're going to talk about gerle which is a unified scheduler",
    "start": "1560",
    "end": "8519"
  },
  {
    "text": "for online and offline workload uh yeah I'm Lon this is my teammate y uh we are",
    "start": "8519",
    "end": "14599"
  },
  {
    "text": "from scheduling and orchestration team from B Dan uh gero actually it's a",
    "start": "14599",
    "end": "20920"
  },
  {
    "text": "alternative uh scheduler in kubernetes cluster uh it was designed for resolve",
    "start": "20920",
    "end": "26840"
  },
  {
    "text": "our major uh major pain points uh inhouse by dance and it was open source",
    "start": "26840",
    "end": "33320"
  },
  {
    "text": "late 20 23 uh here's a agenda for today we're",
    "start": "33320",
    "end": "39640"
  },
  {
    "text": "going to break it down into two part uh four parts uh first uh firstly we're",
    "start": "39640",
    "end": "45000"
  },
  {
    "text": "going to take a high level review of girdle and uh including some backgrounds and the high level uh architecture after",
    "start": "45000",
    "end": "52960"
  },
  {
    "text": "that we're going to work through some key features we introduce in girle and",
    "start": "52960",
    "end": "58239"
  },
  {
    "text": "along with some uh fantastic demo uh after that we are going to cover some of",
    "start": "58239",
    "end": "63280"
  },
  {
    "text": "the achievement we have made so far uh in both uh production and the Academia",
    "start": "63280",
    "end": "69880"
  },
  {
    "text": "uh to wrap up this session we're going to uh cover some of the related uh",
    "start": "69880",
    "end": "75000"
  },
  {
    "text": "future work sitting on our road map okay yeah let's take a look at the",
    "start": "75000",
    "end": "80960"
  },
  {
    "text": "overview first uh somebody might already have the",
    "start": "80960",
    "end": "86200"
  },
  {
    "text": "question in your mind and why do we need another scheduler since there are so many option available in the community",
    "start": "86200",
    "end": "93560"
  },
  {
    "text": "community and why don't we just use that so yeah as I said we we designed the",
    "start": "93560",
    "end": "99799"
  },
  {
    "text": "schedule to resolve our major point we uh faced with in the uh in bance so uh",
    "start": "99799",
    "end": "107040"
  },
  {
    "text": "first thing first uh it is a uh infrastructure scale uh yeah let's take a look at the",
    "start": "107040",
    "end": "113200"
  },
  {
    "text": "number together so within bance there are more than uh 500 uh large scale",
    "start": "113200",
    "end": "120240"
  },
  {
    "text": "cluster of the world uh uh regarding about large CL large scale cluster we",
    "start": "120240",
    "end": "126680"
  },
  {
    "text": "mean uh cluster with more than a thousand uh uh nodes and uh in the",
    "start": "126680",
    "end": "133080"
  },
  {
    "text": "largest cluster there are there are around like 20K heterogeneous server and",
    "start": "133080",
    "end": "139720"
  },
  {
    "text": "around 1 million pods so yeah I believe that's huge and uh from another",
    "start": "139720",
    "end": "146760"
  },
  {
    "text": "perspective uh over 200 uh million of containerized application including both",
    "start": "146760",
    "end": "153440"
  },
  {
    "text": "online and offline application uh ring in within our Global infrastructure",
    "start": "153440",
    "end": "159200"
  },
  {
    "text": "every day okay here's another reason um we",
    "start": "159200",
    "end": "165239"
  },
  {
    "text": "have heterogenous workload from different business group and uh different types of uh workload have",
    "start": "165239",
    "end": "172040"
  },
  {
    "text": "different performance metrics so uh specifically for uh uh offline",
    "start": "172040",
    "end": "178720"
  },
  {
    "text": "applications such as merine learning training badge jobs uh focus more on uh",
    "start": "178720",
    "end": "184319"
  },
  {
    "text": "minimizing the completion time uh while for uh others like uh data processing",
    "start": "184319",
    "end": "191239"
  },
  {
    "text": "video coding and streaming uh emphasize more on uh cput and uh on the other hand like on",
    "start": "191239",
    "end": "199560"
  },
  {
    "text": "online application uh uh something like microservices inference and the",
    "start": "199560",
    "end": "206360"
  },
  {
    "text": "recommendation system are all like uh latency critical and uh yeah that give us",
    "start": "206360",
    "end": "214040"
  },
  {
    "text": "different requirement on scheduling such kind of workload and uh apart from that",
    "start": "214040",
    "end": "220599"
  },
  {
    "text": "and such kind uh we also have some Advanced uh like resource requirement to",
    "start": "220599",
    "end": "227599"
  },
  {
    "text": "achieve better uh performance uh such as like topology Affinity so as way uh um",
    "start": "227599",
    "end": "236680"
  },
  {
    "text": "familiar like for uh training um machine learning jobs uh we have the requirement",
    "start": "236680",
    "end": "243239"
  },
  {
    "text": "of affinity to GPU while for uh machine learning inference we might be uh we",
    "start": "243239",
    "end": "249599"
  },
  {
    "text": "might want our workload to be Numa aware to get uh like faster or higher CP",
    "start": "249599",
    "end": "255959"
  },
  {
    "text": "frequency and faster memory access so uh here's another reason last but not",
    "start": "255959",
    "end": "263560"
  },
  {
    "text": "the least um so before gero uh we have isolated computer infrastructure",
    "start": "263560",
    "end": "270440"
  },
  {
    "text": "uh I believe that's uh the same for many companies and uh yeah we had we Rong",
    "start": "270440",
    "end": "276000"
  },
  {
    "text": "like microservices on kubernetes and uh data intensive application on yarn so it",
    "start": "276000",
    "end": "284440"
  },
  {
    "text": "creates split brain in terms of resource management uh it's not uh it it makes",
    "start": "284440",
    "end": "290520"
  },
  {
    "text": "this uh like resource utilization Improvement uh like cost of optimization",
    "start": "290520",
    "end": "297400"
  },
  {
    "text": "very hard and uh it end up with the issue uh AKA is the major uh like uh",
    "start": "297400",
    "end": "305120"
  },
  {
    "text": "pain points like uh resource fragmentation uh and the low resource",
    "start": "305120",
    "end": "311039"
  },
  {
    "text": "utilization and the poor uh resource",
    "start": "311039",
    "end": "316039"
  },
  {
    "text": "elasticity and uh yeah girle is our solution to resolve all such kind of",
    "start": "316240",
    "end": "322160"
  },
  {
    "text": "issue we identify and here's a high level architecture we have uh in gerle",
    "start": "322160",
    "end": "329400"
  },
  {
    "text": "and compared to the monolitic uh architecture of valina KU scheduler we",
    "start": "329400",
    "end": "337160"
  },
  {
    "text": "designed a girle to be a distributed scheduler with shared cluster State um",
    "start": "337160",
    "end": "344240"
  },
  {
    "text": "we partition the scheduling FR framework into three parts uh uh first of all we",
    "start": "344240",
    "end": "351800"
  },
  {
    "text": "have dispatcher to handling all the prescheduling stuff uh such as queing",
    "start": "351800",
    "end": "357039"
  },
  {
    "text": "sorting and admission and moving along the scheduling path uh we have schedule",
    "start": "357039",
    "end": "363080"
  },
  {
    "text": "to make this uh potential scheduling proposal and uh uh after that at this",
    "start": "363080",
    "end": "370280"
  },
  {
    "text": "layer it's also the most heavy lifting part of the scheduling because remember we are dealing with like 20K nodes and",
    "start": "370280",
    "end": "378880"
  },
  {
    "text": "the 1 million pod uh yeah this is huge and we need to make it efficient so",
    "start": "378880",
    "end": "385039"
  },
  {
    "text": "that's why we uh trying to design a like uh uh scale out uh design as this layer",
    "start": "385039",
    "end": "393440"
  },
  {
    "text": "to make the scheduling especially the sched uh to",
    "start": "393440",
    "end": "398840"
  },
  {
    "text": "dealing with its task very efficiently and other the end we have",
    "start": "398840",
    "end": "405120"
  },
  {
    "text": "binder to resolve this uh conflicts and also employ the optimistic concurrency",
    "start": "405120",
    "end": "412919"
  },
  {
    "text": "to make sure this uh conflict resolution to be handled efficiently and uh PA from",
    "start": "412919",
    "end": "420120"
  },
  {
    "text": "that to make sure we can like uh scheduling this AI workloads much better",
    "start": "420120",
    "end": "427199"
  },
  {
    "text": "we also introduce like two layer scheduling uh abstraction what is uh two layer",
    "start": "427199",
    "end": "433840"
  },
  {
    "text": "scheduling abstraction So within gerle we trying to uh abstract the concept of",
    "start": "433840",
    "end": "439960"
  },
  {
    "text": "scheduling scheduling unit so instead of just scheduling part with uh trying to",
    "start": "439960",
    "end": "446680"
  },
  {
    "text": "scheduling uh the scheduling unit the a schedule unit could be either a part",
    "start": "446680",
    "end": "452000"
  },
  {
    "text": "group or a pod so Part Group is for like game scheduling and part for regular",
    "start": "452000",
    "end": "457759"
  },
  {
    "text": "scheduling and yes there could be much more uh cases uh but yeah we'll revisit",
    "start": "457759",
    "end": "464039"
  },
  {
    "text": "once the requirement coming in and uh yeah in terms of this two layer",
    "start": "464039",
    "end": "469319"
  },
  {
    "text": "scheduling framework uh yeah we have unit level framework and P level",
    "start": "469319",
    "end": "475599"
  },
  {
    "text": "framework and uh yeah so for the unit level framework we trying to in uh so",
    "start": "475599",
    "end": "482720"
  },
  {
    "text": "taking this like machine learning training for example so because in this",
    "start": "482720",
    "end": "487879"
  },
  {
    "text": "such kind of case we need to think about this topology Affinity so that means we need to be topology Affinity aware and",
    "start": "487879",
    "end": "496000"
  },
  {
    "text": "yeah yeah will share some the key feature we have to resolve this issue but I will quickly uh share some",
    "start": "496000",
    "end": "503080"
  },
  {
    "text": "highlights here so within the unit level both unit level framework can P level",
    "start": "503080",
    "end": "510000"
  },
  {
    "text": "framework we make it pluggable and uh the Pod level framework is pretty",
    "start": "510000",
    "end": "516518"
  },
  {
    "text": "similar to the uh Val valina uh the scheduling framework within valina coup",
    "start": "516519",
    "end": "522440"
  },
  {
    "text": "scheduler but uh the unil level framework is pretty much new uh new thing so we have two major plugin the",
    "start": "522440",
    "end": "530519"
  },
  {
    "text": "first thing is a locating plugin and the second one is a grouping plugin so the",
    "start": "530519",
    "end": "535720"
  },
  {
    "text": "locating plugin is um focusing on da de with this hard requirement in specifying",
    "start": "535720",
    "end": "542480"
  },
  {
    "text": "our topology affinity and uh the grouping plugin will uh consider the",
    "start": "542480",
    "end": "550519"
  },
  {
    "text": "preference we specify as a soft constraint in our topology Affinity so",
    "start": "550519",
    "end": "556800"
  },
  {
    "text": "that yeah so think about a a merching training job uh if we are scheduling it",
    "start": "556800",
    "end": "563079"
  },
  {
    "text": "when will consider the network latency could be within the topology and we",
    "start": "563079",
    "end": "569560"
  },
  {
    "text": "trying to make sure uh the the Gan of Po will have the lowest uh uh Network",
    "start": "569560",
    "end": "579760"
  },
  {
    "text": "latency so you guys must be very curious how did we do in terms of the uh with uh",
    "start": "579760",
    "end": "586279"
  },
  {
    "text": "within the hypers scale infrastructure here's some numbers uh we have and uh",
    "start": "586279",
    "end": "592399"
  },
  {
    "text": "yeah we have we are within the service level agreement at the scale of uh up to",
    "start": "592399",
    "end": "599959"
  },
  {
    "text": "20K notes and 1 million P we are targeting on uh achieve this uh schedule",
    "start": "599959",
    "end": "606800"
  },
  {
    "text": "latency uh P99 to be less than 1 minute and scheduling P up to uh like 1,000 PS",
    "start": "606800",
    "end": "615839"
  },
  {
    "text": "per second it's not the uh one we can be higher but yeah this is to consider the",
    "start": "615839",
    "end": "622560"
  },
  {
    "text": "C stability of our cluster the P creation rate is tapped at 1,000 so yeah",
    "start": "622560",
    "end": "630360"
  },
  {
    "text": "that's why we have this kind of number okay so so much for the G",
    "start": "630360",
    "end": "636720"
  },
  {
    "text": "overview and yeah will'll take it up here and share some key features and we'll show you guys some fantastic",
    "start": "636720",
    "end": "645079"
  },
  {
    "text": "demo uh thank you Linton so uh yeah so here are a few uh key features that we",
    "start": "645079",
    "end": "650839"
  },
  {
    "text": "want to highlight in this talk uh so first uh let's look at Job level Affinity which uh Linton mentioned uh",
    "start": "650839",
    "end": "657959"
  },
  {
    "text": "also in his introdu setion uh so job level Affinity is an advanced feature",
    "start": "657959",
    "end": "663320"
  },
  {
    "text": "which was built up upon gun scheduling uh so gun scheduling is able to achieve an All or Nothing syntax uh through a",
    "start": "663320",
    "end": "670839"
  },
  {
    "text": "crd called pod group uh I believe some of you are already familiar with the uh concept of gun scheduling or batch",
    "start": "670839",
    "end": "677480"
  },
  {
    "text": "scheduling uh so in our part group uh we Define this m member so that means uh",
    "start": "677480",
    "end": "683200"
  },
  {
    "text": "the uh only if a part uh greater than me member can be scheduled at the same time",
    "start": "683200",
    "end": "689040"
  },
  {
    "text": "then they can be scheduled in this scheduling cycle otherwise uh all of them will remain in pending state so job",
    "start": "689040",
    "end": "695600"
  },
  {
    "text": "level Affinity uh is uh extended Beyond gun scheduling by pre uh providing with",
    "start": "695600",
    "end": "703240"
  },
  {
    "text": "uh preferred and required Affinity terms uh so L also mentioned so preferred is",
    "start": "703240",
    "end": "708639"
  },
  {
    "text": "like a soft constraint uh which can help you divide your note groups based on uh network topologies but still allows uh",
    "start": "708639",
    "end": "716360"
  },
  {
    "text": "scheduling flexibility across the topology means and a required is a hard",
    "start": "716360",
    "end": "721720"
  },
  {
    "text": "constraint uh which means the group of your parts must be scheduled within the defined topology key so that they can",
    "start": "721720",
    "end": "728399"
  },
  {
    "text": "all be scheduled and uh this feature is very crucial for some workflows such as",
    "start": "728399",
    "end": "733720"
  },
  {
    "text": "large scale machine learning jobs uh because uh for ML job uh they will",
    "start": "733720",
    "end": "739160"
  },
  {
    "text": "require that all of their PS in the same job scheduled to uh the same say Network",
    "start": "739160",
    "end": "744920"
  },
  {
    "text": "topology domain so that they can minimize the uh communication over head across different network",
    "start": "744920",
    "end": "751519"
  },
  {
    "text": "topologies and specifically in this example in this screenshot uh we defined",
    "start": "751519",
    "end": "757920"
  },
  {
    "text": "preferred topology key as in a micronite which represents a like a smaller",
    "start": "757920",
    "end": "763399"
  },
  {
    "text": "Network segment in the cluster and uh that means the 10 parts of this spot",
    "start": "763399",
    "end": "768480"
  },
  {
    "text": "group they prefer to be scheduled within this smaller Network segment but if we can fit them in one uh larger Network",
    "start": "768480",
    "end": "776079"
  },
  {
    "text": "segment called Midnight here they can still be scheduled uh in in this scheduling attempt so uh now let's take",
    "start": "776079",
    "end": "783040"
  },
  {
    "text": "a look at a quick demo of job level Affinity um so before I uh showcase the",
    "start": "783040",
    "end": "789560"
  },
  {
    "text": "demo I want to mention that uh all the yo fils we used here and also the stepbystep guide is also available in",
    "start": "789560",
    "end": "796399"
  },
  {
    "text": "our online code repo so you are free to check it out and uh play with it yourself",
    "start": "796399",
    "end": "802399"
  },
  {
    "text": "locally okay uh let's go to the demo so first uh let's check for",
    "start": "802399",
    "end": "811360"
  },
  {
    "text": "the uh classer environment here so uh here I have six worker nodes in this",
    "start": "811360",
    "end": "817519"
  },
  {
    "text": "kind classer and uh I specifically taged um main night labels to be the same for",
    "start": "817519",
    "end": "823279"
  },
  {
    "text": "the first three workers uh which indicates that they are in the same like larger Network segment and uh for worker",
    "start": "823279",
    "end": "831240"
  },
  {
    "text": "two and three they are in the same uh macronet which is a smaller Network",
    "start": "831240",
    "end": "836560"
  },
  {
    "text": "segment and for worker for f and they are in um different mainnet okay um and",
    "start": "836560",
    "end": "843920"
  },
  {
    "text": "first let's uh look at our uh deployment",
    "start": "843920",
    "end": "849000"
  },
  {
    "text": "yo files for satisfying the preferred topology so here uh is our first",
    "start": "849000",
    "end": "856040"
  },
  {
    "text": "deployment demo file and uh I put in replica sets in 10 parts and the",
    "start": "856040",
    "end": "862560"
  },
  {
    "text": "annotations here suggest that this PS will belong to uh this P group named nnx",
    "start": "862560",
    "end": "868399"
  },
  {
    "text": "in this demo um and let's look at the corresponding P groups",
    "start": "868399",
    "end": "875240"
  },
  {
    "text": "back so here uh I defined a main member as 10 which is the same as in the",
    "start": "877040",
    "end": "883639"
  },
  {
    "text": "screenshot in the uh slides and also I Define the same Part Group Affinity here for preferred and",
    "start": "883639",
    "end": "890000"
  },
  {
    "text": "required okay uh and let's then create this uh Part Group and also this",
    "start": "890000",
    "end": "897199"
  },
  {
    "text": "deployment and after a while we can uh we can check",
    "start": "897199",
    "end": "903720"
  },
  {
    "text": "the scheduling results okay so as you can see that all",
    "start": "903720",
    "end": "909920"
  },
  {
    "text": "of the parts they are scheduled to either worker two or worker three uh and",
    "start": "909920",
    "end": "916120"
  },
  {
    "text": "as we we can recall from the network label here they are in the same micronet",
    "start": "916120",
    "end": "921480"
  },
  {
    "text": "so that means at this time uh the part groups can find uh a micronet that",
    "start": "921480",
    "end": "926680"
  },
  {
    "text": "satisfy their scheduling requirements so that we satisfied their preferred Affinity terms in this context and now",
    "start": "926680",
    "end": "934839"
  },
  {
    "text": "uh next let's look at another case where we cannot satisfy uh the requirements",
    "start": "934839",
    "end": "940040"
  },
  {
    "text": "for preferred um so next I will use another",
    "start": "940040",
    "end": "945880"
  },
  {
    "text": "deployment file uh and new P group yo so",
    "start": "945880",
    "end": "951040"
  },
  {
    "text": "at this time I spended the uh mean member to be 15 so that the resources uh",
    "start": "951040",
    "end": "957600"
  },
  {
    "text": "in that micronet cannot not uh be sufficient for this group of parts and",
    "start": "957600",
    "end": "963240"
  },
  {
    "text": "we will also have another uh deployment yo file here which corresponding to that",
    "start": "963240",
    "end": "969680"
  },
  {
    "text": "new P group and also defines replicas as 15 um and then let's uh clean up the",
    "start": "969680",
    "end": "977160"
  },
  {
    "text": "environment and create this workf close and after a while we can check the",
    "start": "977160",
    "end": "983440"
  },
  {
    "text": "scheduling results and at this time we can see that uh worker 1 2 and three are",
    "start": "983440",
    "end": "988639"
  },
  {
    "text": "used used as the uh nodes for this new deployments so recall that worker one",
    "start": "988639",
    "end": "995319"
  },
  {
    "text": "through three they are in the same U main night so that means the required topology can still be satisfied so",
    "start": "995319",
    "end": "1001959"
  },
  {
    "text": "that's why we can schedule uh this Part Group in this attempt uh yeah so that's the quick demo",
    "start": "1001959",
    "end": "1010040"
  },
  {
    "text": "on job level affinity and uh now let's continue our presentation uh so uh what",
    "start": "1010040",
    "end": "1019000"
  },
  {
    "text": "have shared is like the basic required and preferred Affinity terms in job job level affinity and we also offer some uh",
    "start": "1019000",
    "end": "1026918"
  },
  {
    "text": "other Advanced capabilities beyond that for example we support defending node selector uh at the part group level uh",
    "start": "1026919",
    "end": "1034438"
  },
  {
    "text": "so this is very similar to what you will defend for no selector in the PO spec",
    "start": "1034439",
    "end": "1039959"
  },
  {
    "text": "but it's just you can defend this for group of parts and beyond that we also support sort rules um based on different",
    "start": "1039959",
    "end": "1048120"
  },
  {
    "text": "uh resource dimensions and either in ascending or descending order so for",
    "start": "1048120",
    "end": "1053160"
  },
  {
    "text": "example in this particular uh screenshot uh I defined uh the sort rules as to",
    "start": "1053160",
    "end": "1059880"
  },
  {
    "text": "sort GPU CPU and memory in a sending order so that means uh for the group of",
    "start": "1059880",
    "end": "1066080"
  },
  {
    "text": "phas feasible nodes uh scheduler will rank them based on their uh resource",
    "start": "1066080",
    "end": "1073000"
  },
  {
    "text": "Capac compatibility in the asending order so that means the nodes with less",
    "start": "1073000",
    "end": "1078360"
  },
  {
    "text": "resources we have a higher chance of hosting the P so that can give us sort",
    "start": "1078360",
    "end": "1083960"
  },
  {
    "text": "of a beIN packing effects on the Node groups and that can help us reduce resource",
    "start": "1083960",
    "end": "1090080"
  },
  {
    "text": "fragmentation uh yeah so that's all on job level affinity and uh now we want to",
    "start": "1090080",
    "end": "1095919"
  },
  {
    "text": "share this uh so we're very excited about this feature we just open sourced it recently Scout resource reservation",
    "start": "1095919",
    "end": "1103200"
  },
  {
    "text": "uh so Linton also mentioned about resource elasticity and is that uh doing",
    "start": "1103200",
    "end": "1108880"
  },
  {
    "text": "the off peak hours uh we want some uh lower priority offline jobs to be able",
    "start": "1108880",
    "end": "1113960"
  },
  {
    "text": "to borrow the resources from the online services uh because those resources are idle at that time but however when uh",
    "start": "1113960",
    "end": "1122360"
  },
  {
    "text": "the peak hours are coming we also want those uh online services to be able to",
    "start": "1122360",
    "end": "1128039"
  },
  {
    "text": "get their resources back as quickly as possible and this is where resource reservation comes into play uh because",
    "start": "1128039",
    "end": "1135600"
  },
  {
    "text": "resource reservation can mark the reserve notes of those online PS after",
    "start": "1135600",
    "end": "1142159"
  },
  {
    "text": "this scale down so after this go back they can quickly match those reserved resources uh eved uh the offline PS on",
    "start": "1142159",
    "end": "1150919"
  },
  {
    "text": "the resources and just directly schedule the online services back to their",
    "start": "1150919",
    "end": "1156280"
  },
  {
    "text": "original noes uh so we can uh uh we can achieve a very high efficiency of online",
    "start": "1156280",
    "end": "1163480"
  },
  {
    "text": "services scaling up and now we will also do a quick demo to Showcase how resource",
    "start": "1163480",
    "end": "1169039"
  },
  {
    "text": "reservation Works um so let's first uh take a look at this",
    "start": "1169039",
    "end": "1176919"
  },
  {
    "text": "deployment file so uh here in this annotation there is a specific one",
    "start": "1176919",
    "end": "1182520"
  },
  {
    "text": "called uh go. by.com back/ reservation equals to be true so this is how uh the",
    "start": "1182520",
    "end": "1189600"
  },
  {
    "text": "deployment can Mark itself as the one that want to uh reserve the resource when when scale",
    "start": "1189600",
    "end": "1196400"
  },
  {
    "text": "down okay and then uh let's apply this deployment and after a while we can",
    "start": "1196400",
    "end": "1203240"
  },
  {
    "text": "check the scheduling results uh so now we can see the three parts as scheduled",
    "start": "1203240",
    "end": "1208400"
  },
  {
    "text": "to worker node 3 worker node 2 and worker two uh remember this scheduling",
    "start": "1208400",
    "end": "1214799"
  },
  {
    "text": "results for a bit and now let's scale down uh this",
    "start": "1214799",
    "end": "1221480"
  },
  {
    "text": "deployment and so that this will delete the three part",
    "start": "1221480",
    "end": "1226960"
  },
  {
    "text": "replicas okay and then uh G will do its magic it will create",
    "start": "1226960",
    "end": "1232440"
  },
  {
    "text": "the reservation objects in this classer and we can see that it will",
    "start": "1232440",
    "end": "1240000"
  },
  {
    "text": "create one reservation object corresponding to each part before it was",
    "start": "1240000",
    "end": "1245799"
  },
  {
    "text": "scaled down um and the name is corresponding to the Pod name and",
    "start": "1245799",
    "end": "1251360"
  },
  {
    "text": "also the it recalls the previous note name which the Pod was previously",
    "start": "1251360",
    "end": "1256520"
  },
  {
    "text": "scheduled on so that's how it saves the mapping between the Pod and the reserved",
    "start": "1256520",
    "end": "1262200"
  },
  {
    "text": "resources okay and after a while uh we can Scale app this uh deployment and we",
    "start": "1262200",
    "end": "1269679"
  },
  {
    "text": "can check the scheduling results",
    "start": "1269679",
    "end": "1273440"
  },
  {
    "text": "again okay so we can see that the scaled app Parts they are directly scheduled on",
    "start": "1276480",
    "end": "1282320"
  },
  {
    "text": "the exact the same previous three nodes which is two worker two and uh one",
    "start": "1282320",
    "end": "1287600"
  },
  {
    "text": "worker three nodes so yeah that's basically how reservation",
    "start": "1287600",
    "end": "1294240"
  },
  {
    "text": "works and we can also get an Insider view by checking a specific annotation",
    "start": "1294240",
    "end": "1300240"
  },
  {
    "text": "called reservation placeholder in the uh skilled app P yo and this uh this",
    "start": "1300240",
    "end": "1308080"
  },
  {
    "text": "annotation was put by go scheduler so that means it has found a particular uh",
    "start": "1308080",
    "end": "1313440"
  },
  {
    "text": "reservation object in the cluster for this particular part and uh the placeholder name are corresponding to",
    "start": "1313440",
    "end": "1319799"
  },
  {
    "text": "the a previous reservation object name that we just",
    "start": "1319799",
    "end": "1325159"
  },
  {
    "text": "saw okay so uh if you don't want a resource reservation it's totally fine",
    "start": "1325159",
    "end": "1331480"
  },
  {
    "text": "uh you can disable this by just uh removing the uh reservation annotation",
    "start": "1331480",
    "end": "1338279"
  },
  {
    "text": "in the deployment file so we we just remove this annotation and if we uh",
    "start": "1338279",
    "end": "1343480"
  },
  {
    "text": "scale down this deployment again we can see that uh this there's no reservation",
    "start": "1343480",
    "end": "1351200"
  },
  {
    "text": "objects created in uh in this cluster so that means resource reservation did not take place in this",
    "start": "1351200",
    "end": "1358520"
  },
  {
    "text": "time okay uh this is the demo on resource reservation and beyond that we want to",
    "start": "1358520",
    "end": "1365840"
  },
  {
    "text": "quickly work through another two Advanced features uh so this one is called",
    "start": "1365840",
    "end": "1371400"
  },
  {
    "text": "microtopology scheduling or Numa aware scheduling uh this is particularly",
    "start": "1371400",
    "end": "1376559"
  },
  {
    "text": "useful for some uh work that are very sensitive to latencies that they require",
    "start": "1376559",
    "end": "1382760"
  },
  {
    "text": "their P to be scheduled within the same new man node uh so that they can minimize the uh memory access latency",
    "start": "1382760",
    "end": "1390240"
  },
  {
    "text": "which was caused by cross Numa memory access and this was achieved by uh the",
    "start": "1390240",
    "end": "1396279"
  },
  {
    "text": "microt topology plugins in the godal scheduling system and also this feature",
    "start": "1396279",
    "end": "1401600"
  },
  {
    "text": "will uh require the scheduler to work with a node agent uh which can report",
    "start": "1401600",
    "end": "1407039"
  },
  {
    "text": "new My Level resources from the nodes and at bance we use this resource management system uh Catalyst for its",
    "start": "1407039",
    "end": "1414559"
  },
  {
    "text": "capability to uh report the new my resources through its own custom node",
    "start": "1414559",
    "end": "1419679"
  },
  {
    "text": "resource objects and catalyst is also an open source project and you uh if you're",
    "start": "1419679",
    "end": "1426200"
  },
  {
    "text": "interested you free to look more details into it and one last uh pretty new feature is",
    "start": "1426200",
    "end": "1433480"
  },
  {
    "text": "called a real time load aware scheduling so this can uh get uh so be Beyond using",
    "start": "1433480",
    "end": "1440880"
  },
  {
    "text": "the static CPU configuration say in the PO spec it can uh really check on the uh",
    "start": "1440880",
    "end": "1448400"
  },
  {
    "text": "real time uh load on the noes so take CPU for example it could uh check the",
    "start": "1448400",
    "end": "1455000"
  },
  {
    "text": "realtime CPU usage on each node and based on that information to try to schedule the parts and try to spread uh",
    "start": "1455000",
    "end": "1462000"
  },
  {
    "text": "the work close in the in the cluster so we use this to help us uh address some",
    "start": "1462000",
    "end": "1467600"
  },
  {
    "text": "particular uh spot issues which we observe in production and again load ofwar also requires uh scheduler to work",
    "start": "1467600",
    "end": "1476360"
  },
  {
    "text": "with the node agent to report the real time load usage data in the in the nodes",
    "start": "1476360",
    "end": "1482559"
  },
  {
    "text": "and again at bance we use Catalyst for this capability uh and since we mentioned",
    "start": "1482559",
    "end": "1488200"
  },
  {
    "text": "Catalyst a few times um there's actually another session which is focus on Catalyst in uh tomorrow afternoon and",
    "start": "1488200",
    "end": "1495720"
  },
  {
    "text": "our colleague will uh introduce like how uh nodes aggregate real time resource",
    "start": "1495720",
    "end": "1501360"
  },
  {
    "text": "usage and some amazing Innovations regarding service profiling so we highly",
    "start": "1501360",
    "end": "1506559"
  },
  {
    "text": "recommend you to join that session if you are interested in this topic",
    "start": "1506559",
    "end": "1513200"
  },
  {
    "text": "okay okay so yeah let's get to the ACH achievement uh section uh here we are",
    "start": "1515799",
    "end": "1522399"
  },
  {
    "text": "going to talk about uh the achievements we have made so far in our production",
    "start": "1522399",
    "end": "1527600"
  },
  {
    "text": "exercise and and as well as academic contribution uh yeah I will uh just uh",
    "start": "1527600",
    "end": "1534760"
  },
  {
    "text": "share production uh achievements by uh like in by sharing how did we resolve",
    "start": "1534760",
    "end": "1541799"
  },
  {
    "text": "our main pain points so first of all the resource utilization uh with girdle we",
    "start": "1541799",
    "end": "1547799"
  },
  {
    "text": "were able to achieve uh uh around like 35% uh Pak CPU uh utilization uh in one",
    "start": "1547799",
    "end": "1555760"
  },
  {
    "text": "of the selected uh Ultra large cluster and the uh the most importantly the",
    "start": "1555760",
    "end": "1562600"
  },
  {
    "text": "average C utilization was improved uh to around like a high 50ish to uh from the",
    "start": "1562600",
    "end": "1571720"
  },
  {
    "text": "low 30ish uh without GLE and the second one is the resource",
    "start": "1571720",
    "end": "1579320"
  },
  {
    "text": "elasticity uh as I I mentioned that we before gero we have split brain in terms",
    "start": "1579320",
    "end": "1585520"
  },
  {
    "text": "of the resource management and we need to uh uh we always need to plan ahead to uh",
    "start": "1585520",
    "end": "1592799"
  },
  {
    "text": "have the operation team to move resource around to uh like uh like meet the",
    "start": "1592799",
    "end": "1599039"
  },
  {
    "text": "requirement of the major event some something like Black Friday Spring Festival and uh like Super Bowl and uh",
    "start": "1599039",
    "end": "1608320"
  },
  {
    "text": "but with girle we were able to achieve like minute level uh like automatic resource transferring before uh between",
    "start": "1608320",
    "end": "1615039"
  },
  {
    "text": "this online and offline resource pool so it uh greatly save our uh operational",
    "start": "1615039",
    "end": "1621000"
  },
  {
    "text": "costs and also greatly avoid this uh any unexpected human",
    "start": "1621000",
    "end": "1628000"
  },
  {
    "text": "error and regarding the resource fragmentation yeah we also made some",
    "start": "1628000",
    "end": "1633320"
  },
  {
    "text": "achievement here and uh we introduced some uh like uh enhancement uh in the",
    "start": "1633320",
    "end": "1640760"
  },
  {
    "text": "ban packing algorithm to uh like to achieve like 20% of the uh less uh GP",
    "start": "1640760",
    "end": "1648679"
  },
  {
    "text": "uh resource fragmentation and uh yeah before gero the like resource we lose around like",
    "start": "1648679",
    "end": "1656840"
  },
  {
    "text": "30% uh available capacity what and we were a it was reduced to",
    "start": "1656840",
    "end": "1664399"
  },
  {
    "text": "10% and apart from the achievement in uh production we also we are also very",
    "start": "1664399",
    "end": "1671720"
  },
  {
    "text": "active in academic contribution uh our paper around gerto schedule was accepted incc",
    "start": "1671720",
    "end": "1678240"
  },
  {
    "text": "industry and uh we currently we are also working on some uh paper uh about our",
    "start": "1678240",
    "end": "1685080"
  },
  {
    "text": "the performance enhancement we have uh we made in uh gero to better explain how",
    "start": "1685080",
    "end": "1690320"
  },
  {
    "text": "did we do to achieve the uh low latency and high",
    "start": "1690320",
    "end": "1696240"
  },
  {
    "text": "circuit okay next session will be the future work and we will take that back",
    "start": "1696279",
    "end": "1702000"
  },
  {
    "text": "in thank you uh so we have shared uh other the things we have done so far and",
    "start": "1702000",
    "end": "1707600"
  },
  {
    "text": "some recent updates and we are also excited to share something coming soon and you can expect in the",
    "start": "1707600",
    "end": "1714399"
  },
  {
    "text": "future uh so the first is we are also working on open sourcing our gol risk",
    "start": "1714399",
    "end": "1720399"
  },
  {
    "text": "scheduler uh so risk scheduler will work with a Goa scheduler to reschedule the",
    "start": "1720399",
    "end": "1726559"
  },
  {
    "text": "pause uh that were previously scheduled uh based on like a cluster State changes",
    "start": "1726559",
    "end": "1733159"
  },
  {
    "text": "because we know that uh the cluster uh state is constantly changing after the",
    "start": "1733159",
    "end": "1738200"
  },
  {
    "text": "scheduler scheduled the part once uh so that rescher can uh based on different",
    "start": "1738200",
    "end": "1744880"
  },
  {
    "text": "strategies to keep optimizing your scheduling results in the cluster so for",
    "start": "1744880",
    "end": "1750039"
  },
  {
    "text": "example uh we will offer GPU be packing uh res schedular strategies uh which",
    "start": "1750039",
    "end": "1755640"
  },
  {
    "text": "means that you can use that to uh keep reducing or managing your resource fragmentation issue in your GP",
    "start": "1755640",
    "end": "1763919"
  },
  {
    "text": "clusters um the second one is called uh all in one mode uh so as Linton",
    "start": "1763919",
    "end": "1769000"
  },
  {
    "text": "mentioned in the architecture currently we are designed and deployed in the production as a uh distributed uh mode",
    "start": "1769000",
    "end": "1777799"
  },
  {
    "text": "for those three components and we are currently exploring this all in one mode",
    "start": "1777799",
    "end": "1783480"
  },
  {
    "text": "so that we can provide an all in one Banner for a smaller scale cluster to",
    "start": "1783480",
    "end": "1788679"
  },
  {
    "text": "utilize a good out scheduler so that it is easier to deploy and maintain and",
    "start": "1788679",
    "end": "1793880"
  },
  {
    "text": "we're also benefiting from our he report and our uh Rich function",
    "start": "1793880",
    "end": "1799399"
  },
  {
    "text": "functionalities uh and also we are working to uh further improve our",
    "start": "1799399",
    "end": "1804640"
  },
  {
    "text": "performance optimization in the scheduling algorithms um and also we are",
    "start": "1804640",
    "end": "1809679"
  },
  {
    "text": "working to achieve a more standardized and extensible framework for the open",
    "start": "1809679",
    "end": "1814880"
  },
  {
    "text": "source Community as well um uh last thing is uh we also aim to do more uh",
    "start": "1814880",
    "end": "1821919"
  },
  {
    "text": "ecosystem construction work in the future um for example we want to explore how we can uh provide compatible apis",
    "start": "1821919",
    "end": "1830080"
  },
  {
    "text": "for those popular Upstream framework such as Peters and flow yeah so this Bally concludes our",
    "start": "1830080",
    "end": "1837399"
  },
  {
    "text": "talk to today uh thank you very much for listening to this talk and this is the link to our online uh Cod code reple U",
    "start": "1837399",
    "end": "1846039"
  },
  {
    "text": "you are free to check out and play with the quick start guys and all the features we provided there and uh please",
    "start": "1846039",
    "end": "1852840"
  },
  {
    "text": "also give us a star if you like it we will appreciate it thank you",
    "start": "1852840",
    "end": "1858020"
  },
  {
    "text": "[Applause]",
    "start": "1858020",
    "end": "1862739"
  },
  {
    "text": "yeah we can take some questions all right I'm going to ask the first question thank you so much for sharing the work this is super",
    "start": "1865600",
    "end": "1872080"
  },
  {
    "text": "impressive um one thing I want to double check is that when you say you do the",
    "start": "1872080",
    "end": "1878519"
  },
  {
    "text": "resource awareness part uh not scheduling are you suggesting you are",
    "start": "1878519",
    "end": "1883639"
  },
  {
    "text": "using soft quarter instead of hard quarter um and other thing is that you",
    "start": "1883639",
    "end": "1888760"
  },
  {
    "text": "know when you have this kind of mixed workload scheduling running on the same notes a typical problem we're run to is",
    "start": "1888760",
    "end": "1895159"
  },
  {
    "text": "a noisy Neal issue when there are very you know expensive I things do you have",
    "start": "1895159",
    "end": "1901720"
  },
  {
    "text": "any suggestion to tackle them thank you okay I'll take that okay uh so that's a",
    "start": "1901720",
    "end": "1909039"
  },
  {
    "text": "good question so yeah that's another uh maybe it deserve another session to talk",
    "start": "1909039",
    "end": "1915760"
  },
  {
    "text": "about it cuz it uh it's about some uh major project we are working on uh in",
    "start": "1915760",
    "end": "1921399"
  },
  {
    "text": "house like uh it's called collocation Uh that means we want to collocate the",
    "start": "1921399",
    "end": "1927279"
  },
  {
    "text": "online offline workload maybe on the same note and but regarding the noise",
    "start": "1927279",
    "end": "1933080"
  },
  {
    "text": "and neighbor issue you just mentioned we are going to if the online workload is",
    "start": "1933080",
    "end": "1938840"
  },
  {
    "text": "very Laten critical and we want to enforce that we are going to uh apply",
    "start": "1938840",
    "end": "1944000"
  },
  {
    "text": "some of the uh topology Affinity such as new man Numa aware so we will binding",
    "start": "1944000",
    "end": "1950880"
  },
  {
    "text": "the CPU set we are going also have this uh B binding the Numa Noe to make sure",
    "start": "1950880",
    "end": "1958240"
  },
  {
    "text": "noise neighbor will not in uh have any influence to our uh critical online",
    "start": "1958240",
    "end": "1964519"
  },
  {
    "text": "workload so yeah I hope that answer your question uh hi great talk by the way uh",
    "start": "1964519",
    "end": "1971080"
  },
  {
    "text": "couple quick questions so um you mentioned that the max number of nodes you support right now is around 20,000",
    "start": "1971080",
    "end": "1977519"
  },
  {
    "text": "and one million pods total I'm sure bite dance has a much bigger footprint than that as you mentioned so do you run like",
    "start": "1977519",
    "end": "1984200"
  },
  {
    "text": "different instances of Cal scheduler for like each set of clusters that hit 20,000 NS like what I'm curious about",
    "start": "1984200",
    "end": "1991600"
  },
  {
    "text": "the cluster topology that looks like so yeah I will uh I'm not sure whether",
    "start": "1991600",
    "end": "1998360"
  },
  {
    "text": "it's okay to answer that but yeah we are Runing different work cloud based on the",
    "start": "1998360",
    "end": "2004159"
  },
  {
    "text": "scenario so uh we think think about it like for like GPU",
    "start": "2004159",
    "end": "2010799"
  },
  {
    "text": "training maybe you uh deserve a dedicated workload and we are going to try working on trying to explore if we",
    "start": "2010799",
    "end": "2018279"
  },
  {
    "text": "can collocate some of the like offline application which uh can uh leverage the",
    "start": "2018279",
    "end": "2026120"
  },
  {
    "text": "uh idle resources uh that's one thing like for the 20K cluster it mostly like",
    "start": "2026120",
    "end": "2033320"
  },
  {
    "text": "the uh combination of the microservices uh along with is badge job",
    "start": "2033320",
    "end": "2039200"
  },
  {
    "text": "you know like microservices is pretty usually it request a lot of uh resources",
    "start": "2039200",
    "end": "2044559"
  },
  {
    "text": "but it sometimes it's actually not used uh most of most of is much lower yeah but uh and for the badge job it can wait",
    "start": "2044559",
    "end": "2053118"
  },
  {
    "text": "uh as long as we queue it enters the queue the customer or user will be",
    "start": "2053119",
    "end": "2059320"
  },
  {
    "text": "satisfied and uh maybe uh before get off the work it will submit the job and the",
    "start": "2059320",
    "end": "2066240"
  },
  {
    "text": "next day it will come to check the result and uh yeah and you know like for",
    "start": "2066240",
    "end": "2071440"
  },
  {
    "text": "the in the early morning right it's usually the uh low uh use we we have low",
    "start": "2071440",
    "end": "2077679"
  },
  {
    "text": "usage of online uh resources so that's a time we maybe we will borrow the",
    "start": "2077679",
    "end": "2083560"
  },
  {
    "text": "resources to offline workload and that's a Time the offline workload will start to execute and be running and to uh yeah",
    "start": "2083560",
    "end": "2093240"
  },
  {
    "text": "to try to leverage the idal resources better thank you thank",
    "start": "2093240",
    "end": "2100160"
  },
  {
    "text": "you sorry one more question um could you talk a little bit more about your",
    "start": "2101040",
    "end": "2107680"
  },
  {
    "text": "quarter management uh especially when there is a resource boring you know",
    "start": "2107680",
    "end": "2112920"
  },
  {
    "text": "between offline and online how do you make sure that there qu management right yeah quot management okay yeah currently",
    "start": "2112920",
    "end": "2119520"
  },
  {
    "text": "Goro have no uh design uh like building uh Cota management it's actually external one uh something like Q we also",
    "start": "2119520",
    "end": "2127320"
  },
  {
    "text": "have a alternative uh sum to that so it sit in front of the uh like critical",
    "start": "2127320",
    "end": "2132880"
  },
  {
    "text": "pass and before uh yeah the worklow will entered into the queue and the Kota",
    "start": "2132880",
    "end": "2139520"
  },
  {
    "text": "system and after the Cota check has been completed it will enter gero system to",
    "start": "2139520",
    "end": "2145440"
  },
  {
    "text": "uh complete the scheduling yeah so maybe one thing I down to L's answer is way uh so talking",
    "start": "2145440",
    "end": "2152560"
  },
  {
    "text": "about collocating online and offline workloads we will have different C uh different C skilles for online and",
    "start": "2152560",
    "end": "2158599"
  },
  {
    "text": "offline jobs and for the uh offline jobs we also rely on Catalyst which was",
    "start": "2158599",
    "end": "2164079"
  },
  {
    "text": "mentioned in the presentation earlier for its reported resources through its own custom node resource objects and we",
    "start": "2164079",
    "end": "2171480"
  },
  {
    "text": "can use that information as source of choose and calculate the uh resource",
    "start": "2171480",
    "end": "2176680"
  },
  {
    "text": "code have for the offline job so that we can collocate both of them together yeah so gerle uh schedule system is the whole",
    "start": "2176680",
    "end": "2183960"
  },
  {
    "text": "ecosystem gerto schedule is one part of it we have our uh enhanced CER Cube API",
    "start": "2183960",
    "end": "2189920"
  },
  {
    "text": "server our own metadata store and Catalyst serve as a like uh a node agent",
    "start": "2189920",
    "end": "2196119"
  },
  {
    "text": "to reporting this uh real time metrics yeah it deserve a separate uh session",
    "start": "2196119",
    "end": "2203040"
  },
  {
    "text": "each and yeah we expect to share that in a later session looking forward to all",
    "start": "2203040",
    "end": "2209760"
  },
  {
    "text": "of them thank you thank you thank you all right thank you everyone [Applause]",
    "start": "2209760",
    "end": "2216940"
  }
]