[
  {
    "text": "okay so before we start the session how many of you use LMS today in your regular life other than the GPD other",
    "start": "160",
    "end": "7879"
  },
  {
    "text": "than the interaction you actually use in your applications to build",
    "start": "7879",
    "end": "12960"
  },
  {
    "text": "applications uh how many of you have heard about rag most of you perfect so this is a",
    "start": "12960",
    "end": "20760"
  },
  {
    "text": "perfect stage for uh this session uh well my name is madav sat uh today we're",
    "start": "20760",
    "end": "26640"
  },
  {
    "text": "going to talk about brag your rag with the envelop swag",
    "start": "26640",
    "end": "31960"
  },
  {
    "text": "hey I'm J I'm part of publist Sapient working as a director technology I manage devops and Cloud practice for",
    "start": "32559",
    "end": "38280"
  },
  {
    "text": "North America and uh Mexico yeah and uh yeah I said uh I work",
    "start": "38280",
    "end": "44280"
  },
  {
    "text": "at Google Cloud I'm a principal architect for financial services focused on Banking and capital",
    "start": "44280",
    "end": "51239"
  },
  {
    "text": "markets so we'll start with the agenda we have a packed agenda I know we have 25 minutes so we have a spectrum of LM",
    "start": "51239",
    "end": "57920"
  },
  {
    "text": "customizations uh we'll be talking about the prompt engineering and rag uh along with that we'll be talking about the rag",
    "start": "57920",
    "end": "64600"
  },
  {
    "text": "and implementation on GK with a with a demo and uh the last we'll be talking about the patterns of mlops and optimal",
    "start": "64600",
    "end": "71680"
  },
  {
    "text": "use of exelator so I think all of you have used llms",
    "start": "71680",
    "end": "78560"
  },
  {
    "text": "you're using today to build applications they are very powerful uh AI models neural networks that have the ability to",
    "start": "78560",
    "end": "85200"
  },
  {
    "text": "generate text videos and have reasoning capabilities but the biggest problem",
    "start": "85200",
    "end": "90400"
  },
  {
    "text": "with these llms are that they are not grounded based on your Enterprise data",
    "start": "90400",
    "end": "95600"
  },
  {
    "text": "so for any meaningful use within your Enterprise you need to have some mechanism of training or giving",
    "start": "95600",
    "end": "102640"
  },
  {
    "text": "information or grounding the llms with the Enterprise data with which you can build your business applications so if",
    "start": "102640",
    "end": "109520"
  },
  {
    "text": "you look at the overall industry and how the llms have been used across the",
    "start": "109520",
    "end": "114560"
  },
  {
    "text": "industries we can look at various spectrum of options of managing the llms",
    "start": "114560",
    "end": "120719"
  },
  {
    "text": "to use your data starting with the prompt engineering is one of the most uh",
    "start": "120719",
    "end": "126680"
  },
  {
    "text": "interesting skill that it will be adopted widely across Industries no matter in what shape on form you use llm",
    "start": "126680",
    "end": "132599"
  },
  {
    "text": "and how you customize prompt engineering is going to be the the basis for using llms going forward uh after that we have",
    "start": "132599",
    "end": "140920"
  },
  {
    "text": "rag which is retrieval augmented generation that will not exactly tune",
    "start": "140920",
    "end": "146239"
  },
  {
    "text": "the model but it will give you the grounding basis with your Enterprise data to the model so your model will not",
    "start": "146239",
    "end": "152599"
  },
  {
    "text": "hallucinate as much or it will not hallucinate essentially if you have a large enough data set it will not",
    "start": "152599",
    "end": "158400"
  },
  {
    "text": "hallucinate now most of these Enterprises will basically uh use these",
    "start": "158400",
    "end": "163680"
  },
  {
    "text": "two options on a more regular basis and we already see adoption of rag uh for",
    "start": "163680",
    "end": "169239"
  },
  {
    "text": "Enterprise uh applications beyond that if you are serious about uh going Beyond",
    "start": "169239",
    "end": "175560"
  },
  {
    "text": "rag then you will have to start thinking about tuning the model itself self and",
    "start": "175560",
    "end": "180879"
  },
  {
    "text": "there are a couple of options for tuning the model one is parameter efficient uh fine-tuning and other one is a full",
    "start": "180879",
    "end": "187400"
  },
  {
    "text": "tuning right both of these par both of these techniques involve uh essentially",
    "start": "187400",
    "end": "193000"
  },
  {
    "text": "a large computation resources uh large amount of data that could be uh that",
    "start": "193000",
    "end": "199040"
  },
  {
    "text": "could be labeled or supervised data as it is called In traditional machine learning and after that you have the uh",
    "start": "199040",
    "end": "205840"
  },
  {
    "text": "ultimate pre-training where you actually build up new models itself so if you look at the complexity on one side and",
    "start": "205840",
    "end": "212400"
  },
  {
    "text": "the cost of ownership on the other side they actually tuning will actually cost",
    "start": "212400",
    "end": "217560"
  },
  {
    "text": "you a lot of money it also requires tremendous amount of skills to help you achieve that right so uh if you look at",
    "start": "217560",
    "end": "224879"
  },
  {
    "text": "the Spectrum most Enterprises will typically be using prompt engineering and rag so we'll talk about the prompt",
    "start": "224879",
    "end": "231799"
  },
  {
    "text": "engineering and rag uh you know um so what is promp engineering U you know",
    "start": "231799",
    "end": "237280"
  },
  {
    "text": "madav asked this question to everybody and everybody said that most of the people have been using prompt engineering right nowadays we we call",
    "start": "237280",
    "end": "243480"
  },
  {
    "text": "this an art and science the Art and Science because you the the kind of prompts that you provide the kind of",
    "start": "243480",
    "end": "249480"
  },
  {
    "text": "feeds that you give as an input you get the results based on the input that you provide you keep learning on it maybe",
    "start": "249480",
    "end": "257079"
  },
  {
    "text": "you you know you give one shot or maybe you give you know a chain of thoughts around it but better the inputs are",
    "start": "257079",
    "end": "263160"
  },
  {
    "text": "available as part of the prompt better the output that we get from the L as a response part",
    "start": "263160",
    "end": "270600"
  },
  {
    "text": "so if you look at the rag we can break it down in three phases the first phase you can call it pre-processing this is",
    "start": "271919",
    "end": "278680"
  },
  {
    "text": "where you bring in your Enterprise data and you have a process or you have a pipeline that is essentially going to",
    "start": "278680",
    "end": "284520"
  },
  {
    "text": "break down the data in smaller chunks that smaller chunks with those chunks you will use an embedding model to",
    "start": "284520",
    "end": "290639"
  },
  {
    "text": "create vectors and that will be stored in the vector database so this chunking there are strategies around how to chunk",
    "start": "290639",
    "end": "297120"
  },
  {
    "text": "it what should be the chunk size what embedding model to use where should they run there are strategies around it and",
    "start": "297120",
    "end": "303400"
  },
  {
    "text": "some of those uh patterns we're going to look at during the next slides uh the",
    "start": "303400",
    "end": "308479"
  },
  {
    "text": "second part comes is the retrieval part after the chunking is done basically user provides an input you know use the",
    "start": "308479",
    "end": "314120"
  },
  {
    "text": "same Vector database that where the data is available and the vular database based on the chunk and the kind of uh",
    "start": "314120",
    "end": "320080"
  },
  {
    "text": "you know um request that is put in we we get the the the augmented Generations",
    "start": "320080",
    "end": "326360"
  },
  {
    "text": "coming as a response using the llms after the chunking after the initial part of the pre-processing after the",
    "start": "326360",
    "end": "332440"
  },
  {
    "text": "chunking the final result that comes basically we use model inference to get",
    "start": "332440",
    "end": "337479"
  },
  {
    "text": "the output now we talked about the first two",
    "start": "337479",
    "end": "342919"
  },
  {
    "text": "aspects which is the prompt engineering and drag most Enterprises going to use that but those Enterprises that have the",
    "start": "342919",
    "end": "348479"
  },
  {
    "text": "skills and the the money to sponsor the two fine tuning efforts for them you're going to start with a pre-trend model",
    "start": "348479",
    "end": "355680"
  },
  {
    "text": "those essentially the model that have the generic language capabilities reasoning capabilities but with that",
    "start": "355680",
    "end": "361160"
  },
  {
    "text": "you're going to use a supervised fine tuning you're going to bring lot of Enterprise data that is labeled right",
    "start": "361160",
    "end": "368319"
  },
  {
    "text": "and with that data you're going to fine-tune the model now there are a couple of options we discussed in that Spectrum one is a PFT and otherwi a full",
    "start": "368319",
    "end": "375599"
  },
  {
    "text": "fine tuning and full fine tuning is going to be even more expensive more complex and more expensive to uh",
    "start": "375599",
    "end": "382400"
  },
  {
    "text": "actually train as well as to run uh PFT models on the other hand there are smart ways to actually capture uh the freeze",
    "start": "382400",
    "end": "389960"
  },
  {
    "text": "the model the base model and you can have a Laura adopter layer on top of it which basically adds some extra",
    "start": "389960",
    "end": "395960"
  },
  {
    "text": "parameters that are trained based on the data so when you actually run the model you're not going to have the difficulty",
    "start": "395960",
    "end": "402560"
  },
  {
    "text": "of running multiple uh you can each each Laura layer can be fine-tuned for a",
    "start": "402560",
    "end": "408240"
  },
  {
    "text": "specific task for example you can use one Laura layer for summarization other one to classify documents or the other",
    "start": "408240",
    "end": "414280"
  },
  {
    "text": "one to generate text or images uh based on your Enterprise data",
    "start": "414280",
    "end": "420199"
  },
  {
    "text": "so we'll start with the data uh uh the demo basically uh and for the demo specifically we used open source product",
    "start": "420199",
    "end": "427000"
  },
  {
    "text": "called Kopi and we used U you know uh basically pine cone database so Kopi is",
    "start": "427000",
    "end": "435360"
  },
  {
    "text": "nothing but a package uh you know a web service or Library can we can which can you use to build your own custom",
    "start": "435360",
    "end": "441759"
  },
  {
    "text": "applications basically it has three sections uh the knowledge based context engine and uh the kopy uh chat engine",
    "start": "441759",
    "end": "449520"
  },
  {
    "text": "the knowledge base whatever been speaking about prepares your data you know for the r work flow it autoally",
    "start": "449520",
    "end": "455720"
  },
  {
    "text": "chunks and transform your text Data into you know uh text embeding um the context",
    "start": "455720",
    "end": "461879"
  },
  {
    "text": "engines is basically your retrieval part uh whatever the data that you got in the knowledge base as part of you know the",
    "start": "461879",
    "end": "468800"
  },
  {
    "text": "chunking it is able to you know find the most of the relevant data from from as",
    "start": "468800",
    "end": "474400"
  },
  {
    "text": "part of the context engine the last is the canop chat engine which is the full",
    "start": "474400",
    "end": "480199"
  },
  {
    "text": "workflow it understands your chat history it understands your identifies multiple questions it generates the",
    "start": "480199",
    "end": "485520"
  },
  {
    "text": "answer based on the llm and then embeds and and provides the query results that",
    "start": "485520",
    "end": "490960"
  },
  {
    "text": "that you look for with Kopi the uh the what we found as a challenge was it is",
    "start": "490960",
    "end": "497000"
  },
  {
    "text": "very much working with pine cone U you know if you're not using pine cone then",
    "start": "497000",
    "end": "502280"
  },
  {
    "text": "Kopi mayale may not be a relevant use case uh but we really wanted to show you something how Kopi and P",
    "start": "502280",
    "end": "510039"
  },
  {
    "text": "can be used just to for a small application that we built on as part of the",
    "start": "510039",
    "end": "515279"
  },
  {
    "text": "demo give us one second we'll share the other screen and",
    "start": "515279",
    "end": "520919"
  },
  {
    "text": "uh goodness okay",
    "start": "526200",
    "end": "532360"
  },
  {
    "text": "so in this application basically we developed three kind of applications one with the Lang chain and pine cone the",
    "start": "540680",
    "end": "546519"
  },
  {
    "text": "other application that we build is with Kopi and pine con the third one is we did not use any any of the vector",
    "start": "546519",
    "end": "551640"
  },
  {
    "text": "database we just used uh open source llm to fit the data on the specific",
    "start": "551640",
    "end": "559160"
  },
  {
    "text": "results actually we're going to need uh screen mirroring because the demo has to be uh can somebody help here to do",
    "start": "565720",
    "end": "572399"
  },
  {
    "text": "mirror the",
    "start": "572399",
    "end": "574839"
  },
  {
    "text": "screen uh for just just an information uh as part of this demo uh we went to a",
    "start": "587160",
    "end": "594320"
  },
  {
    "text": "Wikipedia page of uh you know uh 2024 go back to the screen mirror",
    "start": "594320",
    "end": "599959"
  },
  {
    "text": "yeah 2024 um Olympics that happened and",
    "start": "599959",
    "end": "605120"
  },
  {
    "text": "that is so simple that is what we're trying to show here all right thanks for",
    "start": "605120",
    "end": "611720"
  },
  {
    "text": "that uh live demos technical glitch uh all right so we got three different",
    "start": "611720",
    "end": "618560"
  },
  {
    "text": "screens here so Jen you said the llms are so powerful that they know",
    "start": "618560",
    "end": "624440"
  },
  {
    "text": "everything right so can you tell me when was the last Olympics why didn't you try yourself okay you have the system",
    "start": "624440",
    "end": "630800"
  },
  {
    "text": "available just try see what you",
    "start": "630800",
    "end": "634360"
  },
  {
    "text": "get all right so this clearly doesn't have the information that we really wanted to have yeah because it happened",
    "start": "638959",
    "end": "644839"
  },
  {
    "text": "in 20124 given the system is so old as of now the open system is does not have",
    "start": "644839",
    "end": "650079"
  },
  {
    "text": "the data right so as part of the practice what we did in this demo we we",
    "start": "650079",
    "end": "655160"
  },
  {
    "text": "uh took a you know Wikipedia page of 2024 and then try to make sure that the",
    "start": "655160",
    "end": "660839"
  },
  {
    "text": "pine con and the Kopi is able to talk to each other and get the data why why don't you try the other options with the",
    "start": "660839",
    "end": "666279"
  },
  {
    "text": "kopy and sure let's see if it finds",
    "start": "666279",
    "end": "670440"
  },
  {
    "text": "out all right so it looks like not only gave the correct answer it also found",
    "start": "678880",
    "end": "684440"
  },
  {
    "text": "the source uh based on which that answer was provided right so this is where the",
    "start": "684440",
    "end": "691279"
  },
  {
    "text": "uh this data set was used as a the the grounding Source data set for for us to",
    "start": "691279",
    "end": "698320"
  },
  {
    "text": "get the context based answers yeah and then we also have a solution where if",
    "start": "698320",
    "end": "703760"
  },
  {
    "text": "you don't have a pass like any uh rack platform you can still build everything",
    "start": "703760",
    "end": "708920"
  },
  {
    "text": "ground up using L chain and pine cone yeah and so the third one is we did not use any kopy uh we just use l chain you",
    "start": "708920",
    "end": "716639"
  },
  {
    "text": "know as as code and and and use connected that with the pine cone you can connect with any database that you",
    "start": "716639",
    "end": "722040"
  },
  {
    "text": "like and then we ask the same questions and I think we should be getting the same response of",
    "start": "722040",
    "end": "728240"
  },
  {
    "text": "2024 the challenge here is if you do not use kopio basically it you know I have",
    "start": "728240",
    "end": "734519"
  },
  {
    "text": "to write all the customized code on on for for retrieval the data retrieving the data and that is where maybe you",
    "start": "734519",
    "end": "741480"
  },
  {
    "text": "want to show the next slide ma yeah let me go back to the presentation",
    "start": "741480",
    "end": "748519"
  },
  {
    "text": "not so if you so the first snippet here essentially shows just a rest endpoint",
    "start": "752880",
    "end": "759399"
  },
  {
    "text": "and you ask a prompt or send your query and you get a direct answer which is enriched by the grounding data set and",
    "start": "759399",
    "end": "766360"
  },
  {
    "text": "uh you don't have to write any extra code uh it is exposed over the rest and and the second one basically you write",
    "start": "766360",
    "end": "772680"
  },
  {
    "text": "the customiz code not only connecting the database then retrieval the part of it then getting the right chunks and",
    "start": "772680",
    "end": "778440"
  },
  {
    "text": "getting the respon back so for us it becomes really easy but like I said we found that challenge that it is very",
    "start": "778440",
    "end": "785160"
  },
  {
    "text": "much working with cannot be works with pan cone so the other systems available probably we can use that as",
    "start": "785160",
    "end": "791399"
  },
  {
    "text": "well and we're going to publish the git repo also we just need to S some cleanup but when we actually publish the PDF we",
    "start": "791399",
    "end": "797959"
  },
  {
    "text": "will have the git repo as well yeah now let's look at some patterns for yeah so",
    "start": "797959",
    "end": "805519"
  },
  {
    "text": "we'll be talking about the patterns for mlops and optimal use of you the accelerators the first one is the mlops",
    "start": "805519",
    "end": "812360"
  },
  {
    "text": "pipeline for rack um you know if you look at this slide this is architecture diagram for the mlops pipeline for rag",
    "start": "812360",
    "end": "818680"
  },
  {
    "text": "where we have a you know serving subsystem which has your front end which",
    "start": "818680",
    "end": "823880"
  },
  {
    "text": "has your responsive AI system which has a inference system which gets connected to your you know uh embedding system via",
    "start": "823880",
    "end": "831279"
  },
  {
    "text": "you know bya Vector database where you have a data injection pipeline going on and from Once the data injection",
    "start": "831279",
    "end": "837519"
  },
  {
    "text": "pipeline goes to the meing system it is able to get the results back as as",
    "start": "837519",
    "end": "843600"
  },
  {
    "text": "mlops now we also took this architecture mlops application and uh tried it with",
    "start": "844240",
    "end": "849360"
  },
  {
    "text": "another platform which has become really popular uh called Ray and we tried that",
    "start": "849360",
    "end": "854880"
  },
  {
    "text": "with the uh deploying Ray on gke and gcp stack uh gke also has a managed operator",
    "start": "854880",
    "end": "861480"
  },
  {
    "text": "for running Ray uh it's a supported operator if you look at the right hand",
    "start": "861480",
    "end": "866720"
  },
  {
    "text": "side here which is the embedded uh embedding subsystem the it starts with the cloud",
    "start": "866720",
    "end": "872040"
  },
  {
    "text": "storage which is your storage for uh data set on which the rag will be trained or rag will be grounded on then",
    "start": "872040",
    "end": "878639"
  },
  {
    "text": "the embedding system is implemented using the rag uh Ray Ray data pipelines for chunking the data Shing the data and",
    "start": "878639",
    "end": "886279"
  },
  {
    "text": "the data is R through from the uh cloud storage bucket and there are some optimizations done over that and the ray",
    "start": "886279",
    "end": "892600"
  },
  {
    "text": "job will be submitted that will embed those chunks using the embedding model",
    "start": "892600",
    "end": "898040"
  },
  {
    "text": "and store the embeddings into PG Vector database on cloud SQL as far as on the",
    "start": "898040",
    "end": "903680"
  },
  {
    "text": "left hand side we have the serving subsystem which is implemented using two layers one is a serving front end layer",
    "start": "903680",
    "end": "909880"
  },
  {
    "text": "that is implemented using Lang chain I think most of you will be familiar with Lang chain it is the most popular open",
    "start": "909880",
    "end": "915360"
  },
  {
    "text": "source framework uh for working with llm applications uh underneath that you have",
    "start": "915360",
    "end": "921000"
  },
  {
    "text": "something called as inference server you can look at inference server for those who are not aware inference servers can",
    "start": "921000",
    "end": "926079"
  },
  {
    "text": "be treated like a runtime and runtime where you run the models right for example you can run uh hugging face TGI",
    "start": "926079",
    "end": "933440"
  },
  {
    "text": "on gke and you can bring in the model that you want to deploy there so this architecture is essentially for the for",
    "start": "933440",
    "end": "939880"
  },
  {
    "text": "those Advanced users that do not want to use any of the manage services that the cloud providers have and you want to run",
    "start": "939880",
    "end": "946279"
  },
  {
    "text": "your own rag pipeline N2 and rag pipeline mlops Pipeline on the uh on the",
    "start": "946279",
    "end": "951720"
  },
  {
    "text": "kubernetes now we also have taken advantage of uh sensitive data protections and some extensions that the",
    "start": "951720",
    "end": "957480"
  },
  {
    "text": "uh gcp has we'll talk talk about that in the patterns later on uh let's take a closer look at the embedding subsystem",
    "start": "957480",
    "end": "964480"
  },
  {
    "text": "on the right hand side you see here there's a ray cluster that is running on gke and it has two parts to it Ray",
    "start": "964480",
    "end": "970480"
  },
  {
    "text": "cluster will typically have two parts one is a head node or head pod and another is a worker pod and uh on the",
    "start": "970480",
    "end": "976920"
  },
  {
    "text": "the below you going have your embedding model that is going to run on on gpus",
    "start": "976920",
    "end": "982279"
  },
  {
    "text": "are going to run on gpus that is also deployed on the potentially the same cluster uh and your grounding data set",
    "start": "982279",
    "end": "988600"
  },
  {
    "text": "is sitting in the GCS bucket the the ray cluster is going to read the data using",
    "start": "988600",
    "end": "994440"
  },
  {
    "text": "something called as GCS fuse we're going to talk about that shortly after this slide but essentially is going to get",
    "start": "994440",
    "end": "1000399"
  },
  {
    "text": "the data from the data set the data set from the GCS bucket uh broke them down",
    "start": "1000399",
    "end": "1005600"
  },
  {
    "text": "into chunks and embed each of the chunks and store that into the vector database",
    "start": "1005600",
    "end": "1011000"
  },
  {
    "text": "the Jupiter pod you see there is just there to kind of initiate that whole process now when you're talking about",
    "start": "1011000",
    "end": "1018639"
  },
  {
    "text": "this embedding right or rag whether it is rag or fine-tuning the model uh in case of rag your grounding data set is",
    "start": "1018639",
    "end": "1025120"
  },
  {
    "text": "going to sit in that uh cloud storage bucket or your object storage or in case of tuning the whole data set massive",
    "start": "1025120",
    "end": "1031798"
  },
  {
    "text": "amount of data sets will be sitting in that in that GCS uh or object storage now when you talk about reading the or",
    "start": "1031799",
    "end": "1039079"
  },
  {
    "text": "training the models based on that you're going to load all that data into the disk and then start the training process",
    "start": "1039079",
    "end": "1044918"
  },
  {
    "text": "or start the fine tuning or embedding process so until the data is loaded that GPU is wasted it's a very expensive",
    "start": "1044919",
    "end": "1052280"
  },
  {
    "text": "resource right GPU tpus or accelerators are extremely expensive resource so how",
    "start": "1052280",
    "end": "1057799"
  },
  {
    "text": "can you make optimal utilization during your temp tuning or embedding process uh you can use something like as GCS fuse",
    "start": "1057799",
    "end": "1064880"
  },
  {
    "text": "where ideally you start you start streaming the data from the object storage and start the training process",
    "start": "1064880",
    "end": "1071320"
  },
  {
    "text": "or tuning process as soon as your data starts arriving and that is something that the feature that uh groundup gcp",
    "start": "1071320",
    "end": "1077919"
  },
  {
    "text": "provides uh it's called GCS uh fuse it gives you the file system semantics for",
    "start": "1077919",
    "end": "1082960"
  },
  {
    "text": "the developers and the data is starts uh streaming and you can start training the job right away and so basically just",
    "start": "1082960",
    "end": "1091320"
  },
  {
    "text": "like M mentioned right GCS FS so it def for any any Cloud platform in in that",
    "start": "1091320",
    "end": "1097280"
  },
  {
    "text": "sense we need a stable and Affordable Storage for any pre-processing any model weights or even for the checkpoints so",
    "start": "1097280",
    "end": "1103760"
  },
  {
    "text": "what cloud uh storage fuse provides basically for the developer as a seamless application that all your",
    "start": "1103760",
    "end": "1109200"
  },
  {
    "text": "buckets that are available it acts like you know they are the file systems and",
    "start": "1109200",
    "end": "1114320"
  },
  {
    "text": "developer does not have to make any changes so all the data that you have in the bucket it it it shows like a like as",
    "start": "1114320",
    "end": "1121159"
  },
  {
    "text": "a file file Mount point to you and the developer just just reads the file the data like like like a file system it",
    "start": "1121159",
    "end": "1128280"
  },
  {
    "text": "doesn't have to make any changes of course it it is a CSI provider a cache for repetitive uh you know reads as well",
    "start": "1128280",
    "end": "1136320"
  },
  {
    "text": "which helps reducing the cost also",
    "start": "1136320",
    "end": "1140720"
  },
  {
    "text": "now the uh uh other part which is this diagram is a little intense so uh I will",
    "start": "1141840",
    "end": "1149159"
  },
  {
    "text": "first of all talk about why this is required it is for advanced users that require massive amount of training",
    "start": "1149159",
    "end": "1155679"
  },
  {
    "text": "capabilities and compute resources so if you're fine tuning the model or training your own model you're going to need a",
    "start": "1155679",
    "end": "1162559"
  },
  {
    "text": "lot of compute nodes with gpus on them and they need to talk to each other they need to pass on those wavs that neural",
    "start": "1162559",
    "end": "1169000"
  },
  {
    "text": "network will span multiple nodes and for that you need a high performance High throughput uh GPU fabric where gpus can",
    "start": "1169000",
    "end": "1176400"
  },
  {
    "text": "talk to each other without network hops without any choke points of or routers in between right and so that's the",
    "start": "1176400",
    "end": "1183960"
  },
  {
    "text": "network on the on the south side which is the network that is used for distributed training and what you get",
    "start": "1183960",
    "end": "1189880"
  },
  {
    "text": "there is an aggregated bandwidth uh on the North side we have another pattern for data access where the CPU needs to",
    "start": "1189880",
    "end": "1198039"
  },
  {
    "text": "access the store storage layer to download the data or store the checkpoints or interact with any other external system on the cloud or outside",
    "start": "1198039",
    "end": "1204720"
  },
  {
    "text": "the cloud now here what you see here with a uh a particular type of VM in gcp",
    "start": "1204720",
    "end": "1211600"
  },
  {
    "text": "and gke that can be you know GK can run on that on that machine type as well uh",
    "start": "1211600",
    "end": "1217280"
  },
  {
    "text": "that shape allows you to have that shape basically has eight gpus on it and each of the GPU connects to a different Nick",
    "start": "1217280",
    "end": "1224000"
  },
  {
    "text": "and those are each of those Nicks are essentially uh uh on different VPC",
    "start": "1224000",
    "end": "1229600"
  },
  {
    "text": "so gpu1 on machine one can talk to GPU one on any of the machines in the cluster without any uh without any",
    "start": "1229600",
    "end": "1237240"
  },
  {
    "text": "hassle also we have another obstruction based on our software defined Network called NCC network connectivity Center",
    "start": "1237240",
    "end": "1244760"
  },
  {
    "text": "that allows you to have cross communication between uh any GPU without having to do peering of the networks so",
    "start": "1244760",
    "end": "1252000"
  },
  {
    "text": "this creates a ability to create massive bandwidth that is required for such training jobs and in gke we allow you to",
    "start": "1252000",
    "end": "1260480"
  },
  {
    "text": "natively take advantage of that by virtue of having uh Native support for multi networking where your pod can",
    "start": "1260480",
    "end": "1267679"
  },
  {
    "text": "actually get access to all the Nicks that are available on that machine so when we talking about training and",
    "start": "1267679",
    "end": "1273320"
  },
  {
    "text": "tuning typically uh GK node or or the worker node where your training takes place that node will not be shared",
    "start": "1273320",
    "end": "1280440"
  },
  {
    "text": "across multiple Parts the entire node will be occupied by a single pod of that training job so the Pod essentially gets",
    "start": "1280440",
    "end": "1287480"
  },
  {
    "text": "access to all those Nicks that are particularly for gpus by the way those all those eight Nicks or eight uh",
    "start": "1287480",
    "end": "1293919"
  },
  {
    "text": "network connections that we talked about those are dedicated for the gpus only and then there's always a default",
    "start": "1293919",
    "end": "1299760"
  },
  {
    "text": "Network that is a traditional Network that the CPU will use to communicate with the rest of the system uh and the",
    "start": "1299760",
    "end": "1306240"
  },
  {
    "text": "Machine type that we that allows you to have this kind of topology with the uh",
    "start": "1306240",
    "end": "1311400"
  },
  {
    "text": "with you know with massive amount of distributed computing system available is called A3 it has 8 gpus 28 vcpus and",
    "start": "1311400",
    "end": "1319120"
  },
  {
    "text": "it has a Nick arrangement of 8 plus one so eight floor gpus one for CPU and",
    "start": "1319120",
    "end": "1324400"
  },
  {
    "text": "essentially uh that gives you the ability to do multi networking so we can have massive GK clusters and we have",
    "start": "1324400",
    "end": "1329720"
  },
  {
    "text": "some customers actually using that today as well so um so when it comes to serving",
    "start": "1329720",
    "end": "1337000"
  },
  {
    "text": "we talked about training and tuning requirements but we also need to talk about the uh serving aspects of it now",
    "start": "1337000",
    "end": "1343200"
  },
  {
    "text": "these container images that we talk about llm are really large they are slow to load now GK has something called a",
    "start": "1343200",
    "end": "1349799"
  },
  {
    "text": "secondary boot dis so you can create a disk image of the container image uh of that container or data that you want to",
    "start": "1349799",
    "end": "1356200"
  },
  {
    "text": "load and uh it becomes available as a secondary boot dis when so the data is",
    "start": "1356200",
    "end": "1361840"
  },
  {
    "text": "readily available through the cache instead of having to load the container image at the time of startup right so",
    "start": "1361840",
    "end": "1368640"
  },
  {
    "text": "and it's essentially available as a single flag and the last side so U we we",
    "start": "1368640",
    "end": "1373840"
  },
  {
    "text": "all know about that we need you know basically your load b as well right and",
    "start": "1373840",
    "end": "1379360"
  },
  {
    "text": "gen apps apps is not a a web application traditional CPU based round robin balancing may not work here so that is",
    "start": "1379360",
    "end": "1385600"
  },
  {
    "text": "where service extension comes in uh service extension not only helps you know reduce unwanted request coming on",
    "start": "1385600",
    "end": "1393120"
  },
  {
    "text": "my uh models and also make sure that if there's a potential data leak that is",
    "start": "1393120",
    "end": "1398240"
  },
  {
    "text": "happening it is able to make sure that does not happen so that is where service exchange comes in and it has a app level",
    "start": "1398240",
    "end": "1404520"
  },
  {
    "text": "custom metric such as QEP reported back that goes to load balancer using the open request that is available and that",
    "start": "1404520",
    "end": "1410400"
  },
  {
    "text": "goes to GK and basically you get the response back by which we are able to save uh unwanted request coming on any",
    "start": "1410400",
    "end": "1417919"
  },
  {
    "text": "kind of uh you know on on the gpus and",
    "start": "1417919",
    "end": "1422200"
  },
  {
    "text": "dpus uh we want to thank uh people who helped us build this material there is a lot of intense technical depth that we",
    "start": "1424320",
    "end": "1431080"
  },
  {
    "text": "covered uh uh in terms of network topologies the load balancing capabilities the storage capabilities",
    "start": "1431080",
    "end": "1437279"
  },
  {
    "text": "Etc I wanted to thank Victor from Google and many other product engineers and managers from Google cloud and similarly",
    "start": "1437279",
    "end": "1443480"
  },
  {
    "text": "for us from public we have a gvp Mohammad the SE and who's been uh they've been instrumented in getting us",
    "start": "1443480",
    "end": "1451120"
  },
  {
    "text": "you know at this stage that's all from our side guys um uh we going to have the",
    "start": "1451120",
    "end": "1457520"
  },
  {
    "text": "same topic covered focused primarily on the gcp aspects of it uh some of the load Balan and stuff you saw the uh",
    "start": "1457520",
    "end": "1464400"
  },
  {
    "text": "extensions service extensions GCS fusees we can have a detail discussion on that if you visit Google Cloud Booth uh and I",
    "start": "1464400",
    "end": "1472320"
  },
  {
    "text": "will uh pause here and see if there are any questions yeah um how are we doing",
    "start": "1472320",
    "end": "1477960"
  },
  {
    "text": "on time I think we're right on time thank you jendra mother uh are",
    "start": "1477960",
    "end": "1485440"
  },
  {
    "text": "there any questions we can take one or two questions there are two mics over here I'll just pass it",
    "start": "1485440",
    "end": "1492279"
  },
  {
    "text": "away hey thank you for the good talk um you had a slide that had the complexity",
    "start": "1492279",
    "end": "1497440"
  },
  {
    "text": "of rag um you know the whole fine tuning and the whole cost versus efficiency right",
    "start": "1497440",
    "end": "1504240"
  },
  {
    "text": "very beginning of your talk um I'm just curious like you know there's a third dimension of accuracy associated with",
    "start": "1504240",
    "end": "1509279"
  },
  {
    "text": "that did you think about like how to like which of these um and how much you need to push for from an accuracy",
    "start": "1509279",
    "end": "1516200"
  },
  {
    "text": "standpoint I didn't get your question accuracy standpoint what do you mean I mean are you saying like you can bring",
    "start": "1516200",
    "end": "1522240"
  },
  {
    "text": "in your data through a rag or a fine tuning or a pre-training or you know",
    "start": "1522240",
    "end": "1527320"
  },
  {
    "text": "whichever the trend that has been going on right I mean if you use a rag right a chat B solution probably you'll not be",
    "start": "1527320",
    "end": "1533080"
  },
  {
    "text": "spending much money but if you start finding your model completely from the scratch not only you add the complexity",
    "start": "1533080",
    "end": "1539080"
  },
  {
    "text": "of it probably you'll be adding a lot of cost GP cost in terms of gpus tpus your Manpower your your Cloud Solutions you",
    "start": "1539080",
    "end": "1546720"
  },
  {
    "text": "know you'll have to do all sort of complex complex work by yourself so this is this is the way we want to show that",
    "start": "1546720",
    "end": "1552919"
  },
  {
    "text": "in case you are using just the prompt enging part probably are easy to use but if you go on the the preing the entire",
    "start": "1552919",
    "end": "1559919"
  },
  {
    "text": "model of your choice probably you'll spending lot of money and you'll be having lot of complexity as",
    "start": "1559919",
    "end": "1566840"
  },
  {
    "text": "well right yeah thanks yeah I was just curious about like if you have tried any of the others and compared that with you",
    "start": "1566840",
    "end": "1573640"
  },
  {
    "text": "know is rag been better for you in terms of uh you know accur super R versus fine",
    "start": "1573640",
    "end": "1579399"
  },
  {
    "text": "tuning or anything okay yeah yeah I think uh rag essentially provides a",
    "start": "1579399",
    "end": "1585120"
  },
  {
    "text": "grounding aspects but if you want to create uh supervis tuning then you uh essentially are looking for something",
    "start": "1585120",
    "end": "1591159"
  },
  {
    "text": "where you want to create a task specific fine tune model such as as shown here where it is trained for a particular",
    "start": "1591159",
    "end": "1597320"
  },
  {
    "text": "task and then you have to go still have to go through a tremendous amount of work to label that data uh you can't",
    "start": "1597320",
    "end": "1604159"
  },
  {
    "text": "just dump the data said hey this is my data and then you use it as a grounding base but here there there is a task",
    "start": "1604159",
    "end": "1610640"
  },
  {
    "text": "involved where you actually start preparing the data set with your own labels uh that typically you have to",
    "start": "1610640",
    "end": "1616399"
  },
  {
    "text": "take care of so find you invol involves that additional work other than the architecture and the compute cost as",
    "start": "1616399",
    "end": "1622559"
  },
  {
    "text": "well so you have to look at the cost benefit analysis of f tuning or supervis fine tuning versus rag what provides",
    "start": "1622559",
    "end": "1629880"
  },
  {
    "text": "more value uh compared to the cost that you have to invest in to get something like this going did that answer the",
    "start": "1629880",
    "end": "1636799"
  },
  {
    "text": "question thanks but that's a great question for sure thank you any more one one more question yeah I think there's",
    "start": "1636799",
    "end": "1643159"
  },
  {
    "text": "uh there's a mic just next to you",
    "start": "1643159",
    "end": "1648200"
  },
  {
    "text": "yeah so my question is on the same slide uh that you showed last um and uh my question is around uh",
    "start": "1650880",
    "end": "1657200"
  },
  {
    "text": "if you would sort of um go down the uh the training methodologies all the way",
    "start": "1657200",
    "end": "1662799"
  },
  {
    "text": "from model training to prompt right if you would go down that stack my understanding is that the uh that the",
    "start": "1662799",
    "end": "1669360"
  },
  {
    "text": "Quantum of gpus that you need to is sort of drastically drops whereas in this uh",
    "start": "1669360",
    "end": "1675039"
  },
  {
    "text": "presentation I heard you speaking about gpus and tpus if you're doing Rag and PE which is the bottommost corner what is",
    "start": "1675039",
    "end": "1682760"
  },
  {
    "text": "the exact necessity for gpus there I mean and and and secondly if you were to use gpus in",
    "start": "1682760",
    "end": "1688440"
  },
  {
    "text": "tpus what how much additional performance are you exactly unlocking because because yeah pre-training full",
    "start": "1688440",
    "end": "1696720"
  },
  {
    "text": "fine tuning you know maybe you need them but but for Rag and prompt why do you need gpus thank you um well to serve the",
    "start": "1696720",
    "end": "1705080"
  },
  {
    "text": "model you still need gpus rag also you many need gpus to run the embedding uh",
    "start": "1705080",
    "end": "1710480"
  },
  {
    "text": "the the the embedding uh model language models to translate the text into the",
    "start": "1710480",
    "end": "1715919"
  },
  {
    "text": "vectors that you're going to use so but if you're using the LM as just an API",
    "start": "1715919",
    "end": "1721519"
  },
  {
    "text": "then you don't need to host gpus yourself but if you're hosting the model yourself then you need the gpus to run",
    "start": "1721519",
    "end": "1729080"
  },
  {
    "text": "uh that model somewhere so if you take take a look at this architecture uh till the time he shows",
    "start": "1729080",
    "end": "1735799"
  },
  {
    "text": "to be very honest it all depends what kind of data you have have right what kind how much data you have if you're",
    "start": "1735799",
    "end": "1740840"
  },
  {
    "text": "talking about a data in in in in some ptus and all those things probably you would need a better systems right you",
    "start": "1740840",
    "end": "1747120"
  },
  {
    "text": "can't run that on the CPU side so it it all depends on the kind of data that you want to to find you and to see what kind",
    "start": "1747120",
    "end": "1754440"
  },
  {
    "text": "of you know what kind of supervised data or unsupervised data that you may have where you will need such systems for",
    "start": "1754440",
    "end": "1760480"
  },
  {
    "text": "every if I'm doing some PC on my local probably I not need it but if I have an organization like XYZ company who's got",
    "start": "1760480",
    "end": "1767720"
  },
  {
    "text": "some you know some some some data which is like which is unbelievable right then",
    "start": "1767720",
    "end": "1773519"
  },
  {
    "text": "of course I would need those systems available and that is where the cost adds by itself right and our our thought",
    "start": "1773519",
    "end": "1779159"
  },
  {
    "text": "process is not on the P side but a generic framework that comes out of it does that make sense perfect uh yeah",
    "start": "1779159",
    "end": "1788600"
  },
  {
    "text": "I think I think uh we're on time now uh thank you for this talk um we now have",
    "start": "1788600",
    "end": "1794960"
  },
  {
    "text": "like a 15 minutes break so you're free to get some coffee things like that will start at 3:20 over here and meanwhile I",
    "start": "1794960",
    "end": "1802360"
  },
  {
    "text": "would request the next speaker to come and meet us that we can get miked up thank you so much guys You' been a",
    "start": "1802360",
    "end": "1808600"
  },
  {
    "text": "lovely",
    "start": "1808600",
    "end": "1811600"
  }
]