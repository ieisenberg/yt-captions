[
  {
    "text": "Welcome to our talk we're going to be covering um Advanced model serving techniques with uh Ray and kubernetes um",
    "start": "40",
    "end": "5920"
  },
  {
    "text": "so yeah uh let's start with some quick intros I'm Andrew Sim I'm a software engineer at Google working on um gke and",
    "start": "5920",
    "end": "12320"
  },
  {
    "text": "I've been um contributing to maintaining to uh the kubernetes project for several",
    "start": "12320",
    "end": "17600"
  },
  {
    "text": "years and in various areas um and more recently I've been um uh maintaining the cuate project with kaisen and helping um",
    "start": "17600",
    "end": "24680"
  },
  {
    "text": "Google Cloud customers uh build modern machine learning platforms right yeah uh",
    "start": "24680",
    "end": "30160"
  },
  {
    "text": "hi everyone my name is kin Chan I am a software engineer in N scale N scale is a creator of Ray and I'm in a rord te",
    "start": "30160",
    "end": "37600"
  },
  {
    "text": "and I'm maintain cubre project and contribute to like a Rec paragraphs and sound record",
    "start": "37600",
    "end": "44520"
  },
  {
    "text": "stuff I think oh yeah uh I think AI is all around you that in today's life so",
    "start": "46000",
    "end": "53879"
  },
  {
    "text": "uh when you te and Uber browse Tik Tok listen to music on Spotify discuss current events on r and watch video on",
    "start": "53879",
    "end": "61680"
  },
  {
    "text": "Netflix you all have already intera with the AI build using Ray so for scanting",
    "start": "61680",
    "end": "67000"
  },
  {
    "text": "AI workload Ray is almost everywhere yeah so what is Ray uh when",
    "start": "67000",
    "end": "73880"
  },
  {
    "text": "we start to introduce Ray we should start from the fundamental uh the record uh when we program on a laptop we",
    "start": "73880",
    "end": "81479"
  },
  {
    "text": "need to the program consist of the three elements you need to Define function you need to design a class and then you need",
    "start": "81479",
    "end": "87799"
  },
  {
    "text": "to Define a variable and the R cor provides the distributed API that is one",
    "start": "87799",
    "end": "93680"
  },
  {
    "text": "and one mapping with the three elements the red test is the remote function and",
    "start": "93680",
    "end": "98759"
  },
  {
    "text": "re actor is a remote class and the ray object is the remote variable uh which",
    "start": "98759",
    "end": "104159"
  },
  {
    "text": "means that when you submit a ray application across the cluster it will launch this class a function and a",
    "start": "104159",
    "end": "110880"
  },
  {
    "text": "variable across the cluster and they can interact with each other and you don't need to worry uh which uh where is this",
    "start": "110880",
    "end": "118399"
  },
  {
    "text": "like a function class variable is running on which note yeah so this means that it's pretty easy to convert your",
    "start": "118399",
    "end": "125799"
  },
  {
    "text": "Python program into a distributed program because the pattern is PR very similar and the record goal is to make",
    "start": "125799",
    "end": "132840"
  },
  {
    "text": "users programing a distribut system as if they were working on their laptop yeah so the record toen is like the",
    "start": "132840",
    "end": "139879"
  },
  {
    "text": "infinite laptop yeah and uh uh this is a very",
    "start": "139879",
    "end": "145840"
  },
  {
    "text": "simple record example uh first uh you just you need to add an annotation on",
    "start": "145840",
    "end": "151640"
  },
  {
    "text": "the function uh so to declare that this function is a red task and then you call",
    "start": "151640",
    "end": "157560"
  },
  {
    "text": "the f. remote and it will create a red T in the in the rain node and it will",
    "start": "157560",
    "end": "163959"
  },
  {
    "text": "return a nonblocking future uh finally you can Cod the R.G to get the final",
    "start": "163959",
    "end": "170040"
  },
  {
    "text": "results from the cluster and you can see like it's a pretty simple example and you can paralyze the the F this two red",
    "start": "170040",
    "end": "177920"
  },
  {
    "text": "T together yeah so now that we understand what rord",
    "start": "177920",
    "end": "184720"
  },
  {
    "text": "is and this slide provides some high overview of Ray architecture and expans",
    "start": "184720",
    "end": "190319"
  },
  {
    "text": "why it is popular in M infrastructure uh first uh rayor is the fundamental uh",
    "start": "190319",
    "end": "196840"
  },
  {
    "text": "component of Ray uh building on Ray core we build the ray Community builds several AI libraries uh including like a",
    "start": "196840",
    "end": "203360"
  },
  {
    "text": "ray data rra r r and Reserve to cover different worklow to cover end to end",
    "start": "203360",
    "end": "209840"
  },
  {
    "text": "model life cycle and then the community also offers the two deployment Solutions one is for",
    "start": "209840",
    "end": "216360"
  },
  {
    "text": "kubernetes and the other one is for the virtual machines and the official solution for",
    "start": "216360",
    "end": "222200"
  },
  {
    "text": "ran kubernetes in the open source Community is a CRA yeah and then uh why Ray is so",
    "start": "222200",
    "end": "229519"
  },
  {
    "text": "popular in the ml infrastructure I think the first point is that the record API is pretty easy to",
    "start": "229519",
    "end": "235599"
  },
  {
    "text": "use uh user can paralyze workload with a few lives of cod change right if you",
    "start": "235599",
    "end": "240680"
  },
  {
    "text": "guys have experience about like the meru you need to rewrite the program in the M function and reduce function which is uh",
    "start": "240680",
    "end": "247599"
  },
  {
    "text": "pretty different from the your your program in the laptop and the second is that a ray is a",
    "start": "247599",
    "end": "255920"
  },
  {
    "text": "race supports the hog genus Computing resource like the gpus from the different vendors like the TPU like the",
    "start": "255920",
    "end": "261919"
  },
  {
    "text": "a neuron CHP like MP for Huawei and the many many different trips and the",
    "start": "261919",
    "end": "267120"
  },
  {
    "text": "heterogeneous computer resource is become more and more important especially for like a training and the B",
    "start": "267120",
    "end": "274520"
  },
  {
    "text": "inference yeah and the third one is that uh R core is a very low level API and",
    "start": "274520",
    "end": "280080"
  },
  {
    "text": "it's very flexible and the rake also build several AI libr to cover Inn life cycle so you can just use it to cover",
    "start": "280080",
    "end": "287280"
  },
  {
    "text": "the ENT life cycle and it's different workload have the different system requirement for example like a serving",
    "start": "287280",
    "end": "292960"
  },
  {
    "text": "you require the auto scaling and something like the higher ability and for training maybe you need to support",
    "start": "292960",
    "end": "299120"
  },
  {
    "text": "the G SC in and for a battery you need to support hog genius Computing resource",
    "start": "299120",
    "end": "304160"
  },
  {
    "text": "and you need to support like a streaming and R is very flexible so it's support all of",
    "start": "304160",
    "end": "310280"
  },
  {
    "text": "them yeah and the last one is that with the support of the with the support of the kubernetes and a virtual machine you",
    "start": "310280",
    "end": "317400"
  },
  {
    "text": "can almost deploy Ray on everywhere yeah and the next is that",
    "start": "317400",
    "end": "323600"
  },
  {
    "text": "Andrew will will expand the rean kubernetes and expand what is the model",
    "start": "323600",
    "end": "328919"
  },
  {
    "text": "inference and why model inference is important yeah so when when it comes to",
    "start": "328919",
    "end": "334080"
  },
  {
    "text": "deploying Ray on kubernetes um uh we have CU so CU is a open- source uh",
    "start": "334080",
    "end": "339639"
  },
  {
    "text": "kubernetes operator for Ray uh it provides declarative um uh apis or or",
    "start": "339639",
    "end": "345080"
  },
  {
    "text": "crds for managing Ray clusters jobs and serving applications for inference um",
    "start": "345080",
    "end": "350160"
  },
  {
    "text": "CRA is is specifically a powerful tool because it helps bridge the gap between um the infrastructure and platform",
    "start": "350160",
    "end": "355800"
  },
  {
    "text": "Engineers who very familiar with kubernetes with the data scientists machine learning resear researchers who don't want to deal with like all the",
    "start": "355800",
    "end": "361720"
  },
  {
    "text": "kubernetes yamoo and just want to focus on their research uh which is mainly done in in a python environment um so",
    "start": "361720",
    "end": "367160"
  },
  {
    "text": "yeah in our talk we're going to cover how Ray enables online inference and offline inference and we'll also cover",
    "start": "367160",
    "end": "372680"
  },
  {
    "text": "how cuay kind of enables this in a reliable and production ready um way so yeah let's start with um online",
    "start": "372680",
    "end": "379479"
  },
  {
    "text": "inference so online inference refers to the process of generating um output tokens or predictions from a machine learning model in in real time so the",
    "start": "379479",
    "end": "386880"
  },
  {
    "text": "common example that everyone's familiar with is your llms right your GPT Gemini llama and Claud so serving all these",
    "start": "386880",
    "end": "392919"
  },
  {
    "text": "types of llms is an example of of online inference um then we have offline inference or often called batch",
    "start": "392919",
    "end": "399000"
  },
  {
    "text": "inference and as a name implies batch inference involves generating predictions using larger inputs and",
    "start": "399000",
    "end": "404240"
  },
  {
    "text": "often doing this in a in a periodic manner um so the the and and the output of offline um inference is often",
    "start": "404240",
    "end": "410520"
  },
  {
    "text": "persisted somewhere for reference later um and so this is different from online inference where you're generating",
    "start": "410520",
    "end": "416039"
  },
  {
    "text": "predictions and giving giving it to users in real time um so often with um batch inference we care more about uh",
    "start": "416039",
    "end": "423840"
  },
  {
    "text": "the throughput and the cost of inference rather than uh compared to online inference where we actually care a lot",
    "start": "423840",
    "end": "429479"
  },
  {
    "text": "about the latency of the of inference um some examples of bat inference can be uh generating Vector",
    "start": "429479",
    "end": "436280"
  },
  {
    "text": "embeddings for rag where you take a bunch of text documents uh containing important contexts and then you store it",
    "start": "436280",
    "end": "442120"
  },
  {
    "text": "into a vector database for contact retrieval later when you do inference real- time inference through llm or",
    "start": "442120",
    "end": "447680"
  },
  {
    "text": "let's say you want to build uh you know like a recommendation system for your website um it doesn't always make sense",
    "start": "447680",
    "end": "452800"
  },
  {
    "text": "to try to predict new recommendations in real time but rather you'd run a periodic batch job that generates",
    "start": "452800",
    "end": "458319"
  },
  {
    "text": "recommendations and then you would cach that for later use uh part of your",
    "start": "458319",
    "end": "463360"
  },
  {
    "text": "application so yeah let's dive a little deeper into online inference um so uh online inference today has a a few",
    "start": "463360",
    "end": "469240"
  },
  {
    "text": "common challenges uh the first is uh deployment complexity right so models today are becoming larger which also",
    "start": "469240",
    "end": "476039"
  },
  {
    "text": "means that they run on more expensive and and larger gpus and these models also take um longer to start up",
    "start": "476039",
    "end": "482680"
  },
  {
    "text": "especially if you're pulling the model weights from like a a remote bucket um in addition you often have to",
    "start": "482680",
    "end": "489960"
  },
  {
    "text": "orchestrate the deployment of multiple models um together um to achieve the",
    "start": "489960",
    "end": "495240"
  },
  {
    "text": "outcome of what you want out of your machine learning platform and then putting this all together it's really",
    "start": "495240",
    "end": "500400"
  },
  {
    "text": "important to enable really fast prototyping and iteration which is really important for you know the",
    "start": "500400",
    "end": "505560"
  },
  {
    "text": "researchers who want to iterate quickly on on on the output of their models uh and then there's also a framework",
    "start": "505560",
    "end": "512000"
  },
  {
    "text": "complexity where different researchers may have preferences in the underlying framework um and that changes how you end up serving the models in production",
    "start": "512000",
    "end": "519000"
  },
  {
    "text": "and then lastly it's um today it's challenging to fully optimize the performance and utilization and cost of",
    "start": "519000",
    "end": "524959"
  },
  {
    "text": "running the latest and greatest models um especially with you know really expensive Hardware accelerators like",
    "start": "524959",
    "end": "531680"
  },
  {
    "text": "gpus so this is where Ray serve kind of comes to the picture and it tries to um address many of these challenges so Ray",
    "start": "531680",
    "end": "537880"
  },
  {
    "text": "serve is a scalable model serving Library within the ray project for building online online inference apis uh",
    "start": "537880",
    "end": "543760"
  },
  {
    "text": "it enables uh fast prototyping um much like Ray start really easy to start on",
    "start": "543760",
    "end": "549040"
  },
  {
    "text": "your laptop and then you know distribute the serving application to distribute a cluster on the cloud and um it's all",
    "start": "549040",
    "end": "555720"
  },
  {
    "text": "done with python API so it's familiar with to AI researchers and then lastly it comes with a bunch of advanced",
    "start": "555720",
    "end": "561760"
  },
  {
    "text": "capabilities um that are important when you want to actually stitch together the outputs of multiple models um together",
    "start": "561760",
    "end": "567440"
  },
  {
    "text": "which we'll cover in more details later so here's a really short um simple Cod",
    "start": "567440",
    "end": "573120"
  },
  {
    "text": "snippet of how Ray serve is used you take your model code you add a serve. deployment decorator and now your model",
    "start": "573120",
    "end": "578760"
  },
  {
    "text": "is considered a serve deployment um then you can call the model with the serve. Run um add a route prefix if you want",
    "start": "578760",
    "end": "585959"
  },
  {
    "text": "and just like that you have a very simple API endpoint for a model uh and in this example we have a sentiment",
    "start": "585959",
    "end": "591920"
  },
  {
    "text": "analysis model that takes some text so in this example says Ray serve is great with the exclamation point uh and then",
    "start": "591920",
    "end": "597560"
  },
  {
    "text": "it returns uh a label which in this case is positive and a score of",
    "start": "597560",
    "end": "602839"
  },
  {
    "text": "99 uh so yeah racer makes it easy to scale out to multimodel deployments so",
    "start": "602839",
    "end": "608680"
  },
  {
    "text": "specifically here we have an example it's called Model multiplexing where you can deploy multiple models uh together",
    "start": "608680",
    "end": "614000"
  },
  {
    "text": "and then Multiplex traffic to different models based on um n ID um so one use",
    "start": "614000",
    "end": "619680"
  },
  {
    "text": "case of multi multiplexing could be that you run multiple models of varying sizes and you want kind of finer grain control",
    "start": "619680",
    "end": "626440"
  },
  {
    "text": "of uh Which models to use based on the type of question or the complexity of the question um or if you just want to",
    "start": "626440",
    "end": "632480"
  },
  {
    "text": "experiment with the outputs of different models you can use um this",
    "start": "632480",
    "end": "637560"
  },
  {
    "text": "capability and yeah here's just another example it's really simple you just add a python decorator and that does the",
    "start": "637560",
    "end": "644200"
  },
  {
    "text": "multiplexing logic for you and then you can pick um the models to download and serve as part of your uh rer",
    "start": "644200",
    "end": "652079"
  },
  {
    "text": "application uh racer also supports uh model composition um and this",
    "start": "652079",
    "end": "657240"
  },
  {
    "text": "effectively allows you to chain um the input an output of of models together um",
    "start": "657240",
    "end": "662920"
  },
  {
    "text": "we actually St on time so I'm going to skip some stuff here so yeah this is another code example um where you can kind of change the output of two serve",
    "start": "662920",
    "end": "668440"
  },
  {
    "text": "deployments together um and of course yeah none of this um is actually possible without some sort of",
    "start": "668440",
    "end": "674720"
  },
  {
    "text": "orchestration of the ray cluster and the serve application and so qra supports a custom resource called Ray service that",
    "start": "674720",
    "end": "680800"
  },
  {
    "text": "lets you bundle together um your serve application and your ray cluster um and this is a example we have in one of our",
    "start": "680800",
    "end": "686880"
  },
  {
    "text": "guides so you know um it's a public so you can kind of try it out yourself um and then yeah this is kind",
    "start": "686880",
    "end": "693639"
  },
  {
    "text": "of an example using VM to um serve um any really any model that you can",
    "start": "693639",
    "end": "700279"
  },
  {
    "text": "download from hugging face right so the important part of this example is on the right there is this argument called",
    "start": "700279",
    "end": "705440"
  },
  {
    "text": "workers use Ray um and that is the default in VM and this is important because VM uses Ray as the backend for",
    "start": "705440",
    "end": "712720"
  },
  {
    "text": "distributed serving so it's a lot easier to get started with BLM when you have an array cluster already created by um",
    "start": "712720",
    "end": "721000"
  },
  {
    "text": "keybr and then lastly a key benefit of race serve is that it kind of bridges the gap between model inference and your",
    "start": "721000",
    "end": "727279"
  },
  {
    "text": "own business logic so we have a end user example from samsara and they were able to reduce their machine learning",
    "start": "727279",
    "end": "733320"
  },
  {
    "text": "inference cost by 50% because Ray serve allowed them to uh consolidate various",
    "start": "733320",
    "end": "739000"
  },
  {
    "text": "components together into a single Ray serve application where the business logic and the model inference are kind",
    "start": "739000",
    "end": "744160"
  },
  {
    "text": "of sitting closer together and deployed together in a single rate cluster and and yeah next question we'll",
    "start": "744160",
    "end": "750160"
  },
  {
    "text": "cover offline inference yeah thanks Andrew yeah uh in this session I will talk about offline inference with",
    "start": "750160",
    "end": "757519"
  },
  {
    "text": "r yeah uh I think the the challenge with the offl inference is that the first is",
    "start": "757519",
    "end": "764399"
  },
  {
    "text": "that it's pretty common for a battery inference workload to consist of both CPU and GPU task for example like use",
    "start": "764399",
    "end": "771399"
  },
  {
    "text": "CPU to do the io and use the CPU to do the pre-processing and use the GPU to do",
    "start": "771399",
    "end": "776839"
  },
  {
    "text": "the model inference or second uh CPU test such as pre-processing uh I think",
    "start": "776839",
    "end": "782240"
  },
  {
    "text": "it's common to generate like a very large intermediate results for example like image decoding May generate like a",
    "start": "782240",
    "end": "788399"
  },
  {
    "text": "10x larger result compar with the input and the third is that offline inference",
    "start": "788399",
    "end": "794760"
  },
  {
    "text": "is typically need to support parison across multiple nodes for example like we observe the pattern that of loading",
    "start": "794760",
    "end": "801639"
  },
  {
    "text": "the CPU intensive task to the CPU NOS it's a pretty common pattern for this",
    "start": "801639",
    "end": "806920"
  },
  {
    "text": "kind of heterogeneous computation it will improve the super a lot and set a lot of",
    "start": "806920",
    "end": "814120"
  },
  {
    "text": "cost yeah and this is a typical uh B inference jav flow you can see like the",
    "start": "817240",
    "end": "823760"
  },
  {
    "text": "the read pre-process and the SE are the CPU intensive test and the inference",
    "start": "823760",
    "end": "829399"
  },
  {
    "text": "model inference is a GPU intensive testest and then we take a closer look at the pre-process S uh for example if",
    "start": "829399",
    "end": "837240"
  },
  {
    "text": "this is a a DE decode like the compressed image of or videos and then",
    "start": "837240",
    "end": "843920"
  },
  {
    "text": "it will generate like the very large intermediate results uh like the jetpack is like a compression ratio is about",
    "start": "843920",
    "end": "850839"
  },
  {
    "text": "like a 10x and for some video format is maybe around the 2000x yeah so to solve this uh very",
    "start": "850839",
    "end": "858680"
  },
  {
    "text": "large intermediate result uh we have two solution the first one is that the N FL",
    "start": "858680",
    "end": "864480"
  },
  {
    "text": "is that write the intermediate result to the C Storage or dis",
    "start": "864480",
    "end": "869880"
  },
  {
    "text": "however it will add like a minutes of the overhead and the second is the better solution is the streaming the",
    "start": "869880",
    "end": "876800"
  },
  {
    "text": "intermedia data through the class memory to reduce the overhead writing to the cloud storage uh however there are some",
    "start": "876800",
    "end": "884279"
  },
  {
    "text": "issue for the streaming uh for example for the book uh",
    "start": "884279",
    "end": "889440"
  },
  {
    "text": "synchronized a parallel framework such as like the mer and Spark It's hard to",
    "start": "889440",
    "end": "895120"
  },
  {
    "text": "support the streaming uh because is that uh first is that a stage like in a spark",
    "start": "895120",
    "end": "900480"
  },
  {
    "text": "cannot consist of both CPU and GPU will close at the same time so it need to",
    "start": "900480",
    "end": "906480"
  },
  {
    "text": "spit into a different stage and the second part is that this kind of framework like a spark it cannot support",
    "start": "906480",
    "end": "913079"
  },
  {
    "text": "a streaming between the different stage a staging must be start after it is dependency uh finish the job so uh",
    "start": "913079",
    "end": "921120"
  },
  {
    "text": "that's the issue that in this case if is a hetrogeneous uh workload it must need",
    "start": "921120",
    "end": "926519"
  },
  {
    "text": "to write the large intermediate result into a",
    "start": "926519",
    "end": "930959"
  },
  {
    "text": "yeah and Ray data Ray data is the ray Community uh battery inference solution",
    "start": "933399",
    "end": "938880"
  },
  {
    "text": "it provides the streaming execution to avoid with the streaming execution you don't need to write the intermediate",
    "start": "938880",
    "end": "945279"
  },
  {
    "text": "result into the dis or cloud storage so it's set a lot of the",
    "start": "945279",
    "end": "952040"
  },
  {
    "text": "overhead yeah so uh we finished the first uh first challenge is the hogen",
    "start": "952639",
    "end": "959759"
  },
  {
    "text": "and the second part is for the model Shing uh why model Shing becomes more",
    "start": "959759",
    "end": "965000"
  },
  {
    "text": "and more important uh the first part that the model becomes uh more and more",
    "start": "965000",
    "end": "970680"
  },
  {
    "text": "larger uh for example like it's pretty hard for a single GPU node to to serve a",
    "start": "970680",
    "end": "976639"
  },
  {
    "text": "model like the Llama 3.1 of 405b because if you use like fp16 it",
    "start": "976639",
    "end": "983279"
  },
  {
    "text": "still require 800 uh gab of the GPU memory and the",
    "start": "983279",
    "end": "989199"
  },
  {
    "text": "second is that for battery inference a c is often uh more important than the leny",
    "start": "989199",
    "end": "996160"
  },
  {
    "text": "and the model Shing unlock the opportunity to use the lowend GPU to set the",
    "start": "996160",
    "end": "1003040"
  },
  {
    "text": "cast yeah uh this is for some background knowledge if you are not familiar with",
    "start": "1005600",
    "end": "1010839"
  },
  {
    "text": "like a model sharing strategy like the PIP person and the test paron you can",
    "start": "1010839",
    "end": "1016199"
  },
  {
    "text": "see like the PIP paron is that speed model B on the layer so you can see like",
    "start": "1016199",
    "end": "1021319"
  },
  {
    "text": "in this example uh the layer Z and the layer one puts on the GP D and Layer Two and layer three puts on the",
    "start": "1021319",
    "end": "1028520"
  },
  {
    "text": "gpu1 and for the tensor parison it SP the weight in the stand layer across",
    "start": "1028520",
    "end": "1033959"
  },
  {
    "text": "multiple different GPU so in this example you can see like the the 50% of",
    "start": "1033959",
    "end": "1039000"
  },
  {
    "text": "the weight for the layer D put on the GP Dr and the other 50% of the weights puts",
    "start": "1039000",
    "end": "1046000"
  },
  {
    "text": "on the gp1 yeah these are the two popular model shouting",
    "start": "1046000",
    "end": "1052440"
  },
  {
    "text": "strategies yeah and the P parison also brought some new challenges for example",
    "start": "1052440",
    "end": "1060000"
  },
  {
    "text": "uh because like a a pip because a different layer in the model have a",
    "start": "1060000",
    "end": "1065160"
  },
  {
    "text": "different size so for example if the lay L is larger it's will occupy a single",
    "start": "1065160",
    "end": "1071240"
  },
  {
    "text": "GPU D and the layer one and Layer Two are small so both of them that put on",
    "start": "1071240",
    "end": "1077480"
  },
  {
    "text": "the same GPU one and uh so different stage have a different layers and uh have a different",
    "start": "1077480",
    "end": "1085360"
  },
  {
    "text": "computation requirements uh which means that and this is pretty hard to predict",
    "start": "1085360",
    "end": "1090400"
  },
  {
    "text": "before we actually run the program so the elastic Auto auto scaling is pretty",
    "start": "1090400",
    "end": "1098519"
  },
  {
    "text": "important yeah and the R Auto scaling is pretty fible it support Auto scanning in",
    "start": "1098880",
    "end": "1104360"
  },
  {
    "text": "the actor and the reactor and the ray uh uh it is support Auto scaning like in",
    "start": "1104360",
    "end": "1110880"
  },
  {
    "text": "the reactor level so which means that it can schedule it can Auto scaning based on the process so it can pretty easy to",
    "start": "1110880",
    "end": "1118600"
  },
  {
    "text": "support like the auto scanning in the uh stage in independently so which means",
    "start": "1118600",
    "end": "1123960"
  },
  {
    "text": "that if the if a stage is faster than the other pipeline stage uh it can scale",
    "start": "1123960",
    "end": "1129360"
  },
  {
    "text": "down and redas a resource for the slower stage and this is hard to support in",
    "start": "1129360",
    "end": "1134919"
  },
  {
    "text": "like a like a b synchronous parallel frame work such as sparkk because See",
    "start": "1134919",
    "end": "1140799"
  },
  {
    "text": "Spark Auto scanning support doesn't support the stage label Auto",
    "start": "1140799",
    "end": "1146799"
  },
  {
    "text": "scanting yeah and in the CU Community is also offer the other crd and the r drop",
    "start": "1148200",
    "end": "1154520"
  },
  {
    "text": "CD is to help to productionize the B work clows and the r CD is equal to the",
    "start": "1154520",
    "end": "1160679"
  },
  {
    "text": "r cluster crd plus KU job it will provision a r cluster and submit the r",
    "start": "1160679",
    "end": "1166960"
  },
  {
    "text": "job through a cluster and then then it will monitor the job status and the auto",
    "start": "1166960",
    "end": "1172120"
  },
  {
    "text": "automatically clean up the computer resource after it finished and then the underlying uh underlying rast also",
    "start": "1172120",
    "end": "1179559"
  },
  {
    "text": "support the auto scaling then uh the r jop also support the advanc scheding with Q volcano and unicor some popular",
    "start": "1179559",
    "end": "1187720"
  },
  {
    "text": "uh schedules yeah this is just something",
    "start": "1187720",
    "end": "1194320"
  },
  {
    "text": "like it will automatically clean up resource",
    "start": "1194320",
    "end": "1199360"
  },
  {
    "text": "yeah and there are some uh end user story uh like eBay eBay gave a",
    "start": "1199360",
    "end": "1204440"
  },
  {
    "text": "presentation in the r 2024 about their B INF story uh this is this workload is a",
    "start": "1204440",
    "end": "1212080"
  },
  {
    "text": "uh image uh in beding Generations they they use R to put the pre-process on the",
    "start": "1212080",
    "end": "1218440"
  },
  {
    "text": "CPU noes and the the embeding generation GP inference on the GPU noes and uh it's",
    "start": "1218440",
    "end": "1224559"
  },
  {
    "text": "increase the GPU station by 4X with a r data streaming execution and the ray",
    "start": "1224559",
    "end": "1229720"
  },
  {
    "text": "fractional gpus and the Ray and the CU Auto scalling yeah and the second and user",
    "start": "1229720",
    "end": "1237000"
  },
  {
    "text": "story is about bance like a bance has a published engineering block about how do",
    "start": "1237000",
    "end": "1242440"
  },
  {
    "text": "they use R data to do the B inference with their 200 terab data um multimodel",
    "start": "1242440",
    "end": "1249280"
  },
  {
    "text": "L model inference yeah and the the next session",
    "start": "1249280",
    "end": "1256400"
  },
  {
    "text": "is that I will introduce the r comp graph uh I think this is the new announcement",
    "start": "1256400",
    "end": "1261600"
  },
  {
    "text": "in the red 2024 it's a and this is this API is currently in the alpha stat so we",
    "start": "1261600",
    "end": "1267960"
  },
  {
    "text": "come for the feedback yeah uh as in one sentence is that Rec comp par graph are 10 to 20x",
    "start": "1267960",
    "end": "1275200"
  },
  {
    "text": "faster than the red Tas and also support GPU native communication such as niiko",
    "start": "1275200",
    "end": "1280760"
  },
  {
    "text": "for static T Test graphs yeah so to understand the",
    "start": "1280760",
    "end": "1286720"
  },
  {
    "text": "optimation we need to understand how the the default execution for record a",
    "start": "1286720",
    "end": "1292520"
  },
  {
    "text": "record very encourag the dynamic execution so you can see in this example",
    "start": "1292520",
    "end": "1299000"
  },
  {
    "text": "uh first is that we create two reactor A and",
    "start": "1299000",
    "end": "1304919"
  },
  {
    "text": "B and then uh and then we want to pass the hello the this string to the actor a",
    "start": "1304919",
    "end": "1310960"
  },
  {
    "text": "and uh pass the output of the extra a to the exra B so the first the driver",
    "start": "1310960",
    "end": "1317200"
  },
  {
    "text": "writes the hello into the share memory and then send the RPC to the actor a to",
    "start": "1317200",
    "end": "1323480"
  },
  {
    "text": "tell where is the input argument so the actor a read the input then after the",
    "start": "1323480",
    "end": "1330480"
  },
  {
    "text": "actor a finish the computation it writes the output to the share memory and then",
    "start": "1330480",
    "end": "1336520"
  },
  {
    "text": "the actor a send an RPC back to the driver to tell where is the",
    "start": "1336520",
    "end": "1341559"
  },
  {
    "text": "output and then because the output need to pass to the actor B so driver send another RPC to the actor B to tell where",
    "start": "1341559",
    "end": "1348559"
  },
  {
    "text": "is the input argument so it read the input so on so forth and then finally we call read get to get the final result so",
    "start": "1348559",
    "end": "1357320"
  },
  {
    "text": "I think the detail is not that important the important part is that you can see that RPC is a bottle neck there are a",
    "start": "1357320",
    "end": "1362559"
  },
  {
    "text": "lot of RPC yeah so to summarize the previous the default execution of",
    "start": "1362559",
    "end": "1369000"
  },
  {
    "text": "record uh the first is that uh we need to allocate the memory for each each",
    "start": "1369000",
    "end": "1375480"
  },
  {
    "text": "execution and the second is that the driver need to tells the redas where is the input",
    "start": "1375480",
    "end": "1381400"
  },
  {
    "text": "argument and the third one is that the redas need to send back to the driver",
    "start": "1381400",
    "end": "1387080"
  },
  {
    "text": "where is the output so what is the ideal situation",
    "start": "1387080",
    "end": "1393080"
  },
  {
    "text": "the ideal situation like a Rec comp graph the first is that it allocate the",
    "start": "1393080",
    "end": "1398520"
  },
  {
    "text": "memory once and then re during the compilation and then reuse it for the multiple executions so you don't need to",
    "start": "1398520",
    "end": "1405080"
  },
  {
    "text": "allocate the memory every time and the second is that it will tell the actor where to read the",
    "start": "1405080",
    "end": "1412600"
  },
  {
    "text": "data so you don't need to send it another RPC to tell the uh to tell the actor where to read the input",
    "start": "1412600",
    "end": "1420760"
  },
  {
    "text": "argument and the third one is that a rate has to write output to the same place so driver always watch the same",
    "start": "1420760",
    "end": "1427279"
  },
  {
    "text": "place so it doesn't need to anal RPC for the actor to tell the driver where is",
    "start": "1427279",
    "end": "1432320"
  },
  {
    "text": "the output yeah and this is the sound Benchmark you can see there are some",
    "start": "1432320",
    "end": "1439080"
  },
  {
    "text": "like a single node and multiple node and like some run trip you can see it's like",
    "start": "1439080",
    "end": "1444279"
  },
  {
    "text": "around like maybe 10 to 20x and the and this is for the share memory and",
    "start": "1444279",
    "end": "1451960"
  },
  {
    "text": "the second part is for the uh GPU if you want to transfer a GPU to another",
    "start": "1451960",
    "end": "1458080"
  },
  {
    "text": "GPU uh uh at first in the default recorder execution is that you need to",
    "start": "1458080",
    "end": "1463679"
  },
  {
    "text": "copy the GPU from need to copy a tensor from the GPU to the to the senders hip",
    "start": "1463679",
    "end": "1470840"
  },
  {
    "text": "memory in the CPU and then you need to like serialize and like put it into the",
    "start": "1470840",
    "end": "1476080"
  },
  {
    "text": "object store share memory and then the receiver need to uh read the read the",
    "start": "1476080",
    "end": "1482480"
  },
  {
    "text": "read the tensor from the share memory and then copy to the GPU so there are a lot of data copies we",
    "start": "1482480",
    "end": "1489720"
  },
  {
    "text": "although we do some explanation about to support like a little copit but there are still and other like a cation dation",
    "start": "1489720",
    "end": "1496240"
  },
  {
    "text": "it's still a lot of data copit so with the with a red compa graph we",
    "start": "1496240",
    "end": "1502039"
  },
  {
    "text": "support some like a P2 protocol such as ni easily so you can use the NCO",
    "start": "1502039",
    "end": "1507799"
  },
  {
    "text": "directly to send the the tensor from dp0 to",
    "start": "1507799",
    "end": "1512840"
  },
  {
    "text": "gp1 yeah and this is the this is a very simple example uh I just want to showcase that the API is pretty simple",
    "start": "1512840",
    "end": "1519919"
  },
  {
    "text": "you just need to specify the transer equal to nickel and it can support this",
    "start": "1519919",
    "end": "1526559"
  },
  {
    "text": "and we also build the execution schedule for for the users so you don't need to worry about the de C by the",
    "start": "1526559",
    "end": "1534840"
  },
  {
    "text": "niod and this is an other use case uh the vom support two back end one is for",
    "start": "1534840",
    "end": "1541159"
  },
  {
    "text": "the multiprocessing the other is built by the Ray and uh and we optimiz the ray",
    "start": "1541159",
    "end": "1548200"
  },
  {
    "text": "uh the V end Ray back end with the ray compar graph to reduce the overhead from",
    "start": "1548200",
    "end": "1553520"
  },
  {
    "text": "the 2 to 3 millisecond to the 100 to 200 microc",
    "start": "1553520",
    "end": "1559679"
  },
  {
    "text": "and we also implement the tensor person and the P person in the VN and all of",
    "start": "1559720",
    "end": "1565960"
  },
  {
    "text": "them have already been merged to upstream and this is some Benchmark We",
    "start": "1565960",
    "end": "1572679"
  },
  {
    "text": "compare the r compare graph with the multiprocess implementation rather than the rate original rate implementation we",
    "start": "1572679",
    "end": "1580640"
  },
  {
    "text": "still got a better performance result yeah and next Andrew we talk",
    "start": "1580640",
    "end": "1587320"
  },
  {
    "text": "about the Dr yeah so um next I'll cover how we plan to integrate um CU with the latest um",
    "start": "1587320",
    "end": "1595159"
  },
  {
    "text": "Dynamic resource allocation API that is being actively developed in the kubernetes community um so first we want to explore",
    "start": "1595159",
    "end": "1602840"
  },
  {
    "text": "leveraging the new uh Dr apis in CU for GPU time slicing so GPU time slicing is",
    "start": "1602840",
    "end": "1610360"
  },
  {
    "text": "very similar to how multiple processes would share CPU time so each worker each",
    "start": "1610360",
    "end": "1615799"
  },
  {
    "text": "Ray worker gets a slice of GPU time and when it's occupying the GPU it gets full access to the GPU and it it doesn't",
    "start": "1615799",
    "end": "1621480"
  },
  {
    "text": "share that with any other um task or actor in the cluster so while not ideal",
    "start": "1621480",
    "end": "1626640"
  },
  {
    "text": "for online inference where you care a lot about the latency uh you can imagine some uh interesting use cases for",
    "start": "1626640",
    "end": "1632320"
  },
  {
    "text": "offline or batch inference where you really care about fully utilizing all the gpus that you have and the latency",
    "start": "1632320",
    "end": "1638799"
  },
  {
    "text": "is is not not that important um so uh this is some example",
    "start": "1638799",
    "end": "1644039"
  },
  {
    "text": "yaml for how we envision this integration to work so first you create a resource claim API or resource claim",
    "start": "1644039",
    "end": "1650120"
  },
  {
    "text": "template um which is tied to device class um and the new Dr API allows you to specify um device specific parameters",
    "start": "1650120",
    "end": "1658120"
  },
  {
    "text": "which in this example is leveraging the time slicing strategy that's available for NVIDIA gpus um and then you uh",
    "start": "1658120",
    "end": "1664200"
  },
  {
    "text": "specify the resource claim in the plot template that's used uh in the in the ray cluster which can also be referenced",
    "start": "1664200",
    "end": "1669360"
  },
  {
    "text": "in the ray job or the ray service um similarly we want to look",
    "start": "1669360",
    "end": "1675120"
  },
  {
    "text": "into GPU space sharing so this involves dividing the GPU resources or",
    "start": "1675120",
    "end": "1680279"
  },
  {
    "text": "virtualizing the GPU amongst many Reay actors and tasks and this involves um",
    "start": "1680279",
    "end": "1686000"
  },
  {
    "text": "simutaneous consumption of gpus so there's no contact switching required to access um across the um distributed rate",
    "start": "1686000",
    "end": "1693399"
  },
  {
    "text": "cluster um and uh space sharing can be viable for online inference since there's no context switching between",
    "start": "1693399",
    "end": "1699279"
  },
  {
    "text": "gpus and space sharing allows you to run um many smaller models on a on a single",
    "start": "1699279",
    "end": "1704720"
  },
  {
    "text": "uh accelerator and so it gives you a lot more flexibility on like how you want to you know utilize the gpus that you",
    "start": "1704720",
    "end": "1710200"
  },
  {
    "text": "already had um so similar to the time slicing example um we would leverage the new ability to pass in um device",
    "start": "1710200",
    "end": "1717399"
  },
  {
    "text": "specific parameters to enable um Nvidia space partitioning and similarly we would reference the resource claim in",
    "start": "1717399",
    "end": "1723640"
  },
  {
    "text": "the PO template um of the worker Parts um so it's also worth noting that",
    "start": "1723640",
    "end": "1729200"
  },
  {
    "text": "Ray itself also supports um fractional allocation of gpus across this actors",
    "start": "1729200",
    "end": "1734720"
  },
  {
    "text": "and task and so we want to explore combining raise ility to do fractional GPU resource allocation with Dr to",
    "start": "1734720",
    "end": "1742000"
  },
  {
    "text": "unlock new ways to fully control um the utilization and consumption of gpus and",
    "start": "1742000",
    "end": "1747159"
  },
  {
    "text": "and other Hardware accelerators um and likewise we can start thinking about how to use Dr in",
    "start": "1747159",
    "end": "1752840"
  },
  {
    "text": "conjunction with techniques that c cover like tensor parallelism and pipeline parallelism um to enable finer grain",
    "start": "1752840",
    "end": "1759000"
  },
  {
    "text": "control over GPU performance and utilization when you're trying to um serve llms with VM and",
    "start": "1759000",
    "end": "1766320"
  },
  {
    "text": "Ray so yeah that covers kind of the main parts of the talk um so let let's summarize the the key points quickly so",
    "start": "1766320",
    "end": "1771720"
  },
  {
    "text": "first we introduced Ray an unified open source compute framework for distributed machine learning Ray provides a model",
    "start": "1771720",
    "end": "1778080"
  },
  {
    "text": "serving framework called Ray serve that makes it really easy to build um online inference apis with your models Ray also",
    "start": "1778080",
    "end": "1784399"
  },
  {
    "text": "has a library called Ray data for offline and batch inference which often um requires heterogeneous um compute",
    "start": "1784399",
    "end": "1791760"
  },
  {
    "text": "resources uh the ray project recently developed um Ray compile graphs which enables uh Native GPU to GPU",
    "start": "1791760",
    "end": "1798840"
  },
  {
    "text": "communication which significantly improves um performance of techniques such as tensor parallelism and pipeline",
    "start": "1798840",
    "end": "1804960"
  },
  {
    "text": "par parallelism which are um used um uh in inference engines such as VM um and",
    "start": "1804960",
    "end": "1811919"
  },
  {
    "text": "all these techniques in Ray uh combined with kubernetes Dr will unlock new ways",
    "start": "1811919",
    "end": "1817600"
  },
  {
    "text": "to optimize the inference performance uh utilization and and cost and then lastly",
    "start": "1817600",
    "end": "1823080"
  },
  {
    "text": "um cubre is the kubernetes operator that enables all of the above um uh your",
    "start": "1823080",
    "end": "1828200"
  },
  {
    "text": "production kubernetes environment um so yeah before we end the talk um we have a few shamless pugs uh",
    "start": "1828200",
    "end": "1834600"
  },
  {
    "text": "first we encourage folks who are using Ray and CU to join the community there's a ray Community slack uh with a few",
    "start": "1834600",
    "end": "1840399"
  },
  {
    "text": "channels dedicated to CU users and you can ping myself or Kon there to discuss anything related to Cub um we also have",
    "start": "1840399",
    "end": "1846760"
  },
  {
    "text": "a bi-weekly um Cub community meeting um and we welcome folks to join and help us",
    "start": "1846760",
    "end": "1851960"
  },
  {
    "text": "uh drive cubr in in the right direction oh yeah the final is the",
    "start": "1851960",
    "end": "1859200"
  },
  {
    "text": "advertisement yeah the N scale is also sponsored for the cucon this year and we have boost in s43 and the N scale is a",
    "start": "1859200",
    "end": "1867720"
  },
  {
    "text": "creator of Ray and we have some expert about the N scale and the Ray and if you are interested in Ray the N scale you",
    "start": "1867720",
    "end": "1873720"
  },
  {
    "text": "can just uh visit our BS and we have some fancy swag about like Ray yeah so",
    "start": "1873720",
    "end": "1878960"
  },
  {
    "text": "uh yeah thank you for listening great thank [Applause]",
    "start": "1878960",
    "end": "1885840"
  },
  {
    "text": "you I think we have uh three minutes for questions yeah",
    "start": "1885840",
    "end": "1892279"
  },
  {
    "text": "oh oh I I think oh there's a mic there so you have to line up the mic",
    "start": "1892279",
    "end": "1898840"
  },
  {
    "text": "yeah or or maybe we can repeat it mic okay can you try",
    "start": "1902960",
    "end": "1908480"
  },
  {
    "text": "now hello yeah okay okay yeah thanks for the Deep dive the results very promising",
    "start": "1908480",
    "end": "1914600"
  },
  {
    "text": "uh so firstly a quick comment right very nice to see the DI integration I wondering and have you thinking about",
    "start": "1914600",
    "end": "1921440"
  },
  {
    "text": "looking at even like the meek the hard physical partition of the GPU or n the sharing thing and also comment we're",
    "start": "1921440",
    "end": "1928919"
  },
  {
    "text": "going to present some the GPU sharing Benchmark study mod so for those quite interesting yeah please come to our",
    "start": "1928919",
    "end": "1936320"
  },
  {
    "text": "session so I have two quick and high level questions so firstly for people like me without deep knowledge of R can",
    "start": "1936320",
    "end": "1942960"
  },
  {
    "text": "you quickly high level comment and U like the r KY serve and en Triton so",
    "start": "1942960",
    "end": "1948799"
  },
  {
    "text": "what's relationship or the key difference between that you want take that um yeah I think",
    "start": "1948799",
    "end": "1955399"
  },
  {
    "text": "for the you can see like as Tron I think in my thing I think Tron is the inference engine you can see like the",
    "start": "1955399",
    "end": "1961880"
  },
  {
    "text": "Reser is the architector it's just help you to like a Ser the model and like a router Router request and you can use",
    "start": "1961880",
    "end": "1969200"
  },
  {
    "text": "everything that support python uh that's with the reserve together and I think",
    "start": "1969200",
    "end": "1974440"
  },
  {
    "text": "with the kerve uh honestly I don't use the kerve but I think the cas is more like the microservice architecture and",
    "start": "1974440",
    "end": "1981080"
  },
  {
    "text": "maybe it's a better integration with like the kubernetes native Auto scaling something like that but I think for Race",
    "start": "1981080",
    "end": "1986760"
  },
  {
    "text": "Service it's pretty flexible for example you can run multiple model in a single container and then you can also scale",
    "start": "1986760",
    "end": "1993159"
  },
  {
    "text": "the different replica and uh and I think uh and I think it also support like the",
    "start": "1993159",
    "end": "1999519"
  },
  {
    "text": "re have is auto scaling so you can see and this has a better programmability",
    "start": "1999519",
    "end": "2005159"
  },
  {
    "text": "yeah so I think like a case service like a supp like a KU ecosystem maybe better but the Reser has more F ability and",
    "start": "2005159",
    "end": "2012440"
  },
  {
    "text": "programmability and you can put a lot of differ in like course cluster yeah a more fair comparison with Triton would",
    "start": "2012440",
    "end": "2018559"
  },
  {
    "text": "be like VM and Ray serve is like the model serving framework and you can wrap like the",
    "start": "2018559",
    "end": "2023799"
  },
  {
    "text": "VM with with um R serve okay second high level question you mentioned the two",
    "start": "2023799",
    "end": "2030039"
  },
  {
    "text": "different deployment model right on top of kubernetes and the virtual machine can you elaborate a little bit about the",
    "start": "2030039",
    "end": "2035600"
  },
  {
    "text": "pro and the cons and also their adoptions for example how many user already use how many virtual machine",
    "start": "2035600",
    "end": "2041159"
  },
  {
    "text": "thank you I I think for the I I think in our product side we support both virtual",
    "start": "2041159",
    "end": "2046840"
  },
  {
    "text": "machine and kubernetes I think virtual machine is that we have more control yeah but I more advancement and I think",
    "start": "2046840",
    "end": "2054280"
  },
  {
    "text": "kubernetes that you can leverage the kuet ecosystem like the Peres a lot of different stuff and some in stuff and",
    "start": "2054280",
    "end": "2061760"
  },
  {
    "text": "the I I think in the open source side I think the kubernetes is dominant yeah but if you want to",
    "start": "2061760",
    "end": "2068040"
  },
  {
    "text": "optimize and customize yourself maybe you need to start from the VN is better but I think depends on I mean we are Aon",
    "start": "2068040",
    "end": "2075480"
  },
  {
    "text": "so definitely use kubernetes okay thank",
    "start": "2075480",
    "end": "2080040"
  },
  {
    "text": "you all right I think I recognize you from college so it's cool to see you giving a talk here uh but uh I um I work",
    "start": "2080560",
    "end": "2087800"
  },
  {
    "text": "on kaido it's a new project at Microsoft which is also kubernetes operator to run",
    "start": "2087800",
    "end": "2093200"
  },
  {
    "text": "AI workloads similar to cuber uh my question was that given given that Ray",
    "start": "2093200",
    "end": "2099160"
  },
  {
    "text": "has its own scheduler uh that's that's already running and then Cube Ray is running on",
    "start": "2099160",
    "end": "2105440"
  },
  {
    "text": "kubernetes kubernetes has its own scheduler could you talk about like the performance there because my",
    "start": "2105440",
    "end": "2112359"
  },
  {
    "text": "understanding is that Ray was developed to be its own Standalone like you you were talking about Ray uh core and then",
    "start": "2112359",
    "end": "2117800"
  },
  {
    "text": "compiled graph it's developed to be its own Standalone system and now cubr is running on top of kubernetes right so",
    "start": "2117800",
    "end": "2123320"
  },
  {
    "text": "could you talk a little bit about the performance and and the resource efficiency there",
    "start": "2123320",
    "end": "2129000"
  },
  {
    "text": "uh you you say the the performance for what uh which perspective of performance",
    "start": "2129000",
    "end": "2135079"
  },
  {
    "text": "you are interesting so my understanding is you have like two schedulers going on",
    "start": "2135079",
    "end": "2140160"
  },
  {
    "text": "you have the ray scheduler and you have the kuet scheduler so my my guess is there's some overhead there because Ray",
    "start": "2140160",
    "end": "2146760"
  },
  {
    "text": "was intended to be ran Standalone right so could you talk about the if there's a drawback there or",
    "start": "2146760",
    "end": "2153839"
  },
  {
    "text": "the resource uation or performance how that's impacted by using qra",
    "start": "2153839",
    "end": "2158920"
  },
  {
    "text": "um yeah I I I think the uh I I not quite understand the the",
    "start": "2158920",
    "end": "2165839"
  },
  {
    "text": "performance but I guess it's maybe for the scheding part I think for R scheding I think it's I think it will be very",
    "start": "2165839",
    "end": "2172280"
  },
  {
    "text": "fast you can see because it's just down to a process in a running container so it doesn't need to provision the",
    "start": "2172280",
    "end": "2178400"
  },
  {
    "text": "container doesn't need to provision the node yeah so I think maybe it's pretty fast yeah I'm not sure if this your",
    "start": "2178400",
    "end": "2184839"
  },
  {
    "text": "question or not so I think the important distinction is that like when you run Ray on kubernetes uh each work reach Ray",
    "start": "2184839",
    "end": "2191640"
  },
  {
    "text": "worker is not a VM it's a pod right um and so there's still like two tiers of scheduling Ray handles the actor task",
    "start": "2191640",
    "end": "2197800"
  },
  {
    "text": "scheduling on the workers and then kubernetes handles like the Pod scheduling for the ret so Ray is",
    "start": "2197800",
    "end": "2203240"
  },
  {
    "text": "treating each podt is a VM is that uh yes yeah basically yeah and so performance is uh it it just depends",
    "start": "2203240",
    "end": "2209599"
  },
  {
    "text": "like kubernetes if you use rayon kubernetes you have all the advanced scheduling capabilities that come with kubernetes so you could actually",
    "start": "2209599",
    "end": "2216400"
  },
  {
    "text": "increase your cluster performance by using like topology scheduling grouping you know a specific type of gpus closer",
    "start": "2216400",
    "end": "2222760"
  },
  {
    "text": "together things like that whereas on VMS you kind of have to just manage all that yourself so it just depends on like how you deploy R thank",
    "start": "2222760",
    "end": "2230880"
  },
  {
    "text": "you hey it's good to have you in Utah um so I can see how Ray helps with",
    "start": "2230880",
    "end": "2236400"
  },
  {
    "text": "performance at serving time like it helps you get like get more out of your gpus handle more like throughput and",
    "start": "2236400",
    "end": "2242119"
  },
  {
    "text": "stuff um and and some of the other tools out there also do like Bento ML and NV Tron and so like one with all these",
    "start": "2242119",
    "end": "2249800"
  },
  {
    "text": "tools one thing that I've struggled with is helping the data scientists find an ergonomic way to syn the training logic",
    "start": "2249800",
    "end": "2256839"
  },
  {
    "text": "like the pre-processing that they're doing in training with the pre-processing they're doing in serving because like if you train a model and",
    "start": "2256839",
    "end": "2262520"
  },
  {
    "text": "use pre-processing logic a but then accidentally use pre-processing logic B at serving time it like invalidates it",
    "start": "2262520",
    "end": "2268400"
  },
  {
    "text": "like does Ray have facilities to help sync those two like to make the transition from training to serving like",
    "start": "2268400",
    "end": "2275040"
  },
  {
    "text": "ergonomic you say for the for a pre-processing to trending yeah like if I'm a data scientist I'm probably",
    "start": "2275040",
    "end": "2280520"
  },
  {
    "text": "iterating on my pre-processing logic I'm probably like like maybe maybe sometimes I'm tinting images red sometimes I'm tinting images blue and when I",
    "start": "2280520",
    "end": "2286800"
  },
  {
    "text": "transition that to serving I want to make sure if I tinted it red at training time I tint them red at serving as well like anyway and it's just been hard to",
    "start": "2286800",
    "end": "2293160"
  },
  {
    "text": "like coordinate those versions across training and serving time and I think I",
    "start": "2293160",
    "end": "2299000"
  },
  {
    "text": "think that is like an ergonomic failing of some of the serving tools so I was just wondering if if Ry has any like",
    "start": "2299000",
    "end": "2304400"
  },
  {
    "text": "recommended work work flows or facilities for like getting tracking right um I think because I think R is",
    "start": "2304400",
    "end": "2311079"
  },
  {
    "text": "pretty programmable so um I think it's pretty easy to achieve lots and uh you",
    "start": "2311079",
    "end": "2316720"
  },
  {
    "text": "can think I think there's a b like from the netic like a company behind the Pokemon go let say they move to the Ray",
    "start": "2316720",
    "end": "2325200"
  },
  {
    "text": "I think their code redu 170% okay so I think uh good this different stage",
    "start": "2325200",
    "end": "2330359"
  },
  {
    "text": "together is the what the ray do the P so I think what I'm hearing is Ray like other serving Frameworks is pretty",
    "start": "2330359",
    "end": "2336040"
  },
  {
    "text": "unopinionated about how you track that version so it's still sort of up to you to figure out what the workflow is to to",
    "start": "2336040",
    "end": "2341240"
  },
  {
    "text": "S them is that right yeah yeah yeah we we don't I think we are you can see the record it's pretty low level API",
    "start": "2341240",
    "end": "2347359"
  },
  {
    "text": "comparing with like a spark core API as in pretty low level and so I guess in that case one recommendation is if",
    "start": "2347359",
    "end": "2352440"
  },
  {
    "text": "people haven't looked at boml boml like can deploy on top of Ray but like has some facilities for doing for packaging",
    "start": "2352440",
    "end": "2358680"
  },
  {
    "text": "the model together with the pre-process anyway so something thanks hello uh great presentation um so",
    "start": "2358680",
    "end": "2366440"
  },
  {
    "text": "I have a question what is your strategy uh to reduce the cold start time when the ray cluster tries to acquire a new",
    "start": "2366440",
    "end": "2373599"
  },
  {
    "text": "node when ites Auto scales because uh for my benchmarks it's at 8 minutes",
    "start": "2373599",
    "end": "2379000"
  },
  {
    "text": "right now uh even with using uh boil rocket Emi uh baked into the",
    "start": "2379000",
    "end": "2385280"
  },
  {
    "text": "node can yeah so I think for FM clusters because having static clusters",
    "start": "2385280",
    "end": "2391000"
  },
  {
    "text": "is I think for um cold startup it's just going to depend on like what your cloud and like uh how you're deploying your",
    "start": "2391000",
    "end": "2397520"
  },
  {
    "text": "clusters right so um if you're just like deploying models and you're downloading from hugging face every time yeah it's",
    "start": "2397520",
    "end": "2403359"
  },
  {
    "text": "going to take you like 10 minutes to download like llama 70b or something like that um I know like specifically on GK we have a lot of different ways to",
    "start": "2403359",
    "end": "2410480"
  },
  {
    "text": "pull models faster um you can use like GCS fuse to mount like the model weights in a bucket directly into your container",
    "start": "2410480",
    "end": "2417839"
  },
  {
    "text": "um or you can um uh cach the actual model weights on a disc and then Mount the disc on every node and so like",
    "start": "2417839",
    "end": "2424640"
  },
  {
    "text": "there's kind of many different approaches that you could take it it'll depend on like what cloud provider you're using and and what you have",
    "start": "2424640",
    "end": "2430520"
  },
  {
    "text": "available well um so the specific use case is uh so I'm trying to BU like eeral uh rate rate tring clusters like",
    "start": "2430520",
    "end": "2437640"
  },
  {
    "text": "on demand um data scientists provision rate rate tring jobs when they're done",
    "start": "2437640",
    "end": "2442880"
  },
  {
    "text": "that node just ties away saves a lot of money um but the problem is when Ray",
    "start": "2442880",
    "end": "2449000"
  },
  {
    "text": "Cube Ray tries to Autos scale and tries to acquire a new G5 instance for example on AWS it takes it it takes about 8 to",
    "start": "2449000",
    "end": "2457839"
  },
  {
    "text": "10 minutes just to get that node ready and that's before running any kind of uh",
    "start": "2457839",
    "end": "2463560"
  },
  {
    "text": "right yeah so if you're like using cubr Auto scaling in conjunction with like you know eks autoscaler or GK autoscaler",
    "start": "2463560",
    "end": "2469880"
  },
  {
    "text": "like that node startup time is like kind of out of like we can't do anything about that that's kind of up to the",
    "start": "2469880",
    "end": "2476119"
  },
  {
    "text": "provider um but yeah but yeah like I think uh after the node starts up there are like like I mentioned before ways to",
    "start": "2476119",
    "end": "2482800"
  },
  {
    "text": "like pull the image faster or pull like you can cach an image on the Node beforehand or things like that but yeah",
    "start": "2482800",
    "end": "2488640"
  },
  {
    "text": "on the Node startup time like there's there's nothing we do special like at the cube level to improve that got it",
    "start": "2488640",
    "end": "2494319"
  },
  {
    "text": "thanks",
    "start": "2494319",
    "end": "2496960"
  }
]