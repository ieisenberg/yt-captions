[
  {
    "start": "0",
    "end": "273000"
  },
  {
    "text": "they're going to tell you a few stories kubernetes has lost data of some users",
    "start": "240",
    "end": "6770"
  },
  {
    "text": "how component is exposed things that should not be exposed how kubernetes",
    "start": "6770",
    "end": "12540"
  },
  {
    "text": "corrupted data or even how kubernetes did not make data available to post due",
    "start": "12540",
    "end": "20310"
  },
  {
    "text": "to some attach and detach issues and of course we will focus on how we fixed it",
    "start": "20310",
    "end": "26519"
  },
  {
    "text": "and what we learned from these failures so you don't need to repeat the same mistakes we did in the end we will talk",
    "start": "26519",
    "end": "34380"
  },
  {
    "text": "also about the open issues we have right now and how we how are we going to",
    "start": "34380",
    "end": "39600"
  },
  {
    "text": "tackle them so the first thing that users usually layer learn the hard way",
    "start": "39600",
    "end": "45570"
  },
  {
    "text": "is that they should not mess up with persistent volumes and persistent volume claims in this case a user tried to",
    "start": "45570",
    "end": "53629"
  },
  {
    "text": "migrate some data from a distinct cluster to production cluster they",
    "start": "53629",
    "end": "59789"
  },
  {
    "text": "literally took Yama files of the persistent volumes and persistent volume claims and they restore them on the new",
    "start": "59789",
    "end": "67049"
  },
  {
    "text": "cluster and they will store the persistent volumes first and what happened and that was surprising to them",
    "start": "67049",
    "end": "73430"
  },
  {
    "text": "that kubernetes deleted all the data on their storage back-end and they deleted",
    "start": "73430",
    "end": "80610"
  },
  {
    "text": "also the processing volume object so why it happened this is the picture of the",
    "start": "80610",
    "end": "87270"
  },
  {
    "text": "original testing master they have a PVC bound to a pv that points to some storage in the storage back-end and then",
    "start": "87270",
    "end": "95670"
  },
  {
    "text": "they brought the persistent volume object and restored it in the new cluster and the persistent volume was",
    "start": "95670",
    "end": "102930"
  },
  {
    "text": "bound and it was bound to a persistent organ crime that did not exist at that",
    "start": "102930",
    "end": "108990"
  },
  {
    "text": "time they didn't restore it yet so what kubernetes does to bound volumes",
    "start": "108990",
    "end": "117060"
  },
  {
    "text": "that are bound to a claim that doesn't exist it executes the reclaim policy so",
    "start": "117060",
    "end": "122880"
  },
  {
    "text": "in this case the return policy was delete and kubernetes deleted the data",
    "start": "122880",
    "end": "128899"
  },
  {
    "text": "because to the kubernetes it looked exactly same like there was a claim there was a",
    "start": "128899",
    "end": "135090"
  },
  {
    "text": "TV but somebody has deleted the crime it's exactly the same situation an API",
    "start": "135090",
    "end": "140400"
  },
  {
    "text": "server and kubernetes has no way how to distinguish this failed migration or use",
    "start": "140400",
    "end": "145890"
  },
  {
    "text": "the deleting persistent volume claim and how we did fix how did we fix it we did",
    "start": "145890",
    "end": "151890"
  },
  {
    "text": "not fix it at all and we will not fix it this is how kubernetes works if there is",
    "start": "151890",
    "end": "157530"
  },
  {
    "text": "persistent volume claim sorry if there is a persistent volume bound to a claim that doesn't exist kubernetes executes",
    "start": "157530",
    "end": "164910"
  },
  {
    "text": "the reclaim policy so what you should do as users to avoid this error of course",
    "start": "164910",
    "end": "170370"
  },
  {
    "text": "we all do backups right you should not play with your persistent volumes you",
    "start": "170370",
    "end": "179250"
  },
  {
    "text": "should it use some dedicated tools for example yesterday there was a talk about",
    "start": "179250",
    "end": "184879"
  },
  {
    "text": "Valera from happy oh that can migrate objects between clusters and just in",
    "start": "184879",
    "end": "192359"
  },
  {
    "text": "case you want to play with persistent volumes please use retain reclaim policy so",
    "start": "192359",
    "end": "197609"
  },
  {
    "text": "kubernetes will not delete the data and you can restore them later and in this particular case if you restore llam√≥",
    "start": "197609",
    "end": "207510"
  },
  {
    "text": "files from different cluster you should sanitize them in this case in the",
    "start": "207510",
    "end": "212669"
  },
  {
    "text": "persistent volume there is UID of the claim that is it is bound to but you IDs",
    "start": "212669",
    "end": "219269"
  },
  {
    "text": "are assigned by kubernetes and they are random so do you the persistent",
    "start": "219269",
    "end": "224669"
  },
  {
    "text": "vol-plane can't exist basically and you should also clean up some binding",
    "start": "224669",
    "end": "230700"
  },
  {
    "text": "annotation on PV PV C 2 so kubernetes finishes the binding first the view ID",
    "start": "230700",
    "end": "235769"
  },
  {
    "text": "and everything works and then it doesn't really matter if you restore PV first PVC first it will work what we learned",
    "start": "235769",
    "end": "243930"
  },
  {
    "text": "here that we should educate our users so here we are now you know don't play with",
    "start": "243930",
    "end": "250019"
  },
  {
    "text": "your TVs and we know that our documentation is not perfect and",
    "start": "250019",
    "end": "255709"
  },
  {
    "text": "everybody in this room can speak English so it's not hard to update the",
    "start": "255709",
    "end": "263130"
  },
  {
    "text": "documentation you don't need to write any code you don't need to pass any everybody can contribute and every",
    "start": "263130",
    "end": "270250"
  },
  {
    "text": "contribution is welcome another case when user has lost the data this time",
    "start": "270250",
    "end": "277150"
  },
  {
    "start": "273000",
    "end": "412000"
  },
  {
    "text": "due to real back in kubernetes was when",
    "start": "277150",
    "end": "282930"
  },
  {
    "text": "they did it they persistent wouldn't claim while they had a pot running so",
    "start": "282930",
    "end": "289960"
  },
  {
    "text": "the user had the pod running using some volume using PVC when they deleted the",
    "start": "289960",
    "end": "297310"
  },
  {
    "text": "PVC and while the pod was running the data were white up from the volume and",
    "start": "297310",
    "end": "302889"
  },
  {
    "text": "the pod didn't like it losing data underneath why did it happen it's",
    "start": "302889",
    "end": "310330"
  },
  {
    "text": "because kubernetes api server offers no referential integrity you can't tell it to not to delete some object while it's",
    "start": "310330",
    "end": "318520"
  },
  {
    "text": "used by different objects so when user deleted the claim that was used by a pod",
    "start": "318520",
    "end": "324000"
  },
  {
    "text": "we already know what happens to persistent volumes that are bound to non existing claims the reclaim policy is",
    "start": "324000",
    "end": "330610"
  },
  {
    "text": "executed but the pot is still running this error is not that serious because",
    "start": "330610",
    "end": "336909"
  },
  {
    "text": "user ask for the data to be deleted they deleted the claim just the order was not",
    "start": "336909",
    "end": "342940"
  },
  {
    "text": "right the pod was still running how did we fix it we introduced finalizes",
    "start": "342940",
    "end": "348370"
  },
  {
    "text": "finalizer our component is a cooperative concept it's a field in every API object in the",
    "start": "348370",
    "end": "354849"
  },
  {
    "text": "API server and API server will not",
    "start": "354849",
    "end": "359949"
  },
  {
    "text": "delete object that has a finalizer so in this case we put a finalizer",
    "start": "359949",
    "end": "366669"
  },
  {
    "text": "into every persistent foreign claim object in the cluster using a new",
    "start": "366669",
    "end": "372819"
  },
  {
    "text": "admission plugin so when user tries to delete the object kubernetes does not",
    "start": "372819",
    "end": "379210"
  },
  {
    "text": "delete the object it only marks it as terminating and everything works nothing is deleted only after the user",
    "start": "379210",
    "end": "385330"
  },
  {
    "text": "deletes the pod our newly introduced storage in use protection controller",
    "start": "385330",
    "end": "390900"
  },
  {
    "text": "sees that there is a claim that is not used by any port and its size safe to",
    "start": "390900",
    "end": "396339"
  },
  {
    "text": "delete so it removes the finalizar kubernetes api server finishes",
    "start": "396339",
    "end": "401949"
  },
  {
    "text": "termination of the object and the reclaim policy is executed and now the",
    "start": "401949",
    "end": "407470"
  },
  {
    "text": "stuff is deleted in the right order",
    "start": "407470",
    "end": "410970"
  },
  {
    "start": "412000",
    "end": "737000"
  },
  {
    "text": "another case when user restored some data was quite surprising because",
    "start": "413070",
    "end": "418300"
  },
  {
    "text": "basically they restarted cubelet and they lost data on the aerobic volumes",
    "start": "418300",
    "end": "423510"
  },
  {
    "text": "what happened the queue blood got offline in this case it was due to a",
    "start": "423510",
    "end": "429130"
  },
  {
    "text": "crash but it doesn't really matter it could be like for example cubed update and at the same time something deleted a",
    "start": "429130",
    "end": "437680"
  },
  {
    "text": "pot on API server and the newly restarted couplet basically wiped out",
    "start": "437680",
    "end": "445060"
  },
  {
    "text": "all the data on the volume of the pot that was just deleted why did it happen",
    "start": "445060",
    "end": "454740"
  },
  {
    "text": "kubernetes Goulet uses caches to track what volumes are mounted where and for",
    "start": "454740",
    "end": "461770"
  },
  {
    "text": "what pots but these caches were not restored when the nuclear age started",
    "start": "461770",
    "end": "468700"
  },
  {
    "text": "because the API object the pot was not present in the API server and cubelet",
    "start": "468700",
    "end": "474250"
  },
  {
    "text": "had no place where to know how to know that the volume is the mounted and at",
    "start": "474250",
    "end": "481120"
  },
  {
    "text": "the same time cubed has periodic routine where it basically garbage garbage",
    "start": "481120",
    "end": "487930"
  },
  {
    "text": "collects the data of deleted pots and in this case usually the the data are",
    "start": "487930",
    "end": "496180"
  },
  {
    "text": "collected and volume some armored and mounted but in this case the volumes were not mounted yet so what super did",
    "start": "496180",
    "end": "504010"
  },
  {
    "text": "it recursively removed all the data in the garbage in the trash directory but",
    "start": "504010",
    "end": "510940"
  },
  {
    "text": "there was still volume mounted there and it removed all the data from the volume",
    "start": "510940",
    "end": "517140"
  },
  {
    "text": "we fixed it at first we looked at all the places were cubed recursively delete",
    "start": "517140",
    "end": "524529"
  },
  {
    "text": "data to make sure it never happens again and we add that we added a special check",
    "start": "524529",
    "end": "531750"
  },
  {
    "text": "while removing later that we never cloth across a file system boundary so we",
    "start": "531750",
    "end": "539010"
  },
  {
    "text": "accidentally don't enter a mounted volume and don't remove the data there",
    "start": "539010",
    "end": "544389"
  },
  {
    "text": "this is what this was just a safety check the real fix was in two that we",
    "start": "544389",
    "end": "549880"
  },
  {
    "text": "introduced reconstruction you may have noticed that in Varley Pew blood",
    "start": "549880",
    "end": "555820"
  },
  {
    "text": "directory there are some crazy directories with weird names and that's",
    "start": "555820",
    "end": "561430"
  },
  {
    "text": "how we keep a state and from names of these directories cubed can reconstruct",
    "start": "561430",
    "end": "568320"
  },
  {
    "text": "what volume was is mounted where and also how to clean the volume up so when",
    "start": "568320",
    "end": "576130"
  },
  {
    "text": "nuclear blood starts and the pot is not in the API server at least cubed can restore the caches it knows what volumes",
    "start": "576130",
    "end": "583209"
  },
  {
    "text": "belong to which pot and it can unmount them cleanly and then delete all the",
    "start": "583209",
    "end": "589269"
  },
  {
    "text": "garbage left by the pot what we learned here was that you should run some tests",
    "start": "589269",
    "end": "595959"
  },
  {
    "text": "for this behavior so now we have so-called disruptive tests that run about kill cubelet delete the pot from",
    "start": "595959",
    "end": "603209"
  },
  {
    "text": "kubernetes api server start new Coubertin",
    "start": "603209",
    "end": "607949"
  },
  {
    "text": "no I'm not okay so we're under the Optive test that",
    "start": "634430",
    "end": "640860"
  },
  {
    "text": "tests exactly this scenario so we have our safety check never crossfire system",
    "start": "640860",
    "end": "646140"
  },
  {
    "text": "boundary when removing beta we had the reconstruction before the bug is gone until it happened again so how was it",
    "start": "646140",
    "end": "654600"
  },
  {
    "text": "possible the scenario was completely the same like before Hubert was down for",
    "start": "654600",
    "end": "661740"
  },
  {
    "text": "some reason something deleted the pod new hub had started and vibe the data but this time somebody used the root",
    "start": "661740",
    "end": "668910"
  },
  {
    "text": "disk as a persistent volume in kubernetes and the safety check",
    "start": "668910",
    "end": "679380"
  },
  {
    "text": "we use to never cross file system boundary it can't work because this root disk doesn't present a file system",
    "start": "679380",
    "end": "686220"
  },
  {
    "text": "boundary it's the still the same root disk as we use for cached data in cubed",
    "start": "686220",
    "end": "692100"
  },
  {
    "text": "and also the reconstruction didn't work",
    "start": "692100",
    "end": "697199"
  },
  {
    "text": "because in this spot that was deleted the user used so-called subpaths feature",
    "start": "697199",
    "end": "704510"
  },
  {
    "text": "where cubelet doesn't give the whole volume to a pod but just a subdirectory",
    "start": "704510",
    "end": "711560"
  },
  {
    "text": "so one we once we knew that there is something wrong with that path we fix it very quickly but the lessons we learned",
    "start": "711560",
    "end": "719730"
  },
  {
    "text": "that we should test more features to combine together so now we have whole",
    "start": "719730",
    "end": "725070"
  },
  {
    "text": "matrix of tests that test different scenarios and in this case for example",
    "start": "725070",
    "end": "730440"
  },
  {
    "text": "we test disruptive tests with local volumes if subpath so not only",
    "start": "730440",
    "end": "739709"
  },
  {
    "start": "737000",
    "end": "833000"
  },
  {
    "text": "kubernetes can delete your data it can also expose them in a very unsecured way",
    "start": "739709",
    "end": "746420"
  },
  {
    "text": "last year we have this CVE where malicious user could expose hosts",
    "start": "746420",
    "end": "753079"
  },
  {
    "text": "could get access to whole host file system and do anything there",
    "start": "753079",
    "end": "759980"
  },
  {
    "text": "why did this happen and it happened because a sibling that was completely",
    "start": "761710",
    "end": "766900"
  },
  {
    "text": "innocent in the container this evening was evaluated on the host and it was not",
    "start": "766900",
    "end": "774580"
  },
  {
    "text": "that innocent anymore last year me and",
    "start": "774580",
    "end": "780220"
  },
  {
    "text": "Michelle or from Google we had a whole session just about this security issue",
    "start": "780220",
    "end": "786160"
  },
  {
    "text": "how we fixed it and including exploit so you can look at it online I will not",
    "start": "786160",
    "end": "793090"
  },
  {
    "text": "cover details here but would you learn of course we should not trust user but this is their said than done",
    "start": "793090",
    "end": "799480"
  },
  {
    "text": "because what we learned also that containers can improve security issues that were not seen before and actually",
    "start": "799480",
    "end": "806890"
  },
  {
    "text": "the very same security issue was then discovered in docker and in cubelet copy",
    "start": "806890",
    "end": "813070"
  },
  {
    "text": "command sorry cube cut or copy command and we learned also that kubernetes a Security Response",
    "start": "813070",
    "end": "819370"
  },
  {
    "text": "Team how they work and they helped us a lot to develop a fix in some safe",
    "start": "819370",
    "end": "825280"
  },
  {
    "text": "environment without exposing the issue and the fix prematurely all right so",
    "start": "825280",
    "end": "834880"
  },
  {
    "start": "833000",
    "end": "1209000"
  },
  {
    "text": "John has talked about various ways communities lost managed to lose data so",
    "start": "834880",
    "end": "842350"
  },
  {
    "text": "in spite of our best efforts we have in certain cases we have noticed that communities could cause file system",
    "start": "842350",
    "end": "849070"
  },
  {
    "text": "corruption and I'll try to cover the cases where this could happen and this",
    "start": "849070",
    "end": "854290"
  },
  {
    "text": "is a story of two bugs that happened two years apart so this is a stack trace",
    "start": "854290",
    "end": "859990"
  },
  {
    "text": "from a real bug action you can see the XFS metadata is corrupt and this was",
    "start": "859990",
    "end": "865690"
  },
  {
    "text": "reported against kubernetes 1.10 and volume type was fibre channel and was",
    "start": "865690",
    "end": "871990"
  },
  {
    "text": "repaired reported on November 2017 so a while back and this is another stack",
    "start": "871990",
    "end": "878470"
  },
  {
    "text": "trace from a real bug from D message kernel log you can see metadata is corrupt again and this is kubernetes",
    "start": "878470",
    "end": "885790"
  },
  {
    "text": "version 114 so pretty recent and it is chef RB d where CS",
    "start": "885790",
    "end": "891000"
  },
  {
    "text": "broke and was reported in August 2009 t so what happened here the problem is",
    "start": "891000",
    "end": "898350"
  },
  {
    "text": "basically boils down to the sometimes same volume could be temporarily mounted on more than one node at once",
    "start": "898350",
    "end": "904889"
  },
  {
    "text": "and typically block devices with file systems don't like it and it could cause",
    "start": "904889",
    "end": "909930"
  },
  {
    "text": "file system corruption how do we fix it and you could say that storage provider",
    "start": "909930",
    "end": "916649"
  },
  {
    "text": "should fix it if you if you are probably using GC EBS and EBS doesn't allow it",
    "start": "916649",
    "end": "921870"
  },
  {
    "text": "but in some cases depending on what storage type you are using it may not be possible another thing that kubernetes could",
    "start": "921870",
    "end": "928889"
  },
  {
    "text": "itself do is Coonans try to be helpful as much helpful as it can and it has a concept of access modes and kubernetes",
    "start": "928889",
    "end": "937019"
  },
  {
    "text": "could enforce the access modes associated with persistent volumes so",
    "start": "937019",
    "end": "942269"
  },
  {
    "text": "those of you who are not familiar with access modes I'll just give a brief intro what are access modes and these",
    "start": "942269",
    "end": "948600"
  },
  {
    "text": "are the three prominent access modes readwrite ones means a volume could be mounted as readwrite on only one node",
    "start": "948600",
    "end": "954509"
  },
  {
    "text": "readwrite managed could be mounted as readwrite on many nodes it only many means it could be mounted as read-only",
    "start": "954509",
    "end": "961290"
  },
  {
    "text": "on many nodes and this is a PVC spec you can see access mode is specified here",
    "start": "961290",
    "end": "969059"
  },
  {
    "text": "and if you are using dynamic provisioning then kubernetes will try to bring a PV that matches your requested",
    "start": "969059",
    "end": "975569"
  },
  {
    "text": "access mode the thing is that kubernetes did not enforce access mode at all until",
    "start": "975569",
    "end": "980730"
  },
  {
    "text": "version 1.7 1.8 so you could it's a Wild West you could have a volume and you",
    "start": "980730",
    "end": "986009"
  },
  {
    "text": "could use it at two nodes at the same time and and cause problems like this so",
    "start": "986009",
    "end": "992639"
  },
  {
    "text": "how did we fix it we started enforcing access modes in kubernetes and so that",
    "start": "992639",
    "end": "998100"
  },
  {
    "text": "you could not attach a volume that has read heard once on multiple nodes at the",
    "start": "998100",
    "end": "1003110"
  },
  {
    "text": "same time but you may notice that this box that we talked in the beginning was",
    "start": "1003110",
    "end": "1009110"
  },
  {
    "text": "for 1.10 and what at 14 so if this fix was made in 1.78 how come these bugs are",
    "start": "1009110",
    "end": "1014959"
  },
  {
    "text": "still present that's surprising so we need to learn that access mode enforcement in kubernetes what are these",
    "start": "1014959",
    "end": "1021679"
  },
  {
    "text": "limitations is and the first thing is that it only works from volume types that are",
    "start": "1021679",
    "end": "1027079"
  },
  {
    "text": "attachable and I'll talk about what attachable means in a minute and second",
    "start": "1027080",
    "end": "1033140"
  },
  {
    "text": "thing is it does not prevent two parts from using same volume on the same node third thing is it's based on the cast",
    "start": "1033140",
    "end": "1040430"
  },
  {
    "text": "volume state in controller manager so all attach D - goes through a control plane call but that state is cached here",
    "start": "1040430",
    "end": "1049580"
  },
  {
    "text": "is a few examples of attachable volume types EBS open stacks energy series for",
    "start": "1049580",
    "end": "1054800"
  },
  {
    "text": "disk if you are familiar with CSI any CSI volume that has published and published controller capability is",
    "start": "1054800",
    "end": "1061790"
  },
  {
    "text": "considered attachable by kubernetes here are the few examples of volume types which are not attached but like ice",
    "start": "1061790",
    "end": "1068090"
  },
  {
    "text": "cassis fr BD fibre channel and typically CSI volume that do not have publish and",
    "start": "1068090",
    "end": "1073760"
  },
  {
    "text": "publish controller capability these are an attachable volume types and and these are they typically could be attached or",
    "start": "1073760",
    "end": "1080210"
  },
  {
    "text": "published to a node directly they don't go through a control plane call so as you can imagine that for for the bugs",
    "start": "1080210",
    "end": "1087140"
  },
  {
    "text": "that happens fr BD fibre channel because the volume is not published through a control plane they could be directly",
    "start": "1087140",
    "end": "1093620"
  },
  {
    "text": "placed on a node the access modes weren't in force that's why this bugs happened so how we fixed up how we fix",
    "start": "1093620",
    "end": "1101420"
  },
  {
    "text": "this was we implemented a dummy attached the diet interface for that is basically",
    "start": "1101420",
    "end": "1107180"
  },
  {
    "text": "a no op for I scuzzy fiber channel and self RBD for in in kubernetes for entry",
    "start": "1107180",
    "end": "1112430"
  },
  {
    "text": "volume types this would basically turn non attachable volume types into",
    "start": "1112430",
    "end": "1117440"
  },
  {
    "text": "attachable and it will ensure that the volume is made available through a control plane call to a node not",
    "start": "1117440",
    "end": "1124070"
  },
  {
    "text": "directly and it will enforce the access mode so that fix the bug for the most part but it still did not so what are",
    "start": "1124070",
    "end": "1132650"
  },
  {
    "text": "the recommendation for CSI drivers you may have seen that though the bug that happened in 1:14 was it reported against",
    "start": "1132650",
    "end": "1138710"
  },
  {
    "text": "fr BD CSI driver so for CSI drivers our recommendation is whenever possible",
    "start": "1138710",
    "end": "1144340"
  },
  {
    "text": "implement control plane based enforcing in the volume itself so that it's more",
    "start": "1144340",
    "end": "1150050"
  },
  {
    "text": "robust and it pushes the problem back to storage providers it may not be possible if you are running off-the-shelf",
    "start": "1150050",
    "end": "1156350"
  },
  {
    "text": "storage solution there's another alternative is like kubernetes ships external attach a",
    "start": "1156350",
    "end": "1164210"
  },
  {
    "text": "sidecar that you could use so even though your volume is not does not have publish and publish capability the CSI",
    "start": "1164210",
    "end": "1171590"
  },
  {
    "text": "calls will still go through and access modes will be still enforced so you can",
    "start": "1171590",
    "end": "1178700"
  },
  {
    "text": "use this to get around some of the problems and you should not disable attach attach from CSI driver object for",
    "start": "1178700",
    "end": "1186200"
  },
  {
    "text": "to make it work and the last thing is that I will say is that prefer use of",
    "start": "1186200",
    "end": "1191720"
  },
  {
    "text": "stateful set over deployment for workloads that use storage that's kind of obvious but a stateful set has",
    "start": "1191720",
    "end": "1197630"
  },
  {
    "text": "stronger guarantees about pod placement like it you cannot have two pods running at the same time that use the same thing",
    "start": "1197630",
    "end": "1204050"
  },
  {
    "text": "so that's another recommendation that we would make alright so next section that",
    "start": "1204050",
    "end": "1211760"
  },
  {
    "start": "1209000",
    "end": "1408000"
  },
  {
    "text": "we want to talk about is like AWS attached at issues so who here runs AWS",
    "start": "1211760",
    "end": "1218320"
  },
  {
    "text": "kubernetes on AWS either yie case or okay quite a few number so being a",
    "start": "1218320",
    "end": "1227930"
  },
  {
    "text": "largest cloud provider it is not always best tuned to run kubernetes I would say and as we have been doing this for a",
    "start": "1227930",
    "end": "1235070"
  },
  {
    "text": "long time we had some starting problems and and and you know teething problems",
    "start": "1235070",
    "end": "1240260"
  },
  {
    "text": "the first issue that we noticed was that sometimes a volume will stuck in",
    "start": "1240260",
    "end": "1245720"
  },
  {
    "text": "attaching and detaching state forever and kubernetes it was very hard to reproduce what we found out was that AWS",
    "start": "1245720",
    "end": "1254480"
  },
  {
    "text": "does not like to use the device name any of us is in unique in the sense that when you publish a volume or attached",
    "start": "1254480",
    "end": "1261110"
  },
  {
    "text": "volume to a node you have to give a device name generally all other drivers go and do that and that device name must",
    "start": "1261110",
    "end": "1267560"
  },
  {
    "text": "be free another thing is a race condition like if you recently use release the device and we were trying to",
    "start": "1267560",
    "end": "1273740"
  },
  {
    "text": "use it again it could cause the volume to be stuck in a touching State so in kubernetes",
    "start": "1273740",
    "end": "1279020"
  },
  {
    "text": "we just for a SS cloud provider we implemented a least release recently",
    "start": "1279020",
    "end": "1284210"
  },
  {
    "text": "used cash of three device names so that we don't reuse the recently released device another thing was",
    "start": "1284210",
    "end": "1291740"
  },
  {
    "text": "sometimes like saris trying to be helpful and they would force detach a",
    "start": "1291740",
    "end": "1297200"
  },
  {
    "text": "volume from a node on AWS to you know night we just make the workload go make the pod run or whatever but what happens",
    "start": "1297200",
    "end": "1304490"
  },
  {
    "text": "is that just makes problems worse we found out and any other volume that tries to attach on the node will also",
    "start": "1304490",
    "end": "1310580"
  },
  {
    "text": "get stuck in attaching state forever so in the end we this disk or is there",
    "start": "1310580",
    "end": "1315770"
  },
  {
    "text": "actually an entry we started we started painting the node if the attach times out in AWS and you have to reboot the",
    "start": "1315770",
    "end": "1323419"
  },
  {
    "text": "node delete the node object to make it work and kind of solves some of the pain",
    "start": "1323419",
    "end": "1329440"
  },
  {
    "text": "there is another problem that in particular AWS cloud wire we have observed is that it's API is our",
    "start": "1329440",
    "end": "1336620"
  },
  {
    "text": "eventual consistent in particular immutable API calls like describe volume",
    "start": "1336620",
    "end": "1342440"
  },
  {
    "text": "describe instance so some some examples are like we detach a volume and we call describe volume and a tabular says it's",
    "start": "1342440",
    "end": "1349669"
  },
  {
    "text": "attached we attach a volume and we say describe volume and describe all um",
    "start": "1349669",
    "end": "1355970"
  },
  {
    "text": "returns is detached so and sometimes in worst case it can go back in time",
    "start": "1355970",
    "end": "1362200"
  },
  {
    "text": "kubernetes unfortunately well at least initially was not written to deal with",
    "start": "1362200",
    "end": "1368240"
  },
  {
    "text": "this kind of problems so how we fixed it and it benefits other cloud providers",
    "start": "1368240",
    "end": "1374059"
  },
  {
    "text": "and volume types to is we introduced uncertain state in an in controller",
    "start": "1374059",
    "end": "1379520"
  },
  {
    "text": "manager where certain volumes when we cannot be sure if it is attached will mark it uncertain still we are learning",
    "start": "1379520",
    "end": "1387289"
  },
  {
    "text": "and trying to fix the cases but that's how we mostly handled it and we still",
    "start": "1387289",
    "end": "1394159"
  },
  {
    "text": "love AWS is it's been pain and it and we are trying to work with people in Amazon",
    "start": "1394159",
    "end": "1401090"
  },
  {
    "text": "and you know trying to do things so yeah next we are going to so this was about",
    "start": "1401090",
    "end": "1410690"
  },
  {
    "start": "1408000",
    "end": "1497000"
  },
  {
    "text": "the issues that we fixed you also have couple of issues that we did not fix yet and we are working on them the first one",
    "start": "1410690",
    "end": "1417559"
  },
  {
    "text": "that we would like to fix as soon as possible is our FS group FS group is a",
    "start": "1417559",
    "end": "1423080"
  },
  {
    "text": "feature of kubernetes when date on volume can be owned by a",
    "start": "1423080",
    "end": "1428809"
  },
  {
    "text": "random UID and GID and also a poet runs as different random UID orgy ID and so",
    "start": "1428809",
    "end": "1436429"
  },
  {
    "text": "in order to make the data on the volume available to the pot we have a called we",
    "start": "1436429",
    "end": "1442789"
  },
  {
    "text": "have a feature called FS group when we recursively change ownership of every single file on the volume to a certain",
    "start": "1442789",
    "end": "1449929"
  },
  {
    "text": "group that works well until the volume is huge and has ten thousands millions",
    "start": "1449929",
    "end": "1457519"
  },
  {
    "text": "of files and it can take minutes hours days to recursively change the ownership",
    "start": "1457519",
    "end": "1465349"
  },
  {
    "text": "and the problem is that we change the ownership with every single pot or start",
    "start": "1465349",
    "end": "1472840"
  },
  {
    "text": "so yesterday we had a fruit of fruitful discussion here on cube con how to fix",
    "start": "1472840",
    "end": "1479599"
  },
  {
    "text": "it so currently it looks like we will introduce new API field where you could",
    "start": "1479599",
    "end": "1485090"
  },
  {
    "text": "change the own blood would change the ownership just once and not with every",
    "start": "1485090",
    "end": "1490340"
  },
  {
    "text": "pot a start we will see how far it goes and if we can pass API review if that",
    "start": "1490340",
    "end": "1497320"
  },
  {
    "start": "1497000",
    "end": "1651000"
  },
  {
    "text": "another problem that those who are running kubernetes on like maybe vmware",
    "start": "1497320",
    "end": "1504379"
  },
  {
    "text": "or somewhere else might have seen more is like kubernetes actually does not detach volume from shutdown notes so",
    "start": "1504379",
    "end": "1510679"
  },
  {
    "text": "what happens is like if you shut down a node it does a weak parts generally not always it there's a there's other logic",
    "start": "1510679",
    "end": "1518029"
  },
  {
    "text": "built in but it evict the pod but you will see that the the new parts on the new nodes replace nulls do not are not",
    "start": "1518029",
    "end": "1524899"
  },
  {
    "text": "able to come up because the persistent volume is still attached to the old node and they are not able to come up and the",
    "start": "1524899",
    "end": "1533029"
  },
  {
    "text": "the reason for this is that the parts of shutdown not knows typically do not get",
    "start": "1533029",
    "end": "1538729"
  },
  {
    "text": "deleted they stay in unknown state because kubilay cubelet is not there so it's not able to signal the weather pod",
    "start": "1538729",
    "end": "1544820"
  },
  {
    "text": "is still running or not so so it's unsafe state and kubernetes does not detach water from pots in such state and",
    "start": "1544820",
    "end": "1552559"
  },
  {
    "text": "that's the reason we are not quite able to recover volumes from such No",
    "start": "1552559",
    "end": "1557920"
  },
  {
    "text": "our recommendation currently is to terminate the node rather shut the shut down the node if you are running in a",
    "start": "1557920",
    "end": "1565630"
  },
  {
    "text": "cloud provider like AWS GCE GC then if you run your cluster in auto scaling group or gke generally shutting down the",
    "start": "1565630",
    "end": "1573850"
  },
  {
    "text": "node will cause the node to be deleted and replaced and in that case volumes are automatically detached from related",
    "start": "1573850",
    "end": "1579640"
  },
  {
    "text": "note and everything works perfect for better bare metal clusters or cloud",
    "start": "1579640",
    "end": "1584680"
  },
  {
    "text": "providers that don't allow easy replacement of a node it's a bigger problem because in that case you cannot",
    "start": "1584680",
    "end": "1590110"
  },
  {
    "text": "just do that so our current solution is to write external controller that can monitor for shutdown nodes if you have",
    "start": "1590110",
    "end": "1596980"
  },
  {
    "text": "noticed the shutdown or generally have a shutdown taint and you could false delete the parts from those nodes",
    "start": "1596980",
    "end": "1602370"
  },
  {
    "text": "there's a cap open and they're still consensus being built like how do how do",
    "start": "1602370",
    "end": "1608680"
  },
  {
    "text": "we solve this problem for good another",
    "start": "1608680",
    "end": "1614500"
  },
  {
    "text": "issue is that we know about is empty dear if you have a pot that uses empty",
    "start": "1614500",
    "end": "1620320"
  },
  {
    "text": "there all these empty directories are on the same drive so if there is a",
    "start": "1620320",
    "end": "1628240"
  },
  {
    "text": "malicious pot that uses the empty DEP too much it can trash IO for the other",
    "start": "1628240",
    "end": "1633580"
  },
  {
    "text": "users we currently don't have any solution for that you don't even have a",
    "start": "1633580",
    "end": "1638650"
  },
  {
    "text": "design so if you are affected by this please come to us and we would welcome",
    "start": "1638650",
    "end": "1647110"
  },
  {
    "text": "any contribution here this is always",
    "start": "1647110",
    "end": "1653170"
  },
  {
    "start": "1651000",
    "end": "1695000"
  },
  {
    "text": "issues it's looking like a pattern like so this is if you are using encrypted",
    "start": "1653170",
    "end": "1658420"
  },
  {
    "text": "EBS volumes you might notice that sometimes the volumes come with nonzero",
    "start": "1658420",
    "end": "1663640"
  },
  {
    "text": "data and that causes the cubular to detect a random filesystem signature on",
    "start": "1663640",
    "end": "1669070"
  },
  {
    "text": "them on the volume and cubelet is not designed to overwrite file systems so if",
    "start": "1669070",
    "end": "1674650"
  },
  {
    "text": "you are requested ext4 filesystem and encrypted volume says I have ms-dos partition that kubernetes will not",
    "start": "1674650",
    "end": "1681700"
  },
  {
    "text": "delete that ms-dos partition and and it would rather fail the mount operation basically this is something",
    "start": "1681700",
    "end": "1688630"
  },
  {
    "text": "that we are working with Amazon and trying to fix it but it's a pain so yeah",
    "start": "1688630",
    "end": "1695320"
  },
  {
    "start": "1695000",
    "end": "1736000"
  },
  {
    "text": "in summary I would say that fixing bugs is I don't never-ending process six",
    "start": "1695320",
    "end": "1700870"
  },
  {
    "text": "storage has been involved in this effort and we welcome more contribution from",
    "start": "1700870",
    "end": "1705940"
  },
  {
    "text": "folks who are here and we have learned from our failures and one other thing",
    "start": "1705940",
    "end": "1711010"
  },
  {
    "text": "that yon was saying is like we have started running our matrix of tests so",
    "start": "1711010",
    "end": "1716140"
  },
  {
    "text": "we have like sub paths with local volume gcpd plus a path plus all the volume",
    "start": "1716140",
    "end": "1722500"
  },
  {
    "text": "tests so these days like our test suit coverage is being like 40% of tests",
    "start": "1722500",
    "end": "1728919"
  },
  {
    "text": "Internet is is pretty much tourists us as much testing is going on and storage",
    "start": "1728919",
    "end": "1736350"
  },
  {
    "text": "looking in the talk it sounds like we lost data quite a bit but we do not lose",
    "start": "1736410",
    "end": "1742630"
  },
  {
    "text": "data all the time it's it's sometime and then sometimes user interaction causes",
    "start": "1742630",
    "end": "1748600"
  },
  {
    "text": "it and we're still amazed by user creativity how users use persistent",
    "start": "1748600",
    "end": "1754390"
  },
  {
    "text": "volumes we did not expect a user to use local volume the root disk has local",
    "start": "1754390",
    "end": "1759880"
  },
  {
    "text": "volume and sub Park and that is another good but we did not expect that there was a bug so we fixed it and we try to",
    "start": "1759880",
    "end": "1766660"
  },
  {
    "text": "ensure that this does not happen so that's pretty much our talk if there's",
    "start": "1766660",
    "end": "1772840"
  },
  {
    "start": "1770000",
    "end": "2222000"
  },
  {
    "text": "any questions then [Applause]",
    "start": "1772840",
    "end": "1784740"
  },
  {
    "text": "hi I see you recommended using stateful set over deployment what we have seen is",
    "start": "1797640",
    "end": "1805530"
  },
  {
    "text": "in Stateville set whenever a node goes down the problem is it doesn't do a failover and if node went down",
    "start": "1805530",
    "end": "1814950"
  },
  {
    "text": "permanently it's fine but if network partition and all how do we detect those kind of scenario and we make sure that",
    "start": "1814950",
    "end": "1821670"
  },
  {
    "text": "app is not affected by this failure yeah so straight full set the question was we",
    "start": "1821670",
    "end": "1830280"
  },
  {
    "text": "recommended you stateful set our deployment and the when estate when a node goes down the replacement parts are",
    "start": "1830280",
    "end": "1838230"
  },
  {
    "text": "not created when you're using stateful set and the reason for that is because the part on the that that is backed up",
    "start": "1838230",
    "end": "1846330"
  },
  {
    "text": "by stateful set on the shutdown node or the node that is down like crash or whatever is still actually around and",
    "start": "1846330",
    "end": "1852480"
  },
  {
    "text": "still stateful set will not create a replacement part until the old part is",
    "start": "1852480",
    "end": "1857900"
  },
  {
    "text": "deleted so our current recommendation is like is until we solve this problem for",
    "start": "1857900",
    "end": "1863820"
  },
  {
    "text": "both stateful sat and diplomas to force delete the part and and you could detect like if you're running in cloud provider",
    "start": "1863820",
    "end": "1869669"
  },
  {
    "text": "as I said the node such nodes will have a shutdown taint so you could pretty much find out right external controller",
    "start": "1869669",
    "end": "1876660"
  },
  {
    "text": "and list all the all the nodes that have shutdown taint that have parts and in unknown State and you could delete them",
    "start": "1876660",
    "end": "1882809"
  },
  {
    "text": "and that will cause the replacement parts to be created I hope that you are",
    "start": "1882809",
    "end": "1889770"
  },
  {
    "text": "mentioned storage in Missouri AWS and Google wondering if it's anything",
    "start": "1889770",
    "end": "1895890"
  },
  {
    "text": "different in Azure or most of the still applies the same well we run most of our",
    "start": "1895890",
    "end": "1904620"
  },
  {
    "text": "tests on AWS so that's what we tested the most and we noticed these issues",
    "start": "1904620",
    "end": "1909950"
  },
  {
    "text": "like they don't happen always they are super hard to reproduce but they sometimes happen but we haven't noticed",
    "start": "1909950",
    "end": "1917490"
  },
  {
    "text": "these issues on GCE yet but maybe that's because we don't run this the test that that much so we",
    "start": "1917490",
    "end": "1925230"
  },
  {
    "text": "don't know on Azure I think like the we have just started testing like the the",
    "start": "1925230",
    "end": "1930720"
  },
  {
    "text": "full e to e of kubernetes on as you so as it's a",
    "start": "1930720",
    "end": "1936060"
  },
  {
    "text": "process that I think will go through sometimes it takes time to discover bugs on a cloud provider so hmong firm",
    "start": "1936060",
    "end": "1946470"
  },
  {
    "text": "foundation abhi so about your testicles suppose you have a new cronikeys release",
    "start": "1946470",
    "end": "1953400"
  },
  {
    "text": "how long does that has to take can you",
    "start": "1953400",
    "end": "1959520"
  },
  {
    "text": "say it again sorry so so basically how faster it does your test start run to",
    "start": "1959520",
    "end": "1968460"
  },
  {
    "text": "get enough confidence like okay this version of the kubernetes doesn't have",
    "start": "1968460",
    "end": "1974130"
  },
  {
    "text": "all of those boxes mentioned we have several layers of tests we run some",
    "start": "1974130",
    "end": "1979170"
  },
  {
    "text": "tests for every PR we have flow tests that run every I don't know one hour",
    "start": "1979170",
    "end": "1985130"
  },
  {
    "text": "there are some scale tests like you can't say how long does it take for",
    "start": "1985130",
    "end": "1990720"
  },
  {
    "text": "example for every pool requires the tests take I don't know 30 minutes maybe the slow tests again like 30 minutes and",
    "start": "1990720",
    "end": "1997980"
  },
  {
    "text": "I have no clue how the stands for one has the case when to one the hotel suite",
    "start": "1997980",
    "end": "2004430"
  },
  {
    "text": "the hosts way okay but then there are the scale tests and big tests and I",
    "start": "2004430",
    "end": "2011090"
  },
  {
    "text": "don't know how long they take three hours maybe I don't know okay so is this test the switch like",
    "start": "2011090",
    "end": "2017840"
  },
  {
    "text": "open sourced yes of course yeah it's in kubernetes there's a kubernetes",
    "start": "2017840",
    "end": "2024860"
  },
  {
    "text": "sometimes I think you may discover it is some scale tests are kubernetes / scalability testing I guess there's a",
    "start": "2024860",
    "end": "2031880"
  },
  {
    "text": "report where scale tests are managed to see different repository but everything is everything is open and you can see",
    "start": "2031880",
    "end": "2038570"
  },
  {
    "text": "the test results also there are public",
    "start": "2038570",
    "end": "2043090"
  },
  {
    "text": "hey I have a question regarding the access modes so I have been trying",
    "start": "2044350",
    "end": "2051320"
  },
  {
    "text": "different storage providers starting from 1 point 1 4 in 1 point 1 4 I could",
    "start": "2051320",
    "end": "2057800"
  },
  {
    "text": "able to bye and mount my volume to multiple parts on the same node it different",
    "start": "2057800",
    "end": "2066510"
  },
  {
    "text": "storage providers behave differently on 1.16 I'm seeing a different behavior now",
    "start": "2066510",
    "end": "2072138"
  },
  {
    "text": "Luke was working for me where I cut multiple attacks on the same node now it",
    "start": "2072139",
    "end": "2079050"
  },
  {
    "text": "is broken looks like kubernetes upstream code has changed in that way there was a github",
    "start": "2079050",
    "end": "2086810"
  },
  {
    "text": "issue open issue I'm not sure whether it is closed but there was a huge discussion going on",
    "start": "2086810",
    "end": "2092398"
  },
  {
    "text": "so basically reiterate once has to whether it should support or do you want",
    "start": "2092399",
    "end": "2099000"
  },
  {
    "text": "to add a new access mode to support multi-part attach because it is just a bind mount two different paths so what",
    "start": "2099000",
    "end": "2107820"
  },
  {
    "text": "is the expectation here so I think the about the question like the roof like",
    "start": "2107820",
    "end": "2116400"
  },
  {
    "text": "looking particularly I used to have one access mode then access mode changed yeah but so other storages also I have",
    "start": "2116400",
    "end": "2125070"
  },
  {
    "text": "pride so yeah each each storage type could define its own access mode like like NFS has readwrite many you could",
    "start": "2125070",
    "end": "2132270"
  },
  {
    "text": "mount no I'm talking about only readwrite once okay only for this access",
    "start": "2132270",
    "end": "2137850"
  },
  {
    "text": "more storage border for behaving differently okay it depends your so yeah so readwrite once it depends like for",
    "start": "2137850",
    "end": "2144540"
  },
  {
    "text": "example a luke deployment that uses CSI and if they if they disable attach",
    "start": "2144540",
    "end": "2151170"
  },
  {
    "text": "required in CSI driver then as I said the attach detach they will not they",
    "start": "2151170",
    "end": "2157440"
  },
  {
    "text": "will not have attached to start and and volume could be directly mounted on a node and in that case readwrite once",
    "start": "2157440",
    "end": "2163800"
  },
  {
    "text": "access mode will not be enforced the user access mode sorry",
    "start": "2163800",
    "end": "2170070"
  },
  {
    "text": "the reason is that the access modes are not enforced almost anywhere except for",
    "start": "2170070",
    "end": "2175470"
  },
  {
    "text": "attach detach so in theory any storage plug-in can run two ports on the same",
    "start": "2175470",
    "end": "2183660"
  },
  {
    "text": "node we freed run months and",
    "start": "2183660",
    "end": "2187880"
  },
  {
    "text": "I'm not sure we can fix it because this is a behavior that quite some people are",
    "start": "2189119",
    "end": "2195609"
  },
  {
    "text": "used to and we would break their applications if we start enforcing the",
    "start": "2195609",
    "end": "2201819"
  },
  {
    "text": "access mode like too much it is not enforcing I'm asking it is relaxing I",
    "start": "2201819",
    "end": "2209319"
  },
  {
    "text": "mean I know if you could come forward and speak um events are out of time so",
    "start": "2209319",
    "end": "2214560"
  },
  {
    "text": "thank you thank you okay you can talk you can talk",
    "start": "2214560",
    "end": "2221369"
  },
  {
    "text": "[Applause]",
    "start": "2221560",
    "end": "2224260"
  }
]