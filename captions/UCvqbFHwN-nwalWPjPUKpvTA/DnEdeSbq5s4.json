[
  {
    "start": "0",
    "end": "77000"
  },
  {
    "text": "it's okay to start or yeah yeah I think we're scheduled to starts twenty past",
    "start": "810",
    "end": "6750"
  },
  {
    "text": "two so I think it should be alright hopefully be on time okay I run thanks thanks for joining me today",
    "start": "6750",
    "end": "14190"
  },
  {
    "text": "I will be in the next half hour so I'll be taking taking you through my",
    "start": "14190",
    "end": "20699"
  },
  {
    "text": "experience with one of our clients of how we basically were an early adopter",
    "start": "20699",
    "end": "25710"
  },
  {
    "text": "of service measures and half an hour journey as it went through linker d1 and",
    "start": "25710",
    "end": "30720"
  },
  {
    "text": "all the way through linker d2 and what how that looked like from from from that",
    "start": "30720",
    "end": "37590"
  },
  {
    "text": "perspective so quick control so my name is Dylan I'm a senior consultant at open Credo we are",
    "start": "37590",
    "end": "43680"
  },
  {
    "text": "based in London UK and what we do is basically we are server hands-on",
    "start": "43680",
    "end": "49980"
  },
  {
    "text": "consultancy where we help you know our clients all sorts of sizes from smaller companies to big enterprises with their",
    "start": "49980",
    "end": "57109"
  },
  {
    "text": "cloud and if to help design their cloud native solutions and work on making sure",
    "start": "57109",
    "end": "63480"
  },
  {
    "text": "the designs are scalable and good and future-proof you can find me on either",
    "start": "63480",
    "end": "68790"
  },
  {
    "text": "on WeChat or on on Twitter via the handles on on on the slides are",
    "start": "68790",
    "end": "74670"
  },
  {
    "text": "shoulders on at the end as well so what did he agenda for today I'm gonna take",
    "start": "74670",
    "end": "80070"
  },
  {
    "start": "77000",
    "end": "77000"
  },
  {
    "text": "you to a brief intro of service measures and what they are if some of you still is some of you are unfamiliar with them",
    "start": "80070",
    "end": "86189"
  },
  {
    "text": "will be more of a practical example because I think like serve those end up being better than the numerous",
    "start": "86189",
    "end": "92310"
  },
  {
    "text": "theoretical is one that you'll find online I'm going to talk about the evolution of service measures and how",
    "start": "92310",
    "end": "97439"
  },
  {
    "text": "that applies to link ready or losing Cody is a great example of how serious measures evolved through time as the",
    "start": "97439",
    "end": "103409"
  },
  {
    "text": "needs and the landscape changed I'm going to talk about how we did our redesign how we redesign architecture at",
    "start": "103409",
    "end": "110280"
  },
  {
    "text": "least three times if not more and how we went through the how we took the path of migrating you know without much issues",
    "start": "110280",
    "end": "117479"
  },
  {
    "text": "basically and I'm going to talk a little bit a few a little bit about lessons learned on what could we do better",
    "start": "117479",
    "end": "124920"
  },
  {
    "text": "what do we well what you should avoid when doing so in Europe ok let's start so just a quick",
    "start": "124920",
    "end": "131900"
  },
  {
    "text": "note of what is a service mesh I will find many definitions online what is the",
    "start": "131900",
    "end": "137360"
  },
  {
    "text": "service mesh but when our life on is that it service is an approach and a dedicated infrastructure layer for",
    "start": "137360",
    "end": "143780"
  },
  {
    "text": "operating and secure fast and reliable Microcystis ecosystem now that's a loaded statement as in there's a lot in",
    "start": "143780",
    "end": "152840"
  },
  {
    "text": "there and it's sometimes difficult to actually grasp what it is the service my shares and what it is it would do for you when you're developing your cloud",
    "start": "152840",
    "end": "159829"
  },
  {
    "text": "native highly scalable applications so I've heard a bunch of things from all sorts of people about trans Bay to me",
    "start": "159829",
    "end": "165860"
  },
  {
    "text": "what it is and these are just a few things that I heard for instance it's a whole new paradigm of deploying stuff",
    "start": "165860",
    "end": "171319"
  },
  {
    "text": "that cloud ok maybe it is a low latency infrastructure layer it is a level 7",
    "start": "171319",
    "end": "178879"
  },
  {
    "text": "network exclusively for applications for instance it's just kubernetes and",
    "start": "178879",
    "end": "184220"
  },
  {
    "text": "extension to kubernetes I even heard someone say it's magic just does a lot of stuff it's not quite magic but it is",
    "start": "184220",
    "end": "191660"
  },
  {
    "text": "interesting indeed let me show you on an example of what what a service mesh does",
    "start": "191660",
    "end": "197420"
  },
  {
    "text": "for you just a quick one before we before we deep dive in so let's say we have two services now they could be",
    "start": "197420",
    "end": "203780"
  },
  {
    "start": "199000",
    "end": "199000"
  },
  {
    "text": "micro services it could be normal services it for the most part it doesn't really matter what they are for instance let's say we have a payment service and",
    "start": "203780",
    "end": "210380"
  },
  {
    "text": "we have ledger service and that connects to like a fourth system and external fraud system a third party if you may",
    "start": "210380",
    "end": "216730"
  },
  {
    "text": "you make a payment it makes send you the ledger it sends the info to the fault system for analysis very simple example now you might have",
    "start": "216730",
    "end": "224780"
  },
  {
    "text": "an Alexa all those are connected via HTTP or any sorts of other network protocol so all right can we how do I do",
    "start": "224780",
    "end": "238370"
  },
  {
    "text": "that",
    "start": "238370",
    "end": "240579"
  },
  {
    "text": "I figured out hey as ever yes sir better is the colors are a bit",
    "start": "243560",
    "end": "250550"
  },
  {
    "text": "off cool so three simple components are you sir your system I will typically have a lot",
    "start": "250550",
    "end": "257359"
  },
  {
    "text": "more but for probably the sake let's start with these now typically your starts some of your components will",
    "start": "257359",
    "end": "263120"
  },
  {
    "text": "start scaling up for instance we wanted princess is more more intensive for a ledger service because we have a lot",
    "start": "263120",
    "end": "268280"
  },
  {
    "text": "more load on it and we needed to handle it that means that the payment is still now needs to do a few things first of",
    "start": "268280",
    "end": "276290"
  },
  {
    "text": "all needs to know that there is more than one system it needs to load balance between them hey maybe needs to handle",
    "start": "276290",
    "end": "282950"
  },
  {
    "text": "if things go wrong for instance what if one of the instances in ledger is you know foes in a bit slow down one of it",
    "start": "282950",
    "end": "288740"
  },
  {
    "text": "is dead completely how do you deal with that in a fast-paced production environment well it turns out just doing",
    "start": "288740",
    "end": "295610"
  },
  {
    "text": "HTTP request is a lot of times not enough because your your availability will go down your error rates will spike",
    "start": "295610",
    "end": "302150"
  },
  {
    "text": "up so we try to mitigate those by doing a few things for instance what we're trying to do is well first of all we",
    "start": "302150",
    "end": "307970"
  },
  {
    "text": "want it to be discovering instances we want to be dynamic so we'll implement service discovery so we don't need to",
    "start": "307970",
    "end": "313190"
  },
  {
    "text": "statically set IP addresses or URLs for instances we want to load balance",
    "start": "313190",
    "end": "318320"
  },
  {
    "text": "between them we could use a little bouncer but that's an extra hop we might do collage mode balancing for instance",
    "start": "318320",
    "end": "323330"
  },
  {
    "text": "it's a good way of doing a bit about it we want the circuit backing if an instance is dead or performs badly or",
    "start": "323330",
    "end": "329150"
  },
  {
    "text": "doesn't perform at all we want to stop sending messages to it potentially with a fallback that is local to the service",
    "start": "329150",
    "end": "335210"
  },
  {
    "text": "itself of course you may want to implement retries for instance if a call",
    "start": "335210",
    "end": "340940"
  },
  {
    "text": "fails maybe if I try it again a different service you'll succeed and you know when from the outside looking in",
    "start": "340940",
    "end": "347330"
  },
  {
    "text": "the call would ultimately be fine and successful even though there was a retry that happened internally how do we do",
    "start": "347330",
    "end": "353539"
  },
  {
    "text": "with authentication authorization between the services a lot of times you just don't deal with it we assume that",
    "start": "353539",
    "end": "359330"
  },
  {
    "text": "because it's internal to a network it's fine but that might not be the case over time especially in a cloud cloud native",
    "start": "359330",
    "end": "365660"
  },
  {
    "text": "environment and of course you would like all this to be automatic and all of",
    "start": "365660",
    "end": "370789"
  },
  {
    "text": "those features are nowadays usually abbreviated under the term resiliency so we want this to be resilient so we",
    "start": "370789",
    "end": "378560"
  },
  {
    "text": "want the communication between all of this between the services to be resilient and to have all those features",
    "start": "378560",
    "end": "384560"
  },
  {
    "text": "we just mentioned now we could implement those ourselves there's a bunch of libraries you can use whether for Java",
    "start": "384560",
    "end": "390110"
  },
  {
    "text": "for go for c-sharp whatever whatever your services are built-in you can have",
    "start": "390110",
    "end": "395990"
  },
  {
    "text": "for instance history cause a popular one or resiliency for Jay is a new one now and that server supersedes it we could",
    "start": "395990",
    "end": "401240"
  },
  {
    "text": "use kubernetes for service discovery we could use ribbon for client-side load balancing and we can use some custom code to implement retries and",
    "start": "401240",
    "end": "408910"
  },
  {
    "text": "authentications and authorization okay that could work but you know with doing",
    "start": "408910",
    "end": "417139"
  },
  {
    "text": "it only on the pavements it's not enough we're gonna have to do it everywhere we're gonna have to down ledger we're gonna have to do it in all of our",
    "start": "417139",
    "end": "423680"
  },
  {
    "text": "services at the same time now what if we",
    "start": "423680",
    "end": "430669"
  },
  {
    "text": "don't own all of our services what if our ledger is now instead of a third party product that we bought from someone like a core banking product well",
    "start": "430669",
    "end": "437210"
  },
  {
    "text": "we don't have access the code we can't have anything in it well we're out of luck here we can't use any of that over",
    "start": "437210",
    "end": "443449"
  },
  {
    "text": "here so what do we do in this case what are we in this case and in the case",
    "start": "443449",
    "end": "449030"
  },
  {
    "text": "where we want to potentially implement a new thing within our resiliency stack so to speak well we need to go to all we",
    "start": "449030",
    "end": "455060"
  },
  {
    "text": "need to go to every single one of our teams in our organizations and put a ticket or at or a feature request on",
    "start": "455060",
    "end": "461060"
  },
  {
    "text": "their backlog and coordinate everything and it might take months for everyone to get on board with new feature we want to",
    "start": "461060",
    "end": "467449"
  },
  {
    "text": "and of course it takes away from from developers productivity and so on so forth so we want to take that burden",
    "start": "467449",
    "end": "472970"
  },
  {
    "text": "away from the services selves and instead rip it out and put it in a in a",
    "start": "472970",
    "end": "479120"
  },
  {
    "text": "proxy so instead of having all of that in in in inside a citizen code one we have all of that in a single component a",
    "start": "479120",
    "end": "485900"
  },
  {
    "text": "single proxy and the port that next to to our components and just route all the traffic through it and then let the",
    "start": "485900",
    "end": "493909"
  },
  {
    "text": "proxy handle all of the resiliency problems let the proxy handle that ocation let the proxy handle circuit",
    "start": "493909",
    "end": "499099"
  },
  {
    "text": "breaking load balancing service discovery why do we have a deputy decayed component that a team develops",
    "start": "499099",
    "end": "504260"
  },
  {
    "text": "or maintains that does all of this for everyone automatically but that sounds like a lot",
    "start": "504260",
    "end": "509510"
  },
  {
    "text": "of a much better idea actually it is another component to another hop but it does give you a lot of features",
    "start": "509510",
    "end": "515360"
  },
  {
    "text": "that you would need to build manually yourself and sudden and also a problem of third-party vendors it goes away",
    "start": "515360",
    "end": "521210"
  },
  {
    "text": "completely we can now just put it next to our third-party product which we couldn't change before but now we can",
    "start": "521210",
    "end": "526850"
  },
  {
    "text": "augment by using a proxy and we route traffic all traffic to it so basically",
    "start": "526850",
    "end": "533240"
  },
  {
    "start": "533000",
    "end": "533000"
  },
  {
    "text": "what is it service mesh if it offers all",
    "start": "533240",
    "end": "538279"
  },
  {
    "text": "of these features that would that be that I mentioned before service discovery load balancing circle breaking to retry certification authorization so",
    "start": "538279",
    "end": "545630"
  },
  {
    "text": "on so forth and it's automatic and we do that by ripping out the functionality and putting in a proxy and then creating",
    "start": "545630",
    "end": "552710"
  },
  {
    "text": "a mesh of proxies that that connects all the services together so basically if we",
    "start": "552710",
    "end": "558410"
  },
  {
    "text": "dumb it down a little bit even more it's just a collect collective of small smart configurable autonomous proxy and",
    "start": "558410",
    "end": "564160"
  },
  {
    "text": "because that's all there is it's not any more complicated than that in reality",
    "start": "564160",
    "end": "571210"
  },
  {
    "text": "there are certain complexities to how you implement it but generally that is",
    "start": "571210",
    "end": "576920"
  },
  {
    "text": "what it boils down to so proxy which which one to use",
    "start": "576920",
    "end": "582520"
  },
  {
    "text": "did you see nginx H a proxy well those are usually not built to handle all the scenarios so we'll probably go after a",
    "start": "582520",
    "end": "590870"
  },
  {
    "text": "few general if you purposely built one so you'll find nowadays you'll find",
    "start": "590870",
    "end": "596450"
  },
  {
    "text": "quite a few products actually that solve the issues that we're talking about so for instance Linkwood is a popular one",
    "start": "596450",
    "end": "603020"
  },
  {
    "text": "sto is another popular one you could use council for your cert for yours as a",
    "start": "603020",
    "end": "608030"
  },
  {
    "text": "service mesh envoi is usually used as a proxy in sto for instance or in other systems even have Amazon's managed app",
    "start": "608030",
    "end": "615350"
  },
  {
    "text": "mesh or even kong's which is primarily an API gateway now moved into the serious mesh space as well so there's a",
    "start": "615350",
    "end": "622160"
  },
  {
    "text": "lot of open providers that are established and a lot of up-and-coming ones as well but let's go back to when",
    "start": "622160",
    "end": "628610"
  },
  {
    "text": "it all started like one of the first one that was linker D so that's let's let's",
    "start": "628610",
    "end": "634250"
  },
  {
    "text": "take a journey that I took and the journey that the general community of service meshes took of how we came up",
    "start": "634250",
    "end": "640710"
  },
  {
    "text": "from where we started to where we are right now so link ready one was one of the first service measures I suppose",
    "start": "640710",
    "end": "647400"
  },
  {
    "start": "646000",
    "end": "646000"
  },
  {
    "text": "service matchboxes out there and what it was it was basically a single app it was a single app and an all-in-one network",
    "start": "647400",
    "end": "655890"
  },
  {
    "text": "Boxey based on pinnacle made out of came out of Twitter basically vinegar was a part I came out of Twitter it was a good",
    "start": "655890",
    "end": "662400"
  },
  {
    "text": "idea clients that support a lot of the features that we need it and got bundled into link OD as a way of dynamically",
    "start": "662400",
    "end": "667770"
  },
  {
    "text": "configuring it and acting as a proxy on a mesh now it runs in the JVM which may",
    "start": "667770",
    "end": "673080"
  },
  {
    "text": "or may not be ok but in this case it was okay as it allows agency now it supports writing policies it supports most of the",
    "start": "673080",
    "end": "680010"
  },
  {
    "text": "resiliency requirements that we talked about before it is it has a pluggable design which means that it doesn't make",
    "start": "680010",
    "end": "685710"
  },
  {
    "text": "any assumption but later on so you can run it on a box you can run on a Raspberry Pi I ran it on kubernetes whatever you want and you can plug in",
    "start": "685710",
    "end": "692910"
  },
  {
    "text": "different components into it for instance service discovery into kubernetes or service discovery into console or somewhere or there's a lot of",
    "start": "692910",
    "end": "700290"
  },
  {
    "text": "plugins you can take and has a single relatively simple config and I'll see",
    "start": "700290",
    "end": "706260"
  },
  {
    "text": "what I mean it looks something like this basically so for instance on the left -",
    "start": "706260",
    "end": "711690"
  },
  {
    "text": "just an example there's things called D taps which is a bit of a convoluted way",
    "start": "711690",
    "end": "717030"
  },
  {
    "text": "of configuring routing for instance request comes in and then you can have a survey a kind of a table of a path of",
    "start": "717030",
    "end": "723930"
  },
  {
    "text": "branching paths the requests can take for instance request comes in and I need to forward it to the server service ok",
    "start": "723930",
    "end": "730410"
  },
  {
    "text": "sounds like a box you were talking about we do need to learn the language so that's a bit of a pain but we'll do what",
    "start": "730410",
    "end": "736170"
  },
  {
    "text": "ed listen Ischl e and of course on the right for instance that's using llamo we can configure all sorts of parameters",
    "start": "736170",
    "end": "741870"
  },
  {
    "text": "for instance do we want to have retries retries budget do you want to have a",
    "start": "741870",
    "end": "747060"
  },
  {
    "text": "load balancer what type of all bounds do you want to use do we want to use latency based load balancing or just round-robin on balancing and so on so",
    "start": "747060",
    "end": "754080"
  },
  {
    "text": "forth ok so you know other box it sports a few good future features so how do we use it",
    "start": "754080",
    "end": "761790"
  },
  {
    "text": "how do we use it so one of our first use cases for it and this is where really sort of shine is where we weren't using",
    "start": "761790",
    "end": "768990"
  },
  {
    "start": "763000",
    "end": "763000"
  },
  {
    "text": "kubernetes yet so as a few years ago most of the client most of the services",
    "start": "768990",
    "end": "775890"
  },
  {
    "text": "were running either on Prem on virtual machines and some of them we started moving to the cloud on but on normal ec2",
    "start": "775890",
    "end": "781200"
  },
  {
    "text": "instances this is so before the bigger enterprises became comfortable with the idea of running everything in the cloud",
    "start": "781200",
    "end": "786930"
  },
  {
    "text": "or the idea of running everything kubernetes so but still we wanted to",
    "start": "786930",
    "end": "792240"
  },
  {
    "text": "want to try out we wanted to bring some of the resiliency features that sounded so promising and bring into what we have",
    "start": "792240",
    "end": "797910"
  },
  {
    "text": "right now instead of waiting for the promised land later down the line and Linkwood e enables to do linker d1",
    "start": "797910",
    "end": "803550"
  },
  {
    "text": "specifically enables to do just that because it was just a normal JVM app we just run it on the box next to the",
    "start": "803550",
    "end": "809820"
  },
  {
    "text": "service itself now it was a VM or an ec2 instance or a vendor app or any other",
    "start": "809820",
    "end": "817110"
  },
  {
    "text": "cloud app it didn't matter it we just installed it in there and it sort of worked now we had to you know work with",
    "start": "817110",
    "end": "822330"
  },
  {
    "text": "the configuration a little bit had to make sure that serve all them see each other and know where to go but for the",
    "start": "822330",
    "end": "828480"
  },
  {
    "text": "most part this worked and it was very flexible and allows us to deploy it in a hybrid environment so there was a good",
    "start": "828480",
    "end": "835440"
  },
  {
    "text": "take 1 we were very at this point we were quite happy about it and almost nobody was doing this at this point so it was a big",
    "start": "835440",
    "end": "840660"
  },
  {
    "text": "step forward and allows us to to bring back a lot of that into the mesh itself",
    "start": "840660",
    "end": "846270"
  },
  {
    "text": "and basically release the burden from the developers themselves they didn't need to know not know but this didn't",
    "start": "846270",
    "end": "852990"
  },
  {
    "text": "necessarily need to care about doing all that stuff for every one of their micro services and tweaking every one of their",
    "start": "852990",
    "end": "858330"
  },
  {
    "text": "Microsoft's individually you instead had a single unified place of doing that ok",
    "start": "858330",
    "end": "865770"
  },
  {
    "text": "worked out for a while and then we started looking at kubernetes well we actually start once using it we wanted",
    "start": "865770",
    "end": "872100"
  },
  {
    "text": "to deploy some of our work since kubernetes how do we plug kubernetes into this well as you probably can guess",
    "start": "872100",
    "end": "879620"
  },
  {
    "text": "we'll probably an have a kubernetes cluster we're gonna have to put a link ready on it somehow and then hook it up",
    "start": "879620",
    "end": "884910"
  },
  {
    "text": "into this mesh so that's straightforward but it was a problem linka d is not light on resources no",
    "start": "884910",
    "end": "894370"
  },
  {
    "text": "recommendation is a recommended amount of heat memories at least a gig if not more you can easily listen to the CPU so",
    "start": "894370",
    "end": "901030"
  },
  {
    "text": "this worked fine for ec2 instance SL and virtual machines because usually they had enough overhead to handle this but",
    "start": "901030",
    "end": "907420"
  },
  {
    "text": "for containers or for parts better running it as a sidecar it's probably",
    "start": "907420",
    "end": "913570"
  },
  {
    "text": "not the best idea to consume way too many resources and would blow to our basically kubernetes installation and",
    "start": "913570",
    "end": "919210"
  },
  {
    "text": "make it slow so step we do it something like this which was the recommended approach at that time anyway was instead",
    "start": "919210",
    "end": "925870"
  },
  {
    "text": "of running it as a sidecar brand as a daemon set now effectively a single",
    "start": "925870",
    "end": "931180"
  },
  {
    "text": "linker the instance would handle all of the parts on a single note and then all the communications would go through it",
    "start": "931180",
    "end": "936940"
  },
  {
    "text": "and then you could it would communicate between itself on a node level and then",
    "start": "936940",
    "end": "942370"
  },
  {
    "text": "once the request hits a target note if we then forward it to a target service so effectively we had a mesh on a node",
    "start": "942370",
    "end": "947980"
  },
  {
    "text": "level in each node had a collection of services it took a little bit of time",
    "start": "947980",
    "end": "954340"
  },
  {
    "text": "figuring this out on how to make it work but eventually we plugged it in here and sort of just connected to everything else and again it worked",
    "start": "954340",
    "end": "960790"
  },
  {
    "text": "although our config at this point was really bloated and luckily there was only a few of us working on this so it",
    "start": "960790",
    "end": "967120"
  },
  {
    "text": "wasn't it wasn't that big of deal being weird but we had to introduce this to hundreds of developers are working on",
    "start": "967120",
    "end": "973750"
  },
  {
    "text": "everything else it would been a pain in getting everyone to understand how all",
    "start": "973750",
    "end": "979720"
  },
  {
    "text": "of this sort of comes together so in general it became a pain to manage this",
    "start": "979720",
    "end": "985560"
  },
  {
    "text": "now eventually compliance took over the",
    "start": "985560",
    "end": "992230"
  },
  {
    "text": "world and so it was it took over us as well and we got rid of all of the leg a legacy stuff no more ec2 instances no",
    "start": "992230",
    "end": "998740"
  },
  {
    "text": "more on-prem stuff and we just everything running kubernetes and it",
    "start": "998740",
    "end": "1004110"
  },
  {
    "text": "looks kind of like this which is basically is a standard MSI deployment and extended its all of our work lots in common areas now again this worked fine",
    "start": "1004110",
    "end": "1010740"
  },
  {
    "text": "for most of our but introduced a lot of problems as we'll see soon enough",
    "start": "1010740",
    "end": "1015840"
  },
  {
    "text": "the Lister which is that what we have nice and great dashboards for for instance our our server or Lincoln",
    "start": "1015840",
    "end": "1021840"
  },
  {
    "text": "instance like wit services it is calling what's the traffic and so on you only got a dashboard per instance you did not",
    "start": "1021840",
    "end": "1029939"
  },
  {
    "text": "get a collective dashboard of everything that was happening or mesh which means it was quite a pain and actually figuring out what was going on because",
    "start": "1029940",
    "end": "1036420"
  },
  {
    "text": "you have to go to every instance and figure out where the traffic is coming from where it's going and you had to and",
    "start": "1036420",
    "end": "1043079"
  },
  {
    "text": "if you had like hundreds of nodes it's just just not possible now eventually yes we did we didn't move to Prometheus",
    "start": "1043080",
    "end": "1048870"
  },
  {
    "text": "and start building or fauna dashboards to go around the issues but there was a lot of steps between us wanting to",
    "start": "1048870",
    "end": "1054960"
  },
  {
    "text": "figure out what was going on on our network and us solving it and as viewing it in a single dashboard we had to it",
    "start": "1054960",
    "end": "1060570"
  },
  {
    "text": "Prometheus Griffin I everyone had to manage his design graphs so on so forth there was a lot of all things that",
    "start": "1060570",
    "end": "1067590"
  },
  {
    "start": "1067000",
    "end": "1067000"
  },
  {
    "text": "happened there so what actually the problem well as I mentioned solid instances in every note because of large",
    "start": "1067590",
    "end": "1075120"
  },
  {
    "text": "uses consumptions which means you had a proxy per node instead of per instance the configuration became complex so were",
    "start": "1075120",
    "end": "1082680"
  },
  {
    "text": "the updates it didn't support dynamic updates monitoring was very disjointed as you saw and of course we didn't have",
    "start": "1082680",
    "end": "1088950"
  },
  {
    "text": "any proper MPLS support because we didn't we couldn't do service to service and the last only note to note which was",
    "start": "1088950",
    "end": "1094380"
  },
  {
    "text": "better than nothing there is not what we wanted at the end and the developer friction was quite high we found out",
    "start": "1094380",
    "end": "1101160"
  },
  {
    "text": "that understanding that developers had a hard time understanding what was going on and a lot of times blamed the service",
    "start": "1101160",
    "end": "1106530"
  },
  {
    "text": "mesh for things are really not the service meshes fault or they just didn't understand how it worked so that's what",
    "start": "1106530",
    "end": "1114030"
  },
  {
    "text": "we ended up with linker t1 at the end of",
    "start": "1114030",
    "end": "1119520"
  },
  {
    "text": "the day so clearly the architecture there was designed for as reached an answer or speak so where to go next",
    "start": "1119520",
    "end": "1126900"
  },
  {
    "text": "where did the community all right all the different providers go from there how how would you design a service mesh",
    "start": "1126900",
    "end": "1134310"
  },
  {
    "start": "1127000",
    "end": "1127000"
  },
  {
    "text": "that works for a modern cloud native kubernetes based environment well you",
    "start": "1134310",
    "end": "1139620"
  },
  {
    "text": "would say how epoxy but instead of having all of those features within a proxy you would strip those out",
    "start": "1139620",
    "end": "1146429"
  },
  {
    "text": "and move them into a control plane and instead keep the poxy only as a proxy",
    "start": "1146429",
    "end": "1151919"
  },
  {
    "text": "and leave all of the configuration and the bloatedness and the setup and the management of",
    "start": "1151919",
    "end": "1158070"
  },
  {
    "text": "certificate within a control plane which is just a normal service and noble deployment into your cluster now how",
    "start": "1158070",
    "end": "1165240"
  },
  {
    "text": "does that control plane look like well it's just a normal deployment and what it does is it manages it configures the",
    "start": "1165240",
    "end": "1171539"
  },
  {
    "start": "1166000",
    "end": "1166000"
  },
  {
    "text": "actual proxies it's a standard stateless deployment it has a public API it",
    "start": "1171539",
    "end": "1178200"
  },
  {
    "text": "collects the telemetry from all the proxies in your clusters and exposes them with a single interface and now",
    "start": "1178200",
    "end": "1184259"
  },
  {
    "text": "super attractive to us because we finally got a single place where we can actually we will finally get a single",
    "start": "1184259",
    "end": "1189659"
  },
  {
    "text": "place require actually put monitor our service measure and can enforce policies",
    "start": "1189659",
    "end": "1194700"
  },
  {
    "text": "it can issue certificates to the target services to enable an TLS and it is",
    "start": "1194700",
    "end": "1199799"
  },
  {
    "text": "fully cloud native fully integrating into kubernetes fully make use of custom resource definitions to configure the",
    "start": "1199799",
    "end": "1206279"
  },
  {
    "text": "service mesh and everything around it as we'll see so that was a control place so",
    "start": "1206279",
    "end": "1211919"
  },
  {
    "text": "we took out all the management all the blow from the single all-in-one all-in-one product and move it into a",
    "start": "1211919",
    "end": "1217980"
  },
  {
    "text": "control plane and as for the proxy while the proxy had only one job left and that",
    "start": "1217980",
    "end": "1223409"
  },
  {
    "start": "1219000",
    "end": "1219000"
  },
  {
    "text": "was just a proxy request and instead of having it in JVM we should write in a",
    "start": "1223409",
    "end": "1229440"
  },
  {
    "text": "language that's a bit more appropriate for it that uses less resources and can be at with the ultimate goal of",
    "start": "1229440",
    "end": "1234899"
  },
  {
    "text": "deploying it as a sidecar to every instance to achieve the true one-to-one proxy to instance ratio that we wanted",
    "start": "1234899",
    "end": "1242100"
  },
  {
    "text": "to and that was basically the general architecture that two of the most",
    "start": "1242100",
    "end": "1247669"
  },
  {
    "text": "popular service measures now took both linker D and sto if you broke them",
    "start": "1247669",
    "end": "1252899"
  },
  {
    "text": "operate the same way they're implemented differently use different technologies under the hood but ultimately they",
    "start": "1252899",
    "end": "1258059"
  },
  {
    "text": "function roughly the same way and perform roughly the same functions now this for us was very exciting like where",
    "start": "1258059",
    "end": "1264720"
  },
  {
    "text": "do we go from here we basically had now because these two are so radically",
    "start": "1264720",
    "end": "1271200"
  },
  {
    "text": "different that we had before there wasn't a straight migration path near this they are all included too so what",
    "start": "1271200",
    "end": "1276600"
  },
  {
    "text": "do we do do we do we go to link early - you go to steer well we deliberated a lot and and you you",
    "start": "1276600",
    "end": "1282190"
  },
  {
    "text": "couldn't go wrong either way to be frank in the in this case but for us we",
    "start": "1282190",
    "end": "1287260"
  },
  {
    "text": "decided to go to linker D and one of the deciding credit to in or deciding the reasons was that well we had good",
    "start": "1287260",
    "end": "1292720"
  },
  {
    "text": "experiences record d1 we wanted to continue that and second of all one of the greatest features of link 32 is that",
    "start": "1292720",
    "end": "1298300"
  },
  {
    "text": "it's a lot more simpler than sto and one of the biggest problems we had to think ready one was that it was so daunting to",
    "start": "1298300",
    "end": "1303940"
  },
  {
    "text": "to hundreds of developers and organizations that they didn't even want to learn it so we wanted something more",
    "start": "1303940",
    "end": "1309820"
  },
  {
    "text": "simple as something that everyone can pick up and they don't need to fully understand it they can if they want to but something they are know how it works",
    "start": "1309820",
    "end": "1317350"
  },
  {
    "text": "and they can sort of grasp I relatively quickly and that was a very attractive for us in this particular case so what",
    "start": "1317350",
    "end": "1323860"
  },
  {
    "text": "we did was we started to plan a migration how do we get from the",
    "start": "1323860",
    "end": "1330550"
  },
  {
    "text": "architecture we had before with linka d1 to the one that we just showed you link",
    "start": "1330550",
    "end": "1337540"
  },
  {
    "text": "ready to so that's so our architecture take three basically so that was our",
    "start": "1337540",
    "end": "1343930"
  },
  {
    "start": "1341000",
    "end": "1341000"
  },
  {
    "text": "third redesign of our whole service match architecture witness scope of two years was it I think and here's what we",
    "start": "1343930",
    "end": "1350350"
  },
  {
    "text": "needed to do of course we had to first planet but ultimately here's what we ended up doing so on the left was our",
    "start": "1350350",
    "end": "1357100"
  },
  {
    "text": "previous architecture we mentioned so we had a demon set per note which handle our service smash and we need to move it",
    "start": "1357100",
    "end": "1363250"
  },
  {
    "text": "to something look like this so we had a control plane which consists of a few deployments and ports there's more than",
    "start": "1363250",
    "end": "1371350"
  },
  {
    "text": "one component but ultimately it's just a single a single normal standard community stateless deployment that",
    "start": "1371350",
    "end": "1376540"
  },
  {
    "text": "stores all of its state in in kubernetes config maps and custom research definitions and so on so it's fully",
    "start": "1376540",
    "end": "1381700"
  },
  {
    "text": "cloud native and fully embraces the club no more custom conflicts custom reloads any sorts of things like that and of",
    "start": "1381700",
    "end": "1388990"
  },
  {
    "text": "course you had to move away from using a demon set to using a sidecar container",
    "start": "1388990",
    "end": "1394320"
  },
  {
    "text": "now it's quite different so how do we do that without force walk without any downtime",
    "start": "1394320",
    "end": "1403270"
  },
  {
    "text": "and without the without completely confusing the developer so what's going on",
    "start": "1403270",
    "end": "1409280"
  },
  {
    "text": "well there's a few things oh yeah so this is we think that we'll get to that",
    "start": "1409280",
    "end": "1414800"
  },
  {
    "text": "in a second and then the second thing we did was just make sure we can configure",
    "start": "1414800",
    "end": "1420290"
  },
  {
    "text": "it correctly so this is how a configuration of a service for instance looks like an included - as you see it's a normal standard kubernetes deployment",
    "start": "1420290",
    "end": "1426920"
  },
  {
    "text": "another master kunis manifest that uses a CR D in this case a service profile",
    "start": "1426920",
    "end": "1432110"
  },
  {
    "text": "that then hooks into a link reduce control plane which then configures the boxes and cells to act as the proxies or",
    "start": "1432110",
    "end": "1438370"
  },
  {
    "text": "based on what your configured and the good thing about this is that ya the",
    "start": "1438370",
    "end": "1447350"
  },
  {
    "text": "goddess's that we could just simply migrate over all of our existing convicts into combination with deployment and package new Mac next to",
    "start": "1447350",
    "end": "1453620"
  },
  {
    "text": "our apps next to our deployments or even help chart or whatever it is it used at the time so yeah we couldn't set retries",
    "start": "1453620",
    "end": "1461810"
  },
  {
    "text": "you can set timeout stuff like that as a lot of things we can configure we'll go through all of them but it was quite",
    "start": "1461810",
    "end": "1467450"
  },
  {
    "text": "powerful and we could centrally central figure and the developers DL develops when we show them is they're a lot more",
    "start": "1467450",
    "end": "1472550"
  },
  {
    "text": "happy with OU can just use this just like we can figure anything out so kubernetes to do it so yeah what's",
    "start": "1472550",
    "end": "1478550"
  },
  {
    "text": "positive so what were our goals so one of the main things we want to do is we",
    "start": "1478550",
    "end": "1484570"
  },
  {
    "start": "1479000",
    "end": "1479000"
  },
  {
    "text": "wanted no required to developer interaction to make the switch and what",
    "start": "1484570",
    "end": "1491210"
  },
  {
    "text": "does that mean so it doesn't mean we don't want it developers can interact with it but we didn't want to require it",
    "start": "1491210",
    "end": "1497480"
  },
  {
    "text": "so if somebody didn't care about it to them it would be invisible and the same for our users and customers that use the",
    "start": "1497480",
    "end": "1503690"
  },
  {
    "text": "platform we didn't want any disruption and we have minimal changes to our current architecture yeah",
    "start": "1503690",
    "end": "1511490"
  },
  {
    "text": "chose to add append plumbing pipeline so for number one I think number one was a particular interesting one how do we not",
    "start": "1511490",
    "end": "1519590"
  },
  {
    "text": "require infant developers well if you go back to linker d1 the way we configured the parts themselves to use the actual",
    "start": "1519590",
    "end": "1527270"
  },
  {
    "text": "proxy was that we set environment variable HTTP proxy to point to the load",
    "start": "1527270",
    "end": "1532400"
  },
  {
    "text": "to the node that the instance was running on using a snippet like that on the left so basically just got the known",
    "start": "1532400",
    "end": "1538790"
  },
  {
    "text": "name and then set it as the HTTP proxy and all of the requests would then be the app would pick up that our",
    "start": "1538790",
    "end": "1544009"
  },
  {
    "text": "environment variable and would set up all the send all the requests down to the proxy and then end up running code",
    "start": "1544009",
    "end": "1549409"
  },
  {
    "text": "it would took it from there and even knew what to do it in a center-right place so that was one thing and the",
    "start": "1549409",
    "end": "1555529"
  },
  {
    "text": "second thing is we sort of agreed on the default standard way of how we identify our services which is basically just",
    "start": "1555529",
    "end": "1561919"
  },
  {
    "text": "service name dot name spacing kubernetes and that was it luckily the names that the service names",
    "start": "1561919",
    "end": "1568789"
  },
  {
    "text": "ourself stayed the same in linker D but even if they weren't we could all configure them to be the same for",
    "start": "1568789",
    "end": "1574159"
  },
  {
    "text": "instance do or even include two we can always configure what the host names actually are so we can migrate over without any services but how do we solve",
    "start": "1574159",
    "end": "1581720"
  },
  {
    "text": "the left problem how do we because if we're going to move over we're gonna need to remove that everyone's gonna",
    "start": "1581720",
    "end": "1587359"
  },
  {
    "text": "have to remove that and it's gonna have to put in whatever it is we need for Lync ready to so we wanted to remove",
    "start": "1587359",
    "end": "1594320"
  },
  {
    "text": "that manual step of having everyone need to do it themselves so we embraced kubernetes and made use of Admissions",
    "start": "1594320",
    "end": "1601309"
  },
  {
    "start": "1598000",
    "end": "1598000"
  },
  {
    "text": "web hooks so we created our own Duchamp web hooks that basically did that automatically on the cluster so what",
    "start": "1601309",
    "end": "1608690"
  },
  {
    "text": "this does is basically deploy and commanders will call the service before each part is scheduled and what you can",
    "start": "1608690",
    "end": "1615590"
  },
  {
    "text": "do at that point is you can edit the depart the finishin itself to for instance add environment variable or add",
    "start": "1615590",
    "end": "1621019"
  },
  {
    "text": "a container to the pod without you having to do anything it will happen automatically in the background and actually link ready to uses this",
    "start": "1621019",
    "end": "1626899"
  },
  {
    "text": "mechanism as well as the as desist here and all but a bunch of other places as well so this was a good a good search to",
    "start": "1626899",
    "end": "1633499"
  },
  {
    "text": "a problem so what we did was and it's completely transparent to anyone using it so what we did was we made sure that",
    "start": "1633499",
    "end": "1639289"
  },
  {
    "text": "that snippet was moved into initial controller and then everyone removed that so yes there was a step that needed",
    "start": "1639289",
    "end": "1646009"
  },
  {
    "text": "developer intervention but once once we move that moved away from manual setup into a like an automated set up within",
    "start": "1646009",
    "end": "1651950"
  },
  {
    "text": "the cluster we could then update this in",
    "start": "1651950",
    "end": "1657259"
  },
  {
    "text": "the background without anyone have to update anything so effectively what did we do so plan",
    "start": "1657259",
    "end": "1667450"
  },
  {
    "start": "1667000",
    "end": "1667000"
  },
  {
    "text": "was actually sort of simple when I look at it back so we had all of our",
    "start": "1667450",
    "end": "1673240"
  },
  {
    "text": "infrastructure definitions in energy depository we version everything we serve followed the infrastructure Isco",
    "start": "1673240",
    "end": "1678790"
  },
  {
    "text": "DevOps approach that everyone follows so we build a pipeline to do the migration and what this pipeline did was basically",
    "start": "1678790",
    "end": "1686050"
  },
  {
    "text": "we start the migration by like merging the changes into the correct branch for the correct repository of course we",
    "start": "1686050",
    "end": "1691600"
  },
  {
    "text": "tested as a bunch of times before we moved to a poor environment so what we",
    "start": "1691600",
    "end": "1697150"
  },
  {
    "text": "did was we so what the job did was we wanted to deploy both service meshes at",
    "start": "1697150",
    "end": "1703480"
  },
  {
    "text": "the same time and gradually move what service one by one from link ID 1 source",
    "start": "1703480",
    "end": "1709750"
  },
  {
    "text": "mesh to link ready - so the way we did that we first we deployed the control plane now remember the control plane is",
    "start": "1709750",
    "end": "1716350"
  },
  {
    "text": "just a normal deployment it doesn't interfere with anything there's no harm in it being there if nobody is using",
    "start": "1716350",
    "end": "1722440"
  },
  {
    "text": "there so that was straightforward we deployed the control plane and that was in there and ready to go at that point",
    "start": "1722440",
    "end": "1728800"
  },
  {
    "text": "we also made sure to apply all the configurations that the services would need it that with it beforehand now most",
    "start": "1728800",
    "end": "1734590"
  },
  {
    "text": "of it was generic automatic configurations that we could tweak later on but for the initial migrations we",
    "start": "1734590",
    "end": "1739840"
  },
  {
    "text": "didn't need to so it was fairly straightforward so we deployed the control plane and then the important step came up how do we actually now move",
    "start": "1739840",
    "end": "1746650"
  },
  {
    "text": "every service from using the old service smash into the new service mesh without",
    "start": "1746650",
    "end": "1752040"
  },
  {
    "text": "causing a disruption without developers noticing or us noticing or the customers are missing or anyone under singles like",
    "start": "1752040",
    "end": "1758230"
  },
  {
    "text": "a personal challenge for us to do that so because we moved over to the admissions webhooks before to configure",
    "start": "1758230",
    "end": "1764770"
  },
  {
    "text": "our ports we simply stopped it which means any new parts that were deployed",
    "start": "1764770",
    "end": "1770880"
  },
  {
    "text": "would not have the linker to configure a d-link ready one configuration applied and the same time that we stopped it we",
    "start": "1770880",
    "end": "1779400"
  },
  {
    "text": "started up the link ready to admission web hooks and the way link ready to admission epoch work is that it's",
    "start": "1779400",
    "end": "1785320"
  },
  {
    "text": "automatically there when they deploy the control plane but it's not activated and the way you activate it is is you",
    "start": "1785320",
    "end": "1790780"
  },
  {
    "text": "basically put a label on the namespace or on the deployment themselves of where you one to activate where he wanted to be",
    "start": "1790780",
    "end": "1796190"
  },
  {
    "text": "activated now we didn't again we didn't want to edit the deployment so we put them all automatically in all the",
    "start": "1796190",
    "end": "1802190"
  },
  {
    "text": "namespaces there when in use and and that way we started the admission we're",
    "start": "1802190",
    "end": "1808220"
  },
  {
    "text": "considering could it - so what am i wanted up happening else is that any new part that was gonna get deployed was",
    "start": "1808220",
    "end": "1814730"
  },
  {
    "text": "gonna have the link ready to admission weapons applied which would add the",
    "start": "1814730",
    "end": "1819830"
  },
  {
    "text": "sidecut container to the pot which will automatically connect to the service mesh to the control plane of linker d2",
    "start": "1819830",
    "end": "1825410"
  },
  {
    "text": "and start using the new service mesh now the welding hoody works which is very convenient for us is that if the other",
    "start": "1825410",
    "end": "1830930"
  },
  {
    "text": "service is not yet on link ready - it will skip the mutuality alasc",
    "start": "1830930",
    "end": "1836180"
  },
  {
    "text": "check so that we didn't so we didn't need to worry about not being able to connect to sides of the other sides of a",
    "start": "1836180",
    "end": "1842270"
  },
  {
    "text": "request because one didn't migrate yet but once ever migrated MPLS will be activated and and everything would work",
    "start": "1842270",
    "end": "1849860"
  },
  {
    "text": "so once we once we flip the switch on those two we simply initiate a cluster right rolling deploy one service at a",
    "start": "1849860",
    "end": "1856520"
  },
  {
    "text": "time to be able to pick up to destroy the old parts create the new parts and pick up the new pick up the new",
    "start": "1856520",
    "end": "1862790"
  },
  {
    "text": "configuration new service mesh and basically connect to the new service mesh all transparency in the background",
    "start": "1862790",
    "end": "1868910"
  },
  {
    "text": "while serving request without any downtime mean that we need to thank",
    "start": "1868910",
    "end": "1874490"
  },
  {
    "text": "goodness and that because wrong deployments worked well for us and it worked well for us in this case now that",
    "start": "1874490",
    "end": "1880910"
  },
  {
    "text": "took a few hours to get everything everything rolling because we want to make sure we want to make sure everything's working so on and then once",
    "start": "1880910",
    "end": "1888260"
  },
  {
    "text": "everything was Margaret we did a simple cleanup we removed the old admission web worker and controller and remove the old",
    "start": "1888260",
    "end": "1895130"
  },
  {
    "text": "linker diamond set from the notes and how was it and again the reason we spend",
    "start": "1895130",
    "end": "1901280"
  },
  {
    "text": "much more time on on the wrong redeploy is that if at any point something went went wrong we could always throw back we",
    "start": "1901280",
    "end": "1908780"
  },
  {
    "text": "could always reenable the old one disabled new one and then reapply every re redeploy the instances that we",
    "start": "1908780",
    "end": "1914450"
  },
  {
    "text": "restore it and it would go back to using the old service mesh so we had a back-up plan if if things didn't go well of",
    "start": "1914450",
    "end": "1919580"
  },
  {
    "text": "course we tested a bunch of times in production so that was a general plan of how we got things across and how we used",
    "start": "1919580",
    "end": "1927720"
  },
  {
    "text": "Automation and the features of kubernetes to make big architectural",
    "start": "1927720",
    "end": "1933210"
  },
  {
    "text": "changes to our service mesh and that's just an example what you can do you could use the same sort of concepts to",
    "start": "1933210",
    "end": "1938610"
  },
  {
    "text": "make any sort of changes in kubernetes or other content environments so this",
    "start": "1938610",
    "end": "1944190"
  },
  {
    "text": "will end up and this is where we're now and so far has been working good and we're looking forward to basically",
    "start": "1944190",
    "end": "1949410"
  },
  {
    "text": "moving along as the ecosystem of service meshes matures before I finished just a",
    "start": "1949410",
    "end": "1957840"
  },
  {
    "text": "few notes on suppose what we learn and what what you can take away from this is that do you fully automate your",
    "start": "1957840",
    "end": "1965520"
  },
  {
    "text": "infrastructure from the get-go or as soon as you can as you saw once we have",
    "start": "1965520",
    "end": "1972120"
  },
  {
    "text": "all the building blocks in place for for the for automating the configurations of",
    "start": "1972120",
    "end": "1977970"
  },
  {
    "text": "infrastructure components it actually became quite trivial to switch things over so just deployed the new version of",
    "start": "1977970",
    "end": "1983820"
  },
  {
    "text": "the or all of a webhook and so on and restart and new things pick up new",
    "start": "1983820",
    "end": "1990390"
  },
  {
    "text": "things and all things will be restored when when the time comes so don't do it",
    "start": "1990390",
    "end": "1995610"
  },
  {
    "text": "manually don't rely on hundreds of teams hunt at tens of teams and hunts of",
    "start": "1995610",
    "end": "2000980"
  },
  {
    "text": "developers and having to do everything at the same time it just leads to problems try and take the burden away",
    "start": "2000980",
    "end": "2006200"
  },
  {
    "text": "from them and automate as much as possible and of course to follow up on that",
    "start": "2006200",
    "end": "2011300"
  },
  {
    "text": "let the platform do the heavy lifting that's why it's there is the reason kubernetes is so popular nowadays is",
    "start": "2011300",
    "end": "2018050"
  },
  {
    "text": "because we don't need to deal with where containers are scheduled how they're used and how they're how they move",
    "start": "2018050",
    "end": "2023960"
  },
  {
    "text": "around the cluster same with stuff like this don't do it yourselves don't need to think about it you can just let the",
    "start": "2023960",
    "end": "2029210"
  },
  {
    "text": "platform how to throw you you can write the automation so you need to be able to do that as well and our last note is I",
    "start": "2029210",
    "end": "2039010"
  },
  {
    "text": "suppose I talked a lot about in the beginning about what a service mesh is",
    "start": "2039010",
    "end": "2045280"
  },
  {
    "text": "and all the times it's when I talk to people it's a lot it it's difficult for",
    "start": "2045280",
    "end": "2051470"
  },
  {
    "text": "people to grasp or what is it like I can explain three different ways but it's still difficult expell explain what it",
    "start": "2051470",
    "end": "2056540"
  },
  {
    "text": "actually is so I prefer to say it or what it actually gives you and does give you is it provides you a",
    "start": "2056540",
    "end": "2061850"
  },
  {
    "text": "transparent reliable an autonomous network up for any service running or cluster and the important part is is",
    "start": "2061850",
    "end": "2067669"
  },
  {
    "text": "that you don't actually need to know about it as long as you're aware that it's there you can go into is deep into",
    "start": "2067670",
    "end": "2074240"
  },
  {
    "text": "it as you want and configure it as much as you want but you don't need to it's there and what it gives you is you don't",
    "start": "2074240",
    "end": "2082460"
  },
  {
    "text": "need to think about network anymore the service mystic so for you and there's a",
    "start": "2082460",
    "end": "2087830"
  },
  {
    "text": "lot of cool components working underneath to make that happen cool right on time thank you very much I",
    "start": "2087830",
    "end": "2097840"
  },
  {
    "text": "hope I hope that wasn't that was interesting for you and yeah let me know",
    "start": "2097840",
    "end": "2105680"
  },
  {
    "text": "if you have any question any questions about that you like that if not thank",
    "start": "2105680",
    "end": "2112670"
  },
  {
    "text": "thanks for coming and hope you enjoy the conference",
    "start": "2112670",
    "end": "2116950"
  }
]