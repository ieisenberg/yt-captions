[
  {
    "text": "this if you can believe it is my first time giving a talk since uh november",
    "start": "320",
    "end": "5839"
  },
  {
    "text": "2019 at kubecon san diego and it's the first time for our little group of maintainers to see",
    "start": "5839",
    "end": "12719"
  },
  {
    "text": "each other in about the same amount of time so we're excited to see everyone again and give you an",
    "start": "12719",
    "end": "19039"
  },
  {
    "text": "update on the project if you were at kubecon san diego or you watched",
    "start": "19039",
    "end": "25039"
  },
  {
    "text": "what we called the mini summit we've given other updates since then",
    "start": "25039",
    "end": "30160"
  },
  {
    "text": "obviously there's been some virtual kubecons and other events since since then but i thought you know it's our",
    "start": "30160",
    "end": "35600"
  },
  {
    "text": "first time back together in person uh let's just recap the last two years for those who may",
    "start": "35600",
    "end": "41200"
  },
  {
    "text": "have seen the last 18 months as a fog and you don't really remember what happened in all that time",
    "start": "41200",
    "end": "47600"
  },
  {
    "text": "from a release standpoint we've had uh really three major release lines the third one is just now starting if you",
    "start": "47600",
    "end": "54640"
  },
  {
    "text": "saw our tweet today or the release that came out yesterday we're just starting the 160 beta",
    "start": "54640",
    "end": "60640"
  },
  {
    "text": "but again in those two years we've come out with 1.4 1.5 and if you count",
    "start": "60640",
    "end": "66560"
  },
  {
    "text": "up all the service releases and point releases we've done 29 total releases",
    "start": "66560",
    "end": "72400"
  },
  {
    "text": "since kubecon san diego across three release branches including at the time 1.3 was just in service",
    "start": "72400",
    "end": "80000"
  },
  {
    "text": "which is now end of life also in that time we've fully automated our release process",
    "start": "80000",
    "end": "86880"
  },
  {
    "text": "we don't rely on derrick to run some commands on his laptop anymore we moved our entire ci to github actions",
    "start": "86880",
    "end": "94960"
  },
  {
    "text": "and migrated all our sub repos and have all that working uh for",
    "start": "94960",
    "end": "102240"
  },
  {
    "text": "almost two years now uh the the team at microsoft some of them are reviewers and committers in the",
    "start": "102240",
    "end": "108799"
  },
  {
    "text": "container d project uh each one of those major releases there have been improvements in the windows support and",
    "start": "108799",
    "end": "115680"
  },
  {
    "text": "the cri support for windows and now there's actually official packaging with each of our releases for windows",
    "start": "115680",
    "end": "123520"
  },
  {
    "text": "also in that time we've one of our maintainers has added testing for arm",
    "start": "123520",
    "end": "129280"
  },
  {
    "text": "64. we haven't integrated it into our release process",
    "start": "129280",
    "end": "134959"
  },
  {
    "text": "those of you who use github actions know there's no built-in arm 64 runners and we haven't",
    "start": "134959",
    "end": "141120"
  },
  {
    "text": "been able to do self-hosted but again that's another architecture that's getting tested",
    "start": "141120",
    "end": "147200"
  },
  {
    "text": "automatically and hopefully will be part of our release process soon uh also in that time we firmed up kind",
    "start": "147200",
    "end": "154560"
  },
  {
    "text": "of our disclosure and security process and we've gotten to test it with three cves",
    "start": "154560",
    "end": "160319"
  },
  {
    "text": "in the last couple years and so that seems uh fairly well streamlined as well and we even have a new role a security",
    "start": "160319",
    "end": "167760"
  },
  {
    "text": "advisor role that came along with rethinking our process and there's some other governance",
    "start": "167760",
    "end": "173920"
  },
  {
    "text": "updates you can find that in our container d slash project repo [Music] we've added new reviewers from multiple",
    "start": "173920",
    "end": "180720"
  },
  {
    "text": "employers and there have been over 370 unique contributors just in the last two years",
    "start": "180720",
    "end": "186640"
  },
  {
    "text": "to the project 10 of those are not just you know one time and gone",
    "start": "186640",
    "end": "191920"
  },
  {
    "text": "actually have submitted 10 or more prs so we've got a great community that's",
    "start": "191920",
    "end": "197200"
  },
  {
    "text": "been continuing to grow a little more detail on that if you ever use dev stats",
    "start": "197200",
    "end": "203440"
  },
  {
    "text": "that the cncf have put together there's lots of other community statistics you can dig into",
    "start": "203440",
    "end": "209760"
  },
  {
    "text": "you can see that 67 unique companies are participating",
    "start": "209760",
    "end": "214879"
  },
  {
    "text": "we now have 13 committers and 14 reviewers from again a good cross-section of",
    "start": "214879",
    "end": "221120"
  },
  {
    "text": "companies and we'd like to see that as well as the community comes together to build",
    "start": "221120",
    "end": "228640"
  },
  {
    "text": "container d um one of the other things if you were at that mini sub summit or watched the",
    "start": "228640",
    "end": "235280"
  },
  {
    "text": "video or have followed since then we've added this concept of non-core sub-projects",
    "start": "235280",
    "end": "240879"
  },
  {
    "text": "of container d so at the mini summit i believe kohai was there and presented",
    "start": "240879",
    "end": "246560"
  },
  {
    "text": "star gz snapshotter that's now part of the container d repo as a non-core sub-project",
    "start": "246560",
    "end": "253040"
  },
  {
    "text": "image encryption was being worked out at the time that's also now a subproject in the continuity repo",
    "start": "253040",
    "end": "260320"
  },
  {
    "text": "and then something that didn't exist a few years ago but has gotten a lot of community activity recently is nerd ctl",
    "start": "260320",
    "end": "266800"
  },
  {
    "text": "the akihiro one of our maintainers put together it also is now a non-core subproject of",
    "start": "266800",
    "end": "273199"
  },
  {
    "text": "container d and one of the nice things about that is that it packages up a lot of these features into a simple installable",
    "start": "273199",
    "end": "281199"
  },
  {
    "text": "client that lets you play with things like lazy loading and container encryption",
    "start": "281199",
    "end": "286560"
  },
  {
    "text": "and even rootless mode the rest of this presentation",
    "start": "286560",
    "end": "291600"
  },
  {
    "text": "is going to be a lot about the what's new so i'm not going to try and be exhaustive there's tons happening in the cri that",
    "start": "291600",
    "end": "297680"
  },
  {
    "text": "michael cover we've started to add the capabilities for open",
    "start": "297680",
    "end": "302880"
  },
  {
    "text": "telemetry and tracing more metrics there's been interest from the confidential computing consortium and",
    "start": "302880",
    "end": "310240"
  },
  {
    "text": "adding capabilities into container d around that in fact the inclavar project is already using",
    "start": "310240",
    "end": "316960"
  },
  {
    "text": "container d and there's a bunch of work maxim will talk about around sandbox api and shim and a bunch",
    "start": "316960",
    "end": "324720"
  },
  {
    "text": "of rework that's going on even right now which we hope to get into the 1 6 release",
    "start": "324720",
    "end": "330639"
  },
  {
    "text": "so with that a quick intro i'm going to turn it over to derek to continue",
    "start": "330639",
    "end": "338038"
  },
  {
    "text": "[Music]",
    "start": "342510",
    "end": "345600"
  },
  {
    "text": "so first i'm going to start off by talking about the different parts of container keys wait wait a second",
    "start": "350720",
    "end": "356400"
  },
  {
    "text": "i think you just came off are we good okay so i'm going to talk about the deep",
    "start": "356400",
    "end": "362720"
  },
  {
    "text": "dive section of container d i'm going to kind of go through the break it up roughly into three different parts um so",
    "start": "362720",
    "end": "368800"
  },
  {
    "text": "everyone can be familiar with what we're talking about here so in container d we have the client and phil mentioned nerd",
    "start": "368800",
    "end": "374720"
  },
  {
    "text": "ctl this is one of the clients ctr if you're familiar with container d it's",
    "start": "374720",
    "end": "379759"
  },
  {
    "text": "like our kind of our debugging tool that we ship with container d it it calls into all",
    "start": "379759",
    "end": "386080"
  },
  {
    "text": "our underlying functionality um and then our go ap our go library we actually",
    "start": "386080",
    "end": "391120"
  },
  {
    "text": "consider part of part of our client and if you're using container d",
    "start": "391120",
    "end": "396720"
  },
  {
    "text": "as uh integrating you're probably using using our google library today",
    "start": "396720",
    "end": "401759"
  },
  {
    "text": "also our cri implementation actually uses that same go library in the container d daemon",
    "start": "401759",
    "end": "407199"
  },
  {
    "text": "which you're probably all familiar with running on your hosts um there we have our api server",
    "start": "407199",
    "end": "412720"
  },
  {
    "text": "this actual cri plug-in and then we have all our resource managers that do data storage garbage collection",
    "start": "412720",
    "end": "419840"
  },
  {
    "text": "everything around shim management which actually owns the containers and then the actual container d shim is",
    "start": "419840",
    "end": "426400"
  },
  {
    "text": "we have a shim that's per container or pod instance and this is what actually manages all",
    "start": "426400",
    "end": "432160"
  },
  {
    "text": "the running processes so to break that out it ends up looking like this you'll see the client all the",
    "start": "432160",
    "end": "438319"
  },
  {
    "text": "way there on the on the left uh box you'll see the daemons actually in the middle and then the shim on the",
    "start": "438319",
    "end": "444319"
  },
  {
    "text": "side so in the client here so this is this is roughly how our go library is kind of",
    "start": "444319",
    "end": "450960"
  },
  {
    "text": "laid out all image distribution is actually done inside of our go library it's not inside",
    "start": "450960",
    "end": "456800"
  },
  {
    "text": "the daemon per se as a service today everything that's related to",
    "start": "456800",
    "end": "464639"
  },
  {
    "text": "setting up a container so creating your oci spec setting all your container options are",
    "start": "464639",
    "end": "469759"
  },
  {
    "text": "actually defined inside of our inside of our go library all the configuration for",
    "start": "469759",
    "end": "476160"
  },
  {
    "text": "the services as well as um so like we have the ability here",
    "start": "476160",
    "end": "481520"
  },
  {
    "text": "so for the client you can use these service proxies which uh talk over our grpc api",
    "start": "481520",
    "end": "488319"
  },
  {
    "text": "uh but we actually use the same client in the cri plugin which actually doesn't go through the",
    "start": "488319",
    "end": "494080"
  },
  {
    "text": "grpc api it actually directly uses the service implementations so inside the daemon",
    "start": "494080",
    "end": "500479"
  },
  {
    "text": "we have our api servers we have our cri plugin and then in the core we actually have service implementations for",
    "start": "500479",
    "end": "506879"
  },
  {
    "text": "uh all of the all the services that we define uh so everything that related to containers",
    "start": "506879",
    "end": "513120"
  },
  {
    "text": "related to accessing content relax accessing the actual images uh name spaces and snapshots",
    "start": "513120",
    "end": "520640"
  },
  {
    "text": "all have a definition here and the api",
    "start": "520640",
    "end": "527040"
  },
  {
    "text": "gives you access to those through uh through grpc so if you know the container d api today you might be",
    "start": "527040",
    "end": "532720"
  },
  {
    "text": "familiar with some of our some of these apis they're very low level if you're used to using docker",
    "start": "532720",
    "end": "539920"
  },
  {
    "text": "there's probably a lot of stuff here that you wouldn't necessarily use directly",
    "start": "539920",
    "end": "545279"
  },
  {
    "text": "you'd see stuff more that you're used to pull push inside inside the actual go library",
    "start": "545279",
    "end": "552560"
  },
  {
    "text": "and then here's where we actually do our data storage so in our metadata layer this is where we store everything",
    "start": "552560",
    "end": "558480"
  },
  {
    "text": "related to any sort of label you have every single image",
    "start": "558480",
    "end": "563920"
  },
  {
    "text": "what it stores uh every snapshot every container is actually stored here and we have a garbage collection process that",
    "start": "563920",
    "end": "570160"
  },
  {
    "text": "will go in and it will delete stuff that's that's no longer no longer referenced by anything um",
    "start": "570160",
    "end": "576959"
  },
  {
    "text": "that's so that we can have our approach of failing clean so if if something becomes unreferenced we'll clean it up",
    "start": "576959",
    "end": "583360"
  },
  {
    "text": "uh the back end underneath that this is where the actual snapshots will live the actual large content blobs that",
    "start": "583360",
    "end": "589920"
  },
  {
    "text": "you pull down from the registry the actual clients that that talk to the the shims are at this layer",
    "start": "589920",
    "end": "597200"
  },
  {
    "text": "here and the metadata layer will actually manage",
    "start": "597200",
    "end": "602800"
  },
  {
    "text": "everything that's done there so the shims this is probably where you've seen a lot of uh",
    "start": "602800",
    "end": "609279"
  },
  {
    "text": "a lot of discussion lately around you know g visor kata containers",
    "start": "609279",
    "end": "614320"
  },
  {
    "text": "run c which everybody knows firecracker all implement this this gem interface",
    "start": "614320",
    "end": "620079"
  },
  {
    "text": "and what this allows us to do is to separate the underlying container implementation from",
    "start": "620079",
    "end": "627279"
  },
  {
    "text": "our actual resource management of that container so from a container d perspective we can treat",
    "start": "627279",
    "end": "633040"
  },
  {
    "text": "these shims as a resource we don't have to worry about",
    "start": "633040",
    "end": "638480"
  },
  {
    "text": "everything that's being managed necessarily underneath it whether you're using c groups or you",
    "start": "638480",
    "end": "644320"
  },
  {
    "text": "have a vm implementation all that's actually related to how the shim implements that",
    "start": "644320",
    "end": "650240"
  },
  {
    "text": "and we can we can implement it a little bit more abstractly",
    "start": "650240",
    "end": "656240"
  },
  {
    "text": "i've added since the last time we did this talk um",
    "start": "656240",
    "end": "662079"
  },
  {
    "text": "actually i guess we didn't have this two years ago this this diagram but at least since since last kubecon we have the",
    "start": "662079",
    "end": "667760"
  },
  {
    "text": "sandbox manager um that's a new interface that we're adding um and we'll be discussed by uh i think",
    "start": "667760",
    "end": "675680"
  },
  {
    "text": "maxim is going to discuss a little bit more coming up all right so let's let's quickly go through what a what a poll looks like in",
    "start": "675680",
    "end": "682720"
  },
  {
    "text": "this diagram so when you do a poll the first thing that's going to be done is we're going to get a lease",
    "start": "682720",
    "end": "688480"
  },
  {
    "text": "and a leasing container d is basically a way to tell the garbage collector that i'm using this content right now and if",
    "start": "688480",
    "end": "695440"
  },
  {
    "text": "something fails then the garbage collector will eventually clean it up when that once that lease",
    "start": "695440",
    "end": "701040"
  },
  {
    "text": "expires so this is our way if as we all know like polling sometimes doesn't always complete or your process",
    "start": "701040",
    "end": "708160"
  },
  {
    "text": "gets killed randomly we don't we always try to make sure we never leave anything around",
    "start": "708160",
    "end": "714320"
  },
  {
    "text": "so after that the pull process is actually going to resolve uh your content um so it's going to",
    "start": "714320",
    "end": "719920"
  },
  {
    "text": "resolve your tag to some oci registry it's going to grab that that data from the oci registry and",
    "start": "719920",
    "end": "726320"
  },
  {
    "text": "it's going to go into the content store and the path it's going to make from the client is through grpc through our service layer",
    "start": "726320",
    "end": "733120"
  },
  {
    "text": "through our metadata layer and then actually to the storage after that",
    "start": "733120",
    "end": "738880"
  },
  {
    "text": "every poll you're going to unpack that into disk so the poll is going to actually create",
    "start": "738880",
    "end": "744079"
  },
  {
    "text": "the snapshots and then for every snapshot it creates it's going to unpack that data using our",
    "start": "744079",
    "end": "750240"
  },
  {
    "text": "using our diff service and all the disk service does is it takes these layers",
    "start": "750240",
    "end": "755839"
  },
  {
    "text": "and it unpacks them into into the snapshots and then after that we can just create",
    "start": "755839",
    "end": "761920"
  },
  {
    "text": "an image this is the lightest weight part it just links to the",
    "start": "761920",
    "end": "767360"
  },
  {
    "text": "content that we've now unpacked on disk uh likewise let me go through a run flow",
    "start": "767360",
    "end": "773519"
  },
  {
    "text": "real quick you'll kind of see unlike poll we don't have a we don't have a",
    "start": "773519",
    "end": "778800"
  },
  {
    "text": "top level run function um so nerd ctl or ctr will actually like",
    "start": "778800",
    "end": "784320"
  },
  {
    "text": "orchestrate these it's going to do the same thing first it's going to grab it's going to grab a lease",
    "start": "784320",
    "end": "790720"
  },
  {
    "text": "and then it's actually going to go resolve your image normally when you run a container you start with an image",
    "start": "790720",
    "end": "796399"
  },
  {
    "text": "in this case it's just going to read what that is it's going to resolve that it's going to look at the image configuration so it's going to resolve",
    "start": "796399",
    "end": "802959"
  },
  {
    "text": "all that read the content so that it can actually resolve your resolve your",
    "start": "802959",
    "end": "809279"
  },
  {
    "text": "configuration that you want to use for your container it's going to set up the snapshot so this is normally the read write layer",
    "start": "809279",
    "end": "815440"
  },
  {
    "text": "that you're going to use inside of your container it's going to create the oci",
    "start": "815440",
    "end": "821760"
  },
  {
    "text": "specification so it's going to use that image config that it read back from the content store it's going",
    "start": "821760",
    "end": "827440"
  },
  {
    "text": "to use those to generate this oci configuration",
    "start": "827440",
    "end": "832880"
  },
  {
    "text": "create the container now in this case the container is actually a metadata object that we use",
    "start": "832880",
    "end": "838800"
  },
  {
    "text": "in container d to keep track of container as a resource and then after that is where we actually",
    "start": "838800",
    "end": "845279"
  },
  {
    "text": "start the tasks and the tasks here doesn't actually have",
    "start": "845279",
    "end": "850800"
  },
  {
    "text": "data that we store in container d you can see it kind of skips over our storage layer there and goes directly to",
    "start": "850800",
    "end": "857680"
  },
  {
    "text": "the back end that's because tasks are the ephemeral object that are associated",
    "start": "857680",
    "end": "863120"
  },
  {
    "text": "with the actual running container if you were to restart your machine at this point we would still see that",
    "start": "863120",
    "end": "868560"
  },
  {
    "text": "container metadata but you wouldn't see any of your task data with how it's with how it's implemented",
    "start": "868560",
    "end": "874560"
  },
  {
    "text": "today and in this case all your remaining operations are going to be actually",
    "start": "874560",
    "end": "880399"
  },
  {
    "text": "through this task interface so you're going to start the container you're going to wait for the container",
    "start": "880399",
    "end": "885600"
  },
  {
    "text": "you're going to exec or kill or whatever you're going to do is actually going to go through the same flow",
    "start": "885600",
    "end": "891519"
  },
  {
    "text": "from the client all the way back to the shim",
    "start": "891519",
    "end": "896560"
  },
  {
    "text": "and as i mentioned before i added sandboxes in here this is something that we're in the",
    "start": "896560",
    "end": "902000"
  },
  {
    "text": "process of adding we don't have a flow all the way back through the client today because the sandbox api is still being worked on",
    "start": "902000",
    "end": "909199"
  },
  {
    "text": "but this is one of the areas that we're working on we're working on today so this will help in cases where you have",
    "start": "909199",
    "end": "915199"
  },
  {
    "text": "a vm sandbox you'll actually be able to manage manage that sandbox through the",
    "start": "915199",
    "end": "921040"
  },
  {
    "text": "container d api in the life cycle so this is this will be useful for some of the more advanced",
    "start": "921040",
    "end": "927199"
  },
  {
    "text": "use cases uh that will be discussed later so now we're going to",
    "start": "927199",
    "end": "934000"
  },
  {
    "text": "dig into cri a little deeper with a mic",
    "start": "934000",
    "end": "939399"
  },
  {
    "text": "hey guys well i said deeper but really i'm going to step back a bit and talk more about or how we interact with",
    "start": "946240",
    "end": "953759"
  },
  {
    "text": "kubernetes itself okay as i'm sure a lot of you guys are you know kubernetes users and you don't",
    "start": "953759",
    "end": "959199"
  },
  {
    "text": "really know what goes on in node except that you configure a container runtime so i'll go ahead and talk about this a little bit um the the concept the idea",
    "start": "959199",
    "end": "966800"
  },
  {
    "text": "is on your node you have kubelet registering with api server to",
    "start": "966800",
    "end": "972959"
  },
  {
    "text": "host all the pods and resource types that pods need on on the note on the worker node right",
    "start": "972959",
    "end": "979600"
  },
  {
    "text": "and it's the job of kublet to run its managers and converse with",
    "start": "979600",
    "end": "986000"
  },
  {
    "text": "the container runtime allocate volumes and attach things to to the container through these cry",
    "start": "986000",
    "end": "993440"
  },
  {
    "text": "apis okay the cry apis were were done by tim some you know some guys in google uh probably",
    "start": "993440",
    "end": "1000320"
  },
  {
    "text": "about five years ago um as a way to to modify how it currently we were",
    "start": "1000320",
    "end": "1006240"
  },
  {
    "text": "integrating with docker right for for launching containers managing we thought would be a good idea to have",
    "start": "1006240",
    "end": "1012079"
  },
  {
    "text": "a cry api so that other people could come in and implement without having to fork kubele which is",
    "start": "1012079",
    "end": "1018320"
  },
  {
    "text": "basically what we were doing originally we had there was a rocking at ease and a docker fork and and currently we",
    "start": "1018320",
    "end": "1026400"
  },
  {
    "text": "let's go ahead uh let's see let's go in the next short down arrow",
    "start": "1026400",
    "end": "1032160"
  },
  {
    "text": "currently there's some coding kubelet that implements the docker shim",
    "start": "1032160",
    "end": "1037600"
  },
  {
    "text": "and you can actually find the tree in there and when kubelet comes up it launches the docker shim",
    "start": "1037600",
    "end": "1042959"
  },
  {
    "text": "uh to to host these cry services but they also have some internal you know integration",
    "start": "1042959",
    "end": "1049120"
  },
  {
    "text": "you know techniques and things that are in there and it was decided that all that stuff needs to be pushed over",
    "start": "1049120",
    "end": "1054720"
  },
  {
    "text": "into the cry so that the container run times that are running using only the cry interface",
    "start": "1054720",
    "end": "1059760"
  },
  {
    "text": "would have you know more legitimate api to work with um it just felt easier so",
    "start": "1059760",
    "end": "1065280"
  },
  {
    "text": "what we're going to do and what has been happened is we've deprecated docker shim that's the internal version not the",
    "start": "1065280",
    "end": "1071600"
  },
  {
    "text": "external version the external version of docker shim is still being worked on and it'll still work with docker",
    "start": "1071600",
    "end": "1077760"
  },
  {
    "text": "i don't know if you know about it but underneath the covers docker actually uses container d also",
    "start": "1077760",
    "end": "1083280"
  },
  {
    "text": "um that's another piece of information but we've got some we're going in a way to",
    "start": "1083280",
    "end": "1088880"
  },
  {
    "text": "make all the test buckets work for the container run times either cryo and or container d",
    "start": "1088880",
    "end": "1095840"
  },
  {
    "text": "so that you guys will be able to you know trust that everything that can happen with",
    "start": "1095840",
    "end": "1101039"
  },
  {
    "text": "docker can also happen with these other container runtime types that you're all migrating to",
    "start": "1101039",
    "end": "1106880"
  },
  {
    "text": "when it gets completely pulled out i guess maybe 124 or 125 we'll see what happens",
    "start": "1106880",
    "end": "1112080"
  },
  {
    "text": "okay uh along this route we've just the the cry api was has been in alpha",
    "start": "1112080",
    "end": "1119039"
  },
  {
    "text": "for about three years i'm sure you know the api cycle for kubernetes apis can be",
    "start": "1119039",
    "end": "1124080"
  },
  {
    "text": "very long but we are moving to a version one beta status and sasha has a pull request in",
    "start": "1124080",
    "end": "1131840"
  },
  {
    "text": "there right now to to add the v1 api is an optional api in the 2-3 cycle",
    "start": "1131840",
    "end": "1138799"
  },
  {
    "text": "so you know one v1 would be one two three kubernetes",
    "start": "1138799",
    "end": "1144000"
  },
  {
    "text": "so what what we're going to do is make sure that all of the container run times then implement the cry",
    "start": "1144000",
    "end": "1149520"
  },
  {
    "text": "interface support both the v1 alpha api and and the v1 api at the same time so when",
    "start": "1149520",
    "end": "1157200"
  },
  {
    "text": "kubelet comes up it'll check to see if you've got v1 in the container runtime or not",
    "start": "1157200",
    "end": "1162799"
  },
  {
    "text": "if you if you have v1 it'll go ahead and start using that uh as soon as this this pr gets merged in in two three of",
    "start": "1162799",
    "end": "1169280"
  },
  {
    "text": "kubernetes and kubelet and and then all be great right and then later on",
    "start": "1169280",
    "end": "1175120"
  },
  {
    "text": "if container d reboots when and you have to reconnect kubelet to container d then",
    "start": "1175120",
    "end": "1181919"
  },
  {
    "text": "it'll be using continually using the b1 api if however you're using an older version of container d or cryo and it's",
    "start": "1181919",
    "end": "1189760"
  },
  {
    "text": "connected initially to the v1 alpha api then it'll still be using the v1 alpha",
    "start": "1189760",
    "end": "1194799"
  },
  {
    "text": "api unless you reboot kubelet and then it'll recheck to see if the b1 is there just give you an idea that",
    "start": "1194799",
    "end": "1201600"
  },
  {
    "text": "yes there's going to be a migration kind of issue but we're going to tackle it in the container run times and include",
    "start": "1201600",
    "end": "1207840"
  },
  {
    "text": "to be okay with either one migrating up but you'd have to reboot both to get",
    "start": "1207840",
    "end": "1215280"
  },
  {
    "text": "up to the next version of the api and i imagine we'll do the same thing when we go from the b1 beta to the v1 ga okay so",
    "start": "1215280",
    "end": "1223120"
  },
  {
    "text": "that's going to happen confidential computing very exciting a",
    "start": "1223120",
    "end": "1228400"
  },
  {
    "text": "lot of interesting stuff going on in multi-tenancy we've run into some",
    "start": "1228400",
    "end": "1235039"
  },
  {
    "text": "issues the one of the oldest issues in in kubernetes is a little problem with",
    "start": "1235039",
    "end": "1241120"
  },
  {
    "text": "if a pod a pulls an image down to down to the node and pod b wants to use that",
    "start": "1241120",
    "end": "1247360"
  },
  {
    "text": "image and you're only you're only saying pull if not present then it'll plot b will get to use the",
    "start": "1247360",
    "end": "1253919"
  },
  {
    "text": "image from the first poll whether or not it had access to it so if you're storing an image as a private image with it can",
    "start": "1253919",
    "end": "1260640"
  },
  {
    "text": "only get access with a password you know an authentication then it may already be on the node so",
    "start": "1260640",
    "end": "1266240"
  },
  {
    "text": "what we want to do with this ensure uh insert secret you know images is is actually check and",
    "start": "1266240",
    "end": "1273120"
  },
  {
    "text": "keep a running list of who actually pulled it and whether authors require it or not if you follow these links you'll you'll",
    "start": "1273120",
    "end": "1279440"
  },
  {
    "text": "find the cap it's been approved for one two three and and it's and we've got the code for",
    "start": "1279440",
    "end": "1285600"
  },
  {
    "text": "it as well um just just a heads up you know this this problem should be fixed however the the",
    "start": "1285600",
    "end": "1292159"
  },
  {
    "text": "workaround right now for multi-tenancy is to force a cont you know controller to always say pull always now if you use",
    "start": "1292159",
    "end": "1299600"
  },
  {
    "text": "pull always policy image policy then what the container runtimes will do is only pull the manifest",
    "start": "1299600",
    "end": "1306240"
  },
  {
    "text": "so if the if it's a one gigabyte image it's only going to pull the manifest part to make sure that",
    "start": "1306240",
    "end": "1312000"
  },
  {
    "text": "that you have authentication okay and if we've got it already locally cached",
    "start": "1312000",
    "end": "1317200"
  },
  {
    "text": "we'll just use that those blobs okay let's see",
    "start": "1317200",
    "end": "1322320"
  },
  {
    "text": "the the other thing that we've been running into a lot with confidential computing is that",
    "start": "1322320",
    "end": "1328159"
  },
  {
    "text": "we need this this ability to decide where we want to store",
    "start": "1328159",
    "end": "1333679"
  },
  {
    "text": "the the container image cache information the metadata information",
    "start": "1333679",
    "end": "1338799"
  },
  {
    "text": "the snapshotted images and you know whatever data you're storing in your in",
    "start": "1338799",
    "end": "1344159"
  },
  {
    "text": "your container and what the cata containers guys want to do and a few other groups is is to put that",
    "start": "1344159",
    "end": "1351520"
  },
  {
    "text": "information inside the virtual machine sounds like it'll be expensive but it will be very secure and so",
    "start": "1351520",
    "end": "1358720"
  },
  {
    "text": "instead of right now we pull the images and cache them on on your host what will happen is if you're if you say",
    "start": "1358720",
    "end": "1365600"
  },
  {
    "text": "you want uh runtime handlers handler to be secured containers you know with",
    "start": "1365600",
    "end": "1371039"
  },
  {
    "text": "internal then that runtime handle or switch would go to a configuration that's pointing to",
    "start": "1371039",
    "end": "1377520"
  },
  {
    "text": "pull the image into the virtual machine all right so so that's going on uh we can answer questions on this later",
    "start": "1377520",
    "end": "1384480"
  },
  {
    "text": "on hoping you guys are interested now another another set of changes that",
    "start": "1384480",
    "end": "1390720"
  },
  {
    "text": "we we would want to do um if you if you're looking at k native or istio or some some other situations",
    "start": "1390720",
    "end": "1398400"
  },
  {
    "text": "where maybe edge type situations where you want the pods to be really really fast",
    "start": "1398400",
    "end": "1403760"
  },
  {
    "text": "well there's a somewhat a couple of problems with the way kubernetes is currently designed it wasn't designed to run pods",
    "start": "1403760",
    "end": "1412880"
  },
  {
    "text": "you know every couple of milliseconds it was designed to run",
    "start": "1412880",
    "end": "1417919"
  },
  {
    "text": "run pods based on you know how they've been deployed and distributed around the node",
    "start": "1417919",
    "end": "1423120"
  },
  {
    "text": "and then after a couple of minutes we'll check to see if they're finished or not and if they're still running and then",
    "start": "1423120",
    "end": "1428960"
  },
  {
    "text": "and then we and if they're not we'll rerun the pods and keep matching the contract that you have but",
    "start": "1428960",
    "end": "1434880"
  },
  {
    "text": "kubernetes isn't really built for that contract is you know just a couple of milliseconds for a quick service or",
    "start": "1434880",
    "end": "1441760"
  },
  {
    "text": "even approach even half a second or two seconds so what we'd like to do is improve how",
    "start": "1441760",
    "end": "1447600"
  },
  {
    "text": "we handle probes um to move them from a per second kind of kind of level to milliseconds",
    "start": "1447600",
    "end": "1456000"
  },
  {
    "text": "and we'd also like to change the the cube process kubelik process actually",
    "start": "1456000",
    "end": "1461279"
  },
  {
    "text": "for for how it determines the current state of pods today it determines the",
    "start": "1461279",
    "end": "1466480"
  },
  {
    "text": "current state on a configurable but default i believe it is two minute pro cycle time",
    "start": "1466480",
    "end": "1472799"
  },
  {
    "text": "so it it you won't know at the scheduler side until after a couple of minutes whether",
    "start": "1472799",
    "end": "1479279"
  },
  {
    "text": "or not you know you need to run another pot and that's not really fast enough",
    "start": "1479279",
    "end": "1484720"
  },
  {
    "text": "for you know for a k-native kind of kind of scenario so we'd like to move to a subscription process it sounds it sounds",
    "start": "1484720",
    "end": "1491200"
  },
  {
    "text": "obvious we'll go with eventing we already have inventing support for tasks inside of",
    "start": "1491200",
    "end": "1496720"
  },
  {
    "text": "inside the runtime container so we'd like to move that outside and that's that's it",
    "start": "1496720",
    "end": "1504679"
  },
  {
    "text": "okay um i'd like to briefly highlight what we've been working on lately and",
    "start": "1516720",
    "end": "1522320"
  },
  {
    "text": "what to expect in upcoming continuity releases we plan to focus to focus more on our",
    "start": "1522320",
    "end": "1528960"
  },
  {
    "text": "streamview runtime the one responsible for container tasks tasks and for managing chain processes",
    "start": "1528960",
    "end": "1535760"
  },
  {
    "text": "uh up to canteen id 1.5 shim runtime interface didn't change much",
    "start": "1535760",
    "end": "1541039"
  },
  {
    "text": "so we implement just one runtime service called task service to manage the whole",
    "start": "1541039",
    "end": "1546080"
  },
  {
    "text": "container lifecycle and it was sufficient to run containers or and even to",
    "start": "1546080",
    "end": "1551919"
  },
  {
    "text": "launch pots and boxes however with micro vms there are a lot of new",
    "start": "1551919",
    "end": "1557279"
  },
  {
    "text": "uh use cases that we want to cover there are some new use cases the drive",
    "start": "1557279",
    "end": "1563919"
  },
  {
    "text": "runtime changes that we are working on uh some blocks api is ongoing for for",
    "start": "1563919",
    "end": "1569120"
  },
  {
    "text": "quite a while the goal is to make micro vm or any other sandbox first class citizen in continuity",
    "start": "1569120",
    "end": "1576159"
  },
  {
    "text": "there is an effort we use computation container containers to apply more security and container images so the",
    "start": "1576159",
    "end": "1581919"
  },
  {
    "text": "host can't access sensitive",
    "start": "1581919",
    "end": "1585519"
  },
  {
    "text": "added available support to our streams in 1.6 beta they use the same plugin system as",
    "start": "1596960",
    "end": "1603760"
  },
  {
    "text": "continuity daemon so it's easy to add new services to the runtime depend define dependencies between these",
    "start": "1603760",
    "end": "1609919"
  },
  {
    "text": "plugins and so on and in order to support new services on demand side we want to introduce new",
    "start": "1609919",
    "end": "1615679"
  },
  {
    "text": "ocean service that will manage sims processes independently from api that this shims",
    "start": "1615679",
    "end": "1622799"
  },
  {
    "text": "implement and we give more controls to clients how to schedule and how to manage workloads",
    "start": "1622799",
    "end": "1629279"
  },
  {
    "text": "through containing the apis a quick recap how our runtime system works today",
    "start": "1629279",
    "end": "1636799"
  },
  {
    "text": "so as i mentioned shims implement interface called test service and continuity daemon offers same back-end",
    "start": "1636960",
    "end": "1644240"
  },
  {
    "text": "api to manage tasks from client which hides their communications and",
    "start": "1644240",
    "end": "1650480"
  },
  {
    "text": "design communication with runtime so whenever we launch container from client we",
    "start": "1650480",
    "end": "1656559"
  },
  {
    "text": "call continue this service api and behind the scene it starts a new sim process for us",
    "start": "1656559",
    "end": "1662320"
  },
  {
    "text": "and then it forwards creates requests to that chain we just created",
    "start": "1662320",
    "end": "1667360"
  },
  {
    "text": "the same applies to subsequent calls containid keeps the list of running machines looks for the one we need and",
    "start": "1667360",
    "end": "1673840"
  },
  {
    "text": "forwards requests to proper instance with the new approach we allow to call",
    "start": "1673840",
    "end": "1679200"
  },
  {
    "text": "sims api directly so can the ldd still manages uh shim processes life cycle so we can ask",
    "start": "1679200",
    "end": "1685600"
  },
  {
    "text": "continuity to start a new stream for us or request existing one by id",
    "start": "1685600",
    "end": "1691600"
  },
  {
    "text": "but containing daemon is not aware what kind of services particular swim",
    "start": "1691600",
    "end": "1696640"
  },
  {
    "text": "implement instead we'll add content id client to talk to shim and depending",
    "start": "1696640",
    "end": "1703200"
  },
  {
    "text": "depending on services runtime offer client can decide proper flow of calls so for instance if stream implements",
    "start": "1703200",
    "end": "1709919"
  },
  {
    "text": "star service we launch container from client like we the same way we do it today",
    "start": "1709919",
    "end": "1715679"
  },
  {
    "text": "if in addition to that stream supports inbox api client can start sandboxing",
    "start": "1715679",
    "end": "1720799"
  },
  {
    "text": "sandbox instant before launching containers if in addition to that stream supports",
    "start": "1720799",
    "end": "1725919"
  },
  {
    "text": "confidential containers apis client can prepare image for that sandbox",
    "start": "1725919",
    "end": "1730960"
  },
  {
    "text": "and this is flexible enough to enable all kind of service combinations and support it's",
    "start": "1730960",
    "end": "1737840"
  },
  {
    "text": "easy to support new use cases the concept behind stream service is",
    "start": "1737840",
    "end": "1743279"
  },
  {
    "text": "really simple it can create and it can delete ships to be precise we already do that today",
    "start": "1743279",
    "end": "1749919"
  },
  {
    "text": "we just split existing touch service into many streams and does tasks",
    "start": "1749919",
    "end": "1755120"
  },
  {
    "text": "independently as two different plugins and we'll expose its api to clients so clients can control shins",
    "start": "1755120",
    "end": "1762559"
  },
  {
    "text": "containing this still expects to track a shim lifecycle so whenever shim process dies containid will clean it up",
    "start": "1762559",
    "end": "1770880"
  },
  {
    "text": "this service is going to be a foundation for high level services which depends on",
    "start": "1770880",
    "end": "1775919"
  },
  {
    "text": "specific ship implementations task is the only service we require to implement to be implemented today so",
    "start": "1775919",
    "end": "1782799"
  },
  {
    "text": "continuity can launch new containers by first calling action service to start new sim instance and then by calling the",
    "start": "1782799",
    "end": "1790159"
  },
  {
    "text": "service api from that stream and this way we keep backward compatibility with previous versions of",
    "start": "1790159",
    "end": "1797360"
  },
  {
    "text": "continuity same way sandbox api will invoke different set of services offered by",
    "start": "1797360",
    "end": "1803120"
  },
  {
    "text": "shins um sandbox api will be one of one more consumer option service",
    "start": "1803120",
    "end": "1810159"
  },
  {
    "text": "it aims to bring a notion of group of containers to containid the goal is to provide the well-defined",
    "start": "1810159",
    "end": "1816240"
  },
  {
    "text": "shim interface to test task api that can be used to add new sandbox implementations and",
    "start": "1816240",
    "end": "1822640"
  },
  {
    "text": "runtime level behind sandbox concept can be really anything depending on how runtime",
    "start": "1822640",
    "end": "1828559"
  },
  {
    "text": "implements it's so for instance it can be post container micro vm or even virtual machine",
    "start": "1828559",
    "end": "1835278"
  },
  {
    "text": "that's all i have thank you so much for listening and we'll be happy to answer your questions",
    "start": "1835600",
    "end": "1842120"
  },
  {
    "text": "[Applause]",
    "start": "1843800",
    "end": "1847760"
  }
]