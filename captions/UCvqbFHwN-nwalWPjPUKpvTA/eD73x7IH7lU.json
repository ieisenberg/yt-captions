[
  {
    "text": "thank you hello everyone so I'm Kimmy uh very happy to be here today so the idea",
    "start": "359",
    "end": "6359"
  },
  {
    "text": "behind this talk is essentially a project that the University of Alabama and canonical did together last winter",
    "start": "6359",
    "end": "12080"
  },
  {
    "text": "so at the University they already had an HPC environment and an openstack based",
    "start": "12080",
    "end": "19140"
  },
  {
    "text": "environment for their users but in terms of containers all of the different",
    "start": "19140",
    "end": "24300"
  },
  {
    "text": "environments were like on laptops containers here and there some Docker swarm over there and the idea was to",
    "start": "24300",
    "end": "30420"
  },
  {
    "text": "provide a dedicated kubernetes platform to their users and so we worked together",
    "start": "30420",
    "end": "35460"
  },
  {
    "text": "to get this done and we're going to go today through the kubernetes",
    "start": "35460",
    "end": "40860"
  },
  {
    "text": "architecture that we chose and a few of the Integrations that we've done and then my colleague John Paul will go a",
    "start": "40860",
    "end": "47520"
  },
  {
    "text": "little bit more through what type of research they do and and how they leverage that infrastructure for the",
    "start": "47520",
    "end": "55020"
  },
  {
    "text": "research so for the kubernetes architecture in this case we're talking about the",
    "start": "55020",
    "end": "61260"
  },
  {
    "text": "curving cheese on bare metal deployment so um when we talk about bare metal",
    "start": "61260",
    "end": "67380"
  },
  {
    "text": "deployments we want to have some infrastructure nodes you can also call them like management nodes essentially",
    "start": "67380",
    "end": "73260"
  },
  {
    "text": "those are outside of the kubernetes deployment to be able to manage the",
    "start": "73260",
    "end": "78900"
  },
  {
    "text": "kubernetes environment um so the First Technology I'll highlight here is the metal as a service",
    "start": "78900",
    "end": "86060"
  },
  {
    "text": "platform so Mars is a burmesole provisioning tool super useful when you",
    "start": "86060",
    "end": "91439"
  },
  {
    "text": "have a large estate of bare metal servers it provides you with your asset",
    "start": "91439",
    "end": "96840"
  },
  {
    "text": "inventory you can do a layout of the type of storage layout networking and",
    "start": "96840",
    "end": "103979"
  },
  {
    "text": "all of the layout that you want to do when you pixiboot and deploy your servers can be automated through Mass so",
    "start": "103979",
    "end": "110579"
  },
  {
    "text": "that was a really useful tool to use um then to",
    "start": "110579",
    "end": "116700"
  },
  {
    "text": "choose which machine we're deploying and to deploy them with application to",
    "start": "116700",
    "end": "121860"
  },
  {
    "text": "deploy the OS and to deploy different applications we're using Juju",
    "start": "121860",
    "end": "127140"
  },
  {
    "text": "um Juju is called also a operator lifecycle controller essentially it's",
    "start": "127140",
    "end": "133620"
  },
  {
    "text": "one of the best aspects of it is that there's a lot of day two operations built in so once you deploy applications",
    "start": "133620",
    "end": "141360"
  },
  {
    "text": "you can relate them to each other so let's say you want to relate your",
    "start": "141360",
    "end": "147000"
  },
  {
    "text": "kubernetes worker to the control plane it's a simple relation in a model that you define in yaml so it makes it simple",
    "start": "147000",
    "end": "154140"
  },
  {
    "text": "and when you want to scale your cluster for example you want to add your kubernetes node you can simply do like",
    "start": "154140",
    "end": "159599"
  },
  {
    "text": "Juju add unit kubernetes worker and it's going to scale for you you can also reduce you can upgrade and update",
    "start": "159599",
    "end": "167519"
  },
  {
    "text": "um so that's the main manager that we're using In This Cloud in terms of observability it's pretty",
    "start": "167519",
    "end": "175080"
  },
  {
    "text": "standard uh we're using open source project that I'm sure a lot of you are using as well so elasticsearch grafana",
    "start": "175080",
    "end": "182160"
  },
  {
    "text": "Prometheus since the whole deployment is done on Ubuntu servers we're also using the landscape server that tool lets you",
    "start": "182160",
    "end": "190379"
  },
  {
    "text": "see if you have security vulnerabilities in your environment depending on the",
    "start": "190379",
    "end": "196019"
  },
  {
    "text": "version of packages that you're using so we're also using that and then for the",
    "start": "196019",
    "end": "201599"
  },
  {
    "text": "Securus management tool for kubernetes we're using Vault and since it's",
    "start": "201599",
    "end": "207180"
  },
  {
    "text": "deployed on those three infrastructure nodes it's also ha and we're using AG proxy in front of it",
    "start": "207180",
    "end": "214980"
  },
  {
    "text": "so now for the actual kubernetes deployment um control plane we decided to do",
    "start": "214980",
    "end": "222420"
  },
  {
    "text": "um the control panel three nodes with standard components all of these",
    "start": "222420",
    "end": "227519"
  },
  {
    "text": "components are operators or charms that are deployed with Juju so Calico for the",
    "start": "227519",
    "end": "234420"
  },
  {
    "text": "networking we got hcd qbpi load balancer and then for the workers we had two",
    "start": "234420",
    "end": "240540"
  },
  {
    "text": "different sets of workers we have generic workers Dell machines uh pretty",
    "start": "240540",
    "end": "245879"
  },
  {
    "text": "standard and powerful and then we have specific GPU nodes so those are Nvidia",
    "start": "245879",
    "end": "253319"
  },
  {
    "text": "dgx a100 that you use in this case and I'll go a bit more in detail into how we",
    "start": "253319",
    "end": "259079"
  },
  {
    "text": "integrate that to provide gpus for the pods in a few minutes",
    "start": "259079",
    "end": "265860"
  },
  {
    "text": "um and finally for the storage aspect of the cluster we use ceph in some",
    "start": "265860",
    "end": "272280"
  },
  {
    "text": "architecture you may see ceph declared hyper converge with kubernetes so you",
    "start": "272280",
    "end": "277380"
  },
  {
    "text": "could have your cephosds directly on each step for it on each kubernetes worker",
    "start": "277380",
    "end": "283560"
  },
  {
    "text": "um it's not the case in this case we connected to an external well Standalone theft cluster separate from the",
    "start": "283560",
    "end": "289740"
  },
  {
    "text": "kubernetes cluster the main reason for that is that that set cluster is used",
    "start": "289740",
    "end": "294960"
  },
  {
    "text": "for a few other clusters in the environment so for example the openstack connects to it",
    "start": "294960",
    "end": "301639"
  },
  {
    "text": "some users can use the file share system from it separately from the case cluster",
    "start": "301639",
    "end": "309600"
  },
  {
    "text": "so ceph provides persistent volumes for for kids it can provide block storage we",
    "start": "309600",
    "end": "316139"
  },
  {
    "text": "have S3 and also yes ffs",
    "start": "316139",
    "end": "320960"
  },
  {
    "text": "um let's go into the different Integrations we've done um first of course the Nvidia gpus",
    "start": "321419",
    "end": "329460"
  },
  {
    "text": "um one of the neat things about the Nvidia nodes is that they published the",
    "start": "329460",
    "end": "335820"
  },
  {
    "text": "GPU operator which makes it pretty easy to um to set up if you install the GPU",
    "start": "335820",
    "end": "343020"
  },
  {
    "text": "operator on your Nvidia nodes you don't have to set up your OS in a very specific way like we just had Ubuntu",
    "start": "343020",
    "end": "349800"
  },
  {
    "text": "vanilla then you have the GPU operator deployed in kubernetes and that's a",
    "start": "349800",
    "end": "355080"
  },
  {
    "text": "little bit what you see with the different pods here there's a discovery pod that will go and scan your nodes and",
    "start": "355080",
    "end": "360240"
  },
  {
    "text": "find which one have gpus and it will install the proper drivers inside of",
    "start": "360240",
    "end": "365639"
  },
  {
    "text": "PODS to really simplify the setup overall",
    "start": "365639",
    "end": "371280"
  },
  {
    "text": "and then one feature that we used from Nvidia is the multi-instance GPU",
    "start": "371280",
    "end": "377759"
  },
  {
    "text": "profiles essentially that lets you slice your GPU into smaller instances",
    "start": "377759",
    "end": "384820"
  },
  {
    "text": "[Music] um one dgx node has eight GPU cards and",
    "start": "384820",
    "end": "390000"
  },
  {
    "text": "they're really powerful um and those nodes are also really expensive so there's a limit to like how",
    "start": "390000",
    "end": "396120"
  },
  {
    "text": "many you want to purchase for your cluster in this case if you slice your GPU into like smaller slices you're able",
    "start": "396120",
    "end": "402780"
  },
  {
    "text": "to provide more uh gpus for different types of workloads that may have",
    "start": "402780",
    "end": "407880"
  },
  {
    "text": "different requirements and it also it makes it so that each GPU is uh",
    "start": "407880",
    "end": "414539"
  },
  {
    "text": "independent from each other so in our in our case we deployed three nodes",
    "start": "414539",
    "end": "420479"
  },
  {
    "text": "with no make profiles and then one node with the seven slices per GPU",
    "start": "420479",
    "end": "426600"
  },
  {
    "text": "um profile and that's also something that you can change later on when you see that the need or the request for one",
    "start": "426600",
    "end": "433620"
  },
  {
    "text": "type of GPU is is greater than the other so it makes it a pretty uh easy to use",
    "start": "433620",
    "end": "439860"
  },
  {
    "text": "and then to configure those profiles it's simply",
    "start": "439860",
    "end": "445259"
  },
  {
    "text": "a label on the nodes so once you label your node with a specific type of",
    "start": "445259",
    "end": "450300"
  },
  {
    "text": "profile the GPU operator picks up that configuration and configures it for you",
    "start": "450300",
    "end": "457620"
  },
  {
    "text": "um so you see an account of 56 here for the seven slices for eight cards on the",
    "start": "457620",
    "end": "463440"
  },
  {
    "text": "one node that we set up like that on the networking side we use metal lb",
    "start": "463440",
    "end": "470280"
  },
  {
    "text": "so for those of you who are familiar with kubernetes and public clouds well you have access to applicational",
    "start": "470280",
    "end": "476099"
  },
  {
    "text": "balancers that you don't have to to do magic to have set up in your environment",
    "start": "476099",
    "end": "481860"
  },
  {
    "text": "but on bare metal unless you purchase expensive appliances that do you know",
    "start": "481860",
    "end": "488400"
  },
  {
    "text": "hard roll bouncing uh you don't have that many options so metal B is a really useful open source",
    "start": "488400",
    "end": "494099"
  },
  {
    "text": "project that can simplify your life for load balancing in kids so essentially",
    "start": "494099",
    "end": "500340"
  },
  {
    "text": "you give a set of external or public IPS that you want metal B to be able to assign to pods and if a pod requests a",
    "start": "500340",
    "end": "508440"
  },
  {
    "text": "service type of a type well if service stuff type load balancer it will get",
    "start": "508440",
    "end": "514500"
  },
  {
    "text": "additional IP there's two different modes Layer Two is preferred unless you have switches that are able to do bgp so",
    "start": "514500",
    "end": "521820"
  },
  {
    "text": "in our case we did a layer 2 setup next authentication",
    "start": "521820",
    "end": "529200"
  },
  {
    "text": "um you can do you know local users for your kids you can connect to ldap you can connect to IDC private providers but",
    "start": "529200",
    "end": "537420"
  },
  {
    "text": "in the University's environment saml is used extensively in a lot of",
    "start": "537420",
    "end": "543180"
  },
  {
    "text": "different environments so we decided to do a saml authentication it might surprise you that we're using",
    "start": "543180",
    "end": "550560"
  },
  {
    "text": "Keystone to make that possible for those of you who are familiar with openstack Keystone is an openstack project but the",
    "start": "550560",
    "end": "557519"
  },
  {
    "text": "way that we developed the operators you can connect the Keystone operator to",
    "start": "557519",
    "end": "564180"
  },
  {
    "text": "kubernetes control plane and it will request its authentication through Keystone and then we connected Keystone",
    "start": "564180",
    "end": "570899"
  },
  {
    "text": "to the saml back-end so that makes it a lot easier for ueb to",
    "start": "570899",
    "end": "576959"
  },
  {
    "text": "manage their their users and you can Define the access level in your case",
    "start": "576959",
    "end": "582360"
  },
  {
    "text": "cluster through policies and in the saml back-end",
    "start": "582360",
    "end": "587300"
  },
  {
    "text": "um the sample part was probably the trickiest part a little bit because of networking when",
    "start": "590339",
    "end": "596940"
  },
  {
    "text": "you have to make sure that your pods talk to the right thing",
    "start": "596940",
    "end": "602480"
  },
  {
    "text": "that you have Keystone talking to the saml backend and all that I'm not going to go in detail through this but I",
    "start": "603060",
    "end": "609360"
  },
  {
    "text": "wanted to give a shout out to Gustavo Sanchez a guy on my team who helped a lot with making this work",
    "start": "609360",
    "end": "615600"
  },
  {
    "text": "and then finally the last integration I'm going to highlight is the gitlab integration so we were able to do",
    "start": "615600",
    "end": "623160"
  },
  {
    "text": "um integration with gitlab so all of your Runners from UCI CD pipeline can run in",
    "start": "623160",
    "end": "629040"
  },
  {
    "text": "kubernetes",
    "start": "629040",
    "end": "631820"
  },
  {
    "text": "sorry you got something in my throat um and save your container into your",
    "start": "634800",
    "end": "640560"
  },
  {
    "text": "gitlab registry so on that I'm going to leave it to John Paul to continue with the research",
    "start": "640560",
    "end": "646980"
  },
  {
    "text": "aspect of the project thanks Camille um so what I thought I'd do is uh give",
    "start": "646980",
    "end": "654180"
  },
  {
    "text": "you a little bit of a overview of the University of Alabama at Birmingham um just to kind of",
    "start": "654180",
    "end": "660540"
  },
  {
    "text": "help you understand the the scope and the place in which this deployment is occurring so uh UAB is a large public",
    "start": "660540",
    "end": "666959"
  },
  {
    "text": "institution located in Birmingham but the metro area has about a million a little bit more than a million people in",
    "start": "666959",
    "end": "672120"
  },
  {
    "text": "it that represents about a fifth of the state the population of the state we also are the largest employer in the",
    "start": "672120",
    "end": "678660"
  },
  {
    "text": "state and have a very large economic impact on the region so uh academically",
    "start": "678660",
    "end": "683820"
  },
  {
    "text": "we have about over 20 000 enrolled students a good third of those are in",
    "start": "683820",
    "end": "689459"
  },
  {
    "text": "The Graduate research space post undergraduate space and we generate",
    "start": "689459",
    "end": "697019"
  },
  {
    "text": "about 600 million annually in research funding from the different national",
    "start": "697019",
    "end": "702260"
  },
  {
    "text": "funding agencies like the NIH heavily and the NSF research Computing the group",
    "start": "702260",
    "end": "708000"
  },
  {
    "text": "that I'm with we're part of the I.T organization at UAB that serves the campus",
    "start": "708000",
    "end": "713640"
  },
  {
    "text": "we have about 200 monthly users and um our our researchers represent about 30",
    "start": "713640",
    "end": "720120"
  },
  {
    "text": "percent of the research revenue at the University so",
    "start": "720120",
    "end": "726480"
  },
  {
    "text": "um if you know anything about sports in Alabama um just to highlight where the",
    "start": "726480",
    "end": "731519"
  },
  {
    "text": "University of Alabama at Birmingham our mascot is a dragon not an elephant",
    "start": "731519",
    "end": "737820"
  },
  {
    "text": "um so anyway the um what I'll do is I'll just talk a",
    "start": "737820",
    "end": "743459"
  },
  {
    "text": "little bit about why we were interested in Kate's to begin with I'll kind of talk about how it sits in our research",
    "start": "743459",
    "end": "749600"
  },
  {
    "text": "infrastructure and then kind of go over some of the future use cases we see for",
    "start": "749600",
    "end": "755519"
  },
  {
    "text": "for Kate's in the research environment so a lot of the Kate's use cases that um",
    "start": "755519",
    "end": "762300"
  },
  {
    "text": "we have are what you might consider you know a standard fare for microservices we have a self-service application that",
    "start": "762300",
    "end": "769620"
  },
  {
    "text": "we're building for users at our University for our researchers to be able to kind of manage their lab",
    "start": "769620",
    "end": "775320"
  },
  {
    "text": "environments manage the resources that we provide for them the I.T Services specifically around computation",
    "start": "775320",
    "end": "783240"
  },
  {
    "text": "and we have the you know an automatic user provisioning workflow a group management workflow that we're",
    "start": "783240",
    "end": "789180"
  },
  {
    "text": "developing and we've deployed that on a traditional kind of message based application on traditional",
    "start": "789180",
    "end": "794220"
  },
  {
    "text": "infrastructure we're very interested in moving that over to a Kate's platform and making that a microservices",
    "start": "794220",
    "end": "799680"
  },
  {
    "text": "experience we also are working with uh essentially",
    "start": "799680",
    "end": "805260"
  },
  {
    "text": "the leading adopters of Kate's on campus we have a few users that have built their own case platform that they're",
    "start": "805260",
    "end": "813000"
  },
  {
    "text": "running in the cloud they're interested in bringing that onto campus and we have some use cases that are kind of leading",
    "start": "813000",
    "end": "819019"
  },
  {
    "text": "research use cases for applications that already exist um that are case oriented",
    "start": "819019",
    "end": "825180"
  },
  {
    "text": "um so one of the primary spaces though that we're working on right now is that CI CD integration basically as we build",
    "start": "825180",
    "end": "832740"
  },
  {
    "text": "these applications we want to be able to make sure that we don't fall behind in our in our workloads one of the problems",
    "start": "832740",
    "end": "838380"
  },
  {
    "text": "that we had in our own development was that we would just essentially get a huge merge backlog in our workflow and",
    "start": "838380",
    "end": "844920"
  },
  {
    "text": "so we're able to use the CI CD pipelines that we're now moving over to Kate's to",
    "start": "844920",
    "end": "850560"
  },
  {
    "text": "help clear that out by having nightly builds and moving forward with that so the kind of the Leading Edge of the use",
    "start": "850560",
    "end": "858300"
  },
  {
    "text": "cases that kind of drove us to saying hey we really need to get a Kate's capability on campus is this machine",
    "start": "858300",
    "end": "864240"
  },
  {
    "text": "learning Ops workflow that's becoming more and more popular out there one of",
    "start": "864240",
    "end": "869760"
  },
  {
    "text": "the characteristics of ml Ops if you know kubeflow or ml flow or next flow is",
    "start": "869760",
    "end": "875700"
  },
  {
    "text": "that machine learning applications tend to want a lot of control over the environmental configuration that they",
    "start": "875700",
    "end": "882000"
  },
  {
    "text": "have and they want to kind of be deeply integrated with the workflow of the machine learning Pipeline and so that",
    "start": "882000",
    "end": "888959"
  },
  {
    "text": "makes them a hard fit into what might be more traditional compute environments and so we wanted to make sure we had a",
    "start": "888959",
    "end": "894660"
  },
  {
    "text": "Kate's platform available that's why we kind of peppered it with some gpus also along the way so",
    "start": "894660",
    "end": "902459"
  },
  {
    "text": "I think it's also helpful to understand what we mean when we speak in terms of like research Computing and high",
    "start": "902459",
    "end": "908579"
  },
  {
    "text": "performance Computing we tend to think of our platform as uh units of high performance Computing and a high",
    "start": "908579",
    "end": "915420"
  },
  {
    "text": "performance computer in our environment is something that has a considerable amount of ram",
    "start": "915420",
    "end": "920579"
  },
  {
    "text": "um you know three to 100 gigabytes is very common we have a few that have a",
    "start": "920579",
    "end": "925680"
  },
  {
    "text": "above a terabyte we have a couple of CPUs they have you know 24 to 64 cores",
    "start": "925680",
    "end": "931019"
  },
  {
    "text": "on them and then we may have accelerators in those nodes each of those nodes definitely has a high-speed",
    "start": "931019",
    "end": "937260"
  },
  {
    "text": "Nick on them to let the data move on and off the node itself and of course in in",
    "start": "937260",
    "end": "942779"
  },
  {
    "text": "the HPC context you think in terms of speeds of network and so you want to keep your your data close to your CPU",
    "start": "942779",
    "end": "949380"
  },
  {
    "text": "with the highest speed and then your internal networks at the next highest speed and then your data ingestion onto",
    "start": "949380",
    "end": "956639"
  },
  {
    "text": "the node at the the lowest speed if you will so um we combine those compute resources into",
    "start": "956639",
    "end": "964139"
  },
  {
    "text": "clusters so we just kind of buy a bunch of them at a time and we stick them together and we operate them on a",
    "start": "964139",
    "end": "969600"
  },
  {
    "text": "connected Network and then we make make it possible for them to pass data in between each other",
    "start": "969600",
    "end": "975360"
  },
  {
    "text": "and reach the essentially the global file system that we run as gpfs that's what we use on our HPC side and we have",
    "start": "975360",
    "end": "982800"
  },
  {
    "text": "ceph as Camille mentioned for various use cases for Block object and also some",
    "start": "982800",
    "end": "989360"
  },
  {
    "text": "essentially leading file system type solutions that we're exploring",
    "start": "989360",
    "end": "995279"
  },
  {
    "text": "we we combine those into a fabric that we then expose in different uh compute",
    "start": "995279",
    "end": "1001220"
  },
  {
    "text": "flavors and our kind of bread and butter compute flavor is the HPC batch compute",
    "start": "1001220",
    "end": "1007339"
  },
  {
    "text": "flavor we run the slurm scheduler if you're familiar with that what you do inside slurm is you ask for a certain",
    "start": "1007339",
    "end": "1013639"
  },
  {
    "text": "amount of ram a certain amount of cores and nodes for a certain period of time and then slurm allocates that when it's",
    "start": "1013639",
    "end": "1020600"
  },
  {
    "text": "available and ensures that nobody else is competing for that resource on with you typically in a batch Computing",
    "start": "1020600",
    "end": "1027860"
  },
  {
    "text": "environment you tend to think in terms of a terminal based access and SSH access um",
    "start": "1027860",
    "end": "1033500"
  },
  {
    "text": "and uh uh that is certainly true in our case as well but we've also deployed an",
    "start": "1033500",
    "end": "1039860"
  },
  {
    "text": "on-demand open on demand platform which is a web GUI that you wrap around your cluster and it provides a really easy to",
    "start": "1039860",
    "end": "1047480"
  },
  {
    "text": "use resource for researchers where they can do pretty much everything that they need to do with an HPC cluster inside",
    "start": "1047480",
    "end": "1053480"
  },
  {
    "text": "their browser so including running like Matlab and other traditional X11 applications in a",
    "start": "1053480",
    "end": "1059600"
  },
  {
    "text": "vndc session inside their desktop it also provides uh essentially web",
    "start": "1059600",
    "end": "1066559"
  },
  {
    "text": "proxy capabilities so you can tie a web proxy into a job and you can run things that are kind of web native like Jupiter",
    "start": "1066559",
    "end": "1072320"
  },
  {
    "text": "notebooks and rstudio and things of that nature but a lot of our researchers just run their traditional SMP or MPI",
    "start": "1072320",
    "end": "1079760"
  },
  {
    "text": "applications or even their pleasantly parallel single core independent data",
    "start": "1079760",
    "end": "1085000"
  },
  {
    "text": "workflows on the HPC batch and like I said that's our biggest workload engine right now today for for research at UAB",
    "start": "1085000",
    "end": "1092740"
  },
  {
    "text": "we also have an openstack cloud and we provided that",
    "start": "1092740",
    "end": "1098080"
  },
  {
    "text": "primarily so that we could have the an easy to use platform when the impedance",
    "start": "1098080",
    "end": "1104799"
  },
  {
    "text": "mismatch to our cluster is high right so if you have for example a science",
    "start": "1104799",
    "end": "1111020"
  },
  {
    "text": "Gateway that's a very common tool that people want to run it's some sort of a web application that hides the details",
    "start": "1111020",
    "end": "1117260"
  },
  {
    "text": "of a complex platform behind the back end and exposes to the and users",
    "start": "1117260",
    "end": "1123500"
  },
  {
    "text": "something that they can get started with right away kind of like our on-demand platform but often these are geared",
    "start": "1123500",
    "end": "1129320"
  },
  {
    "text": "towards specific science domains uh that lets a new graduate student or new researcher get on board using",
    "start": "1129320",
    "end": "1135679"
  },
  {
    "text": "computation and Analysis in their world much more quickly so you can't readily",
    "start": "1135679",
    "end": "1141380"
  },
  {
    "text": "deploy one of those or easily deploy one of those into an HPC batch cluster there's a lot of things that don't work",
    "start": "1141380",
    "end": "1147080"
  },
  {
    "text": "with that so that's one of the reasons why we have openstack we also have it to help with our development workloads we",
    "start": "1147080",
    "end": "1153020"
  },
  {
    "text": "actually do development cluster Builds on on that openstack platform even when",
    "start": "1153020",
    "end": "1158480"
  },
  {
    "text": "you want to use a container-like model in HPC bash with Singularity you still need an environment where you can be",
    "start": "1158480",
    "end": "1164240"
  },
  {
    "text": "route to build that container so for all of those reasons we have an openstack cloud and then as I mentioned",
    "start": "1164240",
    "end": "1172340"
  },
  {
    "text": "when we were pursuing Kate's because we know we need a container-based abstraction and kind of an orchestration",
    "start": "1172340",
    "end": "1178220"
  },
  {
    "text": "workload manager for uh you know next-gen machine",
    "start": "1178220",
    "end": "1183740"
  },
  {
    "text": "learning applications we also are interested as I mentioned moving our CI",
    "start": "1183740",
    "end": "1189260"
  },
  {
    "text": "CD workflows on there and starting to build more of a composable application environment for users",
    "start": "1189260",
    "end": "1196220"
  },
  {
    "text": "um and as you can see I mentioned some of the um the tooling the r shiny streamlit",
    "start": "1196220",
    "end": "1201559"
  },
  {
    "text": "snake making X flow those are a lot of the tools that we see as you know Desiring the container back ends and",
    "start": "1201559",
    "end": "1208160"
  },
  {
    "text": "right now we we didn't have a platform on which to um uh make that easy to just consume",
    "start": "1208160",
    "end": "1214520"
  },
  {
    "text": "those uh um containerized uh components of those workflows easily so just as a kind of a",
    "start": "1214520",
    "end": "1221720"
  },
  {
    "text": "reference related to Camille's uh slides this is where we're currently",
    "start": "1221720",
    "end": "1227780"
  },
  {
    "text": "using Maz and JuJu and charm deployments inside of our environment so we use it",
    "start": "1227780",
    "end": "1234080"
  },
  {
    "text": "for our openstack Cloud we use it for our Kate's container environment and we're working actively to migrate our",
    "start": "1234080",
    "end": "1240980"
  },
  {
    "text": "ceph platform over to that it's still going to be an independent theft cluster for the same reasons that we needed to",
    "start": "1240980",
    "end": "1246500"
  },
  {
    "text": "have it independent to begin with but the provisioning is going to happen through",
    "start": "1246500",
    "end": "1252919"
  },
  {
    "text": "through mass and JuJu our HPC batch is still done in a traditional HPC batch",
    "start": "1252919",
    "end": "1258140"
  },
  {
    "text": "way and the gpfs is an independent file system so um kind of in conclusion I",
    "start": "1258140",
    "end": "1264559"
  },
  {
    "text": "want to just talk a little bit about the the next gen use case that we really see for for Kate's the one that I think is",
    "start": "1264559",
    "end": "1271520"
  },
  {
    "text": "among the more exciting spaces that we can spend our time with over the coming years",
    "start": "1271520",
    "end": "1277059"
  },
  {
    "text": "as you can see on the uh you know in the in the research environment we have a",
    "start": "1277059",
    "end": "1282200"
  },
  {
    "text": "lot of different data sources we have scanners we have microscopes we have telescopes not so much at UAV we don't",
    "start": "1282200",
    "end": "1288140"
  },
  {
    "text": "use telescopes we're a Medical University so we tend to think in terms of scanners and microscopes but you also",
    "start": "1288140",
    "end": "1294980"
  },
  {
    "text": "have other data sources you know Gene Banks and other things like that that already have existing data sets that come in and they come into your",
    "start": "1294980",
    "end": "1301220"
  },
  {
    "text": "environment and ideally those just kind of stream into your environment and they have the associated",
    "start": "1301220",
    "end": "1307299"
  },
  {
    "text": "workloads uh kind of just managed and the customizations that you have to do",
    "start": "1307299",
    "end": "1314000"
  },
  {
    "text": "to your data to publish that data automatically run and then on the on the right hand side we have our essentially",
    "start": "1314000",
    "end": "1321500"
  },
  {
    "text": "our analysts sitting there composing uh analytical workflows through the",
    "start": "1321500",
    "end": "1327919"
  },
  {
    "text": "through the Kate's platform so they can work with things like Jupiter up at the",
    "start": "1327919",
    "end": "1333559"
  },
  {
    "text": "top as an example of a Jupiter application that's loading a lot of data up inside it and presenting a you know a",
    "start": "1333559",
    "end": "1340220"
  },
  {
    "text": "satellite image of the of the globe obviously but um that needs to basically",
    "start": "1340220",
    "end": "1345740"
  },
  {
    "text": "allow the users not just to consume the data coming off the platform but package",
    "start": "1345740",
    "end": "1350900"
  },
  {
    "text": "up that computation that they did to produce this new data set back into the platform so that it can be used by other",
    "start": "1350900",
    "end": "1358840"
  },
  {
    "text": "researchers Downstream so they don't just become consumers of data sets they",
    "start": "1358840",
    "end": "1363980"
  },
  {
    "text": "also become kind of instantaneously or immediately producers of data sets as",
    "start": "1363980",
    "end": "1369020"
  },
  {
    "text": "well I think that's a really powerful enabler that we're going to be seeing more of over time as we continue to",
    "start": "1369020",
    "end": "1376520"
  },
  {
    "text": "adopt the Kate's platform another use case that I think is kind of",
    "start": "1376520",
    "end": "1381740"
  },
  {
    "text": "critical for the reproducibility of research in higher education is to be",
    "start": "1381740",
    "end": "1387799"
  },
  {
    "text": "able to encapsulate the HPC workloads so the the work that they did on our HPC",
    "start": "1387799",
    "end": "1392960"
  },
  {
    "text": "batch computer to be able to capture that and then be able to reproduce it at",
    "start": "1392960",
    "end": "1398840"
  },
  {
    "text": "some point in the future and get out the same data that they had when they did their analysis and Drew their",
    "start": "1398840",
    "end": "1404900"
  },
  {
    "text": "conclusions on the cluster originally and obviously they could just go back",
    "start": "1404900",
    "end": "1410179"
  },
  {
    "text": "and run it on the cluster again but clusters even though they're kind of a slow-moving animal their os's are",
    "start": "1410179",
    "end": "1417020"
  },
  {
    "text": "reasonably stable they don't change very often the the os's the applications the",
    "start": "1417020",
    "end": "1422600"
  },
  {
    "text": "the storage environments They do change over time and so if you want to come back five years later and reproduce a",
    "start": "1422600",
    "end": "1429620"
  },
  {
    "text": "run it's going to be very difficult to do that so being able to essentially capture all of that up into a container",
    "start": "1429620",
    "end": "1436580"
  },
  {
    "text": "that then completely reproduces is that batch Computing experience that they had",
    "start": "1436580",
    "end": "1441980"
  },
  {
    "text": "by loading modules and you know referencing their different data sources",
    "start": "1441980",
    "end": "1447200"
  },
  {
    "text": "on the on the disk would be a very very helpful",
    "start": "1447200",
    "end": "1452320"
  },
  {
    "text": "reproducibility tool for researchers over time and that requires that it",
    "start": "1452320",
    "end": "1458360"
  },
  {
    "text": "doesn't you know this is not something that the researchers really want to sit around and design so you want to make it",
    "start": "1458360",
    "end": "1463820"
  },
  {
    "text": "possible for them to essentially say okay now capture this environment that I used and give me a container that allows",
    "start": "1463820",
    "end": "1471020"
  },
  {
    "text": "me to come back and use it again in the future so this is what I'm kind of determining like the The Next Gen",
    "start": "1471020",
    "end": "1477500"
  },
  {
    "text": "Science Gateway this platform where you essentially are able to move across the",
    "start": "1477500",
    "end": "1483620"
  },
  {
    "text": "different areas of your work and flow in research and modify it and reproduce it",
    "start": "1483620",
    "end": "1490460"
  },
  {
    "text": "effectively so with that I'd like to thank you all for attending our our talk",
    "start": "1490460",
    "end": "1495500"
  },
  {
    "text": "you're welcome to come reach out to us individually if you want to know more about either of these uh efforts or stop",
    "start": "1495500",
    "end": "1503059"
  },
  {
    "text": "by the booth or just to kind of keep a conversation going here if you have any questions",
    "start": "1503059",
    "end": "1508760"
  },
  {
    "text": "yeah thank you for the presentation we have a we have quite a few minutes for",
    "start": "1508760",
    "end": "1514520"
  },
  {
    "text": "for questions any questions just address the hand oh okay",
    "start": "1514520",
    "end": "1521559"
  },
  {
    "text": "thank you that was very entertaining um so on your traditional HPC badge",
    "start": "1528200",
    "end": "1533779"
  },
  {
    "text": "cluster you're obviously able to use tools like torque and slim to handle resource contention and queuing",
    "start": "1533779",
    "end": "1540020"
  },
  {
    "text": "um do you already have an infrastructure like that in place for kubernetes to handle say",
    "start": "1540020",
    "end": "1547220"
  },
  {
    "text": "um you know a similar Resort um resource contention between researchers so if one researcher wants all the gpus",
    "start": "1547220",
    "end": "1554360"
  },
  {
    "text": "and definitely um is there another tool or middleware to kind of handle that you know batch",
    "start": "1554360",
    "end": "1560900"
  },
  {
    "text": "queuing so that not everything is allocated to specific to a single or",
    "start": "1560900",
    "end": "1565940"
  },
  {
    "text": "specific workload yeah so that's a that's a really great question and the answer the quick answer is no we don't",
    "start": "1565940",
    "end": "1571460"
  },
  {
    "text": "yet but we know that that's an issue obviously in the in the batch Computing the reason why people do batch Computing",
    "start": "1571460",
    "end": "1578000"
  },
  {
    "text": "is for really kind of I guess two two reasons is one access to Bare Metal uh",
    "start": "1578000",
    "end": "1583640"
  },
  {
    "text": "performance right you don't get any kind of interference from abstraction layers and two is that you can share the",
    "start": "1583640",
    "end": "1589159"
  },
  {
    "text": "resources back schedules are very good at disciplining that over time our Kate's environment does not have",
    "start": "1589159",
    "end": "1594980"
  },
  {
    "text": "anything to essentially stop a researcher from having all the resources what I would like to see and where I'm",
    "start": "1594980",
    "end": "1601100"
  },
  {
    "text": "exploring with my team the capabilities of Kate's and HPC to come together is",
    "start": "1601100",
    "end": "1608179"
  },
  {
    "text": "how we might be able to man Fest some of the HPC capacity as a Kate's worker",
    "start": "1608179",
    "end": "1615080"
  },
  {
    "text": "platform so that's one approach that you could potentially use where you basically say okay well I have this",
    "start": "1615080",
    "end": "1620299"
  },
  {
    "text": "Kate's workload or this Kate's demand for for resources and I can schedule",
    "start": "1620299",
    "end": "1626000"
  },
  {
    "text": "that into my batch scheduling environment now that has problems obviously for immediate reaction the way",
    "start": "1626000",
    "end": "1632120"
  },
  {
    "text": "Kate's tends to have you know immediate responses so there's going to have to be other ways that we look at that and I've",
    "start": "1632120",
    "end": "1638299"
  },
  {
    "text": "only really started scratching the surface on how the Kate's containers can you know schedule or let's say make",
    "start": "1638299",
    "end": "1645020"
  },
  {
    "text": "reservations for their course and RAM but one of the things that I haven't really come to understand or come across",
    "start": "1645020",
    "end": "1650900"
  },
  {
    "text": "a solution for is how to stop them at a particular point in time right Kate's tends to run forever and batch tends to",
    "start": "1650900",
    "end": "1658100"
  },
  {
    "text": "run to completion so but if you have any if you have any suggestions on it then I'd be happy to",
    "start": "1658100",
    "end": "1663679"
  },
  {
    "text": "follow up with you on that thank you okay listen knowledge",
    "start": "1663679",
    "end": "1670039"
  },
  {
    "text": "on the way my suggestion is that you need to have a two level scheduler so in theory that could be the MP complete",
    "start": "1670039",
    "end": "1676820"
  },
  {
    "text": "problem but let's go for the next question",
    "start": "1676820",
    "end": "1681820"
  },
  {
    "text": "thank you um my question is regarding Cuda versions so we run in the cloud I have",
    "start": "1682460",
    "end": "1688760"
  },
  {
    "text": "some ml Engineers that want to use cuda10 some that want to use cuda11 right now I have to spend out two",
    "start": "1688760",
    "end": "1695659"
  },
  {
    "text": "different node groups one with two with manual Amis some of them have q to 10",
    "start": "1695659",
    "end": "1700820"
  },
  {
    "text": "some have Kudo 11. is that a good way to manage that how would you suggest maybe doing something like that",
    "start": "1700820",
    "end": "1707600"
  },
  {
    "text": "um well I mean when we when we had that requirement we just spin up different",
    "start": "1707600",
    "end": "1712760"
  },
  {
    "text": "um environments as well we we have typically been able to move forward with",
    "start": "1712760",
    "end": "1718039"
  },
  {
    "text": "Cuda so we tend to be on Cuda 11 on our cluster right now and so we're pretty consistent across Code 11 but um for",
    "start": "1718039",
    "end": "1726140"
  },
  {
    "text": "something where we would have to go to Cuda 10 that would be a case where we would use our openstack environment to",
    "start": "1726140",
    "end": "1732740"
  },
  {
    "text": "use a GPU from that environment so every one of our environments has gpus available to it but also with the um one",
    "start": "1732740",
    "end": "1740360"
  },
  {
    "text": "of our kind of our Visions for starting to leverage Mass more heavily is to be able to kind of say well we need more",
    "start": "1740360",
    "end": "1746120"
  },
  {
    "text": "capacity for this kind of workflow in one environment over the other and so we",
    "start": "1746120",
    "end": "1751159"
  },
  {
    "text": "can potentially move that compute capacity over for a period of time but right now it's just uh we're kind of in",
    "start": "1751159",
    "end": "1757700"
  },
  {
    "text": "the same space You Are no great solution thank you so now it's a last question",
    "start": "1757700",
    "end": "1764600"
  },
  {
    "text": "for today uh thanks very much the",
    "start": "1764600",
    "end": "1769940"
  },
  {
    "text": "typically when you are using containers for machine learning projects like this the",
    "start": "1769940",
    "end": "1776659"
  },
  {
    "text": "models or the containers can be very large to incorporate these things is there anything in your architecture",
    "start": "1776659",
    "end": "1783440"
  },
  {
    "text": "specifically to handle those types of issues uh to you mean to to ensure that there's",
    "start": "1783440",
    "end": "1789260"
  },
  {
    "text": "enough Jeep memory resources for that is that what you're kind of referring to or yeah",
    "start": "1789260",
    "end": "1795860"
  },
  {
    "text": "memory or or bandwidth on the network or things like that um I wouldn't say that there's something",
    "start": "1795860",
    "end": "1801740"
  },
  {
    "text": "specific in the architecture to handle that we have we have used nodes in our",
    "start": "1801740",
    "end": "1806840"
  },
  {
    "text": "environment that are generously provisioned so our a100s have their 40",
    "start": "1806840",
    "end": "1812179"
  },
  {
    "text": "gig a 100 so they have a lot of RAM on there and then we have a lot of memory",
    "start": "1812179",
    "end": "1818120"
  },
  {
    "text": "available on the nodes that are the worker nodes so I guess we're kind of",
    "start": "1818120",
    "end": "1823340"
  },
  {
    "text": "maybe cheating a little bit in the sense that we we know we have those um that capacity in our environment and",
    "start": "1823340",
    "end": "1830779"
  },
  {
    "text": "we don't have it uh you know over um kind of over subscribed yet because we",
    "start": "1830779",
    "end": "1837260"
  },
  {
    "text": "don't have a demand that goes beyond it yet but um in my mind the way we would go is with that additional scheduling",
    "start": "1837260",
    "end": "1844700"
  },
  {
    "text": "model where we would basically say okay well you need this much RAM and so we'll just have to go and get that out of a",
    "start": "1844700",
    "end": "1850820"
  },
  {
    "text": "resource pool either by you know physically moving uh physically but virtually moving one Reit compute",
    "start": "1850820",
    "end": "1857419"
  },
  {
    "text": "resource over into another Fabric or by doing one of those uh secondary scheduling layers where we could",
    "start": "1857419",
    "end": "1863299"
  },
  {
    "text": "basically bring in slurm to help us allocate some resources for a particular",
    "start": "1863299",
    "end": "1868820"
  },
  {
    "text": "compute workload uh okay so now we aim for the research",
    "start": "1868820",
    "end": "1875000"
  },
  {
    "text": "and dynamical session for the Google account thank you for everyone the participation see you next year",
    "start": "1875000",
    "end": "1881070"
  },
  {
    "text": "[Applause]",
    "start": "1881070",
    "end": "1886569"
  }
]