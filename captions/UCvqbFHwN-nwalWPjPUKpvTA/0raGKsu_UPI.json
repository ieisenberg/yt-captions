[
  {
    "text": "uh a very good morning everyone thank you so much for choosing to be here I know you had a number of uh parallel",
    "start": "320",
    "end": "5400"
  },
  {
    "text": "tracks to choose from we are excited to share our work today uh and I hope you'll learn",
    "start": "5400",
    "end": "11040"
  },
  {
    "text": "something uh from our talk if you don't we have a moneyback policy come talk to me at the end of the",
    "start": "11040",
    "end": "16278"
  },
  {
    "text": "talk okay so ah okay I'm selie cville I am the",
    "start": "16279",
    "end": "23160"
  },
  {
    "text": "engineering lead at a startup called otal we build uh kubernetes management",
    "start": "23160",
    "end": "28279"
  },
  {
    "text": "products uh we uh two of them one is called Luna it is an intelligent cluster autoscaler the other product is Nova a",
    "start": "28279",
    "end": "35360"
  },
  {
    "text": "multicluster scheduler and orchestrator uh I've been in the container communities management space since 2015",
    "start": "35360",
    "end": "42079"
  },
  {
    "text": "at uh another startup called container X and at Cisco previously I was working on",
    "start": "42079",
    "end": "47120"
  },
  {
    "text": "uh using machine learning techniques for infam management at VMware and as part of my PhD thesis uh I'll let man",
    "start": "47120",
    "end": "53960"
  },
  {
    "text": "introduce himself hi am Aman uh I work at yoby DB as a software engineer uh uh",
    "start": "53960",
    "end": "59280"
  },
  {
    "text": "I work on primarily work on intersection of uh kubernetes uh database management plan and database U prior to UK DB I",
    "start": "59280",
    "end": "66880"
  },
  {
    "text": "worked at uh in the virtualization and infrastructure space at newonics and",
    "start": "66880",
    "end": "73159"
  },
  {
    "text": "VMR okay so we'll start by describing the problem we targeted to solve by",
    "start": "74520",
    "end": "80880"
  },
  {
    "text": "passing through our long title zero touch fall tolerance for cloud native geod distributed databases we'll talk",
    "start": "80880",
    "end": "86280"
  },
  {
    "text": "about two different components the uh distributed SQL database specifically yoga by DB and what a multicluster",
    "start": "86280",
    "end": "92920"
  },
  {
    "text": "orchestrator is and how they work well together to provide us with zero touch fall tolerance we'll end with the",
    "start": "92920",
    "end": "99600"
  },
  {
    "text": "demo okay so what is Geo distribution uh simply defined it is when your database",
    "start": "99600",
    "end": "105079"
  },
  {
    "text": "is spread across two or more distinct geographical locations uh and it is done",
    "start": "105079",
    "end": "110960"
  },
  {
    "text": "in such a way that it's capable of uh operating without degraded transaction performance and why do we need it uh top",
    "start": "110960",
    "end": "118240"
  },
  {
    "text": "three reasons are typically uh we want our businesses to run highly responsive services and uh one way to do that is to",
    "start": "118240",
    "end": "125960"
  },
  {
    "text": "move your uh user data close to where your end users are we also want our businesses to comply to sovereignty",
    "start": "125960",
    "end": "132680"
  },
  {
    "text": "regulations and most importantly we want to be resilient to a wide variety of failures that the complex software and",
    "start": "132680",
    "end": "139599"
  },
  {
    "text": "Hardware Stacks that we operate in our businesses now uh what is a cloud native",
    "start": "139599",
    "end": "144640"
  },
  {
    "text": "database it primarily serves the use case of modern Cloud native applications which have three important requirements",
    "start": "144640",
    "end": "151040"
  },
  {
    "text": "the database uh needs to be able to scale we want to be able to deploy it on clouds on premises kubernetes virtual",
    "start": "151040",
    "end": "157920"
  },
  {
    "text": "environments or bare metal once again we needed to be resilient to failures it is this requirement of resilience to",
    "start": "157920",
    "end": "163840"
  },
  {
    "text": "failures that brings our need for zero touch uh fall tolerance now uh for the",
    "start": "163840",
    "end": "169319"
  },
  {
    "text": "past decade as soon as we started running our operations on public and private clouds we've had to deal with a",
    "start": "169319",
    "end": "174599"
  },
  {
    "text": "wide variety of uh failures despite that uh you know the amount amount of dollars",
    "start": "174599",
    "end": "180800"
  },
  {
    "text": "associated with it downtime has only continued to Skyrocket and this is an",
    "start": "180800",
    "end": "185879"
  },
  {
    "text": "example of from a Gartner survey from a few years ago um uh data center downtime",
    "start": "185879",
    "end": "191080"
  },
  {
    "text": "can cost companies between $140,000 to $540,000 per hour and this",
    "start": "191080",
    "end": "197360"
  },
  {
    "text": "is only uh for verticals that does not include critical services like uh banking manufacturing or Healthcare",
    "start": "197360",
    "end": "204480"
  },
  {
    "text": "where your uh losses could run up to a few million dollars per hour and typically we'd associate this with lost",
    "start": "204480",
    "end": "210720"
  },
  {
    "text": "revenue or missed SLA uh Financial penalties but in addition there's a number of other factors such as lost",
    "start": "210720",
    "end": "217120"
  },
  {
    "text": "productivity your teams are firefighting rather than actually adding to your business logic there's brand reputation",
    "start": "217120",
    "end": "222920"
  },
  {
    "text": "loss there's customer churn and there's employee retention if I were an employee and my pag of duty calls were exploding",
    "start": "222920",
    "end": "229519"
  },
  {
    "text": "I am going to look elsewhere to more mature stable environments now uh we",
    "start": "229519",
    "end": "236040"
  },
  {
    "text": "categorize uh who is responsible for providing such zero touch tolerance right is it your application architect",
    "start": "236040",
    "end": "241760"
  },
  {
    "text": "your application Ops your database architect or your database admin or your infr teams we categorize failures in uh",
    "start": "241760",
    "end": "248920"
  },
  {
    "text": "into three categories uh those that can be handled by inherent resilience within your uh Cloud native DB uh this includes",
    "start": "248920",
    "end": "257519"
  },
  {
    "text": "storage failures Network partitions and soft other software failures there is uh",
    "start": "257519",
    "end": "262880"
  },
  {
    "text": "the category of node in zone failures where kubernetes as your orchestrator within a single cluster helps solve so",
    "start": "262880",
    "end": "269639"
  },
  {
    "text": "you have your typical pod controllers that can bring up pods on new nodes you have Zone failures that can be handled",
    "start": "269639",
    "end": "275639"
  },
  {
    "text": "by um uh topology spread deployments uh and using multiple availability zones",
    "start": "275639",
    "end": "281440"
  },
  {
    "text": "and node groups associated with them it is when your fault domain becomes Regional uh failures or cluster level",
    "start": "281440",
    "end": "288240"
  },
  {
    "text": "failures that the combination of a cluster orchestrator along with the resilient database comes in handy uh",
    "start": "288240",
    "end": "295080"
  },
  {
    "text": "before we go into talk about how we do this uh Aman will talk to us about yugabyte yeah so yugabyte is a",
    "start": "295080",
    "end": "302240"
  },
  {
    "text": "transactional SQL distributed database um that is designed for resilience scale and global data distribution um it is",
    "start": "302240",
    "end": "309160"
  },
  {
    "text": "fully postgress compatible and it has support for advanced postgress features such as uh uh such as triggers uh stored",
    "start": "309160",
    "end": "316199"
  },
  {
    "text": "procedures and partial indexes yug DB can be deployed on VMS and kubernetes in the cloud or on premise um yug DB can",
    "start": "316199",
    "end": "324120"
  },
  {
    "text": "automatically heal from uh certain class of failures and does its own native uh",
    "start": "324120",
    "end": "329199"
  },
  {
    "text": "rep application it is um uh proven uh and it's a proven database uh designed",
    "start": "329199",
    "end": "336400"
  },
  {
    "text": "for scale uh and and Geo distribution um so so just sort of a a quick overview of",
    "start": "336400",
    "end": "344360"
  },
  {
    "text": "like a th000 ft overview of what uh yugy DB looks like so um in this slide We",
    "start": "344360",
    "end": "350319"
  },
  {
    "text": "have basically three main components one is the yugy DB Master which is uh referred to by you Vib Master this is",
    "start": "350319",
    "end": "357280"
  },
  {
    "text": "the control plane of the database this is responsible for for bringing up the database like bootstrapping responsible for uh Shard metadata placement",
    "start": "357280",
    "end": "363880"
  },
  {
    "text": "responsible for ddl operations like initializing and uh and modifying schema of a database then we have ybt server",
    "start": "363880",
    "end": "370240"
  },
  {
    "text": "which is the uh uh data plane of the database it is responsible for end user IO um internally uh tables of a database",
    "start": "370240",
    "end": "379240"
  },
  {
    "text": "are sharded in what's known as uh tablets and uh these tablets are replicated uh times uh whatever is the",
    "start": "379240",
    "end": "386880"
  },
  {
    "text": "replication factor of that particular database install and then uh each of these tablets",
    "start": "386880",
    "end": "392520"
  },
  {
    "text": "replicate uh replicated tablets are called tablet peers and each t- server is responsible for a section of uh",
    "start": "392520",
    "end": "398360"
  },
  {
    "text": "depending on the data placement policy responsible for a section of U serving IO to uh uh these tablets uh and then we",
    "start": "398360",
    "end": "406479"
  },
  {
    "text": "have a doc TB storage engine which is an extension of uh open source rocks DB we added a a raft based replication and",
    "start": "406479",
    "end": "414319"
  },
  {
    "text": "leader election layer on top of uh uh Rox TB and Doc TB is used",
    "start": "414319",
    "end": "419919"
  },
  {
    "text": "by ybt server and Vib Master both as the persistence layer um so also sort of how",
    "start": "419919",
    "end": "426400"
  },
  {
    "text": "this looks like on a single kubernetes cluster so on a single kubernetes cluster we deploy uh ukb using hem",
    "start": "426400",
    "end": "432879"
  },
  {
    "text": "charts um two main State full sets are deployed one is uh for Vib Master the",
    "start": "432879",
    "end": "438599"
  },
  {
    "text": "another one is for vibt Server um an appropriate um pod deception budgets are",
    "start": "438599",
    "end": "444319"
  },
  {
    "text": "Set uh in the stateful sets uh so that uh you know if there's a Planned or unplanned outage that takes a node out",
    "start": "444319",
    "end": "450199"
  },
  {
    "text": "or takes a section of PODS out we still respect the replication uh Factor uh of",
    "start": "450199",
    "end": "455360"
  },
  {
    "text": "the underlying database uh we also set Affinity rules um on each of the pods to make sure that they land on different",
    "start": "455360",
    "end": "461199"
  },
  {
    "text": "nodes so if there's a dis failure on a node or if a node uh needs to be upgraded uh ports can be scheduled by",
    "start": "461199",
    "end": "468520"
  },
  {
    "text": "the state full set controller on a different note State full set controller is pretty neat here because it gives us consistent naming and consistent storage",
    "start": "468520",
    "end": "474840"
  },
  {
    "text": "so when a pod moves from one Noe to another it takes care of making sure it shows up with the same name name",
    "start": "474840",
    "end": "479960"
  },
  {
    "text": "identifier um and then also takes care of attaching the right volumes to the right parts um in case uh so the data is",
    "start": "479960",
    "end": "487000"
  },
  {
    "text": "stored on persistent volumes in case one of the persistent volumes get corrupted uh ubb has inbuilt replication so it can",
    "start": "487000",
    "end": "493440"
  },
  {
    "text": "rebuild data by reboot strapping from its peers um oh yeah and then also we have the",
    "start": "493440",
    "end": "500360"
  },
  {
    "text": "headlight service that comes as part of the stateful set that uh the that is used by admin or uh app clients that to",
    "start": "500360",
    "end": "507639"
  },
  {
    "text": "talk to uh uh uh the VB master or ybt",
    "start": "507639",
    "end": "513240"
  },
  {
    "text": "server so all um so in general cunes clusters are usually single region so",
    "start": "513240",
    "end": "520399"
  },
  {
    "text": "how do you take a multi- region database and deploy it on a kubernetes cluster that is a single region technology uh",
    "start": "520399",
    "end": "526360"
  },
  {
    "text": "what we did we we made copies right so we run um so this is an example setup where we are running it across three",
    "start": "526360",
    "end": "531839"
  },
  {
    "text": "different regions so we have uh three different kubernetes clusters connected by esto um uho is a service mesh that",
    "start": "531839",
    "end": "538600"
  },
  {
    "text": "allows deploying workloads across multiple kubernetes clusters um and uh",
    "start": "538600",
    "end": "544000"
  },
  {
    "text": "uh pods and basically it allows us to configure uh the uh configure the data",
    "start": "544000",
    "end": "549760"
  },
  {
    "text": "plan such that pods running on one cluster can then talk to pods running on another cluster in this setup we Runo in",
    "start": "549760",
    "end": "555760"
  },
  {
    "text": "a multi- primary multi Network Mode which means that we have three copies of the esto eess Gateway Ingress Gateway",
    "start": "555760",
    "end": "562279"
  },
  {
    "text": "Ando running in the setup so one on each cluster um and uh we also expose using",
    "start": "562279",
    "end": "568480"
  },
  {
    "text": "DNS proxy expose the services that are running on each of the Clusters um to",
    "start": "568480",
    "end": "573720"
  },
  {
    "text": "the other two clusters this enables the ports to have full connectivity so um any part running on each any of these",
    "start": "573720",
    "end": "580519"
  },
  {
    "text": "clusters can then talk to um pods running on the other two clusters and at this point we deploy the yug by DB hel",
    "start": "580519",
    "end": "587360"
  },
  {
    "text": "chart that we talked about but we deploy three copies of it um and we deploy one so so that one of uh basically one ends",
    "start": "587360",
    "end": "594600"
  },
  {
    "text": "up on each of the Clusters and um we also uh Set uh uh as configuration in in",
    "start": "594600",
    "end": "602440"
  },
  {
    "text": "YB Master we set appropriate placement policies so that the database when it gets data places one replica of the data",
    "start": "602440",
    "end": "609279"
  },
  {
    "text": "in each region and uh also sets the replication factor to three what this gives us is that if one of the regions",
    "start": "609279",
    "end": "616040"
  },
  {
    "text": "of in the setup was to go down um the database availability is still maintained uh application availability",
    "start": "616040",
    "end": "622640"
  },
  {
    "text": "is unaffected so um application can uh basically transfer to a different region",
    "start": "622640",
    "end": "627880"
  },
  {
    "text": "if you're using uh UK has something called smart clients so if you're using smart clients they're intelligent about it and they can transparently load",
    "start": "627880",
    "end": "633920"
  },
  {
    "text": "balance or uh move the traffic to another region automatically so so that",
    "start": "633920",
    "end": "639000"
  },
  {
    "text": "gives us fall tolerance in terms of a region failure but this is uh so let's say a region does fail this is not a",
    "start": "639000",
    "end": "645600"
  },
  {
    "text": "great state to uh let's say a region does fail this is not a great state to be in because um even though your",
    "start": "645600",
    "end": "651880"
  },
  {
    "text": "application availability is maintained and your application continue to function another failure in this setup",
    "start": "651880",
    "end": "657360"
  },
  {
    "text": "will cause data unavailability will cause application availability to be affected so so we want to recover from",
    "start": "657360",
    "end": "663920"
  },
  {
    "text": "this setup right um in the previous setup state will set sort of gave us that ability right it will bring up the part if a not fails but like how do we",
    "start": "663920",
    "end": "669920"
  },
  {
    "text": "do that in this kind of setup um and by the way St is representative here this can be done with uh any MCS solution GK",
    "start": "669920",
    "end": "677279"
  },
  {
    "text": "MCS eks MCS so um so so basically you know how do we",
    "start": "677279",
    "end": "684480"
  },
  {
    "text": "recover from these outages right so to recover from these outages what we need to do is first we need to detect as a database that um you know what kind of",
    "start": "684480",
    "end": "691519"
  },
  {
    "text": "an outage is it is it a permanent outage which is a hard problem because you might not have the service accounts permissions or roles to be able to",
    "start": "691519",
    "end": "698360"
  },
  {
    "text": "detect this right um the next thing we need to do once we've detected it and confirmed an outage we have to provision",
    "start": "698360",
    "end": "704160"
  },
  {
    "text": "a new or use a standby kubernetes cluster right and then we have to reconfigure our service mesh to add this",
    "start": "704160",
    "end": "709399"
  },
  {
    "text": "new cluster in um deploy the you DB hem chart again and reconfigure the ydb",
    "start": "709399",
    "end": "714519"
  },
  {
    "text": "cluster to add um to add basically to add the new replica remove the old failed replicas um then wait for data",
    "start": "714519",
    "end": "721600"
  },
  {
    "text": "migration and hope that there is no more failures while this is in progress um that's doesn't sound like fun times to",
    "start": "721600",
    "end": "728320"
  },
  {
    "text": "me especially when you're you know dealing with an outage and in a stressful environment like that so uh",
    "start": "728320",
    "end": "734639"
  },
  {
    "text": "you know uh we can automate these things and have like you know build a run book",
    "start": "734639",
    "end": "740160"
  },
  {
    "text": "but uh every kubernetes cluster in in on-prem or even in the cloud setup is is slightly different right so consistency",
    "start": "740160",
    "end": "747079"
  },
  {
    "text": "consistently executing and expressing these run books is non-trivial and that's where uh multicluster",
    "start": "747079",
    "end": "753760"
  },
  {
    "text": "orchestrator steps in and helps us uh thank you man for the Deep dive",
    "start": "753760",
    "end": "760639"
  },
  {
    "text": "into yabai DB uh we'll now learn about what a multicluster orchestrator is uh to put it very simply it is a control",
    "start": "760639",
    "end": "767560"
  },
  {
    "text": "plane that enables deployment of your kubernetes workloads across a fleet of clusters there are a number of",
    "start": "767560",
    "end": "773360"
  },
  {
    "text": "orchestrators now available in the ecosystem you have kada from Huawei open cluster management and AC from from or",
    "start": "773360",
    "end": "779560"
  },
  {
    "text": "ACM from Red Hat Rancher Fleet you have cubell and KCM which is being uh contributed to by IBM research in red",
    "start": "779560",
    "end": "785920"
  },
  {
    "text": "hat and elal product Nova so uh what is coming into your uh cluster orchestrator",
    "start": "785920",
    "end": "793120"
  },
  {
    "text": "it is your typical set of workload manifest and in addition to that is a schedule policy which is the core",
    "start": "793120",
    "end": "798399"
  },
  {
    "text": "essense of how these orchestrators work we'll look at uh what that consists of",
    "start": "798399",
    "end": "803760"
  },
  {
    "text": "okay uh the scheduled policy does your mapping between your kubernetes resources and the specific cluster that",
    "start": "803760",
    "end": "809240"
  },
  {
    "text": "you want to run it on here's a simplified uh schedule policy it has a resource selector which says here's my",
    "start": "809240",
    "end": "815160"
  },
  {
    "text": "subset of resources that I want to match with this policy and a cluster selector which chooses uh the specific uh",
    "start": "815160",
    "end": "822480"
  },
  {
    "text": "clusters you're interested in uh deploying this workload too there are a number of uh scheduled policy types an",
    "start": "822480",
    "end": "828199"
  },
  {
    "text": "annotation based policy is one in which you just take your workload and add an annotation to it with a cluster ID a",
    "start": "828199",
    "end": "834759"
  },
  {
    "text": "capacity based scheduling policy is uh much more interesting say as a developer I have a CI workload that I want to run",
    "start": "834759",
    "end": "840959"
  },
  {
    "text": "on any one of my Dev clusters that has sufficient resources you'd use a policy like this uh the orchestrator looks at",
    "start": "840959",
    "end": "847839"
  },
  {
    "text": "your pod resource requests and looks at your cluster availability and does the uh mapping we also have the concept of",
    "start": "847839",
    "end": "854600"
  },
  {
    "text": "include and exclude lists uh this would be useful in the case of say I'm ready to move my workloads from staging to",
    "start": "854600",
    "end": "860920"
  },
  {
    "text": "production uh in which case uh typically we uh push all our workloads to all US regions except West which is say our Max",
    "start": "860920",
    "end": "868199"
  },
  {
    "text": "loaded Reg which we push uh 24 hours later you could use an exclude list uh we could also use an exclude list for",
    "start": "868199",
    "end": "874560"
  },
  {
    "text": "certain clusters that are being upgraded or in maintenance mode uh so these come in handy uh now there are certain",
    "start": "874560",
    "end": "881320"
  },
  {
    "text": "Advanced policies that help specifically for yugabyte and stto workloads an example of this is a spread policy this",
    "start": "881320",
    "end": "887920"
  },
  {
    "text": "simply says that take my incoming workload and duplicate it on uh multiple clusters and add overrides this is the",
    "start": "887920",
    "end": "895240"
  },
  {
    "text": "key feature that allows you to modify certain pieces of your manifest uh with uh custom values on each cluster",
    "start": "895240",
    "end": "902279"
  },
  {
    "text": "here's a snippet picked up from an actual uh policy that we're going to show in our demo as you can see it",
    "start": "902279",
    "end": "907680"
  },
  {
    "text": "specifies some spread constraints it uses a duplicate mode uh The Divide mode is an alternate uh mode which we're not",
    "start": "907680",
    "end": "913360"
  },
  {
    "text": "using here we've had uh prospective customers who' want to split a single deployment uh into multiple clusters",
    "start": "913360",
    "end": "920639"
  },
  {
    "text": "through percentage specifications so that's what divide allows us as you can see the particular Ingress Gateway",
    "start": "920639",
    "end": "927040"
  },
  {
    "text": "needed to be overridden with two values that had uh cluster specific values here",
    "start": "927040",
    "end": "932160"
  },
  {
    "text": "our cluster was conveniently named West and so we had to change that value to a West Network and a West cluster for it",
    "start": "932160",
    "end": "937279"
  },
  {
    "text": "to work okay so now yeah a castrator helped me set up yugabyte and sto on my uh",
    "start": "937279",
    "end": "944639"
  },
  {
    "text": "Fleet of clusters why is it involved in Fall tolerance uh the key property of",
    "start": "944639",
    "end": "949800"
  },
  {
    "text": "the orchestrator that allows this is that it has both visibility into your Fleet as well as control it is in the",
    "start": "949800",
    "end": "956040"
  },
  {
    "text": "critical path uh and it has two aspects that it control both your workload and your cluster so this is what makes it",
    "start": "956040",
    "end": "962920"
  },
  {
    "text": "different from say a typical cluster life cycle management tool that you'd be using for uh your crud of your clusters",
    "start": "962920",
    "end": "970240"
  },
  {
    "text": "so this is what allows it to coordinate the set of uh complex recovery steps needed to bring your database back from",
    "start": "970240",
    "end": "977079"
  },
  {
    "text": "a degraded mode of operation into your highly available mode after a failure so summarizing our problem a",
    "start": "977079",
    "end": "985000"
  },
  {
    "text": "cloud native DB like uh yugabyte provides the scale resilience and performance you would need for your uh",
    "start": "985000",
    "end": "991959"
  },
  {
    "text": "Geo distributed applications it does have inherent resilience which makes it essential uh however it handles uh in",
    "start": "991959",
    "end": "999240"
  },
  {
    "text": "within cluster failures such as nodes disk failures and network partitions to be handling when your fault domain",
    "start": "999240",
    "end": "1005319"
  },
  {
    "text": "becomes your region or cluster the orchestrator comes in and provides you with the zero touch fall tolerance uh",
    "start": "1005319",
    "end": "1011040"
  },
  {
    "text": "this is a graphic of the demo we look at right now on top on the Green Block is the orchestrator it has auler which",
    "start": "1011040",
    "end": "1018480"
  },
  {
    "text": "which handles the schedule policy custom resource it has a recovery web hook which can receive alerts from your",
    "start": "1018480",
    "end": "1024918"
  },
  {
    "text": "monitoring stack which could be any on Prem monitoring solution in our case we'll use Google Cloud managed",
    "start": "1024919",
    "end": "1030558"
  },
  {
    "text": "Prometheus uh at the bottom are your uh Fleet of clusters named East West and Central as they are deployed in those",
    "start": "1030559",
    "end": "1037160"
  },
  {
    "text": "regions the blue boxes represent your sto and yaby workloads uh the green",
    "start": "1037160",
    "end": "1042480"
  },
  {
    "text": "agent is what allows your orchestrator to control your uh Fleet on the left the",
    "start": "1042480",
    "end": "1047798"
  },
  {
    "text": "dashed box shows a standby cluster which will pick up workloads and uh do the reconfiguration needed in case of uh",
    "start": "1047799",
    "end": "1055880"
  },
  {
    "text": "failure so uh the recovery web Hook is listening in as soon as it reses",
    "start": "1055880",
    "end": "1061039"
  },
  {
    "text": "receives an alert it sets up your workload it reconfigures it and make sure it is able to communicate with the",
    "start": "1061039",
    "end": "1067400"
  },
  {
    "text": "west and Central clusters that are still up and functioning okay what does recovery",
    "start": "1067400",
    "end": "1073400"
  },
  {
    "text": "involve it requires two steps one is a recovery policy which simply says take my recovery job run it on the East Prime",
    "start": "1073400",
    "end": "1080120"
  },
  {
    "text": "standby cluster it consists of a sequence of steps um which include uh",
    "start": "1080120",
    "end": "1085159"
  },
  {
    "text": "the esto prerequisites deploying stto validating it creating secrets for communication followed by deploying",
    "start": "1085159",
    "end": "1091880"
  },
  {
    "text": "yugabyte followed by uh using the yugabyte administration tool to reconfigure your yaby",
    "start": "1091880",
    "end": "1097640"
  },
  {
    "text": "Universe uh we list them in detail because this is what your infr team would be doing manually uh during an",
    "start": "1097640",
    "end": "1104840"
  },
  {
    "text": "outage and we want to uh take that toil away from them",
    "start": "1104840",
    "end": "1110320"
  },
  {
    "text": "okay since we're going to be uh going through a number of terminals we'll include slides to kind of overview what you're going to see the first step we'll",
    "start": "1114120",
    "end": "1121120"
  },
  {
    "text": "have five uh kubernetes clusters on gke we'll deploy uh the orchestrator as well",
    "start": "1121120",
    "end": "1126559"
  },
  {
    "text": "as your workload clusters we'll deploy esto and yab by DB and uh then see what",
    "start": "1126559",
    "end": "1131840"
  },
  {
    "text": "happens we'll do it a little faster uh this is all the cube configs",
    "start": "1131840",
    "end": "1137240"
  },
  {
    "text": "in my local environment uh this is a GK in which I have five",
    "start": "1137240",
    "end": "1142400"
  },
  {
    "text": "clusters uh I first install the Nova control plane uh takes a few",
    "start": "1142400",
    "end": "1150480"
  },
  {
    "text": "minutes and then we install the agents on the fleet and once we do this we do",
    "start": "1151679",
    "end": "1157080"
  },
  {
    "text": "not talk need to talk to any one of our fleets we just keep talking to the control plane um we then do a coup cutle get",
    "start": "1157080",
    "end": "1164080"
  },
  {
    "text": "clusters cluster is not an inherent uh resource in kubernetes it is what your orchestrator makes available We call we",
    "start": "1164080",
    "end": "1170640"
  },
  {
    "text": "rename those contexts to Central east east Prime and West All in different regions we see that all of them are",
    "start": "1170640",
    "end": "1177440"
  },
  {
    "text": "ready and uh willing to accept workloads we start with deploying ISO",
    "start": "1177440",
    "end": "1182760"
  },
  {
    "text": "the service meesh needs to be deployed first before we can get to our uh DB workload uh this is a script that",
    "start": "1182760",
    "end": "1189240"
  },
  {
    "text": "basically does a sequence of cube cutle apply commands of both the policies and the iso",
    "start": "1189240",
    "end": "1195559"
  },
  {
    "text": "workload uh once we deploy it we check the sto name space uh we ensure that uh the",
    "start": "1197240",
    "end": "1205559"
  },
  {
    "text": "Ingress Gateway the East West Gateway and the esto pods are available we make sure that an external IP has been",
    "start": "1205559",
    "end": "1211360"
  },
  {
    "text": "provided to it by the cloud provider we then install uh some remote",
    "start": "1211360",
    "end": "1217280"
  },
  {
    "text": "Secrets generated by the esto cutle command we double check that these",
    "start": "1217280",
    "end": "1223760"
  },
  {
    "text": "secrets are available West will have secrets titled Central and East so it can talk to its uh YB node",
    "start": "1223760",
    "end": "1231440"
  },
  {
    "text": "Partners I'm going to take a quick peek at the time okay we do have time",
    "start": "1233960",
    "end": "1240840"
  },
  {
    "text": "okay okay we then start deploying yoga by DB uh which is once again a Helm chart that can be targeted at just the",
    "start": "1240840",
    "end": "1247159"
  },
  {
    "text": "uh top level cluster orchestrators API server does not need to talk to your um",
    "start": "1247159",
    "end": "1253720"
  },
  {
    "text": "Fleet okay once yab by DB is deployed once again we check all its services we make sure uh the UI the master ports UI is",
    "start": "1257120",
    "end": "1264760"
  },
  {
    "text": "available we'll then go to the browser and ensure that it is",
    "start": "1264760",
    "end": "1270039"
  },
  {
    "text": "up we check that the T master and the YB server p t server pods that Aman mentioned to us let's pause here a bit",
    "start": "1271880",
    "end": "1279240"
  },
  {
    "text": "on the right hand side is what is most interesting you see three YB notes on the second column you'll see their raft",
    "start": "1279240",
    "end": "1285159"
  },
  {
    "text": "roles there's two followers and a leader and you see that they've all come up their up time is about 2",
    "start": "1285159",
    "end": "1291559"
  },
  {
    "text": "minutes okay uh next we'll set up the recovery steps as we said it's a policy",
    "start": "1295880",
    "end": "1301159"
  },
  {
    "text": "and a job we'll be applying them both uh currently we use a schedule policy where we highlight that it's not being",
    "start": "1301159",
    "end": "1307400"
  },
  {
    "text": "deployed which means it's uh the job is uh p in pending State it's waiting for",
    "start": "1307400",
    "end": "1312480"
  },
  {
    "text": "an alert and uh it'll be deployed as soon as the alert is received",
    "start": "1312480",
    "end": "1320039"
  },
  {
    "text": "we apply the recovery job we check that it's running uh it's available but not running it's at0 bar",
    "start": "1321120",
    "end": "1327559"
  },
  {
    "text": "one we then start an end user workload this is what is going to be running continuous SQL operations uh we keep a",
    "start": "1327559",
    "end": "1335400"
  },
  {
    "text": "lookout on the read and write Ops I'll you know it's about 140 here we'll notice that it does not change we then",
    "start": "1335400",
    "end": "1341760"
  },
  {
    "text": "go into our Google Cloud console we set up a Google uh alert system uh for",
    "start": "1341760",
    "end": "1348480"
  },
  {
    "text": "Simplicity we're using a master pod ready status this is not what you'd be doing in production you would have a",
    "start": "1348480",
    "end": "1354279"
  },
  {
    "text": "complex alert with a number of different application and um system level metrics",
    "start": "1354279",
    "end": "1359960"
  },
  {
    "text": "so we use metric absence as the trigger",
    "start": "1359960",
    "end": "1364399"
  },
  {
    "text": "condition we then set up the notifications this is the key part that closes the loop in addition to sending",
    "start": "1365039",
    "end": "1371080"
  },
  {
    "text": "your admins an email it will talk to the orchestrator uh UI uh endpoint we then",
    "start": "1371080",
    "end": "1378080"
  },
  {
    "text": "inject a failure we'll edit our stateful sets to set the replicas to",
    "start": "1378080",
    "end": "1384440"
  },
  {
    "text": "zero thank you to some of you who are nodding your heads this is making",
    "start": "1386640",
    "end": "1391840"
  },
  {
    "text": "sense okay so the final step is the recovery uh we see that the incidents uh",
    "start": "1392360",
    "end": "1398120"
  },
  {
    "text": "a few incidents get um alert created these incidents uh you're not doing",
    "start": "1398120",
    "end": "1404400"
  },
  {
    "text": "anything the uh orchestrator edits its policy chooses a standby cluster to deploy its workloads you'll notice here",
    "start": "1404400",
    "end": "1411279"
  },
  {
    "text": "that as you can see here East Prime is the standby cluster that's been chosen and we see that the node has come",
    "start": "1411279",
    "end": "1418400"
  },
  {
    "text": "back up the two uh yugabyte Pods at the bottom of the screen the stateful sets are up and running they've been running",
    "start": "1418400",
    "end": "1423919"
  },
  {
    "text": "for about 2",
    "start": "1423919",
    "end": "1426440"
  },
  {
    "text": "minutes and we'll go to the UI we kind of uh refresh it we see that uh the",
    "start": "1429039",
    "end": "1434600"
  },
  {
    "text": "bottommost is the new YB node you'll notice that its up time is about uh 59 seconds the other two have been running",
    "start": "1434600",
    "end": "1441000"
  },
  {
    "text": "for about an hour and 20 minutes uh to make sure everything did go okay we once again go into our end",
    "start": "1441000",
    "end": "1447440"
  },
  {
    "text": "user workloads and uh see",
    "start": "1447440",
    "end": "1454278"
  },
  {
    "text": "that oh there's another check we do we go into yugabyte metrics that are made available we uh Aman would you like to",
    "start": "1455520",
    "end": "1461720"
  },
  {
    "text": "talk about those yeah so um each of the V Masters exposes a metric called follower lag basically that's coming in",
    "start": "1461720",
    "end": "1468399"
  },
  {
    "text": "from the underlying dtb and uh it shows how far a follower is behind the leader",
    "start": "1468399",
    "end": "1473799"
  },
  {
    "text": "in this case it was I think 120 milliseconds behind so so that's a good indicator of that the cluster is healed",
    "start": "1473799",
    "end": "1480520"
  },
  {
    "text": "and um it's ready to accept um like uh workload basically and we can see here",
    "start": "1480520",
    "end": "1485919"
  },
  {
    "text": "all this while while the recovery was going on none of the data availability was lost um you know reads and wres kept",
    "start": "1485919",
    "end": "1491039"
  },
  {
    "text": "continuing the smart client was redirecting traffic transparently to the other replicas that were alive okay uh",
    "start": "1491039",
    "end": "1498120"
  },
  {
    "text": "so yeah we do notice that it's about the op Ops uh performances for read and write continue to remain the same",
    "start": "1498120",
    "end": "1503360"
  },
  {
    "text": "without any issues okay so what are our takeaways uh",
    "start": "1503360",
    "end": "1509200"
  },
  {
    "text": "Cloud native apps are well served by uh databases that are resilient like you go by DB uh uh which have self-healing",
    "start": "1509200",
    "end": "1516120"
  },
  {
    "text": "capabilities to rebuild state in the presence of failures multicluster orchestrators complement this by",
    "start": "1516120",
    "end": "1521720"
  },
  {
    "text": "automating infr level recovery tasks in the presence of region and cluster level failures so what is the benefit for your",
    "start": "1521720",
    "end": "1528120"
  },
  {
    "text": "business you're ensured that business continuity goes on as usual you minimize Revenue loss uh by reducing the toil",
    "start": "1528120",
    "end": "1535080"
  },
  {
    "text": "involved in setting up your clusters your workloads and reconfiguring them all during an expensive outage and",
    "start": "1535080",
    "end": "1541320"
  },
  {
    "text": "another important effect of this is that you able to run periodic fire drills and",
    "start": "1541320",
    "end": "1546520"
  },
  {
    "text": "Chaos tests it is said that an HR policy that is not actually periodically tested is equivalent to having no HR policy so",
    "start": "1546520",
    "end": "1553960"
  },
  {
    "text": "this will warm up your teams to be able to do these tests uh much more regularly as part of your product",
    "start": "1553960",
    "end": "1559919"
  },
  {
    "text": "testing okay uh we have some exciting ways in which we're extending this work we want to avoid the use of standby",
    "start": "1559919",
    "end": "1565320"
  },
  {
    "text": "clusters instead use just in time clusters uh the orchestrator is capable of cloning existing clusters on demand",
    "start": "1565320",
    "end": "1572000"
  },
  {
    "text": "uh secondly some of our prospects have S told us that they want a human in the loop they don't want zero touch they",
    "start": "1572000",
    "end": "1577080"
  },
  {
    "text": "would like some friction and some all okay uh uh um auditing Trail too in their uh fall tolerance triggers uh",
    "start": "1577080",
    "end": "1584760"
  },
  {
    "text": "secondly you notice that all of our recovery was in captured in a uh kubernetes job we would like to make",
    "start": "1584760",
    "end": "1589799"
  },
  {
    "text": "that a CD to make it more flexible and to make it more declarative and generic across different DB use cases uh finally",
    "start": "1589799",
    "end": "1596880"
  },
  {
    "text": "we're also extending our spread capacity scheduled policies to be cost aware and latency aware which will help in these",
    "start": "1596880",
    "end": "1603159"
  },
  {
    "text": "DB environments uh further and uh most importantly thank you to our teams we have maik here uh an",
    "start": "1603159",
    "end": "1610760"
  },
  {
    "text": "experiment that takes me to hours take took him about 200 hours to get all this magic in place so thank you maek and uh",
    "start": "1610760",
    "end": "1617039"
  },
  {
    "text": "rest of team lle and sankit nckl Bin Michael from team yaby so if you want to",
    "start": "1617039",
    "end": "1623960"
  },
  {
    "text": "be involved uh please try out our products uh and come talk to us and if you have other uh day two operations for",
    "start": "1623960",
    "end": "1630399"
  },
  {
    "text": "orchestrators we would love to talk to you and happy to take questions if you want to learn more about yo DB uh we",
    "start": "1630399",
    "end": "1636200"
  },
  {
    "text": "have a booth C1 and also um a concurrent event DSS day going on right now in level four Horizon Ballroom so yeah",
    "start": "1636200",
    "end": "1644520"
  },
  {
    "text": "there's a lot of detailed talks about okay happy to take any",
    "start": "1644520",
    "end": "1650440"
  },
  {
    "text": "questions anybody have questions hey I'm an awesome talk so",
    "start": "1650440",
    "end": "1657200"
  },
  {
    "text": "question on how two-part question so how long does it take for a failed database",
    "start": "1657200",
    "end": "1662679"
  },
  {
    "text": "to come back up in a different Cloud that's part one part two is could you talk to the complexity of uh the",
    "start": "1662679",
    "end": "1671559"
  },
  {
    "text": "recovery Point objective from a database point of view database are pretty complex short version of what you had to",
    "start": "1671559",
    "end": "1676679"
  },
  {
    "text": "do in the database to to make sure that came that it came back up with the right snapshot in place in the uh in the",
    "start": "1676679",
    "end": "1682919"
  },
  {
    "text": "Target uh infrastructure thank you um do you want me to take that sure um so uh",
    "start": "1682919",
    "end": "1689000"
  },
  {
    "text": "from the database uh perspective the like our RPO is uh zero and our RTO is 3",
    "start": "1689000",
    "end": "1695240"
  },
  {
    "text": "seconds U this is because U uh we uh like as soon as a failure is deducted",
    "start": "1695240",
    "end": "1701480"
  },
  {
    "text": "internally in the database it transfers leaders from that fail Zone into another one and basically availability keeps",
    "start": "1701480",
    "end": "1708039"
  },
  {
    "text": "functioning as far as what is needed to get the the new replica up in a single cluster scenario it's I mean straight",
    "start": "1708039",
    "end": "1714559"
  },
  {
    "text": "full side brings it up moves the volume over even in case there is like there was the volume got corrupted or got",
    "start": "1714559",
    "end": "1719600"
  },
  {
    "text": "missing for some reason it's it's it's a very simple actually t- servers automatically recover and rebuild data",
    "start": "1719600",
    "end": "1725399"
  },
  {
    "text": "from state for master because it's a control plane node and it's also responsible for bootstrapping on the others uh we have to run uh one actually",
    "start": "1725399",
    "end": "1733120"
  },
  {
    "text": "just one command to get master back uh to state and it's it U from as as far as the time perspective go it also sort of",
    "start": "1733120",
    "end": "1739519"
  },
  {
    "text": "depends on how much data it needs like if there is existing data from from an existing snapshot if it's in this case",
    "start": "1739519",
    "end": "1745039"
  },
  {
    "text": "building from a completely new scratch snapshot so if it's copying a bunch of data then it takes a little bit longer",
    "start": "1745039",
    "end": "1750760"
  },
  {
    "text": "to be fully functional um and uh and we can like for replication we can drive line rate so whatever is we can drive",
    "start": "1750760",
    "end": "1756600"
  },
  {
    "text": "line rate as in like whatever is the line rate on between the regions and um um in this setup I think it took about",
    "start": "1756600",
    "end": "1763440"
  },
  {
    "text": "like 50 seconds to get the master up and running and everything healed up I think it was 50 seconds or maybe like 2",
    "start": "1763440",
    "end": "1768519"
  },
  {
    "text": "minutes 2 minutes 2 minutes okay yeah",
    "start": "1768519",
    "end": "1774519"
  },
  {
    "text": "yeah thank you I mean the gentleman here asked about RPO that's primarily for",
    "start": "1775159",
    "end": "1781279"
  },
  {
    "text": "high availability I suppose what about uh Disaster Recovery what about backups",
    "start": "1781279",
    "end": "1787200"
  },
  {
    "text": "if you need for example to do point in time recovery how do you address that issue so ugb uh uh has a fully mature",
    "start": "1787200",
    "end": "1797360"
  },
  {
    "text": "sure uh backups in point in time recovery system um uh uh it we use uh we",
    "start": "1797360",
    "end": "1805320"
  },
  {
    "text": "expose it via a our management plane so and also building a kubernetes operator",
    "start": "1805320",
    "end": "1810720"
  },
  {
    "text": "that talks to the management plane to do backups and uh point in time recovery and is that does that answer",
    "start": "1810720",
    "end": "1817519"
  },
  {
    "text": "your question or understand so there is another product",
    "start": "1817519",
    "end": "1824519"
  },
  {
    "text": "uh another product that we have called YY anywhere which is our management plane and backups and point9 Recovery",
    "start": "1824519",
    "end": "1829559"
  },
  {
    "text": "constructs are exposed via that management plane",
    "start": "1829559",
    "end": "1834200"
  },
  {
    "text": "uh yeah so so depending on yeah so depending on how backups are configured so like we support a few options there",
    "start": "1836080",
    "end": "1841880"
  },
  {
    "text": "is something called xcluster which is you can have a remote you go by DB cluster take uh uh basically act as a",
    "start": "1841880",
    "end": "1847480"
  },
  {
    "text": "backup cluster um and then constantly transfer workload between them and this is this supports incremental backups as",
    "start": "1847480",
    "end": "1853000"
  },
  {
    "text": "well you can do offside backups to S3 um this also support incremental and full backups you we have scheduled backups as",
    "start": "1853000",
    "end": "1859000"
  },
  {
    "text": "well so you can do it on a schedule um so because uh a lot of that construct uh for a distrib um like uh because you",
    "start": "1859000",
    "end": "1865559"
  },
  {
    "text": "also um need to um do this for multiple databases right not just one so that's",
    "start": "1865559",
    "end": "1871200"
  },
  {
    "text": "why it's exposed via our uh uh management plane however we're also bringing it uh via to the uh to the",
    "start": "1871200",
    "end": "1879039"
  },
  {
    "text": "communities cluster by building an operator around it yeah okay thank you uh and to add to that answer so the",
    "start": "1879039",
    "end": "1885919"
  },
  {
    "text": "multical orchestrator can also help with uh Dr we actually have a talk later this afternoon at do where we work with",
    "start": "1885919",
    "end": "1892200"
  },
  {
    "text": "perona and uh postgress to uh initiate and Trigger this automated Dr process",
    "start": "1892200",
    "end": "1897880"
  },
  {
    "text": "too so any other questions yeah um thanks",
    "start": "1897880",
    "end": "1903080"
  },
  {
    "text": "silie thanks Aman um I had a question about the recovery job itself so how",
    "start": "1903080",
    "end": "1908279"
  },
  {
    "text": "much context can be passed to that recovery job from the point of failure that's detected meaning that is that",
    "start": "1908279",
    "end": "1914760"
  },
  {
    "text": "recovery job sort of a statically defined thing or can there be input into that to know maybe how to recover from",
    "start": "1914760",
    "end": "1920600"
  },
  {
    "text": "different types of failures um yes it had to be handcrafted with someone who understood the yoga by",
    "start": "1920600",
    "end": "1927320"
  },
  {
    "text": "uh architecture but yes it can totally be customized it is uh you know it is a kubernetes job it's going to be checked",
    "start": "1927320",
    "end": "1933039"
  },
  {
    "text": "into your git repo and it can be customized to your particular use case perfect",
    "start": "1933039",
    "end": "1938919"
  },
  {
    "text": "thanks anybody else have questions",
    "start": "1938960",
    "end": "1945080"
  }
]