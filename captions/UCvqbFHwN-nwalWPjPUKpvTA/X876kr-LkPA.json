[
  {
    "text": "good afternoon everyone my name is marlin and i'm a good group",
    "start": "1280",
    "end": "6799"
  },
  {
    "text": "product manager in google cloud working on kubernetes engine",
    "start": "6799",
    "end": "12400"
  },
  {
    "text": "we all know that gpus are very expensive resource",
    "start": "12400",
    "end": "18640"
  },
  {
    "text": "and utilization of a gpu is a core concern for all the gpu users",
    "start": "18640",
    "end": "25199"
  },
  {
    "text": "poor utilization of the gpus costs them dearly so in this talk",
    "start": "25199",
    "end": "31599"
  },
  {
    "text": "we are going to show you how to improve gpu utilization using",
    "start": "31599",
    "end": "36719"
  },
  {
    "text": "kubernetes so in my humble opinion i believe",
    "start": "36719",
    "end": "42320"
  },
  {
    "text": "kubernetes is an ideal platform for aiml and high performance computing workload",
    "start": "42320",
    "end": "49280"
  },
  {
    "text": "and there are three core reasons why i think kubernetes is best suited for aiml and",
    "start": "49280",
    "end": "56719"
  },
  {
    "text": "high performance computing workloads number one is portability",
    "start": "56719",
    "end": "61840"
  },
  {
    "text": "kubernetes provide open standard based and cloud native apis",
    "start": "61840",
    "end": "68479"
  },
  {
    "text": "this allows the practitioner to seamlessly port workloads between the laptops",
    "start": "68479",
    "end": "74880"
  },
  {
    "text": "private data center and the public cloud second",
    "start": "74880",
    "end": "80320"
  },
  {
    "text": "kubernetes can seamlessly scale from a single node to thousands of nodes",
    "start": "80320",
    "end": "87439"
  },
  {
    "text": "it supports auto scaling auto provisioning gpus tpus and many other",
    "start": "87439",
    "end": "93280"
  },
  {
    "text": "advanced features that allows them to do a very large scale training and inference",
    "start": "93280",
    "end": "100320"
  },
  {
    "text": "third is productivity kubernetes makes the practitioner more proactive by",
    "start": "100320",
    "end": "106240"
  },
  {
    "text": "freeing them up from having to manage the underlying resources and",
    "start": "106240",
    "end": "111360"
  },
  {
    "text": "compatibility issue so they can actually focus on their core business mission be",
    "start": "111360",
    "end": "117040"
  },
  {
    "text": "it be training serving or high performance computing",
    "start": "117040",
    "end": "122560"
  },
  {
    "text": "so let me quickly walk you through an architecture of google kubernetes engine",
    "start": "123759",
    "end": "129039"
  },
  {
    "text": "google kubernetes engine is a fully managed container orchestration platform",
    "start": "129039",
    "end": "134160"
  },
  {
    "text": "provided by google it has two main components control plane",
    "start": "134160",
    "end": "140319"
  },
  {
    "text": "and data plane control plane comprises of many things including master nodes",
    "start": "140319",
    "end": "146319"
  },
  {
    "text": "api server scheduler hcd and many other services",
    "start": "146319",
    "end": "151760"
  },
  {
    "text": "so control plane provisions the data plane which comprises of worker nodes",
    "start": "151760",
    "end": "158080"
  },
  {
    "text": "worker node is the place where workloads run and worker nodes can run the",
    "start": "158080",
    "end": "163599"
  },
  {
    "text": "workloads using gpus and tpus worker nodes are grouped together",
    "start": "163599",
    "end": "170560"
  },
  {
    "text": "as a node pool all the nodes that belong to a single node pool",
    "start": "170560",
    "end": "176400"
  },
  {
    "text": "share the configuration node pool is also the basic unit of auto",
    "start": "176400",
    "end": "181519"
  },
  {
    "text": "scaling all the nodes in the node pool either have a cpu or gpu to run the workload",
    "start": "181519",
    "end": "190640"
  },
  {
    "text": "so as i mentioned before gpu utilization is a core concern for",
    "start": "190879",
    "end": "197519"
  },
  {
    "text": "gpu users and poor utilization cost them very very dearly",
    "start": "197519",
    "end": "202879"
  },
  {
    "text": "what we have observed in our gk fleet is that gpu utilization for typical",
    "start": "202879",
    "end": "208159"
  },
  {
    "text": "workload is quite low and the gpu utilization is actually getting worse day by day",
    "start": "208159",
    "end": "214720"
  },
  {
    "text": "as gpus are getting more and more powerful a single workload may not even be able",
    "start": "214720",
    "end": "220560"
  },
  {
    "text": "to saturate a very powerful gpu under utilization problem is especially",
    "start": "220560",
    "end": "227200"
  },
  {
    "text": "acute for certain type of workloads such as inference gaming visualization",
    "start": "227200",
    "end": "233840"
  },
  {
    "text": "and notebooks so let's look at some of the examples so data scientists build model",
    "start": "233840",
    "end": "241680"
  },
  {
    "text": "using the notebooks and most of the notebooks today are attached to gpus and as we all know",
    "start": "241680",
    "end": "249120"
  },
  {
    "text": "notebooks stay idle for a prolonged period of time wasting very very expensive resource",
    "start": "249120",
    "end": "255519"
  },
  {
    "text": "let's look at some more examples like chat box vision product search and product recommendation",
    "start": "255519",
    "end": "262079"
  },
  {
    "text": "these are all real-time applications that are latency sensitive and business",
    "start": "262079",
    "end": "267600"
  },
  {
    "text": "critical so kubernetes auto scaling and auto producing features are essential for",
    "start": "267600",
    "end": "274720"
  },
  {
    "text": "such application but not sufficient for two reasons one",
    "start": "274720",
    "end": "280080"
  },
  {
    "text": "is it takes minutes to spin up a new node in the kubernetes",
    "start": "280080",
    "end": "285199"
  },
  {
    "text": "and most of these applications are latency sensitive so they cannot tolerate that delay",
    "start": "285199",
    "end": "290960"
  },
  {
    "text": "also until now we could not do a very effective bin packing for gpu workloads",
    "start": "290960",
    "end": "297520"
  },
  {
    "text": "so how do we help this workloads be more cost efficient that's the main purpose of today's talk",
    "start": "297520",
    "end": "304160"
  },
  {
    "text": "so the main challenge today we face is that kubernetes allows fractional",
    "start": "304160",
    "end": "309440"
  },
  {
    "text": "utilization of cpus but it does not allow fractional utilization of gpus",
    "start": "309440",
    "end": "315600"
  },
  {
    "text": "what it means is that a kubernetes workload can ask for 0.5 virtual cpu and kubernetes knows how to",
    "start": "315600",
    "end": "323120"
  },
  {
    "text": "give the workload 0.5 cpus but today you cannot ask 0.5 gpus in kubernetes",
    "start": "323120",
    "end": "330639"
  },
  {
    "text": "so what happens is one workload one gpu is fully allocated to one workload even though workload",
    "start": "330639",
    "end": "337680"
  },
  {
    "text": "needs a fraction of the gpu to execute its task so how do we fix this",
    "start": "337680",
    "end": "343759"
  },
  {
    "text": "so there are many solutions to allow workloads to share a gpu",
    "start": "343759",
    "end": "349039"
  },
  {
    "text": "some solutions work the application level some solutions work at the gpu system and software level in some work",
    "start": "349039",
    "end": "356160"
  },
  {
    "text": "at the hardware layer so today i'm going to talk about two solutions that we recently launched",
    "start": "356160",
    "end": "362639"
  },
  {
    "text": "first one is time sharing and the second one is multi-instance gpu",
    "start": "362639",
    "end": "367840"
  },
  {
    "text": "and both of them are very popular mechanisms to share gpus",
    "start": "367840",
    "end": "372960"
  },
  {
    "text": "and both of them together address most of the use cases and most workload needs",
    "start": "372960",
    "end": "378639"
  },
  {
    "text": "so let's walk them through some one by one so first i'm going to explain the",
    "start": "378639",
    "end": "384720"
  },
  {
    "text": "temporal multiplexing solution popularly known as time sharing so time sharing allows multiple",
    "start": "384720",
    "end": "392240"
  },
  {
    "text": "container to run on a single gpu each container gets a time slice",
    "start": "392240",
    "end": "400319"
  },
  {
    "text": "so gpus are allocated fairly to all the containers",
    "start": "400319",
    "end": "405520"
  },
  {
    "text": "and under the hood it it does time sharing through contact",
    "start": "405520",
    "end": "411120"
  },
  {
    "text": "switching what it does is that at any given point on time",
    "start": "411120",
    "end": "416319"
  },
  {
    "text": "one container has exclusive use of a gpu but at a periodic time interval it does",
    "start": "416319",
    "end": "423440"
  },
  {
    "text": "a context switching and the next container gets the exclusively used the gpus",
    "start": "423440",
    "end": "428960"
  },
  {
    "text": "and this is done in a round robin fashion so all the container gets a certain fair time slice that they",
    "start": "428960",
    "end": "435919"
  },
  {
    "text": "deserve so the beauty of this solution is that",
    "start": "435919",
    "end": "441039"
  },
  {
    "text": "when there is only one container allocated to a gpu it gets to use the",
    "start": "441039",
    "end": "446800"
  },
  {
    "text": "entire compute time of the gpu but as soon as you add the second container",
    "start": "446800",
    "end": "453440"
  },
  {
    "text": "now two containers share the gpu so each one gets about half the time compute",
    "start": "453440",
    "end": "460000"
  },
  {
    "text": "time and that's how you can enable a fair sharing of gpus",
    "start": "460000",
    "end": "466160"
  },
  {
    "text": "so for nvidia gpus series the older generations",
    "start": "466160",
    "end": "471680"
  },
  {
    "text": "they were doing context switching or preemption at the cuda current boundary",
    "start": "471680",
    "end": "476800"
  },
  {
    "text": "however the new generation of nvidia gpus pascal and later they do preemptions and contact",
    "start": "476800",
    "end": "483919"
  },
  {
    "text": "switching at the instruction level boundaries and that basically facilitates fair sharing",
    "start": "483919",
    "end": "490560"
  },
  {
    "text": "of gpu and the solution that i'm going to explain today is fully managed solution",
    "start": "490560",
    "end": "497280"
  },
  {
    "text": "by gke so all the configuration and management and underlying heavy lifting is done by gke",
    "start": "497280",
    "end": "506680"
  },
  {
    "text": "so now i have already explained what time sharing means now i'm going to explain to you",
    "start": "507280",
    "end": "514240"
  },
  {
    "text": "how we can enable time sharing on a g key what the user experience looks like",
    "start": "514240",
    "end": "520560"
  },
  {
    "text": "so in order to create a gpu node where the time sharing is enabled",
    "start": "520560",
    "end": "526640"
  },
  {
    "text": "you can provide a configuration parameter either at the cluster create time",
    "start": "526640",
    "end": "532880"
  },
  {
    "text": "or at the node will create time and in this chart you can see an example like",
    "start": "532880",
    "end": "538560"
  },
  {
    "text": "it has a configuration parameter called max time share clients per gpu and the value is set to 10.",
    "start": "538560",
    "end": "544959"
  },
  {
    "text": "what it means is that in this example nvidia tesla t4 gpu can",
    "start": "544959",
    "end": "550640"
  },
  {
    "text": "be shared by up to 10 containers so 10 is the upper limit",
    "start": "550640",
    "end": "556320"
  },
  {
    "text": "between 1 and 10 containers can use this gpu all this configuration you can do it",
    "start": "556320",
    "end": "562560"
  },
  {
    "text": "with api calls we are also going to launch user interface so you will be able to do the same configuration using",
    "start": "562560",
    "end": "568720"
  },
  {
    "text": "ui ux so let's say you run the command either the",
    "start": "568720",
    "end": "575360"
  },
  {
    "text": "cluster create or note will create it will automatically set up the nodes",
    "start": "575360",
    "end": "581040"
  },
  {
    "text": "with the time sharing configuration so you can after the nodes are created and drivers are installed",
    "start": "581040",
    "end": "588000"
  },
  {
    "text": "you can actually inspect a node so let's inspect the node by running group code describe nodes",
    "start": "588000",
    "end": "595440"
  },
  {
    "text": "so you can see here that the nvidia.com gpu resource value is actually set to",
    "start": "595440",
    "end": "601920"
  },
  {
    "text": "10. so what does that mean it means 10",
    "start": "601920",
    "end": "607120"
  },
  {
    "text": "shared gpu resources are available and each resource represents a time",
    "start": "607120",
    "end": "613760"
  },
  {
    "text": "slice so in a simple speak up to 10 container",
    "start": "613760",
    "end": "618800"
  },
  {
    "text": "can share this one nvidia t4 gpus",
    "start": "618800",
    "end": "624160"
  },
  {
    "text": "so after the nodes are created the next thing it does is basically it labels the",
    "start": "625360",
    "end": "631440"
  },
  {
    "text": "node so with each node that is created through this configuration",
    "start": "631440",
    "end": "637760"
  },
  {
    "text": "will be labeled so that workloads which request this timeshare gpu can be landed",
    "start": "637760",
    "end": "644880"
  },
  {
    "text": "on this particular node so in this example you can see that two parameters are specified one is time",
    "start": "644880",
    "end": "651440"
  },
  {
    "text": "sharing and the maximum number of containers that can share this gpu",
    "start": "651440",
    "end": "656640"
  },
  {
    "text": "third thing it does is basically change this node why is this needed this is needed to",
    "start": "656640",
    "end": "662800"
  },
  {
    "text": "avoid or prevent whole gpu workloads from being scheduled on this node",
    "start": "662800",
    "end": "670000"
  },
  {
    "text": "you only want the workloads that want a sharable gpu to land here you don't want",
    "start": "670000",
    "end": "675680"
  },
  {
    "text": "a workload that needs a full gpu to land on this particular node",
    "start": "675680",
    "end": "681600"
  },
  {
    "text": "so now we have set up the nodes next thing is to basically configure the workload so what do we do we",
    "start": "682320",
    "end": "688959"
  },
  {
    "text": "specify the deployment so",
    "start": "688959",
    "end": "694160"
  },
  {
    "text": "within the deployment spec you can actually specify node selectors",
    "start": "694160",
    "end": "699200"
  },
  {
    "text": "or affinities to schedule the workload to run on a time share gpu",
    "start": "699200",
    "end": "706800"
  },
  {
    "text": "if the nodes don't exist then gk is smart enough to either auto scale",
    "start": "706880",
    "end": "713920"
  },
  {
    "text": "an existing node pool which matches the configuration or it can create a brand new node pool",
    "start": "713920",
    "end": "721200"
  },
  {
    "text": "that matches the request from the workload and i'm going to talk more about that as throughout this",
    "start": "721200",
    "end": "728839"
  },
  {
    "text": "talk so now the workloads have to request",
    "start": "728839",
    "end": "735079"
  },
  {
    "text": "nvidia.com gpu and the request count in this example is",
    "start": "735079",
    "end": "742839"
  },
  {
    "text": "one you have to remember here is that one is not the measure of gpu time",
    "start": "742839",
    "end": "749200"
  },
  {
    "text": "allocated to a particular container how much time a gpu how much time a container gets depends",
    "start": "749200",
    "end": "756320"
  },
  {
    "text": "on how many containers are running on the gpu so if there are 10 containers",
    "start": "756320",
    "end": "761360"
  },
  {
    "text": "each will get 1 10 of the time if there are only one container that will get the exclusively used it gets to",
    "start": "761360",
    "end": "768399"
  },
  {
    "text": "exclusively use the gpu so now we have",
    "start": "768399",
    "end": "774639"
  },
  {
    "text": "figure out how to provision a node how to land workloads on those time shared",
    "start": "774639",
    "end": "780160"
  },
  {
    "text": "nodes but there are some nuances that we need to be familiar with and there are some",
    "start": "780160",
    "end": "785360"
  },
  {
    "text": "issues and corner cases that we have to be mindful so in the time sharing gpus all the",
    "start": "785360",
    "end": "792480"
  },
  {
    "text": "processes get their separate address space so there is no issue of data overlapping",
    "start": "792480",
    "end": "799040"
  },
  {
    "text": "however no memory limits are enforced what it means is that if the containers",
    "start": "799040",
    "end": "804560"
  },
  {
    "text": "are not well behaved then you can get into out of memory situations",
    "start": "804560",
    "end": "810079"
  },
  {
    "text": "so the responsibility of restricting memory usage is up to each workload so",
    "start": "810079",
    "end": "815680"
  },
  {
    "text": "how can we do this so two ways in which you can avoid out of memory situation",
    "start": "815680",
    "end": "822399"
  },
  {
    "text": "the first one is you can actually use cuda unified memory what it does it basically enables",
    "start": "822399",
    "end": "829680"
  },
  {
    "text": "on-demand paging between host and gpu memory so that way",
    "start": "829680",
    "end": "834800"
  },
  {
    "text": "it avoid out-of-memory situation the second solution is that you can configure this in the applications so",
    "start": "834800",
    "end": "842160"
  },
  {
    "text": "application frameworks like tensorflow or pytorch they expose you knobs which",
    "start": "842160",
    "end": "847839"
  },
  {
    "text": "you can set to avoid out of memory situations so this is something to remember to",
    "start": "847839",
    "end": "853920"
  },
  {
    "text": "avoid when you are sharing too many too many containers on the same gpu",
    "start": "853920",
    "end": "859920"
  },
  {
    "text": "you want to avoid out of memory situations so now i'm going to walk you through how",
    "start": "859920",
    "end": "866880"
  },
  {
    "text": "auto scaling works when you have a time share gpu so auto scaling is a very key feature of",
    "start": "866880",
    "end": "873199"
  },
  {
    "text": "kubernetes it enables workloads to avoid over provisioning and under",
    "start": "873199",
    "end": "880480"
  },
  {
    "text": "provisioning situations thereby saving the cost while offering a better",
    "start": "880480",
    "end": "885519"
  },
  {
    "text": "performance so auto scaling is quite a complex topic so i'm going to walk you through a very",
    "start": "885519",
    "end": "892240"
  },
  {
    "text": "simple workflow so in this case we already have a timeshare gpu node",
    "start": "892240",
    "end": "899279"
  },
  {
    "text": "now this node will expose a gpu utilization metric per container",
    "start": "899279",
    "end": "906880"
  },
  {
    "text": "and you can actually also use custom matrix so if you wanted to specify query per container that could be your custom",
    "start": "906880",
    "end": "913519"
  },
  {
    "text": "matrix so the horizontal part or scalar auto scaler actually watches for this metric",
    "start": "913519",
    "end": "920160"
  },
  {
    "text": "it actually tracks this metric and what it does is that when this metric exceeds the threshold",
    "start": "920160",
    "end": "926480"
  },
  {
    "text": "that you specify it will actually add replicas of your container",
    "start": "926480",
    "end": "932160"
  },
  {
    "text": "so let's say you are watching for um gpu utilization metric and the threshold is",
    "start": "932160",
    "end": "937519"
  },
  {
    "text": "70 when the utilization goes more than 71 percent or higher right it will start",
    "start": "937519",
    "end": "944399"
  },
  {
    "text": "adding replicas because it thinks that the application is running hot and it",
    "start": "944399",
    "end": "950240"
  },
  {
    "text": "needs some help so when it does this it can do it in couple of ways now we",
    "start": "950240",
    "end": "956480"
  },
  {
    "text": "have added more replicas of the pod pod needs to land on a node if there was",
    "start": "956480",
    "end": "961920"
  },
  {
    "text": "an existing node which can accommodate that it will happen but if there was no such node available to land this extra",
    "start": "961920",
    "end": "969120"
  },
  {
    "text": "pod then it will automatically add new nodes in your cluster",
    "start": "969120",
    "end": "976160"
  },
  {
    "text": "so cluster autoscaler is smart and it will take care of this for you",
    "start": "977360",
    "end": "983279"
  },
  {
    "text": "now there are three main scenarios when we talk about auto scaling",
    "start": "983279",
    "end": "988480"
  },
  {
    "text": "scale up scale down and auto provisioning so i'm going to quickly walk you through all the three",
    "start": "988480",
    "end": "996279"
  },
  {
    "text": "so when the nodes are unscheduleable then gk is smart enough",
    "start": "996399",
    "end": "1002560"
  },
  {
    "text": "to scale up the most cost effective node pool this is to your advantage",
    "start": "1002560",
    "end": "1008399"
  },
  {
    "text": "how does it do that it basically looks at the parts back and it looks at the nodes back and sees which are the nodes",
    "start": "1008399",
    "end": "1016480"
  },
  {
    "text": "that can satisfy this parts back and out of those nodes which one will be the most cost effective to scale up so if",
    "start": "1016480",
    "end": "1024558"
  },
  {
    "text": "there are too many parts waiting to be scheduled then it's also smart enough to figure out",
    "start": "1024559",
    "end": "1030079"
  },
  {
    "text": "how many nodes it should add whether it should be adding one node or five node or ten node to address or service",
    "start": "1030079",
    "end": "1036959"
  },
  {
    "text": "all this outstanding port you can also ask what happens if there",
    "start": "1036959",
    "end": "1042959"
  },
  {
    "text": "was no existing node and you are starting from scratch there is no node pool that is running on the cluster in",
    "start": "1042959",
    "end": "1048960"
  },
  {
    "text": "this case also gk is very sophisticated and smart to automatically provision the",
    "start": "1048960",
    "end": "1055039"
  },
  {
    "text": "node that will satisfy the needs of the workload so we call this auto provisioning",
    "start": "1055039",
    "end": "1063280"
  },
  {
    "text": "so let's say you're running your cluster and suddenly the load drops",
    "start": "1065280",
    "end": "1070559"
  },
  {
    "text": "then gk is smart enough to scale it down and the way it does is by monitoring the",
    "start": "1070559",
    "end": "1076320"
  },
  {
    "text": "utilization of all the nodes in the cluster when the utilization drops below",
    "start": "1076320",
    "end": "1081440"
  },
  {
    "text": "a threshold then what it will do is it will try to figure out if all the workloads",
    "start": "1081440",
    "end": "1088400"
  },
  {
    "text": "that are running on underutilized node whether they can be consolidated in",
    "start": "1088400",
    "end": "1094640"
  },
  {
    "text": "fewer number of nodes safely so if the answer is true then it will",
    "start": "1094640",
    "end": "1100240"
  },
  {
    "text": "basically move the parts from underutilized nodes into fewer number of",
    "start": "1100240",
    "end": "1106160"
  },
  {
    "text": "nodes and free up the extra resources this will save you money by reducing the",
    "start": "1106160",
    "end": "1112080"
  },
  {
    "text": "number of nodes needed to handle the workload",
    "start": "1112080",
    "end": "1117600"
  },
  {
    "text": "now we talked about auto scaling and we talked about in the context of",
    "start": "1119039",
    "end": "1125520"
  },
  {
    "text": "you already have an existing node or node poll and you scale it up when you have",
    "start": "1125520",
    "end": "1130720"
  },
  {
    "text": "appending parts what happens if there is no existing node poll you want to bootstrap from",
    "start": "1130720",
    "end": "1136880"
  },
  {
    "text": "scratch in that case if you enable auto provisioning on gke then it will",
    "start": "1136880",
    "end": "1142799"
  },
  {
    "text": "automatically figure out what is the best node configuration and node pool",
    "start": "1142799",
    "end": "1148080"
  },
  {
    "text": "configuration that it can bootstrap the workload so it will automatically add",
    "start": "1148080",
    "end": "1153679"
  },
  {
    "text": "those nodes from zero nodes so that's called auto provisioning this",
    "start": "1153679",
    "end": "1158880"
  },
  {
    "text": "basically saves you time and effort of configuring the node poles so on the right hand side here is an",
    "start": "1158880",
    "end": "1165520"
  },
  {
    "text": "example let's say you had enabled auto provisioning",
    "start": "1165520",
    "end": "1171679"
  },
  {
    "text": "and you're just starting your cluster there are no node pools so based on this deployment spec it",
    "start": "1171679",
    "end": "1177600"
  },
  {
    "text": "actually knows that you have enabled time sharing and it can actually figure out it needs to add a time share node to",
    "start": "1177600",
    "end": "1185360"
  },
  {
    "text": "the cluster and not node auto provisioning will automatically do that for you so it saves you effort",
    "start": "1185360",
    "end": "1193840"
  },
  {
    "text": "so we talked about time sharing and in the beginning i said we are going to talk about two distinct mechanisms for",
    "start": "1194000",
    "end": "1200320"
  },
  {
    "text": "gpu sharing so now i'm going to switch the gears and talk about special multiplexing",
    "start": "1200320",
    "end": "1206559"
  },
  {
    "text": "so this is a relatively new technology that was launched by nvidia it is known as multi-instance gpus",
    "start": "1206559",
    "end": "1213679"
  },
  {
    "text": "it basically allows multi-instance gpu-enabled gpus to partition into gpu instances",
    "start": "1213679",
    "end": "1223039"
  },
  {
    "text": "and the key difference here is that partitions are physically isolated",
    "start": "1223039",
    "end": "1228960"
  },
  {
    "text": "with dedicated compute and memory so this physical isolation that's why it's",
    "start": "1228960",
    "end": "1234080"
  },
  {
    "text": "called spatial multiplexing in the previous case there was a temporal multiplexing you're just time slicing",
    "start": "1234080",
    "end": "1240799"
  },
  {
    "text": "single gpu across multiple containers so in this case it supports simultaneous",
    "start": "1240799",
    "end": "1246960"
  },
  {
    "text": "workload execution with guarantee of service so now you have physical partition so you",
    "start": "1246960",
    "end": "1252880"
  },
  {
    "text": "can actually run those containers in parallel where all of them are executing",
    "start": "1252880",
    "end": "1258080"
  },
  {
    "text": "at the same time and that gives you a better quality of service this is only supported on a100 gpus as",
    "start": "1258080",
    "end": "1265039"
  },
  {
    "text": "of now and we have done a lot of testing on this and we have found that throughput",
    "start": "1265039",
    "end": "1270720"
  },
  {
    "text": "increases linearly when you add more instances which makes logical sense",
    "start": "1270720",
    "end": "1277840"
  },
  {
    "text": "so in the a100 gpu case there are seven compute units and eight memory units",
    "start": "1278640",
    "end": "1285600"
  },
  {
    "text": "each unit of memory is about 5 gb this",
    "start": "1285600",
    "end": "1290640"
  },
  {
    "text": "compute and memory units you can combine in different configuration to slice",
    "start": "1290640",
    "end": "1296400"
  },
  {
    "text": "the a100 gpu in a different instances and this table actually shows",
    "start": "1296400",
    "end": "1302880"
  },
  {
    "text": "what combination a legit so each combination is basically tagged as",
    "start": "1302880",
    "end": "1310080"
  },
  {
    "text": "a compute g dot memory gb what it implies let's take an example",
    "start": "1310080",
    "end": "1315280"
  },
  {
    "text": "here when we say 1g.5 gb it implies one compute in it",
    "start": "1315280",
    "end": "1320880"
  },
  {
    "text": "and 5 gb of memory and when you specify that you can create seven instances with",
    "start": "1320880",
    "end": "1327520"
  },
  {
    "text": "this particular configuration if you picked a different configuration like 3g dot 20 gb you will get two",
    "start": "1327520",
    "end": "1335360"
  },
  {
    "text": "instances so when you get seven instances you can run seven containers on this particular",
    "start": "1335360",
    "end": "1341600"
  },
  {
    "text": "gpu if you have two instances you run two containers in parallel on this particular gpu",
    "start": "1341600",
    "end": "1349360"
  },
  {
    "text": "so similar to time sharing you can configure this gpu with however many",
    "start": "1349679",
    "end": "1355120"
  },
  {
    "text": "partitions that are listed on the previous table and in order to do that you have to",
    "start": "1355120",
    "end": "1360240"
  },
  {
    "text": "specify this particular parameter called gpu partition size in this example i picked 1g.5 gb and as we saw in the",
    "start": "1360240",
    "end": "1368400"
  },
  {
    "text": "previous table this creates seven instances",
    "start": "1368400",
    "end": "1373440"
  },
  {
    "text": "so this can run up to seven container in parallel",
    "start": "1373440",
    "end": "1379039"
  },
  {
    "text": "so when you inspect those nodes you will see nvidia.com gpu resource with a resource",
    "start": "1379679",
    "end": "1386159"
  },
  {
    "text": "count of seven so this is a particular example you can slice it differently",
    "start": "1386159",
    "end": "1391280"
  },
  {
    "text": "depending on your needs now how do we deploy workloads on those",
    "start": "1391280",
    "end": "1396320"
  },
  {
    "text": "nodes very similar to time sharing you will have a deployment spec first thing you will notice is that",
    "start": "1396320",
    "end": "1403280"
  },
  {
    "text": "there is a resource count in this case you request one previously we talked about the nodes are",
    "start": "1403280",
    "end": "1410159"
  },
  {
    "text": "already labeled with the kind of sharing solution that the nodes are configured",
    "start": "1410159",
    "end": "1416080"
  },
  {
    "text": "with so with the combination of node selectors",
    "start": "1416080",
    "end": "1422240"
  },
  {
    "text": "the scheduler can figure out which workloads can land on a which",
    "start": "1422960",
    "end": "1428000"
  },
  {
    "text": "slice of which gpu so you",
    "start": "1428000",
    "end": "1433360"
  },
  {
    "text": "in this case you can see the replica count is seven because there are seven instances so you can run up to seven",
    "start": "1433360",
    "end": "1440400"
  },
  {
    "text": "containers on a single a100 gpu which is partitioned into seven instances",
    "start": "1440400",
    "end": "1448080"
  },
  {
    "text": "so now let's compare and contrast the two mechanisms so you actually understand when to use mig and when to",
    "start": "1449279",
    "end": "1456159"
  },
  {
    "text": "use time sharing so as i mentioned before in the case of mig",
    "start": "1456159",
    "end": "1463279"
  },
  {
    "text": "the partitions are physical in the case of time sharing partitions",
    "start": "1463279",
    "end": "1468559"
  },
  {
    "text": "are logical so when you have a physical partition it will have a max partition limit",
    "start": "1468559",
    "end": "1474640"
  },
  {
    "text": "which is by design a hardware limitation so a100 you can only partition in to max",
    "start": "1474640",
    "end": "1480720"
  },
  {
    "text": "seven instances in the logical case you can partition a",
    "start": "1480720",
    "end": "1485760"
  },
  {
    "text": "gpu as many ways you want like you can load too many containers on a single gpu but",
    "start": "1485760",
    "end": "1491840"
  },
  {
    "text": "i will caution you that if you will if you added too many containers on the same gpu to be shared",
    "start": "1491840",
    "end": "1499200"
  },
  {
    "text": "then you have to watch out for the overhead of context switching so",
    "start": "1499200",
    "end": "1504720"
  },
  {
    "text": "be careful how what how many containers you want to share a single gpu with",
    "start": "1504720",
    "end": "1510720"
  },
  {
    "text": "so in the case of mig by virtue of physical partitioning it provides a lot of benefits for",
    "start": "1510720",
    "end": "1517919"
  },
  {
    "text": "example it provides a physical isolation and in many applications isolation is an",
    "start": "1517919",
    "end": "1523279"
  },
  {
    "text": "important requirement it also provides memory protection again that avoids out-of-memory kind of",
    "start": "1523279",
    "end": "1530000"
  },
  {
    "text": "situation so very beneficial and provides the quality of service guarantees so clearly when you're",
    "start": "1530000",
    "end": "1537120"
  },
  {
    "text": "looking for quality of service or better isolation mig is a better choice",
    "start": "1537120",
    "end": "1543279"
  },
  {
    "text": "none of this is possible in a time setting sharing scenario because you're just sharing a single physical gpu",
    "start": "1543279",
    "end": "1549279"
  },
  {
    "text": "across many containers because migs are physical partitions",
    "start": "1549279",
    "end": "1555520"
  },
  {
    "text": "the reconfiguration of a make gpu if you wanted to change partition requires a",
    "start": "1555520",
    "end": "1560720"
  },
  {
    "text": "little bit of effort in the case of time sharing reconfiguration is very easy so if you",
    "start": "1560720",
    "end": "1566640"
  },
  {
    "text": "specify like you wanted to share a single physical gpu with 10 containers",
    "start": "1566640",
    "end": "1571760"
  },
  {
    "text": "in the case of time sharing tomorrow you decide no i only want to share it with five containers you change one parameter",
    "start": "1571760",
    "end": "1578080"
  },
  {
    "text": "and then you are done so reconfiguration is quite easy in the case of time sharing",
    "start": "1578080",
    "end": "1585120"
  },
  {
    "text": "so when do you choose choose what as i mentioned if quality of service or isolation or",
    "start": "1585120",
    "end": "1592799"
  },
  {
    "text": "prevention from out of memory are your main criteria then certainly mig will be",
    "start": "1592799",
    "end": "1599520"
  },
  {
    "text": "quite beneficial because it provides those guarantees on the other hand what we have found in",
    "start": "1599520",
    "end": "1605279"
  },
  {
    "text": "the practice is that time sharing is very good for a bursty",
    "start": "1605279",
    "end": "1610840"
  },
  {
    "text": "workload so the benefit here is that let's say you specify a gpu to be time",
    "start": "1610840",
    "end": "1617120"
  },
  {
    "text": "share across 10 containers but you only have one container to begin with it will get the full power of the gpu if you are",
    "start": "1617120",
    "end": "1624559"
  },
  {
    "text": "two containers you will still together will get the full power of gpu on the other hand if you are working",
    "start": "1624559",
    "end": "1630559"
  },
  {
    "text": "with mig you specify seven slices but you only use one container to run on",
    "start": "1630559",
    "end": "1636799"
  },
  {
    "text": "that those many seven slices then you are only getting one seventh of the performance because other six slices are",
    "start": "1636799",
    "end": "1643520"
  },
  {
    "text": "going to stay idle so that's the trade-off so when you have a bursary traffic",
    "start": "1643520",
    "end": "1648720"
  },
  {
    "text": "you can use time sharing and you're going to get much better utilization of the gpu and much more flexibility",
    "start": "1648720",
    "end": "1655440"
  },
  {
    "text": "the other benefit of time sharing is that it's actually works on all the gpu families we have including a100",
    "start": "1655440",
    "end": "1662000"
  },
  {
    "text": "including mig partitions versus mic only works on a100 so",
    "start": "1662000",
    "end": "1668640"
  },
  {
    "text": "you don't have that flexibility in every single families of gpu",
    "start": "1668640",
    "end": "1674320"
  },
  {
    "text": "there are things to consider beyond the time sharing versus make",
    "start": "1674559",
    "end": "1681039"
  },
  {
    "text": "so we recommend that you do gpu sharing only within a single trust boundary",
    "start": "1681039",
    "end": "1688080"
  },
  {
    "text": "so what that means is that if you have a scenario where a single user needs to run",
    "start": "1688080",
    "end": "1694640"
  },
  {
    "text": "multiple applications it's totally legit and okay to do gpu sharing because you are working",
    "start": "1694640",
    "end": "1701039"
  },
  {
    "text": "with a single trust boundary similarly if you have a single company or single tenant but multiple users an",
    "start": "1701039",
    "end": "1707600"
  },
  {
    "text": "example would be like multiple data scientists running notebooks and those notes books want to share a gpu",
    "start": "1707600",
    "end": "1713679"
  },
  {
    "text": "that should be okay again we are within the same trust boundary however we don't recommend these",
    "start": "1713679",
    "end": "1720159"
  },
  {
    "text": "solutions in a multi-tenant scenario so if you have multiple customers where",
    "start": "1720159",
    "end": "1726159"
  },
  {
    "text": "you have to cross the trust boundary we do not recommend that because we don't",
    "start": "1726159",
    "end": "1731919"
  },
  {
    "text": "think the isolation properties of any of the solutions are",
    "start": "1731919",
    "end": "1737279"
  },
  {
    "text": "meeting the security bar to allow sharing across the customers so please",
    "start": "1737279",
    "end": "1742720"
  },
  {
    "text": "keep in mind within a single customer totally okay to share across the customers is not advisable at the",
    "start": "1742720",
    "end": "1749679"
  },
  {
    "text": "current state of technology so in summary the key takeaways from",
    "start": "1749679",
    "end": "1757679"
  },
  {
    "text": "this discussion is that we offer two solutions for gpu sharing on kubernetes",
    "start": "1757679",
    "end": "1764640"
  },
  {
    "text": "the time sharing solution works in every single gpu family and offers better solution for bursary",
    "start": "1764640",
    "end": "1771120"
  },
  {
    "text": "workloads mig only works on a 100 gpu but it does",
    "start": "1771120",
    "end": "1776159"
  },
  {
    "text": "provide better isolation quality of service and out of memory protection",
    "start": "1776159",
    "end": "1783039"
  },
  {
    "text": "it's your choice depending on your workload needs you can choose either of them but keep in mind this is only good for a",
    "start": "1783039",
    "end": "1789760"
  },
  {
    "text": "single trust boundary don't use it across the customers that's not what we",
    "start": "1789760",
    "end": "1794960"
  },
  {
    "text": "recommend thank you for listening to my talk and open for any questions",
    "start": "1794960",
    "end": "1802130"
  },
  {
    "text": "[Applause]",
    "start": "1802130",
    "end": "1810499"
  },
  {
    "text": "all right thank you everyone um i'm your moderator for this session so i'll be running around the mic i see there's one over",
    "start": "1813200",
    "end": "1820320"
  },
  {
    "text": "there and i'm gonna get to you in just a second first off i'm actually gonna ask a question from online",
    "start": "1820320",
    "end": "1826559"
  },
  {
    "text": "uh can you enforce to use the same physical gpu by different containers for example",
    "start": "1826559",
    "end": "1832559"
  },
  {
    "text": "you want to run x server in one container and desktop in another these two containers need to share a single",
    "start": "1832559",
    "end": "1838240"
  },
  {
    "text": "gpu it won't work otherwise you should ask for gpu in both containers if a node has more than one gpu there are no",
    "start": "1838240",
    "end": "1845200"
  },
  {
    "text": "guarantees that they get the same gpu that make sense yes",
    "start": "1845200",
    "end": "1850399"
  },
  {
    "text": "i missed the last part but yes you can share the gpus across two different applications",
    "start": "1850399",
    "end": "1855760"
  },
  {
    "text": "and the one thing to watch out for is that out of memory situation so you can",
    "start": "1855760",
    "end": "1860880"
  },
  {
    "text": "actually if you're not careful then in the time sharing case uh you can encounter out of memory situations",
    "start": "1860880",
    "end": "1867840"
  },
  {
    "text": "but if you're doing with the mig that should work fine and we do have customers using and sharing single gque",
    "start": "1867840",
    "end": "1873840"
  },
  {
    "text": "across totally two different totally up different applications",
    "start": "1873840",
    "end": "1879720"
  },
  {
    "text": "thank you okay i'll bring this over to the person over here who had a question",
    "start": "1881039",
    "end": "1887120"
  },
  {
    "text": "thank you thanks thanks a lot for this talk um in a time sharing case",
    "start": "1890240",
    "end": "1896240"
  },
  {
    "text": "do you observe any uh calculation of performance drop due to",
    "start": "1896240",
    "end": "1901760"
  },
  {
    "text": "cash refresh while context switching or not",
    "start": "1901760",
    "end": "1906799"
  },
  {
    "text": "so we have done extensive testing and the results are very workload",
    "start": "1906799",
    "end": "1912240"
  },
  {
    "text": "specific so it cannot be translated across the workload however if you limit the number of containers you share a",
    "start": "1912240",
    "end": "1918960"
  },
  {
    "text": "single gpu with then the performance hit is very negligible",
    "start": "1918960",
    "end": "1924399"
  },
  {
    "text": "but if you try to go extreme like i want to do 50 containers on a single gpu and gpu is",
    "start": "1924399",
    "end": "1931200"
  },
  {
    "text": "t4 tesla then certainly you will have too much overhead from contact switching",
    "start": "1931200",
    "end": "1937519"
  },
  {
    "text": "but the specific example like nvidia does a really clever job in managing the memory and other things",
    "start": "1937519",
    "end": "1944159"
  },
  {
    "text": "we haven't seen a huge performance hit because of contact switching in this scenario as long as you limit the number",
    "start": "1944159",
    "end": "1949679"
  },
  {
    "text": "of containers the overhead is very small all right",
    "start": "1949679",
    "end": "1954720"
  },
  {
    "text": "uh i saw someone back here first i don't know who it was though",
    "start": "1954720",
    "end": "1960399"
  },
  {
    "text": "okay i'll come back",
    "start": "1960399",
    "end": "1963840"
  },
  {
    "text": "hi thank you for your talk uh very insightful i have a question about the memory i know that for mig",
    "start": "1966240",
    "end": "1973600"
  },
  {
    "text": "you actually can use only one slice of the memory so if you use two instances you can use only half the memory that",
    "start": "1973600",
    "end": "1979519"
  },
  {
    "text": "the gpu actually provides with the time slicing is that the same or is it not the same",
    "start": "1979519",
    "end": "1986000"
  },
  {
    "text": "it didn't really become clear from my talk so the question is can two",
    "start": "1986000",
    "end": "1991200"
  },
  {
    "text": "individual containers both use the full memory one after the other or can they use only",
    "start": "1991200",
    "end": "1996640"
  },
  {
    "text": "half and is this caps yeah so let me first clarify the question so in the case of meg you will",
    "start": "1996640",
    "end": "2003120"
  },
  {
    "text": "specify a memory slice for a particular instance so that's the memory that is available",
    "start": "2003120",
    "end": "2008880"
  },
  {
    "text": "to a given slice so that is very clear in the case of time sharing it's not clear",
    "start": "2008880",
    "end": "2014000"
  },
  {
    "text": "so what you have to watch out for is that sum total of memory used by all the containers does not exceed the total",
    "start": "2014000",
    "end": "2020559"
  },
  {
    "text": "memory that is available on gpu that's why we mentioned that out of memory situation is real in the case of time",
    "start": "2020559",
    "end": "2027360"
  },
  {
    "text": "sharing and you have to make sure that applications are well behaved and they don't claim more memory than",
    "start": "2027360",
    "end": "2034399"
  },
  {
    "text": "physically available on a single gpu",
    "start": "2034399",
    "end": "2038799"
  },
  {
    "text": "right so they together have to still fit in the total amount of member because it's not being offloaded",
    "start": "2039919",
    "end": "2045360"
  },
  {
    "text": "between one and the other correct nice yeah",
    "start": "2045360",
    "end": "2050078"
  },
  {
    "text": "all right we've got a couple other questions over here but first i'm gonna do one online again uh if you run",
    "start": "2050480",
    "end": "2055760"
  },
  {
    "text": "something like tensorflow it likes to allocate and keep a whole gpu",
    "start": "2055760",
    "end": "2060878"
  },
  {
    "text": "keep whole gpu memory uh letting no space for sharing any commentary on those types of situations yeah but it",
    "start": "2060879",
    "end": "2067520"
  },
  {
    "text": "also allows you to limit the memory that you can request for your container so as long as you use tensorflow",
    "start": "2067520",
    "end": "2074720"
  },
  {
    "text": "carefully and when you're doing sharing you make sure that total requests do not exceed the physical capacity of the gpu",
    "start": "2074720",
    "end": "2081839"
  },
  {
    "text": "you can do it you can we have customers using tensorflow and sharing the gpus okay cool and",
    "start": "2081839",
    "end": "2090480"
  },
  {
    "text": "uh hi thanks for the talk uh just the clarification in case of time sharing",
    "start": "2090480",
    "end": "2096560"
  },
  {
    "text": "uh i can allocate uneven time for the different containers right by",
    "start": "2096560",
    "end": "2102640"
  },
  {
    "text": "providing the number of gpus greater than one like in your example there was one but i can provide two and it will",
    "start": "2102640",
    "end": "2108160"
  },
  {
    "text": "get two times of time that's a great question uh thanks for asking",
    "start": "2108160",
    "end": "2114000"
  },
  {
    "text": "there are a lot of new answers to that so if you ask more than one",
    "start": "2114000",
    "end": "2119040"
  },
  {
    "text": "then basically it allows you to bin pack basically heterogeneous workloads on the same gpu",
    "start": "2119040",
    "end": "2126400"
  },
  {
    "text": "but in the end the compute time is evenly divided so",
    "start": "2126400",
    "end": "2132079"
  },
  {
    "text": "each container will get the same amount of compute time but in terms of memory",
    "start": "2132079",
    "end": "2137119"
  },
  {
    "text": "and other requirements you can fit in heterogeneous workloads on the same gpu so that's the",
    "start": "2137119",
    "end": "2143040"
  },
  {
    "text": "trick you have to play with that counter oh i see so it's static one that means so time wise everybody gets",
    "start": "2143040",
    "end": "2150000"
  },
  {
    "text": "same time slice but in terms of memory now how you want to fit the",
    "start": "2150000",
    "end": "2155280"
  },
  {
    "text": "different workloads heterogeneous workloads on not available",
    "start": "2155280",
    "end": "2159559"
  },
  {
    "text": "what you're using under the hood so the solution we launch does not use mps that's on our roadmap",
    "start": "2172320",
    "end": "2180960"
  },
  {
    "text": "cool and there's another question over here yes",
    "start": "2180960",
    "end": "2187480"
  },
  {
    "text": "hi thanks for uh for the talk uh i have two questions uh first one is it in ga",
    "start": "2188880",
    "end": "2195119"
  },
  {
    "text": "this solution or still in preview so this is available it's generally available yes",
    "start": "2195119",
    "end": "2201440"
  },
  {
    "text": "so technical question can i request more than one slice for a",
    "start": "2201440",
    "end": "2206960"
  },
  {
    "text": "single container for example i decided to use one cpu five gigabyte for example my but i",
    "start": "2206960",
    "end": "2214640"
  },
  {
    "text": "need two slice for my container okay yes can i do it yes you can do that so you put two in the request yes",
    "start": "2214640",
    "end": "2221920"
  },
  {
    "text": "okay so in the case of time slicing it's pretty straightforward you ask for like two and you get two slices and you can",
    "start": "2221920",
    "end": "2228000"
  },
  {
    "text": "run container on two slices in time sharing case is a little bit tricky if you ask more than one you",
    "start": "2228000",
    "end": "2234079"
  },
  {
    "text": "still get a proportionate time slice so intuitively it's a little bit hard to",
    "start": "2234079",
    "end": "2239680"
  },
  {
    "text": "make sense out of it like i asked five but i if there are two containers h1 is going",
    "start": "2239680",
    "end": "2244960"
  },
  {
    "text": "to get equal amount of time but you can use that cleverly to fit in heterogeneous workloads if they have",
    "start": "2244960",
    "end": "2251760"
  },
  {
    "text": "different memory requirements thank you all right we're a couple minutes over",
    "start": "2251760",
    "end": "2257280"
  },
  {
    "text": "time so i'm going to go ahead and cut off questions there but i'm sure mullen will probably hang out for a few minutes",
    "start": "2257280",
    "end": "2262640"
  },
  {
    "text": "if anyone would like to come up and ask him questions yes please come here and i'm happy to answer more questions",
    "start": "2262640",
    "end": "2270440"
  },
  {
    "text": "you",
    "start": "2271839",
    "end": "2273920"
  }
]