[
  {
    "text": "good afternoon London I would say How's everybody",
    "start": "160",
    "end": "5760"
  },
  {
    "text": "doing come on It's been a long day I know you want to go home I had to go to parties",
    "start": "6839",
    "end": "12599"
  },
  {
    "text": "right come on Um but before you're doing that shall we make our",
    "start": "12599",
    "end": "20039"
  },
  {
    "text": "session optimizing matrix collection and serving when autoscaling large language",
    "start": "20039",
    "end": "26080"
  },
  {
    "text": "model workloads as a warm up of your fun",
    "start": "26080",
    "end": "31920"
  },
  {
    "text": "time My name is Vincent Ho It's my honor to team up with Hey my name is today",
    "start": "32200",
    "end": "40559"
  },
  {
    "text": "I'm a senior software engineer working with Bloomberg My team is specializ in building and maintaining AI inference",
    "start": "40559",
    "end": "49160"
  },
  {
    "text": "platform I've been the lead of K native operation work group for six years I've",
    "start": "49160",
    "end": "54480"
  },
  {
    "text": "been evangelizing open technology and contributing to open source projects During the",
    "start": "54480",
    "end": "60520"
  },
  {
    "text": "past 10 years maybe maybe more um like open stack open",
    "start": "60520",
    "end": "68119"
  },
  {
    "text": "kurf but don't worry we'll focus on the three dots today Jerry Yeah So my name",
    "start": "68119",
    "end": "73760"
  },
  {
    "text": "is Urka I work on a cool company called Kify where we try to do production grade",
    "start": "73760",
    "end": "80360"
  },
  {
    "text": "KDA and I'm also contributor to the project called KGB I like open source 3D",
    "start": "80360",
    "end": "87479"
  },
  {
    "text": "printing and you will see me in the uh second half of the of the session So see",
    "start": "87479",
    "end": "93079"
  },
  {
    "text": "you Okay Thank you",
    "start": "93079",
    "end": "97600"
  },
  {
    "text": "Jerry This is our agenda for today I will start with how in general",
    "start": "98920",
    "end": "105479"
  },
  {
    "text": "autoscaling works horizontal auto scaling works in Kubernetes and talk about the challenges which large",
    "start": "105479",
    "end": "112000"
  },
  {
    "text": "language work uh with large language model workloads and the existing",
    "start": "112000",
    "end": "117360"
  },
  {
    "text": "autoscaling solution in the market and how we optimize the matrix collection",
    "start": "117360",
    "end": "123600"
  },
  {
    "text": "and serving with our solutions In the end we'll run a",
    "start": "123600",
    "end": "129680"
  },
  {
    "text": "demo previous episodes in CubeCon Tell me why and tell me why I'm sorry I",
    "start": "130599",
    "end": "136560"
  },
  {
    "text": "cannot think here today Uh I used to carry many questions about how we can scale large model workloads and what",
    "start": "136560",
    "end": "143440"
  },
  {
    "text": "kind of matches we can leverage Have fun time on that stage And thanks to these awesome folks",
    "start": "143440",
    "end": "151120"
  },
  {
    "text": "I met in Salt Lake City Arshock from Google Lumila from Microsoft and David",
    "start": "151120",
    "end": "157440"
  },
  {
    "text": "from Red Hat After attending their sessions my confusion will resolved a",
    "start": "157440",
    "end": "162720"
  },
  {
    "text": "lot I put the links over here to their talks You can refer to",
    "start": "162720",
    "end": "168879"
  },
  {
    "text": "later Okay For our session as it's in London let's shall we do something special for London",
    "start": "169800",
    "end": "175720"
  },
  {
    "text": "Huh to be or not to be I am going to do",
    "start": "175720",
    "end": "180760"
  },
  {
    "text": "it Act one Auto skating is heavy and light bright and dark hot and cold seek",
    "start": "180760",
    "end": "189440"
  },
  {
    "text": "and healthy asleep and awake is everything except what it is Oh it is so",
    "start": "189440",
    "end": "197000"
  },
  {
    "text": "complicated In cloud computing autoscaling is a method to dynamically",
    "start": "197000",
    "end": "202080"
  },
  {
    "text": "adjust the resource based on the load automatically In",
    "start": "202080",
    "end": "207239"
  },
  {
    "text": "cube it means a feature that allows the cluster to increase or decrease the number of pods or just a pod resource in",
    "start": "207239",
    "end": "214879"
  },
  {
    "text": "response to demand We have HPA for horizontal scaler and VPA as a vertical",
    "start": "214879",
    "end": "220799"
  },
  {
    "text": "scaler but we'll focus on the horizontal autoscaling in cube",
    "start": "220799",
    "end": "227720"
  },
  {
    "text": "today Why do we need autoscaling because we like to efficiently leverage",
    "start": "227720",
    "end": "235040"
  },
  {
    "text": "use our resources without influencing the uh service uptime performance",
    "start": "235040",
    "end": "241200"
  },
  {
    "text": "Neither of them is good over or underprovisioning This diagram will show",
    "start": "241200",
    "end": "247280"
  },
  {
    "text": "in general how autoscaling works in cubernetes Uh what we can scale based on",
    "start": "247280",
    "end": "252879"
  },
  {
    "text": "is something we can call metrics Metrics has a broad range They",
    "start": "252879",
    "end": "258079"
  },
  {
    "text": "can be the basic resource like CPU and memory They can be the custom metrics",
    "start": "258079",
    "end": "263919"
  },
  {
    "text": "exposed by our workload and they can be metrics that possibly come from outside",
    "start": "263919",
    "end": "269960"
  },
  {
    "text": "workload Okay Metric that need to be collected and save it somewhere Well we",
    "start": "269960",
    "end": "275759"
  },
  {
    "text": "call it metric registry After that we need to make sure the metric can be consumed by the HPA",
    "start": "275759",
    "end": "283199"
  },
  {
    "text": "otherwise have no meaning And that is what the uh matrix server is doing right",
    "start": "283199",
    "end": "288639"
  },
  {
    "text": "over here On one end it can discover and aggregate those metrics from registry on",
    "start": "288639",
    "end": "294960"
  },
  {
    "text": "the other end and make sure that the HPA can read the metric from it So once the metric reach the",
    "start": "294960",
    "end": "301320"
  },
  {
    "text": "HPA you run some calculation maybe easy maybe complex and compare with the",
    "start": "301320",
    "end": "309600"
  },
  {
    "text": "target value and make scanning decisions that generally how auto works with",
    "start": "309600",
    "end": "317360"
  },
  {
    "text": "coup by my head here come the challenges by my heel we",
    "start": "318520",
    "end": "324759"
  },
  {
    "text": "cannot sorry wrong line by my heel We care a",
    "start": "324759",
    "end": "330440"
  },
  {
    "text": "lot Not not in in yeah in Kubernetes environment",
    "start": "330440",
    "end": "339280"
  },
  {
    "text": "actually especially in my company Bloomber we build on premises AI inference platform empowered by Ker Quer",
    "start": "339280",
    "end": "346560"
  },
  {
    "text": "makes the model highly scalable as a service as it is a standard cloud",
    "start": "346560",
    "end": "352160"
  },
  {
    "text": "agnostic model inference platform to serve both predictive and generative",
    "start": "352160",
    "end": "358960"
  },
  {
    "text": "model on cube Now it's an independent project but it's under Apache license",
    "start": "358960",
    "end": "365440"
  },
  {
    "text": "and it's on the way to become a CNCF project Well my mission was to build the",
    "start": "365440",
    "end": "372080"
  },
  {
    "text": "autoscaling solution for large language model workloads with",
    "start": "372080",
    "end": "376960"
  },
  {
    "text": "KSER However the large language model workload has brought us a paradigm shift",
    "start": "377560",
    "end": "383919"
  },
  {
    "text": "in terms of what okay computer resource We used",
    "start": "383919",
    "end": "389039"
  },
  {
    "text": "to use CPU now we use GPUs and at the same time the",
    "start": "389039",
    "end": "397080"
  },
  {
    "text": "matrix the matrix also become different some something that work before but",
    "start": "397080",
    "end": "402240"
  },
  {
    "text": "doesn't work right now for example default could use like CPU or memory to",
    "start": "402240",
    "end": "407840"
  },
  {
    "text": "scale something but they do not reflect the usage of GPU and later we got",
    "start": "407840",
    "end": "414319"
  },
  {
    "text": "another matrix like the number of requests per second but this also So doesn't reflect the GPU usage latency or",
    "start": "414319",
    "end": "421120"
  },
  {
    "text": "super process either And speaking of the some criteria that we can auto spell the large",
    "start": "421120",
    "end": "428240"
  },
  {
    "text": "language model workloads we can think of either latency or throughut But in",
    "start": "428240",
    "end": "434400"
  },
  {
    "text": "general the problem is that none of the existing workloads autoscaling metrics can fit our needs",
    "start": "434400",
    "end": "442880"
  },
  {
    "text": "So I say projecting back to this diagram of how autoscaling works in kubernetes",
    "start": "442880",
    "end": "449680"
  },
  {
    "text": "we got like five not I will say five group of questions five group of",
    "start": "449680",
    "end": "455360"
  },
  {
    "text": "question we need to address so one we need to figure out where to read the matrix and what kind of metric we can",
    "start": "455360",
    "end": "462400"
  },
  {
    "text": "leverage to scale our workloads and second we have no idea where to save",
    "start": "462400",
    "end": "468720"
  },
  {
    "text": "them and um yeah that's probably what's the metrics metric registry is three we",
    "start": "468720",
    "end": "474960"
  },
  {
    "text": "need to find out what can be used as metric server to discover these metrics and ser them to",
    "start": "474960",
    "end": "481720"
  },
  {
    "text": "HPA and four large a model usually should be can be very large yeah that's why they call large it could be uh over",
    "start": "481720",
    "end": "489360"
  },
  {
    "text": "100 gigabytes or something so how can I make it load and scale fast after this",
    "start": "489360",
    "end": "494479"
  },
  {
    "text": "matrix value has been changed well can we even configure how fast we scale our",
    "start": "494479",
    "end": "500199"
  },
  {
    "text": "workloads and Five Is there a solution that is flexible and more portable to all kinds",
    "start": "500199",
    "end": "508639"
  },
  {
    "text": "of platforms all right Act three And trust",
    "start": "508639",
    "end": "513680"
  },
  {
    "text": "me love in my eyes So do you Dry sorrow drinks our",
    "start": "513680",
    "end": "519479"
  },
  {
    "text": "blood Oh man Huh",
    "start": "519479",
    "end": "524720"
  },
  {
    "text": "well speaking of the existing ways that we're doing autoscaling well let's start",
    "start": "524720",
    "end": "529839"
  },
  {
    "text": "from the easiest one No that's it We try to leverage the matrix",
    "start": "529839",
    "end": "539000"
  },
  {
    "text": "server The benefit is that it's a integral that cumulative native",
    "start": "539000",
    "end": "546040"
  },
  {
    "text": "component that can scale very fast by leveraging small amount of resources",
    "start": "546040",
    "end": "552720"
  },
  {
    "text": "It can read the matrix from the cubate and export to the HPA",
    "start": "552720",
    "end": "559080"
  },
  {
    "text": "directly but the limitation is that only CPU and memory are supported",
    "start": "559080",
    "end": "569160"
  },
  {
    "text": "Second this is a good thing This is a good project As one of",
    "start": "570760",
    "end": "577480"
  },
  {
    "text": "dependencies for Ker K native serving implements its own",
    "start": "577480",
    "end": "582959"
  },
  {
    "text": "K native pod autoscaler to directly control the number of paths It can scale",
    "start": "582959",
    "end": "590399"
  },
  {
    "text": "based on the number of requests per second And this metric can be collected",
    "start": "590399",
    "end": "597480"
  },
  {
    "text": "by either Q proxy scar or the activator",
    "start": "597480",
    "end": "603560"
  },
  {
    "text": "components The number of the requests can change very fast Well it can be reflected in the KP very fast as well",
    "start": "603560",
    "end": "611959"
  },
  {
    "text": "because no mattress pulling is involved It always push well once there was a",
    "start": "611959",
    "end": "618120"
  },
  {
    "text": "change KPA got change immediately And it can scale down to zero as well",
    "start": "618120",
    "end": "624800"
  },
  {
    "text": "Scale workload down to zero Well but the following drawbacks just make it impossible for us to use it",
    "start": "624800",
    "end": "633880"
  },
  {
    "text": "One RPS request per second does not work for large language model Second a",
    "start": "633880",
    "end": "641200"
  },
  {
    "text": "complex algorithm or calculation has to be maintained the source code Third to",
    "start": "641200",
    "end": "647480"
  },
  {
    "text": "extend this model No I mean this architecture with more metrics we have",
    "start": "647480",
    "end": "653920"
  },
  {
    "text": "to write our new plugins Four the RPS metric associate with K native only and cannot be ported",
    "start": "653920",
    "end": "661279"
  },
  {
    "text": "to other platform unless you use it Third one h this going to be more",
    "start": "661279",
    "end": "668720"
  },
  {
    "text": "interesting Recently the virtual large language",
    "start": "668720",
    "end": "674800"
  },
  {
    "text": "model community has announced the uh AI bricks as the cloud native solution",
    "start": "674800",
    "end": "680800"
  },
  {
    "text": "optimize for deploying managing and scaling large language model inference It supports",
    "start": "680800",
    "end": "687440"
  },
  {
    "text": "both matrix based autoscaling and optimizer based autoscaling It",
    "start": "687440",
    "end": "693160"
  },
  {
    "text": "implements autoscaling in a similar way to K native serving and all the large language model",
    "start": "693160",
    "end": "701279"
  },
  {
    "text": "matrix can be blended into this APA to control the number of paths Well these",
    "start": "701279",
    "end": "708160"
  },
  {
    "text": "are all good Very good But the limitation is that Ker is a backbone of",
    "start": "708160",
    "end": "713920"
  },
  {
    "text": "our infant platform We cannot leverage this solution that unable to integrate our",
    "start": "713920",
    "end": "722920"
  },
  {
    "text": "platform Walking through those challenges one more time So what's the",
    "start": "722920",
    "end": "729279"
  },
  {
    "text": "source of the matrix and what kind of criteria to autoscale a large language model workload well we have the answer",
    "start": "729279",
    "end": "736240"
  },
  {
    "text": "The source could be the workload itself As long as my workload has exposed the",
    "start": "736240",
    "end": "742399"
  },
  {
    "text": "matrix as a pass/mmetric kind of per standard no worry about that and uh we can scale",
    "start": "742399",
    "end": "750320"
  },
  {
    "text": "either think of is more latency sensitive or throughut those kind of",
    "start": "750320",
    "end": "757120"
  },
  {
    "text": "standards So two second question how can we scale it fast or in a config and",
    "start": "757120",
    "end": "762720"
  },
  {
    "text": "configurable way what we try to we can try to leverage uh the lightweight",
    "start": "762720",
    "end": "768279"
  },
  {
    "text": "components as many as possible using a pushing mode not the push mode not the pulling",
    "start": "768279",
    "end": "776160"
  },
  {
    "text": "the push mode not the pulling mode as many as as much as possible I think and",
    "start": "776160",
    "end": "781680"
  },
  {
    "text": "the mo most importantly we can make it scale fast because there is a very nice",
    "start": "781680",
    "end": "788000"
  },
  {
    "text": "feature called model cache which is",
    "start": "788000",
    "end": "793519"
  },
  {
    "text": "implement but the answer to the rest of the question still remains unknown uh",
    "start": "794680",
    "end": "801360"
  },
  {
    "text": "what can we do for what can we use for the matrix registry or server how can we",
    "start": "801360",
    "end": "806959"
  },
  {
    "text": "make it flexible or extensible once there any kind of changes we have no we don't know so",
    "start": "806959",
    "end": "817519"
  },
  {
    "text": "The thing is that we do not try to invent the wheel We try to stand on a giant shoulders So who are the",
    "start": "818800",
    "end": "825000"
  },
  {
    "text": "giants and can we find them when can we find",
    "start": "825000",
    "end": "830600"
  },
  {
    "text": "them so act four as it been If it be now this not to come If it be not to come it",
    "start": "830600",
    "end": "838959"
  },
  {
    "text": "will be now Wow Good news optimization of metric collection and",
    "start": "838959",
    "end": "845560"
  },
  {
    "text": "serving We'll show you how Jerry stage is yours Thank you Vincent I don't have",
    "start": "845560",
    "end": "852399"
  },
  {
    "text": "any Shakespeare lines for you but thanks for the great intro and state-of-the-art",
    "start": "852399",
    "end": "858000"
  },
  {
    "text": "Yeah I don't know So one of the giants to continue with the analogy is Open Telemetry Are you guys familiar familiar",
    "start": "858000",
    "end": "864399"
  },
  {
    "text": "with Open Telemetry yeah a lot of boos out there having the",
    "start": "864399",
    "end": "870320"
  },
  {
    "text": "logo on it on them So it's like standardization effort umbrella slash",
    "start": "870320",
    "end": "875920"
  },
  {
    "text": "sets of standards for traces metrics and and logs right and they've got some",
    "start": "875920",
    "end": "881760"
  },
  {
    "text": "handy utility called hotel collector which we'll be using a lot So another one is a keta I've got",
    "start": "881760",
    "end": "889600"
  },
  {
    "text": "config interest because I work on it so but it's great right uh it's it's uh",
    "start": "889600",
    "end": "894959"
  },
  {
    "text": "built around scale object which is a custom CR that points to a deployment or",
    "start": "894959",
    "end": "901279"
  },
  {
    "text": "any scalable resource and the main benefit uh compared to pure HPA is that",
    "start": "901279",
    "end": "907360"
  },
  {
    "text": "it can scale things to zero but also it has a multiple different ways to plug",
    "start": "907360",
    "end": "913199"
  },
  {
    "text": "metrics in So it doesn't have to scale only based on CPU or memory but",
    "start": "913199",
    "end": "918800"
  },
  {
    "text": "basically based on anything And the third part we'll be using is my own contribution It's hotel",
    "start": "918800",
    "end": "925839"
  },
  {
    "text": "add-on for keta And this thing actually bridges the gap between the open",
    "start": "925839",
    "end": "931920"
  },
  {
    "text": "telemetry world When you have this hotel collectors that forms kind of pipelines we have some receivers processors and",
    "start": "931920",
    "end": "940040"
  },
  {
    "text": "exporters And this this guy can uh listen to those uh other exporters So it",
    "start": "940040",
    "end": "946959"
  },
  {
    "text": "contains OTLP receiver in their terminology So it's kind of a sync to of",
    "start": "946959",
    "end": "952639"
  },
  {
    "text": "those metrics but at the same time can talk to KDA and can scale and can can",
    "start": "952639",
    "end": "959360"
  },
  {
    "text": "advise SCADA the the scaling decision decisions It also contains short-term",
    "start": "959360",
    "end": "964639"
  },
  {
    "text": "memory storage for couple of data points So it's like really small promeus",
    "start": "964639",
    "end": "970880"
  },
  {
    "text": "lightweight promeuse crafted just for scaling because that's uh ka also can",
    "start": "970880",
    "end": "976040"
  },
  {
    "text": "incorporate incorporate promeus metrics but the issue with this setup is that",
    "start": "976040",
    "end": "981279"
  },
  {
    "text": "prometers get overwhelmed and imagine a scenario when you have a graphana setup",
    "start": "981279",
    "end": "986480"
  },
  {
    "text": "and also like alert alert manager and if you use uh prometers with this scenario",
    "start": "986480",
    "end": "991839"
  },
  {
    "text": "also for scaling it's overwhelmed So that's that's the reasoning behind it Our setup looks like this Oh this is",
    "start": "991839",
    "end": "999360"
  },
  {
    "text": "like one of the possible setups when we have one open telemetry collector scraping the replica ports with models",
    "start": "999360",
    "end": "1007440"
  },
  {
    "text": "This is the uh bottom left segment and this sends the metrics to hotel add-on",
    "start": "1007440",
    "end": "1014959"
  },
  {
    "text": "from which kada takes them and scales the number of replicas of the",
    "start": "1014959",
    "end": "1020279"
  },
  {
    "text": "model Another possible uh setup is this This one when we plug uh hotel collector",
    "start": "1020279",
    "end": "1028319"
  },
  {
    "text": "to each pot using a sidecar sidecar model and this setup is a little bit",
    "start": "1028319",
    "end": "1034438"
  },
  {
    "text": "better because it can react uh more quickly and we can leverage the open telemetry operator to to help with with",
    "start": "1034439",
    "end": "1042079"
  },
  {
    "text": "the setup So without further ado let's check the demo Uh so what I'm is it visible",
    "start": "1042079",
    "end": "1052000"
  },
  {
    "text": "yeah Cool starts Well I don't have network",
    "start": "1052000",
    "end": "1059080"
  },
  {
    "text": "Oops Nice So let me check the",
    "start": "1063160",
    "end": "1068400"
  },
  {
    "text": "Wi-Fi on stage I will create a setup I was created hotspot",
    "start": "1070760",
    "end": "1076840"
  },
  {
    "text": "Uh yeah Oh yeah I will use my my own own own Wi-Fi Hopefully it's going to work",
    "start": "1080360",
    "end": "1089120"
  },
  {
    "text": "Live demo right yeah it's live demo It's it's a live cluster running in GCP Yeah it works First problem solved So we have",
    "start": "1089120",
    "end": "1097200"
  },
  {
    "text": "a open web UI pod which is a web UI to LLMs and llama version 3 deployed in our",
    "start": "1097200",
    "end": "1104640"
  },
  {
    "text": "cluster in GCP Uh just to convince you we have a ingress ingress resource",
    "start": "1104640",
    "end": "1111679"
  },
  {
    "text": "called llm webcamzer.dev and I'm using the same URL in here so I can talk with the uh model",
    "start": "1111679",
    "end": "1120919"
  },
  {
    "text": "Okay I am on cube con and wifi is not",
    "start": "1120919",
    "end": "1130639"
  },
  {
    "text": "great and it responds with something but I can use also curl API to",
    "start": "1131480",
    "end": "1138160"
  },
  {
    "text": "talk with the model the same way So this is basically the open open AAI",
    "start": "1138160",
    "end": "1145919"
  },
  {
    "text": "um HTTP protocol There is stream equals true or false If it's true the model",
    "start": "1145919",
    "end": "1152080"
  },
  {
    "text": "will start sending the tokens in a stream fashion So it feels it responds quickly more quickly So if I do it it's",
    "start": "1152080",
    "end": "1160320"
  },
  {
    "text": "doing the same thing as we show as we see before but in a command line Right",
    "start": "1160320",
    "end": "1166000"
  },
  {
    "text": "this max token actually denotes like how long the response should be And this is",
    "start": "1166000",
    "end": "1171679"
  },
  {
    "text": "actually the thing that makes the uh request different because one client can",
    "start": "1171679",
    "end": "1177120"
  },
  {
    "text": "ask like short answer the other one can ask for multiple pages of",
    "start": "1177120",
    "end": "1182360"
  },
  {
    "text": "responses Uh if I set the stream to false it works",
    "start": "1182360",
    "end": "1187919"
  },
  {
    "text": "the same way but it waits a little bit longer but then returns everything at once Right we'll be using this curl or",
    "start": "1187919",
    "end": "1195440"
  },
  {
    "text": "this HTTP uh API to create some pressure to our model to be able to scale",
    "start": "1195440",
    "end": "1202039"
  },
  {
    "text": "it like what's the criteria what why or not why but what's the scaling decision",
    "start": "1202039",
    "end": "1207840"
  },
  {
    "text": "we will be using we'll be using internal metrics from the model itself we are using uh VLM runtime and they exposes",
    "start": "1207840",
    "end": "1214880"
  },
  {
    "text": "some uh GPU stats about around the the system so we'll use KV cache in particular and in the next part of the",
    "start": "1214880",
    "end": "1221919"
  },
  {
    "text": "demo we'll use also the waiting Q for for the request So let me",
    "start": "1221919",
    "end": "1227080"
  },
  {
    "text": "first port forward the request from the model This will share the metric",
    "start": "1227080",
    "end": "1232559"
  },
  {
    "text": "endpoints from one of the replica ports If",
    "start": "1232559",
    "end": "1237919"
  },
  {
    "text": "I yeah I'm basically curling the metrics because now it's local host right and",
    "start": "1238600",
    "end": "1244000"
  },
  {
    "text": "grabbing two of them which we will be using It's zero",
    "start": "1244000",
    "end": "1251679"
  },
  {
    "text": "uh let me split the screen and I can also describe the scale",
    "start": "1251679",
    "end": "1259120"
  },
  {
    "text": "scaled object So scaled object remember it is the thing from keta that has a",
    "start": "1259120",
    "end": "1264240"
  },
  {
    "text": "target deployment that it's scaling So",
    "start": "1264240",
    "end": "1270880"
  },
  {
    "text": "describe so model it has something called uh where",
    "start": "1271080",
    "end": "1278880"
  },
  {
    "text": "scale target references the deployment in our case is the this is the model and this part is",
    "start": "1278880",
    "end": "1286240"
  },
  {
    "text": "actually uh promql like syntax is not full-blown promqql and this is sent",
    "start": "1286240",
    "end": "1291760"
  },
  {
    "text": "through our hotel add-on where it's evaluated and the correct metric is",
    "start": "1291760",
    "end": "1297200"
  },
  {
    "text": "returned and collect metric value is returned We don't support full primitives uh syntax because it's crazy",
    "start": "1297200",
    "end": "1303919"
  },
  {
    "text": "right and and there are ways to overcome this this issue with using processors in",
    "start": "1303919",
    "end": "1309039"
  },
  {
    "text": "hotel world For instance uh keta wasn't able to work with um float values So",
    "start": "1309039",
    "end": "1317280"
  },
  {
    "text": "what we do did here is we scaled the value by multiplying by by 100 and it worked And this multiplication is done",
    "start": "1317280",
    "end": "1323679"
  },
  {
    "text": "in the hotel collector itself I can also show you",
    "start": "1323679",
    "end": "1329159"
  },
  {
    "text": "the the open telemetry CR which is responsible for",
    "start": "1329159",
    "end": "1336080"
  },
  {
    "text": "open telemetry operator injecting the sidec cars to our models Oops And this",
    "start": "1336080",
    "end": "1343000"
  },
  {
    "text": "guy contains the configuration for those small sidecars hotel",
    "start": "1343000",
    "end": "1349080"
  },
  {
    "text": "collectors So for instance this is the target address where we send the metrics This is the hotel",
    "start": "1349080",
    "end": "1355320"
  },
  {
    "text": "add-on This is the filtering part So we are interesting only in those three metrics and this is the uh part where we",
    "start": "1355320",
    "end": "1364240"
  },
  {
    "text": "manipulate the metric a bit to multiply it by 100 In the next release of keta it",
    "start": "1364240",
    "end": "1369280"
  },
  {
    "text": "will be fixed and it will also support float",
    "start": "1369280",
    "end": "1374360"
  },
  {
    "text": "values Right So now let's me do some some load on the server I will be using",
    "start": "1374360",
    "end": "1380880"
  },
  {
    "text": "hey command which is basically creating 300 threads and we'll be uh doing post",
    "start": "1380880",
    "end": "1387919"
  },
  {
    "text": "request the same with the same same same request as we saw before This is the",
    "start": "1387919",
    "end": "1394080"
  },
  {
    "text": "biggest difference It has uh 4k tokens So it's much more uh much more",
    "start": "1394080",
    "end": "1402159"
  },
  {
    "text": "uh pressure there will much more pressure on the GPU So if I do that we should be we should start uh seeing the",
    "start": "1402159",
    "end": "1408799"
  },
  {
    "text": "the metrics coming up The top top line is the number of uh",
    "start": "1408799",
    "end": "1414400"
  },
  {
    "text": "requests being ceued I should show you the pots right so yeah there's still one",
    "start": "1414400",
    "end": "1420799"
  },
  {
    "text": "if I be watching it Uh",
    "start": "1420799",
    "end": "1428080"
  },
  {
    "text": "aliases Yeah I wasn't fast enough So it scaled up already because it detected it",
    "start": "1429240",
    "end": "1435919"
  },
  {
    "text": "it's above the threshold The threshold was 30 So it spawn three more replicas",
    "start": "1435919",
    "end": "1443240"
  },
  {
    "text": "Unfortunately we have only two GPUs So that's another topic or like subtopic in",
    "start": "1443240",
    "end": "1449840"
  },
  {
    "text": "our talk How to handle not enough GPU problem right because provided you have",
    "start": "1449840",
    "end": "1456320"
  },
  {
    "text": "100 GPUs you just scale the replicas to 100 and it will just work But we don't",
    "start": "1456320",
    "end": "1462320"
  },
  {
    "text": "have endless number number of GPUs So we can use Carpenter It's a nice tool that",
    "start": "1462320",
    "end": "1467440"
  },
  {
    "text": "can add new G uh Kubernetes nodes with uh another GPUs and it works well with",
    "start": "1467440",
    "end": "1474200"
  },
  {
    "text": "AWS or Azure but for instance no solution for GCP yet or we can use",
    "start": "1474200",
    "end": "1480240"
  },
  {
    "text": "cluster API which I really love and it's truly open source project It's harder to set up but uh we can scale it with keta",
    "start": "1480240",
    "end": "1490159"
  },
  {
    "text": "With this we have a lot of troubles to solve like for instance data locality because these models are huge They are",
    "start": "1490159",
    "end": "1496320"
  },
  {
    "text": "like gigabytes in memory they needs to be loaded to me to GPU and we need to",
    "start": "1496320",
    "end": "1501679"
  },
  {
    "text": "make sure that the data is close to to the code that's running actually and also bootstrapping the node like",
    "start": "1501679",
    "end": "1507360"
  },
  {
    "text": "installing Nvidia drivers making sure that images are are pulled and it",
    "start": "1507360",
    "end": "1512480"
  },
  {
    "text": "doesn't take ages to to to spin up new new replicas and we totally will neglect the",
    "start": "1512480",
    "end": "1518559"
  },
  {
    "text": "load balancing aspect of the problem because imagine that we have a GPU that is underutilized we should be probably",
    "start": "1518559",
    "end": "1526159"
  },
  {
    "text": "wanting to having the traffic going to that GPU not and not to the overressured one but these metrics can be used for",
    "start": "1526159",
    "end": "1532960"
  },
  {
    "text": "for it as well it can be further improvements so a couple of words about",
    "start": "1532960",
    "end": "1538080"
  },
  {
    "text": "cluster API it's a there are some talks in the c this coupon as well it's for me",
    "start": "1538080",
    "end": "1544159"
  },
  {
    "text": "it's bunch of CRDs and controllers that can help you with deploying another Kubernetes cluster but they've also have",
    "start": "1544159",
    "end": "1551279"
  },
  {
    "text": "a is have a have a have a support for creating those so-called um self-managed",
    "start": "1551279",
    "end": "1556320"
  },
  {
    "text": "clusters these they they've got CLI called cluster Ctl and it can do cluster Ctl move and then the cluster in a way",
    "start": "1556320",
    "end": "1564320"
  },
  {
    "text": "knows about it about itself and can scale its nodes or even create even upgrade Kubernetes version and things",
    "start": "1564320",
    "end": "1570720"
  },
  {
    "text": "like that A nice thing about it is it contains machine deployment which is a",
    "start": "1570720",
    "end": "1575760"
  },
  {
    "text": "scalable resource So it has number of replicas and you can do things like that a cube cut scale machine deployment and",
    "start": "1575760",
    "end": "1581600"
  },
  {
    "text": "say I I want two replicas and that's uh that's that's why it clicks well with keta because keta can scale this machine",
    "start": "1581600",
    "end": "1587600"
  },
  {
    "text": "deployment guys So let's do that see that in action I actually have yet",
    "start": "1587600",
    "end": "1594320"
  },
  {
    "text": "another scale object for for the nodes",
    "start": "1594320",
    "end": "1599960"
  },
  {
    "text": "itself but this one is actually paused that's why it's not triggering So let me unpause",
    "start": "1599960",
    "end": "1606679"
  },
  {
    "text": "it Annotate not the model one this one So",
    "start": "1606679",
    "end": "1613039"
  },
  {
    "text": "it should be false So now it should start doing something I can split the screen",
    "start": "1613039",
    "end": "1621400"
  },
  {
    "text": "again List a Kubernetes notes in here This is going to take a little bit",
    "start": "1621400",
    "end": "1626559"
  },
  {
    "text": "longer because it's not that quick I'll be watching it here And also I can watch the machine",
    "start": "1626559",
    "end": "1633360"
  },
  {
    "text": "deployments So remember machine deployments shortly MD is the resource for cluster",
    "start": "1633360",
    "end": "1638600"
  },
  {
    "text": "API that's scalable resource and we can see there is just one replica but soon there should be more",
    "start": "1638600",
    "end": "1646120"
  },
  {
    "text": "replic and I can meanwhile I can describe also the yeah so already it's clearing up so it detected in here now",
    "start": "1646120",
    "end": "1653760"
  },
  {
    "text": "it takes like minutes or two to prepare the VM It's actually talking to G to GCP cluster creating VM for us Then this uh",
    "start": "1653760",
    "end": "1661840"
  },
  {
    "text": "new VM needs to run cubadm join our cluster It takes some time and I use u",
    "start": "1661840",
    "end": "1668559"
  },
  {
    "text": "it's called uh image builder from cluster API to actually uh bake those",
    "start": "1668559",
    "end": "1674240"
  },
  {
    "text": "container images into the VM image to make it make the startup a little bit faster But for Nvidia drivers I'm using",
    "start": "1674240",
    "end": "1681520"
  },
  {
    "text": "GPU operator which makes it a little bit longer because it installs Nvidia drivers and this this step takes like 3",
    "start": "1681520",
    "end": "1688000"
  },
  {
    "text": "minutes much better approach would be to compile the drivers directly to the scal version but for GCP I didn't have luck",
    "start": "1688000",
    "end": "1695600"
  },
  {
    "text": "to to do that but it's for further micro optimization I guess uh I can show you",
    "start": "1695600",
    "end": "1701679"
  },
  {
    "text": "the scaled object for for the",
    "start": "1701679",
    "end": "1705679"
  },
  {
    "text": "nodes nodes and we can see that here here the",
    "start": "1707799",
    "end": "1715120"
  },
  {
    "text": "scale target ref is not the deployment anymore but our machine deployment called demo GPU nodes So it's this",
    "start": "1715120",
    "end": "1722440"
  },
  {
    "text": "guy and also nice feature about keta is that you can have actually multiple triggers in our in one scaled object and",
    "start": "1722440",
    "end": "1730559"
  },
  {
    "text": "by default it takes the maximum value if there are multiple triggers and one of them is uh chromebased chromebased",
    "start": "1730559",
    "end": "1737799"
  },
  {
    "text": "scheduleuler schedule and uh you can see that minimum number of replica for this scale object is zero but during this",
    "start": "1737799",
    "end": "1745279"
  },
  {
    "text": "time uh during between office hours so starting from 10 to to 8:00 am the",
    "start": "1745279",
    "end": "1751360"
  },
  {
    "text": "replica should be one So what it effectively does is during off hours it will scale our GPU zero GPU nodes to",
    "start": "1751360",
    "end": "1758320"
  },
  {
    "text": "zero It's because they cost money They are expensive and your AI experts don't",
    "start": "1758320",
    "end": "1764640"
  },
  {
    "text": "work at during nights So why to why to pay for them it's still scaling up Maybe we can",
    "start": "1764640",
    "end": "1772480"
  },
  {
    "text": "switch back to demo and in the end it will just work Yeah Live demo It's gonna",
    "start": "1772480",
    "end": "1778320"
  },
  {
    "text": "work but it takes some time Yeah So maybe just to sum it up right we have we",
    "start": "1778320",
    "end": "1783520"
  },
  {
    "text": "have a reference Yeah Yeah Just for my company that's we are hiring and uh that's our my our booth number and uh",
    "start": "1783520",
    "end": "1790320"
  },
  {
    "text": "you can reach out to anyone reach out to me or anyone with a bloomberg badge or",
    "start": "1790320",
    "end": "1795360"
  },
  {
    "text": "or recruiters and um yeah same for us we are onify boo come say hello",
    "start": "1795360",
    "end": "1803120"
  },
  {
    "text": "Okay let's uh do the thing together and can we can go to the question and answer",
    "start": "1803120",
    "end": "1809039"
  },
  {
    "text": "session and still waiting for the uh no coming up for the demo is online is it's",
    "start": "1809039",
    "end": "1814159"
  },
  {
    "text": "available online so everything can be replicated the cluster API script is everything is there do we do we put our",
    "start": "1814159",
    "end": "1819600"
  },
  {
    "text": "link in our chart uh sorry do we put the link to our repositories in our this is",
    "start": "1819600",
    "end": "1824720"
  },
  {
    "text": "the link like if you want to scale scale the QR this is the link for the yeah yeah it's a very important man or there's a short term if you want yeah",
    "start": "1824720",
    "end": "1831760"
  },
  {
    "text": "Okay Yeah Yeah Okay So I can switch back to demo Yeah And uh we can Yeah we can",
    "start": "1831760",
    "end": "1840320"
  },
  {
    "text": "questions or if you have any We can see the node is ready And now actually the GPU operator will start do the work It's",
    "start": "1840320",
    "end": "1847200"
  },
  {
    "text": "not like like this but it works",
    "start": "1847200",
    "end": "1851760"
  },
  {
    "text": "Yeah Live demo man First time We have some time so we take any questions Yes",
    "start": "1855159",
    "end": "1862880"
  },
  {
    "text": "have a question Yeah went to the mic We use SCA also and sometimes when we scale",
    "start": "1870000",
    "end": "1875440"
  },
  {
    "text": "up it's already quite late because maybe we receive an spike of data and then we scale up but when we already have our",
    "start": "1875440",
    "end": "1882080"
  },
  {
    "text": "collector going up data is already going to stable Makes sense Any suggestion how",
    "start": "1882080",
    "end": "1887200"
  },
  {
    "text": "we can Yeah sure those problems Sure Sure Question Sure You are asking basically about the stabilization window",
    "start": "1887200",
    "end": "1893360"
  },
  {
    "text": "for um I I can describe it in here scale operator",
    "start": "1893360",
    "end": "1898559"
  },
  {
    "text": "just one of them both of them has it It's called stabilization window and you can uh modify the way HPA actually works",
    "start": "1898559",
    "end": "1906880"
  },
  {
    "text": "because HPA is still being being used but under the covers it's like implementation detail for KA So here we",
    "start": "1906880",
    "end": "1912880"
  },
  {
    "text": "say with this number that SC for scaling up the metrics needs to be the threshold needs to be reached only for one second",
    "start": "1912880",
    "end": "1919600"
  },
  {
    "text": "If you increase this number for 100 it means like 100 seconds needs to measure above the threshold and only then it",
    "start": "1919600",
    "end": "1925760"
  },
  {
    "text": "will scale up And same goes for scaling down That's actually a good question because these nodes takes a while to",
    "start": "1925760",
    "end": "1931919"
  },
  {
    "text": "spin up It doesn't make sense if you measure less to immediately kill them right so here we are saying that it",
    "start": "1931919",
    "end": "1937600"
  },
  {
    "text": "needs to be there for I don't know 20 minutes or what is it",
    "start": "1937600",
    "end": "1942799"
  },
  {
    "text": "yeah 20 20 minutes",
    "start": "1942799",
    "end": "1947158"
  },
  {
    "text": "Okay Yeah",
    "start": "1950720",
    "end": "1955919"
  },
  {
    "text": "I think that um for the beginning uh if we show the architecture of cases like",
    "start": "1955919",
    "end": "1961200"
  },
  {
    "text": "this is running on top of K Yeah We can we can use RPS as so and this",
    "start": "1961200",
    "end": "1970559"
  },
  {
    "text": "case we switch to like our custom Yes Yes So would it be possible to to use",
    "start": "1970559",
    "end": "1977919"
  },
  {
    "text": "like as a in case that here we it is we have only LM",
    "start": "1977919",
    "end": "1984679"
  },
  {
    "text": "but in case if we have another funny",
    "start": "1984679",
    "end": "1989799"
  },
  {
    "text": "sure can okay I can I can go a bit with",
    "start": "1989799",
    "end": "1994960"
  },
  {
    "text": "for example if you want to use a case serve to run your serving platform this",
    "start": "1994960",
    "end": "2000399"
  },
  {
    "text": "is kind of road map we're having now say in the old",
    "start": "2000399",
    "end": "2005960"
  },
  {
    "text": "um workloads we either can scale based on CPU or memory this basic one and",
    "start": "2005960",
    "end": "2013279"
  },
  {
    "text": "later as in kative serving can based on the number of requests concurrent requests but in the large language model",
    "start": "2013279",
    "end": "2020559"
  },
  {
    "text": "they didn't work for us for example I get 100 requests coming at the same time",
    "start": "2020559",
    "end": "2029200"
  },
  {
    "text": "that's doesn't necessarily means my GPU consumption is high they do not have",
    "start": "2029200",
    "end": "2034880"
  },
  {
    "text": "that kind of direct correlation to each other So like in logical model there are",
    "start": "2034880",
    "end": "2041039"
  },
  {
    "text": "two kind of criteria you can look at as we just said you either look at latency",
    "start": "2041039",
    "end": "2046320"
  },
  {
    "text": "or is look look at the throughput So by latency if they compile or conform to",
    "start": "2046320",
    "end": "2053919"
  },
  {
    "text": "this virtual logic algorith model spec 0 to one metric you can look at which is called the KV cache percentage it's from",
    "start": "2053919",
    "end": "2063200"
  },
  {
    "text": "zero to one one means 100% it means how much GPU usage you have been used for",
    "start": "2063200",
    "end": "2070000"
  },
  {
    "text": "that KV cache if it's high which means I'm actively processing your requests",
    "start": "2070000",
    "end": "2075919"
  },
  {
    "text": "doing inference at the same time if it's low which means Okay that's fine I can handle But if go above a kind of",
    "start": "2075919",
    "end": "2081919"
  },
  {
    "text": "threshold for example you can set to 50 50% or 60% if you go above that I can",
    "start": "2081919",
    "end": "2088240"
  },
  {
    "text": "scale up for larger model workloads So matrix collection you we need to pick up the correct matrix to scale our",
    "start": "2088240",
    "end": "2094480"
  },
  {
    "text": "workloads But for serverless well we can also do that like we not model workloads based on",
    "start": "2094480",
    "end": "2102720"
  },
  {
    "text": "different workload pick up different metrics and scale based on them",
    "start": "2102720",
    "end": "2109160"
  },
  {
    "text": "Runn Okay Thank you Yeah Thank you Thank you guys Thank you folks You have a wonderful night",
    "start": "2113079",
    "end": "2120880"
  }
]