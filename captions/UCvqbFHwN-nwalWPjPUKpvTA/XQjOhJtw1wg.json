[
  {
    "start": "0",
    "end": "108000"
  },
  {
    "text": "hello everybody we'll be talking for about service discovery today at Airbnb",
    "start": "329",
    "end": "6379"
  },
  {
    "text": "we'll be kind of walking through the transition from Airbnb over the years",
    "start": "6379",
    "end": "11429"
  },
  {
    "text": "talking about where we started where we're at now and like where we plan to go and a lot of the challenges that have",
    "start": "11429",
    "end": "17310"
  },
  {
    "text": "come with maintaining our own service discovery stack so it's a little about me I'm insight little reliability",
    "start": "17310",
    "end": "24390"
  },
  {
    "text": "engineer will kind of talk about why I'm up here talking about this and not some other engineers throughout the",
    "start": "24390",
    "end": "30390"
  },
  {
    "text": "presentation but it has to mostly do is just as we grow we know things get reorg and things get shifted around and",
    "start": "30390",
    "end": "36870"
  },
  {
    "text": "incidentally sree has contributed the most over the years on our service discovery stack also at Airbnb whenever",
    "start": "36870",
    "end": "45660"
  },
  {
    "text": "you joined the company we always do fun facts so whenever you meet a new person at Airbnb they're always like what's your fun fact",
    "start": "45660",
    "end": "51360"
  },
  {
    "text": "so my fun fact is I've stayed at like 12 different air bee-bees I was twelve",
    "start": "51360",
    "end": "56640"
  },
  {
    "text": "different countries with Airbnb also my team specifically the SRE team does",
    "start": "56640",
    "end": "61800"
  },
  {
    "text": "boring facts so the boring facts are always just something really lame like probably I drink coffee this morning",
    "start": "61800",
    "end": "67260"
  },
  {
    "text": "which is actually true I wrote made the slide a while ago so and also since I - already once again",
    "start": "67260",
    "end": "73860"
  },
  {
    "text": "the last incident that I caused was in July of this year which is actually also incidentally because of service",
    "start": "73860",
    "end": "79439"
  },
  {
    "text": "discovery so what kind of we might cover yeah we'll cover in the migration section spoiler alert so this is kind of",
    "start": "79439",
    "end": "88799"
  },
  {
    "text": "the agenda today these are like the four main things I'll be talking about we've talked about service discovery we were",
    "start": "88799",
    "end": "94110"
  },
  {
    "text": "getting about scaling and why scaling is hard also my migrating and why migrating is",
    "start": "94110",
    "end": "99540"
  },
  {
    "text": "hard as well as like all the engineering work that goes with it and as we all know that like engineering is hard",
    "start": "99540",
    "end": "105000"
  },
  {
    "text": "otherwise we wouldn't get paid right so let's kind of go back to the beginning",
    "start": "105000",
    "end": "111119"
  },
  {
    "start": "108000",
    "end": "362000"
  },
  {
    "text": "the beginning of service discovery at Airbnb so initially what we came up with us as",
    "start": "111119",
    "end": "116579"
  },
  {
    "text": "we came with a smart stack so this is an open source project that we created for a distributed service discovery",
    "start": "116579",
    "end": "123079"
  },
  {
    "text": "framework and here we're gonna dive into some of the parts just so I Reich is like a general idea of what what's",
    "start": "123079",
    "end": "129239"
  },
  {
    "text": "happening here but it's made up of a few different parts here nerve so nerve is actually",
    "start": "129239",
    "end": "135130"
  },
  {
    "text": "execute aunt on the host of the service that you want to connect to it's really",
    "start": "135130",
    "end": "141370"
  },
  {
    "text": "just an advertiser it's really broadcasting where you're at and it broadcasts this - zookeeper nerve also",
    "start": "141370",
    "end": "148570"
  },
  {
    "text": "does a simple health check to the local service before it broadcasts this information so if you're running on",
    "start": "148570",
    "end": "154180"
  },
  {
    "text": "silicon or bare metal it will actually just like you can configure your health check to see like is the local",
    "start": "154180",
    "end": "160090"
  },
  {
    "text": "application running and if it is running it will actually register this service - zookeeper after we broadcast the",
    "start": "160090",
    "end": "168730"
  },
  {
    "text": "information now a client wants to talk to a service so the way we a client can connect to services via synapse so soon",
    "start": "168730",
    "end": "175270"
  },
  {
    "text": "UPS is actually a Zagat so synapse is a",
    "start": "175270",
    "end": "180880"
  },
  {
    "text": "service that runs on the on the client host and it pulls the information about",
    "start": "180880",
    "end": "186520"
  },
  {
    "text": "all the backends that it wants to connect to from zookeeper and from there it actually rewrites each a proxy config",
    "start": "186520",
    "end": "193270"
  },
  {
    "text": "so we actually use proxy a proxy for Howey load balance and distribute our",
    "start": "193270",
    "end": "199150"
  },
  {
    "text": "traffic to all the backend hosts and once again this is it this is run on the client machine or the client host and",
    "start": "199150",
    "end": "205500"
  },
  {
    "text": "zookeeper so this is kind of part of our control plane where a zookeeper is the data store back in that is backing our",
    "start": "205500",
    "end": "212230"
  },
  {
    "text": "control plane but everything else is actually distributed it's decentralized in a sense and also how do we configure",
    "start": "212230",
    "end": "220660"
  },
  {
    "text": "this how do we well this is more of how we used to configure it whenever we were running on ec2 we would actually use",
    "start": "220660",
    "end": "226720"
  },
  {
    "text": "chef so we use chef or our hardware orchestration and we were to configure configure both nerve and synapse in chef",
    "start": "226720",
    "end": "234220"
  },
  {
    "text": "and a lot of this configuration could be customized within chef and whenever L",
    "start": "234220",
    "end": "239890"
  },
  {
    "text": "host was converged it would pick up the new configurations and the the",
    "start": "239890",
    "end": "245350"
  },
  {
    "text": "configuration you can see here would be things that like if you want to configure specific things in a chat proxy like time to keep alive or any of",
    "start": "245350",
    "end": "253570"
  },
  {
    "text": "other they're like proxy configurations you could go there especially even if you wanted to like change like your load balancing algorithm you could also go",
    "start": "253570",
    "end": "259600"
  },
  {
    "text": "there as well I'm on and on the nurse website you can actually configure how you're doing the local health checks",
    "start": "259600",
    "end": "264880"
  },
  {
    "text": "because all its really doing is something kind of really naive it's like well let me ping an endpoint my local",
    "start": "264880",
    "end": "270130"
  },
  {
    "text": "endpoint what port do I ping in my opinion or am I going to slash health but all these configurations are stored",
    "start": "270130",
    "end": "277000"
  },
  {
    "text": "in chef itself so this is all like",
    "start": "277000",
    "end": "282330"
  },
  {
    "text": "framed in the in the ec2 slash like bare metal world so kind of moving forward we",
    "start": "282330",
    "end": "289720"
  },
  {
    "text": "wanted to our infrastructure was migrating from easy to to kubernetes so",
    "start": "289720",
    "end": "295780"
  },
  {
    "text": "a lot of them the work kind of go back to that point of ownership is a lot of the kubernetes work was actually not",
    "start": "295780",
    "end": "301060"
  },
  {
    "text": "done by SRE team it was originally owned by the surface orchestration team this",
    "start": "301060",
    "end": "306160"
  },
  {
    "text": "this also predates our current traffic team so a lot of this work the first",
    "start": "306160",
    "end": "312400"
  },
  {
    "text": "drafts of it were done by people that are are not unnecessary or not even owned by on the traffic team which now",
    "start": "312400",
    "end": "319180"
  },
  {
    "text": "owned this so moving the kubernetes world how do we get all our service",
    "start": "319180",
    "end": "325210"
  },
  {
    "text": "discovery to work we containerize everything right we're just going to throw all the applications and all the processes into different",
    "start": "325210",
    "end": "332260"
  },
  {
    "text": "containers so this was kind of what we expected to happen all right we expected",
    "start": "332260",
    "end": "337690"
  },
  {
    "text": "that like now that we're in a containerized world we're going to wrap the application in container we're gonna wrap nerve in the container we're gonna",
    "start": "337690",
    "end": "344110"
  },
  {
    "text": "wrap synapse in the container we're gonna wrap H a proxy in a container and that's gonna and it's",
    "start": "344110",
    "end": "349450"
  },
  {
    "text": "gonna like just work right unfortunately that's not one enough ended up happening this is more of a closer reality of to",
    "start": "349450",
    "end": "356260"
  },
  {
    "text": "how our current framework works and we're gonna dive into each of these individual parts here so first mini",
    "start": "356260",
    "end": "363610"
  },
  {
    "start": "362000",
    "end": "512000"
  },
  {
    "text": "announcer this actually is the replacement for nerve so nerve was how we were initially announcing how our",
    "start": "363610",
    "end": "371020"
  },
  {
    "text": "services or broadcasting the availability and where our services are located so now we actually replace it",
    "start": "371020",
    "end": "377919"
  },
  {
    "text": "with mini announcer mini announcer does nerve remember was very naive it",
    "start": "377919",
    "end": "383229"
  },
  {
    "text": "basically like looked at the local hosts and they knew how to help check and ping the local hosts in the local application",
    "start": "383229",
    "end": "388570"
  },
  {
    "text": "so now mini announcer as we move to kubernetes we can't just check that",
    "start": "388570",
    "end": "393919"
  },
  {
    "text": "container anymore we actually want to make sure that all the other containers are up so the way many announcer works",
    "start": "393919",
    "end": "399169"
  },
  {
    "text": "is that it actually pulls the k8 api and it says it waits for all the other",
    "start": "399169",
    "end": "406490"
  },
  {
    "text": "containers to be ready before it actually broadcasted on top of that the way IP addresses and ports worker is",
    "start": "406490",
    "end": "413870"
  },
  {
    "text": "also different in kubernetes so it actually gets that information from the cube api as well and then it also",
    "start": "413870",
    "end": "420499"
  },
  {
    "text": "broadcast us back to zookeeper one of the other things to note is that before hand a lot of the port configuration in",
    "start": "420499",
    "end": "426770"
  },
  {
    "text": "nerve was like very static and chef so if you are launching a service that",
    "start": "426770",
    "end": "432830"
  },
  {
    "text": "needed to be communicated to you would always be on the same port and chef made this like naive assumption and nerve",
    "start": "432830",
    "end": "438110"
  },
  {
    "text": "made this naive assumption so actually when we move to the mini announcer we also had to update a lot of this so that",
    "start": "438110",
    "end": "444620"
  },
  {
    "text": "it broadcasts the port and that way synapse could also read from the port because this assumption is like incorrect now because whenever you're",
    "start": "444620",
    "end": "450259"
  },
  {
    "text": "connecting to kubernetes a kubernetes container the port is also can change as",
    "start": "450259",
    "end": "456560"
  },
  {
    "text": "well at based on like how your application gets scheduled on the container on top of that so then it also",
    "start": "456560",
    "end": "465889"
  },
  {
    "text": "publishes the IP and all the port but mini answer we also use for keeping the",
    "start": "465889",
    "end": "471919"
  },
  {
    "text": "service discovery and like turning off the lights correctly in our application as we get me scheduled so if you turn",
    "start": "471919",
    "end": "478339"
  },
  {
    "text": "off if you reschedule a part or if a pod is scheduled for removal sometimes it",
    "start": "478339",
    "end": "483979"
  },
  {
    "text": "would actually just terminate the pod and we get a bunch of 500s because either the application was still trying",
    "start": "483979",
    "end": "489949"
  },
  {
    "text": "to communicate or the the request had not completed at all so we actually do a pre a pre stop hook in our mini",
    "start": "489949",
    "end": "497509"
  },
  {
    "text": "announcer to make sure that all of our connections closed correctly so our application starts starts closing down",
    "start": "497509",
    "end": "502879"
  },
  {
    "text": "and then mini announcer also starts closing down before before the containers actually just removed all",
    "start": "502879",
    "end": "509089"
  },
  {
    "text": "together so we're going to dive into now we have the",
    "start": "509089",
    "end": "516469"
  },
  {
    "start": "512000",
    "end": "663000"
  },
  {
    "text": "on way container so the on void container was actually so this is done in part with service orchestration as",
    "start": "516470",
    "end": "524000"
  },
  {
    "text": "well as security so one of the things that moving to scooper Nettie is a security wanting to get their foot in the door on getting em TLS to work so M",
    "start": "524000",
    "end": "531680"
  },
  {
    "text": "TLS required us to also have an ingress proxy so we started using envoy for our",
    "start": "531680",
    "end": "537020"
  },
  {
    "text": "ingress proxy here and it was out like out of the box ingress proxy we we were using it for",
    "start": "537020",
    "end": "545080"
  },
  {
    "text": "pulling configs and like how are configuring the local cluster the local",
    "start": "545080",
    "end": "550430"
  },
  {
    "text": "application and a white listing all traffic to the local application the way",
    "start": "550430",
    "end": "555950"
  },
  {
    "text": "that we set up envoy is like slightly different than a lot of other companies set up envoy so we actually use what we",
    "start": "555950",
    "end": "563180"
  },
  {
    "text": "call SDSU which is actually gonna be this next side card that we're talking about so SDS GM is essentially a shim",
    "start": "563180",
    "end": "570020"
  },
  {
    "text": "container that wraps the XDS XDS",
    "start": "570020",
    "end": "575960"
  },
  {
    "text": "interface that envoy uses to poll for the all the cluster information in the",
    "start": "575960",
    "end": "581000"
  },
  {
    "text": "end point in point information as well as I could listen to information we actually it's we call it an SDS gym",
    "start": "581000",
    "end": "587840"
  },
  {
    "text": "because Indy wine it was actually SDS before v2 they changed everything to EDS and LDS and all the other discovery",
    "start": "587840",
    "end": "595040"
  },
  {
    "text": "services on top of that SDS JM is actually made to merge multiple",
    "start": "595040",
    "end": "600050"
  },
  {
    "text": "configurations from different sources so on top of like being able to pull information from zookeeper which is",
    "start": "600050",
    "end": "605660"
  },
  {
    "text": "where we're keeping all of our data about all of our backends and all of our listeners and as well as all of our configs it can actually read from",
    "start": "605660",
    "end": "611930"
  },
  {
    "text": "another local or other file stores to actually merge these configurations",
    "start": "611930",
    "end": "617630"
  },
  {
    "text": "before it feeds it up to on voice so this is actually how we get our in TLS to work is that we're talking about the",
    "start": "617630",
    "end": "623930"
  },
  {
    "text": "next container the MPLS container and it merges that information with the zookeeper information so that way we get",
    "start": "623930",
    "end": "630380"
  },
  {
    "text": "the ingress information or lease the ingress services that are actually allowed to talk to the services so like",
    "start": "630380",
    "end": "638870"
  },
  {
    "text": "I was talking about our MPLS sinker so basically this is just security to develop this and it's on by all",
    "start": "638870",
    "end": "645720"
  },
  {
    "text": "for all new kubernetes services you they just get this out of the box whenever they create a new application in TLS is automatically turned on and then it just",
    "start": "645720",
    "end": "653220"
  },
  {
    "text": "pulls some basic configs from s3 that another application actually uploads to",
    "start": "653220",
    "end": "658320"
  },
  {
    "text": "it for what services can talk to to what other services so as we're moving to",
    "start": "658320",
    "end": "666180"
  },
  {
    "start": "663000",
    "end": "754000"
  },
  {
    "text": "kubernetes we're still maintaining the smart stack and now we actually have new containers but we know that like nothing",
    "start": "666180",
    "end": "672780"
  },
  {
    "text": "as we were growing over time that these things weren't scaling well so we're",
    "start": "672780",
    "end": "678600"
  },
  {
    "text": "gonna talk about here is that kind of these points then how service discovery started to fall over essentially as we",
    "start": "678600",
    "end": "684630"
  },
  {
    "text": "were maintaining it so every view is growing over the years the number of hosts number of guests are growing the",
    "start": "684630",
    "end": "690300"
  },
  {
    "text": "amount of traffic is growing and as we do this like we have to scale up a lot of our hosts scalloping scaling up our back ends as",
    "start": "690300",
    "end": "698130"
  },
  {
    "text": "well as scaling out some of our services and as a result of these things we ended",
    "start": "698130",
    "end": "703890"
  },
  {
    "text": "up having incidents or issues so it's kind of go back to the smart stack",
    "start": "703890",
    "end": "708920"
  },
  {
    "text": "example here and we know that the thing that's really important to that we're gonna call out here oh sorry we're gonna",
    "start": "708920",
    "end": "715830"
  },
  {
    "text": "run through a quick demo here where we're going to kind of see what the",
    "start": "715830",
    "end": "721230"
  },
  {
    "text": "effect of scale is as we go forward and like it's really easy to think about these things in retrospect beforehand we",
    "start": "721230",
    "end": "727050"
  },
  {
    "text": "never actually thought we'd get to this kind of scale so more specifically we're actually gonna be talking about the",
    "start": "727050",
    "end": "732990"
  },
  {
    "text": "synapse and H a proxy container so going back synapse generates an H a proxy",
    "start": "732990",
    "end": "739980"
  },
  {
    "text": "config and it reloads H a proxy and HT proxy loads that config and then",
    "start": "739980",
    "end": "745140"
  },
  {
    "text": "restarts it does it does a restart as well as like closing out connections and",
    "start": "745140",
    "end": "750390"
  },
  {
    "text": "like spinning up new connections for all the traffic so this configures the thing",
    "start": "750390",
    "end": "756540"
  },
  {
    "start": "754000",
    "end": "811000"
  },
  {
    "text": "that we kind of care most about but in the idea is that in the config each back in is its own line and as with",
    "start": "756540",
    "end": "764580"
  },
  {
    "text": "each service there's like front ends and there's back ends and each back-end is don't line within this config so how",
    "start": "764580",
    "end": "771480"
  },
  {
    "text": "large is this config when you have like a few backends like 10 backends like probably like 15 20 lines it's nothing",
    "start": "771480",
    "end": "777390"
  },
  {
    "text": "really much to think about as we go like now that we have services that are like a hundred back ends this",
    "start": "777390",
    "end": "782970"
  },
  {
    "text": "config is also getting a larger a thousand back ends and even then if you have a thousand back ends you could also",
    "start": "782970",
    "end": "789270"
  },
  {
    "text": "have multiple services you're talking to so you get actually so you're actually now getting like an M by M effect because it's like you're talking to in",
    "start": "789270",
    "end": "795420"
  },
  {
    "text": "different services and a lot of those in different services have like in different backends so we're seeing these",
    "start": "795420",
    "end": "800640"
  },
  {
    "text": "configs are getting really large and so what is the effect of this so a lot of",
    "start": "800640",
    "end": "805860"
  },
  {
    "text": "this so what ends up happening is that we end up creating a very large config and and we end up hitting memory",
    "start": "805860",
    "end": "812880"
  },
  {
    "start": "811000",
    "end": "899000"
  },
  {
    "text": "exhaustion so the memory exhaustion actually comes from the fact that H a proxy has to take time to now load this",
    "start": "812880",
    "end": "818970"
  },
  {
    "text": "convey we're actually doing two things we're writing the config and we're loading the afaik and we're actually reloading the config into memory hu",
    "start": "818970",
    "end": "826800"
  },
  {
    "text": "proxies loading it every into memory every time on top of that since it's trying to persist and like make sure it",
    "start": "826800",
    "end": "832980"
  },
  {
    "text": "closed on all the connections safely at the same time it is using not just like",
    "start": "832980",
    "end": "838110"
  },
  {
    "text": "2x2 memory it can use more than that memory especially if you're doing a rolling deploy we're rolling deploy is",
    "start": "838110",
    "end": "844110"
  },
  {
    "text": "that like you have a thousand backends and you're just doing all the deploys you may be you may have some velocity",
    "start": "844110",
    "end": "849630"
  },
  {
    "text": "around it some protection around like standard deployments but at the same time now you're how long that deploy",
    "start": "849630",
    "end": "854940"
  },
  {
    "text": "takes you're essentially refreshing your backends periodically and you're trying to reload this file you're trying to",
    "start": "854940",
    "end": "860670"
  },
  {
    "text": "reload the site multiply make file lots of times very quickly which ends up actually doing memory exhaustion because",
    "start": "860670",
    "end": "866820"
  },
  {
    "text": "we're trying to write the file we're trying to read the file and then at the same time we're not like giving up all",
    "start": "866820",
    "end": "872310"
  },
  {
    "text": "of our threads 80 procs is not giving all the threads back so now we actually hit in memory exhaustion so you can so",
    "start": "872310",
    "end": "878610"
  },
  {
    "text": "these graphs are just examples of things that we'd see during deploys so during this report we would see essentially a",
    "start": "878610",
    "end": "883950"
  },
  {
    "text": "spike in memory usage on the in host from and additionally we would just with",
    "start": "883950",
    "end": "890820"
  },
  {
    "text": "it take two to five milliseconds or two to five seconds in some cases depending on the size of the backends and the size",
    "start": "890820",
    "end": "897570"
  },
  {
    "text": "of the deploys themselves and then on top of that a lot of these services as",
    "start": "897570",
    "end": "903120"
  },
  {
    "start": "899000",
    "end": "953000"
  },
  {
    "text": "they're restarting they're actually we were seeing a lot of network saturation here too we're seeing the saturation",
    "start": "903120",
    "end": "908640"
  },
  {
    "text": "just because as things come up we're also health checking it so we're not we're not optimistic so",
    "start": "908640",
    "end": "914660"
  },
  {
    "text": "whenever a service gets registered in zookeeper synapse before it's a cheap",
    "start": "914660",
    "end": "920490"
  },
  {
    "text": "proxy and synapse before they send traffic to that service they do a quick health check so now we just have like a thundering word problem where we're just",
    "start": "920490",
    "end": "926610"
  },
  {
    "text": "like DDoS taking our own services and our own network which is get creating like propagation delays and a lot of",
    "start": "926610",
    "end": "932880"
  },
  {
    "text": "this propagation delays are not great especially when it's in your service discovery because you end up either",
    "start": "932880",
    "end": "938130"
  },
  {
    "text": "trying to connect a stell host or you're connecting to a host that what you're trying to connect way sorry if you're",
    "start": "938130",
    "end": "944670"
  },
  {
    "text": "connect you can either end up connecting to stale host or it just may like not have any hosts at all because they",
    "start": "944670",
    "end": "950310"
  },
  {
    "text": "haven't come up in time because it's trying to reload some of these things so",
    "start": "950310",
    "end": "955560"
  },
  {
    "text": "what so did we end up doing so to actually fix some of this problem so because we maintain this and because we",
    "start": "955560",
    "end": "960750"
  },
  {
    "text": "do this we own smart sack and we own the internal application of smart start we",
    "start": "960750",
    "end": "966959"
  },
  {
    "text": "fixed it by to prompt like two different ways one thing that we did was they actually started moving things away from",
    "start": "966959",
    "end": "972390"
  },
  {
    "text": "smart stack or yeah we started moving things away from agent proxy specifically we actually took the SDS",
    "start": "972390",
    "end": "978990"
  },
  {
    "text": "shim and our way that we were working with and we actually put it back to ec2 and so now we're using envoy one of the",
    "start": "978990",
    "end": "985770"
  },
  {
    "text": "big benefits that we're getting from envoy is that we're not actually having to so SDS shim it will reload the",
    "start": "985770",
    "end": "993480"
  },
  {
    "text": "information and just give it to envoy periodically and it has some of these built-in protections to prevent us from",
    "start": "993480",
    "end": "1000190"
  },
  {
    "text": "some of the stale data and some of these other things via either outlier detection or some of the other real",
    "start": "1000190",
    "end": "1007540"
  },
  {
    "text": "resiliency features that are built into envoy we're actually saving us from some of these things on top of that envoy",
    "start": "1007540",
    "end": "1012680"
  },
  {
    "text": "actually had a smaller memory footprint because it wasn't trying to reload everything so so quickly because we were",
    "start": "1012680",
    "end": "1019700"
  },
  {
    "text": "actually forced reloading a chip proxy so because envoy does this periodic polling to get new information it wasn't",
    "start": "1019700",
    "end": "1025938"
  },
  {
    "text": "considered consuming nearly as much memory and on top of that we ended up",
    "start": "1025939",
    "end": "1032780"
  },
  {
    "text": "charting some of the roles as well we ended up starting some of the roles in shifts so we had a lot of these large",
    "start": "1032780",
    "end": "1038089"
  },
  {
    "text": "backends a lot of the services talked to a lot of these backends so we ended up doing is we try to figure out like",
    "start": "1038089",
    "end": "1043970"
  },
  {
    "text": "traffic actually needed to talk to what traffic so a lot of these services were actually very monolithic so they did a lot of different things and they scaled",
    "start": "1043970",
    "end": "1050659"
  },
  {
    "text": "very large so instead we started at cutting it out and saying like are you are you like a pathway so are you like a",
    "start": "1050659",
    "end": "1056960"
  },
  {
    "text": "proxy like a DB proxy service are you just like proxying through another service like what are you actually doing",
    "start": "1056960",
    "end": "1063020"
  },
  {
    "text": "and I cutting these a host down so that the back number of backends that these hosts have it's much smaller okay so",
    "start": "1063020",
    "end": "1072289"
  },
  {
    "start": "1070000",
    "end": "1133000"
  },
  {
    "text": "like because of this we kind of already hinted at that like we have SDS amen we have envoy so kind of moving forward",
    "start": "1072289",
    "end": "1078679"
  },
  {
    "text": "that kind of gives us to our next phase where as we migrate into kubernetes we're also going to start migrating",
    "start": "1078679",
    "end": "1085340"
  },
  {
    "text": "things to envoy and this has to do with the fact that that memory issue it",
    "start": "1085340",
    "end": "1090950"
  },
  {
    "text": "doesn't go away if you have synapse and H a proxy in containers actually makes the problem worse because now that you",
    "start": "1090950",
    "end": "1097820"
  },
  {
    "text": "it's really hard because you're requesting the amount of resources in your container and as you request those",
    "start": "1097820",
    "end": "1103970"
  },
  {
    "text": "resources inside the container what ends up happening is that I you it's really",
    "start": "1103970",
    "end": "1110210"
  },
  {
    "text": "hard to configure those correctly because you don't want to over resource these and you don't want to under",
    "start": "1110210",
    "end": "1115460"
  },
  {
    "text": "resource um if you under resource something you're gonna get rescheduled if you over resource them now you're just like remember that these are",
    "start": "1115460",
    "end": "1121010"
  },
  {
    "text": "service discovery and there are like on every in every pot so like you're gonna end up with a lot of resources consumed",
    "start": "1121010",
    "end": "1128090"
  },
  {
    "text": "if you over scale it so the idea was to move the Envoy so this is a quick",
    "start": "1128090",
    "end": "1135530"
  },
  {
    "start": "1133000",
    "end": "1248000"
  },
  {
    "text": "reminder of like essentially breaking down the containers that we have where now we have the Envoy container and the",
    "start": "1135530",
    "end": "1142610"
  },
  {
    "text": "big thing here is that we are going to be adding our ingress having sorry egress traffic as well so beforehand it",
    "start": "1142610",
    "end": "1149059"
  },
  {
    "text": "was mostly set up for ingress traffic but now we're actually going to start using a for egress traffic as well because we have most of the framework",
    "start": "1149059",
    "end": "1155330"
  },
  {
    "text": "and most of the bare-bones set up to actually use this for the egress so one",
    "start": "1155330",
    "end": "1161659"
  },
  {
    "text": "thing that we want to talk about real quick touch on here is that you probably see this in some of the other Airbnb",
    "start": "1161659",
    "end": "1167330"
  },
  {
    "text": "presentations or you heard a mention but we have a tool called coop Jen I think a lot of other companies have a similar",
    "start": "1167330",
    "end": "1173659"
  },
  {
    "text": "tooling where we essentially use this on top of like coupe carnal where we can generate our",
    "start": "1173659",
    "end": "1180830"
  },
  {
    "text": "configs it takes some of our api internal configs and it puts it in and it transforms it into like kubernetes",
    "start": "1180830",
    "end": "1187250"
  },
  {
    "text": "configs which then you can deploy straight to kubernetes so the idea is",
    "start": "1187250",
    "end": "1192410"
  },
  {
    "text": "that like the developer itself doesn't actually have to know all the kubernetes configs because they just know some of",
    "start": "1192410",
    "end": "1197630"
  },
  {
    "text": "the basic configs and then it will generate all the containers and all the configs and all the stall you met it",
    "start": "1197630",
    "end": "1203960"
  },
  {
    "text": "needs to push the kubernetes and so this is kind of important because a lot of this going back to all the containers a",
    "start": "1203960",
    "end": "1210800"
  },
  {
    "text": "lot of this was done by service orchestration service orchestration owns couvert kujan so this work was actually",
    "start": "1210800",
    "end": "1217130"
  },
  {
    "text": "done by the service orchestration team so now we also have to control plain",
    "start": "1217130",
    "end": "1223010"
  },
  {
    "text": "containers and so how do we decide who's actually going to whit's proxy to use on",
    "start": "1223010",
    "end": "1229970"
  },
  {
    "text": "in grocery on your egress traffic so actually introduced a new container here",
    "start": "1229970",
    "end": "1236200"
  },
  {
    "text": "we we Inc we introduced the SD configurator so this is actually in a",
    "start": "1236200",
    "end": "1241400"
  },
  {
    "text": "NIC container so it doesn't actually run as a sidecar it runs on an it time and it makes in it time decisions so it",
    "start": "1241400",
    "end": "1249050"
  },
  {
    "text": "makes the ploy time decisions and it's an easy way for us to get generate the bootstrap configs from now both either",
    "start": "1249050",
    "end": "1255980"
  },
  {
    "text": "synapse or for STS gem which actually creates a problem for us in the future",
    "start": "1255980",
    "end": "1261470"
  },
  {
    "text": "because now we have another source of truth here but as with the with the SD",
    "start": "1261470",
    "end": "1268640"
  },
  {
    "text": "configurator we're actually talking it will configure the H a proxy as well as",
    "start": "1268640",
    "end": "1275530"
  },
  {
    "text": "Envoy via synapse and SCS gem and it will also make a runtime decision for",
    "start": "1275530",
    "end": "1282110"
  },
  {
    "text": "all these things on schedule it has some deterministic randomness in it in that",
    "start": "1282110",
    "end": "1287450"
  },
  {
    "text": "we actually use the pods name to to create a hash and that will actually we",
    "start": "1287450",
    "end": "1293990"
  },
  {
    "text": "can introduce probabilities here so we actually introduce probabilities of like if you have a 50% chance of be using",
    "start": "1293990",
    "end": "1300830"
  },
  {
    "text": "Envoy versus Envoy versus H a proxy if you have the same pod name and you get",
    "start": "1300830",
    "end": "1307280"
  },
  {
    "text": "rescheduled you're always going to have the same that results like if you get rescheduled under the",
    "start": "1307280",
    "end": "1312890"
  },
  {
    "text": "same name it will also have the same it would have the same result and this is this was mostly in effect so that way we",
    "start": "1312890",
    "end": "1318679"
  },
  {
    "text": "could do for reproducibility so that way we could determine like how many how many services were actually using a chat",
    "start": "1318679",
    "end": "1325010"
  },
  {
    "text": "proxy how many were using envoy and we can good we can get a reproducible value",
    "start": "1325010",
    "end": "1330230"
  },
  {
    "text": "under those two and also the biggest thing here is that the reason we also did it that way is that we were afraid",
    "start": "1330230",
    "end": "1335809"
  },
  {
    "text": "that if pods were to get if for some reason on well he wasn't working or hte proxy wasn't working it would get",
    "start": "1335809",
    "end": "1342500"
  },
  {
    "text": "rescheduled you know in some of the pods can actually be rescheduled if we're not careful and Twitter's like all on white",
    "start": "1342500",
    "end": "1349580"
  },
  {
    "text": "because H a proxy is felling or all of H a proxy because our envoy is felling because those sidecar chickens are felling it will like crash back off loop",
    "start": "1349580",
    "end": "1356929"
  },
  {
    "text": "and then it'll reschedule and then it'll actually pick up a different config so we're trying to do this in a way that we can actually reproduce some of this",
    "start": "1356929",
    "end": "1363380"
  },
  {
    "text": "information so on top of that so now this is all these are kind of all the",
    "start": "1363380",
    "end": "1368660"
  },
  {
    "text": "containers that we have which is a lot going forward but it like to actually",
    "start": "1368660",
    "end": "1375620"
  },
  {
    "text": "take a step back remember that in the beginning we're talking about those configs that how you config synapse and",
    "start": "1375620",
    "end": "1381860"
  },
  {
    "text": "how you config nerve so we've been putting those in chef and so what happens these can fix now right so the",
    "start": "1381860",
    "end": "1388700"
  },
  {
    "start": "1383000",
    "end": "1420000"
  },
  {
    "text": "problem is that we want to keep these configs or we customers like me these",
    "start": "1388700",
    "end": "1393830"
  },
  {
    "text": "configs or I'm gonna say like quote-unquote need these configs and don't reason I'm gonna say that is",
    "start": "1393830",
    "end": "1399500"
  },
  {
    "text": "because over time a lot of the config in chef were kind of copy pasta and iooks",
    "start": "1399500",
    "end": "1405020"
  },
  {
    "text": "lightly modified and people that modified them may have left and they may have modified them for reasons whether",
    "start": "1405020",
    "end": "1410750"
  },
  {
    "text": "we know that or not or the services service owners may have modified it for other reasons but we need to keep those",
    "start": "1410750",
    "end": "1416650"
  },
  {
    "text": "configs around so what do we do with them so we actually modified synapse we",
    "start": "1416650",
    "end": "1423770"
  },
  {
    "text": "modified nerve we modified STS ship and some of and actually we have our own innate container as well in kubernetes",
    "start": "1423770",
    "end": "1432740"
  },
  {
    "text": "for services to publish these configs so actually on a chef converge it will",
    "start": "1432740",
    "end": "1438020"
  },
  {
    "text": "actually upload this config to zookeeper in kubernetes on a deploy it will upload",
    "start": "1438020",
    "end": "1444930"
  },
  {
    "text": "the config - zookeeper as well and so then on top of that we modified synapse and STS shim to actually be able to pull",
    "start": "1444930",
    "end": "1452070"
  },
  {
    "text": "these configs from zookeeper so that way we can actually propagate it that way",
    "start": "1452070",
    "end": "1457290"
  },
  {
    "text": "instead of having to rely on converges but now now that we've done that the",
    "start": "1457290",
    "end": "1463770"
  },
  {
    "start": "1460000",
    "end": "1470000"
  },
  {
    "text": "question is was that a good idea was it actually a good idea to put all these configs in zookeeper so going back to",
    "start": "1463770",
    "end": "1472170"
  },
  {
    "start": "1470000",
    "end": "1482000"
  },
  {
    "text": "our example here we're going to talk about same thing when service lots of backends this could be now that we have",
    "start": "1472170",
    "end": "1479010"
  },
  {
    "text": "like pods it actually grows as well but now we're talking about how we're",
    "start": "1479010",
    "end": "1485190"
  },
  {
    "start": "1482000",
    "end": "1584000"
  },
  {
    "text": "talking to zookeeper so we're actually so we talked to zookeeper we could actually be pushing a lot of information",
    "start": "1485190",
    "end": "1490740"
  },
  {
    "text": "here and so if we have if you're pulling configs from one host or from one back",
    "start": "1490740",
    "end": "1495930"
  },
  {
    "text": "end each back in is pulling it ten times from zookeeper what if you have 100",
    "start": "1495930",
    "end": "1501090"
  },
  {
    "text": "backends these were all requested zookeeper 100 early 2000 and then like",
    "start": "1501090",
    "end": "1506520"
  },
  {
    "text": "ten thousand right what ends up happening is that we have two that we",
    "start": "1506520",
    "end": "1511590"
  },
  {
    "text": "end up like consuming a lot of network bandwidth here and like whenever we see the poised we also see the packet rate",
    "start": "1511590",
    "end": "1518670"
  },
  {
    "text": "on ZK our zookeeper cluster actually go up so this is actually one of the",
    "start": "1518670",
    "end": "1524670"
  },
  {
    "text": "metrics that way so this is actually the metric from that previous graph that we're actually monitoring so we actually have an alert on this so what was",
    "start": "1524670",
    "end": "1531390"
  },
  {
    "text": "happening was that the traffic team was getting this alert but they weren't exactly sure what it was and this has to go to the fact that like sre said a",
    "start": "1531390",
    "end": "1537900"
  },
  {
    "text": "zookeeper up sre like set up this alert and then we kind of threw it over the",
    "start": "1537900",
    "end": "1543300"
  },
  {
    "text": "shelf - like traffic team and they were like oh this looks firing it can't be that bad right so this is kind of this",
    "start": "1543300",
    "end": "1549480"
  },
  {
    "text": "was the alert message that we set up with it as you can see here that we're kind of talking about what's going on",
    "start": "1549480",
    "end": "1556350"
  },
  {
    "text": "here we're seeing an increase in packet rate - zookeeper and we also know that",
    "start": "1556350",
    "end": "1561890"
  },
  {
    "text": "it kind of tells us like how many packets that we think is like okay they saw it like",
    "start": "1561890",
    "end": "1568360"
  },
  {
    "text": "our own like fun research via failure we don't know that like in ec2 that we're",
    "start": "1568360",
    "end": "1575269"
  },
  {
    "text": "going to see we're gonna see these these limits and but it once when we do vbc we actually see higher limits the problem",
    "start": "1575269",
    "end": "1581389"
  },
  {
    "text": "was is that we're actually starting to hit some of these other invisible limits so what actually happens whenever we see",
    "start": "1581389",
    "end": "1587690"
  },
  {
    "start": "1584000",
    "end": "1663000"
  },
  {
    "text": "the these network saturation or these like packet rate limits we actually see",
    "start": "1587690",
    "end": "1593379"
  },
  {
    "text": "we see request queuing which is like not good because now you're you doing service recovery even though we increase",
    "start": "1593379",
    "end": "1600649"
  },
  {
    "text": "the a lot of that work was done for to propagate configs like now though we're pushing more data every time and we're",
    "start": "1600649",
    "end": "1607100"
  },
  {
    "text": "actually pulling data we're actually going to see a lot more request queuing which means you can see propagation",
    "start": "1607100",
    "end": "1612350"
  },
  {
    "text": "delays once again in your service discovery which we know is like not great I'm on top of that you're gonna",
    "start": "1612350",
    "end": "1618679"
  },
  {
    "text": "see this this gives you like delayed a config updates come basically all the",
    "start": "1618679",
    "end": "1623779"
  },
  {
    "text": "same problems that we saw before we're like back in hosts aren't gonna be there you're gonna see five hundred erratically you're not really sure and",
    "start": "1623779",
    "end": "1630889"
  },
  {
    "text": "then in the problem is that service owners are seeing this and it makes it really hard to cool it was for a while",
    "start": "1630889",
    "end": "1639200"
  },
  {
    "text": "it was really hard to correlate it because the traffic team was seeing this alert and they weren't really sure what was happening but then like certain",
    "start": "1639200",
    "end": "1645409"
  },
  {
    "text": "services would do deploys or they or downstream would actually deploy it it was more of the downstream service",
    "start": "1645409",
    "end": "1651379"
  },
  {
    "text": "deploys affecting the clients and they'd be like I don't know why but I just got like an increase in 504 like no reason",
    "start": "1651379",
    "end": "1656960"
  },
  {
    "text": "or an increase in latency for no reason and they'd be very confused so eventually we had to connect the dots",
    "start": "1656960",
    "end": "1663049"
  },
  {
    "start": "1663000",
    "end": "1683000"
  },
  {
    "text": "and we got back to it and then we figured out that like like SRS kind of",
    "start": "1663049",
    "end": "1669529"
  },
  {
    "text": "stepped in and we were talking about what is this what is this actually made and was that you just necessary it was like traffic sre service orchestration",
    "start": "1669529",
    "end": "1676000"
  },
  {
    "text": "because like they're also dealing with the kubernetes side of this so digging",
    "start": "1676000",
    "end": "1681529"
  },
  {
    "text": "to the learn more it's like easy we just like upgrade the host right and so previously we're talking about",
    "start": "1681529",
    "end": "1689419"
  },
  {
    "start": "1683000",
    "end": "1700000"
  },
  {
    "text": "like there's like 200 to 250 like thousand packets per second by upgrading the house we were able to get a",
    "start": "1689419",
    "end": "1696049"
  },
  {
    "text": "throughput of 400,000 QPS by changing the ECT OS that were running",
    "start": "1696049",
    "end": "1701509"
  },
  {
    "start": "1700000",
    "end": "1732000"
  },
  {
    "text": "but it was like not an easy migration because we're maintaining our own",
    "start": "1701509",
    "end": "1706590"
  },
  {
    "text": "zookeeper class our own zookeeper cluster here it actually took a couple",
    "start": "1706590",
    "end": "1711690"
  },
  {
    "text": "months to upgrade mainly because we set it up forever ago and it's one of those things that like nobody touches and",
    "start": "1711690",
    "end": "1718139"
  },
  {
    "text": "because nobody touches it nobody really knew how to do a fool like they knew how to do like single host replacement but",
    "start": "1718139",
    "end": "1723210"
  },
  {
    "text": "nobody actually knew how to do it like an observer and a leader or node replacement so this actually took a lot",
    "start": "1723210",
    "end": "1728999"
  },
  {
    "text": "of like two to three months of engineering time to do on top of that that dinner on top of that we actually",
    "start": "1728999",
    "end": "1736529"
  },
  {
    "start": "1732000",
    "end": "1877000"
  },
  {
    "text": "tried solving the problem a few other ways we actually because what that really did is that just bought us",
    "start": "1736529",
    "end": "1741539"
  },
  {
    "text": "Headroom really because like we know that we're increasing the packets per second we know that we're doing all these things but in reality we're just",
    "start": "1741539",
    "end": "1748259"
  },
  {
    "text": "like buying us more time because as we know that we're like scaling we're growing like you go from 250 selling to",
    "start": "1748259",
    "end": "1754379"
  },
  {
    "text": "like 400 sitting not like you essentially put yourself on a timer so to kind of get us out from under that we",
    "start": "1754379",
    "end": "1759989"
  },
  {
    "text": "actually tried we actually made a lot of the different solutions as well to",
    "start": "1759989",
    "end": "1765059"
  },
  {
    "text": "essentially get us out of this mess so one of those things is actually to start",
    "start": "1765059",
    "end": "1770279"
  },
  {
    "text": "migrating some of these services away from was--let list so basically we're also doing this again we're like",
    "start": "1770279",
    "end": "1775559"
  },
  {
    "text": "figuring out what things are still really large and causing us problems they have a lot of backends that are causing these propagation delays and",
    "start": "1775559",
    "end": "1781859"
  },
  {
    "text": "like storming our suit keeper and we actually tried breaking them up I'm on top of that we actually inside of nerve",
    "start": "1781859",
    "end": "1788399"
  },
  {
    "text": "and synapse we actually added ZK whenever we talked to CK we're actually doing some data encoding as well so",
    "start": "1788399",
    "end": "1795509"
  },
  {
    "text": "those configs that we're talking about those the backends that were talking about we're actually doing encoding now so no we're not sitting like raw like",
    "start": "1795509",
    "end": "1801899"
  },
  {
    "text": "strings across anymore now we're actually sending out like serialize and deserialize data to help optimize the",
    "start": "1801899",
    "end": "1807809"
  },
  {
    "text": "packet number on top of that we actually found that there was a slight bug I",
    "start": "1807809",
    "end": "1813659"
  },
  {
    "text": "don't know if it's a bug or how we want to classify it but there's an D optimization in inside synapse as well",
    "start": "1813659",
    "end": "1820070"
  },
  {
    "text": "where essentially we would be reading we'd be traversing the tree which means",
    "start": "1820070",
    "end": "1825450"
  },
  {
    "text": "that we would say like give me this node give me this then children of this nodes now let me read all the children of this",
    "start": "1825450",
    "end": "1831359"
  },
  {
    "text": "nodes which actually gave us a lot like seven or eight round trips depending on what",
    "start": "1831359",
    "end": "1836909"
  },
  {
    "text": "the request was but actually in the library that we're using there is actually just like get like children and",
    "start": "1836909",
    "end": "1842999"
  },
  {
    "text": "so by actually optimizing that they actually gave us the most Headroom because now we're not making accelerate",
    "start": "1842999",
    "end": "1848399"
  },
  {
    "text": "round trips just to do some of these things we're actually getting it all in one roundtrip and then on top of that we we added a",
    "start": "1848399",
    "end": "1855029"
  },
  {
    "text": "mix of things like jitter and retry mechanics as well as self throttling so that way we're actually not just going",
    "start": "1855029",
    "end": "1861480"
  },
  {
    "text": "to start storming ourselves so storming our CK as well okay so now that we've",
    "start": "1861480",
    "end": "1868320"
  },
  {
    "text": "talked about the issues that we've had dealing with service discovery as well",
    "start": "1868320",
    "end": "1873990"
  },
  {
    "text": "as like scaling zookeeper and all the stuff like what are we doing here so we",
    "start": "1873990",
    "end": "1880200"
  },
  {
    "start": "1877000",
    "end": "1905000"
  },
  {
    "text": "have envoy we have all these containers but this isn't like where we want to be obviously because we have seven",
    "start": "1880200",
    "end": "1886049"
  },
  {
    "text": "containers up here and that's like way too many containers as we know that like resources are precious and if you have",
    "start": "1886049",
    "end": "1891330"
  },
  {
    "text": "seven containers or six sidecars and one 1/8 container on every deploy or every",
    "start": "1891330",
    "end": "1896460"
  },
  {
    "text": "application you're just like kind of throwing money out the window right like because you're Europe if there's no way",
    "start": "1896460",
    "end": "1902610"
  },
  {
    "text": "you can optimize these for like every service so the idea here is actually",
    "start": "1902610",
    "end": "1908159"
  },
  {
    "start": "1905000",
    "end": "1927000"
  },
  {
    "text": "just to move everything on the Envoy so currently we're actually maintaining both smart SEC as well as like our",
    "start": "1908159",
    "end": "1913980"
  },
  {
    "text": "future STS shim as well as it envoy so",
    "start": "1913980",
    "end": "1919019"
  },
  {
    "text": "we're maintaining both of these like service discovery frameworks on our applications so this is kind of what our",
    "start": "1919019",
    "end": "1925289"
  },
  {
    "text": "current progress is right now and one of the things with so we're migrating and",
    "start": "1925289",
    "end": "1931980"
  },
  {
    "start": "1927000",
    "end": "1960000"
  },
  {
    "text": "part of this is that as we've learned and as traffic is learned as all the teams in every museum structures are the",
    "start": "1931980",
    "end": "1938129"
  },
  {
    "text": "learners they're like if you break service discovery you kind of like screw over everything because if you break a",
    "start": "1938129",
    "end": "1943379"
  },
  {
    "text": "service or discovery the services can't talk to each other and then like nothing works right because like services are",
    "start": "1943379",
    "end": "1949710"
  },
  {
    "text": "inherently meant to talk to each other meant to communicate to each other into inside your your infrastructure but they",
    "start": "1949710",
    "end": "1955019"
  },
  {
    "text": "can't talk to each other then it's just like everybody's gonna have a really bad time so one of the things that we wanted",
    "start": "1955019",
    "end": "1962609"
  },
  {
    "start": "1960000",
    "end": "1980000"
  },
  {
    "text": "to do during these migrations is that we wanted to create transparency to and",
    "start": "1962609",
    "end": "1968440"
  },
  {
    "text": "from service discovery for service owners so what I mean is that like",
    "start": "1968440",
    "end": "1973690"
  },
  {
    "text": "service owners can either not be engaged or if they want to be engaged we give them enough data so that they can't be",
    "start": "1973690",
    "end": "1979030"
  },
  {
    "text": "engaged so one things that we create is we actually create a be dashboards so",
    "start": "1979030",
    "end": "1984970"
  },
  {
    "start": "1980000",
    "end": "2060000"
  },
  {
    "text": "this is this was actually to solve the problem where a lot of times we initially when he started like tests",
    "start": "1984970",
    "end": "1991150"
  },
  {
    "text": "migrating some services a lot of service owners were very hesitant and they",
    "start": "1991150",
    "end": "1996400"
  },
  {
    "text": "weren't sure they were afraid like oh is envoy gonna like increase my p95 is it gonna like increase my error rate like",
    "start": "1996400",
    "end": "2002549"
  },
  {
    "text": "what are all the things that like could go wrong a service discovery and this is mostly on like infrastructure spot",
    "start": "2002549",
    "end": "2008190"
  },
  {
    "text": "mainly because we tried and we had all these scaling problems so there wasn't a lot of trust there so to help rebuild a",
    "start": "2008190",
    "end": "2015360"
  },
  {
    "text": "lot of the stress we built a lot of these dashboards so that way that service owners could actually see what",
    "start": "2015360",
    "end": "2021510"
  },
  {
    "text": "service discovery was working on if they did come in because a lot of times they did come in on and slack can be like did",
    "start": "2021510",
    "end": "2026669"
  },
  {
    "text": "you change something of service discovery because my error rate is is wrong and and like sometimes it would be",
    "start": "2026669",
    "end": "2031919"
  },
  {
    "text": "us but like 95% of the time 99% of time it actually just be like someone pushed back code and now they're like 508",
    "start": "2031919",
    "end": "2038429"
  },
  {
    "text": "because there's an a no pointer exception but like they've come ask us first they wouldn't even try debugging first because they're like oh it must be",
    "start": "2038429",
    "end": "2043770"
  },
  {
    "text": "traffic because they have such a bad track record and I mean they weren't completely wrong but like we want to",
    "start": "2043770",
    "end": "2052230"
  },
  {
    "text": "make it very easy for both of us and for them to determine what really quickly if",
    "start": "2052230",
    "end": "2057388"
  },
  {
    "text": "it was us or if it was them okay on top",
    "start": "2057389",
    "end": "2062550"
  },
  {
    "start": "2060000",
    "end": "2228000"
  },
  {
    "text": "of this we wanted to we still want to take iterate because we're not in our final state here we actually wanted to",
    "start": "2062550",
    "end": "2068340"
  },
  {
    "text": "build in new features or enable new envoy features because we own SDS Jim which actually feeds in this is the XDS",
    "start": "2068340",
    "end": "2074960"
  },
  {
    "text": "API is for envoy if we want to support new features or we want to configure new features or non-void like outlier",
    "start": "2074960",
    "end": "2081300"
  },
  {
    "text": "detection we actually have to build that ourselves in so we want to so like to enable some of these new things we",
    "start": "2081300",
    "end": "2087000"
  },
  {
    "text": "wanted to make sure that we can maintain iterating while doing this migration and so part of that was that we actually had",
    "start": "2087000",
    "end": "2092638"
  },
  {
    "text": "to take we had to take a step back and build a lot of CI and C d-four there or in just different ways that we could",
    "start": "2092639",
    "end": "2098519"
  },
  {
    "text": "verify a lot of this communication when we migrated so that meant is that we",
    "start": "2098519",
    "end": "2104010"
  },
  {
    "text": "built in using our SD configurator we actually built in flags kill switches as well as like hot switches so basically",
    "start": "2104010",
    "end": "2110160"
  },
  {
    "text": "if you turn it on or you can turn it off you can see like what the differences were and those were the playtime decisions so basically you deploy it",
    "start": "2110160",
    "end": "2116670"
  },
  {
    "text": "with the flag on and it would turn it on and you could roll it back and it would turn it off as well on top of that we",
    "start": "2116670",
    "end": "2123720"
  },
  {
    "text": "actually built a verification fleet so we actually have a fleet that it employs to where it will try all of the service",
    "start": "2123720",
    "end": "2129599"
  },
  {
    "text": "discovery side cars and try to get them to communicate to each other on top of that we built services that were",
    "start": "2129599",
    "end": "2135230"
  },
  {
    "text": "essentially we use it on some of our canary services as well so we'd actually deploy some of these changes and the",
    "start": "2135230",
    "end": "2140460"
  },
  {
    "text": "canary services where there's like lower traffic and it and actually wasn't even like production there's like some of our",
    "start": "2140460",
    "end": "2145710"
  },
  {
    "text": "staging environments just to see that the service discovery was actually working and a verify that some of these things were working and a lot of times",
    "start": "2145710",
    "end": "2152309"
  },
  {
    "text": "it actually caught a lot of the corner cases where people I could figure things differently or just theirs their service",
    "start": "2152309",
    "end": "2158910"
  },
  {
    "text": "was a standard it wasn't configured standardly and then on top of that we",
    "start": "2158910",
    "end": "2164220"
  },
  {
    "text": "actually went through invented a lot of services and we we supported like the general case what that meant is that we",
    "start": "2164220",
    "end": "2169890"
  },
  {
    "text": "supported a lot of our basic traffic but then the if anything was talking that",
    "start": "2169890",
    "end": "2175049"
  },
  {
    "text": "was very specific we're actually like drawn a disallow list we're like actually not migrating all the services",
    "start": "2175049",
    "end": "2180450"
  },
  {
    "text": "so one of those things that are in the the big things on our disallow list right now our databases just because we",
    "start": "2180450",
    "end": "2186059"
  },
  {
    "text": "haven't traffic team and SRA and everybody hasn't actually taken time to invest",
    "start": "2186059",
    "end": "2191910"
  },
  {
    "text": "into what does envoy two databases look like yet and so how we register",
    "start": "2191910",
    "end": "2197190"
  },
  {
    "text": "databases into zookeeper and all that we haven't actually thought that through so we're actually just going to make sure",
    "start": "2197190",
    "end": "2203250"
  },
  {
    "text": "that like none of the none of the DB proxies or anything can actually enable it and we do that via that SD",
    "start": "2203250",
    "end": "2210660"
  },
  {
    "text": "configurator as well so the SD configurator does that runtime decision and we just say you like hard notes a lot of these things are on the disallow",
    "start": "2210660",
    "end": "2216809"
  },
  {
    "text": "list and on top of that we actually just migrate some of them so going back all",
    "start": "2216809",
    "end": "2221910"
  },
  {
    "text": "the services that were standard we actually just migrate them for the end-user them end-user actually doesn't have to do anything",
    "start": "2221910",
    "end": "2227360"
  },
  {
    "text": "to get this migration on top of that we talked about how people will come in and",
    "start": "2227360",
    "end": "2234800"
  },
  {
    "start": "2228000",
    "end": "2302000"
  },
  {
    "text": "they make their own configs throw and synapse configs their own nerve configs we translated a lot of",
    "start": "2234800",
    "end": "2240170"
  },
  {
    "text": "those to like what would the STS Jim or what would the Envoy a config look like for that but the problem is that we",
    "start": "2240170",
    "end": "2245510"
  },
  {
    "text": "actually had a lot of ad hoc and like went off configs just kind of floating around going back to the point where",
    "start": "2245510",
    "end": "2250520"
  },
  {
    "text": "people would copy pasta some configs they change like their keepalive or like",
    "start": "2250520",
    "end": "2255770"
  },
  {
    "text": "how many threads and like all these different things and then they'd leave and then we'd be like well your",
    "start": "2255770",
    "end": "2261530"
  },
  {
    "text": "configures like this is the different configured like our general config do you know if like do you know what the",
    "start": "2261530",
    "end": "2268880"
  },
  {
    "text": "effects of this are and like nobody anyone actually know they're like well you can just like configure it and pray",
    "start": "2268880",
    "end": "2273950"
  },
  {
    "text": "like it's something breaks we'll just like let you know which wasn't like a great decision so on the same time we we",
    "start": "2273950",
    "end": "2280460"
  },
  {
    "text": "actually tried to we handheld a lot of these services and we basically made calls on like how different the config",
    "start": "2280460",
    "end": "2286670"
  },
  {
    "text": "was from our standard config so if there were like more than a few settings that were completely off we actually put them",
    "start": "2286670",
    "end": "2292460"
  },
  {
    "text": "back on our disallow list until they could resolve those and they could kind of move on to our paved road essentially",
    "start": "2292460",
    "end": "2298190"
  },
  {
    "text": "like our standardized configuration at their own pace and then on top of that",
    "start": "2298190",
    "end": "2304670"
  },
  {
    "start": "2302000",
    "end": "2378000"
  },
  {
    "text": "we actually had a problem now where we actually had multiple sources of truth for our default config because we have",
    "start": "2304670",
    "end": "2311750"
  },
  {
    "text": "the deep we have config in chef we have configs in coop gen which is essentially our kubernetes wrapper and the problem",
    "start": "2311750",
    "end": "2318620"
  },
  {
    "text": "was that whenever as we're in this my end middle to migration process what would happen as a service would deploy",
    "start": "2318620",
    "end": "2323870"
  },
  {
    "text": "and then maybe someone would actually try to tune their service discovery convey but they go in and be like it's",
    "start": "2323870",
    "end": "2330230"
  },
  {
    "text": "not applying my service to cut the discovery config so we'd actually have to look at a lot of different places because we have to be like is it a",
    "start": "2330230",
    "end": "2336050"
  },
  {
    "text": "zookeeper it's a zookeeper configure right is it in chef is that configure right is it in",
    "start": "2336050",
    "end": "2341090"
  },
  {
    "text": "like kubernetes is it in the SD configurator is that configure 8 so a",
    "start": "2341090",
    "end": "2346160"
  },
  {
    "text": "lot of so what we ended up doing is that whenever we're reading or writing they can fix the zookeeper we actually say",
    "start": "2346160",
    "end": "2351320"
  },
  {
    "text": "who wrote that or who read that convey or who wrote that config so they're never you pull it down you can actually see like where that configures published",
    "start": "2351320",
    "end": "2357590"
  },
  {
    "text": "from was a Polish from chef was it published from kubernetes so that way we naturally know who is",
    "start": "2357590",
    "end": "2363740"
  },
  {
    "text": "like causing the issue so like someone to change the chef but in reality like the source of truth is in kubernetes and",
    "start": "2363740",
    "end": "2369350"
  },
  {
    "text": "their application so we did some of that modifications in syntax as well so that way we could tell who is actually",
    "start": "2369350",
    "end": "2375410"
  },
  {
    "text": "reading and writing these configs and then on top of that we actually did",
    "start": "2375410",
    "end": "2381650"
  },
  {
    "start": "2378000",
    "end": "2400000"
  },
  {
    "text": "service cover version rollout so we have SD configurator we have Envoy we we use",
    "start": "2381650",
    "end": "2387350"
  },
  {
    "text": "envoy one tin right now I think and then on top of that we have SDS chamber ship",
    "start": "2387350",
    "end": "2395030"
  },
  {
    "text": "so we actually have these three different versions so the problem was is like how do we track these different things so this is actually a throwback",
    "start": "2395030",
    "end": "2401420"
  },
  {
    "start": "2400000",
    "end": "2443000"
  },
  {
    "text": "to that first slide with the dashboards so this is actually just as you can see here it lets you in each of the",
    "start": "2401420",
    "end": "2407810"
  },
  {
    "text": "different colors is actually a different service discovery package so that way you could see that if there was an",
    "start": "2407810",
    "end": "2413090"
  },
  {
    "text": "increase in error rate at the same time this color were to change that you would see like oh it's actually the the",
    "start": "2413090",
    "end": "2419060"
  },
  {
    "text": "service discovery package that is breaking my service and that made that gave more visibility and more version",
    "start": "2419060",
    "end": "2424520"
  },
  {
    "text": "tracking for us so that way we could actually determine if it was a service discovery change or if it was actually",
    "start": "2424520",
    "end": "2431060"
  },
  {
    "text": "like a service change going back to like service owners not being clear on whether it's their service or service",
    "start": "2431060",
    "end": "2437180"
  },
  {
    "text": "discovery okay so kind of a quick recap of everything today so we already talked",
    "start": "2437180",
    "end": "2444650"
  },
  {
    "text": "about service cover ease is hard right we know that everybody has maintained their own server discovery framework",
    "start": "2444650",
    "end": "2450950"
  },
  {
    "text": "over the years and a lot of this like a lot of this work has been through a lot of like blood sweat and tears and just",
    "start": "2450950",
    "end": "2456770"
  },
  {
    "text": "like late nights up trying to like maintain and make sure service discovery like doesn't go down which it's kind of",
    "start": "2456770",
    "end": "2463760"
  },
  {
    "text": "we're moving forward we're working towards like looking at like different service discovery options as well",
    "start": "2463760",
    "end": "2469670"
  },
  {
    "text": "instead of having to maintain it internally ourselves just because there's so much pain and so many hours of doing this we also know that scaling",
    "start": "2469670",
    "end": "2477980"
  },
  {
    "start": "2475000",
    "end": "2515000"
  },
  {
    "text": "is hard right like that as everybody grows it's really hard to think about some of these scales when you're a smaller company over and you're a much",
    "start": "2477980",
    "end": "2484280"
  },
  {
    "text": "smaller less services less back-end all these things like really why would I",
    "start": "2484280",
    "end": "2489890"
  },
  {
    "text": "ever think that we get to like 10,000 hosts 10,000 back-end it's 10,000 different like services that",
    "start": "2489890",
    "end": "2496200"
  },
  {
    "text": "we could be and even then like what what happens tomorrow when it's like a hundred thousand how does envoy handle this we haven't thought about these",
    "start": "2496200",
    "end": "2501900"
  },
  {
    "text": "things because sometimes you're moving so fast you don't really think about these things and it and next thing you",
    "start": "2501900",
    "end": "2507900"
  },
  {
    "text": "know like you're seeing five hundreds and you have like 20,000 backends like but that's just part of like scaling and",
    "start": "2507900",
    "end": "2515839"
  },
  {
    "start": "2515000",
    "end": "2545000"
  },
  {
    "text": "on top of that like as we move forward migrating is hard a lot of time like I",
    "start": "2515839",
    "end": "2520980"
  },
  {
    "text": "think this is a universal truth for most people I assume that like whatever you're migrating from either one service",
    "start": "2520980",
    "end": "2526349"
  },
  {
    "text": "to another service from one application or when back-end or one cloud provider to somewhere else it's always you're",
    "start": "2526349",
    "end": "2532230"
  },
  {
    "text": "always gonna have a lot of friction and especially if your infrastructure you're gonna get a lot of flack if you break",
    "start": "2532230",
    "end": "2537599"
  },
  {
    "text": "things because service owners lose trust in you and a lot of that trust like you have to build back it from like the",
    "start": "2537599",
    "end": "2543420"
  },
  {
    "text": "service owners and all this like thinking back like would we still have",
    "start": "2543420",
    "end": "2549780"
  },
  {
    "start": "2545000",
    "end": "2583000"
  },
  {
    "text": "created her own service discovery would we does it make sense at the same time my thinking back on doing all this",
    "start": "2549780",
    "end": "2556319"
  },
  {
    "text": "engineering work actually like with everything that we that we knew at the time which is like some of the stuff was",
    "start": "2556319",
    "end": "2563160"
  },
  {
    "text": "that made like seven seven years ago like when we're at that small of a scale",
    "start": "2563160",
    "end": "2568200"
  },
  {
    "text": "and without all the options out there I think it does makes it it didn't make sense internally at that time especially because there weren't things like since",
    "start": "2568200",
    "end": "2575880"
  },
  {
    "text": "you have to like offer a lot of these open-source projects as well to us as well as like vendors that are supporting",
    "start": "2575880",
    "end": "2581880"
  },
  {
    "text": "that as well so that's all I have this is my contact information also there are",
    "start": "2581880",
    "end": "2589589"
  },
  {
    "start": "2583000",
    "end": "2606000"
  },
  {
    "text": "other kubernetes talks that we're offering one is today after this by Ben",
    "start": "2589589",
    "end": "2595020"
  },
  {
    "text": "Hughes and then we have one another tomorrow about p90 5s and the kubernetes",
    "start": "2595020",
    "end": "2600200"
  },
  {
    "text": "thank you [Applause]",
    "start": "2600200",
    "end": "2608559"
  }
]