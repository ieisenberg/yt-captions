[
  {
    "start": "0",
    "end": "24000"
  },
  {
    "text": "all right we're gonna go ahead and get started I'm Tauron I'm from Starr I work on open policy agent but today I'm",
    "start": "30",
    "end": "5339"
  },
  {
    "text": "introducing David Zhu from Google and Yahoo phrenic from Red Hat and they're",
    "start": "5339",
    "end": "10889"
  },
  {
    "text": "gonna talk about kubernetes storage 101 and break down some really important concepts for you guys so go ahead all right hi everyone thank",
    "start": "10889",
    "end": "19680"
  },
  {
    "text": "you for coming to our storage 101 talk so let's just get started then",
    "start": "19680",
    "end": "24840"
  },
  {
    "start": "24000",
    "end": "54000"
  },
  {
    "text": "so first of all kubernetes I'm sure since this is the second day of the conference you already know what",
    "start": "24840",
    "end": "30720"
  },
  {
    "text": "kubernetes is but basically you know it's like a container Orchestrator but",
    "start": "30720",
    "end": "35730"
  },
  {
    "text": "we like to think of it as a pod Orchestrator because it actually works on pods which are collections of",
    "start": "35730",
    "end": "41100"
  },
  {
    "text": "containers right and so as we may know containers are generally stateless",
    "start": "41100",
    "end": "46700"
  },
  {
    "text": "they're cleared on exit but this is unless persistent storage is used so",
    "start": "46700",
    "end": "52590"
  },
  {
    "text": "that's what we're gonna be talking about today so here is an example of a pod and",
    "start": "52590",
    "end": "58620"
  },
  {
    "start": "54000",
    "end": "91000"
  },
  {
    "text": "this is actually a stateless pod right it's just got a my sequel image and notice that there's no like storage",
    "start": "58620",
    "end": "66420"
  },
  {
    "text": "related concepts in here and what happens is that this pod actually on exit the entire database is cleared and",
    "start": "66420",
    "end": "73890"
  },
  {
    "text": "pods actually are quite ephemeral they can you know go down for any reason and",
    "start": "73890",
    "end": "79020"
  },
  {
    "text": "they can go down because of preemption so another pod with a higher priority comes in or they can go down just",
    "start": "79020",
    "end": "84630"
  },
  {
    "text": "because the node went down so this my sequel database is not going to be very resilient so that's why we introduced",
    "start": "84630",
    "end": "92009"
  },
  {
    "start": "91000",
    "end": "236000"
  },
  {
    "text": "some storage concepts in kubernetes so we have these three main objects here on",
    "start": "92009",
    "end": "97890"
  },
  {
    "text": "this slide the pod I just introduced and the two main storage objects that we're going to be talking about one is the",
    "start": "97890",
    "end": "105240"
  },
  {
    "text": "persistent volume claim and this is the applications request for storage so the",
    "start": "105240",
    "end": "110790"
  },
  {
    "text": "user will put this on the system to request for storage and the persistent volume this is the kind of the admin",
    "start": "110790",
    "end": "117960"
  },
  {
    "text": "object for storage so this is how the admin will create a pointer to actual",
    "start": "117960",
    "end": "124380"
  },
  {
    "text": "physical storage and it will contain all the like gory details of our Alang storage",
    "start": "124380",
    "end": "129789"
  },
  {
    "text": "and and these two objects kind of bind together and don't worry we'll talk a lot more about these in the future in",
    "start": "129789",
    "end": "135939"
  },
  {
    "text": "detail I just wanted to introduce them right now so you might be asking okay so if I want storage I just want a volume",
    "start": "135939",
    "end": "143739"
  },
  {
    "text": "right I just want a disk that connects to my container why do I need two objects why do I need a persistent",
    "start": "143739",
    "end": "149590"
  },
  {
    "text": "volume claim and a persistent volume well by separating these concerns the",
    "start": "149590",
    "end": "154840"
  },
  {
    "text": "concerns of the user and the concerns of the admin on the other side we're able",
    "start": "154840",
    "end": "159879"
  },
  {
    "text": "to get both application portability as well as a little bit more security right",
    "start": "159879",
    "end": "165489"
  },
  {
    "text": "so let me talk a little bit more about the portability side since the persistent volume claim online contains",
    "start": "165489",
    "end": "172120"
  },
  {
    "text": "very basic information that the user workload requests of storage so we'll",
    "start": "172120",
    "end": "178030"
  },
  {
    "text": "talk a little bit more about what is inside it later but it only contains very basic requests you can make these",
    "start": "178030",
    "end": "184389"
  },
  {
    "text": "types of requests on any type of cluster on any type of underlying infrastructure right and the persistent volumes which",
    "start": "184389",
    "end": "191440"
  },
  {
    "text": "the admins create that are not portable these ones are different per cluster based on your underlying infrastructure",
    "start": "191440",
    "end": "197609"
  },
  {
    "text": "when you create these Christa's and volumes they contain all of the details",
    "start": "197609",
    "end": "202629"
  },
  {
    "text": "about the underlying storage like for example if you're using an NFS storage",
    "start": "202629",
    "end": "207760"
  },
  {
    "text": "underneath this would contain things like your credentials and the IP address with where to find this NFS and the",
    "start": "207760",
    "end": "216459"
  },
  {
    "text": "security concern is also you don't want your users to be able to kind of create these persistent volume objects that",
    "start": "216459",
    "end": "224169"
  },
  {
    "text": "reference underlying storage because generally you don't want users to be able to just point to whatever",
    "start": "224169",
    "end": "229569"
  },
  {
    "text": "underlying storage they want to write in the case of NFS you don't want them to just put some random IP address and find",
    "start": "229569",
    "end": "235780"
  },
  {
    "text": "whatever okay a third object that we'll be talking about today is this storage",
    "start": "235780",
    "end": "241299"
  },
  {
    "start": "236000",
    "end": "266000"
  },
  {
    "text": "Clause object John will be going into more detail about this later but basically what you need to know now is",
    "start": "241299",
    "end": "248049"
  },
  {
    "text": "that this is for dynamic provisioning it's a collection of persistent volumes",
    "start": "248049",
    "end": "254290"
  },
  {
    "text": "so if we take a bunch of persistent volumes grouped them kind of by class this goes all under one storage class",
    "start": "254290",
    "end": "261669"
  },
  {
    "text": "type object and we'll also use this to provision things dynamically alright so",
    "start": "261669",
    "end": "268030"
  },
  {
    "start": "266000",
    "end": "367000"
  },
  {
    "text": "in a little bit more detail I'm gonna be kind of role playing as the workload or",
    "start": "268030",
    "end": "274060"
  },
  {
    "text": "the user in this case so I'm trying to put this my sequel pod on on my",
    "start": "274060",
    "end": "279520"
  },
  {
    "text": "kubernetes cluster and I want it to actually persist data instead of you know when it dies because it's an",
    "start": "279520",
    "end": "286390"
  },
  {
    "text": "ephemeral pod I don't want the data to go away so I'm gonna persist it and I'm gonna use a persistent volume clamp so",
    "start": "286390",
    "end": "292900"
  },
  {
    "text": "we can see here that in this container we have some volume mounts and we've",
    "start": "292900",
    "end": "298060"
  },
  {
    "text": "specified the data volume and we want it to go into this path into the into the container okay",
    "start": "298060",
    "end": "304600"
  },
  {
    "text": "and we've also defined in the same yeah mol file actually the what this volume",
    "start": "304600",
    "end": "309700"
  },
  {
    "text": "is and we've just defined this volume as a pointer to the persistent volume claim that were that we're gonna reference",
    "start": "309700",
    "end": "316440"
  },
  {
    "text": "okay so what does this persistent volume claim that we're referencing look like here's the name that we're referencing",
    "start": "316440",
    "end": "322870"
  },
  {
    "text": "and as I told you before it's actually very simple right it actually only contains the most basic information that",
    "start": "322870",
    "end": "329380"
  },
  {
    "text": "we need for our storage so we we only care about you know we don't care how",
    "start": "329380",
    "end": "335170"
  },
  {
    "text": "our storage is given to us or what underlying volume we have as a as a person putting this workload on to the",
    "start": "335170",
    "end": "340900"
  },
  {
    "text": "mate on to the cluster all I really care about is the size things like what type",
    "start": "340900",
    "end": "346630"
  },
  {
    "text": "of file system or whether I want block or the access mode right so there are",
    "start": "346630",
    "end": "352480"
  },
  {
    "text": "many such parameters but basically they're just the ones that the workload cares about okay so yeah so for example",
    "start": "352480",
    "end": "360040"
  },
  {
    "text": "in this one I want one gigabyte of storage and I want it and read write mode okay and I don't really care about",
    "start": "360040",
    "end": "366760"
  },
  {
    "text": "the rest here so in order to create this claim on the cluster you just do a cube",
    "start": "366760",
    "end": "373540"
  },
  {
    "start": "367000",
    "end": "397000"
  },
  {
    "text": "total create - F which is for the file and we have the claimed animal that we",
    "start": "373540",
    "end": "379240"
  },
  {
    "text": "just showed earlier and we can also get the PVC and this gives us some more",
    "start": "379240",
    "end": "384820"
  },
  {
    "text": "information about the PVC on our system okay we can also create our pod and",
    "start": "384820",
    "end": "390880"
  },
  {
    "text": "we'll see that since the PVC has been created it has something to reference and the pod will start running",
    "start": "390880",
    "end": "396689"
  },
  {
    "text": "okay a quick debugging thing for PVCs if you just get the PVC sometimes you'll",
    "start": "396689",
    "end": "403769"
  },
  {
    "start": "397000",
    "end": "412000"
  },
  {
    "text": "note that it's pending and you won't know why it doesn't note how this doesn't actually say anything about why",
    "start": "403769",
    "end": "408899"
  },
  {
    "text": "the plot of the PVC is pending really important to know describe your PVCs okay describe all of",
    "start": "408899",
    "end": "416610"
  },
  {
    "start": "412000",
    "end": "437000"
  },
  {
    "text": "your API objects when something goes wrong you should think cube control described okay so when we describe this",
    "start": "416610",
    "end": "423749"
  },
  {
    "text": "we get some events at the very bottom and this actually lets us get into kind",
    "start": "423749",
    "end": "429719"
  },
  {
    "text": "of what the actual issue is here so the actual issue here is that our storage class is not found right so cube control",
    "start": "429719",
    "end": "436229"
  },
  {
    "text": "describe and now yawn so participant volume is the first object that is for",
    "start": "436229",
    "end": "443069"
  },
  {
    "start": "437000",
    "end": "495000"
  },
  {
    "text": "administrators to play administrator in this talk the persistent volume is just",
    "start": "443069",
    "end": "449579"
  },
  {
    "text": "bunch of better metadata and pointer to the rear storage compartment if supports",
    "start": "449579",
    "end": "455069"
  },
  {
    "text": "I think 15 different volume backends right now all of them are hard-coded in",
    "start": "455069",
    "end": "460709"
  },
  {
    "text": "kubernetes so if you want to introduce new back-end you need to go to kubernetes code and write it there and",
    "start": "460709",
    "end": "467320"
  },
  {
    "text": "[Music] the kind of declared stop on new volume plugins and instead we provide two",
    "start": "467320",
    "end": "475619"
  },
  {
    "text": "extension mechanisms how to connect kubernetes to a different storage back-end that's not listed here",
    "start": "475619",
    "end": "480629"
  },
  {
    "text": "we provide old flex and new fancy CSI",
    "start": "480629",
    "end": "485849"
  },
  {
    "text": "container storage interface so you can connect kubernetes to a storage back-end",
    "start": "485849",
    "end": "491669"
  },
  {
    "text": "that kubernetes doesn't know anything about it if you look at the details",
    "start": "491669",
    "end": "496709"
  },
  {
    "start": "495000",
    "end": "544000"
  },
  {
    "text": "about the metadata kubernetes needs to know the capacity of the volume how the",
    "start": "496709",
    "end": "502649"
  },
  {
    "text": "volume can be accessed and to which storage class it belongs so this is some",
    "start": "502649",
    "end": "509309"
  },
  {
    "text": "cheap storage on my old NFS server running somewhere in my infrastructure",
    "start": "509309",
    "end": "514589"
  },
  {
    "text": "so I created a stretch class called cheap and the last bit the sort of",
    "start": "514589",
    "end": "521339"
  },
  {
    "text": "mandatory is what should happen when this persistent volume is not needed when Burnette is",
    "start": "521339",
    "end": "528470"
  },
  {
    "text": "is done with the volume and it should do something we will cover that in detail",
    "start": "528470",
    "end": "534880"
  },
  {
    "text": "later so this person volume is just pointer to some storage perhaps outside",
    "start": "534880",
    "end": "542090"
  },
  {
    "text": "of kubernetes and the last bit that or so created by Christ our main storage",
    "start": "542090",
    "end": "549140"
  },
  {
    "start": "544000",
    "end": "578000"
  },
  {
    "text": "class it's basically a collection of persistent volumes that share some common characteristics like speed so I",
    "start": "549140",
    "end": "556310"
  },
  {
    "text": "can add fast storage class and slow storage class or price so I can have expensive storage class and cheap",
    "start": "556310",
    "end": "562730"
  },
  {
    "text": "storage class or I don't know replication characteristics or I can",
    "start": "562730",
    "end": "568010"
  },
  {
    "text": "have safe storage class with free replicas of data or unsafe storage class",
    "start": "568010",
    "end": "573860"
  },
  {
    "text": "with just one replicas of the data it",
    "start": "573860",
    "end": "579170"
  },
  {
    "start": "578000",
    "end": "650000"
  },
  {
    "text": "also contains parameters for dynamic provisioning that is used when kubernetes when there is no available",
    "start": "579170",
    "end": "586370"
  },
  {
    "text": "persistent volume but kubernetes needs Sam so it reaches some storage back-end and provisions the data the world data",
    "start": "586370",
    "end": "594080"
  },
  {
    "text": "volume automatically it needs to know whom it should talk to",
    "start": "594080",
    "end": "599900"
  },
  {
    "text": "it's the provisional parameter and you can find the exact values you can put",
    "start": "599900",
    "end": "605360"
  },
  {
    "text": "here in either in kubernetes documentation or in documentation of storage back-end if you use CSI or an",
    "start": "605360",
    "end": "612770"
  },
  {
    "text": "external provisioning support and the provisionary usually needs some",
    "start": "612770",
    "end": "618680"
  },
  {
    "text": "parameters that will provision a persistent volume of this given class so",
    "start": "618680",
    "end": "624620"
  },
  {
    "text": "this class is called fast and these parameters will provision the fastest volume is available on AWS the",
    "start": "624620",
    "end": "632600"
  },
  {
    "text": "parameters are different for each storage back-end that means this object",
    "start": "632600",
    "end": "637910"
  },
  {
    "text": "is not very portable if I take this this run from Amazon I can't take it on GC",
    "start": "637910",
    "end": "643490"
  },
  {
    "text": "it will not run there I will need to provide different provision error and different parameters and each cluster I",
    "start": "643490",
    "end": "652400"
  },
  {
    "start": "650000",
    "end": "682000"
  },
  {
    "text": "strongly recommend should have one default storage class the four storage class is a class with this magic",
    "start": "652400",
    "end": "659690"
  },
  {
    "text": "notation and what happens is when user like david creates person volume claim without",
    "start": "659690",
    "end": "667220"
  },
  {
    "text": "referencing any specific storage class he will get the default one so in this",
    "start": "667220",
    "end": "673310"
  },
  {
    "text": "case if david creates a claim without any storage class you will get the first one the persistent volume can be created",
    "start": "673310",
    "end": "687230"
  },
  {
    "start": "682000",
    "end": "861000"
  },
  {
    "text": "in two ways the first one is dynamic provisioning at it you should use that",
    "start": "687230",
    "end": "692360"
  },
  {
    "text": "in your clusters where administrator just just creates a storage class and",
    "start": "692360",
    "end": "698710"
  },
  {
    "text": "when user creates a persistent volume it",
    "start": "698710",
    "end": "703820"
  },
  {
    "text": "either references concrete sludge class or no storage class and gets the default one",
    "start": "703820",
    "end": "709310"
  },
  {
    "text": "and kubernetes then goes and creates a",
    "start": "709310",
    "end": "714700"
  },
  {
    "text": "volume on the storage back-end and a persistent volume object in kubernetes pointing to it kubernetes then by binds",
    "start": "714700",
    "end": "724420"
  },
  {
    "text": "the persistent volume and persistent volume claim together and nobody can use",
    "start": "724420",
    "end": "730670"
  },
  {
    "text": "the persistent volume except the user through the claim nobody else can reach the data at least from kubernetes and",
    "start": "730670",
    "end": "739389"
  },
  {
    "text": "user can finally run samples using the present volume claim the other option we",
    "start": "740110",
    "end": "747200"
  },
  {
    "text": "support is manu provisioning where i as administrator can create a persistent",
    "start": "747200",
    "end": "753050"
  },
  {
    "text": "volume object in Burnet is pointing to some storage outside of kubernetes for",
    "start": "753050",
    "end": "758140"
  },
  {
    "text": "some legacy data that are pre-populated and i want to use them in kubernetes and",
    "start": "758140",
    "end": "767800"
  },
  {
    "text": "user creates exactly the same persistent wouldn't claim like before referencing",
    "start": "767830",
    "end": "773000"
  },
  {
    "text": "some storage class and kubernetes will find the existing volume and bind it to",
    "start": "773000",
    "end": "778730"
  },
  {
    "text": "the claim important note here is that kubernetes always looks for existing",
    "start": "778730",
    "end": "785570"
  },
  {
    "text": "volumes first and if there is no existing volume that would match the claim only after that it goes and",
    "start": "785570",
    "end": "792950"
  },
  {
    "text": "dynamically provisions the that means if I if this is some",
    "start": "792950",
    "end": "800300"
  },
  {
    "text": "pre-populated data some voting with repopulated data I should probably create a special storage class so",
    "start": "800300",
    "end": "807350"
  },
  {
    "text": "somebody accidentally doesn't create a persistent volume claim requesting basically empty volume and accidentally",
    "start": "807350",
    "end": "814399"
  },
  {
    "text": "binds to the volume with the data so this is really for like brown few",
    "start": "814399",
    "end": "823519"
  },
  {
    "text": "scenarios where you want to use some old data in kubernetes you should use dynamic provisioning for the rest your",
    "start": "823519",
    "end": "831019"
  },
  {
    "text": "life will be much easier and when user",
    "start": "831019",
    "end": "836570"
  },
  {
    "text": "can create support you second delete a pot or something can delete the pot the",
    "start": "836570",
    "end": "841970"
  },
  {
    "text": "data are still retained user can create as many pots as you want and still use the data that's fine and when user",
    "start": "841970",
    "end": "850760"
  },
  {
    "text": "doesn't need the data he deletes the persistent volume claim that's the signal to compare Nettie's",
    "start": "850760",
    "end": "856760"
  },
  {
    "text": "I don't need the data do something about that and what happens is encoded in",
    "start": "856760",
    "end": "864620"
  },
  {
    "start": "861000",
    "end": "962000"
  },
  {
    "text": "persistent volume reclaiming policy field in the persistent volume I showed you before we support three policies we",
    "start": "864620",
    "end": "871490"
  },
  {
    "text": "support recycle we support it only on an NFS a kubernetes basically deletes all",
    "start": "871490",
    "end": "878000"
  },
  {
    "text": "the data from the volume and put it back to available state so it's available to a different claim it's kind of clunky it",
    "start": "878000",
    "end": "885800"
  },
  {
    "text": "doesn't work well with different NFS servers and so on so it's deprecated it",
    "start": "885800",
    "end": "891560"
  },
  {
    "text": "may work for you but accept trouble expect troubles the more useful policy",
    "start": "891560",
    "end": "900320"
  },
  {
    "text": "is delete when kubernetes just goes to the storage back-end deletes the volume",
    "start": "900320",
    "end": "906800"
  },
  {
    "text": "there deletes the persistent volume object in kubernetes and the volume is",
    "start": "906800",
    "end": "912230"
  },
  {
    "text": "deleted or you can use policy retain we're just kubernetes puts the",
    "start": "912230",
    "end": "918860"
  },
  {
    "text": "persistent volume aside nobody can bind to it because there can be I don't know qualified numbers on on",
    "start": "918860",
    "end": "925010"
  },
  {
    "text": "the volume but the data is still there and is faster administrator can reach the data",
    "start": "925010",
    "end": "932679"
  },
  {
    "text": "or bind it manually to a different persistent volume claim if David comes",
    "start": "932679",
    "end": "937720"
  },
  {
    "text": "ask nicely I can give him the data in all these three cases the data are lost",
    "start": "937720",
    "end": "945249"
  },
  {
    "text": "for the user so the user can't access the data it either can go to me and ask",
    "start": "945249",
    "end": "952660"
  },
  {
    "text": "nicely or the data are deleted so be very careful and DirecTV sees you will",
    "start": "952660",
    "end": "958929"
  },
  {
    "text": "lose your data and finally highest",
    "start": "958929",
    "end": "966189"
  },
  {
    "start": "962000",
    "end": "993000"
  },
  {
    "text": "administrator can delete persistent volumes manually it could be useful in",
    "start": "966189",
    "end": "971319"
  },
  {
    "text": "some cases but again be very careful you are dealing with some data",
    "start": "971319",
    "end": "977939"
  },
  {
    "text": "don't make users angry important note here if you delete persistent volume in",
    "start": "977939",
    "end": "984429"
  },
  {
    "text": "kubernetes the object it doesn't delete the data in the storage back-end you",
    "start": "984429",
    "end": "989559"
  },
  {
    "text": "must do that as a separate step so now that you know all this kubernetes",
    "start": "989559",
    "end": "996999"
  },
  {
    "start": "993000",
    "end": "1012000"
  },
  {
    "text": "objects is the time to show you how to use them in priority right so we've",
    "start": "996999",
    "end": "1005220"
  },
  {
    "text": "learned about the pod the PVC and the PV but actually we don't recommend using",
    "start": "1005220",
    "end": "1010619"
  },
  {
    "text": "pods in production environments and that's kind of because pods are not",
    "start": "1010619",
    "end": "1015629"
  },
  {
    "start": "1012000",
    "end": "1039000"
  },
  {
    "text": "really for end-user production right there kind of ephemeral I've kind of I",
    "start": "1015629",
    "end": "1021199"
  },
  {
    "text": "alluded to this earlier right they can be kicked off for many different reasons and when your pod gets kicked off of",
    "start": "1021199",
    "end": "1027538"
  },
  {
    "text": "your machine you actually don't have access to your workload or your data anymore so if you're running the service",
    "start": "1027539",
    "end": "1033418"
  },
  {
    "text": "on that pod you can no longer hit it when your pot is gone so how do we fix",
    "start": "1033419",
    "end": "1038428"
  },
  {
    "text": "that well we have some higher level objects in kubernetes that can help you",
    "start": "1038429",
    "end": "1043558"
  },
  {
    "start": "1039000",
    "end": "1107000"
  },
  {
    "text": "with your with your issues here we have the deployment which basically just runs",
    "start": "1043559",
    "end": "1049529"
  },
  {
    "text": "many replicas of your pod template so you give it a pod and it just",
    "start": "1049529",
    "end": "1054800"
  },
  {
    "text": "Eck's many of them right and when your pod gets deleted for any reason like",
    "start": "1054800",
    "end": "1060140"
  },
  {
    "text": "your someone comes in and preempts your pod from from a node the deployment will",
    "start": "1060140",
    "end": "1065750"
  },
  {
    "text": "actually go and recreate that pod for you so if you want three replicas you're kind of always going to get three",
    "start": "1065750",
    "end": "1071270"
  },
  {
    "text": "replicas eventually we can scale this up and down we can have more less but one",
    "start": "1071270",
    "end": "1078860"
  },
  {
    "text": "of the big caveats with the deployment for storage especially is that all of",
    "start": "1078860",
    "end": "1083990"
  },
  {
    "text": "the pods in your deployment share the same persistent volume claim so this is",
    "start": "1083990",
    "end": "1089120"
  },
  {
    "text": "kind of what it ends up looking like you have your deployment which spawns multiple pods but they all reference the",
    "start": "1089120",
    "end": "1096710"
  },
  {
    "text": "same persistent volume claim and as we know from the previous slides that means that it will reference one persistent",
    "start": "1096710",
    "end": "1103250"
  },
  {
    "text": "volume which means we'll get one underlying storage right and you might",
    "start": "1103250",
    "end": "1108650"
  },
  {
    "text": "already have some clue as to why this might be a problem right so all three of",
    "start": "1108650",
    "end": "1113840"
  },
  {
    "text": "these pods are writing to the same volume writing and reading from the same volume so unless your workload is",
    "start": "1113840",
    "end": "1120230"
  },
  {
    "text": "specifically designed to do this to write and read to the same volume it's",
    "start": "1120230",
    "end": "1125360"
  },
  {
    "text": "very likely that they're going to overwrite the data of each other right and this generally causes applications",
    "start": "1125360",
    "end": "1131780"
  },
  {
    "text": "to misbehave in different ways we've seen crashes there are a lot of cases where if your volume is read right once",
    "start": "1131780",
    "end": "1138850"
  },
  {
    "text": "your pods will not actually come up on a second note or a third node because you",
    "start": "1138850",
    "end": "1143960"
  },
  {
    "text": "know you can only attach them to one node so you could have a lot of issues with a deployment like this while using",
    "start": "1143960",
    "end": "1150530"
  },
  {
    "text": "staple volumes but you know there is probably a case where you do want this and you have to be aware that this is",
    "start": "1150530",
    "end": "1158180"
  },
  {
    "text": "exactly what's happening right so if that doesn't exactly work well we have",
    "start": "1158180",
    "end": "1164240"
  },
  {
    "start": "1161000",
    "end": "1208000"
  },
  {
    "text": "another type of high level object for you to use which is called a stateful set so this does a very similar thing in",
    "start": "1164240",
    "end": "1172700"
  },
  {
    "text": "that it creates many replicas of your pods it has all the same kind of it does",
    "start": "1172700",
    "end": "1178730"
  },
  {
    "text": "similar things where it goes you can scale it up and down when the pod dies you can put a new one in but the very",
    "start": "1178730",
    "end": "1186170"
  },
  {
    "text": "the main difference here for store specifically is that each pod will now",
    "start": "1186170",
    "end": "1191210"
  },
  {
    "text": "get its own persistent volume clamp and the way we do that is in the object you",
    "start": "1191210",
    "end": "1196760"
  },
  {
    "text": "actually specify a persistent volume claim template and that templates out",
    "start": "1196760",
    "end": "1202040"
  },
  {
    "text": "the PVC and each pod will get like kind of a stamp out of that template right so",
    "start": "1202040",
    "end": "1209150"
  },
  {
    "start": "1208000",
    "end": "1285000"
  },
  {
    "text": "this is kind of what it ends up looking like in a three pod case we have three",
    "start": "1209150",
    "end": "1214400"
  },
  {
    "text": "pods here and three PVCs each one connected to a pod right so in",
    "start": "1214400",
    "end": "1219590"
  },
  {
    "text": "this case each pod gets its own volume and this is generally more like a good",
    "start": "1219590",
    "end": "1226010"
  },
  {
    "text": "starting place for what you want right but unfortunately kubernetes doesn't",
    "start": "1226010",
    "end": "1232760"
  },
  {
    "text": "really handle your application needs so when you're using a staple set it's very",
    "start": "1232760",
    "end": "1238640"
  },
  {
    "text": "important to keep in mind that you still need to come up with the mechanism sort",
    "start": "1238640",
    "end": "1245090"
  },
  {
    "text": "of to have these pods be aware of each other and be in the some same member set",
    "start": "1245090",
    "end": "1250340"
  },
  {
    "text": "so that they can do leader election or run in some sort of active passive",
    "start": "1250340",
    "end": "1255410"
  },
  {
    "text": "active active mode but kubernetes does not do this application specific",
    "start": "1255410",
    "end": "1261370"
  },
  {
    "text": "synchronization between your pots right so you can use a stateful set as a building block for your workloads but in",
    "start": "1261370",
    "end": "1268520"
  },
  {
    "text": "the end you still need to kind of determine how these pods interact with each other and who gets to serve data",
    "start": "1268520",
    "end": "1274940"
  },
  {
    "text": "who gets to write data this all needs to be determined by the application okay",
    "start": "1274940",
    "end": "1281840"
  },
  {
    "text": "and now yon will talk a little bit more about some storage features in",
    "start": "1281840",
    "end": "1287840"
  },
  {
    "start": "1285000",
    "end": "1385000"
  },
  {
    "text": "kubernetes to be accumulated a lot of features this is not a complete list I'm",
    "start": "1287840",
    "end": "1293630"
  },
  {
    "text": "going to present this is just very high-level overview what you can use and you can mix and match all the features",
    "start": "1293630",
    "end": "1299840"
  },
  {
    "text": "together and build your stateful applications so the first feature is topology topology over-scheduling until",
    "start": "1299840",
    "end": "1308570"
  },
  {
    "text": "now we sort of expected that if you have persistent volume object in kubernetes we expected that the volume is available",
    "start": "1308570",
    "end": "1315380"
  },
  {
    "text": "to all nodes in the cluster if you ran a cluster across multiple availability",
    "start": "1315380",
    "end": "1320990"
  },
  {
    "text": "zones in cloud that's usually not the case a persistent volume is then available only",
    "start": "1320990",
    "end": "1326960"
  },
  {
    "text": "to subset of nodes kubernetes can know about that using topology over scheduling and",
    "start": "1326960",
    "end": "1333950"
  },
  {
    "text": "schedule pots to note where the data is accessible and we also introduced",
    "start": "1333950",
    "end": "1342799"
  },
  {
    "text": "something we called delayed provisioning where if we create a if user creates a",
    "start": "1342799",
    "end": "1350570"
  },
  {
    "text": "persistent volume claim it's not provisioned immediately because it could be provisioned in a wrong zone but we",
    "start": "1350570",
    "end": "1357169"
  },
  {
    "text": "wait until a pot until user creates a pot and then scheduler looks at the pot",
    "start": "1357169",
    "end": "1364960"
  },
  {
    "text": "requirements like it means a GPU it needs eight cores it means five megabytes of memory and the claim it",
    "start": "1364960",
    "end": "1372080"
  },
  {
    "text": "needs like I don't know ten gigabytes of storage and schedules both the pot and the person were inclined to the node to",
    "start": "1372080",
    "end": "1379760"
  },
  {
    "text": "a node that has enough storage and has enough CPU power and GPUs so in reality",
    "start": "1379760",
    "end": "1386570"
  },
  {
    "start": "1385000",
    "end": "1432000"
  },
  {
    "text": "you can find you can create a person Emporium claim and find out that this",
    "start": "1386570",
    "end": "1392210"
  },
  {
    "text": "pending and the reason for pending is that it is waiting for the first",
    "start": "1392210",
    "end": "1397970"
  },
  {
    "text": "consumer and after you create a pot that",
    "start": "1397970",
    "end": "1403340"
  },
  {
    "text": "uses the frame or you create a set of a set of course not a pot only after that",
    "start": "1403340",
    "end": "1409789"
  },
  {
    "text": "the claim is by gets bound so don't panic if your claims your are pending",
    "start": "1409789",
    "end": "1416330"
  },
  {
    "text": "this may be the default mode in your posture especially if you run across",
    "start": "1416330",
    "end": "1421340"
  },
  {
    "text": "multiple availability zones there is a talk today later today",
    "start": "1421340",
    "end": "1426529"
  },
  {
    "text": "Michelle is covering it in a bit other",
    "start": "1426529",
    "end": "1433490"
  },
  {
    "start": "1432000",
    "end": "1473000"
  },
  {
    "text": "feature we have local volumes if you have some spare hard drive or SSDs",
    "start": "1433490",
    "end": "1439309"
  },
  {
    "text": "attached to your nodes you can use them as persistent volumes of course you get",
    "start": "1439309",
    "end": "1445490"
  },
  {
    "text": "super extra speed because you are using local storage but the trade-off is that",
    "start": "1445490",
    "end": "1450919"
  },
  {
    "text": "if the hard drives is broken or the node is go broke your data is lost so maybe that's what",
    "start": "1450919",
    "end": "1460200"
  },
  {
    "text": "you need but for generic storage be aware this is dangerous this could be",
    "start": "1460200",
    "end": "1466170"
  },
  {
    "text": "dangerous it's useful mainly for caches and this kind of stuff you don't really mind if they get lost until now if you",
    "start": "1466170",
    "end": "1477720"
  },
  {
    "start": "1473000",
    "end": "1507000"
  },
  {
    "text": "use the volume in a pot it kubernetes created a file system on it and mount it",
    "start": "1477720",
    "end": "1483870"
  },
  {
    "text": "in inside the pot there are some workloads that don't like it they won't extra speed or they want to handle the",
    "start": "1483870",
    "end": "1492750"
  },
  {
    "text": "block device by themselves so now you can use robot devices and you can get a",
    "start": "1492750",
    "end": "1498360"
  },
  {
    "text": "road block device inside your pot and you can do whatever we want with it kubernetes won't touch any data on the",
    "start": "1498360",
    "end": "1505080"
  },
  {
    "text": "volume you can also resize your volumes",
    "start": "1505080",
    "end": "1510510"
  },
  {
    "start": "1507000",
    "end": "1551000"
  },
  {
    "text": "if we support only expansion you can increase the size you can't shrink it",
    "start": "1510510",
    "end": "1517130"
  },
  {
    "text": "currently what we have as better feature is of fine resize that means that",
    "start": "1517130",
    "end": "1523560"
  },
  {
    "text": "kubernetes resize the volumes but you must stop your pod first so if you have",
    "start": "1523560",
    "end": "1530910"
  },
  {
    "text": "a stateful set you scale it down besides your volumes scale it up and new pods",
    "start": "1530910",
    "end": "1536610"
  },
  {
    "text": "will have bigger volumes we are working on online resize that means that you",
    "start": "1536610",
    "end": "1541830"
  },
  {
    "text": "won't need to resize to stop the volumes at all we have it as alpha you can try it and see",
    "start": "1541830",
    "end": "1547160"
  },
  {
    "text": "bug fix bugs and bug fixes in line",
    "start": "1547160",
    "end": "1554580"
  },
  {
    "start": "1551000",
    "end": "1601000"
  },
  {
    "text": "volumes initially it was not on flood on the slides because it's damn dangerous but you'd need to use persistent volume",
    "start": "1554580",
    "end": "1564150"
  },
  {
    "text": "claims and persistent volumes at all you can reference a volume directly in a port we call them in line volumes they",
    "start": "1564150",
    "end": "1571260"
  },
  {
    "text": "are in line in the pot so this port runs test container and the test container",
    "start": "1571260",
    "end": "1579870"
  },
  {
    "text": "gets mounted this Amazon volume this makes the pot completely not",
    "start": "1579870",
    "end": "1586380"
  },
  {
    "text": "portable you can't take the pot and run it somewhere else because the volume is not there but maybe that's what you want you",
    "start": "1586380",
    "end": "1596900"
  },
  {
    "text": "need to just test something or run a job it is usable what more usable for inline",
    "start": "1596900",
    "end": "1603890"
  },
  {
    "start": "1601000",
    "end": "1661000"
  },
  {
    "text": "volumes are ephemeral volumes you already maybe you already met them in kubernetes we used config maps and",
    "start": "1603890",
    "end": "1610460"
  },
  {
    "text": "secrets and Denver API where you can get a configuration or some secrets user",
    "start": "1610460",
    "end": "1619909"
  },
  {
    "text": "names and passwords or SSH keys as files inside your pods you can create API",
    "start": "1619909",
    "end": "1625610"
  },
  {
    "text": "object called concept map or secret with which are basically key value this is a",
    "start": "1625610",
    "end": "1632840"
  },
  {
    "text": "key value store and for each key you get a file on a file system in inside your",
    "start": "1632840",
    "end": "1639110"
  },
  {
    "text": "pod and the value will be saved as data in the file this is useful for",
    "start": "1639110",
    "end": "1646610"
  },
  {
    "text": "distributing user names and password or I personally use it to distribute SSH keys these volumes are not persistent",
    "start": "1646610",
    "end": "1656210"
  },
  {
    "text": "you basically kind of write to them because it gets overwritten by the value from API server and last feature is",
    "start": "1656210",
    "end": "1663380"
  },
  {
    "start": "1661000",
    "end": "1762000"
  },
  {
    "text": "container interface so yes we also have",
    "start": "1663380",
    "end": "1668900"
  },
  {
    "text": "some new features to talk about some of those that Yann talked about when you as well but we have the container storage",
    "start": "1668900",
    "end": "1675409"
  },
  {
    "text": "interface which is an industry standard that kind of enables storage vendors to",
    "start": "1675409",
    "end": "1680600"
  },
  {
    "text": "write a storage plug-in once and deploy it anywhere so a lot of different",
    "start": "1680600",
    "end": "1685730"
  },
  {
    "text": "container orchestrators have implemented the container storage interface and they're all going to interface with the",
    "start": "1685730",
    "end": "1692090"
  },
  {
    "text": "same drivers so kubernetes as well we've implemented support for CSI and we have",
    "start": "1692090",
    "end": "1698299"
  },
  {
    "text": "a growing number of drivers in the ecosystem I believe it's over 30 now that we're aware of that you can use in",
    "start": "1698299",
    "end": "1705350"
  },
  {
    "text": "kubernetes so from the user perspective there's actually no change whatsoever",
    "start": "1705350",
    "end": "1711679"
  },
  {
    "text": "you still use the pods and PVCs that we've been talking about as usual but",
    "start": "1711679",
    "end": "1717980"
  },
  {
    "text": "this adds some extra work to the cluster admin which written up here but not only extra work",
    "start": "1717980",
    "end": "1724400"
  },
  {
    "text": "it adds a lot of benefits as well right so the benefits of the container storage interface are going to be well it's an",
    "start": "1724400",
    "end": "1731630"
  },
  {
    "text": "extensible plug-in framework for your storage drivers right so now these plugins are not in the system itself if",
    "start": "1731630",
    "end": "1740059"
  },
  {
    "text": "they crash or something you're not going to bring down all of kubernetes with it they're the vendors are going to be able",
    "start": "1740059",
    "end": "1746179"
  },
  {
    "text": "to push out more frequent updates to their drivers that you as a cluster admin can pull in much quicker as well",
    "start": "1746179",
    "end": "1753110"
  },
  {
    "text": "as many other benefits that come with having a plug-in interface instead of what we had before which was all the",
    "start": "1753110",
    "end": "1760040"
  },
  {
    "text": "plugins being built into kubernetes another feature that we're actively",
    "start": "1760040",
    "end": "1765350"
  },
  {
    "start": "1762000",
    "end": "1790000"
  },
  {
    "text": "working on that is currently alpha is volume snapshots so this is a part of",
    "start": "1765350",
    "end": "1771770"
  },
  {
    "text": "the container storage interface basically it lets you take a snapshot of a PVC at a point in time and we get the",
    "start": "1771770",
    "end": "1779570"
  },
  {
    "text": "snapshot and then we can then provision a new volume based on that snapshot and",
    "start": "1779570",
    "end": "1785360"
  },
  {
    "text": "and use it later in a different pod or the same pod or something okay we have",
    "start": "1785360",
    "end": "1791360"
  },
  {
    "start": "1790000",
    "end": "1833000"
  },
  {
    "text": "one final feature to talk about which is CSI migration this is still also in",
    "start": "1791360",
    "end": "1796550"
  },
  {
    "text": "alpha but this is a push to kind of deprecated sort of the entry plugins the",
    "start": "1796550",
    "end": "1804440"
  },
  {
    "text": "API is going to stay if you're using entry plugins today I want to make it very clear that you can still continue",
    "start": "1804440",
    "end": "1811340"
  },
  {
    "text": "to use those in the future but what we're doing is actually removing all the backend implementation and shimming that",
    "start": "1811340",
    "end": "1819290"
  },
  {
    "text": "to the CSI driver so as I mentioned the container storage interface before 30 plus drivers more frequent updates",
    "start": "1819290",
    "end": "1825980"
  },
  {
    "text": "better security guarantees we're shimming the back end to that so you can still use the same entry plug-in",
    "start": "1825980",
    "end": "1831860"
  },
  {
    "text": "interface okay so to summarize the main",
    "start": "1831860",
    "end": "1837550"
  },
  {
    "start": "1833000",
    "end": "1884000"
  },
  {
    "text": "takeaway that you should have from this talk is this image right there are there",
    "start": "1837550",
    "end": "1844880"
  },
  {
    "text": "are two main parts of the storage system in kubernetes right we split it out into",
    "start": "1844880",
    "end": "1850940"
  },
  {
    "text": "a Schrader's and into user workload and we",
    "start": "1850940",
    "end": "1857030"
  },
  {
    "text": "have the persistent volume claim for the user request and we have the persistent volume for the administrator for all of",
    "start": "1857030",
    "end": "1863630"
  },
  {
    "text": "the gory details of your underlying storage okay and also we have the storage class which kind of groups",
    "start": "1863630",
    "end": "1869510"
  },
  {
    "text": "persistent volumes together and we can use that for dynamic provisioning these are the main concepts we also have those",
    "start": "1869510",
    "end": "1875570"
  },
  {
    "text": "higher-level workloads deployments staple sets and all of those features",
    "start": "1875570",
    "end": "1880580"
  },
  {
    "text": "you can look up documentation for online so that's not all we want to if you're",
    "start": "1880580",
    "end": "1888560"
  },
  {
    "start": "1884000",
    "end": "1936000"
  },
  {
    "text": "interested in storage we have a ton of talks about storage in at cube con about",
    "start": "1888560",
    "end": "1894500"
  },
  {
    "text": "half of these actually have already finished but I want to call out if",
    "start": "1894500",
    "end": "1899960"
  },
  {
    "text": "you're interested in contributing or interested in more about what we're working on currently I would recommend",
    "start": "1899960",
    "end": "1906260"
  },
  {
    "text": "coming to the intro deep dive for the kubernetes storage sig and of course if",
    "start": "1906260",
    "end": "1912920"
  },
  {
    "text": "you're interested in any of these other specific storage related tutorials or talks you know take a picture of this",
    "start": "1912920",
    "end": "1920600"
  },
  {
    "text": "look it up and you'll find it on the schedule you can reach out we're the kubernetes Storage sig we have bi-weekly",
    "start": "1920600",
    "end": "1927440"
  },
  {
    "text": "meetings we run slack and we have a mailing list so this these slides are",
    "start": "1927440",
    "end": "1932960"
  },
  {
    "text": "online on the schedule you can download them and click on these links thank you so much for coming and thank you for",
    "start": "1932960",
    "end": "1938960"
  },
  {
    "start": "1936000",
    "end": "2245000"
  },
  {
    "text": "listening",
    "start": "1938960",
    "end": "1941049"
  },
  {
    "text": "again my name is David and this is Yan we're just gonna open up the floor for any questions now hi um with things like",
    "start": "1947820",
    "end": "1963070"
  },
  {
    "text": "snapshots that might or might not work here anyway uh as a user of the closet I can find out if snapshots are gonna work",
    "start": "1963070",
    "end": "1969730"
  },
  {
    "text": "on the particular cluster I'm deploying them so the question is if a user is",
    "start": "1969730",
    "end": "1977290"
  },
  {
    "text": "there any way to know whether snapshots will work for the for the storage that I'm using it's not anywhere it's not in",
    "start": "1977290",
    "end": "1989980"
  },
  {
    "text": "the storage for us it's not in anywhere you just try and do it will fail it",
    "start": "1989980",
    "end": "1998470"
  },
  {
    "text": "should be in the documentation provided by your Questor administer admin your",
    "start": "1998470",
    "end": "2003750"
  },
  {
    "text": "closed or admin should provide that documentation yeah any any bugs you want",
    "start": "2003750",
    "end": "2011580"
  },
  {
    "text": "to file any issues you have you can file them on kubernetes kubernetes and tag",
    "start": "2011580",
    "end": "2017220"
  },
  {
    "text": "sake storage in them all right any other questions I saw Sango so can we make the put to",
    "start": "2017220",
    "end": "2031230"
  },
  {
    "text": "the local volume to crud stray so I mean when we create a percent volume inside",
    "start": "2031230",
    "end": "2040440"
  },
  {
    "text": "in the local and then light something and eventually copied to the crowd",
    "start": "2040440",
    "end": "2048360"
  },
  {
    "text": "switch like we Gugu or a depressed and then my question is can we mount the",
    "start": "2048360",
    "end": "2057350"
  },
  {
    "text": "copied volume as a new Pashtun poem in the crowd I'm not sure I understood the",
    "start": "2057350",
    "end": "2066690"
  },
  {
    "text": "question from if I could try to rephrase were you asking if a pod writes to a",
    "start": "2066690",
    "end": "2072990"
  },
  {
    "text": "local volume and then we copy that data to a cloud volume then can we then mount",
    "start": "2072990",
    "end": "2079740"
  },
  {
    "text": "that cloud volume into a different plot or back into the same pod in any pod okay yeah",
    "start": "2079740",
    "end": "2089020"
  },
  {
    "text": "so so I think we can just break this down into two separate steps right the pod writes to the local volume you just",
    "start": "2089020",
    "end": "2094750"
  },
  {
    "text": "use all of the kubernetes concepts we've covered to mount that and use your local volume copying the data to the cloud I",
    "start": "2094750",
    "end": "2101829"
  },
  {
    "text": "think that's a separate operation that you would do outside of kubernetes kind of right and then mounting that cloud",
    "start": "2101829",
    "end": "2108280"
  },
  {
    "text": "volume to a pod again is just using the same basic concepts to just create the",
    "start": "2108280",
    "end": "2114190"
  },
  {
    "text": "pv for that cloud volume create a PPC that then references it and then consume",
    "start": "2114190",
    "end": "2119230"
  },
  {
    "text": "that by your pot does that answer all right any other questions",
    "start": "2119230",
    "end": "2124859"
  },
  {
    "text": "hi I would like to ask so we have a particular situation we have an NFS",
    "start": "2130849",
    "end": "2136349"
  },
  {
    "text": "storage that we want to present to a governess cluster but it has its very particular every directory needs to be",
    "start": "2136349",
    "end": "2143400"
  },
  {
    "text": "accessed only by specific namespaces so basically we want a namespace to be able",
    "start": "2143400",
    "end": "2148619"
  },
  {
    "text": "to mount one and only one directory within this storage so as far as they",
    "start": "2148619",
    "end": "2154440"
  },
  {
    "text": "know persistent volumes cannot be bound to namespaces and we cannot create a",
    "start": "2154440",
    "end": "2160559"
  },
  {
    "text": "storage class because for host path it's not supported so for now the only way we",
    "start": "2160559",
    "end": "2165660"
  },
  {
    "text": "can control this is using port security policies and preventing the prefix of",
    "start": "2165660",
    "end": "2171390"
  },
  {
    "text": "the path to each user that is connected to each namespace but this is a very",
    "start": "2171390",
    "end": "2177869"
  },
  {
    "text": "complex solution for a very kind of simple problem so is there an",
    "start": "2177869",
    "end": "2182880"
  },
  {
    "text": "alternative way to make sure that a particular persistent volume is mounted",
    "start": "2182880",
    "end": "2188819"
  },
  {
    "text": "only by a specific namespace there is no easy solutions there is no easy solution",
    "start": "2188819",
    "end": "2196440"
  },
  {
    "text": "resistant volumes are sort of global for everybody we don't restrict them by namespace but what we do restrict our",
    "start": "2196440",
    "end": "2203430"
  },
  {
    "text": "storage classes so maybe we didn't say it during the talk but you can restrict",
    "start": "2203430",
    "end": "2209279"
  },
  {
    "text": "a storage class with quota and give access to a storage class only to one",
    "start": "2209279",
    "end": "2214920"
  },
  {
    "text": "namespace that's possible but then in your case you need to create a namespace a class for each namespace I don't know",
    "start": "2214920",
    "end": "2222960"
  },
  {
    "text": "how many namespaces are you going to have if it is view that it's probably feasible if it is thousands maybe you",
    "start": "2222960",
    "end": "2230789"
  },
  {
    "text": "need some operator that will pre create it for you or something like that all",
    "start": "2230789",
    "end": "2238650"
  },
  {
    "text": "right we're out of time for questions now thank you so much for coming and have a great rest of the day",
    "start": "2238650",
    "end": "2244070"
  },
  {
    "text": "[Applause]",
    "start": "2244070",
    "end": "2247249"
  }
]