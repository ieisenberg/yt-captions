[
  {
    "text": "hi um I'm vjit and this is sh we work on",
    "start": "480",
    "end": "4600"
  },
  {
    "text": "a platform called neaf",
    "start": "4600",
    "end": "7200"
  },
  {
    "text": "in turn it off um we work on a platform",
    "start": "7200",
    "end": "11200"
  },
  {
    "text": "called Numa flow that's an open source",
    "start": "11200",
    "end": "12719"
  },
  {
    "text": "solution and what we going to talk about",
    "start": "12719",
    "end": "14480"
  },
  {
    "text": "today is how we use that platform at",
    "start": "14480",
    "end": "16600"
  },
  {
    "text": "scale at in it to do uh streaming",
    "start": "16600",
    "end": "20600"
  },
  {
    "text": "inference we'll talk a little bit a",
    "start": "20600",
    "end": "22880"
  },
  {
    "text": "little about in it we'll talk about",
    "start": "22880",
    "end": "25359"
  },
  {
    "text": "stream processing in AI the challenges",
    "start": "25359",
    "end": "27760"
  },
  {
    "text": "for the ml teams what new flow is about",
    "start": "27760",
    "end": "30840"
  },
  {
    "text": "and we'll show you a live",
    "start": "30840",
    "end": "32480"
  },
  {
    "text": "demo so this is the IND Mission powering",
    "start": "32480",
    "end": "35120"
  },
  {
    "text": "Prosperity around the world and this is",
    "start": "35120",
    "end": "37680"
  },
  {
    "text": "what the technology at IND it look like",
    "start": "37680",
    "end": "39280"
  },
  {
    "text": "we have about 100 million customers we",
    "start": "39280",
    "end": "42600"
  },
  {
    "text": "move billions 100 100 plus billion",
    "start": "42600",
    "end": "44960"
  },
  {
    "text": "Revenue in customer tax we have",
    "start": "44960",
    "end": "47800"
  },
  {
    "text": "trillions of dollars moved around in",
    "start": "47800",
    "end": "50199"
  },
  {
    "text": "invoicing and 18 million plus users in",
    "start": "50199",
    "end": "53440"
  },
  {
    "text": "payroll that IND it uses so this talks",
    "start": "53440",
    "end": "55719"
  },
  {
    "text": "about the scale and the solution we have",
    "start": "55719",
    "end": "57280"
  },
  {
    "text": "to put in to make sure that we are",
    "start": "57280",
    "end": "58719"
  },
  {
    "text": "always highly available and",
    "start": "58719",
    "end": "61160"
  },
  {
    "text": "reliable we are strong believer in open",
    "start": "61160",
    "end": "63519"
  },
  {
    "text": "source uh we have been we have received",
    "start": "63519",
    "end": "66320"
  },
  {
    "text": "the end user award in 2019 and 20122 um",
    "start": "66320",
    "end": "69720"
  },
  {
    "text": "we have also contributed a lot to open",
    "start": "69720",
    "end": "71280"
  },
  {
    "text": "source Argo is the most popular one and",
    "start": "71280",
    "end": "74159"
  },
  {
    "text": "newo is the upcoming one among others we",
    "start": "74159",
    "end": "78400"
  },
  {
    "text": "are also ucer of lot of Open Source",
    "start": "78400",
    "end": "80400"
  },
  {
    "text": "technology on both cloud and",
    "start": "80400",
    "end": "82960"
  },
  {
    "text": "mobile now let me talk about stream",
    "start": "82960",
    "end": "85079"
  },
  {
    "text": "processing in AI before I talk about it",
    "start": "85079",
    "end": "88079"
  },
  {
    "text": "um let me just give you a scale that we",
    "start": "88079",
    "end": "89479"
  },
  {
    "text": "operate just this is a part where we",
    "start": "89479",
    "end": "92680"
  },
  {
    "text": "have the a native development platform",
    "start": "92680",
    "end": "94520"
  },
  {
    "text": "and this just pure a native platform we",
    "start": "94520",
    "end": "96759"
  },
  {
    "text": "do millions of models running in a day",
    "start": "96759",
    "end": "99280"
  },
  {
    "text": "and we make around 40 million inferences",
    "start": "99280",
    "end": "101320"
  },
  {
    "text": "and so",
    "start": "101320",
    "end": "102799"
  },
  {
    "text": "forth now what is stream processing in",
    "start": "102799",
    "end": "105479"
  },
  {
    "text": "AI right it's all about getting data",
    "start": "105479",
    "end": "108040"
  },
  {
    "text": "from an unbounded stream mostly users",
    "start": "108040",
    "end": "110759"
  },
  {
    "text": "click coming in operating on the individ",
    "start": "110759",
    "end": "114719"
  },
  {
    "text": "products and then we do feature",
    "start": "114719",
    "end": "116479"
  },
  {
    "text": "engineering we do inference and some of",
    "start": "116479",
    "end": "118640"
  },
  {
    "text": "them we uh use it back for training and",
    "start": "118640",
    "end": "121560"
  },
  {
    "text": "this data is used for um things like",
    "start": "121560",
    "end": "124920"
  },
  {
    "text": "personalization and things like",
    "start": "124920",
    "end": "127520"
  },
  {
    "text": "that so in this world where the backbone",
    "start": "127520",
    "end": "131920"
  },
  {
    "text": "is all about Eventing right what are the",
    "start": "131920",
    "end": "134519"
  },
  {
    "text": "challenges for ML Engineers that made us",
    "start": "134519",
    "end": "136920"
  },
  {
    "text": "build this",
    "start": "136920",
    "end": "138720"
  },
  {
    "text": "platform first and foremost the stream",
    "start": "138720",
    "end": "141519"
  },
  {
    "text": "processing is usually very much tied",
    "start": "141519",
    "end": "143560"
  },
  {
    "text": "with data engineers and uh they you need",
    "start": "143560",
    "end": "147800"
  },
  {
    "text": "to know a lot of jvm based Technologies",
    "start": "147800",
    "end": "149519"
  },
  {
    "text": "to get do it but most of the ml",
    "start": "149519",
    "end": "151920"
  },
  {
    "text": "Engineers when they do realtime",
    "start": "151920",
    "end": "153599"
  },
  {
    "text": "processing inference they need to do a",
    "start": "153599",
    "end": "155480"
  },
  {
    "text": "lot of data scrubbing lot of data",
    "start": "155480",
    "end": "157440"
  },
  {
    "text": "processing in the tool set which they",
    "start": "157440",
    "end": "159400"
  },
  {
    "text": "are familiar with that is not that's the",
    "start": "159400",
    "end": "162680"
  },
  {
    "text": "basic python or something around that",
    "start": "162680",
    "end": "165640"
  },
  {
    "text": "that they are familiar with and they",
    "start": "165640",
    "end": "167319"
  },
  {
    "text": "they don't want to spend much time on",
    "start": "167319",
    "end": "168800"
  },
  {
    "text": "learning these Big Data Systems so",
    "start": "168800",
    "end": "171120"
  },
  {
    "text": "that's a challenge number one Challenge",
    "start": "171120",
    "end": "173440"
  },
  {
    "text": "number two is that they need to learn a",
    "start": "173440",
    "end": "175879"
  },
  {
    "text": "lot of boiler plate code or right in",
    "start": "175879",
    "end": "177920"
  },
  {
    "text": "fact let's say if you're reading from",
    "start": "177920",
    "end": "179879"
  },
  {
    "text": "Kafka or then we have few systems",
    "start": "179879",
    "end": "182080"
  },
  {
    "text": "connected through Apache pulsar and",
    "start": "182080",
    "end": "183599"
  },
  {
    "text": "other things so there's a lot of boiler",
    "start": "183599",
    "end": "184959"
  },
  {
    "text": "plate code being written and M Engineers",
    "start": "184959",
    "end": "188080"
  },
  {
    "text": "love what they do that is doing",
    "start": "188080",
    "end": "190159"
  },
  {
    "text": "inference and training so we wanted to",
    "start": "190159",
    "end": "192959"
  },
  {
    "text": "have a system that can help them do what",
    "start": "192959",
    "end": "196080"
  },
  {
    "text": "they love the best that is machine",
    "start": "196080",
    "end": "198000"
  },
  {
    "text": "learning and lastly it is",
    "start": "198000",
    "end": "200560"
  },
  {
    "text": "scalability we wanted a system that can",
    "start": "200560",
    "end": "203519"
  },
  {
    "text": "understand ml that is we need to be able",
    "start": "203519",
    "end": "206000"
  },
  {
    "text": "to scale on let's say a model needs GPU",
    "start": "206000",
    "end": "209000"
  },
  {
    "text": "we should be able to place that model on",
    "start": "209000",
    "end": "211640"
  },
  {
    "text": "a GPU so it can scale so scalability and",
    "start": "211640",
    "end": "214640"
  },
  {
    "text": "how you deploy that it has to be",
    "start": "214640",
    "end": "216200"
  },
  {
    "text": "abstracted out right so that was the",
    "start": "216200",
    "end": "218640"
  },
  {
    "text": "these are the three main reasons what we",
    "start": "218640",
    "end": "220360"
  },
  {
    "text": "saw and that was hindering ml",
    "start": "220360",
    "end": "222159"
  },
  {
    "text": "development",
    "start": "222159",
    "end": "223480"
  },
  {
    "text": "velocity so what is Numa",
    "start": "223480",
    "end": "226080"
  },
  {
    "text": "flow the way we Define Numa flow is as a",
    "start": "226080",
    "end": "229080"
  },
  {
    "text": "seress platform for stream processing",
    "start": "229080",
    "end": "232120"
  },
  {
    "text": "and first and foremost what we do is we",
    "start": "232120",
    "end": "234159"
  },
  {
    "text": "abstract the infrastructure we do ml we",
    "start": "234159",
    "end": "237040"
  },
  {
    "text": "enable ml Engineers by making sure that",
    "start": "237040",
    "end": "239360"
  },
  {
    "text": "the engine is very familiar to their",
    "start": "239360",
    "end": "243040"
  },
  {
    "text": "day-to-day python code right it's kind",
    "start": "243040",
    "end": "245040"
  },
  {
    "text": "of language agnostic and we let us us",
    "start": "245040",
    "end": "248599"
  },
  {
    "text": "being the platform itself take care of",
    "start": "248599",
    "end": "250480"
  },
  {
    "text": "the infrastructure and they not worry",
    "start": "250480",
    "end": "252640"
  },
  {
    "text": "about it second is we decoupled the",
    "start": "252640",
    "end": "255760"
  },
  {
    "text": "logic the business logic from Source",
    "start": "255760",
    "end": "257479"
  },
  {
    "text": "sens sync for example uh users are more",
    "start": "257479",
    "end": "260320"
  },
  {
    "text": "interested in the payload they really",
    "start": "260320",
    "end": "261919"
  },
  {
    "text": "don't care whether the payload came from",
    "start": "261919",
    "end": "263479"
  },
  {
    "text": "kafa or Pulsar or HTTP uh so we want to",
    "start": "263479",
    "end": "267520"
  },
  {
    "text": "make sure that their their ml core logic",
    "start": "267520",
    "end": "271320"
  },
  {
    "text": "can be applied to any",
    "start": "271320",
    "end": "273080"
  },
  {
    "text": "system and not be tied to any particular",
    "start": "273080",
    "end": "277039"
  },
  {
    "text": "Source or a sync sync being the",
    "start": "277039",
    "end": "278880"
  },
  {
    "text": "destination where you write it to how or",
    "start": "278880",
    "end": "280680"
  },
  {
    "text": "how the data is being used once it has",
    "start": "280680",
    "end": "282360"
  },
  {
    "text": "been",
    "start": "282360",
    "end": "283360"
  },
  {
    "text": "infered lastly we want to make sure that",
    "start": "283360",
    "end": "285759"
  },
  {
    "text": "it is scalable this is meaning if you",
    "start": "285759",
    "end": "288000"
  },
  {
    "text": "want more CPUs or gpus while doing",
    "start": "288000",
    "end": "290320"
  },
  {
    "text": "inference because there will be burst of",
    "start": "290320",
    "end": "292440"
  },
  {
    "text": "traffic like sessionization traffic",
    "start": "292440",
    "end": "294120"
  },
  {
    "text": "could come in more and then you want to",
    "start": "294120",
    "end": "295919"
  },
  {
    "text": "make sure that these things are taken",
    "start": "295919",
    "end": "298080"
  },
  {
    "text": "care by the platform rather than hand",
    "start": "298080",
    "end": "299919"
  },
  {
    "text": "tuning or fine-tuning it manually by",
    "start": "299919",
    "end": "301680"
  },
  {
    "text": "each ml engineer for their use case it",
    "start": "301680",
    "end": "303600"
  },
  {
    "text": "should be a feature of the platform",
    "start": "303600",
    "end": "305720"
  },
  {
    "text": "rather than something a user",
    "start": "305720",
    "end": "308039"
  },
  {
    "text": "does so what does a stream procing look",
    "start": "308039",
    "end": "311919"
  },
  {
    "text": "like it's all about reading from source",
    "start": "311919",
    "end": "314520"
  },
  {
    "text": "so this is something we out of the box",
    "start": "314520",
    "end": "316520"
  },
  {
    "text": "it could be anything user does not have",
    "start": "316520",
    "end": "317880"
  },
  {
    "text": "to worry about it the first point is the",
    "start": "317880",
    "end": "319680"
  },
  {
    "text": "user defined function where the ml",
    "start": "319680",
    "end": "321600"
  },
  {
    "text": "engineer writes some code which is very",
    "start": "321600",
    "end": "323240"
  },
  {
    "text": "specific to what they need and in many",
    "start": "323240",
    "end": "326400"
  },
  {
    "text": "cases they want to do uh something like",
    "start": "326400",
    "end": "328880"
  },
  {
    "text": "conditional forwarding based on the",
    "start": "328880",
    "end": "330199"
  },
  {
    "text": "featur features that come in for example",
    "start": "330199",
    "end": "332160"
  },
  {
    "text": "if they have low cardinality data they",
    "start": "332160",
    "end": "333759"
  },
  {
    "text": "might want to run against one kind of",
    "start": "333759",
    "end": "335400"
  },
  {
    "text": "model if they have complex data set they",
    "start": "335400",
    "end": "337680"
  },
  {
    "text": "might want to do something else so we",
    "start": "337680",
    "end": "339400"
  },
  {
    "text": "support um conditional forwarding and",
    "start": "339400",
    "end": "341360"
  },
  {
    "text": "then you can write it to anywhere they",
    "start": "341360",
    "end": "342680"
  },
  {
    "text": "want mostly they republish the data into",
    "start": "342680",
    "end": "345319"
  },
  {
    "text": "Kafka or right to blob store so others",
    "start": "345319",
    "end": "347280"
  },
  {
    "text": "can consume this data and do much more",
    "start": "347280",
    "end": "350600"
  },
  {
    "text": "um personalization on the end product",
    "start": "350600",
    "end": "353680"
  },
  {
    "text": "for example",
    "start": "353680",
    "end": "356240"
  },
  {
    "text": "websites now I'll hand it over to sh to",
    "start": "357280",
    "end": "360560"
  },
  {
    "text": "uh show you a demo because there was a",
    "start": "360560",
    "end": "362400"
  },
  {
    "text": "lot of things in in this and I I hope a",
    "start": "362400",
    "end": "364880"
  },
  {
    "text": "demo will bring it closer so sh it's all",
    "start": "364880",
    "end": "368639"
  },
  {
    "text": "your thank you vjit uh for giving a",
    "start": "368639",
    "end": "371680"
  },
  {
    "text": "quick introduction about Nema flow so",
    "start": "371680",
    "end": "373800"
  },
  {
    "text": "this is going to be a completely a live",
    "start": "373800",
    "end": "375160"
  },
  {
    "text": "demo I hope the uh demo gods are with me",
    "start": "375160",
    "end": "378400"
  },
  {
    "text": "uh so let me go to the next slide I",
    "start": "378400",
    "end": "380680"
  },
  {
    "text": "think yeah cool so in this example first",
    "start": "380680",
    "end": "383800"
  },
  {
    "text": "uh what we are going to do is as we said",
    "start": "383800",
    "end": "385240"
  },
  {
    "text": "neopl is primarily a stream processing",
    "start": "385240",
    "end": "387400"
  },
  {
    "text": "platform and uh where you can receive",
    "start": "387400",
    "end": "389680"
  },
  {
    "text": "events from any source that you might be",
    "start": "389680",
    "end": "391199"
  },
  {
    "text": "interested in so there is a source or a",
    "start": "391199",
    "end": "394360"
  },
  {
    "text": "a stream of source that you receiving",
    "start": "394360",
    "end": "395680"
  },
  {
    "text": "the events from and the second that",
    "start": "395680",
    "end": "397319"
  },
  {
    "text": "you're trying to do here is like you're",
    "start": "397319",
    "end": "398639"
  },
  {
    "text": "doing a prediction uh I'm just doing a",
    "start": "398639",
    "end": "401400"
  },
  {
    "text": "simple uh image recognition or rather uh",
    "start": "401400",
    "end": "403919"
  },
  {
    "text": "a description of an image uh that is",
    "start": "403919",
    "end": "405759"
  },
  {
    "text": "given as an input to this Pipeline and",
    "start": "405759",
    "end": "408000"
  },
  {
    "text": "then lastly uh I'm writing uh these",
    "start": "408000",
    "end": "410800"
  },
  {
    "text": "results whatever the uh for the input",
    "start": "410800",
    "end": "412599"
  },
  {
    "text": "that I'm sending for the prediction into",
    "start": "412599",
    "end": "414440"
  },
  {
    "text": "a sync so in this demo uh what I'm going",
    "start": "414440",
    "end": "417160"
  },
  {
    "text": "to do is like I'm just going to use uh",
    "start": "417160",
    "end": "418840"
  },
  {
    "text": "HTP SCE but uh nlow does come with like",
    "start": "418840",
    "end": "421840"
  },
  {
    "text": "lot of uh inbuilt sources like HTTP",
    "start": "421840",
    "end": "424199"
  },
  {
    "text": "Kafka pulser and few others and if you",
    "start": "424199",
    "end": "426520"
  },
  {
    "text": "are interested in writing your own",
    "start": "426520",
    "end": "427800"
  },
  {
    "text": "sources and you'll be able to write that",
    "start": "427800",
    "end": "429400"
  },
  {
    "text": "too because we provide like uh multi",
    "start": "429400",
    "end": "431599"
  },
  {
    "text": "language support uh with all kinds of",
    "start": "431599",
    "end": "433360"
  },
  {
    "text": "sdks ranging from uh python Java go and",
    "start": "433360",
    "end": "437000"
  },
  {
    "text": "even in Rust if you are interested uh so",
    "start": "437000",
    "end": "439560"
  },
  {
    "text": "for this example I'm just using a HTTP",
    "start": "439560",
    "end": "441599"
  },
  {
    "text": "uh source and then uh I'm writing the",
    "start": "441599",
    "end": "443879"
  },
  {
    "text": "results to a log sing uh just for the",
    "start": "443879",
    "end": "445879"
  },
  {
    "text": "demo purpose but you can write your",
    "start": "445879",
    "end": "447639"
  },
  {
    "text": "results to any Downstream that you're",
    "start": "447639",
    "end": "449160"
  },
  {
    "text": "interested in right from a DB to CF car",
    "start": "449160",
    "end": "451280"
  },
  {
    "text": "anything that you would like and uh for",
    "start": "451280",
    "end": "453479"
  },
  {
    "text": "the prediction I'm just using a llm",
    "start": "453479",
    "end": "455360"
  },
  {
    "text": "model serving through VM so essentially",
    "start": "455360",
    "end": "458800"
  },
  {
    "text": "here the model is hosted locally on the",
    "start": "458800",
    "end": "461919"
  },
  {
    "text": "Pod itself so let's take a quick uh demo",
    "start": "461919",
    "end": "466080"
  },
  {
    "text": "of how it's going to look like uh let me",
    "start": "466080",
    "end": "469759"
  },
  {
    "text": "go to so what I'm trying to do here is",
    "start": "469759",
    "end": "472479"
  },
  {
    "text": "I'm just sending a polar bear as an",
    "start": "472479",
    "end": "474319"
  },
  {
    "text": "image and then trying to send it to the",
    "start": "474319",
    "end": "475759"
  },
  {
    "text": "L model to get a description out of that",
    "start": "475759",
    "end": "478599"
  },
  {
    "text": "so let's let's do",
    "start": "478599",
    "end": "481400"
  },
  {
    "text": "that so what I'm trying to do here is uh",
    "start": "481400",
    "end": "484199"
  },
  {
    "text": "we have exposed the our source work as",
    "start": "484199",
    "end": "486639"
  },
  {
    "text": "like essentially HTTP source and uh I'm",
    "start": "486639",
    "end": "489759"
  },
  {
    "text": "sending a polar bear image you can just",
    "start": "489759",
    "end": "492039"
  },
  {
    "text": "see it here like just as an example so",
    "start": "492039",
    "end": "494360"
  },
  {
    "text": "I'm just sending that polar bear image",
    "start": "494360",
    "end": "496240"
  },
  {
    "text": "to the uh I'm just going to send one",
    "start": "496240",
    "end": "499720"
  },
  {
    "text": "more okay now let's go to the",
    "start": "499720",
    "end": "504280"
  },
  {
    "text": "UI so nflow comes with the inbuilt uh UI",
    "start": "504280",
    "end": "508360"
  },
  {
    "text": "where you can uh create and also look at",
    "start": "508360",
    "end": "510879"
  },
  {
    "text": "all of your uh pipelines how they are",
    "start": "510879",
    "end": "513320"
  },
  {
    "text": "performing and all of that I'll I'll",
    "start": "513320",
    "end": "514800"
  },
  {
    "text": "show all of those features that are",
    "start": "514800",
    "end": "516399"
  },
  {
    "text": "available in the yoi right here but uh",
    "start": "516399",
    "end": "518760"
  },
  {
    "text": "let's take a a quick example output of",
    "start": "518760",
    "end": "521159"
  },
  {
    "text": "like what was the uh image and what was",
    "start": "521159",
    "end": "523360"
  },
  {
    "text": "the uh output the llm model gave you",
    "start": "523360",
    "end": "525920"
  },
  {
    "text": "here so like I sent out a polar bear as",
    "start": "525920",
    "end": "528040"
  },
  {
    "text": "an image and that it says that the image",
    "start": "528040",
    "end": "529920"
  },
  {
    "text": "shows a polar bear walking on a snowy",
    "start": "529920",
    "end": "531640"
  },
  {
    "text": "surface and the bear is walking on all",
    "start": "531640",
    "end": "533800"
  },
  {
    "text": "four on fours and with its front bar",
    "start": "533800",
    "end": "536399"
  },
  {
    "text": "slightly raised uh and its head turned",
    "start": "536399",
    "end": "538560"
  },
  {
    "text": "to the side so essentially I'm just",
    "start": "538560",
    "end": "539959"
  },
  {
    "text": "using an llm model and then I'm trying",
    "start": "539959",
    "end": "541560"
  },
  {
    "text": "to send it as a I I sent out an input",
    "start": "541560",
    "end": "543680"
  },
  {
    "text": "and then you're able to see the result",
    "start": "543680",
    "end": "545680"
  },
  {
    "text": "uh right here so in short if I have to",
    "start": "545680",
    "end": "548200"
  },
  {
    "text": "quickly summarize right uh here as an ml",
    "start": "548200",
    "end": "552320"
  },
  {
    "text": "engineer are uh the teams ml teams will",
    "start": "552320",
    "end": "554440"
  },
  {
    "text": "be primarily focusing on the inference",
    "start": "554440",
    "end": "557000"
  },
  {
    "text": "step where you're just Pro provisioning",
    "start": "557000",
    "end": "558680"
  },
  {
    "text": "your model or serving your model and not",
    "start": "558680",
    "end": "560240"
  },
  {
    "text": "worry about your source and sync because",
    "start": "560240",
    "end": "561959"
  },
  {
    "text": "majority of these are taken care by the",
    "start": "561959",
    "end": "563680"
  },
  {
    "text": "platform out of the box for you uh apart",
    "start": "563680",
    "end": "566200"
  },
  {
    "text": "from that there are lot more cool",
    "start": "566200",
    "end": "567399"
  },
  {
    "text": "features and if at all you're interested",
    "start": "567399",
    "end": "568959"
  },
  {
    "text": "in uh debugging and trying to understand",
    "start": "568959",
    "end": "571040"
  },
  {
    "text": "what is happening with any of these",
    "start": "571040",
    "end": "572200"
  },
  {
    "text": "vertices you can look at all of your",
    "start": "572200",
    "end": "573880"
  },
  {
    "text": "metries all of those within the platform",
    "start": "573880",
    "end": "576000"
  },
  {
    "text": "so we are bringing the even the",
    "start": "576000",
    "end": "577839"
  },
  {
    "text": "debugging uh functionality close to",
    "start": "577839",
    "end": "580079"
  },
  {
    "text": "where the teams actually try to deploy",
    "start": "580079",
    "end": "582279"
  },
  {
    "text": "and work on their pipelines upon so this",
    "start": "582279",
    "end": "584880"
  },
  {
    "text": "is like one demo where we are uh sending",
    "start": "584880",
    "end": "587240"
  },
  {
    "text": "out uh I'm just sending out an image and",
    "start": "587240",
    "end": "589040"
  },
  {
    "text": "trying to do a inference on top of that",
    "start": "589040",
    "end": "590959"
  },
  {
    "text": "and then I got uh an output out of it",
    "start": "590959",
    "end": "593440"
  },
  {
    "text": "but here essentially the data is moving",
    "start": "593440",
    "end": "595120"
  },
  {
    "text": "from like left to right but uh there are",
    "start": "595120",
    "end": "597480"
  },
  {
    "text": "other use cases that we have Lear",
    "start": "597480",
    "end": "599399"
  },
  {
    "text": "learned from our community uh and I'll",
    "start": "599399",
    "end": "601760"
  },
  {
    "text": "try to touch upon all the all of those",
    "start": "601760",
    "end": "603640"
  },
  {
    "text": "as well but let me show you like how",
    "start": "603640",
    "end": "605320"
  },
  {
    "text": "does it look like in order to build that",
    "start": "605320",
    "end": "607440"
  },
  {
    "text": "pipeline that I've shown you right uh",
    "start": "607440",
    "end": "609560"
  },
  {
    "text": "first uh a numa flow pipeline has two",
    "start": "609560",
    "end": "612360"
  },
  {
    "text": "important components like one is your uh",
    "start": "612360",
    "end": "614640"
  },
  {
    "text": "vertices and the second is your edges",
    "start": "614640",
    "end": "616800"
  },
  {
    "text": "essentially you are defining your dag",
    "start": "616800",
    "end": "618680"
  },
  {
    "text": "structure U saying that here are all of",
    "start": "618680",
    "end": "620839"
  },
  {
    "text": "my vertices right from your source to",
    "start": "620839",
    "end": "622800"
  },
  {
    "text": "inference and sync so these are all the",
    "start": "622800",
    "end": "624680"
  },
  {
    "text": "three that are the uh uh vertices right",
    "start": "624680",
    "end": "627279"
  },
  {
    "text": "here and then uh in the bottom you can",
    "start": "627279",
    "end": "628920"
  },
  {
    "text": "see that I'm just defining the edges",
    "start": "628920",
    "end": "630680"
  },
  {
    "text": "from like the source to the inference",
    "start": "630680",
    "end": "632399"
  },
  {
    "text": "and inference to the uh sync so it's as",
    "start": "632399",
    "end": "635040"
  },
  {
    "text": "simple as it it's very declarative where",
    "start": "635040",
    "end": "636480"
  },
  {
    "text": "you define your uh pipeline spec and uh",
    "start": "636480",
    "end": "640240"
  },
  {
    "text": "we have been saying that we want teams",
    "start": "640240",
    "end": "642440"
  },
  {
    "text": "just to focus on the piece of code and",
    "start": "642440",
    "end": "643959"
  },
  {
    "text": "not worry about uh any uh like any of",
    "start": "643959",
    "end": "647079"
  },
  {
    "text": "those connecting to Kafka or any of",
    "start": "647079",
    "end": "648480"
  },
  {
    "text": "those event sources so let's try uh take",
    "start": "648480",
    "end": "650760"
  },
  {
    "text": "a look at like how does that piece of",
    "start": "650760",
    "end": "652120"
  },
  {
    "text": "code look like so I I believe it's it's",
    "start": "652120",
    "end": "654839"
  },
  {
    "text": "going to be not legible to all of the",
    "start": "654839",
    "end": "656560"
  },
  {
    "text": "folks in the back so I just I've just",
    "start": "656560",
    "end": "658480"
  },
  {
    "text": "highlighted on the top",
    "start": "658480",
    "end": "660079"
  },
  {
    "text": "uh where it's a simple Handler function",
    "start": "660079",
    "end": "662200"
  },
  {
    "text": "uh where uh you're going to receive a",
    "start": "662200",
    "end": "664040"
  },
  {
    "text": "data object uh and then you can use the",
    "start": "664040",
    "end": "667440"
  },
  {
    "text": "data object and act upon the data object",
    "start": "667440",
    "end": "669480"
  },
  {
    "text": "in whatever the way you want to so here",
    "start": "669480",
    "end": "671279"
  },
  {
    "text": "I'm just like I've written the entire",
    "start": "671279",
    "end": "672680"
  },
  {
    "text": "piece of code AS processing and this",
    "start": "672680",
    "end": "674440"
  },
  {
    "text": "processing could be right from simple",
    "start": "674440",
    "end": "675959"
  },
  {
    "text": "data processing or even calling an llm",
    "start": "675959",
    "end": "678040"
  },
  {
    "text": "model and all of those pieces in the",
    "start": "678040",
    "end": "679680"
  },
  {
    "text": "example that I've shown you here uh we",
    "start": "679680",
    "end": "681360"
  },
  {
    "text": "are just calling an lava U model uh",
    "start": "681360",
    "end": "684639"
  },
  {
    "text": "served through like a VM right here so",
    "start": "684639",
    "end": "687760"
  },
  {
    "text": "in short like uh if you take a look at",
    "start": "687760",
    "end": "689839"
  },
  {
    "text": "it there is no mention of cfar any event",
    "start": "689839",
    "end": "691839"
  },
  {
    "text": "sources on the top it's just like you're",
    "start": "691839",
    "end": "693519"
  },
  {
    "text": "receiving an event act on the event and",
    "start": "693519",
    "end": "695200"
  },
  {
    "text": "then pass on the uh response to the",
    "start": "695200",
    "end": "697160"
  },
  {
    "text": "downstream so essentially in a",
    "start": "697160",
    "end": "699120"
  },
  {
    "text": "serverless fashion you can build all of",
    "start": "699120",
    "end": "701040"
  },
  {
    "text": "your uh stream processing pipelines or",
    "start": "701040",
    "end": "703160"
  },
  {
    "text": "even like doing streaming inference as",
    "start": "703160",
    "end": "704959"
  },
  {
    "text": "such so moving on to the next one uh",
    "start": "704959",
    "end": "708440"
  },
  {
    "text": "apart from the uh simple uh asynchronous",
    "start": "708440",
    "end": "712040"
  },
  {
    "text": "uh stream processing or doing inference",
    "start": "712040",
    "end": "713720"
  },
  {
    "text": "on streaming data like one of the most",
    "start": "713720",
    "end": "715519"
  },
  {
    "text": "common use cases that we see today is uh",
    "start": "715519",
    "end": "718839"
  },
  {
    "text": "we want to to have a rquest response",
    "start": "718839",
    "end": "720920"
  },
  {
    "text": "kind of a pattern uh for on of all of",
    "start": "720920",
    "end": "723240"
  },
  {
    "text": "your llm calls let's say for example",
    "start": "723240",
    "end": "725160"
  },
  {
    "text": "your llm calls might take a little bit",
    "start": "725160",
    "end": "726920"
  },
  {
    "text": "of time it could be sometimes a minute",
    "start": "726920",
    "end": "728600"
  },
  {
    "text": "or 2 minutes based upon the use case",
    "start": "728600",
    "end": "730079"
  },
  {
    "text": "that you're trying to cater to but uh",
    "start": "730079",
    "end": "732560"
  },
  {
    "text": "you want to receive a response for that",
    "start": "732560",
    "end": "734480"
  },
  {
    "text": "kind of uh",
    "start": "734480",
    "end": "736320"
  },
  {
    "text": "uh uh if you want to receive uh response",
    "start": "736320",
    "end": "739639"
  },
  {
    "text": "to such kind of uh use cases like where",
    "start": "739639",
    "end": "742240"
  },
  {
    "text": "you send an uh input through hdp and",
    "start": "742240",
    "end": "744000"
  },
  {
    "text": "then get a response after a few minutes",
    "start": "744000",
    "end": "745720"
  },
  {
    "text": "so that is where we are working on a",
    "start": "745720",
    "end": "747160"
  },
  {
    "text": "serving platform and we are going to do",
    "start": "747160",
    "end": "748920"
  },
  {
    "text": "a quick a demo of like the how the",
    "start": "748920",
    "end": "750320"
  },
  {
    "text": "serving platform also does look like",
    "start": "750320",
    "end": "752279"
  },
  {
    "text": "within Neo flow uh so as I said like you",
    "start": "752279",
    "end": "755480"
  },
  {
    "text": "have a serving end point uh you are",
    "start": "755480",
    "end": "757519"
  },
  {
    "text": "receiving a request through HTTP and",
    "start": "757519",
    "end": "759079"
  },
  {
    "text": "you're doing bit of like pre-processing",
    "start": "759079",
    "end": "760680"
  },
  {
    "text": "whatever you're interested in and doing",
    "start": "760680",
    "end": "762040"
  },
  {
    "text": "inference and after that in number of",
    "start": "762040",
    "end": "763959"
  },
  {
    "text": "steps that you want to add you can add",
    "start": "763959",
    "end": "765399"
  },
  {
    "text": "it to your pipeline and then finally uh",
    "start": "765399",
    "end": "767639"
  },
  {
    "text": "send it to your serving sync and then",
    "start": "767639",
    "end": "769399"
  },
  {
    "text": "finally this what the serving sync does",
    "start": "769399",
    "end": "771000"
  },
  {
    "text": "is that it gives you the response back",
    "start": "771000",
    "end": "772639"
  },
  {
    "text": "to the same endp point that where you",
    "start": "772639",
    "end": "774320"
  },
  {
    "text": "have sent the request to so that is",
    "start": "774320",
    "end": "776760"
  },
  {
    "text": "essentially it gives you that uh pure uh",
    "start": "776760",
    "end": "778760"
  },
  {
    "text": "request response pattern using the Ser",
    "start": "778760",
    "end": "780639"
  },
  {
    "text": "uh serving framework that we are trying",
    "start": "780639",
    "end": "781880"
  },
  {
    "text": "to develop it uh into it through the",
    "start": "781880",
    "end": "783440"
  },
  {
    "text": "nflow",
    "start": "783440",
    "end": "784680"
  },
  {
    "text": "platform uh I'll also try to show a",
    "start": "784680",
    "end": "786959"
  },
  {
    "text": "quick demo of this one uh primarily uh",
    "start": "786959",
    "end": "789680"
  },
  {
    "text": "using a different example so here what",
    "start": "789680",
    "end": "792760"
  },
  {
    "text": "I'm trying to do is like I have multiple",
    "start": "792760",
    "end": "795040"
  },
  {
    "text": "uh models running in the same pipeline",
    "start": "795040",
    "end": "797160"
  },
  {
    "text": "one is like the image recognition that",
    "start": "797160",
    "end": "799040"
  },
  {
    "text": "I've shown you earlier and the other one",
    "start": "799040",
    "end": "800399"
  },
  {
    "text": "is like uh uh image generation where I'm",
    "start": "800399",
    "end": "802959"
  },
  {
    "text": "just using a diffusion model uh where",
    "start": "802959",
    "end": "804839"
  },
  {
    "text": "I'm just going to send a text and then",
    "start": "804839",
    "end": "806320"
  },
  {
    "text": "say that hey generate an image for the",
    "start": "806320",
    "end": "808839"
  },
  {
    "text": "use cas Cas that I've shared with you U",
    "start": "808839",
    "end": "810920"
  },
  {
    "text": "so for this one I will quickly show you",
    "start": "810920",
    "end": "813199"
  },
  {
    "text": "on the uh UI",
    "start": "813199",
    "end": "817160"
  },
  {
    "text": "here so what I'm trying to do is I just",
    "start": "817320",
    "end": "819920"
  },
  {
    "text": "created a uh Json uh file where it has",
    "start": "819920",
    "end": "823839"
  },
  {
    "text": "uh something like a prompt saying that",
    "start": "823839",
    "end": "825800"
  },
  {
    "text": "hey create a picture for me that is",
    "start": "825800",
    "end": "828040"
  },
  {
    "text": "grapes so I'm just sending that uh",
    "start": "828040",
    "end": "830360"
  },
  {
    "text": "request to the uh Pipeline and then that",
    "start": "830360",
    "end": "833199"
  },
  {
    "text": "does give us a response in a bit so I'm",
    "start": "833199",
    "end": "835480"
  },
  {
    "text": "getting the response out as an output",
    "start": "835480",
    "end": "837199"
  },
  {
    "text": "image here so I'll just quickly show you",
    "start": "837199",
    "end": "839560"
  },
  {
    "text": "that how does that output image look",
    "start": "839560",
    "end": "842160"
  },
  {
    "text": "like so I've just asked for grapes and",
    "start": "842160",
    "end": "844720"
  },
  {
    "text": "then the model was able to give me that",
    "start": "844720",
    "end": "846320"
  },
  {
    "text": "output image uh this one and we can try",
    "start": "846320",
    "end": "849279"
  },
  {
    "text": "couple more times and then see how does",
    "start": "849279",
    "end": "850720"
  },
  {
    "text": "it look",
    "start": "850720",
    "end": "853120"
  },
  {
    "text": "like so it's a new one so in short I'm",
    "start": "856800",
    "end": "861079"
  },
  {
    "text": "sending out a request as a HTTP request",
    "start": "861079",
    "end": "863880"
  },
  {
    "text": "and then I'm able to receive the",
    "start": "863880",
    "end": "865040"
  },
  {
    "text": "response on the same uh uh the API that",
    "start": "865040",
    "end": "868480"
  },
  {
    "text": "I've made the call to so let's take a",
    "start": "868480",
    "end": "870399"
  },
  {
    "text": "look at the pipeline as well in the nlow",
    "start": "870399",
    "end": "872480"
  },
  {
    "text": "UI and how does it look",
    "start": "872480",
    "end": "874360"
  },
  {
    "text": "like so I have bunch of pipelines",
    "start": "874360",
    "end": "876839"
  },
  {
    "text": "running here and this is the pipeline",
    "start": "876839",
    "end": "878240"
  },
  {
    "text": "that we have created the one which I've",
    "start": "878240",
    "end": "879800"
  },
  {
    "text": "have demoed here where I'm just sending",
    "start": "879800",
    "end": "881759"
  },
  {
    "text": "the uh text as like create a uh image of",
    "start": "881759",
    "end": "885519"
  },
  {
    "text": "grapes and then there is a planner that",
    "start": "885519",
    "end": "887600"
  },
  {
    "text": "is deciding to which vertex I have to",
    "start": "887600",
    "end": "889959"
  },
  {
    "text": "send my uh events to is it like hey",
    "start": "889959",
    "end": "893240"
  },
  {
    "text": "should is it a text uh that I'm uh ask",
    "start": "893240",
    "end": "896680"
  },
  {
    "text": "sorry am I sending a text or am I",
    "start": "896680",
    "end": "898279"
  },
  {
    "text": "sending an image and accordingly uh",
    "start": "898279",
    "end": "900079"
  },
  {
    "text": "diverts the events to the respective",
    "start": "900079",
    "end": "902040"
  },
  {
    "text": "vertices here uh so here I was sending a",
    "start": "902040",
    "end": "904920"
  },
  {
    "text": "text so it was sending it to a diffusion",
    "start": "904920",
    "end": "906320"
  },
  {
    "text": "model and then finally it was sending",
    "start": "906320",
    "end": "907639"
  },
  {
    "text": "the results to Ser Ser sync and I was",
    "start": "907639",
    "end": "910040"
  },
  {
    "text": "able to see those uh images so in short",
    "start": "910040",
    "end": "913160"
  },
  {
    "text": "here uh you can see that uh it's not",
    "start": "913160",
    "end": "915759"
  },
  {
    "text": "just doing the uh asynchronous event",
    "start": "915759",
    "end": "918399"
  },
  {
    "text": "processing and then trying to do uh",
    "start": "918399",
    "end": "920440"
  },
  {
    "text": "inference on a streaming data from like",
    "start": "920440",
    "end": "922000"
  },
  {
    "text": "left to right uh if you want to have a",
    "start": "922000",
    "end": "924399"
  },
  {
    "text": "request response pattern and try to",
    "start": "924399",
    "end": "925800"
  },
  {
    "text": "serve your models you should be able to",
    "start": "925800",
    "end": "927240"
  },
  {
    "text": "do that uh using the uh the numl",
    "start": "927240",
    "end": "930480"
  },
  {
    "text": "platform and I'll also show you like one",
    "start": "930480",
    "end": "932399"
  },
  {
    "text": "of those uh uh pipelines that we are",
    "start": "932399",
    "end": "934920"
  },
  {
    "text": "running at production at scale like so",
    "start": "934920",
    "end": "936839"
  },
  {
    "text": "that you can understand uh the multi uh",
    "start": "936839",
    "end": "939959"
  },
  {
    "text": "the the polyot approach that you have",
    "start": "939959",
    "end": "941440"
  },
  {
    "text": "mentioned and the auto scaling that we",
    "start": "941440",
    "end": "942759"
  },
  {
    "text": "have mentioned earlier right like if you",
    "start": "942759",
    "end": "944079"
  },
  {
    "text": "think of it so we are receiving all of",
    "start": "944079",
    "end": "945880"
  },
  {
    "text": "these events from Kafka uh this is like",
    "start": "945880",
    "end": "947800"
  },
  {
    "text": "one of our source out of the box uh you",
    "start": "947800",
    "end": "949920"
  },
  {
    "text": "can on the top you can see the number of",
    "start": "949920",
    "end": "951279"
  },
  {
    "text": "PODS that for that particular source is",
    "start": "951279",
    "end": "953279"
  },
  {
    "text": "running and you can see that there is",
    "start": "953279",
    "end": "955480"
  },
  {
    "text": "like a pre-processing step uh",
    "start": "955480",
    "end": "960480"
  },
  {
    "text": "so there's a pre-processing step where",
    "start": "961880",
    "end": "963720"
  },
  {
    "text": "you can see like there are two pods and",
    "start": "963720",
    "end": "966000"
  },
  {
    "text": "then after that after doing the",
    "start": "966000",
    "end": "967199"
  },
  {
    "text": "pre-processing you are doing inference",
    "start": "967199",
    "end": "968959"
  },
  {
    "text": "upon the data that you're receiving here",
    "start": "968959",
    "end": "971040"
  },
  {
    "text": "and then we also if there is no model",
    "start": "971040",
    "end": "972720"
  },
  {
    "text": "available we actually send it out to uh",
    "start": "972720",
    "end": "975399"
  },
  {
    "text": "and like a training uh triggering a a",
    "start": "975399",
    "end": "978040"
  },
  {
    "text": "training workflow as well at the same",
    "start": "978040",
    "end": "979720"
  },
  {
    "text": "time so uh all of these are done in the",
    "start": "979720",
    "end": "982800"
  },
  {
    "text": "same platform where it is nflow as a",
    "start": "982800",
    "end": "985399"
  },
  {
    "text": "platform is enabling like multiple teams",
    "start": "985399",
    "end": "987279"
  },
  {
    "text": "to come together right for example",
    "start": "987279",
    "end": "988959"
  },
  {
    "text": "example uh the pre-processing step could",
    "start": "988959",
    "end": "991279"
  },
  {
    "text": "be in like Java and the inference step",
    "start": "991279",
    "end": "993360"
  },
  {
    "text": "could be in like Python and everything",
    "start": "993360",
    "end": "995160"
  },
  {
    "text": "so you can use any language for the step",
    "start": "995160",
    "end": "997399"
  },
  {
    "text": "that you are interested in and then try",
    "start": "997399",
    "end": "998720"
  },
  {
    "text": "to build out uh your pipelines in order",
    "start": "998720",
    "end": "1000880"
  },
  {
    "text": "to do the stream processing or even like",
    "start": "1000880",
    "end": "1002720"
  },
  {
    "text": "doing the inference on a streaming data",
    "start": "1002720",
    "end": "1005120"
  },
  {
    "text": "so uh and all of the uh debuggability",
    "start": "1005120",
    "end": "1007199"
  },
  {
    "text": "features that I've shown you earlier you",
    "start": "1007199",
    "end": "1008920"
  },
  {
    "text": "can also see all of them here let's say",
    "start": "1008920",
    "end": "1010600"
  },
  {
    "text": "for example you can go to your uh like",
    "start": "1010600",
    "end": "1013959"
  },
  {
    "text": "all the pods you can see the metrics",
    "start": "1013959",
    "end": "1015920"
  },
  {
    "text": "right here and if you are interested in",
    "start": "1015920",
    "end": "1017440"
  },
  {
    "text": "looking at your logs you should be able",
    "start": "1017440",
    "end": "1018759"
  },
  {
    "text": "to see them like quickly right here",
    "start": "1018759",
    "end": "1020079"
  },
  {
    "text": "itself so uh it's a fully uh featured",
    "start": "1020079",
    "end": "1023560"
  },
  {
    "text": "fully baked in platform that helps your",
    "start": "1023560",
    "end": "1025280"
  },
  {
    "text": "ml teams or data teams to do uh stream",
    "start": "1025280",
    "end": "1027400"
  },
  {
    "text": "processing or even doing uh inference on",
    "start": "1027400",
    "end": "1029079"
  },
  {
    "text": "streaming data so that's pretty much it",
    "start": "1029079",
    "end": "1031959"
  },
  {
    "text": "about for the demo uh and then I'll try",
    "start": "1031959",
    "end": "1034038"
  },
  {
    "text": "to summarize like what we have discussed",
    "start": "1034039",
    "end": "1035678"
  },
  {
    "text": "so far right uh Numa flow is as a as we",
    "start": "1035679",
    "end": "1038480"
  },
  {
    "text": "have been discussing like it's a",
    "start": "1038480",
    "end": "1039760"
  },
  {
    "text": "kubernetes native uh stream data",
    "start": "1039760",
    "end": "1041558"
  },
  {
    "text": "processing platform that is very",
    "start": "1041559",
    "end": "1042678"
  },
  {
    "text": "lightweight and we have uh lot of users",
    "start": "1042679",
    "end": "1045798"
  },
  {
    "text": "in the community where they have",
    "start": "1045799",
    "end": "1046880"
  },
  {
    "text": "deployed these nlow Pipelines on edge",
    "start": "1046880",
    "end": "1049480"
  },
  {
    "text": "devices uh doing like signal processing",
    "start": "1049480",
    "end": "1052080"
  },
  {
    "text": "radio signal processing like iot data",
    "start": "1052080",
    "end": "1054000"
  },
  {
    "text": "processing and things like that so it's",
    "start": "1054000",
    "end": "1055640"
  },
  {
    "text": "very lightweight easy to get started in",
    "start": "1055640",
    "end": "1057320"
  },
  {
    "text": "any language that you're interested in",
    "start": "1057320",
    "end": "1059000"
  },
  {
    "text": "and we have a lot of outof the Box",
    "start": "1059000",
    "end": "1060440"
  },
  {
    "text": "sources and syncs that uh that majority",
    "start": "1060440",
    "end": "1063280"
  },
  {
    "text": "of you might be interested in and then",
    "start": "1063280",
    "end": "1065640"
  },
  {
    "text": "lastly uh it's very scalable and cost",
    "start": "1065640",
    "end": "1067640"
  },
  {
    "text": "efficient uh we actually scale down to",
    "start": "1067640",
    "end": "1069640"
  },
  {
    "text": "zero let's say for example if you're not",
    "start": "1069640",
    "end": "1071200"
  },
  {
    "text": "receiving any events in your pipelines",
    "start": "1071200",
    "end": "1073200"
  },
  {
    "text": "the pipeline can absolutely scale down",
    "start": "1073200",
    "end": "1075000"
  },
  {
    "text": "to zero and then come back up whenever",
    "start": "1075000",
    "end": "1076840"
  },
  {
    "text": "you have uh events coming back into the",
    "start": "1076840",
    "end": "1078600"
  },
  {
    "text": "pipel line so in short we are like a",
    "start": "1078600",
    "end": "1081120"
  },
  {
    "text": "sess way of doing stream processing on",
    "start": "1081120",
    "end": "1083679"
  },
  {
    "text": "kubernetes in any language that you",
    "start": "1083679",
    "end": "1085840"
  },
  {
    "text": "might be interested",
    "start": "1085840",
    "end": "1087799"
  },
  {
    "text": "in and uh here are some of our users uh",
    "start": "1087799",
    "end": "1090840"
  },
  {
    "text": "in the community uh it's uh into it we",
    "start": "1090840",
    "end": "1093360"
  },
  {
    "text": "have been using it for a lot of like",
    "start": "1093360",
    "end": "1095120"
  },
  {
    "text": "anomaly detection and simple event",
    "start": "1095120",
    "end": "1096799"
  },
  {
    "text": "processing use cases but at the same",
    "start": "1096799",
    "end": "1098360"
  },
  {
    "text": "time you can see a lot of other use",
    "start": "1098360",
    "end": "1099720"
  },
  {
    "text": "cases from uh folks like uh loid Martin",
    "start": "1099720",
    "end": "1103240"
  },
  {
    "text": "and V chain where they have been using",
    "start": "1103240",
    "end": "1105280"
  },
  {
    "text": "it for iot data processing and fraud",
    "start": "1105280",
    "end": "1107799"
  },
  {
    "text": "detection on uh blockchain data and",
    "start": "1107799",
    "end": "1111480"
  },
  {
    "text": "doing signal processing on the edge",
    "start": "1111480",
    "end": "1113159"
  },
  {
    "text": "devices and things like so essentially",
    "start": "1113159",
    "end": "1114840"
  },
  {
    "text": "in short what I want to say here is that",
    "start": "1114840",
    "end": "1116960"
  },
  {
    "text": "it's a very uh uh platform uh the",
    "start": "1116960",
    "end": "1120480"
  },
  {
    "text": "platform is like very agnostic catering",
    "start": "1120480",
    "end": "1122240"
  },
  {
    "text": "to multiple use cases not just from like",
    "start": "1122240",
    "end": "1124679"
  },
  {
    "text": "uh doing inference on streaming data it",
    "start": "1124679",
    "end": "1126640"
  },
  {
    "text": "could be like simple event processing or",
    "start": "1126640",
    "end": "1128480"
  },
  {
    "text": "stream processing across the board so",
    "start": "1128480",
    "end": "1130400"
  },
  {
    "text": "onstop platform that can help your uh",
    "start": "1130400",
    "end": "1132520"
  },
  {
    "text": "data and ml uh needs as such so if you",
    "start": "1132520",
    "end": "1137440"
  },
  {
    "text": "are interested in looking at our project",
    "start": "1137440",
    "end": "1138760"
  },
  {
    "text": "Pro uh you can quickly scan the QR code",
    "start": "1138760",
    "end": "1141679"
  },
  {
    "text": "that would take you to the uh GitHub",
    "start": "1141679",
    "end": "1143799"
  },
  {
    "text": "repository uh and then if you're",
    "start": "1143799",
    "end": "1145960"
  },
  {
    "text": "interested in contributing uh joining",
    "start": "1145960",
    "end": "1147760"
  },
  {
    "text": "our community of course uh there's a",
    "start": "1147760",
    "end": "1149240"
  },
  {
    "text": "slack link that you can actually uh uh",
    "start": "1149240",
    "end": "1151880"
  },
  {
    "text": "click upon and then join our community",
    "start": "1151880",
    "end": "1153400"
  },
  {
    "text": "and if anyone is interested go try out",
    "start": "1153400",
    "end": "1155159"
  },
  {
    "text": "the project or talk to us I'd be more",
    "start": "1155159",
    "end": "1157000"
  },
  {
    "text": "than happy to uh share our experiences",
    "start": "1157000",
    "end": "1159480"
  },
  {
    "text": "and how we are using and how the",
    "start": "1159480",
    "end": "1160799"
  },
  {
    "text": "community is is using uh numl as a",
    "start": "1160799",
    "end": "1163320"
  },
  {
    "text": "project uh with that thank",
    "start": "1163320",
    "end": "1167399"
  },
  {
    "text": "you thank you thank",
    "start": "1167480",
    "end": "1169840"
  },
  {
    "text": "you thank you very much very nice talk",
    "start": "1169840",
    "end": "1173240"
  },
  {
    "text": "thanks and cool",
    "start": "1173240",
    "end": "1174799"
  },
  {
    "text": "demos um any question you can use any of",
    "start": "1174799",
    "end": "1178480"
  },
  {
    "text": "the two mics here or just wave and I'll",
    "start": "1178480",
    "end": "1181200"
  },
  {
    "text": "run to you yeah can you use that one",
    "start": "1181200",
    "end": "1186440"
  },
  {
    "text": "yeah um I'm mostly wor wondering about",
    "start": "1189240",
    "end": "1192919"
  },
  {
    "text": "Kafka uh as a source more how does um",
    "start": "1192919",
    "end": "1196880"
  },
  {
    "text": "aing messages",
    "start": "1196880",
    "end": "1198880"
  },
  {
    "text": "and um moving offsets work when it comes",
    "start": "1198880",
    "end": "1201640"
  },
  {
    "text": "to the pipelines you set up um I'm",
    "start": "1201640",
    "end": "1205120"
  },
  {
    "text": "wondering I guess after all the udfs is",
    "start": "1205120",
    "end": "1207679"
  },
  {
    "text": "that when you acknowledge and is that",
    "start": "1207679",
    "end": "1209120"
  },
  {
    "text": "handled for you so what happens is that",
    "start": "1209120",
    "end": "1211559"
  },
  {
    "text": "we have you see these dotted lines right",
    "start": "1211559",
    "end": "1213799"
  },
  {
    "text": "these are rough protocol buffers so the",
    "start": "1213799",
    "end": "1216440"
  },
  {
    "text": "moment we read from Kafka and we are",
    "start": "1216440",
    "end": "1218400"
  },
  {
    "text": "able to successfully write to this this",
    "start": "1218400",
    "end": "1220960"
  },
  {
    "text": "uh line out here right the buffer we",
    "start": "1220960",
    "end": "1223159"
  },
  {
    "text": "acknowledged back to Kafka that way we",
    "start": "1223159",
    "end": "1225720"
  },
  {
    "text": "can Auto scale so what happens the",
    "start": "1225720",
    "end": "1227440"
  },
  {
    "text": "reason why we do this is like if I were",
    "start": "1227440",
    "end": "1229080"
  },
  {
    "text": "to click on the planner and I look at",
    "start": "1229080",
    "end": "1230600"
  },
  {
    "text": "into the buffer it tells how much space",
    "start": "1230600",
    "end": "1232720"
  },
  {
    "text": "is being used and every message goes",
    "start": "1232720",
    "end": "1234400"
  },
  {
    "text": "through this ra protocol so we can Auto",
    "start": "1234400",
    "end": "1237120"
  },
  {
    "text": "Scale based on how many messages are",
    "start": "1237120",
    "end": "1239280"
  },
  {
    "text": "pending at each vertex you might be able",
    "start": "1239280",
    "end": "1241360"
  },
  {
    "text": "to read from Kafka a lot because reading",
    "start": "1241360",
    "end": "1243440"
  },
  {
    "text": "from Kafka is easy but let's assume if",
    "start": "1243440",
    "end": "1245640"
  },
  {
    "text": "you are in a inference vertex for",
    "start": "1245640",
    "end": "1247039"
  },
  {
    "text": "example in this vertex right there",
    "start": "1247039",
    "end": "1249120"
  },
  {
    "text": "should be more number of ports which you",
    "start": "1249120",
    "end": "1251600"
  },
  {
    "text": "had horizontally scale right so what we",
    "start": "1251600",
    "end": "1254760"
  },
  {
    "text": "do is we write use these buffers and the",
    "start": "1254760",
    "end": "1257880"
  },
  {
    "text": "buffer depth to understand how to auto",
    "start": "1257880",
    "end": "1260039"
  },
  {
    "text": "scale",
    "start": "1260039",
    "end": "1263039"
  },
  {
    "text": "uh I think there's another question",
    "start": "1263200",
    "end": "1265440"
  },
  {
    "text": "there and I'll I can pass you the mic",
    "start": "1265440",
    "end": "1267440"
  },
  {
    "text": "after yeah uh thanks for the talk uh it",
    "start": "1267440",
    "end": "1269960"
  },
  {
    "text": "was uh quite insightful uh I had a",
    "start": "1269960",
    "end": "1272520"
  },
  {
    "text": "question about um like you said you do",
    "start": "1272520",
    "end": "1274919"
  },
  {
    "text": "processing of data right um I mean on",
    "start": "1274919",
    "end": "1277640"
  },
  {
    "text": "the stream so uh if you had a model that",
    "start": "1277640",
    "end": "1280039"
  },
  {
    "text": "you're uh that that has been trained",
    "start": "1280039",
    "end": "1281600"
  },
  {
    "text": "with an offline Pipeline and online",
    "start": "1281600",
    "end": "1283480"
  },
  {
    "text": "you're doing some stream processing here",
    "start": "1283480",
    "end": "1285400"
  },
  {
    "text": "how do you manage online offline feature",
    "start": "1285400",
    "end": "1287679"
  },
  {
    "text": "discrepancies",
    "start": "1287679",
    "end": "1290519"
  },
  {
    "text": "so that's a tricky one um the first let",
    "start": "1290559",
    "end": "1293600"
  },
  {
    "text": "me talk about this particular pipeline",
    "start": "1293600",
    "end": "1295480"
  },
  {
    "text": "which you talked about training right so",
    "start": "1295480",
    "end": "1296919"
  },
  {
    "text": "what happens in here is that the trainer",
    "start": "1296919",
    "end": "1300000"
  },
  {
    "text": "here which is doing around 29 um",
    "start": "1300000",
    "end": "1302200"
  },
  {
    "text": "training per second what this happens is",
    "start": "1302200",
    "end": "1304440"
  },
  {
    "text": "that this feat so this trainer actually",
    "start": "1304440",
    "end": "1307559"
  },
  {
    "text": "pulls in the data and it does train",
    "start": "1307559",
    "end": "1309320"
  },
  {
    "text": "locally so if you look in the processing",
    "start": "1309320",
    "end": "1311600"
  },
  {
    "text": "grade you will see that this is this is",
    "start": "1311600",
    "end": "1314559"
  },
  {
    "text": "training so you'll see a PO running",
    "start": "1314559",
    "end": "1316400"
  },
  {
    "text": "right this means that we are training in",
    "start": "1316400",
    "end": "1318600"
  },
  {
    "text": "locally and it calls out the feature",
    "start": "1318600",
    "end": "1320840"
  },
  {
    "text": "store to get the data and does",
    "start": "1320840",
    "end": "1322400"
  },
  {
    "text": "processing the reason why it does sit",
    "start": "1322400",
    "end": "1323919"
  },
  {
    "text": "here on this particular node on on this",
    "start": "1323919",
    "end": "1326039"
  },
  {
    "text": "vertex is because it can Auto scale Fair",
    "start": "1326039",
    "end": "1329360"
  },
  {
    "text": "uh the on the training so this is the",
    "start": "1329360",
    "end": "1330880"
  },
  {
    "text": "trainer and then if you look at the",
    "start": "1330880",
    "end": "1332360"
  },
  {
    "text": "inference side of system uh which is",
    "start": "1332360",
    "end": "1335640"
  },
  {
    "text": "here right inference so here it does",
    "start": "1335640",
    "end": "1338720"
  },
  {
    "text": "much more uh throughput because",
    "start": "1338720",
    "end": "1341360"
  },
  {
    "text": "sometimes mostly models are present only",
    "start": "1341360",
    "end": "1343320"
  },
  {
    "text": "during stale models we do training it",
    "start": "1343320",
    "end": "1346120"
  },
  {
    "text": "shares the uh feature store so that that",
    "start": "1346120",
    "end": "1348880"
  },
  {
    "text": "these though uh since it actually we use",
    "start": "1348880",
    "end": "1351480"
  },
  {
    "text": "ml flow inside for uh sharing and pumo",
    "start": "1351480",
    "end": "1354200"
  },
  {
    "text": "flow is agnostic to what you use right",
    "start": "1354200",
    "end": "1356600"
  },
  {
    "text": "so the both the features are stored",
    "start": "1356600",
    "end": "1358799"
  },
  {
    "text": "together and they know the ml FL gets",
    "start": "1358799",
    "end": "1361799"
  },
  {
    "text": "updated with the new model with the",
    "start": "1361799",
    "end": "1363200"
  },
  {
    "text": "trainer rights to is being pulled at the",
    "start": "1363200",
    "end": "1365320"
  },
  {
    "text": "inference level sorry the inference",
    "start": "1365320",
    "end": "1369320"
  },
  {
    "text": "here got so at both training and",
    "start": "1369760",
    "end": "1372600"
  },
  {
    "text": "inferencing you interface with your",
    "start": "1372600",
    "end": "1374279"
  },
  {
    "text": "featur store to uh basically rri the",
    "start": "1374279",
    "end": "1376520"
  },
  {
    "text": "features so that's correct that's",
    "start": "1376520",
    "end": "1377840"
  },
  {
    "text": "correct",
    "start": "1377840",
    "end": "1380440"
  },
  {
    "text": "all nice talk yeah you from Nvidia so my",
    "start": "1381279",
    "end": "1384679"
  },
  {
    "text": "question is yeah you already sh here but",
    "start": "1384679",
    "end": "1386840"
  },
  {
    "text": "like real pipe line right the LM course",
    "start": "1386840",
    "end": "1391480"
  },
  {
    "text": "may have multiple iterations right not",
    "start": "1391480",
    "end": "1394279"
  },
  {
    "text": "simple just to make one call either",
    "start": "1394279",
    "end": "1396360"
  },
  {
    "text": "refinement or The Limited of the context",
    "start": "1396360",
    "end": "1399240"
  },
  {
    "text": "N I have to make multiple Calles I",
    "start": "1399240",
    "end": "1401480"
  },
  {
    "text": "wondering does your system can support",
    "start": "1401480",
    "end": "1404679"
  },
  {
    "text": "and the more complex pipeline right",
    "start": "1404679",
    "end": "1407440"
  },
  {
    "text": "absolutely yes yes so if you were to do",
    "start": "1407440",
    "end": "1410480"
  },
  {
    "text": "Numa flow and just do we do support",
    "start": "1410480",
    "end": "1412760"
  },
  {
    "text": "cycle for iterative learning so if I",
    "start": "1412760",
    "end": "1414760"
  },
  {
    "text": "were to see so the Talk itself is very",
    "start": "1414760",
    "end": "1417880"
  },
  {
    "text": "small it doesn't show talk about all the",
    "start": "1417880",
    "end": "1419520"
  },
  {
    "text": "features but if you see here right we do",
    "start": "1419520",
    "end": "1421559"
  },
  {
    "text": "support Cycles right and you can do it",
    "start": "1421559",
    "end": "1424000"
  },
  {
    "text": "so we are not not a dag rather we are a",
    "start": "1424000",
    "end": "1426000"
  },
  {
    "text": "compute graph where you can do itative",
    "start": "1426000",
    "end": "1430120"
  },
  {
    "text": "processing all right thank you very much",
    "start": "1430279",
    "end": "1432799"
  },
  {
    "text": "again thanks thank again our speakers",
    "start": "1432799",
    "end": "1436080"
  },
  {
    "text": "thank you",
    "start": "1436080",
    "end": "1439240"
  }
]