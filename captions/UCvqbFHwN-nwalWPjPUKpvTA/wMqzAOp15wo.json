[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "[Applause] thank you Chris that is a huge room I need to make sure I take pictures so I",
    "start": "0",
    "end": "5940"
  },
  {
    "text": "can tell my wife I'm actually doing some work here I'm not drinking beers in San Diego so my name is Sebastien I'm from",
    "start": "5940",
    "end": "13170"
  },
  {
    "text": "France I hope you don't mind the French accent today I'm going to talk about kubernetes operators and what I",
    "start": "13170",
    "end": "21570"
  },
  {
    "text": "personally consider the hard parts of writing one so the past year I've been",
    "start": "21570",
    "end": "27539"
  },
  {
    "text": "working on this project we call it e ck elastic cloud on kubernetes it's",
    "start": "27539",
    "end": "33600"
  },
  {
    "text": "basically a kubernetes operator to deploy the elastic stack products manage",
    "start": "33600",
    "end": "39360"
  },
  {
    "text": "your elastic search cluster like smooth migrations rolling up great and so on",
    "start": "39360",
    "end": "44579"
  },
  {
    "text": "cabana and APM server you can check it out it's on github the code is open you",
    "start": "44579",
    "end": "49590"
  },
  {
    "text": "can try it for free but I'm not going to talk about egk today I'm just going to focus on",
    "start": "49590",
    "end": "55920"
  },
  {
    "text": "building operators in general so I will",
    "start": "55920",
    "end": "62340"
  },
  {
    "start": "60000",
    "end": "106000"
  },
  {
    "text": "assume most of you already know what's an operator but if you don't I have a",
    "start": "62340",
    "end": "68189"
  },
  {
    "text": "couple slides to remind you of what it is so usually an operator comes with its",
    "start": "68189",
    "end": "74520"
  },
  {
    "text": "own ceoddi some resource definition that's a way of extending the kubernetes",
    "start": "74520",
    "end": "79530"
  },
  {
    "text": "api with your own set of api's so here an example with elasticsearch the",
    "start": "79530",
    "end": "87479"
  },
  {
    "text": "operator will come with its own schema that would allow you to define an elastic search cluster so here we want",
    "start": "87479",
    "end": "93090"
  },
  {
    "text": "to deploy elastic search in version 7 which we master nodes and two data nodes",
    "start": "93090",
    "end": "98869"
  },
  {
    "text": "users would just apply this channel file to the API server and that's the way we extend the kubernetes api then the",
    "start": "98869",
    "end": "108930"
  },
  {
    "start": "106000",
    "end": "221000"
  },
  {
    "text": "operator itself is basically just a pod running in the communities cluster and",
    "start": "108930",
    "end": "114470"
  },
  {
    "text": "what it does is watching resources in the API server so here it would most",
    "start": "114470",
    "end": "120689"
  },
  {
    "text": "probably watch all resources of kind elastic search where it would also",
    "start": "120689",
    "end": "125969"
  },
  {
    "text": "probably be interested into watching pod services secrets and everything we need to deploy in the last thing",
    "start": "125969",
    "end": "132150"
  },
  {
    "text": "combinators the operator has this",
    "start": "132150",
    "end": "137220"
  },
  {
    "text": "concept of a reconciliation loop so whenever something happens in the API server like resource we're watching gets",
    "start": "137220",
    "end": "143940"
  },
  {
    "text": "created updated deleted it's going to trigger a reconciliation internally and",
    "start": "143940",
    "end": "149489"
  },
  {
    "text": "the reconciliation is just basically a securing a flow of sequential steps doesn't have to be sequential but most",
    "start": "149489",
    "end": "156030"
  },
  {
    "text": "of the time it is so that we eventually",
    "start": "156030",
    "end": "161720"
  },
  {
    "text": "reconcile the expected resources we should have based on CLE specification",
    "start": "161720",
    "end": "167480"
  },
  {
    "text": "so that we have all these resources running on kubernetes so this",
    "start": "167480",
    "end": "173430"
  },
  {
    "text": "reconciliation loop usually can really return early like say for example is",
    "start": "173430",
    "end": "178769"
  },
  {
    "text": "creating a set of pods probably will have to wait for those parts to be created before we can request the",
    "start": "178769",
    "end": "184829"
  },
  {
    "text": "service that is in front of those pods so instead of just waiting forever in a synchronous fashion we'll just return",
    "start": "184829",
    "end": "191849"
  },
  {
    "text": "earlier from the reconciliation and whenever we get a new watch event we would continue and start again the",
    "start": "191849",
    "end": "197760"
  },
  {
    "text": "reconciliation but probably will make some progress so that eventually we",
    "start": "197760",
    "end": "202859"
  },
  {
    "text": "managed to reconcile everything we want and this reconciliation runs over and",
    "start": "202859",
    "end": "208290"
  },
  {
    "text": "over and over again and most of the times it just does nothing because everything is already there and",
    "start": "208290",
    "end": "213989"
  },
  {
    "text": "everything is fine but if something is not it's going to reconcile things",
    "start": "213989",
    "end": "219239"
  },
  {
    "text": "eventually to help us writing operators",
    "start": "219239",
    "end": "225090"
  },
  {
    "start": "221000",
    "end": "280000"
  },
  {
    "text": "there's a bunch of tools out there in the community sector system the one I",
    "start": "225090",
    "end": "230400"
  },
  {
    "text": "use that I find really good is crew builder maintained by the API imaginary",
    "start": "230400",
    "end": "235530"
  },
  {
    "text": "special interest group who builder basically helps you scaffold a go line",
    "start": "235530",
    "end": "241169"
  },
  {
    "text": "project to build your own operator it helps you automatically convert your",
    "start": "241169",
    "end": "247680"
  },
  {
    "text": "internal go model to the ceoddi channel representation and bootstrap everything",
    "start": "247680",
    "end": "254099"
  },
  {
    "text": "related to the reconciliation we talked about behind the scenes crew builder is using the controller runtime library",
    "start": "254099",
    "end": "260690"
  },
  {
    "text": "which sets up everything related to watches has this internal work queue concept so",
    "start": "260690",
    "end": "267200"
  },
  {
    "text": "that any words event triggers a reconciliation I don't know if we have some maintainer of those projects in the",
    "start": "267200",
    "end": "274220"
  },
  {
    "text": "room but if we do thank you folks because that's really helpful okay so",
    "start": "274220",
    "end": "282350"
  },
  {
    "start": "280000",
    "end": "409000"
  },
  {
    "text": "that's it for the introduction to operators let's focus now on wearing the team we consider the hard parts there's",
    "start": "282350",
    "end": "292010"
  },
  {
    "text": "really one thing I want to focus here and that's this concept of the operator",
    "start": "292010",
    "end": "297440"
  },
  {
    "text": "basically living in the past so of course the operator is kind of working",
    "start": "297440",
    "end": "304790"
  },
  {
    "text": "in real time the sense that whatever happens in a cluster it's going to reconcile things over and over again but",
    "start": "304790",
    "end": "312140"
  },
  {
    "text": "you have to assume that every time that code gets executed you kind of one step behind the reality of things in the",
    "start": "312140",
    "end": "318080"
  },
  {
    "text": "state of the world let's take an example here say in the middle of my reconciliation at least I need to list",
    "start": "318080",
    "end": "324800"
  },
  {
    "text": "all the pods I have in a cluster related to my particular application so I would list a pulse and I would see maybe I",
    "start": "324800",
    "end": "330530"
  },
  {
    "text": "have two pods but a and part b and then compared what i expect which would be",
    "start": "330530",
    "end": "337220"
  },
  {
    "text": "having three pods i realize one is missing so i need to create it so i would create Part C here",
    "start": "337220",
    "end": "343910"
  },
  {
    "text": "and then if later on in that same reconciliation or maybe the next reconciliation this part again",
    "start": "343910",
    "end": "350480"
  },
  {
    "text": "I would probably expect to have three pods now but I what I will most probably",
    "start": "350480",
    "end": "356210"
  },
  {
    "text": "see here is that I have only two parts because pod C is not there yet he was still created otherwise I would",
    "start": "356210",
    "end": "362960"
  },
  {
    "text": "have got an error but I don't see it I don't see party here and that's not a",
    "start": "362960",
    "end": "369650"
  },
  {
    "text": "bug that's a feature that's by design the reason is the client we use to reach",
    "start": "369650",
    "end": "377570"
  },
  {
    "text": "the API server uses a cache reader which means we are not sending a request to",
    "start": "377570",
    "end": "383360"
  },
  {
    "text": "the API server every time we want to list things or get a particular resource because that would be pretty expensive",
    "start": "383360",
    "end": "390050"
  },
  {
    "text": "there would be a big performance hit so what we do instead is that we have this local cache to the",
    "start": "390050",
    "end": "395750"
  },
  {
    "text": "shoulder and the cash gets populated from Roach events so whenever we need to",
    "start": "395750",
    "end": "401930"
  },
  {
    "text": "list things we just look at what we have in the cache this can lead to some funny",
    "start": "401930",
    "end": "413630"
  },
  {
    "start": "409000",
    "end": "474000"
  },
  {
    "text": "situation in the history of our project we had several backs that are related",
    "start": "413630",
    "end": "418940"
  },
  {
    "text": "directly to this usage of the cache things like creating too many poles like",
    "start": "418940",
    "end": "424580"
  },
  {
    "text": "you you expect rivers to exist but because every single reconciliation creates one based on an out-of-date",
    "start": "424580",
    "end": "430490"
  },
  {
    "text": "cache you actually end up with 25 poster three seconds or if you need to manage",
    "start": "430490",
    "end": "437900"
  },
  {
    "text": "stateful workloads like elastic search or any complex distributed system pretty often you have this notion of Chrome",
    "start": "437900",
    "end": "444200"
  },
  {
    "text": "where you have a set of map of master nodes and you need to make sure somehow that you maintain some kind of",
    "start": "444200",
    "end": "449720"
  },
  {
    "text": "consistency here for example in the case of elastic search if it's running five master nodes you make you need to make",
    "start": "449720",
    "end": "456020"
  },
  {
    "text": "sure that three master nodes from the crew of the Quran so if your assumption",
    "start": "456020",
    "end": "462979"
  },
  {
    "text": "on the number of nodes relies on the number of pods you're having a cache you may have a problem here because your",
    "start": "462979",
    "end": "469669"
  },
  {
    "text": "cache does not really represent reality so what can we do about it well we're",
    "start": "469669",
    "end": "477229"
  },
  {
    "start": "474000",
    "end": "585000"
  },
  {
    "text": "lucky because in most situations it's just fine we don't have to worry about it the reason is kubernetes has this",
    "start": "477229",
    "end": "483979"
  },
  {
    "text": "concept of optimistic concurrency implemented there by default when you do",
    "start": "483979",
    "end": "490729"
  },
  {
    "text": "a creation the resource you create has a name and probably in the in space say",
    "start": "490729",
    "end": "498020"
  },
  {
    "text": "I'm creating one part at one conciliation and at the next reconciliation I'm crane trying to create the exact same pod because I",
    "start": "498020",
    "end": "504800"
  },
  {
    "text": "don't see it in the cache the API server is probably gonna is well truly actually",
    "start": "504800",
    "end": "510530"
  },
  {
    "text": "going to reject that creation because another resource with the same name already exists so in the reconciliation",
    "start": "510530",
    "end": "517610"
  },
  {
    "text": "I would get a conflict error and that's just fine if I get that I just riku like",
    "start": "517610",
    "end": "524720"
  },
  {
    "text": "returned early from the deviation and another reconciliation is going to be cute again whenever there's",
    "start": "524720",
    "end": "531440"
  },
  {
    "text": "a chance a change in the resources I'd wash so whenever that you thought I created appears in the cash the new",
    "start": "531440",
    "end": "537589"
  },
  {
    "text": "reconciliation is going to be triggered and this time it will probably not fail it conflicts because I'll notice the",
    "start": "537589",
    "end": "544220"
  },
  {
    "text": "body stares but not try to create it again we had the same kind of stuff with",
    "start": "544220",
    "end": "550550"
  },
  {
    "text": "updates every resource has a resource version so whenever you do an update of",
    "start": "550550",
    "end": "557180"
  },
  {
    "text": "a resource what you can do is with your update call you ship this resource",
    "start": "557180",
    "end": "562820"
  },
  {
    "text": "version that you have from the cache and if the resource in the API server does",
    "start": "562820",
    "end": "568490"
  },
  {
    "text": "not match that version so probably it has been updated country on V with what",
    "start": "568490",
    "end": "573560"
  },
  {
    "text": "you're trying to do then you update operation is going to be rejected which is fine because I probably don't want to",
    "start": "573560",
    "end": "579620"
  },
  {
    "text": "update a resource based on incorrect assumptions on what I observe deletions",
    "start": "579620",
    "end": "587480"
  },
  {
    "start": "585000",
    "end": "652000"
  },
  {
    "text": "basically work the same way so here I'm using the controller runtime client to",
    "start": "587480",
    "end": "593990"
  },
  {
    "text": "talk to the API server and whenever I do a deletion I can just pass me in the UID",
    "start": "593990",
    "end": "599839"
  },
  {
    "text": "of the resource I want to delete also the version of that resource so I don't for example by accidentally the resource",
    "start": "599839",
    "end": "607760"
  },
  {
    "text": "I would probably not have deleted if I had a proper version of that resource this is also kind of useful if you",
    "start": "607760",
    "end": "615140"
  },
  {
    "text": "dealing with stateful sets especially if you're using the Anjali strategy we'll talk about that later what could happen",
    "start": "615140",
    "end": "621980"
  },
  {
    "text": "is that you delete a part you expect the staples head controller to recreate that part but at the next reconciliation",
    "start": "621980",
    "end": "630920"
  },
  {
    "text": "maybe you would do that deletion again but if the stateful said controller already like recreated the part it gets",
    "start": "630920",
    "end": "639170"
  },
  {
    "text": "a newer ID so you actually deleting again the but that was just restarted",
    "start": "639170",
    "end": "645079"
  },
  {
    "text": "which is not what you want so shipping the UID here in this call helps with that",
    "start": "645079",
    "end": "652089"
  },
  {
    "start": "652000",
    "end": "688000"
  },
  {
    "text": "so this is all fine but in some situation that's not enough I said",
    "start": "653160",
    "end": "658590"
  },
  {
    "text": "earlier that whenever you manage stateful workloads you have this additional complexity of maintaining",
    "start": "658590",
    "end": "664050"
  },
  {
    "text": "some kind of consistency between two systems on one hand we have the community's resources on the other hand",
    "start": "664050",
    "end": "670500"
  },
  {
    "text": "we have this external maybe distributed complex system to operate and we need to",
    "start": "670500",
    "end": "676590"
  },
  {
    "text": "make sure that there's some kind of consistency here the API server",
    "start": "676590",
    "end": "683120"
  },
  {
    "text": "optimistic looking does not always help with that so then if you reach that",
    "start": "683120",
    "end": "691080"
  },
  {
    "start": "688000",
    "end": "803000"
  },
  {
    "text": "situation like you write some code you know what you want to do but you realize even though mistake concurrency helps",
    "start": "691080",
    "end": "697950"
  },
  {
    "text": "that's not enough there's a risk you can do something wrong here then you can use something else and we",
    "start": "697950",
    "end": "706560"
  },
  {
    "text": "actually found that in the deployment controller and more specifically in the",
    "start": "706560",
    "end": "712050"
  },
  {
    "text": "replicas head controller because when you create a deployment could be Nerys basically there's the same problem when",
    "start": "712050",
    "end": "717720"
  },
  {
    "text": "the deployment decides it needs to scale and create new part you don't want it to",
    "start": "717720",
    "end": "724140"
  },
  {
    "text": "suddenly create 15 parts instead of one because the replicas head controller was",
    "start": "724140",
    "end": "730910"
  },
  {
    "text": "making decisions based on an out-of-date cache so the way it works the",
    "start": "730910",
    "end": "736320"
  },
  {
    "text": "expectations thing is that whenever we do a creation like here whenever we create a pod at the same time we",
    "start": "736320",
    "end": "743840"
  },
  {
    "text": "register in memory the fact that we did this creation so we expect that creation",
    "start": "743840",
    "end": "749790"
  },
  {
    "text": "to happen and we expect that at some point this pod will appear in our cache then at the next reconciliation when",
    "start": "749790",
    "end": "758400"
  },
  {
    "text": "this gets executed again before we do any other creation we look at this expectation we registered is that part",
    "start": "758400",
    "end": "766800"
  },
  {
    "text": "we created before actually in the cache yet or not if it is that's fine we can",
    "start": "766800",
    "end": "772020"
  },
  {
    "text": "move on but if it's not we should just return early because will retry later eventually so in the replication",
    "start": "772020",
    "end": "780210"
  },
  {
    "text": "controller this is all bound to watch an creation on deletion in this EK we try to make make it to be",
    "start": "780210",
    "end": "786669"
  },
  {
    "text": "more generally consumed her in the sense every time we need to check patient's we",
    "start": "786669",
    "end": "792100"
  },
  {
    "text": "just look at what we have in the cache but that's a pretty useful concept if",
    "start": "792100",
    "end": "798669"
  },
  {
    "text": "you need to use it if you don't not use it I have a few set of best practices so",
    "start": "798669",
    "end": "807639"
  },
  {
    "start": "803000",
    "end": "914000"
  },
  {
    "text": "that whatever you write in the reconciliation actually helps you like",
    "start": "807639",
    "end": "812769"
  },
  {
    "text": "saves you from doing anything wrong and the first thing is to use deterministic naming we saw earlier this optimistic",
    "start": "812769",
    "end": "819669"
  },
  {
    "text": "concurrency on creation which works fine as long as the resource you create has",
    "start": "819669",
    "end": "825279"
  },
  {
    "text": "this table name so you created the first time you show you're not going to create it a second time because the second",
    "start": "825279",
    "end": "830499"
  },
  {
    "text": "creation will be rejected but if the resources you create have a random name",
    "start": "830499",
    "end": "836739"
  },
  {
    "text": "then this doesn't work right so you need some kind of stable deterministic naming",
    "start": "836739",
    "end": "842459"
  },
  {
    "text": "for example if you stateful sets you'll notice that every bud and the line pod",
    "start": "842459",
    "end": "847779"
  },
  {
    "text": "that gets created has the name of the stateful set dress and all in all so that's very deterministic stateful set",
    "start": "847779",
    "end": "852970"
  },
  {
    "text": "controller knows exactly what's the name of the body is going to create then at",
    "start": "852970",
    "end": "858549"
  },
  {
    "text": "any point of the reconciliation you have to assume that probably your cache is",
    "start": "858549",
    "end": "863679"
  },
  {
    "text": "scale you have to think about it like whenever you do something try to think what if the resources is in the cache",
    "start": "863679",
    "end": "871869"
  },
  {
    "text": "are not up-to-date is everything going to be okay most probably yes sometimes it isn't and",
    "start": "871869",
    "end": "880769"
  },
  {
    "text": "finally the entire reconciliation should be idempotent which means it's going to",
    "start": "880769",
    "end": "887470"
  },
  {
    "text": "run over and over and over again you want the output of this reconciliation",
    "start": "887470",
    "end": "893079"
  },
  {
    "text": "to be the exact same state of resources in kubernetes it's really fine we",
    "start": "893079",
    "end": "898329"
  },
  {
    "text": "trigger the same reconciliation and at the same time it's if anything goes wrong like if you bought this repair",
    "start": "898329",
    "end": "903999"
  },
  {
    "text": "service disappears if secret disappears you're pretty sure that the next reconciliation is going to fix things up",
    "start": "903999",
    "end": "911069"
  },
  {
    "text": "eventually an example about that say I want to",
    "start": "911069",
    "end": "919040"
  },
  {
    "start": "914000",
    "end": "975000"
  },
  {
    "text": "create a stateful set so like a simple",
    "start": "919040",
    "end": "924440"
  },
  {
    "text": "version of how I would do that is first check does that stateful set exist already and if it does not then I just created but",
    "start": "924440",
    "end": "933200"
  },
  {
    "text": "then as you probably know when we create stable set we most of the times also create ahead of service for that",
    "start": "933200",
    "end": "938930"
  },
  {
    "text": "stateful set but here we have a problem in this piece of code what if the",
    "start": "938930",
    "end": "946580"
  },
  {
    "text": "operator crashes in between those two creation we create the stateful set there's a crash the operator gets",
    "start": "946580",
    "end": "954710"
  },
  {
    "text": "restarted and we don't go through that code path again so we're actually never",
    "start": "954710",
    "end": "959930"
  },
  {
    "text": "going to create the Headless service or maybe the code will be held at service creation returns an error and whenever",
    "start": "959930",
    "end": "966800"
  },
  {
    "text": "we're going to retry that we're actually not going to retry that because the stateful set is already there we created",
    "start": "966800",
    "end": "973010"
  },
  {
    "text": "it so we don't enter the live condition again so one way to fix that of course is to reverse the ordering since our",
    "start": "973010",
    "end": "981400"
  },
  {
    "start": "975000",
    "end": "998000"
  },
  {
    "text": "condition is based on the presence of the stateful set if we create the stateful set last then it's okay because",
    "start": "981400",
    "end": "987350"
  },
  {
    "text": "say we create the Headless service then the operator crashes then when it's",
    "start": "987350",
    "end": "992900"
  },
  {
    "text": "gonna start again State one cell is still not there so we'll do that again",
    "start": "992900",
    "end": "998110"
  },
  {
    "start": "998000",
    "end": "1055000"
  },
  {
    "text": "but still probably this is how you",
    "start": "998110",
    "end": "1003640"
  },
  {
    "text": "really want to do it you want to decouple the creation of every single resource because the problem with that",
    "start": "1003640",
    "end": "1009340"
  },
  {
    "text": "second version is that if someone accidentally deletes the headless",
    "start": "1009340",
    "end": "1014980"
  },
  {
    "text": "service it's never going to be created again because will never enter that code path and here if we try to keep things",
    "start": "1014980",
    "end": "1023110"
  },
  {
    "text": "flat then we're pretty sure whatever happens if the statement said is removed the health service is removed then the",
    "start": "1023110",
    "end": "1030819"
  },
  {
    "text": "next reconciliation is going to fix things again so usually it's very",
    "start": "1030820",
    "end": "1036459"
  },
  {
    "text": "interesting to keep things flat and sequential even though it sounds a bit stupid which doesn't mean by the way",
    "start": "1036460",
    "end": "1042579"
  },
  {
    "text": "that their entire reconciliations to be a single big function of two thousand",
    "start": "1042580",
    "end": "1047709"
  },
  {
    "text": "lines of code right it's just the way things are executed that matters so I",
    "start": "1047709",
    "end": "1056350"
  },
  {
    "text": "talked about reconciling resources handling creations and deletions",
    "start": "1056350",
    "end": "1062950"
  },
  {
    "text": "actually fairly easy but handling updates sometimes very hard say whatever",
    "start": "1062950",
    "end": "1069190"
  },
  {
    "text": "you have in the new CEO of the internally maps to the creation of a stateful set maybe that's yadi was resource but it",
    "start": "1069190",
    "end": "1076630"
  },
  {
    "text": "dated so that now you know you should update the underlying stateful set from three replicas to five replicas there's",
    "start": "1076630",
    "end": "1083529"
  },
  {
    "text": "an update operation here you need to do and usually what happens is that internally in the code you build this",
    "start": "1083529",
    "end": "1091330"
  },
  {
    "start": "1085000",
    "end": "1126000"
  },
  {
    "text": "expected resource that you'd like to have in urban areas and you compare that",
    "start": "1091330",
    "end": "1096580"
  },
  {
    "text": "with the actual resource you have that you fetch from the cache and then based",
    "start": "1096580",
    "end": "1103029"
  },
  {
    "text": "on this comparison you decide whether you need to update the resource or not so how do you do that comparison well a",
    "start": "1103029",
    "end": "1110110"
  },
  {
    "text": "knife way to do that we'll just need to use reflect that the big hole so if the",
    "start": "1110110",
    "end": "1115390"
  },
  {
    "text": "two resources match exactly that's fine you don't need to update otherwise you need to update but that doesn't work",
    "start": "1115390",
    "end": "1123340"
  },
  {
    "text": "quite well because",
    "start": "1123340",
    "end": "1126148"
  },
  {
    "start": "1126000",
    "end": "1176000"
  },
  {
    "text": "say on the Left I'm creating a pot it's like minimalistic but I created this way then if I retrieve it that's that's what",
    "start": "1128520",
    "end": "1135780"
  },
  {
    "text": "I get back that's not exactly the product created right there's a whole bunch of metadata I was I did by kubernetes also that pod",
    "start": "1135780",
    "end": "1145320"
  },
  {
    "text": "creation went through a whole lot of mutating web hooks and for example here",
    "start": "1145320",
    "end": "1151670"
  },
  {
    "text": "things like image pool policy was added maybe I have a limit range in the",
    "start": "1151670",
    "end": "1156990"
  },
  {
    "text": "cluster so now suddenly I have resources requests in the pods back and also maybe",
    "start": "1156990",
    "end": "1162059"
  },
  {
    "text": "I have a bunch of environment variables that we are in here that's what happens for example if you if you do that on a",
    "start": "1162059",
    "end": "1167820"
  },
  {
    "text": "sure kubernetes so obviously you understand we cannot just like reflect",
    "start": "1167820",
    "end": "1173820"
  },
  {
    "text": "that DP call these two resources it doesn't work what can you do then then you can try to go one step further like",
    "start": "1173820",
    "end": "1180090"
  },
  {
    "start": "1176000",
    "end": "1211000"
  },
  {
    "text": "you would build like custom comparison function for everything a single field in the spec and then you end up with",
    "start": "1180090",
    "end": "1186690"
  },
  {
    "text": "this super big list of comparison functions that you execute there are",
    "start": "1186690",
    "end": "1192059"
  },
  {
    "text": "very hard to maintain because there's always something you didn't plan like",
    "start": "1192059",
    "end": "1197820"
  },
  {
    "text": "you didn't plan this new label to be parried you didn't plan this new environment variables so you need to",
    "start": "1197820",
    "end": "1203280"
  },
  {
    "text": "touch the code again and it's never-ending so what how can we better handle that we",
    "start": "1203280",
    "end": "1211710"
  },
  {
    "start": "1211000",
    "end": "1342000"
  },
  {
    "text": "looked into all the controllers that are there already in kubernetes and try to find if they have this problem as well",
    "start": "1211710",
    "end": "1217650"
  },
  {
    "text": "turns out they do and the way the",
    "start": "1217650",
    "end": "1223290"
  },
  {
    "text": "stateful side controller handles it is pretty interesting whenever you update a stateful set there's this concept of",
    "start": "1223290",
    "end": "1228809"
  },
  {
    "text": "revision so your date of the stateful set spec leads to a new revision of the",
    "start": "1228809",
    "end": "1234090"
  },
  {
    "text": "stateful set and the way this revision is built is that it's actually made from",
    "start": "1234090",
    "end": "1240090"
  },
  {
    "text": "a hash of the spake of the specification of the stateful set at the time it was",
    "start": "1240090",
    "end": "1245190"
  },
  {
    "text": "updated and to know if a new revision is necessary well we just have to compute",
    "start": "1245190",
    "end": "1252929"
  },
  {
    "text": "that hash again and see if there's already a revision that exists with the same hash and here we can actually use",
    "start": "1252929",
    "end": "1259830"
  },
  {
    "text": "the same concepts whenever we create resource we can build a hash of that resource so building a hash can be done",
    "start": "1259830",
    "end": "1267059"
  },
  {
    "text": "in multiple ways the way it's done and the way we do in easy case that we just feel like a string representation of the",
    "start": "1267059",
    "end": "1273210"
  },
  {
    "text": "object using a library called spoon goes view and then we hashed that string",
    "start": "1273210",
    "end": "1278390"
  },
  {
    "text": "which gives us a small hash like a few characters that we actually store in an",
    "start": "1278390",
    "end": "1283770"
  },
  {
    "text": "annotation in the resource so that later on say in a subsequent reconciliation",
    "start": "1283770",
    "end": "1289380"
  },
  {
    "text": "when we want to know whether we need to update that resource we just build the hash of the expected resource we want",
    "start": "1289380",
    "end": "1296549"
  },
  {
    "text": "and we compare that with the hash of the actual resource and if they match then",
    "start": "1296549",
    "end": "1301860"
  },
  {
    "text": "we don't have to do the update if they don't we need to update so the world's",
    "start": "1301860",
    "end": "1307350"
  },
  {
    "text": "map thing here is that the actual hash is built at creation time so it does not",
    "start": "1307350",
    "end": "1316530"
  },
  {
    "text": "care at all about any field that was patched like kubernetes like any metadata that was added any any new",
    "start": "1316530",
    "end": "1323789"
  },
  {
    "text": "environment variables or whatever and mutating webhook patch so really we",
    "start": "1323789",
    "end": "1329070"
  },
  {
    "text": "compare hashes of the resource at the time it was built not at the time afterwards that it was patched and it's",
    "start": "1329070",
    "end": "1337740"
  },
  {
    "text": "pretty interesting to use whenever you need to reconcile some complex resources",
    "start": "1337740",
    "end": "1343639"
  },
  {
    "text": "usually when we build here these we try to build an abstraction of reality so",
    "start": "1344929",
    "end": "1350010"
  },
  {
    "text": "users don't have to care a lot about what happens on the underlying Banaras",
    "start": "1350010",
    "end": "1356490"
  },
  {
    "text": "cluster you want to deploy a 3-node elasticsearch cluster you don't care about the underlying dots so we can",
    "start": "1356490",
    "end": "1363419"
  },
  {
    "text": "build this really minimalistic simplistic version of the CLD with only the fields you care about but in reality",
    "start": "1363419",
    "end": "1370200"
  },
  {
    "text": "what happens is that most users want to do more they ask us hey can you can you",
    "start": "1370200",
    "end": "1376380"
  },
  {
    "text": "add this feature so that whatever pod created by the operator actually has this particular label or maybe give me",
    "start": "1376380",
    "end": "1383460"
  },
  {
    "text": "the power to change the affinity rule of the pulse so I can do zone awareness in",
    "start": "1383460",
    "end": "1388590"
  },
  {
    "text": "in the deployment of control and turns out best way to do that is",
    "start": "1388590",
    "end": "1395659"
  },
  {
    "text": "actually to empower the user completely to do whatever they want in the co D and that's what happens when you create a",
    "start": "1395659",
    "end": "1400909"
  },
  {
    "text": "stable set or deployment you have this blood template that you can specify entirely so here for example in the",
    "start": "1400909",
    "end": "1407419"
  },
  {
    "text": "operator what we do is that we have this optional pod template section users can do whatever they want here and in a",
    "start": "1407419",
    "end": "1413840"
  },
  {
    "text": "reconciliation when we build the expected but we want we take that template from the user and we build on",
    "start": "1413840",
    "end": "1419509"
  },
  {
    "text": "top of it we add our own fields in there and if they're already specified by the user we don't touch it that's kind of",
    "start": "1419509",
    "end": "1426049"
  },
  {
    "text": "the user responsibility so they can shoot themselves in the food if they want to usually they don't because you",
    "start": "1426049",
    "end": "1433879"
  },
  {
    "text": "you kind of know whether whatever you specifying here may or not break things",
    "start": "1433879",
    "end": "1440950"
  },
  {
    "text": "finally I'd like to end this presentation with a few things about",
    "start": "1443559",
    "end": "1450289"
  },
  {
    "text": "stateful sets that I learned system volumes that maybe you don't know about",
    "start": "1450289",
    "end": "1456379"
  },
  {
    "text": "and they are really interesting whenever you want to build operators that are managing stateful workloads",
    "start": "1456379",
    "end": "1463519"
  },
  {
    "text": "which by the way are pretty good reason to use an operator like stateful sets by",
    "start": "1463519",
    "end": "1469129"
  },
  {
    "text": "themselves on ice but if the workload you manage is a bit more complex than what the state full sets allows you to",
    "start": "1469129",
    "end": "1475460"
  },
  {
    "text": "do operators a pretty good way to solve that one first thing is that you cannot",
    "start": "1475460",
    "end": "1484669"
  },
  {
    "start": "1480000",
    "end": "1520000"
  },
  {
    "text": "resize volumes in stateful sets it's not supported by the state set controller",
    "start": "1484669",
    "end": "1490179"
  },
  {
    "text": "actually if you create a stateful set and you give each replica ten gigs of",
    "start": "1490179",
    "end": "1495559"
  },
  {
    "text": "storage you try to update it you cannot because that section in the stateful sets back like the volume clean template",
    "start": "1495559",
    "end": "1501950"
  },
  {
    "text": "is immutable you cannot change that locally there's a cap in progress to fix",
    "start": "1501950",
    "end": "1509119"
  },
  {
    "text": "that I'm not sure it's going to learn in the next version of kubernetes that",
    "start": "1509119",
    "end": "1516309"
  },
  {
    "text": "there's an issue for rate at least so it's good so what can you do in the meantime there are some workarounds",
    "start": "1516369",
    "end": "1523870"
  },
  {
    "start": "1520000",
    "end": "1570000"
  },
  {
    "text": "you can do things like delete the stateful set but keep the pods around then behind the scenes we resize the",
    "start": "1523870",
    "end": "1531549"
  },
  {
    "text": "volume then you recreate the stateful set that takes control over the pod again but that's pretty happy the way we",
    "start": "1531549",
    "end": "1538659"
  },
  {
    "text": "do it right now in the ACK operator is that whenever we need to change the",
    "start": "1538659",
    "end": "1545139"
  },
  {
    "text": "storage requirement of a given state offset we actually create a new stateful set with the new storage requirement",
    "start": "1545139",
    "end": "1551460"
  },
  {
    "text": "which is migrate data over the old pods from the new ones and because it's",
    "start": "1551460",
    "end": "1557230"
  },
  {
    "text": "elastic search is kind of easy to do we can just migrate shots from some elasticsearch nodes to some order so we",
    "start": "1557230",
    "end": "1562240"
  },
  {
    "text": "bring up that new stateful set with the new storage we migrate the data over there and once it's done we remove the",
    "start": "1562240",
    "end": "1568000"
  },
  {
    "text": "old safest and other important things",
    "start": "1568000",
    "end": "1575049"
  },
  {
    "start": "1570000",
    "end": "1711000"
  },
  {
    "text": "about volumes is the way storage classes",
    "start": "1575049",
    "end": "1580330"
  },
  {
    "text": "are specified so you probably know that whenever you use persistent volume what you usually do is first create a",
    "start": "1580330",
    "end": "1587500"
  },
  {
    "text": "persistent volume claim specifying which storage class you want and then the",
    "start": "1587500",
    "end": "1593309"
  },
  {
    "text": "controller is going to bind that claim to an actual persistent volume that's",
    "start": "1593309",
    "end": "1599919"
  },
  {
    "text": "actually what the state fools had controller does for example it just created system volume claim and then creates a pod that is mapped this way",
    "start": "1599919",
    "end": "1608370"
  },
  {
    "text": "here in this in the storage class this is important field called the volume binding mode and if you not set it to",
    "start": "1608370",
    "end": "1618720"
  },
  {
    "text": "wait for first consumer here for example what's going to happen is that you",
    "start": "1618720",
    "end": "1623740"
  },
  {
    "text": "create a stateful set the stateful said controller whenever it needs to create a pod is going to create a persistent",
    "start": "1623740",
    "end": "1629289"
  },
  {
    "text": "volume claim first that persistent volume frame is going to be bound to persistent volume but maybe that",
    "start": "1629289",
    "end": "1635230"
  },
  {
    "text": "persistent volume has some affinity constraints so that the volume is only available in a given region or maybe",
    "start": "1635230",
    "end": "1641649"
  },
  {
    "text": "it's a local volume so it's only available in a particular host and then the stateful side console creates the",
    "start": "1641649",
    "end": "1647529"
  },
  {
    "text": "part mapping the persistent volume claim and what could happen is that the pod",
    "start": "1647529",
    "end": "1653230"
  },
  {
    "text": "itself is scheduled with its own Trainz so maybe on a particular region or particular host and it's possible",
    "start": "1653230",
    "end": "1659580"
  },
  {
    "text": "that the place where the body schedule cannot actually access the volume and",
    "start": "1659580",
    "end": "1665400"
  },
  {
    "text": "that's a big problem and solve that there's this wait for first consumer",
    "start": "1665400",
    "end": "1670890"
  },
  {
    "text": "option so that the body schedule first and then the volume is created based on",
    "start": "1670890",
    "end": "1676710"
  },
  {
    "text": "the constraints on the public pot and I'm actually surprised that most persistent volume available out there in",
    "start": "1676710",
    "end": "1684840"
  },
  {
    "text": "many cloud providers don't have this enabled by default don't have this option set in many cases you can just do",
    "start": "1684840",
    "end": "1694530"
  },
  {
    "text": "it yourself you can touch the existing system volume storage class available and use this option or maybe just clone",
    "start": "1694530",
    "end": "1701549"
  },
  {
    "text": "it to a different storage class with a different name which have hashed this option because it's really important",
    "start": "1701549",
    "end": "1707400"
  },
  {
    "text": "especially when dealing with local volumes another funny scenario that we",
    "start": "1707400",
    "end": "1716309"
  },
  {
    "text": "encounter is when dealing with local system volume so you know maybe you're",
    "start": "1716309",
    "end": "1721770"
  },
  {
    "text": "just using the disk attached to particular communities host for that you're using a local system volume now",
    "start": "1721770",
    "end": "1730049"
  },
  {
    "text": "what happens when you need to a great a stateful set because the spec changed",
    "start": "1730049",
    "end": "1736460"
  },
  {
    "text": "normally what will happen is that processing but by pod the stateful set",
    "start": "1736460",
    "end": "1742559"
  },
  {
    "text": "controller would delete a pod then we create it in the new version and then",
    "start": "1742559",
    "end": "1747690"
  },
  {
    "text": "it's going to reattach automatically the same system volume all thanks to deterministic naming by the way because",
    "start": "1747690",
    "end": "1753690"
  },
  {
    "text": "turns out if we take the name of the pod and we use the same name for the volume then it's pretty easy to match them but",
    "start": "1753690",
    "end": "1762150"
  },
  {
    "text": "here between operation one and two what could happen in your kubernetes cluster",
    "start": "1762150",
    "end": "1768450"
  },
  {
    "text": "is that another pod get scheduled concurrently and it's actually taking",
    "start": "1768450",
    "end": "1774660"
  },
  {
    "text": "the resources that were necessary to recreate that pod that was deleted so",
    "start": "1774660",
    "end": "1781230"
  },
  {
    "text": "you expect stable set upgrade to grow through like pause activity then we created one by one but if in the",
    "start": "1781230",
    "end": "1788820"
  },
  {
    "text": "meantime another pod gets allocated through a particular host where your pod",
    "start": "1788820",
    "end": "1793980"
  },
  {
    "text": "needs to be recreated then you have a problem because that pot will then stay pending forever this scheduling",
    "start": "1793980",
    "end": "1801929"
  },
  {
    "text": "concurrency here can be a bit of a problem then whenever you did will we do",
    "start": "1801929",
    "end": "1809940"
  },
  {
    "start": "1806000",
    "end": "1948000"
  },
  {
    "text": "with stateful sets in and operate or you really have to pay attention to the update strategy you pick so by default",
    "start": "1809940",
    "end": "1816539"
  },
  {
    "text": "stateful set works in such a way that each pod gets rotated one by one when",
    "start": "1816539",
    "end": "1822179"
  },
  {
    "text": "you do an upgrade so it's based on the willingness of other pods if all three replicas already I kind of grayed the",
    "start": "1822179",
    "end": "1828539"
  },
  {
    "text": "first one then when it's ready again I can up create the second one and so on and so forth and if that's fine with you",
    "start": "1828539",
    "end": "1837590"
  },
  {
    "text": "chances are you may actually not need an operator at all because you can just",
    "start": "1837590",
    "end": "1842730"
  },
  {
    "text": "create stateful sets and it just works right but if you really need an operator of something else because you need some",
    "start": "1842730",
    "end": "1848700"
  },
  {
    "text": "kind of control whether we're rolling upgrades are done for example then probably this default strategy of",
    "start": "1848700",
    "end": "1854399"
  },
  {
    "text": "upgrading will not be good for you and luckily we have all the choices here we",
    "start": "1854399",
    "end": "1860039"
  },
  {
    "text": "can rely on a rolling or gate partition this is an index in the stateful set",
    "start": "1860039",
    "end": "1866250"
  },
  {
    "text": "spec that says every replicas having an ordinal higher than that particular",
    "start": "1866250",
    "end": "1871529"
  },
  {
    "text": "index can be safely rotated and upgraded by the state set controller for example if I have three replicas and in my",
    "start": "1871529",
    "end": "1879419"
  },
  {
    "text": "operator I I set this rolling update petition to - it means stable set controller can safely upgrade the",
    "start": "1879419",
    "end": "1886860"
  },
  {
    "text": "replicas on the three because probably on my side of things in the operator I did whatever is necessary so that your",
    "start": "1886860",
    "end": "1892799"
  },
  {
    "text": "grid of that bodies isn't is fine another way to deal with that is to use",
    "start": "1892799",
    "end": "1898289"
  },
  {
    "text": "the undelete strategy and the way it works here is that in the operator yourself to have the responsibility to",
    "start": "1898289",
    "end": "1904980"
  },
  {
    "text": "delete the part you want to upgrade because you know they can be safely deleted maybe I don't know you migrated",
    "start": "1904980",
    "end": "1911309"
  },
  {
    "text": "data away from them or you can have prepared your workload for that restart and then as soon as the ability the",
    "start": "1911309",
    "end": "1918809"
  },
  {
    "text": "stateful side controller she's in a way also an operator sort of controller will reconcile the stable set",
    "start": "1918809",
    "end": "1927050"
  },
  {
    "text": "resources again is going to recreate the pods itself so if you need to deal with",
    "start": "1927050",
    "end": "1932630"
  },
  {
    "text": "stateful sets ask yourself the question what's the best strategy I can use here most probably if there are some calls",
    "start": "1932630",
    "end": "1940430"
  },
  {
    "text": "you need to do for an upgrade we perform you not pick the default one but it's flexible so it's very good",
    "start": "1940430",
    "end": "1948760"
  },
  {
    "start": "1948000",
    "end": "2024000"
  },
  {
    "text": "finally what I wanted to say about stateful sell is you don't have to use",
    "start": "1949510",
    "end": "1954590"
  },
  {
    "text": "stateful sets to manage a full workload what a stateful sets really is just a",
    "start": "1954590",
    "end": "1960260"
  },
  {
    "text": "way of creating a pod and binding that pod to persistent volume",
    "start": "1960260",
    "end": "1965540"
  },
  {
    "text": "thanks to deterministic naming you know that if that pod gets killed I need to recreate it and the volume actually",
    "start": "1965540",
    "end": "1972620"
  },
  {
    "text": "already exists with the same name you can just match and bind that volume mounted to that boat so that that's",
    "start": "1972620",
    "end": "1979100"
  },
  {
    "text": "actually possible you don't need the state to set in order to do that you can manage things yourself",
    "start": "1979100",
    "end": "1986110"
  },
  {
    "text": "we tried what happened in reality is",
    "start": "1986110",
    "end": "1992420"
  },
  {
    "text": "that we just we implemented a slightly different version of the state whose",
    "start": "1992420",
    "end": "1998270"
  },
  {
    "text": "head controller within our operator which is probably not something you should do there are dragons everywhere",
    "start": "1998270",
    "end": "2005680"
  },
  {
    "text": "and stateful sets are there for that particular reason that's all I wanted to",
    "start": "2005680",
    "end": "2015010"
  },
  {
    "text": "say today thank you for listening to me [Applause]",
    "start": "2015010",
    "end": "2026790"
  }
]