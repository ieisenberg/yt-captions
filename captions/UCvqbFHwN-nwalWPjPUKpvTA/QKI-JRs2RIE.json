[
  {
    "start": "0",
    "end": "148000"
  },
  {
    "text": "welcome everyone this is a pretty big audience thanks for coming",
    "start": "30",
    "end": "5100"
  },
  {
    "text": "we're gonna tell you about ten ways to shoot yourself in the foot with kubernetes and hopefully number nine surprises you start with a brief",
    "start": "5100",
    "end": "11940"
  },
  {
    "text": "introduction so I'm Rob I am in charge of the compute team at data dogs we run all the kubernetes infrastructure there",
    "start": "11940",
    "end": "17130"
  },
  {
    "text": "and i'm rowhome beyond I am staff engineering the infrastructure organization at data Doug and I work on",
    "start": "17130",
    "end": "24090"
  },
  {
    "text": "communities too so first a little bit",
    "start": "24090",
    "end": "30060"
  },
  {
    "text": "about why you care what we have to say so probably most of you know that data dog is a monitoring service so customers",
    "start": "30060",
    "end": "36840"
  },
  {
    "text": "send us metrics traces logs they build dashboards and alerting through our product and so it's sort of a you know",
    "start": "36840",
    "end": "43079"
  },
  {
    "text": "positioned as a cloud native service provider where we sell a product to all of you as you're running your kubernetes",
    "start": "43079",
    "end": "48860"
  },
  {
    "text": "for us we're running the infrastructure that runs data dog and so our perspective is a little bit more that of",
    "start": "48860",
    "end": "54360"
  },
  {
    "text": "a cloud native end-user we're experiencing it all kind of similar to you and so that's the perspective you're gonna see throughout this talk we've",
    "start": "54360",
    "end": "66299"
  },
  {
    "text": "been working on the communities project for about a year and a half and we've had production workloads for about a",
    "start": "66299",
    "end": "73200"
  },
  {
    "text": "year now and we're starting to run pretty big clusters so in production we now have",
    "start": "73200",
    "end": "78619"
  },
  {
    "text": "several thousand nodes managed by kubernetes and to keep you an idea our",
    "start": "78619",
    "end": "83640"
  },
  {
    "text": "biggest cluster today is about two thousand nodes and the typical size cluster would be about 1,000 to 1,500",
    "start": "83640",
    "end": "89909"
  },
  {
    "text": "nodes and well as you can imagine managing clusters this size is naturally",
    "start": "89909",
    "end": "97920"
  },
  {
    "text": "a walk in the park and we've had like many outages and today we're gonna focus on the ones where we actually broke",
    "start": "97920",
    "end": "104460"
  },
  {
    "text": "things ourselves and try and share lessons learned so you can avoid making",
    "start": "104460",
    "end": "110310"
  },
  {
    "text": "the same mistakes we did so",
    "start": "110310",
    "end": "116780"
  },
  {
    "text": "the first time a team complained about DNS like about a year ago we figured",
    "start": "124450",
    "end": "130880"
  },
  {
    "text": "there was a problem with the application and we kind of dismissed it turns out while there's a lot of way to break DNS",
    "start": "130880",
    "end": "137060"
  },
  {
    "text": "and kubernetes and we've faced quite a few ones and I'm sure that if you run commands at scale you buddy should with",
    "start": "137060",
    "end": "143450"
  },
  {
    "text": "DNS so let's start with the first one so either quick reminder the way cuban",
    "start": "143450",
    "end": "151940"
  },
  {
    "start": "148000",
    "end": "148000"
  },
  {
    "text": "artists managed dns usually is the cubit is going to inject a result dot-com file and you can see all this search domain",
    "start": "151940",
    "end": "158930"
  },
  {
    "text": "there so the first one is gonna be namespace dot service dot plus two dots local then service that cluster dot",
    "start": "158930",
    "end": "164569"
  },
  {
    "text": "local then clustered Oracle and then the search domain inherited from the host you're running on in our case it's easy",
    "start": "164569",
    "end": "170090"
  },
  {
    "text": "to that internal there and there's also this option that's very important it's n dot equal five and if you're not",
    "start": "170090",
    "end": "176900"
  },
  {
    "text": "familiar with it this is how it's gonna are gonna work so imagine you're in a pod and you want to look for ww that",
    "start": "176900",
    "end": "183170"
  },
  {
    "text": "google.com since this name is less than five dots and this is what the Ender's option is for it's going to try and add",
    "start": "183170",
    "end": "190190"
  },
  {
    "text": "auto search domain to it so the first queries to the DNS server is actually going to be W W that google.com that",
    "start": "190190",
    "end": "195980"
  },
  {
    "text": "namespace the service also got local and of course the DNS server is going to say I have no clue why berries and the",
    "start": "195980",
    "end": "202579"
  },
  {
    "text": "resolver is going to go through all these lists and finally send the raw query which is going to actually resolve",
    "start": "202579",
    "end": "207950"
  },
  {
    "text": "to an IP address this is pretty inefficient because of course you do",
    "start": "207950",
    "end": "213980"
  },
  {
    "text": "five queries for n and so that's pretty simple and it is very bad on the client-side because it's much slower and",
    "start": "213980",
    "end": "220459"
  },
  {
    "text": "it's also bad on the core DNS server because we use called DNS because you have a lot of queries that are actually useless",
    "start": "220459",
    "end": "227560"
  },
  {
    "text": "there's this pretty interesting feature in core DNS which is called Auto pass and the way it works if you see the",
    "start": "228720",
    "end": "234390"
  },
  {
    "text": "query the first query to the one that was there was showing earlier this when coordinates is getting the square with",
    "start": "234390",
    "end": "240750"
  },
  {
    "text": "Auto pass enabled is going to strip the namespace your service address without local and try to be clever and find out",
    "start": "240750",
    "end": "247200"
  },
  {
    "text": "what was actually mount and trying to find the proper answer in this case it's",
    "start": "247200",
    "end": "252420"
  },
  {
    "text": "going to answer with the cname 4ww that we got con and then at the IP address as in a records so it's much better and",
    "start": "252420",
    "end": "259410"
  },
  {
    "text": "it's a single DNS query instead of five it's much faster it's better so we had",
    "start": "259410",
    "end": "264750"
  },
  {
    "text": "high hopes what happened one day is some",
    "start": "264750",
    "end": "270030"
  },
  {
    "text": "teams are complaining that DNS was broken and that DNS query is coming out and we looked at the cadenas metrics and",
    "start": "270030",
    "end": "275940"
  },
  {
    "text": "we saw that suddenly the matter of self error to upstream was very high means",
    "start": "275940",
    "end": "281880"
  },
  {
    "text": "usually zero and it was very high and we investigated I turned out who were really limited by the upstream and what",
    "start": "281880",
    "end": "289320"
  },
  {
    "text": "had happened is that the number of queries had increased and since we had",
    "start": "289320",
    "end": "295770"
  },
  {
    "text": "other person able auto pass is trying to be clever and always do an option resolution to match an upstream query",
    "start": "295770",
    "end": "301560"
  },
  {
    "text": "and it doesn't do any cash so at one point we just reached this limit where nothing was working anymore because we",
    "start": "301560",
    "end": "308040"
  },
  {
    "text": "couldn't reach the option server anymore and so any resolution outside of the cursor was failing which was very bad as",
    "start": "308040",
    "end": "314310"
  },
  {
    "text": "you can imagine well this one is also about DNS and",
    "start": "314310",
    "end": "320700"
  },
  {
    "text": "we've had quite a few we're only gonna share two today in our cluster we use IP",
    "start": "320700",
    "end": "329730"
  },
  {
    "start": "326000",
    "end": "326000"
  },
  {
    "text": "vs for load balance services so we access the core DNS service with IP vs",
    "start": "329730",
    "end": "334800"
  },
  {
    "text": "the way it works is the part is going to send DNS query to the virtual server",
    "start": "334800",
    "end": "341100"
  },
  {
    "text": "that is backed by the backend but which are the co DNS pods what's important in you're gonna see next while if that there is also an IP",
    "start": "341100",
    "end": "349080"
  },
  {
    "text": "vs contract that's tracking all the connections managed by APs when you do a rolling update what's",
    "start": "349080",
    "end": "356030"
  },
  {
    "text": "happening is actually when you depart is going to be deleted it's going to be removed from the back-end servers and a",
    "start": "356030",
    "end": "363110"
  },
  {
    "text": "new part isn't be created in this case you can see that put a is gun and put has been added but if you have a high",
    "start": "363110",
    "end": "369560"
  },
  {
    "text": "lot of queries as you can see in the contract down there the contract from",
    "start": "369560",
    "end": "374690"
  },
  {
    "text": "products to put a use actor is still in there and if you have a high or low",
    "start": "374690",
    "end": "380840"
  },
  {
    "text": "that's high enough what X is going to be reused at one point and this query is going to be ready to put a since IPS",
    "start": "380840",
    "end": "387620"
  },
  {
    "text": "doesn't have a back-end for a anymore it's going to be dropped by the kernel which is thirty bad because the genus",
    "start": "387620",
    "end": "393380"
  },
  {
    "text": "pack is just going to be gone there's actually a tunable you can use in the",
    "start": "393380",
    "end": "400070"
  },
  {
    "text": "kennel to make it a lot better but this tunable is not set by it wasn't set by through proxy at the point at the time",
    "start": "400070",
    "end": "406280"
  },
  {
    "text": "so it was actually kind of a problem so we introduced this option in the in the",
    "start": "406280",
    "end": "412640"
  },
  {
    "text": "proxy implemented but we introduced it up after a graceful termination so the",
    "start": "412640",
    "end": "419060"
  },
  {
    "text": "behavior I described before was how IPS was working before a graceful",
    "start": "419060",
    "end": "424130"
  },
  {
    "text": "termination was introduced in 1.12 and back pod in what set in 111 graceful termination is trying to be a lot more",
    "start": "424130",
    "end": "430220"
  },
  {
    "text": "clever instead of removing a background and black holding traffic when you delete a pod is going to say the weight",
    "start": "430220",
    "end": "435830"
  },
  {
    "text": "to zero which means no new connections are gonna be sent to purge with weight zero but establish connection has to",
    "start": "435830",
    "end": "442040"
  },
  {
    "text": "gonna work this is much better for TCP however it's kind of pretty bad for UDP",
    "start": "442040",
    "end": "448100"
  },
  {
    "text": "because it's exactly it's going to be almost the same behavior as before if you reuse the pot that's already mapped",
    "start": "448100",
    "end": "453740"
  },
  {
    "text": "you still gonna get routed to a pod that's not there anymore so it's going to be slightly different the packet is",
    "start": "453740",
    "end": "459350"
  },
  {
    "text": "not can be dropped by the kernel it's gonna be routed to an IP but this IP is not there anymore Eugenia Square is",
    "start": "459350",
    "end": "465320"
  },
  {
    "text": "gonna film the good thing is q proxy maintainer",
    "start": "465320",
    "end": "471560"
  },
  {
    "text": "czar well aware of this and we've been working on it for quite a while and now we have a good idea of how to fix it and",
    "start": "471560",
    "end": "476810"
  },
  {
    "text": "hopefully we have a PR that's gonna make it a lot better in the next few days I think well I think Laurent will talk",
    "start": "476810",
    "end": "485570"
  },
  {
    "text": "about DNS all day if we let them but let's talk about something else let's have some diversity so this is a kind of",
    "start": "485570",
    "end": "492920"
  },
  {
    "text": "an interesting one you know we really don't like when our users come to us with error reports we usually like them we catch them with our monitoring first",
    "start": "492920",
    "end": "498260"
  },
  {
    "text": "but in this case users came to us and they said that their jobs weren't starting and we saw some evidence that it might be related to image polls so we",
    "start": "498260",
    "end": "504920"
  },
  {
    "text": "went to go look at some state of all the pods on this cluster and we see that there is something going on there's",
    "start": "504920",
    "end": "510650"
  },
  {
    "text": "quite a lot of error state pods related to image polls it's not real it's not",
    "start": "510650",
    "end": "515780"
  },
  {
    "text": "affecting running applications so this is good but there's certainly something going on first place we look is that",
    "start": "515780",
    "end": "521270"
  },
  {
    "text": "some metrics related to image polls so you can see that there's an interesting pattern here sort of drastic increase in",
    "start": "521270",
    "end": "528560"
  },
  {
    "text": "the number of image polls that's sustained for a while and then another drastic increase so there's certainly something weird you see this sudden",
    "start": "528560",
    "end": "534560"
  },
  {
    "text": "change and so first place we go is to look at what the actual responses from the image registry are well they're not",
    "start": "534560",
    "end": "540920"
  },
  {
    "text": "good we're seeing quite a lot of 4:29 so indicating we're being rate limited by our by our image registry so at this",
    "start": "540920",
    "end": "547370"
  },
  {
    "text": "point we realized that no nodes in this entire region are able to pull from our",
    "start": "547370",
    "end": "552440"
  },
  {
    "text": "image registry so what happens well we created some change on the permissions",
    "start": "552440",
    "end": "558440"
  },
  {
    "text": "on a bucket that one of our applications required read access to and it could no longer start up and so this demon set",
    "start": "558440",
    "end": "564410"
  },
  {
    "text": "that's running on roughly a thousand nodes on this cluster starts going into crash loop but it had a really dangerous",
    "start": "564410",
    "end": "569510"
  },
  {
    "text": "feature enabled image pool policy always and so every time it's restarting it's doing another pull of the image that's",
    "start": "569510",
    "end": "575240"
  },
  {
    "text": "already existing on the host and so this sort of compounded by the fact that we",
    "start": "575240",
    "end": "581960"
  },
  {
    "text": "actually have all of our traffic to our image registry is going through three and add instances one persona in this region and so the image registry is",
    "start": "581960",
    "end": "588800"
  },
  {
    "text": "looking at this all this source traffic coming from the only those three IPS is seeing it's way way way unacceptable and",
    "start": "588800",
    "end": "594950"
  },
  {
    "text": "rate limit at us well what could we do to fix it we replace the impacted mat",
    "start": "594950",
    "end": "600380"
  },
  {
    "text": "instances and everything should be fine now we're new source IPS and so the traffic is okay but we forgot that we actually",
    "start": "600380",
    "end": "607290"
  },
  {
    "text": "access some critical services over the Internet at that time we were running a cloud sequel database and it wasn't",
    "start": "607290",
    "end": "613980"
  },
  {
    "text": "available over private VPC at at that point and so all the apps can I think the cloud sequel at this critical",
    "start": "613980",
    "end": "619410"
  },
  {
    "text": "database had their connections severed and stopped working so after a couple of",
    "start": "619410",
    "end": "625530"
  },
  {
    "text": "hours of digging around where we managed these static IPs and reconfiguring the firewall rules everything came back and",
    "start": "625530",
    "end": "631580"
  },
  {
    "text": "everything was back to normal but we had quite a bit of follow-up to do after this one and the first one was creating",
    "start": "631580",
    "end": "639390"
  },
  {
    "start": "636000",
    "end": "636000"
  },
  {
    "text": "an image what an admission webhook that disallowed some of these floating tags that we use on images latest is the one",
    "start": "639390",
    "end": "645180"
  },
  {
    "text": "that's sort of most commonly used so we started there and we attached this admission web hook to a number of different resources deployments staple",
    "start": "645180",
    "end": "652650"
  },
  {
    "text": "sets PaymentSense jobs and pods pretty standard right the problem is that the",
    "start": "652650",
    "end": "658920"
  },
  {
    "text": "control loop that's creating pods is generally something that's running in the cluster right so we're not surfacing this feedback to users",
    "start": "658920",
    "end": "664650"
  },
  {
    "text": "there's no users involved when we're creating new pods so existing deployments or other workloads are",
    "start": "664650",
    "end": "669780"
  },
  {
    "text": "creating new pods as a result of nodes dying and it being rescheduled or scale ups or anything like that and these pods",
    "start": "669780",
    "end": "676470"
  },
  {
    "text": "are actually being rejected by the web book so we're unable to schedule anything new in the cluster so we",
    "start": "676470",
    "end": "682350"
  },
  {
    "text": "adjusted our web book to not apply to pod objects just apply to workload objects and several days later a week",
    "start": "682350",
    "end": "687750"
  },
  {
    "text": "later were finally back to normal after our image registry issues when users",
    "start": "687750",
    "end": "695210"
  },
  {
    "text": "because one morning with an issue which was I can't access API server and I can't use cube CTL were like well this",
    "start": "695210",
    "end": "703770"
  },
  {
    "text": "is probably a very simple issue probably just a miss configuration but then someone else just pinged us again so I",
    "start": "703770",
    "end": "710100"
  },
  {
    "text": "tried and I couldn't connect either so things were trying to we're starting to look pretty bad so the first thing we",
    "start": "710100",
    "end": "717240"
  },
  {
    "start": "716000",
    "end": "716000"
  },
  {
    "text": "did is look at the API server and as you can see on this graph the load of the IPS ever is huge",
    "start": "717240",
    "end": "723150"
  },
  {
    "text": "I mean it's amounting to about about 60 and to give you an idea this node have 8",
    "start": "723150",
    "end": "728850"
  },
  {
    "text": "calls so it's it's pretty bad and looking deeper into what was",
    "start": "728850",
    "end": "734570"
  },
  {
    "text": "happening on the note we saw that the memory was going down and that API servers were actually getting uma killed",
    "start": "734570",
    "end": "740000"
  },
  {
    "text": "which as you can imagine is not a really good situation to be in",
    "start": "740000",
    "end": "744460"
  },
  {
    "text": "another interesting metric was that suddenly Advent traffic was much higher",
    "start": "745240",
    "end": "750860"
  },
  {
    "text": "than before on the API server at the add point we had no idea why so we tried to",
    "start": "750860",
    "end": "755900"
  },
  {
    "text": "understand what we had changed and it turned out we had just made a small change to achieve - I am so I don't know",
    "start": "755900",
    "end": "761480"
  },
  {
    "text": "if you know you - I am but it's basically a demon set to run that allows everybody to have the difference do it",
    "start": "761480",
    "end": "766940"
  },
  {
    "text": "to have different permissions on AWS so we run them on every node on AWS and we",
    "start": "766940",
    "end": "772130"
  },
  {
    "text": "had made a small change and keep trying to improve security and start the deployment and the deployment was lining",
    "start": "772130",
    "end": "778610"
  },
  {
    "text": "exactly with with the issue so we looked at two at cube - I am and as you can see on this graph QAM pods were restarting",
    "start": "778610",
    "end": "785480"
  },
  {
    "text": "very easily which wasn't expected at all so the reason this place where we",
    "start": "785480",
    "end": "791480"
  },
  {
    "text": "studied was because they were getting em killed so we started increasing the memory limit on the on the pods and",
    "start": "791480",
    "end": "796750"
  },
  {
    "text": "typical usage would be a few megabytes but after this change but was now consuming 300 megabytes which was pretty",
    "start": "796750",
    "end": "804470"
  },
  {
    "text": "surprising to us the reason for the issue was we had made a patch to improve",
    "start": "804470",
    "end": "810380"
  },
  {
    "text": "security in cube Diane and we had made a small typo on the selector used by cube - I am so cube - I am needs to",
    "start": "810380",
    "end": "816890"
  },
  {
    "text": "synchronize all the pods running on the node where to - LEM is running to get the annotation of all the pod it's",
    "start": "816890",
    "end": "823040"
  },
  {
    "text": "co-located with to know which role to assume so you have an annotation you had and so it needed to know for auto pod",
    "start": "823040",
    "end": "828800"
  },
  {
    "text": "running on the node which annotation they have and by mistake we had removed this selector so instead of sinking",
    "start": "828800",
    "end": "835070"
  },
  {
    "text": "maybe 5 10 20 pots locally on each cube - I am we were now sinking the global",
    "start": "835070",
    "end": "840650"
  },
  {
    "text": "cluster which was a very huge one and of course I mean crypto gem was by",
    "start": "840650",
    "end": "846290"
  },
  {
    "text": "consequence consuming a lot more memory and was being killed and of course I mean requesting information about all",
    "start": "846290",
    "end": "851840"
  },
  {
    "text": "the pots in the pressure on each bird in a diamond set is pretty bad on the API 7zy everything broke we should have seen",
    "start": "851840",
    "end": "862040"
  },
  {
    "text": "that before but we tested on much smaller cluster and prosit was it an issue at all so only",
    "start": "862040",
    "end": "867050"
  },
  {
    "text": "when we deploy to staging with the last confessor that everything started to break all this is another user reported",
    "start": "867050",
    "end": "878000"
  },
  {
    "text": "one and so we're getting a report that no nodes are no new nodes our scheduling",
    "start": "878000",
    "end": "884390"
  },
  {
    "text": "application pods so this is weird right so we're kind of not strangers to scheduling issues and so we take a look",
    "start": "884390",
    "end": "890150"
  },
  {
    "text": "we see a really sharp increase in the number of events coming from kubernetes at this time and there's a few types of",
    "start": "890150",
    "end": "895220"
  },
  {
    "text": "events that we're sort of concerned with right the first one is failed scheduling not a surprise we know it's a scheduling issue already and and the second one is",
    "start": "895220",
    "end": "902270"
  },
  {
    "text": "not trigger scale-up so this is from another component in the cluster the cluster autoscaler and it's sort of",
    "start": "902270",
    "end": "907370"
  },
  {
    "text": "indicating that none of the node types that we have running in our cluster could satisfy what our applications are requesting we know that nodes are",
    "start": "907370",
    "end": "914270"
  },
  {
    "text": "already running applications and so there should be notes available that we don't have any new deployments that are changing resource requests so it's a",
    "start": "914270",
    "end": "920930"
  },
  {
    "text": "little bit confusing so quick aside and sort of how we handle scheduling here we have a custom resource type that we call",
    "start": "920930",
    "end": "927380"
  },
  {
    "start": "924000",
    "end": "924000"
  },
  {
    "text": "a node group and we make a lot of assumptions on our applications that they're single tenants on a node and so we use paints and Toleration as well as",
    "start": "927380",
    "end": "934730"
  },
  {
    "text": "resource requests to sort of take an application and tie it to a particular node type based on what we expect the",
    "start": "934730",
    "end": "940520"
  },
  {
    "text": "resources to be available that includes what we're using for system components and daemon sets as well well we messed",
    "start": "940520",
    "end": "947210"
  },
  {
    "text": "up a little bit and we added a new demon set and we added resource requests to it so all of these nodes that should have",
    "start": "947210",
    "end": "952820"
  },
  {
    "text": "been able to schedule our applications on them based on their existing resource request could no longer fit the applications on there because of this",
    "start": "952820",
    "end": "958670"
  },
  {
    "text": "additional daemon said even worse the daemons that actually had a pod priority",
    "start": "958670",
    "end": "964250"
  },
  {
    "text": "of critical and so we got really lucky that we did this in a 1.10 cluster had",
    "start": "964250",
    "end": "969830"
  },
  {
    "text": "this been on a newer cluster this daemons that would have actually evicted all of our workloads off of our hosts and we would have taken all the data dog",
    "start": "969830",
    "end": "976250"
  },
  {
    "text": "down this one was pretty interesting what",
    "start": "976250",
    "end": "984180"
  },
  {
    "text": "might happen is one day blokes Tim reached out and told us the number of the volume of flow you're actually",
    "start": "984180",
    "end": "989970"
  },
  {
    "text": "sending now is much bigger than it used to be and it's actually the indexing of few logs is getting slower and we wonder",
    "start": "989970",
    "end": "996510"
  },
  {
    "text": "if it's legitimate so we hadn't change anything so we're pretty sure we it wasn't a big deal but still we looked at the volumes of log we",
    "start": "996510",
    "end": "1004730"
  },
  {
    "start": "1003000",
    "end": "1003000"
  },
  {
    "text": "were actually standing so this is the volume of log per person in our case and",
    "start": "1004730",
    "end": "1011090"
  },
  {
    "text": "as you can see like typical normal behavior would be to send about 1 million log per minute on this and this",
    "start": "1011090",
    "end": "1017270"
  },
  {
    "text": "particular infra and suddenly it had over a few hours going from 1 million to",
    "start": "1017270",
    "end": "1022640"
  },
  {
    "text": "about 15 million minutes this is like quite a lot so what shall happen",
    "start": "1022640",
    "end": "1032900"
  },
  {
    "text": "actually is for security reason we had an able audit on the machine so we had deployed a demon set that was using go",
    "start": "1032900",
    "end": "1039230"
  },
  {
    "text": "audit and they bring unobligated and what happens when you're on criminal test is the nodes where you run the",
    "start": "1039230",
    "end": "1045650"
  },
  {
    "text": "cubelet and the containers are actually very heavy users of audit logs because they do a lot of sis calls a lot of",
    "start": "1045650",
    "end": "1051920"
  },
  {
    "text": "execs and a lot of IP table commands and all of these are logged and this is actually the reason for this increase of",
    "start": "1051920",
    "end": "1057800"
  },
  {
    "text": "logs so how we fixed that well we told the logs in that we didn't need that traffic so they dropped all the traffic",
    "start": "1057800",
    "end": "1064910"
  },
  {
    "text": "at intake and on our side we remove the demon sets and things seem to go back to normal",
    "start": "1064910",
    "end": "1071950"
  },
  {
    "text": "where did all my pods go so we have an application that enabled auto-scaling",
    "start": "1075280",
    "end": "1080840"
  },
  {
    "text": "for their for their for their application and all of their pods disappeared so typically when we have a",
    "start": "1080840",
    "end": "1086570"
  },
  {
    "start": "1085000",
    "end": "1085000"
  },
  {
    "text": "deployment we're explicitly setting the number of replicas in this case we have 60 for this application and we want to enable auto scaling using a horizontal",
    "start": "1086570",
    "end": "1092990"
  },
  {
    "text": "pod autoscaler this is basically a description of an auto scaling policy in this case it's a really simple one",
    "start": "1092990",
    "end": "1098870"
  },
  {
    "text": "looking at the CPU usage for the pod scaling it up and down based on what our target is specified as in this case what",
    "start": "1098870",
    "end": "1106550"
  },
  {
    "text": "happens is a controller that's in charge of horizontal pod auto scalars is managing the replicas count of the deployment as it reads through the the",
    "start": "1106550",
    "end": "1114770"
  },
  {
    "text": "metric values and so we don't want to be setting that value explicitly anymore we",
    "start": "1114770",
    "end": "1119990"
  },
  {
    "text": "want to remove it from our spec and we want to allow this to be something a dynamic runtime decision well there's a",
    "start": "1119990",
    "end": "1125600"
  },
  {
    "text": "little trick to this what happens when you have this value specified in a",
    "start": "1125600",
    "end": "1131020"
  },
  {
    "text": "manifest that's applied by a cube control is that cube control adds an",
    "start": "1131020",
    "end": "1136400"
  },
  {
    "text": "annotation on it of what the manifest looked like when it was applied and so we can see all the fields there when we",
    "start": "1136400",
    "end": "1141620"
  },
  {
    "text": "submit the new manifest to the API it compares what it was last and what it is now and it sees when it's removed it",
    "start": "1141620",
    "end": "1147170"
  },
  {
    "text": "actually defaults it to one so we scaled down 59 replicas of the application there's not a great work around for this",
    "start": "1147170",
    "end": "1154280"
  },
  {
    "text": "one unfortunately you need to sort of modify this annotation at the same time as you're submitting your manifest but",
    "start": "1154280",
    "end": "1159410"
  },
  {
    "text": "we're not we're not managing this annotation as part of our manifest and so we need to go in and use Q control",
    "start": "1159410",
    "end": "1164810"
  },
  {
    "text": "edit to change the replica count to remove it and remove the last applied annotation so that we can not scale down",
    "start": "1164810",
    "end": "1172070"
  },
  {
    "text": "all of our applications a little bit tricky you know 80s sometimes computers",
    "start": "1172070",
    "end": "1179090"
  },
  {
    "text": "seems to do things on their own and it feels like there's it goes doing things and this is how this morning started so",
    "start": "1179090",
    "end": "1187670"
  },
  {
    "start": "1187000",
    "end": "1187000"
  },
  {
    "text": "the data store team reached out and told us while we deployed 120 newscasters",
    "start": "1187670",
    "end": "1193400"
  },
  {
    "text": "rehearsal yesterday and everything went up fine and everything was working perfectly but when we got back this",
    "start": "1193400",
    "end": "1199610"
  },
  {
    "text": "morning this cluster was completely broken and nobody had done anything so yeah we were splitting a ghost",
    "start": "1199610",
    "end": "1206890"
  },
  {
    "text": "bye we looked into details and it turns out 25% of the birds were actually",
    "start": "1206890",
    "end": "1212360"
  },
  {
    "text": "pending now and they had volume affinity issues which was very weird we went",
    "start": "1212360",
    "end": "1222530"
  },
  {
    "text": "deeper and it turned out like 25% of the node had been deleted and since we used",
    "start": "1222530",
    "end": "1227630"
  },
  {
    "text": "local volumes for Cassandra clusters this third were bound to local volumes that were associated to notes that were",
    "start": "1227630",
    "end": "1234140"
  },
  {
    "text": "gone and so they were pending because they were not scheduled because they couldn't find their decision volumes so",
    "start": "1234140",
    "end": "1240170"
  },
  {
    "text": "I know I mean we're supposed to be clawed native and be resilient to node failure but 25% of the node is kind of a",
    "start": "1240170",
    "end": "1245990"
  },
  {
    "text": "big deal so we were pretty sure the cloud provider Adam deleted twenty five nodes 20 percent of the node that night",
    "start": "1245990",
    "end": "1254080"
  },
  {
    "text": "what had happened is we were we had deployed the Cassandra cluster on an auto scaling group a single one over",
    "start": "1254080",
    "end": "1260300"
  },
  {
    "text": "three aces but when we did the deployment there was they were capacity issues on one of the Aces and only 20",
    "start": "1260300",
    "end": "1267830"
  },
  {
    "text": "nodes were available so what Elias had done is create mo nodes in you know another area where",
    "start": "1267830",
    "end": "1273680"
  },
  {
    "text": "there was no capacity issues which was fine and this is the last part of the graph but during the night capacity",
    "start": "1273680",
    "end": "1279110"
  },
  {
    "text": "became available on the second zone so the auto scaling group is actually pretty clever and what it does it tried",
    "start": "1279110",
    "end": "1285050"
  },
  {
    "text": "to rebalance so you have an even number of nodes in in obviously and so 4040 per",
    "start": "1285050",
    "end": "1290090"
  },
  {
    "text": "nodes and so it started deleting our instance very actively and of course everything exploded so was like",
    "start": "1290090",
    "end": "1297340"
  },
  {
    "text": "pretty surprising Ron and so the way we fix it is now we have a different auto scaling group in each zone which is a",
    "start": "1297340",
    "end": "1303410"
  },
  {
    "text": "lot safer worse situation is well if we lack a passenger zone at least we know",
    "start": "1303410",
    "end": "1308510"
  },
  {
    "text": "that all the nodes are not going to come up and so we won't be able to create the cluster that we can just wait for it getting this kind of surprise is a bad",
    "start": "1308510",
    "end": "1315890"
  },
  {
    "text": "one we have a report of a slow deploy",
    "start": "1315890",
    "end": "1322350"
  },
  {
    "text": "heartbeat and probably nobody knows what that is so let me explain we run a sort of end-to-end test for our deployments",
    "start": "1322350",
    "end": "1328710"
  },
  {
    "text": "so this is everything from source control through some templating steps through our deploy tooling and to applaud running in kubernetes and this",
    "start": "1328710",
    "end": "1334320"
  },
  {
    "text": "helps us get a sort of end to end tests of latency of how all these systems are behaving and so this is sort of what it",
    "start": "1334320",
    "end": "1341790"
  },
  {
    "text": "looked like over the course of a month in one of our larger clusters and we can see it's getting progressively worse but we're growing the cluster at this",
    "start": "1341790",
    "end": "1348840"
  },
  {
    "text": "time we're aggressively deploying workloads on surveys so we're really not sure if this is just scheduling latency",
    "start": "1348840",
    "end": "1354720"
  },
  {
    "text": "if something's we're reaching some bottlenecks in one of the control plane components or what's going on well it",
    "start": "1354720",
    "end": "1360870"
  },
  {
    "text": "dig a little bit deeper and we find another metric that correlates really well with this and this is the number of pods that the sed that the API server is",
    "start": "1360870",
    "end": "1367679"
  },
  {
    "text": "a has seen and it's stored in sed and so what are we looking at here well we had",
    "start": "1367679",
    "end": "1373200"
  },
  {
    "text": "4,000 pending pods in our cluster that were unschedulable all turns out that we",
    "start": "1373200",
    "end": "1379890"
  },
  {
    "text": "had a cron job set to create a pod to create a job every 10 minutes and it had the wrong Toleration applied and so",
    "start": "1379890",
    "end": "1385679"
  },
  {
    "text": "there were no nodes in the cluster that could satisfy the Toleration on this pod so we've got 4,000 of these pending and",
    "start": "1385679",
    "end": "1392510"
  },
  {
    "text": "every single loop of the scheduler is considering all 4,000 of these pods and trying to place it onto nodes in the",
    "start": "1392510",
    "end": "1398100"
  },
  {
    "text": "cluster so this is impacting the performance of the scheduler the cluster autoscaler and several other components in the control plan",
    "start": "1398100",
    "end": "1405529"
  },
  {
    "text": "I mean will we use containers and well we expect them to be contained and well",
    "start": "1408030",
    "end": "1414750"
  },
  {
    "text": "we're gonna say that it's not always the case so the first topic in this one is broken runtime we use continuity as a",
    "start": "1414750",
    "end": "1422310"
  },
  {
    "text": "runtime and we've broken it in in many ways and we're going to give you a few examples so the first one was a team",
    "start": "1422310",
    "end": "1432300"
  },
  {
    "start": "1429000",
    "end": "1429000"
  },
  {
    "text": "reached out because they couldn't delete a pod in a node and when they deleted it",
    "start": "1432300",
    "end": "1437640"
  },
  {
    "text": "it was it was it kept and being in a terminating state and was never going anyway so I connected to the instance",
    "start": "1437640",
    "end": "1444630"
  },
  {
    "text": "and try and was trying to do things so I was unable to do anything continuity was",
    "start": "1444630",
    "end": "1450210"
  },
  {
    "text": "complaining quite a lot so I looked at the process tree and as you can see there on the processor II we see a lot",
    "start": "1450210",
    "end": "1456330"
  },
  {
    "text": "of zombies and this screenshot there is only a very small part of the list because you can see there on the PS",
    "start": "1456330",
    "end": "1463170"
  },
  {
    "text": "command that we had 16,000 zombies on the machine as you can imagine the key",
    "start": "1463170",
    "end": "1468870"
  },
  {
    "text": "owner was not very happy about that so how did that happen this application",
    "start": "1468870",
    "end": "1477870"
  },
  {
    "text": "was a redis slave and master and that a readiness probe that was doing the common here so it's a small shell script",
    "start": "1477870",
    "end": "1484710"
  },
  {
    "text": "but basically the gist of it is this creep was doing a Redis ping and master and slave nodes it turned out ready",
    "start": "1484710",
    "end": "1491820"
  },
  {
    "text": "Springs are lot faster on master nodes than slave nodes so master node was very fine were completely fine however",
    "start": "1491820",
    "end": "1497490"
  },
  {
    "text": "sometimes it took a few seconds for ping to work on slave on slash buds and since",
    "start": "1497490",
    "end": "1502800"
  },
  {
    "text": "the timeout was set to 1 seconds the exact command was killed and since there",
    "start": "1502800",
    "end": "1508230"
  },
  {
    "text": "was nothing to rip the children until the different process in the in the image in the process tree all these were",
    "start": "1508230",
    "end": "1514410"
  },
  {
    "text": "remaining as zombies so as you can imagine if you run this readiness probe every minute you can get quite a few zombie very fast so the main takeaway",
    "start": "1514410",
    "end": "1521880"
  },
  {
    "text": "under this one is be very careful with exact based probe and please use something like 10 years and in it as pit",
    "start": "1521880",
    "end": "1529740"
  },
  {
    "text": "1 in your containers I think this was this is probably my",
    "start": "1529740",
    "end": "1536970"
  },
  {
    "start": "1532000",
    "end": "1532000"
  },
  {
    "text": "favorite issue it also starting very similarly to the one before sometime rich doubts then telling me like I can't",
    "start": "1536970",
    "end": "1544110"
  },
  {
    "text": "delete a bird on a note so I connect it to a node who looked at the process and",
    "start": "1544110",
    "end": "1549480"
  },
  {
    "text": "I were like okay and we're just going to kill this process so I tried to kill it didn't work I tried to SiC kill it",
    "start": "1549480",
    "end": "1555570"
  },
  {
    "text": "didn't work so I was starting wondering how I mean how could I be in a situation",
    "start": "1555570",
    "end": "1560640"
  },
  {
    "text": "when I couldn't kill a process so I look at what the process was doing and as you can see this process was taking a",
    "start": "1560640",
    "end": "1566340"
  },
  {
    "text": "function called refrigerator so I knew my morning was gonna be interesting what",
    "start": "1566340",
    "end": "1576600"
  },
  {
    "text": "had happened this time is these instances were running Kefka workloads and we had an issue with the nvme driver",
    "start": "1576600",
    "end": "1584220"
  },
  {
    "text": "and one of the IO from Kafka was hanging because the nvme driver had an issue",
    "start": "1584220",
    "end": "1590570"
  },
  {
    "text": "since the IO was blocking there was no way to help the process so this is",
    "start": "1590570",
    "end": "1596039"
  },
  {
    "text": "penguin I couldn't see kill it however what happened when you do when you delete a container the first thing that",
    "start": "1596039",
    "end": "1602039"
  },
  {
    "text": "the runtime is going to do is going to try and freeze the C group so all the process in the C group of the container",
    "start": "1602039",
    "end": "1607200"
  },
  {
    "text": "to make sure that these processes are not going to do anything when they deleted so the Kerner was trying to",
    "start": "1607200",
    "end": "1613919"
  },
  {
    "text": "freeze the she group but it couldn't because one of the processes in the C group was stuck on an IO and that's this",
    "start": "1613919",
    "end": "1621480"
  },
  {
    "text": "refrigerator function is actually the function calls by freezing the she group so yeah this is interesting because the",
    "start": "1621480",
    "end": "1628830"
  },
  {
    "text": "underlying issue is a very low level disk issue",
    "start": "1628830",
    "end": "1633710"
  },
  {
    "text": "the first two examples I gave you were example of things going bad that are",
    "start": "1636310",
    "end": "1642380"
  },
  {
    "text": "actually hopefully kind of rare but one of the issue we see the most in terms of",
    "start": "1642380",
    "end": "1648410"
  },
  {
    "text": "our things contain our performance issues and I'm sure a lot of you have had some of those so a very simple",
    "start": "1648410",
    "end": "1657560"
  },
  {
    "start": "1654000",
    "end": "1654000"
  },
  {
    "text": "symptom our our deployment is slow is getting very slow and we have no clue",
    "start": "1657560",
    "end": "1662660"
  },
  {
    "text": "why so when we start when we investigate this with that look at the nerd where",
    "start": "1662660",
    "end": "1669140"
  },
  {
    "text": "the pod has been scheduled and we noticed that after accepting the port locally the cubelet took like almost a",
    "start": "1669140",
    "end": "1675380"
  },
  {
    "text": "minute to start a container which is much higher than what you would expect",
    "start": "1675380",
    "end": "1680830"
  },
  {
    "text": "so we looked at how the node pools of the group of nodes where this pod was",
    "start": "1681760",
    "end": "1687230"
  },
  {
    "text": "supposed to be scheduled we look at the load as all the nodes and well as you can see on this graph the load was",
    "start": "1687230",
    "end": "1693020"
  },
  {
    "text": "pretty high so these are very small machines and the load is about a concern",
    "start": "1693020",
    "end": "1698120"
  },
  {
    "text": "about about 250 or something so of course with this kind of load the crew blatant continuity and everything was",
    "start": "1698120",
    "end": "1703790"
  },
  {
    "text": "having a hard time so the next step was to investigate was the load was high it",
    "start": "1703790",
    "end": "1710840"
  },
  {
    "text": "turned out is because the instance was writing a lot of i/o so as you can see on the graph at the bottom so the left",
    "start": "1710840",
    "end": "1717560"
  },
  {
    "text": "part is before we fix it on the right path after we had a lot of sustained IO on all this node especially one and as",
    "start": "1717560",
    "end": "1724910"
  },
  {
    "text": "you can see in the graph is the middle or the reason why everything was getting so slow and the load so high is because",
    "start": "1724910",
    "end": "1730190"
  },
  {
    "text": "we use EBS for root disk and it's a small disk about static gigabytes most",
    "start": "1730190",
    "end": "1736250"
  },
  {
    "text": "of the time and on this particular the light blue one as you can see on the",
    "start": "1736250",
    "end": "1741380"
  },
  {
    "text": "left there was no birth balance so this means we were really limited by BBS at the lowest level and we were not able to",
    "start": "1741380",
    "end": "1747350"
  },
  {
    "text": "actually go above the limit and usually this indicate that you're consuming more iOS in the assistant fashion that the",
    "start": "1747350",
    "end": "1754490"
  },
  {
    "text": "EBS can provide and as you can see when we fixed it things went a lot better",
    "start": "1754490",
    "end": "1760750"
  },
  {
    "text": "so why did we have so many so many iOS and this machine so we were looking at",
    "start": "1762040",
    "end": "1767960"
  },
  {
    "text": "everything and while the biggest workload running on this network called DNS so we look at what was happening on",
    "start": "1767960",
    "end": "1774950"
  },
  {
    "text": "the in the Cardenas metrics and we saw that the number of queries had more than double over the last few hours and the",
    "start": "1774950",
    "end": "1782240"
  },
  {
    "text": "thing is it's very difficult to diagnose DNS and to see what is happening so the way we do it is we actually log all the",
    "start": "1782240",
    "end": "1788870"
  },
  {
    "text": "queries and we do sampling at the log when we when you get the logs however we",
    "start": "1788870",
    "end": "1795080"
  },
  {
    "text": "do something very relating to chain which means all the logs are actually written to this before and of course",
    "start": "1795080",
    "end": "1800510"
  },
  {
    "text": "it's generating i/o so we were lucky because since we add logs we were able",
    "start": "1800510",
    "end": "1805580"
  },
  {
    "text": "to actually identify the team I mean the application that had started to do that so we went to the team and well told him",
    "start": "1805580",
    "end": "1813050"
  },
  {
    "text": "well you have a new application that you just deploy and it's doing 50,000 DNS queries per second that seems like quite",
    "start": "1813050",
    "end": "1818780"
  },
  {
    "text": "a lot for one pod and well we're lucky because we with we had had so many issue",
    "start": "1818780",
    "end": "1826790"
  },
  {
    "text": "with the unit at that point that we have an offer us infra which is you can have a core DNA local cache locally on your",
    "start": "1826790",
    "end": "1833420"
  },
  {
    "text": "notes and just a matter of adding an anotation and your pod and we have an admission control that's going to",
    "start": "1833420",
    "end": "1838490"
  },
  {
    "text": "rewrite the dns configuration and connect you to the local cache so we did that as you can see on the graph things",
    "start": "1838490",
    "end": "1844850"
  },
  {
    "text": "immediately went a lot better so you can see on this graph there that well you",
    "start": "1844850",
    "end": "1851210"
  },
  {
    "text": "see the blue line it's getting a lot better very fast however the load the amount of iOS is",
    "start": "1851210",
    "end": "1858830"
  },
  {
    "text": "still pretty high on this group of note that is not doing much and since I was investigating it I was I kept looking at",
    "start": "1858830",
    "end": "1866420"
  },
  {
    "text": "it you remember out it before so we had I mean we had like kind of a bad problem",
    "start": "1866420",
    "end": "1872870"
  },
  {
    "text": "with audit and we had we thought we had to fix it but what we had done is we had removed the demand set we had dropped",
    "start": "1872870",
    "end": "1880180"
  },
  {
    "text": "traffic at intake and log intake so we didn't get all this traffic so it seemed fine however odd it had been enabled in",
    "start": "1880180",
    "end": "1887900"
  },
  {
    "text": "the kernel so all the audio there was no audit daemon running but audit was shown a bamboo cannon which",
    "start": "1887900",
    "end": "1893780"
  },
  {
    "text": "the colonel was loading all the audit messages and generally was logging all the audit messages - so they were still",
    "start": "1893780",
    "end": "1899270"
  },
  {
    "text": "written to disk which was pretty bad so we disabled audit on all the machines",
    "start": "1899270",
    "end": "1905750"
  },
  {
    "text": "and as you can see on the graph it went a lot better almost almost almost immediately at that point I had spent a",
    "start": "1905750",
    "end": "1915440"
  },
  {
    "start": "1913000",
    "end": "1913000"
  },
  {
    "text": "few hours on these machines and I was still not very happy because if you can see this graph there's probably 10 to 15",
    "start": "1915440",
    "end": "1923240"
  },
  {
    "text": "nodes in the group and they're all behaving - same with like a low number of a UPS and one of them was very",
    "start": "1923240",
    "end": "1929030"
  },
  {
    "text": "different which was still confusing I wanted to understand why I zoomed on the",
    "start": "1929030",
    "end": "1936590"
  },
  {
    "text": "graph before and it's not actually sustained when you look at it like when you zoom you can see that it's not like",
    "start": "1936590",
    "end": "1944030"
  },
  {
    "text": "I owe all the time it's spike of I owe every minute I started to wonder if",
    "start": "1944030",
    "end": "1950060"
  },
  {
    "text": "there's something we do every minute on this and this group of nodes and it",
    "start": "1950060",
    "end": "1955220"
  },
  {
    "text": "turned out we had a cron job running every minute and so you would expect",
    "start": "1955220",
    "end": "1960350"
  },
  {
    "text": "when you have 10 nodes with the right Taine's that a cron job is going to run",
    "start": "1960350",
    "end": "1965840"
  },
  {
    "text": "on all the nodes in the group it turned out the scheduler is actually very predictable you can run the same algorithm every time it needs to",
    "start": "1965840",
    "end": "1972680"
  },
  {
    "text": "schedule the pot from the cron job and it's very likely that's gonna make the same decision because either there are",
    "start": "1972680",
    "end": "1978800"
  },
  {
    "text": "more requests available on a node or even the image is already present so it's gonna be faster and this now is",
    "start": "1978800",
    "end": "1984080"
  },
  {
    "text": "going to be preferred and as you can see on the back and the graph at the bottom it turns out that these jobs were always",
    "start": "1984080",
    "end": "1991070"
  },
  {
    "text": "scheduled on the same node or almost always and as you can see the number of containers created on each node over",
    "start": "1991070",
    "end": "1996560"
  },
  {
    "text": "time so yeah the issue was this cron job I",
    "start": "1996560",
    "end": "2003430"
  },
  {
    "text": "mean we we had seen that disabling it's fixed it's changing the script fixed it",
    "start": "2003430",
    "end": "2008770"
  },
  {
    "text": "to just sleep instead of doing what he was doing and this job was actually a very simple one it was just supposed to",
    "start": "2008770",
    "end": "2016080"
  },
  {
    "text": "synchronize the IP address of our console servers not in communities fester as endpoint for a community",
    "start": "2016080",
    "end": "2022660"
  },
  {
    "text": "service so any application in the cluster could reach the console server outside of given Ida's it's believed",
    "start": "2022660",
    "end": "2028180"
  },
  {
    "text": "these two commands so describe areas instances and get the IP of the console servers and update the crips ETL",
    "start": "2028180",
    "end": "2034690"
  },
  {
    "text": "endpoint I mean as you can see there it's pretty difficult to imagine that",
    "start": "2034690",
    "end": "2039880"
  },
  {
    "text": "this is generating a lot of iOS I was getting pretty mad I mean we're replacing this by Sleep we generate 0io",
    "start": "2039880",
    "end": "2048720"
  },
  {
    "text": "so it turns out when you run qts here a brand new path to PGI is doing a lot of",
    "start": "2049830",
    "end": "2055210"
  },
  {
    "text": "things it's caching all the API description locally and it's creating 165 and doing",
    "start": "2055210",
    "end": "2061510"
  },
  {
    "text": "45 HTTP queries to get all the all this information and of course it's explain the spike in iOS because every minute",
    "start": "2061510",
    "end": "2068530"
  },
  {
    "text": "chip CTR would run synchronize all the api's create all this file and yeah this",
    "start": "2068530",
    "end": "2074409"
  },
  {
    "text": "is how we got all these iOS well our",
    "start": "2074410",
    "end": "2080470"
  },
  {
    "text": "final way to shoot yourself in the foot graceful termination so we've got this kind of slick setup with one of our",
    "start": "2080470",
    "end": "2086050"
  },
  {
    "text": "applications it's it's a cue consumer and so we're auto-scaling it based on cue depths we've got a custom metric",
    "start": "2086050",
    "end": "2091060"
  },
  {
    "text": "reporting in and we're able to sort of dynamically scale up this application based on the number of payloads that are",
    "start": "2091060",
    "end": "2096129"
  },
  {
    "text": "in the queue and it's a really great experience for our users there's one caveat to this and that when we scaled",
    "start": "2096130",
    "end": "2103750"
  },
  {
    "text": "down we need to make sure that these jobs are completing the work that they're doing and for this application that can actually take hours up to 48",
    "start": "2103750",
    "end": "2110890"
  },
  {
    "text": "hours for this one so let's look at what happens when we scale down an application at first the pod enters",
    "start": "2110890",
    "end": "2116080"
  },
  {
    "start": "2111000",
    "end": "2111000"
  },
  {
    "text": "terminating state and since it's got the grace period set to 48 hours it's not going to be terminated right away but it",
    "start": "2116080",
    "end": "2122350"
  },
  {
    "text": "enters terminating state first thing that happens is that cubed I am refuses to refresh the credentials because it's",
    "start": "2122350",
    "end": "2128420"
  },
  {
    "text": "terminating state well we're able to solve that one next thing that happens",
    "start": "2128420",
    "end": "2133940"
  },
  {
    "text": "is that inevitably the cubelet restarts the reason for that is because we restart the cubelet every 24 hours to",
    "start": "2133940",
    "end": "2140029"
  },
  {
    "text": "rotate certificates and so inevitably if we're waiting 48 hours for this",
    "start": "2140029",
    "end": "2145430"
  },
  {
    "text": "it's the applications gonna get sig killed when the cubelets communication with container D is severed well we're",
    "start": "2145430",
    "end": "2151789"
  },
  {
    "text": "also able to get that one fixed in container D and so problem solved as a",
    "start": "2151789",
    "end": "2162559"
  },
  {
    "start": "2160000",
    "end": "2160000"
  },
  {
    "text": "few takeaways on this presentation be very careful with diamond set when you start to run large clusters because I",
    "start": "2162559",
    "end": "2169190"
  },
  {
    "text": "think we've broken all the things that diamond set depend upon we've broken gns",
    "start": "2169190",
    "end": "2174289"
  },
  {
    "text": "vault repository be very careful when we're managing things by the thousand",
    "start": "2174289",
    "end": "2180190"
  },
  {
    "text": "it's very easy to to break things DNS is hard it is not new one I know the",
    "start": "2180190",
    "end": "2186650"
  },
  {
    "text": "community is working very hard on making it easier but it's still kind of sort of many problems another important one is",
    "start": "2186650",
    "end": "2193880"
  },
  {
    "text": "we tend to forget that below below the qubit there's actually underlying infra and so having issue with iOS our auto",
    "start": "2193880",
    "end": "2201710"
  },
  {
    "text": "scaling group is something that cannot happen and the last one is one of the",
    "start": "2201710",
    "end": "2207440"
  },
  {
    "text": "trickiest to to manage and this is the reason why we're actually using pools per application team so there's no nosy",
    "start": "2207440",
    "end": "2214039"
  },
  {
    "text": "neighbor problem because containers are naturally content yet we're able to easily isolate memory and CPU but iOS",
    "start": "2214039",
    "end": "2221480"
  },
  {
    "text": "Network a number of processes and all kind of thing are not really contained and so you can very easily break another",
    "start": "2221480",
    "end": "2227990"
  },
  {
    "text": "application by running something that's not not great and that's all thanks a",
    "start": "2227990",
    "end": "2234950"
  },
  {
    "text": "lot for coming [Applause]",
    "start": "2234950",
    "end": "2240989"
  }
]