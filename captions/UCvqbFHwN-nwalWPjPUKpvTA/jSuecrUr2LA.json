[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "and first thanks for everyone for staying you know I'm almost the last session in the in the show so and I see",
    "start": "30",
    "end": "6060"
  },
  {
    "text": "some familiar faces so great I'm gonna speak to you about how to try neutralize",
    "start": "6060",
    "end": "12000"
  },
  {
    "text": "GPU as a service on on kubernetes I'm you're on I'm CTO for Iguazu and first",
    "start": "12000",
    "end": "19800"
  },
  {
    "text": "before we start you know some motivation is when people build today machine learning pipelines or machine learning",
    "start": "19800",
    "end": "25560"
  },
  {
    "text": "based application when I say application I mean they don't use machine learning just for reporting and and doing",
    "start": "25560",
    "end": "31650"
  },
  {
    "text": "something you know along that line they're trying to actually embed machine learning into their application we see",
    "start": "31650",
    "end": "36960"
  },
  {
    "text": "that building the machine learning models or the algorithm is really easy but doing everything else is complicated",
    "start": "36960",
    "end": "44700"
  },
  {
    "text": "so you see that some project that starts with few weeks of PUC and put six to 12 months of deployment and you know some",
    "start": "44700",
    "end": "51600"
  },
  {
    "text": "of the reasons that those things take time is that the first step of development is relatively easy to write",
    "start": "51600",
    "end": "57239"
  },
  {
    "start": "52000",
    "end": "154000"
  },
  {
    "text": "some code and then some people make it a container you know data scientists are using not too educated on that space you",
    "start": "57239",
    "end": "64948"
  },
  {
    "text": "need build and make and all that and then you're starting to hit scale so you need to think about how do i scale out",
    "start": "64949",
    "end": "70470"
  },
  {
    "text": "and scale out requires distributed computing and distributed computing comes in many forms",
    "start": "70470",
    "end": "75479"
  },
  {
    "text": "you know load balancing sharding partitioning hyper parameter etc and",
    "start": "75479",
    "end": "80880"
  },
  {
    "text": "then comes an aspect of tuning tuning means how do we turn things to more",
    "start": "80880",
    "end": "86009"
  },
  {
    "text": "asynchronous more parallel I'll do embed GPU support into those same same code",
    "start": "86009",
    "end": "92729"
  },
  {
    "text": "that we cache elements that we don't go over and over to a database how do we",
    "start": "92729",
    "end": "97939"
  },
  {
    "text": "tune our queries to be more efficient and it's around and then we need to think about instrumentation stimulation",
    "start": "97939",
    "end": "105180"
  },
  {
    "text": "means like starting to do logging and monitoring and maybe versioning of our code and get and and all that stuff and",
    "start": "105180",
    "end": "112200"
  },
  {
    "text": "and then eventually once we got everything working we wanted to make it repetitive so we need to add automation",
    "start": "112200",
    "end": "119159"
  },
  {
    "text": "so see ICD workflows pipelines rolling upgrades because things are running in",
    "start": "119159",
    "end": "124560"
  },
  {
    "text": "production maybe testing so see there's so much work to do where usually the",
    "start": "124560",
    "end": "129840"
  },
  {
    "text": "first part is few weeks the second part is is a lot of months of many many people one of things that",
    "start": "129840",
    "end": "136450"
  },
  {
    "text": "we're trying to do with we're promoting a bunch of open-source prodding including this one and nuclear and",
    "start": "136450",
    "end": "143019"
  },
  {
    "text": "others is essentially automate all this work of all those pipelines and part of",
    "start": "143019",
    "end": "148629"
  },
  {
    "text": "it is automated integration of GPUs and dynamic allocation of GPUs so everyone",
    "start": "148629",
    "end": "156370"
  },
  {
    "start": "154000",
    "end": "245000"
  },
  {
    "text": "here that thinks of GPUs probably thinks of deep learning you know tensorflow and all that so",
    "start": "156370",
    "end": "161980"
  },
  {
    "text": "there is some news GPUs are used to do many different things not just not just",
    "start": "161980",
    "end": "167349"
  },
  {
    "text": "deep learning anyone familiar with the Nvidia rapid for example okay good so",
    "start": "167349",
    "end": "174780"
  },
  {
    "text": "this is part of a joint blog we did together with the I did together with Nvidia they just demonstrate analytics",
    "start": "174780",
    "end": "182590"
  },
  {
    "text": "use case taking some JSON large very large JSON data a very typical use case",
    "start": "182590",
    "end": "188470"
  },
  {
    "text": "your web blogs generate a lot of JSON related data you need to build aggregates on top of that data and then",
    "start": "188470",
    "end": "195489"
  },
  {
    "text": "you know dump the aggregates or the pre-processed data into serve some files",
    "start": "195489",
    "end": "201760"
  },
  {
    "text": "park' that's something that you would usually do in something like spark but that will require a lot of work you know",
    "start": "201760",
    "end": "208180"
  },
  {
    "text": "just writing some java class takes more work than doing everything here combined so so one of the things that you see in",
    "start": "208180",
    "end": "215889"
  },
  {
    "text": "this very simple code is that running this with qdf which is the data frame",
    "start": "215889",
    "end": "222549"
  },
  {
    "text": "and this data frame with GPU acceleration takes about 1.4 seconds and",
    "start": "222549",
    "end": "228160"
  },
  {
    "text": "running the same thing on traditional pandas is 43 seconds so 30 30 times",
    "start": "228160",
    "end": "235870"
  },
  {
    "text": "faster exactly the same code just changing the import on your Python so obviously there is very big advantage of",
    "start": "235870",
    "end": "242949"
  },
  {
    "text": "using GPUs there is one challenge though GPUs are",
    "start": "242949",
    "end": "248440"
  },
  {
    "start": "245000",
    "end": "287000"
  },
  {
    "text": "usually more expensive although in video t4 and some other technologies are now lowering the barrier and we don't want",
    "start": "248440",
    "end": "257019"
  },
  {
    "text": "to pay for their idle idle time so until now every time you here GPU is a service",
    "start": "257019",
    "end": "262300"
  },
  {
    "text": "that the common thread I'm saying is usually GPU partitioning so fix the location of GPU to different",
    "start": "262300",
    "end": "269560"
  },
  {
    "text": "tasks so just saying okay there is a container I'm gonna attach a GPU to that container let's assume it's a Jupiter",
    "start": "269560",
    "end": "275110"
  },
  {
    "text": "container I'm not doing any work I still pay for that GPU so what we want to do",
    "start": "275110",
    "end": "280419"
  },
  {
    "text": "is essentially attach the GPU only when we really need them for processing workload okay so how would we design",
    "start": "280419",
    "end": "290139"
  },
  {
    "start": "287000",
    "end": "380000"
  },
  {
    "text": "such an architecture of a complete data science platform on on kubernetes this",
    "start": "290139",
    "end": "295690"
  },
  {
    "text": "is also something we do for a living but because it's an open source ecosystem everyone can build the same we need the",
    "start": "295690",
    "end": "302590"
  },
  {
    "text": "first layer of data there are comprised of online and offline features usually so things that are more like objects",
    "start": "302590",
    "end": "310000"
  },
  {
    "text": "store data warehouse you know Hadoop file system there are things that are more real time like time",
    "start": "310000",
    "end": "316750"
  },
  {
    "text": "series key value caches etcetera this rapid thing also know how to accelerate",
    "start": "316750",
    "end": "322390"
  },
  {
    "text": "data bases we use it in our product but also there's blazing sequel there are other sequel oriented projects that know",
    "start": "322390",
    "end": "330070"
  },
  {
    "text": "how to leverage the GPU acceleration and run queries really really fast the next",
    "start": "330070",
    "end": "335620"
  },
  {
    "text": "level we need resources resources could be you know GPUs or CPUs even CPUs have",
    "start": "335620",
    "end": "341020"
  },
  {
    "text": "quite a bit of optimization that you could use you know there's for example project from Intel that know how to use",
    "start": "341020",
    "end": "347440"
  },
  {
    "text": "the AVX instructions set to run tensorflow faster okay so we need to",
    "start": "347440",
    "end": "352479"
  },
  {
    "text": "think about those things we have kubernetes and then we have a set of services that deal with machine learning",
    "start": "352479",
    "end": "358360"
  },
  {
    "text": "data science etc and and then we have a layer where people actually use all",
    "start": "358360",
    "end": "363550"
  },
  {
    "text": "those things usually if you look into a cube flow environment you have managed notebooks you have pipelines you we",
    "start": "363550",
    "end": "370539"
  },
  {
    "text": "promote the notion of service and you need other things like job management or defense artifact management and I'll try to",
    "start": "370539",
    "end": "377140"
  },
  {
    "text": "demonstrate all of those things in a minute so how would we build a pipeline the",
    "start": "377140",
    "end": "383169"
  },
  {
    "text": "pipeline usually comprised of different steps the first step is ingestion ingestion means we need to go and bring",
    "start": "383169",
    "end": "391210"
  },
  {
    "text": "data in in order to process it usually do you normalize it in a machine learning use case then we run a training",
    "start": "391210",
    "end": "398140"
  },
  {
    "text": "on that once we have training we need to generate more and we do model serving many people",
    "start": "398140",
    "end": "403920"
  },
  {
    "text": "forget that model serving is not the last mile because model serving usually gets some servant numpy array and the",
    "start": "403920",
    "end": "411060"
  },
  {
    "text": "real data is not Nampa erase real data maybe pictures maybe customer IDs so we",
    "start": "411060",
    "end": "416940"
  },
  {
    "text": "need another component which is this serve API gateway or serve an application to translate real data into",
    "start": "416940",
    "end": "424290"
  },
  {
    "text": "models and we show that in a minute in a demo how we build all those things now all of those steps can use GPUs ok today",
    "start": "424290",
    "end": "433440"
  },
  {
    "text": "with things like Rapids and other libraries coming you can essentially use GPU acceleration across all of those",
    "start": "433440",
    "end": "440340"
  },
  {
    "text": "different steps now one of the things that we want to to do in order to make it simple this is by the way a real and",
    "start": "440340",
    "end": "446550"
  },
  {
    "text": "to an application for real time project recommendation that we've implemented in several places it's about you know",
    "start": "446550",
    "end": "454170"
  },
  {
    "text": "collecting data which is offline like ETL you know records about users re",
    "start": "454170",
    "end": "459300"
  },
  {
    "text": "getting data like real-time transaction and cart changes real-time location information of users scraping data",
    "start": "459300",
    "end": "466460"
  },
  {
    "text": "dynamically from things like weather services then again de normalizing the",
    "start": "466460",
    "end": "471810"
  },
  {
    "text": "data and what you could see is that in many use cases you actually combine machine learning and deep learning it's",
    "start": "471810",
    "end": "477660"
  },
  {
    "text": "not just deep learning or machine learning things like embedding and product product relations are very well",
    "start": "477660",
    "end": "484020"
  },
  {
    "text": "implemented on on deep learning and things that are more regression oriented may actually run better on on machine",
    "start": "484020",
    "end": "491370"
  },
  {
    "text": "learning and even those things are served have dependencies you have to finish the first step of training to the",
    "start": "491370",
    "end": "497160"
  },
  {
    "text": "second step of training so this is essentially a pipeline and what we're trying to promote is a notion of",
    "start": "497160",
    "end": "502440"
  },
  {
    "text": "functions instead of thinking of everything as containers and allocating",
    "start": "502440",
    "end": "507990"
  },
  {
    "text": "them or notebooks or or VMs that you know have resources attached think of",
    "start": "507990",
    "end": "513510"
  },
  {
    "text": "everything as elastic function as things that you know can go growing and even shrink to zero if they shrink to zero",
    "start": "513510",
    "end": "520380"
  },
  {
    "text": "they don't consume any resources okay which is a great great thing and those things are just interleaving of datasets",
    "start": "520380",
    "end": "526890"
  },
  {
    "text": "and and those elastic functions one of the important thing by the way is when",
    "start": "526890",
    "end": "532320"
  },
  {
    "text": "you're doing serving is how do you get data insert more real-time concurrency but that's on",
    "start": "532320",
    "end": "537420"
  },
  {
    "text": "a separate topic so what we've what",
    "start": "537420",
    "end": "543300"
  },
  {
    "start": "540000",
    "end": "629000"
  },
  {
    "text": "you've designed is a notion of functions again very elastic scale can scale all the way",
    "start": "543300",
    "end": "548340"
  },
  {
    "text": "down to zero and those functions have very abstract code within them they could have Python code like this",
    "start": "548340",
    "end": "555390"
  },
  {
    "text": "function that's doing the same thing of real time analysis of JSON and aggregation and dumping of this JSON if",
    "start": "555390",
    "end": "562440"
  },
  {
    "text": "you're gonna run exactly the same function in inside a container because",
    "start": "562440",
    "end": "567870"
  },
  {
    "text": "of all the concurrency issues of Python and lack of parallelism this were only gonna get to you about 20 megabytes per",
    "start": "567870",
    "end": "574920"
  },
  {
    "text": "second if you're going to run exactly the same function without modifying anything within a real time runtime you",
    "start": "574920",
    "end": "580920"
  },
  {
    "text": "can also attach the GPU like nucleo it will get you about 600 megabytes per second and t4 if you look at that was",
    "start": "580920",
    "end": "588600"
  },
  {
    "text": "tested on T 4 T 4 is not that expensive the GPUs relatively low cost so maybe",
    "start": "588600",
    "end": "594870"
  },
  {
    "text": "twice as more expensive 30 times more performance on this architecture but the",
    "start": "594870",
    "end": "600630"
  },
  {
    "text": "other key advantage of service is not just the you know usually it's not associated with performance unless",
    "start": "600630",
    "end": "606060"
  },
  {
    "text": "you're using nucleo but the key advantage of service is the Ellis's elasticity and 0 DevOps so I can write",
    "start": "606060",
    "end": "612180"
  },
  {
    "text": "this code click deploy and it just works in Auto scales it has logging monitoring",
    "start": "612180",
    "end": "617820"
  },
  {
    "text": "instrumentation full integration with queue flow pipelines and all that out of the box so again simplicity ease of use time to",
    "start": "617820",
    "end": "626070"
  },
  {
    "text": "market along with extreme performance another use case is the serving part so",
    "start": "626070",
    "end": "632490"
  },
  {
    "start": "629000",
    "end": "705000"
  },
  {
    "text": "we want to use functions for also serving models again one of the tricks that engines like nuclear now know how",
    "start": "632490",
    "end": "640530"
  },
  {
    "text": "to do is maximize utilization of GPUs how do they do that small trick I can",
    "start": "640530",
    "end": "646650"
  },
  {
    "text": "tell you the secret is essentially creating serve an elevator against the GPU have multiple workers it always",
    "start": "646650",
    "end": "651900"
  },
  {
    "text": "failed the pipe of work towards the GPU so then the GPU is always 100% utilized and what you're getting in this example",
    "start": "651900",
    "end": "658590"
  },
  {
    "text": "is four times more transactions on exactly the same code that you've written ok or use half of the machines",
    "start": "658590",
    "end": "666390"
  },
  {
    "text": "you're still twice as fast using something that's more optimized to to work with GPUs it can be four times",
    "start": "666390",
    "end": "673320"
  },
  {
    "text": "more cost-effective now beyond just having the performance again the simplicity is the important",
    "start": "673320",
    "end": "679110"
  },
  {
    "text": "part is how do I elastically scale and how do I work with in my notebook and",
    "start": "679110",
    "end": "684360"
  },
  {
    "text": "convert my notebook to this real-time function that runs four times faster and dynamically allocates GPUs for me so",
    "start": "684360",
    "end": "692130"
  },
  {
    "text": "this is another part but the biggest challenge is not the ingestion and and",
    "start": "692130",
    "end": "698520"
  },
  {
    "text": "the training or the serving the biggest challenges are the data preparation and job and training and for that we've",
    "start": "698520",
    "end": "706860"
  },
  {
    "start": "705000",
    "end": "848000"
  },
  {
    "text": "developed a new concept which is ml functions that are more batch oriented",
    "start": "706860",
    "end": "713280"
  },
  {
    "text": "they're the leverage distributed computing and those things that when you launch a job they will dynamically",
    "start": "713280",
    "end": "720030"
  },
  {
    "text": "allocate resources by the way everything I'm gonna I'm speaking about these open source project and I'll give you the",
    "start": "720030",
    "end": "725150"
  },
  {
    "text": "references in a minute so everything that you you launch will get dynamically",
    "start": "725150",
    "end": "730560"
  },
  {
    "text": "attached to GPUs that means that if I'm running you know one one approach again",
    "start": "730560",
    "end": "736020"
  },
  {
    "text": "to GPU sharing is you know what I'll have a GPU there are some companies that know how to partition the GPU to two",
    "start": "736020",
    "end": "742700"
  },
  {
    "text": "virtual GPUs you know you get half you get half it reminds me of the solomon you know in",
    "start": "742700",
    "end": "750570"
  },
  {
    "text": "the bible and the point is instead of giving you a half and you have maybe i can give each one of you a burst of GPUs",
    "start": "750570",
    "end": "757740"
  },
  {
    "text": "for the sake of running you'll run ten times faster and that will be way more efficient also when i give you a half a",
    "start": "757740",
    "end": "764250"
  },
  {
    "text": "new half what happens with all the residual data you know you're running a job you use some memory node in the GPU",
    "start": "764250",
    "end": "771030"
  },
  {
    "text": "there's no real way today to isolate memory in GPUs and resources not like CPUs that you can say you're gonna get",
    "start": "771030",
    "end": "778350"
  },
  {
    "text": "this amount of memory and you're gonna get this amount so you rather use it more like an HPC paradigm is where you",
    "start": "778350",
    "end": "784980"
  },
  {
    "text": "just burst everyone gets everything finishes as quick as possible and make room for the next guy so essentially",
    "start": "784980",
    "end": "792330"
  },
  {
    "text": "what we've built is thinking of service in mind what is service service is elastic scaling is essentially paper use",
    "start": "792330",
    "end": "799530"
  },
  {
    "text": "you know you only use resources when you when you need them and no DevOps so in",
    "start": "799530",
    "end": "805470"
  },
  {
    "text": "order to build something like that we now need to adopt it to different types of distributed engines like spark task",
    "start": "805470",
    "end": "811889"
  },
  {
    "text": "with Rapids it Sarah so what we've done is we've taken the approach of service",
    "start": "811889",
    "end": "817350"
  },
  {
    "text": "and decoupled the engine that does the distributed data processing from the",
    "start": "817350",
    "end": "822870"
  },
  {
    "text": "automation of container build and controllers and event loop and run an",
    "start": "822870",
    "end": "828420"
  },
  {
    "text": "artifact management and all that and I'll show it in a minute and also what you want to do is reusability so we want",
    "start": "828420",
    "end": "835230"
  },
  {
    "text": "to be able to create templates and reuse them in different places move them into pipelines debug them quite different",
    "start": "835230",
    "end": "841740"
  },
  {
    "text": "variations over time so this is one of the project that we've initiated and the",
    "start": "841740",
    "end": "848970"
  },
  {
    "text": "way that it works under the hood in order to get this this effect is think about building a",
    "start": "848970",
    "end": "856199"
  },
  {
    "text": "sandbox that can run on many different environment whether it's spark dusk",
    "start": "856199",
    "end": "861269"
  },
  {
    "text": "you know MPI johor board any type of operator even managed services and this",
    "start": "861269",
    "end": "868500"
  },
  {
    "text": "thing if you when you run a job you sense you need to pass three things parameters you need some credentials",
    "start": "868500",
    "end": "875399"
  },
  {
    "text": "because you may your job may access some database and you need data and when your",
    "start": "875399",
    "end": "881339"
  },
  {
    "text": "job finishes it generates some artifacts artifacts are results operational information like logs telemetry data etc",
    "start": "881339",
    "end": "888930"
  },
  {
    "text": "and in data you know models data sets etc so you create a sandbox where you",
    "start": "888930",
    "end": "895889"
  },
  {
    "text": "run your code within a runtime the runtime is this thing that in charge of how you create parallelism and",
    "start": "895889",
    "end": "901910"
  },
  {
    "text": "parallelism in data science is very different for every task in some cases you'll use hyper parameter if you're",
    "start": "901910",
    "end": "908490"
  },
  {
    "text": "doing spark you'll use rdd's if you were doing task it has distributed data frame construct",
    "start": "908490",
    "end": "914339"
  },
  {
    "text": "it's a nuclear there's multi-threading I like with reading so every framework",
    "start": "914339",
    "end": "919920"
  },
  {
    "text": "will do its own so you essentially tell the framework you go your do your parallelism and it will do it the best",
    "start": "919920",
    "end": "925709"
  },
  {
    "text": "way it can so there are about eight eight different engines currently supported under this framework called ml",
    "start": "925709",
    "end": "931709"
  },
  {
    "text": "run and they know how to allocate resources dynamically so I can pass rameters inputs secrets etcetera get it",
    "start": "931709",
    "end": "939220"
  },
  {
    "text": "to run extremely fast and then collect the results and there's another interesting side effect",
    "start": "939220",
    "end": "944590"
  },
  {
    "text": "it's glueless integration with cue flow pipeline and I'll show you how it works",
    "start": "944590",
    "end": "950880"
  },
  {
    "start": "950000",
    "end": "1001000"
  },
  {
    "text": "so well we know that you know let's let's look into some demos so I'll show",
    "start": "950880",
    "end": "957850"
  },
  {
    "text": "you one or two demonstrations the first one is sort of deep learning which is",
    "start": "957850",
    "end": "963780"
  },
  {
    "text": "essentially very optimal for for GPUs and what we want to see every time we",
    "start": "963780",
    "end": "970390"
  },
  {
    "text": "have a pipeline it's essentially comprises to about four steps you know one is the data collection then we need",
    "start": "970390",
    "end": "975730"
  },
  {
    "text": "to do some massage to the data in this case labeling because it's image processing the next thing we want to do",
    "start": "975730",
    "end": "982120"
  },
  {
    "text": "we want to run our training on a distributed set of processors and then",
    "start": "982120",
    "end": "987610"
  },
  {
    "text": "we want to create deployments and in test against our deployment okay but",
    "start": "987610",
    "end": "993400"
  },
  {
    "text": "everything I'm showing is in this repo call them and run slash demos",
    "start": "993400",
    "end": "999630"
  },
  {
    "start": "1001000",
    "end": "1107000"
  },
  {
    "text": "any questions or okay we'll leave them to the end so let's look at this use",
    "start": "1006300",
    "end": "1012959"
  },
  {
    "text": "case we said we have four steps first step is data collection then labeling and then training so what I want to",
    "start": "1012959",
    "end": "1020730"
  },
  {
    "text": "define is functions it's clear I want to",
    "start": "1020730",
    "end": "1031050"
  },
  {
    "text": "define a function so we essentially when you're doing for more training and badge you don't want to do even if in you want",
    "start": "1031050",
    "end": "1036808"
  },
  {
    "text": "to pass parameters to functions so think that you can run a function this is a function that opens an archive like a",
    "start": "1036809",
    "end": "1042870"
  },
  {
    "text": "zip you know in in s3 in this case it has context isn't as an option and a",
    "start": "1042870",
    "end": "1048960"
  },
  {
    "text": "bunch of parameters like where do you want to dump the data to where is it coming from and this function has a",
    "start": "1048960",
    "end": "1054420"
  },
  {
    "text": "context object which is pretty cool so with the context object if any of any of you know ml flow heard of it so one okay",
    "start": "1054420",
    "end": "1063390"
  },
  {
    "text": "two three okay we'll get 10 at the end but there's a notion of logging you want",
    "start": "1063390",
    "end": "1069630"
  },
  {
    "text": "when you run any experiment you want to be able to log the outputs of the models so then you conversion them and you can",
    "start": "1069630",
    "end": "1075600"
  },
  {
    "text": "repeat automatically those experiments so the context allows me to do things",
    "start": "1075600",
    "end": "1080760"
  },
  {
    "text": "like logging labeling tagging time series for monitoring telemetry logging",
    "start": "1080760",
    "end": "1086910"
  },
  {
    "text": "artifact it's ever so I can log my results of the first step then I have a",
    "start": "1086910",
    "end": "1092309"
  },
  {
    "text": "labor function you can get some context you know where where's all the where the all the pictures are where do I want to",
    "start": "1092309",
    "end": "1099240"
  },
  {
    "text": "dump the results of the labeling which is essentially two things one is a category map and the other one is a is a",
    "start": "1099240",
    "end": "1104490"
  },
  {
    "text": "data frame with labels for every image again those things are logging the",
    "start": "1104490",
    "end": "1109620"
  },
  {
    "start": "1107000",
    "end": "1185000"
  },
  {
    "text": "artifacts and then I want to execute those things so the first thing you you",
    "start": "1109620",
    "end": "1115140"
  },
  {
    "text": "want to do is debug the functions you don't want to run it on a pipeline in the cluster because if you have a small",
    "start": "1115140",
    "end": "1120690"
  },
  {
    "text": "bug you forgot you know point somewhere whatever you don't want to go to the",
    "start": "1120690",
    "end": "1126270"
  },
  {
    "text": "cluster so you can just go and create what we call a function if you don't specify what we call a runtime it",
    "start": "1126270",
    "end": "1132390"
  },
  {
    "text": "essentially means here in the notebook in pycharm where you're running so you can essentially define a task",
    "start": "1132390",
    "end": "1139650"
  },
  {
    "text": "the task is you know those inputs and outputs run it locally and it will just run in your notebook and record",
    "start": "1139650",
    "end": "1147450"
  },
  {
    "text": "everything that we just said artifacts etcetera the next thing I want to do I I want to take the results of that import",
    "start": "1147450",
    "end": "1155460"
  },
  {
    "text": "function and and do labeling so again I can just go and create an another function and from the local notebook and",
    "start": "1155460",
    "end": "1162660"
  },
  {
    "text": "essentially execute and I'm done and the third thing I want to do is training the",
    "start": "1162660",
    "end": "1168450"
  },
  {
    "text": "problem with training it really requires a lot of resources and that's something I don't wanna run on my notebook actually one around on my kubernetes",
    "start": "1168450",
    "end": "1175170"
  },
  {
    "text": "cluster so I don't need to build the ammos I don't need to do the aquifers all those crazy things I just want to",
    "start": "1175170",
    "end": "1181920"
  },
  {
    "text": "run my function so I can take my my function which is a just doing training",
    "start": "1181920",
    "end": "1187470"
  },
  {
    "start": "1185000",
    "end": "1290000"
  },
  {
    "text": "with tensorflow and caris I want to pass some data some parameter story into this",
    "start": "1187470",
    "end": "1193950"
  },
  {
    "text": "function you know the image sizes the source directory is you know number of air parks etc and and I want to also",
    "start": "1193950",
    "end": "1200820"
  },
  {
    "text": "provide some datasets into that function and I want to create a function this",
    "start": "1200820",
    "end": "1206220"
  },
  {
    "text": "time this function I'm gonna tell it that it's gonna running on an MPI job operator essentially distributed tends",
    "start": "1206220",
    "end": "1212250"
  },
  {
    "text": "to flow so instead of running locally I'm just telling this function look like essentially running on kubernetes on an",
    "start": "1212250",
    "end": "1218610"
  },
  {
    "text": "MPI job operator and maybe I want to attach some other things like you know give it a volume mount and give it some",
    "start": "1218610",
    "end": "1225450"
  },
  {
    "text": "a number of replicas and maybe giving it GPU and now I want to run it so now when",
    "start": "1225450",
    "end": "1232380"
  },
  {
    "text": "I'm doing run and it looks like it's running on my notebook it's actually not running on my notebook what this engine",
    "start": "1232380",
    "end": "1238020"
  },
  {
    "text": "does is essentially packing all my code my dependencies if it needs to build it will build generate the resources on the",
    "start": "1238020",
    "end": "1244500"
  },
  {
    "text": "cluster and running it having a control loop to make sure this thing is really running and not crashing and gathering",
    "start": "1244500",
    "end": "1251970"
  },
  {
    "text": "all that console learn about their things and all the console is essentially being grabbed directly into my notebook so I get the cozy feeling",
    "start": "1251970",
    "end": "1259320"
  },
  {
    "text": "that it's running on my notebook essentially it's running on a very powerful cluster we GPUs and multiple",
    "start": "1259320",
    "end": "1265080"
  },
  {
    "text": "nodes ok now the point is that once this job is starting is essentially only then",
    "start": "1265080",
    "end": "1271860"
  },
  {
    "text": "it's allocating GPUs so that means if you have a bunch of data scientists and you have a pool of ten GPUs each one can just grab a bunch",
    "start": "1271860",
    "end": "1279239"
  },
  {
    "text": "of GPUs run its job after five five minutes it will finish and it goes do another thing instead of having",
    "start": "1279239",
    "end": "1285059"
  },
  {
    "text": "partitioning your GPUs to multiple people and paying more money for more more GPUs so all this thing can run and",
    "start": "1285059",
    "end": "1293070"
  },
  {
    "start": "1290000",
    "end": "1380000"
  },
  {
    "text": "eventually you may have some results how do that run and the last thing we want",
    "start": "1293070",
    "end": "1298649"
  },
  {
    "text": "to do is deploy the results so again here we need another function but this time it's not a bad function it's a real",
    "start": "1298649",
    "end": "1304889"
  },
  {
    "text": "time function it's what we call nucleus serving function which is anyone knows what's KF serving good so one challenge",
    "start": "1304889",
    "end": "1315629"
  },
  {
    "text": "with KF serving is that you still need to build make Hamill's docker all that stuff no Cleo is a real-time engine",
    "start": "1315629",
    "end": "1321899"
  },
  {
    "text": "service engine so we now let nuclear essentially run off okay of serving",
    "start": "1321899",
    "end": "1327210"
  },
  {
    "text": "models and it could even be extended like this example KF serving models only except numpy arrays as inputs because",
    "start": "1327210",
    "end": "1334080"
  },
  {
    "text": "nuclear is very flexible can even accept JPEG images and I show it in a minute so",
    "start": "1334080",
    "end": "1339499"
  },
  {
    "text": "essentially what I want to do is just create a function I can even create a function from a notebook or from a class",
    "start": "1339499",
    "end": "1346619"
  },
  {
    "text": "and the only thing I need to say is deploy that the Senshi automatically creates my model into production part of",
    "start": "1346619",
    "end": "1353970"
  },
  {
    "text": "the functional specification I can even specify like number of workers phrase number of GPUs etc and I'll show that in",
    "start": "1353970",
    "end": "1360929"
  },
  {
    "text": "a UI name in a minute so Sochi I can dynamically create those functions those function would elastically scale to meet",
    "start": "1360929",
    "end": "1367169"
  },
  {
    "text": "the load if I need more transaction per second essentially it will use more GPUs and more CPUs if I don't use any this",
    "start": "1367169",
    "end": "1374460"
  },
  {
    "text": "function as schedule essentially scales to zero doesn't consume any GPUs and any resources and once I have this function",
    "start": "1374460",
    "end": "1381599"
  },
  {
    "text": "up and running I can just throw some pictures at it some cats okay some some",
    "start": "1381599",
    "end": "1387929"
  },
  {
    "text": "dogs and it will respond with the with the right classification hopefully yeah",
    "start": "1387929",
    "end": "1394349"
  },
  {
    "text": "so but what another interesting thing that you can see that the content type here is jpg and the contact type here is",
    "start": "1394349",
    "end": "1402599"
  },
  {
    "text": "text so in this case I generated the URL of the picture in the second case I actually sent",
    "start": "1402599",
    "end": "1408720"
  },
  {
    "text": "binary image if you are using something like Selden or KF serving that means that you need to have another function",
    "start": "1408720",
    "end": "1414300"
  },
  {
    "text": "that translate your image into you know binary list into serve byte array or",
    "start": "1414300",
    "end": "1421200"
  },
  {
    "text": "sort of numpy array and only then do serving to X amount of work and another",
    "start": "1421200",
    "end": "1426510"
  },
  {
    "text": "interesting thing or nuclear-nuclear doesn't only support HTTP triggers support Kafka Kinesis about 14 different",
    "start": "1426510",
    "end": "1432690"
  },
  {
    "text": "triggers including all the complexity and scaling associated with them so if you use Kafka what you need is auto",
    "start": "1432690",
    "end": "1438570"
  },
  {
    "text": "scaling or in check pointing and restarting if if you have more load you want to essentially serve those shards",
    "start": "1438570",
    "end": "1445230"
  },
  {
    "text": "of more processes so nucleo the matica knows how to scale itself if there's more load and when you're doing extreme",
    "start": "1445230",
    "end": "1451560"
  },
  {
    "text": "you have to deal with orders and load those things it knows how to checkpoint split rebalance and we go back to the",
    "start": "1451560",
    "end": "1458640"
  },
  {
    "text": "same pointers so again all of that is masked from you you don't need to think about any of that now the point is after",
    "start": "1458640",
    "end": "1465540"
  },
  {
    "text": "I've done all of that the next thing I really want is to automate because you know we don't want to build those thing",
    "start": "1465540",
    "end": "1471450"
  },
  {
    "text": "over and over so what I can just do is something very simple I can create a",
    "start": "1471450",
    "end": "1477300"
  },
  {
    "start": "1473000",
    "end": "1547000"
  },
  {
    "text": "cupola pipeline but just daisy chaining functions any of you played with cube",
    "start": "1477300",
    "end": "1482790"
  },
  {
    "text": "flow someone succeeded to generate an artifact by himself only one now I know",
    "start": "1482790",
    "end": "1490800"
  },
  {
    "text": "it's pretty big pretty big problem but I'll show you how you don't even need to do that with this framework so what we",
    "start": "1490800",
    "end": "1496440"
  },
  {
    "text": "need to do what we want to do is not like the cube flow thing of you know container of CMOS all that stuff what we",
    "start": "1496440",
    "end": "1502140"
  },
  {
    "text": "want to say is we have functions we want to daisy-chain those function so essentially I have a function I want to",
    "start": "1502140",
    "end": "1507930"
  },
  {
    "text": "view that function as a coupe step I want this function to generally have inputs outputs and parameters and maybe",
    "start": "1507930",
    "end": "1514740"
  },
  {
    "text": "I want a daisy chain so my training function is and she needs to take the labeling function output called category",
    "start": "1514740",
    "end": "1520800"
  },
  {
    "text": "map etc and my deployment function actually wants to take the training model result and deploy that so the only",
    "start": "1520800",
    "end": "1528480"
  },
  {
    "text": "thing I need to do is I can create this workflow but I can even put this workflow in a function that listens on",
    "start": "1528480",
    "end": "1535080"
  },
  {
    "text": "an HTTP event if you want to replay grow your Cupra recreate your cue pro pipeline",
    "start": "1535080",
    "end": "1540269"
  },
  {
    "text": "every time there's a github update you can even do that by turning this workflow into a function itself okay and",
    "start": "1540269",
    "end": "1548489"
  },
  {
    "start": "1547000",
    "end": "1827000"
  },
  {
    "text": "once I once I have it I just launch Kalu client and it will will actually run and",
    "start": "1548489",
    "end": "1553529"
  },
  {
    "text": "there's another interesting thing here I can even watch the result in in real time so without I generate a cue flow",
    "start": "1553529",
    "end": "1560909"
  },
  {
    "text": "pipeline and if you want to know what is the result I will just go to cue flow",
    "start": "1560909",
    "end": "1567080"
  },
  {
    "text": "and I could see all the results of my trainer with the artifact you know one",
    "start": "1567080",
    "end": "1574109"
  },
  {
    "text": "of the reason I highlight this anyone that really tried to generate artifact in cube load the way it works you have",
    "start": "1574109",
    "end": "1579779"
  },
  {
    "text": "to generate JSON files in specific folders with a lot of boilerplate in here I didn't do anything to generate",
    "start": "1579779",
    "end": "1586049"
  },
  {
    "text": "that the minute I said log artifact it automatically knows how to program to flow in order to make sure this thing is",
    "start": "1586049",
    "end": "1592229"
  },
  {
    "text": "is actually being visualized and all the inputs and outputs of my job are being visualized all my logs you know here's",
    "start": "1592229",
    "end": "1599669"
  },
  {
    "text": "the training this is the deployment step you could see the deploying step automatically turned my function into",
    "start": "1599669",
    "end": "1605759"
  },
  {
    "text": "something that actually runs on a specific end point that you see here so again everything is automated very",
    "start": "1605759",
    "end": "1612509"
  },
  {
    "text": "little amount of work and also everyone has a UI is now being reworked with some",
    "start": "1612509",
    "end": "1619649"
  },
  {
    "text": "UX people but essentially everything that runs everything I did in the notebook has a job as a pipeline not",
    "start": "1619649",
    "end": "1626190"
  },
  {
    "text": "just limited to pipeline could be tracked in real time so essentially when I'm doing my my training I can see where",
    "start": "1626190",
    "end": "1633419"
  },
  {
    "text": "is it running all the labels even labels that you can add within your job I can",
    "start": "1633419",
    "end": "1638759"
  },
  {
    "text": "see all the inputs to the job I can see all the things that were generated by the job and even download those the same",
    "start": "1638759",
    "end": "1645479"
  },
  {
    "text": "thing and I can see the logs of my of my job ok and those will get updated",
    "start": "1645479",
    "end": "1652879"
  },
  {
    "text": "frequently I can even see all the artifacts sort of see everything from a",
    "start": "1652879",
    "end": "1658440"
  },
  {
    "text": "versioning of data and artifact view very similar to ml flow just more",
    "start": "1658440",
    "end": "1663989"
  },
  {
    "text": "comprehensive and and suitable for for kubernetes and it's not limited to to",
    "start": "1663989",
    "end": "1670019"
  },
  {
    "text": "deep learning there are many different use cases here in the demos there are also machine learning one of the interesting",
    "start": "1670019",
    "end": "1676170"
  },
  {
    "text": "machine learning examples is also using a built-in hyper parameter tuning because we said that the engine",
    "start": "1676170",
    "end": "1682110"
  },
  {
    "text": "themselves know how to deal with parallelism so let's assume I want to run the same extra boost example with",
    "start": "1682110",
    "end": "1690090"
  },
  {
    "text": "multiple parameters I can just pass the grid or the different combination and it will essentially distribute it or you",
    "start": "1690090",
    "end": "1697800"
  },
  {
    "text": "know all those different experiments in a sink into all those engines containers",
    "start": "1697800",
    "end": "1702930"
  },
  {
    "text": "and threads in parallel so I can one thing that coupe flow doesn't know how to do today and it's being addressed by",
    "start": "1702930",
    "end": "1710190"
  },
  {
    "text": "this framework is that each one of the steps in couplet today is a single container what happens if you want to",
    "start": "1710190",
    "end": "1717150"
  },
  {
    "text": "apply parallelism like in this training job where we actually use for containers it's very hard and almost impossible to",
    "start": "1717150",
    "end": "1723630"
  },
  {
    "text": "do in coop flow so with what we call Emily run then you can essentially every step could actually be many different",
    "start": "1723630",
    "end": "1729450"
  },
  {
    "text": "things that run in parallel with full parallelism and high concurrency and each one of those can attach two GPUs",
    "start": "1729450",
    "end": "1736490"
  },
  {
    "text": "dynamically and if you want to look at the actual function you can even look into them through a real-time dashboard",
    "start": "1736490",
    "end": "1744150"
  },
  {
    "text": "you can see how much resources they consume you could see their logs you could see if they what's the current",
    "start": "1744150",
    "end": "1749850"
  },
  {
    "text": "amount of you know containers that they're using it could scale to zero you",
    "start": "1749850",
    "end": "1755280"
  },
  {
    "text": "can even use look at the code you could change the resources you know everything",
    "start": "1755280",
    "end": "1760830"
  },
  {
    "text": "that you see here was auto-generated it actually tells you that I think here the dis code was auto-generated from my new",
    "start": "1760830",
    "end": "1768900"
  },
  {
    "text": "Clio from my Jupiter sorry but you could see that they say it's a sense you can configure all the resources all the",
    "start": "1768900",
    "end": "1774750"
  },
  {
    "text": "environment variables volume ons all that stuff it's not stateless you can actually attach storage and volumes into",
    "start": "1774750",
    "end": "1780930"
  },
  {
    "text": "nucleo functions all my package dependencies was sorted up traumatically and generated and and I can even change",
    "start": "1780930",
    "end": "1788820"
  },
  {
    "text": "the triggers right now it's using an HTTP trigger if I need Kafka trigger it's as easy as saying give me Kafka",
    "start": "1788820",
    "end": "1795480"
  },
  {
    "text": "these are the endpoints and all the auto-scaling checkpointing all that is sorted up for you another interesting",
    "start": "1795480",
    "end": "1801840"
  },
  {
    "text": "thing and because of this GPU sharing a notion that like this I have this serve",
    "start": "1801840",
    "end": "1807060"
  },
  {
    "text": "elevator effect that have a bunch of workers hitting the same the same GPU so the utilization of",
    "start": "1807060",
    "end": "1813600"
  },
  {
    "text": "the GPU is much higher when it's running within a nuclear engine okay there are",
    "start": "1813600",
    "end": "1819210"
  },
  {
    "text": "bunch of blogs on that from Nvidia and from from my company so that's on that front I think that's",
    "start": "1819210",
    "end": "1828570"
  },
  {
    "start": "1827000",
    "end": "2046000"
  },
  {
    "text": "sort of ether and any questions or things that you wanna learn how long do",
    "start": "1828570",
    "end": "1838140"
  },
  {
    "text": "it how do you prevent someone from",
    "start": "1838140",
    "end": "1844830"
  },
  {
    "text": "consuming all of the GPU so that's something that it's still not in a man",
    "start": "1844830",
    "end": "1850200"
  },
  {
    "text": "run but we're working on it because finger there is a controller and when you submit the jobs it goes it hits a",
    "start": "1850200",
    "end": "1855780"
  },
  {
    "text": "controller in the controller sends it to generates essentially the the kubernetes",
    "start": "1855780",
    "end": "1860940"
  },
  {
    "text": "resources it's not in the current controller but we're working on essentially adding server admission control in the controller itself and but",
    "start": "1860940",
    "end": "1869670"
  },
  {
    "text": "that's something everyone asked for another interesting thing if you're in the cloud what the controller will do",
    "start": "1869670",
    "end": "1874710"
  },
  {
    "text": "will actually increase the your kubernetes size if needed",
    "start": "1874710",
    "end": "1880580"
  },
  {
    "text": "is there performance difference between using the GPUs and kubernetes versus",
    "start": "1881660",
    "end": "1887850"
  },
  {
    "text": "just pyramidal machines with GPUs same performance because if you think about",
    "start": "1887850",
    "end": "1893160"
  },
  {
    "text": "what what GPUs are you know what Burnett's is or is docker is is",
    "start": "1893160",
    "end": "1898710"
  },
  {
    "text": "essentially just a process the bottom they hit in performance is usually when you think about like networking and",
    "start": "1898710",
    "end": "1905070"
  },
  {
    "text": "storage and things like that that go through another area of abstraction in our solution there is a notion of a real",
    "start": "1905070",
    "end": "1911070"
  },
  {
    "text": "time fabric even memory baseman real time fabric so you'd even don't move data across steps you sense you just",
    "start": "1911070",
    "end": "1917520"
  },
  {
    "text": "move pointers to it there's no even penalty on their movement but usually if you just about computation there is no",
    "start": "1917520",
    "end": "1924270"
  },
  {
    "text": "penalty on moving to docker",
    "start": "1924270",
    "end": "1928309"
  },
  {
    "text": "hey if I have existing tensorflow code how would I go about using it I guess",
    "start": "1938690",
    "end": "1947330"
  },
  {
    "text": "wrap wrapping it in this framework so the you can use your existing tensor tents fluke code and essentially what",
    "start": "1947330",
    "end": "1954769"
  },
  {
    "text": "the runtime is do you have you can have a job run time which is essentially just your tensorflow code but if you want to",
    "start": "1954769",
    "end": "1962419"
  },
  {
    "text": "use if you want to use the you know MPI",
    "start": "1962419",
    "end": "1967909"
  },
  {
    "text": "job again it's again sort of a tensor flow Chara's model but there is a one",
    "start": "1967909",
    "end": "1974179"
  },
  {
    "text": "line that you need to add somewhere don't you remember where which is using Horwood sort of it's again it's nothing",
    "start": "1974179",
    "end": "1980299"
  },
  {
    "text": "to do with this framework it's how you would develop for a distributed tensor flow you need Horwood and then what you",
    "start": "1980299",
    "end": "1985970"
  },
  {
    "text": "see here at the end of the code the logging is done with rank 0 so the",
    "start": "1985970",
    "end": "1991879"
  },
  {
    "text": "master the only math only the master is essentially doing the logging of all the",
    "start": "1991879",
    "end": "1997039"
  },
  {
    "text": "results of the experiments but that's like anyone that works with the Horwood",
    "start": "1997039",
    "end": "2002139"
  },
  {
    "text": "4mph of know how to do yep yeah this is",
    "start": "2002139",
    "end": "2008229"
  },
  {
    "text": "pretty impressive it there are a bunch of features that you float that I have today are you working with them at all",
    "start": "2008229",
    "end": "2014739"
  },
  {
    "text": "to steal that into pull requests or so we're contributing actually one of the",
    "start": "2014739",
    "end": "2020710"
  },
  {
    "text": "many things I'm doing is removing all the GCP specific things there and moving them to environment variables but yes we",
    "start": "2020710",
    "end": "2027609"
  },
  {
    "text": "are doing a lot of work in the community as well cool I think we have enough time",
    "start": "2027609",
    "end": "2033700"
  },
  {
    "text": "for one more question if anyone has one or not cool thank you welcome",
    "start": "2033700",
    "end": "2041280"
  },
  {
    "text": "[Applause]",
    "start": "2041280",
    "end": "2048650"
  }
]