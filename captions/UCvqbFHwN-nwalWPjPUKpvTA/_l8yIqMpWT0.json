[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "well thanks everybody for coming out and sticking around for the last talk of the day and this is my first ever talk at a",
    "start": "30",
    "end": "7859"
  },
  {
    "text": "conference so this is really exciting and if I've learned anything about talks",
    "start": "7859",
    "end": "13200"
  },
  {
    "text": "from other people you always start off with a question so my question is who here is setting resource limits on their",
    "start": "13200",
    "end": "18630"
  },
  {
    "text": "on their pods or containers okay so quite a few people now who here is using",
    "start": "18630",
    "end": "24990"
  },
  {
    "text": "load testing techniques to set those resource limits okay not quite as many",
    "start": "24990",
    "end": "30390"
  },
  {
    "text": "hands but still a few people interesting",
    "start": "30390",
    "end": "38780"
  },
  {
    "text": "well today I want to convince you that that using some of the load testing",
    "start": "38780",
    "end": "44160"
  },
  {
    "start": "40000",
    "end": "231000"
  },
  {
    "text": "techniques to set those are a good idea so I'm an engineer at buffer and a lot",
    "start": "44160",
    "end": "51780"
  },
  {
    "text": "of what I do is help transition from a monolithic application everything's written in PHP and a few front-end",
    "start": "51780",
    "end": "59460"
  },
  {
    "text": "frameworks and we're working towards moving moving towards kubernetes and we've had a fair amount of success with",
    "start": "59460",
    "end": "65729"
  },
  {
    "text": "this and at the moment we've got about 75% of all of our production traffic",
    "start": "65729",
    "end": "70770"
  },
  {
    "text": "served by kubernetes I'm going to jump into a case study and we started off",
    "start": "70770",
    "end": "78990"
  },
  {
    "text": "with a pre-existing endpoint on our monolith and one of our higher throughput endpoints and this particular",
    "start": "78990",
    "end": "84659"
  },
  {
    "text": "endpoints responsibility is to serve the number of times the link has been shared within the buffer app and buffer is",
    "start": "84659",
    "end": "91320"
  },
  {
    "text": "social media applications so knowing how many times somebody has shared your blog",
    "start": "91320",
    "end": "96810"
  },
  {
    "text": "as important information and people actually use this to put a button on",
    "start": "96810",
    "end": "101880"
  },
  {
    "text": "their blog to know how many times people have shared that so you can gauge interest on something so we eventually",
    "start": "101880",
    "end": "109229"
  },
  {
    "text": "settled on the design using node and dynamodb it just happened to be the",
    "start": "109229",
    "end": "114659"
  },
  {
    "text": "right performance and price range for what we were trying to accomplish so we",
    "start": "114659",
    "end": "120840"
  },
  {
    "text": "built the service and then we started to roll it to roll it out to kubernetes and we had about four replicas to start off",
    "start": "120840",
    "end": "127710"
  },
  {
    "text": "with and we manually verified that things were working with curl so we shifted about 1% of our traffic",
    "start": "127710",
    "end": "135880"
  },
  {
    "text": "over to this service things were looking great we had monitoring hooked up with data",
    "start": "135880",
    "end": "140920"
  },
  {
    "text": "dog and you'd but you could barely notice that there was any load running on each of the containers same story at",
    "start": "140920",
    "end": "148060"
  },
  {
    "text": "10 percent then we scaled things up to 50 percent of all of our traffic and",
    "start": "148060",
    "end": "154390"
  },
  {
    "text": "this is where things started to get a little hairy so the first thing we did",
    "start": "154390",
    "end": "159760"
  },
  {
    "text": "was scaled up our replicas up 5 times - so we had 20 pods running and this",
    "start": "159760",
    "end": "164890"
  },
  {
    "text": "helped but we still got that oom Kildare that that that's very dreadful so we",
    "start": "164890",
    "end": "175000"
  },
  {
    "text": "shifted back our traffic under our mana let them started to investigate what was actually going on there so what had",
    "start": "175000",
    "end": "182410"
  },
  {
    "text": "happened is I had copied and pasted a deployment file from another service and this particular service was an old",
    "start": "182410",
    "end": "189880"
  },
  {
    "text": "service that didn't have as much traffic and we were still pretty new to this at the time and this in this deployment",
    "start": "189880",
    "end": "195459"
  },
  {
    "text": "file contains some resource limits so when we were doing the coop kuddle",
    "start": "195459",
    "end": "201850"
  },
  {
    "text": "describe that the pods were reporting oh and killed so let's talk a little bit",
    "start": "201850",
    "end": "208510"
  },
  {
    "text": "about what resource limits actually are there's something that can be set on both CPU and memory and when they're not",
    "start": "208510",
    "end": "216730"
  },
  {
    "text": "set these things can it can run unbounded and they can actually take up all the resources on the node that",
    "start": "216730",
    "end": "222489"
  },
  {
    "text": "they're on and when the limits are exceeded kubernetes is going to go",
    "start": "222489",
    "end": "227560"
  },
  {
    "text": "through and it's going to it's going to kill those pods so how do we go about",
    "start": "227560",
    "end": "232660"
  },
  {
    "start": "231000",
    "end": "259000"
  },
  {
    "text": "actually setting these things so it's it's helpful to discuss what optimal",
    "start": "232660",
    "end": "238930"
  },
  {
    "text": "limits actually look like this means that pods have enough resources to complete whatever tasks",
    "start": "238930",
    "end": "244720"
  },
  {
    "text": "they're given whether that's HTTP maybe it's a worker doing work on a queue and",
    "start": "244720",
    "end": "251190"
  },
  {
    "text": "also each of the nodes can run the maximum number of pods so there's no",
    "start": "251190",
    "end": "256690"
  },
  {
    "text": "waste so it's helpful to talk about ways that things can go wrong and also things",
    "start": "256690",
    "end": "263169"
  },
  {
    "start": "259000",
    "end": "298000"
  },
  {
    "text": "can go right when you're allocating resources is the first one is under allocation",
    "start": "263169",
    "end": "269030"
  },
  {
    "text": "this one's really obvious this is what happened to us we didn't have enough resources allocated and what kubernetes",
    "start": "269030",
    "end": "275480"
  },
  {
    "text": "is going to do is recognize that one of your limits has been crawled crossed and that's going to cook that's going to",
    "start": "275480",
    "end": "281120"
  },
  {
    "text": "kill the container the other one is over allocation this is where you've given",
    "start": "281120",
    "end": "287270"
  },
  {
    "text": "your your pod or your container enough resources that it's never actually going",
    "start": "287270",
    "end": "293360"
  },
  {
    "text": "to be able to use those no but for the load that that you're trying to service and this is a tricky problem to detect",
    "start": "293360",
    "end": "300260"
  },
  {
    "start": "298000",
    "end": "369000"
  },
  {
    "text": "because things things can work for a while and you can be in this place where",
    "start": "300260",
    "end": "306170"
  },
  {
    "text": "you're running you you're you're handling all of your load and things",
    "start": "306170",
    "end": "311660"
  },
  {
    "text": "look great but it becomes a problem when you start to scale up your replicas let's say that you've got one container",
    "start": "311660",
    "end": "318410"
  },
  {
    "text": "that is wasting five megabytes of memory you scale that up to a hundred you're",
    "start": "318410",
    "end": "324170"
  },
  {
    "text": "wasting or you're wasting five hundred and so on and so forth as you scale up",
    "start": "324170",
    "end": "329410"
  },
  {
    "text": "so this the here's a little picture that that illustrates this if I give one",
    "start": "329410",
    "end": "336460"
  },
  {
    "text": "workload half the resources on a node that means I can only run two of the",
    "start": "336460",
    "end": "342110"
  },
  {
    "text": "replicas on a node now if I give it the appropriate amount of resources that's",
    "start": "342110",
    "end": "347180"
  },
  {
    "text": "an extra container that it can be running and if you take away anything from this talk I think this is the one thing is if you set your container",
    "start": "347180",
    "end": "353270"
  },
  {
    "text": "resources limits correctly you're going to you're going to save money and you're",
    "start": "353270",
    "end": "358580"
  },
  {
    "text": "going to be utilizing the resources that cougar Knight kubernetes provides that's",
    "start": "358580",
    "end": "364280"
  },
  {
    "text": "one extra pot in that example",
    "start": "364280",
    "end": "367750"
  },
  {
    "start": "369000",
    "end": "495000"
  },
  {
    "text": "so let's talk a little bit about how kubernetes monitoring works under the hood this graph there's a lot going on",
    "start": "369879",
    "end": "377740"
  },
  {
    "text": "here but I wanted to show this first because I think it's important I think it shows that there's the centralized",
    "start": "377740",
    "end": "384580"
  },
  {
    "text": "thing in the middle called heap stir that's orchestrating or that's collecting information and also",
    "start": "384580",
    "end": "390189"
  },
  {
    "text": "providing an API to other parts of the system so each node is effectively working through heap stir and the",
    "start": "390189",
    "end": "398020"
  },
  {
    "text": "kubernetes master is making that information available also heap store pushes things off to a storage back-end",
    "start": "398020",
    "end": "404979"
  },
  {
    "text": "that's pretty important too so let's dig a little bit into the details here so",
    "start": "404979",
    "end": "411249"
  },
  {
    "text": "the first step is see advisor and what that does is monitors all of the",
    "start": "411249",
    "end": "416909"
  },
  {
    "text": "containers on a given node and it's going to be getting information about network CPU filesystem and memory",
    "start": "416909",
    "end": "427360"
  },
  {
    "text": "utilization and then on top of that the Kubla is going to be getting information",
    "start": "427360",
    "end": "432699"
  },
  {
    "text": "from Z advisor and it's going to use the information from C advisor to make decisions on whether things should",
    "start": "432699",
    "end": "438879"
  },
  {
    "text": "should be killed and also use it for some monitoring and then on top of that",
    "start": "438879",
    "end": "445479"
  },
  {
    "text": "there's heap stir which essentially aggregates all of the information from all the couplets on all the nodes and",
    "start": "445479",
    "end": "451769"
  },
  {
    "text": "makes that information available and it pushes it off to a storage back-end that",
    "start": "451769",
    "end": "457839"
  },
  {
    "text": "could be in flux TB with your fauna it could also be Google Cloud monitoring is another one and there's a bunch of",
    "start": "457839",
    "end": "464110"
  },
  {
    "text": "third-party backends that you could also use so how do we go about setting limits",
    "start": "464110",
    "end": "471099"
  },
  {
    "text": "that the goal is really just to understand what one pod can handle here and you start with a really conservative",
    "start": "471099",
    "end": "478959"
  },
  {
    "text": "set of limits they start really low and you change one thing at a time and you",
    "start": "478959",
    "end": "484809"
  },
  {
    "text": "observe the changes you try to be scientific about it because if you change too many things it's it's kind of",
    "start": "484809",
    "end": "491169"
  },
  {
    "text": "hard to understand what actually happened when you change something so there's a couple of testing strategies",
    "start": "491169",
    "end": "497439"
  },
  {
    "start": "495000",
    "end": "559000"
  },
  {
    "text": "that I'm going to employ to actually set the limits and the first one is a ramp up test",
    "start": "497439",
    "end": "503020"
  },
  {
    "text": "and what we're gonna do is start with one client and then we're gonna scale up the clients until and just watch what",
    "start": "503020",
    "end": "510460"
  },
  {
    "text": "happens to the response times for the for this particular test and the idea is",
    "start": "510460",
    "end": "516039"
  },
  {
    "text": "that you you eventually want to find a breaking point you're looking for major changes maybe you cross a threshold and",
    "start": "516040",
    "end": "522669"
  },
  {
    "text": "then all of a sudden you're getting a bunch of 500s after that we're",
    "start": "522670",
    "end": "530530"
  },
  {
    "text": "essentially going to take the this slice the the tallest slice where we're right before we broke and then we're going to",
    "start": "530530",
    "end": "538510"
  },
  {
    "text": "operate just under the breaking point for an extended period of time and what we're gonna look for here is it",
    "start": "538510",
    "end": "546280"
  },
  {
    "text": "is major changes in response times maybe you get some variance but what you want",
    "start": "546280",
    "end": "551440"
  },
  {
    "text": "to see here is pretty consistent load pretty consistent response times and this is also where you make fine-tuned",
    "start": "551440",
    "end": "557920"
  },
  {
    "text": "adjustments as well so I'm going to do a demo I'm going to set limits for an Etsy",
    "start": "557920",
    "end": "564300"
  },
  {
    "start": "559000",
    "end": "853000"
  },
  {
    "text": "d-pod when I take this off",
    "start": "564300",
    "end": "571500"
  },
  {
    "text": "all right it's like it's flickering is that visible I'm going to increase the",
    "start": "577000",
    "end": "585459"
  },
  {
    "text": "font size here so I'm watching the pods",
    "start": "585459",
    "end": "592120"
  },
  {
    "text": "and I've got one nginx container running right now and if you remember before I",
    "start": "592120",
    "end": "597459"
  },
  {
    "text": "said I was going to be setting things for @cd the reason why I have to do this",
    "start": "597459",
    "end": "603129"
  },
  {
    "text": "and I'm just going to show oh yes",
    "start": "603129",
    "end": "608189"
  },
  {
    "text": "maybe there's too many cables",
    "start": "638170",
    "end": "641760"
  },
  {
    "text": "yep",
    "start": "647110",
    "end": "650110"
  },
  {
    "text": "that looks better I have to readjust all",
    "start": "669360",
    "end": "675160"
  },
  {
    "text": "the settings it's okay",
    "start": "675160",
    "end": "678180"
  },
  {
    "text": "oh wait",
    "start": "687820",
    "end": "690570"
  },
  {
    "text": "all right what oh I didn't do it",
    "start": "708850",
    "end": "724389"
  },
  {
    "text": "yep",
    "start": "742970",
    "end": "745970"
  },
  {
    "text": "and I was getting to the good part -",
    "start": "748840",
    "end": "752940"
  },
  {
    "text": "okay",
    "start": "755340",
    "end": "758340"
  },
  {
    "text": "okay okay let's see if I can wait I'll",
    "start": "767320",
    "end": "793420"
  },
  {
    "text": "make that happen um I'll make this one smaller I don't even know it",
    "start": "793420",
    "end": "810310"
  },
  {
    "text": "all my screens are all right okay uh",
    "start": "810310",
    "end": "829230"
  },
  {
    "text": "[Music] does this really matter that much it's",
    "start": "830170",
    "end": "838180"
  },
  {
    "text": "like right on the edge all right look we're gonna roll with",
    "start": "838180",
    "end": "843209"
  },
  {
    "text": "this okay",
    "start": "843209",
    "end": "846620"
  },
  {
    "text": "all right so I've got an engine X Server",
    "start": "849560",
    "end": "855149"
  },
  {
    "start": "853000",
    "end": "1223000"
  },
  {
    "text": "and that's exposed or that's proxying requests out to at CD and I'm also",
    "start": "855149",
    "end": "862290"
  },
  {
    "text": "exposing a loader i/o token I'm basically using I'm using loader I did IO to run these load tests and I need to",
    "start": "862290",
    "end": "869579"
  },
  {
    "text": "expose a token to give them permission to to run to run load tests so that's",
    "start": "869579",
    "end": "876269"
  },
  {
    "text": "that's why this looks like I thought it was important to call that out because it look a little funky so I'm gonna",
    "start": "876269",
    "end": "884160"
  },
  {
    "text": "create a deployment and while that's",
    "start": "884160",
    "end": "898200"
  },
  {
    "text": "creating I'm gonna take a look at what the deployment file actually looks like it's pretty pretty low resources using",
    "start": "898200",
    "end": "906930"
  },
  {
    "text": "50 M which is a roughly one twentieth of a core and four megabytes and it looks",
    "start": "906930",
    "end": "916440"
  },
  {
    "text": "like the container came up so I'm gonna exact into it so I can grab a shell",
    "start": "916440",
    "end": "927770"
  },
  {
    "text": "and I'm just going to show you that that",
    "start": "930270",
    "end": "935740"
  },
  {
    "text": "there's there's there's a there's some data in that CD service don't worry this",
    "start": "935740",
    "end": "949600"
  },
  {
    "text": "is expected the the resources are so constrained here that I can't even run a",
    "start": "949600",
    "end": "955360"
  },
  {
    "text": "command so it's a pretty good idea that I should probably I should probably give this thing more room to breathe here so",
    "start": "955360",
    "end": "961540"
  },
  {
    "text": "I'm going to edit the deployment and give it a little bit more resources",
    "start": "961540",
    "end": "967620"
  },
  {
    "text": "and I'm going to increase the memory to",
    "start": "977380",
    "end": "982540"
  },
  {
    "text": "250 we're going to watch that container",
    "start": "982540",
    "end": "988040"
  },
  {
    "text": "restart and if you remember before I was",
    "start": "988040",
    "end": "993530"
  },
  {
    "text": "talking about the monitoring I'm actually going to tap directly into the sea advisor and I'm going to use that to",
    "start": "993530",
    "end": "1002080"
  },
  {
    "text": "get the resource limits and that container came up so here's the adviser",
    "start": "1002080",
    "end": "1010540"
  },
  {
    "text": "here it's a little clunky to get to it",
    "start": "1010540",
    "end": "1015700"
  },
  {
    "text": "but once I'm there you can see that I've got my resource limits are showing up here the increased memory shows me CPU",
    "start": "1015700",
    "end": "1025089"
  },
  {
    "text": "and in this case I've only got one core so the total usage and usage per core",
    "start": "1025090",
    "end": "1030640"
  },
  {
    "text": "are going to look the same and then we've also got some some memory to look at and that 50 that I set to I have a",
    "start": "1030640",
    "end": "1039910"
  },
  {
    "text": "lot more room to breathe now if I would have looked before this would have been about a hundred percent as things are",
    "start": "1039910",
    "end": "1045670"
  },
  {
    "text": "looking good I'm gonna start so the I'm gonna start running that ramp up test now so here's a previous test we're",
    "start": "1045670",
    "end": "1052840"
  },
  {
    "text": "gonna go from 0 to about 250",
    "start": "1052840",
    "end": "1057750"
  },
  {
    "text": "so you'll see that the CPU utilization is starting to creep up here a little",
    "start": "1066300",
    "end": "1072780"
  },
  {
    "text": "bit and the data is not exactly real-time but it's about as close as as you can get as you can see here memory",
    "start": "1072780",
    "end": "1091260"
  },
  {
    "text": "jumped up a little bit but not quite as much as the CPU I'm still waiting for",
    "start": "1091260",
    "end": "1096690"
  },
  {
    "text": "another update here so 0:05 just from doing this before this is",
    "start": "1096690",
    "end": "1102500"
  },
  {
    "text": "this is the bottleneck here this is its its hit its resource limit so what I",
    "start": "1102500",
    "end": "1110220"
  },
  {
    "text": "want to do here now since I can see that this is the bottleneck it's kind of flatlined once it reached that point",
    "start": "1110220",
    "end": "1116900"
  },
  {
    "text": "that's what I know I need to increase now so if I take a look here at about 30",
    "start": "1116900",
    "end": "1125310"
  },
  {
    "text": "seconds and see at the halfway point I'm",
    "start": "1125310",
    "end": "1131640"
  },
  {
    "text": "at around 450 to 500 milliseconds for response time so if this was in fact the",
    "start": "1131640",
    "end": "1137670"
  },
  {
    "text": "bottleneck I would expect that number to go down so I'm going to edit the",
    "start": "1137670",
    "end": "1144420"
  },
  {
    "text": "deployment again and I'm going to",
    "start": "1144420",
    "end": "1152760"
  },
  {
    "text": "increase this thing to 500m for the CPU",
    "start": "1152760",
    "end": "1159170"
  },
  {
    "text": "and unfortunately this isn't going to come up and I kind of wanted to show this because this is something that can",
    "start": "1163320",
    "end": "1170200"
  },
  {
    "text": "happen when you're setting resource limits if I do a describe on that pod",
    "start": "1170200",
    "end": "1178860"
  },
  {
    "text": "I'm gonna see that it did fail fail to get scheduling because it didn't have",
    "start": "1182580",
    "end": "1187809"
  },
  {
    "text": "enough resources to for the CPU and this",
    "start": "1187809",
    "end": "1193090"
  },
  {
    "text": "particular cluster is really really small so it's kind of expected to do that so I'm going to set that to",
    "start": "1193090",
    "end": "1198970"
  },
  {
    "text": "something a little bit more reasonable",
    "start": "1198970",
    "end": "1202140"
  },
  {
    "text": "so I'm gonna rerun that test and what I'm expecting now is to see better results at something better than about",
    "start": "1224150",
    "end": "1231810"
  },
  {
    "text": "500 milliseconds that 30 seconds would 500 milliseconds response time at",
    "start": "1231810",
    "end": "1237180"
  },
  {
    "text": "halfway point through the test oh I need to go grab so unfortunately see advisor",
    "start": "1237180",
    "end": "1246330"
  },
  {
    "text": "change or the idea of of things change when the pod goes down so I have to go grab it again",
    "start": "1246330",
    "end": "1253010"
  },
  {
    "text": "I did yeah you wouldn't want to do this in production",
    "start": "1257650",
    "end": "1263190"
  },
  {
    "text": "so we're at about the halfway point and we can already see we're at around 133",
    "start": "1270760",
    "end": "1276340"
  },
  {
    "text": "milliseconds so at the moment that is in fact our bottleneck and I don't have any",
    "start": "1276340",
    "end": "1281740"
  },
  {
    "text": "more resources on this machine to give it so that this is effectively the most I can do with this setup you can also",
    "start": "1281740",
    "end": "1289030"
  },
  {
    "text": "see here that we've we've hit our resource limit 0.15 so at this point I",
    "start": "1289030",
    "end": "1298320"
  },
  {
    "text": "can take a look at loader i/o is pretty nice because it can show you what what",
    "start": "1298320",
    "end": "1304299"
  },
  {
    "text": "requests per second you were actually doing and it looks like we're doing something we're somewhere between 800 and 900 requests per second so the next",
    "start": "1304299",
    "end": "1312460"
  },
  {
    "text": "test that duration test I would actually probably want to run with something like 800 so I've got another test prepared",
    "start": "1312460",
    "end": "1320160"
  },
  {
    "text": "it's going to do 800 clients over one minute and if you are doing this in",
    "start": "1320160",
    "end": "1326020"
  },
  {
    "text": "production you probably want to do this as long as you can something more than 10 minutes maybe an hour depending on",
    "start": "1326020",
    "end": "1332650"
  },
  {
    "text": "what you have bandwidth for so I'm gonna go ahead and kick that test off and",
    "start": "1332650",
    "end": "1339610"
  },
  {
    "text": "really what you want to see here is once things kind of level out your your",
    "start": "1339610",
    "end": "1345820"
  },
  {
    "text": "response times should should remain at the other test running you know your",
    "start": "1345820",
    "end": "1355809"
  },
  {
    "text": "response time should remain relatively flat and this graph is looking kind of",
    "start": "1355809",
    "end": "1363429"
  },
  {
    "text": "interesting usually it sticks around 100 milliseconds would probably want to dig",
    "start": "1363429",
    "end": "1369490"
  },
  {
    "text": "into to what happened there and why why that why that increased for a certain period of time but could also be that",
    "start": "1369490",
    "end": "1377440"
  },
  {
    "text": "somebody here is running load tests on this",
    "start": "1377440",
    "end": "1381419"
  },
  {
    "text": "right",
    "start": "1386810",
    "end": "1389380"
  },
  {
    "text": "so when you're going through this process it's really important to keep a",
    "start": "1402210",
    "end": "1407470"
  },
  {
    "text": "failed log and what what you want to see or what you want to write down or think",
    "start": "1407470",
    "end": "1413290"
  },
  {
    "text": "the way that things failed things are good things are going to break when you do this especially when you get to the",
    "start": "1413290",
    "end": "1418570"
  },
  {
    "text": "point where you're crossing the threshold of the particular pod and some",
    "start": "1418570",
    "end": "1424000"
  },
  {
    "text": "of the stuff that we've seen memory slowly increasing what we saw there was",
    "start": "1424000",
    "end": "1429100"
  },
  {
    "text": "CPU as peg debt at the resource limit we've also seen five hundreds high",
    "start": "1429100",
    "end": "1435130"
  },
  {
    "text": "response times a little more interesting is when you see large variants and response times we had a cron job that",
    "start": "1435130",
    "end": "1443140"
  },
  {
    "text": "was running that that was causing one of these it was pretty interesting that to catch that and also dropped requests is",
    "start": "1443140",
    "end": "1451450"
  },
  {
    "text": "another common one so when we were going through this process with the link",
    "start": "1451450",
    "end": "1457179"
  },
  {
    "text": "service really what we what we were",
    "start": "1457179",
    "end": "1463390"
  },
  {
    "text": "trying to accomplish here was we needed to understand how things break and for",
    "start": "1463390",
    "end": "1469870"
  },
  {
    "text": "us it things don't feel production ready until you go through a process like this",
    "start": "1469870",
    "end": "1475090"
  },
  {
    "text": "because you don't really understand what's gonna happen when you're pushing things towards the edge and really it's",
    "start": "1475090",
    "end": "1483040"
  },
  {
    "text": "about increasing predictability and this is important because when when things",
    "start": "1483040",
    "end": "1489010"
  },
  {
    "text": "are predictable you can understand how to scale this up you have sort of a unit",
    "start": "1489010",
    "end": "1495580"
  },
  {
    "text": "of scaling and though it's not exactly linear it's also important to do this with with more pods to get closer to the",
    "start": "1495580",
    "end": "1505059"
  },
  {
    "text": "load that you want to match but really it's just about predictability so just",
    "start": "1505059",
    "end": "1512590"
  },
  {
    "text": "looking forward with kubernetes there's so many great tools for ops and things",
    "start": "1512590",
    "end": "1520059"
  },
  {
    "text": "that are done on the cluster wide level but I think developers really want to be",
    "start": "1520059",
    "end": "1527260"
  },
  {
    "text": "able to dig into one things times and get get their hands dirty and to do some debugging and there's some",
    "start": "1527260",
    "end": "1533450"
  },
  {
    "text": "tooling that could exist that just doesn't yet and I think it's just kind",
    "start": "1533450",
    "end": "1538940"
  },
  {
    "text": "of the natural progression of kubernetes some of the talks earlier today have reflected this to to developers",
    "start": "1538940",
    "end": "1547160"
  },
  {
    "text": "developers want to be using kubernetes to so I just want to say thanks",
    "start": "1547160",
    "end": "1553280"
  },
  {
    "text": "everybody and open up for her questions",
    "start": "1553280",
    "end": "1557680"
  },
  {
    "text": "you should do a talk man you should do a talk most of most of the experience that",
    "start": "1602910",
    "end": "1610000"
  },
  {
    "text": "we've got is with PHP and node applications so we haven't really done a",
    "start": "1610000",
    "end": "1616330"
  },
  {
    "text": "whole lot of JVM tuning so I can't really speak to that I'm sorry",
    "start": "1616330",
    "end": "1622200"
  },
  {
    "text": "so the question was have we thought about automating this process and the answer that is yes we have loader Eyal",
    "start": "1639890",
    "end": "1647210"
  },
  {
    "text": "actually exposes some API endpoints so we could actually get to the point where we can automate this maybe working with",
    "start": "1647210",
    "end": "1655180"
  },
  {
    "text": "the the dais team and packaging some of those things might be there might be",
    "start": "1655180",
    "end": "1660440"
  },
  {
    "text": "something cool that you could automate that whole flow but yeah we want to do that yes",
    "start": "1660440",
    "end": "1666970"
  },
  {
    "text": "so the question is how do you deal with noisy neighbors and that's that's a really good question the I guess doing",
    "start": "1686840",
    "end": "1696470"
  },
  {
    "text": "load testing like this you want to be as close to what your production setup looks like and if your neighbors are in",
    "start": "1696470",
    "end": "1705800"
  },
  {
    "text": "fact noisy and production and use you scale up your traffic with real load",
    "start": "1705800",
    "end": "1711080"
  },
  {
    "text": "you're still going to be seeing those things so I think it's okay to be to be load testing within within the bounds of",
    "start": "1711080",
    "end": "1718610"
  },
  {
    "text": "your real traffic because that's that's closer to getting a real story of what's",
    "start": "1718610",
    "end": "1725180"
  },
  {
    "text": "happening rather than the the other way which would be maybe spinning up another cluster and doing that on a on a cluster",
    "start": "1725180",
    "end": "1733010"
  },
  {
    "text": "that's running only that one thing so I guess I like that there's noise II",
    "start": "1733010",
    "end": "1738890"
  },
  {
    "text": "neighbors that that helps you",
    "start": "1738890",
    "end": "1742060"
  },
  {
    "text": "any other questions yes yeah yes you're",
    "start": "1747270",
    "end": "1760260"
  },
  {
    "text": "absolutely right",
    "start": "1760260",
    "end": "1762679"
  },
  {
    "text": "so I think that's a really hard question to answer because that there are so it",
    "start": "1776220",
    "end": "1782639"
  },
  {
    "text": "kind of depends on the load you're trying to handle there's there's some",
    "start": "1782639",
    "end": "1790500"
  },
  {
    "text": "people like the AI stuff maybe your your resource type might need GPUs on it or",
    "start": "1790500",
    "end": "1797250"
  },
  {
    "text": "maybe you need instances with lots of memory because you're you're running Redis on on them or something like that",
    "start": "1797250",
    "end": "1803990"
  },
  {
    "text": "for us more compute tends to be better I",
    "start": "1803990",
    "end": "1812039"
  },
  {
    "text": "mean we use the compute instances but I can't remember exactly which one like X",
    "start": "1812039",
    "end": "1819259"
  },
  {
    "text": "large or something I don't know I don't normally I'm more I'm more on the",
    "start": "1819259",
    "end": "1824399"
  },
  {
    "text": "application side of things yes",
    "start": "1824399",
    "end": "1834440"
  },
  {
    "text": "yeah so the question is do we have",
    "start": "1840200",
    "end": "1850190"
  },
  {
    "text": "resource limits on all of our pods the answer is yes we do what say we do not",
    "start": "1850190",
    "end": "1861220"
  },
  {
    "text": "did the question was do we use limit range and we don't I'm gonna look into",
    "start": "1861220",
    "end": "1869539"
  },
  {
    "text": "it okay any other questions well anyways thanks",
    "start": "1869539",
    "end": "1881059"
  },
  {
    "text": "everybody and [Applause]",
    "start": "1881059",
    "end": "1888250"
  }
]