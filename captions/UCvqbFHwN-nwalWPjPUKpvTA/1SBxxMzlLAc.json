[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "hi everyone my name is Sri Zhao Decker I am a software engineer at Intuit and I'm",
    "start": "0",
    "end": "7020"
  },
  {
    "text": "going to talk today about our journey of running Apache Kafka on kubernetes from evaluation to production so I quickly go",
    "start": "7020",
    "end": "16500"
  },
  {
    "start": "15000",
    "end": "54000"
  },
  {
    "text": "over the agenda of the talk we'll quickly talk about why we decided to use Kafka why did we choose to run Kafka on",
    "start": "16500",
    "end": "23550"
  },
  {
    "text": "kubernetes or why did we think about kubernetes as the kind of substrate for running Kafka Kafka on kubernetes",
    "start": "23550",
    "end": "29880"
  },
  {
    "text": "architectures so how did we think about this what kubernetes objects and resources come into play when we are trying to run Kafka on cover IDs and",
    "start": "29880",
    "end": "36030"
  },
  {
    "text": "then I'll go into details of the architecture itself I'll try and focus on the problems as much as possible",
    "start": "36030",
    "end": "42180"
  },
  {
    "text": "because problems are going to be common across all of us depending on where environments and SaaS and everything",
    "start": "42180",
    "end": "48149"
  },
  {
    "text": "else the solutions to those problems might be different it's also oft where there are 500 ways to solve every problem",
    "start": "48149",
    "end": "53600"
  },
  {
    "text": "so why Kafka so within into it I work for an organization or a team called",
    "start": "53600",
    "end": "59910"
  },
  {
    "start": "54000",
    "end": "140000"
  },
  {
    "text": "QuickBooks Intuit QuickBooks is a monolithic application for that is used",
    "start": "59910",
    "end": "65128"
  },
  {
    "text": "by millions of customers small business customers these small businesses use QuickBooks for accounting payroll and",
    "start": "65129",
    "end": "71939"
  },
  {
    "text": "other financial management and QuickBooks is a fairly complex application it's been around for several",
    "start": "71939",
    "end": "77520"
  },
  {
    "text": "years now and it has grown organically and it is it has tightly coupled modules as you know as time has gone by it has",
    "start": "77520",
    "end": "84330"
  },
  {
    "text": "like you know gathered a lot of interesting functionalities but it has become harder to scale it is harder to",
    "start": "84330",
    "end": "90210"
  },
  {
    "text": "do continuous deployment with it and the goal was to take this big giant monolithic application and break it up",
    "start": "90210",
    "end": "96119"
  },
  {
    "text": "into multiple micro services and it's one thing to break it up into micro services run it on kubernetes but then",
    "start": "96119",
    "end": "102390"
  },
  {
    "text": "there is this other interesting question that comes up with it how are these modules or an individual micro services going to be able to talk",
    "start": "102390",
    "end": "108570"
  },
  {
    "text": "to each other and the goal therefore was to use Apache kafka for doing this so",
    "start": "108570",
    "end": "113610"
  },
  {
    "text": "Kafka kind of becomes this central nervous system which connects all these individual components and micro services",
    "start": "113610",
    "end": "119430"
  },
  {
    "text": "and they are able to send their data onto the Kafka bus so at different topics subscribers can subscribe to it",
    "start": "119430",
    "end": "124979"
  },
  {
    "text": "there could be more than one subscribers micro-services don't need to know about who else is running in the system I kind",
    "start": "124979",
    "end": "130560"
  },
  {
    "text": "of gives us the benefits of micro service architecture as as kind of decomposition and therefore",
    "start": "130560",
    "end": "135750"
  },
  {
    "text": "with it continuous deployments and so on and so forth why kubernetes for Kafka so I mean of",
    "start": "135750",
    "end": "143040"
  },
  {
    "start": "140000",
    "end": "338000"
  },
  {
    "text": "course there is you know we all benefit from kubernetes you know we have job security if there is kubernetes everywhere but but but beyond that the",
    "start": "143040",
    "end": "151620"
  },
  {
    "text": "point was kubernetes is becoming the de facto standard for running containerized applications all the kubernetes talk",
    "start": "151620",
    "end": "159420"
  },
  {
    "text": "that I am giving you today or everything that I'm talking about today is strictly about running kubernetes on AWS so in",
    "start": "159420",
    "end": "165510"
  },
  {
    "text": "kubernetes integrates very nicely with AWS it's very nicely it's aware of AWS it has all good features for integrating",
    "start": "165510",
    "end": "171540"
  },
  {
    "text": "with other AWS resources which is really good and then there are several constructs within kubernetes which are",
    "start": "171540",
    "end": "177390"
  },
  {
    "text": "great for kafka things such as stateful sets where you have a pod and you have a volume together as one unit and then you",
    "start": "177390",
    "end": "184410"
  },
  {
    "text": "can take this one unit and create n replicas of this you can have this one unit of a pod and a PVC spread across",
    "start": "184410",
    "end": "191250"
  },
  {
    "text": "multiple zones have them have them run in a configuration where they can always",
    "start": "191250",
    "end": "196650"
  },
  {
    "text": "be together and things like that so stateful sets were great configuration Maps Kafka like many other applications",
    "start": "196650",
    "end": "202530"
  },
  {
    "text": "is a fairly complex application it has a lot of configuration that goes with it you want this configuration to shared",
    "start": "202530",
    "end": "208320"
  },
  {
    "text": "across all the brokers config Maps give you the ability to create one config map and mount it inside all your brokers or",
    "start": "208320",
    "end": "214830"
  },
  {
    "text": "all your n replicas get the same configuration secrets so SSL was important for us I'll go over",
    "start": "214830",
    "end": "220440"
  },
  {
    "text": "that in a little more detail later about why about security and how SSL was kind of the key to the way we achieved",
    "start": "220440",
    "end": "226380"
  },
  {
    "text": "security so secret kubernetes secrets provided us with a way of using of creating an SSL certificate put it into",
    "start": "226380",
    "end": "232290"
  },
  {
    "text": "a company secret mounted in all brokers and again we got what we wanted and then the last point",
    "start": "232290",
    "end": "237480"
  },
  {
    "text": "about pod and node affinity and anti affinity so there are some parts of Kafka which was super interesting from",
    "start": "237480",
    "end": "243660"
  },
  {
    "text": "interesting from from an architectural point of view where we did not want to kafka brokers to land up on the same ec2",
    "start": "243660",
    "end": "249930"
  },
  {
    "text": "instance we did not want a Kafka broker and the zookeeper instance to land up on",
    "start": "249930",
    "end": "254970"
  },
  {
    "text": "the same ec2 instance so we wanted to kind of spread them apart for availability reasons but how do you",
    "start": "254970",
    "end": "260280"
  },
  {
    "text": "express that such that it actually is guaranteed that no such pod will actually land up",
    "start": "260280",
    "end": "265410"
  },
  {
    "text": "on the same node we use pod affinity or pod ante affinity and some node",
    "start": "265410",
    "end": "270570"
  },
  {
    "text": "affinities fault achieving this so I'll go over that also in a little more detail but these kind of basic",
    "start": "270570",
    "end": "275880"
  },
  {
    "text": "constructs that are fundamental to the way kubernetes is made it made us the",
    "start": "275880",
    "end": "281040"
  },
  {
    "text": "kind of you know think about think very seriously about why we should use kubernetes as a substrate for running",
    "start": "281040",
    "end": "286770"
  },
  {
    "text": "Kafka and then last but an extremely important point about extensibility",
    "start": "286770",
    "end": "291810"
  },
  {
    "text": "offered by containerized environments so it's one thing to take this big monolith or big interesting application that can",
    "start": "291810",
    "end": "299130"
  },
  {
    "text": "provide a lot of functionality has great promise and install it and get it to run but it's another thing to kind of figure",
    "start": "299130",
    "end": "305610"
  },
  {
    "text": "out how our team is going to use it what kind of tools and services are needed for the last mile problem so says there",
    "start": "305610",
    "end": "312390"
  },
  {
    "text": "are at least two examples where we had these kinds of last mile problems and we found a solution which was in the form",
    "start": "312390",
    "end": "317970"
  },
  {
    "text": "of another tool or another utility and all we had to do in kubernetes was take that docker image and run it as another",
    "start": "317970",
    "end": "324180"
  },
  {
    "text": "pod and now suddenly this last mile problem was easy to solve so the extensibility offered by containers fundamentally and kubernetes",
    "start": "324180",
    "end": "331020"
  },
  {
    "text": "specifically was amazing in the way we were able to achieve this especially when we wanted to run engine production",
    "start": "331020",
    "end": "337670"
  },
  {
    "text": "so it was this was it was great like we were like okay we thought about this we think this is a great kind of a great",
    "start": "337670",
    "end": "344370"
  },
  {
    "start": "338000",
    "end": "490000"
  },
  {
    "text": "marriage between Kafka and kubernetes this will work well where do we begin so",
    "start": "344370",
    "end": "349650"
  },
  {
    "text": "like all good engineers we looked at Stack Overflow sorry not exactly Stack Overflow but but we",
    "start": "349650",
    "end": "357060"
  },
  {
    "text": "did a Google search and and we found out that there are many other people who think similarly and we found a bunch of",
    "start": "357060",
    "end": "362730"
  },
  {
    "text": "these interesting articles and github repos where people talk about running",
    "start": "362730",
    "end": "368430"
  },
  {
    "text": "Kafka on kubernetes so specifically among them the one from you lean it's a github repo the slides will be made",
    "start": "368430",
    "end": "374940"
  },
  {
    "text": "available you can find that but if you search for your lean Kafka kubernetes will pretty much get there was really",
    "start": "374940",
    "end": "380130"
  },
  {
    "text": "good if there is someone here by the way from yo lean please get in touch with me I owe you a drink or coffee or I know",
    "start": "380130",
    "end": "387180"
  },
  {
    "text": "it's Christmas season maybe you know gingerbread cookies for all UK but this was great",
    "start": "387180",
    "end": "392910"
  },
  {
    "text": "we found this repo it was exactly along the lines of what we wanted it was giving us a multi",
    "start": "392910",
    "end": "399129"
  },
  {
    "text": "zone or if I had a kubernetes cluster which spanned across multiple zones it",
    "start": "399129",
    "end": "404259"
  },
  {
    "text": "was giving us Kafka as a stateful set it trans do keeper as a stateful said it was using config maps for the server dot",
    "start": "404259",
    "end": "410889"
  },
  {
    "text": "properties file which is what the configuration for Kafka is and interestingly enough it also had shell",
    "start": "410889",
    "end": "417039"
  },
  {
    "text": "scripts for actually running these brokers so this was a little interesting because fundamentally like when you want",
    "start": "417039",
    "end": "422499"
  },
  {
    "text": "to run a broker in Kafka it is a Java command it's like Java dash M and a bunch of jars and then you know some",
    "start": "422499",
    "end": "427509"
  },
  {
    "text": "other arguments that go with it instead of just running that command in the container it actually had a shell script",
    "start": "427509",
    "end": "433059"
  },
  {
    "text": "and this command was run but it also provided a way in which these shell scripts could be you know customized for",
    "start": "433059",
    "end": "438309"
  },
  {
    "text": "our use cases so it was really really good that way so we took these shell scripts we took this repo we said okay",
    "start": "438309",
    "end": "444249"
  },
  {
    "text": "let's see if this even that works so we put this on a cluster here and this is basically all of these were ml files so",
    "start": "444249",
    "end": "449559"
  },
  {
    "text": "all we had to do was take them and do a cube serial apply and everything was running fine we did our basic tests you",
    "start": "449559",
    "end": "454719"
  },
  {
    "text": "know we ran we created a topic we tried to write to a topic it was great we'd rather write from the topic it was",
    "start": "454719",
    "end": "459759"
  },
  {
    "text": "working fine so it seemed really really stable we did a little more than that",
    "start": "459759",
    "end": "465129"
  },
  {
    "text": "where he said okay let me kill a pod and see if it comes back up it was working fine let me kill an ec2 instance and see",
    "start": "465129",
    "end": "470379"
  },
  {
    "text": "if like the auto scaling group in AWS brings the instance back does it go back and join the cluster and does it does",
    "start": "470379",
    "end": "476469"
  },
  {
    "text": "Kafka as software recover from all of this and work fine and it was really working well so we were like okay this",
    "start": "476469",
    "end": "482559"
  },
  {
    "text": "isn't sounds very promising we need to show that we have to do more work otherwise again you know job security it",
    "start": "482559",
    "end": "487659"
  },
  {
    "text": "has to go on for awhile so so we decided to do a little bit of a baseline",
    "start": "487659",
    "end": "493089"
  },
  {
    "start": "490000",
    "end": "563000"
  },
  {
    "text": "experiment with this look what if we take this set up it is very rudimentary in some sense it's not production ready",
    "start": "493089",
    "end": "499839"
  },
  {
    "text": "it was kind of day you know five or ten of the getting off the ground what do we",
    "start": "499839",
    "end": "505569"
  },
  {
    "text": "do in terms of kind of finding where we are at so we decided to put the system under a little bit of a load and this",
    "start": "505569",
    "end": "511269"
  },
  {
    "text": "was the experiment that we decided to run with this so the setup basically had five ec2 instances running Kafka brokers",
    "start": "511269",
    "end": "517328"
  },
  {
    "text": "are 4.2 extra ROG which are like 8 V CPUs and 61 gigabytes of memory spread across three zones each broker",
    "start": "517329",
    "end": "524529"
  },
  {
    "text": "the JVM was getting 8 GB CPUs and 6 gigs of memory the application factor of 3 we never were going to be able to run",
    "start": "524529",
    "end": "530680"
  },
  {
    "text": "without like having 3 up in three different zones ten producers and Simas in size of 10 kilobytes the",
    "start": "530680",
    "end": "537850"
  },
  {
    "text": "node on the other side basically talks about how we did not have SSL we did not have background stress I'll go a little",
    "start": "537850",
    "end": "543610"
  },
  {
    "text": "bit or what about what background stress means in details in a little later but fundamentally like the kubernetes",
    "start": "543610",
    "end": "549220"
  },
  {
    "text": "cluster did not have anything else running on it it was only the brokers and zookeeper that was running on this and there was no compression so the data",
    "start": "549220",
    "end": "555970"
  },
  {
    "text": "would go in it would go into a topic and then consumers would consume from there and it would just like consume the data",
    "start": "555970",
    "end": "561220"
  },
  {
    "text": "as is and this was what we got so the end-to-end latency that the y-axis",
    "start": "561220",
    "end": "567640"
  },
  {
    "start": "563000",
    "end": "720000"
  },
  {
    "text": "represents here is basically the latency measured from the time a producer produces a message puts it onto the bus",
    "start": "567640",
    "end": "573070"
  },
  {
    "text": "there is a consumer which consumes this message and actually reads the message itself has a timestamp the consumer",
    "start": "573070",
    "end": "579160"
  },
  {
    "text": "reads this message looks at its own timestamp looks at the difference between the timestamps and then it knows what was the end-to-end latency that was",
    "start": "579160",
    "end": "585190"
  },
  {
    "text": "required for this message to go from the producer to the consumer and of course as the producer and consumer were not",
    "start": "585190",
    "end": "590740"
  },
  {
    "text": "doing anything else so there's nothing else in the system and then looking at the the latency this is what we got so",
    "start": "590740",
    "end": "597250"
  },
  {
    "text": "about 450 thousand messages we were at you know on an average at about 35 maybe",
    "start": "597250",
    "end": "602590"
  },
  {
    "text": "40 milliseconds it was a little up and down so it was a little concerning that you know ideally you would want to have",
    "start": "602590",
    "end": "608860"
  },
  {
    "text": "like a single straight line because there's nothing else happening why do some messages take longer than the other",
    "start": "608860",
    "end": "615060"
  },
  {
    "text": "one of the reasons behind that turned out to be the way AWS handles the AWS",
    "start": "615060",
    "end": "621370"
  },
  {
    "text": "nat gateways can kind of come in the way of complicity communication",
    "start": "621370",
    "end": "626400"
  },
  {
    "text": "fundamentally what happens that also was the reason why the first packet or the",
    "start": "626400",
    "end": "631810"
  },
  {
    "text": "first message always started took longer so the first message actually has a latency of about 70 75 milliseconds and",
    "start": "631810",
    "end": "638410"
  },
  {
    "text": "what we found out was the producer or the consumer is trying to connect with a",
    "start": "638410",
    "end": "644440"
  },
  {
    "text": "kafka broker when that PCB kafka is a tcp based protocol so there is a TCP",
    "start": "644440",
    "end": "649480"
  },
  {
    "text": "connection trying to happen between a system that is outside of AWS or outside of the account for sure and trying to",
    "start": "649480",
    "end": "656080"
  },
  {
    "text": "connect to the broker when that TCP connection is actually trying to be established the if for some reason AWS",
    "start": "656080",
    "end": "663670"
  },
  {
    "text": "or the broker decides to send an ICMP fragmentation needed packet because it does not like the size of the TCP packet the a SS NAT",
    "start": "663670",
    "end": "670750"
  },
  {
    "text": "gateway which was coming in the way would drop these packets on the floor so the client wouldn't even know it would just always try to continue to try and",
    "start": "670750",
    "end": "677800"
  },
  {
    "text": "establish a connection with the packet size which was bigger than what the broker liked and this communication",
    "start": "677800",
    "end": "683170"
  },
  {
    "text": "would always come in the way and the NAT gateway would drop these packets and it was hard to get there it just so happens",
    "start": "683170",
    "end": "689260"
  },
  {
    "text": "that TCP tries out different things as time goes on so it would try for some time if things didn't work it would try",
    "start": "689260",
    "end": "695590"
  },
  {
    "text": "with a different packet size and things would work fine then but it was a hard problem that we had to run into and thankfully some of these aw spokes were",
    "start": "695590",
    "end": "702310"
  },
  {
    "text": "available for us to kind of talk to and they told us about this and you know god bless TCP a Wireshark and trace dump and",
    "start": "702310",
    "end": "709570"
  },
  {
    "text": "everything but after that this was a little up and down but it is still good like it was it was good performance like",
    "start": "709570",
    "end": "716110"
  },
  {
    "text": "this was more than what I mean this was faster than what we were wanting to see so we went around to other engineering",
    "start": "716110",
    "end": "722230"
  },
  {
    "start": "720000",
    "end": "792000"
  },
  {
    "text": "teams that were going to use this with an into it and we were going to talk to them about like what are they requirements what do they want from this",
    "start": "722230",
    "end": "728050"
  },
  {
    "text": "kafka bus and after connecting with multiple different teams this was the final set of requirements that we put together we were going to have to",
    "start": "728050",
    "end": "735100"
  },
  {
    "text": "support a Kafka on kubernetes infrastructure that is going to have 9000 messages being sent per second five",
    "start": "735100",
    "end": "742270"
  },
  {
    "text": "kilobyte message size like each message that would be written would be about 50 kilobytes but given that we can use",
    "start": "742270",
    "end": "748540"
  },
  {
    "text": "compression we can have the message size on disk would be five kilobytes the retention period would be seven days",
    "start": "748540",
    "end": "753910"
  },
  {
    "text": "this was where we wanted it to be longer but given the requirements are given the constraints and the costs involved seven",
    "start": "753910",
    "end": "760210"
  },
  {
    "text": "days was kind of a happy compromise three replicas as I said we always want we cannot have data sitting in only one",
    "start": "760210",
    "end": "767050"
  },
  {
    "text": "zone in AWS they have three zones in every region or the u.s. west region which is where we were we wanted all",
    "start": "767050",
    "end": "772570"
  },
  {
    "text": "three zones to have one copy about 200 topics in Kafka and then the end-to-end latency was awesome like people were",
    "start": "772570",
    "end": "779260"
  },
  {
    "text": "like we need maximum we can I mean it's okay if the internal latency is 100 milliseconds or less so again we were",
    "start": "779260",
    "end": "786580"
  },
  {
    "text": "well below the hundred milliseconds we could have claimed victory but we didn't",
    "start": "786580",
    "end": "791880"
  },
  {
    "text": "so based on these kinds of questions we were like okay this is getting serious we need to figure out how we can now",
    "start": "791880",
    "end": "798040"
  },
  {
    "start": "792000",
    "end": "872000"
  },
  {
    "text": "take this one by towards production so this was the architecture that finally it came up to",
    "start": "798040",
    "end": "804339"
  },
  {
    "text": "so the master and so this was a kubernetes cluster that was going to have three master nodes one in every",
    "start": "804339",
    "end": "809779"
  },
  {
    "text": "zone so the master nodes that you see here that's 2a 2b and 2c one master node in each zone the zookeeper instances",
    "start": "809779",
    "end": "816620"
  },
  {
    "text": "so these zookeeper nodes that you see here it's an auto scaling group this auto scaling group had five instances",
    "start": "816620",
    "end": "821660"
  },
  {
    "text": "this is I mean it has six instances but only five of them get used do keeper is a consensus-based protocol so it needs",
    "start": "821660",
    "end": "827839"
  },
  {
    "text": "an odd number of instances we had five two zones we'll have two instances each and one zone will have one and this",
    "start": "827839",
    "end": "834439"
  },
  {
    "text": "Kafka nodes is another auto scaling group there were given all the requirements that we had based on some",
    "start": "834439",
    "end": "840829"
  },
  {
    "text": "calculations which I can share with you offline if needed but we decided to have nine brokers so nine ec2 instances one",
    "start": "840829",
    "end": "847370"
  },
  {
    "text": "broker on every instance yes it means at time set okay you know containerization",
    "start": "847370",
    "end": "853670"
  },
  {
    "text": "is all about impacting many things together but given the seriousness of how things were we wanted to be kind of",
    "start": "853670",
    "end": "858829"
  },
  {
    "text": "safe than sorry so we said okay let's start with one broker pod per instance and let's see how it goes so nine si two",
    "start": "858829",
    "end": "865459"
  },
  {
    "text": "instances one for each of these nine brokers in its own auto scaling group so",
    "start": "865459",
    "end": "873319"
  },
  {
    "start": "872000",
    "end": "982000"
  },
  {
    "text": "it's one thing to have these nine brokers but the question was we were going to have requests coming in from",
    "start": "873319",
    "end": "880370"
  },
  {
    "text": "multiple different teams or multiple different services that were not necessarily running in AWS so we needed",
    "start": "880370",
    "end": "887569"
  },
  {
    "text": "a public endpoint and how do you give a public endpoint for the nine brokers the",
    "start": "887569",
    "end": "893720"
  },
  {
    "text": "way again Kafka works is that each of these clients each of these producers or consumers needs to be able to access and",
    "start": "893720",
    "end": "899990"
  },
  {
    "text": "directly connect over TCP to each of these brokers so in order to do that one",
    "start": "899990",
    "end": "905240"
  },
  {
    "text": "option would have been to make all those nine brokers have public IPS which was not great because if those instances go",
    "start": "905240",
    "end": "912559"
  },
  {
    "text": "down new ones come up they might change their public IP we could give them static IP addresses but again you know",
    "start": "912559",
    "end": "917839"
  },
  {
    "text": "it might just increase the threat the security folks were a little uneasy with that so it was decided that a better",
    "start": "917839",
    "end": "924199"
  },
  {
    "text": "option would be to kind of have another layer in front of all these brokers and we decided to go with our network load",
    "start": "924199",
    "end": "929660"
  },
  {
    "text": "balancer so this might be something that will come up if you have Kafka setups on kubernetes that need to",
    "start": "929660",
    "end": "935130"
  },
  {
    "text": "be accessed outside of kubernetes are outside certain VPC how would you",
    "start": "935130",
    "end": "940139"
  },
  {
    "text": "present each of these brokers we chose network load balancers but there might be other options so a network load",
    "start": "940139",
    "end": "945240"
  },
  {
    "text": "balancer in Cooper net or in AWS presents in Saleh it's an option for using layer for communication TCP",
    "start": "945240",
    "end": "951120"
  },
  {
    "text": "communication this network load balancer sits in front of or sat in front of all these brokers and it would know exactly",
    "start": "951120",
    "end": "957990"
  },
  {
    "text": "how to communicate with each of these brokers individually and the way this is very similar to what a NAT is that the",
    "start": "957990",
    "end": "964589"
  },
  {
    "text": "network load balancer has a public IP out of public dns name and port p0 would",
    "start": "964589",
    "end": "969930"
  },
  {
    "text": "go to Kafka k0 port p1 would go to Kafka k1 so on and so forth it's basically DNS",
    "start": "969930",
    "end": "975029"
  },
  {
    "text": "name colon port that becomes a unique combination that can uniquely identify each of those nine brokers so so this is",
    "start": "975029",
    "end": "983760"
  },
  {
    "start": "982000",
    "end": "995000"
  },
  {
    "text": "just a little bit of a recap so Kafka and zookeeper an independent auto-scaling groups there's one Kafka broker per ec2 instance one zookeeper",
    "start": "983760",
    "end": "991490"
  },
  {
    "text": "zookeeper node for ec2 instance so let me show you a little bit of this in",
    "start": "991490",
    "end": "996810"
  },
  {
    "start": "995000",
    "end": "1006000"
  },
  {
    "text": "action I have this cluster here with me right now let me just extend this SC so",
    "start": "996810",
    "end": "1006199"
  },
  {
    "start": "1006000",
    "end": "1061000"
  },
  {
    "text": "I have this kubernetes cluster here right now in this you can see that the stateful set which runs in the nine",
    "start": "1006199",
    "end": "1012440"
  },
  {
    "text": "brokers you can see Kafka CAFTA zero through Kafka eight as the eight as the",
    "start": "1012440",
    "end": "1017690"
  },
  {
    "text": "nine broker pods there is the p zu 0 2 p zu 4 which are the 5 zookeeper instances",
    "start": "1017690",
    "end": "1025040"
  },
  {
    "text": "and then this is the extensibility part of kubernetes where we use Yahoo Kafka manager and zookeeper web UI I'll show",
    "start": "1025040",
    "end": "1030678"
  },
  {
    "text": "the actual you guys for doing this a little bit of in a little later so there are if you do if you look for",
    "start": "1030679",
    "end": "1037610"
  },
  {
    "text": "the stateful sets there are two stateful sets if you look for config Maps there are a bunch of config maps the broker",
    "start": "1037610",
    "end": "1043370"
  },
  {
    "text": "config is the config map used by the brokers the zookeeper config is basically the config map used by",
    "start": "1043370",
    "end": "1048590"
  },
  {
    "text": "zookeeper and it has the server dot property is needed for zookeeper on",
    "start": "1048590",
    "end": "1053830"
  },
  {
    "text": "yeah I had screenshots for this I wonder why so ok so this is going back to me",
    "start": "1056500",
    "end": "1064610"
  },
  {
    "start": "1061000",
    "end": "1124000"
  },
  {
    "text": "and it'll be configuration that I was talking to you about so the NLP quantification took a little time to understand and figure out fundamentally",
    "start": "1064610",
    "end": "1070670"
  },
  {
    "text": "as I said it'll be basically sits in front of every broker or it kind of acts like an at the NLP has these things",
    "start": "1070670",
    "end": "1076520"
  },
  {
    "text": "called target groups so we create a target group for each broker : port a",
    "start": "1076520",
    "end": "1082790"
  },
  {
    "text": "port so if you see here a port 3 2 4 0 0 is referencing broker 0 3 2 4 0 1 is",
    "start": "1082790",
    "end": "1089210"
  },
  {
    "text": "referencing broker 1 so the name of the NLB and that's this port number uniquely identifies each broker Kafka also needs",
    "start": "1089210",
    "end": "1096500"
  },
  {
    "text": "a client port essentially this is used for discovery so clients of any producer",
    "start": "1096500",
    "end": "1102650"
  },
  {
    "text": "or consumer needs to be able to connect on a fixed number of fixed port where it goes and says hey give me all the Kafka",
    "start": "1102650",
    "end": "1109340"
  },
  {
    "text": "brokers that are currently live in the system so this is called a client ports of the clock Kafka client port was port",
    "start": "1109340",
    "end": "1115880"
  },
  {
    "text": "9 0 9 4 so that also kind of became an additional target group in the NLB",
    "start": "1115880",
    "end": "1120890"
  },
  {
    "text": "configuration so this was great so we had a Kafka set up we had a stateful",
    "start": "1120890",
    "end": "1127010"
  },
  {
    "start": "1124000",
    "end": "1264000"
  },
  {
    "text": "said config Maps all that good stuff the NLB was working great but again into it is a is a financial",
    "start": "1127010",
    "end": "1133070"
  },
  {
    "text": "services company we need to talk about security and in fact regardless of whether you are a financial company or",
    "start": "1133070",
    "end": "1138799"
  },
  {
    "text": "not you should talk about security so two things first and foremost authentication how do we know which",
    "start": "1138799",
    "end": "1144380"
  },
  {
    "text": "producer or consumer is allowed to be able to write to this Kafka endpoint so",
    "start": "1144380",
    "end": "1149750"
  },
  {
    "text": "there are a bunch of ways in which this can be done and I wish you like you know there was time for kind of looking at",
    "start": "1149750",
    "end": "1155000"
  },
  {
    "text": "each and every single option trying it out getting some results and then coming up with the most optimal one but again",
    "start": "1155000",
    "end": "1161120"
  },
  {
    "text": "given the constraints of time the most feasible one that seemed to kind of fit the bill that we had was mutual TLS so",
    "start": "1161120",
    "end": "1168410"
  },
  {
    "text": "mutual TLS basically requires the brokers to know about every single client that can connect with them and",
    "start": "1168410",
    "end": "1173990"
  },
  {
    "text": "each client it needs to know needs to have access to the broker so that they can present this certificate to the",
    "start": "1173990",
    "end": "1180290"
  },
  {
    "text": "client and say that ok I am Who I am check that I am allowed and then let me in and then let me allow let me publish",
    "start": "1180290",
    "end": "1187280"
  },
  {
    "text": "the bus or let me read from the Kafka broker mutual TLS was a great way of doing this it adds a little bit of an overhead when",
    "start": "1187280",
    "end": "1193670"
  },
  {
    "text": "it comes to onboarding clients onto the Kafka instance so when people when new clients want to come in and there is a",
    "start": "1193670",
    "end": "1199130"
  },
  {
    "text": "little bit of a manual process where we have to like create a certificate and give it to them and let the brokers know that okay there's a new client that you",
    "start": "1199130",
    "end": "1204980"
  },
  {
    "text": "will see but it's okay for the requirements that we had this seemed to be okay so that that takes care of two",
    "start": "1204980",
    "end": "1212270"
  },
  {
    "text": "things it takes care of authentication it also takes care of the fact that over the wire communication is happening over SSL so therefore it's all encrypted both",
    "start": "1212270",
    "end": "1220100"
  },
  {
    "text": "ways so therefore it makes it super secure and then what about data at rest",
    "start": "1220100",
    "end": "1225370"
  },
  {
    "text": "so this is where again the kubernetes was awesome so kubernetes has integrates",
    "start": "1225370",
    "end": "1230960"
  },
  {
    "text": "very well with AWS AWS has way in which an EBS volume can be encrypted at rest",
    "start": "1230960",
    "end": "1236270"
  },
  {
    "text": "all you have to do is create an encryption key in Amazon kms and then use that key for the EBS volume and tell",
    "start": "1236270",
    "end": "1243080"
  },
  {
    "text": "the EBS volume to in confusing that kubernetes the storage class object in kubernetes has an option for doing this",
    "start": "1243080",
    "end": "1249440"
  },
  {
    "text": "it just is literally is one line which has encryption equal to true and that storage class will then or when the pure persistent volume gets created the key",
    "start": "1249440",
    "end": "1256400"
  },
  {
    "text": "gets created and again it then gets used for encryption of EBS volume so we basically just use that so it's one",
    "start": "1256400",
    "end": "1264530"
  },
  {
    "text": "thing to get okay the security folks are happy but then you know if you want to do you know real traffic in production",
    "start": "1264530",
    "end": "1271640"
  },
  {
    "text": "we need visibility we need to know about what's happening on the cluster what's happening in terms of logs we need to",
    "start": "1271640",
    "end": "1278030"
  },
  {
    "text": "know about what are the metrics I'm getting and if something goes wrong that's going to be my number one place to go to in terms of figuring out what",
    "start": "1278030",
    "end": "1284720"
  },
  {
    "text": "is wrong so for logging and monitoring into it has has had a history of using",
    "start": "1284720",
    "end": "1290390"
  },
  {
    "text": "Splunk and other integral Intuit services were using Splunk so we decided to kind of go in the same way but it",
    "start": "1290390",
    "end": "1296480"
  },
  {
    "text": "also helps with you know being consistent across the board we run a fluently daemon said on the kubernetes",
    "start": "1296480",
    "end": "1302929"
  },
  {
    "text": "cluster it's configured to write data to an Splunk h HEC endpoint and the flow",
    "start": "1302929",
    "end": "1309290"
  },
  {
    "text": "indeed even said also each pod takes the logs from each of these instances running in the cluster",
    "start": "1309290",
    "end": "1314540"
  },
  {
    "text": "takes this logs writes it to the HCC endpoint and the HCC endpoint then puts the data into Splunk and then Kafka and",
    "start": "1314540",
    "end": "1321650"
  },
  {
    "text": "zookeeper logs from instances were basically written by flu in D into Splunk so that way we were getting all",
    "start": "1321650",
    "end": "1327650"
  },
  {
    "text": "these all the requests that we are all the logs that we wanted were there and then for monitoring so again you know",
    "start": "1327650",
    "end": "1333740"
  },
  {
    "text": "other applications with an Intuit have had used wavefront so we kind of continue to do this wavefront is a",
    "start": "1333740",
    "end": "1339200"
  },
  {
    "text": "service that I think last year or a year before that was acquired by VMware and",
    "start": "1339200",
    "end": "1344540"
  },
  {
    "text": "it's a great-sized service it gives you a good it stores a time series database and it shows a great way of charts and",
    "start": "1344540",
    "end": "1351740"
  },
  {
    "text": "growing charts and graphs of the metrics that you have pushed to it it also integrates very well with pager duty and",
    "start": "1351740",
    "end": "1356840"
  },
  {
    "text": "things like that so we decided to use that the metrics had to be collected at",
    "start": "1356840",
    "end": "1362240"
  },
  {
    "text": "two levels so there's one about the cluster level metrics so fundamentally how many nodes do I have what's the",
    "start": "1362240",
    "end": "1368060"
  },
  {
    "text": "memory and CPU utilization of each node what about the disk space and so on and so forth more at the infrastructure level and then secondly",
    "start": "1368060",
    "end": "1374650"
  },
  {
    "text": "application-specific metrics so within Kafka like how many leader elections am I getting what's the what's the latency",
    "start": "1374650",
    "end": "1381980"
  },
  {
    "text": "between my two brokers in terms of replication what is my GC time and things like that so application level",
    "start": "1381980",
    "end": "1387170"
  },
  {
    "text": "metrics had to be collected and sent to wavefront so for both of these we found",
    "start": "1387170",
    "end": "1393080"
  },
  {
    "text": "ways of doing this hipster I know it's hipster recently has been deprecated but heap store has a way in",
    "start": "1393080",
    "end": "1399680"
  },
  {
    "text": "which it can write data to wavefront so at the for the cluster level metrics we run hipster it collects all these",
    "start": "1399680",
    "end": "1405680"
  },
  {
    "text": "metrics and send ships it to a friend for application level metrics we use Telegraph so telegraph runs as a sidecar",
    "start": "1405680",
    "end": "1411620"
  },
  {
    "text": "container to each of these brokers it collects all these metrics jmx matrix from Kafka and sends it to a friend I'll",
    "start": "1411620",
    "end": "1417800"
  },
  {
    "text": "show them of this in white okay so this again I wonder why I have the screenshots but let me show this right",
    "start": "1417800",
    "end": "1423800"
  },
  {
    "text": "here so so to start with this is the so",
    "start": "1423800",
    "end": "1432920"
  },
  {
    "text": "this is the dashboard this is a kubernetes cluster level dashboard within wave front so this particular",
    "start": "1432920",
    "end": "1438470"
  },
  {
    "start": "1433000",
    "end": "1470000"
  },
  {
    "text": "cluster that for which I am demoing it has three namespaces 23 different nodes it has 117 pods and a bunch of other",
    "start": "1438470",
    "end": "1444380"
  },
  {
    "text": "information about you know pod count by namespace what's the CP usage by each of",
    "start": "1444380",
    "end": "1449390"
  },
  {
    "text": "these namespaces memory usage by each node again giving you a view of the way",
    "start": "1449390",
    "end": "1455960"
  },
  {
    "text": "the entire in or the kubernetes infrastructure is is currently is currently being used and",
    "start": "1455960",
    "end": "1463290"
  },
  {
    "text": "then there are ways in which you can go actually go back in time or find a specific time slot and do this",
    "start": "1463290",
    "end": "1468990"
  },
  {
    "text": "there is another dashboard that wavefront has which is specific to a namespace so the the pods that I showed",
    "start": "1468990",
    "end": "1474600"
  },
  {
    "text": "earlier are running in namespace called cough Kardashian s so for this cluster in the namespace called cough kind s we",
    "start": "1474600",
    "end": "1480330"
  },
  {
    "text": "have 37 container 37 containers and then within that namespace how much is the",
    "start": "1480330",
    "end": "1486450"
  },
  {
    "text": "network traffic what's the memory and CPU rate and things like that and then lastly the Kafka specific",
    "start": "1486450",
    "end": "1494160"
  },
  {
    "start": "1492000",
    "end": "1540000"
  },
  {
    "text": "matrix so there is as I said telegraph is run as a sidecar container which collects Kafka specific matrix sends it",
    "start": "1494160",
    "end": "1500460"
  },
  {
    "text": "to this - - wavefront so currently in my Kafka cluster there are 0 under",
    "start": "1500460",
    "end": "1506460"
  },
  {
    "text": "replicated partitions 0 offline partitions which is a good thing if this",
    "start": "1506460",
    "end": "1511770"
  },
  {
    "text": "number goes up we have a wave we have an alert that is hooked up with pager duty",
    "start": "1511770",
    "end": "1516960"
  },
  {
    "text": "which then calls someone I am on call right now I hope it doesn't it doesn't fire right now hopefully and then these",
    "start": "1516960",
    "end": "1523650"
  },
  {
    "text": "are specific metrics or metrics specific to - Kafka so the request throughput here is actual requests of Kafka like",
    "start": "1523650",
    "end": "1530970"
  },
  {
    "text": "when I'm trying to write a message or read a message from a Kafka bus what what's the throughput that I am getting",
    "start": "1530970",
    "end": "1536550"
  },
  {
    "text": "so on and so forth and then space and about wavefront so flu in D is used to",
    "start": "1536550",
    "end": "1543540"
  },
  {
    "start": "1540000",
    "end": "1598000"
  },
  {
    "text": "for collecting these logs and shipping it to wavefront the other good thing that flow indy does in this case for us is that it doesn't it it doesn't just",
    "start": "1543540",
    "end": "1550080"
  },
  {
    "text": "take the log message but it also kind of augments it with a lot of metadata about where the message came from which was",
    "start": "1550080",
    "end": "1556170"
  },
  {
    "text": "awesome so this may be the actual message that came up from the from the pod but it kind of puts a lot of other",
    "start": "1556170",
    "end": "1562230"
  },
  {
    "text": "details like what is the broke or of sorry what is the pod it came from so this particular message came from Kafka 7 from the namespace called Kafka - eNOS",
    "start": "1562230",
    "end": "1570440"
  },
  {
    "text": "from this cluster called Cherie Kafka test cluster which is my cluster and which is great because now this gives me",
    "start": "1570440",
    "end": "1576960"
  },
  {
    "text": "the ability to kind of drill down into the Met are the logs of a specific pod",
    "start": "1576960",
    "end": "1582150"
  },
  {
    "text": "or of a specific namespace or what have you and then kind of slice and dice all this information if and when something",
    "start": "1582150",
    "end": "1587850"
  },
  {
    "text": "goes wrong so this was really cool",
    "start": "1587850",
    "end": "1592470"
  },
  {
    "start": "1598000",
    "end": "1712000"
  },
  {
    "text": "so this this was great in the sense that you know once the logging and monitoring data was also kind of getting to kind of",
    "start": "1598640",
    "end": "1604909"
  },
  {
    "text": "these there's the final destinations wherever we wanted it to we had a cluster we had an NLB we had",
    "start": "1604909",
    "end": "1610580"
  },
  {
    "text": "access figured out we had security figure out logging and malting was great but there were always these last mile",
    "start": "1610580",
    "end": "1616669"
  },
  {
    "text": "problems an applicant some other team would come to us and say that hey we want to create a new topic and the only way we had to",
    "start": "1616669",
    "end": "1622580"
  },
  {
    "text": "tell them was to kind of do these obscure ways of telling them that ok here is a cube config now you you know",
    "start": "1622580",
    "end": "1627620"
  },
  {
    "text": "they run these cube CTL commands do a port forwarding and then ssh into the pod and then run these obscure",
    "start": "1627620",
    "end": "1633159"
  },
  {
    "text": "command-line tools that kafka provides for creating a topic which was hard and it was like it took time and effort and",
    "start": "1633159",
    "end": "1639620"
  },
  {
    "text": "it was very error-prone so for these kinds of things again extensibility of kubernetes is phenomenal",
    "start": "1639620",
    "end": "1646010"
  },
  {
    "text": "so we found a yahoo kafka manager as this as a tool that is made available that is open source by Yahoo for doing",
    "start": "1646010",
    "end": "1653539"
  },
  {
    "text": "these kinds of these kinds of these kinds of tasks so you how kafka manager was an open is an open source tool we",
    "start": "1653539",
    "end": "1659269"
  },
  {
    "text": "take it and run it inside the cluster and it makes it easy because it gives you a web-based front-end or web-based UI for for doing these kinds of tasks",
    "start": "1659269",
    "end": "1666440"
  },
  {
    "text": "like creating a pod or sorry creating a topic configuring the topic a certain way and things like that same with zookeeper when people had",
    "start": "1666440",
    "end": "1673100"
  },
  {
    "text": "problems with Casca they would look for solutions and the moment you Google about a POC Africa problem one of the",
    "start": "1673100",
    "end": "1679460"
  },
  {
    "text": "most common reasons people will find out one of the most common problems people will say is oh go look at zookeeper and",
    "start": "1679460",
    "end": "1685010"
  },
  {
    "text": "getting zookeeper getting to zookeeper was a problem because getting like a zookeeper also has a has a command-line",
    "start": "1685010",
    "end": "1690679"
  },
  {
    "text": "tool but it's super hard to use it so we found a zookeeper web UI and it was",
    "start": "1690679",
    "end": "1695809"
  },
  {
    "text": "great to get the zookeeper rebel us because now we could just go and tell people to go to the web UI and they would see basing information that was",
    "start": "1695809",
    "end": "1701570"
  },
  {
    "text": "stored in Sioux Keeper I'll talk a little bit about I'll go in a minute but let me just show this so let's say I'm",
    "start": "1701570",
    "end": "1707809"
  },
  {
    "text": "doing a port forwarding of so on my cluster so this is what the zookeeper",
    "start": "1707809",
    "end": "1713779"
  },
  {
    "start": "1712000",
    "end": "1742000"
  },
  {
    "text": "web UI shows right now I'm just at one particular path called slash blocker",
    "start": "1713779",
    "end": "1720110"
  },
  {
    "text": "slash IDs so it shows me that right now I have nine nodes in the cluster that's the zookeepers view and there are other",
    "start": "1720110",
    "end": "1725600"
  },
  {
    "text": "things what about topics I have two topics right now called consumer offsets and topic one because",
    "start": "1725600",
    "end": "1730620"
  },
  {
    "text": "the test plaster that I have but other information stored within zookeeper if people have doubts and questions this",
    "start": "1730620",
    "end": "1735990"
  },
  {
    "text": "was great to see this and then so yeah",
    "start": "1735990",
    "end": "1742559"
  },
  {
    "start": "1742000",
    "end": "1767000"
  },
  {
    "text": "how Capcom manager has something like this this shows up again it's a very bare-bones UI but but it works great for",
    "start": "1742559",
    "end": "1748590"
  },
  {
    "text": "the use cases that we had about where people can use this look at what topics they have look at the lag configure new",
    "start": "1748590",
    "end": "1754500"
  },
  {
    "text": "topics if they want to create one so if you do go here and create a new topic you can create new topics here configure it the way you want so this was good",
    "start": "1754500",
    "end": "1761250"
  },
  {
    "text": "because it kind of gave people easy access to some of this information inside the cluster and then lastly our",
    "start": "1761250",
    "end": "1768630"
  },
  {
    "start": "1767000",
    "end": "1827000"
  },
  {
    "text": "go so I am biased I work for a team that built our go as the workflow engine for",
    "start": "1768630",
    "end": "1774240"
  },
  {
    "text": "kubernetes and one of the questions people always had was even if I have access to a Kafka cluster how do I do",
    "start": "1774240",
    "end": "1779520"
  },
  {
    "text": "basic operations or basic tests on this cluster if I want to just loop like you know create a topic right you know 100",
    "start": "1779520",
    "end": "1785790"
  },
  {
    "text": "messages on it 300 messages from it how do I know that this is actually completing and doing well so we used our",
    "start": "1785790",
    "end": "1791970"
  },
  {
    "text": "go for this as I said so workflow engine for kubernetes we created one workflow for basic tests we created another",
    "start": "1791970",
    "end": "1798270"
  },
  {
    "text": "workflow for a stress test and we created few other workflows for specific needs that people had and now we could",
    "start": "1798270",
    "end": "1803850"
  },
  {
    "text": "share these workflows with people and say that ok if you ever want to try out something different take this as a",
    "start": "1803850",
    "end": "1809250"
  },
  {
    "text": "sample workflow and you can change it augmented create a new copy a workflow as a file so we shared this file it's a",
    "start": "1809250",
    "end": "1814559"
  },
  {
    "text": "ml file take it change it tweak it the way you want and run this workflow on the cluster and once you run this workflow on the cluster you will be able",
    "start": "1814559",
    "end": "1820950"
  },
  {
    "text": "to get some you know visibility into how your infrastructure or the kafir' cluster looks like and how it's running",
    "start": "1820950",
    "end": "1826640"
  },
  {
    "text": "so this was great so we've had the last mile problems covered people were happy but what about performance we started",
    "start": "1826640",
    "end": "1833670"
  },
  {
    "start": "1827000",
    "end": "1908000"
  },
  {
    "text": "off with this goal that we want to do this with an end-to-end latency of 100 milliseconds so we're after all of this",
    "start": "1833670",
    "end": "1839610"
  },
  {
    "text": "we said let's run this performance experiment and see where we are with respect to baseline so this is the exam",
    "start": "1839610",
    "end": "1846150"
  },
  {
    "text": "the sample setup for the performance experiment we had nine instances are for 2x / 8 V CPUs spread across three zones",
    "start": "1846150",
    "end": "1853260"
  },
  {
    "text": "X equal to all the application factor 3 and so on and so forth and then the no",
    "start": "1853260",
    "end": "1858620"
  },
  {
    "text": "this time it was with SSL so it was going to add a lot more computation for",
    "start": "1858620",
    "end": "1864120"
  },
  {
    "text": "the TCP connection for the ssl overhead for decrypting the data and and everything there was due types of experiments done",
    "start": "1864120",
    "end": "1872039"
  },
  {
    "text": "with and without background stress so basically background stress was we were measuring the latency between ten",
    "start": "1872039",
    "end": "1879330"
  },
  {
    "text": "producers and ten consumers but at the same time there were three other producers generating about 4,500",
    "start": "1879330",
    "end": "1885150"
  },
  {
    "text": "messages and writing them into Kafka so we're generating a bunch of activity within Kafka and kubernetes for other things but measuring the latency for",
    "start": "1885150",
    "end": "1891960"
  },
  {
    "text": "these producers because that was going to be the common case and then snappy compression as because I mean among the",
    "start": "1891960",
    "end": "1898980"
  },
  {
    "text": "compression techniques Kafka supports we picked one we experimented with it snappy was the best one for the for the",
    "start": "1898980",
    "end": "1904620"
  },
  {
    "text": "type of JSON objects that we were sending so we decided to use snappy and this is what we got",
    "start": "1904620",
    "end": "1909630"
  },
  {
    "start": "1908000",
    "end": "1989000"
  },
  {
    "text": "so the two lines here the blue one is basically the end-to-end latency with background stress so it is also up and",
    "start": "1909630",
    "end": "1916559"
  },
  {
    "text": "down but some of that is attributed to the fact that there is a background stress there is a bunch of activity",
    "start": "1916559",
    "end": "1921659"
  },
  {
    "text": "happening behind the scenes and it's a little up and down we need we actually should go even deeper than what we know",
    "start": "1921659",
    "end": "1926669"
  },
  {
    "text": "of today but even then the average latency if you look at that is about 37 milliseconds which is still well below",
    "start": "1926669",
    "end": "1932909"
  },
  {
    "text": "100 millisecond mark that we were aiming for the good really the good part although was the red line actually now",
    "start": "1932909",
    "end": "1938909"
  },
  {
    "text": "shows so once we fixed the nat gateway i mean we didn't fix the nat gateway problem all we had to do is make sure that the jumbo frames was enabled across",
    "start": "1938909",
    "end": "1947130"
  },
  {
    "text": "all the components involved jumbo frames were enabled on the ec2 instance jumbo frames were enabled in calico where you",
    "start": "1947130",
    "end": "1953070"
  },
  {
    "text": "had to set the size of the tcp the tcp MTU size once it was set to 8 1 9 2",
    "start": "1953070",
    "end": "1958230"
  },
  {
    "text": "bytes the the like the nat gateway being a problem in between went away and we",
    "start": "1958230",
    "end": "1964320"
  },
  {
    "text": "now have a much more consistent red line which is the much more consistent performance as the baseline so this was",
    "start": "1964320",
    "end": "1970799"
  },
  {
    "text": "great this is I mean yes there is a little there's some work there's more work to be done in figuring out what what else can be done to kind of bring",
    "start": "1970799",
    "end": "1978029"
  },
  {
    "text": "this latency even below where it is at but compared to where we had begun and compared to the goal that was set for us",
    "start": "1978029",
    "end": "1984299"
  },
  {
    "text": "we were well within that range so at 37 milliseconds this was really working with so if you were to go ahead and",
    "start": "1984299",
    "end": "1991770"
  },
  {
    "start": "1989000",
    "end": "2038000"
  },
  {
    "text": "start doing this I would say watch out for these small small but some things",
    "start": "1991770",
    "end": "1997049"
  },
  {
    "text": "that can you know can hurt watch out for these small tips JMX metrics are not supported by",
    "start": "1997049",
    "end": "2003120"
  },
  {
    "text": "directly by monitoring services you might have to figure out how to convert them between jmx format and some other format that",
    "start": "2003120",
    "end": "2009460"
  },
  {
    "text": "the modeling service understands NAT gateways on AWS don't support TCP flag my IP fragmentation TCP MDU sizes may",
    "start": "2009460",
    "end": "2017080"
  },
  {
    "text": "not be set correctly so you'll have to go ahead and look at that and modify it if needed and then log messages so this happened once where we were we were",
    "start": "2017080",
    "end": "2024400"
  },
  {
    "text": "debugging the Kafka problem we changed the log level of the brokers to debug",
    "start": "2024400",
    "end": "2029920"
  },
  {
    "text": "and it kind of overwhelms plunk and like it started throwing all kinds of errors the log message rate from the brokers",
    "start": "2029920",
    "end": "2035560"
  },
  {
    "text": "can be very high so I wish I could give you a good there could be a conclusion",
    "start": "2035560",
    "end": "2041320"
  },
  {
    "start": "2038000",
    "end": "2058000"
  },
  {
    "text": "where I make you like super stoked about this and make you run back to to your offices and install Kafka on kubernetes",
    "start": "2041320",
    "end": "2047370"
  },
  {
    "text": "but my high-level point is it's possible to do this so you can absolutely do this",
    "start": "2047370",
    "end": "2052419"
  },
  {
    "text": "it's a lot of fun to do this so just go ahead and try it out and lastly thank",
    "start": "2052420",
    "end": "2058840"
  },
  {
    "text": "you so much for being here if you have questions we have some time I'm also available outside after this and we are",
    "start": "2058840",
    "end": "2064000"
  },
  {
    "text": "hiring it into it thank you",
    "start": "2064000",
    "end": "2067440"
  },
  {
    "text": "yes",
    "start": "2070849",
    "end": "2073849"
  }
]