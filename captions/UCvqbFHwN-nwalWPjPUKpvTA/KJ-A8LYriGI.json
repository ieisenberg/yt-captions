[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "um all right so hi my name is Kim thank",
    "start": "30",
    "end": "4500"
  },
  {
    "text": "you guys for being here",
    "start": "4500",
    "end": "6480"
  },
  {
    "text": "this talk is called IBBs for scaled",
    "start": "6480",
    "end": "9150"
  },
  {
    "text": "private cloud load balancing I'll be",
    "start": "9150",
    "end": "11460"
  },
  {
    "text": "talking about how we solved our load",
    "start": "11460",
    "end": "13290"
  },
  {
    "text": "balancing needs for curb Randy I'll",
    "start": "13290",
    "end": "16500"
  },
  {
    "text": "start with a brief introduction I'm a",
    "start": "16500",
    "end": "18570"
  },
  {
    "text": "systems engineer for Comcast VIPRE I've",
    "start": "18570",
    "end": "20550"
  },
  {
    "text": "been with a VIPRE group for about four",
    "start": "20550",
    "end": "22619"
  },
  {
    "text": "years I work on the platform engineering",
    "start": "22619",
    "end": "24840"
  },
  {
    "text": "team and we're responsible for the",
    "start": "24840",
    "end": "26970"
  },
  {
    "text": "design and implementation of new",
    "start": "26970",
    "end": "29130"
  },
  {
    "text": "infrastructure services for VIPRE and",
    "start": "29130",
    "end": "31650"
  },
  {
    "text": "for the past two years that have meant",
    "start": "31650",
    "end": "33360"
  },
  {
    "text": "managing kubernetes clusters in our lab",
    "start": "33360",
    "end": "35550"
  },
  {
    "text": "and scaling production VIPRE stands for",
    "start": "35550",
    "end": "40260"
  },
  {
    "start": "38000",
    "end": "77000"
  },
  {
    "text": "video IP engineering and research and we",
    "start": "40260",
    "end": "42899"
  },
  {
    "text": "manage IP video delivery we're a small",
    "start": "42899",
    "end": "45570"
  },
  {
    "text": "office base at a Denver Colorado this",
    "start": "45570",
    "end": "48450"
  },
  {
    "text": "group started in 2010 with a just a few",
    "start": "48450",
    "end": "51210"
  },
  {
    "text": "people and we've grown to over 200",
    "start": "51210",
    "end": "53699"
  },
  {
    "text": "people since then what this group does",
    "start": "53699",
    "end": "56250"
  },
  {
    "text": "is take the core products that Comcast",
    "start": "56250",
    "end": "58910"
  },
  {
    "text": "offers and implements them in software",
    "start": "58910",
    "end": "61530"
  },
  {
    "text": "and serve them as IP video this is",
    "start": "61530",
    "end": "64080"
  },
  {
    "text": "different from traditional cable",
    "start": "64080",
    "end": "66270"
  },
  {
    "text": "services in that it's a unicast",
    "start": "66270",
    "end": "67799"
  },
  {
    "text": "transmission so we'll only deliver this",
    "start": "67799",
    "end": "70590"
  },
  {
    "text": "video that a customer is currently",
    "start": "70590",
    "end": "72119"
  },
  {
    "text": "watching as opposed to delivering every",
    "start": "72119",
    "end": "74310"
  },
  {
    "text": "channel to all customers all the time so",
    "start": "74310",
    "end": "78509"
  },
  {
    "start": "77000",
    "end": "129000"
  },
  {
    "text": "I briefly want to touch on how and why",
    "start": "78509",
    "end": "80610"
  },
  {
    "text": "we began to use kubernetes in mid-2015",
    "start": "80610",
    "end": "83280"
  },
  {
    "text": "VIPRE was developing an application",
    "start": "83280",
    "end": "84960"
  },
  {
    "text": "called C DD R which is DVR on the cloud",
    "start": "84960",
    "end": "88250"
  },
  {
    "text": "the application requirements were pretty",
    "start": "88250",
    "end": "90960"
  },
  {
    "text": "aggressive with a need for around 300",
    "start": "90960",
    "end": "93600"
  },
  {
    "text": "gigabits per second of network egress",
    "start": "93600",
    "end": "95159"
  },
  {
    "text": "and at the time the architecture had",
    "start": "95159",
    "end": "97680"
  },
  {
    "text": "over 15 microservices",
    "start": "97680",
    "end": "99890"
  },
  {
    "text": "we also had really aggressive timelines",
    "start": "99890",
    "end": "102299"
  },
  {
    "text": "with a plan to deploy over four sites in",
    "start": "102299",
    "end": "104159"
  },
  {
    "text": "2016 and another 10 and 2017 and with",
    "start": "104159",
    "end": "108360"
  },
  {
    "text": "our traditional deployment model it was",
    "start": "108360",
    "end": "109799"
  },
  {
    "text": "going to be extremely expensive and",
    "start": "109799",
    "end": "111689"
  },
  {
    "text": "time-consuming to deploy kubernetes was",
    "start": "111689",
    "end": "114899"
  },
  {
    "text": "going to provide us a flexible way to",
    "start": "114899",
    "end": "116549"
  },
  {
    "text": "deploy this architecture that would",
    "start": "116549",
    "end": "118290"
  },
  {
    "text": "ultimately include over 2,500 pods per",
    "start": "118290",
    "end": "121829"
  },
  {
    "text": "site so VIPRE created an architecture",
    "start": "121829",
    "end": "124530"
  },
  {
    "text": "with most of these micro services",
    "start": "124530",
    "end": "125969"
  },
  {
    "text": "running kubernetes and we ran with it as",
    "start": "125969",
    "end": "129530"
  },
  {
    "text": "we started to build out our COO brandies",
    "start": "129530",
    "end": "131970"
  },
  {
    "text": "clusters in the lab",
    "start": "131970",
    "end": "133440"
  },
  {
    "text": "container adoption grew rapidly and it",
    "start": "133440",
    "end": "135600"
  },
  {
    "text": "seemed that overnight we went from",
    "start": "135600",
    "end": "137070"
  },
  {
    "text": "single currency to multi-tenancy and it",
    "start": "137070",
    "end": "139830"
  },
  {
    "text": "was really great that we were running so",
    "start": "139830",
    "end": "141480"
  },
  {
    "text": "many things in kubernetes but not all of",
    "start": "141480",
    "end": "143840"
  },
  {
    "text": "Comcast's enterprise services were",
    "start": "143840",
    "end": "146340"
  },
  {
    "text": "running in kubernetes and we needed a",
    "start": "146340",
    "end": "148470"
  },
  {
    "text": "reliable way for these other services to",
    "start": "148470",
    "end": "150840"
  },
  {
    "text": "consume a service that was running in",
    "start": "150840",
    "end": "152610"
  },
  {
    "text": "our clusters also at the time cdb are on",
    "start": "152610",
    "end": "155850"
  },
  {
    "text": "its own had three points of ingress and",
    "start": "155850",
    "end": "157650"
  },
  {
    "text": "as a number of tenants and applications",
    "start": "157650",
    "end": "160080"
  },
  {
    "text": "drew so does this number of ingress",
    "start": "160080",
    "end": "161670"
  },
  {
    "text": "points and we quickly realized that we",
    "start": "161670",
    "end": "165270"
  },
  {
    "text": "were going to need to address ingress",
    "start": "165270",
    "end": "166830"
  },
  {
    "text": "load balancing and also in a way that",
    "start": "166830",
    "end": "169170"
  },
  {
    "text": "made a multi-tenancy more efficient so",
    "start": "169170",
    "end": "173130"
  },
  {
    "start": "172000",
    "end": "209000"
  },
  {
    "text": "what do we need out of this load",
    "start": "173130",
    "end": "174030"
  },
  {
    "text": "balancer we needed something that would",
    "start": "174030",
    "end": "176580"
  },
  {
    "text": "support at least CVC DVRs 300 gigabits a",
    "start": "176580",
    "end": "179850"
  },
  {
    "text": "second of egress it needed to handle TCP",
    "start": "179850",
    "end": "182520"
  },
  {
    "text": "and UDP connections they needed to",
    "start": "182520",
    "end": "185100"
  },
  {
    "text": "handle many connections possibly over a",
    "start": "185100",
    "end": "187200"
  },
  {
    "text": "million concurrent and multi-tenancy it",
    "start": "187200",
    "end": "190140"
  },
  {
    "text": "needed to be configured rapidly and",
    "start": "190140",
    "end": "191790"
  },
  {
    "text": "online with no downtime and lastly we",
    "start": "191790",
    "end": "194850"
  },
  {
    "text": "needed a solution that was going to",
    "start": "194850",
    "end": "196020"
  },
  {
    "text": "provide some government governance most",
    "start": "196020",
    "end": "198600"
  },
  {
    "text": "of our tenants are not data center",
    "start": "198600",
    "end": "200670"
  },
  {
    "text": "operators so restricting the pool of IP",
    "start": "200670",
    "end": "203340"
  },
  {
    "text": "addresses available for load balancing",
    "start": "203340",
    "end": "204870"
  },
  {
    "text": "was really important we started looking",
    "start": "204870",
    "end": "208020"
  },
  {
    "text": "for options we looked at a lot of things",
    "start": "208020",
    "end": "211110"
  },
  {
    "start": "209000",
    "end": "244000"
  },
  {
    "text": "and came to the clusion that there",
    "start": "211110",
    "end": "212520"
  },
  {
    "text": "weren't really many solutions for us any",
    "start": "212520",
    "end": "215459"
  },
  {
    "text": "doctor or kubernetes specific solutions",
    "start": "215459",
    "end": "218070"
  },
  {
    "text": "either didn't exist at the time or were",
    "start": "218070",
    "end": "219780"
  },
  {
    "text": "pretty alpha we couldn't run with a",
    "start": "219780",
    "end": "222120"
  },
  {
    "text": "layer 7 load balancer because it wasn't",
    "start": "222120",
    "end": "223950"
  },
  {
    "text": "going to need our requirements and our",
    "start": "223950",
    "end": "225750"
  },
  {
    "text": "throughput requirements specifically and",
    "start": "225750",
    "end": "229410"
  },
  {
    "text": "most options didn't support online",
    "start": "229410",
    "end": "230880"
  },
  {
    "text": "configurations we ruled out any solution",
    "start": "230880",
    "end": "234450"
  },
  {
    "text": "that was required to run on independent",
    "start": "234450",
    "end": "237690"
  },
  {
    "text": "our separate Hardware outside of the",
    "start": "237690",
    "end": "238920"
  },
  {
    "text": "cluster because it would add a lot of",
    "start": "238920",
    "end": "240720"
  },
  {
    "text": "infrastructure complexity and cost so",
    "start": "240720",
    "end": "245760"
  },
  {
    "start": "244000",
    "end": "284000"
  },
  {
    "text": "our quest for solution landed us on IP",
    "start": "245760",
    "end": "247590"
  },
  {
    "text": "vs for anyone not familiar it's a",
    "start": "247590",
    "end": "250560"
  },
  {
    "text": "network load balancer that's implemented",
    "start": "250560",
    "end": "252120"
  },
  {
    "text": "in kernel space it provides online",
    "start": "252120",
    "end": "255209"
  },
  {
    "text": "flexible configuration supports TCP and",
    "start": "255209",
    "end": "257790"
  },
  {
    "text": "UDP for both ipv4 and ipv6 and because",
    "start": "257790",
    "end": "261359"
  },
  {
    "text": "the request routing is done in kernel",
    "start": "261359",
    "end": "263100"
  },
  {
    "text": "space it's really",
    "start": "263100",
    "end": "264270"
  },
  {
    "text": "success we also chose IPS because it was",
    "start": "264270",
    "end": "267810"
  },
  {
    "text": "a known known in VIPRE we already use it",
    "start": "267810",
    "end": "270210"
  },
  {
    "text": "in production for most of our video",
    "start": "270210",
    "end": "272099"
  },
  {
    "text": "applications and we knew that it would",
    "start": "272099",
    "end": "274139"
  },
  {
    "text": "support the use case it was also the",
    "start": "274139",
    "end": "276629"
  },
  {
    "text": "solution with the most reduced cost the",
    "start": "276629",
    "end": "279840"
  },
  {
    "text": "least amount of uncertainty and the most",
    "start": "279840",
    "end": "281400"
  },
  {
    "text": "likelihood of success so a little bit",
    "start": "281400",
    "end": "285960"
  },
  {
    "start": "284000",
    "end": "378000"
  },
  {
    "text": "more about ipbs",
    "start": "285960",
    "end": "286830"
  },
  {
    "text": "it offers a few different options for",
    "start": "286830",
    "end": "289680"
  },
  {
    "text": "load balancing and I'm not going to go",
    "start": "289680",
    "end": "290909"
  },
  {
    "text": "into here but the fastest and most",
    "start": "290909",
    "end": "293400"
  },
  {
    "text": "familiar to Viper was direct reply",
    "start": "293400",
    "end": "295940"
  },
  {
    "text": "direct reply let's of scale egress out",
    "start": "295940",
    "end": "298500"
  },
  {
    "text": "to the some of the interface bandwidth",
    "start": "298500",
    "end": "300690"
  },
  {
    "text": "for all of our back-end nodes",
    "start": "300690",
    "end": "301979"
  },
  {
    "text": "meaning that if I have 10 nodes with 10",
    "start": "301979",
    "end": "305009"
  },
  {
    "text": "gigs 10 gig interfaces I have 100 gigs",
    "start": "305009",
    "end": "307380"
  },
  {
    "text": "of egress it works by enabling back-end",
    "start": "307380",
    "end": "310199"
  },
  {
    "text": "notes to apply it directly to the client",
    "start": "310199",
    "end": "311789"
  },
  {
    "text": "instead of proxying through the master",
    "start": "311789",
    "end": "313650"
  },
  {
    "text": "load balancer where you might have a",
    "start": "313650",
    "end": "314940"
  },
  {
    "text": "limit of 10 gigs in this mode the IQ vs",
    "start": "314940",
    "end": "318419"
  },
  {
    "text": "master owns visit virtual IP and",
    "start": "318419",
    "end": "321479"
  },
  {
    "text": "advertises that zip to the router all",
    "start": "321479",
    "end": "323699"
  },
  {
    "text": "the backends are configured with that",
    "start": "323699",
    "end": "325440"
  },
  {
    "text": "this one at sleazebag interface when a",
    "start": "325440",
    "end": "327780"
  },
  {
    "text": "request comes into the master the",
    "start": "327780",
    "end": "330240"
  },
  {
    "text": "request destination MAC address is",
    "start": "330240",
    "end": "332460"
  },
  {
    "text": "replaced with the MAC address of the",
    "start": "332460",
    "end": "334080"
  },
  {
    "text": "backend and sent to that chosen back-end",
    "start": "334080",
    "end": "336870"
  },
  {
    "text": "the backend node sends its reply",
    "start": "336870",
    "end": "338969"
  },
  {
    "text": "directly back to the client not through",
    "start": "338969",
    "end": "341190"
  },
  {
    "text": "the load balancer so in this load you",
    "start": "341190",
    "end": "343409"
  },
  {
    "text": "can actually handle a more traffic into",
    "start": "343409",
    "end": "345270"
  },
  {
    "text": "the cluster than the IP vs masters",
    "start": "345270",
    "end": "347130"
  },
  {
    "text": "interface actually supports but all this",
    "start": "347130",
    "end": "350190"
  },
  {
    "text": "kind of amazing performance has a cost",
    "start": "350190",
    "end": "352469"
  },
  {
    "text": "and there are two main limitations first",
    "start": "352469",
    "end": "355650"
  },
  {
    "text": "is a requirement for layer two adjacency",
    "start": "355650",
    "end": "357539"
  },
  {
    "text": "between the master and the nodes so it",
    "start": "357539",
    "end": "360300"
  },
  {
    "text": "limits you to the number of nodes in",
    "start": "360300",
    "end": "362340"
  },
  {
    "text": "your cluster 2 as a physical signal and",
    "start": "362340",
    "end": "365659"
  },
  {
    "text": "also there's no disassembly of the",
    "start": "365659",
    "end": "368430"
  },
  {
    "text": "packet header in direct reply so you",
    "start": "368430",
    "end": "371130"
  },
  {
    "text": "can't rewrite the destination port and",
    "start": "371130",
    "end": "373020"
  },
  {
    "text": "all go into the details of the",
    "start": "373020",
    "end": "375900"
  },
  {
    "text": "implications of that a little bit later",
    "start": "375900",
    "end": "378259"
  },
  {
    "start": "378000",
    "end": "425000"
  },
  {
    "text": "but we had chosen a solution and once we",
    "start": "378259",
    "end": "381150"
  },
  {
    "text": "had we needed to get it running",
    "start": "381150",
    "end": "382259"
  },
  {
    "text": "alongside of Cooper at ease so this is",
    "start": "382259",
    "end": "384779"
  },
  {
    "text": "the docker run command for our initial",
    "start": "384779",
    "end": "386669"
  },
  {
    "text": "implementation the container runs in",
    "start": "386669",
    "end": "389130"
  },
  {
    "text": "privileged mode so that this can be",
    "start": "389130",
    "end": "391770"
  },
  {
    "text": "configured on the primary interface of",
    "start": "391770",
    "end": "393509"
  },
  {
    "text": "the master and the loopback interface of",
    "start": "393509",
    "end": "396029"
  },
  {
    "text": "the backends in the master mode IPV sadm",
    "start": "396029",
    "end": "399389"
  },
  {
    "text": "is used to configure the front-end and",
    "start": "399389",
    "end": "401190"
  },
  {
    "text": "port to listen on and also configures",
    "start": "401190",
    "end": "404430"
  },
  {
    "text": "the backends to route traffic to in",
    "start": "404430",
    "end": "407180"
  },
  {
    "text": "back-end mode the bit just gets added to",
    "start": "407180",
    "end": "409500"
  },
  {
    "text": "the loopback interface and any inbound",
    "start": "409500",
    "end": "411300"
  },
  {
    "text": "traffic coming in on the defined port",
    "start": "411300",
    "end": "413250"
  },
  {
    "text": "gets routed into kubernetes you'll",
    "start": "413250",
    "end": "415710"
  },
  {
    "text": "notice that all of the information here",
    "start": "415710",
    "end": "417990"
  },
  {
    "text": "is explicitly passed into the container",
    "start": "417990",
    "end": "419820"
  },
  {
    "text": "has command-line arguments so we had",
    "start": "419820",
    "end": "426990"
  },
  {
    "start": "425000",
    "end": "465000"
  },
  {
    "text": "load balancing great it worked and it",
    "start": "426990",
    "end": "429870"
  },
  {
    "text": "scaled but we quickly realized that the",
    "start": "429870",
    "end": "432240"
  },
  {
    "text": "solution could only work for a single",
    "start": "432240",
    "end": "433830"
  },
  {
    "text": "cluster single application deployment",
    "start": "433830",
    "end": "435710"
  },
  {
    "text": "why was this well let's start with the",
    "start": "435710",
    "end": "438899"
  },
  {
    "text": "fact that we didn't have the option to",
    "start": "438899",
    "end": "440190"
  },
  {
    "text": "port forward with ipbs",
    "start": "440190",
    "end": "441509"
  },
  {
    "text": "which meant we needed to use node ports",
    "start": "441509",
    "end": "444409"
  },
  {
    "text": "IP vs could not listen on a standard",
    "start": "444409",
    "end": "447000"
  },
  {
    "text": "HTTP port and forward to cooper nedeth",
    "start": "447000",
    "end": "449219"
  },
  {
    "text": "noght support which meant that whatever",
    "start": "449219",
    "end": "450539"
  },
  {
    "text": "no port stuff whatever a node port was",
    "start": "450539",
    "end": "455070"
  },
  {
    "text": "assigned to the service that we were",
    "start": "455070",
    "end": "456300"
  },
  {
    "text": "routing to IP vias had to listen and",
    "start": "456300",
    "end": "458610"
  },
  {
    "text": "forward on that same port so this didn't",
    "start": "458610",
    "end": "460949"
  },
  {
    "text": "really give us the flexibility that we",
    "start": "460949",
    "end": "462389"
  },
  {
    "text": "needed at the time we had some",
    "start": "462389",
    "end": "467520"
  },
  {
    "start": "465000",
    "end": "558000"
  },
  {
    "text": "additional problems with this",
    "start": "467520",
    "end": "468509"
  },
  {
    "text": "implementation so node ports are applied",
    "start": "468509",
    "end": "470969"
  },
  {
    "text": "to the cluster across name space no two",
    "start": "470969",
    "end": "473219"
  },
  {
    "text": "tenants could use the same node ports",
    "start": "473219",
    "end": "474870"
  },
  {
    "text": "and as it turns out humans are pretty",
    "start": "474870",
    "end": "477779"
  },
  {
    "text": "bad at being random and a lot of our",
    "start": "477779",
    "end": "480149"
  },
  {
    "text": "developers were very prone to choosing",
    "start": "480149",
    "end": "482009"
  },
  {
    "text": "the same node ports for their services",
    "start": "482009",
    "end": "483960"
  },
  {
    "text": "and this caused a lot of collision",
    "start": "483960",
    "end": "485399"
  },
  {
    "text": "inside the cluster the easy solution to",
    "start": "485399",
    "end": "488550"
  },
  {
    "text": "that use some randomization that",
    "start": "488550",
    "end": "490800"
  },
  {
    "text": "kubernetes supports but node ports can't",
    "start": "490800",
    "end": "494130"
  },
  {
    "text": "be randomized if we wanted to ensure",
    "start": "494130",
    "end": "495599"
  },
  {
    "text": "that a crew brandy service didn't get a",
    "start": "495599",
    "end": "498089"
  },
  {
    "text": "new port every time the service got",
    "start": "498089",
    "end": "499889"
  },
  {
    "text": "recreated",
    "start": "499889",
    "end": "500990"
  },
  {
    "text": "when we needed to provide a reliable way",
    "start": "500990",
    "end": "503729"
  },
  {
    "text": "to retrieve information from a known",
    "start": "503729",
    "end": "505529"
  },
  {
    "text": "address and port so developers because",
    "start": "505529",
    "end": "507959"
  },
  {
    "text": "of those had to statically assign node",
    "start": "507959",
    "end": "509550"
  },
  {
    "text": "ports to their services in addition we",
    "start": "509550",
    "end": "512578"
  },
  {
    "text": "were seeing inconsistency and node port",
    "start": "512579",
    "end": "514289"
  },
  {
    "text": "assignments for single applications",
    "start": "514289",
    "end": "516419"
  },
  {
    "text": "across site so we had to have specific",
    "start": "516419",
    "end": "521029"
  },
  {
    "text": "configurations for each deployment",
    "start": "521029",
    "end": "523930"
  },
  {
    "text": "another problem that we had since we",
    "start": "523930",
    "end": "526790"
  },
  {
    "text": "were explicitly defining almost every",
    "start": "526790",
    "end": "528589"
  },
  {
    "text": "field in the container the",
    "start": "528589",
    "end": "529790"
  },
  {
    "text": "infrastructure was really coupled with",
    "start": "529790",
    "end": "531440"
  },
  {
    "text": "the application and in order for the",
    "start": "531440",
    "end": "533570"
  },
  {
    "text": "platform team to provide a layer of",
    "start": "533570",
    "end": "535310"
  },
  {
    "text": "isolation we wanted to abstract this",
    "start": "535310",
    "end": "538790"
  },
  {
    "text": "configuration and lastly because we were",
    "start": "538790",
    "end": "542209"
  },
  {
    "text": "providing explicit configurations to",
    "start": "542209",
    "end": "544010"
  },
  {
    "text": "container we were giving up on the",
    "start": "544010",
    "end": "545450"
  },
  {
    "text": "flexibility that ipbs provides out of",
    "start": "545450",
    "end": "547610"
  },
  {
    "text": "the box for online configuration so",
    "start": "547610",
    "end": "549860"
  },
  {
    "text": "every time we needed to make an update",
    "start": "549860",
    "end": "551360"
  },
  {
    "text": "we had to restart the container causing",
    "start": "551360",
    "end": "553190"
  },
  {
    "text": "downtime of our load balancer so we",
    "start": "553190",
    "end": "562279"
  },
  {
    "start": "558000",
    "end": "608000"
  },
  {
    "text": "needed to improve this we needed to",
    "start": "562279",
    "end": "564200"
  },
  {
    "text": "implement something that addressed the",
    "start": "564200",
    "end": "566269"
  },
  {
    "text": "problem of application coupling and",
    "start": "566269",
    "end": "567970"
  },
  {
    "text": "static nodes to port note our port to no",
    "start": "567970",
    "end": "571700"
  },
  {
    "text": "port load balancing so we had a VIP we",
    "start": "571700",
    "end": "575600"
  },
  {
    "text": "had a port on that bit and when traffic",
    "start": "575600",
    "end": "578120"
  },
  {
    "text": "came into the VIP where it went beyond",
    "start": "578120",
    "end": "580760"
  },
  {
    "text": "that we wanted to be defined by name and",
    "start": "580760",
    "end": "583810"
  },
  {
    "text": "for each service that runs interpret",
    "start": "583810",
    "end": "586100"
  },
  {
    "text": "Nettie's we are provided with a",
    "start": "586100",
    "end": "587360"
  },
  {
    "text": "namespace of service name and a port",
    "start": "587360",
    "end": "589310"
  },
  {
    "text": "name so this back-end construct that was",
    "start": "589310",
    "end": "591829"
  },
  {
    "text": "already so conveniently provided to us",
    "start": "591829",
    "end": "593540"
  },
  {
    "text": "by kubernetes was where we wanted",
    "start": "593540",
    "end": "595699"
  },
  {
    "text": "traffic to the master to be routed to so",
    "start": "595699",
    "end": "602240"
  },
  {
    "text": "we decided to make this better and we",
    "start": "602240",
    "end": "604130"
  },
  {
    "text": "came up with we call cube tight idea",
    "start": "604130",
    "end": "608380"
  },
  {
    "start": "608000",
    "end": "645000"
  },
  {
    "text": "cube to IBBs uses kubernetes and the",
    "start": "608920",
    "end": "612260"
  },
  {
    "text": "kubernetes api to store load balancer",
    "start": "612260",
    "end": "614390"
  },
  {
    "text": "configurations and retrieve them when",
    "start": "614390",
    "end": "615949"
  },
  {
    "text": "they change it enabled us to port",
    "start": "615949",
    "end": "618079"
  },
  {
    "text": "forward it enables us to add and remove",
    "start": "618079",
    "end": "620180"
  },
  {
    "text": "backends from Louis to backends",
    "start": "620180",
    "end": "622160"
  },
  {
    "text": "it enables us to load balanced traffic",
    "start": "622160",
    "end": "624649"
  },
  {
    "text": "into all of our kubernetes tenants from",
    "start": "624649",
    "end": "626360"
  },
  {
    "text": "a single distributed application if you",
    "start": "626360",
    "end": "628820"
  },
  {
    "text": "recall in the previous diagram load",
    "start": "628820",
    "end": "630890"
  },
  {
    "text": "balancing into the cluster let's perform",
    "start": "630890",
    "end": "632839"
  },
  {
    "text": "using static node ports using cube to",
    "start": "632839",
    "end": "635600"
  },
  {
    "text": "IBBs we're able to receive traffic on an",
    "start": "635600",
    "end": "638029"
  },
  {
    "text": "ingress port and route that traffic into",
    "start": "638029",
    "end": "640160"
  },
  {
    "text": "a cooper niddy service without relying",
    "start": "640160",
    "end": "642019"
  },
  {
    "text": "on any node ports",
    "start": "642019",
    "end": "644860"
  },
  {
    "start": "645000",
    "end": "766000"
  },
  {
    "text": "so to understand how we do this freaking",
    "start": "646170",
    "end": "649140"
  },
  {
    "text": "a little bit of primer about how cube",
    "start": "649140",
    "end": "650400"
  },
  {
    "text": "proxy works with IP tables IP tables",
    "start": "650400",
    "end": "653340"
  },
  {
    "text": "works by chaining different rule sets",
    "start": "653340",
    "end": "654990"
  },
  {
    "text": "together and matches those rules against",
    "start": "654990",
    "end": "657240"
  },
  {
    "text": "an incoming packet an incoming packet",
    "start": "657240",
    "end": "659790"
  },
  {
    "text": "has metadata associated with it such as",
    "start": "659790",
    "end": "661680"
  },
  {
    "text": "a destination address IP tables will",
    "start": "661680",
    "end": "664320"
  },
  {
    "text": "compare the packet with the",
    "start": "664320",
    "end": "665370"
  },
  {
    "text": "pre-configured rule set and if the",
    "start": "665370",
    "end": "667500"
  },
  {
    "text": "packet matches will send the packet to a",
    "start": "667500",
    "end": "669660"
  },
  {
    "text": "target chain cue proxy uses IP tables to",
    "start": "669660",
    "end": "673470"
  },
  {
    "text": "route traffic down for a kubernetes",
    "start": "673470",
    "end": "675150"
  },
  {
    "text": "service into an endpoint for that",
    "start": "675150",
    "end": "677010"
  },
  {
    "text": "service in a kubernetes cluster when a",
    "start": "677010",
    "end": "679290"
  },
  {
    "text": "packet comes into IP tables on a cube",
    "start": "679290",
    "end": "681810"
  },
  {
    "text": "node it first enters the pre routing",
    "start": "681810",
    "end": "683970"
  },
  {
    "text": "chain pre-rounding contains a chain",
    "start": "683970",
    "end": "686880"
  },
  {
    "text": "called cube services that every packet",
    "start": "686880",
    "end": "688800"
  },
  {
    "text": "enters the cube services chain contains",
    "start": "688800",
    "end": "691980"
  },
  {
    "text": "a list of rules that match on service IP",
    "start": "691980",
    "end": "694500"
  },
  {
    "text": "address and destination ports 3 if a",
    "start": "694500",
    "end": "703230"
  },
  {
    "text": "packet matches on a destination port",
    "start": "703230",
    "end": "705090"
  },
  {
    "text": "address and port it will be sent into",
    "start": "705090",
    "end": "707010"
  },
  {
    "text": "the cube the cube service chain specific",
    "start": "707010",
    "end": "710400"
  },
  {
    "text": "to that endpoint the I'm sorry to that",
    "start": "710400",
    "end": "714450"
  },
  {
    "text": "service specific queue service chain",
    "start": "714450",
    "end": "716940"
  },
  {
    "text": "contains a list of endpoints which",
    "start": "716940",
    "end": "719010"
  },
  {
    "text": "correspond to pods that are available to",
    "start": "719010",
    "end": "721920"
  },
  {
    "text": "host the service the chain will pick one",
    "start": "721920",
    "end": "724470"
  },
  {
    "text": "of those rules at random and send the",
    "start": "724470",
    "end": "726750"
  },
  {
    "text": "packet to the chosen cube service",
    "start": "726750",
    "end": "728340"
  },
  {
    "text": "endpoint here queue proxy sets a",
    "start": "728340",
    "end": "731760"
  },
  {
    "text": "masquerade bit on the packet and",
    "start": "731760",
    "end": "733110"
  },
  {
    "text": "destination ask the packet directly into",
    "start": "733110",
    "end": "735180"
  },
  {
    "text": "the pods that's associated with that end",
    "start": "735180",
    "end": "736860"
  },
  {
    "text": "point if the incoming packet doesn't",
    "start": "736860",
    "end": "739440"
  },
  {
    "text": "match on a service report in the cube",
    "start": "739440",
    "end": "741660"
  },
  {
    "text": "services chain and pre routing it will",
    "start": "741660",
    "end": "743250"
  },
  {
    "text": "attempt to match in the last chain and",
    "start": "743250",
    "end": "745050"
  },
  {
    "text": "pre routing up called cube shared ports",
    "start": "745050",
    "end": "746700"
  },
  {
    "text": "if the incoming packet matches the node",
    "start": "746700",
    "end": "749130"
  },
  {
    "text": "port in this chain it will forward to",
    "start": "749130",
    "end": "751500"
  },
  {
    "text": "the specific cube service and that's",
    "start": "751500",
    "end": "755790"
  },
  {
    "text": "associated with it and complete the rest",
    "start": "755790",
    "end": "757470"
  },
  {
    "text": "of this for the flow through IP tables",
    "start": "757470",
    "end": "759450"
  },
  {
    "text": "ultimately getting sent to the pod",
    "start": "759450",
    "end": "763730"
  },
  {
    "start": "766000",
    "end": "802000"
  },
  {
    "text": "so what cube to IBBs does is it injects",
    "start": "766590",
    "end": "769900"
  },
  {
    "text": "its own service chain into pre routing",
    "start": "769900",
    "end": "772150"
  },
  {
    "text": "that bridges traffic found for this",
    "start": "772150",
    "end": "774640"
  },
  {
    "text": "address directly into the specific cube",
    "start": "774640",
    "end": "777160"
  },
  {
    "text": "service chain for a configured service",
    "start": "777160",
    "end": "779170"
  },
  {
    "text": "this means that packets go in to keep",
    "start": "779170",
    "end": "782770"
  },
  {
    "text": "pre routing then into cube to ipbs where",
    "start": "782770",
    "end": "786040"
  },
  {
    "text": "if they match as if and port they get",
    "start": "786040",
    "end": "787900"
  },
  {
    "text": "routed into the service",
    "start": "787900",
    "end": "789010"
  },
  {
    "text": "this approach is transparent the",
    "start": "789010",
    "end": "791050"
  },
  {
    "text": "kubernetes built-in service load",
    "start": "791050",
    "end": "792640"
  },
  {
    "text": "balancing so if the package doesn't",
    "start": "792640",
    "end": "794560"
  },
  {
    "text": "match nineteen cubed IP vs it gets",
    "start": "794560",
    "end": "797020"
  },
  {
    "text": "processed through the queue proxy chain",
    "start": "797020",
    "end": "798880"
  },
  {
    "text": "normally how does this work with our",
    "start": "798880",
    "end": "803710"
  },
  {
    "start": "802000",
    "end": "833000"
  },
  {
    "text": "itvs container for the back-end service",
    "start": "803710",
    "end": "806860"
  },
  {
    "text": "the kubernetes api is query to get a",
    "start": "806860",
    "end": "809050"
  },
  {
    "text": "config map that contains the desired",
    "start": "809050",
    "end": "810700"
  },
  {
    "text": "keep to IBBs configuration the backend",
    "start": "810700",
    "end": "815170"
  },
  {
    "text": "service adds the VIP address to the",
    "start": "815170",
    "end": "816880"
  },
  {
    "text": "nodes loopback interface it also uses",
    "start": "816880",
    "end": "819190"
  },
  {
    "text": "the config map to create the vista",
    "start": "819190",
    "end": "821170"
  },
  {
    "text": "service mapping in this cube by VDS",
    "start": "821170",
    "end": "823420"
  },
  {
    "text": "chain and IP tables this mapping and IP",
    "start": "823420",
    "end": "825940"
  },
  {
    "text": "tables generates the direct jump into",
    "start": "825940",
    "end": "828370"
  },
  {
    "text": "the specific community service chain so",
    "start": "828370",
    "end": "835210"
  },
  {
    "start": "833000",
    "end": "865000"
  },
  {
    "text": "in IP vs master service the IP vs master",
    "start": "835210",
    "end": "838120"
  },
  {
    "text": "manages the front-end in bits and ports",
    "start": "838120",
    "end": "840070"
  },
  {
    "text": "that ipbs listens on it queries the",
    "start": "840070",
    "end": "842890"
  },
  {
    "text": "queue API or the config map populates",
    "start": "842890",
    "end": "845350"
  },
  {
    "text": "the currently configured discs and adds",
    "start": "845350",
    "end": "846970"
  },
  {
    "text": "them to the list of furnance for each",
    "start": "846970",
    "end": "849520"
  },
  {
    "text": "VIP that the master finds in the config",
    "start": "849520",
    "end": "851530"
  },
  {
    "text": "map it adds that bit to the primary",
    "start": "851530",
    "end": "853570"
  },
  {
    "text": "interface of the nodes",
    "start": "853570",
    "end": "854740"
  },
  {
    "text": "the master also creates the API server",
    "start": "854740",
    "end": "857290"
  },
  {
    "text": "for a list of available nodes to add to",
    "start": "857290",
    "end": "859360"
  },
  {
    "text": "the I PDS configuration as back-end",
    "start": "859360",
    "end": "861340"
  },
  {
    "text": "servers for each of these furnace",
    "start": "861340",
    "end": "865020"
  },
  {
    "start": "865000",
    "end": "915000"
  },
  {
    "text": "so in order to maintain the list of",
    "start": "866630",
    "end": "868670"
  },
  {
    "text": "backends the ITV Assessor will create",
    "start": "868670",
    "end": "870560"
  },
  {
    "text": "the API for list of nodes in the cluster",
    "start": "870560",
    "end": "872740"
  },
  {
    "text": "will filter the nodes and they tame a",
    "start": "872740",
    "end": "875269"
  },
  {
    "text": "list of backends of my PDF configuration",
    "start": "875269",
    "end": "877310"
  },
  {
    "text": "based on those health as you can see the",
    "start": "877310",
    "end": "879470"
  },
  {
    "text": "last node and the Kagan nodes I have",
    "start": "879470",
    "end": "883009"
  },
  {
    "text": "visited not ready state and in the",
    "start": "883009",
    "end": "885470"
  },
  {
    "text": "screen shown on the right you can see",
    "start": "885470",
    "end": "886579"
  },
  {
    "text": "that the IPPS master has ensured it's",
    "start": "886579",
    "end": "888470"
  },
  {
    "text": "not in the list of healthy back in the",
    "start": "888470",
    "end": "891440"
  },
  {
    "text": "IP vs master service will also remove",
    "start": "891440",
    "end": "893690"
  },
  {
    "text": "any nodes that are cordoned or",
    "start": "893690",
    "end": "895130"
  },
  {
    "text": "unschedulable and as you can see in the",
    "start": "895130",
    "end": "898339"
  },
  {
    "text": "screenshot on the right the list of",
    "start": "898339",
    "end": "899630"
  },
  {
    "text": "backends is missing the dot 158 node",
    "start": "899630",
    "end": "901790"
  },
  {
    "text": "this is because the icbs master service",
    "start": "901790",
    "end": "904610"
  },
  {
    "text": "is scheduled and running on that node",
    "start": "904610",
    "end": "906050"
  },
  {
    "text": "and when the IPPS master comes up it",
    "start": "906050",
    "end": "908959"
  },
  {
    "text": "will check its own IP address and filter",
    "start": "908959",
    "end": "911000"
  },
  {
    "text": "out of the list of available nodes as",
    "start": "911000",
    "end": "917600"
  },
  {
    "start": "915000",
    "end": "968000"
  },
  {
    "text": "mentioned before we utilizing kubernetes",
    "start": "917600",
    "end": "920060"
  },
  {
    "text": "config maps to manage the IPS and dip",
    "start": "920060",
    "end": "922670"
  },
  {
    "text": "state in the cluster the config master",
    "start": "922670",
    "end": "925220"
  },
  {
    "text": "deploy with a pool of IP addresses that",
    "start": "925220",
    "end": "927490"
  },
  {
    "text": "are available for this we're also using",
    "start": "927490",
    "end": "931779"
  },
  {
    "text": "node labels and selectors to isolate",
    "start": "931779",
    "end": "934519"
  },
  {
    "text": "we're balancing on a per VLAN basis so",
    "start": "934519",
    "end": "937670"
  },
  {
    "text": "we separated it pulled by VLAN so that",
    "start": "937670",
    "end": "939800"
  },
  {
    "text": "when I new abode Balan services added",
    "start": "939800",
    "end": "941480"
  },
  {
    "text": "keep to IBBs will only add that VIP to",
    "start": "941480",
    "end": "944209"
  },
  {
    "text": "nodes that have the correct VLAN node",
    "start": "944209",
    "end": "946250"
  },
  {
    "text": "label and this is something that we do",
    "start": "946250",
    "end": "948019"
  },
  {
    "text": "to address the layer to adjacency",
    "start": "948019",
    "end": "950660"
  },
  {
    "text": "requirement of I PBS the IP vs backends",
    "start": "950660",
    "end": "954439"
  },
  {
    "text": "and masters watch the config map in the",
    "start": "954439",
    "end": "957290"
  },
  {
    "text": "API and dynamically update their bits",
    "start": "957290",
    "end": "959240"
  },
  {
    "text": "and IP tables rules as a change so we",
    "start": "959240",
    "end": "961939"
  },
  {
    "text": "essentially have real-time configuration",
    "start": "961939",
    "end": "964699"
  },
  {
    "text": "a roller balancer",
    "start": "964699",
    "end": "967750"
  },
  {
    "text": "in order to operationalize the updates",
    "start": "969660",
    "end": "972269"
  },
  {
    "text": "to the config map we created a UI",
    "start": "972269",
    "end": "974040"
  },
  {
    "text": "developers or operating teams can use to",
    "start": "974040",
    "end": "976680"
  },
  {
    "text": "manage their own bits on a per site",
    "start": "976680",
    "end": "978209"
  },
  {
    "text": "basis this gives us the ability to run",
    "start": "978209",
    "end": "981120"
  },
  {
    "text": "the config map in a secure namespace so",
    "start": "981120",
    "end": "983579"
  },
  {
    "text": "that only the platform and data center",
    "start": "983579",
    "end": "985230"
  },
  {
    "text": "teams can manage the pool of IPs",
    "start": "985230",
    "end": "986790"
  },
  {
    "text": "available the config map can be",
    "start": "986790",
    "end": "989130"
  },
  {
    "text": "dynamically dynamically updated by a",
    "start": "989130",
    "end": "991589"
  },
  {
    "text": "specific teams for their load balancing",
    "start": "991589",
    "end": "993509"
  },
  {
    "text": "needs without directly interacting with",
    "start": "993509",
    "end": "995850"
  },
  {
    "text": "the underlying infrastructure so this",
    "start": "995850",
    "end": "998040"
  },
  {
    "text": "enables different teams to be",
    "start": "998040",
    "end": "999209"
  },
  {
    "text": "responsible for their own load balancing",
    "start": "999209",
    "end": "1000980"
  },
  {
    "text": "and I'll do a short demo of this in a",
    "start": "1000980",
    "end": "1004759"
  },
  {
    "text": "little bit well so once we had",
    "start": "1004759",
    "end": "1009170"
  },
  {
    "text": "everything running in our clusters we",
    "start": "1009170",
    "end": "1010670"
  },
  {
    "text": "inevitably ran into some integration",
    "start": "1010670",
    "end": "1012709"
  },
  {
    "text": "issues that I want to talk through so my",
    "start": "1012709",
    "end": "1017630"
  },
  {
    "start": "1015000",
    "end": "1096000"
  },
  {
    "text": "crews been running ipbs since as long as",
    "start": "1017630",
    "end": "1020269"
  },
  {
    "text": "it's been around and even though we",
    "start": "1020269",
    "end": "1023389"
  },
  {
    "text": "should know better every time we deploy",
    "start": "1023389",
    "end": "1025400"
  },
  {
    "text": "using a new method of IPPS we seem to",
    "start": "1025400",
    "end": "1028100"
  },
  {
    "text": "run into the art problem and cubed ITBS",
    "start": "1028100",
    "end": "1030650"
  },
  {
    "text": "with no exception",
    "start": "1030650",
    "end": "1032538"
  },
  {
    "text": "we were running happily for a while when",
    "start": "1032539",
    "end": "1034579"
  },
  {
    "text": "QA began recording performance",
    "start": "1034579",
    "end": "1036949"
  },
  {
    "text": "degradation during load testing and the",
    "start": "1036949",
    "end": "1039409"
  },
  {
    "text": "troubleshooting we found that all",
    "start": "1039409",
    "end": "1040668"
  },
  {
    "text": "traffic was coming into and going out of",
    "start": "1040669",
    "end": "1043280"
  },
  {
    "text": "the same server and more interesting and",
    "start": "1043280",
    "end": "1045558"
  },
  {
    "text": "interesting enough it was not my PDF",
    "start": "1045559",
    "end": "1048288"
  },
  {
    "text": "master and we immediately knew this",
    "start": "1048289",
    "end": "1050870"
  },
  {
    "text": "smelled like ARP so what was happening",
    "start": "1050870",
    "end": "1053240"
  },
  {
    "text": "was a race condition where if the",
    "start": "1053240",
    "end": "1055220"
  },
  {
    "text": "loopback address was brought up by a",
    "start": "1055220",
    "end": "1057049"
  },
  {
    "text": "cube to IPPs on any back-end it would",
    "start": "1057049",
    "end": "1059390"
  },
  {
    "text": "announce itself to the router if this",
    "start": "1059390",
    "end": "1061490"
  },
  {
    "text": "timing happened before the IPPS master",
    "start": "1061490",
    "end": "1063679"
  },
  {
    "text": "announced itself its ownership of the",
    "start": "1063679",
    "end": "1066260"
  },
  {
    "text": "VIP the router would send all traffic",
    "start": "1066260",
    "end": "1068120"
  },
  {
    "text": "destined to that IP address to the",
    "start": "1068120",
    "end": "1069919"
  },
  {
    "text": "loopback interface on the backend node",
    "start": "1069919",
    "end": "1071990"
  },
  {
    "text": "and with IP tables rules being set",
    "start": "1071990",
    "end": "1074450"
  },
  {
    "text": "traffic never left that single node so a",
    "start": "1074450",
    "end": "1078470"
  },
  {
    "text": "quick read through IP VSS documentation",
    "start": "1078470",
    "end": "1081049"
  },
  {
    "text": "led us to the familiar arc rules which",
    "start": "1081049",
    "end": "1084289"
  },
  {
    "text": "made the loopback interface on the back",
    "start": "1084289",
    "end": "1086000"
  },
  {
    "text": "end",
    "start": "1086000",
    "end": "1086390"
  },
  {
    "text": "ignore any ARP requests on the network",
    "start": "1086390",
    "end": "1088150"
  },
  {
    "text": "it would also disable our pronounced",
    "start": "1088150",
    "end": "1090740"
  },
  {
    "text": "when it came online so this list left",
    "start": "1090740",
    "end": "1092990"
  },
  {
    "text": "the IPPS masters - all of this",
    "start": "1092990",
    "end": "1097120"
  },
  {
    "start": "1096000",
    "end": "1155000"
  },
  {
    "text": "the next issue we ran into was when we",
    "start": "1097970",
    "end": "1099890"
  },
  {
    "text": "attempted to co-locate the master and",
    "start": "1099890",
    "end": "1101450"
  },
  {
    "text": "back-end server",
    "start": "1101450",
    "end": "1102170"
  },
  {
    "text": "for ipbs we were initially explicitly",
    "start": "1102170",
    "end": "1106250"
  },
  {
    "text": "defining a set of nodes for the masters",
    "start": "1106250",
    "end": "1107960"
  },
  {
    "text": "and a set of nodes for the back end but",
    "start": "1107960",
    "end": "1110210"
  },
  {
    "text": "we wanted to reduce the operational cost",
    "start": "1110210",
    "end": "1112250"
  },
  {
    "text": "so we tried running them on the same set",
    "start": "1112250",
    "end": "1114560"
  },
  {
    "text": "of nodes but when these services got",
    "start": "1114560",
    "end": "1116690"
  },
  {
    "text": "scheduled on the same node they",
    "start": "1116690",
    "end": "1117920"
  },
  {
    "text": "conflicted the zip address was assigned",
    "start": "1117920",
    "end": "1120440"
  },
  {
    "text": "to the primary interface as well as the",
    "start": "1120440",
    "end": "1122150"
  },
  {
    "text": "loopback address and similarly to ARP",
    "start": "1122150",
    "end": "1125140"
  },
  {
    "text": "iptables rules were set and traffic",
    "start": "1125140",
    "end": "1127970"
  },
  {
    "text": "never left the node so we devised the",
    "start": "1127970",
    "end": "1130490"
  },
  {
    "text": "way for the master to advertise its",
    "start": "1130490",
    "end": "1132050"
  },
  {
    "text": "presence and for the presence of a",
    "start": "1132050",
    "end": "1134930"
  },
  {
    "text": "master to disrupt the back-end service",
    "start": "1134930",
    "end": "1137390"
  },
  {
    "text": "so when a master came up it opens a port",
    "start": "1137390",
    "end": "1140450"
  },
  {
    "text": "at the backend servers would check if",
    "start": "1140450",
    "end": "1142370"
  },
  {
    "text": "the backend server receives a response",
    "start": "1142370",
    "end": "1143930"
  },
  {
    "text": "on that port it halts itself on the node",
    "start": "1143930",
    "end": "1146150"
  },
  {
    "text": "removes its VIP IP tables rules and our",
    "start": "1146150",
    "end": "1149180"
  },
  {
    "text": "pools if it does not receive a response",
    "start": "1149180",
    "end": "1151430"
  },
  {
    "text": "that continues running and we",
    "start": "1151430",
    "end": "1152870"
  },
  {
    "text": "continuously on this port check in our",
    "start": "1152870",
    "end": "1157910"
  },
  {
    "start": "1155000",
    "end": "1208000"
  },
  {
    "text": "initial implementation of coop to IBBs",
    "start": "1157910",
    "end": "1159650"
  },
  {
    "text": "we are relying on IP tables out of the",
    "start": "1159650",
    "end": "1161750"
  },
  {
    "text": "box functionality to make rule set",
    "start": "1161750",
    "end": "1163250"
  },
  {
    "text": "changes in this mode IP tables uses X",
    "start": "1163250",
    "end": "1166400"
  },
  {
    "text": "table blocks to perform an individual",
    "start": "1166400",
    "end": "1168620"
  },
  {
    "text": "check and set for each rule that we",
    "start": "1168620",
    "end": "1170240"
  },
  {
    "text": "wanted to create and at the time to",
    "start": "1170240",
    "end": "1172940"
  },
  {
    "text": "proxy was doing the exact same thing so",
    "start": "1172940",
    "end": "1175610"
  },
  {
    "text": "in cases where we had hundreds or even",
    "start": "1175610",
    "end": "1177020"
  },
  {
    "text": "thousands of services queue proxy and",
    "start": "1177020",
    "end": "1179510"
  },
  {
    "text": "keep to IBBs would fight each other over",
    "start": "1179510",
    "end": "1181100"
  },
  {
    "text": "who owns the IP tables locking and this",
    "start": "1181100",
    "end": "1184610"
  },
  {
    "text": "contention resulted in neither to proxy",
    "start": "1184610",
    "end": "1186740"
  },
  {
    "text": "or cubed IBBs being able to make changes",
    "start": "1186740",
    "end": "1189020"
  },
  {
    "text": "and ultimately this resulted in cube",
    "start": "1189020",
    "end": "1191720"
  },
  {
    "text": "services becoming stale so to resolve",
    "start": "1191720",
    "end": "1195260"
  },
  {
    "text": "this we switch to IP table save and",
    "start": "1195260",
    "end": "1196970"
  },
  {
    "text": "restore and using this method instead of",
    "start": "1196970",
    "end": "1199760"
  },
  {
    "text": "using thousands of requests for each",
    "start": "1199760",
    "end": "1201140"
  },
  {
    "text": "change a single request would be made",
    "start": "1201140",
    "end": "1203450"
  },
  {
    "text": "every time a change was necessary and",
    "start": "1203450",
    "end": "1209510"
  },
  {
    "start": "1208000",
    "end": "1704000"
  },
  {
    "text": "lastly once our developers started using",
    "start": "1209510",
    "end": "1211760"
  },
  {
    "text": "cube to ipbs they began to facilitate",
    "start": "1211760",
    "end": "1214820"
  },
  {
    "text": "communication between services and",
    "start": "1214820",
    "end": "1217940"
  },
  {
    "text": "different namespaces using this and",
    "start": "1217940",
    "end": "1220810"
  },
  {
    "text": "occasionally we would see requests from",
    "start": "1220810",
    "end": "1223100"
  },
  {
    "text": "a pod to a fifth timeout and we realized",
    "start": "1223100",
    "end": "1226310"
  },
  {
    "text": "this was occurring when the pod that was",
    "start": "1226310",
    "end": "1228320"
  },
  {
    "text": "making a request was scheduled on the",
    "start": "1228320",
    "end": "1229910"
  },
  {
    "text": "same note as the ITV",
    "start": "1229910",
    "end": "1231320"
  },
  {
    "text": "master and request from the private IP",
    "start": "1231320",
    "end": "1234860"
  },
  {
    "text": "of the pod were unable to reach the VIP",
    "start": "1234860",
    "end": "1237500"
  },
  {
    "text": "on the notes primary interface and the",
    "start": "1237500",
    "end": "1239780"
  },
  {
    "text": "coup Brandis cubelet actually provides",
    "start": "1239780",
    "end": "1242120"
  },
  {
    "text": "an option to run a hairpin mode to",
    "start": "1242120",
    "end": "1246680"
  },
  {
    "text": "handle these edge cases so as soon as we",
    "start": "1246680",
    "end": "1249050"
  },
  {
    "text": "enable that in the cluster of this she",
    "start": "1249050",
    "end": "1250910"
  },
  {
    "text": "was resolved okay okay I'm going to do a",
    "start": "1250910",
    "end": "1256790"
  },
  {
    "text": "short demo or my desktop please",
    "start": "1256790",
    "end": "1266500"
  },
  {
    "text": "okay so we're looking at the Qi PBS UI",
    "start": "1271080",
    "end": "1274690"
  },
  {
    "text": "here this is a load balancer that's",
    "start": "1274690",
    "end": "1278169"
  },
  {
    "text": "running in a VLAN that we call green in",
    "start": "1278169",
    "end": "1280899"
  },
  {
    "text": "the mythical namespace the service that",
    "start": "1280899",
    "end": "1284080"
  },
  {
    "text": "we have configure here is mermaid's the",
    "start": "1284080",
    "end": "1287020"
  },
  {
    "text": "port name is HTTP and the dip on the",
    "start": "1287020",
    "end": "1290710"
  },
  {
    "text": "right is listening on port 80 so let's",
    "start": "1290710",
    "end": "1295390"
  },
  {
    "text": "take a look at this this looks like in",
    "start": "1295390",
    "end": "1297370"
  },
  {
    "text": "the back end here I'm doing a watch and",
    "start": "1297370",
    "end": "1300039"
  },
  {
    "text": "the IPPS ATM command inside of the",
    "start": "1300039",
    "end": "1302200"
  },
  {
    "text": "master container so you can see",
    "start": "1302200",
    "end": "1306960"
  },
  {
    "text": "listening on 165 on port 80 and we have",
    "start": "1306960",
    "end": "1310630"
  },
  {
    "text": "a list of back-end nodes here below the",
    "start": "1310630",
    "end": "1314860"
  },
  {
    "text": "bottom here so at the bottom here these",
    "start": "1314860",
    "end": "1328960"
  },
  {
    "text": "are the IP tables rules that are on the",
    "start": "1328960",
    "end": "1330880"
  },
  {
    "text": "back end note itself so for each service",
    "start": "1330880",
    "end": "1334600"
  },
  {
    "text": "that we bring up we have two IP tables",
    "start": "1334600",
    "end": "1336250"
  },
  {
    "text": "rules and this destination import is the",
    "start": "1336250",
    "end": "1339820"
  },
  {
    "text": "zip address that we have configured in",
    "start": "1339820",
    "end": "1341919"
  },
  {
    "text": "the UI and this is the specific Tube",
    "start": "1341919",
    "end": "1345190"
  },
  {
    "text": "service entry in IP table so where the",
    "start": "1345190",
    "end": "1349840"
  },
  {
    "text": "VIP gets routed to",
    "start": "1349840",
    "end": "1352740"
  },
  {
    "text": "up on the top right here is our config",
    "start": "1355020",
    "end": "1357730"
  },
  {
    "text": "map you can see we have a list of IP",
    "start": "1357730",
    "end": "1359200"
  },
  {
    "text": "addresses that are in the VIP pool and",
    "start": "1359200",
    "end": "1360880"
  },
  {
    "text": "the configuration that describes the",
    "start": "1360880",
    "end": "1363310"
  },
  {
    "text": "service that we configured in in the UI",
    "start": "1363310",
    "end": "1365440"
  },
  {
    "text": "so this is named state service name and",
    "start": "1365440",
    "end": "1368350"
  },
  {
    "text": "port name you can see that we're not",
    "start": "1368350",
    "end": "1369790"
  },
  {
    "text": "using any node ports here and all in our",
    "start": "1369790",
    "end": "1372760"
  },
  {
    "text": "definition so I'm going to add another",
    "start": "1372760",
    "end": "1375970"
  },
  {
    "text": "another service I have a service called",
    "start": "1375970",
    "end": "1378430"
  },
  {
    "text": "unicorns of running in this cluster I",
    "start": "1378430",
    "end": "1380560"
  },
  {
    "text": "add some more mythical port name HTTP",
    "start": "1380560",
    "end": "1384040"
  },
  {
    "text": "we're going to put it on the address 166",
    "start": "1384040",
    "end": "1387580"
  },
  {
    "text": "on port 8080 and we'll create this so",
    "start": "1387580",
    "end": "1392650"
  },
  {
    "text": "now our service is created in the UI and",
    "start": "1392650",
    "end": "1396690"
  },
  {
    "text": "go back to the terminal",
    "start": "1397440",
    "end": "1400470"
  },
  {
    "text": "the master has automatically configured",
    "start": "1400470",
    "end": "1403660"
  },
  {
    "text": "this front end and IP via the back end",
    "start": "1403660",
    "end": "1407110"
  },
  {
    "text": "has now has the IP tables rules for the",
    "start": "1407110",
    "end": "1411250"
  },
  {
    "text": "unicorn services running and again",
    "start": "1411250",
    "end": "1413410"
  },
  {
    "text": "that's getting routed directly into the",
    "start": "1413410",
    "end": "1415090"
  },
  {
    "text": "service chain so that for that service",
    "start": "1415090",
    "end": "1419130"
  },
  {
    "text": "and our config map is updated with the",
    "start": "1419130",
    "end": "1422940"
  },
  {
    "text": "new service on the new this on port 8080",
    "start": "1422940",
    "end": "1427680"
  },
  {
    "text": "if we go to a browser and hit that it",
    "start": "1427680",
    "end": "1432660"
  },
  {
    "text": "can see our unicorn",
    "start": "1434340",
    "end": "1437820"
  },
  {
    "text": "so I say we need to run this unicorn",
    "start": "1439490",
    "end": "1443240"
  },
  {
    "text": "service on multiple ports if you need",
    "start": "1443240",
    "end": "1447860"
  },
  {
    "text": "HTTP and HTTPS for example in this case",
    "start": "1447860",
    "end": "1451610"
  },
  {
    "text": "we don't but so we'll use an admin port",
    "start": "1451610",
    "end": "1454580"
  },
  {
    "text": "to simulate this so we'll put it on the",
    "start": "1454580",
    "end": "1457130"
  },
  {
    "text": "same bib address port 8081",
    "start": "1457130",
    "end": "1459620"
  },
  {
    "text": "and create that service program so to",
    "start": "1459620",
    "end": "1463159"
  },
  {
    "text": "create that load bill",
    "start": "1463159",
    "end": "1465760"
  },
  {
    "text": "so now we have two of its pointed up to",
    "start": "1470220",
    "end": "1472710"
  },
  {
    "text": "the unicorn service on two different",
    "start": "1472710",
    "end": "1474870"
  },
  {
    "text": "ports we come back here the IPPS master",
    "start": "1474870",
    "end": "1478409"
  },
  {
    "text": "has been updated with the Sun proxy",
    "start": "1478409",
    "end": "1480210"
  },
  {
    "text": "I've been port our back-end has the new",
    "start": "1480210",
    "end": "1483929"
  },
  {
    "text": "IP tables rules configured and I just",
    "start": "1483929",
    "end": "1486570"
  },
  {
    "text": "want to point out here that the service",
    "start": "1486570",
    "end": "1487980"
  },
  {
    "text": "on the right is the exact same service",
    "start": "1487980",
    "end": "1489600"
  },
  {
    "text": "that is getting ready to in the backend",
    "start": "1489600",
    "end": "1491029"
  },
  {
    "text": "so even though it's listening on two",
    "start": "1491029",
    "end": "1493320"
  },
  {
    "text": "different ports it will route to the",
    "start": "1493320",
    "end": "1495149"
  },
  {
    "text": "same Cooper nettie service and our",
    "start": "1495149",
    "end": "1502230"
  },
  {
    "text": "config map it's updated with the menu",
    "start": "1502230",
    "end": "1505320"
  },
  {
    "text": "service for the local culture",
    "start": "1505320",
    "end": "1508460"
  },
  {
    "text": "go to a browser 481 more unicorns as",
    "start": "1511830",
    "end": "1516450"
  },
  {
    "text": "expected so let's delete and delete the",
    "start": "1516450",
    "end": "1522690"
  },
  {
    "text": "8081 service and this is automatically",
    "start": "1522690",
    "end": "1530640"
  },
  {
    "text": "removed from the IPPS master and the",
    "start": "1530640",
    "end": "1535380"
  },
  {
    "text": "backend and the config map",
    "start": "1535380",
    "end": "1538760"
  },
  {
    "text": "oh yeah so 8081 will timeout no longer",
    "start": "1548270",
    "end": "1552380"
  },
  {
    "text": "exist not going to wait for that to",
    "start": "1552380",
    "end": "1555350"
  },
  {
    "text": "timeout so say we have another tenant in",
    "start": "1555350",
    "end": "1559190"
  },
  {
    "text": "the cluster that wants to create a load",
    "start": "1559190",
    "end": "1561650"
  },
  {
    "text": "balancer for for its service they're in",
    "start": "1561650",
    "end": "1564920"
  },
  {
    "text": "the same VLAN but they're in a different",
    "start": "1564920",
    "end": "1566809"
  },
  {
    "text": "namespace held typicals the services",
    "start": "1566809",
    "end": "1570380"
  },
  {
    "text": "courses port name is HTTP and put them",
    "start": "1570380",
    "end": "1575780"
  },
  {
    "text": "on the 166 and they also want to listen",
    "start": "1575780",
    "end": "1579200"
  },
  {
    "text": "on port 80 so we'll create that load",
    "start": "1579200",
    "end": "1585440"
  },
  {
    "text": "balancer now we have two services and",
    "start": "1585440",
    "end": "1591830"
  },
  {
    "text": "two different namespaces both listening",
    "start": "1591830",
    "end": "1593750"
  },
  {
    "text": "on port 80 master has been updated with",
    "start": "1593750",
    "end": "1600380"
  },
  {
    "text": "the new ports for 166 address IP tables",
    "start": "1600380",
    "end": "1604309"
  },
  {
    "text": "roles are updated for our new service",
    "start": "1604309",
    "end": "1607179"
  },
  {
    "text": "typical courses in our config map has",
    "start": "1607179",
    "end": "1613460"
  },
  {
    "text": "also been updated",
    "start": "1613460",
    "end": "1616029"
  },
  {
    "text": "so we can go hit that in the browser and",
    "start": "1618490",
    "end": "1621010"
  },
  {
    "text": "there are horses alright so that's the",
    "start": "1621010",
    "end": "1629590"
  },
  {
    "text": "end of my demo there any question yeah",
    "start": "1629590",
    "end": "1638730"
  },
  {
    "text": "yeah we are talking about it we haven't",
    "start": "1646320",
    "end": "1648640"
  },
  {
    "text": "started working on it yet but it is",
    "start": "1648640",
    "end": "1649750"
  },
  {
    "text": "something working into the other",
    "start": "1649750",
    "end": "1653800"
  },
  {
    "text": "question",
    "start": "1653800",
    "end": "1656130"
  },
  {
    "text": "yep",
    "start": "1660309",
    "end": "1663309"
  },
  {
    "text": "well sorry I can't enjoy instead of oh",
    "start": "1675240",
    "end": "1684929"
  },
  {
    "text": "we just wanted to keep it integrated in",
    "start": "1686549",
    "end": "1689169"
  },
  {
    "text": "kubernetes it was the easiest thing to",
    "start": "1689169",
    "end": "1691000"
  },
  {
    "text": "do it was available to us pretty easily",
    "start": "1691000",
    "end": "1696330"
  },
  {
    "text": "yeah right thank you",
    "start": "1697529",
    "end": "1702250"
  },
  {
    "text": "[Applause]",
    "start": "1702250",
    "end": "1705500"
  }
]