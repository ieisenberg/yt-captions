[
  {
    "text": "all right I think we can get started so hi everyone welcome to this",
    "start": "640",
    "end": "6960"
  },
  {
    "text": "talk uh it's titled fantastic ordinals and how to avoid them Autos scaling",
    "start": "6960",
    "end": "12519"
  },
  {
    "text": "challenges in a cloud database uh my name is Manish Gil and I work for a company called click",
    "start": "12519",
    "end": "19840"
  },
  {
    "text": "house so I'm an engineering manager at click house uh I work in a team which is",
    "start": "19840",
    "end": "25000"
  },
  {
    "text": "the scale team and the primary thing we do at click house is think about vertical and horizontal Auto scaling uh",
    "start": "25000",
    "end": "31359"
  },
  {
    "text": "idling scaling to zero and we also heavily contribute to the click house operator in order to facilitate uh the",
    "start": "31359",
    "end": "37719"
  },
  {
    "text": "scaling mechanics so this is the agenda for today we're going to look at click house",
    "start": "37719",
    "end": "43800"
  },
  {
    "text": "the technology uh we're going to talk about click house Cloud we're going to talk about the auto scaling architecture",
    "start": "43800",
    "end": "48960"
  },
  {
    "text": "of uh click house we're going to look at vertical scaling specifically as a problem that we faced in Click house and",
    "start": "48960",
    "end": "54800"
  },
  {
    "text": "then uh what the models of vertical scaling are specifically what we call the break first model of scaling",
    "start": "54800",
    "end": "60719"
  },
  {
    "text": "uh then we're going to go into something we like to call make before break as an alternative approach uh we're going to",
    "start": "60719",
    "end": "66320"
  },
  {
    "text": "look at some uh like uh some nuances of how stateful sets work and the limitations of stateful set and how are",
    "start": "66320",
    "end": "71799"
  },
  {
    "text": "how we dealt with the problems okay so click house click house",
    "start": "71799",
    "end": "77840"
  },
  {
    "text": "itself uh I don't know probably some people are familiar with it it's an open- Source column oriented database",
    "start": "77840",
    "end": "83320"
  },
  {
    "text": "it's a distributed olab database specifically so the workloads that click house typically runs are analytic",
    "start": "83320",
    "end": "89240"
  },
  {
    "text": "analytical nature uh it's got a reputation as being a really really fast uh database so most",
    "start": "89240",
    "end": "96040"
  },
  {
    "text": "of the times like you'll see people running click house clusters with like uh terabytes to terabytes of magnitude",
    "start": "96040",
    "end": "102399"
  },
  {
    "text": "and yeah it's uh feel free to Google it it's a pretty powerful Tool uh for the purpose of our talk what's important is",
    "start": "102399",
    "end": "108640"
  },
  {
    "text": "it's a distributed multimaster database and it's eventually consistent and so whenever I'm talking about like pots or",
    "start": "108640",
    "end": "114920"
  },
  {
    "text": "or replicas in Click house uh there is no such concept of a primary with like a repli repli in turn so all replicas can",
    "start": "114920",
    "end": "122159"
  },
  {
    "text": "ingest traffic and all of the replicas can serve query so like no one replica is special",
    "start": "122159",
    "end": "129640"
  },
  {
    "text": "here so click house cloud is our serverless offering uh of Click house uh we try to uh make uh give you like a",
    "start": "129959",
    "end": "136840"
  },
  {
    "text": "fully managed experience you don't have to like deploy your own shared nothing architectures uh it's serverless it's",
    "start": "136840",
    "end": "142879"
  },
  {
    "text": "got ideling and autoscaling and that's what my team is responsible for it's also got separation of compute and",
    "start": "142879",
    "end": "148239"
  },
  {
    "text": "storage so the storage in Click house uh turns out to be like as is the trend these days uh S3 so the actual data gets",
    "start": "148239",
    "end": "155160"
  },
  {
    "text": "stored inside object storage uh we are available in two major Cloud providers right now AWS and gcp and we have an",
    "start": "155160",
    "end": "162239"
  },
  {
    "text": "Azure offering coming soon as well uh as you can see here so we have these server pods running inside a click house",
    "start": "162239",
    "end": "167720"
  },
  {
    "text": "instance and these are the compute notes uh we also have PVCs attached to these notes right now uh because uh we uh",
    "start": "167720",
    "end": "174239"
  },
  {
    "text": "still have some metadata pointing to that S3 data uh on these PVCs but we're slowly trying to get rid of these PVCs",
    "start": "174239",
    "end": "180720"
  },
  {
    "text": "and eventually move to like a fully stateless model that probably happen sometime soon uh apart from click house Cloud",
    "start": "180720",
    "end": "187560"
  },
  {
    "text": "there's also like a bring your own key kind of a model where uh you know for Enterprise customers who probably don't want data to ever leave their VPN uh",
    "start": "187560",
    "end": "194760"
  },
  {
    "text": "they can just provision uh this entire data plane inside their own account and then uh you know get the benefit of all",
    "start": "194760",
    "end": "200319"
  },
  {
    "text": "the surveill offering inside so uh when you're managing a",
    "start": "200319",
    "end": "205959"
  },
  {
    "text": "database on kubernetes the first thing you start out with is an operator and that's kind of what we did as well so here we can see a very simple uh uh like",
    "start": "205959",
    "end": "213080"
  },
  {
    "text": "specification of how an operator looks like we have a custom resource inside of our kubernetes clusters this is the click house Cloud uh cluster resource",
    "start": "213080",
    "end": "220000"
  },
  {
    "text": "and that's being managed by the operator uh this operator in turn is uh manage is",
    "start": "220000",
    "end": "225120"
  },
  {
    "text": "creating and managing a stateful set and this stateful set has uh pods server pods 01 and two with PVC detach them so",
    "start": "225120",
    "end": "232439"
  },
  {
    "text": "this looks pretty familiar to most people I I hope okay so now we understand the",
    "start": "232439",
    "end": "238920"
  },
  {
    "text": "backdrop of there's an operator there are server PA so let's talk talk a little bit about how autoscaling",
    "start": "238920",
    "end": "245319"
  },
  {
    "text": "works so there are specifically two uh components in any autoscaling infrastructure so and that's what we",
    "start": "245319",
    "end": "251799"
  },
  {
    "text": "built ourselves as well uh the first bit is the recommender right so Auto scaling starts with the data it starts with uh",
    "start": "251799",
    "end": "258120"
  },
  {
    "text": "metrics so in this case we can see from the left we have like click house itself exposes a lot of metrics that we collect",
    "start": "258120",
    "end": "264639"
  },
  {
    "text": "uh we also have query log itself which is actually accounting for uh the memory that a particular quer is going to take",
    "start": "264639",
    "end": "270759"
  },
  {
    "text": "and we are able to do this because actually click house keeps track of in it its own memory internally we also collect kuet metrics",
    "start": "270759",
    "end": "277680"
  },
  {
    "text": "because that's important that also gives us another view about like CPU utilization Etc and all of this data",
    "start": "277680",
    "end": "283440"
  },
  {
    "text": "gets collected inside a metric database which is coincidentally also a click house",
    "start": "283440",
    "end": "288919"
  },
  {
    "text": "cluster and the second half of this equation is the recommender so this is a controller which is periodically running",
    "start": "288919",
    "end": "294840"
  },
  {
    "text": "looking at this data and then making decisions about what should be the right size of any particular uh cluster right",
    "start": "294840",
    "end": "301840"
  },
  {
    "text": "so this is actually the component generating recommendations and this is where the algorithm lives right so you",
    "start": "301840",
    "end": "307680"
  },
  {
    "text": "can start with heuristics and you can make it as complicated and as fancy as you want it to be uh so any uh smart and",
    "start": "307680",
    "end": "313960"
  },
  {
    "text": "sophisticated autoscaling strategies that you want to do it you probably want to do it in the recommender so that's what we use as well and so once you have",
    "start": "313960",
    "end": "321639"
  },
  {
    "text": "once you have this pipeline flowing you start from the metrics and then you go at the end you have these recommendations being generated how do",
    "start": "321639",
    "end": "327560"
  },
  {
    "text": "you actually make use of them how do you actually do do Auto scaling based on these recommendations so this is where the",
    "start": "327560",
    "end": "333639"
  },
  {
    "text": "actual Auto scaling component comes in so we have as you can see from the left we have like two inputs one is the",
    "start": "333639",
    "end": "339039"
  },
  {
    "text": "recommendations that get generated the other is the user defined limits right so users probably want to have like",
    "start": "339039",
    "end": "344360"
  },
  {
    "text": "minimum and maximum limits to control their cost and so those get accounted for inside these recommendations so the",
    "start": "344360",
    "end": "350600"
  },
  {
    "text": "recommender could probably say hey I would like this clust this cluster is just so memory intensive I I want this",
    "start": "350600",
    "end": "355880"
  },
  {
    "text": "to be at like 100 gigabytes or something but the user has like a Max uh uh limit set so we actually account for that as",
    "start": "355880",
    "end": "362680"
  },
  {
    "text": "well and this is where things get a little interesting so once the autoscaler detects that hey I actually",
    "start": "362680",
    "end": "368360"
  },
  {
    "text": "have a cluster which is not rightly sized and I want to kind of scale it up or scale it down it's going to start",
    "start": "368360",
    "end": "373840"
  },
  {
    "text": "doing pod evictions and this is how it works inside the kubernetes vpa as well uh which is uh uh the moment we detect",
    "start": "373840",
    "end": "380880"
  },
  {
    "text": "that we need to do some uh like right sizing we evict a part and then we resize it and this is how it looks like",
    "start": "380880",
    "end": "386680"
  },
  {
    "text": "so the autoscaler is going to like trigger this PO eviction uh you can see the pod on the right here and what's",
    "start": "386680",
    "end": "392240"
  },
  {
    "text": "going to happen is that a controller is going to come in and in this case it's just the state fulls side controller and this controller is going to resubmit",
    "start": "392240",
    "end": "398840"
  },
  {
    "text": "that pod which is very natural we know that and this is where the magic happens uh so as soon as the Pod gets",
    "start": "398840",
    "end": "404800"
  },
  {
    "text": "resubmitted uh we have a mutating web hook and this web Hook is going to intercept that pod submission and then",
    "start": "404800",
    "end": "410639"
  },
  {
    "text": "it's going to mutate the resource request right so what happens here is that uh the controller will will probably have its own idea of what the",
    "start": "410639",
    "end": "417440"
  },
  {
    "text": "size should be but the web hook which is a aware of the recommendation as well as the limits it's going to say no no no",
    "start": "417440",
    "end": "423000"
  },
  {
    "text": "you actually need to write it uh size this pod according to this recommendation that I have and so this",
    "start": "423000",
    "end": "428479"
  },
  {
    "text": "is how uh uh you can actually size this pod and we can see that in this case the Pod got sized to like a bigger",
    "start": "428479",
    "end": "436039"
  },
  {
    "text": "size so uh we understand how the autoscaling infrastructure Works in general now and this is uh uh you know",
    "start": "436120",
    "end": "442120"
  },
  {
    "text": "anyone who knows vpa kind of knows this we just happen to be building these components ourselves in order to get some more fine grain control over how to",
    "start": "442120",
    "end": "449319"
  },
  {
    "text": "do do evictions Etc so we don't use those out of the box components uh specifically in vertical",
    "start": "449319",
    "end": "456199"
  },
  {
    "text": "scaling this is kind of what we want all right so this is like an example of you probably have like three server pods running with 16 gigs each and you want",
    "start": "456199",
    "end": "462840"
  },
  {
    "text": "to scale them up uh to go to 32 gigs and this is like a fairly normal requirement",
    "start": "462840",
    "end": "468159"
  },
  {
    "text": "for vertical scaling and this is what we mean when we say vertical scaling is actually break",
    "start": "468159",
    "end": "473840"
  },
  {
    "text": "first right so it's break first because you actually like we just discussed you have to evict a pod and that eviction is",
    "start": "473840",
    "end": "480240"
  },
  {
    "text": "like a restart and that's causing A disruption to the customer's workload so while we are actually trying to resize",
    "start": "480240",
    "end": "486240"
  },
  {
    "text": "this customer this pod is actually in the process of getting restarted we have a pdb which is uh like uh maximum",
    "start": "486240",
    "end": "492039"
  },
  {
    "text": "unavailable is always going to be one it's uh we don't do zero because that's kind of unreasonable uh so maximum",
    "start": "492039",
    "end": "498199"
  },
  {
    "text": "unavailable Parts is just one and so what happens there is that",
    "start": "498199",
    "end": "503240"
  },
  {
    "text": "you resize these replicas one at a time right so you start from the last replica you resize it it takes its sweet time to",
    "start": "503240",
    "end": "509720"
  },
  {
    "text": "come up and then you uh do the same thing with the second one and then do the same thing again and at the end you",
    "start": "509720",
    "end": "515279"
  },
  {
    "text": "have like a scaleup operation that's complete so this is pretty slow right so and so what happens is customers who",
    "start": "515279",
    "end": "522320"
  },
  {
    "text": "want to like kind of get the benefits of scaling they complain that hey the scaling is just not reactive and this is",
    "start": "522320",
    "end": "528240"
  },
  {
    "text": "the reason Auto scaling in Click house cloud is sometimes not that",
    "start": "528240",
    "end": "533240"
  },
  {
    "text": "reactive the other problem that vertical scaling can cause it it can cause additional pressure on the remaining",
    "start": "533399",
    "end": "538480"
  },
  {
    "text": "replicas right so in this case we can see that while this third replica is getting restarted uh it's actually",
    "start": "538480",
    "end": "543600"
  },
  {
    "text": "putting more pressure on the existing replicas so you have this uh like kind of tension here where you actually",
    "start": "543600",
    "end": "549560"
  },
  {
    "text": "wanted to scale up because you wanted more resources but what you ended up doing was in order to get those",
    "start": "549560",
    "end": "554839"
  },
  {
    "text": "resources you ended up taking away the resources you had for some time and in",
    "start": "554839",
    "end": "560320"
  },
  {
    "text": "the worst scenario what what this can lead to is is like these other replicas they're just going to crumble under the",
    "start": "560320",
    "end": "565560"
  },
  {
    "text": "load and they're just going to like get start getting throttled uh maybe get o killed or uh you know those kind of",
    "start": "565560",
    "end": "571399"
  },
  {
    "text": "things and customers start to complain about this so vertical scaling as it exists is kind of uh a deal breaker we",
    "start": "571399",
    "end": "578480"
  },
  {
    "text": "want to move away from this model right yeah like I just said it's",
    "start": "578480",
    "end": "584360"
  },
  {
    "text": "slow replaces one part at a time it's disruptive puts pressure the remaining replicas get squeezed and one last thing",
    "start": "584360",
    "end": "591839"
  },
  {
    "text": "is we actually maintain some overhead to kind of combat this so in our recommendations we account for this fact",
    "start": "591839",
    "end": "597000"
  },
  {
    "text": "that hey actually we want to resize this replica to you know some number let's say 32 gigs but we know that it's going",
    "start": "597000",
    "end": "603440"
  },
  {
    "text": "to get squeezed so let's like kind of uh have some percentage as an overhead and",
    "start": "603440",
    "end": "608519"
  },
  {
    "text": "uh that also means kind of the customer is actually uh the utilization uh was",
    "start": "608519",
    "end": "613800"
  },
  {
    "text": "the desired workload for the utilization was kind of lower but just because of the nature of vertical scaling we",
    "start": "613800",
    "end": "619120"
  },
  {
    "text": "account for that in the overhead that's kind of bad right we actually want to kind of reduce our overhead and get the",
    "start": "619120",
    "end": "624200"
  },
  {
    "text": "customers utilization should be as close to the allocation as possible",
    "start": "624200",
    "end": "630079"
  },
  {
    "text": "so uh kubernetes uh experts will know that there is this cap uh which allows for this uh interesting new feature",
    "start": "630079",
    "end": "636320"
  },
  {
    "text": "which is inplace po vertical scaling uh this allows for resizing of pods in place and this seems like a very",
    "start": "636320",
    "end": "643240"
  },
  {
    "text": "convenient feature there is actually no restart needed uh and so why am I here talking about vertical scaling and this",
    "start": "643240",
    "end": "649800"
  },
  {
    "text": "disruption when this feature already exists in kubernetes uh the last bullet point kind of gives it away which is",
    "start": "649800",
    "end": "656240"
  },
  {
    "text": "like it's an alpha feature there are some limitations to it uh so we Pro we don't want to use it right now but there",
    "start": "656240",
    "end": "662680"
  },
  {
    "text": "is another important point which is why we don't use it and that's packing uh my",
    "start": "662680",
    "end": "668000"
  },
  {
    "text": "colleagues JN Fe and Vin actually did a talk yesterday about how we are using most allocated scheduling strategy in",
    "start": "668000",
    "end": "673040"
  },
  {
    "text": "inside click house Cloud to pack uh the pots as efficiently as possible but this is kind of just to give you like a",
    "start": "673040",
    "end": "678279"
  },
  {
    "text": "glimpse of what's Happening Here so in this scenario you have like three server pots running on a single node and we can",
    "start": "678279",
    "end": "684240"
  },
  {
    "text": "see there is some room for the uh the the at least one partt to get resized in place",
    "start": "684240",
    "end": "689760"
  },
  {
    "text": "without doing any sort of disruption right so let's see what happens so yeah there is enough room and we can just",
    "start": "689760",
    "end": "696120"
  },
  {
    "text": "resize this replica in place and we're happy right uh but uh this kind again",
    "start": "696120",
    "end": "701240"
  },
  {
    "text": "this picture gives away the problem here in this situation which is that now the node is actually full now there is no more space left uh to kind of do any",
    "start": "701240",
    "end": "708560"
  },
  {
    "text": "more further resizes or like uh kind of increase the memory or the CPU of the this node as well yeah no room left for",
    "start": "708560",
    "end": "715800"
  },
  {
    "text": "the spot so uh the idea here is that even though the this API exists in kubernetes you can kind of take",
    "start": "715800",
    "end": "721680"
  },
  {
    "text": "advantage of this you can do vertical scaling in place it's actually best effort it's never going to give you a",
    "start": "721680",
    "end": "727560"
  },
  {
    "text": "guarantee that hey I will always do vertical skating in place for you it can also you can just request to do vertical",
    "start": "727560",
    "end": "732800"
  },
  {
    "text": "skating in place and this API can come in and it can it can just reject that request right and so the fundamental",
    "start": "732800",
    "end": "739480"
  },
  {
    "text": "tension here is that packing efficiency if you want to kind of optimize for cost and pack your PS as efficiently as",
    "start": "739480",
    "end": "745000"
  },
  {
    "text": "possible this is kind of uh uh like U working against this idea aidea of doing in place resizing so you can either have",
    "start": "745000",
    "end": "751920"
  },
  {
    "text": "like uh in place resizing with some buffer always which means you are eating the cost or you can pack as efficiently",
    "start": "751920",
    "end": "757800"
  },
  {
    "text": "as possible you probably don't want to have you can't have both additionally we actually have a homogeneous Fleet so",
    "start": "757800",
    "end": "763120"
  },
  {
    "text": "there are no like low priority cotant workloads like some sort of an API server or like different tiers of",
    "start": "763120",
    "end": "769079"
  },
  {
    "text": "workloads running where we can kind of evict this low priority pod to make room for this other like bigger to kind of do",
    "start": "769079",
    "end": "775399"
  },
  {
    "text": "in play scaling that's also not something that we do because we always want to pack as efficiently as",
    "start": "775399",
    "end": "781279"
  },
  {
    "text": "possible so now we come to uh like this idea that in place vertical scaling is not really working for us uh default",
    "start": "781279",
    "end": "788160"
  },
  {
    "text": "vertical scaling is kind of disruptive so maybe we can do something better and this is the approach that we've been calling make before break and the name",
    "start": "788160",
    "end": "795639"
  },
  {
    "text": "is also like there's a hint in there like how it works you just have to make new replicas before you break the",
    "start": "795639",
    "end": "800720"
  },
  {
    "text": "existing replicas so here you can see example we started with three different parts of uh",
    "start": "800720",
    "end": "807120"
  },
  {
    "text": "16 gigs each uh you'd actually don't need to do anything you just scale it out and kind of Leverage the horizontal",
    "start": "807120",
    "end": "812839"
  },
  {
    "text": "scaling to kind of uh get the additional capacity right uh so when you scale out",
    "start": "812839",
    "end": "818040"
  },
  {
    "text": "you get like a force replica and because we have the mutating web hook that we discussed early on uh when this Force",
    "start": "818040",
    "end": "823199"
  },
  {
    "text": "replica gets submitted it's always going to come up with like a bigger size and then you can just break any of",
    "start": "823199",
    "end": "828639"
  },
  {
    "text": "the existing three replicas and now you have like a single cycle where one Po got resized without any kind of A",
    "start": "828639",
    "end": "835720"
  },
  {
    "text": "disruption you can also take this like one step further like for maybe like some customers would like to have like",
    "start": "835720",
    "end": "841959"
  },
  {
    "text": "instant or like almost instant staling so you can just immediately double the capacity right so you can just have hey",
    "start": "841959",
    "end": "847480"
  },
  {
    "text": "I have 16 gigs replicas I can just go from 16 to 32 gig replicas and then I can just slowly uh drain away my other",
    "start": "847480",
    "end": "853399"
  },
  {
    "text": "older uh replicas and now I just have exactly what I wanted which is 3 32 gig gigs",
    "start": "853399",
    "end": "860440"
  },
  {
    "text": "replica so what are the advantages of this approach right so it's fast uh we kind of know that there is no more",
    "start": "860440",
    "end": "865600"
  },
  {
    "text": "restarting that's happening uh like you're you the pdb is kind of not getting in your way it's",
    "start": "865600",
    "end": "872079"
  },
  {
    "text": "non-disruptive uh there is no more pressure on existing pods because the existing pods can still continue to",
    "start": "872079",
    "end": "877880"
  },
  {
    "text": "serve queries as safely as possible until they're kind of ready to go away and because we have this we can now",
    "start": "877880",
    "end": "884320"
  },
  {
    "text": "reduce that extra overhead that we have been recommending so now we're going to talk",
    "start": "884320",
    "end": "890600"
  },
  {
    "text": "about like uh stateful sets and why why am I talking about stateful specifically here because mbb as an approach like",
    "start": "890600",
    "end": "896839"
  },
  {
    "text": "it's great on paper but it actually doesn't work with the Practical realities of how stateful sets today",
    "start": "896839",
    "end": "902160"
  },
  {
    "text": "exist in kubernetes and I'm going to explain how so this is the scaling behavior of a",
    "start": "902160",
    "end": "908639"
  },
  {
    "text": "single state full set today in kubernetes it's always going to scale out from left to right so uh and this is",
    "start": "908639",
    "end": "914040"
  },
  {
    "text": "why actually the title of this talk is ordinals because you're going to start with ordinal zero and then you're always",
    "start": "914040",
    "end": "919240"
  },
  {
    "text": "going to scale out uh like 1 2 3 4 five and as you can see when you're doing like this three and four and five",
    "start": "919240",
    "end": "925040"
  },
  {
    "text": "replicas they're kind of like this make so you can make arbitrary replicas inside the single stateful set the",
    "start": "925040",
    "end": "930839"
  },
  {
    "text": "problem is you cannot break the old replicas because if you follow the arrow in the inside the scale in if you try to",
    "start": "930839",
    "end": "937240"
  },
  {
    "text": "tell a stateful said hey I I would like to have just three replicas it's going to remove the three new ones you just",
    "start": "937240",
    "end": "943240"
  },
  {
    "text": "created and that's the opposite of what we want we want to actually break replica 0 1 and two we don't want to",
    "start": "943240",
    "end": "949360"
  },
  {
    "text": "break three four and five so stateful sets are getting in our way like this is the default",
    "start": "949360",
    "end": "956800"
  },
  {
    "text": "Behavior there is one uh slightly hacky way in which you can achieve this and",
    "start": "956880",
    "end": "962440"
  },
  {
    "text": "this is the feature in stateful sets called start ordinal feature uh it was introduced in this kep and so the idea",
    "start": "962440",
    "end": "968399"
  },
  {
    "text": "here is again simple so you start with three replicas you just change the replica count to four and then you get a",
    "start": "968399",
    "end": "974160"
  },
  {
    "text": "make operation that's fine so that's uh you're happy with your make and then you can just tell the stateful Set uh hey I",
    "start": "974160",
    "end": "981319"
  },
  {
    "text": "would like you to start your ordinals from position one instead of position zero and what this does is uh now you",
    "start": "981319",
    "end": "989120"
  },
  {
    "text": "can just what it means is that the PO zero is kind of going to get deleted right so which is again make before",
    "start": "989120",
    "end": "996279"
  },
  {
    "text": "break why is this something we uh like you know I said it's hacky so why is that it's hacky because you want the",
    "start": "996279",
    "end": "1003880"
  },
  {
    "text": "flexibility right you might have a backup running on replica zero and maybe replica one was the one you intended to",
    "start": "1003880",
    "end": "1009759"
  },
  {
    "text": "break because hey that one has enough Capac like it doesn't have enough queries running on it or maybe you can",
    "start": "1009759",
    "end": "1015120"
  },
  {
    "text": "drain it safely uh so you're not getting the flexibility that you need so you're always going to be uh like breaking in",
    "start": "1015120",
    "end": "1021720"
  },
  {
    "text": "this strict order think of it like a sliding window moving from left to right in which you're going to break this so",
    "start": "1021720",
    "end": "1027438"
  },
  {
    "text": "this feature is kind of works kind of gives you what you need in terms of make before break but not really what you wanted to use you want the",
    "start": "1027439",
    "end": "1035279"
  },
  {
    "text": "flexibility yeah so breaking arbitrary PA so we've kind of defined the problem here right so vertical scaling uh what",
    "start": "1035880",
    "end": "1042959"
  },
  {
    "text": "what are the problems there we wanted do make before break and make is fine but we want to break these arbitrary pods",
    "start": "1042959",
    "end": "1048038"
  },
  {
    "text": "and we are not the first first to face these problems there are uh like you know other projects out there which allow you to do selective pod delion",
    "start": "1048039",
    "end": "1054799"
  },
  {
    "text": "specifically we looked at a few of them like Advanced State full sets by pink cap and open cruise and the key idea",
    "start": "1054799",
    "end": "1060240"
  },
  {
    "text": "that all of these projects follow is this idea of delete Slots and I'm going to explain what that",
    "start": "1060240",
    "end": "1066360"
  },
  {
    "text": "is uh so as you can see here in advanced a full sets the way it works is you can just specify the replica count as well",
    "start": "1066400",
    "end": "1072320"
  },
  {
    "text": "as the delete Slots in like in an atomic operation so when you do this what happens is uh you're starting to three",
    "start": "1072320",
    "end": "1079039"
  },
  {
    "text": "replicas because the the controller has to maintain the replica count it will make a new pod which is the third part",
    "start": "1079039",
    "end": "1084919"
  },
  {
    "text": "that's ordinal number three that's coming up and because you also specify the delete Slots it's going to break the",
    "start": "1084919",
    "end": "1090440"
  },
  {
    "text": "delete slot that you wanted it to do right so this does what you wanted to do so what's the problem here like why did",
    "start": "1090440",
    "end": "1096280"
  },
  {
    "text": "we not use this particular solution to do make before break and the answer is really like it",
    "start": "1096280",
    "end": "1103280"
  },
  {
    "text": "was an experimental project we did not want to take ownership of this code base uh the default behavior is still falling",
    "start": "1103280",
    "end": "1109240"
  },
  {
    "text": "back to ordinals and my biggest gripe and my biggest complaint with this sort of solution is that I think ordinals are",
    "start": "1109240",
    "end": "1115360"
  },
  {
    "text": "kind of just hard to keep track of when you when they have gaps in them and I think this goes to the nature of ordinals right they are or you kind of",
    "start": "1115360",
    "end": "1122280"
  },
  {
    "text": "expect replicas to be in a continuous order so when you're like an on call engineer and you see replica 70 replica",
    "start": "1122280",
    "end": "1128039"
  },
  {
    "text": "72 and replica 73 you're going to ask the question hey what happened to 71 is it in the delete slot and you're going",
    "start": "1128039",
    "end": "1133480"
  },
  {
    "text": "to have to do this mental math and look at the CR and see what's going on and over time it's especially with",
    "start": "1133480",
    "end": "1139200"
  },
  {
    "text": "horizontal scaling inside and the mix this is just going to get ugly right so we kind of I started to think okay maybe",
    "start": "1139200",
    "end": "1145679"
  },
  {
    "text": "we want to move away from this idea of ordinals all together and look at a different",
    "start": "1145679",
    "end": "1151360"
  },
  {
    "text": "solution so open Cruise again is it's a separate project it's got two different controllers uh one is also",
    "start": "1151360",
    "end": "1157480"
  },
  {
    "text": "coincidentally called Advanced stateful sets the other is called clone sets so the open Cruise Advanced stateful set",
    "start": "1157480",
    "end": "1163600"
  },
  {
    "text": "has the same uh downsize that uh I just mentioned about the pin cap controller so we didn't really take a look at that",
    "start": "1163600",
    "end": "1169480"
  },
  {
    "text": "very seriously clone sets were the interesting ones they are actually supposed to be a replacement for deployments but they support like volume",
    "start": "1169480",
    "end": "1176159"
  },
  {
    "text": "claim templates so you can do things like uh a stateful you can run stateful workloads on top of them but notice here",
    "start": "1176159",
    "end": "1182039"
  },
  {
    "text": "that the Pod name is actually random it's not ordered and that was kind of where uh I started to move towards that",
    "start": "1182039",
    "end": "1187120"
  },
  {
    "text": "hey instead of ordering pods maybe we just want Randomness inside of our names and kind of treat everything equally so",
    "start": "1187120",
    "end": "1192880"
  },
  {
    "text": "there is no server zero is not special and there's no unintuitive behavior in terms of ordinals",
    "start": "1192880",
    "end": "1200080"
  },
  {
    "text": "this is what a clone set looks like so you have like uh five parts running the one I've highlighted in Bolter like you",
    "start": "1200080",
    "end": "1206360"
  },
  {
    "text": "can just select and delete that you can just add it to the spec and this will get deleted and the key thing to",
    "start": "1206360",
    "end": "1211840"
  },
  {
    "text": "remember here is that the operation of reducing the replica count as well as adding uh the brake part to the set that",
    "start": "1211840",
    "end": "1218559"
  },
  {
    "text": "has to be Atomic because otherwise you'll just get get a new part popping backup to maintain the the original",
    "start": "1218559",
    "end": "1224080"
  },
  {
    "text": "replica count but yeah so this kind of does what we want which is selective pod deletion you just have to like follow",
    "start": "1224080",
    "end": "1229960"
  },
  {
    "text": "this spec and then you will break any arbitrary po that you",
    "start": "1229960",
    "end": "1234919"
  },
  {
    "text": "want the catch which clone sets was actually specifically not with their arbitrary pod delion but with their pod",
    "start": "1235640",
    "end": "1241960"
  },
  {
    "text": "update strategy so clone set actually supports like recreate strategy as well as Inplay pot updates and uh where we",
    "start": "1241960",
    "end": "1249679"
  },
  {
    "text": "got stuck was just uh a PVC deletion as well as PVC Recreation so when you",
    "start": "1249679",
    "end": "1255840"
  },
  {
    "text": "recreate a PVC when you update a pod uh the the Clone set controller is always going to recreate the PVC and that's not",
    "start": "1255840",
    "end": "1261720"
  },
  {
    "text": "what you want for a database for clod you actually and so the only thing you're left to do is do in place updates",
    "start": "1261720",
    "end": "1268799"
  },
  {
    "text": "the catch there is if you do in place updates you actually can't modify your resources or even update your environments so you get a choice you",
    "start": "1268799",
    "end": "1275919"
  },
  {
    "text": "either can resize your pod or you can retain your PVC you can't do both and that was kind of like a place where we",
    "start": "1275919",
    "end": "1283039"
  },
  {
    "text": "got stuck and clone sets were just we wanted to have the best of both world right so we can't have that with clone",
    "start": "1283039",
    "end": "1288240"
  },
  {
    "text": "sets Okay so we've looked at a few Solutions and we realized okay no popular active",
    "start": "1288240",
    "end": "1295240"
  },
  {
    "text": "outof thebox solution is going to work for us for various reasons that we discussed uh and this is like a quick",
    "start": "1295240",
    "end": "1300880"
  },
  {
    "text": "recap of our requirement we want a stateful set or something like a stateful set uh we would like to move",
    "start": "1300880",
    "end": "1306360"
  },
  {
    "text": "away from ordinals uh delete Slots are a good idea so we want that we want to retain our PVCs we want to respect the",
    "start": "1306360",
    "end": "1312600"
  },
  {
    "text": "disruption budget for things like upgrades we still want to have that and I'm going to talk about topology spread",
    "start": "1312600",
    "end": "1317760"
  },
  {
    "text": "constraints later because they get pretty important once we go towards our solution and so the idea was we can just",
    "start": "1317760",
    "end": "1325640"
  },
  {
    "text": "use the stateful sets just use them in a different way so this is what we call multi SS and",
    "start": "1325640",
    "end": "1332159"
  },
  {
    "text": "it's very simple uh the vanilla stateful set is pretty powerful we can leverage it uh the very interesting property that",
    "start": "1332159",
    "end": "1338480"
  },
  {
    "text": "the stateful set gives us is that we can still have operator downtime right so the operator is the code that we are",
    "start": "1338480",
    "end": "1344640"
  },
  {
    "text": "changing almost every day while we're developing it and if we wanted to kind",
    "start": "1344640",
    "end": "1349960"
  },
  {
    "text": "of directly manage our thoughts inside the operator we would have to kind of risk the fact that we could introduce",
    "start": "1349960",
    "end": "1355840"
  },
  {
    "text": "bugs inside of our operator and the cost of that will be pretty high right so imagine like you're writing code which",
    "start": "1355840",
    "end": "1362200"
  },
  {
    "text": "is kind of doing something with a pod and the Pod gets kind of accidentally deleted or something that's not what you",
    "start": "1362200",
    "end": "1367320"
  },
  {
    "text": "want to do so the stateful set is a battle tou controller so we wanted to leverage that and the other thing is",
    "start": "1367320",
    "end": "1373360"
  },
  {
    "text": "it's relatively easy to go from like one thing to many things inside your code base just in terms of refactoring",
    "start": "1373360",
    "end": "1379480"
  },
  {
    "text": "instead of introducing like a new custom resource like an advanced stateful set or a clone set and then doing a migration remember we still had",
    "start": "1379480",
    "end": "1385480"
  },
  {
    "text": "customers running on the single STDs so we had to migrate which is which can be like a whole separate talk of its own",
    "start": "1385480",
    "end": "1392240"
  },
  {
    "text": "from like single STDs to this multi SS model but yeah so the key idea here is just use one stateful set per",
    "start": "1392240",
    "end": "1398919"
  },
  {
    "text": "pod and this is what it looks like right so this is the the same image from the beginning you have a sing the operator",
    "start": "1398919",
    "end": "1404559"
  },
  {
    "text": "creating the stateful set managing three parts but now we can just do this so we have like three different stateful sets",
    "start": "1404559",
    "end": "1411159"
  },
  {
    "text": "each of them with a random suffix uh so when you have these suffix and then you can kind of implement your own delete",
    "start": "1411159",
    "end": "1417679"
  },
  {
    "text": "Slots and then you will have the ability to kind of make new replicas as well as break any arbitrary replicas and you can",
    "start": "1417679",
    "end": "1423440"
  },
  {
    "text": "achieve make before break vertical scaling without any of the problems that we discussed",
    "start": "1423440",
    "end": "1429600"
  },
  {
    "text": "before yeah so this is just a quick recap of what it looks like so you have three stateful sets managing three parts",
    "start": "1429600",
    "end": "1436240"
  },
  {
    "text": "uh you do a make the fourth part comes in it's a b it's like a bigger one uh we introduced something like a",
    "start": "1436240",
    "end": "1442960"
  },
  {
    "text": "condemn step and this is kind of an intermediate part uh step where the Pod is marked for deletion and this is done",
    "start": "1442960",
    "end": "1449240"
  },
  {
    "text": "mostly for database reasons where uh click house it's not really trivial to immediately get rid of a replica and the",
    "start": "1449240",
    "end": "1455480"
  },
  {
    "text": "database figures everything out automatically you have to do some sort of synchronization operations not very many but there are still a few steps you",
    "start": "1455480",
    "end": "1462480"
  },
  {
    "text": "have to do so inside the operator code path you actually condemn the replica first do this synchronization and then",
    "start": "1462480",
    "end": "1468799"
  },
  {
    "text": "you break that replica and so this is the way we achieve make before break and again all the benefits that I just",
    "start": "1468799",
    "end": "1474559"
  },
  {
    "text": "described previously you can do instant uh scaling uh uh no disruptions you can do like uh your performance is great",
    "start": "1474559",
    "end": "1482640"
  },
  {
    "text": "now so the requirements for implementing multi SS were not very many but still worth talking about uh we needed stable",
    "start": "1482640",
    "end": "1489960"
  },
  {
    "text": "identities so this is something that the ordinals kind of give you always so po zero is always going to be po zero if",
    "start": "1489960",
    "end": "1495279"
  },
  {
    "text": "you remove it it's always going to come back with the same name and this table identity is important for your uh you",
    "start": "1495279",
    "end": "1500440"
  },
  {
    "text": "know host name as well as PVC name other reasons uh the way we do it is we kind",
    "start": "1500440",
    "end": "1506000"
  },
  {
    "text": "of start to manage the state inside the operator ourselves and uh kind of use the stable identity uh for the random",
    "start": "1506000",
    "end": "1512679"
  },
  {
    "text": "suffixes that we are generating as well you have to do horizontal scaling which is uh which was fairly trivial uh to do",
    "start": "1512679",
    "end": "1518320"
  },
  {
    "text": "in terms of scaling out scaling in is where some of the challenges uh came into place I'm going to talk about them",
    "start": "1518320",
    "end": "1523919"
  },
  {
    "text": "uh rolling updates you kind of get for free because the Pod disruption budget actually works on top of lab label selectors so it doesn't matter if one",
    "start": "1523919",
    "end": "1530480"
  },
  {
    "text": "stateful set is managing the Pod or many stateful sets as long as you have the right labels you're still going to get",
    "start": "1530480",
    "end": "1535520"
  },
  {
    "text": "your pdb and again topology spread constraints I'm going to talk about them specifically because that was the one",
    "start": "1535520",
    "end": "1541240"
  },
  {
    "text": "area where we ran into a lot of challenges and the only new feature that you're really adding here is the delete",
    "start": "1541240",
    "end": "1549240"
  },
  {
    "text": "Slots so topology spread constraints and what were the problems with it uh inside",
    "start": "1550120",
    "end": "1556039"
  },
  {
    "text": "click house clouds just to set the context if you have like a service with three different replicas joining all the",
    "start": "1556039",
    "end": "1561440"
  },
  {
    "text": "three replicas are going to be placed in different zones and of course we do this for availability reasons and what",
    "start": "1561440",
    "end": "1566600"
  },
  {
    "text": "happens when you try to do like make before break with topology spread constraints so here we have an example",
    "start": "1566600",
    "end": "1571799"
  },
  {
    "text": "of we have like initially we have two pods A and B and now we want to make two new parts we can do this and these two",
    "start": "1571799",
    "end": "1577919"
  },
  {
    "text": "parts will go into Zone C so a b and c here are availability zones and ab BCC is actually just a valid",
    "start": "1577919",
    "end": "1585080"
  },
  {
    "text": "topology the problem comes now when we actually want to break those old replicas right so what happens when you",
    "start": "1585080",
    "end": "1590399"
  },
  {
    "text": "break a and b you actually have both of the the remaining replicas inside Zone C",
    "start": "1590399",
    "end": "1595559"
  },
  {
    "text": "and now your Zone C becomes a single zone point of failure and it's actually not a valid topology if you have your",
    "start": "1595559",
    "end": "1600880"
  },
  {
    "text": "max Q set to one another problem we ran into with",
    "start": "1600880",
    "end": "1606880"
  },
  {
    "text": "topology spread constraints in mbb was the fact that we actually like I mentioned earlier we actually have idling inside click house Cloud as well",
    "start": "1606880",
    "end": "1613640"
  },
  {
    "text": "so when you idle a cluster you actually have uh three stateful sets the parts are not scheduling anymore but the PVCs",
    "start": "1613640",
    "end": "1619640"
  },
  {
    "text": "are still lying around so when the state full set controller says the desired count to one again uh these SPS will",
    "start": "1619640",
    "end": "1626399"
  },
  {
    "text": "wake up they will get scheduled and then they will reattach to this these PVCs right but you can have a situation where",
    "start": "1626399",
    "end": "1633080"
  },
  {
    "text": "a customer wants to add a new replica before their cluster wakes up so you can imagine I have three replicas before but",
    "start": "1633080",
    "end": "1640799"
  },
  {
    "text": "before uh and my cluster is idling I can just dial like a setting in my uh UI and",
    "start": "1640799",
    "end": "1646159"
  },
  {
    "text": "just say please give me four replicas when you wake up and this is where a race condition happens where the stateful set",
    "start": "1646159",
    "end": "1652679"
  },
  {
    "text": "controller can just IM immediately schedule the new replica but the old one can sometimes take its sweet time to",
    "start": "1652679",
    "end": "1658360"
  },
  {
    "text": "come back up and when that happens you can have a race here because the stateful set controller only takes the",
    "start": "1658360",
    "end": "1665360"
  },
  {
    "text": "scheduled Parts into account when making the decision about Zone placement right so you can see here like you have like",
    "start": "1665360",
    "end": "1671760"
  },
  {
    "text": "the initial two parts are in zones A and B and the third the the new replica that came in it also came in with Zone a",
    "start": "1671760",
    "end": "1677919"
  },
  {
    "text": "because the only uh consideration we had while we were making the decision about the zones Was A and B we actually never",
    "start": "1677919",
    "end": "1684200"
  },
  {
    "text": "looked at this replica marked in red because we never took the PVCs into account and that's something I think",
    "start": "1684200",
    "end": "1689760"
  },
  {
    "text": "this is also true for the default stateful set but this situation actually would not have happened with default",
    "start": "1689760",
    "end": "1695960"
  },
  {
    "text": "State full sets because it will if you remember the arrow that we were going from right to left uh if you were to",
    "start": "1695960",
    "end": "1702440"
  },
  {
    "text": "kind of do like a replica count redu it to two it will remove CC so this situation is avoidable with stateful",
    "start": "1702440",
    "end": "1708720"
  },
  {
    "text": "sets not with mbb but this one I think is still kind of something I I might have to test that",
    "start": "1708720",
    "end": "1713880"
  },
  {
    "text": "but I think this race condition also exists with the stateful set controller today so uh there are all these uh",
    "start": "1713880",
    "end": "1721159"
  },
  {
    "text": "problems that we ran into with uh topology spread constraints and really the way to solve them was kind of do",
    "start": "1721159",
    "end": "1727399"
  },
  {
    "text": "Zone pinning which is kind of we started to decide uh which availability Zone pod",
    "start": "1727399",
    "end": "1733279"
  },
  {
    "text": "is going to get placed into and this this logic kind of is now living inside",
    "start": "1733279",
    "end": "1738440"
  },
  {
    "text": "of our operator where the operator makes the decision about which zones to place the Pod into and because uh like",
    "start": "1738440",
    "end": "1745320"
  },
  {
    "text": "previously like we mentioned like uh it wasn't taking PVCs into account the operator can kind of just do that so we",
    "start": "1745320",
    "end": "1751000"
  },
  {
    "text": "count the PVCs figure out what's the Zone balance and then decide which zone to place the pot",
    "start": "1751000",
    "end": "1757240"
  },
  {
    "text": "into so yeah long story short uh we have like uh breaking arbitrary pots it does not work naturally with topology spread",
    "start": "1757240",
    "end": "1763760"
  },
  {
    "text": "constraints so that's the only catch if you would like to implement make before break yourself",
    "start": "1763760",
    "end": "1768799"
  },
  {
    "text": "uh and the same reasoning also applies with horizontal scaling so your scaling in operations have to be Zone aware so",
    "start": "1768799",
    "end": "1774720"
  },
  {
    "text": "just keep that in mind the scheduler like I said is not really aware of existing PVCs it's only",
    "start": "1774720",
    "end": "1781679"
  },
  {
    "text": "aware of scheduled pods so uh We've covered all this and so",
    "start": "1781679",
    "end": "1787799"
  },
  {
    "text": "why am I talking about make before break multiple State full sets and like you know we ran into these problems but we are not really the first ones stateful",
    "start": "1787799",
    "end": "1794399"
  },
  {
    "text": "services on kubernetes is hard I'm sure many of you here know this already Cloud native PG which is a very famous uh post",
    "start": "1794399",
    "end": "1801000"
  },
  {
    "text": "operator uh also decided to write a custom controller to manage the PO themselves uh stmy actually the",
    "start": "1801000",
    "end": "1808000"
  },
  {
    "text": "highlighted section here mentions the exact reason that we have which is uh you know the ordinals are a problem when",
    "start": "1808000",
    "end": "1814640"
  },
  {
    "text": "you want to do any arbitrary pod deletion inside stateful set stream for for those of you who don't know it's a Kafka operator for kubernetes so yeah",
    "start": "1814640",
    "end": "1822679"
  },
  {
    "text": "it's like not a new problem but the stateful set controller just doesn't give us the flexibility that we need",
    "start": "1822679",
    "end": "1829880"
  },
  {
    "text": "so uh just a recap over what we've covered so far vertical scaling for stateful services it can be very slow it",
    "start": "1829880",
    "end": "1835960"
  },
  {
    "text": "can be disruptive database workloads are kind of important you don't want to you want to be very very careful about this",
    "start": "1835960",
    "end": "1841919"
  },
  {
    "text": "in place pod resizing is a tempting feature does not always work if you want to pack as efficiently as possible the",
    "start": "1841919",
    "end": "1848880"
  },
  {
    "text": "stateful sets are not really as flexible as we would like them to be you can't do arbitrary pod uh",
    "start": "1848880",
    "end": "1854799"
  },
  {
    "text": "deletion and uh a work uh the solution to this is something like that we have been calling make before break you can",
    "start": "1854799",
    "end": "1861080"
  },
  {
    "text": "try to use it delete Slots because making again is easy Breaking is uh the hard bit here",
    "start": "1861080",
    "end": "1868080"
  },
  {
    "text": "and so we've been using delete Slots to kind of facilitate breaking of arbitrary replicas and you can use third- party",
    "start": "1868080",
    "end": "1873960"
  },
  {
    "text": "controllers that we discussed such as clone sets or Advanced stateful sets if you kind of don't want to go too much into writing your own code we had that",
    "start": "1873960",
    "end": "1881240"
  },
  {
    "text": "bandwidth I guess so we were able to kind of implement this multi SS approach but of course you can always use these",
    "start": "1881240",
    "end": "1886880"
  },
  {
    "text": "third party controllers be mindful of the caveats that I discussed and yeah topology issues are the ones that you",
    "start": "1886880",
    "end": "1893080"
  },
  {
    "text": "need to be very very careful about uh when we doing this kind of thing that is what I have for you today",
    "start": "1893080",
    "end": "1899399"
  },
  {
    "text": "thank you so much uh we are hiring so come talk to me if you are interested in these problems thank",
    "start": "1899399",
    "end": "1906180"
  },
  {
    "text": "[Applause]",
    "start": "1906180",
    "end": "1911720"
  },
  {
    "text": "you yeah so if anyone has",
    "start": "1911720",
    "end": "1915760"
  },
  {
    "text": "questions",
    "start": "1916840",
    "end": "1919840"
  },
  {
    "text": "hey Manish uh very interesting uh story on how you went through all these different things uh with the scaling one",
    "start": "1926200",
    "end": "1934639"
  },
  {
    "text": "uh important thing does your operator now because we didn't see much about the limits and requests uh in in this",
    "start": "1934639",
    "end": "1942039"
  },
  {
    "text": "context right we are talking about just the scaling part so in your case for your state full offer your databases now",
    "start": "1942039",
    "end": "1950240"
  },
  {
    "text": "how do you um have your limits and requests do you have a fixed reservation yeah so our limits are always equal to",
    "start": "1950240",
    "end": "1956799"
  },
  {
    "text": "the request we have like a quality of service guaranteed okay so that makes your life easy but not all state full",
    "start": "1956799",
    "end": "1963080"
  },
  {
    "text": "sets would be like that so we actually have for our click house keeper so we uh didn't mention this but click house uses",
    "start": "1963080",
    "end": "1968960"
  },
  {
    "text": "uh its own like a zookeeper replacement called click house keeper uh and inside the keeper stateful set we actually have",
    "start": "1968960",
    "end": "1974880"
  },
  {
    "text": "a a recently set burstable CPU so the limits are actually different can work I",
    "start": "1974880",
    "end": "1980120"
  },
  {
    "text": "think so in that case also now you go through all these things uh to schedule the no no no so this is we do this only",
    "start": "1980120",
    "end": "1986039"
  },
  {
    "text": "for the server pods okay",
    "start": "1986039",
    "end": "1989679"
  },
  {
    "text": "thanks all right thank",
    "start": "1993880",
    "end": "1997519"
  },
  {
    "text": "you",
    "start": "1999360",
    "end": "2002360"
  }
]