[
  {
    "text": "good evening everyone good evening",
    "start": "480",
    "end": "3080"
  },
  {
    "text": "fellow Tech",
    "start": "3080",
    "end": "5520"
  },
  {
    "text": "enthusiasts this is a session that uh",
    "start": "5520",
    "end": "9160"
  },
  {
    "text": "would be jointly delivered by me I am NV",
    "start": "9160",
    "end": "12599"
  },
  {
    "text": "Kumar and by my colleague joural",
    "start": "12599",
    "end": "16720"
  },
  {
    "text": "Ahmed as part of this session we are",
    "start": "16720",
    "end": "20240"
  },
  {
    "text": "going to focus on effortless",
    "start": "20240",
    "end": "24240"
  },
  {
    "text": "scalability",
    "start": "24240",
    "end": "26080"
  },
  {
    "text": "now as we are moving into this journey",
    "start": "26080",
    "end": "29160"
  },
  {
    "text": "wherein we are moving towards",
    "start": "29160",
    "end": "32078"
  },
  {
    "text": "AI uh we have llms which are the common",
    "start": "32079",
    "end": "38399"
  },
  {
    "text": "place over which we have our",
    "start": "38399",
    "end": "41920"
  },
  {
    "text": "applications now one of the key",
    "start": "41920",
    "end": "43840"
  },
  {
    "text": "challenges like if we go back a few",
    "start": "43840",
    "end": "46440"
  },
  {
    "text": "years back we had our CPUs we had our",
    "start": "46440",
    "end": "50399"
  },
  {
    "text": "backend applications web applications",
    "start": "50399",
    "end": "53320"
  },
  {
    "text": "and we were able to run multiple backend",
    "start": "53320",
    "end": "58160"
  },
  {
    "text": "applications uh split",
    "start": "58160",
    "end": "60960"
  },
  {
    "text": "uh across the",
    "start": "60960",
    "end": "63040"
  },
  {
    "text": "CPUs now we are moving into an age",
    "start": "63040",
    "end": "68040"
  },
  {
    "text": "wherein AI applications are going to",
    "start": "68040",
    "end": "70920"
  },
  {
    "text": "become a center stage and we are moving",
    "start": "70920",
    "end": "75000"
  },
  {
    "text": "beyond CPUs Into the",
    "start": "75000",
    "end": "78080"
  },
  {
    "text": "gpus Now do we have a similar support",
    "start": "78080",
    "end": "82400"
  },
  {
    "text": "today uh are we able to scale our AI",
    "start": "82400",
    "end": "86200"
  },
  {
    "text": "applications",
    "start": "86200",
    "end": "87759"
  },
  {
    "text": "similarly Maybe it has not been there",
    "start": "87759",
    "end": "91320"
  },
  {
    "text": "till now and this is where we are going",
    "start": "91320",
    "end": "94159"
  },
  {
    "text": "to talk about how we can do this part so",
    "start": "94159",
    "end": "98600"
  },
  {
    "text": "as you all know uh a critical challenge",
    "start": "98600",
    "end": "102399"
  },
  {
    "text": "that exist is in terms of whether you",
    "start": "102399",
    "end": "105520"
  },
  {
    "text": "can have multiple applications running",
    "start": "105520",
    "end": "108759"
  },
  {
    "text": "onto a particular GPU or can you split",
    "start": "108759",
    "end": "115000"
  },
  {
    "text": "your applications and run across",
    "start": "115000",
    "end": "118920"
  },
  {
    "text": "different gpus",
    "start": "118920",
    "end": "121840"
  },
  {
    "text": "that that is the problem that we are",
    "start": "122280",
    "end": "124640"
  },
  {
    "text": "trying to solve and let me take you",
    "start": "124640",
    "end": "126560"
  },
  {
    "text": "through the Journey so when we look at",
    "start": "126560",
    "end": "129800"
  },
  {
    "text": "the large language models uh we had at",
    "start": "129800",
    "end": "133360"
  },
  {
    "text": "at the start of 2020 we had gpt3 that",
    "start": "133360",
    "end": "136720"
  },
  {
    "text": "had just come into the market we were",
    "start": "136720",
    "end": "139840"
  },
  {
    "text": "very new into the AI",
    "start": "139840",
    "end": "142080"
  },
  {
    "text": "ecosystem uh and when I am talking uh AI",
    "start": "142080",
    "end": "147160"
  },
  {
    "text": "I'm more meaning from the llm side and",
    "start": "147160",
    "end": "150280"
  },
  {
    "text": "as we have moved further and as we look",
    "start": "150280",
    "end": "153599"
  },
  {
    "text": "lately we have an influx of models uh",
    "start": "153599",
    "end": "157959"
  },
  {
    "text": "they could be from Amazon",
    "start": "157959",
    "end": "161080"
  },
  {
    "text": "Google Microsoft anthropic so different",
    "start": "161080",
    "end": "165480"
  },
  {
    "text": "llms from different organizations and it",
    "start": "165480",
    "end": "167879"
  },
  {
    "text": "is becoming a clouded space and as you",
    "start": "167879",
    "end": "170760"
  },
  {
    "text": "would look uh the large language models",
    "start": "170760",
    "end": "174840"
  },
  {
    "text": "now they are being trained on higher",
    "start": "174840",
    "end": "177959"
  },
  {
    "text": "number of parameters so 175 billion as I",
    "start": "177959",
    "end": "182319"
  },
  {
    "text": "would say uh the cut off Mark wherein",
    "start": "182319",
    "end": "185799"
  },
  {
    "text": "you would see lot of uh larger",
    "start": "185799",
    "end": "189000"
  },
  {
    "text": "organizations are coming with these llms",
    "start": "189000",
    "end": "192440"
  },
  {
    "text": "which are fed on more than 175 billion",
    "start": "192440",
    "end": "196599"
  },
  {
    "text": "parameters now let's look at the",
    "start": "196599",
    "end": "199319"
  },
  {
    "text": "challenges of",
    "start": "199319",
    "end": "200640"
  },
  {
    "text": "deploying uh llm so one of the key",
    "start": "200640",
    "end": "204360"
  },
  {
    "text": "challenge obviously they are very",
    "start": "204360",
    "end": "206280"
  },
  {
    "text": "compute resource uh intensive uh you",
    "start": "206280",
    "end": "210280"
  },
  {
    "text": "would have to do the training you would",
    "start": "210280",
    "end": "213640"
  },
  {
    "text": "have to do uh inferencing and in between",
    "start": "213640",
    "end": "217159"
  },
  {
    "text": "there is lot of fine-tuning of the model",
    "start": "217159",
    "end": "219360"
  },
  {
    "text": "that needs to be done so all these take",
    "start": "219360",
    "end": "222560"
  },
  {
    "text": "lot of Compu resources now uh scaling",
    "start": "222560",
    "end": "227360"
  },
  {
    "text": "up uh scaling up with gpus that is",
    "start": "227360",
    "end": "231959"
  },
  {
    "text": "challenging because there are two",
    "start": "231959",
    "end": "234040"
  },
  {
    "text": "aspects that we have to keep in mind one",
    "start": "234040",
    "end": "236280"
  },
  {
    "text": "is the GPU RAM and the other is the AV",
    "start": "236280",
    "end": "240120"
  },
  {
    "text": "cach that we have and we all know that",
    "start": "240120",
    "end": "243640"
  },
  {
    "text": "internally we are using self ATT",
    "start": "243640",
    "end": "246400"
  },
  {
    "text": "attention mechanism and as part of that",
    "start": "246400",
    "end": "250159"
  },
  {
    "text": "uh this scaling is always happening",
    "start": "250159",
    "end": "255079"
  },
  {
    "text": "quadratically so what that means is if",
    "start": "255079",
    "end": "258120"
  },
  {
    "text": "we are going for the 100th token we",
    "start": "258120",
    "end": "262639"
  },
  {
    "text": "would have 10,000 operations that would",
    "start": "262639",
    "end": "265120"
  },
  {
    "text": "need to happen similarly if we are going",
    "start": "265120",
    "end": "268120"
  },
  {
    "text": "for the 100th to token uh sorry the",
    "start": "268120",
    "end": "271360"
  },
  {
    "text": "1,000 token then we would need to have 1",
    "start": "271360",
    "end": "275240"
  },
  {
    "text": "million operations have happening in the",
    "start": "275240",
    "end": "278120"
  },
  {
    "text": "background okay so that's and for all of",
    "start": "278120",
    "end": "282800"
  },
  {
    "text": "these operations to happen we need to",
    "start": "282800",
    "end": "285120"
  },
  {
    "text": "have a cash you would want those",
    "start": "285120",
    "end": "287720"
  },
  {
    "text": "inferences to be cash so that they can",
    "start": "287720",
    "end": "289759"
  },
  {
    "text": "be used next time and uh this leads to a",
    "start": "289759",
    "end": "295120"
  },
  {
    "text": "need of having a well set batch size the",
    "start": "295120",
    "end": "299360"
  },
  {
    "text": "number number of operations that we are",
    "start": "299360",
    "end": "301160"
  },
  {
    "text": "going to have and typically when you",
    "start": "301160",
    "end": "303800"
  },
  {
    "text": "look at that ratio it is somewhere",
    "start": "303800",
    "end": "305720"
  },
  {
    "text": "around 500 plus to",
    "start": "305720",
    "end": "308360"
  },
  {
    "text": "one now we also have the metrics that we",
    "start": "308360",
    "end": "312680"
  },
  {
    "text": "need to measure and finally we need to",
    "start": "312680",
    "end": "316479"
  },
  {
    "text": "have the gpus that need to become",
    "start": "316479",
    "end": "318520"
  },
  {
    "text": "available obviously uh with this we",
    "start": "318520",
    "end": "321199"
  },
  {
    "text": "would have latency challenges because",
    "start": "321199",
    "end": "323960"
  },
  {
    "text": "there has to be uh lot of bandwidth",
    "start": "323960",
    "end": "326520"
  },
  {
    "text": "needs are there uh for inferencing",
    "start": "326520",
    "end": "330080"
  },
  {
    "text": "uh responses have to come in processing",
    "start": "330080",
    "end": "332560"
  },
  {
    "text": "needs to happen and obviously as I was",
    "start": "332560",
    "end": "335600"
  },
  {
    "text": "talking that batch size becomes very",
    "start": "335600",
    "end": "338479"
  },
  {
    "text": "critical and when you look uh at the",
    "start": "338479",
    "end": "341400"
  },
  {
    "text": "model sizes uh typically they are from",
    "start": "341400",
    "end": "345000"
  },
  {
    "text": "300 to 400 GB uh in terms of model sizes",
    "start": "345000",
    "end": "350199"
  },
  {
    "text": "uh typical llm and the memory",
    "start": "350199",
    "end": "352840"
  },
  {
    "text": "requirements that we are going to have",
    "start": "352840",
    "end": "354639"
  },
  {
    "text": "for search light sizes are at the",
    "start": "354639",
    "end": "357960"
  },
  {
    "text": "minimum double",
    "start": "357960",
    "end": "360479"
  },
  {
    "text": "so if we have a 300 GB model uh at",
    "start": "360479",
    "end": "364039"
  },
  {
    "text": "minimum 600 GB is the memory",
    "start": "364039",
    "end": "366840"
  },
  {
    "text": "needs so now let us look at uh if we",
    "start": "366840",
    "end": "372319"
  },
  {
    "text": "have ai on kubernetes so it is same as",
    "start": "372319",
    "end": "377479"
  },
  {
    "text": "what you would have an application on a",
    "start": "377479",
    "end": "380440"
  },
  {
    "text": "kubernetes uh provide the capability to",
    "start": "380440",
    "end": "383360"
  },
  {
    "text": "scale uh it will allow AI applications",
    "start": "383360",
    "end": "386759"
  },
  {
    "text": "to",
    "start": "386759",
    "end": "387800"
  },
  {
    "text": "run uh whether you doing it on your",
    "start": "387800",
    "end": "390680"
  },
  {
    "text": "local machine you're doing it on the",
    "start": "390680",
    "end": "393120"
  },
  {
    "text": "cloud or in a private server okay uh and",
    "start": "393120",
    "end": "398199"
  },
  {
    "text": "these are typical characteristics that",
    "start": "398199",
    "end": "399960"
  },
  {
    "text": "you have from kubernetes and similar",
    "start": "399960",
    "end": "403080"
  },
  {
    "text": "things you expect for an AI application",
    "start": "403080",
    "end": "405560"
  },
  {
    "text": "resource eff uh efficiency in terms of",
    "start": "405560",
    "end": "408919"
  },
  {
    "text": "Maximum utilization of the resources to",
    "start": "408919",
    "end": "411880"
  },
  {
    "text": "happen Okay and we already have a good",
    "start": "411880",
    "end": "415319"
  },
  {
    "text": "tool set that we have coming in now uh",
    "start": "415319",
    "end": "419599"
  },
  {
    "text": "when we look at a typical uh workload",
    "start": "419599",
    "end": "423080"
  },
  {
    "text": "architecture we would have a get",
    "start": "423080",
    "end": "425879"
  },
  {
    "text": "repository uh and then we would have a",
    "start": "425879",
    "end": "428879"
  },
  {
    "text": "lake house this is more where all the",
    "start": "428879",
    "end": "430720"
  },
  {
    "text": "data will set and this will be the",
    "start": "430720",
    "end": "433120"
  },
  {
    "text": "differential data that we are going to",
    "start": "433120",
    "end": "434800"
  },
  {
    "text": "get and then we have the compute layer",
    "start": "434800",
    "end": "437199"
  },
  {
    "text": "where all the computation is happening",
    "start": "437199",
    "end": "439759"
  },
  {
    "text": "and obviously as part of the ml flow",
    "start": "439759",
    "end": "442520"
  },
  {
    "text": "this is where N2 and uh uh AI",
    "start": "442520",
    "end": "446520"
  },
  {
    "text": "application uh running flow into to and",
    "start": "446520",
    "end": "449960"
  },
  {
    "text": "part that's happening wherein uh you're",
    "start": "449960",
    "end": "453319"
  },
  {
    "text": "tracking you're creating the model",
    "start": "453319",
    "end": "456120"
  },
  {
    "text": "pushing it into a container registry and",
    "start": "456120",
    "end": "458840"
  },
  {
    "text": "then you have the serving layer through",
    "start": "458840",
    "end": "460800"
  },
  {
    "text": "which you expose the end points and then",
    "start": "460800",
    "end": "464000"
  },
  {
    "text": "obviously you have the responses so",
    "start": "464000",
    "end": "466720"
  },
  {
    "text": "basically this is a fine-tune process",
    "start": "466720",
    "end": "468720"
  },
  {
    "text": "wherein when you get a response you push",
    "start": "468720",
    "end": "470560"
  },
  {
    "text": "it into the lake and then you see the",
    "start": "470560",
    "end": "473479"
  },
  {
    "text": "differential and you see whether the",
    "start": "473479",
    "end": "475199"
  },
  {
    "text": "inference or the output was based on uh",
    "start": "475199",
    "end": "478520"
  },
  {
    "text": "right or wrong or near right based on",
    "start": "478520",
    "end": "481879"
  },
  {
    "text": "the query that you had put in and then",
    "start": "481879",
    "end": "483879"
  },
  {
    "text": "you try to fine-tune it okay and",
    "start": "483879",
    "end": "486800"
  },
  {
    "text": "typically as part of the kubernetes we",
    "start": "486800",
    "end": "489080"
  },
  {
    "text": "expect 10 to 50 times performance now",
    "start": "489080",
    "end": "492560"
  },
  {
    "text": "some of the best practices uh",
    "start": "492560",
    "end": "495440"
  },
  {
    "text": "obviously uh using persistent volume as",
    "start": "495440",
    "end": "498280"
  },
  {
    "text": "part of the data management you do",
    "start": "498280",
    "end": "500639"
  },
  {
    "text": "monitor uh and log uh all the uh a",
    "start": "500639",
    "end": "505879"
  },
  {
    "text": "applications that are running out there",
    "start": "505879",
    "end": "508159"
  },
  {
    "text": "for security you would use Ro based",
    "start": "508159",
    "end": "510759"
  },
  {
    "text": "access controls uh you're going to use",
    "start": "510759",
    "end": "513240"
  },
  {
    "text": "Falco which is basically uh for looking",
    "start": "513240",
    "end": "516839"
  },
  {
    "text": "at if there are any security",
    "start": "516839",
    "end": "518800"
  },
  {
    "text": "vulnerabilities within the containers",
    "start": "518800",
    "end": "521880"
  },
  {
    "text": "okay and as part of the cicd obviously",
    "start": "521880",
    "end": "524440"
  },
  {
    "text": "you would want to have updates and roll",
    "start": "524440",
    "end": "529680"
  },
  {
    "text": "backs",
    "start": "529680",
    "end": "531240"
  },
  {
    "text": "now now coming to what are some of the",
    "start": "531240",
    "end": "534440"
  },
  {
    "text": "challenges that we have for an a",
    "start": "534440",
    "end": "536680"
  },
  {
    "text": "application running on kubernetes now",
    "start": "536680",
    "end": "540640"
  },
  {
    "text": "typically uh we have to download and",
    "start": "540640",
    "end": "544600"
  },
  {
    "text": "store these large language models",
    "start": "544600",
    "end": "548320"
  },
  {
    "text": "typically uh they are to 300 to 400 GB",
    "start": "548320",
    "end": "552200"
  },
  {
    "text": "size so first of all the size is very",
    "start": "552200",
    "end": "554959"
  },
  {
    "text": "high memory requirements are going to be",
    "start": "554959",
    "end": "559040"
  },
  {
    "text": "at minimum double of those and hence",
    "start": "559040",
    "end": "562880"
  },
  {
    "text": "even if you want to load those files it",
    "start": "562880",
    "end": "564800"
  },
  {
    "text": "takes time okay going to the second part",
    "start": "564800",
    "end": "568399"
  },
  {
    "text": "wherein we we have to create a container",
    "start": "568399",
    "end": "571279"
  },
  {
    "text": "file and obviously there is the weight",
    "start": "571279",
    "end": "573320"
  },
  {
    "text": "management that you would have to do and",
    "start": "573320",
    "end": "577160"
  },
  {
    "text": "then you would have to host the model",
    "start": "577160",
    "end": "580720"
  },
  {
    "text": "image now for all of this you would have",
    "start": "580720",
    "end": "584000"
  },
  {
    "text": "to also have the provisioning of the GPU",
    "start": "584000",
    "end": "588440"
  },
  {
    "text": "infra and obviously exposing the",
    "start": "588440",
    "end": "592200"
  },
  {
    "text": "endpoint and if there are issues with",
    "start": "592200",
    "end": "594320"
  },
  {
    "text": "regards",
    "start": "594320",
    "end": "595279"
  },
  {
    "text": "to the",
    "start": "595279",
    "end": "597760"
  },
  {
    "text": "coda now if if you look one of the key",
    "start": "597760",
    "end": "601680"
  },
  {
    "text": "issues here are these are all different",
    "start": "601680",
    "end": "605079"
  },
  {
    "text": "steps that you need to do okay uh as",
    "start": "605079",
    "end": "609000"
  },
  {
    "text": "part of your model putting uh putting an",
    "start": "609000",
    "end": "612360"
  },
  {
    "text": "AI model and then having an endpoint",
    "start": "612360",
    "end": "614640"
  },
  {
    "text": "through which you can",
    "start": "614640",
    "end": "616440"
  },
  {
    "text": "access all these processes you have to",
    "start": "616440",
    "end": "619040"
  },
  {
    "text": "develop and write by",
    "start": "619040",
    "end": "621079"
  },
  {
    "text": "yourself and this is where we want",
    "start": "621079",
    "end": "624959"
  },
  {
    "text": "things to be very automated and this is",
    "start": "624959",
    "end": "628000"
  },
  {
    "text": "where k comes in",
    "start": "628000",
    "end": "631320"
  },
  {
    "text": "picture okay and this is Kito which is",
    "start": "631320",
    "end": "636519"
  },
  {
    "text": "the short form for kubernetes AI",
    "start": "636519",
    "end": "639000"
  },
  {
    "text": "toolchain",
    "start": "639000",
    "end": "640160"
  },
  {
    "text": "operator",
    "start": "640160",
    "end": "641880"
  },
  {
    "text": "and if you look at it the biggest",
    "start": "641880",
    "end": "645000"
  },
  {
    "text": "capability that it provides today is in",
    "start": "645000",
    "end": "648040"
  },
  {
    "text": "terms of having a same GPU you could",
    "start": "648040",
    "end": "653920"
  },
  {
    "text": "have one GPU and you could split your",
    "start": "653920",
    "end": "657399"
  },
  {
    "text": "API application AI",
    "start": "657399",
    "end": "659959"
  },
  {
    "text": "application and you could have it",
    "start": "659959",
    "end": "662120"
  },
  {
    "text": "running on uh lower set of GPU if you",
    "start": "662120",
    "end": "667399"
  },
  {
    "text": "don't have one single high highend GPU",
    "start": "667399",
    "end": "670360"
  },
  {
    "text": "you could have lower GPU and you could",
    "start": "670360",
    "end": "673000"
  },
  {
    "text": "split split your application AI",
    "start": "673000",
    "end": "675279"
  },
  {
    "text": "application run across GPU or you could",
    "start": "675279",
    "end": "678839"
  },
  {
    "text": "have multiple AI applications running or",
    "start": "678839",
    "end": "683320"
  },
  {
    "text": "single GPU and typically this is",
    "start": "683320",
    "end": "686279"
  },
  {
    "text": "something that's not happening to today",
    "start": "686279",
    "end": "690160"
  },
  {
    "text": "GPU resources are not well",
    "start": "690160",
    "end": "692839"
  },
  {
    "text": "optimized and K2 provides this",
    "start": "692839",
    "end": "695680"
  },
  {
    "text": "capability and obviously it does it",
    "start": "695680",
    "end": "698560"
  },
  {
    "text": "through two components one is the GPU",
    "start": "698560",
    "end": "702279"
  },
  {
    "text": "provisional controller and the other is",
    "start": "702279",
    "end": "705079"
  },
  {
    "text": "workspace controller so the idea here is",
    "start": "705079",
    "end": "708320"
  },
  {
    "text": "first of all it Provisions the GPU infra",
    "start": "708320",
    "end": "711880"
  },
  {
    "text": "and the second part is if we have a GPU",
    "start": "711880",
    "end": "716160"
  },
  {
    "text": "available that it gets it from the CL",
    "start": "716160",
    "end": "719240"
  },
  {
    "text": "cloud and Provisions it and if there is",
    "start": "719240",
    "end": "723680"
  },
  {
    "text": "a need uh of splitting it does that",
    "start": "723680",
    "end": "728760"
  },
  {
    "text": "part uh now I will hand over to my",
    "start": "728760",
    "end": "731600"
  },
  {
    "text": "colleague joural to take it",
    "start": "731600",
    "end": "734959"
  },
  {
    "text": "Forward hello thank you",
    "start": "736279",
    "end": "741079"
  },
  {
    "text": "uh uh yeah so so far what we discussed",
    "start": "741120",
    "end": "744040"
  },
  {
    "text": "was what is what is that pain in U in",
    "start": "744040",
    "end": "747480"
  },
  {
    "text": "you excuse me you mute his",
    "start": "747480",
    "end": "752000"
  },
  {
    "text": "mic hello yeah so I think so far what we",
    "start": "752000",
    "end": "755079"
  },
  {
    "text": "have discussed was you know what is the",
    "start": "755079",
    "end": "757160"
  },
  {
    "text": "challenge",
    "start": "757160",
    "end": "759680"
  },
  {
    "text": "that uh the challenges of deploying",
    "start": "760399",
    "end": "762639"
  },
  {
    "text": "these models right and which is where",
    "start": "762639",
    "end": "764079"
  },
  {
    "text": "Kao helps us to do things like uh",
    "start": "764079",
    "end": "766959"
  },
  {
    "text": "selecting the optimal size",
    "start": "766959",
    "end": "768800"
  },
  {
    "text": "infrastructure right every time when we",
    "start": "768800",
    "end": "770120"
  },
  {
    "text": "go ahead and deploy a model right we",
    "start": "770120",
    "end": "771480"
  },
  {
    "text": "need to do the guessing gamer right of",
    "start": "771480",
    "end": "773320"
  },
  {
    "text": "what is the right infrastructure which",
    "start": "773320",
    "end": "775199"
  },
  {
    "text": "GPU machine should I be using uh is it",
    "start": "775199",
    "end": "778000"
  },
  {
    "text": "an h00 is it a 00 is it an L4 GPU do I",
    "start": "778000",
    "end": "781480"
  },
  {
    "text": "have it right and also wait for times",
    "start": "781480",
    "end": "784040"
  },
  {
    "text": "when requesting my cloud provider I need",
    "start": "784040",
    "end": "786160"
  },
  {
    "text": "an h00 GPU give them business",
    "start": "786160",
    "end": "788199"
  },
  {
    "text": "justification saying why I need it and",
    "start": "788199",
    "end": "790560"
  },
  {
    "text": "my cloud provider just can say no I",
    "start": "790560",
    "end": "792360"
  },
  {
    "text": "don't have it right these problems occur",
    "start": "792360",
    "end": "794240"
  },
  {
    "text": "every day and we need now to wait to get",
    "start": "794240",
    "end": "797320"
  },
  {
    "text": "the GPU to run right and second was",
    "start": "797320",
    "end": "799920"
  },
  {
    "text": "these models right they don't like",
    "start": "799920",
    "end": "801519"
  },
  {
    "text": "sharing gpus right if you give uh even a",
    "start": "801519",
    "end": "804360"
  },
  {
    "text": "smaller Model A large GPU it just tries",
    "start": "804360",
    "end": "806120"
  },
  {
    "text": "to expand and you know fits the whole",
    "start": "806120",
    "end": "808240"
  },
  {
    "text": "memory right so that it can run faster",
    "start": "808240",
    "end": "810160"
  },
  {
    "text": "the problem that comes now you cannot",
    "start": "810160",
    "end": "811639"
  },
  {
    "text": "fit multiple smaller models into one GPU",
    "start": "811639",
    "end": "815399"
  },
  {
    "text": "because uh you know breaking that or",
    "start": "815399",
    "end": "818120"
  },
  {
    "text": "Shing that GPU is is tough",
    "start": "818120",
    "end": "821600"
  },
  {
    "text": "uh which is where Kao comes right so",
    "start": "821600",
    "end": "824120"
  },
  {
    "text": "essentially this is the internal",
    "start": "824120",
    "end": "825279"
  },
  {
    "text": "architecture of of this toolkit right so",
    "start": "825279",
    "end": "827279"
  },
  {
    "text": "you have that API server and then you",
    "start": "827279",
    "end": "828880"
  },
  {
    "text": "have this two custom crds right custom",
    "start": "828880",
    "end": "831199"
  },
  {
    "text": "resource definitions and essentially one",
    "start": "831199",
    "end": "833680"
  },
  {
    "text": "is what n told right one is the",
    "start": "833680",
    "end": "835519"
  },
  {
    "text": "workspace controller so whenever I'm",
    "start": "835519",
    "end": "837480"
  },
  {
    "text": "giving a task to the controller saying",
    "start": "837480",
    "end": "839120"
  },
  {
    "text": "can now deploy this model the workspace",
    "start": "839120",
    "end": "841279"
  },
  {
    "text": "controller first takes the task does",
    "start": "841279",
    "end": "842800"
  },
  {
    "text": "things like uh you know if a GPU is not",
    "start": "842800",
    "end": "845120"
  },
  {
    "text": "available we'll do node uh Auto",
    "start": "845120",
    "end": "847199"
  },
  {
    "text": "provisioning with create the inference",
    "start": "847199",
    "end": "848880"
  },
  {
    "text": "workload on top of those right uh and",
    "start": "848880",
    "end": "851320"
  },
  {
    "text": "and it uses Carpenter internally right",
    "start": "851320",
    "end": "853120"
  },
  {
    "text": "so it's incd which is extracted from",
    "start": "853120",
    "end": "854680"
  },
  {
    "text": "Carpenter and now you're using here and",
    "start": "854680",
    "end": "857040"
  },
  {
    "text": "then it it triggers uh the C of node",
    "start": "857040",
    "end": "859680"
  },
  {
    "text": "provisioner to say okay I need this GPU",
    "start": "859680",
    "end": "861720"
  },
  {
    "text": "go ahead and now create the GPU for me",
    "start": "861720",
    "end": "863360"
  },
  {
    "text": "and give it to me right uh and then",
    "start": "863360",
    "end": "866079"
  },
  {
    "text": "either you'll have a public registry of",
    "start": "866079",
    "end": "868160"
  },
  {
    "text": "so Kao already has this public resch of",
    "start": "868160",
    "end": "870199"
  },
  {
    "text": "images which is given to you for all of",
    "start": "870199",
    "end": "872040"
  },
  {
    "text": "the uh common models right let's say if",
    "start": "872040",
    "end": "874440"
  },
  {
    "text": "you're using Lama uh you know 52 53",
    "start": "874440",
    "end": "877680"
  },
  {
    "text": "Falcon these models right already these",
    "start": "877680",
    "end": "879360"
  },
  {
    "text": "images available created for you so even",
    "start": "879360",
    "end": "881839"
  },
  {
    "text": "if you let's say uh using the M model to",
    "start": "881839",
    "end": "884399"
  },
  {
    "text": "F tune your model you can just connect",
    "start": "884399",
    "end": "886000"
  },
  {
    "text": "to the same uh workload right so you",
    "start": "886000",
    "end": "887519"
  },
  {
    "text": "don't have to do image creation a lot of",
    "start": "887519",
    "end": "889759"
  },
  {
    "text": "these things or you could have your",
    "start": "889759",
    "end": "891800"
  },
  {
    "text": "images stor stored in a public reg or",
    "start": "891800",
    "end": "893839"
  },
  {
    "text": "sorry a private registry and those",
    "start": "893839",
    "end": "896079"
  },
  {
    "text": "images will pulled your model which is",
    "start": "896079",
    "end": "898040"
  },
  {
    "text": "from in your own",
    "start": "898040",
    "end": "899600"
  },
  {
    "text": "like model reg will be pulled and then",
    "start": "899600",
    "end": "902240"
  },
  {
    "text": "essentially that workload uh the AI",
    "start": "902240",
    "end": "904320"
  },
  {
    "text": "workload will be created right and then",
    "start": "904320",
    "end": "906160"
  },
  {
    "text": "it is exposed uh within Ingress server",
    "start": "906160",
    "end": "908639"
  },
  {
    "text": "now you can talk to that give your INF",
    "start": "908639",
    "end": "910839"
  },
  {
    "text": "workload and get your uh uh responses",
    "start": "910839",
    "end": "914839"
  },
  {
    "text": "back",
    "start": "914839",
    "end": "916120"
  },
  {
    "text": "right uh so couple of things right uh",
    "start": "916120",
    "end": "918759"
  },
  {
    "text": "you know it manages that model uh large",
    "start": "918759",
    "end": "921199"
  },
  {
    "text": "model files using containers right so",
    "start": "921199",
    "end": "922920"
  },
  {
    "text": "and on top of that at HTTP server is",
    "start": "922920",
    "end": "924959"
  },
  {
    "text": "present for you to do those inference",
    "start": "924959",
    "end": "926959"
  },
  {
    "text": "calls uh then secondly is uh you know",
    "start": "926959",
    "end": "929920"
  },
  {
    "text": "you don't have to tune your deployment",
    "start": "929920",
    "end": "932440"
  },
  {
    "text": "to the GPU versus the platform does it",
    "start": "932440",
    "end": "934319"
  },
  {
    "text": "for you uh autoprovisioning node means",
    "start": "934319",
    "end": "937000"
  },
  {
    "text": "you're not wasting any resources right",
    "start": "937000",
    "end": "938639"
  },
  {
    "text": "whenever you are needing resources you",
    "start": "938639",
    "end": "940199"
  },
  {
    "text": "creating them running your workload and",
    "start": "940199",
    "end": "941880"
  },
  {
    "text": "just coming out of it uh another problem",
    "start": "941880",
    "end": "945120"
  },
  {
    "text": "is let's say if even if you are using a",
    "start": "945120",
    "end": "947240"
  },
  {
    "text": "batch use case right your your llm is",
    "start": "947240",
    "end": "949600"
  },
  {
    "text": "still used for a batch use case you",
    "start": "949600",
    "end": "950920"
  },
  {
    "text": "still need to run right and then",
    "start": "950920",
    "end": "952560"
  },
  {
    "text": "provisioning the GPU every time when you",
    "start": "952560",
    "end": "954639"
  },
  {
    "text": "have to go provision that that is a pain",
    "start": "954639",
    "end": "956800"
  },
  {
    "text": "right versus Kito does uh that",
    "start": "956800",
    "end": "959000"
  },
  {
    "text": "automatically for you and you could",
    "start": "959000",
    "end": "961240"
  },
  {
    "text": "essentially host these images in public",
    "start": "961240",
    "end": "962880"
  },
  {
    "text": "registry if your license allows or if",
    "start": "962880",
    "end": "964360"
  },
  {
    "text": "you are open to exposing that right uh",
    "start": "964360",
    "end": "967079"
  },
  {
    "text": "so essentially uh you know this whole",
    "start": "967079",
    "end": "969800"
  },
  {
    "text": "workload will we'll go through and see",
    "start": "969800",
    "end": "971240"
  },
  {
    "text": "how it works but entally it is",
    "start": "971240",
    "end": "972959"
  },
  {
    "text": "simplifying the whole deployment process",
    "start": "972959",
    "end": "974800"
  },
  {
    "text": "right simplifying uh from creating",
    "start": "974800",
    "end": "977880"
  },
  {
    "text": "taking that model file to deploying it",
    "start": "977880",
    "end": "979880"
  },
  {
    "text": "an htd endpoint so that whole process is",
    "start": "979880",
    "end": "983440"
  },
  {
    "text": "simplified uh couple of benefits right",
    "start": "983440",
    "end": "986040"
  },
  {
    "text": "uh you know your GPU provisioning is a",
    "start": "986040",
    "end": "988920"
  },
  {
    "text": "automated your configurations are done",
    "start": "988920",
    "end": "990480"
  },
  {
    "text": "automatically U you know you're not",
    "start": "990480",
    "end": "992800"
  },
  {
    "text": "since you are splitting inference across",
    "start": "992800",
    "end": "994519"
  },
  {
    "text": "multiple workload so you're not waiting",
    "start": "994519",
    "end": "996360"
  },
  {
    "text": "for getting a larger GPU as well as you",
    "start": "996360",
    "end": "998319"
  },
  {
    "text": "are saving your cost there right uh it",
    "start": "998319",
    "end": "1002319"
  },
  {
    "text": "it it automatically supports a lot of",
    "start": "1002319",
    "end": "1004480"
  },
  {
    "text": "Open Source mod models right off the box",
    "start": "1004480",
    "end": "1006440"
  },
  {
    "text": "so even a smaller organization you want",
    "start": "1006440",
    "end": "1008519"
  },
  {
    "text": "to use that fancy new model that came",
    "start": "1008519",
    "end": "1010959"
  },
  {
    "text": "you don't have to spend a lot of time in",
    "start": "1010959",
    "end": "1012880"
  },
  {
    "text": "IR to figure out how do I deploy this",
    "start": "1012880",
    "end": "1014560"
  },
  {
    "text": "right that is uh automatically taken",
    "start": "1014560",
    "end": "1017000"
  },
  {
    "text": "care of that by the community itself and",
    "start": "1017000",
    "end": "1019199"
  },
  {
    "text": "then you have more fine grain controls",
    "start": "1019199",
    "end": "1020759"
  },
  {
    "text": "over data security privacy model",
    "start": "1020759",
    "end": "1023519"
  },
  {
    "text": "deployment and and configurations and",
    "start": "1023519",
    "end": "1025240"
  },
  {
    "text": "all of that right uh any questions so",
    "start": "1025240",
    "end": "1029199"
  },
  {
    "text": "far and yeah uh so so this is the",
    "start": "1029199",
    "end": "1033720"
  },
  {
    "text": "process right you can um basically your",
    "start": "1033720",
    "end": "1036240"
  },
  {
    "text": "Ken sits right between your uh cluster",
    "start": "1036240",
    "end": "1040000"
  },
  {
    "text": "and your interface right so you uh can",
    "start": "1040000",
    "end": "1042760"
  },
  {
    "text": "give your uh deployment inference task",
    "start": "1042760",
    "end": "1045319"
  },
  {
    "text": "to the provisioner and then the",
    "start": "1045319",
    "end": "1046480"
  },
  {
    "text": "provisioner talks to your uh infr to now",
    "start": "1046480",
    "end": "1049559"
  },
  {
    "text": "create those images deploy the model and",
    "start": "1049559",
    "end": "1051640"
  },
  {
    "text": "and create an endpoint on top of",
    "start": "1051640",
    "end": "1054799"
  },
  {
    "text": "that",
    "start": "1054799",
    "end": "1056360"
  },
  {
    "text": "okay so just look at sample example demo",
    "start": "1056360",
    "end": "1061600"
  },
  {
    "text": "right and then how do you provision how",
    "start": "1061600",
    "end": "1063039"
  },
  {
    "text": "do you deploy the models and right from",
    "start": "1063039",
    "end": "1064760"
  },
  {
    "text": "there how to use it right uh so this is",
    "start": "1064760",
    "end": "1067400"
  },
  {
    "text": "the architecture of the entire let's say",
    "start": "1067400",
    "end": "1069840"
  },
  {
    "text": "when you deploy an solution right this",
    "start": "1069840",
    "end": "1072440"
  },
  {
    "text": "is your overall solution architecture",
    "start": "1072440",
    "end": "1074120"
  },
  {
    "text": "would look like right right from things",
    "start": "1074120",
    "end": "1076280"
  },
  {
    "text": "like uh you know using storage accounts",
    "start": "1076280",
    "end": "1078039"
  },
  {
    "text": "container history to to deploy this",
    "start": "1078039",
    "end": "1079360"
  },
  {
    "text": "model to essentially uh using multiple",
    "start": "1079360",
    "end": "1082240"
  },
  {
    "text": "API servers other deployment pools where",
    "start": "1082240",
    "end": "1084440"
  },
  {
    "text": "other applications are talking right",
    "start": "1084440",
    "end": "1086000"
  },
  {
    "text": "essentially this model is a part of the",
    "start": "1086000",
    "end": "1088000"
  },
  {
    "text": "solution it's not the entire solution",
    "start": "1088000",
    "end": "1090280"
  },
  {
    "text": "and then essentially you have this Kito",
    "start": "1090280",
    "end": "1091840"
  },
  {
    "text": "demo this node where your llm is",
    "start": "1091840",
    "end": "1094960"
  },
  {
    "text": "actually running right and top of that",
    "start": "1094960",
    "end": "1096520"
  },
  {
    "text": "you would have an Ingress server uh",
    "start": "1096520",
    "end": "1099240"
  },
  {
    "text": "created which will allow you to now talk",
    "start": "1099240",
    "end": "1100919"
  },
  {
    "text": "to that model get your uh uh like talk",
    "start": "1100919",
    "end": "1105440"
  },
  {
    "text": "to the model give your inputs and get",
    "start": "1105440",
    "end": "1107360"
  },
  {
    "text": "the outputs out from there",
    "start": "1107360",
    "end": "1110120"
  },
  {
    "text": "uh so I'll just go through the terraform",
    "start": "1110120",
    "end": "1111440"
  },
  {
    "text": "scripts to how do you deploy this I",
    "start": "1111440",
    "end": "1113320"
  },
  {
    "text": "think uh you'll also get access to the",
    "start": "1113320",
    "end": "1115080"
  },
  {
    "text": "node uh uh the deck and the code uh",
    "start": "1115080",
    "end": "1118360"
  },
  {
    "text": "later right so first is you uh uh this",
    "start": "1118360",
    "end": "1121559"
  },
  {
    "text": "Command right so this retrieves the",
    "start": "1121559",
    "end": "1122880"
  },
  {
    "text": "properties of the node group so remember",
    "start": "1122880",
    "end": "1125080"
  },
  {
    "text": "there was a node provisional right you",
    "start": "1125080",
    "end": "1126320"
  },
  {
    "text": "need to give access to that node group",
    "start": "1126320",
    "end": "1128360"
  },
  {
    "text": "for the for the Kao not to go and talk",
    "start": "1128360",
    "end": "1130280"
  },
  {
    "text": "to and create those right then",
    "start": "1130280",
    "end": "1132360"
  },
  {
    "text": "essentially you create you enable the",
    "start": "1132360",
    "end": "1134360"
  },
  {
    "text": "Kido add-on in your cluster so this",
    "start": "1134360",
    "end": "1137120"
  },
  {
    "text": "basically installs the Kido addon in",
    "start": "1137120",
    "end": "1139080"
  },
  {
    "text": "your cluster in your a cluster now makes",
    "start": "1139080",
    "end": "1141240"
  },
  {
    "text": "it available for you to create your",
    "start": "1141240",
    "end": "1143280"
  },
  {
    "text": "workloads create your",
    "start": "1143280",
    "end": "1145200"
  },
  {
    "text": "endpoints uh second is then you create a",
    "start": "1145200",
    "end": "1147919"
  },
  {
    "text": "Kyo identity basically your user",
    "start": "1147919",
    "end": "1150360"
  },
  {
    "text": "assigned manage identity which is now",
    "start": "1150360",
    "end": "1153039"
  },
  {
    "text": "used to create those nodes talk to your",
    "start": "1153039",
    "end": "1155200"
  },
  {
    "text": "cloud account to to do various uh steps",
    "start": "1155200",
    "end": "1158840"
  },
  {
    "text": "right then finally you create a",
    "start": "1158840",
    "end": "1161440"
  },
  {
    "text": "Federated identity right between your",
    "start": "1161440",
    "end": "1163159"
  },
  {
    "text": "Kido and and the and the service account",
    "start": "1163159",
    "end": "1165520"
  },
  {
    "text": "R so that your Kido controller Now",
    "start": "1165520",
    "end": "1167720"
  },
  {
    "text": "controls your CU system Nam space",
    "start": "1167720",
    "end": "1170320"
  },
  {
    "text": "particularly the Kito GPU provisional",
    "start": "1170320",
    "end": "1172360"
  },
  {
    "text": "crd that we just created right so that",
    "start": "1172360",
    "end": "1174159"
  },
  {
    "text": "it can now control that and create those",
    "start": "1174159",
    "end": "1175840"
  },
  {
    "text": "GPU nodes whenever",
    "start": "1175840",
    "end": "1179080"
  },
  {
    "text": "required uh yeah and then I think this",
    "start": "1179760",
    "end": "1181960"
  },
  {
    "text": "is the last step where you basically",
    "start": "1181960",
    "end": "1183360"
  },
  {
    "text": "assign a contributor role you know to",
    "start": "1183360",
    "end": "1185760"
  },
  {
    "text": "the Kyo manage identity so that now you",
    "start": "1185760",
    "end": "1187480"
  },
  {
    "text": "can you're not essentially giving it",
    "start": "1187480",
    "end": "1189440"
  },
  {
    "text": "entire access but just access whatever",
    "start": "1189440",
    "end": "1191000"
  },
  {
    "text": "it is needed to run that right",
    "start": "1191000",
    "end": "1194240"
  },
  {
    "text": "uh yeah and I think yeah that's all so",
    "start": "1194240",
    "end": "1198200"
  },
  {
    "text": "now that I think you see here right or",
    "start": "1198200",
    "end": "1202360"
  },
  {
    "text": "you could do step by step commands as",
    "start": "1202360",
    "end": "1204320"
  },
  {
    "text": "well right so you could essentially go",
    "start": "1204320",
    "end": "1206320"
  },
  {
    "text": "ahead create that name space uh you know",
    "start": "1206320",
    "end": "1208440"
  },
  {
    "text": "AI tool chain",
    "start": "1208440",
    "end": "1210520"
  },
  {
    "text": "operator uh then essentially go ahead",
    "start": "1210520",
    "end": "1213520"
  },
  {
    "text": "create that uh Resource Group in your",
    "start": "1213520",
    "end": "1216240"
  },
  {
    "text": "cluster right uh once you do that it",
    "start": "1216240",
    "end": "1219480"
  },
  {
    "text": "takes couple 10 15 minutes to done and",
    "start": "1219480",
    "end": "1222840"
  },
  {
    "text": "essentially your cluster would be",
    "start": "1222840",
    "end": "1224120"
  },
  {
    "text": "created with Kito enable and and",
    "start": "1224120",
    "end": "1226640"
  },
  {
    "text": "everything available right and then",
    "start": "1226640",
    "end": "1229200"
  },
  {
    "text": "finally you get the credentials for the",
    "start": "1229200",
    "end": "1230760"
  },
  {
    "text": "resource Group assigned it to uh the",
    "start": "1230760",
    "end": "1235919"
  },
  {
    "text": "operator and yeah so once once it is",
    "start": "1236720",
    "end": "1240720"
  },
  {
    "text": "available when it's running you could",
    "start": "1240720",
    "end": "1242080"
  },
  {
    "text": "just see okay now your Kao is running",
    "start": "1242080",
    "end": "1244760"
  },
  {
    "text": "your GPU node provisioner and and your",
    "start": "1244760",
    "end": "1246880"
  },
  {
    "text": "WorkSource controller is available in",
    "start": "1246880",
    "end": "1248520"
  },
  {
    "text": "cluster now you can use this go ahead",
    "start": "1248520",
    "end": "1250480"
  },
  {
    "text": "and U run your workload right uh now",
    "start": "1250480",
    "end": "1254360"
  },
  {
    "text": "this is that was your over system right",
    "start": "1254360",
    "end": "1256640"
  },
  {
    "text": "uh so if you remember here",
    "start": "1256640",
    "end": "1259559"
  },
  {
    "text": "when you talked about this is the center",
    "start": "1259559",
    "end": "1261240"
  },
  {
    "text": "is the part where we're talking right so",
    "start": "1261240",
    "end": "1262760"
  },
  {
    "text": "this Kao demo right so far we just",
    "start": "1262760",
    "end": "1265039"
  },
  {
    "text": "created this entire process system as",
    "start": "1265039",
    "end": "1267080"
  },
  {
    "text": "well as uh installed or enabled Kito in",
    "start": "1267080",
    "end": "1270440"
  },
  {
    "text": "the system right now that you have Kao",
    "start": "1270440",
    "end": "1273720"
  },
  {
    "text": "available right uh essentially this is",
    "start": "1273720",
    "end": "1275880"
  },
  {
    "text": "what we are going to do right so your uh",
    "start": "1275880",
    "end": "1278559"
  },
  {
    "text": "the Kyo chat service gets created and",
    "start": "1278559",
    "end": "1280880"
  },
  {
    "text": "then you can now create things like",
    "start": "1280880",
    "end": "1283120"
  },
  {
    "text": "Falcon 7B Service uh you know service",
    "start": "1283120",
    "end": "1285400"
  },
  {
    "text": "ports or or any other model ports",
    "start": "1285400",
    "end": "1287440"
  },
  {
    "text": "available so that you can now go ahead",
    "start": "1287440",
    "end": "1288880"
  },
  {
    "text": "and talk to right and essentially this",
    "start": "1288880",
    "end": "1291080"
  },
  {
    "text": "is the step when Kido actually shows you",
    "start": "1291080",
    "end": "1293360"
  },
  {
    "text": "what it is doing right uh it is a very",
    "start": "1293360",
    "end": "1295600"
  },
  {
    "text": "simple step right this is all you need",
    "start": "1295600",
    "end": "1296840"
  },
  {
    "text": "to write so you're writing uh I want to",
    "start": "1296840",
    "end": "1300279"
  },
  {
    "text": "create a model in the space this is the",
    "start": "1300279",
    "end": "1302960"
  },
  {
    "text": "model and preset is something available",
    "start": "1302960",
    "end": "1305480"
  },
  {
    "text": "which is in the the Kito presets are",
    "start": "1305480",
    "end": "1307279"
  },
  {
    "text": "available for all these models right the",
    "start": "1307279",
    "end": "1308640"
  },
  {
    "text": "configurations are already available for",
    "start": "1308640",
    "end": "1310360"
  },
  {
    "text": "let's say Falcon 7B and you just Define",
    "start": "1310360",
    "end": "1313360"
  },
  {
    "text": "okay I need uh this model needs this",
    "start": "1313360",
    "end": "1316000"
  },
  {
    "text": "machine and just sit right now when once",
    "start": "1316000",
    "end": "1318240"
  },
  {
    "text": "you do KFC apply uh this and this is the",
    "start": "1318240",
    "end": "1321520"
  },
  {
    "text": "three things you need to this U say",
    "start": "1321520",
    "end": "1324360"
  },
  {
    "text": "right your workspace name your GPU",
    "start": "1324360",
    "end": "1325679"
  },
  {
    "text": "instance type and your preset image to",
    "start": "1325679",
    "end": "1327360"
  },
  {
    "text": "be used right which image essentially",
    "start": "1327360",
    "end": "1328960"
  },
  {
    "text": "image you need to use to deploy this",
    "start": "1328960",
    "end": "1330760"
  },
  {
    "text": "model right once you do KFL apply on",
    "start": "1330760",
    "end": "1333360"
  },
  {
    "text": "this ml file your uh model would be",
    "start": "1333360",
    "end": "1336039"
  },
  {
    "text": "created right uh you see something like",
    "start": "1336039",
    "end": "1338840"
  },
  {
    "text": "this right so for example I have this",
    "start": "1338840",
    "end": "1341880"
  },
  {
    "text": "right so this is my",
    "start": "1341880",
    "end": "1343679"
  },
  {
    "text": "configuration uh this is the machine",
    "start": "1343679",
    "end": "1347159"
  },
  {
    "text": "type and and the inference uh config",
    "start": "1347159",
    "end": "1350400"
  },
  {
    "text": "right now I go ahead uh and run this",
    "start": "1350400",
    "end": "1354600"
  },
  {
    "text": "right so when I uh run this when I do I",
    "start": "1354600",
    "end": "1358640"
  },
  {
    "text": "have I have a cic system that is in",
    "start": "1358640",
    "end": "1361360"
  },
  {
    "text": "place which takes this CD controller and",
    "start": "1361360",
    "end": "1363039"
  },
  {
    "text": "now go and apply this right so when it's",
    "start": "1363039",
    "end": "1365640"
  },
  {
    "text": "applied uh you'll see something like",
    "start": "1365640",
    "end": "1367960"
  },
  {
    "text": "this right uh yeah so now you'll see",
    "start": "1367960",
    "end": "1370919"
  },
  {
    "text": "that uh we defined okay I need this to",
    "start": "1370919",
    "end": "1373320"
  },
  {
    "text": "be created it has created the worker",
    "start": "1373320",
    "end": "1375679"
  },
  {
    "text": "note where the model is running",
    "start": "1375679",
    "end": "1378880"
  },
  {
    "text": "and then essentially now I can yeah it",
    "start": "1378880",
    "end": "1381039"
  },
  {
    "text": "is created and then finally what I can",
    "start": "1381039",
    "end": "1382679"
  },
  {
    "text": "do is go ahead and and it uses the same",
    "start": "1382679",
    "end": "1386080"
  },
  {
    "text": "configuration right whatever I defined",
    "start": "1386080",
    "end": "1387600"
  },
  {
    "text": "there you use the same configuration to",
    "start": "1387600",
    "end": "1388960"
  },
  {
    "text": "now go ahead and uh deploy and once done",
    "start": "1388960",
    "end": "1392600"
  },
  {
    "text": "once deployed you can now go ahead and",
    "start": "1392600",
    "end": "1394080"
  },
  {
    "text": "use this end point uh to now talk to the",
    "start": "1394080",
    "end": "1397360"
  },
  {
    "text": "model and get",
    "start": "1397360",
    "end": "1399039"
  },
  {
    "text": "uh uh",
    "start": "1399039",
    "end": "1402240"
  },
  {
    "text": "responses yeah so essentially this is",
    "start": "1402480",
    "end": "1404760"
  },
  {
    "text": "what right so in earlier when n was",
    "start": "1404760",
    "end": "1406520"
  },
  {
    "text": "talking he was talking about the current",
    "start": "1406520",
    "end": "1407679"
  },
  {
    "text": "experience right you do an environment",
    "start": "1407679",
    "end": "1409520"
  },
  {
    "text": "setup you do a model setup you do",
    "start": "1409520",
    "end": "1411159"
  },
  {
    "text": "containerization of the model itself and",
    "start": "1411159",
    "end": "1413600"
  },
  {
    "text": "then finally go ahead and deploy versus",
    "start": "1413600",
    "end": "1415960"
  },
  {
    "text": "with you say okay go ahead and deploy",
    "start": "1415960",
    "end": "1417840"
  },
  {
    "text": "this model the add-on takes care of the",
    "start": "1417840",
    "end": "1420679"
  },
  {
    "text": "whole overhead of doing this four steps",
    "start": "1420679",
    "end": "1422360"
  },
  {
    "text": "to finally deploying the workspace and",
    "start": "1422360",
    "end": "1424679"
  },
  {
    "text": "then just exposing your model right and",
    "start": "1424679",
    "end": "1427279"
  },
  {
    "text": "that's all I think if I have to",
    "start": "1427279",
    "end": "1429000"
  },
  {
    "text": "summarize these are the key takeaways",
    "start": "1429000",
    "end": "1430799"
  },
  {
    "text": "from this session right so your Kos kind",
    "start": "1430799",
    "end": "1432559"
  },
  {
    "text": "of stimul the overall um uh deployment",
    "start": "1432559",
    "end": "1435440"
  },
  {
    "text": "process with the custom crd your uh GPU",
    "start": "1435440",
    "end": "1438520"
  },
  {
    "text": "provisioning and operations are",
    "start": "1438520",
    "end": "1440600"
  },
  {
    "text": "optimized since you eliminate manual",
    "start": "1440600",
    "end": "1442760"
  },
  {
    "text": "tuning preset configurations are used to",
    "start": "1442760",
    "end": "1444840"
  },
  {
    "text": "Auto provision these gpus and then your",
    "start": "1444840",
    "end": "1447360"
  },
  {
    "text": "whole workflow is efficient right",
    "start": "1447360",
    "end": "1448760"
  },
  {
    "text": "instead of taking weeks to do each step",
    "start": "1448760",
    "end": "1451440"
  },
  {
    "text": "one by one figuring out okay how do I do",
    "start": "1451440",
    "end": "1453279"
  },
  {
    "text": "this versus U the the toolkit does it",
    "start": "1453279",
    "end": "1455799"
  },
  {
    "text": "yours for you automatically so I think",
    "start": "1455799",
    "end": "1457960"
  },
  {
    "text": "that's all you can access the slides",
    "start": "1457960",
    "end": "1459440"
  },
  {
    "text": "here I also linked a lot of resources at",
    "start": "1459440",
    "end": "1462159"
  },
  {
    "text": "the end these are clickable links go",
    "start": "1462159",
    "end": "1463840"
  },
  {
    "text": "ahead you can uh look at the links",
    "start": "1463840",
    "end": "1465919"
  },
  {
    "text": "figure out how it works maybe test it",
    "start": "1465919",
    "end": "1467559"
  },
  {
    "text": "out on on your environments right uh",
    "start": "1467559",
    "end": "1470399"
  },
  {
    "text": "that's all U that's all from us thank",
    "start": "1470399",
    "end": "1472360"
  },
  {
    "text": "you any questions we'll be happy to",
    "start": "1472360",
    "end": "1475919"
  },
  {
    "text": "take y",
    "start": "1483200",
    "end": "1487200"
  },
  {
    "text": "yeah I think H5 file ggf a couple of",
    "start": "1519440",
    "end": "1522000"
  },
  {
    "text": "these yeah so these are different kinds",
    "start": "1522000",
    "end": "1524279"
  },
  {
    "text": "of I would say you know I would say the",
    "start": "1524279",
    "end": "1527480"
  },
  {
    "text": "formats this model let's say if you are",
    "start": "1527480",
    "end": "1529120"
  },
  {
    "text": "quantized and the model is supposed to",
    "start": "1529120",
    "end": "1530480"
  },
  {
    "text": "run in a very low environment right",
    "start": "1530480",
    "end": "1531880"
  },
  {
    "text": "which is why you use ggf right to",
    "start": "1531880",
    "end": "1533760"
  },
  {
    "text": "quantize the model put and then very",
    "start": "1533760",
    "end": "1535320"
  },
  {
    "text": "smaller size but then what happens is in",
    "start": "1535320",
    "end": "1536880"
  },
  {
    "text": "that process you lose performance right",
    "start": "1536880",
    "end": "1539159"
  },
  {
    "text": "because uh simply the model was might be",
    "start": "1539159",
    "end": "1541480"
  },
  {
    "text": "trained on F16 now you're converting",
    "start": "1541480",
    "end": "1543159"
  },
  {
    "text": "into int 8 right so that process loses",
    "start": "1543159",
    "end": "1545399"
  },
  {
    "text": "computer like loses Precision on the",
    "start": "1545399",
    "end": "1547240"
  },
  {
    "text": "model right uh and and while you could",
    "start": "1547240",
    "end": "1550799"
  },
  {
    "text": "use AMA and the ggf format file to run",
    "start": "1550799",
    "end": "1553399"
  },
  {
    "text": "that model essentially like practically",
    "start": "1553399",
    "end": "1554919"
  },
  {
    "text": "it will work but you'll lose a lot of",
    "start": "1554919",
    "end": "1556960"
  },
  {
    "text": "accuracy versus the base model model",
    "start": "1556960",
    "end": "1558640"
  },
  {
    "text": "right uh uh because of the file format",
    "start": "1558640",
    "end": "1561240"
  },
  {
    "text": "itself because we're converting the",
    "start": "1561240",
    "end": "1562360"
  },
  {
    "text": "model into a different file format and",
    "start": "1562360",
    "end": "1563919"
  },
  {
    "text": "in that process we're quantizing the",
    "start": "1563919",
    "end": "1566440"
  },
  {
    "text": "model actually more question because I",
    "start": "1566440",
    "end": "1570880"
  },
  {
    "text": "was wondering it was",
    "start": "1570880",
    "end": "1573520"
  },
  {
    "text": "actually yeah so essentially AMA also",
    "start": "1573520",
    "end": "1575679"
  },
  {
    "text": "uses containers right you know it your",
    "start": "1575679",
    "end": "1578600"
  },
  {
    "text": "models are deployed as",
    "start": "1578600",
    "end": "1581520"
  },
  {
    "text": "containers it using like",
    "start": "1582480",
    "end": "1585760"
  },
  {
    "text": "normal layers",
    "start": "1585760",
    "end": "1589760"
  },
  {
    "text": "I have not have not explored that",
    "start": "1591039",
    "end": "1594600"
  },
  {
    "text": "so uh any anything",
    "start": "1594600",
    "end": "1598440"
  },
  {
    "text": "else no okay thank you everyone",
    "start": "1598440",
    "end": "1603440"
  },
  {
    "text": "[Applause]",
    "start": "1603640",
    "end": "1606419"
  }
]