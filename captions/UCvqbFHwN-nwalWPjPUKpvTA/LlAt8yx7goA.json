[
  {
    "text": "welcome welcome everyone uh welcome to",
    "start": "240",
    "end": "3120"
  },
  {
    "text": "scaling hargo CD from symptoms to",
    "start": "3120",
    "end": "6480"
  },
  {
    "text": "solution my name is Alexander i'm from",
    "start": "6480",
    "end": "10240"
  },
  {
    "text": "Quebec Canada i'm a software engineer at",
    "start": "10240",
    "end": "13280"
  },
  {
    "text": "INT and also an Argo CD",
    "start": "13280",
    "end": "17080"
  },
  {
    "text": "maintainer so let's talk a bit about in",
    "start": "17080",
    "end": "19520"
  },
  {
    "text": "it int is one of the reason I'm here",
    "start": "19520",
    "end": "21439"
  },
  {
    "text": "today and in it is also the company that",
    "start": "21439",
    "end": "23600"
  },
  {
    "text": "took the bold decision a few years ago",
    "start": "23600",
    "end": "25519"
  },
  {
    "text": "to open source Argo CD and also open",
    "start": "25519",
    "end": "28800"
  },
  {
    "text": "source a couple of different projects so",
    "start": "28800",
    "end": "30960"
  },
  {
    "text": "go check them",
    "start": "30960",
    "end": "32599"
  },
  {
    "text": "out so today the agenda is quite simple",
    "start": "32600",
    "end": "36000"
  },
  {
    "text": "but it is an advanced talk so you need",
    "start": "36000",
    "end": "38079"
  },
  {
    "text": "to be familiar with Argo CD you need",
    "start": "38079",
    "end": "40320"
  },
  {
    "text": "probably to have deployed it and if you",
    "start": "40320",
    "end": "42879"
  },
  {
    "text": "have not don't worry about it it's also",
    "start": "42879",
    "end": "45440"
  },
  {
    "text": "going to provide you good tips to",
    "start": "45440",
    "end": "47520"
  },
  {
    "text": "investigate scaling issues that you",
    "start": "47520",
    "end": "50000"
  },
  {
    "text": "might have in the future so we're going",
    "start": "50000",
    "end": "53600"
  },
  {
    "text": "to go over a list of different scaling",
    "start": "53600",
    "end": "55360"
  },
  {
    "text": "problem that we've experienced or the",
    "start": "55360",
    "end": "57440"
  },
  {
    "text": "commun that I've experienced the",
    "start": "57440",
    "end": "59039"
  },
  {
    "text": "community have experienced and we're",
    "start": "59039",
    "end": "61120"
  },
  {
    "text": "going to look at the symptoms we're",
    "start": "61120",
    "end": "62480"
  },
  {
    "text": "going to look at the graphs and see what",
    "start": "62480",
    "end": "64720"
  },
  {
    "text": "happens in Argo normally people will",
    "start": "64720",
    "end": "67360"
  },
  {
    "text": "just mitigate the symptoms and be done",
    "start": "67360",
    "end": "70080"
  },
  {
    "text": "with it but today we're going to dig",
    "start": "70080",
    "end": "72159"
  },
  {
    "text": "deeper we're going to try to diagnose",
    "start": "72159",
    "end": "73920"
  },
  {
    "text": "them find the root cause and fix it for",
    "start": "73920",
    "end": "78920"
  },
  {
    "text": "good so let's talk a bit about the Inuit",
    "start": "78920",
    "end": "82000"
  },
  {
    "text": "platform for a second at in it we have",
    "start": "82000",
    "end": "84159"
  },
  {
    "text": "multiple instances and our instances",
    "start": "84159",
    "end": "87119"
  },
  {
    "text": "have different footprints so we have in",
    "start": "87119",
    "end": "89680"
  },
  {
    "text": "instances configured with five and 10",
    "start": "89680",
    "end": "92159"
  },
  {
    "text": "clusters we have other instances",
    "start": "92159",
    "end": "94759"
  },
  {
    "text": "configured with 300 clusters we have",
    "start": "94759",
    "end": "98159"
  },
  {
    "text": "instances using monor repos and",
    "start": "98159",
    "end": "100000"
  },
  {
    "text": "application set other instance are using",
    "start": "100000",
    "end": "103400"
  },
  {
    "text": "applications like straight applications",
    "start": "103400",
    "end": "105840"
  },
  {
    "text": "and have a lot of them so these variety",
    "start": "105840",
    "end": "108880"
  },
  {
    "text": "of instances allow us to experience",
    "start": "108880",
    "end": "111200"
  },
  {
    "text": "different scaling issues and make sure",
    "start": "111200",
    "end": "114159"
  },
  {
    "text": "our go scale in",
    "start": "114159",
    "end": "116920"
  },
  {
    "text": "general the most common scaling issue",
    "start": "116920",
    "end": "119759"
  },
  {
    "text": "that you've probably experienced before",
    "start": "119759",
    "end": "122159"
  },
  {
    "text": "is this one so you're on call and you",
    "start": "122159",
    "end": "125920"
  },
  {
    "text": "get ICPU",
    "start": "125920",
    "end": "128280"
  },
  {
    "text": "alert you go look at your CPU request in",
    "start": "128280",
    "end": "132560"
  },
  {
    "text": "your chart in your uh graphs and you get",
    "start": "132560",
    "end": "135760"
  },
  {
    "text": "something like this so you have two",
    "start": "135760",
    "end": "137599"
  },
  {
    "text": "choice right you can bump the CPU",
    "start": "137599",
    "end": "139920"
  },
  {
    "text": "request which I'm sure you've done a",
    "start": "139920",
    "end": "142160"
  },
  {
    "text": "couple of times before or you can decide",
    "start": "142160",
    "end": "144800"
  },
  {
    "text": "to investigate it i see a lot of people",
    "start": "144800",
    "end": "146879"
  },
  {
    "text": "laughing so I'm not the only",
    "start": "146879",
    "end": "149239"
  },
  {
    "text": "one or you can uh investigate why is the",
    "start": "149239",
    "end": "153920"
  },
  {
    "text": "CPU that high is Argo supposed to be",
    "start": "153920",
    "end": "156319"
  },
  {
    "text": "like this so yeah sorry about the light",
    "start": "156319",
    "end": "159760"
  },
  {
    "text": "but uh let's try to understand where the",
    "start": "159760",
    "end": "164160"
  },
  {
    "text": "CPU is consumed in the application",
    "start": "164160",
    "end": "166640"
  },
  {
    "text": "controller there are three mains",
    "start": "166640",
    "end": "168160"
  },
  {
    "text": "components the first one is the GitHubs",
    "start": "168160",
    "end": "170480"
  },
  {
    "text": "engine the role of the GitHubs engine is",
    "start": "170480",
    "end": "173040"
  },
  {
    "text": "to watch all the resource updates in",
    "start": "173040",
    "end": "176080"
  },
  {
    "text": "your connected clusters the re the",
    "start": "176080",
    "end": "178879"
  },
  {
    "text": "GitHubs engine will then evaluate if",
    "start": "178879",
    "end": "180879"
  },
  {
    "text": "that resource belongs to an application",
    "start": "180879",
    "end": "183280"
  },
  {
    "text": "or not if it doesn't belong to an",
    "start": "183280",
    "end": "185040"
  },
  {
    "text": "application it will discard it if it",
    "start": "185040",
    "end": "187680"
  },
  {
    "text": "does belong to an app it's going to",
    "start": "187680",
    "end": "190080"
  },
  {
    "text": "cause the reconcile loop so the role of",
    "start": "190080",
    "end": "193680"
  },
  {
    "text": "the reconcile is to figure out if your",
    "start": "193680",
    "end": "196239"
  },
  {
    "text": "app is out of sync or not for that the",
    "start": "196239",
    "end": "199280"
  },
  {
    "text": "reconcile is going to reach to the repo",
    "start": "199280",
    "end": "201760"
  },
  {
    "text": "server most of the time it's going to be",
    "start": "201760",
    "end": "205040"
  },
  {
    "text": "uh in the the git repo is going to be in",
    "start": "205040",
    "end": "208159"
  },
  {
    "text": "the cache so the connection to the repo",
    "start": "208159",
    "end": "210720"
  },
  {
    "text": "server is going to be quite fast and",
    "start": "210720",
    "end": "213519"
  },
  {
    "text": "it's going to compare that with your",
    "start": "213519",
    "end": "215920"
  },
  {
    "text": "Kubernetes manifest also in the cache",
    "start": "215920",
    "end": "218319"
  },
  {
    "text": "it's going to compute ignore differences",
    "start": "218319",
    "end": "220799"
  },
  {
    "text": "so there's a lot of CPU time spent in",
    "start": "220799",
    "end": "224239"
  },
  {
    "text": "the reconciliation and if your app is",
    "start": "224239",
    "end": "226720"
  },
  {
    "text": "out of sync well whether you have autos",
    "start": "226720",
    "end": "229040"
  },
  {
    "text": "sync enabled or you manually trigger a",
    "start": "229040",
    "end": "231360"
  },
  {
    "text": "sync the sync loop is going to kick in",
    "start": "231360",
    "end": "234799"
  },
  {
    "text": "uh the sync loop it doesn't happen quite",
    "start": "234799",
    "end": "237519"
  },
  {
    "text": "often obviously and uh the sync loop is",
    "start": "237519",
    "end": "241519"
  },
  {
    "text": "mostly IO bound because it's going to",
    "start": "241519",
    "end": "243519"
  },
  {
    "text": "call the Kubernetes API to apply the",
    "start": "243519",
    "end": "247959"
  },
  {
    "text": "resource if you don't trust me you can",
    "start": "247959",
    "end": "251280"
  },
  {
    "text": "profile your application with puff and",
    "start": "251280",
    "end": "253920"
  },
  {
    "text": "get exactly which method are consuming",
    "start": "253920",
    "end": "256959"
  },
  {
    "text": "the CPU so you use pruff you get a fling",
    "start": "256959",
    "end": "259840"
  },
  {
    "text": "graph like this and you can investigate",
    "start": "259840",
    "end": "262479"
  },
  {
    "text": "deeper but today we're not going to go",
    "start": "262479",
    "end": "264080"
  },
  {
    "text": "too much into puff you'll have to trust",
    "start": "264080",
    "end": "267080"
  },
  {
    "text": "me so it's on Sunday we get the ICPU",
    "start": "267080",
    "end": "271360"
  },
  {
    "text": "alert we go look at the reconciliation",
    "start": "271360",
    "end": "273560"
  },
  {
    "text": "activity and we see that we have a",
    "start": "273560",
    "end": "275919"
  },
  {
    "text": "constant rate of 32,000",
    "start": "275919",
    "end": "278759"
  },
  {
    "text": "reconciliations now that's weird right",
    "start": "278759",
    "end": "280880"
  },
  {
    "text": "it's Sunday no one's supposed to be",
    "start": "280880",
    "end": "282639"
  },
  {
    "text": "working except us because we're on call",
    "start": "282639",
    "end": "286360"
  },
  {
    "text": "uh so we know that people are not",
    "start": "286360",
    "end": "288639"
  },
  {
    "text": "deploying we know that people are not",
    "start": "288639",
    "end": "290479"
  },
  {
    "text": "syncing they're not changing git why is",
    "start": "290479",
    "end": "293040"
  },
  {
    "text": "Argo doing that much work well we've",
    "start": "293040",
    "end": "296160"
  },
  {
    "text": "seen that the GitHubs engine watch",
    "start": "296160",
    "end": "298160"
  },
  {
    "text": "everything and then it cause reconcile",
    "start": "298160",
    "end": "300000"
  },
  {
    "text": "so let's try to improve that filter so",
    "start": "300000",
    "end": "302880"
  },
  {
    "text": "we have less reconcile events so there's",
    "start": "302880",
    "end": "305919"
  },
  {
    "text": "two ways you can we can do that the",
    "start": "305919",
    "end": "308479"
  },
  {
    "text": "first one is to exclude the resource if",
    "start": "308479",
    "end": "311520"
  },
  {
    "text": "Argo is watching resource that we never",
    "start": "311520",
    "end": "314400"
  },
  {
    "text": "see in the Argo UI we never deploy uh in",
    "start": "314400",
    "end": "318479"
  },
  {
    "text": "our applications we can exclude that",
    "start": "318479",
    "end": "320960"
  },
  {
    "text": "argo doesn't need to watch them if it",
    "start": "320960",
    "end": "323600"
  },
  {
    "text": "does needs to watch these resource then",
    "start": "323600",
    "end": "326240"
  },
  {
    "text": "we can ignore parts of it so we can",
    "start": "326240",
    "end": "329280"
  },
  {
    "text": "configure the ignore resource updates",
    "start": "329280",
    "end": "332080"
  },
  {
    "text": "but the art part is to try to figure out",
    "start": "332080",
    "end": "334560"
  },
  {
    "text": "which resource are actually causing the",
    "start": "334560",
    "end": "337560"
  },
  {
    "text": "reconciliation for that we it's all",
    "start": "337560",
    "end": "340240"
  },
  {
    "text": "documented in the Argo doc so I'm not",
    "start": "340240",
    "end": "343440"
  },
  {
    "text": "going to go over the steps in detail but",
    "start": "343440",
    "end": "345440"
  },
  {
    "text": "we need to enable the bug logs the",
    "start": "345440",
    "end": "348240"
  },
  {
    "text": "reason why we don't want a constant rate",
    "start": "348240",
    "end": "350880"
  },
  {
    "text": "of 32,000 logs to see which resource is",
    "start": "350880",
    "end": "354240"
  },
  {
    "text": "triggering updates the log or the Splunk",
    "start": "354240",
    "end": "356880"
  },
  {
    "text": "team they're not going to like that um",
    "start": "356880",
    "end": "360800"
  },
  {
    "text": "so we enable debug logs for maybe 30",
    "start": "360800",
    "end": "363759"
  },
  {
    "text": "minutes we get enough data to know which",
    "start": "363759",
    "end": "365919"
  },
  {
    "text": "resource are causing or reconcile and we",
    "start": "365919",
    "end": "368800"
  },
  {
    "text": "figure out that in at in it was actually",
    "start": "368800",
    "end": "371600"
  },
  {
    "text": "the HPA the horizontal pod autoscaler",
    "start": "371600",
    "end": "374400"
  },
  {
    "text": "that's causing a big chunk of the",
    "start": "374400",
    "end": "377960"
  },
  {
    "text": "reconcile why is that so if we think",
    "start": "377960",
    "end": "381039"
  },
  {
    "text": "about the HPA we know that the HPA is",
    "start": "381039",
    "end": "383520"
  },
  {
    "text": "going to constantly reach out to our",
    "start": "383520",
    "end": "386479"
  },
  {
    "text": "metrics or monitoring system to get some",
    "start": "386479",
    "end": "388800"
  },
  {
    "text": "value and then try to compute if it",
    "start": "388800",
    "end": "391600"
  },
  {
    "text": "should increase the number or lower the",
    "start": "391600",
    "end": "393680"
  },
  {
    "text": "number of pods and it's going to persist",
    "start": "393680",
    "end": "396400"
  },
  {
    "text": "that in the HP status so whenever it",
    "start": "396400",
    "end": "399520"
  },
  {
    "text": "purses that in the status it cause a",
    "start": "399520",
    "end": "402720"
  },
  {
    "text": "Kubernetes change that the GitHubs",
    "start": "402720",
    "end": "405680"
  },
  {
    "text": "engine received that change will",
    "start": "405680",
    "end": "407919"
  },
  {
    "text": "evaluate is this resource part of my",
    "start": "407919",
    "end": "409840"
  },
  {
    "text": "application yes trigger reconcile now we",
    "start": "409840",
    "end": "413280"
  },
  {
    "text": "know that any change to the status",
    "start": "413280",
    "end": "416000"
  },
  {
    "text": "shouldn't cause our application to be",
    "start": "416000",
    "end": "417600"
  },
  {
    "text": "out of sync right because we don't",
    "start": "417600",
    "end": "419360"
  },
  {
    "text": "commit our status part of our desired",
    "start": "419360",
    "end": "421960"
  },
  {
    "text": "manifest so we try to configure ignore",
    "start": "421960",
    "end": "425120"
  },
  {
    "text": "resource update to ignore all the status",
    "start": "425120",
    "end": "428479"
  },
  {
    "text": "and let's not just do that for the HPA",
    "start": "428479",
    "end": "430960"
  },
  {
    "text": "let's do that for all resource so we do",
    "start": "430960",
    "end": "433840"
  },
  {
    "text": "it for all resource we deploy that and",
    "start": "433840",
    "end": "437120"
  },
  {
    "text": "we now get our reconciliation activity",
    "start": "437120",
    "end": "440000"
  },
  {
    "text": "almost dropping to zero we can see",
    "start": "440000",
    "end": "442639"
  },
  {
    "text": "little bumps but we know that these",
    "start": "442639",
    "end": "444560"
  },
  {
    "text": "bumps are caused by our reconciliation",
    "start": "444560",
    "end": "447199"
  },
  {
    "text": "timeout because they fit with the",
    "start": "447199",
    "end": "448880"
  },
  {
    "text": "reconciliation timeout so every three",
    "start": "448880",
    "end": "451440"
  },
  {
    "text": "minutes we have a little spike of",
    "start": "451440",
    "end": "453800"
  },
  {
    "text": "reconciliation but that's that's a good",
    "start": "453800",
    "end": "456479"
  },
  {
    "text": "improvement right so we deploy that and",
    "start": "456479",
    "end": "460720"
  },
  {
    "text": "now our CPU looks like this what was a",
    "start": "460720",
    "end": "464479"
  },
  {
    "text": "constant rate of a six and seven CPU 67",
    "start": "464479",
    "end": "469360"
  },
  {
    "text": "core is now just spiking a lot and",
    "start": "469360",
    "end": "473280"
  },
  {
    "text": "spikes aren't that bad but we know that",
    "start": "473280",
    "end": "475759"
  },
  {
    "text": "spikes will cause stress in our system",
    "start": "475759",
    "end": "478560"
  },
  {
    "text": "at a specific moment our system needs to",
    "start": "478560",
    "end": "481280"
  },
  {
    "text": "do a lot of work and in a distributed",
    "start": "481280",
    "end": "484720"
  },
  {
    "text": "system like Argo well that can affect",
    "start": "484720",
    "end": "488319"
  },
  {
    "text": "other components one of this component",
    "start": "488319",
    "end": "491280"
  },
  {
    "text": "is the repo server so I've said earlier",
    "start": "491280",
    "end": "494960"
  },
  {
    "text": "that during a reconcile it was going to",
    "start": "494960",
    "end": "497759"
  },
  {
    "text": "reach to the repo server but most of the",
    "start": "497759",
    "end": "500240"
  },
  {
    "text": "time it's going to be in catch so it's",
    "start": "500240",
    "end": "502639"
  },
  {
    "text": "blazing fast but the cache expires if",
    "start": "502639",
    "end": "506479"
  },
  {
    "text": "the cache expire and you try to",
    "start": "506479",
    "end": "509199"
  },
  {
    "text": "reconcile everything in your Argo",
    "start": "509199",
    "end": "512159"
  },
  {
    "text": "instance well you're gonna have a spike",
    "start": "512159",
    "end": "514640"
  },
  {
    "text": "of activity that is transferred to the",
    "start": "514640",
    "end": "517440"
  },
  {
    "text": "repo server and the queue of the repo",
    "start": "517440",
    "end": "519440"
  },
  {
    "text": "server is going to fill up because it's",
    "start": "519440",
    "end": "522080"
  },
  {
    "text": "not in cache the operation takes some",
    "start": "522080",
    "end": "524240"
  },
  {
    "text": "time it needs to run Helm customize and",
    "start": "524240",
    "end": "526800"
  },
  {
    "text": "this may take a minute",
    "start": "526800",
    "end": "529240"
  },
  {
    "text": "um and you might have an autoscaler",
    "start": "529240",
    "end": "532880"
  },
  {
    "text": "configured so if the queue of the repo",
    "start": "532880",
    "end": "534839"
  },
  {
    "text": "server gets filled up well you just",
    "start": "534839",
    "end": "538240"
  },
  {
    "text": "scale more pods the problem with the",
    "start": "538240",
    "end": "540680"
  },
  {
    "text": "spike you do all the operation at the",
    "start": "540680",
    "end": "543600"
  },
  {
    "text": "same time and the autoscaler is going to",
    "start": "543600",
    "end": "546720"
  },
  {
    "text": "come after so the repo server is going",
    "start": "546720",
    "end": "549920"
  },
  {
    "text": "to accept all the requests that it puts",
    "start": "549920",
    "end": "553040"
  },
  {
    "text": "them in queue maybe they're going to",
    "start": "553040",
    "end": "554880"
  },
  {
    "text": "process these requests in 45 minutes uh",
    "start": "554880",
    "end": "558160"
  },
  {
    "text": "so you don't want that and then the new",
    "start": "558160",
    "end": "560640"
  },
  {
    "text": "pods that are scaled up by the",
    "start": "560640",
    "end": "562760"
  },
  {
    "text": "autoscaler they're just going to have a",
    "start": "562760",
    "end": "564880"
  },
  {
    "text": "small amount of the request because by",
    "start": "564880",
    "end": "566959"
  },
  {
    "text": "default how does it work when you route",
    "start": "566959",
    "end": "569680"
  },
  {
    "text": "requests in Kubernetes it's run Robin so",
    "start": "569680",
    "end": "572959"
  },
  {
    "text": "the pod was there at the beginning that",
    "start": "572959",
    "end": "575279"
  },
  {
    "text": "the queue is full there's like a",
    "start": "575279",
    "end": "577920"
  },
  {
    "text": "thousand items in the queue still",
    "start": "577920",
    "end": "580000"
  },
  {
    "text": "receive request because it's just doing",
    "start": "580000",
    "end": "582480"
  },
  {
    "text": "a",
    "start": "582480",
    "end": "584360"
  },
  {
    "text": "roundroin so spikes are bad it puts",
    "start": "584360",
    "end": "587480"
  },
  {
    "text": "pressure in different components in your",
    "start": "587480",
    "end": "590240"
  },
  {
    "text": "system you want something that's steady",
    "start": "590240",
    "end": "592399"
  },
  {
    "text": "and predictable there's are some mig",
    "start": "592399",
    "end": "595200"
  },
  {
    "text": "mitigation option to uh to reduce the",
    "start": "595200",
    "end": "600560"
  },
  {
    "text": "spikes so you can configure the",
    "start": "600560",
    "end": "602800"
  },
  {
    "text": "reconciliation timeout to be an hour or",
    "start": "602800",
    "end": "605920"
  },
  {
    "text": "two now it's not going to fix the issue",
    "start": "605920",
    "end": "608160"
  },
  {
    "text": "it's just going to delay it if you do",
    "start": "608160",
    "end": "610560"
  },
  {
    "text": "that you also need to configure the git",
    "start": "610560",
    "end": "612640"
  },
  {
    "text": "web hook because you don't want your",
    "start": "612640",
    "end": "614320"
  },
  {
    "text": "application to be out of sync an hour",
    "start": "614320",
    "end": "616079"
  },
  {
    "text": "after you've committed you want the",
    "start": "616079",
    "end": "618560"
  },
  {
    "text": "GitHub event as soon as the commit is",
    "start": "618560",
    "end": "622279"
  },
  {
    "text": "made and you must configure the manifest",
    "start": "622279",
    "end": "628160"
  },
  {
    "text": "generate path now this is mostly used",
    "start": "628160",
    "end": "631920"
  },
  {
    "text": "for monor repo but you should also",
    "start": "631920",
    "end": "634399"
  },
  {
    "text": "configure it for a single application if",
    "start": "634399",
    "end": "637040"
  },
  {
    "text": "you have a genkins file if you have a CI",
    "start": "637040",
    "end": "639200"
  },
  {
    "text": "a",
    "start": "639200",
    "end": "640839"
  },
  {
    "text": "reme update to cause a reconcile in Argo",
    "start": "640839",
    "end": "645040"
  },
  {
    "text": "that's",
    "start": "645040",
    "end": "645880"
  },
  {
    "text": "unnecessary uh and if you use monor repo",
    "start": "645880",
    "end": "649040"
  },
  {
    "text": "you you absolutely want to configure",
    "start": "649040",
    "end": "652720"
  },
  {
    "text": "this annotation because if you have a",
    "start": "652720",
    "end": "654320"
  },
  {
    "text": "thousand app that syncs the same repo",
    "start": "654320",
    "end": "657760"
  },
  {
    "text": "URL whenever there's one comet it's",
    "start": "657760",
    "end": "660320"
  },
  {
    "text": "going to cause a spike because there's",
    "start": "660320",
    "end": "662240"
  },
  {
    "text": "1,000 app that are going to be",
    "start": "662240",
    "end": "664200"
  },
  {
    "text": "reconciled at the same time",
    "start": "664200",
    "end": "667040"
  },
  {
    "text": "but we said we're not mitigating issues",
    "start": "667040",
    "end": "669200"
  },
  {
    "text": "today we're fixing them so fixing it is",
    "start": "669200",
    "end": "672720"
  },
  {
    "text": "actually quite easy you just configure a",
    "start": "672720",
    "end": "675040"
  },
  {
    "text": "jitter now what's a",
    "start": "675040",
    "end": "677240"
  },
  {
    "text": "jitter at at the end of a the",
    "start": "677240",
    "end": "680240"
  },
  {
    "text": "reconciliation timeout it's going to add",
    "start": "680240",
    "end": "682880"
  },
  {
    "text": "an amount of time that's a buffer so",
    "start": "682880",
    "end": "685600"
  },
  {
    "text": "let's say you have your",
    "start": "685600",
    "end": "688839"
  },
  {
    "text": "reconciliation timeout is an hour and",
    "start": "688839",
    "end": "691519"
  },
  {
    "text": "you configure the jitter to be 30",
    "start": "691519",
    "end": "693519"
  },
  {
    "text": "minutes you now have the guarantee that",
    "start": "693519",
    "end": "696399"
  },
  {
    "text": "your app is going to be refreshed",
    "start": "696399",
    "end": "699040"
  },
  {
    "text": "between an hour and an hour 30 minutes",
    "start": "699040",
    "end": "702000"
  },
  {
    "text": "so instead of having a spike at an hour",
    "start": "702000",
    "end": "705720"
  },
  {
    "text": "exactly these requests will be spread",
    "start": "705720",
    "end": "708959"
  },
  {
    "text": "over 30",
    "start": "708959",
    "end": "711680"
  },
  {
    "text": "minutes so what if reconciliation is",
    "start": "711880",
    "end": "715920"
  },
  {
    "text": "still high uh you have different things",
    "start": "715920",
    "end": "718480"
  },
  {
    "text": "that can cause I reconciliation but I'm",
    "start": "718480",
    "end": "722160"
  },
  {
    "text": "already behind schedule so let me sh let",
    "start": "722160",
    "end": "726240"
  },
  {
    "text": "me see raise your hand if you use orphan",
    "start": "726240",
    "end": "729160"
  },
  {
    "text": "resource",
    "start": "729160",
    "end": "731560"
  },
  {
    "text": "monitoring okay some of you so I'll go",
    "start": "731560",
    "end": "734399"
  },
  {
    "text": "quickly but I'll be at the Argo boot",
    "start": "734399",
    "end": "737600"
  },
  {
    "text": "after for your questions but basically",
    "start": "737600",
    "end": "740639"
  },
  {
    "text": "orphan resource monitoring is going to",
    "start": "740639",
    "end": "743360"
  },
  {
    "text": "cause an update whenever there's a",
    "start": "743360",
    "end": "747279"
  },
  {
    "text": "resource in the name space that changes",
    "start": "747279",
    "end": "750399"
  },
  {
    "text": "at the beginning I said the resource",
    "start": "750399",
    "end": "752160"
  },
  {
    "text": "needs to be part of your application",
    "start": "752160",
    "end": "754639"
  },
  {
    "text": "well with orphan resource monitoring",
    "start": "754639",
    "end": "756560"
  },
  {
    "text": "it's not true it just needs to be in the",
    "start": "756560",
    "end": "758639"
  },
  {
    "text": "same name space so if you have random",
    "start": "758639",
    "end": "761760"
  },
  {
    "text": "resource in your namespace that you",
    "start": "761760",
    "end": "763680"
  },
  {
    "text": "don't know about them and these resource",
    "start": "763680",
    "end": "765519"
  },
  {
    "text": "change then the reconciliations are",
    "start": "765519",
    "end": "769200"
  },
  {
    "text": "going to go up so if you for the ones",
    "start": "769200",
    "end": "771360"
  },
  {
    "text": "that use orphan resource monitoring try",
    "start": "771360",
    "end": "774320"
  },
  {
    "text": "to turn it off and see what happens",
    "start": "774320",
    "end": "776720"
  },
  {
    "text": "someone in the community tried that and",
    "start": "776720",
    "end": "778720"
  },
  {
    "text": "they posted their graph on a GitHub",
    "start": "778720",
    "end": "780399"
  },
  {
    "text": "issue and that's their reconciliation",
    "start": "780399",
    "end": "782959"
  },
  {
    "text": "activity after disabling it that's their",
    "start": "782959",
    "end": "786240"
  },
  {
    "text": "CPU activity after disabling it so we",
    "start": "786240",
    "end": "789279"
  },
  {
    "text": "can see that the CPU and the",
    "start": "789279",
    "end": "791760"
  },
  {
    "text": "reconciliation activity are almost the",
    "start": "791760",
    "end": "795000"
  },
  {
    "text": "same so there are some limitation we",
    "start": "795000",
    "end": "798240"
  },
  {
    "text": "don't have time to talk about them let's",
    "start": "798240",
    "end": "800959"
  },
  {
    "text": "do a little recap of what we've learned",
    "start": "800959",
    "end": "803120"
  },
  {
    "text": "so far so we've learned that there's a",
    "start": "803120",
    "end": "805760"
  },
  {
    "text": "strong correlation between CPU usage and",
    "start": "805760",
    "end": "808800"
  },
  {
    "text": "reconciliation activities so if you want",
    "start": "808800",
    "end": "811360"
  },
  {
    "text": "less CPU you want less reconciliation to",
    "start": "811360",
    "end": "814720"
  },
  {
    "text": "have less reconciliation you can ignore",
    "start": "814720",
    "end": "817360"
  },
  {
    "text": "the updates you don't want we've seen",
    "start": "817360",
    "end": "821040"
  },
  {
    "text": "that spikes cause problems so you want a",
    "start": "821040",
    "end": "824160"
  },
  {
    "text": "smooth CPU you don't want spikes so you",
    "start": "824160",
    "end": "827440"
  },
  {
    "text": "can configure jitter to smooth that that",
    "start": "827440",
    "end": "830000"
  },
  {
    "text": "out and we haven't seen that orphan mon",
    "start": "830000",
    "end": "833920"
  },
  {
    "text": "orphan resource can be very",
    "start": "833920",
    "end": "837880"
  },
  {
    "text": "costly so we fix the CPU what's the next",
    "start": "837880",
    "end": "841639"
  },
  {
    "text": "alert high memory alert we go look at",
    "start": "841639",
    "end": "845279"
  },
  {
    "text": "our memory usage and it looks like this",
    "start": "845279",
    "end": "848399"
  },
  {
    "text": "for the people in the back it looks like",
    "start": "848399",
    "end": "851079"
  },
  {
    "text": "this so we've reached about a 100",
    "start": "851079",
    "end": "854079"
  },
  {
    "text": "gigabyte in our application controller",
    "start": "854079",
    "end": "857519"
  },
  {
    "text": "why is that we can we're not going to",
    "start": "857519",
    "end": "859920"
  },
  {
    "text": "bump the request at this point because",
    "start": "859920",
    "end": "862800"
  },
  {
    "text": "or pawn may not even be scheduled on the",
    "start": "862800",
    "end": "865800"
  },
  {
    "text": "node so what's consuming the memory well",
    "start": "865800",
    "end": "869279"
  },
  {
    "text": "we've talked about GitHuben and we know",
    "start": "869279",
    "end": "871360"
  },
  {
    "text": "it watches all the",
    "start": "871360",
    "end": "873160"
  },
  {
    "text": "cluster well when it watch a a resource",
    "start": "873160",
    "end": "876320"
  },
  {
    "text": "it keeps that resource in cache so as",
    "start": "876320",
    "end": "880079"
  },
  {
    "text": "you add cluster in your app Argo",
    "start": "880079",
    "end": "883560"
  },
  {
    "text": "instance the cache is going to grow and",
    "start": "883560",
    "end": "886240"
  },
  {
    "text": "the memory is going to grow with it so",
    "start": "886240",
    "end": "889199"
  },
  {
    "text": "how can we reduce the memory well we can",
    "start": "889199",
    "end": "892399"
  },
  {
    "text": "watch less object again we can exclude",
    "start": "892399",
    "end": "896079"
  },
  {
    "text": "what we don't need to watch there's",
    "start": "896079",
    "end": "898720"
  },
  {
    "text": "another setting that we often don't",
    "start": "898720",
    "end": "900880"
  },
  {
    "text": "think about and it's the cluster",
    "start": "900880",
    "end": "902399"
  },
  {
    "text": "namespace filter now if you have a",
    "start": "902399",
    "end": "905680"
  },
  {
    "text": "cluster that has a 100 namespace in it",
    "start": "905680",
    "end": "908320"
  },
  {
    "text": "and you only deploy in five name space",
    "start": "908320",
    "end": "910480"
  },
  {
    "text": "in that cluster well don't watch all the",
    "start": "910480",
    "end": "913279"
  },
  {
    "text": "namespace in the cluster you can",
    "start": "913279",
    "end": "915040"
  },
  {
    "text": "configure it to only watch five name",
    "start": "915040",
    "end": "918000"
  },
  {
    "text": "space and you'll get a 95% memory",
    "start": "918000",
    "end": "921880"
  },
  {
    "text": "improvement another way to configure",
    "start": "921880",
    "end": "924000"
  },
  {
    "text": "that is by using respect arback so",
    "start": "924000",
    "end": "926480"
  },
  {
    "text": "instead of configure instead of",
    "start": "926480",
    "end": "928639"
  },
  {
    "text": "configuring in Argo which resource to",
    "start": "928639",
    "end": "931519"
  },
  {
    "text": "watch you configure your RO and RO",
    "start": "931519",
    "end": "933880"
  },
  {
    "text": "bindings and Argo is not going to watch",
    "start": "933880",
    "end": "936800"
  },
  {
    "text": "the the resource that it doesn't have",
    "start": "936800",
    "end": "939279"
  },
  {
    "text": "permission",
    "start": "939279",
    "end": "940680"
  },
  {
    "text": "on and realistically if we configure all",
    "start": "940680",
    "end": "944880"
  },
  {
    "text": "that how much memory do we have now",
    "start": "944880",
    "end": "947360"
  },
  {
    "text": "maybe we have 80 GB that's still a lot",
    "start": "947360",
    "end": "951519"
  },
  {
    "text": "so our last resort at this point is",
    "start": "951519",
    "end": "954480"
  },
  {
    "text": "sharding how does sharding work it's",
    "start": "954480",
    "end": "957440"
  },
  {
    "text": "simple you split the application",
    "start": "957440",
    "end": "959519"
  },
  {
    "text": "controller in two shards and either with",
    "start": "959519",
    "end": "962320"
  },
  {
    "text": "roundroin or a uniform distribution your",
    "start": "962320",
    "end": "965519"
  },
  {
    "text": "cluster are going to get assigned to a",
    "start": "965519",
    "end": "967680"
  },
  {
    "text": "specific",
    "start": "967680",
    "end": "968839"
  },
  {
    "text": "shard so right now it may look like this",
    "start": "968839",
    "end": "973639"
  },
  {
    "text": "but it may also look like this because",
    "start": "973639",
    "end": "976959"
  },
  {
    "text": "we're doing a uniform distribution we're",
    "start": "976959",
    "end": "979519"
  },
  {
    "text": "doing roundrobins we're not sharding",
    "start": "979519",
    "end": "982560"
  },
  {
    "text": "based on the cluster memory",
    "start": "982560",
    "end": "985240"
  },
  {
    "text": "footprint and or cluster are not all",
    "start": "985240",
    "end": "989040"
  },
  {
    "text": "equal we have production clusters these",
    "start": "989040",
    "end": "991839"
  },
  {
    "text": "production cluster will have a lot of",
    "start": "991839",
    "end": "993800"
  },
  {
    "text": "resources if you we have test cluster",
    "start": "993800",
    "end": "997120"
  },
  {
    "text": "test cluster maybe they won't have as",
    "start": "997120",
    "end": "999519"
  },
  {
    "text": "much resource but for Argo they are two",
    "start": "999519",
    "end": "1002240"
  },
  {
    "text": "cluster they're the",
    "start": "1002240",
    "end": "1003880"
  },
  {
    "text": "same so what can we do about it uh we",
    "start": "1003880",
    "end": "1007440"
  },
  {
    "text": "can manually assign",
    "start": "1007440",
    "end": "1010160"
  },
  {
    "text": "uh cluster to specific shards so we can",
    "start": "1010160",
    "end": "1014160"
  },
  {
    "text": "recalibrate our memory usage in our",
    "start": "1014160",
    "end": "1016560"
  },
  {
    "text": "shards there's another project that's",
    "start": "1016560",
    "end": "1019040"
  },
  {
    "text": "pretty cool it's called the Argo CD",
    "start": "1019040",
    "end": "1021040"
  },
  {
    "text": "agent i don't know if you've seen the",
    "start": "1021040",
    "end": "1022720"
  },
  {
    "text": "talks about it today but it's also going",
    "start": "1022720",
    "end": "1026480"
  },
  {
    "text": "to be a way to fix this issue in the",
    "start": "1026480",
    "end": "1028959"
  },
  {
    "text": "future but right now we have manual",
    "start": "1028959",
    "end": "1032160"
  },
  {
    "text": "assignment we manually assign our",
    "start": "1032160",
    "end": "1034640"
  },
  {
    "text": "cluster to shards and we spin up",
    "start": "1034640",
    "end": "1038319"
  },
  {
    "text": "multiple application controller and we",
    "start": "1038319",
    "end": "1040480"
  },
  {
    "text": "get something like this now 15 gigabyte",
    "start": "1040480",
    "end": "1044079"
  },
  {
    "text": "is not that bad it's much easier to",
    "start": "1044079",
    "end": "1046880"
  },
  {
    "text": "schedule we're proud of oursel we can go",
    "start": "1046880",
    "end": "1049520"
  },
  {
    "text": "to our Sunday brunch and then we go to",
    "start": "1049520",
    "end": "1052720"
  },
  {
    "text": "the office on Monday because some people",
    "start": "1052720",
    "end": "1055520"
  },
  {
    "text": "need to go to the",
    "start": "1055520",
    "end": "1057000"
  },
  {
    "text": "office and someone tells us \"Hey Argo is",
    "start": "1057000",
    "end": "1060720"
  },
  {
    "text": "slow.\" You've heard that",
    "start": "1060720",
    "end": "1062760"
  },
  {
    "text": "before realistically of",
    "start": "1062760",
    "end": "1065880"
  },
  {
    "text": "course it's a tough one to investigate",
    "start": "1065880",
    "end": "1068880"
  },
  {
    "text": "why is Argo slow there are many things",
    "start": "1068880",
    "end": "1070960"
  },
  {
    "text": "that can go wrong but today we're",
    "start": "1070960",
    "end": "1074000"
  },
  {
    "text": "interested if all our applications are",
    "start": "1074000",
    "end": "1076880"
  },
  {
    "text": "slow is it only one specific app or is",
    "start": "1076880",
    "end": "1079280"
  },
  {
    "text": "it all the applications there are a few",
    "start": "1079280",
    "end": "1081760"
  },
  {
    "text": "questions we can ask we can check if our",
    "start": "1081760",
    "end": "1084640"
  },
  {
    "text": "work cues are actually uh filling up or",
    "start": "1084640",
    "end": "1088080"
  },
  {
    "text": "if our items are being processed we can",
    "start": "1088080",
    "end": "1090880"
  },
  {
    "text": "look at our external dependencies so we",
    "start": "1090880",
    "end": "1093440"
  },
  {
    "text": "have two big one in Argo we have git and",
    "start": "1093440",
    "end": "1097360"
  },
  {
    "text": "we have Kubernetes API",
    "start": "1097360",
    "end": "1100120"
  },
  {
    "text": "server so let's look at our cues if our",
    "start": "1100120",
    "end": "1103440"
  },
  {
    "text": "cues looks like this uh that's bad cues",
    "start": "1103440",
    "end": "1107200"
  },
  {
    "text": "should almost be",
    "start": "1107200",
    "end": "1110799"
  },
  {
    "text": "zero",
    "start": "1111240",
    "end": "1113720"
  },
  {
    "text": "so there's one thing that may cause uh",
    "start": "1113720",
    "end": "1117520"
  },
  {
    "text": "Argo to be slow globally it's still",
    "start": "1117520",
    "end": "1120480"
  },
  {
    "text": "related to CPU i don't have time to talk",
    "start": "1120480",
    "end": "1122960"
  },
  {
    "text": "about it uh but go watch this talk about",
    "start": "1122960",
    "end": "1126400"
  },
  {
    "text": "uh CPU limits and throttling it's super",
    "start": "1126400",
    "end": "1128799"
  },
  {
    "text": "interesting but there's dozens of talk",
    "start": "1128799",
    "end": "1131280"
  },
  {
    "text": "on CPU limits but basically if C the CPU",
    "start": "1131280",
    "end": "1135280"
  },
  {
    "text": "gets throttled your operation takes more",
    "start": "1135280",
    "end": "1137600"
  },
  {
    "text": "time so what you want to do is increase",
    "start": "1137600",
    "end": "1140000"
  },
  {
    "text": "the CPU limits or you may want to",
    "start": "1140000",
    "end": "1142160"
  },
  {
    "text": "consider removing it totally so let's",
    "start": "1142160",
    "end": "1145440"
  },
  {
    "text": "assume low",
    "start": "1145440",
    "end": "1147160"
  },
  {
    "text": "reconciliation or CPU is fine we can",
    "start": "1147160",
    "end": "1150160"
  },
  {
    "text": "configure increasing the number of",
    "start": "1150160",
    "end": "1152520"
  },
  {
    "text": "parallelism in uh Argo so we have the",
    "start": "1152520",
    "end": "1156000"
  },
  {
    "text": "status processor for the reconciliation",
    "start": "1156000",
    "end": "1159039"
  },
  {
    "text": "and we have the operation processor for",
    "start": "1159039",
    "end": "1161200"
  },
  {
    "text": "the sync now I'll say it one last time",
    "start": "1161200",
    "end": "1164080"
  },
  {
    "text": "if your CPU is high don't increase that",
    "start": "1164080",
    "end": "1168559"
  },
  {
    "text": "if Argo is there and you try to increase",
    "start": "1168559",
    "end": "1171760"
  },
  {
    "text": "the parallelism it's not going to go",
    "start": "1171760",
    "end": "1175480"
  },
  {
    "text": "well so another thing is I said we have",
    "start": "1175480",
    "end": "1179200"
  },
  {
    "text": "to monitor external dependency so one",
    "start": "1179200",
    "end": "1181760"
  },
  {
    "text": "way to do that for the interaction with",
    "start": "1181760",
    "end": "1184600"
  },
  {
    "text": "Kubernetes is to configure the is to",
    "start": "1184600",
    "end": "1188400"
  },
  {
    "text": "look at the audit logs so Kubernetes",
    "start": "1188400",
    "end": "1192960"
  },
  {
    "text": "will produce an audit log for each call",
    "start": "1192960",
    "end": "1195760"
  },
  {
    "text": "to its API you can use the user agent",
    "start": "1195760",
    "end": "1198720"
  },
  {
    "text": "Argo CD application controller group",
    "start": "1198720",
    "end": "1202080"
  },
  {
    "text": "those requests and you can see if any",
    "start": "1202080",
    "end": "1204880"
  },
  {
    "text": "kind is using a lot of the API calls",
    "start": "1204880",
    "end": "1209520"
  },
  {
    "text": "that happens because when we interact",
    "start": "1209520",
    "end": "1212640"
  },
  {
    "text": "with Kubernetes we have a clientside",
    "start": "1212640",
    "end": "1215919"
  },
  {
    "text": "rate limit so if one client is using all",
    "start": "1215919",
    "end": "1219360"
  },
  {
    "text": "the API call because there's a problem",
    "start": "1219360",
    "end": "1221600"
  },
  {
    "text": "we're going to get client side rate",
    "start": "1221600",
    "end": "1223200"
  },
  {
    "text": "limited and everything is going to be",
    "start": "1223200",
    "end": "1225039"
  },
  {
    "text": "slower so in Argo 3.0 we added the",
    "start": "1225039",
    "end": "1229360"
  },
  {
    "text": "client sign metrics so now it's easier",
    "start": "1229360",
    "end": "1231440"
  },
  {
    "text": "to monitor if we're being rate limited",
    "start": "1231440",
    "end": "1234080"
  },
  {
    "text": "or if we have lat latency when uh",
    "start": "1234080",
    "end": "1237360"
  },
  {
    "text": "interacting with the API server if we do",
    "start": "1237360",
    "end": "1241360"
  },
  {
    "text": "you can configure uh any of those",
    "start": "1241360",
    "end": "1245039"
  },
  {
    "text": "metrics now I'll try to uh I'll try to",
    "start": "1245039",
    "end": "1249120"
  },
  {
    "text": "explain why we added the client smetrics",
    "start": "1249120",
    "end": "1252000"
  },
  {
    "text": "is do you remember earlier when I said",
    "start": "1252000",
    "end": "1254720"
  },
  {
    "text": "to optimize memory you can configure the",
    "start": "1254720",
    "end": "1257480"
  },
  {
    "text": "cluster namespace filter well that's",
    "start": "1257480",
    "end": "1260799"
  },
  {
    "text": "what we did before we were watching all",
    "start": "1260799",
    "end": "1263440"
  },
  {
    "text": "our cluster so let's say there's a 100",
    "start": "1263440",
    "end": "1266000"
  },
  {
    "text": "kind in your cluster we were working",
    "start": "1266000",
    "end": "1268559"
  },
  {
    "text": "making a 100 API calls now we've",
    "start": "1268559",
    "end": "1272000"
  },
  {
    "text": "configured six namespace in that cluster",
    "start": "1272000",
    "end": "1274960"
  },
  {
    "text": "so Now we're making a 100 API call per",
    "start": "1274960",
    "end": "1277679"
  },
  {
    "text": "namespace and we're now making 600 API",
    "start": "1277679",
    "end": "1280720"
  },
  {
    "text": "call you can see that the maxidal",
    "start": "1280720",
    "end": "1283840"
  },
  {
    "text": "connections is 500 so by just changing",
    "start": "1283840",
    "end": "1288480"
  },
  {
    "text": "our memory and how we use the memory",
    "start": "1288480",
    "end": "1290799"
  },
  {
    "text": "we've now uh",
    "start": "1290799",
    "end": "1293159"
  },
  {
    "text": "reach well we've we've now we are now",
    "start": "1293159",
    "end": "1296240"
  },
  {
    "text": "over the limits of the default 500 and",
    "start": "1296240",
    "end": "1299679"
  },
  {
    "text": "this created another bottleneck when we",
    "start": "1299679",
    "end": "1302480"
  },
  {
    "text": "were interacting with the API server",
    "start": "1302480",
    "end": "1304640"
  },
  {
    "text": "temporary deadlock until or connection",
    "start": "1304640",
    "end": "1309840"
  },
  {
    "text": "expired all right so that was a lot for",
    "start": "1310120",
    "end": "1313840"
  },
  {
    "text": "today i'm running out of time but this",
    "start": "1313840",
    "end": "1317120"
  },
  {
    "text": "isn't even half of the scaling issues",
    "start": "1317120",
    "end": "1320320"
  },
  {
    "text": "you can encounter uh there are a lot of",
    "start": "1320320",
    "end": "1323760"
  },
  {
    "text": "different possibilities where Argo will",
    "start": "1323760",
    "end": "1327520"
  },
  {
    "text": "uh won't scale because whenever you fix",
    "start": "1327520",
    "end": "1331039"
  },
  {
    "text": "or configure Argo",
    "start": "1331039",
    "end": "1334159"
  },
  {
    "text": "uh whenever you configure some Argo",
    "start": "1334159",
    "end": "1336480"
  },
  {
    "text": "settings you and you fix the bottleneck",
    "start": "1336480",
    "end": "1339039"
  },
  {
    "text": "somewhere the pressure point goes at",
    "start": "1339039",
    "end": "1341919"
  },
  {
    "text": "other places so you always have",
    "start": "1341919",
    "end": "1345440"
  },
  {
    "text": "bottlenecks in your system and you need",
    "start": "1345440",
    "end": "1347520"
  },
  {
    "text": "to address them",
    "start": "1347520",
    "end": "1351159"
  },
  {
    "text": "now I'm sure some of you have learned",
    "start": "1352280",
    "end": "1355760"
  },
  {
    "text": "about new settings here that you're not",
    "start": "1355760",
    "end": "1357880"
  },
  {
    "text": "using and you're excited to configure",
    "start": "1357880",
    "end": "1360799"
  },
  {
    "text": "them",
    "start": "1360799",
    "end": "1362039"
  },
  {
    "text": "uh configuring the scalability in Argo",
    "start": "1362039",
    "end": "1365840"
  },
  {
    "text": "CD it's like trying to pour the perfect",
    "start": "1365840",
    "end": "1368960"
  },
  {
    "text": "espresso shot now I drink a lot of",
    "start": "1368960",
    "end": "1371960"
  },
  {
    "text": "espresso you never change all the",
    "start": "1371960",
    "end": "1374480"
  },
  {
    "text": "setting at the same time you change one",
    "start": "1374480",
    "end": "1377120"
  },
  {
    "text": "setting you pour a shot you taste it and",
    "start": "1377120",
    "end": "1380799"
  },
  {
    "text": "then you iterate on that if you try to",
    "start": "1380799",
    "end": "1383200"
  },
  {
    "text": "change all the setting at the same time",
    "start": "1383200",
    "end": "1385280"
  },
  {
    "text": "you won't like the taste and even if it",
    "start": "1385280",
    "end": "1387280"
  },
  {
    "text": "tastes good you won't know why",
    "start": "1387280",
    "end": "1390440"
  },
  {
    "text": "so if you've experienced uh one last",
    "start": "1390440",
    "end": "1393679"
  },
  {
    "text": "thing uh if you've experienced scaling",
    "start": "1393679",
    "end": "1396640"
  },
  {
    "text": "issues and you fix them uh we're",
    "start": "1396640",
    "end": "1399720"
  },
  {
    "text": "interested interested to know so you can",
    "start": "1399720",
    "end": "1402400"
  },
  {
    "text": "reach out uh to six scalability or you",
    "start": "1402400",
    "end": "1405280"
  },
  {
    "text": "can reach out to me uh specifically and",
    "start": "1405280",
    "end": "1408799"
  },
  {
    "text": "uh if you have questions I'll be",
    "start": "1408799",
    "end": "1410559"
  },
  {
    "text": "available after this talk uh at the in",
    "start": "1410559",
    "end": "1413280"
  },
  {
    "text": "boot so thank you everyone thanks for",
    "start": "1413280",
    "end": "1416000"
  },
  {
    "text": "listening and I hope you'll improve Argo",
    "start": "1416000",
    "end": "1421200"
  },
  {
    "text": "performance we are all weapons",
    "start": "1421240",
    "end": "1425880"
  }
]