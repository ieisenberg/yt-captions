[
  {
    "start": "0",
    "end": "108000"
  },
  {
    "text": "it's so good already oh I thought I",
    "start": "1730",
    "end": "8029"
  },
  {
    "text": "mean I'm going to prick",
    "start": "8030",
    "end": "14389"
  },
  {
    "text": "actually help put",
    "start": "14389",
    "end": "20119"
  },
  {
    "text": "in count as the solutions with the",
    "start": "22010",
    "end": "28189"
  },
  {
    "text": "stories were based on or like on cameras with strong load",
    "start": "28189",
    "end": "35149"
  },
  {
    "text": "generically so",
    "start": "38059",
    "end": "43190"
  },
  {
    "text": "producers so we are it um Joe China and",
    "start": "43190",
    "end": "48539"
  },
  {
    "text": "from the peak Oh his baby it's also there the olive",
    "start": "48539",
    "end": "55820"
  },
  {
    "text": "that quarter is based so our funding members are not",
    "start": "55820",
    "end": "62690"
  },
  {
    "text": "thing contributing to the internal Google cluster management system and the",
    "start": "62690",
    "end": "68100"
  },
  {
    "text": "open source kubernetes projects also as a proud member of CN CF we're all coding",
    "start": "68100",
    "end": "77270"
  },
  {
    "text": "of like meter training session deification as",
    "start": "78950",
    "end": "84560"
  },
  {
    "text": "public like articles or even books so if any of you whoever wants to like speak",
    "start": "84560",
    "end": "92310"
  },
  {
    "text": "at one of our events so do contact us so it's very welcome us as a company",
    "start": "92310",
    "end": "98869"
  },
  {
    "text": "providing risk product to help transform their IT",
    "start": "98869",
    "end": "106170"
  },
  {
    "text": "infrastructure so the stories and the morals were a",
    "start": "106170",
    "end": "113250"
  },
  {
    "start": "108000",
    "end": "147000"
  },
  {
    "text": "gift from our own customers from a wide array from the e-commerce Finance carry",
    "start": "113250",
    "end": "122780"
  },
  {
    "text": "like in the industry stateful app and up",
    "start": "124040",
    "end": "131629"
  },
  {
    "text": "legacy so yeah just a first quick check so not",
    "start": "131629",
    "end": "139290"
  },
  {
    "text": "no Corbin at ease",
    "start": "139290",
    "end": "142340"
  },
  {
    "start": "147000",
    "end": "208000"
  },
  {
    "text": "bananas has become a kind of a common sense right so in Q Khan or in the US so",
    "start": "148730",
    "end": "155520"
  },
  {
    "text": "the question is how it okay studies that need to speak too loud [Music]",
    "start": "155520",
    "end": "164160"
  },
  {
    "text": "all right so you may wonder like how kubernetes is",
    "start": "165680",
    "end": "170970"
  },
  {
    "text": "doing in China you know these photos were taken from our events so they can",
    "start": "170970",
    "end": "177240"
  },
  {
    "text": "pretty much give the answer we have held about like to meetups every",
    "start": "177240",
    "end": "182850"
  },
  {
    "text": "quarter in China and there are like hundreds of attendees in each of these",
    "start": "182850",
    "end": "188370"
  },
  {
    "text": "events you know we have got about 3,000 followers on our kubernetes study group",
    "start": "188370",
    "end": "196440"
  },
  {
    "text": "so that is the status of the kubernetes in China so basically there is the",
    "start": "196440",
    "end": "203070"
  },
  {
    "text": "thriving community with growing user base and engaged partners",
    "start": "203070",
    "end": "208160"
  },
  {
    "start": "208000",
    "end": "297000"
  },
  {
    "text": "so before I dive into the details I wanted to give a quick summary of what",
    "start": "208160",
    "end": "214170"
  },
  {
    "text": "we found upfront from our case studies so first the positives",
    "start": "214170",
    "end": "220490"
  },
  {
    "text": "we found one of the biggest reasons that people buy into kubernetes in China is",
    "start": "220490",
    "end": "226560"
  },
  {
    "text": "due to the strong community and the big brand names behind it and also people",
    "start": "226560",
    "end": "232890"
  },
  {
    "text": "loved its native service-oriented design and elegant API and architecture",
    "start": "232890",
    "end": "238440"
  },
  {
    "text": "especially it is very extensive and also it provides rich functionality compared",
    "start": "238440",
    "end": "245010"
  },
  {
    "text": "to swarm or mesos so it goes really beyond just scheduling and a resource",
    "start": "245010",
    "end": "250500"
  },
  {
    "text": "management it also provides a lot of like config configuration management Network policy secrets etc so they're",
    "start": "250500",
    "end": "259320"
  },
  {
    "text": "not just about resource management or scheduling but in reality so we also",
    "start": "259320",
    "end": "265500"
  },
  {
    "text": "found there are some factors that affected kubernetes user experience",
    "start": "265500",
    "end": "270610"
  },
  {
    "text": "so from the list you can see there are some of them are due to like the",
    "start": "270610",
    "end": "275930"
  },
  {
    "text": "different environments between the US and the Chinese enterprises like the different cloud providers",
    "start": "275930",
    "end": "281470"
  },
  {
    "text": "and also people's operational habits because in China so most of the",
    "start": "281470",
    "end": "287990"
  },
  {
    "text": "operators are more familiar with user interface right so they're not so much a",
    "start": "287990",
    "end": "294950"
  },
  {
    "text": "big fan of commits lines when people do get kubernetes up and",
    "start": "294950",
    "end": "301160"
  },
  {
    "start": "297000",
    "end": "351000"
  },
  {
    "text": "running so there are also some pitfalls we found that need to also be addressed you order to run kubernetes continuously",
    "start": "301160",
    "end": "307850"
  },
  {
    "text": "and successfully in production systems so this one is a short list for example",
    "start": "307850",
    "end": "313340"
  },
  {
    "text": "for the first one so even though we know kubernetes has the notion of resource quota and the limits but in practice how",
    "start": "313340",
    "end": "320750"
  },
  {
    "text": "to set the exact values for them represent a big challenge because if we",
    "start": "320750",
    "end": "326030"
  },
  {
    "text": "set the quota too low then applications may crash because they",
    "start": "326030",
    "end": "331460"
  },
  {
    "text": "usually require a higher CPU spikes when they start up but if we set the quota",
    "start": "331460",
    "end": "336470"
  },
  {
    "text": "too high that may result in a very low utilization right so these are the challenges we have to resolve and again",
    "start": "336470",
    "end": "343850"
  },
  {
    "text": "so these are the common challenges we've seen in our own customers but they could be seen in other places as well",
    "start": "343850",
    "end": "351250"
  },
  {
    "start": "351000",
    "end": "429000"
  },
  {
    "text": "alright so in the second half of the talk I'll cross over some of these",
    "start": "351250",
    "end": "356480"
  },
  {
    "text": "challenges and present our solutions and the good news is so some of the",
    "start": "356480",
    "end": "362690"
  },
  {
    "text": "solutions are open sourced so contributions are welcome so the first",
    "start": "362690",
    "end": "367700"
  },
  {
    "text": "thing we have to do is to get kubernetes deployed even without internet inaccessibility right so when looking at",
    "start": "367700",
    "end": "375230"
  },
  {
    "text": "the title people may think of ok so this is probably due to like the firewalls",
    "start": "375230",
    "end": "380300"
  },
  {
    "text": "right but in fact surprisingly it's a quite frequent requirement in some",
    "start": "380300",
    "end": "385880"
  },
  {
    "text": "traditional industries or some like state-owned companies like in energy or",
    "start": "385880",
    "end": "391250"
  },
  {
    "text": "in banking or finance where all the communications have to happen within",
    "start": "391250",
    "end": "396260"
  },
  {
    "text": "their own private corporate network so to compliment the upstream deployment",
    "start": "396260",
    "end": "403210"
  },
  {
    "text": "instructions we devised a ansible based deployment",
    "start": "403210",
    "end": "408280"
  },
  {
    "text": "mechanism so the gist is basically to pre-download",
    "start": "408280",
    "end": "413620"
  },
  {
    "text": "kubernetes packages Python packages and a bunch of young sources files and we",
    "start": "413620",
    "end": "420340"
  },
  {
    "text": "also need to localize our docker registry by populating it with a bunch",
    "start": "420340",
    "end": "425800"
  },
  {
    "text": "of pre saved docker images so speaking of deployment from our own",
    "start": "425800",
    "end": "432760"
  },
  {
    "start": "429000",
    "end": "487000"
  },
  {
    "text": "experience deploying kubernetes in our or in on-premise or bare Mendel environments",
    "start": "432760",
    "end": "440670"
  },
  {
    "text": "accounts for about 70% of all the cases and a the other 30% of",
    "start": "440670",
    "end": "447910"
  },
  {
    "text": "customers deploy kubernetes on public cloud but in China pretty much all the 30% is Alibaba cloud right so in fact",
    "start": "447910",
    "end": "457240"
  },
  {
    "text": "Alibaba cloud is already among the top three largest cloud providers or the public clouds in the world and and also",
    "start": "457240",
    "end": "464470"
  },
  {
    "text": "it has the fastest growth rate so which means we have to integrate kubernetes",
    "start": "464470",
    "end": "470680"
  },
  {
    "text": "with a bunch of like components like the computing service the load balancer and",
    "start": "470680",
    "end": "476590"
  },
  {
    "text": "also the object storage service right and we also put together a pull request with our work in the PR link here",
    "start": "476590",
    "end": "486840"
  },
  {
    "text": "so once we have a running korver Nettie's the next thing we have to do is",
    "start": "486840",
    "end": "492370"
  },
  {
    "start": "487000",
    "end": "521000"
  },
  {
    "text": "to migrate existing applications into the new platform and one of the things",
    "start": "492370",
    "end": "497830"
  },
  {
    "text": "that developers actually are concerned about is how to do logging right so if",
    "start": "497830",
    "end": "503020"
  },
  {
    "text": "we recall this is the the standard way of doing logging so where we recommend",
    "start": "503020",
    "end": "509590"
  },
  {
    "text": "applications to write their locks onto standard out or standard error right instead of writing to file and then",
    "start": "509590",
    "end": "515550"
  },
  {
    "text": "docker or some other container rent runtime will like intercept the messages and a write that into a local JSON file",
    "start": "515550",
    "end": "524830"
  },
  {
    "text": "under a darker directory and are now cubelet will comes into play and set up",
    "start": "524830",
    "end": "531820"
  },
  {
    "text": "a soft link to map that file into a directory that flinty can recognize",
    "start": "531820",
    "end": "537649"
  },
  {
    "text": "right and if Lindy we're now script the files attached some additional metadata and a",
    "start": "537649",
    "end": "544640"
  },
  {
    "text": "sent the file into the elasticsearch store that comes with Corbin at ease",
    "start": "544640",
    "end": "549709"
  },
  {
    "text": "right so this is the standard model and uh it should work in most cases but",
    "start": "549709",
    "end": "554899"
  },
  {
    "text": "however we found there are some limitations where this model doesn't work as well as expected so the first",
    "start": "554899",
    "end": "562160"
  },
  {
    "text": "thing is in some traditional industries people or legacy applications still",
    "start": "562160",
    "end": "568519"
  },
  {
    "text": "write to files right so if we wanted to require to use that model it means",
    "start": "568519",
    "end": "573589"
  },
  {
    "text": "people have to change their code or applications you order to use kubernetes that may become a big hurdle to some",
    "start": "573589",
    "end": "581540"
  },
  {
    "text": "enterprises and also surprisingly so that's not the only reason so sometimes people have a kind of legit scenario",
    "start": "581540",
    "end": "589820"
  },
  {
    "text": "where they wanted to use different files to actually segregate logs even from the",
    "start": "589820",
    "end": "595250"
  },
  {
    "text": "same application right so if we are using the standard out approach so basically all the messages from the same",
    "start": "595250",
    "end": "602240"
  },
  {
    "text": "application writes to the same JSON file right generated by docker and also we",
    "start": "602240",
    "end": "608660"
  },
  {
    "text": "found some enterprises already have an existing VLK or start storage platform",
    "start": "608660",
    "end": "614510"
  },
  {
    "text": "for the logs so they wanted to reuse their existing platforms instead of using the ëokay stack built into",
    "start": "614510",
    "end": "621230"
  },
  {
    "text": "kubernetes so here's our enhancement so now in order to allow people to still",
    "start": "621230",
    "end": "629600"
  },
  {
    "start": "622000",
    "end": "640000"
  },
  {
    "text": "write files for the larks so we first set up a empty directory or the empty",
    "start": "629600",
    "end": "635000"
  },
  {
    "text": "dirt storage option for kubernetes and amount that into the application pot so from",
    "start": "635000",
    "end": "643310"
  },
  {
    "start": "640000",
    "end": "670000"
  },
  {
    "text": "now the application we will read logs into that empty directory and",
    "start": "643310",
    "end": "648399"
  },
  {
    "text": "so the next step is to modify cubelet to still set up a soft link but this time",
    "start": "648399",
    "end": "655100"
  },
  {
    "text": "is not mapping from the Dockers JSON file but it is mapping from the empty",
    "start": "655100",
    "end": "661519"
  },
  {
    "text": "directory generated by kubernetes and again map that into the directory that",
    "start": "661519",
    "end": "667399"
  },
  {
    "text": "Flint it recognizes right and now Soph Lindy you can do a similar thing but",
    "start": "667399",
    "end": "674120"
  },
  {
    "text": "it'll also attach a metadata about the file name from which the logs are",
    "start": "674120",
    "end": "680530"
  },
  {
    "text": "collected and finally when sending the logs we also twig off Lindy so that we",
    "start": "680530",
    "end": "687830"
  },
  {
    "text": "can point fool indeed to send the logs to anywhere beyond just the building in",
    "start": "687830",
    "end": "693400"
  },
  {
    "text": "elasticsearch platform so this is our enhancement so another possible option",
    "start": "693400",
    "end": "698570"
  },
  {
    "text": "would be to use the set car pattern where we have a set car container for",
    "start": "698570",
    "end": "704570"
  },
  {
    "text": "each application pod and that set car will collect the logs and a pipe that logs to any existing like a login",
    "start": "704570",
    "end": "712910"
  },
  {
    "text": "platform but after experiments we found that approach actually requires more resource because you have to deploy a",
    "start": "712910",
    "end": "720500"
  },
  {
    "text": "sidecar for each of the application container in your system",
    "start": "720500",
    "end": "725560"
  },
  {
    "start": "725000",
    "end": "839000"
  },
  {
    "text": "so after logging like kind of a twin problem is monitoring right so again",
    "start": "725560",
    "end": "731510"
  },
  {
    "text": "let's first review the current standard approach for monitoring kubernetes so",
    "start": "731510",
    "end": "736970"
  },
  {
    "text": "essentially we're running a CO divisor agent on each node or each machine and on top of them there's a centralized",
    "start": "736970",
    "end": "744290"
  },
  {
    "text": "hipster that collects and aggregates all the metrics from Co divisor",
    "start": "744290",
    "end": "749680"
  },
  {
    "text": "persist those data in in flux DB and finally graph Ana will visualize and",
    "start": "749680",
    "end": "756170"
  },
  {
    "text": "present all those metrics in very nice format so again so this works pretty",
    "start": "756170",
    "end": "761990"
  },
  {
    "text": "well in most cases but in a large-scale production environment so it represents",
    "start": "761990",
    "end": "767720"
  },
  {
    "text": "it incurred some challenges for example the hipster or the influx DB themselves",
    "start": "767720",
    "end": "773650"
  },
  {
    "text": "our single points of failure which also incur scaling issues and also if people",
    "start": "773650",
    "end": "780200"
  },
  {
    "text": "are familiar with the internal cluster management in Google which we call which we call Borg so there is also a Borgman",
    "start": "780200",
    "end": "787790"
  },
  {
    "text": "a system that monitors the Borg system itself so if we look at kubernetes even",
    "start": "787790",
    "end": "793730"
  },
  {
    "text": "though this architecture monitors the applications running on top of kubernetes but nothing monitors",
    "start": "793730",
    "end": "800030"
  },
  {
    "text": "kubernetes itself right so because kubernetes itself is also a complex software system with like cube dns",
    "start": "800030",
    "end": "807170"
  },
  {
    "text": "cubelet and all the networking setups so there is no such Koopman to monitor the",
    "start": "807170",
    "end": "812899"
  },
  {
    "text": "system components and finally there is no alerting so operators in our case have complained",
    "start": "812899",
    "end": "820550"
  },
  {
    "text": "about so they said we cannot stare at our like refine a dashboard 24/7 in order to detect any anomaly right so we",
    "start": "820550",
    "end": "827510"
  },
  {
    "text": "need to have a automatic and intelligent or alerting module to send any like",
    "start": "827510",
    "end": "835240"
  },
  {
    "text": "learning emails or short messages so this is our enhancement and this is",
    "start": "835240",
    "end": "842870"
  },
  {
    "text": "the overall picture so you can immediately noticed we have replaced two",
    "start": "842870",
    "end": "848660"
  },
  {
    "text": "components so the first one is for the storage we used open TS DB instead of in",
    "start": "848660",
    "end": "855290"
  },
  {
    "text": "flux TB and also we used Prometheus instead of hipster for scraping the",
    "start": "855290",
    "end": "861890"
  },
  {
    "text": "metrics and these are the rationales why we made the",
    "start": "861890",
    "end": "867140"
  },
  {
    "text": "change oh and also I forgot to mention so there is also a alert manager and that aggregates the metrics and also",
    "start": "867140",
    "end": "875839"
  },
  {
    "text": "performs like alerting based on the user's rules so now these are the",
    "start": "875839",
    "end": "882320"
  },
  {
    "start": "880000",
    "end": "936000"
  },
  {
    "text": "rationales of why we did these two replacements so the first one the reason",
    "start": "882320",
    "end": "889040"
  },
  {
    "text": "we're using prometheus is because it supports really powerful querying language",
    "start": "889040",
    "end": "895360"
  },
  {
    "text": "so it's very nice you can very easily craft your own query to see how the",
    "start": "895360",
    "end": "902060"
  },
  {
    "text": "system is behaving and also we are using open TS DB because it is based on HBase",
    "start": "902060",
    "end": "908390"
  },
  {
    "text": "so it has much better scalability in nature and it can be horizontally scaled",
    "start": "908390",
    "end": "914529"
  },
  {
    "text": "but also there are like challenges coming along if you want to use these two approaches for example to use open",
    "start": "914529",
    "end": "922279"
  },
  {
    "text": "TS DB so you have to deal with how to run it on top of Corbin at ease and also",
    "start": "922279",
    "end": "929300"
  },
  {
    "text": "you have to deal with some very demo details like solving the reverse IP lookup problem alright",
    "start": "929300",
    "end": "936529"
  },
  {
    "start": "936000",
    "end": "1083000"
  },
  {
    "text": "so after logging and monitoring so what is the next tricky business so to us it",
    "start": "936529",
    "end": "942809"
  },
  {
    "text": "is achieving external load balancer right so recall that in our cases about 70 70",
    "start": "942809",
    "end": "950519"
  },
  {
    "text": "percent of all the deployments happen on premise or on bearman dough so how in",
    "start": "950519",
    "end": "956339"
  },
  {
    "text": "these cases how can you expose external services without a cloud provider right",
    "start": "956339",
    "end": "962339"
  },
  {
    "text": "so if you're if you're using like AWS or PCP or ager so you can use their like",
    "start": "962339",
    "end": "968309"
  },
  {
    "text": "public load balancers to balance traffic from the outside into your internal",
    "start": "968309",
    "end": "974459"
  },
  {
    "text": "services right but it's a luxury for on-premise or bare metal deployments so",
    "start": "974459",
    "end": "980429"
  },
  {
    "text": "let's see what are the options currently in upstream so the first one and probably the easiest one is to use a",
    "start": "980429",
    "end": "987360"
  },
  {
    "text": "note port right so just a first a quick recap so essentially if you want to",
    "start": "987360",
    "end": "993389"
  },
  {
    "text": "expose a service to the external world so you can use a open a node part which",
    "start": "993389",
    "end": "999029"
  },
  {
    "text": "opens a high port on every machine right so in this case it's it's 31 0 80 right",
    "start": "999029",
    "end": "1008720"
  },
  {
    "text": "so it's open on every machine in the cluster and under under the hood so",
    "start": "1008720",
    "end": "1014569"
  },
  {
    "text": "essentially kubernetes will set up a bunch of IP table rules to actually redirect the traffic's so if",
    "start": "1014569",
    "end": "1022879"
  },
  {
    "text": "any traffic comes to that port so these rules will come into play and randomly",
    "start": "1022879",
    "end": "1029510"
  },
  {
    "text": "redirect the traffic's to the multiple back-end instances behind that service",
    "start": "1029510",
    "end": "1035178"
  },
  {
    "text": "so this is the North node port approach so we can think about so what are like",
    "start": "1035179",
    "end": "1041178"
  },
  {
    "text": "the limitations for this approach right so any guest so the first one is all the",
    "start": "1041179",
    "end": "1049669"
  },
  {
    "text": "outbound traffic or the or the external traffic has to target at a single",
    "start": "1049669",
    "end": "1055549"
  },
  {
    "text": "machine and the destination address is like IP Colin port right so it still",
    "start": "1055549",
    "end": "1062480"
  },
  {
    "text": "represents a single point of failure and also incurs scaling issues and",
    "start": "1062480",
    "end": "1067920"
  },
  {
    "text": "also because the load balancing is done via IP tables so it only supports level",
    "start": "1067920",
    "end": "1073140"
  },
  {
    "text": "for load balancing so you cannot do like session affinity or like a load",
    "start": "1073140",
    "end": "1079170"
  },
  {
    "text": "balancing based on like like client regions etc right so just a quick summary in ops",
    "start": "1079170",
    "end": "1087030"
  },
  {
    "text": "trip so currently we can do external load balancers through like cloud load",
    "start": "1087030",
    "end": "1092370"
  },
  {
    "text": "balancer providers node port or ingress so ingress is a new concept right so in",
    "start": "1092370",
    "end": "1098870"
  },
  {
    "text": "in kubernetes 1.4 but again they all have some limitations",
    "start": "1098870",
    "end": "1104400"
  },
  {
    "text": "like I already mentioned so public cloud providers are not always available note",
    "start": "1104400",
    "end": "1109590"
  },
  {
    "text": "party is kind of hacky and uh and not highly scalable and an ingress so which",
    "start": "1109590",
    "end": "1115170"
  },
  {
    "text": "is the new cool concept it's nice but its current implementation in upstream",
    "start": "1115170",
    "end": "1120690"
  },
  {
    "text": "only supports using GCP to perform the load balancers right so what if we",
    "start": "1120690",
    "end": "1127800"
  },
  {
    "text": "wanted to do a on-premise or a bare-metal deployment so here is our solution right so we call it hi",
    "start": "1127800",
    "end": "1135330"
  },
  {
    "start": "1132000",
    "end": "1476000"
  },
  {
    "text": "available ingress so in order to gather so let's first see how the default",
    "start": "1135330",
    "end": "1140850"
  },
  {
    "text": "ingress works so essentially if you want to use a ingress so it'll deploy a",
    "start": "1140850",
    "end": "1146730"
  },
  {
    "text": "ingress pod in a one of the machines in your cluster and this ingress pod will",
    "start": "1146730",
    "end": "1152520"
  },
  {
    "text": "do basically two things so the first one is to set up a cloud load balancer which",
    "start": "1152520",
    "end": "1158940"
  },
  {
    "text": "could be on TCP and also it'll it'll automatically generate forwarding rules",
    "start": "1158940",
    "end": "1164700"
  },
  {
    "text": "based on the user's the user's preference and after that so",
    "start": "1164700",
    "end": "1170910"
  },
  {
    "text": "all the incoming traffic will hit the GCP load balancer first and then according to the rules it will be",
    "start": "1170910",
    "end": "1177600"
  },
  {
    "text": "redirected to any back-end instance so this is how the upstream ingress works but we can see so one of the the other",
    "start": "1177600",
    "end": "1185430"
  },
  {
    "text": "limitations for that is there is only one ingress part running in the system",
    "start": "1185430",
    "end": "1190650"
  },
  {
    "text": "right so again not highly available not really scaling so that's why we need to",
    "start": "1190650",
    "end": "1195720"
  },
  {
    "text": "have a height of highly available ingress so from this example picture we",
    "start": "1195720",
    "end": "1201090"
  },
  {
    "text": "can see that so first we can deploy more than one ingress pod in multiple",
    "start": "1201090",
    "end": "1207760"
  },
  {
    "text": "machines right so probably the fun is too small so it's not really readable",
    "start": "1207760",
    "end": "1214290"
  },
  {
    "text": "but we can just imagine so this one is the ingress controller right so we have",
    "start": "1214290",
    "end": "1221320"
  },
  {
    "text": "two of them all right and so now it has a new problem so now we have two ingress",
    "start": "1221320",
    "end": "1227830"
  },
  {
    "text": "controllers before for all the incoming traffic it can just directly hit into the only one ingress controller but now",
    "start": "1227830",
    "end": "1235120"
  },
  {
    "text": "we have two of them so how can we do the load balancing for the ingress controllers themselves so our approach",
    "start": "1235120",
    "end": "1242050"
  },
  {
    "text": "is to basically use keepalive D so that keep alive D will maintain a",
    "start": "1242050",
    "end": "1248020"
  },
  {
    "text": "stable virtual IP and this virtual IP will be assigned on multiple machines",
    "start": "1248020",
    "end": "1253650"
  },
  {
    "text": "which run the ingress controller and for",
    "start": "1253650",
    "end": "1259330"
  },
  {
    "text": "all the incoming traffic so keep alive D will guarantee that there is only one",
    "start": "1259330",
    "end": "1265630"
  },
  {
    "text": "active instance one active ingress controller at any time and all the",
    "start": "1265630",
    "end": "1272440"
  },
  {
    "text": "incoming traffic will be routed to that active ingress controller and within",
    "start": "1272440",
    "end": "1277780"
  },
  {
    "text": "this ingress pod so there's also a H a proxy or engine njx running in it so",
    "start": "1277780",
    "end": "1285130"
  },
  {
    "text": "that we can configure level-7 forwarding rules to support like session affinity",
    "start": "1285130",
    "end": "1291130"
  },
  {
    "text": "and more complicated load balancing rules right and at this H a proxy or ngx",
    "start": "1291130",
    "end": "1297040"
  },
  {
    "text": "will be in charge of further directing the traffic to a back-end pod for that",
    "start": "1297040",
    "end": "1302080"
  },
  {
    "text": "service so this is a basic model for for the high available ingress right so yeah",
    "start": "1302080",
    "end": "1310570"
  },
  {
    "text": "things are looking better so then the then the single single ingress controller case but there are like still",
    "start": "1310570",
    "end": "1318190"
  },
  {
    "text": "remaining problem with this approach so if we want to make a guess so we can see",
    "start": "1318190",
    "end": "1324520"
  },
  {
    "text": "that all right so before I get to that so let's first see how this hi available",
    "start": "1324520",
    "end": "1331450"
  },
  {
    "text": "approach will handle failures so again back to the picture we have to ingress controllers or to ingress pots running",
    "start": "1331450",
    "end": "1338790"
  },
  {
    "text": "on the two machines on the right side so assuming now like the the ingress",
    "start": "1338790",
    "end": "1346200"
  },
  {
    "text": "controller on the right most failed and then so migration will happen so first",
    "start": "1346200",
    "end": "1352250"
  },
  {
    "text": "the application containers running on the failure machine will be migrated to",
    "start": "1352250",
    "end": "1357900"
  },
  {
    "text": "a healthy machine on the leftmost site and also the ingress controller pod will",
    "start": "1357900",
    "end": "1363450"
  },
  {
    "text": "also be migrated as the the arrow denotes to the healthy machine on the",
    "start": "1363450",
    "end": "1369720"
  },
  {
    "text": "the leftmost side all right so now keep alive D you will come into play and it",
    "start": "1369720",
    "end": "1375660"
  },
  {
    "text": "correctly also maintain the mapping from the stable virtual IP to the new machine",
    "start": "1375660",
    "end": "1382650"
  },
  {
    "text": "so that all the further traffic will come into the new healthy machine instead of the failed machine right so",
    "start": "1382650",
    "end": "1390030"
  },
  {
    "text": "this is how the high high ability the high availability helps in this case all",
    "start": "1390030",
    "end": "1396060"
  },
  {
    "text": "right so now let's think about the problems with this approach so the first",
    "start": "1396060",
    "end": "1401760"
  },
  {
    "text": "one is we may encounter what we call a reload storm because remember all the",
    "start": "1401760",
    "end": "1408660"
  },
  {
    "text": "forwarding rules are now being taken care of by the H a proxy or the ng X so",
    "start": "1408660",
    "end": "1415710"
  },
  {
    "text": "if any of the positive with the sub behind the service failed and there is a",
    "start": "1415710",
    "end": "1420990"
  },
  {
    "text": "migration so we have to update so by we I mean the ingress controller pod so",
    "start": "1420990",
    "end": "1426870"
  },
  {
    "text": "it's automatic so it's better than manual but we still have to update like the config files for ng X or H a proxy",
    "start": "1426870",
    "end": "1434520"
  },
  {
    "text": "right so this is for one service but imagine if we have like hundreds or thousands of services and each with like",
    "start": "1434520",
    "end": "1441750"
  },
  {
    "text": "tens of pots so the problem the probability for this update is very high and we have to reload the config for the",
    "start": "1441750",
    "end": "1449670"
  },
  {
    "text": "H a proxy or ng x pretty often right so this is what we call the reload storm",
    "start": "1449670",
    "end": "1455210"
  },
  {
    "text": "and also there is the scaling issue as well because even",
    "start": "1455210",
    "end": "1462180"
  },
  {
    "text": "though we have multiple instances of the the ingress pots but they still have to",
    "start": "1462180",
    "end": "1467770"
  },
  {
    "text": "carrot of the forewarning and a load balancing for all the hundreds or thousands of services right so it can",
    "start": "1467770",
    "end": "1474640"
  },
  {
    "text": "still represent a scaling issue and to solve this problem we actually",
    "start": "1474640",
    "end": "1479770"
  },
  {
    "start": "1476000",
    "end": "1538000"
  },
  {
    "text": "introduced the scheduling like it's scheduling of jobs that a concept into",
    "start": "1479770",
    "end": "1486010"
  },
  {
    "text": "here alright so here is the gist so we essentially perform a sharding so",
    "start": "1486010",
    "end": "1492460"
  },
  {
    "text": "suppose we have multiple ingress controllers instead of letting all of them to handle all the other services we",
    "start": "1492460",
    "end": "1500740"
  },
  {
    "text": "perform a sharding so that some of the ingress controllers are only taking card",
    "start": "1500740",
    "end": "1505780"
  },
  {
    "text": "of a subset of the services and they will only perform load balancing as well",
    "start": "1505780",
    "end": "1511750"
  },
  {
    "text": "as like forwarding for those subset of services right so now so it basically",
    "start": "1511750",
    "end": "1518050"
  },
  {
    "text": "resembles a scheduling scheduling scenario where we have like a bunch of",
    "start": "1518050",
    "end": "1523810"
  },
  {
    "text": "services and we have another bunch of ingress controllers so we have to assign which ingress controllers take care of",
    "start": "1523810",
    "end": "1530830"
  },
  {
    "text": "which subset of services right so it's a scheduling problem so to solve that",
    "start": "1530830",
    "end": "1536200"
  },
  {
    "text": "problem we introduce a new concept called the ingress claim right so a",
    "start": "1536200",
    "end": "1541990"
  },
  {
    "start": "1538000",
    "end": "1575000"
  },
  {
    "text": "ingress claim if you recall the PVC the persistent vol claim is pretty similar",
    "start": "1541990",
    "end": "1548320"
  },
  {
    "text": "so you define a class of forwarding like service",
    "start": "1548320",
    "end": "1554490"
  },
  {
    "text": "upfront so again so the fun is too small to read so",
    "start": "1554490",
    "end": "1559780"
  },
  {
    "text": "but basically these two black boxes define two ingress claims so each with",
    "start": "1559780",
    "end": "1565330"
  },
  {
    "text": "different like performance guarantees like what is a delay what is what is a",
    "start": "1565330",
    "end": "1570610"
  },
  {
    "text": "timeout and what is a like the bandwidth for this ingress claim and now let's see",
    "start": "1570610",
    "end": "1576700"
  },
  {
    "start": "1575000",
    "end": "1651000"
  },
  {
    "text": "how things work so first we add an EO controller into",
    "start": "1576700",
    "end": "1582520"
  },
  {
    "text": "the master component and we call it the ingress claim controller so what it does is whenever there is a request for a new",
    "start": "1582520",
    "end": "1590020"
  },
  {
    "text": "service to be exposed externally so it submits a request to the master saying",
    "start": "1590020",
    "end": "1595840"
  },
  {
    "text": "that hey I want it to be exposed externally so please assign me a ingress controller to take care of my",
    "start": "1595840",
    "end": "1603200"
  },
  {
    "text": "Alma forwarding and load balancing and also it requests a certain ingress claim",
    "start": "1603200",
    "end": "1609170"
  },
  {
    "text": "class saying that I wanted to have a very low like a latency to do my",
    "start": "1609170",
    "end": "1615650"
  },
  {
    "text": "load balancing and this ingress claim controller will look at are there any suitable",
    "start": "1615650",
    "end": "1621670"
  },
  {
    "text": "ingress controllers already in the system and if not so it'll create the",
    "start": "1621670",
    "end": "1627770"
  },
  {
    "text": "certain number of ingress controllers for that service right so in this example there are two instances and the",
    "start": "1627770",
    "end": "1635270"
  },
  {
    "text": "number two actually comes from the ingress claim so it's also property for",
    "start": "1635270",
    "end": "1640490"
  },
  {
    "text": "the ingress claim class right because that means how reliable you want it you",
    "start": "1640490",
    "end": "1646250"
  },
  {
    "text": "want your like load balancer to be alright",
    "start": "1646250",
    "end": "1651370"
  },
  {
    "start": "1651000",
    "end": "1729000"
  },
  {
    "text": "so that's pretty much about our high available ingress mechanism and the last",
    "start": "1651370",
    "end": "1659030"
  },
  {
    "text": "tricky business business I wanted to mention is how to achieve high availability masters so you may already",
    "start": "1659030",
    "end": "1665930"
  },
  {
    "text": "noticed it's kind of like an option already in the upstream but the problem",
    "start": "1665930",
    "end": "1671480"
  },
  {
    "text": "again is the upstream solution requires to use like TCP or AWS load balancers in",
    "start": "1671480",
    "end": "1678890"
  },
  {
    "text": "order to achieve this high availability right so but in bare mental or in",
    "start": "1678890",
    "end": "1685370"
  },
  {
    "text": "on-premise cases we basically have to devise our own mechanism to achieve this",
    "start": "1685370",
    "end": "1691430"
  },
  {
    "text": "so the reason is intuitively if we want to have high availability master we have",
    "start": "1691430",
    "end": "1697820"
  },
  {
    "text": "to deploy multiple instances right so as depicted here so we have multiple master",
    "start": "1697820",
    "end": "1704600"
  },
  {
    "text": "instances running in here but again the problem is now when a traffic comes so -",
    "start": "1704600",
    "end": "1711320"
  },
  {
    "text": "which of these master instances should we redirect the traffic again so we need",
    "start": "1711320",
    "end": "1716780"
  },
  {
    "text": "a load balancer and so it's similar story so we use a keepalive D and H here",
    "start": "1716780",
    "end": "1722510"
  },
  {
    "text": "proxy in front of all these master instances to achieve the high available load balancers themselves alright",
    "start": "1722510",
    "end": "1730600"
  },
  {
    "start": "1729000",
    "end": "2225000"
  },
  {
    "text": "alright so in the remainder of the talk so hopefully I'll talk about like something",
    "start": "1730600",
    "end": "1737600"
  },
  {
    "text": "more interesting so for the previous like like the issues we mentioned so",
    "start": "1737600",
    "end": "1743929"
  },
  {
    "text": "there are kind of the enhancements to the core core Benares like components",
    "start": "1743929",
    "end": "1749059"
  },
  {
    "text": "but in order to run kubernetes smoothly in a production system so kubernetes",
    "start": "1749059",
    "end": "1754130"
  },
  {
    "text": "along is not enough right so we have to have a way to do like continuous",
    "start": "1754130",
    "end": "1759260"
  },
  {
    "text": "continuous like integration and a deployment and we also need to have like",
    "start": "1759260",
    "end": "1764779"
  },
  {
    "text": "a registry a docker registry or like a email registry to securely store the",
    "start": "1764779",
    "end": "1772250"
  },
  {
    "text": "images so now let's see how we solve those problems with some open source efforts so the first one I wanted to",
    "start": "1772250",
    "end": "1779330"
  },
  {
    "text": "introduce is a system called cyclone so this is our open sourced CI CD system",
    "start": "1779330",
    "end": "1787130"
  },
  {
    "text": "and it's more kubernetes native so people today may have been using like a",
    "start": "1787130",
    "end": "1792260"
  },
  {
    "text": "jenkins with like either like plugins or some custom scripts in order to like",
    "start": "1792260",
    "end": "1798950"
  },
  {
    "text": "integrate jenkins into a container platform but cyclone is kubernetes",
    "start": "1798950",
    "end": "1804559"
  },
  {
    "text": "native so it is more lightweight and it is much faster so it basically shapes your code",
    "start": "1804559",
    "end": "1810710"
  },
  {
    "text": "directly from from your source repository to any kubernetes cluster",
    "start": "1810710",
    "end": "1815720"
  },
  {
    "text": "that you specified it also has some very nice properties that jenkins or some",
    "start": "1815720",
    "end": "1822049"
  },
  {
    "text": "other CICE tools do not have for example so we know the modern world is is all",
    "start": "1822049",
    "end": "1829610"
  },
  {
    "text": "with micro services right but another challenge with micro services is now it",
    "start": "1829610",
    "end": "1836059"
  },
  {
    "text": "has too many components so when you're doing a push or you're doing a release so there are like multiple components",
    "start": "1836059",
    "end": "1843620"
  },
  {
    "text": "that you may have to update at the same time and they may have like interdependencies right so if you change",
    "start": "1843620",
    "end": "1849950"
  },
  {
    "text": "the API of a back-end you probably want to update that first and then update your front-end so your order to better",
    "start": "1849950",
    "end": "1857480"
  },
  {
    "text": "support that so cyclone is a dependency aware so it supports a API and in the",
    "start": "1857480",
    "end": "1865250"
  },
  {
    "text": "near future a UI that that's a user to use a drag-and-drop approach to first",
    "start": "1865250",
    "end": "1871260"
  },
  {
    "text": "set up a dependency graph like shown here in the picture and so then when you",
    "start": "1871260",
    "end": "1878070"
  },
  {
    "text": "are doing a release so it will follow the dependency order and it will start from the root so that is dependency",
    "start": "1878070",
    "end": "1885870"
  },
  {
    "text": "awareness and also we know that about 60% of the doctor images have some like",
    "start": "1885870",
    "end": "1891870"
  },
  {
    "text": "suspicious portion it in them so security for the doctor images is also",
    "start": "1891870",
    "end": "1897720"
  },
  {
    "text": "very important so we integrate in cyclone with a clear to basically scan",
    "start": "1897720",
    "end": "1904980"
  },
  {
    "text": "each layer of the of the docker images against a database of known like",
    "start": "1904980",
    "end": "1911400"
  },
  {
    "text": "vulnerability signatures in order to detect any any like suspicious code in the in the",
    "start": "1911400",
    "end": "1918120"
  },
  {
    "text": "base image or in any layer of the image so this is a like a sample output from",
    "start": "1918120",
    "end": "1923520"
  },
  {
    "text": "cyclone all right so this one is how it",
    "start": "1923520",
    "end": "1928740"
  },
  {
    "text": "is designed but I'm not going into the details so you can visit our like a github page",
    "start": "1928740",
    "end": "1936240"
  },
  {
    "text": "for more details so I also mentioned we also need to have a private registry to",
    "start": "1936240",
    "end": "1941940"
  },
  {
    "text": "store the docker images and the options out there are probably like either docker hub or like the standard docker",
    "start": "1941940",
    "end": "1950340"
  },
  {
    "text": "registry distribution project right so those are open source as well but we are using a",
    "start": "1950340",
    "end": "1957809"
  },
  {
    "text": "open source project called harbor so harbor was initiated by vmware and we",
    "start": "1957809",
    "end": "1964950"
  },
  {
    "text": "are one of the facial partners on that project so this link here is our fork but from there you can look at the app",
    "start": "1964950",
    "end": "1972299"
  },
  {
    "text": "stream the VMware version so the reason we are using like a fork here is because",
    "start": "1972299",
    "end": "1978240"
  },
  {
    "text": "so in our version so we have a more concise user experience or UI so we're",
    "start": "1978240",
    "end": "1984720"
  },
  {
    "text": "on a single page you can see various image details about like a different images like where are they deployed when",
    "start": "1984720",
    "end": "1992130"
  },
  {
    "text": "are they like updated and who did which updates etc and also another novel",
    "start": "1992130",
    "end": "1999510"
  },
  {
    "text": "feature we introduced here is labeling so if we think of the canonical dr.",
    "start": "1999510",
    "end": "2004760"
  },
  {
    "text": "image URL so it's basically just three layers right so you have the registry location slash the project name or the",
    "start": "2004760",
    "end": "2012980"
  },
  {
    "text": "name space right slash basically the repo name of course there is the label",
    "start": "2012980",
    "end": "2018770"
  },
  {
    "text": "in the end but so in our cases so for some of the large enterprise users they",
    "start": "2018770",
    "end": "2026120"
  },
  {
    "text": "wanted to have a more flexible like labeling mechanism to really enable them",
    "start": "2026120",
    "end": "2032809"
  },
  {
    "text": "to classify and imagine managed images more flexibly for example they want to",
    "start": "2032809",
    "end": "2039020"
  },
  {
    "text": "classify images per their customer so they may in a like ISV industry right or",
    "start": "2039020",
    "end": "2044990"
  },
  {
    "text": "they may store and I classify images by types or by regions right so if you",
    "start": "2044990",
    "end": "2050480"
  },
  {
    "text": "think of just the connect canonical talked about the image names so how can you like express this information so of",
    "start": "2050480",
    "end": "2059300"
  },
  {
    "text": "course you can put everything into the tag right in the very end or put that into the image names but these are like",
    "start": "2059300",
    "end": "2066200"
  },
  {
    "text": "a key so we basically borrowed the concept of kubernetes right so if you think of how kubernetes manage its",
    "start": "2066200",
    "end": "2072648"
  },
  {
    "text": "resources so it is using like two labels right so we're doing the same thing so you can tag any doctor image with any",
    "start": "2072649",
    "end": "2080240"
  },
  {
    "text": "number of labels and then you can use the api's to query those labels in order",
    "start": "2080240",
    "end": "2086510"
  },
  {
    "text": "to perform like management and classification right and beside these features of",
    "start": "2086510",
    "end": "2092480"
  },
  {
    "text": "course it is highly available compared to the like the docker distribution or the docker registry and also it has a",
    "start": "2092480",
    "end": "2099560"
  },
  {
    "text": "nice feature of automatically syncing images between multiple instances of this Harbor as of",
    "start": "2099560",
    "end": "2106970"
  },
  {
    "text": "Harbor registry and what is even better is we also enable on a one-click",
    "start": "2106970",
    "end": "2112970"
  },
  {
    "text": "deployment of hover on kubernetes so that we can make hover even more highly",
    "start": "2112970",
    "end": "2118130"
  },
  {
    "text": "available so this is an example like user interface for harbor alright so the last",
    "start": "2118130",
    "end": "2125330"
  },
  {
    "text": "point I wanted to mention although I'm running out of time is a big data support because it's pre is a pretty",
    "start": "2125330",
    "end": "2132470"
  },
  {
    "text": "often Airy requirement to run some big data applications on kubernetes as well",
    "start": "2132470",
    "end": "2137839"
  },
  {
    "text": "although it is not very well officially supported in the upstream right so",
    "start": "2137839",
    "end": "2143180"
  },
  {
    "text": "people maybe are still running more like stateless application on kubernetes but we wanted to beat data so besides like",
    "start": "2143180",
    "end": "2150829"
  },
  {
    "text": "Hadoop and spark so here with maybe one or two minutes I wanted to highlight our work for running tensorflow on",
    "start": "2150829",
    "end": "2157940"
  },
  {
    "text": "kubernetes so for those of you who are not familiar with tensorflow so it's a",
    "start": "2157940",
    "end": "2163130"
  },
  {
    "text": "deep learning tool also open sourced by Google which uses neutral networks to",
    "start": "2163130",
    "end": "2169609"
  },
  {
    "text": "perform some more efficient learning on non structured data right like pictures or like audio files or etc right so",
    "start": "2169609",
    "end": "2179359"
  },
  {
    "text": "essentially the architecture for tensorflow is a is at the server and a worker model so where the server",
    "start": "2179359",
    "end": "2185510"
  },
  {
    "text": "essentially stores all the parameters for the training model and the worker is",
    "start": "2185510",
    "end": "2190549"
  },
  {
    "text": "the one that really performs the prediction and a classification and the",
    "start": "2190549",
    "end": "2195770"
  },
  {
    "text": "the actual job so the order to run tensorflow on kubernetes the gist is to",
    "start": "2195770",
    "end": "2201440"
  },
  {
    "text": "use the job concept in kubernetes so that you first start the tensorflow",
    "start": "2201440",
    "end": "2207980"
  },
  {
    "text": "server and whenever it needs to start a worker so it basically creates a",
    "start": "2207980",
    "end": "2213230"
  },
  {
    "text": "kubernetes job to do that and also to achieve more reliable",
    "start": "2213230",
    "end": "2218470"
  },
  {
    "text": "like effect so we can set we start on failure policy for that job to finish",
    "start": "2218470",
    "end": "2226059"
  },
  {
    "start": "2225000",
    "end": "2384000"
  },
  {
    "text": "so to summarize so our takeaway for how",
    "start": "2226059",
    "end": "2231230"
  },
  {
    "text": "to increase the kubernetes user base in China or maybe in a global scale is",
    "start": "2231230",
    "end": "2236900"
  },
  {
    "text": "basically threefold so the first one is either deployment is not only about having like a one-click deployment",
    "start": "2236900",
    "end": "2244660"
  },
  {
    "text": "solution but it's also more about being able to deployment kubernetes in a more like diverse",
    "start": "2244660",
    "end": "2252200"
  },
  {
    "text": "environments either with or without internet access either with or without AWS or OpenStack so this is number one",
    "start": "2252200",
    "end": "2260359"
  },
  {
    "text": "so number two is in order to use kubernetes in a large especially production systems we have to",
    "start": "2260359",
    "end": "2267410"
  },
  {
    "text": "do more like performance tuning and optimizations and finally that's the",
    "start": "2267410",
    "end": "2272930"
  },
  {
    "text": "case for any active projects so we always require like better documentation and a training as well sorry and",
    "start": "2272930",
    "end": "2279350"
  },
  {
    "text": "probably for like none english-speaking countries so any internet internationalization is also preferred",
    "start": "2279350",
    "end": "2286640"
  },
  {
    "text": "so that's pretty much conclude my talk and finally I also",
    "start": "2286640",
    "end": "2292730"
  },
  {
    "text": "wanted to thank two of our team members who are also kubernetes contributors they're also here like",
    "start": "2292730",
    "end": "2300530"
  },
  {
    "text": "durian and mean ciao so they're also helping a lot with all these work and",
    "start": "2300530",
    "end": "2306410"
  },
  {
    "text": "are preparing this slides alright so it seems we only have one minute to go so I would like to take any questions",
    "start": "2306410",
    "end": "2315010"
  },
  {
    "text": "yes they they do because like historically like Hadoop and a spark",
    "start": "2325480",
    "end": "2332030"
  },
  {
    "text": "these are more like a mature and more like a well understand tools through the",
    "start": "2332030",
    "end": "2337100"
  },
  {
    "text": "customers yes so they're using Hadoop and a spark actually more fruit complete than tensorflow",
    "start": "2337100",
    "end": "2344050"
  },
  {
    "text": "yeah inside kubernetes customers clusters yeah",
    "start": "2344050",
    "end": "2349900"
  },
  {
    "text": "so data storage so these are different options so 30% of the customers who are",
    "start": "2350170",
    "end": "2355700"
  },
  {
    "text": "using like Alibaba cloud so they basically use a like the object storage service it's a public cloud but for our",
    "start": "2355700",
    "end": "2363020"
  },
  {
    "text": "like on-premise storage we basically use SEF in order to do that",
    "start": "2363020",
    "end": "2368770"
  },
  {
    "text": "all right so if you have further questions so I'll be around to chat",
    "start": "2374080",
    "end": "2379760"
  },
  {
    "text": "thank you very much [Applause]",
    "start": "2379760",
    "end": "2386329"
  }
]