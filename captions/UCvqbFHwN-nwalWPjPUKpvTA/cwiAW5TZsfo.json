[
  {
    "text": "hi everyone um so my name is Abdullah um I work for Google and I'm going to present uh our work on jobset",
    "start": "160",
    "end": "7960"
  },
  {
    "text": "API um specifically uh for on demand systems um HBC systems and scale",
    "start": "7960",
    "end": "14120"
  },
  {
    "text": "training this is a presentation that I prepared with Vanessa's um soad from uh",
    "start": "14120",
    "end": "20080"
  },
  {
    "text": "Lawrence Livermore National Laboratory unfortunately she was not able to attend today um but she has a recording so this",
    "start": "20080",
    "end": "27439"
  },
  {
    "text": "presentation is going to be um two parts I'm going to present the first part and I will play a recording for the for the",
    "start": "27439",
    "end": "33399"
  },
  {
    "text": "second part um so for a while we've been talking about kubernetes as not being",
    "start": "33399",
    "end": "41440"
  },
  {
    "text": "too advocate for batch workloads but I'm going to start by saying kubernetes is aducate for batch workloads we've been",
    "start": "41440",
    "end": "47320"
  },
  {
    "text": "working for a while on improving uh kubernetes um to support batch workloads",
    "start": "47320",
    "end": "52520"
  },
  {
    "text": "uh as you've seen the past uh few kubernetes a few C cubec con",
    "start": "52520",
    "end": "57640"
  },
  {
    "text": "talks um mainly through through the job API um so we've been working on",
    "start": "57640",
    "end": "64080"
  },
  {
    "text": "improving it to support uh various cases of patch workloads including training",
    "start": "64080",
    "end": "69200"
  },
  {
    "text": "and HBC and whatnot um the main feature that we've added in the job API was",
    "start": "69200",
    "end": "75040"
  },
  {
    "text": "index job um and in addition to that we have new features related to how you",
    "start": "75040",
    "end": "81240"
  },
  {
    "text": "decide when um like P failure policies um also the ability to set up um a",
    "start": "81240",
    "end": "87400"
  },
  {
    "text": "stable Network um IDs um um for pods that are created through index job which",
    "start": "87400",
    "end": "93479"
  },
  {
    "text": "all are requirements um if you've ever worked with MPI or um you know",
    "start": "93479",
    "end": "98960"
  },
  {
    "text": "distributed um training even though that I claim that U",
    "start": "98960",
    "end": "104719"
  },
  {
    "text": "kubernetes now is a home for batch I think we can still do better um so there's",
    "start": "104719",
    "end": "110439"
  },
  {
    "text": "always there's always a bot um and so we been working on a new API in a sub",
    "start": "110439",
    "end": "118240"
  },
  {
    "text": "project under kubernetes called job set um as I mentioned it's a it's a new API",
    "start": "118240",
    "end": "124799"
  },
  {
    "text": "the idea behind it is that it manages a group of jobs as a single workload so we're not trying to reinvent the wheel",
    "start": "124799",
    "end": "131920"
  },
  {
    "text": "we're trying to basically reuse the core kubernetes uh job API to try to expand",
    "start": "131920",
    "end": "139040"
  },
  {
    "text": "its use cases um as you can see on the graphic on the um uh on the right side",
    "start": "139040",
    "end": "146560"
  },
  {
    "text": "with job set you are able to create or specify multiple job templates and for",
    "start": "146560",
    "end": "152800"
  },
  {
    "text": "each template you can specify how many jobs or replicas of that template I want",
    "start": "152800",
    "end": "158800"
  },
  {
    "text": "and it also allows you to automate um set up like you know success and failure policies for this new uh workload that",
    "start": "158800",
    "end": "166560"
  },
  {
    "text": "is represented as a collection of jobs and so in a in general what it tries to",
    "start": "166560",
    "end": "172120"
  },
  {
    "text": "do is really automate multiple uh patterns that we've discovered in uh",
    "start": "172120",
    "end": "177280"
  },
  {
    "text": "training and HBC type workloads as I mentioned multi-template jobs uh setting",
    "start": "177280",
    "end": "182640"
  },
  {
    "text": "up part-to-part communication um with index job you used to create an index job and then you create a headless",
    "start": "182640",
    "end": "188440"
  },
  {
    "text": "service yourself um and set up everything and then when you clean them up you have to do it manually uh with",
    "start": "188440",
    "end": "194159"
  },
  {
    "text": "jobs I we trying to automate these uh these things as I mentioned also we are uh trying to provide some common failure",
    "start": "194159",
    "end": "201360"
  },
  {
    "text": "and success policies like when do we consider the whole workload as failed um so in some cases for example you have",
    "start": "201360",
    "end": "208000"
  },
  {
    "text": "this like leader follower pattern uh if the one of the followers fails you don't necessarily need to fail the whole",
    "start": "208000",
    "end": "214080"
  },
  {
    "text": "workload but if the leader fails you want to say okay yeah fail the whole workload same thing with success when do",
    "start": "214080",
    "end": "221319"
  },
  {
    "text": "we consider the whole workload as successful if we are managing uh you know a group of uh a group of jobs which",
    "start": "221319",
    "end": "228319"
  },
  {
    "text": "job when it succeeds should we consider the whole job the whole work successful or should we say all the child jobs",
    "start": "228319",
    "end": "235640"
  },
  {
    "text": "should succeed to consider the workload as successful um also it tries to",
    "start": "235640",
    "end": "240959"
  },
  {
    "text": "provide some uh knobs to decide how to place these jobs in some cases with like",
    "start": "240959",
    "end": "247840"
  },
  {
    "text": "distributed training you try to place the jobs on different parts of your um you know infrastructure consider like",
    "start": "247840",
    "end": "254439"
  },
  {
    "text": "for example you have different tracks and so each job represents um you know A",
    "start": "254439",
    "end": "260280"
  },
  {
    "text": "Shard of the of the training workload and you want to have really high bandwidth communication between uh the",
    "start": "260280",
    "end": "267479"
  },
  {
    "text": "pods that are responsible for these chart and so you want to create a job for each track and you want to ensure",
    "start": "267479",
    "end": "272960"
  },
  {
    "text": "that each job lands on a different track um so I'm going to talk about two",
    "start": "272960",
    "end": "279039"
  },
  {
    "text": "use cases that we use jobset for the first one I'm going to talk about is a scale distributor on tpus and uh vessa",
    "start": "279039",
    "end": "285919"
  },
  {
    "text": "is going to talk about the second one which is um the HPC use case um so what",
    "start": "285919",
    "end": "291720"
  },
  {
    "text": "is a TPU this is the the only uh uh pitch for tpus it's it's Google's uh",
    "start": "291720",
    "end": "297479"
  },
  {
    "text": "basically accelerator we designed for um machine specifically for machine",
    "start": "297479",
    "end": "302520"
  },
  {
    "text": "learning um it is it works with like most Frameworks um supporting pyto Jack",
    "start": "302520",
    "end": "309840"
  },
  {
    "text": "sensor flow and integrates S with kubernetes um there are two main form",
    "start": "309840",
    "end": "315160"
  },
  {
    "text": "factors for tpus one is the what we call like the TPU device where you have like a single VM uh attached to a TPU device",
    "start": "315160",
    "end": "323720"
  },
  {
    "text": "there's no special network communication with other devices in the in the cluster uh usually is designed for",
    "start": "323720",
    "end": "330120"
  },
  {
    "text": "type workloads um but that's not our Focus for this talk our focus is on uh",
    "start": "330120",
    "end": "335479"
  },
  {
    "text": "the other form factor which is TPU slices where when you provision you actually provision a group of VMS as a",
    "start": "335479",
    "end": "341039"
  },
  {
    "text": "unit and those VMS they are the the dev TP devices on those VMS they are connected with special uh highspeed",
    "start": "341039",
    "end": "347840"
  },
  {
    "text": "interconnect links like they are pretty much like fiber links that are like point to point",
    "start": "347840",
    "end": "353919"
  },
  {
    "text": "between these devices and it is specifically designed for for for training distributed training",
    "start": "353919",
    "end": "360280"
  },
  {
    "text": "workloads uh and those slices they can be provided in different shapes so here I'm showing like a 2 by uh a 2X two but",
    "start": "360280",
    "end": "367639"
  },
  {
    "text": "it can be all the way up to like 64 BMS in a single or no in a single single",
    "start": "367639",
    "end": "372680"
  },
  {
    "text": "slice so how do we train workloads on on SL on on pods on on TPU slices the",
    "start": "372680",
    "end": "379080"
  },
  {
    "text": "training setup is really simple From kubernetes perspective so you need to have a pod pair node that pod basically",
    "start": "379080",
    "end": "385039"
  },
  {
    "text": "consumes all the TPU device on each node and each pod needs a unque ID we need to",
    "start": "385039",
    "end": "391479"
  },
  {
    "text": "set up uh stable Network IDs between the pods uh so that the lower level communication Library it's called lip",
    "start": "391479",
    "end": "398240"
  },
  {
    "text": "TPU but it's pretty much similar to uh you know MPI um it requires those you know stable",
    "start": "398240",
    "end": "407520"
  },
  {
    "text": "IDs to be set up so that it can it can it can do the distributed uh communication um and for failure",
    "start": "407520",
    "end": "414400"
  },
  {
    "text": "basically it's like again any any distribute training framework if any pod fails we need to restart the whole",
    "start": "414400",
    "end": "421280"
  },
  {
    "text": "thing so how do we do this with uh with kubernetes well easy just use index drop",
    "start": "421280",
    "end": "426360"
  },
  {
    "text": "like I just described how index drop work so you specify like uh the number",
    "start": "426360",
    "end": "431759"
  },
  {
    "text": "of workers in the parallelism or completions um and completions U parameter uh you set a back of limit to",
    "start": "431759",
    "end": "437960"
  },
  {
    "text": "zero basically if any pod fails just fail the whole job because we can't even if that pod was recreated um it's not",
    "start": "437960",
    "end": "444280"
  },
  {
    "text": "going to be able to continue to to to train um and then we ually create the",
    "start": "444280",
    "end": "450080"
  },
  {
    "text": "service and we we set up the subdomain so that the the pods get uh stable IDs",
    "start": "450080",
    "end": "455280"
  },
  {
    "text": "and we set up some environment variables um for the uh you know training framework um uh uh uh you know um to",
    "start": "455280",
    "end": "465440"
  },
  {
    "text": "work now the issue is now we're talking about llms with like you know significantly larger no single even TPU",
    "start": "465440",
    "end": "472960"
  },
  {
    "text": "slice can train uh the the type of LMS",
    "start": "472960",
    "end": "478400"
  },
  {
    "text": "that we're trying to uh to uh to train and so we're looking now at multi slice",
    "start": "478400",
    "end": "484199"
  },
  {
    "text": "training uh and so the training is shed between multiple slices and we have two levels of orchestration uh there is",
    "start": "484199",
    "end": "490360"
  },
  {
    "text": "orchestration within the same slice which I just described is the same but then there is also coordination between",
    "start": "490360",
    "end": "495879"
  },
  {
    "text": "multiple slices um the failure policy is similar so basically if any pod in any slice",
    "start": "495879",
    "end": "502560"
  },
  {
    "text": "fail it's not enough to just recreate that pod everybody else needs to",
    "start": "502560",
    "end": "508240"
  },
  {
    "text": "restart so how do we do this well easy just",
    "start": "508240",
    "end": "513640"
  },
  {
    "text": "create this massive vml file with an index job for every single slice that um",
    "start": "513640",
    "end": "519680"
  },
  {
    "text": "you have set up some environment variables to make it work",
    "start": "519680",
    "end": "525320"
  },
  {
    "text": "but this is hard to manage right what if my training workload spans hundreds of",
    "start": "525320",
    "end": "530440"
  },
  {
    "text": "slices and how do I monitor the status of the whole job right like now you have 100 or 200 index jobs there's no one",
    "start": "530440",
    "end": "538200"
  },
  {
    "text": "place to look for the status of the whole workload and how do I manage failures as well like how do I fail all",
    "start": "538200",
    "end": "544200"
  },
  {
    "text": "the other jobs if one job fail with index job if you had one index job we",
    "start": "544200",
    "end": "549320"
  },
  {
    "text": "had that supported by index job right like if one point fails I can sit back off limit to zero and then the job",
    "start": "549320",
    "end": "554839"
  },
  {
    "text": "controller would say okay that job is failed but now I need to do that for all other jobs as",
    "start": "554839",
    "end": "560399"
  },
  {
    "text": "well and last but not least like how do I ensure that each job land exclusively on a slice um and this is where job set",
    "start": "560399",
    "end": "567920"
  },
  {
    "text": "helps us so this is an example work that we we run on on GP at this scale we had",
    "start": "567920",
    "end": "575399"
  },
  {
    "text": "uh over 12,000 nodes 50 over 50,000 TPU chips we had almost 200 TPU slices",
    "start": "575399",
    "end": "583200"
  },
  {
    "text": "basically 200 index jobs that we had to create and for each index 64",
    "start": "583200",
    "end": "589000"
  },
  {
    "text": "pods and we managed it using um jobset uh and and also we had Q installed in",
    "start": "589000",
    "end": "594600"
  },
  {
    "text": "the cluster uh because we had other smaller jobs running that we needed to manage uh uh uh the resources using so",
    "start": "594600",
    "end": "601560"
  },
  {
    "text": "how did we do that with with jobet well as I mentioned with jobset",
    "start": "601560",
    "end": "607200"
  },
  {
    "text": "allows us to create a number of replicated jobs for for the use case that I'm describing here we didn't",
    "start": "607200",
    "end": "613760"
  },
  {
    "text": "really need to create different jobs of different templates that's the second use case that vaness is going to talk about for our use case here it's was",
    "start": "613760",
    "end": "621519"
  },
  {
    "text": "mostly about creating multiple index jobs from the same table they are",
    "start": "621519",
    "end": "627000"
  },
  {
    "text": "replicas so here um in the like uh round in the uh blue box you can see we have",
    "start": "627399",
    "end": "635800"
  },
  {
    "text": "the template here which is really an index job it's it's a job template you specify the same thing that we specified",
    "start": "635800",
    "end": "642279"
  },
  {
    "text": "before um and and in this parameter number of replicas is basically uh specifies how many index job needs to be",
    "start": "642279",
    "end": "650920"
  },
  {
    "text": "created and in the success policy um we you could specify uh basically that the",
    "start": "651320",
    "end": "658360"
  },
  {
    "text": "whole workload is successful only if all the jobs are successful and finished successfully",
    "start": "658360",
    "end": "664320"
  },
  {
    "text": "with restart with with failure policy um right now we have only one type which is if anybody fails the whole job fails but",
    "start": "664320",
    "end": "671519"
  },
  {
    "text": "you you can specify how many times you want to restart it so this also gives you an automated way to restart the job",
    "start": "671519",
    "end": "678120"
  },
  {
    "text": "at the whole workload basically when this happens and the way that it does is basically it recreates all the jobs if a",
    "start": "678120",
    "end": "683880"
  },
  {
    "text": "failure happens so to force basically a restart um and then there are a bunch of",
    "start": "683880",
    "end": "690000"
  },
  {
    "text": "uh you know environment variables that you can set up we with job set you get",
    "start": "690000",
    "end": "695079"
  },
  {
    "text": "um a number of uh labels and annotations injected into each job that gets created",
    "start": "695079",
    "end": "700600"
  },
  {
    "text": "like the index of the each job and what not that makes it fairly easy and very straightforward to map it to um the",
    "start": "700600",
    "end": "708000"
  },
  {
    "text": "lower Lev Frameworks as environment variables um one last thing here is",
    "start": "708000",
    "end": "713040"
  },
  {
    "text": "still under development which why we only have it as an annotation uh we need to migrate to a proper API is exclusive",
    "start": "713040",
    "end": "719560"
  },
  {
    "text": "placement so with this what we're saying is that each job that you create that jobet",
    "start": "719560",
    "end": "726920"
  },
  {
    "text": "creates should ensure that it it lands on a unique set of nodes with a shared",
    "start": "726920",
    "end": "732800"
  },
  {
    "text": "uh TPU slice ID so here on the right side you can see that each TPU slice has",
    "start": "732800",
    "end": "739040"
  },
  {
    "text": "a bunch of BMS they will get a a shared TPU slice ID like a managed group for",
    "start": "739040",
    "end": "745880"
  },
  {
    "text": "example they have it's for example this is TPU slice ID zero one and two uh with",
    "start": "745880",
    "end": "752040"
  },
  {
    "text": "job say while specifying this parameter it's going to ensure that each job that you create for example job zero Its",
    "start": "752040",
    "end": "758560"
  },
  {
    "text": "Spots will land only on one slice and only job zero will land on one slice it will repel all other jobs and so I find",
    "start": "758560",
    "end": "765839"
  },
  {
    "text": "this a bit interesting maybe to to Deep dive a bit into it how we implemented",
    "start": "765839",
    "end": "770880"
  },
  {
    "text": "this so we implemented it using really just like for affinity and anti-affinity we didn't really integrate a new schul",
    "start": "770880",
    "end": "777160"
  },
  {
    "text": "into the job set operator uh what we did was injecting um two scheduling",
    "start": "777160",
    "end": "785240"
  },
  {
    "text": "constraints a a constraint that basically ensures that all the PODS of a job land on the same",
    "start": "785240",
    "end": "793199"
  },
  {
    "text": "slice but that's not enough right like I want to make sure that no other SL no other pod land on the same slice as well",
    "start": "793199",
    "end": "799040"
  },
  {
    "text": "and avoid these like you know types of race conditions and so we had to insert as well an anti-affinity constraint to",
    "start": "799040",
    "end": "805000"
  },
  {
    "text": "make to say all other part like uh any pod from any other job cannot land on the same",
    "start": "805000",
    "end": "812240"
  },
  {
    "text": "slice and so this is like this combination of of p a f Affinity um",
    "start": "812240",
    "end": "818800"
  },
  {
    "text": "allows us to implement this exclusive um placement",
    "start": "818800",
    "end": "824160"
  },
  {
    "text": "so our plan here is to really um you know um enhance this API make",
    "start": "824160",
    "end": "832519"
  },
  {
    "text": "Transition it to a proper API into the spec but we also want to include other",
    "start": "832519",
    "end": "839320"
  },
  {
    "text": "um um you know uh semantics so this the exclusive placement is the thing that we",
    "start": "839320",
    "end": "844399"
  },
  {
    "text": "needed at first but I can see other types right like for example a job should land on the",
    "start": "844399",
    "end": "851440"
  },
  {
    "text": "same slice or maybe think about it rack but it's not necessarily an exclusive",
    "start": "851440",
    "end": "856519"
  },
  {
    "text": "way right like you could fit multiple jobs under the same rack uh or you could in this case it's a required right like",
    "start": "856519",
    "end": "862959"
  },
  {
    "text": "it's it's a hard uh requirement we can also allow it to specify for example",
    "start": "862959",
    "end": "868120"
  },
  {
    "text": "it's preferred right like so that for performance reasons we prefer that the PODS of a job land on the same um",
    "start": "868120",
    "end": "876240"
  },
  {
    "text": "topology but that's not a requirement uh so we can continue to make progress if that doesn't",
    "start": "876240",
    "end": "882360"
  },
  {
    "text": "exist um so that concludes the first part of",
    "start": "882360",
    "end": "888279"
  },
  {
    "text": "the talk the second part I'm going to play um Vanessa's uh presentation",
    "start": "888279",
    "end": "895120"
  },
  {
    "text": "for um the HPC use case",
    "start": "895120",
    "end": "900680"
  },
  {
    "text": "hey ccon I'm Vanessa so I'm a computer scientist at Lawrence Livermore National Lab and today I'm",
    "start": "905680",
    "end": "912279"
  },
  {
    "text": "going to be talking to you about application building in kubernetes using jobet so about a year ago I started my",
    "start": "912279",
    "end": "919480"
  },
  {
    "text": "adventure as a developer for kubernetes and specifically I had one goal to implement a job manager our job manager",
    "start": "919480",
    "end": "926079"
  },
  {
    "text": "at the lab called flux framework inside of kubernetes and so I wanted to take slux and I wanted to take kubernetes and",
    "start": "926079",
    "end": "932399"
  },
  {
    "text": "I wanted to them together because that's how software engineering works right not",
    "start": "932399",
    "end": "938120"
  },
  {
    "text": "exactly so there's these two communities we have cloud and HBC and in between them is this beautiful opportunity for",
    "start": "938120",
    "end": "943880"
  },
  {
    "text": "the convergence of Technologies and culture we call this converge Computing and there was some rhyme to our reason",
    "start": "943880",
    "end": "949959"
  },
  {
    "text": "in choosing kubernetes and flux both of our communities use them for running workloads and jobs and they're also both",
    "start": "949959",
    "end": "956560"
  },
  {
    "text": "modular so for example I can take a comp component from flux and I can implement it inside of kubernetes and vice versa",
    "start": "956560",
    "end": "963720"
  },
  {
    "text": "so tldr long story short we created the flux operator to implement the entirety of flux in framework inside of",
    "start": "963720",
    "end": "970680"
  },
  {
    "text": "kubernetes and it was an awesome experience it was the first taste of",
    "start": "970680",
    "end": "976160"
  },
  {
    "text": "convergence so our next logical question was okay we have our workload manager now how do we run applications let's go",
    "start": "976160",
    "end": "983240"
  },
  {
    "text": "and talk to some fish in that bay how are these cloud and HPC apps actually different first looking at application",
    "start": "983240",
    "end": "989839"
  },
  {
    "text": "coupling in Cloud we have very Loosely coupled apps in HPC we have very tightly",
    "start": "989839",
    "end": "995759"
  },
  {
    "text": "coupled apps and we'll talk more about what that means later in the talk for research scheduling in Cloud we may want",
    "start": "995759",
    "end": "1001519"
  },
  {
    "text": "to run a pod with a certain amount of CPU in HPC we're actually scheduling more than software we're scheduling",
    "start": "1001519",
    "end": "1007519"
  },
  {
    "text": "Hardware down to the PCI bus for job queuing in Cloud we made do something",
    "start": "1007519",
    "end": "1012959"
  },
  {
    "text": "like calculate a priority score in HPC we have very sophisticated queuing algorithms that often are graph-based",
    "start": "1012959",
    "end": "1019839"
  },
  {
    "text": "finally workflow management well Cloud does this really well with automated declarative management bring in the yaml",
    "start": "1019839",
    "end": "1026600"
  },
  {
    "text": "camel in HBC we also have workflow tools but sometimes it's not really great it's",
    "start": "1026600",
    "end": "1031798"
  },
  {
    "text": "like bash scripts all the way down so our first step after obviously",
    "start": "1031799",
    "end": "1038120"
  },
  {
    "text": "getting our workload manager in kubernetes was like let's run some applications let's get into our baiting",
    "start": "1038120",
    "end": "1043678"
  },
  {
    "text": "suits dive into that bay and our applications they they",
    "start": "1043679",
    "end": "1050919"
  },
  {
    "text": "didn't run oh why doesn't it work the question for developers of all time please",
    "start": "1050919",
    "end": "1056799"
  },
  {
    "text": "someone tell me why doesn't it work so we needed to actually look",
    "start": "1056799",
    "end": "1062240"
  },
  {
    "text": "deeper into this mystery we needed to dive into the trench of treachery wait",
    "start": "1062240",
    "end": "1067640"
  },
  {
    "text": "how how did that get there I mean the trench of Discovery to really debug this",
    "start": "1067640",
    "end": "1073520"
  },
  {
    "text": "problem and so my team and I we loaded up in our submarine except I was the one on the outside that like in the movie",
    "start": "1073520",
    "end": "1079200"
  },
  {
    "text": "gets eaten by the shark or like the underwater sea monster no I'm I'm just kidding so we went down down down and",
    "start": "1079200",
    "end": "1085720"
  },
  {
    "text": "our first stop was to try to better understand how we could model our complex HPC applications in kubernetes",
    "start": "1085720",
    "end": "1092360"
  },
  {
    "text": "and we already had some experience with this because of the flux operator we had used an index job so we had taken flux",
    "start": "1092360",
    "end": "1098400"
  },
  {
    "text": "put it in a container the containers go in pods and by way of the index job that",
    "start": "1098400",
    "end": "1103799"
  },
  {
    "text": "gives us duplicates we can add some system configuration files and then headless service gives us unique host",
    "start": "1103799",
    "end": "1110200"
  },
  {
    "text": "names so they can all talk to one another and this Encompass the flux mini cluster was awesome but there were",
    "start": "1110200",
    "end": "1117240"
  },
  {
    "text": "hashtag developer problems there's always developer problems aren't they so the first one was that we have to have different entry point logic based on the",
    "start": "1117240",
    "end": "1124600"
  },
  {
    "text": "index in one script we also needed to manually create the Headless service and",
    "start": "1124600",
    "end": "1130480"
  },
  {
    "text": "finally we really needed a way to say like when this main index zero is done the entire job is done so okay we",
    "start": "1130480",
    "end": "1137799"
  },
  {
    "text": "understood the the limitations of job we kept going in our adventure and we encountered job",
    "start": "1137799",
    "end": "1144480"
  },
  {
    "text": "set hey Job set what's going on what are you doing under the ocean you know what",
    "start": "1144480",
    "end": "1151600"
  },
  {
    "text": "don't answer that it's it's totally cool I do not judge yes we would absolutely",
    "start": "1151600",
    "end": "1156799"
  },
  {
    "text": "love your help so with job set we could add this other abstraction of a job set",
    "start": "1156799",
    "end": "1162720"
  },
  {
    "text": "and by way of having more than one replicated job we could separate the logic of our different components spe",
    "start": "1162720",
    "end": "1168280"
  },
  {
    "text": "specifically into a lead broker and follower Brokers we could still be have them on the same",
    "start": "1168280",
    "end": "1173720"
  },
  {
    "text": "network and with a success policy we could actually say okay when this lead broker is done the entire job is done",
    "start": "1173720",
    "end": "1181760"
  },
  {
    "text": "awesome so we knew that job set would be the substraction to allow us to build these complex HPC",
    "start": "1181760",
    "end": "1187919"
  },
  {
    "text": "applications but we still we still have this problem which is why we're underwater in the first place that our",
    "start": "1187919",
    "end": "1193760"
  },
  {
    "text": "applications didn't run so let's not forget about that so we needed to take a trip and do the cave of debugging so we",
    "start": "1193760",
    "end": "1200640"
  },
  {
    "text": "go in this cave and it really wasn't such a bad cave because we found some friends in there and our friends had",
    "start": "1200640",
    "end": "1205919"
  },
  {
    "text": "some good ideas about what might be going wrong I had some ideas too and what I ultimately found is that when I",
    "start": "1205919",
    "end": "1211559"
  },
  {
    "text": "added a local DNS cache the cluster that for first didn't come up at all all of a",
    "start": "1211559",
    "end": "1217320"
  },
  {
    "text": "sudden came up and so who was lurking in the darkness it was the DNS fish yes you",
    "start": "1217320",
    "end": "1223640"
  },
  {
    "text": "get out of here you nobody wants to hear about you anymore yet you somehow are always around so I said okay great let's",
    "start": "1223640",
    "end": "1230000"
  },
  {
    "text": "run our application again maybe we'll we'll see it run this time so just to give you a sense of what I'm seeing what",
    "start": "1230000",
    "end": "1235080"
  },
  {
    "text": "you're looking at here are application times for a fairly good problem size for our application and you're looking at",
    "start": "1235080",
    "end": "1240799"
  },
  {
    "text": "time as a function of the cluster size and so what we're doing is something called strong scaling where you hold the",
    "start": "1240799",
    "end": "1246600"
  },
  {
    "text": "problem size constant and you give it more resources and what you'd expect to see is that it gets faster and to some",
    "start": "1246600",
    "end": "1252760"
  },
  {
    "text": "extent we do see that so we do see that up to size 32 but then it's size 64 like",
    "start": "1252760",
    "end": "1257840"
  },
  {
    "text": "oh something happens and that something suggests that the cost of communication",
    "start": "1257840",
    "end": "1263159"
  },
  {
    "text": "offsets any potential benefit to adding more resources and also I just want to point",
    "start": "1263159",
    "end": "1269039"
  },
  {
    "text": "out that these times are not very good so oh like it's still slow thankfully",
    "start": "1269039",
    "end": "1275880"
  },
  {
    "text": "Antonio swam out of the collaborator sub and he was like I can help V you see",
    "start": "1275880",
    "end": "1280919"
  },
  {
    "text": "there's all these things that you aren't considering from the base image to how you're building MPI networking flags and",
    "start": "1280919",
    "end": "1287279"
  },
  {
    "text": "importantly the instance type so we did many more experiments and by the way this is over many months and we found we",
    "start": "1287279",
    "end": "1294559"
  },
  {
    "text": "ran into new errors like yes on so both of us were like oh no oh",
    "start": "1294559",
    "end": "1301000"
  },
  {
    "text": "no but I have this insight and many developers have this Insight when you don't understand something you need to",
    "start": "1301000",
    "end": "1306880"
  },
  {
    "text": "measure things we needed to measure the network and understand what was going on with our application and because I had",
    "start": "1306880",
    "end": "1312480"
  },
  {
    "text": "just done this work with jobet this was at the Forefront of my mind I was like hey jobet can you come down here and",
    "start": "1312480",
    "end": "1319039"
  },
  {
    "text": "jobset was like I don't know it's kind of creepy down there but jobset gives us",
    "start": "1319039",
    "end": "1324279"
  },
  {
    "text": "this flexibility to model not just our applications but also tools for performance analysis and the Insight that I have",
    "start": "1324279",
    "end": "1331360"
  },
  {
    "text": "spef I had specifically was that so many of our applications along with actually machine learning applications have this",
    "start": "1331360",
    "end": "1337400"
  },
  {
    "text": "launcher and workers design so I could actually Implement everything from proxy",
    "start": "1337400",
    "end": "1342520"
  },
  {
    "text": "apps to networking and storage or IO tools using this common design so this",
    "start": "1342520",
    "end": "1348200"
  },
  {
    "text": "turned into what the metrix operator yes he's absolutely adorable with the metrix",
    "start": "1348200",
    "end": "1354679"
  },
  {
    "text": "operator you create something called a metric set and the metric set is going to allow us to look at that networking",
    "start": "1354679",
    "end": "1360799"
  },
  {
    "text": "which we thought was the culprit specifically using something called the OSU benchmarks so we needed to dive deeper",
    "start": "1360799",
    "end": "1367640"
  },
  {
    "text": "so first change of wardrobe and we again went down down down down still going",
    "start": "1367640",
    "end": "1373919"
  },
  {
    "text": "down into the abyss of expectations which is a very dangerous place to be might I add and so my first expectation",
    "start": "1373919",
    "end": "1382880"
  },
  {
    "text": "was that HPC applications need low latency we always have these beautiful infin band Fabrics why would they not",
    "start": "1382880",
    "end": "1388919"
  },
  {
    "text": "need it so let's go back to this idea of tight coupling what does that mean I want to talk more about MPI the message",
    "start": "1388919",
    "end": "1395840"
  },
  {
    "text": "passing interface so MPI if we look again at this launcher worker design hbca is going to be using a tightly",
    "start": "1395840",
    "end": "1402120"
  },
  {
    "text": "couple design so we may start with thinking of the concept of nodes so here we have nodes there's a launch node and",
    "start": "1402120",
    "end": "1408240"
  },
  {
    "text": "worker nodes but in HPC because we need to run complex scientific simulations we",
    "start": "1408240",
    "end": "1414799"
  },
  {
    "text": "actually care about the relevant unit of a process so instead of looking at nodes",
    "start": "1414799",
    "end": "1420120"
  },
  {
    "text": "here what you're looking at are 14 CPU that do the work and they are going to be communicating using the message",
    "start": "1420120",
    "end": "1426240"
  },
  {
    "text": "passing interface which is called an interface because it's more like a standard and there's different implementations of it so I need to",
    "start": "1426240",
    "end": "1434400"
  },
  {
    "text": "stress that because these processes are on different nodes that often means that we're going to have communication of",
    "start": "1434400",
    "end": "1439880"
  },
  {
    "text": "processes from different nodes and that's why the networking is so important so there's different",
    "start": "1439880",
    "end": "1446039"
  },
  {
    "text": "patterns of communication that we Define and we're going to talk about two kind of families today those are point-to-point and Collective calls so",
    "start": "1446039",
    "end": "1453600"
  },
  {
    "text": "pointto Point communication may be a send and receive and this is exactly what it sounds like you have one p CPU",
    "start": "1453600",
    "end": "1459240"
  },
  {
    "text": "that is sending a packet of data to another one hello it's me I've got a",
    "start": "1459240",
    "end": "1464600"
  },
  {
    "text": "packet of data that you might want to receive and then hopefully the CPU is going to send a message back or other",
    "start": "1464600",
    "end": "1470520"
  },
  {
    "text": "otherwise the first CPU is going to be making music videos in black and white that are really sad and you know how",
    "start": "1470520",
    "end": "1475640"
  },
  {
    "text": "that goes so the second pattern is called Collective and these are Communications between collections or",
    "start": "1475640",
    "end": "1481600"
  },
  {
    "text": "groups of processes so here you see an example of NPI sum this is a little like map reduce where you start with a bunch",
    "start": "1481600",
    "end": "1487799"
  },
  {
    "text": "of numbers and then they reduce down to one sum now the most common Collective call is called NPI all reduce this is",
    "start": "1487799",
    "end": "1494279"
  },
  {
    "text": "going to add something called NPI bcast which is going to broadcast that data out to more than one CPU and we can",
    "start": "1494279",
    "end": "1502039"
  },
  {
    "text": "actually look at these calls for Collective and pointto point across HPC applications and as I mentioned MPI all",
    "start": "1502039",
    "end": "1509159"
  },
  {
    "text": "reduce is the most popular Collective call and then send and receive are the most popular point-to-point functions so",
    "start": "1509159",
    "end": "1515919"
  },
  {
    "text": "I must stress that because of these communication patterns without a low latency Network the apps absolutely",
    "start": "1515919",
    "end": "1522799"
  },
  {
    "text": "cannot be performant so the OSU benchmarks are going to allow us to measure these",
    "start": "1522799",
    "end": "1528640"
  },
  {
    "text": "through two things OSU all reduced for Collective OSU latency for pointto point",
    "start": "1528640",
    "end": "1534159"
  },
  {
    "text": "and we're going to be using the metrics operator here's what that looks like to create a metric set and the goal of the",
    "start": "1534159",
    "end": "1539600"
  },
  {
    "text": "metric operator is just really make it stupid easy to run these HPC apps and applications powered by job",
    "start": "1539600",
    "end": "1547080"
  },
  {
    "text": "set all righty so here we're looking at OSU latency this is a point-to-point benchmark on the cloud we see latency in",
    "start": "1547080",
    "end": "1554320"
  },
  {
    "text": "microseconds in log scale as a function of the packet size and you're probably looking at this like well how do I know",
    "start": "1554320",
    "end": "1560440"
  },
  {
    "text": "if this is any good well we can compare it to HPC which is orders of magnitude better between 10 and 20 times depending",
    "start": "1560440",
    "end": "1567919"
  },
  {
    "text": "on where you are in this graph and so I looked at this and and my little abys abyss of expectations and I was like oh",
    "start": "1567919",
    "end": "1574559"
  },
  {
    "text": "it's the latency I knew it that's the culprit but I also had another mouth",
    "start": "1574559",
    "end": "1580320"
  },
  {
    "text": "formed expectation wink wink I thought that I knew that my application would run better on HPC I assumed that it was",
    "start": "1580320",
    "end": "1587399"
  },
  {
    "text": "run better but I hadn't actually run it yet oops and at this time there was also sort of a blessing in disguise which",
    "start": "1587399",
    "end": "1593480"
  },
  {
    "text": "sucked at the time but the blessing was that I did not have quota so the instance type that I wanted to use I",
    "start": "1593480",
    "end": "1599760"
  },
  {
    "text": "didn't have quota for so I had to fall back to a different instance type that I hadn't really tested as much and I was",
    "start": "1599760",
    "end": "1607799"
  },
  {
    "text": "shocked so remember we talked about strong scaling the strong scaling looked beautiful we saw that as we increased",
    "start": "1607799",
    "end": "1614320"
  },
  {
    "text": "the mki ranks this time went down and all although this wasn't like the best that I'd ever seen I was like in shock",
    "start": "1614320",
    "end": "1621200"
  },
  {
    "text": "because this was the result that I had wanted to see way back in May many months",
    "start": "1621200",
    "end": "1626480"
  },
  {
    "text": "earlier and so I'm sitting in my abyss of expectations with a nice stewing pot",
    "start": "1626480",
    "end": "1631960"
  },
  {
    "text": "of confusion like what just happened this didn't even work before and of",
    "start": "1631960",
    "end": "1637520"
  },
  {
    "text": "course my confusion very quickly turned into delusion I was like it's okay I'm still going to try on HPC and I'm sure",
    "start": "1637520",
    "end": "1643320"
  },
  {
    "text": "it will be better again shook so what you're looking at here we added",
    "start": "1643320",
    "end": "1649960"
  },
  {
    "text": "in HPC the cloud it has the red boxes around it and what we actually see is that the Cloud Times were faster for our",
    "start": "1649960",
    "end": "1656840"
  },
  {
    "text": "MPI application except for the largest size where one of our HPC clusters was slightly faster and so what that suggest",
    "start": "1656840",
    "end": "1664279"
  },
  {
    "text": "is there's still some issue with communication between nodes when we get to those larger",
    "start": "1664279",
    "end": "1670120"
  },
  {
    "text": "sizes H what could be that cost what's going on here and we can actually then",
    "start": "1670120",
    "end": "1676360"
  },
  {
    "text": "look at our all red bench mark because that is a proxy for communication so I know this looks like someone like",
    "start": "1676360",
    "end": "1682200"
  },
  {
    "text": "sneezed on the slide but let me try to explain what you're looking at here you're still looking at average latency in microseconds in log scale as the",
    "start": "1682200",
    "end": "1689360"
  },
  {
    "text": "function of the packet size there's two groups here the top group this is cloud",
    "start": "1689360",
    "end": "1694960"
  },
  {
    "text": "and those little dots in between are actually an increase in cluster size so the the dot on the bottom is the",
    "start": "1694960",
    "end": "1700200"
  },
  {
    "text": "smallest the one on the top is the largest and so what you see is that as you add more nodes that need to talk",
    "start": "1700200",
    "end": "1705320"
  },
  {
    "text": "together the time goes up a lot now HPC is the second group and it's much lower",
    "start": "1705320",
    "end": "1711120"
  },
  {
    "text": "again orders of magnitude and this is a cluster that has infin ban Fabric and",
    "start": "1711120",
    "end": "1716159"
  },
  {
    "text": "look at the difference in units I just want to point out these are sort of in the tens versus over 100s so there is a",
    "start": "1716159",
    "end": "1721320"
  },
  {
    "text": "very large difference so we do suspect that network",
    "start": "1721320",
    "end": "1726360"
  },
  {
    "text": "is still becoming an issue at these larger sizes so how could we further",
    "start": "1726360",
    "end": "1731519"
  },
  {
    "text": "look at this well we brought in our metrics operator again that also can run a tool called the HPC toolkit because we",
    "start": "1731519",
    "end": "1737120"
  },
  {
    "text": "wanted wanted to verify that the time spent in all reduce was actually going up and I was running out of cloud credit",
    "start": "1737120",
    "end": "1744720"
  },
  {
    "text": "so I only could run this on three sizes but what we saw is that as the size increased indeed the time and all",
    "start": "1744720",
    "end": "1750640"
  },
  {
    "text": "reduced the time spent communicating between knows was increasing and this in a way did validate what we thought",
    "start": "1750640",
    "end": "1756880"
  },
  {
    "text": "communication is definitely the bottleneck at larger sizes so I can say this comically now",
    "start": "1756880",
    "end": "1764519"
  },
  {
    "text": "but I woke up the next morning after after all of this after these results and being surprised and I was",
    "start": "1764519",
    "end": "1771640"
  },
  {
    "text": "overwhelmed there were so many variables to think about the story was not clear",
    "start": "1771640",
    "end": "1776960"
  },
  {
    "text": "and logical and there was so so many things I wanted to test and it totally went against my expectation this is why",
    "start": "1776960",
    "end": "1783360"
  },
  {
    "text": "it's so dangerous to be in this abyss and it felt really bad but this is also",
    "start": "1783360",
    "end": "1788559"
  },
  {
    "text": "why it's so important to have a supportive team because my team came down and they were like hey V it's a",
    "start": "1788559",
    "end": "1796360"
  },
  {
    "text": "little dark down here I know you're having some kind of party or something but maybe it's time to come back up and",
    "start": "1796360",
    "end": "1804159"
  },
  {
    "text": "so we left the cave of debugging and we had learned so much and actually the first point was almost Joyful We were",
    "start": "1804159",
    "end": "1810440"
  },
  {
    "text": "like wow we were actually wrong about always having the need for the lowest of",
    "start": "1810440",
    "end": "1815720"
  },
  {
    "text": "latency the latency doesn't have to be super low it just has to be good enough and in fact our application problem is",
    "start": "1815720",
    "end": "1822440"
  },
  {
    "text": "CPU bound at lower ranks and network latency at higher ranks and the reason it works so well on CL is because the",
    "start": "1822440",
    "end": "1828000"
  },
  {
    "text": "cloud has really awesome shiny new CPU so at this point we could kind of return to the service and this is where we are",
    "start": "1828000",
    "end": "1834600"
  },
  {
    "text": "today these are the things we still do not understand we do not understand why this particular instance type did not",
    "start": "1834600",
    "end": "1841240"
  },
  {
    "text": "work originally we also need to better look at our expectations what expectations or sort of lore do all of",
    "start": "1841240",
    "end": "1848640"
  },
  {
    "text": "us carry that come from our community that might just be entirely wrong and then application Design This is the",
    "start": "1848640",
    "end": "1854360"
  },
  {
    "text": "coolest bit how can we take our applications and kind of put understand the patterns that they need to map them",
    "start": "1854360",
    "end": "1860840"
  },
  {
    "text": "to the right environments and so to kind of summarize the story we started in this new space",
    "start": "1860840",
    "end": "1867440"
  },
  {
    "text": "that we didn't understand we found a design strategy to map between spaces this was jobset and then we could figure",
    "start": "1867440",
    "end": "1875320"
  },
  {
    "text": "out a means to easily measure and run things this was the metrics operator that ran everything from proxy apps to",
    "start": "1875320",
    "end": "1880919"
  },
  {
    "text": "benchmarks and operators and kubernetes are so cool they are like developer Legos for building things High recommend",
    "start": "1880919",
    "end": "1888120"
  },
  {
    "text": "in parallel no pun intended we also need to understand the application patterns",
    "start": "1888120",
    "end": "1893200"
  },
  {
    "text": "and needs so that we can really best optimize the environment for them and you know what these are really",
    "start": "1893200",
    "end": "1900720"
  },
  {
    "text": "complex hard problems this is not just the work that one team can do not even",
    "start": "1900720",
    "end": "1905880"
  },
  {
    "text": "one lab we need to bring in collaborators from you know not other labs industry academic all over the",
    "start": "1905880",
    "end": "1913399"
  },
  {
    "text": "place and we have to work together on this to solve these hard problems problems and you know this is just the",
    "start": "1913399",
    "end": "1919519"
  },
  {
    "text": "start we are hoping to like build a little bridge to get across HBC to Cloudland but really what we want to",
    "start": "1919519",
    "end": "1926720"
  },
  {
    "text": "move toward is this future where we can have the best of both worlds this converged Computing and you know what",
    "start": "1926720",
    "end": "1933720"
  },
  {
    "text": "there are a ton of Adventures ahead and we hope that you join us I can promise",
    "start": "1933720",
    "end": "1938799"
  },
  {
    "text": "that I won't be out somewhere on adventure but you know what you are invited to come to hoop con back to",
    "start": "1938799",
    "end": "1946080"
  },
  {
    "text": "you",
    "start": "1946080",
    "end": "1949080"
  },
  {
    "text": "all right you can see why I went first it's hard hard to match Vanessa's energy",
    "start": "1955440",
    "end": "1963120"
  },
  {
    "text": "um so just one last uh piece here on future work for job set um I mentioned",
    "start": "1963120",
    "end": "1970880"
  },
  {
    "text": "um the placement policy a little bit um uh about like our plans to have a proper",
    "start": "1970880",
    "end": "1976039"
  },
  {
    "text": "API with better um you know configurability and the other thing is as you might know if",
    "start": "1976039",
    "end": "1982679"
  },
  {
    "text": "you've ever used P Affinity Affinity it's not the most performance you know constraint that you could have on on job",
    "start": "1982679",
    "end": "1989240"
  },
  {
    "text": "on on pods so we're thinking about approaches how we can accelerate it um one solution we have is like having a",
    "start": "1989240",
    "end": "1995639"
  },
  {
    "text": "lader forward pattern like only one part of each job would have like for example index zero you can use a web hog to do",
    "start": "1995639",
    "end": "2001679"
  },
  {
    "text": "that would have the Pod Affinity anti-affinity constraints once that pod schedules all the other pods on of that",
    "start": "2001679",
    "end": "2009200"
  },
  {
    "text": "job basically we would have a validating web hook that blocks creating them until its leader is",
    "start": "2009200",
    "end": "2015159"
  },
  {
    "text": "scheduled once the leader is scheduled we inject node Affinity to follow the leader pod into the same topology for",
    "start": "2015159",
    "end": "2022200"
  },
  {
    "text": "example if it's a pod slice it will go to the same slice ID um so that makes it",
    "start": "2022200",
    "end": "2027919"
  },
  {
    "text": "basically much faster so we split the scheduling into two pieces one is the",
    "start": "2027919",
    "end": "2033200"
  },
  {
    "text": "200 po that represent the leaders of the 200 uh job that we create create those are going to schedule a bit slowly but",
    "start": "2033200",
    "end": "2040639"
  },
  {
    "text": "the rest is going to be um fast um and failure policy as well we have some",
    "start": "2040639",
    "end": "2046559"
  },
  {
    "text": "ideas um right now we recreate all the jobs when a failure happen which again",
    "start": "2046559",
    "end": "2051560"
  },
  {
    "text": "it's too it's a big hammer right like it's too expensive uh we are thinking about ways how we can do in place pod",
    "start": "2051560",
    "end": "2058358"
  },
  {
    "text": "restarts in a reasonable way um the main issue here is how we broadcast all the",
    "start": "2058359",
    "end": "2063919"
  },
  {
    "text": "parts that they should restart um so some ideas is like maybe we can have a config map that all the pods Mount and",
    "start": "2063919",
    "end": "2070839"
  },
  {
    "text": "they receive that broadcast with a side car that basically uh tries to make it",
    "start": "2070839",
    "end": "2076118"
  },
  {
    "text": "more composable once it receives this restart and goes and does like a kill",
    "start": "2076119",
    "end": "2082398"
  },
  {
    "text": "five uh to the main main main uh container",
    "start": "2082399",
    "end": "2088520"
  },
  {
    "text": "um and that's pretty much it so this is our repo uh it's uh sponsored by the",
    "start": "2088520",
    "end": "2094118"
  },
  {
    "text": "batch working group uh if you have any questions please uh uh reach reach out",
    "start": "2094119",
    "end": "2099480"
  },
  {
    "text": "to us on the batch workking group slack Channel or uh uh email and uh also special uh acknowledgement to Daniel who",
    "start": "2099480",
    "end": "2107839"
  },
  {
    "text": "helped Implement our jobsite and couldn't be here today and thank",
    "start": "2107839",
    "end": "2114400"
  },
  {
    "text": "you",
    "start": "2115240",
    "end": "2118240"
  }
]