[
  {
    "start": "0",
    "end": "39000"
  },
  {
    "text": "hello everyone my name is paul thank you",
    "start": "2320",
    "end": "4640"
  },
  {
    "text": "for coming back to this session after a",
    "start": "4640",
    "end": "7359"
  },
  {
    "text": "good lunch",
    "start": "7359",
    "end": "9839"
  },
  {
    "text": "so yeah welcome to",
    "start": "10639",
    "end": "12240"
  },
  {
    "text": "this the day of edge",
    "start": "12240",
    "end": "14080"
  },
  {
    "text": "model serving at the edge made easier is",
    "start": "14080",
    "end": "15759"
  },
  {
    "text": "the name of my talk",
    "start": "15759",
    "end": "17600"
  },
  {
    "text": "i am paul van eck i am an open source",
    "start": "17600",
    "end": "20080"
  },
  {
    "text": "engineer software engineer at ibm based",
    "start": "20080",
    "end": "22480"
  },
  {
    "text": "in silicon valley labs at in california",
    "start": "22480",
    "end": "25599"
  },
  {
    "text": "um my co-speaker could not be here today",
    "start": "25599",
    "end": "27840"
  },
  {
    "text": "my co-speaker anna mesh singh so it'll",
    "start": "27840",
    "end": "30720"
  },
  {
    "text": "just be me but he will be",
    "start": "30720",
    "end": "32960"
  },
  {
    "text": "with us in spirit",
    "start": "32960",
    "end": "35040"
  },
  {
    "text": "or more likely probably asleep in",
    "start": "35040",
    "end": "37200"
  },
  {
    "text": "california",
    "start": "37200",
    "end": "39520"
  },
  {
    "start": "39000",
    "end": "39000"
  },
  {
    "text": "so just to kind of give an outline for",
    "start": "39520",
    "end": "41200"
  },
  {
    "text": "the talk we're going to first kind of go",
    "start": "41200",
    "end": "43680"
  },
  {
    "text": "over edge computing then the whole model",
    "start": "43680",
    "end": "45280"
  },
  {
    "text": "serving overview",
    "start": "45280",
    "end": "46960"
  },
  {
    "text": "then the whole notion of using",
    "start": "46960",
    "end": "48640"
  },
  {
    "text": "kubernetes uh for model serving at the",
    "start": "48640",
    "end": "51280"
  },
  {
    "text": "edge right then using model mesh",
    "start": "51280",
    "end": "53760"
  },
  {
    "text": "something called model mesh which i will",
    "start": "53760",
    "end": "55280"
  },
  {
    "text": "introduce to easily manage kind of",
    "start": "55280",
    "end": "57360"
  },
  {
    "text": "higher density",
    "start": "57360",
    "end": "58879"
  },
  {
    "text": "edge model deployments",
    "start": "58879",
    "end": "60640"
  },
  {
    "text": "on kubernetes on edge",
    "start": "60640",
    "end": "62640"
  },
  {
    "text": "so yeah we'll go into an example",
    "start": "62640",
    "end": "64239"
  },
  {
    "text": "deployment and some challenges that were",
    "start": "64239",
    "end": "66640"
  },
  {
    "text": "encountered and some lessons learned",
    "start": "66640",
    "end": "70840"
  },
  {
    "start": "72000",
    "end": "72000"
  },
  {
    "text": "so yeah so as you all probably already",
    "start": "72240",
    "end": "74240"
  },
  {
    "text": "know we are in the midst",
    "start": "74240",
    "end": "76320"
  },
  {
    "text": "of a new generation of computing you",
    "start": "76320",
    "end": "78080"
  },
  {
    "text": "know edge computing that's why we're",
    "start": "78080",
    "end": "79759"
  },
  {
    "text": "here and so with each generation of",
    "start": "79759",
    "end": "82080"
  },
  {
    "text": "computing",
    "start": "82080",
    "end": "83200"
  },
  {
    "text": "uh",
    "start": "83200",
    "end": "84080"
  },
  {
    "text": "before it edge will impact every",
    "start": "84080",
    "end": "85920"
  },
  {
    "text": "industry um and force you know i.t",
    "start": "85920",
    "end": "88560"
  },
  {
    "text": "departments to adapt to new",
    "start": "88560",
    "end": "90640"
  },
  {
    "text": "architectures um new deployment models",
    "start": "90640",
    "end": "93280"
  },
  {
    "text": "and business models and so here we focus",
    "start": "93280",
    "end": "95920"
  },
  {
    "text": "on bringing computing to offices this",
    "start": "95920",
    "end": "98880"
  },
  {
    "text": "distribution centers manufacturing sites",
    "start": "98880",
    "end": "102240"
  },
  {
    "text": "and so this kind of",
    "start": "102240",
    "end": "105119"
  },
  {
    "text": "create this decentralized approach to",
    "start": "105119",
    "end": "107280"
  },
  {
    "text": "application design and bring with it the",
    "start": "107280",
    "end": "109920"
  },
  {
    "text": "new challenges of workload management",
    "start": "109920",
    "end": "111680"
  },
  {
    "text": "across thousands or millions of edge",
    "start": "111680",
    "end": "114560"
  },
  {
    "text": "nodes",
    "start": "114560",
    "end": "116159"
  },
  {
    "text": "and so it helps to visualize edge",
    "start": "116159",
    "end": "118320"
  },
  {
    "text": "computing through the continuum of",
    "start": "118320",
    "end": "121040"
  },
  {
    "text": "physical infrastructure from centralized",
    "start": "121040",
    "end": "123280"
  },
  {
    "text": "data centers to",
    "start": "123280",
    "end": "126240"
  },
  {
    "text": "devices right so to the far right of the",
    "start": "126399",
    "end": "128879"
  },
  {
    "text": "diagram that shows the centralized",
    "start": "128879",
    "end": "132000"
  },
  {
    "text": "data centers um representing you know",
    "start": "132000",
    "end": "134480"
  },
  {
    "text": "cloud-based compute",
    "start": "134480",
    "end": "136080"
  },
  {
    "text": "and you know here cloud resources are",
    "start": "136080",
    "end": "138640"
  },
  {
    "text": "practically unlimited",
    "start": "138640",
    "end": "140239"
  },
  {
    "text": "whereas device resources are inherently",
    "start": "140239",
    "end": "142879"
  },
  {
    "text": "constrained",
    "start": "142879",
    "end": "144480"
  },
  {
    "text": "and so so moving along the continuum",
    "start": "144480",
    "end": "146239"
  },
  {
    "text": "from centralized data centers towards",
    "start": "146239",
    "end": "147920"
  },
  {
    "text": "devices",
    "start": "147920",
    "end": "149040"
  },
  {
    "text": "the first main edge tier is the",
    "start": "149040",
    "end": "152000"
  },
  {
    "text": "service provider edge and this is",
    "start": "152000",
    "end": "154720"
  },
  {
    "text": "distributed and brings edge computing",
    "start": "154720",
    "end": "156319"
  },
  {
    "text": "resources",
    "start": "156319",
    "end": "157440"
  },
  {
    "text": "uh resources much closer to end users",
    "start": "157440",
    "end": "160640"
  },
  {
    "text": "and so moving even more right we have",
    "start": "160640",
    "end": "162640"
  },
  {
    "text": "the user edge which represents you know",
    "start": "162640",
    "end": "165200"
  },
  {
    "text": "a highly diverse mixture of resources",
    "start": "165200",
    "end": "169040"
  },
  {
    "text": "and you know as a general role the more",
    "start": "169040",
    "end": "170879"
  },
  {
    "text": "uh",
    "start": "170879",
    "end": "171920"
  },
  {
    "text": "the closer that edge compute resources",
    "start": "171920",
    "end": "173519"
  },
  {
    "text": "get to the physical world",
    "start": "173519",
    "end": "175599"
  },
  {
    "text": "you know the more constrained and",
    "start": "175599",
    "end": "176800"
  },
  {
    "text": "specialized they become",
    "start": "176800",
    "end": "179680"
  },
  {
    "text": "so this user edge space is essentially",
    "start": "179680",
    "end": "181519"
  },
  {
    "text": "the focus",
    "start": "181519",
    "end": "182720"
  },
  {
    "text": "for today",
    "start": "182720",
    "end": "184640"
  },
  {
    "text": "and so as we look at workloads that",
    "start": "184640",
    "end": "186480"
  },
  {
    "text": "benefit from running at the edge",
    "start": "186480",
    "end": "189360"
  },
  {
    "text": "we see things like business logic",
    "start": "189360",
    "end": "191360"
  },
  {
    "text": "applications network modernization",
    "start": "191360",
    "end": "194480"
  },
  {
    "text": "then the focus where i want to kind of",
    "start": "194480",
    "end": "196159"
  },
  {
    "text": "focus on today is the notion of",
    "start": "196159",
    "end": "197680"
  },
  {
    "text": "stretching ai",
    "start": "197680",
    "end": "199680"
  },
  {
    "text": "and analytics to the edge",
    "start": "199680",
    "end": "201680"
  },
  {
    "text": "so in this scenario a user's predefined",
    "start": "201680",
    "end": "204159"
  },
  {
    "text": "trained models",
    "start": "204159",
    "end": "205440"
  },
  {
    "text": "can then be deployed to the edge",
    "start": "205440",
    "end": "207920"
  },
  {
    "text": "additionally customers can train the",
    "start": "207920",
    "end": "209440"
  },
  {
    "text": "models on the edge as well",
    "start": "209440",
    "end": "211599"
  },
  {
    "text": "in real time by capturing the data and",
    "start": "211599",
    "end": "213680"
  },
  {
    "text": "letting the models",
    "start": "213680",
    "end": "215120"
  },
  {
    "text": "and having the models be retrained as",
    "start": "215120",
    "end": "217440"
  },
  {
    "text": "they operate at the edge",
    "start": "217440",
    "end": "220400"
  },
  {
    "text": "and so",
    "start": "220400",
    "end": "221360"
  },
  {
    "start": "221000",
    "end": "221000"
  },
  {
    "text": "as we look at the machine learning life",
    "start": "221360",
    "end": "222799"
  },
  {
    "text": "cycle we see that",
    "start": "222799",
    "end": "224239"
  },
  {
    "text": "ml models are kind of constantly being",
    "start": "224239",
    "end": "226640"
  },
  {
    "text": "updated you know either your training",
    "start": "226640",
    "end": "228080"
  },
  {
    "text": "your retraining and deployment and so",
    "start": "228080",
    "end": "231200"
  },
  {
    "text": "model deployment is a very key aspect um",
    "start": "231200",
    "end": "234159"
  },
  {
    "text": "as without deployment how else will you",
    "start": "234159",
    "end": "236239"
  },
  {
    "text": "consume the model and you know bring ai",
    "start": "236239",
    "end": "238720"
  },
  {
    "text": "to your systems",
    "start": "238720",
    "end": "241760"
  },
  {
    "text": "and so as it turns out actually doing",
    "start": "241760",
    "end": "243680"
  },
  {
    "start": "242000",
    "end": "242000"
  },
  {
    "text": "production grade",
    "start": "243680",
    "end": "245040"
  },
  {
    "text": "uh model deployment and inference is",
    "start": "245040",
    "end": "246959"
  },
  {
    "text": "actually you know plagued with",
    "start": "246959",
    "end": "248720"
  },
  {
    "text": "complexities",
    "start": "248720",
    "end": "250400"
  },
  {
    "text": "um",
    "start": "250400",
    "end": "251280"
  },
  {
    "text": "there are quite a number of things to",
    "start": "251280",
    "end": "252560"
  },
  {
    "text": "consider and you know here are some of",
    "start": "252560",
    "end": "254480"
  },
  {
    "text": "the questions that the user might have",
    "start": "254480",
    "end": "255760"
  },
  {
    "text": "to navigate",
    "start": "255760",
    "end": "257199"
  },
  {
    "text": "when considering model serving",
    "start": "257199",
    "end": "258720"
  },
  {
    "text": "approaches",
    "start": "258720",
    "end": "261280"
  },
  {
    "text": "so but again the focus for today is",
    "start": "262960",
    "end": "265120"
  },
  {
    "text": "model serving at the edge on you know",
    "start": "265120",
    "end": "267120"
  },
  {
    "text": "resource constrained devices you know",
    "start": "267120",
    "end": "269680"
  },
  {
    "text": "single board computers or and systems on",
    "start": "269680",
    "end": "272080"
  },
  {
    "text": "modules",
    "start": "272080",
    "end": "273120"
  },
  {
    "text": "um sometimes you know it's a necessity",
    "start": "273120",
    "end": "275600"
  },
  {
    "text": "to use uh on-premise and distributed um",
    "start": "275600",
    "end": "278720"
  },
  {
    "text": "compute resources that are close to the",
    "start": "278720",
    "end": "280240"
  },
  {
    "text": "end users",
    "start": "280240",
    "end": "282880"
  },
  {
    "text": "so user edge deployment in this sense",
    "start": "283040",
    "end": "285120"
  },
  {
    "text": "comes with",
    "start": "285120",
    "end": "286240"
  },
  {
    "text": "like many benefits",
    "start": "286240",
    "end": "287840"
  },
  {
    "text": "data locality is the first one you know",
    "start": "287840",
    "end": "290560"
  },
  {
    "text": "we perform inference where the data is",
    "start": "290560",
    "end": "293360"
  },
  {
    "text": "you know this ties in the quicker",
    "start": "293360",
    "end": "294800"
  },
  {
    "text": "inference response times and",
    "start": "294800",
    "end": "297520"
  },
  {
    "text": "bandwidth consumption savings",
    "start": "297520",
    "end": "300080"
  },
  {
    "text": "so you no longer have to",
    "start": "300080",
    "end": "302240"
  },
  {
    "text": "necessarily send your data or your you",
    "start": "302240",
    "end": "304560"
  },
  {
    "text": "know your inference payload to the cloud",
    "start": "304560",
    "end": "306800"
  },
  {
    "text": "or across some expansive network",
    "start": "306800",
    "end": "310639"
  },
  {
    "text": "so and this also has the benefits of",
    "start": "310639",
    "end": "312479"
  },
  {
    "text": "increased security and you know data",
    "start": "312479",
    "end": "314479"
  },
  {
    "text": "privacy",
    "start": "314479",
    "end": "316560"
  },
  {
    "text": "so",
    "start": "316560",
    "end": "317600"
  },
  {
    "text": "remember we still have those",
    "start": "317600",
    "end": "318720"
  },
  {
    "text": "complexities we want to tackle those",
    "start": "318720",
    "end": "320560"
  },
  {
    "text": "complexities we saw on the on the",
    "start": "320560",
    "end": "322240"
  },
  {
    "text": "previous slide so how do we handle these",
    "start": "322240",
    "end": "323919"
  },
  {
    "text": "complexities at the edge and so",
    "start": "323919",
    "end": "326240"
  },
  {
    "text": "one of the ways is of course kubernetes",
    "start": "326240",
    "end": "328160"
  },
  {
    "start": "328000",
    "end": "328000"
  },
  {
    "text": "at the edge and that's",
    "start": "328160",
    "end": "329840"
  },
  {
    "text": "kind of the",
    "start": "329840",
    "end": "330960"
  },
  {
    "text": "topic for today so",
    "start": "330960",
    "end": "334320"
  },
  {
    "text": "so we want to leverage the orchestration",
    "start": "334320",
    "end": "336240"
  },
  {
    "text": "capabilities of kubernetes at the edge",
    "start": "336240",
    "end": "338800"
  },
  {
    "text": "okay so as you all may know there are",
    "start": "338800",
    "end": "341440"
  },
  {
    "text": "several ways to use kubernetes at the",
    "start": "341440",
    "end": "343199"
  },
  {
    "text": "edge um you can deploy a whole",
    "start": "343199",
    "end": "345120"
  },
  {
    "text": "lightweight cluster on the edge devices",
    "start": "345120",
    "end": "348320"
  },
  {
    "text": "using things like k3s micro k8 and",
    "start": "348320",
    "end": "352080"
  },
  {
    "text": "something called micro shift",
    "start": "352080",
    "end": "353919"
  },
  {
    "text": "which is kind of a relatively new",
    "start": "353919",
    "end": "356000"
  },
  {
    "text": "project that",
    "start": "356000",
    "end": "357280"
  },
  {
    "text": "offers a small form factor",
    "start": "357280",
    "end": "359840"
  },
  {
    "text": "open shift designed for field deployed",
    "start": "359840",
    "end": "363280"
  },
  {
    "text": "low resource devices",
    "start": "363280",
    "end": "366319"
  },
  {
    "text": "and so",
    "start": "366319",
    "end": "367280"
  },
  {
    "text": "multiple edge clusters can be deployed",
    "start": "367280",
    "end": "370160"
  },
  {
    "text": "and you can use something like",
    "start": "370160",
    "end": "372400"
  },
  {
    "text": "cluster management tools such as open",
    "start": "372400",
    "end": "374560"
  },
  {
    "text": "cluster management to manage each",
    "start": "374560",
    "end": "377199"
  },
  {
    "text": "individual cluster",
    "start": "377199",
    "end": "380639"
  },
  {
    "text": "and then there's the option of",
    "start": "380639",
    "end": "383840"
  },
  {
    "text": "having the control plane on some cloud",
    "start": "384000",
    "end": "386080"
  },
  {
    "text": "somewhere and",
    "start": "386080",
    "end": "388319"
  },
  {
    "text": "this will be your main kubernetes",
    "start": "388319",
    "end": "389759"
  },
  {
    "text": "cluster and then you add edge nodes",
    "start": "389759",
    "end": "392400"
  },
  {
    "text": "managed by something like kubedge",
    "start": "392400",
    "end": "395520"
  },
  {
    "text": "and so these edgeworker nodes will be",
    "start": "395520",
    "end": "397840"
  },
  {
    "text": "available for",
    "start": "397840",
    "end": "399039"
  },
  {
    "text": "deployment",
    "start": "399039",
    "end": "401680"
  },
  {
    "text": "and so so for us i'm going to go with",
    "start": "401680",
    "end": "404000"
  },
  {
    "text": "the edge deployed kubernetes clusters to",
    "start": "404000",
    "end": "405680"
  },
  {
    "text": "keep everything",
    "start": "405680",
    "end": "406960"
  },
  {
    "text": "at the edge and you know i guess one",
    "start": "406960",
    "end": "408880"
  },
  {
    "text": "typical approach for",
    "start": "408880",
    "end": "411280"
  },
  {
    "text": "deploying apps or models on the edge is",
    "start": "411280",
    "end": "413440"
  },
  {
    "text": "to just containerize the model server",
    "start": "413440",
    "end": "416479"
  },
  {
    "text": "and create a kubernetes deployment",
    "start": "416479",
    "end": "418720"
  },
  {
    "text": "and maybe you might mount the volume and",
    "start": "418720",
    "end": "421280"
  },
  {
    "text": "which contains your model files your",
    "start": "421280",
    "end": "422880"
  },
  {
    "text": "assets",
    "start": "422880",
    "end": "424000"
  },
  {
    "text": "and uh",
    "start": "424000",
    "end": "425840"
  },
  {
    "text": "eventually you'll do a coupe code to",
    "start": "425840",
    "end": "427199"
  },
  {
    "text": "apply and deploy the actual",
    "start": "427199",
    "end": "430240"
  },
  {
    "text": "containers so this is essentially what",
    "start": "430240",
    "end": "432720"
  },
  {
    "text": "other model serving platforms you know",
    "start": "432720",
    "end": "434800"
  },
  {
    "text": "what i guess most kubernetes based uh",
    "start": "434800",
    "end": "437120"
  },
  {
    "text": "model serving platforms and never",
    "start": "437120",
    "end": "438560"
  },
  {
    "text": "inevitably do",
    "start": "438560",
    "end": "440080"
  },
  {
    "text": "um however this can be quite cumbersome",
    "start": "440080",
    "end": "442080"
  },
  {
    "text": "especially when you're dealing with a",
    "start": "442080",
    "end": "443280"
  },
  {
    "text": "lot of model frameworks um",
    "start": "443280",
    "end": "445599"
  },
  {
    "text": "different",
    "start": "445599",
    "end": "446560"
  },
  {
    "text": "like tensorflow and pi torch each have",
    "start": "446560",
    "end": "448800"
  },
  {
    "text": "their own images and different arguments",
    "start": "448800",
    "end": "451039"
  },
  {
    "text": "you have to worry about",
    "start": "451039",
    "end": "452560"
  },
  {
    "text": "um",
    "start": "452560",
    "end": "454319"
  },
  {
    "text": "so it's kind of like a rabbit hole you",
    "start": "454319",
    "end": "456160"
  },
  {
    "text": "have to jump in and might be a little",
    "start": "456160",
    "end": "458000"
  },
  {
    "text": "daunting",
    "start": "458000",
    "end": "459199"
  },
  {
    "text": "and so",
    "start": "459199",
    "end": "460800"
  },
  {
    "text": "you might lose out on certain aspects",
    "start": "460800",
    "end": "462960"
  },
  {
    "text": "like it's the feature richness of like",
    "start": "462960",
    "end": "465360"
  },
  {
    "text": "scale to zero or",
    "start": "465360",
    "end": "467440"
  },
  {
    "text": "um",
    "start": "467440",
    "end": "468639"
  },
  {
    "text": "you might",
    "start": "468639",
    "end": "469759"
  },
  {
    "text": "have varying inference request",
    "start": "469759",
    "end": "472240"
  },
  {
    "text": "formats or protocols that are used on",
    "start": "472240",
    "end": "475199"
  },
  {
    "text": "different model servers which can be",
    "start": "475199",
    "end": "477199"
  },
  {
    "text": "annoying so can traditional model",
    "start": "477199",
    "end": "479759"
  },
  {
    "text": "serving platforms for kubernetes extend",
    "start": "479759",
    "end": "482960"
  },
  {
    "text": "to edge devices and so",
    "start": "482960",
    "end": "486000"
  },
  {
    "start": "486000",
    "end": "486000"
  },
  {
    "text": "first i'm going to introduce k-serve and",
    "start": "486000",
    "end": "488080"
  },
  {
    "text": "so k-serve",
    "start": "488080",
    "end": "491198"
  },
  {
    "text": "started off in the kubeflow umbrella as",
    "start": "491280",
    "end": "493360"
  },
  {
    "text": "the model serving platform for kubeflow",
    "start": "493360",
    "end": "498159"
  },
  {
    "text": "so",
    "start": "498319",
    "end": "499039"
  },
  {
    "text": "it is now a incubating project in the",
    "start": "499039",
    "end": "501840"
  },
  {
    "text": "linux",
    "start": "501840",
    "end": "503039"
  },
  {
    "text": "the lfai and data foundation",
    "start": "503039",
    "end": "505599"
  },
  {
    "text": "and so",
    "start": "505599",
    "end": "506879"
  },
  {
    "text": "what it is it's a",
    "start": "506879",
    "end": "508479"
  },
  {
    "text": "highly scalable and standards-based",
    "start": "508479",
    "end": "510160"
  },
  {
    "text": "model inference platform on kubernetes",
    "start": "510160",
    "end": "511919"
  },
  {
    "text": "for trusted ai",
    "start": "511919",
    "end": "514240"
  },
  {
    "text": "typically i mean traditionally deploy",
    "start": "514240",
    "end": "516959"
  },
  {
    "text": "this on the cloud",
    "start": "516959",
    "end": "518800"
  },
  {
    "text": "you know you have",
    "start": "518800",
    "end": "520479"
  },
  {
    "text": "underlying underlying case serve is k",
    "start": "520479",
    "end": "522479"
  },
  {
    "text": "native n is to this is an optional layer",
    "start": "522479",
    "end": "525279"
  },
  {
    "text": "um you can either deploy models using k",
    "start": "525279",
    "end": "527680"
  },
  {
    "text": "native or you can",
    "start": "527680",
    "end": "529200"
  },
  {
    "text": "use a option uh raw deployment mode",
    "start": "529200",
    "end": "531440"
  },
  {
    "text": "where you just deploy models using",
    "start": "531440",
    "end": "535360"
  },
  {
    "text": "just standard",
    "start": "535360",
    "end": "536880"
  },
  {
    "text": "kubernetes resources like deployments",
    "start": "536880",
    "end": "539040"
  },
  {
    "text": "ingresses and services",
    "start": "539040",
    "end": "541920"
  },
  {
    "text": "so",
    "start": "541920",
    "end": "542800"
  },
  {
    "text": "some of the benefits of k-serve it",
    "start": "542800",
    "end": "544800"
  },
  {
    "start": "544000",
    "end": "544000"
  },
  {
    "text": "provides an easy to use interface and or",
    "start": "544800",
    "end": "547200"
  },
  {
    "text": "crd called an infant service",
    "start": "547200",
    "end": "550000"
  },
  {
    "text": "for deploying models given a format and",
    "start": "550000",
    "end": "553680"
  },
  {
    "text": "uh",
    "start": "553680",
    "end": "554399"
  },
  {
    "text": "storage endpoint",
    "start": "554399",
    "end": "556240"
  },
  {
    "text": "so typically you just need to provide",
    "start": "556240",
    "end": "557760"
  },
  {
    "text": "these two items and you can give the",
    "start": "557760",
    "end": "559680"
  },
  {
    "text": "k-serve and k-serve will know what to do",
    "start": "559680",
    "end": "561360"
  },
  {
    "text": "with it we'll find the appropriate",
    "start": "561360",
    "end": "563519"
  },
  {
    "text": "appropriate images to load",
    "start": "563519",
    "end": "567040"
  },
  {
    "text": "and we'll load it into the container",
    "start": "567200",
    "end": "570640"
  },
  {
    "start": "572000",
    "end": "572000"
  },
  {
    "text": "another another another advantage of",
    "start": "572080",
    "end": "574160"
  },
  {
    "text": "k-serve is that it revolves around a",
    "start": "574160",
    "end": "576320"
  },
  {
    "text": "standardized inference protocol okay so",
    "start": "576320",
    "end": "578880"
  },
  {
    "text": "this is something that the community has",
    "start": "578880",
    "end": "580399"
  },
  {
    "text": "helped shape um people from you know",
    "start": "580399",
    "end": "583120"
  },
  {
    "text": "other model servers like nvidia i think",
    "start": "583120",
    "end": "584880"
  },
  {
    "text": "pi torch torch serve supports this",
    "start": "584880",
    "end": "586880"
  },
  {
    "text": "protocol and selden's ml server and so",
    "start": "586880",
    "end": "590640"
  },
  {
    "text": "by implementing this protocol both",
    "start": "590640",
    "end": "592240"
  },
  {
    "text": "inference clients and servers will",
    "start": "592240",
    "end": "594480"
  },
  {
    "text": "our inference clients and servers will",
    "start": "594480",
    "end": "596399"
  },
  {
    "text": "increase their kind of their utility in",
    "start": "596399",
    "end": "598959"
  },
  {
    "text": "portability",
    "start": "598959",
    "end": "600399"
  },
  {
    "text": "um",
    "start": "600399",
    "end": "601519"
  },
  {
    "text": "you know can you can see me seamlessly",
    "start": "601519",
    "end": "603760"
  },
  {
    "text": "integrate on other platforms and perform",
    "start": "603760",
    "end": "606880"
  },
  {
    "text": "standardized inference using this",
    "start": "606880",
    "end": "608399"
  },
  {
    "text": "protocol",
    "start": "608399",
    "end": "610800"
  },
  {
    "text": "so yeah so some of the main ones are",
    "start": "610800",
    "end": "612959"
  },
  {
    "text": "triton server",
    "start": "612959",
    "end": "614399"
  },
  {
    "text": "pi torch torch serve and then ml server",
    "start": "614399",
    "end": "617600"
  },
  {
    "text": "and these are some of the standardized",
    "start": "617600",
    "end": "619519"
  },
  {
    "text": "rest or grpc endpoints",
    "start": "619519",
    "end": "623600"
  },
  {
    "text": "so with this",
    "start": "624160",
    "end": "625440"
  },
  {
    "text": "case of rings",
    "start": "625440",
    "end": "626800"
  },
  {
    "text": "it's not necessarily",
    "start": "626800",
    "end": "628560"
  },
  {
    "text": "good for edge",
    "start": "628560",
    "end": "630480"
  },
  {
    "text": "you know there's resource overhead",
    "start": "630480",
    "end": "631600"
  },
  {
    "text": "because of you know side cars might be",
    "start": "631600",
    "end": "633839"
  },
  {
    "text": "injected",
    "start": "633839",
    "end": "634880"
  },
  {
    "text": "into each pod you know",
    "start": "634880",
    "end": "636720"
  },
  {
    "text": "having an independent model server uh",
    "start": "636720",
    "end": "639360"
  },
  {
    "text": "for or having independent model server",
    "start": "639360",
    "end": "641120"
  },
  {
    "text": "per model",
    "start": "641120",
    "end": "642399"
  },
  {
    "text": "or a model per pod um that's kind of",
    "start": "642399",
    "end": "645360"
  },
  {
    "text": "you really um",
    "start": "645360",
    "end": "648160"
  },
  {
    "text": "really",
    "start": "648160",
    "end": "648959"
  },
  {
    "text": "kind of",
    "start": "648959",
    "end": "650640"
  },
  {
    "text": "it's a lot of resource consumption being",
    "start": "650640",
    "end": "653200"
  },
  {
    "text": "being done okay",
    "start": "653200",
    "end": "654880"
  },
  {
    "text": "so it doesn't make the best use of",
    "start": "654880",
    "end": "656959"
  },
  {
    "text": "you're already resource constrained on",
    "start": "656959",
    "end": "658640"
  },
  {
    "text": "these edge devices and so",
    "start": "658640",
    "end": "660399"
  },
  {
    "text": "we need a way where we need to where we",
    "start": "660399",
    "end": "662160"
  },
  {
    "text": "can serve multiple models in a singular",
    "start": "662160",
    "end": "664640"
  },
  {
    "text": "pod or container",
    "start": "664640",
    "end": "666320"
  },
  {
    "text": "and that's where k serves uh multi-model",
    "start": "666320",
    "end": "669360"
  },
  {
    "start": "669000",
    "end": "669000"
  },
  {
    "text": "serving back-end for k-serve",
    "start": "669360",
    "end": "671519"
  },
  {
    "text": "or for",
    "start": "671519",
    "end": "672519"
  },
  {
    "text": "multi-multi-model back-end",
    "start": "672519",
    "end": "674880"
  },
  {
    "text": "comes into play",
    "start": "674880",
    "end": "676240"
  },
  {
    "text": "so this is a project that was",
    "start": "676240",
    "end": "678560"
  },
  {
    "text": "open sourced by ibm last year and has",
    "start": "678560",
    "end": "681680"
  },
  {
    "text": "joined the case of organization",
    "start": "681680",
    "end": "685200"
  },
  {
    "text": "so",
    "start": "685279",
    "end": "686079"
  },
  {
    "text": "as users were getting hit with these",
    "start": "686079",
    "end": "688240"
  },
  {
    "text": "these scalability issues with the",
    "start": "688240",
    "end": "690000"
  },
  {
    "text": "traditional model serving on k native um",
    "start": "690000",
    "end": "694320"
  },
  {
    "text": "we decided we needed to open source or",
    "start": "694320",
    "end": "697200"
  },
  {
    "text": "consolidate on a",
    "start": "697200",
    "end": "698800"
  },
  {
    "text": "on a path or approach for handle",
    "start": "698800",
    "end": "701279"
  },
  {
    "text": "multi-model serving aka the having",
    "start": "701279",
    "end": "704720"
  },
  {
    "text": "multiple models inside a container",
    "start": "704720",
    "end": "708240"
  },
  {
    "text": "so",
    "start": "708240",
    "end": "710399"
  },
  {
    "text": "model mesh is a platform yeah it's been",
    "start": "710399",
    "end": "712480"
  },
  {
    "text": "running inside ibm or in production for",
    "start": "712480",
    "end": "714800"
  },
  {
    "text": "quite a few years now and you know it's",
    "start": "714800",
    "end": "716800"
  },
  {
    "text": "the backbone for quite a few of watson's",
    "start": "716800",
    "end": "718720"
  },
  {
    "text": "services um",
    "start": "718720",
    "end": "720399"
  },
  {
    "text": "yeah watson assistant watson and natural",
    "start": "720399",
    "end": "722399"
  },
  {
    "text": "language understanding and watson",
    "start": "722399",
    "end": "724160"
  },
  {
    "text": "discovery",
    "start": "724160",
    "end": "725920"
  },
  {
    "text": "and so",
    "start": "725920",
    "end": "727440"
  },
  {
    "text": "model mesh allows for you know",
    "start": "727440",
    "end": "731600"
  },
  {
    "text": "multiple models per container but it",
    "start": "731600",
    "end": "733680"
  },
  {
    "text": "allows models to be paged out if it's",
    "start": "733680",
    "end": "735680"
  },
  {
    "text": "not being used",
    "start": "735680",
    "end": "737680"
  },
  {
    "text": "or",
    "start": "737680",
    "end": "738560"
  },
  {
    "text": "loaded just in time if a request comes",
    "start": "738560",
    "end": "740800"
  },
  {
    "text": "in that he needs that model",
    "start": "740800",
    "end": "743360"
  },
  {
    "text": "it has some parallels to k native and",
    "start": "743360",
    "end": "745200"
  },
  {
    "text": "serverless but",
    "start": "745200",
    "end": "746720"
  },
  {
    "text": "um",
    "start": "746720",
    "end": "748160"
  },
  {
    "text": "it's kind of just relegated to a single",
    "start": "748160",
    "end": "751200"
  },
  {
    "text": "container",
    "start": "751200",
    "end": "753760"
  },
  {
    "text": "so yeah so it does strike an intelligent",
    "start": "753920",
    "end": "756399"
  },
  {
    "text": "trade-off between responsiveness to",
    "start": "756399",
    "end": "757920"
  },
  {
    "text": "users and their computational footprint",
    "start": "757920",
    "end": "760320"
  },
  {
    "text": "so i would say it does make the use",
    "start": "760320",
    "end": "762639"
  },
  {
    "text": "makes excellent use of resources",
    "start": "762639",
    "end": "766399"
  },
  {
    "text": "on your cluster",
    "start": "766560",
    "end": "768560"
  },
  {
    "text": "so the overall architecture might look",
    "start": "768560",
    "end": "770240"
  },
  {
    "start": "769000",
    "end": "769000"
  },
  {
    "text": "something like this so when the user",
    "start": "770240",
    "end": "771760"
  },
  {
    "text": "applies an infant service yaml",
    "start": "771760",
    "end": "774399"
  },
  {
    "text": "containing the model details",
    "start": "774399",
    "end": "777519"
  },
  {
    "text": "the model mesh controller will select",
    "start": "777519",
    "end": "778880"
  },
  {
    "text": "the suitable serving runtime pod in",
    "start": "778880",
    "end": "781120"
  },
  {
    "text": "which to host the model",
    "start": "781120",
    "end": "782959"
  },
  {
    "text": "so the pods for these serving runtimes",
    "start": "782959",
    "end": "784720"
  },
  {
    "text": "that's kind of the important part where",
    "start": "784720",
    "end": "786800"
  },
  {
    "text": "you see the green and orange",
    "start": "786800",
    "end": "789680"
  },
  {
    "text": "so these pods are",
    "start": "789680",
    "end": "793120"
  },
  {
    "text": "typically they contain three containers",
    "start": "793120",
    "end": "794800"
  },
  {
    "text": "you have your the model server container",
    "start": "794800",
    "end": "796959"
  },
  {
    "text": "which is typically a third part of the",
    "start": "796959",
    "end": "798560"
  },
  {
    "text": "infant service or server like triton or",
    "start": "798560",
    "end": "800720"
  },
  {
    "text": "ml server",
    "start": "800720",
    "end": "801920"
  },
  {
    "text": "these",
    "start": "801920",
    "end": "802800"
  },
  {
    "text": "support loading multiple models",
    "start": "802800",
    "end": "805200"
  },
  {
    "text": "and have unload and loading endpoints",
    "start": "805200",
    "end": "807519"
  },
  {
    "text": "which we can use",
    "start": "807519",
    "end": "809279"
  },
  {
    "text": "to kind of dynamically",
    "start": "809279",
    "end": "812079"
  },
  {
    "text": "alter which models are deployed and so",
    "start": "812079",
    "end": "815040"
  },
  {
    "text": "then we have the model mesh sidecar",
    "start": "815040",
    "end": "816800"
  },
  {
    "text": "container which handles the model",
    "start": "816800",
    "end": "818720"
  },
  {
    "text": "management",
    "start": "818720",
    "end": "819920"
  },
  {
    "text": "and handles both control and data plane",
    "start": "819920",
    "end": "822959"
  },
  {
    "text": "request routing",
    "start": "822959",
    "end": "825440"
  },
  {
    "text": "then we have the puller which handles",
    "start": "825440",
    "end": "827920"
  },
  {
    "text": "pulling models from external object",
    "start": "827920",
    "end": "829680"
  },
  {
    "text": "stores or",
    "start": "829680",
    "end": "831120"
  },
  {
    "text": "endpoints",
    "start": "831120",
    "end": "832800"
  },
  {
    "text": "so this handle is pulling it into the",
    "start": "832800",
    "end": "834800"
  },
  {
    "text": "actual cluster pulling model files into",
    "start": "834800",
    "end": "836639"
  },
  {
    "text": "the cluster",
    "start": "836639",
    "end": "838079"
  },
  {
    "text": "so a single kubernetes service at the",
    "start": "838079",
    "end": "840000"
  },
  {
    "text": "top points to all pods across all",
    "start": "840000",
    "end": "842079"
  },
  {
    "text": "deployments",
    "start": "842079",
    "end": "843360"
  },
  {
    "text": "and",
    "start": "843360",
    "end": "844480"
  },
  {
    "text": "external inferencing requests are made",
    "start": "844480",
    "end": "846320"
  },
  {
    "text": "via the service and the",
    "start": "846320",
    "end": "848880"
  },
  {
    "text": "whichever ingress model mesh pod",
    "start": "848880",
    "end": "853199"
  },
  {
    "text": "it hits",
    "start": "853440",
    "end": "854800"
  },
  {
    "text": "model mesh will determine where the",
    "start": "854800",
    "end": "856160"
  },
  {
    "text": "model is actually load which actual",
    "start": "856160",
    "end": "858240"
  },
  {
    "text": "model server the model is actually",
    "start": "858240",
    "end": "860000"
  },
  {
    "text": "deployed in and will route the request",
    "start": "860000",
    "end": "862320"
  },
  {
    "text": "as needed",
    "start": "862320",
    "end": "863839"
  },
  {
    "text": "so",
    "start": "863839",
    "end": "865120"
  },
  {
    "text": "just a side note ncd is used as uh the",
    "start": "865120",
    "end": "868480"
  },
  {
    "text": "coordinate operations and you know",
    "start": "868480",
    "end": "869760"
  },
  {
    "text": "persist model and instant states",
    "start": "869760",
    "end": "873600"
  },
  {
    "start": "873000",
    "end": "873000"
  },
  {
    "text": "yeah so like i said the model servers",
    "start": "874160",
    "end": "875760"
  },
  {
    "text": "are kind of an integral part of model",
    "start": "875760",
    "end": "878079"
  },
  {
    "text": "mesh so",
    "start": "878079",
    "end": "880160"
  },
  {
    "text": "k serve or model mesh has serving",
    "start": "880160",
    "end": "882160"
  },
  {
    "text": "runtimes this is a crd that",
    "start": "882160",
    "end": "885519"
  },
  {
    "text": "is used for defining model serving",
    "start": "885519",
    "end": "887839"
  },
  {
    "text": "environments and which container images",
    "start": "887839",
    "end": "890000"
  },
  {
    "text": "should be used or loaded and you know",
    "start": "890000",
    "end": "892160"
  },
  {
    "text": "what the supported model formats are",
    "start": "892160",
    "end": "895279"
  },
  {
    "text": "so currently out of the box",
    "start": "895279",
    "end": "897519"
  },
  {
    "text": "these are the two main ones triton",
    "start": "897519",
    "end": "899120"
  },
  {
    "text": "infant server and ml server",
    "start": "899120",
    "end": "902560"
  },
  {
    "text": "and so you can pretty much use anything",
    "start": "903279",
    "end": "905279"
  },
  {
    "text": "i think we're working on we have",
    "start": "905279",
    "end": "906880"
  },
  {
    "text": "openvino and we have",
    "start": "906880",
    "end": "909519"
  },
  {
    "text": "i think",
    "start": "909519",
    "end": "911360"
  },
  {
    "text": "serve is in the works but",
    "start": "911360",
    "end": "913440"
  },
  {
    "text": "as long as it supports dynamic loading",
    "start": "913440",
    "end": "915199"
  },
  {
    "text": "and unloading",
    "start": "915199",
    "end": "917040"
  },
  {
    "text": "model mesh should be compatible with it",
    "start": "917040",
    "end": "919680"
  },
  {
    "text": "and so",
    "start": "919680",
    "end": "921600"
  },
  {
    "start": "921000",
    "end": "921000"
  },
  {
    "text": "as mentioned model mesh has been running",
    "start": "921600",
    "end": "923360"
  },
  {
    "text": "in production cloud environments for",
    "start": "923360",
    "end": "924880"
  },
  {
    "text": "quite a while so the question that",
    "start": "924880",
    "end": "927600"
  },
  {
    "text": "i guess i was thinking about was is it",
    "start": "927600",
    "end": "929199"
  },
  {
    "text": "tenable to bring it",
    "start": "929199",
    "end": "930800"
  },
  {
    "text": "you know on the kubernetes on the edge",
    "start": "930800",
    "end": "933040"
  },
  {
    "text": "so let's talk about some of the",
    "start": "933040",
    "end": "934560"
  },
  {
    "text": "advantage",
    "start": "934560",
    "end": "935519"
  },
  {
    "text": "advantages it might provide",
    "start": "935519",
    "end": "937839"
  },
  {
    "text": "so first",
    "start": "937839",
    "end": "939680"
  },
  {
    "text": "through k-serve's infant service",
    "start": "939680",
    "end": "941120"
  },
  {
    "text": "interface users are able to easily",
    "start": "941120",
    "end": "943680"
  },
  {
    "text": "deploy multiple models",
    "start": "943680",
    "end": "945839"
  },
  {
    "text": "into singular serving runtime containers",
    "start": "945839",
    "end": "947839"
  },
  {
    "text": "or pods",
    "start": "947839",
    "end": "949199"
  },
  {
    "text": "which drastically reduces the resource",
    "start": "949199",
    "end": "950959"
  },
  {
    "text": "overhead compared to the single mod a",
    "start": "950959",
    "end": "953519"
  },
  {
    "text": "single model per pod",
    "start": "953519",
    "end": "956320"
  },
  {
    "text": "pattern that we were",
    "start": "956320",
    "end": "957680"
  },
  {
    "text": "that is typical with traditional model",
    "start": "957680",
    "end": "959680"
  },
  {
    "text": "serving",
    "start": "959680",
    "end": "962079"
  },
  {
    "text": "so each pod typically has you know",
    "start": "962079",
    "end": "963680"
  },
  {
    "text": "resource requests and",
    "start": "963680",
    "end": "966000"
  },
  {
    "text": "it'll be quite easy to hit allocation",
    "start": "966000",
    "end": "967759"
  },
  {
    "text": "limits when dealing with even just a few",
    "start": "967759",
    "end": "970320"
  },
  {
    "text": "models on resource constrained",
    "start": "970320",
    "end": "972720"
  },
  {
    "text": "devices",
    "start": "972720",
    "end": "975199"
  },
  {
    "text": "so another thing we",
    "start": "975519",
    "end": "978399"
  },
  {
    "text": "pay attention or that model mesh is",
    "start": "978399",
    "end": "981440"
  },
  {
    "text": "of the key features of model mesh is its",
    "start": "981440",
    "end": "983279"
  },
  {
    "text": "whole aspect of cache management and",
    "start": "983279",
    "end": "986880"
  },
  {
    "text": "and to some extent high availability",
    "start": "986880",
    "end": "988959"
  },
  {
    "text": "right so",
    "start": "988959",
    "end": "990399"
  },
  {
    "text": "malo mesh treats the set of",
    "start": "990399",
    "end": "992000"
  },
  {
    "text": "pre-provisioned pods on the kubernetes",
    "start": "992000",
    "end": "994720"
  },
  {
    "text": "cluster as an lru cache and so malamesh",
    "start": "994720",
    "end": "998240"
  },
  {
    "text": "decides which models are loaded or",
    "start": "998240",
    "end": "1000399"
  },
  {
    "text": "unloaded based on usage recency",
    "start": "1000399",
    "end": "1003440"
  },
  {
    "text": "or current request volumes so you know",
    "start": "1003440",
    "end": "1006480"
  },
  {
    "text": "if you have multiple edge nodes a model",
    "start": "1006480",
    "end": "1008320"
  },
  {
    "text": "might be scaled out to have copies on",
    "start": "1008320",
    "end": "1011040"
  },
  {
    "text": "each of the nodes if there are serving",
    "start": "1011040",
    "end": "1012880"
  },
  {
    "text": "runtimes available",
    "start": "1012880",
    "end": "1014560"
  },
  {
    "text": "and so",
    "start": "1014560",
    "end": "1015920"
  },
  {
    "text": "if a",
    "start": "1015920",
    "end": "1016959"
  },
  {
    "text": "specific model is",
    "start": "1016959",
    "end": "1018720"
  },
  {
    "text": "getting is getting swamped with traffic",
    "start": "1018720",
    "end": "1020720"
  },
  {
    "text": "it might scale out to might",
    "start": "1020720",
    "end": "1022800"
  },
  {
    "text": "produce model mesh might decide hey",
    "start": "1022800",
    "end": "1024319"
  },
  {
    "text": "let's load this into other of two",
    "start": "1024319",
    "end": "1026720"
  },
  {
    "text": "additional serving runtime pods to kind",
    "start": "1026720",
    "end": "1028959"
  },
  {
    "text": "of create more",
    "start": "1028959",
    "end": "1030480"
  },
  {
    "text": "um",
    "start": "1030480",
    "end": "1031760"
  },
  {
    "text": "uh availability for that model",
    "start": "1031760",
    "end": "1035038"
  },
  {
    "text": "but if a loaded model hasn't received",
    "start": "1035039",
    "end": "1036798"
  },
  {
    "text": "any traffic in the while and the new",
    "start": "1036799",
    "end": "1038240"
  },
  {
    "text": "model comes in",
    "start": "1038240",
    "end": "1039600"
  },
  {
    "text": "and the cache happens to be full",
    "start": "1039600",
    "end": "1043199"
  },
  {
    "text": "the least recently used model will be",
    "start": "1043199",
    "end": "1045199"
  },
  {
    "text": "evicted from the cache that means",
    "start": "1045199",
    "end": "1047038"
  },
  {
    "text": "unloaded from the model server's memory",
    "start": "1047039",
    "end": "1049840"
  },
  {
    "text": "to make room for the new model to be",
    "start": "1049840",
    "end": "1051280"
  },
  {
    "text": "loaded and used",
    "start": "1051280",
    "end": "1053840"
  },
  {
    "text": "yeah so with this cache management you",
    "start": "1053840",
    "end": "1055760"
  },
  {
    "text": "know model mesh works well for you know",
    "start": "1055760",
    "end": "1058240"
  },
  {
    "text": "handling",
    "start": "1058240",
    "end": "1059679"
  },
  {
    "text": "uh unevenly distributed uh",
    "start": "1059679",
    "end": "1062320"
  },
  {
    "text": "inference request load you know you",
    "start": "1062320",
    "end": "1064080"
  },
  {
    "text": "might have 20 models registered and",
    "start": "1064080",
    "end": "1066480"
  },
  {
    "text": "perhaps only five or",
    "start": "1066480",
    "end": "1068480"
  },
  {
    "text": "are more commonly used you know kind of",
    "start": "1068480",
    "end": "1070640"
  },
  {
    "text": "like the 80 20 rule where",
    "start": "1070640",
    "end": "1072720"
  },
  {
    "text": "20 of the models handle 80 percent or",
    "start": "1072720",
    "end": "1076000"
  },
  {
    "text": "you know handling 80 of the traffic",
    "start": "1076000",
    "end": "1080280"
  },
  {
    "text": "and so",
    "start": "1080320",
    "end": "1081520"
  },
  {
    "text": "some additional uh",
    "start": "1081520",
    "end": "1084240"
  },
  {
    "text": "highlights of model mesh is it's you",
    "start": "1084240",
    "end": "1086160"
  },
  {
    "text": "know intelligent placement and loading",
    "start": "1086160",
    "end": "1088000"
  },
  {
    "text": "you know we try to malamesh tries to",
    "start": "1088000",
    "end": "1089679"
  },
  {
    "text": "balance the cash age",
    "start": "1089679",
    "end": "1091280"
  },
  {
    "text": "across the pods as well as the request",
    "start": "1091280",
    "end": "1093760"
  },
  {
    "text": "to load so",
    "start": "1093760",
    "end": "1096160"
  },
  {
    "text": "again it's striking a balance trying to",
    "start": "1096160",
    "end": "1097919"
  },
  {
    "text": "keep it balanced",
    "start": "1097919",
    "end": "1099280"
  },
  {
    "text": "and there is a priority loading of",
    "start": "1099280",
    "end": "1100960"
  },
  {
    "text": "models so a model with a request waiting",
    "start": "1100960",
    "end": "1103600"
  },
  {
    "text": "for it will be bumped to the front of",
    "start": "1103600",
    "end": "1104960"
  },
  {
    "text": "the line if there is a request waiting",
    "start": "1104960",
    "end": "1107280"
  },
  {
    "text": "on it and if there is a loading queue",
    "start": "1107280",
    "end": "1110720"
  },
  {
    "text": "so this all ensures we balance ensures",
    "start": "1110720",
    "end": "1112559"
  },
  {
    "text": "we balance resource usage across nodes",
    "start": "1112559",
    "end": "1114720"
  },
  {
    "text": "or edge devices as well as improved",
    "start": "1114720",
    "end": "1117120"
  },
  {
    "text": "responsiveness",
    "start": "1117120",
    "end": "1118880"
  },
  {
    "text": "so",
    "start": "1118880",
    "end": "1119919"
  },
  {
    "text": "after having deployed models um what",
    "start": "1119919",
    "end": "1122400"
  },
  {
    "text": "about day two operations so key aspect",
    "start": "1122400",
    "end": "1124640"
  },
  {
    "text": "is the operational com uh simplicity of",
    "start": "1124640",
    "end": "1127280"
  },
  {
    "text": "model mesh you know it supports rolling",
    "start": "1127280",
    "end": "1129360"
  },
  {
    "text": "updates automatically",
    "start": "1129360",
    "end": "1130960"
  },
  {
    "text": "so if you deploy a new model",
    "start": "1130960",
    "end": "1133200"
  },
  {
    "text": "the old version of that model",
    "start": "1133200",
    "end": "1136000"
  },
  {
    "text": "will continue to receive all the traffic",
    "start": "1136000",
    "end": "1138080"
  },
  {
    "text": "until the new model is loaded into",
    "start": "1138080",
    "end": "1140000"
  },
  {
    "text": "memory and is ready for inference",
    "start": "1140000",
    "end": "1142640"
  },
  {
    "text": "and at that point the traffic will be",
    "start": "1142640",
    "end": "1144400"
  },
  {
    "text": "shifted",
    "start": "1144400",
    "end": "1146799"
  },
  {
    "text": "so yeah this all sounds great so",
    "start": "1146880",
    "end": "1149520"
  },
  {
    "start": "1148000",
    "end": "1148000"
  },
  {
    "text": "i wanted to try model mesh on edge and",
    "start": "1149520",
    "end": "1152240"
  },
  {
    "text": "that's that's what i did",
    "start": "1152240",
    "end": "1154320"
  },
  {
    "text": "so here",
    "start": "1154320",
    "end": "1156240"
  },
  {
    "text": "i",
    "start": "1156240",
    "end": "1157039"
  },
  {
    "text": "unfortunately cannot bring my jetson",
    "start": "1157039",
    "end": "1158480"
  },
  {
    "text": "nano to this event but that's a picture",
    "start": "1158480",
    "end": "1160880"
  },
  {
    "text": "of my justin nano on the right and so on",
    "start": "1160880",
    "end": "1163120"
  },
  {
    "text": "my jetson nano which has a",
    "start": "1163120",
    "end": "1165840"
  },
  {
    "text": "quad core arms you know arm 64 base",
    "start": "1165840",
    "end": "1168480"
  },
  {
    "text": "processor and four gigabytes of ram",
    "start": "1168480",
    "end": "1171440"
  },
  {
    "text": "i deployed",
    "start": "1171440",
    "end": "1172960"
  },
  {
    "text": "micro shift which is a small form factor",
    "start": "1172960",
    "end": "1175440"
  },
  {
    "text": "open shift as i mentioned",
    "start": "1175440",
    "end": "1177280"
  },
  {
    "text": "um",
    "start": "1177280",
    "end": "1178720"
  },
  {
    "text": "it's a it's a project that's currently",
    "start": "1178720",
    "end": "1180160"
  },
  {
    "text": "being developed by the red hat emerging",
    "start": "1180160",
    "end": "1182160"
  },
  {
    "text": "technologies group",
    "start": "1182160",
    "end": "1183840"
  },
  {
    "text": "um",
    "start": "1183840",
    "end": "1184960"
  },
  {
    "text": "and for those that don't know what",
    "start": "1184960",
    "end": "1186000"
  },
  {
    "text": "openshift is it's kind of it's a you",
    "start": "1186000",
    "end": "1188000"
  },
  {
    "text": "know the red hat's enterprise kubernetes",
    "start": "1188000",
    "end": "1190000"
  },
  {
    "text": "platform",
    "start": "1190000",
    "end": "1191360"
  },
  {
    "text": "so in any case microshift does work",
    "start": "1191360",
    "end": "1193520"
  },
  {
    "text": "remarkably well on this especially on",
    "start": "1193520",
    "end": "1195360"
  },
  {
    "text": "this jetson nano",
    "start": "1195360",
    "end": "1197360"
  },
  {
    "text": "it runs as a single binary and runs on",
    "start": "1197360",
    "end": "1200000"
  },
  {
    "text": "less than one gigabyte of ram and",
    "start": "1200000",
    "end": "1202720"
  },
  {
    "text": "generally less than a core a single cpu",
    "start": "1202720",
    "end": "1206240"
  },
  {
    "text": "core",
    "start": "1206240",
    "end": "1208159"
  },
  {
    "text": "so with microshift i have an all-in-one",
    "start": "1208159",
    "end": "1211520"
  },
  {
    "text": "minimal installation of kubernetes on my",
    "start": "1211520",
    "end": "1213600"
  },
  {
    "text": "jetson nano",
    "start": "1213600",
    "end": "1215120"
  },
  {
    "text": "and",
    "start": "1215120",
    "end": "1216000"
  },
  {
    "text": "on top of this i deployed model mesh as",
    "start": "1216000",
    "end": "1219120"
  },
  {
    "text": "a kind of like a stand-alone",
    "start": "1219120",
    "end": "1220880"
  },
  {
    "text": "installation",
    "start": "1220880",
    "end": "1222640"
  },
  {
    "text": "so since yeah so model mesh is a part of",
    "start": "1222640",
    "end": "1225280"
  },
  {
    "text": "the",
    "start": "1225280",
    "end": "1226000"
  },
  {
    "text": "it's part of k-serve but in this case i",
    "start": "1226000",
    "end": "1227679"
  },
  {
    "text": "don't care i didn't really care too much",
    "start": "1227679",
    "end": "1229120"
  },
  {
    "text": "about the",
    "start": "1229120",
    "end": "1230320"
  },
  {
    "text": "single model serving so i opted to not",
    "start": "1230320",
    "end": "1232559"
  },
  {
    "text": "install the k-serve controller and just",
    "start": "1232559",
    "end": "1235120"
  },
  {
    "text": "use the",
    "start": "1235120",
    "end": "1236559"
  },
  {
    "text": "model mesh controller because right now",
    "start": "1236559",
    "end": "1238320"
  },
  {
    "text": "model mesh controller is its own",
    "start": "1238320",
    "end": "1239679"
  },
  {
    "text": "standalone controller",
    "start": "1239679",
    "end": "1242400"
  },
  {
    "text": "and you know the saves saves resources",
    "start": "1242400",
    "end": "1244400"
  },
  {
    "text": "right trying to squeeze out the most",
    "start": "1244400",
    "end": "1246000"
  },
  {
    "text": "from your device",
    "start": "1246000",
    "end": "1248400"
  },
  {
    "text": "so with model mesh i was able to deploy",
    "start": "1248400",
    "end": "1250799"
  },
  {
    "text": "a mix of tensorflow and",
    "start": "1250799",
    "end": "1253200"
  },
  {
    "text": "onyx machine learning models which by",
    "start": "1253200",
    "end": "1255280"
  },
  {
    "text": "default with model mesh automatically",
    "start": "1255280",
    "end": "1257840"
  },
  {
    "text": "maps to the triton inference server",
    "start": "1257840",
    "end": "1260320"
  },
  {
    "text": "serving runtime and yeah because yeah",
    "start": "1260320",
    "end": "1262960"
  },
  {
    "text": "triton serving try and serving runtimes",
    "start": "1262960",
    "end": "1265120"
  },
  {
    "text": "supports quite a few of the model",
    "start": "1265120",
    "end": "1266640"
  },
  {
    "text": "formats",
    "start": "1266640",
    "end": "1267840"
  },
  {
    "text": "and",
    "start": "1267840",
    "end": "1269200"
  },
  {
    "text": "you know performing grpc inference",
    "start": "1269200",
    "end": "1272080"
  },
  {
    "text": "was pretty fast uh",
    "start": "1272080",
    "end": "1273760"
  },
  {
    "text": "model mesh the primary api is grpc",
    "start": "1273760",
    "end": "1278880"
  },
  {
    "text": "and so",
    "start": "1279360",
    "end": "1280320"
  },
  {
    "text": "even if",
    "start": "1280320",
    "end": "1281520"
  },
  {
    "text": "so if a model was loaded in memory",
    "start": "1281520",
    "end": "1284240"
  },
  {
    "text": "you get about a second less than a",
    "start": "1284240",
    "end": "1286480"
  },
  {
    "text": "second inference response time",
    "start": "1286480",
    "end": "1288559"
  },
  {
    "text": "and then",
    "start": "1288559",
    "end": "1290480"
  },
  {
    "text": "you know if it has to unload and load",
    "start": "1290480",
    "end": "1292240"
  },
  {
    "text": "the model uh you might might have to",
    "start": "1292240",
    "end": "1294240"
  },
  {
    "text": "wait a few seconds",
    "start": "1294240",
    "end": "1295840"
  },
  {
    "text": "and so",
    "start": "1295840",
    "end": "1297280"
  },
  {
    "start": "1297000",
    "end": "1297000"
  },
  {
    "text": "after that i wanted to try you know",
    "start": "1297280",
    "end": "1299120"
  },
  {
    "text": "higher density model packing so i wanted",
    "start": "1299120",
    "end": "1301760"
  },
  {
    "text": "to play apply more inference services to",
    "start": "1301760",
    "end": "1304640"
  },
  {
    "text": "the cluster to register more models with",
    "start": "1304640",
    "end": "1307120"
  },
  {
    "text": "model mesh and so i deployed around",
    "start": "1307120",
    "end": "1311440"
  },
  {
    "text": "so in this case in my scenario i",
    "start": "1311679",
    "end": "1313600"
  },
  {
    "text": "deployed around 17 or so densenet onyx",
    "start": "1313600",
    "end": "1315840"
  },
  {
    "text": "models",
    "start": "1315840",
    "end": "1317120"
  },
  {
    "text": "so these are about",
    "start": "1317120",
    "end": "1318480"
  },
  {
    "text": "i want to say about 36 megabytes each",
    "start": "1318480",
    "end": "1321440"
  },
  {
    "text": "um",
    "start": "1321440",
    "end": "1322640"
  },
  {
    "text": "they're for image classification over i",
    "start": "1322640",
    "end": "1324799"
  },
  {
    "text": "think a thousand or so categories",
    "start": "1324799",
    "end": "1328240"
  },
  {
    "text": "and so we",
    "start": "1328240",
    "end": "1329600"
  },
  {
    "text": "on the right you have a simple diagram",
    "start": "1329600",
    "end": "1331600"
  },
  {
    "text": "where which kind of shows what's kind of",
    "start": "1331600",
    "end": "1333440"
  },
  {
    "text": "deployed on the model mesh side of",
    "start": "1333440",
    "end": "1335280"
  },
  {
    "text": "things",
    "start": "1335280",
    "end": "1336880"
  },
  {
    "text": "and so",
    "start": "1336880",
    "end": "1338480"
  },
  {
    "text": "i expose the service as a node port and",
    "start": "1338480",
    "end": "1341039"
  },
  {
    "text": "from anywhere on my network i can",
    "start": "1341039",
    "end": "1344000"
  },
  {
    "text": "send inference response or inference",
    "start": "1344000",
    "end": "1345840"
  },
  {
    "text": "request to",
    "start": "1345840",
    "end": "1347120"
  },
  {
    "text": "garner to kind of get whatever inference",
    "start": "1347120",
    "end": "1349760"
  },
  {
    "text": "request",
    "start": "1349760",
    "end": "1350720"
  },
  {
    "text": "uh response",
    "start": "1350720",
    "end": "1352320"
  },
  {
    "text": "for my request",
    "start": "1352320",
    "end": "1355200"
  },
  {
    "text": "so again this uses the caser v2 protocol",
    "start": "1355280",
    "end": "1358240"
  },
  {
    "text": "grpc specifically and so",
    "start": "1358240",
    "end": "1361440"
  },
  {
    "text": "with this after applying yes",
    "start": "1361440",
    "end": "1364480"
  },
  {
    "text": "when i do kubeco get inferent services",
    "start": "1364480",
    "end": "1366640"
  },
  {
    "text": "you know i was able to get a list of",
    "start": "1366640",
    "end": "1368000"
  },
  {
    "text": "registered models and all of them are",
    "start": "1368000",
    "end": "1369600"
  },
  {
    "text": "listed as ready even though not all of",
    "start": "1369600",
    "end": "1372159"
  },
  {
    "text": "them are necessarily loaded in memory",
    "start": "1372159",
    "end": "1374960"
  },
  {
    "text": "so",
    "start": "1374960",
    "end": "1375679"
  },
  {
    "text": "using",
    "start": "1375679",
    "end": "1377840"
  },
  {
    "text": "so for those familiar with grpc usually",
    "start": "1377840",
    "end": "1380240"
  },
  {
    "text": "you have a protobuf that kind of",
    "start": "1380240",
    "end": "1381520"
  },
  {
    "text": "corresponds to the",
    "start": "1381520",
    "end": "1383280"
  },
  {
    "text": "api you're using so with that i was able",
    "start": "1383280",
    "end": "1385280"
  },
  {
    "text": "to generate a python client for which i",
    "start": "1385280",
    "end": "1388159"
  },
  {
    "text": "could send grpc inference requests with",
    "start": "1388159",
    "end": "1391280"
  },
  {
    "text": "arbitrary images for classification",
    "start": "1391280",
    "end": "1395120"
  },
  {
    "text": "so in the sample command i send an",
    "start": "1395120",
    "end": "1397679"
  },
  {
    "text": "inference request to one of the deployed",
    "start": "1397679",
    "end": "1399520"
  },
  {
    "text": "onyx models",
    "start": "1399520",
    "end": "1401039"
  },
  {
    "text": "and receive my inference results based",
    "start": "1401039",
    "end": "1403039"
  },
  {
    "text": "on",
    "start": "1403039",
    "end": "1403760"
  },
  {
    "text": "this is my cat mabel",
    "start": "1403760",
    "end": "1406320"
  },
  {
    "text": "who volunteered or got volunteered to be",
    "start": "1406320",
    "end": "1409360"
  },
  {
    "text": "a part of my image classification",
    "start": "1409360",
    "end": "1412880"
  },
  {
    "text": "so the response times even with all of",
    "start": "1412880",
    "end": "1415600"
  },
  {
    "text": "these all these models response times",
    "start": "1415600",
    "end": "1417360"
  },
  {
    "text": "still sub second",
    "start": "1417360",
    "end": "1420240"
  },
  {
    "text": "but if it is a cache miss so again not",
    "start": "1420240",
    "end": "1422880"
  },
  {
    "text": "all of these can be loaded in the memory",
    "start": "1422880",
    "end": "1424400"
  },
  {
    "text": "at once if it's a cache miss",
    "start": "1424400",
    "end": "1426320"
  },
  {
    "text": "there is a",
    "start": "1426320",
    "end": "1427520"
  },
  {
    "text": "there is a quite a",
    "start": "1427520",
    "end": "1429679"
  },
  {
    "text": "few seconds of latency involved because",
    "start": "1429679",
    "end": "1431919"
  },
  {
    "text": "again malamesh has to unload the model",
    "start": "1431919",
    "end": "1434320"
  },
  {
    "text": "to make room for the target model of",
    "start": "1434320",
    "end": "1436559"
  },
  {
    "text": "inference",
    "start": "1436559",
    "end": "1438559"
  },
  {
    "text": "so now of course even with just this",
    "start": "1438559",
    "end": "1440559"
  },
  {
    "text": "single device you know it's an",
    "start": "1440559",
    "end": "1442000"
  },
  {
    "text": "all-in-one cluster",
    "start": "1442000",
    "end": "1444080"
  },
  {
    "text": "installed with model mesh",
    "start": "1444080",
    "end": "1445919"
  },
  {
    "text": "you know it was it was very",
    "start": "1445919",
    "end": "1447840"
  },
  {
    "text": "i was pleasantly surprised to see i was",
    "start": "1447840",
    "end": "1449679"
  },
  {
    "text": "able to deploy",
    "start": "1449679",
    "end": "1450960"
  },
  {
    "text": "and host",
    "start": "1450960",
    "end": "1452159"
  },
  {
    "text": "uh quite a few models more than i",
    "start": "1452159",
    "end": "1454000"
  },
  {
    "text": "expected actually and so",
    "start": "1454000",
    "end": "1456000"
  },
  {
    "text": "with multiple devices or",
    "start": "1456000",
    "end": "1458480"
  },
  {
    "text": "a separate control plane right so my",
    "start": "1458480",
    "end": "1460320"
  },
  {
    "text": "control plane and my worker know they're",
    "start": "1460320",
    "end": "1462000"
  },
  {
    "text": "all the same device",
    "start": "1462000",
    "end": "1464799"
  },
  {
    "text": "but still i was able to get um quite a",
    "start": "1464799",
    "end": "1466720"
  },
  {
    "text": "lot out of this installation especially",
    "start": "1466720",
    "end": "1470480"
  },
  {
    "text": "on such a resource constrained device",
    "start": "1470480",
    "end": "1472480"
  },
  {
    "text": "and so",
    "start": "1472480",
    "end": "1474799"
  },
  {
    "text": "with multiple devices for instance if",
    "start": "1474799",
    "end": "1476559"
  },
  {
    "text": "you maybe use a k3s cluster",
    "start": "1476559",
    "end": "1479360"
  },
  {
    "text": "multi worker node you could definitely",
    "start": "1479360",
    "end": "1481679"
  },
  {
    "text": "get larger cache sizes for model loading",
    "start": "1481679",
    "end": "1484320"
  },
  {
    "text": "and the opportunity for high",
    "start": "1484320",
    "end": "1485919"
  },
  {
    "text": "availability you can have multiple rep",
    "start": "1485919",
    "end": "1487760"
  },
  {
    "text": "or copies of models loaded on different",
    "start": "1487760",
    "end": "1489600"
  },
  {
    "text": "nodes",
    "start": "1489600",
    "end": "1490640"
  },
  {
    "text": "and",
    "start": "1490640",
    "end": "1491440"
  },
  {
    "text": "you know even more compute power",
    "start": "1491440",
    "end": "1492960"
  },
  {
    "text": "dedicated compute power for the model",
    "start": "1492960",
    "end": "1495039"
  },
  {
    "text": "inference service",
    "start": "1495039",
    "end": "1496240"
  },
  {
    "text": "servers themselves",
    "start": "1496240",
    "end": "1499440"
  },
  {
    "text": "so yeah this is overall a",
    "start": "1499600",
    "end": "1502080"
  },
  {
    "text": "very uh interesting interesting",
    "start": "1502080",
    "end": "1504400"
  },
  {
    "text": "challenge for me um it was a since i'm a",
    "start": "1504400",
    "end": "1506640"
  },
  {
    "start": "1505000",
    "end": "1505000"
  },
  {
    "text": "model mesh developer um",
    "start": "1506640",
    "end": "1508640"
  },
  {
    "text": "i am looking more into kind of how we",
    "start": "1508640",
    "end": "1510240"
  },
  {
    "text": "can bring model mesh to other kind of",
    "start": "1510240",
    "end": "1512240"
  },
  {
    "text": "areas",
    "start": "1512240",
    "end": "1513360"
  },
  {
    "text": "and so",
    "start": "1513360",
    "end": "1514880"
  },
  {
    "text": "you know generally when you're dealing",
    "start": "1514880",
    "end": "1516480"
  },
  {
    "text": "with um",
    "start": "1516480",
    "end": "1518080"
  },
  {
    "text": "i guess",
    "start": "1518080",
    "end": "1519679"
  },
  {
    "text": "pointing things to edge devices you",
    "start": "1519679",
    "end": "1521840"
  },
  {
    "text": "generally",
    "start": "1521840",
    "end": "1522799"
  },
  {
    "text": "run into the lack of arm builds and",
    "start": "1522799",
    "end": "1525120"
  },
  {
    "text": "support you know like for instance",
    "start": "1525120",
    "end": "1526880"
  },
  {
    "text": "k-serve has some dependencies on some",
    "start": "1526880",
    "end": "1529360"
  },
  {
    "text": "python python packages which don't",
    "start": "1529360",
    "end": "1531679"
  },
  {
    "text": "necessarily have arm support and a lot",
    "start": "1531679",
    "end": "1534640"
  },
  {
    "text": "of container images are only built you",
    "start": "1534640",
    "end": "1537600"
  },
  {
    "text": "know you might look for a container",
    "start": "1537600",
    "end": "1539039"
  },
  {
    "text": "image for k-serve on docker hook right",
    "start": "1539039",
    "end": "1540559"
  },
  {
    "text": "now that just be only",
    "start": "1540559",
    "end": "1542720"
  },
  {
    "text": "amd 64 or x86",
    "start": "1542720",
    "end": "1546159"
  },
  {
    "text": "and so",
    "start": "1546159",
    "end": "1547360"
  },
  {
    "text": "yeah and so",
    "start": "1547360",
    "end": "1549679"
  },
  {
    "text": "right now i guess a lot of the images",
    "start": "1549679",
    "end": "1551360"
  },
  {
    "text": "are huge over a gigabyte especially so",
    "start": "1551360",
    "end": "1554559"
  },
  {
    "text": "there's a lot of trimming down that",
    "start": "1554559",
    "end": "1556000"
  },
  {
    "text": "needs to be done",
    "start": "1556000",
    "end": "1557440"
  },
  {
    "text": "and so",
    "start": "1557440",
    "end": "1559679"
  },
  {
    "text": "that's something that i kind of put into",
    "start": "1559679",
    "end": "1561679"
  },
  {
    "text": "and considered when i was kind of",
    "start": "1561679",
    "end": "1563440"
  },
  {
    "text": "building my own arm based images and of",
    "start": "1563440",
    "end": "1566559"
  },
  {
    "text": "course when you're dealing with edge",
    "start": "1566559",
    "end": "1567520"
  },
  {
    "text": "devices the whole need to",
    "start": "1567520",
    "end": "1569360"
  },
  {
    "text": "create",
    "start": "1569360",
    "end": "1570799"
  },
  {
    "text": "kind of smaller models or doing some",
    "start": "1570799",
    "end": "1573120"
  },
  {
    "text": "techniques like pruning or quantization",
    "start": "1573120",
    "end": "1575520"
  },
  {
    "text": "that's also needs that's also",
    "start": "1575520",
    "end": "1578000"
  },
  {
    "text": "something to be considered",
    "start": "1578000",
    "end": "1580680"
  },
  {
    "text": "[Music]",
    "start": "1580680",
    "end": "1582240"
  },
  {
    "text": "but anyway if you want to learn more i",
    "start": "1582240",
    "end": "1583600"
  },
  {
    "text": "do have some links here um definitely",
    "start": "1583600",
    "end": "1586880"
  },
  {
    "text": "um try out any of these technologies um",
    "start": "1586880",
    "end": "1589120"
  },
  {
    "text": "i do think model mesh",
    "start": "1589120",
    "end": "1590960"
  },
  {
    "text": "depending on your use case can be a",
    "start": "1590960",
    "end": "1592240"
  },
  {
    "text": "tenable solution for model serving on",
    "start": "1592240",
    "end": "1594640"
  },
  {
    "text": "edge",
    "start": "1594640",
    "end": "1595919"
  },
  {
    "text": "and with that uh i thank you and i'll",
    "start": "1595919",
    "end": "1598240"
  },
  {
    "text": "take any questions if there are any",
    "start": "1598240",
    "end": "1602520"
  },
  {
    "text": "[Applause]",
    "start": "1602530",
    "end": "1605690"
  }
]