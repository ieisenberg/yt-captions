[
  {
    "text": "I'm Shiva merla U I'm from the cloud native engineering team at Nvidia um I've been focusing um on GPU",
    "start": "80",
    "end": "8200"
  },
  {
    "text": "operator for last few years um and mainly work working on orchestration GPU",
    "start": "8200",
    "end": "13360"
  },
  {
    "text": "orchestration in kubernetes um so we had a long journey um uh in terms of using",
    "start": "13360",
    "end": "20160"
  },
  {
    "text": "operator pattern uh to make it easy to consume gpus in kubernetes um so today",
    "start": "20160",
    "end": "25640"
  },
  {
    "text": "we're going to talk about um how we um how we went through this approach and the learnings we had uh through this",
    "start": "25640",
    "end": "33200"
  },
  {
    "text": "journey introduce my name is Kevin Clues um I'm on the same team as Shiva at Nvidia um",
    "start": "33200",
    "end": "41039"
  },
  {
    "text": "and the the way I always pitch the team that that that we have what we do is we we do everything that's necessary to",
    "start": "41039",
    "end": "47160"
  },
  {
    "text": "enable GPU support in kubernetes in in containers in kubernetes um and then we",
    "start": "47160",
    "end": "52199"
  },
  {
    "text": "build all the tooling around that to make to make using gpus in this environment easier um and so yeah we're",
    "start": "52199",
    "end": "58280"
  },
  {
    "text": "the the operator is our way to pack it all these things together and make it that much easier for you guys to take",
    "start": "58280",
    "end": "63320"
  },
  {
    "text": "advantage of of all the of all of these things on kubernetes thanks Kevin um so this is",
    "start": "63320",
    "end": "70320"
  },
  {
    "text": "the outline for the talk today um we're going to talk about why gpus um so why",
    "start": "70320",
    "end": "75960"
  },
  {
    "text": "GPS have become so ubiquitous and and why um gpus in",
    "start": "75960",
    "end": "81079"
  },
  {
    "text": "kubernetes um and also we'll walk through um how the typical GPU software stack looks like",
    "start": "81079",
    "end": "87079"
  },
  {
    "text": "um uh what are the main um operational pain points we have when enabling gpus",
    "start": "87079",
    "end": "92280"
  },
  {
    "text": "um GPU software snack and then we'll um uh delve into uh operator pattern uh how",
    "start": "92280",
    "end": "98799"
  },
  {
    "text": "we have implemented GPU operator and some of the technical details uh of the GPU operator itself then we'll end with",
    "start": "98799",
    "end": "104799"
  },
  {
    "text": "a demo and and some of the the lessons that we learn um through this",
    "start": "104799",
    "end": "110360"
  },
  {
    "text": "journey um so why gpus um are so popular right um it's no surprise um given the",
    "start": "110799",
    "end": "119520"
  },
  {
    "text": "given the massive um computational Power and and given the the way we are um we",
    "start": "119520",
    "end": "127320"
  },
  {
    "text": "took a giant leap in terms of computational capacity over the last decade um so they become um so",
    "start": "127320",
    "end": "133800"
  },
  {
    "text": "ubiquitous in in kubernetes um to run IML jobs deep learning um even in the",
    "start": "133800",
    "end": "139239"
  },
  {
    "text": "scientific research um so become so um common everywhere um and recently we",
    "start": "139239",
    "end": "145080"
  },
  {
    "text": "have announced Blackwell um which took a giant leap in terms of computation",
    "start": "145080",
    "end": "151840"
  },
  {
    "text": "capability and and kubernetes on the other hand um also over the last few years have become a defacto standard uh",
    "start": "153720",
    "end": "160480"
  },
  {
    "text": "to run IML jobs um be it in um IML uh deep learning um in the scientific",
    "start": "160480",
    "end": "167599"
  },
  {
    "text": "fields or data processing so everywhere kubernetes have become a defacto standard because of its scalability um",
    "start": "167599",
    "end": "175120"
  },
  {
    "text": "easy to scale your applications um because of its resiliency um right and",
    "start": "175120",
    "end": "180159"
  },
  {
    "text": "also um uh uh the way to kind of manage these applications seamlessly um so this",
    "start": "180159",
    "end": "186959"
  },
  {
    "text": "become so common uh to use kubernetes so what do we need um to to",
    "start": "186959",
    "end": "193239"
  },
  {
    "text": "enable gpus in kubernetes so typically uh we start with a device driver any kind of GPU we have uh a device driver",
    "start": "193239",
    "end": "200200"
  },
  {
    "text": "um to be installed on the host um and we have uh some sort of uh hooks to enable",
    "start": "200200",
    "end": "205879"
  },
  {
    "text": "with the container runtime because sometimes we we don't have the native support um in in the container runtime",
    "start": "205879",
    "end": "211400"
  },
  {
    "text": "itself to kind of enable the gpus um so we need uh these custom runtime handlers",
    "start": "211400",
    "end": "217239"
  },
  {
    "text": "to be installed on the host uh we also have a a kubernetes standard device plugin um so we have a",
    "start": "217239",
    "end": "224439"
  },
  {
    "text": "device plug-in framework in kubernetes uh to to kind of advertise gpus kind discover and advertise gpus to to the",
    "start": "224439",
    "end": "232079"
  },
  {
    "text": "cube and also it's it's common to have a GPU monitoring software we want to monitor GPU performance um so we want to",
    "start": "232079",
    "end": "239360"
  },
  {
    "text": "um get certain alerts in terms of they go in cases when they go faulty so it's also common to have a GPU monitoring um",
    "start": "239360",
    "end": "246360"
  },
  {
    "text": "stack so this is a typical GPU software stack with with any kind of GPU um so what are the pain points um uh",
    "start": "246360",
    "end": "254480"
  },
  {
    "text": "that the operational teams are are seeing these days uh when enabling the GPU stack so we're going to talk about",
    "start": "254480",
    "end": "260720"
  },
  {
    "text": "um at the high level uh these pinpoints uh we have um a heterogeneous uh node",
    "start": "260720",
    "end": "266160"
  },
  {
    "text": "software snack um so I'll talk about uh the challenges we having in terms of managing this stack as we add newer gpus",
    "start": "266160",
    "end": "273000"
  },
  {
    "text": "into the cluster and we add as we release newer versions of the drivers over a period of time um how it becomes",
    "start": "273000",
    "end": "278479"
  },
  {
    "text": "a paino um to manage um also the driver configuration",
    "start": "278479",
    "end": "283680"
  },
  {
    "text": "itself um so most people um install drivers on on the OS itself um they they",
    "start": "283680",
    "end": "289280"
  },
  {
    "text": "have to configure certain things on the driver so I will walk through um how that is painful",
    "start": "289280",
    "end": "294479"
  },
  {
    "text": "today and the most common thing that we are also Hearing in this um uh in this conference is how to kind of efficiently",
    "start": "294479",
    "end": "301320"
  },
  {
    "text": "use gpus it's it's most common um to learn about um how we can configure gpus",
    "start": "301320",
    "end": "307039"
  },
  {
    "text": "to share among multiple workloads um so again that the configuration is a per node and also per GPU um and and also",
    "start": "307039",
    "end": "315800"
  },
  {
    "text": "something that the operational teams have to manage um over a period of time and as I mentioned uh we do apply",
    "start": "315800",
    "end": "322280"
  },
  {
    "text": "certain container runtime configuration we have certain hooks to be placed on the host based on the runtime type um so",
    "start": "322280",
    "end": "328520"
  },
  {
    "text": "we have to configure we have to Rel the demon so there's certain things we need to uh we need to know and also monitoring GPU Health uh",
    "start": "328520",
    "end": "336759"
  },
  {
    "text": "um so it's also there is no robust solution today to kind of monitor GPU health and take action um that's also a",
    "start": "336759",
    "end": "343160"
  },
  {
    "text": "common paino whenever some GPU uh becomes unhealthy um how do we monitor that and how do we kind of make sure",
    "start": "343160",
    "end": "349840"
  },
  {
    "text": "that we don't schedule anything on on that note um so to Del more into um the",
    "start": "349840",
    "end": "357440"
  },
  {
    "text": "hetrogeneous node software stack um typically Day Zero um uh we install uh",
    "start": "357440",
    "end": "364880"
  },
  {
    "text": "operating system so we have a GPU software snack which I talked about and a standard kubernetes stack everything",
    "start": "364880",
    "end": "371599"
  },
  {
    "text": "works fine initial setup um works fine and then we cap keep adding more nodes with maybe like a newer gpus uh newer",
    "start": "371599",
    "end": "378599"
  },
  {
    "text": "driver versions um so these versions are constantly changing um we have maybe it",
    "start": "378599",
    "end": "383720"
  },
  {
    "text": "might be because of the performance improvements or it it's because of the cves um right so you kind of have to",
    "start": "383720",
    "end": "390319"
  },
  {
    "text": "keep updating the drivers um over a period of time um so definitely we'll run into",
    "start": "390319",
    "end": "398000"
  },
  {
    "text": "some sort of inoperative issues so we'll run into uh issues in terms of the container runtime hooks that we have or",
    "start": "398000",
    "end": "404840"
  },
  {
    "text": "the the driver and and the uh the Cuda stack that you have on the on the on the",
    "start": "404840",
    "end": "410560"
  },
  {
    "text": "on the on the Node itself so we'll run into different sort of interop",
    "start": "410560",
    "end": "416120"
  },
  {
    "text": "issues also there is a a CPU uh versus GPU stack um so the the the operational",
    "start": "416120",
    "end": "421879"
  },
  {
    "text": "teams how to maintain uh two different Stacks depending on if it's a GPU node or or a CPU node because they kind of",
    "start": "421879",
    "end": "428360"
  },
  {
    "text": "buil um build drivers into the OS image itself so end up using uh two different",
    "start": "428360",
    "end": "433680"
  },
  {
    "text": "um OS images so when when the cluster grows in size um so it becomes um very common to",
    "start": "433680",
    "end": "442039"
  },
  {
    "text": "see um how challenging it is to manage it across different nodes and how it is how hard it is to configure uh these",
    "start": "442039",
    "end": "449240"
  },
  {
    "text": "things uh when it comes to driver install um so",
    "start": "449240",
    "end": "455120"
  },
  {
    "text": "we have various components in the driver we have Kel mode components we have user space components um on the user space we",
    "start": "455120",
    "end": "461360"
  },
  {
    "text": "have multiple services to be launched um right and also there's the clii utilities to configure parameters on on",
    "start": "461360",
    "end": "467319"
  },
  {
    "text": "the gpus um again um on a node by node basis um so depending on the the version of",
    "start": "467319",
    "end": "474520"
  },
  {
    "text": "the driver they have depending on the type of gpus they have um so uh the SR teams might have configured a few things",
    "start": "474520",
    "end": "480440"
  },
  {
    "text": "on these nodes so again it becomes challenging right to kind of uh keep track of all these things in the",
    "start": "480440",
    "end": "487280"
  },
  {
    "text": "cluster um another common thing is a parod GPU configuration so uh how we partition the gpus um on in each of the",
    "start": "487759",
    "end": "495599"
  },
  {
    "text": "node um so you can you need to have some sort of declarative mechanism to to partition gpus um this is when when when",
    "start": "495599",
    "end": "503919"
  },
  {
    "text": "done with the Standalone tools um so that the admins have to keep track of um",
    "start": "503919",
    "end": "510039"
  },
  {
    "text": "uh the config that is applied on each node right the the different settings that are applied on each of the gpus so",
    "start": "510039",
    "end": "515440"
  },
  {
    "text": "they'll have to kind of keep track of uh these things on each of the node and some sometimes these are not",
    "start": "515440",
    "end": "521039"
  },
  {
    "text": "persistent like every time the node reboots uh some of these are not persistent so on every reboot they they'll have to have some sort of unit",
    "start": "521039",
    "end": "527560"
  },
  {
    "text": "scripts or service demons right to to apply the same changes uh",
    "start": "527560",
    "end": "532880"
  },
  {
    "text": "again um when it comes to runtime configuration um we do have",
    "start": "534160",
    "end": "540079"
  },
  {
    "text": "um as I said we have a custom runtime hooks in place um we are kind of standardizing towards CDI but we still",
    "start": "540079",
    "end": "545600"
  },
  {
    "text": "not yet uh completely there um so we still have to install certain hooks on the host based on the runtime",
    "start": "545600",
    "end": "552120"
  },
  {
    "text": "type uh for example container D we have to um uh modify the config file we have",
    "start": "552120",
    "end": "557399"
  },
  {
    "text": "to apply the runtime class configuration um we have to reload the demon right so same thing with um a Docker Docker demon",
    "start": "557399",
    "end": "564880"
  },
  {
    "text": "um for uh same thing with CIO uh uh we also need to modify the",
    "start": "564880",
    "end": "570240"
  },
  {
    "text": "default runtime itself um if um user want to run mostly GPU jop so they can",
    "start": "570240",
    "end": "575640"
  },
  {
    "text": "add Nvidia as a default runtime um in which case if they don't request a gpus so we'll just fall back to the",
    "start": "575640",
    "end": "581480"
  },
  {
    "text": "underlying underlying runtime um and also um reloading a",
    "start": "581480",
    "end": "588480"
  },
  {
    "text": "runtime demon is also a challenge because whenever we apply these changes we have to reload the demon um and with",
    "start": "588480",
    "end": "594200"
  },
  {
    "text": "every new version we might add some new configuration into the runtime class right so so that admin have to keep",
    "start": "594200",
    "end": "600040"
  },
  {
    "text": "track of uh how these changes are um are made in in the cluster uh so this is",
    "start": "600040",
    "end": "605240"
  },
  {
    "text": "something they have to they have to keep monitoring another paino is is GPU",
    "start": "605240",
    "end": "612120"
  },
  {
    "text": "Health um so we have a a dcgm exporter uh we have a standalone Helm chart to",
    "start": "612120",
    "end": "617600"
  },
  {
    "text": "install dcgm exporter um again there is no life cycle management um right so the operational teams have to apply apply",
    "start": "617600",
    "end": "624160"
  },
  {
    "text": "these changes uh they'll have to set up service monitor uh so make sure that um they conf figured right with with",
    "start": "624160",
    "end": "630920"
  },
  {
    "text": "Prometheus so it's all manual process today um and and also um there is no",
    "start": "630920",
    "end": "636720"
  },
  {
    "text": "robust solution in terms of handling GPU errors uh in case if there are GPU errors um how how we need to propagate",
    "start": "636720",
    "end": "642959"
  },
  {
    "text": "these to the kubernetes itself um so how how can we avoid scheduling jobs onto these nodes so there is no robust",
    "start": "642959",
    "end": "649200"
  },
  {
    "text": "solution um that is built um for this as well we have a kubernetes device plugin",
    "start": "649200",
    "end": "655200"
  },
  {
    "text": "but again we have like a very basic health checking in the device plugin just to make sure that we don't use those gpus um but on on the kubernetes side there",
    "start": "655200",
    "end": "661880"
  },
  {
    "text": "is no easy way to kind of um orchestrate um and say that you can't schedule any jobs uh on on these notes",
    "start": "661880",
    "end": "669760"
  },
  {
    "text": "itself um so with that yeah let's get into um kubernetes operators um so we",
    "start": "670560",
    "end": "676200"
  },
  {
    "text": "started looking into um initially we had um various ways to to kind of deploy GPU",
    "start": "676200",
    "end": "682560"
  },
  {
    "text": "stack uh we had Standalone Helm charts um we had OS na2 packaging right it was",
    "start": "682560",
    "end": "687680"
  },
  {
    "text": "everywhere so so few years back we started looking at to kind of create like a unified solution um to have a",
    "start": "687680",
    "end": "693959"
  },
  {
    "text": "common API to configure these things in kubernetes um so operators was was obvious choice um uh it gives it gives a",
    "start": "693959",
    "end": "702120"
  },
  {
    "text": "common controller pattern where um you kind of declaratively Define uh the config you need and and it goes and",
    "start": "702120",
    "end": "708279"
  },
  {
    "text": "applies the desired State um um to make sure the actual State U matches with the desired State the control Loop Paradigm",
    "start": "708279",
    "end": "715320"
  },
  {
    "text": "really worked great for us we were able to bring all the software together and deploy this with with operator um um",
    "start": "715320",
    "end": "722000"
  },
  {
    "text": "recently with recent years we have seen many cases where uh there were a lot of interrup issues between these components",
    "start": "722000",
    "end": "727279"
  },
  {
    "text": "so it's become so easy um by configuring everything in in a with a single API it",
    "start": "727279",
    "end": "732639"
  },
  {
    "text": "become so easy to make sure that they stay consistent whenever we deploy these things they stay consistent um and don't",
    "start": "732639",
    "end": "738880"
  },
  {
    "text": "break on upgrades um how are the built um so",
    "start": "738880",
    "end": "744519"
  },
  {
    "text": "they're built uh using uh common tools like Cube Builder uh we have operator framework uh just with a few handful",
    "start": "744519",
    "end": "750880"
  },
  {
    "text": "commands we can easily um build operators um uh these tools uh will help with",
    "start": "750880",
    "end": "758440"
  },
  {
    "text": "generating the initial scaffolding um uh we can define an API and also it automatically generate all the manifests",
    "start": "758440",
    "end": "765079"
  },
  {
    "text": "that is required to deploy in kubernetes um for open shift uh there is again like a life cycle manager operator life cycle",
    "start": "765079",
    "end": "771279"
  },
  {
    "text": "manager um using operator framework we can also generate all the Manifest that are required to to deploy them in open",
    "start": "771279",
    "end": "777680"
  },
  {
    "text": "shift as well um so why are they useful so uh today",
    "start": "777680",
    "end": "784600"
  },
  {
    "text": "we're going to uh talk more into GPU operator how how we have used this pattern right um and and and and solve",
    "start": "784600",
    "end": "791519"
  },
  {
    "text": "these issues um so to give an overview of GPU",
    "start": "791519",
    "end": "797079"
  },
  {
    "text": "operator um so we wanted to have a unified API to to configure everything",
    "start": "797079",
    "end": "802240"
  },
  {
    "text": "uh in kubernetes um it gives a single pan of glass to to kind of um configure",
    "start": "802240",
    "end": "808600"
  },
  {
    "text": "and manage the life cycle of all the components uh that I talked about um starting with Nvidia driver container",
    "start": "808600",
    "end": "814720"
  },
  {
    "text": "runtime um a device plug-in and and the monitoring software we also have some Advanced components like Mig manager um",
    "start": "814720",
    "end": "821279"
  },
  {
    "text": "deployed through GPU operator so um it's given as a single API uh to kind of",
    "start": "821279",
    "end": "826880"
  },
  {
    "text": "easily configure these things um again how we install uh this",
    "start": "826880",
    "end": "833160"
  },
  {
    "text": "in in kubernetes um we have a standard tooling um package manager called as",
    "start": "833160",
    "end": "838320"
  },
  {
    "text": "helm with just with a single click install we can we can deploy uh GPU operator um and if you're using mm then",
    "start": "838320",
    "end": "846320"
  },
  {
    "text": "we can deploy using operator SDK uh we can easily spin up uh GPU operator pods and we can create a custom resource uh",
    "start": "846320",
    "end": "853320"
  },
  {
    "text": "which will uh which will enable deploying all the the operant that we have um at the high level this is the",
    "start": "853320",
    "end": "860240"
  },
  {
    "text": "state machine um the soon after install uh GPU operator pod comes up um right",
    "start": "860240",
    "end": "865560"
  },
  {
    "text": "and and we also have a crd called as a cluster policy uh um and also we have crd called as Nvidia driver API um so",
    "start": "865560",
    "end": "872880"
  },
  {
    "text": "using these um apis uh users can Define what is the configuration that they need",
    "start": "872880",
    "end": "878079"
  },
  {
    "text": "for the GPU stack uh the operator comes up um so it kind of um fetches fetches the API",
    "start": "878079",
    "end": "885240"
  },
  {
    "text": "config from from cluster policy and Nvidia driver um so but but nothing is",
    "start": "885240",
    "end": "890360"
  },
  {
    "text": "deployed yet um so we have a dependency um on a bootstrap operator called as NFD",
    "start": "890360",
    "end": "896160"
  },
  {
    "text": "um so NFD is a node feature Discovery operator um um which is open source and",
    "start": "896160",
    "end": "901360"
  },
  {
    "text": "and mainly the functionality of NFD is to to discover the hardware features on each of the node uh and kind of enable",
    "start": "901360",
    "end": "907759"
  },
  {
    "text": "other applications to to detect them and and run um uh suitable applications on them so NFD uh enabled us to kind of",
    "start": "907759",
    "end": "915519"
  },
  {
    "text": "seamlessly uh uh identify GPU nodes um so it has a crd called as a node features uh and based on the Node",
    "start": "915519",
    "end": "921959"
  },
  {
    "text": "feature crd um NFD uh NFD parts will kind of label saying this node has a GPU",
    "start": "921959",
    "end": "928480"
  },
  {
    "text": "and this is architecture of the node this is the kernel version of the node uh there are like multiple properties of the of the server itself that are",
    "start": "928480",
    "end": "934800"
  },
  {
    "text": "labeled on the Node uh that the applications can use to to schedule uh schedule onto those",
    "start": "934800",
    "end": "940319"
  },
  {
    "text": "nodes um so once the once the NFD labeling is done U so we can start with",
    "start": "940319",
    "end": "945560"
  },
  {
    "text": "the bootstrap uh GPU operator comes up and and we deploy the containerized driver uh to start with um the driver",
    "start": "945560",
    "end": "952560"
  },
  {
    "text": "installation takes around like 3 to 5 minutes so we have uh we currently support installing drivers uh through uh",
    "start": "952560",
    "end": "958959"
  },
  {
    "text": "through a run file uh which will dynamically compile um and install and also we have a pre-compiled um",
    "start": "958959",
    "end": "965480"
  },
  {
    "text": "pre-compiled packages uh supportting the driver containers as well so depending on the type of uh installation we use uh",
    "start": "965480",
    "end": "971759"
  },
  {
    "text": "it takes about like 3 to 5 minutes um to to boot SLP the driver on the Node and",
    "start": "971759",
    "end": "976920"
  },
  {
    "text": "once the the driver is uh installed once the the driver is loaded and all the libraries are installed uh we bring up",
    "start": "976920",
    "end": "982839"
  },
  {
    "text": "container toolkit uh which is again our core service to kind of uh inject all the gpus into application parts",
    "start": "982839",
    "end": "990399"
  },
  {
    "text": "so until the the core services are uh have come up we can't really run anything else so all the other parts uh",
    "start": "990399",
    "end": "996600"
  },
  {
    "text": "will be waiting on these things to come up it'll that's if you see as soon as uh you install GPU operator if you see um",
    "start": "996600",
    "end": "1003560"
  },
  {
    "text": "most of the parts stuck in in it state it is due to the reason that they're waiting for the driver installed to",
    "start": "1003560",
    "end": "1009160"
  },
  {
    "text": "complete um so these things really order well among themselves uh so once the driver installation is done once the",
    "start": "1009160",
    "end": "1015240"
  },
  {
    "text": "container toolkit setup is complete uh that's when we uh bring up a rest of the stack uh that includes the device plugin",
    "start": "1015240",
    "end": "1022000"
  },
  {
    "text": "um the GPU feature Discovery Mig manager right and also the the rest of the monitoring stack so all of the the rest",
    "start": "1022000",
    "end": "1028600"
  },
  {
    "text": "of the services will uh will come up um and also we have validation built in uh into the operator itself at every stage",
    "start": "1028600",
    "end": "1035558"
  },
  {
    "text": "we we perform Luda validation uh and the plug-in validation to make sure that the stack um is completely functional and",
    "start": "1035559",
    "end": "1042520"
  },
  {
    "text": "finally we mug the node as ready uh so bit more on NFD itself so we",
    "start": "1042520",
    "end": "1049280"
  },
  {
    "text": "have this dependency on node labeling because we didn't want to build another controller to to kind of manage node",
    "start": "1049280",
    "end": "1055559"
  },
  {
    "text": "labels um um so it has uh some apis defined like node features and node feature",
    "start": "1055559",
    "end": "1061360"
  },
  {
    "text": "rules uh where users can come in and Define custom rules and say if server have these properties this is the label",
    "start": "1061360",
    "end": "1066760"
  },
  {
    "text": "I want on the Node um and also it has certain standard labels for example certain standard PCI labels uh in this",
    "start": "1066760",
    "end": "1073080"
  },
  {
    "text": "case it says okay it's Nvidia GPU and it labels a node with Nvidia vendor ID",
    "start": "1073080",
    "end": "1080840"
  },
  {
    "text": "so to talk more on the the driver management itself um so the first thing we did is containerizing the driver so",
    "start": "1081880",
    "end": "1088159"
  },
  {
    "text": "we have we had this from long time we were using Docker containers to just do like a a testing mostly in the testing",
    "start": "1088159",
    "end": "1093679"
  },
  {
    "text": "environments um later with the GPU operator um we have built the whole life",
    "start": "1093679",
    "end": "1098760"
  },
  {
    "text": "cycle components into into the driver itself so um we install everything through the container we bind Mount the",
    "start": "1098760",
    "end": "1104919"
  },
  {
    "text": "onto the host path right so the rest of the components can use them uh um and",
    "start": "1104919",
    "end": "1110000"
  },
  {
    "text": "also we built the whole life cycle um uh aspect of the driver itself so we",
    "start": "1110000",
    "end": "1115039"
  },
  {
    "text": "recently we built an uh upgrade controller all right and we were also making progress um in terms of uh having",
    "start": "1115039",
    "end": "1121520"
  },
  {
    "text": "a common upgrade controller between multiple operators we we do have Network operator which does mofed installs mofed",
    "start": "1121520",
    "end": "1127520"
  },
  {
    "text": "driver installs right and we do GPU driver installs we have like a synchronization between them uh to to",
    "start": "1127520",
    "end": "1132640"
  },
  {
    "text": "manage these things um and typically um so we install everything to a container um the",
    "start": "1132640",
    "end": "1139440"
  },
  {
    "text": "container comes up and we can exit into the container and and do all the commands that that typically we do on",
    "start": "1139440",
    "end": "1145200"
  },
  {
    "text": "the host with with the drivers installed so please refer to the talk um um uh done by my colleague yesterday um",
    "start": "1145200",
    "end": "1152559"
  },
  {
    "text": "about the uh if you want to learn more about the driver installation and how the upgrade and all those things",
    "start": "1152559",
    "end": "1159320"
  },
  {
    "text": "work um so just to uh give perspective of um how we've been adding um different",
    "start": "1159640",
    "end": "1165919"
  },
  {
    "text": "features into um into the driver container itself over the past 2 to 3 years um we added significant features",
    "start": "1165919",
    "end": "1173600"
  },
  {
    "text": "um into into these driver containers making sure that we cover all the the functionality that is supported by NVIDIA drivers um so initially in 2021",
    "start": "1173600",
    "end": "1181120"
  },
  {
    "text": "we had basic driver installs just compile and load the the modules bind mount on the host nothing else uh but we",
    "start": "1181120",
    "end": "1187640"
  },
  {
    "text": "we added uh the life cycle components uh we added Nvidia vgpu uh driver support um and later in 2022 we added uh",
    "start": "1187640",
    "end": "1195480"
  },
  {
    "text": "GPU direct RDMA uh support um loading Nvidia PRM um GDs drivers gdr copy um so",
    "start": "1195480",
    "end": "1203280"
  },
  {
    "text": "we added complete support for uh um GPU direct Technologies as well um so other notable ones are uh",
    "start": "1203280",
    "end": "1210559"
  },
  {
    "text": "upgrade controller Advanced upgrade controller U you can listen to the talk um again um on this one um and recently",
    "start": "1210559",
    "end": "1218960"
  },
  {
    "text": "the focus have been um how we make it kind of easy to bootstrap the node so we've been um looking into pre-compiled",
    "start": "1218960",
    "end": "1225360"
  },
  {
    "text": "uh drivers um how to use pre-compiled drivers across all the operating systems that we use um so Canon U builds",
    "start": "1225360",
    "end": "1232760"
  },
  {
    "text": "pre-compiled packages for NVIDIA and publishes them um so at least for for",
    "start": "1232760",
    "end": "1238360"
  },
  {
    "text": "Ruban to uh we started publishing pre-compiled driver containers with NGC",
    "start": "1238360",
    "end": "1243640"
  },
  {
    "text": "our Nvidia container registry um with certain kernels and Driver versions we",
    "start": "1243640",
    "end": "1249080"
  },
  {
    "text": "we we on a daily basis we kind of build and publish these container images to to",
    "start": "1249080",
    "end": "1255000"
  },
  {
    "text": "NGC um so again uh we are expanding promile drivers to other operating systems uh we are looking at cloud cloud",
    "start": "1255720",
    "end": "1262480"
  },
  {
    "text": "uh Native CSP native operating systems we are looking at Rel um or cor uh to have these pre-compiled",
    "start": "1262480",
    "end": "1269919"
  },
  {
    "text": "packages um again like hetrogeneous drivers um right this is one common use case I want to run different drivers in",
    "start": "1269919",
    "end": "1275320"
  },
  {
    "text": "the same cluster I want to run different um kernels in the same cluster or um operating system versions in the same",
    "start": "1275320",
    "end": "1281039"
  },
  {
    "text": "cluster so uh or different drivers types in the same cluster so we supported this",
    "start": "1281039",
    "end": "1286200"
  },
  {
    "text": "um as well so the current ly as I mentioned the focus have been um um",
    "start": "1286200",
    "end": "1292279"
  },
  {
    "text": "having pre-compiled drivers everywhere remove the dynamic dependencies that we have to install these drivers um right",
    "start": "1292279",
    "end": "1298039"
  },
  {
    "text": "we don't have to pull any packages um or build them so the focus have been mainly to kind of enable pre-compiled drivers",
    "start": "1298039",
    "end": "1305000"
  },
  {
    "text": "everywhere and we are also looking into CSP native operating systems uh with this I'll hand over to",
    "start": "1305000",
    "end": "1312000"
  },
  {
    "text": "Kevin to talk about how the GPU configuration is done on each node yeah so in addition to all the you",
    "start": "1312000",
    "end": "1319039"
  },
  {
    "text": "know great features that the GPU operator gives you in terms of driver management um and so on um one of the",
    "start": "1319039",
    "end": "1325320"
  },
  {
    "text": "big things that you're able to do with the GPU operators actually configure the ways that you want the individual gpus",
    "start": "1325320",
    "end": "1330679"
  },
  {
    "text": "um to be set up by the time your workloads go to run on them um and in particular um things you can do is uh",
    "start": "1330679",
    "end": "1337640"
  },
  {
    "text": "set up the different sharing strategies that I talked about uh in the keynote on Wednesday so being able to set up a set",
    "start": "1337640",
    "end": "1343720"
  },
  {
    "text": "of MiG partitions on the GPU being able to set up an mpf server to run um and uh",
    "start": "1343720",
    "end": "1350120"
  },
  {
    "text": "you know space partition your GPU in various ways set up time slicing on these gpus um and one thing to note that",
    "start": "1350120",
    "end": "1356080"
  },
  {
    "text": "at least with the current GPU operator and the way that um it it it currently works with the with the apis that are",
    "start": "1356080",
    "end": "1362279"
  },
  {
    "text": "available to it from kubernetes only the CIS admin has the ability to do this so you have to kind of a priori decide how",
    "start": "1362279",
    "end": "1368240"
  },
  {
    "text": "do I want to divide my GPU up into different Mig devices how do I want sharing set up via time slicing an MPS",
    "start": "1368240",
    "end": "1373799"
  },
  {
    "text": "you have to drain the nodes you have to apply these configuration settings and once those are in place a user workload",
    "start": "1373799",
    "end": "1379720"
  },
  {
    "text": "can come along and consume them based on whatever the admin has decided is the the right way to set this up on an",
    "start": "1379720",
    "end": "1385159"
  },
  {
    "text": "individual node um in the future once we have uh Dynamic resource allocation uh",
    "start": "1385159",
    "end": "1390679"
  },
  {
    "text": "support um it won't be admin driven anymore at the time that you create a claim to reference one of your your gpus",
    "start": "1390679",
    "end": "1397159"
  },
  {
    "text": "you can decide how you want this sharing set up you can decide what configuration parameters you want on the GPU that",
    "start": "1397159",
    "end": "1402880"
  },
  {
    "text": "you're going to be given access to and so it kind of moves this ability to Define what sharing settings you want on your GPU",
    "start": "1402880",
    "end": "1408960"
  },
  {
    "text": "from the admin a priori to the just in time usage of the GPU when you're when you get access to it on your workload um",
    "start": "1408960",
    "end": "1415880"
  },
  {
    "text": "but at least in terms of how you use this today uh the diagrams here on the right show how an admin can come come along and configure uh a set of gpus on",
    "start": "1415880",
    "end": "1423880"
  },
  {
    "text": "a node uh with with a set of MiG devices so the the one on the top shows how you can uh divide all of the gpus into uh",
    "start": "1423880",
    "end": "1432240"
  },
  {
    "text": "what's called a 1G 5gb device and you can get seven of these on a single GPU and you can advertise them as Nvidia /",
    "start": "1432240",
    "end": "1439279"
  },
  {
    "text": "GPU and um if you request that you'll get one of these M devices rather than a full GPU um and this is what we call the",
    "start": "1439279",
    "end": "1445919"
  },
  {
    "text": "single mode because the um the GPU itself or sorry the resource used to",
    "start": "1445919",
    "end": "1451039"
  },
  {
    "text": "advertise this is the same as what you would have from a full GPU so from an end user perspective he doesn't necessarily know or care that he's",
    "start": "1451039",
    "end": "1456679"
  },
  {
    "text": "getting a MIG device or a GPU so you might want to advertise it this way um so that he doesn't have to change his",
    "start": "1456679",
    "end": "1462000"
  },
  {
    "text": "his podspec when he's requesting these on the flip side you can set it up in something we call mixed mode which",
    "start": "1462000",
    "end": "1467320"
  },
  {
    "text": "allows you to um change the name of the resource that you're actually trying to get access to because you know that you exactly want",
    "start": "1467320",
    "end": "1473399"
  },
  {
    "text": "one of these 1G 5gb devices versus a 2g1 g uh GB device and so on um so there's",
    "start": "1473399",
    "end": "1479679"
  },
  {
    "text": "different you know modes of operation that you you as the admin can decide for how you want to share uh these gpus and",
    "start": "1479679",
    "end": "1485799"
  },
  {
    "text": "how you want to set these things up and the apis will be very similar once we we get with Dr but as I said you'll be able",
    "start": "1485799",
    "end": "1491320"
  },
  {
    "text": "to do that as an then user as you request access to these gpus rather than it having to be set up a priori and you",
    "start": "1491320",
    "end": "1497480"
  },
  {
    "text": "just uh grabbing uh a reference to what's already there next",
    "start": "1497480",
    "end": "1503640"
  },
  {
    "text": "slide um and to enable all this as Shiva mentioned we have uh um this this",
    "start": "1503640",
    "end": "1509720"
  },
  {
    "text": "component in the system called the Nvidia container toolkit and just like the the driver that you need to install",
    "start": "1509720",
    "end": "1515279"
  },
  {
    "text": "on your host that runs you know in a containerized environment where you you take the driver container you run on the",
    "start": "1515279",
    "end": "1520600"
  },
  {
    "text": "host it's going to um install a uh a kernel um a kernel module to represent",
    "start": "1520600",
    "end": "1527679"
  },
  {
    "text": "the Nvidia driver and as well install some um you know the user space libraries inside the container itself",
    "start": "1527679",
    "end": "1534240"
  },
  {
    "text": "and you mount that back onto the host um similar thing happens with uh with our toolkit component it's it's a",
    "start": "1534240",
    "end": "1539840"
  },
  {
    "text": "containerized installer of the in toolkit so you run this container and what it's going to do and you see the",
    "start": "1539840",
    "end": "1545159"
  },
  {
    "text": "the three boxes on the or the three cylinders on the right of the box it's going to go through and back on the host",
    "start": "1545159",
    "end": "1551320"
  },
  {
    "text": "install this uh this binary called uh the Nvidia container toolkit which your",
    "start": "1551320",
    "end": "1556360"
  },
  {
    "text": "container run times need to call out to in order to actually inject GPU support into a container and that gets installed",
    "start": "1556360",
    "end": "1561520"
  },
  {
    "text": "back on the host it's going to expose some of these socket files you see here and depending on the runtime that you actually have configured it's also going",
    "start": "1561520",
    "end": "1567640"
  },
  {
    "text": "to update those uh the configuration files for those runtimes so that they know how to call out to this this",
    "start": "1567640",
    "end": "1573559"
  },
  {
    "text": "toolkit at the appropriate time um and the GPU operator automates this entire process so if we go from top to bottom",
    "start": "1573559",
    "end": "1579039"
  },
  {
    "text": "on that list that you see there on the right um it first first thing it does is it determines whether you have your",
    "start": "1579039",
    "end": "1584520"
  },
  {
    "text": "driver installed directly on the host or if you've used our uh GP operator managed um uh driver installation",
    "start": "1584520",
    "end": "1591799"
  },
  {
    "text": "process to use a driver container to it install it to Texs which you know which mode of operations been used to install",
    "start": "1591799",
    "end": "1597399"
  },
  {
    "text": "your driver um it then optionally updates the default runtime that you have in place so you don't necessarily",
    "start": "1597399",
    "end": "1603039"
  },
  {
    "text": "have to go via the tool kit in order to um to to run every single container that",
    "start": "1603039",
    "end": "1608120"
  },
  {
    "text": "you have on your system whether it uses gpus or not if you don't want that set up as the default runtime then you can",
    "start": "1608120",
    "end": "1614080"
  },
  {
    "text": "say that you want to use this time this runtime for uh uh containers that want",
    "start": "1614080",
    "end": "1619120"
  },
  {
    "text": "access to gpus but you don't have to make that the default um and in order to enable this you can uh it adds a runtime",
    "start": "1619120",
    "end": "1625159"
  },
  {
    "text": "class spec so that you can reference specifically the runtime that's going to use um gpus versus not um it also once",
    "start": "1625159",
    "end": "1633120"
  },
  {
    "text": "it updates the uh configuration files for these run times it will hook back into the system and if you're using",
    "start": "1633120",
    "end": "1638640"
  },
  {
    "text": "systemd it will you know restart systemd the systemd unit that represents um say",
    "start": "1638640",
    "end": "1644360"
  },
  {
    "text": "container d as an example um and then it will you know do do a bunch of other",
    "start": "1644360",
    "end": "1649679"
  },
  {
    "text": "stuff basically to just get this toolkit up and running um in order for GPU support to to work inside your",
    "start": "1649679",
    "end": "1657000"
  },
  {
    "text": "containers thanks Kevin um um so let's also look at the uh the",
    "start": "1657000",
    "end": "1662919"
  },
  {
    "text": "diverse workloads um that we support um so typically container workloads uh",
    "start": "1662919",
    "end": "1668440"
  },
  {
    "text": "injecting gpus into containers but recently we also seen um use cases uh",
    "start": "1668440",
    "end": "1674120"
  },
  {
    "text": "around like more around multi multi tent use cases and also how to securely run um certain IML applications um so we've",
    "start": "1674120",
    "end": "1681519"
  },
  {
    "text": "been looking at the the virtualization Solutions like cuart and Kata uh cuber uh it's been a few years now we we we we",
    "start": "1681519",
    "end": "1688840"
  },
  {
    "text": "we had support for qo uh we support um virtual gpus we support pass through gpus um right uh we have a very good",
    "start": "1688840",
    "end": "1696159"
  },
  {
    "text": "solution out there for cuboard VMS um CA containers uh currently we have in Tech",
    "start": "1696159",
    "end": "1701960"
  },
  {
    "text": "preview um you can launch launch a CA container with a pass through GPU um so",
    "start": "1701960",
    "end": "1707039"
  },
  {
    "text": "that they we also document mented and published as a tech preview um so GP operator makes it so easy to kind of",
    "start": "1707039",
    "end": "1713640"
  },
  {
    "text": "determine based on the workload type you want to run on each node uh it automatically detects um the software",
    "start": "1713640",
    "end": "1719480"
  },
  {
    "text": "stack because each of these workloads need a different software stack different kind of runtime um WR",
    "start": "1719480",
    "end": "1724840"
  },
  {
    "text": "different kind of plugins uh for for these ones so so just by labeling the node saying if you want to run a",
    "start": "1724840",
    "end": "1730760"
  },
  {
    "text": "container work Cloud VM vgpu or VM pass through we automatically bring up um the",
    "start": "1730760",
    "end": "1736279"
  },
  {
    "text": "the necessary software stack so again refer to the talk uh by my colleague uh done yesterday on um on the",
    "start": "1736279",
    "end": "1743480"
  },
  {
    "text": "work we are doing with Cara containers and also confidential Computing with this so GPU monitoring um I'll not go",
    "start": "1743480",
    "end": "1751679"
  },
  {
    "text": "much into this one so we we do uh deploy dcgm exporter um we have dcgm engine",
    "start": "1751679",
    "end": "1756919"
  },
  {
    "text": "builtin or we also support launching dcgm as a separate container we automate the life cycle of the dcgm uh component",
    "start": "1756919",
    "end": "1763720"
  },
  {
    "text": "itself we also create automatically service monitor um and and dynamically you can also change the the kind of",
    "start": "1763720",
    "end": "1769519"
  },
  {
    "text": "metrics you want to collect using dcgm um so we also have a metrics built",
    "start": "1769519",
    "end": "1775840"
  },
  {
    "text": "into the operator we we have uh operator level as well as operant level metrics uh so you can easily the SR teams can",
    "start": "1775840",
    "end": "1782200"
  },
  {
    "text": "easily monitor uh if every component is functioning correctly um or if the",
    "start": "1782200",
    "end": "1787399"
  },
  {
    "text": "upgrades are in process right you can see that the progress of upgrades as",
    "start": "1787399",
    "end": "1792799"
  },
  {
    "text": "well um so we supp broader ecosystem um be it on Pam clusters or Cloud providers",
    "start": "1792799",
    "end": "1798720"
  },
  {
    "text": "um we support various um um Cloud providers um we also have support for um",
    "start": "1798720",
    "end": "1804720"
  },
  {
    "text": "on-prem kubernetes variants um uh container run times uh all the container run times um and and and operating",
    "start": "1804720",
    "end": "1812039"
  },
  {
    "text": "systems we are uh we are adding more um but currently we are supporting mostly entu and Coro and Rel uh and we are",
    "start": "1812039",
    "end": "1819399"
  },
  {
    "text": "planning to add support for other operating systems soon and and yeah please find the support metrics uh at this",
    "start": "1819399",
    "end": "1826679"
  },
  {
    "text": "link uh troubleshooting so so we have a um a kind of script to kind of gather",
    "start": "1826679",
    "end": "1832120"
  },
  {
    "text": "all the required um U uh logs uh to troubleshoot any issues so if you see any issues um use this command to to get",
    "start": "1832120",
    "end": "1839159"
  },
  {
    "text": "all the logs um so let's go into the demo um so we're",
    "start": "1839159",
    "end": "1844640"
  },
  {
    "text": "going to show in this demo yeah so how to how to deploy the",
    "start": "1844640",
    "end": "1850159"
  },
  {
    "text": "operator um so uh we're going to show how to how the driver gets deployed how the device plugin gets deployed U and",
    "start": "1850159",
    "end": "1856480"
  },
  {
    "text": "also we're going to show um So currently we are adding support for MPS there is we're planning to release an RC version",
    "start": "1856480",
    "end": "1863120"
  },
  {
    "text": "um next week um so we're going to have MPS support as well but I'm going to show uh in this",
    "start": "1863120",
    "end": "1870000"
  },
  {
    "text": "demo so um in this demo we're going to show how to install um just with a",
    "start": "1876480",
    "end": "1881960"
  },
  {
    "text": "simple Helm command um you can install GPU operator um and once the GPU",
    "start": "1881960",
    "end": "1887360"
  },
  {
    "text": "operator install all is complete uh we can show how the different node labels are applied all right to bring up um the",
    "start": "1887360",
    "end": "1893399"
  },
  {
    "text": "rest of the stack and how how to do validation uh to make sure that the driver is installed",
    "start": "1893399",
    "end": "1898559"
  },
  {
    "text": "successfully I'll script",
    "start": "1898559",
    "end": "1902158"
  },
  {
    "text": "through so we can see soon after the the GPU operator install um the the NFD pods",
    "start": "1904039",
    "end": "1910159"
  },
  {
    "text": "come up uh they do add certain node labels saying okay this node has a GPU um and based on that the the GPU",
    "start": "1910159",
    "end": "1916159"
  },
  {
    "text": "operator will bring up rest of the part so we can see the driver demon set coming up uh the container toolkit",
    "start": "1916159",
    "end": "1921399"
  },
  {
    "text": "coming up um and we can see some of them in the unit State um so this is the",
    "start": "1921399",
    "end": "1926880"
  },
  {
    "text": "ordering I was talking about and the driver initializes um once",
    "start": "1926880",
    "end": "1932639"
  },
  {
    "text": "the driver initialize um so while it's taking place we can see the different node labels that the GPU operator adds",
    "start": "1932639",
    "end": "1938240"
  },
  {
    "text": "to control each of these um so the device plugin so once",
    "start": "1938240",
    "end": "1946240"
  },
  {
    "text": "the device plugin comes up we can see the allocatable gpus on each of the node",
    "start": "1946240",
    "end": "1951480"
  },
  {
    "text": "um So currently it's still zero the driver installation um is",
    "start": "1951480",
    "end": "1957919"
  },
  {
    "text": "complete so we can see the allocatable GPU count um increased so each of them have one gpus one node has a00 GPU and",
    "start": "1963120",
    "end": "1969760"
  },
  {
    "text": "two nodes have L4 GPU",
    "start": "1969760",
    "end": "1974399"
  },
  {
    "text": "um",
    "start": "1976840",
    "end": "1979840"
  },
  {
    "text": "um so now we'll go into um applying uh different um GPU sharing uh techniques",
    "start": "1982799",
    "end": "1988960"
  },
  {
    "text": "one is applying Mig um so just by applying U Mig config uh on the Node",
    "start": "1988960",
    "end": "1994120"
  },
  {
    "text": "saying um 1G 10gb uh so I want all 1G 10gb profiles uh we created seven Mig",
    "start": "1994120",
    "end": "1999639"
  },
  {
    "text": "instances on this node the Mig manager created seven Mig instances on this node um and from the driver container we can",
    "start": "1999639",
    "end": "2005760"
  },
  {
    "text": "see all the Mig partitions on the GPU",
    "start": "2005760",
    "end": "2009760"
  },
  {
    "text": "and also we apply um custom um um configuration for a Time slicing so and",
    "start": "2012720",
    "end": "2020200"
  },
  {
    "text": "also MPS so we can define a custom configuration for device plug-in to to um to create MPS instances and also to",
    "start": "2020200",
    "end": "2026960"
  },
  {
    "text": "create time slicing um",
    "start": "2026960",
    "end": "2030158"
  },
  {
    "text": "replicas um so what I'm doing is uh on on the first node I'm applying the Mig",
    "start": "2032799",
    "end": "2038720"
  },
  {
    "text": "configuration on the second node I'm applying the time slicing configuration on the third node we are doing MPS configuration so we can see like",
    "start": "2038720",
    "end": "2045000"
  },
  {
    "text": "different replicas created for each of these so we can see the first node has",
    "start": "2045000",
    "end": "2051760"
  },
  {
    "text": "seven instance seven uh um instances Mig instances and the second node has three replicas and the third node has um one",
    "start": "2051760",
    "end": "2058240"
  },
  {
    "text": "replica and we configuring uh MPS on the third",
    "start": "2058240",
    "end": "2062878"
  },
  {
    "text": "node so on the third note we can see the MPS demon coming up uh MPS demon comes up um",
    "start": "2066720",
    "end": "2074079"
  },
  {
    "text": "and the device plugin um um device plugin and then the gfd will restart um so soon we'll see how the allocatable",
    "start": "2074079",
    "end": "2080398"
  },
  {
    "text": "gpus increase from from 0 to 10 replicas um that we are applying for",
    "start": "2080399",
    "end": "2086799"
  },
  {
    "text": "MPS so we can see 10 replicas on each of those node as you remember um so there are like one GPU on in each of the node",
    "start": "2087879",
    "end": "2094480"
  },
  {
    "text": "and now with the time slicing um and MPS we have multiple replicas uh for each of this notes so I'll go",
    "start": "2094480",
    "end": "2101520"
  },
  {
    "text": "into running some workloads um so with each of the the time slicing we run",
    "start": "2101520",
    "end": "2107720"
  },
  {
    "text": "workloads I have a MIG U Mig workloads um so we launch some time slicing work um workloads and also and",
    "start": "2107720",
    "end": "2114800"
  },
  {
    "text": "some with uh",
    "start": "2114800",
    "end": "2117520"
  },
  {
    "text": "MPS so time slicing we're launching a job with three replicas",
    "start": "2123240",
    "end": "2129640"
  },
  {
    "text": "um so we can see three replicas coming up um and similarly with MPS we launch 10 replicas and uh we can",
    "start": "2131480",
    "end": "2140480"
  },
  {
    "text": "see uh 10 replicas coming up for",
    "start": "2141560",
    "end": "2146960"
  },
  {
    "text": "MPS okay with that um we'll move on um so",
    "start": "2156640",
    "end": "2163880"
  },
  {
    "text": "some of the lesson learned um throughout this journey of developing operator is is containerizing drivers is not easy I",
    "start": "2163880",
    "end": "2170520"
  },
  {
    "text": "mean to it's easy to get the driver installs done but to manage the life cycle um it's very challenging so we",
    "start": "2170520",
    "end": "2177119"
  },
  {
    "text": "learned um some of these challenges and built uh updated Advanced controllers to manage these drivers and crd management",
    "start": "2177119",
    "end": "2184440"
  },
  {
    "text": "is another big issue um as the versions change as the API change you kind of end",
    "start": "2184440",
    "end": "2189599"
  },
  {
    "text": "up managing multiple versions of crd um and also with Helm we also have challenges in terms of updating crd so",
    "start": "2189599",
    "end": "2195960"
  },
  {
    "text": "we are learning through those and we also had some um some features to kind of handle this in the",
    "start": "2195960",
    "end": "2201760"
  },
  {
    "text": "operator uh memory consumption is another issue we are a cluster level operator so the client cache um gets",
    "start": "2201760",
    "end": "2207720"
  },
  {
    "text": "huge um we are looking into building a Le um using uh watching selected um um",
    "start": "2207720",
    "end": "2213720"
  },
  {
    "text": "resources so to to make sure that we don't consume too much memory and also we have a dependency on node labels uh",
    "start": "2213720",
    "end": "2219839"
  },
  {
    "text": "that is NFD um so um again like we we are looking into see how to make sure",
    "start": "2219839",
    "end": "2225560"
  },
  {
    "text": "that NFD um kind of don't bring down any of our components in case if it loses access to the API",
    "start": "2225560",
    "end": "2232960"
  },
  {
    "text": "server uh and here are the the quickly uh some of the upcoming features um",
    "start": "2232960",
    "end": "2238240"
  },
  {
    "text": "we're handl we're adding Better Health monitoring and Reporting with the GP operator um we we we are adding support",
    "start": "2238240",
    "end": "2244760"
  },
  {
    "text": "for kataa containers I mentioned this is Tech preview this is going to be be GA um and also we are adding confidential",
    "start": "2244760",
    "end": "2250160"
  },
  {
    "text": "container support uh that is going to be GA this year um and and and more on the heterogenous and and pre-compiled",
    "start": "2250160",
    "end": "2256040"
  },
  {
    "text": "drivers and Dr um obviously it's again a big topic uh with this conference Dr integration is another um main feature",
    "start": "2256040",
    "end": "2262880"
  },
  {
    "text": "that we are looking to integrate with the GPU operator um so these are the resources",
    "start": "2262880",
    "end": "2269359"
  },
  {
    "text": "so please um uh if you have any feedback um create an issue or reach out um we",
    "start": "2269359",
    "end": "2276000"
  },
  {
    "text": "it's a open source project so can reach out um create an issue um and this is",
    "start": "2276000",
    "end": "2281119"
  },
  {
    "text": "the documentation uh link so yeah please Pro provide feedback",
    "start": "2281119",
    "end": "2286760"
  },
  {
    "text": "um and and reach out to us",
    "start": "2286760",
    "end": "2291119"
  },
  {
    "text": "questions I have I have a question about your plans for the future operating",
    "start": "2297079",
    "end": "2302839"
  },
  {
    "text": "systems because in Germany Su Linux is uh used by lot of companies and it's",
    "start": "2302839",
    "end": "2309280"
  },
  {
    "text": "also uh the preferred operating system for sap thank you thanks for the question I",
    "start": "2309280",
    "end": "2315520"
  },
  {
    "text": "think yeah this comes up a lot uh we are looking into Su Linux um I think the main challenge we had is supporting",
    "start": "2315520",
    "end": "2321760"
  },
  {
    "text": "driver containers um so we we didn't have the the base images right base images to kind of build and publish",
    "start": "2321760",
    "end": "2327599"
  },
  {
    "text": "these so what we did was um so we we do support users to build their own container images we have a process we",
    "start": "2327599",
    "end": "2333960"
  },
  {
    "text": "have steps documented so users can build their own uh container images for driver and deploy through the GPU operator so",
    "start": "2333960",
    "end": "2340760"
  },
  {
    "text": "that part is uh users can still run them but officially um we are working through to see how we can publish and manage",
    "start": "2340760",
    "end": "2347079"
  },
  {
    "text": "through NGC so how to publish through NGC so something we are looking into yeah we can we can keep you posted is a",
    "start": "2347079",
    "end": "2353800"
  },
  {
    "text": "process official process to apply for sus to apply for for this uh build for",
    "start": "2353800",
    "end": "2360839"
  },
  {
    "text": "this that you build the or the build the images or make them generally available",
    "start": "2360839",
    "end": "2368040"
  },
  {
    "text": "um I think our product managers are usually uh going through the requirements uh maybe you can create an",
    "start": "2368040",
    "end": "2373520"
  },
  {
    "text": "issue GitHub issue um and we can definitely prioritize this um if you have a good use case right a lot of",
    "start": "2373520",
    "end": "2379839"
  },
  {
    "text": "customers are using Su so we can definitely prioritize this okay thank you thank",
    "start": "2379839",
    "end": "2386400"
  },
  {
    "text": "you hey hi thanks for the demo and session very useful um You probably",
    "start": "2386400",
    "end": "2392440"
  },
  {
    "text": "covered it but I just wanted to understand a little bit more in terms of GPU management and for efficiency right",
    "start": "2392440",
    "end": "2398480"
  },
  {
    "text": "you mentioned MPS and you also demoed MPS Mig and time slicing but what what",
    "start": "2398480",
    "end": "2404599"
  },
  {
    "text": "was not very clear for me at least was when to use what um is there like",
    "start": "2404599",
    "end": "2409839"
  },
  {
    "text": "scenarios or best practices as to what what what would be used when yeah it",
    "start": "2409839",
    "end": "2415560"
  },
  {
    "text": "depends a lot on your workload um and we have a blog post that tries to help walk",
    "start": "2415560",
    "end": "2421240"
  },
  {
    "text": "through why you might use one versus the other um we didn't link the blog post in",
    "start": "2421240",
    "end": "2426839"
  },
  {
    "text": "in these slides but if you uh go to the keynote talk that I gave on Wednesday I",
    "start": "2426839",
    "end": "2431880"
  },
  {
    "text": "have a link at the bottom of one of the slides that should help you make this decision there's a giant Matrix of what",
    "start": "2431880",
    "end": "2437480"
  },
  {
    "text": "advantages and disadvantages of the different approaches are and it also the blog post itself talks about types of",
    "start": "2437480",
    "end": "2442800"
  },
  {
    "text": "applications that could benefit from the different strategies and if you still have questions reach out to us",
    "start": "2442800",
    "end": "2448079"
  },
  {
    "text": "afterwards and we can try and help guide you based on your specific requirements there's no there's no one right answer I",
    "start": "2448079",
    "end": "2453839"
  },
  {
    "text": "guess is the the the short answer okay take a look thank you hi",
    "start": "2453839",
    "end": "2460880"
  },
  {
    "text": "there over the side um if if you're using uh a managed uh cloud provider for",
    "start": "2460880",
    "end": "2469160"
  },
  {
    "text": "AK something like AKs and you're selecting Nvidia no pools do people like",
    "start": "2469160",
    "end": "2474280"
  },
  {
    "text": "as your already manually install these drivers and then how will that work if I want to use the operator with some of",
    "start": "2474280",
    "end": "2479400"
  },
  {
    "text": "the advantages that has just wonder if you could speak to that yeah it's a great question so we we do support that",
    "start": "2479400",
    "end": "2485200"
  },
  {
    "text": "kind of configuration if you have U typical um by by default if you're using Azure Linux so they do have sorry Amazon Linux",
    "start": "2485200",
    "end": "2492200"
  },
  {
    "text": "they do have drivers pre-installed container toolkit pre-installed so we do support that kind of configuration you can still take advantage of the Mig",
    "start": "2492200",
    "end": "2498119"
  },
  {
    "text": "configuration or the Mig manager uh the toolkit container that we have you can still take care of all the advantage of",
    "start": "2498119",
    "end": "2504160"
  },
  {
    "text": "those components but what we are working with u with uh AKs or eks uh",
    "start": "2504160",
    "end": "2510240"
  },
  {
    "text": "specifically right to add support for their native operating systems itself in this case Amazon Linux we looking to add",
    "start": "2510240",
    "end": "2515440"
  },
  {
    "text": "support for Amazon Linux itself so you you can use the driver container instead of using a pre-installed uh driver management so that that is in the in the",
    "start": "2515440",
    "end": "2521960"
  },
  {
    "text": "works so it's it's actually possible if if if as your already had those drivers on um I can still put the operator on",
    "start": "2521960",
    "end": "2529680"
  },
  {
    "text": "and then kind of use some of the things where you were labeling the nodes to say set this up to be Meg or set inst it up",
    "start": "2529680",
    "end": "2537040"
  },
  {
    "text": "okay yeah that is a we have documented the process for that one so we automatically detect that the driver is pre-installed and we we disable the",
    "start": "2537040",
    "end": "2543160"
  },
  {
    "text": "driver container on those notes so yeah we support that thank you very much thank you",
    "start": "2543160",
    "end": "2549119"
  },
  {
    "text": "thank you uh so yes my first question was actually kind of the same so if uh",
    "start": "2551319",
    "end": "2556920"
  },
  {
    "text": "pre build images are already available in the cloud providers apparently yes uh so the second one um you mentioned that",
    "start": "2556920",
    "end": "2564640"
  },
  {
    "text": "uh it's a problem for monitoring the GPU Health uh from your experience what",
    "start": "2564640",
    "end": "2569920"
  },
  {
    "text": "would be the main issues that would happen because from what I've seen if you have issues with your gpus your",
    "start": "2569920",
    "end": "2577319"
  },
  {
    "text": "machine would not boot up or everything would freeze or let me answer the first question so um in general all of the",
    "start": "2577319",
    "end": "2585680"
  },
  {
    "text": "operands that the GPU operator manages you can decide if you want them to be",
    "start": "2585680",
    "end": "2591000"
  },
  {
    "text": "managed by the operator or pre-installed on the system so if you want to use the host manage you if you've installed your",
    "start": "2591000",
    "end": "2596440"
  },
  {
    "text": "driver manually on the host and you don't want the operator to manage the driver life cycle you can turn off it",
    "start": "2596440",
    "end": "2601599"
  },
  {
    "text": "using the driver container if you've manually installed the Nvidia container toolkit on your host you can tell the",
    "start": "2601599",
    "end": "2607119"
  },
  {
    "text": "operator you don't want it to install that because you've already done that step same thing with any of the other components so there's there's uh options",
    "start": "2607119",
    "end": "2614319"
  },
  {
    "text": "you can set when you deploy the helm chart as to what you want on versus off when you deploy it um and then the",
    "start": "2614319",
    "end": "2620200"
  },
  {
    "text": "second question maybe you can uh in terms of error monitoring um that is true we can't recover most of the time",
    "start": "2620200",
    "end": "2626200"
  },
  {
    "text": "but U currently what is lacking today is properly propagating those errors at the kubernetes level right when when a kuet",
    "start": "2626200",
    "end": "2632240"
  },
  {
    "text": "level uh there is no indication at the node level saying if some gpus is unhealthy and what is the issue with the GPU so what what what is happening today",
    "start": "2632240",
    "end": "2639640"
  },
  {
    "text": "is the device plugin will detect these errors and make sure that that's not allocatable anymore that's all so you'll see um the number of allocatable gpus is",
    "start": "2639640",
    "end": "2647000"
  },
  {
    "text": "going down but there is no indication in terms of which GPU has gone bad or what is the error with the GPU so this where",
    "start": "2647000",
    "end": "2652680"
  },
  {
    "text": "we are improving saying we'll propagate those as a node conditions and say that okay this is GPU has gone bad um and how",
    "start": "2652680",
    "end": "2658880"
  },
  {
    "text": "to kind of recover those things recover that no yeah it's also worth pointing out that we're internally right now",
    "start": "2658880",
    "end": "2664839"
  },
  {
    "text": "trying to come up with a more comprehensive solution for not just GPU Health but node Health in general and",
    "start": "2664839",
    "end": "2670880"
  },
  {
    "text": "the plan would be to eventually integrate this kind of node GPU Health Solution as an operand that the operator",
    "start": "2670880",
    "end": "2678079"
  },
  {
    "text": "can deploy and people can make use of we we're still trying to figure out what that's going to look like but that's the",
    "start": "2678079",
    "end": "2683160"
  },
  {
    "text": "long-term plan thank you thank you uh hey you mentioned the for GPU driver you",
    "start": "2683160",
    "end": "2689960"
  },
  {
    "text": "have cized the driver so can you explain a little bit between the native GPU",
    "start": "2689960",
    "end": "2696559"
  },
  {
    "text": "driver and the containerized the driver what's the difference yeah yeah so um so",
    "start": "2696559",
    "end": "2702960"
  },
  {
    "text": "first off there there was a talk yesterday um he referenced it Shiva referenced it in his slides here that",
    "start": "2702960",
    "end": "2708240"
  },
  {
    "text": "goes into great detail what our uh host operator sorry our our GPU operator U",
    "start": "2708240",
    "end": "2714520"
  },
  {
    "text": "managed driver looks like and how we manage the life cycle of it but the the main idea is that you know it's it's",
    "start": "2714520",
    "end": "2720079"
  },
  {
    "text": "kind of a misnomer we call it a containerized driver but it's really a driver installer wrapped inside of a",
    "start": "2720079",
    "end": "2725839"
  },
  {
    "text": "container and so what it does is when you run this container it will install the driver the kernel module into the",
    "start": "2725839",
    "end": "2731920"
  },
  {
    "text": "host it'll install the user space libraries into the Container image that you have but then it'll make that the",
    "start": "2731920",
    "end": "2738160"
  },
  {
    "text": "the root of that container's image uh or the the container that's running it'll make that the root of that file system",
    "start": "2738160",
    "end": "2744079"
  },
  {
    "text": "available back on the host so that from the host's perspective he has a path to using that driver from anything that's",
    "start": "2744079",
    "end": "2750480"
  },
  {
    "text": "running on the host directly so it looks different from what you install directly on the host only really by where the",
    "start": "2750480",
    "end": "2756000"
  },
  {
    "text": "path to those user space libraries exist okay so that mean does it mean",
    "start": "2756000",
    "end": "2761760"
  },
  {
    "text": "Nvidia will maintain this kind of two different GPU driver version and user",
    "start": "2761760",
    "end": "2766880"
  },
  {
    "text": "will have option to choose one of them you can't have both installed at the same time you either do a host install",
    "start": "2766880",
    "end": "2772119"
  },
  {
    "text": "driver or you use the driver container you can't do them both at the same time so either way but both will be",
    "start": "2772119",
    "end": "2779480"
  },
  {
    "text": "coexist they Co will be coexist as a project it's the same it's the same driver so whether you're installing it",
    "start": "2779480",
    "end": "2786359"
  },
  {
    "text": "directly on the host and then you get it at the root file system where all these files are or you install it in the container think of the container is just",
    "start": "2786359",
    "end": "2793119"
  },
  {
    "text": "the the OS where you're installing this now it's the exact same driver that you're installing though it's just the method at which it's being made",
    "start": "2793119",
    "end": "2799559"
  },
  {
    "text": "available to software on the host I say okay thank you I don't know if you mean in by the same project if you mean same",
    "start": "2799559",
    "end": "2805760"
  },
  {
    "text": "cluster so then there still possible to have some nodes pre-install driver I think what I think what he meant was are",
    "start": "2805760",
    "end": "2811079"
  },
  {
    "text": "we maintaining different drivers whether it's containerized versus not it's the same driver at the end same D yeah same",
    "start": "2811079",
    "end": "2816319"
  },
  {
    "text": "driver yeah okay thank you yeah thanks for your good talk so you",
    "start": "2816319",
    "end": "2822000"
  },
  {
    "text": "mentioned catac container in your talk so I want to enhance our isolation um so",
    "start": "2822000",
    "end": "2828800"
  },
  {
    "text": "I want to use it but as far as I know the catac container is only support don't support doesn't support a mar GPU",
    "start": "2828800",
    "end": "2836000"
  },
  {
    "text": "or a bgpu right so do you have a plan to uh improve their C container to support",
    "start": "2836000",
    "end": "2842440"
  },
  {
    "text": "it definitely it's in the plan so this in the road map to add support for multiple GP and also vgpus so so I think",
    "start": "2842440",
    "end": "2849160"
  },
  {
    "text": "the current Focus for us is to to take a single GPU pass through as a ga right and then the confidential containers",
    "start": "2849160",
    "end": "2855480"
  },
  {
    "text": "with single GPU so then eventually or or or this or next year we plan we'll plan",
    "start": "2855480",
    "end": "2860960"
  },
  {
    "text": "to add vgpu and multi-gpu support okay thanks yeah thank",
    "start": "2860960",
    "end": "2866000"
  },
  {
    "text": "you all right I think that's all the questions we have time for thanks everyone thanks thanks",
    "start": "2866000",
    "end": "2871119"
  },
  {
    "text": "everyone",
    "start": "2871119",
    "end": "2874119"
  }
]