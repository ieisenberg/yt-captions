[
  {
    "text": "uh good afternoon uh welcome to a session so my name is Olivia Tardu i'm a",
    "start": "240",
    "end": "5359"
  },
  {
    "text": "principal research scientist and manager at IBM research and today I'm joined with my by my IBM research colleague",
    "start": "5359",
    "end": "11519"
  },
  {
    "text": "Dave Groove and and Claudia Misali and we're going to talk about how to build",
    "start": "11519",
    "end": "17520"
  },
  {
    "text": "operate and use GPU clusters for AI right so by now we've all heard about AI",
    "start": "17520",
    "end": "24400"
  },
  {
    "text": "and Gen AI we are all probably excited about it but we're also sometimes a little bit anxious about it for lots of",
    "start": "24400",
    "end": "30720"
  },
  {
    "text": "reasons and as an organization maybe the first reason to be anxious about AI is to actually procure GPU to do AI right",
    "start": "30720",
    "end": "38239"
  },
  {
    "text": "it's both time consuming expensive slow you know whether we rent them from a",
    "start": "38239",
    "end": "43600"
  },
  {
    "text": "cloud provider whether we buy them whether we lease them whether we want a dozen of them or thousand GPUs it's a",
    "start": "43600",
    "end": "49680"
  },
  {
    "text": "big problem but someday we'll get them and and then the second panic arises right it's how can we take these GPUs",
    "start": "49680",
    "end": "57120"
  },
  {
    "text": "this pool of resource and somehow share it across all the teams all the users we have in our organization all the",
    "start": "57120",
    "end": "63120"
  },
  {
    "text": "projects that need access to these resources so that we can manage them and we can maximize utilization and we can",
    "start": "63120",
    "end": "70000"
  },
  {
    "text": "maximize the return on investment so this is something we've been working on on IBM research on our own cluster for",
    "start": "70000",
    "end": "76960"
  },
  {
    "text": "several years now and we've essentially built a point of view uh incrementally",
    "start": "76960",
    "end": "82240"
  },
  {
    "text": "refined methodology uh a setup and we're going to try to share this with you today uh with the",
    "start": "82240",
    "end": "89040"
  },
  {
    "text": "hope that it can help you with your own uh you know AI journey and also that you will tell us everything we could do",
    "start": "89040",
    "end": "95240"
  },
  {
    "text": "better so let's get started so um in uh",
    "start": "95240",
    "end": "100479"
  },
  {
    "text": "what we're going to show you today is entirely open source uh everything is available online including this tutorial",
    "start": "100479",
    "end": "106640"
  },
  {
    "text": "so if you go to this QR code and this URL you'll find uh all the comments all",
    "start": "106640",
    "end": "112960"
  },
  {
    "text": "the scripts all the uh YAML that we're going to use today and you can you",
    "start": "112960",
    "end": "118560"
  },
  {
    "text": "conceivably follow along and and or you know replay that at home right so uh",
    "start": "118560",
    "end": "123920"
  },
  {
    "text": "because we're going to talk about GPUs and I don't typically travel with two dozen GPUs in my backpack uh this is all",
    "start": "123920",
    "end": "130879"
  },
  {
    "text": "going to be recorded demos but uh you know on a real GPU cluster so I'm going",
    "start": "130879",
    "end": "136720"
  },
  {
    "text": "to start by a brief background on a IML workload on the kind of hardware with",
    "start": "136720",
    "end": "141920"
  },
  {
    "text": "targets on what exactly are the key components and maybe the key goals of the platform we've been developing at",
    "start": "141920",
    "end": "148319"
  },
  {
    "text": "IBM research and then we'll get into the the the the deep dive of the tutorial",
    "start": "148319",
    "end": "153360"
  },
  {
    "text": "how do we set up a cluster how do we on board users how do we do things like monitoring forance and also what kind of",
    "start": "153360",
    "end": "161440"
  },
  {
    "text": "workload templates we give our new users to get them started on these kind of of",
    "start": "161440",
    "end": "169000"
  },
  {
    "text": "clusters so um let's start with AI workloads uh again it's probably not",
    "start": "169000",
    "end": "174640"
  },
  {
    "text": "worth spending too much time on this because you probably heard a lot about that before there's essentially an",
    "start": "174640",
    "end": "180160"
  },
  {
    "text": "entire zoo of workload the collections of things we want to do all the way from having the idea of a genai application",
    "start": "180160",
    "end": "186720"
  },
  {
    "text": "to actually running this application in production right and what we're interested in discussing today is this",
    "start": "186720",
    "end": "193599"
  },
  {
    "text": "piece of the pipeline again it's more than a pipeline all kinds of feedback loops bypass and so on that are uh",
    "start": "193599",
    "end": "199680"
  },
  {
    "text": "dependent on accelerators such as GPUs and that are typically batch workloads right it's not about running the",
    "start": "199680",
    "end": "206080"
  },
  {
    "text": "application at the end of the day you have lots of other talks at CubeCon about running large language model",
    "start": "206080",
    "end": "212080"
  },
  {
    "text": "running identic frameworks and so on we're not going really to get into that today because you know also in sense",
    "start": "212080",
    "end": "217920"
  },
  {
    "text": "it's the scope really there's a lot of optimization and special things you want to do for that what we're going to focus",
    "start": "217920",
    "end": "223360"
  },
  {
    "text": "on is everything that happens before that right everything starting from things like data preparation even you",
    "start": "223360",
    "end": "229840"
  },
  {
    "text": "know like when we do a profanity filtering for instance on on our data app before we train models we actually",
    "start": "229840",
    "end": "235840"
  },
  {
    "text": "use already machine learning models to do that so we need GPUs we need a lot of resources you know training models",
    "start": "235840",
    "end": "243040"
  },
  {
    "text": "fine-tuning models running batch inference for instance for validation of other kinds of workloads this is the",
    "start": "243040",
    "end": "248640"
  },
  {
    "text": "scope of what we're trying to run on these clusters Right other than that uh what's also interesting about AIM",
    "start": "248640",
    "end": "255439"
  },
  {
    "text": "workloads is they are very intensive so they can use lots and lots of GPUs at once and run for a very long time and",
    "start": "255439",
    "end": "262720"
  },
  {
    "text": "finally um they depend a lot on a IML",
    "start": "262720",
    "end": "268080"
  },
  {
    "text": "frameworks like for instance PyTorch and having good support for that out of the box is really critical for enabling",
    "start": "268080",
    "end": "274720"
  },
  {
    "text": "users to do anything with these clusters so from these requirements you can see uh on the other side of the slide a",
    "start": "274720",
    "end": "281759"
  },
  {
    "text": "number of of uh or from these properties you can see a number of requirements arising such as how we're going to queue",
    "start": "281759",
    "end": "288800"
  },
  {
    "text": "workloads how going to prioritize them manage quotas between teams how do do we",
    "start": "288800",
    "end": "293840"
  },
  {
    "text": "deal with large workloads in particular with this uh kind of hardware I think uh",
    "start": "293840",
    "end": "299680"
  },
  {
    "text": "everybody in the industry has been reporting that GPU nodes are more complex than traditional nodes therefore",
    "start": "299680",
    "end": "306240"
  },
  {
    "text": "they have more maybe creative ways ways to fail we have longer running workloads",
    "start": "306240",
    "end": "311520"
  },
  {
    "text": "how do we do fault detection and all of these things so uh in terms of clusters uh",
    "start": "311520",
    "end": "318800"
  },
  {
    "text": "what we typically run at IBM research is uh large clusters with uh Nvidia GPUs",
    "start": "318800",
    "end": "324560"
  },
  {
    "text": "like A100 or H100 so here you have two kind of node generations we've used or",
    "start": "324560",
    "end": "330000"
  },
  {
    "text": "we're using uh that we've published uh you know detailed specs for so on the left the more like the first generation",
    "start": "330000",
    "end": "337120"
  },
  {
    "text": "A100 nodes uh on the right you have something more recent which is an H100",
    "start": "337120",
    "end": "342680"
  },
  {
    "text": "node and you know the key characteristics of these nodes is they have a high density of GPUs right so",
    "start": "342680",
    "end": "349600"
  },
  {
    "text": "they have eight GPUs in both of these pictures and then lots of compute you know CPUs memory local storage uh high",
    "start": "349600",
    "end": "358240"
  },
  {
    "text": "performance network and so on so that's the kind of things we want to run with so and finally want to briefly tell you",
    "start": "358240",
    "end": "365919"
  },
  {
    "text": "a little bit about what we're going to see in the rest of the tutorial what is the kind of platform we're trying to build on because I think we're all",
    "start": "365919",
    "end": "372639"
  },
  {
    "text": "engineers here rather than starting with abstract goals let's be very concrete and start by looking a little bit at the",
    "start": "372639",
    "end": "378720"
  },
  {
    "text": "components right this is the kind of things we put together in this platform and we're going to review one step at a",
    "start": "378720",
    "end": "385360"
  },
  {
    "text": "time right of course in the middle you have Kubernetes we'll start there and it's not just because it's CubeCon i",
    "start": "385360",
    "end": "391360"
  },
  {
    "text": "think yesterday in cloud AI day we had somebody bold enough to ask the question why Kubernetes and I think for us the",
    "start": "391360",
    "end": "397680"
  },
  {
    "text": "answer is really simple is because of the vitality of the community and the you know the the the the CNCF landscape",
    "start": "397680",
    "end": "405120"
  },
  {
    "text": "right how many projects how many things we can leverage from the community",
    "start": "405120",
    "end": "410319"
  },
  {
    "text": "contribute to uh the rate of innovation that makes if any you know is second to",
    "start": "410319",
    "end": "416400"
  },
  {
    "text": "none right any other choice would require a lot more work to get to a platform And so that means not only we",
    "start": "416400",
    "end": "422479"
  },
  {
    "text": "have Kubernetes we have projects in the Kubernetes community like Q but you know if you take something like PyTorch which",
    "start": "422479",
    "end": "428400"
  },
  {
    "text": "has nothing to do with Kubernetes in the first place we do get things like Qflow trainer that the Qflow community has",
    "start": "428400",
    "end": "434880"
  },
  {
    "text": "developed so that you know we can uh easily uh run PyTorch on Kubernetes we",
    "start": "434880",
    "end": "441120"
  },
  {
    "text": "also have the on the right hand side some of the new projects we as IBM research have introduced where we found",
    "start": "441120",
    "end": "447120"
  },
  {
    "text": "that the maybe the the community landscape we're still lacking some some critical capabilities for what we're",
    "start": "447120",
    "end": "452960"
  },
  {
    "text": "trying to do and we'll get to that in a few minutes so what is you know the goal",
    "start": "452960",
    "end": "460240"
  },
  {
    "text": "of our stack again at the high level it's it's to manage workloads and resources on Kubernetes cluster for AI",
    "start": "460240",
    "end": "468560"
  },
  {
    "text": "uh it is has to be open source you know the components the upstream components",
    "start": "468560",
    "end": "474080"
  },
  {
    "text": "the new components the combination the composition of these components everything I'll repeat that multiple",
    "start": "474080",
    "end": "479840"
  },
  {
    "text": "times throughout the the talk it has to integrate key a IML framework it is",
    "start": "479840",
    "end": "485520"
  },
  {
    "text": "about multi-users right within an organization how to manage having many users without conflict popping up every",
    "start": "485520",
    "end": "492319"
  },
  {
    "text": "every day between them is about productive utilization because at the end of the day we're paying a lot of",
    "start": "492319",
    "end": "497759"
  },
  {
    "text": "money for these GPUs they have to be running all the time doing something useful all the time and and we insist on",
    "start": "497759",
    "end": "504400"
  },
  {
    "text": "fra and of course uh to the side you have monitoring observability and so on that always are critical uh piece of",
    "start": "504400",
    "end": "511360"
  },
  {
    "text": "such an infrastructure so um before I move on as well again I said open source so",
    "start": "511360",
    "end": "518880"
  },
  {
    "text": "everything we're doing today is open source everything is meant to run on vanilla kubernetes but obviously IBM",
    "start": "518880",
    "end": "525120"
  },
  {
    "text": "redat we work together also provided supported versions of this kind of technology so in this space what we're",
    "start": "525120",
    "end": "530800"
  },
  {
    "text": "doing is working on open shift AI so if you want uh a supported solution that",
    "start": "530800",
    "end": "536480"
  },
  {
    "text": "essentially overlaps what we're describing today this is uh you'll find this in open shift AI open shift AI is",
    "start": "536480",
    "end": "542399"
  },
  {
    "text": "does more things than what we're discussing today for instance looking at how to run applications at the end of",
    "start": "542399",
    "end": "547440"
  },
  {
    "text": "the day ml match does more thing because it's a research platform and we're trying to figure out the next thing and",
    "start": "547440",
    "end": "552959"
  },
  {
    "text": "the next thing and the next thing we want to add to Open Shift AI but again for the rest of the talk you can forget",
    "start": "552959",
    "end": "558399"
  },
  {
    "text": "about that i should also say again to be clear that we are going to run our demo on an open shift cluster because this is",
    "start": "558399",
    "end": "564320"
  },
  {
    "text": "the Kubernetes flavor we prefer in IBM but there's nothing that is really",
    "start": "564320",
    "end": "570480"
  },
  {
    "text": "dependent on that so um here's uh let's start from the top",
    "start": "570480",
    "end": "578640"
  },
  {
    "text": "here's uh to give you a sense of the cluster we're going to run this uh",
    "start": "578640",
    "end": "584800"
  },
  {
    "text": "tutorial on it's it's it's a small version of what we run in practice it is",
    "start": "584800",
    "end": "590240"
  },
  {
    "text": "a cluster with six nodes three of them are control plane nodes and three of them are worker nodes and you know the",
    "start": "590240",
    "end": "597760"
  },
  {
    "text": "the critical bit here is that this piece of information is that these worker nodes have a number of Nvidia GPUs you",
    "start": "597760",
    "end": "604560"
  },
  {
    "text": "know three times 8 GPU 24 GPUs and vlink and what you know I had pictured a",
    "start": "604560",
    "end": "610080"
  },
  {
    "text": "little bit earlier this cluster is kind of in between the two generations I was showing you",
    "start": "610080",
    "end": "616200"
  },
  {
    "text": "before so um before we start you know right now we have a base cluster nothing",
    "start": "616200",
    "end": "622000"
  },
  {
    "text": "on it uh we need two prerequisites essentially we need to fulfill two prerequisite because before we move on",
    "start": "622000",
    "end": "627839"
  },
  {
    "text": "to actually installing ML batch on this cluster the first one is we have to teach the cluster uh how to manage GPUs",
    "start": "627839",
    "end": "635120"
  },
  {
    "text": "as you probably know Kubernetes out of the box understands CPU understands memory ephemeral storage you know has a",
    "start": "635120",
    "end": "641440"
  },
  {
    "text": "s certain number of resources built in into the scheduleuler but not GPUs in order to do GPUs Nvidia GPU in this case",
    "start": "641440",
    "end": "648720"
  },
  {
    "text": "you need to install uh Nvidia GPU operator you know was jointly developed",
    "start": "648720",
    "end": "653839"
  },
  {
    "text": "by Red Hat and Nvidia and that will add an extended resource type to your cluster",
    "start": "653839",
    "end": "659079"
  },
  {
    "text": "nvidia.com/gpu and that means now you can start having workloads with GPU so we've done this beforehand you know it's",
    "start": "659079",
    "end": "665680"
  },
  {
    "text": "open shift so we click on the boot button on the console it gets deployed but if you have a van like kubernetes",
    "start": "665680",
    "end": "670720"
  },
  {
    "text": "cluster and has a lot of documentation online how you can go and deploy a hamm",
    "start": "670720",
    "end": "676320"
  },
  {
    "text": "chart on your cluster and have all these uh set up for you so we're not going to review that second we're doing AI AI",
    "start": "676320",
    "end": "684320"
  },
  {
    "text": "depends a lot on data therefore storage so we need some kind of storage solution for our cluster so in practice we want",
    "start": "684320",
    "end": "692160"
  },
  {
    "text": "to have high performance storage solutions for instance IBM we use IBM spec spectrum scale but for the purpose",
    "start": "692160",
    "end": "697760"
  },
  {
    "text": "of this demo and what I'm including here is that we can let's say just assume that we have storage available through",
    "start": "697760",
    "end": "703600"
  },
  {
    "text": "an NSF server on a local subnets and then you know just following uh adding",
    "start": "703600",
    "end": "708880"
  },
  {
    "text": "an Am repository and installing an HM chart we can make this available on the cluster as a as a as a storage class and",
    "start": "708880",
    "end": "716640"
  },
  {
    "text": "from that we can create persistent volume claims and access storage and and",
    "start": "716640",
    "end": "721760"
  },
  {
    "text": "so So now we move on to the cluster setup",
    "start": "721760",
    "end": "729320"
  },
  {
    "text": "itself right and there are two parts to it first when we set up a cluster there's what we do once you know what",
    "start": "729320",
    "end": "736720"
  },
  {
    "text": "what does the admin do when we create the cluster and you know I normally",
    "start": "736720",
    "end": "742399"
  },
  {
    "text": "never has to worry again about right and then there's a second step which will be the team and user setup how do we on",
    "start": "742399",
    "end": "749440"
  },
  {
    "text": "board teams how do we create user how do we give them the right permissions so I'm going to review these two things and",
    "start": "749440",
    "end": "755760"
  },
  {
    "text": "to start with the cluster setup is is is pretty straightforward uh you can do this by cloning the",
    "start": "755760",
    "end": "761680"
  },
  {
    "text": "repository tree where we have all of our supporting again tools or you know scripts and so on we set up priority",
    "start": "761680",
    "end": "768800"
  },
  {
    "text": "classes on our cluster in practice we've seen that having low medium and high priority um uh AI workloads is is enough",
    "start": "768800",
    "end": "776320"
  },
  {
    "text": "for what we need we deploy the key components some of them are scheduled plugins some of them are operators we",
    "start": "776320",
    "end": "782160"
  },
  {
    "text": "are going to actually talk about each one of them in in sequence in the next hour or so and and so I'm not going to",
    "start": "782160",
    "end": "789440"
  },
  {
    "text": "describe them more here as I said we can deploy them through Open Shift AI or without we configure Q",
    "start": "789440",
    "end": "797360"
  },
  {
    "text": "which is our uh queuing system we'll talk about that and we finally have to",
    "start": "797360",
    "end": "802399"
  },
  {
    "text": "create a role that is the role we're going to give users to let give them permissions to do I mean to run",
    "start": "802399",
    "end": "809000"
  },
  {
    "text": "workloads monitor workloads debug workloads on this cluster so that's the cluster setup and that's that's pretty",
    "start": "809000",
    "end": "817040"
  },
  {
    "text": "much it right then we have the team setup uh and again that's that's pretty",
    "start": "817040",
    "end": "823440"
  },
  {
    "text": "boilerplate right in our uh experience it's uh makes a lot of sense to",
    "start": "823440",
    "end": "828720"
  },
  {
    "text": "associate a team and a namespace so when we want a new team on the cluster we create a new namespace and then we have",
    "start": "828720",
    "end": "834720"
  },
  {
    "text": "to only do three other things we have to set up a queue for that team which is uh",
    "start": "834720",
    "end": "840800"
  },
  {
    "text": "where the jobs submitted by that that team will be cued we have to make this queue the local the default for that",
    "start": "840800",
    "end": "847920"
  },
  {
    "text": "particular team which is the local Q definition on the left and finally here we're creating uh user Alice in team",
    "start": "847920",
    "end": "854959"
  },
  {
    "text": "blue we have to give Alice this uh this arbback this role that I was describing",
    "start": "854959",
    "end": "860720"
  },
  {
    "text": "earlier in the namespace blue so that now Alice is officially a member of team blue and can use the capabilities of the",
    "start": "860720",
    "end": "867360"
  },
  {
    "text": "cluster in that team but I forget maybe you know we need to label the namespace so that we say MLB batch is managing the",
    "start": "867360",
    "end": "874079"
  },
  {
    "text": "lamespace and really the only thing that is not entirely boilerplate on this slide is what I've highlighted on the",
    "start": "874079",
    "end": "879600"
  },
  {
    "text": "right it's defining how much quotota we're giving to this team right so here we're saying well team blue should have",
    "start": "879600",
    "end": "887199"
  },
  {
    "text": "nominally access to eight GPU on that cluster so I'm saying nominal here uh",
    "start": "887199",
    "end": "893760"
  },
  {
    "text": "and we'll get back to that but essentially uh you know they they will go into great details into quotas that",
    "start": "893760",
    "end": "899440"
  },
  {
    "text": "doesn't mean that team blue is limited to eight GPUs if nobody else is running on the cluster they could access the",
    "start": "899440",
    "end": "905279"
  },
  {
    "text": "entire cluster but that means they essentially have guaranteed access to eight GPUs on the",
    "start": "905279",
    "end": "911240"
  },
  {
    "text": "cluster so I have a video going through that uh that you can also find on",
    "start": "911240",
    "end": "916880"
  },
  {
    "text": "YouTube i'm going to go through so this is an 8 minute video on YouTube because",
    "start": "916880",
    "end": "922880"
  },
  {
    "text": "I try to describe what's going on as I'm running this but really fundamentally uh",
    "start": "922880",
    "end": "928480"
  },
  {
    "text": "this is uh just a minute to do this setup and this is pretty boring so I'm",
    "start": "928480",
    "end": "934480"
  },
  {
    "text": "going to go quickly uh again you have the tutorial scripts I've described uh",
    "start": "934480",
    "end": "939600"
  },
  {
    "text": "you know I gave a pointer to earlier and what we do in this in this setup is we go and we copy paste instructions from",
    "start": "939600",
    "end": "947519"
  },
  {
    "text": "the left to the terminal on the right so first what I'm showing on the right is what I was showing before on the slide",
    "start": "947519",
    "end": "953120"
  },
  {
    "text": "that we have this cluster with uh 24 GPUs available so if I for instance connect to one of the nodes I can see",
    "start": "953120",
    "end": "960240"
  },
  {
    "text": "that there one of the worker nodes I can see I have eight GPUs available here uh",
    "start": "960240",
    "end": "965440"
  },
  {
    "text": "second I can check that the GPU operator is already installed on the cluster so we can see the oper GPU operator is only",
    "start": "965440",
    "end": "971680"
  },
  {
    "text": "already installed which means if I query the capacity of the node of a worker",
    "start": "971680",
    "end": "977440"
  },
  {
    "text": "node I can see here that I have eight GPUs available as the capacity of that particular node and then I can set up my",
    "start": "977440",
    "end": "984880"
  },
  {
    "text": "storage class um which I was again showing earlier again it's as simple as",
    "start": "984880",
    "end": "991600"
  },
  {
    "text": "copy pasting the setup so this particular case and we're doing something simple a simple storage class",
    "start": "991600",
    "end": "999040"
  },
  {
    "text": "you know the way we you would set up storage will depend quite a lot about the kind of storage solution you adopt",
    "start": "999040",
    "end": "1004880"
  },
  {
    "text": "for a cluster also what's maybe oversimplified here where have no authentication of any kind we're just",
    "start": "1004880",
    "end": "1011040"
  },
  {
    "text": "assuming the the storage server is on the same local network and that's where the security is happening right nothing",
    "start": "1011040",
    "end": "1018240"
  },
  {
    "text": "no credentials and so on so real setup will have a little bit more than there and then we move on to the MLB batch",
    "start": "1018240",
    "end": "1024480"
  },
  {
    "text": "setup itself and you know this is just running uh a long script that has all",
    "start": "1024480",
    "end": "1031678"
  },
  {
    "text": "the components of of ML batch I was describing earlier or we'll describe later actually such as QRF flow and so",
    "start": "1031679",
    "end": "1040760"
  },
  {
    "text": "on that's that's all it takes to set up the cluster The last thing I didn't",
    "start": "1040760",
    "end": "1046400"
  },
  {
    "text": "mention in the slides is we're also going to reserve some capacity in the",
    "start": "1046400",
    "end": "1051520"
  },
  {
    "text": "cluster as an admin queue for admin to do maintenance operation and again this is something we'll explain in details in",
    "start": "1051520",
    "end": "1058320"
  },
  {
    "text": "two minutes in that particular setup I'm also reserving eight GPUs for this admin queue that's it for the cluster setup",
    "start": "1058320",
    "end": "1066160"
  },
  {
    "text": "then you have the team setup here for the purpose of the demo we're going to actually set up two teams team blue",
    "start": "1066160",
    "end": "1073200"
  },
  {
    "text": "actually I went too fast we we set up team blue with user Alice as we've shown in the slide we're also setting up team",
    "start": "1073200",
    "end": "1080000"
  },
  {
    "text": "red with user Bob which is again the exact same thing with the two names",
    "start": "1080000",
    "end": "1086440"
  },
  {
    "text": "replaced i think that's it for the video so we can close this that's it for the",
    "start": "1086440",
    "end": "1092640"
  },
  {
    "text": "cluster setup uh before I pass on to uh Dave I want to describe the very first",
    "start": "1092640",
    "end": "1097840"
  },
  {
    "text": "components of the setup very briefly uh happy to answer questions um uh maybe",
    "start": "1097840",
    "end": "1103520"
  },
  {
    "text": "after the tutorial if you care about that particular piece one of the components of our um MLB batch setup is",
    "start": "1103520",
    "end": "1109600"
  },
  {
    "text": "to actually deploy and configure schedule plugins for a number of reasons",
    "start": "1109600",
    "end": "1114919"
  },
  {
    "text": "essentially this is about trying to avoid fragmentation in the cluster so that's the purpose of the node resource",
    "start": "1114919",
    "end": "1121360"
  },
  {
    "text": "fit schedule plug-in what it says is if I have let's say a workload that requires one GPU I should try to fit it",
    "start": "1121360",
    "end": "1127919"
  },
  {
    "text": "in a small hole in my cluster right if I have a node that has already seven GPU utilized and one left I'd better put it",
    "start": "1127919",
    "end": "1134320"
  },
  {
    "text": "there than put it on a on a node that is entirely empty because if later I want to run an AGPU job of my cluster then",
    "start": "1134320",
    "end": "1142000"
  },
  {
    "text": "you know I'm no longer going to be possible if I fragmented my cluster so we use schedule plug-in to try to avoid",
    "start": "1142000",
    "end": "1149000"
  },
  {
    "text": "fragmentation we use scheduleuler plugin to uh do gang scheduling the idea there",
    "start": "1149000",
    "end": "1154160"
  },
  {
    "text": "is if you have a large u distributed a IML workload that needs 20 256 pods to",
    "start": "1154160",
    "end": "1160880"
  },
  {
    "text": "run at the same time we want to make sure we can fit the 256 pods before we actually deploy any of those pods",
    "start": "1160880",
    "end": "1167600"
  },
  {
    "text": "because deploying 250 pods is completely useless for most AI workloads so we use",
    "start": "1167600",
    "end": "1172640"
  },
  {
    "text": "cocheduling for that finally we have an experimental uh plugin we've also developed at IBM research to do topology",
    "start": "1172640",
    "end": "1179360"
  },
  {
    "text": "aware scheduling uh this is this is something that is coming in the community is becoming important q is",
    "start": "1179360",
    "end": "1185280"
  },
  {
    "text": "working on that a number of people are working on that the idea is there is to uh if you have a distributed workloads",
    "start": "1185280",
    "end": "1191840"
  },
  {
    "text": "we're trying to make sure we distribute it as as few racks as possible to maximize the bandwidth minimize the",
    "start": "1191840",
    "end": "1197840"
  },
  {
    "text": "latency of communication between the different components dave just take it",
    "start": "1197840",
    "end": "1204200"
  },
  {
    "text": "good right so thanks so now what we're going to do is we're going to dive in into some detail into some of the components in the stack starting with",
    "start": "1204200",
    "end": "1210080"
  },
  {
    "text": "how we do queuing and quota management um and for MLB batch for queuing and quota management um why do we want",
    "start": "1210080",
    "end": "1216559"
  },
  {
    "text": "quotas so quotas we want to set up these large shared clusters so we can make more efficient uses of resources so we can have large clusters with a large",
    "start": "1216559",
    "end": "1222640"
  },
  {
    "text": "number of GPUs on them we can share them across a bunch of teams and that lets us both share the resource pool so people",
    "start": "1222640",
    "end": "1227919"
  },
  {
    "text": "can have a little bit more funible resource pool and we share the control plane and we also share the maintenance the admins it takes to run the run these",
    "start": "1227919",
    "end": "1234000"
  },
  {
    "text": "things and set them up it's easier to set up a small number of large clusters and a large number of small clusters um",
    "start": "1234000",
    "end": "1239520"
  },
  {
    "text": "but native Kubernetes quotas by themselves are not enough right so quotas first of all quotas aren't are",
    "start": "1239520",
    "end": "1244559"
  },
  {
    "text": "done on on the level of pods quotas aren't aware of workloads they aren't aware that you're actually running something that contains a lot of pods",
    "start": "1244559",
    "end": "1250320"
  },
  {
    "text": "and so the quota needs to be done on a sort of workload granularity not the individual uh fine grain resource",
    "start": "1250320",
    "end": "1255919"
  },
  {
    "text": "secondly quotas are are are fairly inflexible so this picture on the right I have I want to take my cluster and",
    "start": "1255919",
    "end": "1261039"
  },
  {
    "text": "divide it between two teams i can either divide it exclusively a and B get pieces of it and then there's no you know if if",
    "start": "1261039",
    "end": "1267280"
  },
  {
    "text": "A isn't working and B is in another time zone and they're asleep no one's using B's quota or I can overlap them and I",
    "start": "1267280",
    "end": "1273840"
  },
  {
    "text": "can have some reserve for A some reserve for B and some stuff in the middle that either one can use um but then there's",
    "start": "1273840",
    "end": "1279440"
  },
  {
    "text": "uh you know there's sort of no funible no fair borrowing no funible quotas and really no trade-offs that way and so",
    "start": "1279440",
    "end": "1284799"
  },
  {
    "text": "this quota management is one thing that the Q project really does a much better job of um so Q is a Kubernetes native",
    "start": "1284799",
    "end": "1291520"
  },
  {
    "text": "system to manage queuing quotas and jobs um and in our in ML batch Q is entirely",
    "start": "1291520",
    "end": "1297120"
  },
  {
    "text": "in charge of for users figuring out when their when their jobs they should wait uh when they when should they be",
    "start": "1297120",
    "end": "1302559"
  },
  {
    "text": "admitted so when do we actually go off and create the pods and resources to start these start running these things when they get quota assigned to them and",
    "start": "1302559",
    "end": "1308159"
  },
  {
    "text": "when things should be preempted so when something's already running but something more important comes in or something from another team comes in",
    "start": "1308159",
    "end": "1314080"
  },
  {
    "text": "that needs a quota back um so should that job be suspended or so on um so Q",
    "start": "1314080",
    "end": "1319360"
  },
  {
    "text": "has a number of features uh for us some of the key features are the it's has workload aware quotas so the quotas are",
    "start": "1319360",
    "end": "1325120"
  },
  {
    "text": "done on the level of the top level jobs not on the individual pods and Q knows how to admit that way um it it has",
    "start": "1325120",
    "end": "1332159"
  },
  {
    "text": "mechanisms for fair sharing for queuing with priorities for preempting jobs and",
    "start": "1332159",
    "end": "1337200"
  },
  {
    "text": "so on um and it's also externally extensible so it allows people to customize what's going on and has",
    "start": "1337200",
    "end": "1343200"
  },
  {
    "text": "built-in support for popular job sites and so what built-in support means is the way Q works is each of the",
    "start": "1343200",
    "end": "1348400"
  },
  {
    "text": "controllers for those resource types has had a small extension made to it it understands a suspension protocol so",
    "start": "1348400",
    "end": "1354159"
  },
  {
    "text": "when the resources are first created they're suspended and then when Q wants to admit them it flips a bit on the on",
    "start": "1354159",
    "end": "1360400"
  },
  {
    "text": "the on the spec saying \"Okay your suspend bit is now false you can go ahead and and run.\" And that all the",
    "start": "1360400",
    "end": "1365520"
  },
  {
    "text": "controllers are extended to do that um so there's the project there and uh we have actually Q has a booth over on the",
    "start": "1365520",
    "end": "1371760"
  },
  {
    "text": "project pavilion so uh come over there if you want to learn more about Q in a lot more detail than here i'm going to",
    "start": "1371760",
    "end": "1376880"
  },
  {
    "text": "talk a little bit more how about how Q works in particular to give you a sense of it so the key thing is Q is that",
    "start": "1376880",
    "end": "1382240"
  },
  {
    "text": "green box in the middle and what Q is going to do from a user perspective is interpose on the creation of resources",
    "start": "1382240",
    "end": "1387840"
  },
  {
    "text": "so the user wants to submit a job um they just you you do the usual cube cuddle apply or create the resource gets",
    "start": "1387840",
    "end": "1394159"
  },
  {
    "text": "created then Q has web hooks that interpose on the creation and look at it",
    "start": "1394159",
    "end": "1399200"
  },
  {
    "text": "decide whether based on the name space and other characteristics of it is that something that Q is supposed to be managing if so it suspends it it",
    "start": "1399200",
    "end": "1406400"
  },
  {
    "text": "modifies the spec before the thing gets created to set suspend to true and that causes the controller to then look at it",
    "start": "1406400",
    "end": "1412080"
  },
  {
    "text": "and say oh this is this is inactive i'm not supposed to do anything with this yet for the resource and so it sits there until Q decides that it's time for",
    "start": "1412080",
    "end": "1418960"
  },
  {
    "text": "that thing to run it has quota and that sort of admission protocol can involve looking at external resource checks",
    "start": "1418960",
    "end": "1424240"
  },
  {
    "text": "that's the the purple box over there on the far side it can also interact with the cluster autoscaler to ask it is",
    "start": "1424240",
    "end": "1429760"
  },
  {
    "text": "there capacity yet oh there isn't capacity can you create some capacity for me let me know when it's available and finally when all that happens then Q",
    "start": "1429760",
    "end": "1436480"
  },
  {
    "text": "can decide to admit it at which point the suspend bit has changed from true to false and the underlying controller",
    "start": "1436480",
    "end": "1442480"
  },
  {
    "text": "whether it's the job controller the PyTorch the training operator whoever is actually really in charge of that resource now sees oh the suspend bit is",
    "start": "1442480",
    "end": "1449039"
  },
  {
    "text": "false now I'm supposed to do my normal job and everything just sort of continues as normally the the the",
    "start": "1449039",
    "end": "1454159"
  },
  {
    "text": "controller for that will go off and create the resources it'll eventually create pods the pods will be submitted to the Kubernetes they'll be run they'll",
    "start": "1454159",
    "end": "1460240"
  },
  {
    "text": "be run and so on right so the key here is having that interposition mechanism in the middle where you can suspend",
    "start": "1460240",
    "end": "1466080"
  },
  {
    "text": "something and and cause it to wait for its turn um and one of the key abstractions in Q",
    "start": "1466080",
    "end": "1473200"
  },
  {
    "text": "that we're going to talking about is these is the notion of a cluster Q so Olivia talked about this for a second a little bit already so the cluster Q is",
    "start": "1473200",
    "end": "1479279"
  },
  {
    "text": "what is assigned quota so one of the things the cluster admin does for each one of our teams is he creates a cluster queue for them and the the YML on the",
    "start": "1479279",
    "end": "1485919"
  },
  {
    "text": "right gives you a sense for what that might look like in particular the resources the the quotas that's assigned to that team is shown in the cluster",
    "start": "1485919",
    "end": "1492159"
  },
  {
    "text": "queue um and there's also some additional detail about there about what are the preeemption policies and how do these cluster cues work together in",
    "start": "1492159",
    "end": "1499039"
  },
  {
    "text": "particular Q lets you put cluster cues together into a cohort and these cohorts the cluster cues within a cohort can",
    "start": "1499039",
    "end": "1505120"
  },
  {
    "text": "then coordinate to borrow quota from each other so if if there's quota available in cluster QA and cluster of",
    "start": "1505120",
    "end": "1511919"
  },
  {
    "text": "QB needs it and they're in the same cohort it can actually borrow that quota from it and then the quota can be taken",
    "start": "1511919",
    "end": "1516960"
  },
  {
    "text": "back when it's needed by by the other cluster Q so it's a way to sort of have a more flexible quota system where these",
    "start": "1516960",
    "end": "1522080"
  },
  {
    "text": "things are grouped together um and we don't use it in ML batch but Q more recent versions of Q actually support hierarchical quotas you can hierarch",
    "start": "1522080",
    "end": "1528720"
  },
  {
    "text": "sorry hierarchical cohorts so you can sort of put these things into a pyramid together of of sharing and preeemption",
    "start": "1528720",
    "end": "1535279"
  },
  {
    "text": "um and get that all to work together so your your cluster Q quota cluster Q structure could match your",
    "start": "1535279",
    "end": "1540400"
  },
  {
    "text": "organizational structure where you have teams that are associated with a particular division and a particular vice president and they all have their",
    "start": "1540400",
    "end": "1545760"
  },
  {
    "text": "quotas and they're signed around and so they can negotiate among themselves how they want their quota assigned and so on",
    "start": "1545760",
    "end": "1551440"
  },
  {
    "text": "so that's the notion of a cluster queue um a second thing one of the products we built within IBM research is this thing",
    "start": "1551440",
    "end": "1557039"
  },
  {
    "text": "called app wrapper and this came out of our experience of running these more complex workloads and app wrappers really do two things for us um so one of",
    "start": "1557039",
    "end": "1563840"
  },
  {
    "text": "the things they do is they allow us to group multiple resources that logically belong together into a single workload",
    "start": "1563840",
    "end": "1569120"
  },
  {
    "text": "that we can then hand to Q to manage um so Apprapper implements the Q suspension protocol it's sort of our our life cycle",
    "start": "1569120",
    "end": "1575360"
  },
  {
    "text": "diagram on the right there um so you know they start out suspended if Q is supposed to manage them when Q admits",
    "start": "1575360",
    "end": "1581039"
  },
  {
    "text": "them we realize that the controller then goes through an internal life cycle which we're going to talk about a little",
    "start": "1581039",
    "end": "1586080"
  },
  {
    "text": "bit later to actually create the resources um and if it gets preempted it'll be suspended and take those",
    "start": "1586080",
    "end": "1591279"
  },
  {
    "text": "resources back um so it lets us group resources together so it can we can put",
    "start": "1591279",
    "end": "1596400"
  },
  {
    "text": "any number several compute resources together anything that's defined using a podspec template um that includes all",
    "start": "1596400",
    "end": "1602080"
  },
  {
    "text": "the built-in kinds that come with Q that Q has built-in integrations for can be put inside an app wrapper but also",
    "start": "1602080",
    "end": "1607600"
  },
  {
    "text": "additional kinds that Q doesn't know about yet but have are defined in terms of podspec templates app wrapper gives",
    "start": "1607600",
    "end": "1612799"
  },
  {
    "text": "you an easy onboarding path to get those to run on Q and it also lets us put auxiliary resources in there so",
    "start": "1612799",
    "end": "1618159"
  },
  {
    "text": "sometimes people people's workloads require services and secrets and ingresses and other things associated with them we would like those to be all",
    "start": "1618159",
    "end": "1624320"
  },
  {
    "text": "sort of logically managed as a single unit deployed on the cluster when the cluster is admitted when the workload is",
    "start": "1624320",
    "end": "1630480"
  },
  {
    "text": "admitted by Q have all those things created and when the workload is done have them cleanly cleaned up so we don't",
    "start": "1630480",
    "end": "1635600"
  },
  {
    "text": "have lingering services and ingress is kicking around be some because someone forgot them um the second thing that apprappers do for us which we're going",
    "start": "1635600",
    "end": "1641679"
  },
  {
    "text": "to talk about in about 15 minutes is they harden workloads so app wrappers are a key part of our fault recovery",
    "start": "1641679",
    "end": "1646799"
  },
  {
    "text": "process and they allow us to monitor the workload health have a way to reconfigure our retry and failure policy",
    "start": "1646799",
    "end": "1652720"
  },
  {
    "text": "and make sure that all the resources are cleaned up and and and done like that so I'm going to have a demo of that later",
    "start": "1652720",
    "end": "1658480"
  },
  {
    "text": "what I'm going to do now is switch to this demo which is talking a little bit about demonstrating some of the quota",
    "start": "1658480",
    "end": "1664480"
  },
  {
    "text": "and preeemption properties of the system all right so we're going to start",
    "start": "1664480",
    "end": "1670400"
  },
  {
    "text": "off um for this particular demo we're just going to be running sort of synthetic workloads as they're all sort",
    "start": "1670400",
    "end": "1675679"
  },
  {
    "text": "of variations of this YAML that's shown over here on the left um and so these are just sleep jobs they they start up",
    "start": "1675679",
    "end": "1681200"
  },
  {
    "text": "they create two pods they request some GPUs you know the four kinds of jobs are going to be running the the",
    "start": "1681200",
    "end": "1687039"
  },
  {
    "text": "characteristics are shown in the table down here and we're going to use these to illustrate some of the characteristics you know some of the",
    "start": "1687039",
    "end": "1692159"
  },
  {
    "text": "properties of Q it's is queuing it's preeemption how quota gets shared across teams um up on the right this thing over",
    "start": "1692159",
    "end": "1699039"
  },
  {
    "text": "here is called Q viz this is a new visualization tool that's being developed as part of the Q project um and it gives us a way to sort of look at",
    "start": "1699039",
    "end": "1705600"
  },
  {
    "text": "the state of the system so in particular what it's showing here is the cluster cues for Q so these are the two we have",
    "start": "1705600",
    "end": "1711360"
  },
  {
    "text": "three cues the red team Q the blue team Q and the Slack cluster Q so now we're",
    "start": "1711360",
    "end": "1716480"
  },
  {
    "text": "going to go ahead and start so Alice comes into work and Alice is going to create a a burst of short running jobs",
    "start": "1716480",
    "end": "1722159"
  },
  {
    "text": "so each of these jobs is going to run for 30 seconds uh they're each going to want create two pods each of those wants",
    "start": "1722159",
    "end": "1727360"
  },
  {
    "text": "four GPUs so she's submitted uh 32 GPUs of work to a cluster that has 24 GPUs",
    "start": "1727360",
    "end": "1733120"
  },
  {
    "text": "and so some of them are going to have to queue up my math isn't very good 6* 8 Well she's done more than that but so",
    "start": "1733120",
    "end": "1739039"
  },
  {
    "text": "she's she's created seven jobs um so 7* 8 is 56 uh but only three of them are",
    "start": "1739039",
    "end": "1744480"
  },
  {
    "text": "running so she's got three jobs that have been admitted they're using the 24 GPUs on the cluster and the other ones",
    "start": "1744480",
    "end": "1749679"
  },
  {
    "text": "are waiting because there isn't enough resources for them so they're just queued up the pods aren't even created yet um and you can notice here that so",
    "start": "1749679",
    "end": "1756159"
  },
  {
    "text": "Alice actually has is using the whole cluster because the the cluster is healthy we don't need the slack capacity",
    "start": "1756159",
    "end": "1761760"
  },
  {
    "text": "and the red team hasn't shown up to work yet so Alice even though her nominal quota is eight she can run uh 24 GPUs",
    "start": "1761760",
    "end": "1768640"
  },
  {
    "text": "and we can look in Q viz you can drill down into the local cues so here's the view of uh the red team and the blue",
    "start": "1768640",
    "end": "1774480"
  },
  {
    "text": "team local Q this is their mechanism in their namespace for submitting stuff to that cluster queue um and we can see",
    "start": "1774480",
    "end": "1779600"
  },
  {
    "text": "that she has those three workloads and Cubiz lets you drill down into the workloads as well so you can sort of drill down and see what each one of",
    "start": "1779600",
    "end": "1784880"
  },
  {
    "text": "those workloads is doing what its state is and so on all right so now some of her shortrunning jobs have finished and",
    "start": "1784880",
    "end": "1790000"
  },
  {
    "text": "so some additional ones have be have been emitted now she only has one pending job um but unfortunately for",
    "start": "1790000",
    "end": "1795840"
  },
  {
    "text": "Alice uh you know Bob on the red team has just shown up to work so she has some Oh sorry no that's what we're doing",
    "start": "1795840",
    "end": "1801600"
  },
  {
    "text": "all right so now what we're going to do is we're going to have Alice so we can talk about queuing and prompt we're going to submit some longunning jobs so",
    "start": "1801600",
    "end": "1806880"
  },
  {
    "text": "now Alice has now submitted uh four jobs that each are going to run for 10 minutes all right and so as the",
    "start": "1806880",
    "end": "1812720"
  },
  {
    "text": "shortrunning jobs finish off the longrunning jobs are going to start running all right so here we are and if",
    "start": "1812720",
    "end": "1818640"
  },
  {
    "text": "we look at the pods we're going to look and see uh first of all we can see that I can't type oh there we go all right so",
    "start": "1818640",
    "end": "1825039"
  },
  {
    "text": "we can see that the the pods that Alice has in the name space some are running some are completed um so we can see a",
    "start": "1825039",
    "end": "1830320"
  },
  {
    "text": "mix of the shortrunning jobs which are mostly done and then normal jobs which are what's starting to to go and we can",
    "start": "1830320",
    "end": "1836399"
  },
  {
    "text": "see that those are there all right so now I think it's time for uh all right",
    "start": "1836399",
    "end": "1842320"
  },
  {
    "text": "so Alice has created a bunch of sort of normal priority jobs um but then and she's queued up and some of them are waiting but then she realized she had",
    "start": "1842320",
    "end": "1848399"
  },
  {
    "text": "something really important to do first so Alice can use priorities to have one of her workloads jump in front of the queue so she just created an important",
    "start": "1848399",
    "end": "1855279"
  },
  {
    "text": "job um and that was you know created after all the other ones but because its priority was higher Q has already gone",
    "start": "1855279",
    "end": "1861600"
  },
  {
    "text": "in and suspended one of her normal priority jobs to make room for her important job so Alice's Alice and her",
    "start": "1861600",
    "end": "1867200"
  },
  {
    "text": "team are able to manage to use priorities to organize their own work and to make sure the things they really want to do get done first and we can see",
    "start": "1867200",
    "end": "1873679"
  },
  {
    "text": "on the on the list there that there are two suspended um normal jobs one of which is the one that hadn't had a",
    "start": "1873679",
    "end": "1879440"
  },
  {
    "text": "chance to go yet and the other one was the one that was running and then was suspended to make room for the important",
    "start": "1879440",
    "end": "1885000"
  },
  {
    "text": "job okay right all right so now I think it's finally Bob time for Bob to come into work so Alice has been using all of",
    "start": "1885000",
    "end": "1891760"
  },
  {
    "text": "the GPUs in the in the in the cluster including eight that are reserved for the red team but now the red team is",
    "start": "1891760",
    "end": "1896880"
  },
  {
    "text": "here and Bob submitted 16 GPUs worth of work um and he'd like to go and and since the quota belongs to him one of",
    "start": "1896880",
    "end": "1903679"
  },
  {
    "text": "Alice's jobs that's running using the borrow quota has been suspended and Bob is now running so the you know with the",
    "start": "1903679",
    "end": "1910880"
  },
  {
    "text": "borrowing and and quota prevention system is flexible enough to allow the teams to get the quota that they're",
    "start": "1910880",
    "end": "1916320"
  },
  {
    "text": "assigned to but when they're not using it other teams can use it right so",
    "start": "1916320",
    "end": "1921640"
  },
  {
    "text": "that's the end of that demo and I think now we're going to switch",
    "start": "1921640",
    "end": "1928320"
  },
  {
    "text": "to All right so now I'm going to hand over to Claudia to talk about fault detection and observability",
    "start": "1932919",
    "end": "1939440"
  },
  {
    "text": "and hopefully I'll know how to use this all right so we're going to switch a little bit",
    "start": "1939440",
    "end": "1946559"
  },
  {
    "text": "context now still in the same area of how do we run uh workloads efficiently",
    "start": "1946559",
    "end": "1952000"
  },
  {
    "text": "and effectively on the cluster um but we want now to focus a little bit more on",
    "start": "1952000",
    "end": "1958640"
  },
  {
    "text": "the reasons why we really care about fall tolerance and advanced fall",
    "start": "1958640",
    "end": "1963679"
  },
  {
    "text": "tolerance in this case so ideally in the perfect world we don't need any human",
    "start": "1963679",
    "end": "1969760"
  },
  {
    "text": "intervention whatsoever in a cluster so people just go Ellis and Bob submit all their job the cluster is always fine no",
    "start": "1969760",
    "end": "1976559"
  },
  {
    "text": "GPU breaks everything is fine that's of course not how uh things work um and",
    "start": "1976559",
    "end": "1983760"
  },
  {
    "text": "therefore to have um to have the cluster to being used as much as possible and",
    "start": "1983760",
    "end": "1989720"
  },
  {
    "text": "have and reduce the human intervention as much as possible we need to focus on",
    "start": "1989720",
    "end": "1994960"
  },
  {
    "text": "two things one is the infrastructure itself the other part is making sure that the workloads are running for the",
    "start": "1994960",
    "end": "2001840"
  },
  {
    "text": "workloads part we're seeing all the all these um uh the queue management um fair",
    "start": "2001840",
    "end": "2008559"
  },
  {
    "text": "sharing so all the good stuff that Dave and Olivia um showed so far on the other",
    "start": "2008559",
    "end": "2014799"
  },
  {
    "text": "hand we have the the hardware so we have to we need to make sure that the GPUs",
    "start": "2014799",
    "end": "2021200"
  },
  {
    "text": "are running correctly and if they're not we want to know that so even to make",
    "start": "2021200",
    "end": "2027039"
  },
  {
    "text": "sure that all the jobs can run correctly and also the reason why we need to one",
    "start": "2027039",
    "end": "2032080"
  },
  {
    "text": "of the reason why we would need to adjust quotas is because maybe at some point a couple GPUs don't work anymore",
    "start": "2032080",
    "end": "2037840"
  },
  {
    "text": "uh so we need also to adjust the quotas accordingly depending on uh on the cluster state and this is not something",
    "start": "2037840",
    "end": "2046240"
  },
  {
    "text": "that's uh new and unexpected um I took some screenshots here from um some of",
    "start": "2046240",
    "end": "2053839"
  },
  {
    "text": "the most recent um reports on GPU failures and how they relate to uh AI",
    "start": "2053839",
    "end": "2061118"
  },
  {
    "text": "jobs running on more or less large clusters um on the top left side I think",
    "start": "2061119",
    "end": "2068158"
  },
  {
    "text": "that that's probably the most famous uh one when AI meta uh released the white",
    "start": "2068159",
    "end": "2074720"
  },
  {
    "text": "paper about the uh training of the llama 300 herd of models they showed that very",
    "start": "2074720",
    "end": "2080720"
  },
  {
    "text": "nice tables uh showing where how many uh",
    "start": "2080720",
    "end": "2085919"
  },
  {
    "text": "errors have been happening during the the entire uh training period and those",
    "start": "2085919",
    "end": "2091599"
  },
  {
    "text": "are they can be software or hardware uh issues in particular the hardware ones",
    "start": "2091599",
    "end": "2096638"
  },
  {
    "text": "they account for 78 I think yes percent of the interruption of the workloads all",
    "start": "2096639",
    "end": "2103680"
  },
  {
    "text": "those are only hardware and of this 78% almost 60% is GPU failures so you really",
    "start": "2103680",
    "end": "2111599"
  },
  {
    "text": "really need to make sure that you can notice when things happen and hopefully also how to recover the jobs on the",
    "start": "2111599",
    "end": "2118800"
  },
  {
    "text": "right hand uh this is maybe like three weeks old two weeks old uh technical",
    "start": "2118800",
    "end": "2124320"
  },
  {
    "text": "blog post from Nvidia uh showing the error rates both both uh hardware and",
    "start": "2124320",
    "end": "2129920"
  },
  {
    "text": "software over a four months four or five months period where they've been running",
    "start": "2129920",
    "end": "2135359"
  },
  {
    "text": "uh training jobs on their clusters and again a lot of hardware errors um on the",
    "start": "2135359",
    "end": "2141440"
  },
  {
    "text": "bottom right and center um I reported also a screenshot from a paper that was",
    "start": "2141440",
    "end": "2149760"
  },
  {
    "text": "just published one or two weeks ago um by IBM",
    "start": "2149760",
    "end": "2155359"
  },
  {
    "text": "research and uh University of Illinois's Urban Champagne uh where that table",
    "start": "2155359",
    "end": "2161200"
  },
  {
    "text": "there is showing how many jobs have been failing because of GPU errors during a",
    "start": "2161200",
    "end": "2167119"
  },
  {
    "text": "time span of two and a half years and those jobs were more than 4,000 so long",
    "start": "2167119",
    "end": "2173839"
  },
  {
    "text": "story short need to take a look at those GPUs and only G GPUs also uh network",
    "start": "2173839",
    "end": "2181280"
  },
  {
    "text": "that's a another big point of failure storage can be also point of failure because you deploy an operator that",
    "start": "2181280",
    "end": "2187520"
  },
  {
    "text": "that's taking care of the uh of the storage of the storage nodes so that's",
    "start": "2187520",
    "end": "2192640"
  },
  {
    "text": "also subject to failure um so we've uh in our system we rely on",
    "start": "2192640",
    "end": "2199280"
  },
  {
    "text": "two components native tooling uh everyone knows Prometheus Graphana uh for Prometheus is going to provide um",
    "start": "2199280",
    "end": "2207119"
  },
  {
    "text": "node statistics about hardware CPU memory network uh but also um",
    "start": "2207119",
    "end": "2213839"
  },
  {
    "text": "information about the pods or the workloads that are running how they're doing and all of that graphana a great",
    "start": "2213839",
    "end": "2219280"
  },
  {
    "text": "tool for visualization and on the GPU side um",
    "start": "2219280",
    "end": "2224480"
  },
  {
    "text": "with the GPU operator they also provide the DCGM exporter which is creating a",
    "start": "2224480",
    "end": "2231040"
  },
  {
    "text": "lot of metrics that report the status of the GPUs so all of that is a great great",
    "start": "2231040",
    "end": "2237760"
  },
  {
    "text": "starting point unfortunately it's not enough and we've been working uh to",
    "start": "2237760",
    "end": "2243680"
  },
  {
    "text": "provide also an extended set of what we call health checks on the GPUs uh",
    "start": "2243680",
    "end": "2250000"
  },
  {
    "text": "network and storage that's been packaged into this tool called autopilot and that's integrated so the results of",
    "start": "2250000",
    "end": "2257760"
  },
  {
    "text": "autopilot health checks are um are used by the app wrapper to um figure out",
    "start": "2257760",
    "end": "2265040"
  },
  {
    "text": "which nodes are good or not let's get into some details now um so why we have",
    "start": "2265040",
    "end": "2273359"
  },
  {
    "text": "this autopilot thing why we needed to do this um so during the operations and bring up and running all the jobs on the",
    "start": "2273359",
    "end": "2280400"
  },
  {
    "text": "Vela cluster that uh Olivia showed at the beginning uh we've been seeing uh",
    "start": "2280400",
    "end": "2286320"
  },
  {
    "text": "errors happening or like a training job that would crash entirely or that the",
    "start": "2286320",
    "end": "2292640"
  },
  {
    "text": "the performance would degrade like the token per second and all the metrics showing how good the training is going",
    "start": "2292640",
    "end": "2299680"
  },
  {
    "text": "that would degrade but if you look at the um at the metrics that are exported",
    "start": "2299680",
    "end": "2305119"
  },
  {
    "text": "regularly by the uh native um operators you would see that everything",
    "start": "2305119",
    "end": "2311839"
  },
  {
    "text": "is fine until there is a real crash a hardware crash you don't really see some",
    "start": "2311839",
    "end": "2316960"
  },
  {
    "text": "of the aspect that are degrading the performance of training job and not not only training jobs also of course",
    "start": "2316960",
    "end": "2322320"
  },
  {
    "text": "fine-tuning and inference are also impacted um so we've been seeing where",
    "start": "2322320",
    "end": "2327680"
  },
  {
    "text": "those errors are what what to look into the GPUs to see why a job is",
    "start": "2327680",
    "end": "2334480"
  },
  {
    "text": "underperforming and there is nothing new that we are inventing really it's just a bunch of Nvidia Smi commands or DCGMI",
    "start": "2334480",
    "end": "2341720"
  },
  {
    "text": "commands um and but those things that we are looking at are not exposed um by the",
    "start": "2341720",
    "end": "2350480"
  },
  {
    "text": "DCGM exporter for instance so we know what we're looking what we're looking for um so the ideal the thing we want to",
    "start": "2350480",
    "end": "2359119"
  },
  {
    "text": "do is package all those things and into periodic health checks on GPU network",
    "start": "2359119",
    "end": "2365280"
  },
  {
    "text": "and storage we run those tests all the time and we can leverage Prometheus to",
    "start": "2365280",
    "end": "2372160"
  },
  {
    "text": "export export the results that we're getting and also we label the worker",
    "start": "2372160",
    "end": "2377280"
  },
  {
    "text": "nodes healthy unhealthy how unhealthy are they and those labels are used to uh",
    "start": "2377280",
    "end": "2384240"
  },
  {
    "text": "steer the life cycle of the jobs um so this is pretty much the tool uh",
    "start": "2384240",
    "end": "2392320"
  },
  {
    "text": "like a summary slide so we uh we use this to run automatically uh health",
    "start": "2392320",
    "end": "2398960"
  },
  {
    "text": "checks on the cluster we export data Prometheus and and um uh and lab and",
    "start": "2398960",
    "end": "2405440"
  },
  {
    "text": "labeling nodes and we use this information to have the jobs running",
    "start": "2405440",
    "end": "2412320"
  },
  {
    "text": "effectively um this is an list of the the health",
    "start": "2412320",
    "end": "2417359"
  },
  {
    "text": "checks that we're running gpu um mainly related to GPU also network some",
    "start": "2417359",
    "end": "2423520"
  },
  {
    "text": "benchmarking too i'm not going to get into the details of that um so okay so now we're going to uh",
    "start": "2423520",
    "end": "2433119"
  },
  {
    "text": "to see a short demo we're going to install uh autopilot and the entire u",
    "start": "2433119",
    "end": "2438960"
  },
  {
    "text": "observability stack uh by coupramus stack and the graphana dashboards and we",
    "start": "2438960",
    "end": "2444000"
  },
  {
    "text": "see how those work all right so let's see if I managed to do",
    "start": "2444000",
    "end": "2450560"
  },
  {
    "text": "this okay okay great all right so",
    "start": "2453640",
    "end": "2461200"
  },
  {
    "text": "um here we install on autopilot this is a a Helm chart so it's pretty straightforward to install and I'm",
    "start": "2461200",
    "end": "2469119"
  },
  {
    "text": "looking from here because I don't see down there um okay so as a Helm chart we",
    "start": "2469119",
    "end": "2474319"
  },
  {
    "text": "install it and it will run on the GPU nodes it can also run on nonGPU nodes but in this case we that's all we have",
    "start": "2474319",
    "end": "2481040"
  },
  {
    "text": "so we're running on uh with the GPU setup so since we are also creating an",
    "start": "2481040",
    "end": "2487920"
  },
  {
    "text": "NFS client at this one we can also extend the autopilot configuration and check periodically that also the PVC",
    "start": "2487920",
    "end": "2495520"
  },
  {
    "text": "creation and deletion is still working so taking also look at the NFS client",
    "start": "2495520",
    "end": "2500880"
  },
  {
    "text": "and we can just edit the uh configuration file uh actually create",
    "start": "2500880",
    "end": "2506880"
  },
  {
    "text": "one uh that we will going to pass to the uh to the Helm chart this is a class",
    "start": "2506880",
    "end": "2512800"
  },
  {
    "text": "that is not necessary but since we have a storage class there that we manage we",
    "start": "2512800",
    "end": "2518240"
  },
  {
    "text": "want always to take a look at that so we just upgrade the Helm chart take a",
    "start": "2518240",
    "end": "2524880"
  },
  {
    "text": "look that everything is fine so this is going to run a rolling upgrade so it's going to take a little bit of",
    "start": "2524880",
    "end": "2532318"
  },
  {
    "text": "time so I think I have Yeah I have sped up a little bit",
    "start": "2535160",
    "end": "2542480"
  },
  {
    "text": "okay so from the logs you can take you can see briefly what is going on it has run all the health checks everything is",
    "start": "2542480",
    "end": "2549680"
  },
  {
    "text": "fine so it's labeling the node uh with a pass saying that the GPUs are okay the",
    "start": "2549680",
    "end": "2556480"
  },
  {
    "text": "node overall is okay um now we can move on to the monitoring setup so we're",
    "start": "2556480",
    "end": "2561760"
  },
  {
    "text": "starting with Prometheus uh Prometheus and Grafana actually come together in the Prometheus community the C",
    "start": "2561760",
    "end": "2567680"
  },
  {
    "text": "Prometheus stack helm chart uh that's all on the G repository that everyone",
    "start": "2567680",
    "end": "2573119"
  },
  {
    "text": "you can all find super easily links are there um so we just install the the Helm",
    "start": "2573119",
    "end": "2579359"
  },
  {
    "text": "reposi the the the helm chart we run an update and so this chart will provide",
    "start": "2579359",
    "end": "2584720"
  },
  {
    "text": "Prometheus Graphana alert manager the node exporter and coup state matrix",
    "start": "2584720",
    "end": "2590079"
  },
  {
    "text": "that's all that we need to monitor the cluster so it's all there is super easy",
    "start": "2590079",
    "end": "2595200"
  },
  {
    "text": "to install so in our case since we're running on on open shift we we need to",
    "start": "2595200",
    "end": "2600960"
  },
  {
    "text": "um change a few things uh so we need to uh override the the node the node",
    "start": "2600960",
    "end": "2607119"
  },
  {
    "text": "exporter port because that's already news uh and also we need to not deploy",
    "start": "2607119",
    "end": "2612319"
  },
  {
    "text": "the CRDs but that's something that you may not uh need to do so if you're running on an open shift cluster just",
    "start": "2612319",
    "end": "2618400"
  },
  {
    "text": "remember that that's something that you're going to to need so we can just uh create a configuration file for hem",
    "start": "2618400",
    "end": "2624079"
  },
  {
    "text": "chart that's usual way of doing things uh as you all know already and we just",
    "start": "2624079",
    "end": "2629520"
  },
  {
    "text": "deploy um the Helm chart and it will take a little bit but we're we're",
    "start": "2629520",
    "end": "2637040"
  },
  {
    "text": "speeding up the process so on depending on where you're deploying if open shift",
    "start": "2637040",
    "end": "2642240"
  },
  {
    "text": "or kubernetes you will need to um especially in this case with open shift",
    "start": "2642240",
    "end": "2647280"
  },
  {
    "text": "you will need to add uh the service accounts that are created by the helm chart to uh the privileged uh service um",
    "start": "2647280",
    "end": "2657200"
  },
  {
    "text": "a service account and so um so not service account so the",
    "start": "2657200",
    "end": "2663280"
  },
  {
    "text": "security context and that will be needed so that the uh all the pods able to",
    "start": "2663280",
    "end": "2669119"
  },
  {
    "text": "scrape all the the information from the from the nodes so we have we have all of that running and the chart will also",
    "start": "2669119",
    "end": "2677359"
  },
  {
    "text": "tell you how to get the um to expose the service the graphana service to access",
    "start": "2677359",
    "end": "2683599"
  },
  {
    "text": "the uh to access the the the web console and we also give you the the default um",
    "start": "2683599",
    "end": "2691440"
  },
  {
    "text": "the way to retrieve that default uh password which can be changed in the Helm chart but we're not doing that in",
    "start": "2691440",
    "end": "2697440"
  },
  {
    "text": "this case um so in this case we're just exposing uh the with port forward",
    "start": "2697440",
    "end": "2704880"
  },
  {
    "text": "command we're exposing the graphana dashboard to local host uh so we're going",
    "start": "2704880",
    "end": "2710280"
  },
  {
    "text": "to access it and default uses user is",
    "start": "2710280",
    "end": "2716160"
  },
  {
    "text": "admin and the password is prom",
    "start": "2716160",
    "end": "2720160"
  },
  {
    "text": "operator all right and at this point so this is going to be empty so you're not",
    "start": "2721640",
    "end": "2726960"
  },
  {
    "text": "really seeing anything here other than that default uh dashboards that are uh",
    "start": "2726960",
    "end": "2732560"
  },
  {
    "text": "in graphana just to showcase the um the the capabilities",
    "start": "2732560",
    "end": "2738960"
  },
  {
    "text": "uh but we can ex uh we can import graphana dashboards and with that um the",
    "start": "2738960",
    "end": "2744640"
  },
  {
    "text": "one that we have created for autopilot that is going to show all the health checks uh that is running what is wrong",
    "start": "2744640",
    "end": "2750880"
  },
  {
    "text": "with the cluster if anything is wrong and we're also going to deploy the DCGM",
    "start": "2750880",
    "end": "2757440"
  },
  {
    "text": "um dashboard from Nvidia and that's a pretty extensive dashboard with that",
    "start": "2757440",
    "end": "2763359"
  },
  {
    "text": "contains a lot of information about the the GPUs so we this uh that's super easy",
    "start": "2763359",
    "end": "2769520"
  },
  {
    "text": "to do both dashboards are of course on the Graphana um uh website and you can",
    "start": "2769520",
    "end": "2778160"
  },
  {
    "text": "just copy the the ID that's all in the in the demo of course um on our gate repository and you can just copy paste",
    "start": "2778160",
    "end": "2784640"
  },
  {
    "text": "the ID from the website and yes that's the Nvidia",
    "start": "2784640",
    "end": "2792280"
  },
  {
    "text": "one and of course you need to select the uh the source of the information might",
    "start": "2792280",
    "end": "2797680"
  },
  {
    "text": "be multiple but of course the default is is Promeia Um so depending on which cluster you are",
    "start": "2797680",
    "end": "2804640"
  },
  {
    "text": "in if it's Kubernetes or open shift you would need to let Prometheus know from",
    "start": "2804640",
    "end": "2810240"
  },
  {
    "text": "where to scrape the data in the Kubernetes case you would need to label",
    "start": "2810240",
    "end": "2816319"
  },
  {
    "text": "the service monitor which is an object that you attach to a secondary source of",
    "start": "2816319",
    "end": "2821920"
  },
  {
    "text": "uh of information of metrics in our case going to be autopilot and the DCGM",
    "start": "2821920",
    "end": "2827400"
  },
  {
    "text": "exporter um in open shift you would need to label the name space where those",
    "start": "2827400",
    "end": "2833760"
  },
  {
    "text": "metrics are being produced with the open shift monitoring label uh so it will",
    "start": "2833760",
    "end": "2839760"
  },
  {
    "text": "take for the the GPU will take a little bit to start scraping the matrix and and",
    "start": "2839760",
    "end": "2847119"
  },
  {
    "text": "then yeah it showed up that everything was configured fine",
    "start": "2847119",
    "end": "2852960"
  },
  {
    "text": "and I think okay I didn't destroy it",
    "start": "2852960",
    "end": "2862359"
  },
  {
    "text": "okay who's next i'm next okay okay great so now we've seen how um we we how a",
    "start": "2863079",
    "end": "2869520"
  },
  {
    "text": "pilot can detect the faults for us and how it uh labels and nodes for us we're going to look at how the next level levels up of the software stack use that",
    "start": "2869520",
    "end": "2876079"
  },
  {
    "text": "information to automate fault recovery um so the we're going to get back to app",
    "start": "2876079",
    "end": "2881200"
  },
  {
    "text": "wrappers first so I said you know app rappers are our mechanism for hardening workloads and MLB batch we really push users to wrap pretty much all their",
    "start": "2881200",
    "end": "2887680"
  },
  {
    "text": "workloads inside of app wrappers um and you know there are a lot of reasons why a workload might be unhealthy um pods",
    "start": "2887680",
    "end": "2893920"
  },
  {
    "text": "can fail and they stay failed for a while longer than a grace period so the underlying controller didn't didn't",
    "start": "2893920",
    "end": "2899119"
  },
  {
    "text": "restart them didn't recover them um not enough running or completed pods so for some reason pods didn't start they",
    "start": "2899119",
    "end": "2905119"
  },
  {
    "text": "aren't running they didn't finish again with grace periods um autopilot could have labeled a node as evict meaning",
    "start": "2905119",
    "end": "2911359"
  },
  {
    "text": "there's a severe fault in that node and then the app wrapper controller is looking and it sees there's a pod running on a node that's been labeled",
    "start": "2911359",
    "end": "2917520"
  },
  {
    "text": "evict and that pod is using a resource in particular GPU that's unhealthy um",
    "start": "2917520",
    "end": "2922800"
  },
  {
    "text": "that's an unhealthy workload we need to do something about it um similarly uh the top level kinds like a PyTorch job",
    "start": "2922800",
    "end": "2929520"
  },
  {
    "text": "RAID job things like that the job batch job they have statuses and so if one of those goes to a failed status then that",
    "start": "2929520",
    "end": "2935200"
  },
  {
    "text": "means the workload has failed and we can recover that and finally the user may go in and delete their top level resource",
    "start": "2935200",
    "end": "2940480"
  },
  {
    "text": "they may kind of forget there's an app wrapper there and just delete the PyTorch job or delete some secret that was created as part of the job and we",
    "start": "2940480",
    "end": "2946480"
  },
  {
    "text": "interpret that as the user saying \"There's something wrong with this workload please get rid of it for me or I forgot about it but it's it you know I",
    "start": "2946480",
    "end": "2952559"
  },
  {
    "text": "need to get rid of it um and so the the diving a little bit more into the picture there um the apps life the app",
    "start": "2952559",
    "end": "2959599"
  },
  {
    "text": "app life cycle sort of has three stages when something's admitted so the jobs admitted and it gets to a resuming state",
    "start": "2959599",
    "end": "2965440"
  },
  {
    "text": "this is when we're creating the resources so we're going off we're creating all the resources we're waiting for them to get to a to a start running",
    "start": "2965440",
    "end": "2971200"
  },
  {
    "text": "state running which is the sort of normal state um and then if we had to do a retry we kick into this retry state",
    "start": "2971200",
    "end": "2977440"
  },
  {
    "text": "and in that retry state what happens is uh sorry resetting state and that's where the app wrapper controller will go",
    "start": "2977440",
    "end": "2983359"
  },
  {
    "text": "in and sort of first gently ask the the top level resources to please delete",
    "start": "2983359",
    "end": "2988400"
  },
  {
    "text": "themselves and then sort of monitor make sure that actually happens and after a while go in and forcibly delete them",
    "start": "2988400",
    "end": "2993440"
  },
  {
    "text": "without a without a without giving them a choice to make sure that everything is fully cleaned up before it goes back",
    "start": "2993440",
    "end": "2998960"
  },
  {
    "text": "into the resuming phase and tries again um and so this uniform retry loop has a bunch of parameters around grace periods",
    "start": "2998960",
    "end": "3004960"
  },
  {
    "text": "and pauses and how many times you got to go around it and we the system is set up so that the cluster admin can sort of",
    "start": "3004960",
    "end": "3010960"
  },
  {
    "text": "set reasonable defaults for these um and then users can override them with annotations within bound so the cluster",
    "start": "3010960",
    "end": "3016880"
  },
  {
    "text": "admin can also say you know you can retry you can do certain things but you can't make this pause longer than this",
    "start": "3016880",
    "end": "3022559"
  },
  {
    "text": "long because that's against my policy i want to make sure I get good utilization out of the cluster um so that's the sort",
    "start": "3022559",
    "end": "3028559"
  },
  {
    "text": "of app wrapper piece of it um another thing we do is we adapt to available resources so this picture on the bottom",
    "start": "3028559",
    "end": "3034559"
  },
  {
    "text": "here is showing a a cluster where there actually three teams plus a Slack cluster queue uh the green nodes",
    "start": "3034559",
    "end": "3039920"
  },
  {
    "text": "represent nodes so sorry the the the squares on top represent nodes so the green ones are healthy ones the red ones",
    "start": "3039920",
    "end": "3046160"
  },
  {
    "text": "are unhealthy ones um and so we have a mechanism we we reserve this slack cluster queue and so what we do is we",
    "start": "3046160",
    "end": "3053040"
  },
  {
    "text": "have a reconciler that's watching the status of nodes it notices when autopilot labels a node as unhealthy and",
    "start": "3053040",
    "end": "3058800"
  },
  {
    "text": "when that happens it adjusts uh the what Q calls the uh lending limit on the",
    "start": "3058800",
    "end": "3064000"
  },
  {
    "text": "Slack cluster Q so that the available quota to be borrowed by other by other teams in the co cohort actually matches",
    "start": "3064000",
    "end": "3070720"
  },
  {
    "text": "the the capacity in the cluster so at all times we're sort of dynamically adjusting this slack clust capacity so",
    "start": "3070720",
    "end": "3077040"
  },
  {
    "text": "that what we have available to hand out to the jobs actually matches what's healthy um we and then and then uh that",
    "start": "3077040",
    "end": "3085280"
  },
  {
    "text": "this uh gives us the ability both to uh have the thing so to prevent so sorry",
    "start": "3085280",
    "end": "3090800"
  },
  {
    "text": "this gives the ability to prevent admitting too many new jobs in the future um and we combine this with node anti-affffinity so as jobs get admitted",
    "start": "3090800",
    "end": "3097359"
  },
  {
    "text": "to the cluster they're get steered away from nodes that have been labeled as unhealthy by autopilot um it also gives",
    "start": "3097359",
    "end": "3102480"
  },
  {
    "text": "us the ability to reserve the quota to migrate jobs away from unhealthy nodes so if we're reducing the the the the quota we're handing out that gives us",
    "start": "3102480",
    "end": "3108640"
  },
  {
    "text": "the room to then reset the app wrappers and re reset those things and have them go back to healthy nodes when they come",
    "start": "3108640",
    "end": "3113680"
  },
  {
    "text": "in and it also allows us to sort of view maintenance as just another unhealthy node so if admins need to come in and",
    "start": "3113680",
    "end": "3119760"
  },
  {
    "text": "start doing things they can coordinate the nodes and the system reacts to that just the same way it does as if the uh",
    "start": "3119760",
    "end": "3125599"
  },
  {
    "text": "the app as if autopilot had labeled something as unhealthy it goes ahead and adjusts the quota and sort of starts",
    "start": "3125599",
    "end": "3130960"
  },
  {
    "text": "steering new workloads away from those nodes that have been that have been marked as unhealthy so sort of summing this all up",
    "start": "3130960",
    "end": "3137520"
  },
  {
    "text": "we have autopilot at the bottom sort of periodically checking things labeling the health of nodes GPUs based on",
    "start": "3137520",
    "end": "3142640"
  },
  {
    "text": "networks uh GPUs and and storage labeling nodes to let the rest of rest of the system react to that we have the",
    "start": "3142640",
    "end": "3148960"
  },
  {
    "text": "app wrapper which is sort of watching the workloads uh it's injecting the affinities automatically so the users don't have to get that hunk of affinity",
    "start": "3148960",
    "end": "3155839"
  },
  {
    "text": "ammo correct um it gets it put into all their their workloads for them to steer their pods away from unhealthy nodes it",
    "start": "3155839",
    "end": "3162400"
  },
  {
    "text": "detects when pods are running on evict nodes and triggers triggers a reset and it automates this whole control and reset control and reset and resume cycle",
    "start": "3162400",
    "end": "3169760"
  },
  {
    "text": "on failures and it also does this capacity management to inform Q of the unhealthy nodes so now we're going to",
    "start": "3169760",
    "end": "3176000"
  },
  {
    "text": "show show a demo of this and oops show a demo of this happening over",
    "start": "3176000",
    "end": "3181720"
  },
  {
    "text": "here all right and so in this demo what we're going to do is we're going to have Alice submit uh one single large job our",
    "start": "3181720",
    "end": "3189200"
  },
  {
    "text": "cluster isn't very big so uh it's a three it's a three node cluster with 24 GPUs so Alice is going to submit a job",
    "start": "3189200",
    "end": "3194880"
  },
  {
    "text": "that takes 16 GPUs um all right so here she goes she's going to submit this and",
    "start": "3194880",
    "end": "3200319"
  },
  {
    "text": "we can see that the job is is running and then what we're going to do is we're going to go ahead and pick one of these nodes as a victim and we're going to",
    "start": "3200319",
    "end": "3206559"
  },
  {
    "text": "label it as if autopilot had detected a severe GPU fault on it and we're going to see the system then react to that so",
    "start": "3206559",
    "end": "3212559"
  },
  {
    "text": "we're going to pick this uh victim node go ahead and stick in the autopilot uh evict label on it and then we'll wait",
    "start": "3212559",
    "end": "3218800"
  },
  {
    "text": "for the system to react this will take about 30 to 45 seconds and so we're going to do is we're going to go ahead and watch the app wrappers so there's",
    "start": "3218800",
    "end": "3224960"
  },
  {
    "text": "the single app wrapper for her job it's running and what's happening now in the background is the system has a node",
    "start": "3224960",
    "end": "3230160"
  },
  {
    "text": "monitor it's noticed that this node has been labeled it's then looking to see are there pods that are running on a",
    "start": "3230160",
    "end": "3236800"
  },
  {
    "text": "node this node in particular that's bad um that have that are actually using resources so is there is there no is",
    "start": "3236800",
    "end": "3242400"
  },
  {
    "text": "there a pod running on uh this node that's using a GPU in which case we ought to reset it um and so once it does",
    "start": "3242400",
    "end": "3249680"
  },
  {
    "text": "that it'll then go off and from the pods go back to the app wrappers figure out which app wrappers need to be reset to",
    "start": "3249680",
    "end": "3255440"
  },
  {
    "text": "to to do this and we'll put them into a resetting state um and then and then the",
    "start": "3255440",
    "end": "3261680"
  },
  {
    "text": "the the over there he goes all right so now it's reset it and the resources are being deleted the pods are being deleted",
    "start": "3261680",
    "end": "3268319"
  },
  {
    "text": "being cleaned up and they're gone and so now we can resume and the workload is now running again and if we look to see",
    "start": "3268319",
    "end": "3274640"
  },
  {
    "text": "where it is the antifinity shown there on the on the right or left whichever side they are have steered the new pods",
    "start": "3274640",
    "end": "3280400"
  },
  {
    "text": "away from getting onto that bad node and so now they're running on the on the two remaining healthy nodes of the cluster",
    "start": "3280400",
    "end": "3285520"
  },
  {
    "text": "and that's all happened without any human help all right this all just happened uh with the system taking care",
    "start": "3285520",
    "end": "3291040"
  },
  {
    "text": "of everything okay so that's the demo of that",
    "start": "3291040",
    "end": "3296440"
  },
  {
    "text": "and get the mouse over here okay so what we want to do now for the",
    "start": "3296440",
    "end": "3302319"
  },
  {
    "text": "last section of the tutorial is we're going to take you through three sort of representative workloads that we run in this cluster um and all of the YAML and",
    "start": "3302319",
    "end": "3309119"
  },
  {
    "text": "definitions these are available as part of the tutorial but we're just going to talk about each of them quickly and show them running um so the first one of",
    "start": "3309119",
    "end": "3315920"
  },
  {
    "text": "these is the coupeflow trainer so the coupeflow trainer is an operator for uh it's Kubernetes native framework for",
    "start": "3315920",
    "end": "3321200"
  },
  {
    "text": "fine-tuning scalable distributed learning of training of LLM it supports a bunch of machine learning frameworks",
    "start": "3321200",
    "end": "3327119"
  },
  {
    "text": "and for our demo what we're going to do is we're going to take a bite a pietorch job and do uh FST uh FSTP so uh fully",
    "start": "3327119",
    "end": "3335599"
  },
  {
    "text": "sharded distributed parallel uh training of a small job we're going to put that inside of an app wrapper for enhanced",
    "start": "3335599",
    "end": "3341520"
  },
  {
    "text": "fall tolerance we're going to go ahead and do that um and so we're going to look at the",
    "start": "3341520",
    "end": "3348318"
  },
  {
    "text": "demo okay so again so this stuff is out here so we're going to do is um first we're",
    "start": "3350839",
    "end": "3358079"
  },
  {
    "text": "going to go ahead and do a the cube cuddle apply for this job all right so we're going to grab the",
    "start": "3358079",
    "end": "3364760"
  },
  {
    "text": "command go ahead and apply this yl and so this is this is create an app wrapper that wraps around a pietorch job that's",
    "start": "3364760",
    "end": "3371200"
  },
  {
    "text": "going to go into this training job for us and we can see that it's starting to starting to go it's created it's running",
    "start": "3371200",
    "end": "3378240"
  },
  {
    "text": "um so we used the here's the the YAML it's way too big for you to read um but",
    "start": "3378240",
    "end": "3383680"
  },
  {
    "text": "we created this ammo actually using a coupeflow trainer notebook so we used a Python notebook to take some fairly",
    "start": "3383680",
    "end": "3389680"
  },
  {
    "text": "standard training uh code for Python code for doing this and then we use the",
    "start": "3389680",
    "end": "3394720"
  },
  {
    "text": "uh training the training client to actually go off and create that YAML for us which",
    "start": "3394720",
    "end": "3400480"
  },
  {
    "text": "we then grabbed and use a tool from MLB batch to take that YAML and inject it inside of an app wrapper for you and so",
    "start": "3400480",
    "end": "3406319"
  },
  {
    "text": "all this stuff is available and the job is actually finished now um and if we go and look at its uh logs we can see we",
    "start": "3406319",
    "end": "3412640"
  },
  {
    "text": "can sort of see what it did so it it ran this training job um there's some snippets down there in the bottom you",
    "start": "3412640",
    "end": "3418240"
  },
  {
    "text": "can see that it ran uh you know it's a it ran with two GPUs inside of each each of the two pods um and then it here's",
    "start": "3418240",
    "end": "3425119"
  },
  {
    "text": "the output from the the actual training and when we're all done we can go ahead and delete things and clean up after",
    "start": "3425119",
    "end": "3432280"
  },
  {
    "text": "ourselves okay so that",
    "start": "3432280",
    "end": "3436680"
  },
  {
    "text": "Okay and I think I'm handing it off to Claudia next I think",
    "start": "3440319",
    "end": "3446078"
  },
  {
    "text": "right where are we",
    "start": "3450839",
    "end": "3455240"
  },
  {
    "text": "okay um so now we're going to see an example of fine-tuning and for that",
    "start": "3458160",
    "end": "3464079"
  },
  {
    "text": "we're going to use uh cube kubra is the Kubernetes version of Apache Ray um it's",
    "start": "3464079",
    "end": "3470880"
  },
  {
    "text": "Ray is a framework uh that goes end to end in entire ML life cycle so you go",
    "start": "3470880",
    "end": "3476640"
  },
  {
    "text": "from uh data prep-processing up to model serving and it gives you a runtime for",
    "start": "3476640",
    "end": "3482640"
  },
  {
    "text": "distributed uh work to run distributed workload and also an API uh with a very",
    "start": "3482640",
    "end": "3488799"
  },
  {
    "text": "nice uh Python um Python module and also a CLI to run",
    "start": "3488799",
    "end": "3495920"
  },
  {
    "text": "uh to submit jobs and also create jobs through the the Ray um the RA the Ray",
    "start": "3495920",
    "end": "3501480"
  },
  {
    "text": "API um so that works uh out of Kubernetes and in Kubernetes kubra is",
    "start": "3501480",
    "end": "3507599"
  },
  {
    "text": "the one that will work uh in Kubernetes and the one that that's where that we're going to use uh the install is super",
    "start": "3507599",
    "end": "3513119"
  },
  {
    "text": "easy through a um through a helm chart and it the nice uh among the nice",
    "start": "3513119",
    "end": "3520319"
  },
  {
    "text": "feature is the ray autoscaler that will help you scaling up and down up and down the workloads in a very transparent way",
    "start": "3520319",
    "end": "3527760"
  },
  {
    "text": "um so the fine-tuning demo is an adaptation uh from a blog post from Red",
    "start": "3527760",
    "end": "3534000"
  },
  {
    "text": "Hat uh which in turn is an adaptation from uh a a demo that you can find on",
    "start": "3534000",
    "end": "3539839"
  },
  {
    "text": "the uh Ray um repository um so we're doing going to do again fine-tuning uh",
    "start": "3539839",
    "end": "3546480"
  },
  {
    "text": "with with Ray and we're going to train uh Llama 3.18 billion on a grade school",
    "start": "3546480",
    "end": "3553200"
  },
  {
    "text": "math uh data set uh with Laura enablement so the demo is going to be a",
    "start": "3553200",
    "end": "3559200"
  },
  {
    "text": "set up the environment we're going to run the fine-tuning job uh through the ray API uh and within an app wrapper and",
    "start": "3559200",
    "end": "3568640"
  },
  {
    "text": "so we're going to submit the job and we're going to follow how that's going how that goes on the array",
    "start": "3568640",
    "end": "3575640"
  },
  {
    "text": "dashboard so",
    "start": "3575640",
    "end": "3581839"
  },
  {
    "text": "okay all right so we we need to install some",
    "start": "3583079",
    "end": "3588760"
  },
  {
    "text": "prerequisite um it's just to set up the Python environment so it's a oneliner uh",
    "start": "3588760",
    "end": "3594880"
  },
  {
    "text": "where we're going to install uh the major is going to be the the ray CLI from which we're going to submit the job",
    "start": "3594880",
    "end": "3602000"
  },
  {
    "text": "uh so it's a super easy and quick setup um to run the job we also need some",
    "start": "3602000",
    "end": "3609040"
  },
  {
    "text": "storage because we need to first of course download uh the llama uh uh model",
    "start": "3609040",
    "end": "3615280"
  },
  {
    "text": "and that's also where we're going to save the checkpoints where the fine-tuned model is going to be so we",
    "start": "3615280",
    "end": "3620880"
  },
  {
    "text": "are creating a uh PVC and for the this entire um demo",
    "start": "3620880",
    "end": "3628400"
  },
  {
    "text": "we're going to impersonate Alis so we're going to run in the blue uh name space",
    "start": "3628400",
    "end": "3633680"
  },
  {
    "text": "and with the d-sis we're saying that we are alis in this case uh so we created the PBC uh at",
    "start": "3633680",
    "end": "3641440"
  },
  {
    "text": "this point we're going to create the ray job so here we're using an image from Quay produced by Red Hat with the ray um",
    "start": "3641440",
    "end": "3650440"
  },
  {
    "text": "runtime uh of course you can use the regular uh ray images provided in the",
    "start": "3650440",
    "end": "3656559"
  },
  {
    "text": "docker hub so we're going to run on one GPU node so we're going to use eight GPUs and we're going to set up uh set",
    "start": "3656559",
    "end": "3664240"
  },
  {
    "text": "one GPU for the ray head which is like the master uh and seven for the ray",
    "start": "3664240",
    "end": "3670880"
  },
  {
    "text": "worker so we're going to have a total of eight pods so we're going to wrap this",
    "start": "3670880",
    "end": "3676079"
  },
  {
    "text": "in the app in an app wrapper and then there is a very nice utility in the MLB batch uh repository that given a um any",
    "start": "3676079",
    "end": "3684960"
  },
  {
    "text": "API object will wrap that into an appro",
    "start": "3684960",
    "end": "3690440"
  },
  {
    "text": "uh the default Q which is the one that Alis uh can use so we create the app",
    "start": "3690640",
    "end": "3697280"
  },
  {
    "text": "proper we just take a look and we see that in the the default Q is the one",
    "start": "3697280",
    "end": "3703200"
  },
  {
    "text": "that's going to be used and the right cluster is exactly the one that we specified in the YAML file there is no change there uh so we're going to submit",
    "start": "3703200",
    "end": "3711520"
  },
  {
    "text": "the job by applying the the YL file so at this point an app wrapper is",
    "start": "3711520",
    "end": "3719440"
  },
  {
    "text": "being created and since the the node is uh is empty it can run immediately so we",
    "start": "3719440",
    "end": "3725839"
  },
  {
    "text": "can see that uh the ray head and the ray workers are being initialized it will take a few",
    "start": "3725839",
    "end": "3732040"
  },
  {
    "text": "minutes um okay so at this point we can use uh also the the ray dashboard to",
    "start": "3732040",
    "end": "3740079"
  },
  {
    "text": "monitor the job which is pretty handy instead of looking at the logs you can",
    "start": "3740079",
    "end": "3745280"
  },
  {
    "text": "just take a look at the dashboard so we're port forwarding so you can see that on local host on the default port",
    "start": "3745280",
    "end": "3752119"
  },
  {
    "text": "8265 uh so at this point now we can set up the uh the fine tuning um so we're",
    "start": "3752119",
    "end": "3758559"
  },
  {
    "text": "cloning the repository where the uh where we can find the code that's uh of",
    "start": "3758559",
    "end": "3763839"
  },
  {
    "text": "course open um and through the Python Python API um",
    "start": "3763839",
    "end": "3771680"
  },
  {
    "text": "the RA Python API we are creating the job with the specification that we need and we set up all other things like",
    "start": "3771680",
    "end": "3779040"
  },
  {
    "text": "where um where we where to download the model in this case in the uh HF chef",
    "start": "3779040",
    "end": "3786720"
  },
  {
    "text": "home so the hugging face home is going to be on the PBC so we have one one copy",
    "start": "3786720",
    "end": "3792000"
  },
  {
    "text": "for for all the pods um we are saying that uh we're",
    "start": "3792000",
    "end": "3797839"
  },
  {
    "text": "running on eight devices in total which is the number of the GPU nodes in this case one for the ray head and and seven",
    "start": "3797839",
    "end": "3804400"
  },
  {
    "text": "for seven different workers so we we create this very short Python code uh",
    "start": "3804400",
    "end": "3811520"
  },
  {
    "text": "and we run it and this is through this ray",
    "start": "3811520",
    "end": "3818119"
  },
  {
    "text": "um this ray API it's pretty much packaging the entire thing is you can",
    "start": "3818119",
    "end": "3823760"
  },
  {
    "text": "imagine it as like a when you package things into a docker container so we'll create one package and that package",
    "start": "3823760",
    "end": "3829680"
  },
  {
    "text": "contains code and instruction dependencies everything that's that that's needed for the job and that code",
    "start": "3829680",
    "end": "3836160"
  },
  {
    "text": "will be running on the ray head and the ray workers so also the head despite",
    "start": "3836160",
    "end": "3841760"
  },
  {
    "text": "that's the one that's um managing the the job distribution that's also doing",
    "start": "3841760",
    "end": "3847760"
  },
  {
    "text": "some actual useful work um so we're waiting for the",
    "start": "3847760",
    "end": "3854799"
  },
  {
    "text": "uh for the job to be submitted and then we can uh we can",
    "start": "3854799",
    "end": "3859960"
  },
  {
    "text": "follow the the actual work either through the ray API uh on the CLI or",
    "start": "3859960",
    "end": "3866640"
  },
  {
    "text": "through the array dashboard so this is taking 15 minutes so of course we're not staying here for 15 minutes i sped up",
    "start": "3866640",
    "end": "3873440"
  },
  {
    "text": "the process uh but there are a few things that we can uh put some attention on uh so in the dashboard you're going",
    "start": "3873440",
    "end": "3880640"
  },
  {
    "text": "to see on the bottom the logs of what is happening and those log those logs are",
    "start": "3880640",
    "end": "3887599"
  },
  {
    "text": "in the ray head so you can also monitor from the coupube cli um there is also",
    "start": "3887599",
    "end": "3893280"
  },
  {
    "text": "the ray uh overview where you can see all the tests that are being executed",
    "start": "3893280",
    "end": "3898319"
  },
  {
    "text": "and through the cluster uh menu you can also see what's going on in in terms of",
    "start": "3898319",
    "end": "3904559"
  },
  {
    "text": "uh utiliz resource utilization so we see that the GPUs are working good because",
    "start": "3904559",
    "end": "3909599"
  },
  {
    "text": "that's what we want to do and yeah through the the ray core the ray core",
    "start": "3909599",
    "end": "3915680"
  },
  {
    "text": "overview we can see which jobs are the total number of jobs the one that are",
    "start": "3915680",
    "end": "3921599"
  },
  {
    "text": "currently running how many are in the queue and all of that it showed up at some point but since we're speeding up",
    "start": "3921599",
    "end": "3927839"
  },
  {
    "text": "the process we may have missed it so when the job is succeeded you can see that on top and at this point we're just",
    "start": "3927839",
    "end": "3935039"
  },
  {
    "text": "going to take a look at the where the data is so in the we're uh executing a",
    "start": "3935039",
    "end": "3941200"
  },
  {
    "text": "terminal in one any ray pod will work we are in the red head in this case and",
    "start": "3941200",
    "end": "3947920"
  },
  {
    "text": "we're seeing that in the slash model that's where the PVC is and we have both",
    "start": "3947920",
    "end": "3954480"
  },
  {
    "text": "the this checkpoint is last one that's where the the the checkpoint that we",
    "start": "3954480",
    "end": "3960160"
  },
  {
    "text": "have been um with the the fine tune model that we just that we just run",
    "start": "3960160",
    "end": "3966280"
  },
  {
    "text": "and I think that's it for",
    "start": "3966280",
    "end": "3971680"
  },
  {
    "text": "And now thanks so we have just one last uh",
    "start": "3972079",
    "end": "3979119"
  },
  {
    "text": "example workload to share with you today uh another really important thing on our cluster is to run models and inference",
    "start": "3979119",
    "end": "3986319"
  },
  {
    "text": "so here I'm just briefly going to show you uh an example batch inference in",
    "start": "3986319",
    "end": "3992000"
  },
  {
    "text": "this example we're running VLM you have already seen VLM mentioned in other talks this conference this is an",
    "start": "3992000",
    "end": "3999039"
  },
  {
    "text": "inference runtime originally uh coming out of Berkeley that is now part of the",
    "start": "3999039",
    "end": "4004240"
  },
  {
    "text": "Linux Foundation and one way to run a machine learning models and what we want to do here so we're going to run an IBM",
    "start": "4004240",
    "end": "4010160"
  },
  {
    "text": "granite model in a in a pod and what one want to do is submit a batch of requests and collect some statistics about the",
    "start": "4010160",
    "end": "4016480"
  },
  {
    "text": "behavior of the model right so the way we do that is sketch on the right we",
    "start": "4016480",
    "end": "4021760"
  },
  {
    "text": "have an app wrapper that contains a job this job itself has two container again one container runs the inference runtime",
    "start": "4021760",
    "end": "4027520"
  },
  {
    "text": "one container ers run the load generator that creates the request and the request",
    "start": "4027520",
    "end": "4032799"
  },
  {
    "text": "collects the responses and so on and I think as we mentioned before one of the things with models they're big so we",
    "start": "4032799",
    "end": "4039119"
  },
  {
    "text": "don't want to have to load them every time we run such a job we want to cache them so in this case we use uh a",
    "start": "4039119",
    "end": "4045200"
  },
  {
    "text": "persistent volume uh set up in our at the bottom here set up here in our in",
    "start": "4045200",
    "end": "4050880"
  },
  {
    "text": "our pod so that the first time we run the workload we're going to download the model waste from hugging face but the",
    "start": "4050880",
    "end": "4056720"
  },
  {
    "text": "next time we run on the same workload we won't have to do this again the details of the containers are here uh pretty",
    "start": "4056720",
    "end": "4064720"
  },
  {
    "text": "straightforward on the left hand side we're running VLM and on the right hand side we're running all generator with a",
    "start": "4064720",
    "end": "4071680"
  },
  {
    "text": "number of parameters here we're just submitting for simplicity demoake we're just submitting random request and we're",
    "start": "4071680",
    "end": "4077599"
  },
  {
    "text": "measuring what happens with the model the only uh non-trivial somewhat hadock",
    "start": "4077599",
    "end": "4083280"
  },
  {
    "text": "somewhat huggy thing here is we need these two things to synchronize because we want the server to be running before",
    "start": "4083280",
    "end": "4089119"
  },
  {
    "text": "we start measurement and when we're done with measurement we want to kill the server right there different ways to do",
    "start": "4089119",
    "end": "4094559"
  },
  {
    "text": "that uh what we found in practice is that u users are not really u good at",
    "start": "4094559",
    "end": "4102560"
  },
  {
    "text": "building their own container images there are lots of reasons not to build container images so here we use an",
    "start": "4102560",
    "end": "4107600"
  },
  {
    "text": "upstream container image unmodified that we can trace and so on and then we do whatever kind of synchronization we can",
    "start": "4107600",
    "end": "4113520"
  },
  {
    "text": "manage between these two things uh I could show you a demo but it's probably",
    "start": "4113520",
    "end": "4118640"
  },
  {
    "text": "yeah maybe I have a second to do that so essentially again this is exactly the",
    "start": "4118640",
    "end": "4124318"
  },
  {
    "text": "same as all of these demos this is what you'll find in the you know supporting documentation from the tutorial we have",
    "start": "4124319",
    "end": "4130318"
  },
  {
    "text": "the specification on the left of the workload i mean first I I'm going through the but I'm going to skip that",
    "start": "4130319",
    "end": "4136318"
  },
  {
    "text": "the persistent volume uh definition but skipping on that we just copy paste the",
    "start": "4136319",
    "end": "4141600"
  },
  {
    "text": "workload definition the YAML you've seen the previous slides in my terminal here and then I can run uh I can capture the",
    "start": "4141600",
    "end": "4149600"
  },
  {
    "text": "logs of the two containers in my pod the first one is starting to load the model the second one is waiting for the model",
    "start": "4149600",
    "end": "4155679"
  },
  {
    "text": "to be loaded eventually the model is loaded the uh inference can start you",
    "start": "4155679",
    "end": "4162640"
  },
  {
    "text": "the the load generated can start sending requests we're processing requests and at some point in the end oops I went too",
    "start": "4162640",
    "end": "4169199"
  },
  {
    "text": "fast at some point in the end we get the statistics out of this particular test run and we can compare let's say with a",
    "start": "4169199",
    "end": "4175679"
  },
  {
    "text": "previous checkpoint of the model and say if we're doing better or worse so let's",
    "start": "4175679",
    "end": "4181120"
  },
  {
    "text": "let me uh try to wrap up here uh no I don't want to play this again um so what",
    "start": "4181120",
    "end": "4187960"
  },
  {
    "text": "we what we realized when we started trying to have a Kubernetes cluster with",
    "start": "4187960",
    "end": "4193520"
  },
  {
    "text": "GPUs to do AI is two fairly obvious things the first one is our admin had no",
    "start": "4193520",
    "end": "4198880"
  },
  {
    "text": "clue they didn't know about GPUs they didn't know about AI so they needed a turnkey solution something they would",
    "start": "4198880",
    "end": "4204880"
  },
  {
    "text": "really could deploy on their cluster without having to think about pretty much anything right the only and and",
    "start": "4204880",
    "end": "4211760"
  },
  {
    "text": "what we've tried to share with you today is that you know the only thing you really have to think about is what are the quotas you want to give to the",
    "start": "4211760",
    "end": "4218239"
  },
  {
    "text": "various teams and that's essentially the only thing that then admin has to do to deploy this setup on the cluster the",
    "start": "4218239",
    "end": "4224800"
  },
  {
    "text": "second thing we've realized is that AI ML experts domain experts they don't understand anything about Kubernetes",
    "start": "4224800",
    "end": "4231440"
  },
  {
    "text": "right so what they need is pre-baked templates you know if you want to pre-train a model this is what you do",
    "start": "4231440",
    "end": "4238800"
  },
  {
    "text": "you know maybe a number of ways to do this but here's the collection if you want to fine-tune the model this is what",
    "start": "4238800",
    "end": "4244640"
  },
  {
    "text": "you do if you want to do data prep-processing this is what you do if you want to do inference this is what we",
    "start": "4244640",
    "end": "4250880"
  },
  {
    "text": "do right so what we've tried to share with you today is again uh some of these",
    "start": "4250880",
    "end": "4257280"
  },
  {
    "text": "templates here right so unfortunately uh we cannot really provision uh a GPU",
    "start": "4257280",
    "end": "4262800"
  },
  {
    "text": "cluster for every member of the audience so we went through this through video which I is not entirely ideal but",
    "start": "4262800",
    "end": "4270159"
  },
  {
    "text": "hopefully you'll get a sense and again the key the key idea here is that all of what we've shown you is just turnkey",
    "start": "4270159",
    "end": "4277199"
  },
  {
    "text": "kind of solutions so I before we conclude I want to just share what it",
    "start": "4277199",
    "end": "4282239"
  },
  {
    "text": "means in practice right here's uh one of our clusters we uh manage in IBM",
    "start": "4282239",
    "end": "4288880"
  },
  {
    "text": "research it has about 1200 GPUs and this is a snapshot I took last week as I were",
    "start": "4288880",
    "end": "4294239"
  },
  {
    "text": "preparing those slides so at that point in time we had about 20 teams uh you",
    "start": "4294239",
    "end": "4299440"
  },
  {
    "text": "know uh on boarded on that cluster right and the quotas for those teams were essentially what you see on the left pie",
    "start": "4299440",
    "end": "4305920"
  },
  {
    "text": "chart here which is you know one you have two teams that more or less have access to a quarter nominal access to a",
    "start": "4305920",
    "end": "4311760"
  },
  {
    "text": "quarter of the cluster each and then a bunch of other teams right and there's a bunch of extra capacity that is not",
    "start": "4311760",
    "end": "4318000"
  },
  {
    "text": "nominally associated with any particular team and what you see in the middle is",
    "start": "4318000",
    "end": "4323440"
  },
  {
    "text": "what we end up with Right with all these teams submitting a number of workloads to the cluster we end up at the point I",
    "start": "4323440",
    "end": "4330159"
  },
  {
    "text": "was I capture this particular snapshot of the state of the cluster where 126 GPU utilized in the cluster and just a a",
    "start": "4330159",
    "end": "4338560"
  },
  {
    "text": "very small fraction less than 1% I guess uh of of 10 GPUs not act not currently",
    "start": "4338560",
    "end": "4345120"
  },
  {
    "text": "in use in the cluster either because we were transitioning between workloads or because there wasn't any really small",
    "start": "4345120",
    "end": "4351199"
  },
  {
    "text": "workloads that could still fit in the cluster Right so we've you know if we",
    "start": "4351199",
    "end": "4356719"
  },
  {
    "text": "were to compare that for instance to using Kubernetes quotas to do the same thing which is to enforce straight",
    "start": "4356719",
    "end": "4362880"
  },
  {
    "text": "quotas and saying for instance the the blue gray team cannot go above its cotton we would have lost",
    "start": "4362880",
    "end": "4370560"
  },
  {
    "text": "essentially oneird oneird of our cluster would have been idled at this point in time right so what we're trying to say",
    "start": "4370560",
    "end": "4376960"
  },
  {
    "text": "here is with this system we've managed to do these two really uh antagonistic",
    "start": "4376960",
    "end": "4382480"
  },
  {
    "text": "things which is one guarantees kota to team so for instance if the uh green",
    "start": "4382480",
    "end": "4389040"
  },
  {
    "text": "team at the bottom comes back and say hey I need my 16 or my 32 GPUs they're going to get them but at the same time",
    "start": "4389040",
    "end": "4396560"
  },
  {
    "text": "we're never really wasting any kind of GPU capacity that we have that is again",
    "start": "4396560",
    "end": "4401679"
  },
  {
    "text": "really fundamentally unacceptable so uh I've shown this before again we",
    "start": "4401679",
    "end": "4408159"
  },
  {
    "text": "try to run through all of these kind of key uh uh features that our stack need",
    "start": "4408159",
    "end": "4414480"
  },
  {
    "text": "to have to really be functional right other than what I just said which is essentially be self uh maintaining and",
    "start": "4414480",
    "end": "4421520"
  },
  {
    "text": "self you know no question asked when you deploy and you manage it and again uh",
    "start": "4421520",
    "end": "4428800"
  },
  {
    "text": "you know a lot of that has to do with uh this community and and and taking advant",
    "start": "4428800",
    "end": "4434480"
  },
  {
    "text": "advantage of all the great work that is doing that is happening in in in many different places and finding the proper",
    "start": "4434480",
    "end": "4441280"
  },
  {
    "text": "way to get these things together jointly configure them so that they you know work together and and and and give all",
    "start": "4441280",
    "end": "4448400"
  },
  {
    "text": "the capabilities that that we want so thank you very much for attending uh you",
    "start": "4448400",
    "end": "4454159"
  },
  {
    "text": "can uh look at uh you know again you can find all the details of what we've shown you uh videos everything at this QR code",
    "start": "4454159",
    "end": "4462400"
  },
  {
    "text": "if you're also interested if you're in general interested in you know platform uh for AIS right on Kubernetes we have a",
    "start": "4462400",
    "end": "4469760"
  },
  {
    "text": "number of talks in this in the rest of this week we've uh you know from IBM",
    "start": "4469760",
    "end": "4474880"
  },
  {
    "text": "research and friends like Nvidia like Redat and others about different aspects other aspects of this platform",
    "start": "4474880",
    "end": "4481159"
  },
  {
    "text": "sustainability compliance uh benchmarking actually and and and uh",
    "start": "4481159",
    "end": "4488080"
  },
  {
    "text": "data prep-processing also which is one uh one we have an entire talk about that we didn't show any templates uh um",
    "start": "4488080",
    "end": "4495760"
  },
  {
    "text": "workload for data processing but the second talk we'll we'll have a lot about that once again thank you very much for",
    "start": "4495760",
    "end": "4502480"
  },
  {
    "text": "attending",
    "start": "4502480",
    "end": "4504800"
  }
]