[
  {
    "start": "0",
    "end": "59000"
  },
  {
    "text": "welcome to monitor the world so quick",
    "start": "60",
    "end": "6930"
  },
  {
    "text": "agenda I'm Nick I'm going to first walk through a pretty high-level intro to",
    "start": "6930",
    "end": "14910"
  },
  {
    "text": "monitoring to get us all on the same page I'm gonna briefly discuss",
    "start": "14910",
    "end": "20520"
  },
  {
    "text": "monitoring applications in a curb kubernetes environment but a lot of it applies to just monitoring applications",
    "start": "20520",
    "end": "26849"
  },
  {
    "text": "in general and then I'm going to touch on monitoring the kubernetes control",
    "start": "26849",
    "end": "32189"
  },
  {
    "text": "plane especially in the context of as you scale your cluster and then my",
    "start": "32189",
    "end": "39690"
  },
  {
    "text": "co-presenter Nick will walk us through how planet labs set up their monitoring",
    "start": "39690",
    "end": "45510"
  },
  {
    "text": "infrastructure which is the actual interesting part of the talk and so our",
    "start": "45510",
    "end": "50850"
  },
  {
    "text": "theme is going to be on sort of how to focus on fewer but more important",
    "start": "50850",
    "end": "56190"
  },
  {
    "text": "metrics so Who am I I'm currently",
    "start": "56190",
    "end": "63000"
  },
  {
    "start": "59000",
    "end": "130000"
  },
  {
    "text": "working for Amazon eks and I formerly worked at a couple Seattle startups",
    "start": "63000",
    "end": "69409"
  },
  {
    "text": "porch and offer up if you've heard of them so quick story the reason that I",
    "start": "69409",
    "end": "74790"
  },
  {
    "text": "came to Seattle was actually in 2015 I moved up to work at porch and my first",
    "start": "74790",
    "end": "82830"
  },
  {
    "text": "day on the job my colleague mentioned that over the last couple days he had",
    "start": "82830",
    "end": "88979"
  },
  {
    "text": "moved the entire production website onto this really cool new technology which I",
    "start": "88979",
    "end": "96030"
  },
  {
    "text": "had never heard of called kubernetes so I was like okay that's that sounds very interesting I did a quick Google search",
    "start": "96030",
    "end": "101159"
  },
  {
    "text": "I noticed that it was at version zero point four and so I asked him about it",
    "start": "101159",
    "end": "108479"
  },
  {
    "text": "and he explained about at the time replication controllers and all this",
    "start": "108479",
    "end": "113790"
  },
  {
    "text": "stuff and then he mentioned he's like oh by the way actually I got a job at",
    "start": "113790",
    "end": "120210"
  },
  {
    "text": "Google and my last days on Friday so have fun with it so that was my very exciting",
    "start": "120210",
    "end": "127440"
  },
  {
    "text": "introduction to kubernetes and it went well so why do we monitor this is fairly",
    "start": "127440",
    "end": "136770"
  },
  {
    "start": "130000",
    "end": "200000"
  },
  {
    "text": "self-explanatory but obviously the most important thing is we need to be able to",
    "start": "136770",
    "end": "141810"
  },
  {
    "text": "detect problems as they happen and then secondarily but almost as important we",
    "start": "141810",
    "end": "148709"
  },
  {
    "text": "want to be able to prevent future outages for example if you have a disk somewhere that's on its way to filling",
    "start": "148709",
    "end": "155160"
  },
  {
    "text": "up we'd like to get an alert maybe twelve hours before it fills up so that we can actually do something about it or",
    "start": "155160",
    "end": "160530"
  },
  {
    "text": "you know for us we would like to know if a data center is running out of capacity",
    "start": "160530",
    "end": "165720"
  },
  {
    "text": "and somebody needs to go and build a rack before we can before our customers can launch more clusters and then",
    "start": "165720",
    "end": "173750"
  },
  {
    "text": "another reason to monitor would be optimization so for example if you want",
    "start": "173750",
    "end": "180360"
  },
  {
    "text": "to understand how to make your applications more efficient and potentially save on cost and",
    "start": "180360",
    "end": "186709"
  },
  {
    "text": "additionally for me because I'm incredibly nosy I want to understand why Nick told me that his service was going",
    "start": "186709",
    "end": "194370"
  },
  {
    "text": "to have a certain QPS but it's actually a hundred times more than he promised right but monitoring in a modern micro",
    "start": "194370",
    "end": "204810"
  },
  {
    "start": "200000",
    "end": "262000"
  },
  {
    "text": "service environment is actually pretty difficult a lot of us are in the process of breaking our monoliths up into many",
    "start": "204810",
    "end": "212100"
  },
  {
    "text": "micro services and as we do that each additional micro service introduces a wealth of metrics and each additional",
    "start": "212100",
    "end": "220980"
  },
  {
    "text": "micro service also means we have all these complex interactions between them and the way that we often use containers",
    "start": "220980",
    "end": "228660"
  },
  {
    "text": "and kubernetes environments is a little bit more transient so looking at the",
    "start": "228660",
    "end": "234600"
  },
  {
    "text": "picture at the VM level doesn't tell us the whole story anymore so we need new",
    "start": "234600",
    "end": "239970"
  },
  {
    "text": "tools and then something that Nick is going to touch on later is sort of the organizational aspects of monitoring",
    "start": "239970",
    "end": "246420"
  },
  {
    "text": "right in an organization where you have many different service teams running many different services they have",
    "start": "246420",
    "end": "251880"
  },
  {
    "text": "different requirements and so it's hard to find a one-size-fits-all strategy",
    "start": "251880",
    "end": "258510"
  },
  {
    "text": "when you have all these different users with different needs but it's helpful to",
    "start": "258510",
    "end": "265350"
  },
  {
    "start": "262000",
    "end": "325000"
  },
  {
    "text": "approach this problem with a method and I'm sure many of you are familiar with both of these but I'll reiterate them so",
    "start": "265350",
    "end": "272310"
  },
  {
    "text": "a common way of thinking about monitoring resources is the used method this was first coined by Brandon Gregg",
    "start": "272310",
    "end": "279270"
  },
  {
    "text": "and it states for every resource we want to check utilization saturation and errors where utilization is a",
    "start": "279270",
    "end": "286710"
  },
  {
    "text": "measurement of the time that we use the research resource over the total time where and saturation is more of an",
    "start": "286710",
    "end": "293460"
  },
  {
    "text": "instantaneous measurement of use where we could we could think of it as like a",
    "start": "293460",
    "end": "299690"
  },
  {
    "text": "counter and Prometheus or an integer that represents maybe the number of processes waiting for a CPU or waiting",
    "start": "299690",
    "end": "306090"
  },
  {
    "text": "on disk and then the other methodology is the read method which was introduced",
    "start": "306090",
    "end": "311640"
  },
  {
    "text": "by Tom Wilkie and this is primarily geared towards monitoring web services",
    "start": "311640",
    "end": "316700"
  },
  {
    "text": "and it states for every service we want to monitor the request rate the the",
    "start": "316700",
    "end": "322740"
  },
  {
    "text": "errors and the duration or latency so here's a graphic of what a kubernetes",
    "start": "322740",
    "end": "332250"
  },
  {
    "start": "325000",
    "end": "427000"
  },
  {
    "text": "cluster or metrics environment might look like on your cluster so we have two",
    "start": "332250",
    "end": "337770"
  },
  {
    "text": "worker nodes and we have our applications themselves you can see the pods running there in this case we've",
    "start": "337770",
    "end": "344970"
  },
  {
    "text": "chosen Prometheus as our metrics pipeline so each of our applications is",
    "start": "344970",
    "end": "350190"
  },
  {
    "text": "instrumented with a Prometheus endpoint and Prometheus is going to pull the metrics from there and since we're",
    "start": "350190",
    "end": "356310"
  },
  {
    "text": "running in kubernetes we also have a cubelet running on each node which has C",
    "start": "356310",
    "end": "361320"
  },
  {
    "text": "advisor embedded in it and that gives us very rich container level metrics so and",
    "start": "361320",
    "end": "369300"
  },
  {
    "text": "then for at the node level we have the node problem detector and the node exporter which both give us information",
    "start": "369300",
    "end": "375570"
  },
  {
    "text": "about our our nodes themselves and the node exporter actually works outside of kubernetes as well and then we have a",
    "start": "375570",
    "end": "384120"
  },
  {
    "text": "couple of things for example HP a which actually acts on the metrics that we gather allows us to scale up our pods",
    "start": "384120",
    "end": "392100"
  },
  {
    "text": "and then there's some orthotic orthogonal topics such as we have a logging demon in there we also have the",
    "start": "392100",
    "end": "398850"
  },
  {
    "text": "metric server which is part of the kubernetes core metrics pipeline so that's something that is gives us",
    "start": "398850",
    "end": "407160"
  },
  {
    "text": "metrics in the moment on pods and nodes so CPU and memory something that can be used by consume by the horizontal pod",
    "start": "407160",
    "end": "413550"
  },
  {
    "text": "autoscaler and then lastly we have cube state metrics running there and that's",
    "start": "413550",
    "end": "419460"
  },
  {
    "text": "something that actually looks at the it derives metrics off the entire kubernetes api so we'll quickly talk a",
    "start": "419460",
    "end": "430320"
  },
  {
    "start": "427000",
    "end": "535000"
  },
  {
    "text": "little bit about monitoring applications and what i want to focus on is how we pick our most important metrics right",
    "start": "430320",
    "end": "437370"
  },
  {
    "text": "what do we what are we putting at the top of our dashboards and and so where do we start and our advice is start with",
    "start": "437370",
    "end": "444330"
  },
  {
    "text": "your users and if you can that means starting with",
    "start": "444330",
    "end": "449700"
  },
  {
    "text": "your business metrics so if you're an online bookseller it might be orders fulfilled successfully maybe you're a",
    "start": "449700",
    "end": "456690"
  },
  {
    "text": "rideshare company so it'd be rights completed or maybe you're a company that",
    "start": "456690",
    "end": "462120"
  },
  {
    "text": "launches satellites into space and takes pictures so then you might be measuring how many pictures you successfully take",
    "start": "462120",
    "end": "468120"
  },
  {
    "text": "and send back down to earth but beyond those business specific metrics what",
    "start": "468120",
    "end": "475680"
  },
  {
    "text": "comes next is essentially the metrics that work across the board so this is",
    "start": "475680",
    "end": "480750"
  },
  {
    "text": "where we go back to the read method for monitoring web services and these can be",
    "start": "480750",
    "end": "485910"
  },
  {
    "text": "these metrics can be really helpful for for example for an Operations team that's setting up default metrics and",
    "start": "485910",
    "end": "491970"
  },
  {
    "text": "dashboards for their organization that work for everyone and Nick's gonna talk a little bit about",
    "start": "491970",
    "end": "497190"
  },
  {
    "text": "that later so application request errors are a really good first place to start",
    "start": "497190",
    "end": "504479"
  },
  {
    "text": "right because they give you a very strong signal of something that's going wrong and then you can use tracing and",
    "start": "504479",
    "end": "510030"
  },
  {
    "text": "logs and and supplementary signals to tell you where to look next so and then",
    "start": "510030",
    "end": "516300"
  },
  {
    "text": "application latency similarly also gives you almost a strong signal although it's",
    "start": "516300",
    "end": "522180"
  },
  {
    "text": "not a like a boolean it's more of a gage but it can tell you about something",
    "start": "522180",
    "end": "528949"
  },
  {
    "text": "that's actually affecting user experience or something maybe it's leading it could lead to cascading failures but there's some supplementary",
    "start": "528949",
    "end": "537410"
  },
  {
    "start": "535000",
    "end": "610000"
  },
  {
    "text": "signals like for example one of the most",
    "start": "537410",
    "end": "543709"
  },
  {
    "text": "frequent causes of outages is ourselves when we deploy code so something that",
    "start": "543709",
    "end": "550160"
  },
  {
    "text": "you can do really easily in a kubernetes environment is just attaching for example a version label which I'm sure",
    "start": "550160",
    "end": "556220"
  },
  {
    "text": "many of you do to your pods your deployments and then understanding as",
    "start": "556220",
    "end": "561379"
  },
  {
    "text": "your deployment progresses throughout your clusters in your environments where",
    "start": "561379",
    "end": "566420"
  },
  {
    "text": "for example the new version is and how many environments is progressed - and",
    "start": "566420",
    "end": "571910"
  },
  {
    "text": "this is this is so we can actually get this metric from cube state metrics and",
    "start": "571910",
    "end": "577970"
  },
  {
    "text": "the metric is cube pod labels here but that applies to all so you have cube deployment labels cube daemon set labels",
    "start": "577970",
    "end": "584749"
  },
  {
    "text": "etc another supplementary signal that we can talk about and this is included in",
    "start": "584749",
    "end": "590480"
  },
  {
    "text": "the read method is request rate so that we can understand when our applications are overloaded by our users and then",
    "start": "590480",
    "end": "598089"
  },
  {
    "text": "saturation as well and I want to touch on what saturation actually means so an",
    "start": "598089",
    "end": "603920"
  },
  {
    "text": "application is saturated when its resources are saturated so how do we",
    "start": "603920",
    "end": "609470"
  },
  {
    "text": "know when its resources are saturated well we can actually get if we're talking about for example CPU memory",
    "start": "609470",
    "end": "616430"
  },
  {
    "start": "610000",
    "end": "666000"
  },
  {
    "text": "disk network we can get a lot of these metrics from cubelet or cube state metrics but in general I would recommend",
    "start": "616430",
    "end": "625819"
  },
  {
    "text": "starting with the metrics from cubelet and that's because as we scale up cubelet is still responsible for the",
    "start": "625819",
    "end": "632059"
  },
  {
    "text": "pods that are running on its node so it's going to be a little bit more efficient in that sense whereas cube",
    "start": "632059",
    "end": "638720"
  },
  {
    "text": "state metrics is still responsible for the entire cluster and driving metrics off the entire clusters state so you",
    "start": "638720",
    "end": "645170"
  },
  {
    "text": "have to be conscious of the resources that keep state metrics requires but",
    "start": "645170",
    "end": "650209"
  },
  {
    "text": "here we have from cubelet from the first - from the embedded sea advisor itself",
    "start": "650209",
    "end": "655370"
  },
  {
    "text": "actually we have the CPU and memory additionally there's some complementary",
    "start": "655370",
    "end": "660500"
  },
  {
    "text": "metrics for example if you have persistent volumes you can get some interesting metrics off that and to",
    "start": "660500",
    "end": "668329"
  },
  {
    "start": "666000",
    "end": "696000"
  },
  {
    "text": "touch on cube state metrics so it is very it does provide some very interesting metrics and useful metrics",
    "start": "668329",
    "end": "674629"
  },
  {
    "text": "one of them being container restarts another one is just getting an",
    "start": "674629",
    "end": "680149"
  },
  {
    "text": "understanding of if you have a deployment like how many pods and that deployment are available at any given",
    "start": "680149",
    "end": "685519"
  },
  {
    "text": "time or how much of if you have a pod disruption budget how much of your pod",
    "start": "685519",
    "end": "691850"
  },
  {
    "text": "disruption budget you've actually used and this brings me to monitoring the",
    "start": "691850",
    "end": "700129"
  },
  {
    "start": "696000",
    "end": "718000"
  },
  {
    "text": "control plane so one question would be how is monitoring the control plane",
    "start": "700129",
    "end": "706009"
  },
  {
    "text": "different from monitoring applications and actually it's very similar a lot of",
    "start": "706009",
    "end": "712850"
  },
  {
    "text": "the stuff that we've learned from monitoring applications applies to monitoring there's the control plane especially the API server right we can",
    "start": "712850",
    "end": "719300"
  },
  {
    "start": "718000",
    "end": "768000"
  },
  {
    "text": "we can basically treat the API server is another one of our applications we can apply the read method to it monitor",
    "start": "719300",
    "end": "726920"
  },
  {
    "text": "request rate errors in duration and then two other things that I would say cover",
    "start": "726920",
    "end": "733970"
  },
  {
    "text": "the bare minimum of monitoring the control plane would be just a general sense of your everything running in your",
    "start": "733970",
    "end": "740209"
  },
  {
    "text": "cube system is it available do you see a lot of container restarts and then I",
    "start": "740209",
    "end": "747160"
  },
  {
    "text": "would also look at SCD specifically and just make sure that it's you know it's",
    "start": "747160",
    "end": "752779"
  },
  {
    "text": "the cluster is healthy it has quorum its accepting rights and you don't have like really slow disk that's causing you know",
    "start": "752779",
    "end": "760899"
  },
  {
    "text": "slow slow writes and client timeouts so that I would say that covers the bare",
    "start": "760899",
    "end": "766639"
  },
  {
    "text": "minimum honoring the control claim and then as you start to scale up your cluster the first thing that you want to",
    "start": "766639",
    "end": "773930"
  },
  {
    "start": "768000",
    "end": "834000"
  },
  {
    "text": "be cognizant of is your API server starting to consume more and more CPU",
    "start": "773930",
    "end": "779689"
  },
  {
    "text": "and memory so you might watch for dropped requests which manifests as 429",
    "start": "779689",
    "end": "785870"
  },
  {
    "text": "s and you can actually adjust the number of requests that the API",
    "start": "785870",
    "end": "791390"
  },
  {
    "text": "server can handle simultaneously with the max requests in-flight flag but you",
    "start": "791390",
    "end": "799100"
  },
  {
    "text": "have to be cognizant of the fact that as you do that it's going to take more resources so you have to in kind adjust",
    "start": "799100",
    "end": "805400"
  },
  {
    "text": "your requests and limits based on you know your note count as your note count",
    "start": "805400",
    "end": "810620"
  },
  {
    "text": "goes up and your pod density increases and your objects churn increases and to",
    "start": "810620",
    "end": "816020"
  },
  {
    "text": "reduce load on the API server you can bring down you can you can reduce the amount that your API server clients can",
    "start": "816020",
    "end": "822560"
  },
  {
    "text": "call it but as you scale up you probably won't be able to do that you'll probably",
    "start": "822560",
    "end": "828020"
  },
  {
    "text": "actually have to go the opposite direction and allow them to do more work so so to those clients actually as we as",
    "start": "828020",
    "end": "836330"
  },
  {
    "start": "834000",
    "end": "887000"
  },
  {
    "text": "we scale we want to pay attention to what's going on with the scheduler and the controller manager one issue that",
    "start": "836330",
    "end": "843820"
  },
  {
    "text": "that we see is that the scheduler is going to slow down as you have more pods to schedule and more nodes to schedule",
    "start": "843820",
    "end": "850910"
  },
  {
    "text": "them on so there's two metrics actually one of them exposed from cube state",
    "start": "850910",
    "end": "856550"
  },
  {
    "text": "metrics and one of them is exposed from the scheduler which tells us how long the scheduler is taking to schedule pods",
    "start": "856550",
    "end": "863890"
  },
  {
    "text": "and then the controller manager we want to know for each controller how long",
    "start": "863890",
    "end": "870200"
  },
  {
    "text": "it's taking to do a unit of work and how long that unit of work is sitting around in the queue before it gets acted on so",
    "start": "870200",
    "end": "877730"
  },
  {
    "text": "they keep weren't there is we really just want to for those control plane components at a minimum we want to",
    "start": "877730",
    "end": "883340"
  },
  {
    "text": "understand how well they're doing their key jobs and finally the last thing that",
    "start": "883340",
    "end": "889400"
  },
  {
    "start": "887000",
    "end": "932000"
  },
  {
    "text": "I want to talk about is monitoring sed so it's fairly straightforward that CD",
    "start": "889400",
    "end": "895520"
  },
  {
    "text": "is generally healthy when it has a quorum and it's accepting rights and so",
    "start": "895520",
    "end": "900560"
  },
  {
    "text": "the first thing to look at is leader elections and just to understand whether or not your sed cluster has a leader and",
    "start": "900560",
    "end": "907270"
  },
  {
    "text": "whether or not you're seeing constant leader changes that can signify an",
    "start": "907270",
    "end": "912440"
  },
  {
    "text": "unhealthy cluster and then another thing that can lead to degraded performance is",
    "start": "912440",
    "end": "918520"
  },
  {
    "text": "you know your disk write performance so how long it's taking at CD to commit",
    "start": "918520",
    "end": "924170"
  },
  {
    "text": "rights and if it's if it's really slow then you can see clients timing out as they're",
    "start": "924170",
    "end": "930199"
  },
  {
    "text": "trying to write finally so another thing",
    "start": "930199",
    "end": "935509"
  },
  {
    "start": "932000",
    "end": "966000"
  },
  {
    "text": "to keep in mind is the database size so when Etsy DB fills up its quota for its",
    "start": "935509",
    "end": "940699"
  },
  {
    "text": "DB file it's going to trigger a no space alarm so we're gonna watch for that and",
    "start": "940699",
    "end": "946129"
  },
  {
    "text": "additionally SCD has a flag to where it will periodically check for corruption",
    "start": "946129",
    "end": "952579"
  },
  {
    "text": "so in terms of corruption we can let @cd sort of monitor itself and with that i'm",
    "start": "952579",
    "end": "958519"
  },
  {
    "text": "going to hand over to my co-presenter nick and he's going to talk us through a little bit about how planetlab set up",
    "start": "958519",
    "end": "964970"
  },
  {
    "text": "their monitoring infrastructure hey so I'm Nick I am the kubernetes tech lead at plant labs",
    "start": "964970",
    "end": "971560"
  },
  {
    "start": "966000",
    "end": "1001000"
  },
  {
    "text": "I've previously been sre at Google and Spotify onofre with Nick so planet labs",
    "start": "971560",
    "end": "978589"
  },
  {
    "text": "we're an earth imaging company we have about 160 satellites in space",
    "start": "978589",
    "end": "984610"
  },
  {
    "text": "my team has debated as to we said that they were at the size of a loaf of bread and we've debated what kind of loaf of",
    "start": "984829",
    "end": "990500"
  },
  {
    "text": "bread we've decided they're about baguette sized so we image 300 million square kilometers every day which is",
    "start": "990500",
    "end": "997850"
  },
  {
    "text": "about twice the land surface of the earth so my team unfortunately for y'all",
    "start": "997850",
    "end": "1004569"
  },
  {
    "text": "we work on the boring terrestrial side of things there is a team that manages the spacecraft we do computer vision on",
    "start": "1004569",
    "end": "1009670"
  },
  {
    "text": "top of the imagery 300 million square kilometers every day that is a lot of",
    "start": "1009670",
    "end": "1014680"
  },
  {
    "text": "imagery it's a lot to look at with your human eyes so my organization our greater goal is to come up with a time",
    "start": "1014680",
    "end": "1022569"
  },
  {
    "text": "series database we call it the queryable earth so if you rather than looking at a ton of imagery if you just want to",
    "start": "1022569",
    "end": "1027850"
  },
  {
    "text": "subscribe and see is there 20 ships container ships in Seattle today and 30",
    "start": "1027850",
    "end": "1033250"
  },
  {
    "text": "tomorrow you can sort of subscribe to that so our organization is about 70 people across a lot of time servants",
    "start": "1033250",
    "end": "1039010"
  },
  {
    "text": "tens of microservices operational teams who are on-call Pharaon services run",
    "start": "1039010",
    "end": "1044740"
  },
  {
    "text": "their own services within that there is the kubernetes team Hobbs we're a tiger team originally and we got that name I",
    "start": "1044740",
    "end": "1051870"
  },
  {
    "text": "run that team there's five of us and we do we both run the production kubernetes clusters and anything to do with kubernetes including",
    "start": "1051870",
    "end": "1058009"
  },
  {
    "text": "monitoring logging etc so I got to plant it a little over a year ago and my first day they were like so we're doing this",
    "start": "1058009",
    "end": "1063769"
  },
  {
    "text": "queryable last thing we need a 5000 Reuben a DS cluster can you have it ready at the end of the quarter and I was like sure how hard could that be in",
    "start": "1063769",
    "end": "1071960"
  },
  {
    "text": "practice a little over a year later we're actually between about a hundred and four hundred nodes at any particular point in time we have highly elastic",
    "start": "1071960",
    "end": "1077480"
  },
  {
    "text": "workloads we see about fifteen thousand requests per second at ingress across to",
    "start": "1077480",
    "end": "1084889"
  },
  {
    "text": "production pastures we run on Google compute engine we don't use DKE because",
    "start": "1084889",
    "end": "1090500"
  },
  {
    "text": "reasons we probably would if we sell there again today but we're not using GK at the moment so the every different",
    "start": "1090500",
    "end": "1099080"
  },
  {
    "start": "1096000",
    "end": "1108000"
  },
  {
    "text": "organization in team has a slightly different monitoring philosophy I think it's useful to talk about mine before I talk about all my teams before I talk",
    "start": "1099080",
    "end": "1104659"
  },
  {
    "text": "about how we actually monitoring practice the planet so first we're gonna",
    "start": "1104659",
    "end": "1109669"
  },
  {
    "start": "1108000",
    "end": "1155000"
  },
  {
    "text": "page only when customers are affected you may notice there's an L in that word at Planet costumers is synonymous with",
    "start": "1109669",
    "end": "1114710"
  },
  {
    "text": "service owners cluster mate is a customer of our clusters if they are affected our business customers might be",
    "start": "1114710",
    "end": "1120049"
  },
  {
    "text": "affected and that would be bad so one example of this is when we first set up a kubernetes clusters core OS provided",
    "start": "1120049",
    "end": "1127580"
  },
  {
    "text": "prometheus although manager config that just had a bunch of stock alerts Fred CD and one of those alerts would page you",
    "start": "1127580",
    "end": "1132950"
  },
  {
    "text": "if it city disk latency was high or if there was a spike in it and that's bad that could lead to leader election which",
    "start": "1132950",
    "end": "1138710"
  },
  {
    "text": "could happening all the time which can lead to failed Rises Nick was talking about but then we found that we get page four in the middle of nine oh there was",
    "start": "1138710",
    "end": "1144740"
  },
  {
    "text": "a spike in Lindsey and went away there was a leader election and the cluster kept working so we don't get",
    "start": "1144740",
    "end": "1150980"
  },
  {
    "text": "woken up for that anymore because there was no actual impact on our customers so",
    "start": "1150980",
    "end": "1157039"
  },
  {
    "start": "1155000",
    "end": "1243000"
  },
  {
    "text": "sort of following oh that's thing we want a page only when software won't fix the problem kubernetes and most cloud providers are really good they work",
    "start": "1157039",
    "end": "1163159"
  },
  {
    "text": "really hard to be self-healing so for example in kubernetes you're all probably familiar with liveness and readiness pervs that will replace",
    "start": "1163159",
    "end": "1169039"
  },
  {
    "text": "unhealthy pods at the cloud provider level for us we health check our instances couplets so if a couplet is",
    "start": "1169039",
    "end": "1175490"
  },
  {
    "text": "unhealthy we'll have GCE delete it and recreate it and we don't want a page until those processes have had a chance to actually",
    "start": "1175490",
    "end": "1181580"
  },
  {
    "text": "fix the problem another more concrete example of this is that there's a bug that was recently fixed in kubernetes 1.1 3",
    "start": "1181580",
    "end": "1189370"
  },
  {
    "text": "that would cause processes to get stuck in uninterruptible sleep and basically that would cause any pods depending on",
    "start": "1189370",
    "end": "1194650"
  },
  {
    "text": "those processes or containing those processes to be stuck for a long time or forever so we used to get paged when",
    "start": "1194650",
    "end": "1201970"
  },
  {
    "text": "this would happen we'd see wedge pods we would drain the node that they were on according the notes of indicate new pods",
    "start": "1201970",
    "end": "1208150"
  },
  {
    "text": "train the nodes so that healthy pods got moved away successfully or cleanly and then to leave a note I got really",
    "start": "1208150",
    "end": "1213880"
  },
  {
    "text": "repetitive so we wrote a piece of software which is open source called Drano which uses the kubernetes node",
    "start": "1213880",
    "end": "1219550"
  },
  {
    "text": "problem detector other open source software from Google the node problem detector looks at signals on a node for",
    "start": "1219550",
    "end": "1226600"
  },
  {
    "text": "instance tailing logs or running arbitrary scripts and if it sees something that considers bad it sets a node condition to indicate a permanent",
    "start": "1226600",
    "end": "1232750"
  },
  {
    "text": "problem with the node Drano just looks for arbitrary configured node conditions if it sees them",
    "start": "1232750",
    "end": "1238270"
  },
  {
    "text": "corns the node drains the node let's the class to autoscaler get rid of it so we don't need to get paged anymore when it happens finally what a page only the",
    "start": "1238270",
    "end": "1245950"
  },
  {
    "start": "1243000",
    "end": "1286000"
  },
  {
    "text": "team who can fix it so my team is on call for kubernetes infrastructure my costume is were on call for their services this is the hardest thing in my",
    "start": "1245950",
    "end": "1253090"
  },
  {
    "text": "opinion if we see for instance we use the link a tea service mesh so we get",
    "start": "1253090",
    "end": "1259179"
  },
  {
    "text": "standardized request metrics across all of our services if we see a spike in HTTP 500s or an increase in latency if",
    "start": "1259179",
    "end": "1265990"
  },
  {
    "text": "it's only affecting one service there's a good chance that maybe they've run out of microsd workers or something else",
    "start": "1265990",
    "end": "1271240"
  },
  {
    "text": "with their service and they need to fix that I don't really want to get paid for that I want them to get paid for that on the other hand if I see that 10",
    "start": "1271240",
    "end": "1277150"
  },
  {
    "text": "different services have had an increase in latency or increase in 500 so there's a good chance that something to do with my infrastructure and I want to get paid",
    "start": "1277150",
    "end": "1283150"
  },
  {
    "text": "for that I wanna fear not have my customers so what are we actually",
    "start": "1283150",
    "end": "1288700"
  },
  {
    "start": "1286000",
    "end": "1297000"
  },
  {
    "text": "monitor I will say that we just gonna talk about sort of time series based monitoring here we do use distributed request tracing we do use structured",
    "start": "1288700",
    "end": "1294370"
  },
  {
    "text": "logging but not gonna talk about that in this talk so this is at a glance",
    "start": "1294370",
    "end": "1299920"
  },
  {
    "start": "1297000",
    "end": "1504000"
  },
  {
    "text": "dashboard or the top part of it at least if you're in my team at planet and you get page or something",
    "start": "1299920",
    "end": "1305050"
  },
  {
    "text": "your page probably has a link to this dashboard and went to the rotten book for your load there are other dashboards that we have that drill down deeper into",
    "start": "1305050",
    "end": "1311440"
  },
  {
    "text": "the cluster autoscaler health coordinators health control plane health at City Health etc etc this is typically",
    "start": "1311440",
    "end": "1318340"
  },
  {
    "text": "where you start and this is structured similar to what Nick described before based on the business metrics that we have given",
    "start": "1318340",
    "end": "1325570"
  },
  {
    "text": "that many different things running my cluster and they have different actual business metrics like images downloaded and whatnot we go for that lowest common",
    "start": "1325570",
    "end": "1331510"
  },
  {
    "text": "denominator of the read method so that top dashboard is powered by link 2d that is just the request rate error rate and",
    "start": "1331510",
    "end": "1337900"
  },
  {
    "text": "to the left of that the duration p99 I think or p95 histogram for requests as",
    "start": "1337900",
    "end": "1345160"
  },
  {
    "text": "you go further down we have a lot of highly elastic sort of machine learning workloads that cause many nodes to get",
    "start": "1345160",
    "end": "1350169"
  },
  {
    "text": "spun up and spun down we actually rather than alerting on schedule or latency we",
    "start": "1350169",
    "end": "1355299"
  },
  {
    "text": "actually just all work on the total time that pods are impending so if pods are impending for any reason for too long for instance nodes can't spit up or",
    "start": "1355299",
    "end": "1361299"
  },
  {
    "text": "whatever will be alerted to fix that but we also graph that there a fun fact all those little purple triangles beneath",
    "start": "1361299",
    "end": "1367270"
  },
  {
    "text": "that dashboard that is Draenor remediating broken notes so we see that a lot and Drano comes in and kicks in and shoots those nodes at the very",
    "start": "1367270",
    "end": "1374500"
  },
  {
    "text": "bottom you'll see what Nick talked about before we show you the running software versions this cluster was running pretty much all the same set of software",
    "start": "1374500",
    "end": "1380919"
  },
  {
    "text": "versions and to the right of that the bottom right corner you will see the running container version so we can correlate if there's a new version of",
    "start": "1380919",
    "end": "1386410"
  },
  {
    "text": "core OS which we run and latency goes up at the same time then we can see those all those dashboards as sort of matching",
    "start": "1386410",
    "end": "1393340"
  },
  {
    "text": "patterns alerts wise we break everything",
    "start": "1393340",
    "end": "1398440"
  },
  {
    "text": "into high and low urgency alerts so for instance that NCD disk latency thing that I spoke about before we didn't",
    "start": "1398440",
    "end": "1403990"
  },
  {
    "text": "completely get rid of that alert we just made it not wake people up hired to see alerts for us means you get woken up in",
    "start": "1403990",
    "end": "1409720"
  },
  {
    "text": "the morning you fix it low urgency means you fix us some time during your on-call shift and fixing it might just mean silencing it for a couple of months and",
    "start": "1409720",
    "end": "1416260"
  },
  {
    "text": "putting a bug in to look at it at some point the future so high urgency alerts",
    "start": "1416260",
    "end": "1421450"
  },
  {
    "text": "pretty much is something that is critical to the cluster broken and do we",
    "start": "1421450",
    "end": "1427750"
  },
  {
    "text": "feel that Auto remediation hasn't worked so our ingress have we lost ingress nodes and they didn't come back and we expect them to come back within n",
    "start": "1427750",
    "end": "1433630"
  },
  {
    "text": "minutes because of Auto remediation or have we lost control pods you know core",
    "start": "1433630",
    "end": "1438910"
  },
  {
    "text": "DNS is that any all about control pods going to biston so basically if any part is unhealthy and group system then that's bad and we we load on that after",
    "start": "1438910",
    "end": "1445270"
  },
  {
    "text": "we've given kubernetes a chance to fix it we also we use link 81 so we use that",
    "start": "1445270",
    "end": "1450429"
  },
  {
    "text": "service mesh and that means that we are providing as a service to all of our customers so we will",
    "start": "1450429",
    "end": "1455650"
  },
  {
    "text": "also page if we see average errors increasing across like ten different",
    "start": "1455650",
    "end": "1461020"
  },
  {
    "text": "services multiple different service dimensions as I mentioned before low urgency alerts there's a lot of them",
    "start": "1461020",
    "end": "1468340"
  },
  {
    "text": "some of the interesting ones are if we lose one EDD follower that's low urgency because we can tolerate two failures so",
    "start": "1468340",
    "end": "1474130"
  },
  {
    "text": "we lose one follower low urgency we fix it that way if we lose two followers one more means the clusters bought so we",
    "start": "1474130",
    "end": "1480010"
  },
  {
    "text": "page probably the most noisy and also the most useful is this alert that we've",
    "start": "1480010",
    "end": "1485770"
  },
  {
    "text": "tuned a lot about pod restarts or container restarts within pod we actually just we used to try new fancy",
    "start": "1485770",
    "end": "1491110"
  },
  {
    "text": "things now just if any part has a container that restarts ten times we page low agency it's almost never something that's actually seriously",
    "start": "1491110",
    "end": "1497410"
  },
  {
    "text": "broken but it's very often a sort of canary for underlying problems in the cluster that you might want to go follow",
    "start": "1497410",
    "end": "1502510"
  },
  {
    "text": "up on later so as I mentioned before our customers are operationally responsible",
    "start": "1502510",
    "end": "1508930"
  },
  {
    "start": "1504000",
    "end": "1582000"
  },
  {
    "text": "at the end of the day they get to choose what metrics are important to them are they instrument their services they said",
    "start": "1508930",
    "end": "1514120"
  },
  {
    "text": "that their own dashboards and their own alerts we provide the monitoring",
    "start": "1514120",
    "end": "1520180"
  },
  {
    "text": "infrastructure for them and we also try and provide some standardized metrics and those come from the lincolni service mesh and from Cupido days they mostly",
    "start": "1520180",
    "end": "1526780"
  },
  {
    "text": "come from cube state metrics that Nick mentioned we provide them with the count upon restarts for their services we",
    "start": "1526780",
    "end": "1532030"
  },
  {
    "text": "provide them with the health of their pods the you know for a deployment how many replicas they want versus how many",
    "start": "1532030",
    "end": "1537040"
  },
  {
    "text": "they have we also provide them concrete resource utilization which is semi",
    "start": "1537040",
    "end": "1542380"
  },
  {
    "text": "useful we more interesting ly we provide them CPU and memory etc utilization as a percentage of their requests and limits",
    "start": "1542380",
    "end": "1548950"
  },
  {
    "text": "so they can tune those I were in the process of open sourcing some software called Costanzo that will actually tell",
    "start": "1548950",
    "end": "1555850"
  },
  {
    "text": "you roughly how much your services cost rather than just it's using ten Zieve use its using fifty dollars a month or",
    "start": "1555850",
    "end": "1561520"
  },
  {
    "text": "something like that I would say the main value here is just that these requests",
    "start": "1561520",
    "end": "1567340"
  },
  {
    "text": "are standardized so if I understand how my service works I understand how your service works from a large monitoring perspective if I move",
    "start": "1567340",
    "end": "1572740"
  },
  {
    "text": "to your team I don't have to learn too much if I if your service is overloading",
    "start": "1572740",
    "end": "1577840"
  },
  {
    "text": "mine I can go and sort of understand all of that pretty quickly so you may have",
    "start": "1577840",
    "end": "1583090"
  },
  {
    "text": "noticed that we said we were sort of talking about open source and that was actually a signal effects dashboard that I showed you before",
    "start": "1583090",
    "end": "1588660"
  },
  {
    "text": "signal fix is our monitoring vendor we've used them a planet for a long time we like them a lot particularly their billing model works well for us and but",
    "start": "1588660",
    "end": "1595500"
  },
  {
    "text": "it ties to cardinality basically they bill you four data points per minute or at least that's what they bill us for so",
    "start": "1595500",
    "end": "1600840"
  },
  {
    "text": "if you have three time series and you sample once a minute you're paying for three data points per minute when we",
    "start": "1600840",
    "end": "1606270"
  },
  {
    "text": "first set up our naive monitoring implementation it worked great it did everything that we wanted for and we were sending about 110,000 data points",
    "start": "1606270",
    "end": "1612809"
  },
  {
    "text": "per minute to signal effects which was more than a fifth of planet's total budget for monitoring just with the kubernetes cluster infrastructure not",
    "start": "1612809",
    "end": "1619020"
  },
  {
    "text": "even the things running on the clusters so we took somewhat of a novel approach",
    "start": "1619020",
    "end": "1624540"
  },
  {
    "text": "to fixing this and we basically put a Prometheus in it before I go too much into this I want to say that this is a",
    "start": "1624540",
    "end": "1629940"
  },
  {
    "text": "controversial pattern within the curve prometheus community you look at Prometheus issues 3902 there's a lot of",
    "start": "1629940",
    "end": "1635070"
  },
  {
    "text": "discussion about this but as many of you may know Prometheus supports remote right so it can store its time series in",
    "start": "1635070",
    "end": "1641850"
  },
  {
    "text": "s3 or GCS or various different Atkins and signal effects implements that back-end so we can use Prometheus to",
    "start": "1641850",
    "end": "1647940"
  },
  {
    "text": "basically write to signal effects furthermore Prometheus supports recording rules so you can take",
    "start": "1647940",
    "end": "1654330"
  },
  {
    "text": "something that might be let's say HD to be a request rate across 100 pods broken down by status code that might be",
    "start": "1654330",
    "end": "1662460"
  },
  {
    "text": "hundreds of time series we average that or rather we sum it up on the on the",
    "start": "1662460",
    "end": "1667559"
  },
  {
    "text": "service name and we just sent your signal effects your service is doing this many requests per second in this cluster not on this pod or on this",
    "start": "1667559",
    "end": "1673470"
  },
  {
    "text": "container or anything like that dramatically sort of reduced our DBM that we were sending a signal effects and sort of this is obvious in",
    "start": "1673470",
    "end": "1680400"
  },
  {
    "text": "retrospect but one other thing I was really pleased by was it also really increased our observability like Prometheus is the lingua franca of cloud",
    "start": "1680400",
    "end": "1686400"
  },
  {
    "text": "native monitoring and monitoring and other things tend to trail it a little bit so just moving to Prometheus meant",
    "start": "1686400",
    "end": "1691559"
  },
  {
    "text": "that we sort of suddenly had more visibility into cloud sorry into core DNS the auto scale of things like that",
    "start": "1691559",
    "end": "1699470"
  },
  {
    "text": "so I think the main things that I've learned after sort of a year of trying to trying to deliver this is that it's",
    "start": "1700520",
    "end": "1706860"
  },
  {
    "text": "difficult to balance under versus overloading I'm a grumpy SRA and I tend to basically be like you shouldn't page my team unless you know that it's my",
    "start": "1706860",
    "end": "1713190"
  },
  {
    "text": "problem and this has led to situations where my costumers have been paged due to what was my problem and we have to have a conversation with them and work",
    "start": "1713190",
    "end": "1719130"
  },
  {
    "text": "with them improve both of our loading systems or both of our loading patterns to make sure the right people are getting alerted you have to kind of look deep",
    "start": "1719130",
    "end": "1727790"
  },
  {
    "text": "when your service teams are asking for availability Sariah observability patterns one",
    "start": "1727790",
    "end": "1733820"
  },
  {
    "text": "example was that we had one team that made it seem like the most critical thing there are all could do was get distributed request tracing in place so",
    "start": "1733820",
    "end": "1740690"
  },
  {
    "text": "we got it in place I love it I use it a lot almost no one else uses it a planet they're all sort of stuck in this sort of less you structured logging instead",
    "start": "1740690",
    "end": "1746690"
  },
  {
    "text": "so now we sort of pay a lot more attention when someone asked for something we talked to everyone and like",
    "start": "1746690",
    "end": "1752030"
  },
  {
    "text": "hey well you all really find this useful can we like have a big discussion put some design Doc's out there find what what everyone actually wants as I say",
    "start": "1752030",
    "end": "1758840"
  },
  {
    "text": "it's hard to be Prometheus in the monitoring space for kubernetes but a lot of people shy away from it because you have to run your own alert manager",
    "start": "1758840",
    "end": "1764990"
  },
  {
    "text": "and you're in cavada you're on the hook for like really important stuff within your org we have found a planet that we",
    "start": "1764990",
    "end": "1771470"
  },
  {
    "text": "can find a compromise that works for us at least where prometheus is kind of like the monitoring agent and",
    "start": "1771470",
    "end": "1776660"
  },
  {
    "text": "aggregation layer and then we just let our monitoring vendor take care of making sure that their dashboards are up",
    "start": "1776660",
    "end": "1782390"
  },
  {
    "text": "and although it's happening and things like that all right and now I'll hand you back over to Nick to wrap it up so to summarise kubernetes",
    "start": "1782390",
    "end": "1798260"
  },
  {
    "start": "1792000",
    "end": "1879000"
  },
  {
    "text": "monitoring monitoring environment is fairly complex and when you're when",
    "start": "1798260",
    "end": "1803540"
  },
  {
    "text": "we're monitoring applications you know start with the metrics that actually affect your users the most important",
    "start": "1803540",
    "end": "1809450"
  },
  {
    "text": "metrics but when we're talking about the control plane and scaling up your",
    "start": "1809450",
    "end": "1815360"
  },
  {
    "text": "cluster that involves reactively tuning based on some key metrics for example",
    "start": "1815360",
    "end": "1823850"
  },
  {
    "text": "your scheduler latency or controller manager how much time it's taking to consume or do an item of work and and",
    "start": "1823850",
    "end": "1833510"
  },
  {
    "text": "then as Nick mentioned it really helps to approach your philosophy around",
    "start": "1833510",
    "end": "1840490"
  },
  {
    "text": "setting up a monitoring environment to do what works for you and your",
    "start": "1840490",
    "end": "1847520"
  },
  {
    "text": "organization because every organization is different so with that thank you guys",
    "start": "1847520",
    "end": "1852650"
  },
  {
    "text": "for coming appreciate it and some links here you",
    "start": "1852650",
    "end": "1858350"
  },
  {
    "text": "can find us on Twitter and github and some of the open source projects that Nick mentioned are linked here as well",
    "start": "1858350",
    "end": "1864490"
  },
  {
    "text": "so we're gonna take questions after the talk and just grab us come talk to us for questions feedback if you want to",
    "start": "1864490",
    "end": "1871820"
  },
  {
    "text": "grab beers that's fine too thank you [Applause]",
    "start": "1871820",
    "end": "1881049"
  }
]