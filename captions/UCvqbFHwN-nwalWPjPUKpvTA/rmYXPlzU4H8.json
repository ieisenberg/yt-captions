[
  {
    "text": "hey thanks and thanks for being here uh in person uh seeing a lot of you uh",
    "start": "240",
    "end": "6080"
  },
  {
    "text": "after a long while very very excited face you know life is back to normal i've been too many coupons in the past",
    "start": "6080",
    "end": "13519"
  },
  {
    "text": "so i think you know after almost a two year gap right this is this is uh starting field things are coming back to",
    "start": "13519",
    "end": "19439"
  },
  {
    "text": "normal right uh a bit about myself my name is anime shing i'm the cto for watson ai and machine learning open",
    "start": "19439",
    "end": "25599"
  },
  {
    "text": "technology and you know today's session we are going to talk about you know how to",
    "start": "25599",
    "end": "30720"
  },
  {
    "text": "serve machine learning models at scale using k-serve and for those of you who haven't heard",
    "start": "30720",
    "end": "37200"
  },
  {
    "text": "of k-serve um you know i'll go into it a bit as we proceed through the talk",
    "start": "37200",
    "end": "43840"
  },
  {
    "text": "right so to begin with one of the things which we are seeing that you know around 90 of",
    "start": "43840",
    "end": "48879"
  },
  {
    "text": "the corporate ai initiatives are still struggling to move uh to production right and and not only",
    "start": "48879",
    "end": "55360"
  },
  {
    "text": "move to production even if you know uh they are moving in production there is a lot of skepticism around",
    "start": "55360",
    "end": "61920"
  },
  {
    "text": "you know are these models giving predictions which can be trusted how do we measure business kpis",
    "start": "61920",
    "end": "67680"
  },
  {
    "text": "but large part of the leg up is essentially you know in terms of taking these experiments which are happening uh",
    "start": "67680",
    "end": "73200"
  },
  {
    "text": "within you know large number of enterprises but then you know deploying them in production and i think as we can see right uh the",
    "start": "73200",
    "end": "80479"
  },
  {
    "text": "proliferation of models is everywhere right more and more we are seeing them uh actually you know",
    "start": "80479",
    "end": "86960"
  },
  {
    "text": "being used uh in for example when you are playing your next song on alexa right you are essentially you know",
    "start": "86960",
    "end": "93439"
  },
  {
    "text": "behind the scenes there is there is a model which is determining okay what is the next song you will like right and then in some cases right even in uh very",
    "start": "93439",
    "end": "100799"
  },
  {
    "text": "important situations like when example you're submitting your resume some companies have started employing right",
    "start": "100799",
    "end": "106320"
  },
  {
    "text": "ai in terms of making sure that you know your resume is getting to the right person or not so these models are making",
    "start": "106320",
    "end": "112320"
  },
  {
    "text": "you know and have started making and progressively will be making more and more life-changing decisions for us",
    "start": "112320",
    "end": "119040"
  },
  {
    "text": "but you know how hard is actually to do production grade model inference right",
    "start": "119040",
    "end": "124240"
  },
  {
    "text": "what are the things which are needed right one thing which you definitely need to consider at the very beginning is the cost",
    "start": "124240",
    "end": "129520"
  },
  {
    "text": "right models run on expensive hardware gpus tpus and you want to make sure you",
    "start": "129520",
    "end": "135440"
  },
  {
    "text": "know they're not over or under scale you want to be able to monitor their endpoints are they healthy right you",
    "start": "135440",
    "end": "141599"
  },
  {
    "text": "want to be able to roll out new versions of the model because machine learning is very experimental and while rolling out you probably want to swap and send only",
    "start": "141599",
    "end": "148480"
  },
  {
    "text": "a percentage of the traffic once they are there you want to be able to send you know inference requests using http kafka grpc",
    "start": "148480",
    "end": "156400"
  },
  {
    "text": "and then you know when we look at some of more machine learning capabilities which specific capabilities which need to be there like how do you do batch",
    "start": "156400",
    "end": "162319"
  },
  {
    "text": "predictions right how do you have a standardized data plane protocol so that if you have written clients which are talking to these models you don't have",
    "start": "162319",
    "end": "168640"
  },
  {
    "text": "to rewrite them when you're moving your models from uh azure uh to sagemaker to watson right",
    "start": "168640",
    "end": "176080"
  },
  {
    "text": "so you want to have that standardization in the industry around that and obviously you want to be able to sell multiple kind of frameworks like",
    "start": "176080",
    "end": "183040"
  },
  {
    "text": "tensorflow pytorch sexy boost right and not only that i think you know once you have deployed your models and you",
    "start": "183040",
    "end": "189519"
  },
  {
    "text": "figured out you know that they are running in production you want to be able to monitor them because models will",
    "start": "189519",
    "end": "194560"
  },
  {
    "text": "have drift right there will be anomalies you want to be able to make sure that the models are not giving bias",
    "start": "194560",
    "end": "199760"
  },
  {
    "text": "predictions you want to ensure that your models are not vulnerable to adversarial attacks so you essentially need very",
    "start": "199760",
    "end": "205680"
  },
  {
    "text": "very strong you know capabilities around monitoring and metrics and last but not least you also want to",
    "start": "205680",
    "end": "212000"
  },
  {
    "text": "be able to bring your your own pre-processes and post processes because most of the models typically take",
    "start": "212000",
    "end": "217120"
  },
  {
    "text": "tensors in you give tensors out while from an end user you're probably taking an image you are taking a text",
    "start": "217120",
    "end": "222879"
  },
  {
    "text": "you are giving back an image or a text right so you need to have pluggable pre-processes post processes",
    "start": "222879",
    "end": "229680"
  },
  {
    "text": "so k-serve essentially is a project which handles all of these problems and more right it gives you solutions and",
    "start": "229680",
    "end": "235599"
  },
  {
    "text": "answers to all of this it's a standard model inferencing platform built on top of communities for communities right and",
    "start": "235599",
    "end": "242239"
  },
  {
    "text": "standards is a key word right and definitely you know kubernetes is the ecosystem on which it is being uh built",
    "start": "242239",
    "end": "247599"
  },
  {
    "text": "upon so essentially what are the capabilities at a high level uh you know by virtue of",
    "start": "247599",
    "end": "253360"
  },
  {
    "text": "it being on kubernetes run it anywhere you're not getting any vendor lock-in standardized inference protocol that",
    "start": "253360",
    "end": "258560"
  },
  {
    "text": "means you know you can move across multiple cloud providers or even ml providers so for example at this point",
    "start": "258560",
    "end": "264240"
  },
  {
    "text": "nvidia's triton server uh pytorch torch serve server uh seldom",
    "start": "264240",
    "end": "271040"
  },
  {
    "text": "ml server all of them are actually following this protocol watson is implementing this uh kf serving v2",
    "start": "271040",
    "end": "276400"
  },
  {
    "text": "protocol a case of v2 protocol so you know there is a huge emphasis on standardization and multiple of us are",
    "start": "276400",
    "end": "281919"
  },
  {
    "text": "coming together and ensuring that is happening it has built in serverless capabilities so that means you know you can auto scale and you can scale back to",
    "start": "281919",
    "end": "288800"
  },
  {
    "text": "zero right and then obviously a lot of these pluggable metrics which you can bring metrics around bias detection",
    "start": "288800",
    "end": "294800"
  },
  {
    "text": "adversarial detection explainability drift detection anomaly detection and more advanced deployment so that you",
    "start": "294800",
    "end": "301360"
  },
  {
    "text": "know when you are rolling out new versions or doing candidate deployments a b testing pin testing you are getting these capabilities out",
    "start": "301360",
    "end": "307520"
  },
  {
    "text": "of the box using uh k server now when you look at the architecture i think the things typically you know it looks like",
    "start": "307520",
    "end": "314960"
  },
  {
    "text": "standard kubernetes architecture on the right hand side are essentially the things to focus predictor essentially by",
    "start": "314960",
    "end": "321520"
  },
  {
    "text": "virtue of its name is the component which takes in the inference request and gives you the response",
    "start": "321520",
    "end": "327759"
  },
  {
    "text": "uh transformer as i was talking about like you know models rarely uh will give i mean take inputs in the",
    "start": "327759",
    "end": "333680"
  },
  {
    "text": "form you want and give input outputs in the form you want they essentially for example will take tensors in or give",
    "start": "333680",
    "end": "339440"
  },
  {
    "text": "tensors out and you need to have capabilities to plug in your own preprocessor and post processors so",
    "start": "339440",
    "end": "344800"
  },
  {
    "text": "transformers bring that capability and last but not least uh the major piece which we call the explainer which",
    "start": "344800",
    "end": "350960"
  },
  {
    "text": "is around all the metrics in the monitoring right so it explains your model predictions not only it explains",
    "start": "350960",
    "end": "356319"
  },
  {
    "text": "it gives you all the advanced metrics around drift anomaly bias adversary robustness etc",
    "start": "356319",
    "end": "363440"
  },
  {
    "text": "so uh yeah as i said like you know you probably wouldn't have heard of k serve but you know",
    "start": "364720",
    "end": "371120"
  },
  {
    "text": "i think uh quite a few of you would have heard of cape serving right so essentially casey",
    "start": "371120",
    "end": "376240"
  },
  {
    "text": "was previously known as kf serving uh and and if you go back a bit in the past",
    "start": "376240",
    "end": "381520"
  },
  {
    "text": "it was in kubecon december 2018 uh myself my colleague tommy lee we gave a talk in",
    "start": "381520",
    "end": "387520"
  },
  {
    "text": "terms of you know how to serve models using k native and and serverless paradigm",
    "start": "387520",
    "end": "392720"
  },
  {
    "text": "at the same time bloomberg was also experimenting you know how to actually use serverless paradigms and build a",
    "start": "392720",
    "end": "398000"
  },
  {
    "text": "model serving platforms on canada and then in july of 2019 we got in google office in sunnyvale with ls",
    "start": "398000",
    "end": "404720"
  },
  {
    "text": "and and some of the folks from google you know you'll probably recognize david uh the bloomberg folks the sudden folks",
    "start": "404720",
    "end": "411039"
  },
  {
    "text": "we all got together and and brainstormed and you know started creating and putting this platform together",
    "start": "411039",
    "end": "417440"
  },
  {
    "text": "and released it in september and november of 2019 at another cube contract we actually launched it in a",
    "start": "417440",
    "end": "423520"
  },
  {
    "text": "big way where there were quite a few talks uh all you know standing only talks very",
    "start": "423520",
    "end": "428560"
  },
  {
    "text": "very packed as you can see from the picture right so uh that went out and that's uh capsule ring and that's",
    "start": "428560",
    "end": "434319"
  },
  {
    "text": "probably when the pandemic happened right probably one of the big conferences after which you know things",
    "start": "434319",
    "end": "440080"
  },
  {
    "text": "uh went into a bit of limbo so then here we are approximately two years",
    "start": "440080",
    "end": "445199"
  },
  {
    "text": "later and what has happened this right so uh kf serving is now uh k serve",
    "start": "445199",
    "end": "452560"
  },
  {
    "text": "and i think our our reasons you know it was not just you know dropping that f but",
    "start": "452560",
    "end": "458400"
  },
  {
    "text": "primarily what we started seeing that you know the project started growing a lot there were a lot of capabilities which started coming in",
    "start": "458400",
    "end": "464800"
  },
  {
    "text": "and as part of that right the repository started becoming a monolith right and and we wanted to make sure that you know",
    "start": "464800",
    "end": "470639"
  },
  {
    "text": "it can grow with the scope because as you heard there are a lot of capabilities to deploy your models in production there are capabilities to",
    "start": "470639",
    "end": "476000"
  },
  {
    "text": "monitor your models in production the metrics and and for that it made sense for this project to be its in own",
    "start": "476000",
    "end": "482240"
  },
  {
    "text": "independent github organization so by while we were doing this right we ended up renaming it as well made the",
    "start": "482240",
    "end": "487360"
  },
  {
    "text": "name shorter and sweeter and here it is it's called caser so if you summarize you know in general",
    "start": "487360",
    "end": "493520"
  },
  {
    "text": "the the story of kser december 2018 when the first you know",
    "start": "493520",
    "end": "498560"
  },
  {
    "text": "set of presentations demos concepts started uh going out at cubecon and until now in september 2021 where",
    "start": "498560",
    "end": "505680"
  },
  {
    "text": "you know the project which incubated in the queue for umbrella is now in its own independent github organization and it's",
    "start": "505680",
    "end": "511440"
  },
  {
    "text": "being called caser",
    "start": "511440",
    "end": "514560"
  },
  {
    "text": "okay so uh yes this was not our reason right to to rename it like i got",
    "start": "517279",
    "end": "522479"
  },
  {
    "text": "questions uh from quite a few folks uh but definitely you know it's not that he the word all the projects with the",
    "start": "522479",
    "end": "528880"
  },
  {
    "text": "word flow are out but yeah who knows like you know we'll start seeing projects more with quantum",
    "start": "528880",
    "end": "534320"
  },
  {
    "text": "ml quantum ai uh moving forward now in terms of the contributors right a",
    "start": "534320",
    "end": "540640"
  },
  {
    "text": "lot of the companies are contributing uh majorly obviously you know uh via ibm uh",
    "start": "540640",
    "end": "545920"
  },
  {
    "text": "bloomberg nvidia sheldon facebook we are getting contributions",
    "start": "545920",
    "end": "550959"
  },
  {
    "text": "from all across the community and that what makes it such a robust and strong project right it's not a single vendor",
    "start": "550959",
    "end": "556800"
  },
  {
    "text": "driven project the community is coming along either as users or as contributors and helping shape it",
    "start": "556800",
    "end": "563519"
  },
  {
    "text": "now when i say helping shape it you know that also is leading to the standardization because",
    "start": "563519",
    "end": "568720"
  },
  {
    "text": "you know we were very clear from the beginning that we wanted to create a standards-based uh platform model",
    "start": "568720",
    "end": "574399"
  },
  {
    "text": "serving platform right so one of the things which we are doing around standards is essentially you know what we call the caser protocol currently",
    "start": "574399",
    "end": "580959"
  },
  {
    "text": "it's case of v2 protocol uh which essentially you know a lot of these major uh providers in the industry are",
    "start": "580959",
    "end": "586399"
  },
  {
    "text": "already following for example you know as i mentioned and as you can see on the slide nvidia strite on inference server",
    "start": "586399",
    "end": "591680"
  },
  {
    "text": "uh the pytorch from facebook sellers ml server they're all implementing the v2 protocol we at",
    "start": "591680",
    "end": "597519"
  },
  {
    "text": "watson are implementing the v2 protocol and what that gives you is that you know if you're writing your clients talking",
    "start": "597519",
    "end": "602720"
  },
  {
    "text": "to this protocol you won't be worried when you're moving your models across platforms now what does the protocol actually",
    "start": "602720",
    "end": "608640"
  },
  {
    "text": "provide it has a lot of you know standardization and metrics and apis around you know to check the",
    "start": "608640",
    "end": "614000"
  },
  {
    "text": "health of the model servers to check the health of the models itself and obviously to do prediction and we also intend to evolve it you know",
    "start": "614000",
    "end": "620560"
  },
  {
    "text": "beyond just the prediction right in terms of explainability and other things",
    "start": "620560",
    "end": "625519"
  },
  {
    "text": "now talking about explainability um and metrics some of the uh projects linux foundation ai projects in trusted ai",
    "start": "625600",
    "end": "632560"
  },
  {
    "text": "space like ai explainability 360 ai fairness adversarial robustness they have been integrated into k-serve and",
    "start": "632560",
    "end": "639200"
  },
  {
    "text": "made available so if you want to build your ai using trusted and ethical means and capture",
    "start": "639200",
    "end": "644800"
  },
  {
    "text": "those metrics those capabilities are available as well and not only that right you have more advanced capabilities around outlier",
    "start": "644800",
    "end": "651600"
  },
  {
    "text": "detection adversarial detection concept drift right so there are a lot more metrics you can capture as part of this",
    "start": "651600",
    "end": "657360"
  },
  {
    "text": "platform okay so far so good right so so the",
    "start": "657360",
    "end": "663680"
  },
  {
    "text": "project has been there uh for around a year and a half a lot of people are actually using it in production right in",
    "start": "663680",
    "end": "669600"
  },
  {
    "text": "fact running at a very very large scale inside bloomberg itself as well uh but then you know we started seeing uh some",
    "start": "669600",
    "end": "675200"
  },
  {
    "text": "of the new scalability problems over the course of last year now what are these right so typically",
    "start": "675200",
    "end": "680560"
  },
  {
    "text": "when you look at how the models are packaged inside a case serve there is what we call an inference service right",
    "start": "680560",
    "end": "686959"
  },
  {
    "text": "which gets provisioned on a per part basis and that's where your model lies right so typically conceptually you can map",
    "start": "686959",
    "end": "693680"
  },
  {
    "text": "you know a model to a pod or a model to a container in that context now what happens is you know kubernetes",
    "start": "693680",
    "end": "699839"
  },
  {
    "text": "cluster as its own started imposing you know quite a bit of these limitations right there are limitations around",
    "start": "699839",
    "end": "705360"
  },
  {
    "text": "computer resources there are limitations around you know maximum ports uh the number of ife addresses",
    "start": "705360",
    "end": "710800"
  },
  {
    "text": "for example you know typically each inference service has a side car which has a overhead of around you know half a cpu or of half a gig of memory",
    "start": "710800",
    "end": "718959"
  },
  {
    "text": "and if you're having 10 models you're replicating that overhead 10 times right so then you are looking at 5 cpus and 5",
    "start": "718959",
    "end": "724560"
  },
  {
    "text": "gig of memory for 10 models in that context versus if you pack these 10 models into one single inference service",
    "start": "724560",
    "end": "731360"
  },
  {
    "text": "you can over i mean reduce that overhead a lot right so that is one of the the",
    "start": "731360",
    "end": "736800"
  },
  {
    "text": "things right which uh actually started happening a lot where we started running into these limitations around how much",
    "start": "736800",
    "end": "742240"
  },
  {
    "text": "capacity we have and how much models we need to provision the second thing is around you know some",
    "start": "742240",
    "end": "747519"
  },
  {
    "text": "of the kubernetes best practices now typically humanities recommends around",
    "start": "747519",
    "end": "753440"
  },
  {
    "text": "110 or 100 ports per node on an average so for example if you have a 50 node",
    "start": "753440",
    "end": "758800"
  },
  {
    "text": "cluster you are looking at uh around 5000 uh pods so at the most even if you have a model",
    "start": "758800",
    "end": "765200"
  },
  {
    "text": "per port at the most you can on a 50 node cluster you can provision around 5000 models which is very very less",
    "start": "765200",
    "end": "771680"
  },
  {
    "text": "now in real world use cases you're not even having one uh model per pod because your model comes with its own replica",
    "start": "771680",
    "end": "777760"
  },
  {
    "text": "then there is transformer right we talked about the preprocessor in post processor then there is the the",
    "start": "777760",
    "end": "783120"
  },
  {
    "text": "explainer right so each model on an average is around four pods right so you're looking at the most like hundred",
    "start": "783120",
    "end": "788880"
  },
  {
    "text": "thousand models in in regular scenarios right so we can see that you know we are hitting these limitations and obviously",
    "start": "788880",
    "end": "795200"
  },
  {
    "text": "you know that coupled with some of these things that you know each of these things when you are going with a per pod or a per container paradigm you",
    "start": "795200",
    "end": "801760"
  },
  {
    "text": "know these ip addresses you can assign to these replicas of models transformers right um that's also becoming a",
    "start": "801760",
    "end": "807440"
  },
  {
    "text": "bottleneck so many of these scalability issues we started are seeing at very very huge numbers and these numbers we",
    "start": "807440",
    "end": "813040"
  },
  {
    "text": "are talking you know at much larger scale so essentially you know",
    "start": "813040",
    "end": "818160"
  },
  {
    "text": "that's where model mesh comes in right and model mesh is a platform which has been running uh inside ibm watson",
    "start": "818160",
    "end": "825519"
  },
  {
    "text": "uh for quite a few years now and it is actually the backbone which serves a lot of the watson models for example",
    "start": "825519",
    "end": "832000"
  },
  {
    "text": "watson's speech to text what's an nlu which is what's in natural language understanding what's in discovery so it has been in production",
    "start": "832000",
    "end": "838800"
  },
  {
    "text": "you know for quite a few years and it's essentially the de facto model serving management layer for ibm watson",
    "start": "838800",
    "end": "844720"
  },
  {
    "text": "products right and what it actually gives is essentially a high scale high density",
    "start": "844720",
    "end": "849760"
  },
  {
    "text": "uh use case right for frequently changing model use cases so if your use cases where you have hundreds and",
    "start": "849760",
    "end": "855360"
  },
  {
    "text": "thousands of models rights probably you know small to medium-sized models and",
    "start": "855360",
    "end": "860639"
  },
  {
    "text": "you know they are being used in very random fashion this is a perfect platform where you can actually do much",
    "start": "860639",
    "end": "866000"
  },
  {
    "text": "more with much less right and it does it by doing an intelligent trade-off between you know how it responds to",
    "start": "866000",
    "end": "871600"
  },
  {
    "text": "users within you know very i would say milliseconds response time and then you know how does it actually calculate the computational footprint",
    "start": "871600",
    "end": "879920"
  },
  {
    "text": "so this is the architecture for model mesh now typically what you see here are the",
    "start": "880399",
    "end": "886720"
  },
  {
    "text": "pods pods are the ones which are essentially holding your model serving run times right so for",
    "start": "886720",
    "end": "892240"
  },
  {
    "text": "example if it is try ton server the pod is actually running the triton",
    "start": "892240",
    "end": "897279"
  },
  {
    "text": "server or it can be running a torch surf server or it can be running the ml server for cyclone and actually boost",
    "start": "897279",
    "end": "902880"
  },
  {
    "text": "models and then there is a side car called mesh right and mesh is where you know the most most of the",
    "start": "902880",
    "end": "908720"
  },
  {
    "text": "intelligence is in terms of you know how to do uh placement how to do routing how to",
    "start": "908720",
    "end": "914880"
  },
  {
    "text": "create that uh lru cache so essentially what model mesh does is you know treats",
    "start": "914880",
    "end": "919920"
  },
  {
    "text": "your whole kubernetes cluster and the ports pre-provisioned on it as a distributed lru cache least recently",
    "start": "919920",
    "end": "925760"
  },
  {
    "text": "used cache and then you know uses that to actually place these models now one of the things which is also you",
    "start": "925760",
    "end": "931519"
  },
  {
    "text": "know in this architecture as you see model mesh also brings in its own xcd",
    "start": "931519",
    "end": "937279"
  },
  {
    "text": "right and and it uses it to store a lot of the metadata around these models uh and it is not relying on kubernetes hdd",
    "start": "937279",
    "end": "944000"
  },
  {
    "text": "for uh quite a bit of these things so when we look at the model mesh github",
    "start": "944000",
    "end": "949519"
  },
  {
    "text": "repository uh there is the model mesh serving which is the controller uh kubernetes controller for admitting your",
    "start": "949519",
    "end": "955519"
  },
  {
    "text": "models reconciling them right so it has all the logic the model mesh itself which has all the logic and the",
    "start": "955519",
    "end": "960959"
  },
  {
    "text": "intelligence to place and and route requests to your models and then the runtime adapters",
    "start": "960959",
    "end": "966399"
  },
  {
    "text": "which are essentially serving many functions one for example you know they're the ones responsible for you know pulling your models from storage",
    "start": "966399",
    "end": "973600"
  },
  {
    "text": "and then presenting it to the runtime right whether it's the triton inference server from nvidia or",
    "start": "973600",
    "end": "979920"
  },
  {
    "text": "the torso from pi torch right they actually present that in the format in which the model server expects",
    "start": "979920",
    "end": "987759"
  },
  {
    "text": "so currently out of the box these are the two model servers which are supported uh from model mesh the nvidia",
    "start": "988320",
    "end": "993519"
  },
  {
    "text": "strite on infinite server and ml server uh the next which we are going to add support for is uh pytorch from",
    "start": "993519",
    "end": "999839"
  },
  {
    "text": "touchserve but there is a custom resource right which somebody can extend an ad support for their own model serving server if",
    "start": "999839",
    "end": "1006160"
  },
  {
    "text": "they need to now okay so model mush does all these",
    "start": "1006160",
    "end": "1012079"
  },
  {
    "text": "things how does it do it right so there is cash management and high availability built in right so for example as i",
    "start": "1012079",
    "end": "1017440"
  },
  {
    "text": "mentioned it treats the set of pre-provision ports on a humanities cluster as lru cache right",
    "start": "1017440",
    "end": "1024240"
  },
  {
    "text": "and then you know it decides when and where to load and unload these models right and how does it decide it actually",
    "start": "1024240",
    "end": "1029600"
  },
  {
    "text": "takes into account two factors which is the cash right the cash usage as well as the request count right how many requests",
    "start": "1029600",
    "end": "1035839"
  },
  {
    "text": "are coming versus you know which are the least recently used models right these are the two key things which go into their decision making and once",
    "start": "1035839",
    "end": "1042400"
  },
  {
    "text": "it has you know deployed your models it also acts as a router right where in terms of being able to load balance the request across them",
    "start": "1042400",
    "end": "1050400"
  },
  {
    "text": "it also has intelligent placement and chewing right so as i was saying um the the placement essentially is happening",
    "start": "1051440",
    "end": "1057280"
  },
  {
    "text": "you know using both the cache age as well as you know the request load both these parameters are used",
    "start": "1057280",
    "end": "1063120"
  },
  {
    "text": "and then it has inbuilt queuing right so if a lot of concurrent requests are coming in at one particular time it will",
    "start": "1063120",
    "end": "1068400"
  },
  {
    "text": "actually cure your request but it also gives you capabilities for priority queueing or you know being able",
    "start": "1068400",
    "end": "1073840"
  },
  {
    "text": "to jump the queue if some request is at a high priority now if you're looking at a lot of these functionalities right there is um",
    "start": "1073840",
    "end": "1081360"
  },
  {
    "text": "not only you know intelligent calculation in terms of the placement using the lru cache mechanism",
    "start": "1081360",
    "end": "1086480"
  },
  {
    "text": "there is the routing and load balancing there is uh chewing possibly if you're building a project in",
    "start": "1086480",
    "end": "1092559"
  },
  {
    "text": "kubernetes community using these functionalities you are looking at combining three or four different projects right and that's the great part",
    "start": "1092559",
    "end": "1098559"
  },
  {
    "text": "here that you know all these capabilities come into one single compact platform uh into model mesh",
    "start": "1098559",
    "end": "1103840"
  },
  {
    "text": "right and there is resiliency built in as well now so the first thing is okay you have",
    "start": "1103840",
    "end": "1109200"
  },
  {
    "text": "been deplo you have been able to deploy these models what about day two operational simplicity right how do you",
    "start": "1109200",
    "end": "1114559"
  },
  {
    "text": "actually roll out new versions of the model uh will it allow you to roll out new versions of the models right so it",
    "start": "1114559",
    "end": "1119600"
  },
  {
    "text": "actually supports you know rolling update automatically you can roll out new versions of the model it uses this",
    "start": "1119600",
    "end": "1124799"
  },
  {
    "text": "concept of you know v model endpoints which are then shifted to your new versions of the models once they",
    "start": "1124799",
    "end": "1129919"
  },
  {
    "text": "have been loaded into the cache and they're active and serving the requests",
    "start": "1129919",
    "end": "1136039"
  },
  {
    "text": "last but not least the scalability right so model mesh essentially you know can support hundreds and thousands of these",
    "start": "1138000",
    "end": "1144000"
  },
  {
    "text": "models in a single production deployment of for example eight pods yes you can have",
    "start": "1144000",
    "end": "1149360"
  },
  {
    "text": "just eight pods running and and be able to serve hundreds and thousands of models right and and we have seen this in practice in fact",
    "start": "1149360",
    "end": "1155919"
  },
  {
    "text": "you know we have been doing some uh just experimental tests uh outside of watson just to see on a single uh node cluster",
    "start": "1155919",
    "end": "1162640"
  },
  {
    "text": "right which is a community slave machine which has just a single node of 8 cpu and 64 gig of memory we can actually",
    "start": "1162640",
    "end": "1168720"
  },
  {
    "text": "deploy up to 20 000 models uh very uh in a very fast manner and get for the most",
    "start": "1168720",
    "end": "1174320"
  },
  {
    "text": "part you know single millisecond digit response time right in some cases just the response time goes into double digit",
    "start": "1174320",
    "end": "1179600"
  },
  {
    "text": "milliseconds but for the most models this is a single node cluster we are talking about",
    "start": "1179600",
    "end": "1185440"
  },
  {
    "text": "so uh as part of this what we are uh announcing one of the things which we announced at chuccon and the announcement went you know just",
    "start": "1185600",
    "end": "1192080"
  },
  {
    "text": "yesterday is essentially we are contributing model mesh from ibm watson to open source",
    "start": "1192080",
    "end": "1198480"
  },
  {
    "text": "and as part of that contribution we are actually combining it with caser project right so now you have",
    "start": "1198480",
    "end": "1204080"
  },
  {
    "text": "a single project which will provide you interfaces and apis and a standard way",
    "start": "1204080",
    "end": "1209120"
  },
  {
    "text": "to deploy in kubernetes for both a single model per container paradigm as well as multiple models per container",
    "start": "1209120",
    "end": "1214799"
  },
  {
    "text": "paradigm right so depending on your use cases and needs you can choose either",
    "start": "1214799",
    "end": "1220720"
  },
  {
    "text": "and in fact you know it's already available so we just cut the case of 0.7 release yesterday and model mesh is available as",
    "start": "1220720",
    "end": "1228240"
  },
  {
    "text": "part of it so now as part of this you have a single inference service custom resource which you can use to deploy models not only on",
    "start": "1228240",
    "end": "1235039"
  },
  {
    "text": "the traditional case or single model paradigm but also you know on the model mesh if you want to pack it with",
    "start": "1235039",
    "end": "1241120"
  },
  {
    "text": "multiple models co-residing on the same container",
    "start": "1241120",
    "end": "1245360"
  },
  {
    "text": "uh looking at some of the road map um you know some of the items which we are currently focusing",
    "start": "1247520",
    "end": "1254080"
  },
  {
    "text": "obviously we want to improve the storage support uh we are actually working on you know uh enabling multi-name space",
    "start": "1254080",
    "end": "1260240"
  },
  {
    "text": "support for model mesh but q1 and uh of next year is when we want to get all the capabilities which are available for",
    "start": "1260240",
    "end": "1266720"
  },
  {
    "text": "single models in k-serve and if you are using them available with model mesh as well which includes you know candidate",
    "start": "1266720",
    "end": "1272400"
  },
  {
    "text": "rollouts uh transformers and then bring more advanced capabilities like transformers which allows you to change multiple",
    "start": "1272400",
    "end": "1278720"
  },
  {
    "text": "models together uh so that you know the output of one model can be fed as an input to another model and you can create a chain of",
    "start": "1278720",
    "end": "1285120"
  },
  {
    "text": "these models together so all these capabilities you know what we are targeting for q1 of 2022",
    "start": "1285120",
    "end": "1291640"
  },
  {
    "text": "okay we'll probably do",
    "start": "1294000",
    "end": "1298480"
  },
  {
    "text": "a quick demo of model mesh and",
    "start": "1299360",
    "end": "1305799"
  },
  {
    "text": "then i can take some questions running good on time here so",
    "start": "1306640",
    "end": "1312799"
  },
  {
    "text": "so let me see if this is",
    "start": "1315120",
    "end": "1319480"
  },
  {
    "text": "so this is essentially you know as i mentioned the some of the experiments in the test we were doing we were essentially doing on a single node",
    "start": "1321520",
    "end": "1327760"
  },
  {
    "text": "cluster right so this is a cluster which has just 8 cpu of memory i mean 8 cpus and 64 gig of memory and that's",
    "start": "1327760",
    "end": "1333919"
  },
  {
    "text": "essentially you know the place where we are launching some of these models the other thing which you see is around you",
    "start": "1333919",
    "end": "1340320"
  },
  {
    "text": "know for example the number of parts we are running so only two parts in this case so these are",
    "start": "1340320",
    "end": "1346480"
  },
  {
    "text": "the ports right which are the tritone server from nvidia which we are running so very fairly small deployment in this",
    "start": "1346480",
    "end": "1352880"
  },
  {
    "text": "particular case right and when you look at the number of models deployed here we have already deployed around 15 000",
    "start": "1352880",
    "end": "1359200"
  },
  {
    "text": "models currently right so uh looking at the grafana dashboard from here",
    "start": "1359200",
    "end": "1365520"
  },
  {
    "text": "yeah so around 15 020 models are already loaded and managed so there is now these",
    "start": "1365840",
    "end": "1371360"
  },
  {
    "text": "numbers are same in this context but what that typically means is you know 15 000 models are registered and 15 000",
    "start": "1371360",
    "end": "1378960"
  },
  {
    "text": "models are actually available through cash now if you look at the cash graph we essentially have assigned just 2.5 gig",
    "start": "1378960",
    "end": "1385440"
  },
  {
    "text": "or 2.75 gig i think uh in the context of these two model uh servers right so",
    "start": "1385440",
    "end": "1390640"
  },
  {
    "text": "that's the amount of memory which is available to serve these models and all of these models are loaded within that particular context",
    "start": "1390640",
    "end": "1397039"
  },
  {
    "text": "now when you look at the numbers here like how many models are available on per tritone server instance the math is",
    "start": "1397039",
    "end": "1403440"
  },
  {
    "text": "more than 15 000 right because some of the models have their replicas which are deployed across both the ports",
    "start": "1403440",
    "end": "1410640"
  },
  {
    "text": "and the other thing which you see is you know you don't see any activity on the serving runtime model roading late or on",
    "start": "1410640",
    "end": "1416559"
  },
  {
    "text": "the cache message right right and there is no request it's being sent uh and the worker node cpu is you know",
    "start": "1416559",
    "end": "1423360"
  },
  {
    "text": "utilization is pretty small this is the uh you know usage of the controller where the model mesh controller is running",
    "start": "1423360",
    "end": "1430080"
  },
  {
    "text": "pretty small at this point as well and if you see the memory usage both the triton servers are checking you",
    "start": "1430080",
    "end": "1435600"
  },
  {
    "text": "know most of the memory so one thing which we will probably do at this point is",
    "start": "1435600",
    "end": "1441200"
  },
  {
    "text": "we will launch this is a q4 pipeline right so um",
    "start": "1441200",
    "end": "1446480"
  },
  {
    "text": "if you're aware of the q flow umbrella there are different projects q flow pipeline is one uh another project and",
    "start": "1446480",
    "end": "1451600"
  },
  {
    "text": "we have uh user version of q flow pipelines which we have written to run on top of tecton right so we will use to actually",
    "start": "1451600",
    "end": "1458000"
  },
  {
    "text": "deploy some additional models so 15 000 models are already deployed right one of the things other things which i wanted",
    "start": "1458000",
    "end": "1463919"
  },
  {
    "text": "to highlight right 15 000 are deployed out of which around 5000 are very simple string tensorflow models",
    "start": "1463919",
    "end": "1470400"
  },
  {
    "text": "and which you see here simple string tensorflow models and then uh i believe there are 5000 onyx models",
    "start": "1470400",
    "end": "1478400"
  },
  {
    "text": "yes so there are mnist on x models 5000 of them and then there are 5 000 of",
    "start": "1478400",
    "end": "1483520"
  },
  {
    "text": "high torch models yeah so python c4 models right so all these different kinds of varied models",
    "start": "1483520",
    "end": "1489360"
  },
  {
    "text": "are deployed uh total 15000 on this cluster and i'm going to deploy",
    "start": "1489360",
    "end": "1496240"
  },
  {
    "text": "a few more right now typically you know when i'm running these tests i run like typically deploy",
    "start": "1496240",
    "end": "1502240"
  },
  {
    "text": "2 000 3000 all at once here because we want to finish this in in time right i will just push 10 more",
    "start": "1502240",
    "end": "1509120"
  },
  {
    "text": "at this particular point in time but what i will do is you know uh like psn payload after deploying those 10 more",
    "start": "1509120",
    "end": "1515120"
  },
  {
    "text": "models i will send payload to around thousand of the models in together right so 15 000 models are already deployed",
    "start": "1515120",
    "end": "1522000"
  },
  {
    "text": "i'm going to deploy 10 more and send payload requests 2 000 more models and probably send it for",
    "start": "1522000",
    "end": "1528400"
  },
  {
    "text": "let's say 90 seconds okay so i launched this pipeline",
    "start": "1528400",
    "end": "1537200"
  },
  {
    "text": "so i think again if you're familiar with shoe floor pipelines right this is hopefully not net new to you",
    "start": "1539360",
    "end": "1545520"
  },
  {
    "text": "right so this is a q flow pipeline which runs in components this is the first component which is deploying",
    "start": "1545520",
    "end": "1551840"
  },
  {
    "text": "so it will start streaming the logs you know fairly uh it is going to deploy these models onto that cluster",
    "start": "1552080",
    "end": "1558480"
  },
  {
    "text": "yes and now one of the things we will see is when we go on the grafana dashboard right to see",
    "start": "1558480",
    "end": "1565120"
  },
  {
    "text": "let me we'll start seeing you know as as this",
    "start": "1565120",
    "end": "1570159"
  },
  {
    "text": "rolls out right it actually goes ahead and starts deploying these models you'll see you know some of the activity on some of",
    "start": "1570159",
    "end": "1576320"
  },
  {
    "text": "these graphs tracing right so essentially the cpu percentage for example you know",
    "start": "1576320",
    "end": "1583200"
  },
  {
    "text": "on the uh controller uh is is getting increased now the number of loaded models as you",
    "start": "1583200",
    "end": "1589039"
  },
  {
    "text": "can see is increasing as well right the managed and loaded models and then what you see is you know some",
    "start": "1589039",
    "end": "1594720"
  },
  {
    "text": "activity here in terms of the loaded models right so it's trying to now load new models because the second uh you",
    "start": "1594720",
    "end": "1600159"
  },
  {
    "text": "know those 10 models are being deployed the second thing which is happening is after those models are deployed we are",
    "start": "1600159",
    "end": "1606799"
  },
  {
    "text": "sending payloads right so i talked about we are going to send queries to around uh",
    "start": "1606799",
    "end": "1612880"
  },
  {
    "text": "1000 of these models and we'll see so as you can see now these graphs and dials are showing right so",
    "start": "1612880",
    "end": "1619440"
  },
  {
    "text": "so this is actually you know serving at this point if you see around 200 queries uh",
    "start": "1619440",
    "end": "1624480"
  },
  {
    "text": "or 200 inference requests which are coming now if you see the request latency it is all in single digits right",
    "start": "1624480",
    "end": "1631039"
  },
  {
    "text": "this is all showing four milliseconds four milliseconds right so uh for the number of requests which we are saying around thousand concurrent requests",
    "start": "1631039",
    "end": "1638000"
  },
  {
    "text": "for a couple of minutes you know we are seeing foliage uh a single digit millisecond digit response time",
    "start": "1638000",
    "end": "1643679"
  },
  {
    "text": "and this essentially you know is is uh essentially just showing you know how much each of the triton inference",
    "start": "1643679",
    "end": "1648960"
  },
  {
    "text": "servers how many concurrent requests they are handling at that particular point in time right so hopefully uh this gives you",
    "start": "1648960",
    "end": "1655039"
  },
  {
    "text": "some of the capabilities of model mesh now there are other things which as i was describing which i wanted to show",
    "start": "1655039",
    "end": "1661840"
  },
  {
    "text": "which are typically edge case scenarios right uh let's see if we can replicate it for example one of the things which",
    "start": "1661840",
    "end": "1667520"
  },
  {
    "text": "you don't see is model unloads and the reason model unloads are not happening is because you",
    "start": "1667520",
    "end": "1672559"
  },
  {
    "text": "know our cache is still not full so if you see like for the total cache capacity we have applied to model mesh to the lru cache",
    "start": "1672559",
    "end": "1680640"
  },
  {
    "text": "it's still running at this particular level right so hence it doesn't need to push out some of the models otherwise",
    "start": "1680640",
    "end": "1685840"
  },
  {
    "text": "what will happen is you know when you're pushing new models it will start pushing out some of the least recently used",
    "start": "1685840",
    "end": "1691679"
  },
  {
    "text": "models and that's hence the name you know lru cache the models which haven't been used of late you know start pushing",
    "start": "1691679",
    "end": "1697520"
  },
  {
    "text": "out and you'll start seeing activity on this model unloading rate the other thing which might happen is also you know the cache miss rate which is",
    "start": "1697520",
    "end": "1703360"
  },
  {
    "text": "essentially if you send inference requests for models which are not available in the cache you know you'll",
    "start": "1703360",
    "end": "1708559"
  },
  {
    "text": "start seeing that the cache mistress as well so let me launch",
    "start": "1708559",
    "end": "1713600"
  },
  {
    "text": "one more pipeline just to see if you can hit you know any of the models which are not in the cache",
    "start": "1713679",
    "end": "1720480"
  },
  {
    "text": "this one just ran and it completed this pipeline without completed so as you can see more than 50 of them models caught",
    "start": "1722880",
    "end": "1729760"
  },
  {
    "text": "around four millisecond response time uh for you know 95 uh",
    "start": "1729760",
    "end": "1735120"
  },
  {
    "text": "more than 95 percent right there is 24 millisecond response time for this particular use case so",
    "start": "1735120",
    "end": "1740880"
  },
  {
    "text": "fairly good right for the number of models which we have in that cluster which is now running at 15 000 plus",
    "start": "1740880",
    "end": "1746640"
  },
  {
    "text": "okay um here we will try and send payload to all",
    "start": "1746640",
    "end": "1751679"
  },
  {
    "text": "the models right so i'll choose an experiment i will run it in my",
    "start": "1751679",
    "end": "1757679"
  },
  {
    "text": "payload duration in second let me make it two minutes payload queries per second",
    "start": "1759039",
    "end": "1765440"
  },
  {
    "text": "let me make it 50. and we are sending you know payload",
    "start": "1765440",
    "end": "1771440"
  },
  {
    "text": "requests to all kinds of models like the tensorflow simple string model the python c4 model and the onyx mnist model",
    "start": "1771440",
    "end": "1777520"
  },
  {
    "text": "right and i start this particular pipeline",
    "start": "1777520",
    "end": "1781840"
  },
  {
    "text": "again as soon as it starts executing you'll see you know it will start pulling out some",
    "start": "1784799",
    "end": "1789840"
  },
  {
    "text": "of the logs okay it started creating the request and",
    "start": "1789840",
    "end": "1795679"
  },
  {
    "text": "hopefully you know we'll see some of these requests by uh spiking up and maybe if it starts hitting some of",
    "start": "1795679",
    "end": "1801279"
  },
  {
    "text": "these models which are not available in cache we can see you know some of these cache miss rates uh available here as",
    "start": "1801279",
    "end": "1806880"
  },
  {
    "text": "well okay and while this is going through um",
    "start": "1806880",
    "end": "1813360"
  },
  {
    "text": "i think i can take certain questions so there are any questions",
    "start": "1813360",
    "end": "1818480"
  },
  {
    "text": "folks have please bring it up",
    "start": "1818480",
    "end": "1823000"
  },
  {
    "text": "yeah do you want to repeat it or uh i think uh there are people watching online as well so it will be good for",
    "start": "1848840",
    "end": "1854640"
  },
  {
    "text": "their benefit right okay sure sure yeah so my question is uh",
    "start": "1854640",
    "end": "1860799"
  },
  {
    "text": "somewhere you mentioned that you running around 20 000 model on an 8 cpu machine",
    "start": "1860799",
    "end": "1866640"
  },
  {
    "text": "so i'm trying to understand the use case because in this case you kind of running 0.0004",
    "start": "1866640",
    "end": "1872480"
  },
  {
    "text": "cpu for each model that means most of the time models are sitting idle right",
    "start": "1872480",
    "end": "1878159"
  },
  {
    "text": "so in what scenario you would like to have this if you're talking about this",
    "start": "1878159",
    "end": "1884559"
  },
  {
    "text": "like why you want to load up so many models in so much less cpu or smaller",
    "start": "1884559",
    "end": "1889760"
  },
  {
    "text": "footprint yes like so yeah if you say like you know so how does how does model mesh",
    "start": "1889760",
    "end": "1895760"
  },
  {
    "text": "handle and and provide you so much scalability is essentially you know by over committing the aggregate available resources",
    "start": "1895760",
    "end": "1901840"
  },
  {
    "text": "right now so we have been um running this in production for many years right and and uh a lot of the models like watson",
    "start": "1901840",
    "end": "1908559"
  },
  {
    "text": "speech to text nlu etc which are running in public cloud scale more often than not what you see is you",
    "start": "1908559",
    "end": "1914080"
  },
  {
    "text": "know uh when you have these hundred thousands of models right not everything is getting used all at once all right uh",
    "start": "1914080",
    "end": "1920159"
  },
  {
    "text": "you're getting based on different scenarios based on different use cases based on a particular time of the year a particular",
    "start": "1920159",
    "end": "1927440"
  },
  {
    "text": "set of models are being hit heavily a particular set of models are you know not being heavily so this kind of um",
    "start": "1927440",
    "end": "1933760"
  },
  {
    "text": "scenario works very well when you have these hundreds and thousands of models small to medium size right which you",
    "start": "1933760",
    "end": "1939679"
  },
  {
    "text": "need to run and deploy and typically think of them as being registered but they don't need to be active 100",
    "start": "1939679",
    "end": "1945600"
  },
  {
    "text": "throughout the year right they are getting these requests right so that's where it shines now",
    "start": "1945600",
    "end": "1951679"
  },
  {
    "text": "you know so so how it actually does is essentially you know the cache the lru cache is essentially it has a list of",
    "start": "1951679",
    "end": "1957519"
  },
  {
    "text": "the models which have been deployed uh recently and how often they are being used the ones which are being used least",
    "start": "1957519",
    "end": "1963600"
  },
  {
    "text": "recently although used was they are the ones you know being moved out from cache progressively and the new ones are",
    "start": "1963600",
    "end": "1968640"
  },
  {
    "text": "getting loaded as they are being used right uh the other kind of scenarios where you're seeing for example multi-armed bandits",
    "start": "1968640",
    "end": "1974559"
  },
  {
    "text": "right so if you if you see that's one of the mechanisms to deploy a large number of the models uh where you don't want",
    "start": "1974559",
    "end": "1981760"
  },
  {
    "text": "your one big monolith model to handle all kind of requests for example if you're doing uh speech to text translation i mean",
    "start": "1981760",
    "end": "1988880"
  },
  {
    "text": "sometimes the speech files you get is like gigabits and gigabytes in size and sometimes you get",
    "start": "1988880",
    "end": "1994480"
  },
  {
    "text": "kb's or mbs right and if you're loading up that huge monolith model for everything even if you're getting",
    "start": "1994480",
    "end": "1999919"
  },
  {
    "text": "kilobytes of speech or megabytes of speech you're not doing justice to the underlying infrastructure you probably",
    "start": "1999919",
    "end": "2005360"
  },
  {
    "text": "want to encase a multi-on bandwidth you have different models which are handling based on different sizes for example in",
    "start": "2005360",
    "end": "2010480"
  },
  {
    "text": "that case if this amount of speech is very less the model which wants to handle is you know has",
    "start": "2010480",
    "end": "2016080"
  },
  {
    "text": "much lesser features and much lesser capability sometimes you know you need to be able to translate to 20 languages",
    "start": "2016080",
    "end": "2021360"
  },
  {
    "text": "sometimes the need is to translate to only one language so again you know even though the functionality is same you",
    "start": "2021360",
    "end": "2026799"
  },
  {
    "text": "don't want to be loading a model which translates to 20 languages for that simple request which just needs a translation in one language",
    "start": "2026799",
    "end": "2033440"
  },
  {
    "text": "right so in a lot of these cases because you are employing these multi-um bandwidth kind of scenarios you have that need where multiple variations of",
    "start": "2033440",
    "end": "2040240"
  },
  {
    "text": "these models are running and ready and registered to handle depending on the kind of request which is coming in",
    "start": "2040240",
    "end": "2047279"
  },
  {
    "text": "got it sorry just one more question so at in one of the diagram you are showing",
    "start": "2047279",
    "end": "2053358"
  },
  {
    "text": "where you show the architecture so in multi-modal case we are loading",
    "start": "2053359",
    "end": "2059440"
  },
  {
    "text": "multiple model in a single part right multiple container single board",
    "start": "2059440",
    "end": "2065118"
  },
  {
    "text": "so from kubernetes aspect is this fixed what i mean to say like you running four containers in one one",
    "start": "2065119",
    "end": "2072800"
  },
  {
    "text": "pod and if you deploy let's say 100 more models",
    "start": "2072800",
    "end": "2078480"
  },
  {
    "text": "is it going to dynamically insert containers in the port or it will how it gets readjusted",
    "start": "2078480",
    "end": "2085440"
  },
  {
    "text": "do you get my question no if you can repeat this once more so so when you're running multiple",
    "start": "2085440",
    "end": "2091679"
  },
  {
    "text": "containers in a single part right let's say uh 10 containers per pot and when i'm saying container that's one",
    "start": "2091679",
    "end": "2097760"
  },
  {
    "text": "model each right you have let's say a thousand model already deployed now you're adding",
    "start": "2097760",
    "end": "2103520"
  },
  {
    "text": "hundred more models so how do you rebalance those hundred model to let's say four nodes multiple",
    "start": "2103520",
    "end": "2110800"
  },
  {
    "text": "parts right so do do they get inserted to existing pod as a new container or",
    "start": "2110800",
    "end": "2117119"
  },
  {
    "text": "you will be deploying new parts for that and rebalance uh rest of the thousand",
    "start": "2117119",
    "end": "2122640"
  },
  {
    "text": "containers so uh there are two modes like so one is like the original k serve mode which is essentially you know",
    "start": "2122640",
    "end": "2128800"
  },
  {
    "text": "models are mapped to pods right so if you're deploying new models they are essentially new pods",
    "start": "2128800",
    "end": "2134960"
  },
  {
    "text": "in that context that answer your questions right so in in context of the k serve which is you know single model",
    "start": "2134960",
    "end": "2141359"
  },
  {
    "text": "per container paradigm whenever you're deploying a new model you know they're getting into new parts in context of model mesh you have",
    "start": "2141359",
    "end": "2147520"
  },
  {
    "text": "multiple models per pod right so essentially in that case the weight is actually",
    "start": "2147520",
    "end": "2153040"
  },
  {
    "text": "you know doing that calculation it's using two metrics right which is the number of requests which are coming for",
    "start": "2153040",
    "end": "2158240"
  },
  {
    "text": "a particular kind of model plus uh the cache age on a particular port right so if a model is being used",
    "start": "2158240",
    "end": "2164320"
  },
  {
    "text": "heavily it will probably place it on a pod you know which is not overloaded right in that context but here it's not",
    "start": "2164320",
    "end": "2171200"
  },
  {
    "text": "provisioning a container per model right in context of moral mesh right and that is how we are actually getting past the",
    "start": "2171200",
    "end": "2176320"
  },
  {
    "text": "community's limitation because the single model per container paradigm if you have to scale you are either using",
    "start": "2176320",
    "end": "2181839"
  },
  {
    "text": "kpa from k native right which is request based scaling or you know you're using horizontal port scaling but in both",
    "start": "2181839",
    "end": "2187920"
  },
  {
    "text": "these cases you essentially have to scale the number of containers as the number of requests increase right that's",
    "start": "2187920",
    "end": "2193040"
  },
  {
    "text": "the original case of paradigm in context of model mesh you don't have to do that the number of pods currently you know",
    "start": "2193040",
    "end": "2198640"
  },
  {
    "text": "when you just create that distributed lru cache the number of pods are static right that is your starting",
    "start": "2198640",
    "end": "2204800"
  },
  {
    "text": "cluster capacity and when the requests are coming then it's using that capability how much cache",
    "start": "2204800",
    "end": "2209920"
  },
  {
    "text": "space i have which are the pods where the models are loaded which are not loaded which are the ones which have been used in you",
    "start": "2209920",
    "end": "2216560"
  },
  {
    "text": "know which are the least recently used which ones i can kick them out now if that",
    "start": "2216560",
    "end": "2221760"
  },
  {
    "text": "lru cache gets filled up it's total at a capacity then you can you know employ more",
    "start": "2221760",
    "end": "2228000"
  },
  {
    "text": "advanced techniques like kpa and hpa based auto scaling right which is now provisioning new parts right but that's",
    "start": "2228000",
    "end": "2233440"
  },
  {
    "text": "the secondary mechanism here right for the most part as i mentioned in this case like you know on on eight node",
    "start": "2233440",
    "end": "2238960"
  },
  {
    "text": "cluster we are able to run hundreds and thousands of these models right so majority of the use cases you won't even",
    "start": "2238960",
    "end": "2244480"
  },
  {
    "text": "need to hit kp or hpa if you have done the right sizing thank you",
    "start": "2244480",
    "end": "2252359"
  },
  {
    "text": "good question right so okay before i answer your question right so one of the things which we can see like when we ran",
    "start": "2266160",
    "end": "2271520"
  },
  {
    "text": "the last one right so this one showed no cash miss now a cash mess happens because you see",
    "start": "2271520",
    "end": "2277520"
  },
  {
    "text": "why it happened record like the cash serving run time cash usage it actually hit its capacity at some point a cache",
    "start": "2277520",
    "end": "2283359"
  },
  {
    "text": "miss happens when a request comes in for a model and the model is not there loaded in the cache so that's when it",
    "start": "2283359",
    "end": "2288480"
  },
  {
    "text": "all loads new model now when it loads new model and there is not enough space in the cache because it actually hit the",
    "start": "2288480",
    "end": "2293599"
  },
  {
    "text": "capacity it unloads for you so as you can see now the unloading rate is there so it has been able to push out some of",
    "start": "2293599",
    "end": "2299680"
  },
  {
    "text": "the models from the cache and load new models right and and that happened because you know there was a cache miss",
    "start": "2299680",
    "end": "2304960"
  },
  {
    "text": "okay now to answer your question are we changing the architecture from k native so",
    "start": "2304960",
    "end": "2310400"
  },
  {
    "text": "i think in general with k-serve the the vision of the project is to actually you",
    "start": "2310400",
    "end": "2315599"
  },
  {
    "text": "know do more with less and when we say do more with less we essentially want to",
    "start": "2315599",
    "end": "2320880"
  },
  {
    "text": "leverage k native where it shines otherwise you know if we can take the",
    "start": "2320880",
    "end": "2326560"
  },
  {
    "text": "native kubernetes capabilities and provide all these capabilities on top of it we essentially want to do that right",
    "start": "2326560",
    "end": "2332000"
  },
  {
    "text": "so now there is a raw mode available even if you don't use model mesh let's say you just have go to kserv there is a",
    "start": "2332000",
    "end": "2338640"
  },
  {
    "text": "case of raw mode which gives you the basic capabilities now you can deploy it on communities natively you don't need k",
    "start": "2338640",
    "end": "2344079"
  },
  {
    "text": "native right but you know native uh is available and all the full-blown capabilities of",
    "start": "2344079",
    "end": "2349760"
  },
  {
    "text": "k-serve are only available with the canadian version like when we are talking about transformers when we are talking about explainers all of those",
    "start": "2349760",
    "end": "2355920"
  },
  {
    "text": "advanced capabilities the canady rollouts they are only available with the k native version but now we are moving forward and making sure you know",
    "start": "2355920",
    "end": "2362240"
  },
  {
    "text": "most more and more of these capabilities we translate directly to communities as well so both the modes will be available",
    "start": "2362240",
    "end": "2367599"
  },
  {
    "text": "um there are cases where you would want to have the knitting model right for example if you have very heavy duty bert based",
    "start": "2367599",
    "end": "2374960"
  },
  {
    "text": "models which probably are gigabytes in size so you still want to run them on a per container basis but you don't want",
    "start": "2374960",
    "end": "2380640"
  },
  {
    "text": "those containers to be active when there are no requests coming in right so that's where something like kennedy signs in",
    "start": "2380640",
    "end": "2386000"
  },
  {
    "text": "so um yeah so long answer short we are making changes but the canadian",
    "start": "2386000",
    "end": "2391599"
  },
  {
    "text": "paradigm remains that's essentially what is also running in production at bloomberg and quite a few of the users right but we are bringing more and more",
    "start": "2391599",
    "end": "2398320"
  },
  {
    "text": "of these capabilities just on raw communities directly",
    "start": "2398320",
    "end": "2402960"
  },
  {
    "text": "same okay thank you and and thanks for being here guys hopefully you enjoyed the tour thank you",
    "start": "2403359",
    "end": "2411838"
  }
]