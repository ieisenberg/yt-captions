[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "Claire get started it's definitely the year of",
    "start": "110",
    "end": "6580"
  },
  {
    "text": "huh",
    "start": "6580",
    "end": "8670"
  },
  {
    "text": "little bit about me so I'm the tech lead on in for a compute team at lyft and",
    "start": "15700",
    "end": "20920"
  },
  {
    "text": "I've spent the last few years there working on kubernetes and trying to make it a real thing that takes over the",
    "start": "20920",
    "end": "27369"
  },
  {
    "text": "entirety of the stack so quick agenda in",
    "start": "27369",
    "end": "34210"
  },
  {
    "text": "terms of what I'm going to cover first is sort of an overview of a lift and our scale and what that looks like then I'm",
    "start": "34210",
    "end": "41320"
  },
  {
    "text": "gonna talk about some of the network fundamentals of kubernetes then I'll do a deep dive into our stack I'll talk",
    "start": "41320",
    "end": "48160"
  },
  {
    "text": "about how we use it with envoy and then I'll cover some notes about future work that we have planned okay so in terms of",
    "start": "48160",
    "end": "59590"
  },
  {
    "start": "58000",
    "end": "58000"
  },
  {
    "text": "lift scale so we have millions of riders per day more than 30 million riders more",
    "start": "59590",
    "end": "66549"
  },
  {
    "text": "than 2 million drivers and we're available in all 50 US states Toronto and Ottawa and hopefully you took a lift",
    "start": "66549",
    "end": "72820"
  },
  {
    "text": "from the airport and/or took a lift to get to coop gun and there's requests",
    "start": "72820",
    "end": "77920"
  },
  {
    "text": "definitely went through kubernetes as part of as part of getting here so in",
    "start": "77920",
    "end": "84789"
  },
  {
    "text": "terms of what the scale of criminais looks like at lift today it's sort of divided into three main areas for the",
    "start": "84789",
    "end": "92649"
  },
  {
    "text": "section of a company that I'm in so we have our machine running workloads they're whole bunch of GPUs they're lots",
    "start": "92649",
    "end": "101259"
  },
  {
    "text": "of training jobs that we're on Jupiter notebook support in terms of scale it's",
    "start": "101259",
    "end": "107020"
  },
  {
    "text": "about 5,000 plus pods and 10,000 cores are dedicated to that effort flight I",
    "start": "107020",
    "end": "113170"
  },
  {
    "text": "won't go into too much detail on flight they're a whole bunch of other talks at coop con on flight so I encourage you to go see those but that's our distributed",
    "start": "113170",
    "end": "121420"
  },
  {
    "text": "workflow orchestration system and that's kind of the inverse the machine learning in terms of scale it's flip-flopping the",
    "start": "121420",
    "end": "127359"
  },
  {
    "text": "number of pods versus number of cores they didn't run what's inside cards on their side and then the cluster is that",
    "start": "127359",
    "end": "133870"
  },
  {
    "text": "my team I spend most the time focused on",
    "start": "133870",
    "end": "138030"
  },
  {
    "text": "and that's made up of plus stateless microservices all of those so we have our attendant clusters",
    "start": "139420",
    "end": "146310"
  },
  {
    "text": "that exist on a per availability own basis I'll talk about that more in detail and all those are part of a",
    "start": "146310",
    "end": "152580"
  },
  {
    "text": "single production envoy mesh and in terms of like the the scale in terms the",
    "start": "152580",
    "end": "157590"
  },
  {
    "text": "numbers we use HPA's aggressively so the number of pods that we have and the",
    "start": "157590",
    "end": "163170"
  },
  {
    "text": "number of ec2 instances that were running greatly varies based on time of the day and and load and we run a whole",
    "start": "163170",
    "end": "170760"
  },
  {
    "text": "bunch of sidecars this talk does get into it but a lot of the work that we've",
    "start": "170760",
    "end": "175890"
  },
  {
    "text": "done has been related to sidecar ordering and making sure that like what what a lyft service developer expects to",
    "start": "175890",
    "end": "182640"
  },
  {
    "text": "be the case like the same from legacy stack holds true for the communities world it's about 1,000 containers plus",
    "start": "182640",
    "end": "188640"
  },
  {
    "text": "and 3,000 plus course for that environment in terms of the timeline so",
    "start": "188640",
    "end": "198470"
  },
  {
    "start": "193000",
    "end": "193000"
  },
  {
    "text": "in December of 2015 we started internal docker based effort at the time to sort",
    "start": "198470",
    "end": "206099"
  },
  {
    "text": "of create a container based system for handling our development and continuous integration stacks but all of our",
    "start": "206099",
    "end": "213410"
  },
  {
    "text": "production side was basically stateless services running on auto scaling groups",
    "start": "213410",
    "end": "220019"
  },
  {
    "text": "and just raw ec2 instances so it's about May of 2017 when you started",
    "start": "220019",
    "end": "225750"
  },
  {
    "text": "investigating options for running kubernetes label us and at least at the time we didn't feel that any of the available options were can necessarily",
    "start": "225750",
    "end": "234000"
  },
  {
    "text": "like work at the scale that we needed for them to and we're we're too complicated and so that was when we kind",
    "start": "234000",
    "end": "239880"
  },
  {
    "text": "of went back to first principles and tried to figure out the most performant and the best method for mapping abus VPC",
    "start": "239880",
    "end": "248489"
  },
  {
    "text": "constructs on to kubernetes and the scene i stacked specifically so we did a",
    "start": "248489",
    "end": "254280"
  },
  {
    "text": "whole bunch of work and then in December 2017 is when we open sourced that stack",
    "start": "254280",
    "end": "260539"
  },
  {
    "text": "it just so happens that the ATS stack they were doing their development at the same time we were and so both of us",
    "start": "260539",
    "end": "267030"
  },
  {
    "text": "release our stacks at roughly the same time and then we spent most of 2018",
    "start": "267030",
    "end": "272580"
  },
  {
    "text": "getting all of our batch and Amell workloads migrated over to communities in addition to sort of doing",
    "start": "272580",
    "end": "279350"
  },
  {
    "text": "the plumbing work that we needed to do for the the stateless services and inside cars there's a lot of stuff that happened as part of needing to get",
    "start": "279350",
    "end": "286280"
  },
  {
    "text": "stateless there and so this year has been focused mostly on migrating services or the stateless services that",
    "start": "286280",
    "end": "293300"
  },
  {
    "text": "are the core of the platform overt if you were Nettie's and that does include tier 0 services so the tier 0 services",
    "start": "293300",
    "end": "299540"
  },
  {
    "text": "are the ones if that service is down then lifts itself is hard down so in",
    "start": "299540",
    "end": "309860"
  },
  {
    "start": "308000",
    "end": "308000"
  },
  {
    "text": "terms of what our production environment looks like today more on communities 114 the plan is to",
    "start": "309860",
    "end": "316010"
  },
  {
    "text": "move to 116 by the end of the year that works starting this week probably in terms of",
    "start": "316010",
    "end": "321950"
  },
  {
    "text": "the operating system that we use we tend to hold back one release from the current production Fedora release",
    "start": "321950",
    "end": "328840"
  },
  {
    "text": "minimal small OS we use cryo as our container runtime environment so it",
    "start": "328840",
    "end": "336470"
  },
  {
    "text": "gives us like a handful of good things in the sense of like we have kernels that are very close to mainline lots of",
    "start": "336470",
    "end": "342890"
  },
  {
    "text": "bug fixes the handle container as well and we use system D to handle all the",
    "start": "342890",
    "end": "348050"
  },
  {
    "text": "secret management so on the flip side of that is that most of the internal",
    "start": "348050",
    "end": "353570"
  },
  {
    "text": "developers that lift don't know that we run Fedora as sort of like the bootstrap OS because they just use a bun to and",
    "start": "353570",
    "end": "360590"
  },
  {
    "text": "sims 2 that is based on Bionic all of our infrastructure is immutable in the",
    "start": "360590",
    "end": "365780"
  },
  {
    "text": "community side we build a Mize from Packer we use terraform orchestration we try very much and there's other talks at",
    "start": "365780",
    "end": "372620"
  },
  {
    "text": "cube con as well that sort of talk about how we how we roll the clusters and how we maintain that that state yeah so well",
    "start": "372620",
    "end": "379580"
  },
  {
    "text": "it's entirely an AWS our largest build-out is us East one we also have some clusters over in u.s. West too as",
    "start": "379580",
    "end": "386120"
  },
  {
    "text": "well as I mentioned before we have crazy",
    "start": "386120",
    "end": "391130"
  },
  {
    "text": "clusters we have separate clusters for staging production this lets us do staggered rollouts so we can have a very",
    "start": "391130",
    "end": "398450"
  },
  {
    "text": "well-defined process so if we end up taking a change to say a staging cluster you know it's likely that we",
    "start": "398450",
    "end": "406100"
  },
  {
    "text": "only packed one staging cluster is opposed to all staging clusters and then in the case of production as well it's not likely that",
    "start": "406100",
    "end": "412279"
  },
  {
    "text": "we have a chain change rollout that affects all abroad and then of course on",
    "start": "412279",
    "end": "417590"
  },
  {
    "text": "top of all this we run our stack which I'll talk about but yes we PC native low latency high throughput and most",
    "start": "417590",
    "end": "424550"
  },
  {
    "text": "importantly pods are directly part of the Envoy mesh okay so covering some",
    "start": "424550",
    "end": "432379"
  },
  {
    "text": "brief like networking 101 for communities and these are the",
    "start": "432379",
    "end": "437689"
  },
  {
    "start": "434000",
    "end": "434000"
  },
  {
    "text": "requirements that make it in particularly pretty difficult to map",
    "start": "437689",
    "end": "443870"
  },
  {
    "text": "committees constructs on to how the ats-v PC was originally created I'm not",
    "start": "443870",
    "end": "451460"
  },
  {
    "text": "sure the year of that date but it's been around for a long time it's very stable",
    "start": "451460",
    "end": "456580"
  },
  {
    "text": "so we have one IP per pod nodes have to support at least 110 pods so that's like",
    "start": "456580",
    "end": "462439"
  },
  {
    "text": "the minimum of the the maximum they can do more if they want to but they have to support at least hundred ten which by",
    "start": "462439",
    "end": "467810"
  },
  {
    "text": "that definition it means that we have to have at least 110 IPS per per kubernetes node and so then they're a whole bunch",
    "start": "467810",
    "end": "474349"
  },
  {
    "text": "of like rules related to not about how you basically can't have it so containers have talked to other",
    "start": "474349",
    "end": "480409"
  },
  {
    "text": "containers off pointer Nats nodes can talk to containers etc and then another",
    "start": "480409",
    "end": "486169"
  },
  {
    "text": "another point which is also fairly tricky depending on like how your network is set up is that the IP that a",
    "start": "486169",
    "end": "492409"
  },
  {
    "text": "pod sees itself as has to be the same IP that everyone else sees it as so you",
    "start": "492409",
    "end": "498319"
  },
  {
    "text": "can't have effectively like two identities for the same pod ok so in",
    "start": "498319",
    "end": "504949"
  },
  {
    "start": "503000",
    "end": "503000"
  },
  {
    "text": "terms of sort of the network options and this is sort of almost time-based as well in terms of how CNI plugins sort of",
    "start": "504949",
    "end": "511729"
  },
  {
    "text": "developed and what was available the very most like simplest very straightforward which is what I think a",
    "start": "511729",
    "end": "518240"
  },
  {
    "text": "lot of people used early on it's very high performant is you just take a slash",
    "start": "518240",
    "end": "523279"
  },
  {
    "text": "24 per node and you just pull it down to the ec2 instance so you think we have",
    "start": "523279",
    "end": "529010"
  },
  {
    "text": "around 285 IPS and you get a max of fifth-year out some talk about that a",
    "start": "529010",
    "end": "534350"
  },
  {
    "text": "little bit or default max of 50 routes and then the overlay network space is where most United plugins I'd say",
    "start": "534350",
    "end": "541130"
  },
  {
    "text": "operate or miss the ones are used in production and that sort of abstracts away or tries to abstract away a lot of the underlying network",
    "start": "541130",
    "end": "548720"
  },
  {
    "text": "semantics and sort of lets you operate at a higher level and then you have your",
    "start": "548720",
    "end": "553850"
  },
  {
    "text": "VC VPC native networks of which the little stack is one of those okay so",
    "start": "553850",
    "end": "559850"
  },
  {
    "text": "I'll go into each of these briefly so we have a slash paper per ECT note as one option so previously the limit on this",
    "start": "559850",
    "end": "568070"
  },
  {
    "text": "you could only have 100 routes per VPC",
    "start": "568070",
    "end": "573709"
  },
  {
    "text": "back in 2017 we first start looking at all this which meant that roughly you had on the order of 100 node cluster was",
    "start": "573709",
    "end": "579740"
  },
  {
    "text": "the most you could get out of out of EPC since then abs has increased limits added an order of magnitude so now you",
    "start": "579740",
    "end": "587330"
  },
  {
    "text": "can have up to a thousand route so you can have a much larger cluster if this is your like if this is your method of",
    "start": "587330",
    "end": "594320"
  },
  {
    "text": "getting IPs down to an ECT node for Communities to use okay so the sort of",
    "start": "594320",
    "end": "604640"
  },
  {
    "start": "601000",
    "end": "601000"
  },
  {
    "text": "like the the next set of of options involve early networks these tend to be clouded Gnostic you tend not have once",
    "start": "604640",
    "end": "612080"
  },
  {
    "text": "on your cluster size and you get just like all these complexities you have IP",
    "start": "612080",
    "end": "617660"
  },
  {
    "text": "and IP all then you might be managing BGP and effect",
    "start": "617660",
    "end": "624070"
  },
  {
    "text": "you turn on this mic and held yes good people hear me okay okay sorry about",
    "start": "648820",
    "end": "655570"
  },
  {
    "text": "that um okay caught a gnostic new one it's one cluster sighs okay so effectively like AWS itself and the V",
    "start": "655570",
    "end": "661480"
  },
  {
    "text": "PC is a software-defined network that has a very it's like it's a very good",
    "start": "661480",
    "end": "666790"
  },
  {
    "text": "API it's been around for a long time but it's it's very stable and it does what it does and so effectively what you're",
    "start": "666790",
    "end": "673000"
  },
  {
    "text": "doing if you run an overlay network is you're choosing to run a software-defined network on top of a",
    "start": "673000",
    "end": "679150"
  },
  {
    "text": "software-defined network so you're in this like whole other business which I don't know that people necessarily want",
    "start": "679150",
    "end": "684460"
  },
  {
    "text": "to be in if you're you know paying Abe you asked to run your network for you and also because you have these sorts of",
    "start": "684460",
    "end": "690790"
  },
  {
    "text": "limitations you might have issues connecting to the Envoy mesh you might have to use NAT and there's just like",
    "start": "690790",
    "end": "696910"
  },
  {
    "text": "lots of like hard edges if you will I guess someone one thing is like a lot of these are fairly easy to set up in terms",
    "start": "696910",
    "end": "704110"
  },
  {
    "text": "of just like point-and-click and get them up and running but it's like once you have a problem it can be really complicated to the bug what's going on",
    "start": "704110",
    "end": "712320"
  },
  {
    "start": "712000",
    "end": "712000"
  },
  {
    "text": "okay so next category which didn't exist at the time when we started writing all this two years ago where options to talk",
    "start": "712680",
    "end": "719800"
  },
  {
    "text": "to the EPC network natively so all this is from our view at least it's very simple and straightforward there's not a",
    "start": "719800",
    "end": "726190"
  },
  {
    "text": "lot of code that exists to make all this work pods receive full VPC IP addresses",
    "start": "726190",
    "end": "731910"
  },
  {
    "text": "and they get full connectivity within the V PC so anything else in the V PC whether it's Amazon service or some",
    "start": "731910",
    "end": "739060"
  },
  {
    "text": "legacy service you're running running that you have like you just can talk to it and it's another V PC IP address and",
    "start": "739060",
    "end": "745720"
  },
  {
    "text": "then most one of the most important points is that you get native network performance or you should get like near native network performance and so the",
    "start": "745720",
    "end": "752950"
  },
  {
    "text": "two main options there you know the AWS option and also lift and both",
    "start": "752950",
    "end": "759400"
  },
  {
    "text": "were developed around the same time and so my talk covers mostly the both side of things",
    "start": "759400",
    "end": "765810"
  },
  {
    "text": "or sack so it's very minimal there's no daman sets there's pods there's no",
    "start": "767010",
    "end": "772120"
  },
  {
    "text": "runtimes is a handful of stateless go binaries we're on everything production with cryo data dog runs their production workloads",
    "start": "772120",
    "end": "779830"
  },
  {
    "text": "and container D as I mentioned overlay the two interfaces I'll talk about that that live inside of of our environment",
    "start": "779830",
    "end": "787450"
  },
  {
    "text": "which is the IP VLAN interface and an unnumbered point-to-point interface and",
    "start": "787450",
    "end": "793390"
  },
  {
    "text": "kind of most importantly is like you don't need to change anything about your V PC so you don't have to fiddle with like asymmetric routing you don't have",
    "start": "793390",
    "end": "800740"
  },
  {
    "text": "to like change your route table it's intended to just work however your VP is",
    "start": "800740",
    "end": "806590"
  },
  {
    "text": "C is currently configured and from our perspective it's basically feature complete we've been running in production for two years with very",
    "start": "806590",
    "end": "813160"
  },
  {
    "text": "minimal changes so regarding an IP VLAN you're not aware of it there's some good",
    "start": "813160",
    "end": "820180"
  },
  {
    "text": "papers about it there's a lot of talks been presented over the years about it but it was created by Google in 2014 it",
    "start": "820180",
    "end": "825610"
  },
  {
    "text": "shipped with Linux 318 the primary benefit it was it was essentially",
    "start": "825610",
    "end": "830770"
  },
  {
    "text": "designed to work with containers and it avoids going back and forth between the",
    "start": "830770",
    "end": "836380"
  },
  {
    "text": "default network namespace or the host namespace so with a lot of systems for getting IPs and the containers or",
    "start": "836380",
    "end": "843280"
  },
  {
    "text": "internetwork namespace with Linux you've got to effectively have a virtual Ethernet device or like some sort of bridge that since the packets back and",
    "start": "843280",
    "end": "850690"
  },
  {
    "text": "forth through the main namespace so in our case we can take a TBS elastic",
    "start": "850690",
    "end": "856420"
  },
  {
    "text": "network interfaces which are like our host network adapters and then tie those directly into pods so there's very",
    "start": "856420",
    "end": "863200"
  },
  {
    "text": "little overhead I'd argue that at least it's probably the least amount of overhead that you can have in order to",
    "start": "863200",
    "end": "869620"
  },
  {
    "text": "in order to get IPs and to into pots and then in particular I don't know if",
    "start": "869620",
    "end": "876100"
  },
  {
    "text": "Google like knew this at the time but IP VLAN Maps basically perfectly to the",
    "start": "876100",
    "end": "883090"
  },
  {
    "text": "constraints on the AWS VPC design particularly in our case your own IP",
    "start": "883090",
    "end": "888970"
  },
  {
    "text": "VLAN and layer 2 mode and that just it works out really well ok",
    "start": "888970",
    "end": "895350"
  },
  {
    "start": "894000",
    "end": "894000"
  },
  {
    "text": "I mentioned earlier it's a virtual NIC and so the number of ni is that you can",
    "start": "895350",
    "end": "900870"
  },
  {
    "text": "have her ec2 instance is based on the instant size so right now that's two to fifteen and then also the number of IPs",
    "start": "900870",
    "end": "907650"
  },
  {
    "text": "you can have per II and I also varies based on the instant size and so the the",
    "start": "907650",
    "end": "913800"
  },
  {
    "text": "point I mentioned earlier about needing to support up to a hundred and ten pod 310 IPS per node so if you want like a",
    "start": "913800",
    "end": "920820"
  },
  {
    "text": "fully conformant implementation of Corinne Enys using the stack then that means that you have to use at least four",
    "start": "920820",
    "end": "927360"
  },
  {
    "text": "like the modern ec2 instances you need to use a 4x large or larger so this is not a system that's appropriate for say",
    "start": "927360",
    "end": "933300"
  },
  {
    "text": "like you know t2 or t3 micro but again",
    "start": "933300",
    "end": "938610"
  },
  {
    "text": "this is more for a production environment not necessarily for like you know a development environment in terms",
    "start": "938610",
    "end": "943740"
  },
  {
    "text": "of how we managing eyes that's all managed by the by the CNI so we reserved",
    "start": "943740",
    "end": "949350"
  },
  {
    "text": "the beauty knife or the kubernetes control plane and also any services that that need the host IP and then we",
    "start": "949350",
    "end": "957420"
  },
  {
    "text": "effectively pull down in eyes as we need to and then IP addresses get added to",
    "start": "957420",
    "end": "962640"
  },
  {
    "text": "the in is until they're full and then once the eye is full we make a new en i and we add IP staff and there's some",
    "start": "962640",
    "end": "968010"
  },
  {
    "text": "options like pre allocating eyes on boot if you want to just to save the overhead from that by default we use a 60 second",
    "start": "968010",
    "end": "974520"
  },
  {
    "text": "TTL so there's IP addresses get get reused as as we need to and that's but",
    "start": "974520",
    "end": "982860"
  },
  {
    "text": "effectively it like an accurate ease environment it's very common to have a lot of pod turn and so we try to hang on",
    "start": "982860",
    "end": "988620"
  },
  {
    "text": "to those IPS when when we can and just reuse them okay so if you end up coop",
    "start": "988620",
    "end": "996090"
  },
  {
    "start": "994000",
    "end": "994000"
  },
  {
    "text": "coddling exec into into pod you'll see in addition to loopback interface you'll",
    "start": "996090",
    "end": "1002300"
  },
  {
    "text": "see two main interfaces one is e 0 which is your IP VLAN interface and so that's",
    "start": "1002300",
    "end": "1009230"
  },
  {
    "text": "the one that's tied directly back to the you know I and I that you're attached to all V PC traffic goes out at that",
    "start": "1009230",
    "end": "1016520"
  },
  {
    "text": "address and additionally it's also isolated from all the other n eyes on the system and then the other interface",
    "start": "1016520",
    "end": "1024110"
  },
  {
    "text": "that we have since you'll probably going to have traffic that is not directly over the V PC is we have an unnumbered p2p interface",
    "start": "1024110",
    "end": "1033500"
  },
  {
    "text": "so this is effectively a high-speed interconnect back to the host namespace so if you have any pods with host",
    "start": "1033500",
    "end": "1039680"
  },
  {
    "text": "networking or if you have any cupric see virtual IP addresses any sort of like",
    "start": "1039680",
    "end": "1045470"
  },
  {
    "text": "the networking magic layer that kubernetes does all this traffic goes back goes out over a v-0 and so there's",
    "start": "1045470",
    "end": "1053150"
  },
  {
    "text": "sort of this I think it's kind of a neat little hack but it's specified in RFC 5309 which is",
    "start": "1053150",
    "end": "1062360"
  },
  {
    "text": "that if an IP address doesn't exist for an interface then an IP address gets",
    "start": "1062360",
    "end": "1067580"
  },
  {
    "text": "borrowed so in our case this particular interface doesn't have an IP address at all on either side and it's going to",
    "start": "1067580",
    "end": "1075170"
  },
  {
    "text": "borrow the DPC IP address from each zero and so if you recall back when I was",
    "start": "1075170",
    "end": "1080450"
  },
  {
    "text": "talking about how like everyone else has to see you with the same IP that's what you see yourself as that's how we",
    "start": "1080450",
    "end": "1087170"
  },
  {
    "text": "achieve it for this particular interface and then also this interface gets used",
    "start": "1087170",
    "end": "1092960"
  },
  {
    "text": "for Internet egress if that's something that you need to do in your environment which we do so how many Internet us",
    "start": "1092960",
    "end": "1100250"
  },
  {
    "text": "works so this is a feature of ec2 hosts where you can have private IP of your",
    "start": "1100250",
    "end": "1107870"
  },
  {
    "text": "boot network adapter which can map to a public IP and then any traffic that gets",
    "start": "1107870",
    "end": "1114740"
  },
  {
    "text": "sent out that gets sent out that interface ends up getting added by by a",
    "start": "1114740",
    "end": "1121340"
  },
  {
    "text": "TOS in order to talk to the public internet and so in our case we because",
    "start": "1121340",
    "end": "1127760"
  },
  {
    "text": "we can like get to the public internet over this interface and it's redundant and scalable doesn't involve any sort of",
    "start": "1127760",
    "end": "1133730"
  },
  {
    "start": "1133000",
    "end": "1133000"
  },
  {
    "text": "like single points of failure we end up having our pods we end up sourcing adding for those pods and so that's how",
    "start": "1133730",
    "end": "1139370"
  },
  {
    "text": "the traffic gets out so it doesn't mean that if you're going to the public internet then you incur a latency",
    "start": "1139370",
    "end": "1145130"
  },
  {
    "text": "overhead and then you also have to go through down so the the network path there is a bit more complex whereas if",
    "start": "1145130",
    "end": "1152210"
  },
  {
    "text": "you're on the V PC it's just direct communications between two IPs and so kind of the takeaway there is that if",
    "start": "1152210",
    "end": "1159290"
  },
  {
    "text": "it's possible you should use V PC endpoints for great services because that means that you're going to eat grass on your V PC",
    "start": "1159290",
    "end": "1165440"
  },
  {
    "text": "interface over IP VLAN as opposed to going out through through the NAP side",
    "start": "1165440",
    "end": "1172390"
  },
  {
    "text": "okay so notes about us running on volume production okay so this is a kind of a",
    "start": "1173230",
    "end": "1182990"
  },
  {
    "text": "rough diagram of what it looks like for our production network so we have five",
    "start": "1182990",
    "end": "1189080"
  },
  {
    "text": "core clusters that are each Parisi we also have a staging environment that",
    "start": "1189080",
    "end": "1194150"
  },
  {
    "text": "looks similar to this and then we have a legacy stack as well and so as I",
    "start": "1194150",
    "end": "1200660"
  },
  {
    "text": "mentioned before it lets us like incriminating rollout and it also means that if we lose a whole entire cluster then left as a whole stays up so",
    "start": "1200660",
    "end": "1207230"
  },
  {
    "text": "services end up deploying to all of the core clusters and HPA's run on each of",
    "start": "1207230",
    "end": "1214850"
  },
  {
    "text": "those clusters to scale services up and down okay so pods are made up of a whole",
    "start": "1214850",
    "end": "1223490"
  },
  {
    "text": "bunch of containers I'm not going to go too much in detail about those kind of the important thing is that we have a lift service which is Python goer",
    "start": "1223490",
    "end": "1229610"
  },
  {
    "text": "JavaScript most of our services are in Python or go and then we have an envoy sidecar but they're headed for other",
    "start": "1229610",
    "end": "1235550"
  },
  {
    "text": "side cars and clewd runtime changes logging stats and and business metrics sort of what you'd expect and a lot of",
    "start": "1235550",
    "end": "1241910"
  },
  {
    "text": "what we've done for a production environment has been related to orchestrating how these containers come up in and shut down okay for the lift",
    "start": "1241910",
    "end": "1254480"
  },
  {
    "start": "1250000",
    "end": "1250000"
  },
  {
    "text": "envoy control plane as I mentioned like a million times all pods have a GC IP",
    "start": "1254480",
    "end": "1259730"
  },
  {
    "text": "address and so in particular one of the cool aspects of this in terms of how we were like initially rolling this out and",
    "start": "1259730",
    "end": "1266210"
  },
  {
    "text": "how it could be incremental is that the earlier versions of envoy and our control plane could not tell the",
    "start": "1266210",
    "end": "1272660"
  },
  {
    "text": "difference whether or not a service was running in kubernetes or was running on",
    "start": "1272660",
    "end": "1278270"
  },
  {
    "text": "our legacy stack just using ec2 instances with all the scaling groups so from envoys perspective and has an IP",
    "start": "1278270",
    "end": "1285910"
  },
  {
    "text": "and that just becomes part of the mesh so health checks it like everything that you'd expect would work so we had a",
    "start": "1285910",
    "end": "1292280"
  },
  {
    "text": "controller that was on all of our port clusters",
    "start": "1292280",
    "end": "1297380"
  },
  {
    "text": "and that would watch for pods to come up and then register those and to",
    "start": "1297380",
    "end": "1303520"
  },
  {
    "text": "open-source our open source service discovery layer and so since then we've",
    "start": "1303520",
    "end": "1309530"
  },
  {
    "text": "revamp the control plane envoy managers what it's called today but I've way",
    "start": "1309530",
    "end": "1315320"
  },
  {
    "text": "manager itself uses informers to determine pod status and it connects to",
    "start": "1315320",
    "end": "1321020"
  },
  {
    "text": "all the core clusters and that's how all the different communities clusters get bridged together and to into a single",
    "start": "1321020",
    "end": "1327740"
  },
  {
    "text": "mesh so in terms of how so I mentioned",
    "start": "1327740",
    "end": "1334730"
  },
  {
    "start": "1330000",
    "end": "1330000"
  },
  {
    "text": "before about pods and we have like all these sidecar containers and one of those containers it's the on way mesh like cart and so those need to get XDS",
    "start": "1334730",
    "end": "1344810"
  },
  {
    "text": "data as part of their startup in order to do that we run a headless service or",
    "start": "1344810",
    "end": "1350900"
  },
  {
    "text": "Envoy manager and so effectively all the Envoy managers that are in ren√©e's cluster it just has a list of all their",
    "start": "1350900",
    "end": "1359180"
  },
  {
    "text": "IP addresses which again are V PC IPs nothing special about them and then the envoy sidecar can use dns to bootstrap",
    "start": "1359180",
    "end": "1368720"
  },
  {
    "text": "itself and get up and running okay",
    "start": "1368720",
    "end": "1376370"
  },
  {
    "text": "the migration takeaways or things that we've learned throughout this like to",
    "start": "1376370",
    "end": "1381590"
  },
  {
    "text": "your process so having this ability to do hybrid deployments has been a huge",
    "start": "1381590",
    "end": "1388190"
  },
  {
    "text": "win in particular we can have these services that are on a SGS on ec2",
    "start": "1388190",
    "end": "1394010"
  },
  {
    "text": "instances that are scaling up and down and like while that is still running we can deploy a community's version of",
    "start": "1394010",
    "end": "1401150"
  },
  {
    "text": "their service on HPA's and that can scale as well and so basically we can like incrementally decrease the size of",
    "start": "1401150",
    "end": "1409880"
  },
  {
    "text": "the ASG and then let the HPA handle more and more of the work where it eventually like everything gets migrated and that",
    "start": "1409880",
    "end": "1416480"
  },
  {
    "text": "service gets turned off and we inflate the ASG on the mo legacy side and I think kind of one of the other were sort",
    "start": "1416480",
    "end": "1425240"
  },
  {
    "text": "of like driving factor for us in terms of design is to avoid complexity on the network",
    "start": "1425240",
    "end": "1430309"
  },
  {
    "text": "I think communities as a whole lets you there's a lot of magic there which i",
    "start": "1430309",
    "end": "1436850"
  },
  {
    "text": "think is great for development environments and is is also good for sort of like setting up things quickly",
    "start": "1436850",
    "end": "1444470"
  },
  {
    "text": "or maybe like smaller production environments but if you have envoy up",
    "start": "1444470",
    "end": "1449720"
  },
  {
    "start": "1447000",
    "end": "1447000"
  },
  {
    "text": "and running and you have a production network you want to try to avoid any technology you don't absolutely need and",
    "start": "1449720",
    "end": "1455059"
  },
  {
    "text": "so if you need to technology please feel free to use it but I guess it's important to understand the complexity",
    "start": "1455059",
    "end": "1460610"
  },
  {
    "text": "and what it's doing behind the scenes to make that happen because if it breaks",
    "start": "1460610",
    "end": "1465740"
  },
  {
    "text": "it's gonna be that much harder to bug because at that point you're gonna have to understand how all the magic works",
    "start": "1465740",
    "end": "1471730"
  },
  {
    "text": "and in terms of IPP LAN it's worked really well for us we keep latency stats relating numbers for all of our lift",
    "start": "1471730",
    "end": "1478730"
  },
  {
    "text": "services that's all exported as part of the Envoy metrics P 95 and P 99 has been",
    "start": "1478730",
    "end": "1484520"
  },
  {
    "text": "same across this so in that sense from a network perspective the migration has been very transparent for service owners",
    "start": "1484520",
    "end": "1490880"
  },
  {
    "text": "it lifts I think also another sort of",
    "start": "1490880",
    "end": "1496190"
  },
  {
    "text": "important item is that the VP C continues to work as intended it means",
    "start": "1496190",
    "end": "1503419"
  },
  {
    "text": "that if you can like if you are seeing a problem with the VP C it means you can file a ticket with AWS and ask",
    "start": "1503419",
    "end": "1509600"
  },
  {
    "text": "what's going on and it's not like this convoluted set up with like IP IP or",
    "start": "1509600",
    "end": "1514730"
  },
  {
    "text": "you're not sure where the performance issue is or what's going on but effectively you end up with the same performance and throughput as you would",
    "start": "1514730",
    "end": "1520850"
  },
  {
    "text": "expect without using this setup the",
    "start": "1520850",
    "end": "1526520"
  },
  {
    "text": "other thing is if you have a way to to get your tears your services on to",
    "start": "1526520",
    "end": "1531700"
  },
  {
    "text": "multiple clusters I would say do that I think one of the one of my fears or kept",
    "start": "1531700",
    "end": "1536960"
  },
  {
    "text": "me up a lot at night it was like okay well if we lose a cluster does lift go down and so it's very important to me",
    "start": "1536960",
    "end": "1543230"
  },
  {
    "text": "that that we can lose a cluster the goal is to is to not lose a cluster but if we",
    "start": "1543230",
    "end": "1548630"
  },
  {
    "text": "do lose a production cluster it's no different than losing an available at least own so Easy's go away from time to",
    "start": "1548630",
    "end": "1554179"
  },
  {
    "text": "time sometimes they blip or Brown out but overall the on void mesh to be able to",
    "start": "1554179",
    "end": "1560090"
  },
  {
    "text": "deal with that and she should be able to scale and to other AZ's that are healthy",
    "start": "1560090",
    "end": "1566260"
  },
  {
    "text": "and so far this this set up has been as an organ wall for us in terms of a",
    "start": "1566260",
    "end": "1575630"
  },
  {
    "text": "future work so we're not looking to add any significant features to the codebase",
    "start": "1575630",
    "end": "1582880"
  },
  {
    "text": "so again Tears little Mole changes if there's something that we feel would be useful we're happy to add it but in",
    "start": "1582880",
    "end": "1588830"
  },
  {
    "text": "general we feel that the that it's that it's fairly done we don't use ipv6 internally maybe one",
    "start": "1588830",
    "end": "1595010"
  },
  {
    "text": "day we will when that day happens we'll write the the code for ipv6 if anyone",
    "start": "1595010",
    "end": "1600350"
  },
  {
    "text": "else wants to add that we're more than welcome to set patches for that and one particular take as well which I",
    "start": "1600350",
    "end": "1606559"
  },
  {
    "text": "think was like a lot of discussion early on is how to have network policies and I",
    "start": "1606559",
    "end": "1613850"
  },
  {
    "text": "think c9 chaining is probably the correct approach there and that you have a specific stack that is not your core",
    "start": "1613850",
    "end": "1620780"
  },
  {
    "text": "stack which is responsible for handling network policy enforcement so in general",
    "start": "1620780",
    "end": "1626450"
  },
  {
    "text": "it looks like selenium should be pretty easy to wire up with our stack so we're",
    "start": "1626450",
    "end": "1631730"
  },
  {
    "text": "looking up into doing that with there seen I chaining another thing that's currently under development is using",
    "start": "1631730",
    "end": "1637460"
  },
  {
    "text": "traffic control to restrict bandwidth based on CPU count so this isn't really",
    "start": "1637460",
    "end": "1642620"
  },
  {
    "text": "a production issue for us right now or has not been just because of how large of the instances that we run and so when we're",
    "start": "1642620",
    "end": "1649340"
  },
  {
    "text": "scheduling pods against nodes we can run out of CP memory like way before we end",
    "start": "1649340",
    "end": "1654559"
  },
  {
    "text": "up running out of network bandwidth so I",
    "start": "1654559",
    "end": "1664910"
  },
  {
    "text": "want to thank yous main contributors our lift and data dog to the stack but there",
    "start": "1664910",
    "end": "1670520"
  },
  {
    "text": "been a handful of other people that have also contributed to the github project so thanks very much",
    "start": "1670520",
    "end": "1676779"
  },
  {
    "text": "[Applause] very last happy hour tonight 7:00 to",
    "start": "1676790",
    "end": "1685700"
  },
  {
    "start": "1678000",
    "end": "1678000"
  },
  {
    "text": "10:00 p.m. feel free to come hang out any questions just to reminder before",
    "start": "1685700",
    "end": "1697070"
  },
  {
    "text": "folks leave if you can rate the session and other sessions you attend on the schedule it really helps us with planning future cube cons can you speak",
    "start": "1697070",
    "end": "1708470"
  },
  {
    "text": "a bit about what the difference between the lift one is and the AWS BBC CNI",
    "start": "1708470",
    "end": "1715720"
  },
  {
    "text": "there's somewhere I don't want to speak too much until about I'd rather have a TAS talk about that ours uses IP VLAN",
    "start": "1715720",
    "end": "1723190"
  },
  {
    "text": "they currently don't use that I think I'd argue that our stack is simpler and does less but it intentionally does less",
    "start": "1723190",
    "end": "1732850"
  },
  {
    "text": "how do you manage the IP address because typically Amazon VPC gives life 16 but",
    "start": "1736800",
    "end": "1742620"
  },
  {
    "text": "with containers under K containers do you reuse the IP addresses or what do we manage the IP addresses yeah so you",
    "start": "1742620",
    "end": "1751590"
  },
  {
    "text": "effectively tag what subnets you want to use within that and our plug-in will",
    "start": "1751590",
    "end": "1756810"
  },
  {
    "text": "burn down IP addresses at an equal rate so whatever subnets are are allowed for",
    "start": "1756810",
    "end": "1762120"
  },
  {
    "text": "it to use it'll make sure that there's get burned down evenly and then additionally you can have secondary sliders on your V PC which I think abs",
    "start": "1762120",
    "end": "1770160"
  },
  {
    "text": "introduced within the past couple of years but it's you can add more slash 16 to your V PC and you can assign those to",
    "start": "1770160",
    "end": "1775800"
  },
  {
    "text": "clusters so like effectively there's a lot of runway for v4 address space",
    "start": "1775800",
    "end": "1780870"
  },
  {
    "text": "within within a single V PC hi I like the / AZ kind of cluster concept could",
    "start": "1780870",
    "end": "1788640"
  },
  {
    "text": "you talk just a little bit about the failover mechanism and how that works its envoy yeah",
    "start": "1788640",
    "end": "1796850"
  },
  {
    "text": "so it's an envoy mesh if if services",
    "start": "1796850",
    "end": "1802320"
  },
  {
    "text": "start to degrade and one cluster then they'll scale up in another cluster",
    "start": "1802320",
    "end": "1808370"
  },
  {
    "text": "so what was the actual latency impact like how much does it lower it does it",
    "start": "1809810",
    "end": "1814830"
  },
  {
    "text": "manage of all negligible and we actually internally we don't have really good stats unlike P triple nine so there's",
    "start": "1814830",
    "end": "1820200"
  },
  {
    "text": "probably some weight definitions on like P triple nine but there's a handful of papers out there that cover IP VL and",
    "start": "1820200",
    "end": "1825750"
  },
  {
    "text": "Layton sees and performance testing but and our practice for what we care about which is P 99 it's been equivalent hi do",
    "start": "1825750",
    "end": "1835350"
  },
  {
    "text": "you have any figures to support that it has low latency you are able to achieve",
    "start": "1835350",
    "end": "1840930"
  },
  {
    "text": "low latency internal data from your Envoy - I'm happy to point you to like",
    "start": "1840930",
    "end": "1847260"
  },
  {
    "text": "other other papers that talk about IP VLAN latency oh hi with multiple",
    "start": "1847260",
    "end": "1856770"
  },
  {
    "text": "clusters how do you keep them in sync as far as what's provisioned the versions of the applications etc so we have a",
    "start": "1856770",
    "end": "1864300"
  },
  {
    "text": "very sort of like lightweight I'd say it's sort of like we'll call it Federation but",
    "start": "1864300",
    "end": "1870269"
  },
  {
    "text": "it's a very lightweight stack that merges together all the clusters and that if you're a lyft developer or if",
    "start": "1870269",
    "end": "1877229"
  },
  {
    "text": "you're using on deployment snack you end up deploying to all the clusters and so as part of the deploy you can determine",
    "start": "1877229",
    "end": "1882269"
  },
  {
    "text": "whether or not your application failed deploying to a particular cluster but",
    "start": "1882269",
    "end": "1887399"
  },
  {
    "text": "each cluster is treated independently hi",
    "start": "1887399",
    "end": "1893519"
  },
  {
    "text": "thank you for the talk um it's any attempt been made to get lefse and i working with other cloud providers like",
    "start": "1893519",
    "end": "1900269"
  },
  {
    "text": "GCP i know in this case like it is again there's not a lot of code there and it's",
    "start": "1900269",
    "end": "1906899"
  },
  {
    "text": "heavily dependent upon a TBS api's so it's very much depended on the B PC I'm so yeah it wouldn't be appropriate for a",
    "start": "1906899",
    "end": "1913619"
  },
  {
    "text": "different cloud provider Thanks I mean",
    "start": "1913619",
    "end": "1918869"
  },
  {
    "text": "having said that I would say that regardless of what cloud provider you're using you for production you want",
    "start": "1918869",
    "end": "1925589"
  },
  {
    "text": "something that gives you this like one-to-one mapping of a VPC IP address you've had IP like you want it to be as",
    "start": "1925589",
    "end": "1931379"
  },
  {
    "text": "simple as possible and in our case it's for that Amazon stack are you using the",
    "start": "1931379",
    "end": "1936509"
  },
  {
    "text": "AWS security construct like security groups for the communication between your continues yes there are some",
    "start": "1936509",
    "end": "1944700"
  },
  {
    "text": "caveats there in terms of how we handle Ni and security groups I can talk with you about it about offline ok",
    "start": "1944700",
    "end": "1951979"
  },
  {
    "text": "thanks for a great talk I wasn't quite clear on how you got between regions offends them creatively as VP C's don't",
    "start": "1959770",
    "end": "1965890"
  },
  {
    "text": "span between regions and your track they don't so how do you bridge that gap did",
    "start": "1965890",
    "end": "1971950"
  },
  {
    "text": "I miss something no we were in predominantly in US East one so it kind of determines on where what the services and what the",
    "start": "1971950",
    "end": "1977770"
  },
  {
    "text": "application is but in the sense this this particular design and setup is for",
    "start": "1977770",
    "end": "1982840"
  },
  {
    "text": "its very single region okay so you're saying you don't span regions at all or you do sometimes and you're not telling",
    "start": "1982840",
    "end": "1989500"
  },
  {
    "text": "us how you do it or those are those are separate meshes okay okay separate",
    "start": "1989500",
    "end": "1994660"
  },
  {
    "text": "environments okay gotcha yeah all right we have time for one more",
    "start": "1994660",
    "end": "2000809"
  },
  {
    "text": "I just want to reconfirm in terms of where do you update the security policies for the containers like the sec",
    "start": "2000809",
    "end": "2009420"
  },
  {
    "text": "the containers are accessible outside to the internet or you apply the policies that this container has to talk outside",
    "start": "2009420",
    "end": "2015240"
  },
  {
    "text": "this container is not have this security groups are on a Purina basis and so",
    "start": "2015240",
    "end": "2020730"
  },
  {
    "text": "phenomenally whatever in eyes you have set up for which cluster that determines you can talk to you at the 80s level so",
    "start": "2020730",
    "end": "2026160"
  },
  {
    "text": "our pods are not addressable from the public Internet so there is a way for our pods to egress to the public",
    "start": "2026160",
    "end": "2032070"
  },
  {
    "text": "internet over an out but that doesn't mean that someone from public internet can access those pods all right thank",
    "start": "2032070",
    "end": "2039300"
  },
  {
    "text": "you everyone for attending [Applause]",
    "start": "2039300",
    "end": "2044359"
  }
]