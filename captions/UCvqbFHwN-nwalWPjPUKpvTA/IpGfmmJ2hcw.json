[
  {
    "text": "alright let's get started second-to-last session today everybody feeling good",
    "start": "30",
    "end": "6140"
  },
  {
    "text": "getting good or I'm okay okay I feel bit but today we're here to talk about",
    "start": "6140",
    "end": "15139"
  },
  {
    "text": "declarative cluster monitoring with the Prometheus operator I'm Frederick and hi",
    "start": "15139",
    "end": "21630"
  },
  {
    "text": "I'm Matthias okay yeah so I actually work on the prometheus operator and",
    "start": "21630",
    "end": "28460"
  },
  {
    "text": "Matthias uses it at Nazi so we thought",
    "start": "28460",
    "end": "33540"
  },
  {
    "text": "it would be a really cool idea to sort of give everybody an introduction to do into the Prometheus operator itself and",
    "start": "33540",
    "end": "39210"
  },
  {
    "text": "then show a practical example of how Lutz who uses it so without further ado",
    "start": "39210",
    "end": "44879"
  },
  {
    "text": "let's do a quick introduction into Prometheus just to make sure that everybody understands how Prometheus",
    "start": "44879",
    "end": "51590"
  },
  {
    "text": "prometheus works and how then the Prometheus operator adds value to this",
    "start": "51590",
    "end": "57379"
  },
  {
    "text": "so and how Prometheus works is that you have your application here on the left",
    "start": "57379",
    "end": "63750"
  },
  {
    "text": "side and you instrument it with Prometheus style metrics and the way that works is that you have your metrics",
    "start": "63750",
    "end": "70530"
  },
  {
    "text": "in in memory and then for example in this case we have a request counter so",
    "start": "70530",
    "end": "77009"
  },
  {
    "text": "you atomically incremented whenever there is a request and then Prometheus",
    "start": "77009",
    "end": "82409"
  },
  {
    "text": "comes and scrapes those metrics over an HTTP server which is how you expose",
    "start": "82409",
    "end": "87750"
  },
  {
    "text": "those metrics to Prometheus and scrapes those in some interval by default that's",
    "start": "87750",
    "end": "92939"
  },
  {
    "text": "happy 15 seconds and when Prometheus grabs those metrics it then writes that",
    "start": "92939",
    "end": "99329"
  },
  {
    "text": "in to its internal time series database and in this case then we're at T 0 and",
    "start": "99329",
    "end": "106520"
  },
  {
    "text": "there has not been any traffic to our application so it's zero and then we",
    "start": "106520",
    "end": "112590"
  },
  {
    "text": "actually create a load balancer or something or there load balancer picks it up and user traffic actually hits our",
    "start": "112590",
    "end": "118890"
  },
  {
    "text": "application and our request counter increases and then the scrape interval",
    "start": "118890",
    "end": "124740"
  },
  {
    "text": "has passed 15 seconds have passed so Prometheus comes around again and",
    "start": "124740",
    "end": "130069"
  },
  {
    "text": "grabs those metrics and writes at interest serious database and then just continues",
    "start": "130069",
    "end": "135500"
  },
  {
    "text": "doing that this forever and just to make sure that we understand all the other",
    "start": "135500",
    "end": "142580"
  },
  {
    "text": "parts of Prometheus and that nothing is black magic alert evaluation or alerting",
    "start": "142580",
    "end": "148220"
  },
  {
    "text": "in general works by Prometheus ingesting all of these metrics and then every",
    "start": "148220",
    "end": "153620"
  },
  {
    "text": "interval again if I believe this is also by default every 15 seconds it just goes",
    "start": "153620",
    "end": "160220"
  },
  {
    "text": "through all the alert definitions that you've given it evaluates them and if it's an alerting rule and it triggers it",
    "start": "160220",
    "end": "167120"
  },
  {
    "text": "actually it fires it against the alert manager and yeah that's that's pretty",
    "start": "167120",
    "end": "173120"
  },
  {
    "text": "much how how Prometheus works so now I already mentioned some more of the I",
    "start": "173120",
    "end": "182380"
  },
  {
    "text": "already mentioned what a target so what do I actually when I when I say a target",
    "start": "182380",
    "end": "187520"
  },
  {
    "text": "what I'm what do I mean that by that well really anything that has metrics endpoint by default",
    "start": "187520",
    "end": "193550"
  },
  {
    "text": "Prometheus wants this to be the slash metrics and point on your HTTP server but it could really be anything it's",
    "start": "193550",
    "end": "198950"
  },
  {
    "text": "configurable and for this to actually be picked up by Prometheus maybe it's",
    "start": "198950",
    "end": "205250"
  },
  {
    "text": "obvious but it needs to actually be discovered and Prometheus has a number of mechanisms to do this it can just be",
    "start": "205250",
    "end": "213230"
  },
  {
    "text": "a static list of targets it can be one of the service discovery mechanisms like DNS or because we're at coop con",
    "start": "213230",
    "end": "220330"
  },
  {
    "text": "kubernetes discovery and it has all of these built in so that's really neat and",
    "start": "220330",
    "end": "226640"
  },
  {
    "text": "kubernetes discovery works in a way where you can say you want to discover",
    "start": "226640",
    "end": "231680"
  },
  {
    "text": "pods directly or you want to discover nodes or something that's actually most commonly used is the endpoints discovery",
    "start": "231680",
    "end": "238550"
  },
  {
    "text": "so because in within Prometheus or within kubernetes we already structure",
    "start": "238550",
    "end": "244970"
  },
  {
    "text": "our applications in a way and service discovery mechanisms are meant to reflect how you internally in your",
    "start": "244970",
    "end": "252650"
  },
  {
    "text": "organization structure your applications so this is a very common way because that's that's how we already do this in",
    "start": "252650",
    "end": "259190"
  },
  {
    "text": "communities and thus we want to be able to display this reflect this on two-on-two Prometheus and what's really",
    "start": "259190",
    "end": "266720"
  },
  {
    "text": "cool about the built-in discovery mechanisms specifically with kubernetes is that we",
    "start": "266720",
    "end": "271970"
  },
  {
    "text": "can automatically depending of our life cycle within kubernetes add remove",
    "start": "271970",
    "end": "278810"
  },
  {
    "text": "update our targets without having to reconfigure Prometheus all the time",
    "start": "278810",
    "end": "283970"
  },
  {
    "text": "because in kubernetes all of these things are super dynamic they can happen within sub second time intervals and we",
    "start": "283970",
    "end": "292130"
  },
  {
    "text": "we want to be able to that our monitoring always picks up everything so this is what kubernetes discovery is",
    "start": "292130",
    "end": "298100"
  },
  {
    "text": "super useful for so let's look at a more more concrete example of this so we have",
    "start": "298100",
    "end": "303500"
  },
  {
    "text": "our number of pods and I think this case we just have our random web application",
    "start": "303500",
    "end": "308840"
  },
  {
    "text": "and as so often it is backed by some database and the way we do this in",
    "start": "308840",
    "end": "316730"
  },
  {
    "text": "criminales is that we then have a set of labels that identify our pods or a",
    "start": "316730",
    "end": "323540"
  },
  {
    "text": "number of pods and that selector we use in a service object and the way a",
    "start": "323540",
    "end": "329390"
  },
  {
    "text": "service object actually works in kubernetes is that the service is what you specify right with the selector and",
    "start": "329390",
    "end": "335780"
  },
  {
    "text": "then respective to that service AAA endpoints object is automatically",
    "start": "335780",
    "end": "341930"
  },
  {
    "text": "created and the endpoints object actually has the concrete part information in it and this is actually",
    "start": "341930",
    "end": "349430"
  },
  {
    "text": "where Prometheus then goes ahead and grabs this information so because we",
    "start": "349430",
    "end": "355120"
  },
  {
    "text": "because we want to know we want to actually do white box monitoring right we want to go to our targets and grab",
    "start": "355120",
    "end": "361310"
  },
  {
    "text": "the metrics from our targets we need to know how where to actually find that and this is exactly how endpoints discovery",
    "start": "361310",
    "end": "368330"
  },
  {
    "text": "works in kubernetes and then for some additional metadata we can also go and",
    "start": "368330",
    "end": "374570"
  },
  {
    "text": "refer to the service object because we know most endpoints objects are",
    "start": "374570",
    "end": "380710"
  },
  {
    "text": "associated to a service object in kubernetes so that's how we can have",
    "start": "380710",
    "end": "387590"
  },
  {
    "text": "some useful additional meta information so then how we start doing this is we",
    "start": "387590",
    "end": "396680"
  },
  {
    "text": "have one one service for a front-end one for our API and one for our database and we have",
    "start": "396680",
    "end": "402979"
  },
  {
    "text": "Prometheus discover all of these and then trickle down to all of those parts and we can monitor them now when we have",
    "start": "402979",
    "end": "410360"
  },
  {
    "text": "such a such a setup it turns out that managing a",
    "start": "410360",
    "end": "416479"
  },
  {
    "text": "configuration for your entire infrastructure can get quite large so",
    "start": "416479",
    "end": "422899"
  },
  {
    "text": "I'm just showing skipping through a couple of slides of configuration here and this was just 66 lines out of 613",
    "start": "422899",
    "end": "431599"
  },
  {
    "text": "line configuration and we used to actually maintain a configuration that's huge and it's kind of crazy this is the",
    "start": "431599",
    "end": "438319"
  },
  {
    "text": "kind of thing that we want a configuration management system to take care of us for us so we thought to",
    "start": "438319",
    "end": "446239"
  },
  {
    "text": "ourselves there's got to be a better way and that's where we created the",
    "start": "446239",
    "end": "452179"
  },
  {
    "text": "Prometheus operator so the Prometheus operator with a Prometheus operator we",
    "start": "452179",
    "end": "457519"
  },
  {
    "text": "looked at the problem space that we that we wanted to apply this to and the",
    "start": "457519",
    "end": "465019"
  },
  {
    "text": "problem here was that a lot of how services are being discovered is not",
    "start": "465019",
    "end": "470899"
  },
  {
    "text": "very natural to how how we handle teams because usually we have a number of",
    "start": "470899",
    "end": "476479"
  },
  {
    "text": "teams which own respective set of services and these are sometimes even",
    "start": "476479",
    "end": "483019"
  },
  {
    "text": "expanding across namespaces so we want to be able to offer Prometheus as sort",
    "start": "483019",
    "end": "490519"
  },
  {
    "text": "of as a SAS offering to the rest of our organization so that people who actually",
    "start": "490519",
    "end": "495709"
  },
  {
    "text": "care about maintaining those prometheus servers can do that but do it independently of the people that use",
    "start": "495709",
    "end": "503349"
  },
  {
    "text": "Prometheus so that the people who actually just want to use Prometheus to monitor their applications they can just",
    "start": "503349",
    "end": "509749"
  },
  {
    "text": "say this is where to find my application please monitor so this is this is what",
    "start": "509749",
    "end": "517879"
  },
  {
    "text": "we did and this is cool because you don't need to learn the Prometheus",
    "start": "517879",
    "end": "523818"
  },
  {
    "text": "configuration paradigms which at least in the start can be quite complicated to",
    "start": "523819",
    "end": "532579"
  },
  {
    "text": "get into but extremely flexible because Prometheus we have to remember came out of an",
    "start": "532579",
    "end": "538550"
  },
  {
    "text": "environment that was not strictly built for kubernetes it's extremely fact flexible we can do pretty much anything",
    "start": "538550",
    "end": "545420"
  },
  {
    "text": "with it but that also means that with flexibility there comes some complexity",
    "start": "545420",
    "end": "551000"
  },
  {
    "text": "but specifically the Prometheus operator case we know we are going to Bria be",
    "start": "551000",
    "end": "557540"
  },
  {
    "text": "running in kubernetes and we are going to be monitoring targets in kubernetes so we can make paradigms that are that",
    "start": "557540",
    "end": "566029"
  },
  {
    "text": "people within communities are already used to like label selectors available to configure their monitoring system in",
    "start": "566029",
    "end": "573770"
  },
  {
    "text": "the exact same way so we built use useful or at least that's what we think",
    "start": "573770",
    "end": "579190"
  },
  {
    "text": "abstractions for this and the Prometheus operator you can find in this repository if you aren't already familiar with it",
    "start": "579190",
    "end": "585680"
  },
  {
    "text": "and because we did this we could we be we could encode of this knowledge that",
    "start": "585680",
    "end": "592550"
  },
  {
    "text": "we as developers who also work on Prometheus like a lot of us or actually",
    "start": "592550",
    "end": "599990"
  },
  {
    "text": "all of the maintainer of the Prometheus operator our core developers of the Prometheus team so we can encode this",
    "start": "599990",
    "end": "607700"
  },
  {
    "text": "knowledge that we have of running Prometheus in various environments mostly kubernetes we can encode that",
    "start": "607700",
    "end": "614870"
  },
  {
    "text": "into the Prometheus operator code base and therefore have that knowledge of how to run this relatively complex stateful",
    "start": "614870",
    "end": "623779"
  },
  {
    "text": "workload in a way that we can put our knowledge into into code and share it",
    "start": "623779",
    "end": "630290"
  },
  {
    "text": "with the rest of the community and and this includes things like upgrades",
    "start": "630290",
    "end": "636260"
  },
  {
    "text": "migrations and all this operational knowledge that goes into this and we're really proud that we can we actually",
    "start": "636260",
    "end": "643190"
  },
  {
    "text": "could and castel can automatically update from all the Prometheus versions",
    "start": "643190",
    "end": "649220"
  },
  {
    "text": "from 1.4 which is when we started the Prometheus operator project up until the",
    "start": "649220",
    "end": "654589"
  },
  {
    "text": "latest prometheus versions which is 2.2.1 and we can automatically upgrade through all of these and we can handle all the",
    "start": "654589",
    "end": "664480"
  },
  {
    "text": "incompatible changes for this and most importantly we have kubernetes",
    "start": "664480",
    "end": "670510"
  },
  {
    "text": "native native paradigms to do this and",
    "start": "670510",
    "end": "675810"
  },
  {
    "text": "in terms of this talk because we're talking about declarative monitoring",
    "start": "675810",
    "end": "681330"
  },
  {
    "text": "because we have now applied all of these concepts that kubernetes has which are",
    "start": "681330",
    "end": "687490"
  },
  {
    "text": "declarative our pods are a declarative definition of how we want to run our application art deployment is a",
    "start": "687490",
    "end": "693790"
  },
  {
    "text": "declarative definition of how many pods there should be and now we have a declarative configuration that is",
    "start": "693790",
    "end": "700990"
  },
  {
    "text": "cribben andes native and that describes how we want to monitor our applications",
    "start": "700990",
    "end": "706210"
  },
  {
    "text": "and alerting and kubernetes and prometheus is also declarative you say",
    "start": "706210",
    "end": "712540"
  },
  {
    "text": "the alert name the expression and Prometheus just goes ahead regularly",
    "start": "712540",
    "end": "717930"
  },
  {
    "text": "evaluates that and when it triggers we send an alert and really this boils down",
    "start": "717930",
    "end": "723760"
  },
  {
    "text": "to just declarative everything and the only thing that is now left is the clarity of core fauna and something that",
    "start": "723760",
    "end": "731350"
  },
  {
    "text": "the graph on our folks have been working on which is really cool and actually we used to write this really really ugly",
    "start": "731350",
    "end": "737500"
  },
  {
    "text": "hack where we ran a sidecar next to grow fauna and it deleted the entire graph on",
    "start": "737500",
    "end": "742600"
  },
  {
    "text": "a database and then reprovision stuff from files so I demoed that last year at prom con",
    "start": "742600",
    "end": "747730"
  },
  {
    "text": "and then Karl from from your panel says maybe maybe it's useful to have this in Griffin itself and they actually",
    "start": "747730",
    "end": "754030"
  },
  {
    "text": "implemented it so I can now throw away my code that's awesome so now with Griffin r5 we can just drop",
    "start": "754030",
    "end": "761170"
  },
  {
    "text": "a couple of dashboard definitions next to our graph Hana instance and it will just provision itself itself from these",
    "start": "761170",
    "end": "768580"
  },
  {
    "text": "from these files at startup so that's really awesome and something that has",
    "start": "768580",
    "end": "775000"
  },
  {
    "text": "just started this is a an initiative that Tom and I have been working on and",
    "start": "775000",
    "end": "780610"
  },
  {
    "text": "maybe you were just in his talk I know Matias was where we want to be able to",
    "start": "780610",
    "end": "785950"
  },
  {
    "text": "share alerting rules and dashboards more and still have this declarative nature",
    "start": "785950",
    "end": "791230"
  },
  {
    "text": "so if you haven't been able to catch Tom's talk do check out these these",
    "start": "791230",
    "end": "796480"
  },
  {
    "text": "repositories or watch the recording online and with that we have all of the pieces",
    "start": "796480",
    "end": "803080"
  },
  {
    "text": "to do the character monitoring and now my tears will show you how they apply to SAP you'll see",
    "start": "803080",
    "end": "809860"
  },
  {
    "text": "yeah so we run multiple data centers and what you don't want to go into each data",
    "start": "809860",
    "end": "815260"
  },
  {
    "text": "center and have to do that manually so we want to have something that is the declarative the configure rule and we",
    "start": "815260",
    "end": "823000"
  },
  {
    "text": "can just upload configuration to each cluster and then in the end that's going to be applied and yeah that's it so we",
    "start": "823000",
    "end": "829660"
  },
  {
    "text": "can apply the same configuration always slight differences - to each cluster so",
    "start": "829660",
    "end": "835870"
  },
  {
    "text": "this is like an overview of what we are doing we are running one Prometheus for the company basically so we we want to",
    "start": "835870",
    "end": "843640"
  },
  {
    "text": "have a a Prometheus that knows like across all data send us across",
    "start": "843640",
    "end": "849310"
  },
  {
    "text": "everything that is being monitored what the current state is so we can drill into like the data center that's that is",
    "start": "849310",
    "end": "856300"
  },
  {
    "text": "making problems for example yeah but this is like the overview and then below that we are running multiple",
    "start": "856300",
    "end": "862290"
  },
  {
    "text": "Prometheus's that are yeah inside the data center and and they only care about",
    "start": "862290",
    "end": "868450"
  },
  {
    "text": "the data inside that data center and as we are deploying kubernetes in",
    "start": "868450",
    "end": "875020"
  },
  {
    "text": "kubernetes we also want to monitor these kubernetes cluster classes that are",
    "start": "875020",
    "end": "882070"
  },
  {
    "text": "running in inside the name space with a new Prometheus to have that like as close to the cluster that we are",
    "start": "882070",
    "end": "889030"
  },
  {
    "text": "monitoring inside there as well and then in the end everything should bubble up",
    "start": "889030",
    "end": "894460"
  },
  {
    "text": "to the company Prometheus so that we as I said have the global view so how do we actually deploy",
    "start": "894460",
    "end": "902560"
  },
  {
    "text": "the Prometheus in into the namespace we are running a controller for our cluster so inside our code we basically just say",
    "start": "902560",
    "end": "911410"
  },
  {
    "text": "hey into this namespace that was just created please also deploy the CRVs so",
    "start": "911410",
    "end": "917650"
  },
  {
    "text": "there's a Prometheus object as an alert manager object and there are the service",
    "start": "917650",
    "end": "923350"
  },
  {
    "text": "monitors which are the declarative way as Frederick told you earlier to to find",
    "start": "923350",
    "end": "929980"
  },
  {
    "text": "the targets in in the namespace and we do all of that with an controller and yeah within that",
    "start": "929980",
    "end": "937360"
  },
  {
    "text": "namespace the alerts for that specific class are being shipped to the alert",
    "start": "937360",
    "end": "943240"
  },
  {
    "text": "manager for in which is running in the data center then we are running",
    "start": "943240",
    "end": "948510"
  },
  {
    "text": "Prometheus in the data center so there's like one level above and this is going to federate into the different",
    "start": "948510",
    "end": "957250"
  },
  {
    "text": "namespaces and caches only the specific metrics that we are interested in so we don't want to grab everything that is in",
    "start": "957250",
    "end": "963340"
  },
  {
    "text": "there but only the things that I use for one level above so this is one example",
    "start": "963340",
    "end": "969340"
  },
  {
    "text": "of a service monitor that we are using and that per data center Prometheus and",
    "start": "969340",
    "end": "975370"
  },
  {
    "text": "as you can see there's this path federal rate yeah key value pair and this just has",
    "start": "975370",
    "end": "982470"
  },
  {
    "text": "Prometheus to go to the Prometheus running in inside the namespace to then",
    "start": "982470",
    "end": "988270"
  },
  {
    "text": "in this example get metrics which are",
    "start": "988270",
    "end": "993150"
  },
  {
    "text": "which are starting with a machine controller so we only want in this example to to fetch those metrics and",
    "start": "993840",
    "end": "1002220"
  },
  {
    "text": "then like at the very top we are only like getting very very specific data",
    "start": "1002220",
    "end": "1008520"
  },
  {
    "text": "with a high retention we want to solve that very for a very long time and yeah",
    "start": "1008520",
    "end": "1015240"
  },
  {
    "text": "it's mostly for dashboarding so we want to have like a long term view of everything that is going on and that's I",
    "start": "1015240",
    "end": "1022260"
  },
  {
    "text": "mean it's it's good for SLA SSL laws and so on yeah so there's a drawback with a",
    "start": "1022260",
    "end": "1029730"
  },
  {
    "text": "current a Federation approach inside the cluster it's fine but if you are going",
    "start": "1029730",
    "end": "1034920"
  },
  {
    "text": "from one data center to the other you have to go via the load balancers and if",
    "start": "1034920",
    "end": "1041040"
  },
  {
    "text": "you're running multiple Prometheus's as replicas then you need to decide like I only want to monitor one of these",
    "start": "1041040",
    "end": "1048329"
  },
  {
    "text": "because if you're like changing every single time if you scrape the other Prometheus then you are like yeah I mean",
    "start": "1048330",
    "end": "1054030"
  },
  {
    "text": "you are getting different metrics every time you scrape in the worst case so we",
    "start": "1054030",
    "end": "1062340"
  },
  {
    "text": "are going to talk about the solution there but first I want to show a quick demo",
    "start": "1062340",
    "end": "1067399"
  },
  {
    "text": "let's see if this car it's actually HD so yeah this is the Prometheus which is",
    "start": "1067399",
    "end": "1074820"
  },
  {
    "text": "running in the namespace as you can see we have the API server the controller manager at CD the scheduler and also the",
    "start": "1074820",
    "end": "1083340"
  },
  {
    "text": "Machine controller I'm a bit too fast for the demo machine controller and we",
    "start": "1083340",
    "end": "1093720"
  },
  {
    "text": "are going to focus on that just as an example okay so in this Prometheus which",
    "start": "1093720",
    "end": "1104610"
  },
  {
    "text": "is running in the namespace we have this metric and as you can see we are currently running three notes in our",
    "start": "1104610",
    "end": "1109889"
  },
  {
    "text": "cluster and that's basically just to show that this metric is available and yeah this is like my private cluster ID",
    "start": "1109889",
    "end": "1116850"
  },
  {
    "text": "so we are going to see that across all different Prometheus's so this is one",
    "start": "1116850",
    "end": "1122669"
  },
  {
    "text": "level above this is running in the data center and we are again like scraping every service that is running in there",
    "start": "1122669",
    "end": "1129950"
  },
  {
    "text": "so yeah I like the couplets and so on",
    "start": "1129950",
    "end": "1134658"
  },
  {
    "text": "and then again we are also scraping the Prometheus that are running inside the",
    "start": "1136399",
    "end": "1142320"
  },
  {
    "text": "each namespace so this is the Federation approach right there and this my my",
    "start": "1142320",
    "end": "1150360"
  },
  {
    "text": "Prometheus for my class at that we looked into earlier so again I can specify the namespace please give me the",
    "start": "1150360",
    "end": "1157470"
  },
  {
    "text": "number of notes that we are using in in this cluster and yeah we get the same",
    "start": "1157470",
    "end": "1162749"
  },
  {
    "text": "metric again so this is now the Prometheus which is running at the",
    "start": "1162749",
    "end": "1169080"
  },
  {
    "text": "company level so at the very top and this one is federating into the",
    "start": "1169080",
    "end": "1174600"
  },
  {
    "text": "different data centers and this example we have to yeah and this only fetches",
    "start": "1174600",
    "end": "1181590"
  },
  {
    "text": "the Machine controller and some other very specific metrics that we actually care about and this is like the park",
    "start": "1181590",
    "end": "1188999"
  },
  {
    "text": "where the sticky sessions come in and it's not that really nice but it kind of works but yeah there are better",
    "start": "1188999",
    "end": "1195240"
  },
  {
    "text": "solutions",
    "start": "1195240",
    "end": "1197480"
  },
  {
    "text": "yeah as you can see the retention on",
    "start": "1201350",
    "end": "1208440"
  },
  {
    "text": "that prometheus is only one day and as I said before on that very top level we want to have a very long retention to to",
    "start": "1208440",
    "end": "1215700"
  },
  {
    "text": "see what's going on for the last half year or something like that and this actually were a new project from the",
    "start": "1215700",
    "end": "1223680"
  },
  {
    "text": "core maintain as fabian is involved he is running a project called tonneau snow",
    "start": "1223680",
    "end": "1229580"
  },
  {
    "text": "and Frederic is going to tell us a bit more about that in the next slides so",
    "start": "1229580",
    "end": "1236790"
  },
  {
    "text": "there's like an overview of everything that is going on inside our cluster or between the different data centers and",
    "start": "1236790",
    "end": "1245910"
  },
  {
    "text": "as you can see even though in the Prometheus itself we only have a retention of one day with tunnels we we",
    "start": "1245910",
    "end": "1253020"
  },
  {
    "text": "are able to get a retention of four weeks now which is kind of nice and there's like we could do that for month",
    "start": "1253020",
    "end": "1261529"
  },
  {
    "text": "oops so yeah the future as I said there's the",
    "start": "1263840",
    "end": "1271710"
  },
  {
    "text": "problem with the sticky sessions and yeah Frederick is going to tell you a bit more about like what's coming next",
    "start": "1271710",
    "end": "1278190"
  },
  {
    "text": "yeah so as much he has already said there are there are a couple of problems with with Federation one is that you",
    "start": "1278190",
    "end": "1285600"
  },
  {
    "text": "when you use sticky sessions that you you must use sticky sessions in order to be able to always request the same",
    "start": "1285600",
    "end": "1293960"
  },
  {
    "text": "Prometheus server underneath that's it like an H a problem with Prometheus but",
    "start": "1293960",
    "end": "1300090"
  },
  {
    "text": "also one of the most asked questions and in fact we had a Prometheus deep dive",
    "start": "1300090",
    "end": "1305150"
  },
  {
    "text": "yesterday and of course and this is something we were prepared for the",
    "start": "1305150",
    "end": "1311250"
  },
  {
    "text": "question of long-term storage came up and this is something that has been has",
    "start": "1311250",
    "end": "1317340"
  },
  {
    "text": "been a question in the Prometheus community for quite some time and probably some of you are asking",
    "start": "1317340",
    "end": "1322400"
  },
  {
    "text": "themselves as well and Thomas is just one of the solutions out there but we",
    "start": "1322400",
    "end": "1329730"
  },
  {
    "text": "think that particularly for this use case it would make sense to use something like tano's just to",
    "start": "1329730",
    "end": "1335579"
  },
  {
    "text": "mention the other solutions there's also we've cortex which is more of more more built to be like a SAS offering so that",
    "start": "1335579",
    "end": "1341909"
  },
  {
    "text": "some company runs it and we think that tano's really works really well for like",
    "start": "1341909",
    "end": "1347999"
  },
  {
    "text": "an on-premise solution so the goals that tano's wants to solve and just to make",
    "start": "1347999",
    "end": "1355799"
  },
  {
    "text": "sure fabian worked on this together with someone from improbable and this project",
    "start": "1355799",
    "end": "1363509"
  },
  {
    "text": "actually is under the improbable organization under github so there are",
    "start": "1363509",
    "end": "1369659"
  },
  {
    "text": "the two people that primarily created this this project so sadly fabian is not",
    "start": "1369659",
    "end": "1377249"
  },
  {
    "text": "in this talk to represent this but we think it's really awesome and we're",
    "start": "1377249",
    "end": "1382559"
  },
  {
    "text": "thankful for them for building this and really that the things that we want to",
    "start": "1382559",
    "end": "1388649"
  },
  {
    "text": "solve with this is a long-term storage we want to be able to view our entire infrastructure globally at real-time we",
    "start": "1388649",
    "end": "1397379"
  },
  {
    "text": "want to have down sampling so that when we do queries over a year we want to get",
    "start": "1397379",
    "end": "1403379"
  },
  {
    "text": "a good idea of of the actual data but we don't need to actually query 15-second",
    "start": "1403379",
    "end": "1409379"
  },
  {
    "text": "granny granularity so something like one hour averages or whatever actually it's good",
    "start": "1409379",
    "end": "1415919"
  },
  {
    "text": "enough and then when we zoom in we actually get higher resolution and tannaz does exactly this and how this is",
    "start": "1415919",
    "end": "1422729"
  },
  {
    "text": "built maybe you've seen fav Ian's keynote he explained how the new",
    "start": "1422729",
    "end": "1429329"
  },
  {
    "text": "Prometheus storage that we that was built for Prometheus 2.0 that is exactly",
    "start": "1429329",
    "end": "1435119"
  },
  {
    "text": "what is the core of this project basically what Prometheus uses on disk now is just mapped into a remote remote",
    "start": "1435119",
    "end": "1443579"
  },
  {
    "text": "storage and really how this works is that we run vanilla kubernetes vanilla",
    "start": "1443579",
    "end": "1449219"
  },
  {
    "text": "prometheus always mixes though but apparently other",
    "start": "1449219",
    "end": "1454619"
  },
  {
    "text": "people have problems with this as well so you run Prometheus unmodified no fork",
    "start": "1454619",
    "end": "1460259"
  },
  {
    "text": "no nothing you run a sidecar next to it and this sidecar then over time take",
    "start": "1460259",
    "end": "1467320"
  },
  {
    "text": "the data that Prometheus built so basically the it really basically uploads the database of Prometheus into",
    "start": "1467320",
    "end": "1475030"
  },
  {
    "text": "object storage and then makes us available to the tano's crew layer and",
    "start": "1475030",
    "end": "1480820"
  },
  {
    "text": "the courier then just goes ahead and crease this data now if you know how the",
    "start": "1480820",
    "end": "1488020"
  },
  {
    "text": "Prometheus storage works you know that the actually persistent data on disk",
    "start": "1488020",
    "end": "1493440"
  },
  {
    "text": "like the immutable chunks are only written at a by default to our interval",
    "start": "1493440",
    "end": "1500980"
  },
  {
    "text": "and so if you if we just had a setup like this we could only scribe read all",
    "start": "1500980",
    "end": "1507400"
  },
  {
    "text": "of the data that is at least two hours old so in addition to that the courier",
    "start": "1507400",
    "end": "1513160"
  },
  {
    "text": "can also go out and query all the life Prometheus servers too and then merge",
    "start": "1513160",
    "end": "1518800"
  },
  {
    "text": "all of this this data together and then have the global view over long-term storage and active servers and this is",
    "start": "1518800",
    "end": "1525700"
  },
  {
    "text": "really neat because in the past we wanted to create Prometheus servers that have really large disks so that we could",
    "start": "1525700",
    "end": "1533860"
  },
  {
    "text": "have long-term storage on like normal hard disk or large SSDs or something",
    "start": "1533860",
    "end": "1539230"
  },
  {
    "text": "like that but it's not really necessary here anymore we can have small local disks and just ship off into long-term",
    "start": "1539230",
    "end": "1546190"
  },
  {
    "text": "storage and have whatever is needed for alerting purposes local to the Prometheus server because in this model",
    "start": "1546190",
    "end": "1552580"
  },
  {
    "text": "will we keep all the reliability in the alerting mechanisms that we know and",
    "start": "1552580",
    "end": "1557890"
  },
  {
    "text": "love from Prometheus which is that the alerting happens exactly where we ingest our information and then is sent to",
    "start": "1557890",
    "end": "1565420"
  },
  {
    "text": "alert manager like we used to and the problem that matthias mentioned about",
    "start": "1565420",
    "end": "1572140"
  },
  {
    "text": "the j problem so whenever we accessed Prometheus in this model and we had Prometheus H a because we we want to be",
    "start": "1572140",
    "end": "1580330"
  },
  {
    "text": "reliable the problem was that we never knew which Prometheus should we be requesting and maybe if we were flapping",
    "start": "1580330",
    "end": "1587070"
  },
  {
    "text": "because Prometheus may not be exactly synchronized when you do scrapes so even",
    "start": "1587070",
    "end": "1592300"
  },
  {
    "text": "though they are configured to do scrapes every 15 seconds they might not do this",
    "start": "1592300",
    "end": "1598540"
  },
  {
    "text": "at the exact same time so cow for example or really any other metric may not be exactly synchronized for",
    "start": "1598540",
    "end": "1604660"
  },
  {
    "text": "alerting purposes this is fine because they're good and similar enough so that they produce the same alerts but in per",
    "start": "1604660",
    "end": "1613000"
  },
  {
    "text": "dashboarding this is strange because every time when you refresh you get a different graph so this is something",
    "start": "1613000",
    "end": "1618520"
  },
  {
    "text": "that tonneaus also solves and there are some heuristics that it applies in order",
    "start": "1618520",
    "end": "1623650"
  },
  {
    "text": "to merge this data but then you can also say that you want to disable this in",
    "start": "1623650",
    "end": "1628840"
  },
  {
    "text": "case you want to say I want to see exactly the data from this replica and",
    "start": "1628840",
    "end": "1634410"
  },
  {
    "text": "I'm happy to say that we have recently implemented all necessities for Thanos",
    "start": "1634410",
    "end": "1640840"
  },
  {
    "text": "to be run with the Prometheus operator this was released last week and with this we really have close to",
    "start": "1640840",
    "end": "1648490"
  },
  {
    "text": "close the gap of being able to declaratively monitor your entire",
    "start": "1648490",
    "end": "1654880"
  },
  {
    "text": "infrastructure and have global view long-term storage and do all of this declaratively",
    "start": "1654880",
    "end": "1660240"
  },
  {
    "text": "thank you [Applause]",
    "start": "1660240",
    "end": "1665970"
  },
  {
    "text": "any questions yes so the question was do",
    "start": "1669139",
    "end": "1682769"
  },
  {
    "text": "we have plans to make blackbox probes first-class citizen in the prometheus",
    "start": "1682769",
    "end": "1688529"
  },
  {
    "text": "operator yes there is an issue for this we want to do it we just haven't been able to get to it but as it is something",
    "start": "1688529",
    "end": "1694769"
  },
  {
    "text": "that we would like to do any other questions yes so the question was with",
    "start": "1694769",
    "end": "1711659"
  },
  {
    "text": "the federated setup when you have the top level Prometheus how do you attend a Kate the federated setup maybe you can",
    "start": "1711659",
    "end": "1719399"
  },
  {
    "text": "answer it yeah so basically right now this is a basic of authentication going",
    "start": "1719399",
    "end": "1725789"
  },
  {
    "text": "into the cluster the data center and then from their own it's like everything is normal again basic over HTTP yeah but",
    "start": "1725789",
    "end": "1734700"
  },
  {
    "text": "Prometheus is configurable in the way that it scrapes so it can also do a client a TLS client certificate or",
    "start": "1734700",
    "end": "1741480"
  },
  {
    "text": "something like that so that that would probably also be a good option that was",
    "start": "1741480",
    "end": "1747200"
  },
  {
    "text": "so we want to do that on the per namespace level just because there might",
    "start": "1754249",
    "end": "1760499"
  },
  {
    "text": "be the case or there's actually the case of running hundreds of clusters inside one kubernetes cluster again and we just",
    "start": "1760499",
    "end": "1769679"
  },
  {
    "text": "want to have like the very raw data right next to the cluster itself and",
    "start": "1769679",
    "end": "1775049"
  },
  {
    "text": "only like federal rate the data that is really important what level of",
    "start": "1775049",
    "end": "1782330"
  },
  {
    "text": "okay any other questions so in terms of",
    "start": "1784700",
    "end": "1801109"
  },
  {
    "text": "the the API objects that we that you have here you have the Prometheus object which has a selector for the service",
    "start": "1801109",
    "end": "1808039"
  },
  {
    "text": "monitors it should select and it can do this within the names within the same namespace as the Prometheus object as",
    "start": "1808039",
    "end": "1814580"
  },
  {
    "text": "well as a namespace selector so you can select the namespaces from which you want to select service monitors from so",
    "start": "1814580",
    "end": "1828470"
  },
  {
    "text": "the point of why we do this is so that users who want to monitor their",
    "start": "1828470",
    "end": "1834830"
  },
  {
    "text": "application can set can specify how to grab those metrics but don't need to",
    "start": "1834830",
    "end": "1840590"
  },
  {
    "text": "care about running the Prometheus server itself so that can be something that an infrastructure team offers to the rest",
    "start": "1840590",
    "end": "1847429"
  },
  {
    "text": "of their organization then they just need to say these are the this is how you monitor my thing I think you were",
    "start": "1847429",
    "end": "1858169"
  },
  {
    "text": "first",
    "start": "1858169",
    "end": "1860378"
  },
  {
    "text": "okay so the question is have we thought of writing a graph on ax operator",
    "start": "1870380",
    "end": "1877680"
  },
  {
    "text": "basically yes we have thought of that and actually one of the first issues that we had on the Prometheus operator",
    "start": "1877680",
    "end": "1884400"
  },
  {
    "text": "was to have graph an ax tightly integrated into the Prometheus operator we decided against this because with the",
    "start": "1884400",
    "end": "1890250"
  },
  {
    "text": "new model of how graph Ana works it's actually super simple to do this without",
    "start": "1890250",
    "end": "1897090"
  },
  {
    "text": "an operator there's no really there's no stateful application that we're managing here that needs extra care the only the",
    "start": "1897090",
    "end": "1903960"
  },
  {
    "text": "only way the only reason why I would maybe want to do this is for exactly the same across namespace selection so that",
    "start": "1903960",
    "end": "1910740"
  },
  {
    "text": "would be a valid use case for that we haven't we don't we're not planning to",
    "start": "1910740",
    "end": "1917460"
  },
  {
    "text": "do this ourselves but we would love to to see the community do this yes",
    "start": "1917460",
    "end": "1923010"
  },
  {
    "text": "any other questions yes",
    "start": "1923010",
    "end": "1929030"
  },
  {
    "text": "okay can you repeat so the question is",
    "start": "1942260",
    "end": "1950210"
  },
  {
    "text": "every time we change the configuration the sidecar goes out and reloads it or",
    "start": "1950210",
    "end": "1955860"
  },
  {
    "text": "grabs the so the the prometheus operator already makes sure that only actual",
    "start": "1955860",
    "end": "1962130"
  },
  {
    "text": "changes get propagated down to the Prometheus so only if you actually check",
    "start": "1962130",
    "end": "1968730"
  },
  {
    "text": "if someone if someone updates the service monitor object for example and doesn't actually doesn't actually change",
    "start": "1968730",
    "end": "1974279"
  },
  {
    "text": "anything or doesn't change something that actually influences the Prometheus object then nothing happens is that was",
    "start": "1974279",
    "end": "1982049"
  },
  {
    "text": "that does that answer your question or okay if not we can take this also offline and discuss more any other",
    "start": "1982049",
    "end": "1988320"
  },
  {
    "text": "questions we have like one minute left yes",
    "start": "1988320",
    "end": "1993919"
  },
  {
    "text": "so a time-series in prometheus is uniquely identified by its set of labels",
    "start": "2004730",
    "end": "2011640"
  },
  {
    "text": "right and then Thanos a requirement for every Prometheus server is that every",
    "start": "2011640",
    "end": "2018210"
  },
  {
    "text": "Prometheus server has a set of external labels configured and actually created a",
    "start": "2018210",
    "end": "2023309"
  },
  {
    "text": "pull request to make tonneaus fail entirely like like error out of it the",
    "start": "2023309",
    "end": "2030179"
  },
  {
    "text": "external labels it once wants to register itself with are not unique so",
    "start": "2030179",
    "end": "2035750"
  },
  {
    "text": "yeah there's that there's definitely a unique time series at this point I think",
    "start": "2035750",
    "end": "2044070"
  },
  {
    "text": "okay 50 seconds which one was first okay",
    "start": "2044070",
    "end": "2049320"
  },
  {
    "text": "you yeah",
    "start": "2049320",
    "end": "2053450"
  },
  {
    "text": "yes it's funny that you mention this because this is exactly what Tom's talk",
    "start": "2083980",
    "end": "2089929"
  },
  {
    "text": "was about so I recommend just checking out his his targets about kubernetes",
    "start": "2089929",
    "end": "2095358"
  },
  {
    "text": "monitoring make sense yeah ok we're out of time out of time",
    "start": "2095359",
    "end": "2101180"
  },
  {
    "text": "just quickly want to mention if you want to test on us it's just a sidecar it doesn't like affect your Prometheus",
    "start": "2101180",
    "end": "2106430"
  },
  {
    "text": "itself so you can just grab it and go for it ok thank you thanks [Applause]",
    "start": "2106430",
    "end": "2114849"
  }
]