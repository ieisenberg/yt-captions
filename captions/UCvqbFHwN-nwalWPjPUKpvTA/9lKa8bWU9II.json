[
  {
    "text": "K uh we came from Japan and today we are going to talk about the uh new Alpha",
    "start": "160",
    "end": "6240"
  },
  {
    "text": "feature called um in place resources sites uh so among the attendees uh have",
    "start": "6240",
    "end": "12480"
  },
  {
    "text": "you ever heard about this feature okay good number so over half of",
    "start": "12480",
    "end": "19439"
  },
  {
    "text": "the people will learn about what it is um yeah let me start with self",
    "start": "19439",
    "end": "27039"
  },
  {
    "text": "introduction my name is co and I work at Apple as a CIF field engineer and I work",
    "start": "27039",
    "end": "33200"
  },
  {
    "text": "on open source and um yeah um my focus right now is a cloud native",
    "start": "33200",
    "end": "40079"
  },
  {
    "text": "Technologies hi I'm aawa I'm working at CL natics and working on ph Ops to like",
    "start": "40200",
    "end": "47280"
  },
  {
    "text": "Auto scatterer and automate kubernetes operations",
    "start": "47280",
    "end": "53199"
  },
  {
    "text": "thanks so how do we set the right po race resource resource management is key",
    "start": "56239",
    "end": "62760"
  },
  {
    "text": "to smove the sing of maintaining kubernetes clusters both resource can be configured",
    "start": "62760",
    "end": "70200"
  },
  {
    "text": "using request and limit request means minimum resource requirement and limit",
    "start": "70200",
    "end": "76119"
  },
  {
    "text": "Capa resour it seems like to simple Fields but there are lots of complexity",
    "start": "76119",
    "end": "82159"
  },
  {
    "text": "and the food to maintaining KU Crosser effectively it's important for us to",
    "start": "82159",
    "end": "89720"
  },
  {
    "text": "understand them this is a rough diagram of pot",
    "start": "89720",
    "end": "95280"
  },
  {
    "text": "creation we'll look into the detail later so you don't need to understand",
    "start": "95280",
    "end": "100360"
  },
  {
    "text": "everything here but as you can see resource request and limit are used in",
    "start": "100360",
    "end": "106360"
  },
  {
    "text": "several components like schedule check request and cubert combat request and limit to",
    "start": "106360",
    "end": "114040"
  },
  {
    "text": "container settings and then one time set them to The Container now",
    "start": "114040",
    "end": "120719"
  },
  {
    "text": "let's dive into what happens when we set these",
    "start": "120719",
    "end": "126240"
  },
  {
    "text": "values at first let's think about scheduling scheduler uses resource",
    "start": "126320",
    "end": "133160"
  },
  {
    "text": "request to check if the part fits the node as the left diagram shows if the",
    "start": "133160",
    "end": "140239"
  },
  {
    "text": "poort request satisfy the available resource the scheduler assigns the to",
    "start": "140239",
    "end": "145560"
  },
  {
    "text": "the node on the other hand the schuer does not con resource limit and actual",
    "start": "145560",
    "end": "151959"
  },
  {
    "text": "usage but actually pot can use resource up to the limit so CPU and memory can be",
    "start": "151959",
    "end": "160400"
  },
  {
    "text": "overcommitted as shown in the right diagram pot is allocated regardless of",
    "start": "160400",
    "end": "166319"
  },
  {
    "text": "its resource limits next let's explore what happens",
    "start": "166319",
    "end": "173959"
  },
  {
    "text": "when a running container hits a limit as you can see from the left",
    "start": "173959",
    "end": "179319"
  },
  {
    "text": "diagram if a container goes over its memory limits it's cured by the O killer as for",
    "start": "179319",
    "end": "187120"
  },
  {
    "text": "CPU you know it's a Time stress resource if a container exceed its limit it will",
    "start": "187120",
    "end": "194040"
  },
  {
    "text": "be throttled and the Cod period ends of course it allows to resume in next",
    "start": "194040",
    "end": "200840"
  },
  {
    "text": "quarter period but this DeRay leads to a performance",
    "start": "200840",
    "end": "206560"
  },
  {
    "text": "delegation keep in mind reses can be committed this means if other container",
    "start": "206560",
    "end": "213560"
  },
  {
    "text": "on the same node have already used up node resource your container run into",
    "start": "213560",
    "end": "219599"
  },
  {
    "text": "the same station even before reaching its own",
    "start": "219599",
    "end": "225560"
  },
  {
    "text": "limit now let's consider results of entire node if a node runs out of memory",
    "start": "226599",
    "end": "234360"
  },
  {
    "text": "the O killer kills the process with the highest om score the cubert adjust disc",
    "start": "234360",
    "end": "241079"
  },
  {
    "text": "score based on the PO quality of service we usually called qos",
    "start": "241079",
    "end": "246920"
  },
  {
    "text": "class as you can see from the Manifest in the upper right corner you can find its Qs class in the",
    "start": "246920",
    "end": "255079"
  },
  {
    "text": "status field pot curus has three classes the",
    "start": "255079",
    "end": "260919"
  },
  {
    "text": "classification is based on how pot resour are",
    "start": "260919",
    "end": "266000"
  },
  {
    "text": "configured best effort is a class with the highest room score if a pot has no",
    "start": "266000",
    "end": "272639"
  },
  {
    "text": "request and limit for all containers it's classified in this",
    "start": "272639",
    "end": "277759"
  },
  {
    "text": "class this the next possible is class that use when at least one container has",
    "start": "277759",
    "end": "284360"
  },
  {
    "text": "a request limit is optional the last guaranteed is class that used when all",
    "start": "284360",
    "end": "292080"
  },
  {
    "text": "containers have both requests and limit this way CU adjust the O score",
    "start": "292080",
    "end": "299120"
  },
  {
    "text": "based on the Cur class when no goes home the um killer kills the best effort P",
    "start": "299120",
    "end": "308600"
  },
  {
    "text": "first next we move on to the relationship between resource request",
    "start": "309919",
    "end": "316000"
  },
  {
    "text": "and put eviction during no pressure once um happens the workload is",
    "start": "316000",
    "end": "323639"
  },
  {
    "text": "affected and it's making the backing difficult so we wanted to avoid um as",
    "start": "323639",
    "end": "329240"
  },
  {
    "text": "much as possible cuet provides such a feature to make results available by",
    "start": "329240",
    "end": "337919"
  },
  {
    "text": "eviction when memory usage exceed the eviction threshold uh this threshold is",
    "start": "337919",
    "end": "344080"
  },
  {
    "text": "one of calent settings cuet AIG to make node resource",
    "start": "344080",
    "end": "350520"
  },
  {
    "text": "reusable so how does CET decide order for a big",
    "start": "350520",
    "end": "356039"
  },
  {
    "text": "import it depends on the request you think stage and",
    "start": "356039",
    "end": "361600"
  },
  {
    "text": "priority taking this diagram as an example cuet select port in order from",
    "start": "361600",
    "end": "368800"
  },
  {
    "text": "right to left the rightmost sport is to be evicted for because usage is higher than",
    "start": "368800",
    "end": "376840"
  },
  {
    "text": "the request then looking at the middle poort the poort with the lower priority comes",
    "start": "376840",
    "end": "385880"
  },
  {
    "text": "next if the priority are the same precedence is based on gaps between",
    "start": "385880",
    "end": "392599"
  },
  {
    "text": "request and usage the left mport will be a biged at",
    "start": "392599",
    "end": "398680"
  },
  {
    "text": "the end let's take a moment to reflect on",
    "start": "398680",
    "end": "407440"
  },
  {
    "text": "the importance of pot Resource Management so what can go wrong if we",
    "start": "407440",
    "end": "414160"
  },
  {
    "text": "set wrong Valu consider this we set to low r",
    "start": "414160",
    "end": "420160"
  },
  {
    "text": "it could damage to service performance and availability overcoming is useful to",
    "start": "420160",
    "end": "426800"
  },
  {
    "text": "cover temporary spikes but if ports concentr compete for resour with assets",
    "start": "426800",
    "end": "433440"
  },
  {
    "text": "it's R to freaking evictions and own conversely setting too much resource",
    "start": "433440",
    "end": "441039"
  },
  {
    "text": "Beyond Deport actually needs is also a problem Computing resour are not free so",
    "start": "441039",
    "end": "448599"
  },
  {
    "text": "this is a waste of cost of course risk setting affect the",
    "start": "448599",
    "end": "456000"
  },
  {
    "text": "entire Crosset not just individual pots taking cross the auto skater as an",
    "start": "456000",
    "end": "462639"
  },
  {
    "text": "example it also relies on pot request to make decision about clusters carrying up",
    "start": "462639",
    "end": "469479"
  },
  {
    "text": "and down if you do not set the right amount of Racor the entire cluster will not",
    "start": "469479",
    "end": "477360"
  },
  {
    "text": "scale properly there before allocating resource actually needed no more no less",
    "start": "477360",
    "end": "484479"
  },
  {
    "text": "is important so how do we set the right",
    "start": "484479",
    "end": "491599"
  },
  {
    "text": "value the key is to monitor the actual resource consumption understand usage PN and then",
    "start": "491599",
    "end": "499800"
  },
  {
    "text": "adjust the setting to mat these observations bear in mind the setting",
    "start": "499800",
    "end": "506919"
  },
  {
    "text": "request is not a onetime t ask let's say a new feature is added or your service",
    "start": "506919",
    "end": "514719"
  },
  {
    "text": "grows resource usage may change so we need to continuously adjust",
    "start": "514719",
    "end": "522080"
  },
  {
    "text": "them furthermore we are typically managing multiple workows it takes some",
    "start": "522080",
    "end": "528440"
  },
  {
    "text": "real effort to handle for all of them so is there a better way to do",
    "start": "528440",
    "end": "537680"
  },
  {
    "text": "this body Auto skater can be your heer in Dearing with",
    "start": "537680",
    "end": "544079"
  },
  {
    "text": "that the vertical po Auto scater we usually called BPA is one of kuate side",
    "start": "544079",
    "end": "551000"
  },
  {
    "text": "project if you write a manifest like this BP will analyze the resource usage",
    "start": "551000",
    "end": "557959"
  },
  {
    "text": "and then provide scaling recommendations supported Works a scaleable objects such as deployment and",
    "start": "557959",
    "end": "565760"
  },
  {
    "text": "stateful sets as the left",
    "start": "565760",
    "end": "572360"
  },
  {
    "text": "manifest BB calculate the recommended value and set it to the Target",
    "start": "572360",
    "end": "578440"
  },
  {
    "text": "fails now you notice other fails with the recommendations but they are many",
    "start": "578440",
    "end": "584839"
  },
  {
    "text": "for refence the first lower Bond means if you set value lower than that the",
    "start": "584839",
    "end": "592839"
  },
  {
    "text": "container made damage performance and availability compulsary setting a value",
    "start": "592839",
    "end": "599440"
  },
  {
    "text": "higher than upper bound means it's likely to be",
    "start": "599440",
    "end": "604680"
  },
  {
    "text": "wasted Target is a recommendation for resource request and BPS had it into the",
    "start": "604680",
    "end": "611320"
  },
  {
    "text": "part request fils in this example the CPU will be 72",
    "start": "611320",
    "end": "617959"
  },
  {
    "text": "M core and the memory will be about 230",
    "start": "617959",
    "end": "623959"
  },
  {
    "text": "megabytes now what about limit BP keeps the original ratio between request and",
    "start": "623959",
    "end": "630920"
  },
  {
    "text": "limit looking at this memory example the original ratio of request to limit is",
    "start": "630920",
    "end": "638320"
  },
  {
    "text": "one to one so BPS set the same value to",
    "start": "638320",
    "end": "643839"
  },
  {
    "text": "limit so how does VP estimate the",
    "start": "645160",
    "end": "650360"
  },
  {
    "text": "recommendations periodically VP collect container medies and add them to the",
    "start": "650360",
    "end": "657120"
  },
  {
    "text": "histograms here we use what's called a half decade histogram this means after",
    "start": "657120",
    "end": "663920"
  },
  {
    "text": "24 hours the samples weight is half why do",
    "start": "663920",
    "end": "669880"
  },
  {
    "text": "this by giving more weight to the new a dator we can quickly adapt to changes",
    "start": "669880",
    "end": "675600"
  },
  {
    "text": "and resource usage Trends also BPA handles Mets differently",
    "start": "675600",
    "end": "681600"
  },
  {
    "text": "for each resource type let's start of seu here things are pretty",
    "start": "681600",
    "end": "688240"
  },
  {
    "text": "straightforward each metric sample directory get placed into the Matched",
    "start": "688240",
    "end": "694959"
  },
  {
    "text": "bucket as for memory on the other hand find Peak usage within a 5 minute window",
    "start": "694959",
    "end": "701279"
  },
  {
    "text": "and add only the peak value to the bucket additionally BBA has a special",
    "start": "701279",
    "end": "707920"
  },
  {
    "text": "consideration for memory when o happens BBA does not just",
    "start": "707920",
    "end": "713959"
  },
  {
    "text": "thrw it to the packet but multipos the last usage by 1 Point 2 yeah 1.2 is a",
    "start": "713959",
    "end": "722040"
  },
  {
    "text": "default so it depends on your settings but in this way BPA Cates the",
    "start": "722040",
    "end": "728839"
  },
  {
    "text": "recommendation considering actual usage and own",
    "start": "728839",
    "end": "735040"
  },
  {
    "text": "events next let's look at the relationship between horizontal po Auto",
    "start": "736839",
    "end": "742880"
  },
  {
    "text": "scare and vertical P Auto scare there are two different method for",
    "start": "742880",
    "end": "748160"
  },
  {
    "text": "poort auto scaring many of you are likely familiar with HPA",
    "start": "748160",
    "end": "753800"
  },
  {
    "text": "as it's a standard AP object it's sched horizontally by",
    "start": "753800",
    "end": "759680"
  },
  {
    "text": "adjusting number of reptical on the other hand as we've explored already",
    "start": "759680",
    "end": "764959"
  },
  {
    "text": "webs SC Bally by adjusting the poort resour",
    "start": "764959",
    "end": "770120"
  },
  {
    "text": "themselves one thing to note is that the current open source version of BP cannot",
    "start": "770120",
    "end": "776880"
  },
  {
    "text": "use together with HPA on the same metrix let's consider The Fad",
    "start": "776880",
    "end": "783720"
  },
  {
    "text": "scenario when the resource usage exceed its threshold HBA creates a new replica",
    "start": "783720",
    "end": "791279"
  },
  {
    "text": "at that time the new replica is still low utilized then BPF finds that the",
    "start": "791279",
    "end": "798800"
  },
  {
    "text": "utilization is low and scale down the pot if this kind of cycle is repeated",
    "start": "798800",
    "end": "807079"
  },
  {
    "text": "eventually many of these small P will be created therefore please use custom",
    "start": "807079",
    "end": "814519"
  },
  {
    "text": "Matrix for HP instead of CPU or memory until HPA integration is",
    "start": "814519",
    "end": "823079"
  },
  {
    "text": "introduced next let's move on to the update mode there are three distinct mode to",
    "start": "825199",
    "end": "832519"
  },
  {
    "text": "apply recommendation to the part let's break them down for up",
    "start": "832519",
    "end": "839759"
  },
  {
    "text": "off mode is a hands off approach it Cates recommendation but not making any",
    "start": "839759",
    "end": "846600"
  },
  {
    "text": "changes on the pot this mod is Greatful those who wanted to make man",
    "start": "846600",
    "end": "854000"
  },
  {
    "text": "adjustments next there's initial the mod the recommendation only at the part",
    "start": "854000",
    "end": "860759"
  },
  {
    "text": "creation time this many comes when you're changing the replic",
    "start": "860759",
    "end": "866440"
  },
  {
    "text": "counts the third recreate is most proactive approach not only applies to",
    "start": "866440",
    "end": "873519"
  },
  {
    "text": "the New Port but also evicting and recreate running Parts it's to good when",
    "start": "873519",
    "end": "880040"
  },
  {
    "text": "requests do not align with the recommended values and last order mode currently it",
    "start": "880040",
    "end": "887800"
  },
  {
    "text": "works same as",
    "start": "887800",
    "end": "890880"
  },
  {
    "text": "recreate let's do in recreate mode a bit more take a look at this graph",
    "start": "893000",
    "end": "899839"
  },
  {
    "text": "the Blue Line shows resource usage while the Orange Line the recommended",
    "start": "899839",
    "end": "906160"
  },
  {
    "text": "values liquer mode can be well optimized for many applications of course pot will",
    "start": "906160",
    "end": "914839"
  },
  {
    "text": "be disrupted by addiction to apply a new recommendation that poort disruption",
    "start": "914839",
    "end": "920920"
  },
  {
    "text": "budget we call pdb can mitigate the impact on service",
    "start": "920920",
    "end": "926240"
  },
  {
    "text": "availability pdb can limit h many poort will be disrupted at the same",
    "start": "926240",
    "end": "932600"
  },
  {
    "text": "time therefore the combination of recreate mode and pdb allows resource",
    "start": "932600",
    "end": "939519"
  },
  {
    "text": "optimization with that sacrifying service availability but here's a catch recreate",
    "start": "939519",
    "end": "947800"
  },
  {
    "text": "mode doesn't for all workflows consider this some worker",
    "start": "947800",
    "end": "954600"
  },
  {
    "text": "fractu in usage only during sudden period their life",
    "start": "954600",
    "end": "959639"
  },
  {
    "text": "cycle as you can see from the right",
    "start": "959639",
    "end": "964399"
  },
  {
    "text": "graphs the cre mode could not truly optimize in this",
    "start": "964920",
    "end": "971079"
  },
  {
    "text": "case another example is cases where even a brief disruption that has a serious",
    "start": "971079",
    "end": "978240"
  },
  {
    "text": "impact on a varability and cost like ml long running jobs that can be posed and",
    "start": "978240",
    "end": "985600"
  },
  {
    "text": "resume without consequences so how do we manage this kind of",
    "start": "985600",
    "end": "994759"
  },
  {
    "text": "workflows so here's the thing we want to bring and introduce here to you a new",
    "start": "998199",
    "end": "1005279"
  },
  {
    "text": "Alpha feature called in place resources sizing for kuet SPS has been available",
    "start": "1005279",
    "end": "1010560"
  },
  {
    "text": "since actually 1.27 this flag has the field called",
    "start": "1010560",
    "end": "1017040"
  },
  {
    "text": "resid policy to pod specs the default mod is recreate and it's basically how",
    "start": "1017040",
    "end": "1024160"
  },
  {
    "text": "current pod behaves when the resource field in the Pod spec has changed the new in place mode enables a",
    "start": "1024160",
    "end": "1032000"
  },
  {
    "text": "pod to modify resource uh resource limit field without Recreation or",
    "start": "1032000",
    "end": "1039360"
  },
  {
    "text": "restart to enable this feature in current kubernets versions We just need to enable the feature gate called in",
    "start": "1040559",
    "end": "1048960"
  },
  {
    "text": "Place vertical scaling in place po vertical scaling feature",
    "start": "1048960",
    "end": "1054320"
  },
  {
    "text": "gate so we will show this uh in the demo that comes after fingers",
    "start": "1054320",
    "end": "1061279"
  },
  {
    "text": "crossed as I mentioned earlier this new feature gate add a new po field resid",
    "start": "1062000",
    "end": "1067919"
  },
  {
    "text": "policy this field takes two parameters resource name and reset policy this this",
    "start": "1067919",
    "end": "1075760"
  },
  {
    "text": "example shows that CPU change Now does not trigger pod restart which means you",
    "start": "1075760",
    "end": "1081480"
  },
  {
    "text": "can resize CP value on this Pod without restart or",
    "start": "1081480",
    "end": "1087440"
  },
  {
    "text": "Recreation restart container also does not trigger pod Recreation but it",
    "start": "1087440",
    "end": "1093360"
  },
  {
    "text": "restarts pod but either way this part will not be created when those resource",
    "start": "1093360",
    "end": "1100280"
  },
  {
    "text": "value are modified so the resource values become became mutable regardless of the PO life",
    "start": "1100280",
    "end": "1107440"
  },
  {
    "text": "cycle this feature is useful for multiple workloads I have presented previously that this can be useful for",
    "start": "1107440",
    "end": "1114640"
  },
  {
    "text": "ML workload but my personal favorite is game Lobby server which clients expect the",
    "start": "1114640",
    "end": "1122400"
  },
  {
    "text": "persistent connection while playing on a multiplayer game you don't want to get connect disconnected while you're",
    "start": "1122400",
    "end": "1128720"
  },
  {
    "text": "playing Apex or um some other um games like Minecraft even so those server can",
    "start": "1128720",
    "end": "1135039"
  },
  {
    "text": "scale up and down with this Alpha feature without",
    "start": "1135039",
    "end": "1140600"
  },
  {
    "text": "disconnection if Reise failed and cuur cannot be executed the pot status becomes INF",
    "start": "1140600",
    "end": "1148880"
  },
  {
    "text": "feasible in such cases this part will not be modified and keeps running with",
    "start": "1148880",
    "end": "1154559"
  },
  {
    "text": "the previous config while the value gets modified let's try to see what happens",
    "start": "1154559",
    "end": "1161400"
  },
  {
    "text": "in the demo let me have a",
    "start": "1161400",
    "end": "1167720"
  },
  {
    "text": "moment",
    "start": "1167720",
    "end": "1170720"
  },
  {
    "text": "uh",
    "start": "1186159",
    "end": "1189159"
  },
  {
    "text": "okay so I'm in the directory uh called cuon",
    "start": "1197640",
    "end": "1203760"
  },
  {
    "text": "EU 2024 demo and uh I have the uh future gate in my",
    "start": "1203760",
    "end": "1212919"
  },
  {
    "text": "uh kind cluster configuration",
    "start": "1212919",
    "end": "1219320"
  },
  {
    "text": "so yeah let's create the C now I hope don't doesn't blow up but",
    "start": "1223720",
    "end": "1232960"
  },
  {
    "text": "yeah okay my control plan is a good",
    "start": "1243799",
    "end": "1248080"
  },
  {
    "text": "boy yeah I got acrosser not yet but",
    "start": "1257600",
    "end": "1265400"
  },
  {
    "text": "sorry okay um this is the Pod spec I share uh this has the um resourceid",
    "start": "1265400",
    "end": "1273279"
  },
  {
    "text": "policy uh yeah and uh request limits and uh resource",
    "start": "1273279",
    "end": "1278880"
  },
  {
    "text": "requests with the one GB of memory and one CPU then let's",
    "start": "1278880",
    "end": "1285100"
  },
  {
    "text": "[Music] see",
    "start": "1285100",
    "end": "1290520"
  },
  {
    "text": "oh okay don't worry I have a plan",
    "start": "1291039",
    "end": "1296960"
  },
  {
    "text": "B oh wait wait wait wait wait",
    "start": "1304679",
    "end": "1311000"
  },
  {
    "text": "yeah okay here we go oh no sorry",
    "start": "1312480",
    "end": "1320520"
  },
  {
    "text": "okay yeah so we have the Pod spec and we create the",
    "start": "1325080",
    "end": "1331559"
  },
  {
    "text": "Pod yeah I knew this going to happen sorry uh then the container is creating",
    "start": "1332320",
    "end": "1337559"
  },
  {
    "text": "here yeah then I saw it's running now",
    "start": "1337559",
    "end": "1344799"
  },
  {
    "text": "yeah then it has the 20 seconds agent noats for",
    "start": "1347520",
    "end": "1353799"
  },
  {
    "text": "now and it has the res res policy",
    "start": "1357000",
    "end": "1362039"
  },
  {
    "text": "inside then CPU is not required and the memory is restart container so if you",
    "start": "1362960",
    "end": "1368080"
  },
  {
    "text": "change the CPU it doesn't change anything just uh change the value in C",
    "start": "1368080",
    "end": "1374720"
  },
  {
    "text": "group yeah so right now it has the resour limit with one CPU",
    "start": "1374720",
    "end": "1380440"
  },
  {
    "text": "yeah uh I just check the secr value inside the Pod it has the One Core",
    "start": "1385960",
    "end": "1391440"
  },
  {
    "text": "CPU and now I apply the patch uh with the new res resource request limit with",
    "start": "1391440",
    "end": "1397480"
  },
  {
    "text": "this number",
    "start": "1397480",
    "end": "1400080"
  },
  {
    "text": "two now I know why he's taking this time okay yeah it's",
    "start": "1406960",
    "end": "1413200"
  },
  {
    "text": "changed and let's see if it's recreated oh yeah before we check the",
    "start": "1413200",
    "end": "1418279"
  },
  {
    "text": "secret",
    "start": "1418279",
    "end": "1420640"
  },
  {
    "text": "body yeah so it doesn't restart and the age is 296 which means it is the same",
    "start": "1424480",
    "end": "1432960"
  },
  {
    "text": "pod now we try something else",
    "start": "1434760",
    "end": "1439840"
  },
  {
    "text": "do you see it's one at 10,000 CPUs do you have 10,000 CPU at home I don't but",
    "start": "1439840",
    "end": "1447240"
  },
  {
    "text": "it's an experiment for the stas invisible which I explained uh this cannot um schedule the new pod I mean",
    "start": "1447240",
    "end": "1454640"
  },
  {
    "text": "new CPU resource so the resource has 10K which means it's modified",
    "start": "1454640",
    "end": "1461480"
  },
  {
    "text": "but in the container status yeah uh",
    "start": "1461480",
    "end": "1469080"
  },
  {
    "text": "where is it yeah the AL resource has not changed CPU Remains",
    "start": "1469080",
    "end": "1475840"
  },
  {
    "text": "Two so it means uh oh there also the status is invisible so this is what it happens",
    "start": "1475840",
    "end": "1483000"
  },
  {
    "text": "when you put too much resources which is over the capacity okay we can go",
    "start": "1483000",
    "end": "1490360"
  },
  {
    "text": "on uh",
    "start": "1491200",
    "end": "1497320"
  },
  {
    "text": "wait I lost my mic mouse",
    "start": "1497320",
    "end": "1501840"
  },
  {
    "text": "Carle oh I lost my present",
    "start": "1514919",
    "end": "1520600"
  },
  {
    "text": "mode well okay I can explain um so this diagram shows what actually happens",
    "start": "1524279",
    "end": "1531120"
  },
  {
    "text": "inside of the kubernets components so uh API server uh takes your pod request",
    "start": "1531120",
    "end": "1538200"
  },
  {
    "text": "like if you cuc apply the new manifest will be stored in your SD and it will be",
    "start": "1538200",
    "end": "1545200"
  },
  {
    "text": "uh go through API server and cubet in each node periodically check the podspec",
    "start": "1545200",
    "end": "1552399"
  },
  {
    "text": "modification from API server then if the change has been detected it goes through",
    "start": "1552399",
    "end": "1558799"
  },
  {
    "text": "uh all the way through CI runtime and OC runtime and eventually uh this new CPU",
    "start": "1558799",
    "end": "1564600"
  },
  {
    "text": "value or memory value will be stored in the cgroup uh",
    "start": "1564600",
    "end": "1570360"
  },
  {
    "text": "value so in order to make this function happen um several things has been",
    "start": "1577080",
    "end": "1583520"
  },
  {
    "text": "implemented uh as the alpha flag feature which is a spec Schuler cuet run",
    "start": "1583520",
    "end": "1591120"
  },
  {
    "text": "time especially run time was a big change for me because this feature",
    "start": "1591120",
    "end": "1596440"
  },
  {
    "text": "relies on C version 2 completely also testing as",
    "start": "1596440",
    "end": "1601919"
  },
  {
    "text": "well but um this is basically the kubernetes component change only that",
    "start": "1601919",
    "end": "1609039"
  },
  {
    "text": "means autoscaling needs to uh implement this feature in a different",
    "start": "1609039",
    "end": "1616679"
  },
  {
    "text": "place so when do we have it actually we really don't know um so yeah we are",
    "start": "1616679",
    "end": "1625320"
  },
  {
    "text": "currently uh uh I am not a developer I'm just introduc but this feature but um the community is trying to make the API",
    "start": "1625320",
    "end": "1633399"
  },
  {
    "text": "stable but um we have a lot of consideration still so we don't know",
    "start": "1633399",
    "end": "1638760"
  },
  {
    "text": "when vpa is eventually get implemented so this is the consideration",
    "start": "1638760",
    "end": "1645600"
  },
  {
    "text": "we have right now in GitHub um",
    "start": "1645600",
    "end": "1651039"
  },
  {
    "text": "so these are examples so current implementation um that we have in Alpha",
    "start": "1651039",
    "end": "1656480"
  },
  {
    "text": "feature it depends on the runtime like container D or cryo then cuet decision",
    "start": "1656480",
    "end": "1662760"
  },
  {
    "text": "on restarting containers but in the proposal uh it has to be in vpa so we",
    "start": "1662760",
    "end": "1670559"
  },
  {
    "text": "have to change the um decision making logic towards there also um",
    "start": "1670559",
    "end": "1679120"
  },
  {
    "text": "even though this is exciting I I I think you understand now how it can be it can",
    "start": "1679120",
    "end": "1685039"
  },
  {
    "text": "resolve many use cases and many problems but this feature doesn't have many",
    "start": "1685039",
    "end": "1691279"
  },
  {
    "text": "attention so more use cases and feedback are wanted from you and all the people",
    "start": "1691279",
    "end": "1697360"
  },
  {
    "text": "who's watching this video on YouTube so uh yeah this is one big concern as well",
    "start": "1697360",
    "end": "1705840"
  },
  {
    "text": "and when it comes to scaling down we really need to be careful about it",
    "start": "1705840",
    "end": "1711360"
  },
  {
    "text": "because if you have this part and slightly shrinking the memory usage but",
    "start": "1711360",
    "end": "1717960"
  },
  {
    "text": "just because that you cannot just scale it down because what if request like a",
    "start": "1717960",
    "end": "1723679"
  },
  {
    "text": "few more request comes on the memory uses like spikes a little bit it brings the oom killer again so it's not that",
    "start": "1723679",
    "end": "1733960"
  },
  {
    "text": "easy yeah so this is the T takeway um as we explained resource management is the",
    "start": "1733960",
    "end": "1741279"
  },
  {
    "text": "key for the cluster and workload resource management and vpa has several",
    "start": "1741279",
    "end": "1748000"
  },
  {
    "text": "components like recommander updator uh to make it uh",
    "start": "1748000",
    "end": "1754679"
  },
  {
    "text": "autonomously and the current resource uh has some um resetting part problem yeah",
    "start": "1754679",
    "end": "1761200"
  },
  {
    "text": "so it can work on many workloads but there are some use cases where it",
    "start": "1761200",
    "end": "1766720"
  },
  {
    "text": "specifically needs no result so inour res can uh solve this",
    "start": "1766720",
    "end": "1774640"
  },
  {
    "text": "problem and if you combine vpa and this in place resource size as we saw um it",
    "start": "1774640",
    "end": "1782480"
  },
  {
    "text": "can um bring more autonomous uh resource management but there are so many things",
    "start": "1782480",
    "end": "1789519"
  },
  {
    "text": "we have to do so if you are interested uh I highly recommend you take on the",
    "start": "1789519",
    "end": "1796000"
  },
  {
    "text": "cap and AP and AP is the um autoscalers",
    "start": "1796000",
    "end": "1801679"
  },
  {
    "text": "uh enhancement proposal uh I share some uh links and the AP kep number in the slides so uh",
    "start": "1801679",
    "end": "1809679"
  },
  {
    "text": "you can show it later yeah that's it um that is our",
    "start": "1809679",
    "end": "1815799"
  },
  {
    "text": "presentation um yeah thank you for",
    "start": "1815799",
    "end": "1820440"
  },
  {
    "text": "[Applause] listening so if you have any question questions um please uh P ping us on",
    "start": "1824040",
    "end": "1831720"
  },
  {
    "text": "slack or email or Twitter or X I don't know X yeah and um feedback means a lot",
    "start": "1831720",
    "end": "1838960"
  },
  {
    "text": "to us so please scan the QR code and press the good or bad button I mean",
    "start": "1838960",
    "end": "1844000"
  },
  {
    "text": "hopefully good but yeah it's up to you please yeah thank you and um if you have",
    "start": "1844000",
    "end": "1850960"
  },
  {
    "text": "any questions uh I think we can do it now but a lot of people are going out so",
    "start": "1850960",
    "end": "1856880"
  },
  {
    "text": "I don't know any",
    "start": "1856880",
    "end": "1863200"
  },
  {
    "text": "questions okay oh actually before that um V is like one of the big contributors on",
    "start": "1863200",
    "end": "1871559"
  },
  {
    "text": "this feature and he's actually here so I want him to come up on the stage and",
    "start": "1871559",
    "end": "1876919"
  },
  {
    "text": "answer the questions with us please come up you don't have to",
    "start": "1876919",
    "end": "1882919"
  },
  {
    "text": "but yeah okay uh let let's keep going the question yeah you can sit there actually",
    "start": "1882919",
    "end": "1890440"
  },
  {
    "text": "yeah okay you come",
    "start": "1890440",
    "end": "1893960"
  },
  {
    "text": "on thank yeah I I was just wondering about like if you",
    "start": "1906480",
    "end": "1912679"
  },
  {
    "text": "have if your cluster is being managed by a tool like flux for example that is",
    "start": "1912679",
    "end": "1918200"
  },
  {
    "text": "constantly reconciling with a G repo then isn't that going how do you solve",
    "start": "1918200",
    "end": "1924440"
  },
  {
    "text": "the conflict that the vpa is trying to constantly adjust the resources within",
    "start": "1924440",
    "end": "1930120"
  },
  {
    "text": "the cluster but then flux will pull the resources that you set in the repo and",
    "start": "1930120",
    "end": "1936279"
  },
  {
    "text": "we'll override them probably I'm asking because I had the",
    "start": "1936279",
    "end": "1942279"
  },
  {
    "text": "same problem with HPA but it was with the amount of replicas not the amount of resources",
    "start": "1942279",
    "end": "1948399"
  },
  {
    "text": "so the question is uh if you set the vpa or HPA uh gups can bring the problem",
    "start": "1948399",
    "end": "1956519"
  },
  {
    "text": "like scaling problem um is it like leic number or the resource like a request",
    "start": "1956519",
    "end": "1961840"
  },
  {
    "text": "limit yeah for example the way I sold it with the HPA you just remove the number",
    "start": "1961840",
    "end": "1967679"
  },
  {
    "text": "of replicas entirely from the deployment okay I can answer that question so don't set any replicas on deployment because",
    "start": "1967679",
    "end": "1974840"
  },
  {
    "text": "that should be taken over by HBA yeah but I'm asking about the vpa how do",
    "start": "1974840",
    "end": "1982760"
  },
  {
    "text": "you remove the resources from the deployment as well like the requests and limits and you just let the vpa take",
    "start": "1982760",
    "end": "1990440"
  },
  {
    "text": "over completely VP changes to BS not to theoy",
    "start": "1990440",
    "end": "1996559"
  },
  {
    "text": "yeah I mean I can try to answer this one so um you normally deploy uh deployments",
    "start": "1996559",
    "end": "2003960"
  },
  {
    "text": "with your GitHub pipeline right uh and uh this just works on the Pod level so",
    "start": "2003960",
    "end": "2010440"
  },
  {
    "text": "probably G giops won't intervene with this one uh you don't directly deploy pods with uh flux or so so it should be",
    "start": "2010440",
    "end": "2018559"
  },
  {
    "text": "just okay oh problem",
    "start": "2018559",
    "end": "2024480"
  },
  {
    "text": "Sol uh there's another one raising hand you want to say something",
    "start": "2024480",
    "end": "2030960"
  },
  {
    "text": "V okay I was just going to say that if you have something else that's going to override what vpa is going to do you'll",
    "start": "2030960",
    "end": "2037200"
  },
  {
    "text": "have to disable you can't have like two two people driving the car it's not going to go anywhere so that is a",
    "start": "2037200",
    "end": "2042960"
  },
  {
    "text": "solution that's a problem that needs to be solved uh at the gitops level because",
    "start": "2042960",
    "end": "2048280"
  },
  {
    "text": "uh really the true Master for this is going to be vpa when it is doing in the auto mode and uh it's going to make the",
    "start": "2048280",
    "end": "2054960"
  },
  {
    "text": "recommendations as it sees fit uh if somebody else is overriding it then uh",
    "start": "2054960",
    "end": "2060320"
  },
  {
    "text": "you know that has to be kind of uh uh disabled that's yeah so what we Tim",
    "start": "2060320",
    "end": "2066839"
  },
  {
    "text": "manage think of sorry yeah what ultimately we manage is the vpa values not the each pod resource size",
    "start": "2066839",
    "end": "2077158"
  },
  {
    "text": "itself uh so from the demo it was clear that it will be possible to change secure",
    "start": "2077320",
    "end": "2083800"
  },
  {
    "text": "resources without restarting pods but what about memory like will it still be required to restart pods to change",
    "start": "2083800",
    "end": "2090079"
  },
  {
    "text": "memory resources because inma the restart policy was to uh restart pods if",
    "start": "2090079",
    "end": "2096480"
  },
  {
    "text": "there are changes to memory resources but we will be possible to not do this the question was memory without",
    "start": "2096480",
    "end": "2102720"
  },
  {
    "text": "restarting yes uh it really depends on the workload I believe like for example some like ml workloads uh you shouldn't",
    "start": "2102720",
    "end": "2110280"
  },
  {
    "text": "restart it so yeah it is U actually possible but for example like jvm you",
    "start": "2110280",
    "end": "2117040"
  },
  {
    "text": "have to specify the Heap size memory which means you have to restart either",
    "start": "2117040",
    "end": "2122520"
  },
  {
    "text": "way thank you just the last question",
    "start": "2122520",
    "end": "2128200"
  },
  {
    "text": "has a bit more of a non-technical question but so the the beginning situation that you talked about about",
    "start": "2128200",
    "end": "2134480"
  },
  {
    "text": "you know like uh this this problem I feel like a lot of companies are dealing with this but not really addressing what",
    "start": "2134480",
    "end": "2140200"
  },
  {
    "text": "was the compelling event for you to kind of start working on it and addressing it",
    "start": "2140200",
    "end": "2145440"
  },
  {
    "text": "uh within your organization or as a project with most people just kind of ignore it and just you know allocate too",
    "start": "2145440",
    "end": "2151280"
  },
  {
    "text": "much and or provision well cost uh like resource utilization and cost management is like",
    "start": "2151280",
    "end": "2157839"
  },
  {
    "text": "one of the biggest motivation for me but I can ask a and v as well about",
    "start": "2157839",
    "end": "2165720"
  },
  {
    "text": "that so yeah uh what you said is right um You can solve this by giving too much",
    "start": "2167640",
    "end": "2174920"
  },
  {
    "text": "memory too much CPU and that'll kick the can down the road but uh eventually",
    "start": "2174920",
    "end": "2180200"
  },
  {
    "text": "you're going to hit the wall of okay how much am I paying what's my cloud bill right and uh in this economy people are",
    "start": "2180200",
    "end": "2186280"
  },
  {
    "text": "starting to watch that so it's starting to take on a little bit of more priority I don't know whether it's going to be",
    "start": "2186280",
    "end": "2192560"
  },
  {
    "text": "it's enough momentum here that it will you know uh have companies put a push on",
    "start": "2192560",
    "end": "2198040"
  },
  {
    "text": "put more resources on this and then uh you know make it get it all the way through so that it can be used but yeah",
    "start": "2198040",
    "end": "2205960"
  },
  {
    "text": "uh there are use cases not enough compelling enough that you know uh enough resources are going to be put on",
    "start": "2205960",
    "end": "2212119"
  },
  {
    "text": "this at this point uh hi",
    "start": "2212119",
    "end": "2218400"
  },
  {
    "text": "uh I am uh D and I am a contributor in signote community I don't have any",
    "start": "2218400",
    "end": "2224920"
  },
  {
    "text": "question it's just an announcement first of all great presentation and thanks Vin for uh actually starting this feature",
    "start": "2224920",
    "end": "2232000"
  },
  {
    "text": "and writing the cap for it so I wanted to say that this feature is in Alpha right now and we have been trying to",
    "start": "2232000",
    "end": "2238400"
  },
  {
    "text": "address the issues uh in the alpha feature for a long time and we want to Pro promote this feature to Beta soonish",
    "start": "2238400",
    "end": "2246240"
  },
  {
    "text": "And we are kind of looking looking for feedback if you have any feedback and if you could try this uh feature out it",
    "start": "2246240",
    "end": "2253280"
  },
  {
    "text": "will really help us uh work on the beta uh functionality of this one and also",
    "start": "2253280",
    "end": "2258880"
  },
  {
    "text": "feel free to uh contribute and attend the maintainance track for signote Community if you're interested in",
    "start": "2258880",
    "end": "2265280"
  },
  {
    "text": "contributing to the fature that's all thank you",
    "start": "2265280",
    "end": "2272640"
  },
  {
    "text": "sorry so I guess the question is over now yeah okay so thank you so much also",
    "start": "2273599",
    "end": "2279960"
  },
  {
    "text": "uh please give another Applause uh to the contributors thank",
    "start": "2279960",
    "end": "2285130"
  },
  {
    "text": "[Applause] you",
    "start": "2285130",
    "end": "2291800"
  }
]