[
  {
    "text": "so uh I think let's get started first of all thank you so much for making it it's the last talk of a very very long week",
    "start": "120",
    "end": "10200"
  },
  {
    "text": "so thank you everyone for making it and I've had I I hope you've had a",
    "start": "10200",
    "end": "17880"
  },
  {
    "text": "nice fun productive successful cubec con if not anything else I hope you made a",
    "start": "17880",
    "end": "22920"
  },
  {
    "text": "new friend so uh my name is madav I I work at VMware um I I'm I'm a technical",
    "start": "22920",
    "end": "30480"
  },
  {
    "text": "lead of s contributor experience I'm a GitHub adman for the kubernetes project and I've worked on the kubernetes",
    "start": "30480",
    "end": "36440"
  },
  {
    "text": "storage layer on some really really fun parts of it uh so because of that I've had to shed many a teer trying to",
    "start": "36440",
    "end": "43280"
  },
  {
    "text": "understand what was going on so hopefully I can peel that onion of abstraction for you without the tears so",
    "start": "43280",
    "end": "49480"
  },
  {
    "text": "let's get started before we start uh just a public call to help we need help migrating",
    "start": "49480",
    "end": "55199"
  },
  {
    "text": "proud jobs to community clusters so if you're a person interested in infrastructure and want to get involved",
    "start": "55199",
    "end": "60640"
  },
  {
    "text": "in the kubernetes community check that link out uh so before we get into like the",
    "start": "60640",
    "end": "67080"
  },
  {
    "text": "core of it let's get a few prerequisites out of the way so we'll take a 50,000 super super high level view of what the",
    "start": "67080",
    "end": "73600"
  },
  {
    "text": "kubernetes machine looks like and how it works we have something called the API server which clients such as cube cutle",
    "start": "73600",
    "end": "80479"
  },
  {
    "text": "interact with and kubernetes being something called a declarative system we declare what we want our intended state",
    "start": "80479",
    "end": "86600"
  },
  {
    "text": "to be so in this case we want a deployment which has three pods and we",
    "start": "86600",
    "end": "91680"
  },
  {
    "text": "specify that as something called replicas so if I do Cube cutle apply with this deployment what happens behind",
    "start": "91680",
    "end": "97439"
  },
  {
    "text": "the scenes uh API server takes this deployment persists it into hcd and also",
    "start": "97439",
    "end": "103479"
  },
  {
    "text": "now we have a bunch of controllers that run on the Clusters these controllers are binaries or the tldr of this is that",
    "start": "103479",
    "end": "110600"
  },
  {
    "text": "they are binaries that look for changes that happen in the cluster State they need to know where we are currently they",
    "start": "110600",
    "end": "116560"
  },
  {
    "text": "need to know where we need to be and then based on these two things the sort of calculate a set of actions to take in",
    "start": "116560",
    "end": "121719"
  },
  {
    "text": "order to get to where we need to be so we have uh I've there are a lot of",
    "start": "121719",
    "end": "126840"
  },
  {
    "text": "controllers in the kubernetes cluster by default but I've specified three of them here we have the scheduler we have the deployment controller and we have the",
    "start": "126840",
    "end": "132879"
  },
  {
    "text": "replica set controller so the deployment controller sees that okay also we have a bunch of nodes in the cluster each node",
    "start": "132879",
    "end": "139440"
  },
  {
    "text": "has a cube letter which also is a controller and each of them talk to API",
    "start": "139440",
    "end": "144640"
  },
  {
    "text": "server no one talks to HD directly everything goes through the API server",
    "start": "144640",
    "end": "149840"
  },
  {
    "text": "so deployment controller sees that there's a deployment it's like okay cool but the deployment doesn't have a",
    "start": "149840",
    "end": "154920"
  },
  {
    "text": "replica set so it creates a replica set great replica set controller wakes up and see that oh there's a new deployment",
    "start": "154920",
    "end": "160920"
  },
  {
    "text": "there's a new replica set we have and then it sees that it has no pods in it because that pods are supposed to be",
    "start": "160920",
    "end": "166400"
  },
  {
    "text": "three so it creates three pods for us great and the scheduler wakes up and sees that we have three pods but then",
    "start": "166400",
    "end": "172040"
  },
  {
    "text": "none of them are sheduled onto any of the nodes so it uses its own algorithm to figure out what nodes to schedule",
    "start": "172040",
    "end": "178040"
  },
  {
    "text": "these pods on and schedules supports onto these noes so shed's job is done",
    "start": "178040",
    "end": "183159"
  },
  {
    "text": "finally cubet on the nodes wake up and see that oh there are pods on my note that are scheduled that I don't have yet",
    "start": "183159",
    "end": "189400"
  },
  {
    "text": "running so it takes these pods makes it a part of its local desired state and",
    "start": "189400",
    "end": "194680"
  },
  {
    "text": "reconciles its state on the Node locally so the cuet sees that okay I have pod zero and pod one and the other cubet SE",
    "start": "194680",
    "end": "200239"
  },
  {
    "text": "I have pod two but none of them are running so it starts these pods and now you have three pods off your deployment",
    "start": "200239",
    "end": "207519"
  },
  {
    "text": "manage via replica set all via the controller magic right",
    "start": "207519",
    "end": "213319"
  },
  {
    "text": "now the whole point over here and how we got to where how we got to these three",
    "start": "213319",
    "end": "219000"
  },
  {
    "text": "PS running is we have these bunch of controllers that are running and the",
    "start": "219000",
    "end": "224519"
  },
  {
    "text": "fundamental concept here is that the controllers need to know what the current state of the system is because",
    "start": "224519",
    "end": "230519"
  },
  {
    "text": "if they don't know the current state there is no way they can get to the desired State second the controllers need to know what the desired state of",
    "start": "230519",
    "end": "235959"
  },
  {
    "text": "the system is because we need a not star to go to and as soon as it knows both of these things it can calculate a set of",
    "start": "235959",
    "end": "242239"
  },
  {
    "text": "corrective actions to move from the current from the current state to the desired state but how does it get to",
    "start": "242239",
    "end": "248959"
  },
  {
    "text": "know what the desired state is or how does it get to know what the current state is rather so to understand that",
    "start": "248959",
    "end": "254799"
  },
  {
    "text": "let's uh take this sentence right kubernetes is a declarative event driven system kubernetes is declarative what",
    "start": "254799",
    "end": "261400"
  },
  {
    "text": "does that mean it means that we specify intent we don't specify how we want something to be done we specify what",
    "start": "261400",
    "end": "267280"
  },
  {
    "text": "needs to be done and the the way it works is as I said we have the current state we have the desired State and the",
    "start": "267280",
    "end": "273360"
  },
  {
    "text": "controller takes actions on the current state to drive it to the desired state so we need to start somewhere and in",
    "start": "273360",
    "end": "279800"
  },
  {
    "text": "order to take actions we need to know what the current state looks like so right now we know what the desired state is because we are the ones who applied",
    "start": "279800",
    "end": "285880"
  },
  {
    "text": "Cube cutle and the controller knows what the deployment file was but we don't know what the current state of the",
    "start": "285880",
    "end": "291240"
  },
  {
    "text": "cluster is so to do so to get to know what the current state is we perform",
    "start": "291240",
    "end": "296280"
  },
  {
    "text": "what are called list operations against the cluster so the here for for example this is saying give me all the pods in",
    "start": "296280",
    "end": "302639"
  },
  {
    "text": "the cluster that are in the default namespace and you get a response object like this which is a collection called",
    "start": "302639",
    "end": "308639"
  },
  {
    "text": "pod list and uh items is the key which holds all of the pods in the default",
    "start": "308639",
    "end": "314840"
  },
  {
    "text": "namespace um also responses can get pretty huge you might not want all of",
    "start": "314840",
    "end": "321039"
  },
  {
    "text": "the PS in the default so for example if you have 100,000 pods in across your clusters for some reason uh and you want",
    "start": "321039",
    "end": "327960"
  },
  {
    "text": "all of them and you list all of them that response can get pretty huge so kubernetes also supports paginating these lists so you can you can say give",
    "start": "327960",
    "end": "335360"
  },
  {
    "text": "me the first 100 pods and then once that happens kubernetes Returns the first 100 pods and also gives you a continuation",
    "start": "335360",
    "end": "342319"
  },
  {
    "text": "token with it so what the continuation token does is you can give it back to the API server on on the next list",
    "start": "342319",
    "end": "348280"
  },
  {
    "text": "request and it gives you the next set of 100 PS and the next set of 100 PS till you get all of the PS in your",
    "start": "348280",
    "end": "354080"
  },
  {
    "text": "cluster so event driven system what does that mean right",
    "start": "354080",
    "end": "360120"
  },
  {
    "text": "now we know what the current state is because of our list operation but now we",
    "start": "360120",
    "end": "365199"
  },
  {
    "text": "need to know what changes occur in the system in order to react to these changes and take corrective actions one",
    "start": "365199",
    "end": "372479"
  },
  {
    "text": "alternative is we can keep listing all the time which is super inefficient but what kubernetes gives us is this sort of",
    "start": "372479",
    "end": "380840"
  },
  {
    "text": "event driven mechanism where you you get events such as add events modify events",
    "start": "380840",
    "end": "386479"
  },
  {
    "text": "delete events and so on so if I see that uh deployment is added my action is is",
    "start": "386479",
    "end": "393039"
  },
  {
    "text": "does it have a replica set if no create one and the replica set controller sees that okay I have an ad event for a replica set does it have pods no create",
    "start": "393039",
    "end": "399639"
  },
  {
    "text": "them and so on and so forth so uh kubernetes in itself you won't find",
    "start": "399639",
    "end": "406080"
  },
  {
    "text": "things like message buses and all of that inside kubernetes but as a mental model it's helpful thinking of it as an",
    "start": "406080",
    "end": "411240"
  },
  {
    "text": "event driven architecture and that's a great blog post if you want to check it out so I have my state of the world",
    "start": "411240",
    "end": "416400"
  },
  {
    "text": "through list I now need to know when events happen that modify the state so I can take corrective action for them and",
    "start": "416400",
    "end": "423440"
  },
  {
    "text": "you might be seeing this term resource version in all of these response objects resource versions essentially tell give",
    "start": "423440",
    "end": "430599"
  },
  {
    "text": "you recency information about your cluster or the data that you're getting back so over here resource version is",
    "start": "430599",
    "end": "436840"
  },
  {
    "text": "1452 it means that the response that I got back was at a version 1452 and we'll",
    "start": "436840",
    "end": "443199"
  },
  {
    "text": "talk about what a version means and this basically tells you how recent your data is so now that I know I I have data as",
    "start": "443199",
    "end": "450280"
  },
  {
    "text": "recent as 1452 I can tell kubernetes that give me all changes in the cluster",
    "start": "450280",
    "end": "456160"
  },
  {
    "text": "for all pods starting from 1452 I don't care what's happened before because I already know till 1452 so give me",
    "start": "456160",
    "end": "463919"
  },
  {
    "text": "everything post that so as in when these events happen I get a stream of",
    "start": "463919",
    "end": "469000"
  },
  {
    "text": "notifications on a single connection that I can react to so this is the watch mechanism of kubernetes so",
    "start": "469000",
    "end": "475800"
  },
  {
    "text": "together list and watch help enable this controller pattern and sort of is the",
    "start": "475800",
    "end": "481599"
  },
  {
    "text": "key to understanding the kubernetes architecture as a whole so before we move forward we me we talked a little",
    "start": "481599",
    "end": "486680"
  },
  {
    "text": "bit about resource version but we weren't very specific about that so let's dive a little bit into what that means um so resource versions are opaque",
    "start": "486680",
    "end": "493840"
  },
  {
    "text": "strings representing an internal version of an object when I say an internal version we basically surface the",
    "start": "493840",
    "end": "501000"
  },
  {
    "text": "multiversion concurrency control that hcd uses internally to keep a history of changes for each object and we surface",
    "start": "501000",
    "end": "507680"
  },
  {
    "text": "that back to users via resource version and essentially in kubernetes resource",
    "start": "507680",
    "end": "512719"
  },
  {
    "text": "versions are one big Global logical clock you can order all the events in your system globally and you can do you",
    "start": "512719",
    "end": "520159"
  },
  {
    "text": "can you can linearize all the events in your system globally uh through a resource version resource versions are",
    "start": "520159",
    "end": "526200"
  },
  {
    "text": "backed by rcd store versions which by Design provide a global ordering and they increase monotonically anytime any",
    "start": "526200",
    "end": "533279"
  },
  {
    "text": "change in the cluster occurs it doesn't really matter if you change a pod or a replica or a service if any change happens the resource version is going",
    "start": "533279",
    "end": "539200"
  },
  {
    "text": "going to change monotonically increasing so uh most importantly they",
    "start": "539200",
    "end": "545440"
  },
  {
    "text": "enable something called optimistic concurrency control so U what that means is if I have two controllers running and",
    "start": "545440",
    "end": "551600"
  },
  {
    "text": "one of them writes State the other one won't be able to write state if it doesn't already know that this state is written so it will try to by being",
    "start": "551600",
    "end": "558680"
  },
  {
    "text": "optimistic that I know what's the latest state but it might not be able to so resource versions enable that as",
    "start": "558680",
    "end": "565839"
  },
  {
    "text": "well um if you want more insights into how resource versions sort of map onto",
    "start": "565839",
    "end": "571959"
  },
  {
    "text": "HD storage versions uh there was a great talk by a friend and colleague Priyanka",
    "start": "571959",
    "end": "577079"
  },
  {
    "text": "sagu on Tuesday so you should check that out so coming to the coming to the",
    "start": "577079",
    "end": "584120"
  },
  {
    "text": "storage layer right let's look at what the stor is layer looked like in the past so that we can better understand",
    "start": "584120",
    "end": "589519"
  },
  {
    "text": "how it is right now uh so we have the API server and we have ETD a client",
    "start": "589519",
    "end": "594760"
  },
  {
    "text": "comes up opens up a watch request against pods great API server opens up a watch request against CD another client",
    "start": "594760",
    "end": "600760"
  },
  {
    "text": "comes up opens up a voice request against pods apsi was like okay you know what cool I know how to open up a voice request so it opens up a voice request I",
    "start": "600760",
    "end": "607120"
  },
  {
    "text": "against s CD and another client comes up it opens up aot's request great this is",
    "start": "607120",
    "end": "612480"
  },
  {
    "text": "fine I mean kubernetes doesn't really run scalable applications right it it's",
    "start": "612480",
    "end": "617640"
  },
  {
    "text": "meant to run my personal blog right which no one visits ever but that's not the case more people might open up watch",
    "start": "617640",
    "end": "624079"
  },
  {
    "text": "requests for pods and other resources and API server is going to open a",
    "start": "624079",
    "end": "629120"
  },
  {
    "text": "separate watch connection to hcd every single time and this isn't scalable not just is it opening of a separate watch",
    "start": "629120",
    "end": "635240"
  },
  {
    "text": "request every single time hcd has to send all of this data package it in a",
    "start": "635240",
    "end": "640959"
  },
  {
    "text": "way compress it if compressions are enabled Marshall it all of that send it back to API server API server takes this",
    "start": "640959",
    "end": "647639"
  },
  {
    "text": "data unmarshal it performs conversion prepares a response sends it back so it's a whole lot of work that happens",
    "start": "647639",
    "end": "654519"
  },
  {
    "text": "both at the CCD side of things and the API server side of things so like both both are loaded in this case what this",
    "start": "654519",
    "end": "660959"
  },
  {
    "text": "means is if you have more replicas of your controller for high availability",
    "start": "660959",
    "end": "666320"
  },
  {
    "text": "higher availability after a certain point is going to result in lesser scalability for you because you're",
    "start": "666320",
    "end": "672000"
  },
  {
    "text": "opening up that many more duplicate resource uh connections to atcd so how does the kubernetes storage",
    "start": "672000",
    "end": "678680"
  },
  {
    "text": "layer look like in the present and these are diagrams I'm very very proud of because they look so pretty and they're all pink so let's take a look at that um",
    "start": "678680",
    "end": "686760"
  },
  {
    "text": "as with any computer science problem we solved this problem with a layer of interaction so we have our API server",
    "start": "686760",
    "end": "692880"
  },
  {
    "text": "and we have all these clients opening up watch requests but let's zoom in a little bit here so we have the API",
    "start": "692880",
    "end": "698440"
  },
  {
    "text": "server but inside the API server we have something called the cacher and all of",
    "start": "698440",
    "end": "703920"
  },
  {
    "text": "these things in the brackets that I mentioned there these are actual kubernetes type names so in case you",
    "start": "703920",
    "end": "709959"
  },
  {
    "text": "want to Deep dive into the code later on for whatever reason you can follow along with these slides along as a resource uh",
    "start": "709959",
    "end": "716680"
  },
  {
    "text": "to look at the code because I find I found that to be a challenging and hopefully this helps you so you have the",
    "start": "716680",
    "end": "722120"
  },
  {
    "text": "cacher and then you have the store you have something called the store and a store is nothing but an IND it's it's a",
    "start": "722120",
    "end": "729440"
  },
  {
    "text": "golang map but it has some indexing support built on top of it but the idea",
    "start": "729440",
    "end": "734720"
  },
  {
    "text": "here is that the store is meant to be a reflection of the state in etcd that's",
    "start": "734720",
    "end": "739920"
  },
  {
    "text": "the whole idea also you have a watch cach something called a watch cach so",
    "start": "739920",
    "end": "745240"
  },
  {
    "text": "this is a F4 buffer of finite size that exists along with the",
    "start": "745240",
    "end": "750880"
  },
  {
    "text": "store now the watch cache is populated asynchronously from hcd because the",
    "start": "750880",
    "end": "757480"
  },
  {
    "text": "cacher opens up a watch connection so whenever you have all these add events modify events delete events that happen",
    "start": "757480",
    "end": "764120"
  },
  {
    "text": "in ETD all of that get propagated back into the watch cache and the watch cach",
    "start": "764120",
    "end": "769839"
  },
  {
    "text": "populates all of that back into the store via callbacks now you have the client which",
    "start": "769839",
    "end": "776760"
  },
  {
    "text": "opens up pods great we have a connection can open now that watch request can be served either from the watch cache or",
    "start": "776760",
    "end": "784000"
  },
  {
    "text": "from the store as we will soon see I will tell you what each of these things mean another client comes up opens up a",
    "start": "784000",
    "end": "790880"
  },
  {
    "text": "watch request it's served from the store for example and another client comes up at store it's serve from the watch cash for example but the interesting part is",
    "start": "790880",
    "end": "797720"
  },
  {
    "text": "that each of these clients have come up opened up watch requests we haven't opened up a new watch connection to HD",
    "start": "797720",
    "end": "802800"
  },
  {
    "text": "we've only kept the single Watch connection that's helping us serve all these requests now the that's for pods but",
    "start": "802800",
    "end": "810120"
  },
  {
    "text": "what about for resource uh for replica sets for services for all of those other object types you have one cacher per",
    "start": "810120",
    "end": "818519"
  },
  {
    "text": "object type per group resource in kubernetes terminology and all of these are created at startup time all of these",
    "start": "818519",
    "end": "824399"
  },
  {
    "text": "are created when API server is initialized so you have all these clients coming in opening up watch",
    "start": "824399",
    "end": "830839"
  },
  {
    "text": "requests great we have a Casher for all of them if you have multiple of those the same cash is going to serve the",
    "start": "830839",
    "end": "836519"
  },
  {
    "text": "request so the store component is meant to reflect the state of hcd as I said uh",
    "start": "836519",
    "end": "842480"
  },
  {
    "text": "cacher per object type is created at the API server startup time and the caching",
    "start": "842480",
    "end": "847560"
  },
  {
    "text": "layer can be disabled altogether if you don't want it because sometimes you have really really tight memory constraints",
    "start": "847560",
    "end": "853560"
  },
  {
    "text": "and a caching layer might just add on to that so if you choose to disable the cacher you're are essentially saying",
    "start": "853560",
    "end": "858720"
  },
  {
    "text": "that you know what I need extreme data consistency that I'm always going to go to hcd for and I don't mind the extra",
    "start": "858720",
    "end": "865079"
  },
  {
    "text": "latency that I get with that and we'll talk about what that means soon but you have an option of disabling caching",
    "start": "865079",
    "end": "870959"
  },
  {
    "text": "altogether but if you want to keep caching for some object types but not for the other you can do that as well",
    "start": "870959",
    "end": "876399"
  },
  {
    "text": "you have Flags in the API server that can do that for you you can disable caching on a per object type",
    "start": "876399",
    "end": "883720"
  },
  {
    "text": "basis now now we've seen what the three layers",
    "start": "883720",
    "end": "888880"
  },
  {
    "text": "sort of of storage look like uh but to understand how they impact scalability",
    "start": "888880",
    "end": "894839"
  },
  {
    "text": "for your requests and how you can interact with them in a better manner while signing the tradeoffs it's",
    "start": "894839",
    "end": "900160"
  },
  {
    "text": "important to look at how different requests interact with the present storage layer before we do that we need",
    "start": "900160",
    "end": "906959"
  },
  {
    "text": "to look at this thing called resource versions again we need to look at how they're interpreted by kubernetes",
    "start": "906959",
    "end": "912959"
  },
  {
    "text": "because this is I'm not going to lie it gets hairy but the reason for that is",
    "start": "912959",
    "end": "918759"
  },
  {
    "text": "because we don't want to break our users kubernetes did things a certain wave of while ago but then in order to make sure",
    "start": "918759",
    "end": "925959"
  },
  {
    "text": "that we scale while maintaining compatibility we had to make some not so neat trade-offs in terms of how resource",
    "start": "925959",
    "end": "932440"
  },
  {
    "text": "versions are interpreted and uh analyzed so I'm going to give you a brief",
    "start": "932440",
    "end": "937480"
  },
  {
    "text": "overview of that it's still not the whole picture but I'm going to link to where you can read more about that uh so",
    "start": "937480",
    "end": "943240"
  },
  {
    "text": "in each type in each type of grud request that is you can pass a resource version parameter that is you can say",
    "start": "943240",
    "end": "950040"
  },
  {
    "text": "that hey give me uh all give me all the pods starting at this resource version",
    "start": "950040",
    "end": "955519"
  },
  {
    "text": "or you can say give me all the replica sets at this resource version or you can even say I don't care about consistency",
    "start": "955519",
    "end": "961000"
  },
  {
    "text": "at all just give me whatever you know that is also an option because that's Max that's least latency but also",
    "start": "961000",
    "end": "967800"
  },
  {
    "text": "arbitrarily stale data so all of these are also possibilities so knowing how Behavior changes with resource version",
    "start": "967800",
    "end": "973800"
  },
  {
    "text": "resource version interpretation can be crucial to scalability so for get requests like get",
    "start": "973800",
    "end": "980560"
  },
  {
    "text": "get list so get list is just a list request and watch watch is also a get request with just a get request",
    "start": "980560",
    "end": "987800"
  },
  {
    "text": "parameter attached to it so if resource version is just an empty string that is if it's not set",
    "start": "987800",
    "end": "994319"
  },
  {
    "text": "it's interpreted as give me the most recent data what that means is most of",
    "start": "994319",
    "end": "999480"
  },
  {
    "text": "these requests that have a empty resource version are going to go tocd by default because etcd is the source of",
    "start": "999480",
    "end": "1005480"
  },
  {
    "text": "Truth it has to have the most recent data uh so this is what that is interpreted as if the resource version",
    "start": "1005480",
    "end": "1012680"
  },
  {
    "text": "is a string zero then it means that any data is fine arbitrarily stale data is",
    "start": "1012680",
    "end": "1018160"
  },
  {
    "text": "also Al fine and finally you have some specific resource version n that means",
    "start": "1018160",
    "end": "1024760"
  },
  {
    "text": "data at n and this is also this has two interpretations that I will tell you now uh before I do that most recent data is",
    "start": "1024760",
    "end": "1032240"
  },
  {
    "text": "ensured by doing a quorum read in NCD so whenever you do a mo whenever the read request goes tocd via this empty",
    "start": "1032240",
    "end": "1039319"
  },
  {
    "text": "resource version etcd does a round of raft it makes sure that you get the most",
    "start": "1039319",
    "end": "1044438"
  },
  {
    "text": "consistent read and all reads in atcd by default are linearized there is an option for serializable reads and",
    "start": "1044439",
    "end": "1050679"
  },
  {
    "text": "transactions but all reads in HD that is at least all reads at kubernetes issues to hcd are linearized by default so",
    "start": "1050679",
    "end": "1058000"
  },
  {
    "text": "that's what the empty resource version means now I said that resource version equal to n is interpreted in two",
    "start": "1058000",
    "end": "1063640"
  },
  {
    "text": "different ways um it's via this parameter called resource version match you have to specify this parameter if",
    "start": "1063640",
    "end": "1070120"
  },
  {
    "text": "you specify a resource version n in your requests so you have two of those which",
    "start": "1070120",
    "end": "1075640"
  },
  {
    "text": "is not older than and exact not older then basically tells you give me data that is at least as new as n and exact",
    "start": "1075640",
    "end": "1083280"
  },
  {
    "text": "tells you give me exactly N I want the resource version n so you either get n or you get a response saying that n has",
    "start": "1083280",
    "end": "1090320"
  },
  {
    "text": "been compacted or a resource version two old or 410 I think the error code is in",
    "start": "1090320",
    "end": "1096159"
  },
  {
    "text": "kubernetes so as I said this still is in the full picture please see that link for more",
    "start": "1096159",
    "end": "1102640"
  },
  {
    "text": "information okay so now let's look at how each request behaves with kubernetes um",
    "start": "1102640",
    "end": "1108960"
  },
  {
    "text": "first you have the create request create requests go straight to hcd they go straight to hcd an object is created in",
    "start": "1108960",
    "end": "1115320"
  },
  {
    "text": "hcd all that is all that happens great but as I said watch cash is watching hcd",
    "start": "1115320",
    "end": "1121159"
  },
  {
    "text": "right now so as soon as an object is created in hcd that change is propagated back to watch cash which propagates it",
    "start": "1121159",
    "end": "1127559"
  },
  {
    "text": "back to the store so now we have a consistent state of hcd from off the created object back in the watch cach",
    "start": "1127559",
    "end": "1133480"
  },
  {
    "text": "again so as when you do reads it's now available in the watch cach delete is the same way you send the",
    "start": "1133480",
    "end": "1140000"
  },
  {
    "text": "request directly to hcd but there is an extra step here if you want to read the code we try to delete the version of the",
    "start": "1140000",
    "end": "1146039"
  },
  {
    "text": "object that is present in ETD because as I said you can have multiple versions of objects depending on how things are",
    "start": "1146039",
    "end": "1152600"
  },
  {
    "text": "updated because ETD implements multiversion concurrency control so we try to delete the version of the object",
    "start": "1152600",
    "end": "1159440"
  },
  {
    "text": "that the API server knows about but if it's not able to it just deletes the object incd and as usual that change is",
    "start": "1159440",
    "end": "1166240"
  },
  {
    "text": "propagated back to watch cach bya the watch that's open against n CD Now updat",
    "start": "1166240",
    "end": "1171480"
  },
  {
    "text": "operations right uh same thing you try to update the version of the object that's there in the watch cache if not",
    "start": "1171480",
    "end": "1177799"
  },
  {
    "text": "you just update the object and then propagate back all the changes via the watch now get request this is where",
    "start": "1177799",
    "end": "1183600"
  },
  {
    "text": "things start to get interesting and this is where I'm going to probably start rambling a little bit uh just the heads",
    "start": "1183600",
    "end": "1190039"
  },
  {
    "text": "up if that happens uh we will talk offline but yeah if resource version is",
    "start": "1190039",
    "end": "1197919"
  },
  {
    "text": "empty string as I said this means give me the most recent data give me the most consistent data so the read goes to hcdd",
    "start": "1197919",
    "end": "1205520"
  },
  {
    "text": "hcd does a Corum read a round of raft happens and so on and so forth now if",
    "start": "1205520",
    "end": "1211240"
  },
  {
    "text": "it's zero it means that any arbitrarily stale data is fine so it goes to the watch cach and it just re it just does a",
    "start": "1211240",
    "end": "1218440"
  },
  {
    "text": "look up on the map and be like okay this is the data cool I don't care how old it is just give it to me so that what",
    "start": "1218440",
    "end": "1224039"
  },
  {
    "text": "that's what happens but if you specify resource version n the cash actually waits for the data that's an NCD to be",
    "start": "1224039",
    "end": "1231280"
  },
  {
    "text": "propagated back for a little while it waits for a little while so as soon as it's propagated back it returns that but",
    "start": "1231280",
    "end": "1237039"
  },
  {
    "text": "the request does not go to hcd all of this happens asynchronously so it waits for approximately 3 seconds till this",
    "start": "1237039",
    "end": "1242799"
  },
  {
    "text": "happens if it doesn't if the if the request is not if the data is not propagated back in 3 seconds it's going",
    "start": "1242799",
    "end": "1247880"
  },
  {
    "text": "to return an error code and this error code if you're using default kubernetes clients is interpreted to be a retry and",
    "start": "1247880",
    "end": "1254400"
  },
  {
    "text": "as soon as that retry happens that request is going going to go to hcd directly so you don't need to intervene",
    "start": "1254400",
    "end": "1259840"
  },
  {
    "text": "at all it's just that you tried to get you tried to do a l low latency read from the cach it didn't work out cool",
    "start": "1259840",
    "end": "1266120"
  },
  {
    "text": "you're going to go you're going to go to HD which is guaranteed to have your answer for you um so once that happens the read",
    "start": "1266120",
    "end": "1272600"
  },
  {
    "text": "happens on the watch cache and so on and so forth now get list this is where I was talking",
    "start": "1272600",
    "end": "1279600"
  },
  {
    "text": "about things get a little rambly this is the function that decides if the request go goes to atcd or not",
    "start": "1279600",
    "end": "1287880"
  },
  {
    "text": "and we're going to walk through this function line by line and break it down and try to understand how you can maximize scalability",
    "start": "1287880",
    "end": "1294120"
  },
  {
    "text": "here so first check is consistent rate from Storage that is if resource version",
    "start": "1294120",
    "end": "1299960"
  },
  {
    "text": "is empty by default means go to LD because we want the most recent data",
    "start": "1299960",
    "end": "1305960"
  },
  {
    "text": "straightforward enough second has continuation so remember initially I told you kubernetes",
    "start": "1305960",
    "end": "1312120"
  },
  {
    "text": "supports paginating R requests so you can say give me the first 100 pods it gives you the first 100 pods but along",
    "start": "1312120",
    "end": "1317720"
  },
  {
    "text": "with that that gives you a continuation token and in the next list request you can provide that continuation token and gives you the next 100 pods and so on",
    "start": "1317720",
    "end": "1324640"
  },
  {
    "text": "and so on and so on but if you specify a continuation token in your list",
    "start": "1324640",
    "end": "1330320"
  },
  {
    "text": "request that is always going to go to hcd the reason for this is that watch",
    "start": "1330320",
    "end": "1336760"
  },
  {
    "text": "cast does not support pagination it cannot serve paginated list requests just yet I actually did some work on the",
    "start": "1336760",
    "end": "1344919"
  },
  {
    "text": "watch cast to paginate it and this sort of worked pretty well uh but it adds a",
    "start": "1344919",
    "end": "1350039"
  },
  {
    "text": "lot of complexity so I don't know what's going to happen with this but the work was pretty interesting so we instead of",
    "start": "1350039",
    "end": "1355600"
  },
  {
    "text": "implementing the watch cach as a as a map or the store as a map rather we implemented it using B trees and we did",
    "start": "1355600",
    "end": "1362279"
  },
  {
    "text": "like copy on right semantics and it was a whole bunch of cool stuff so if you want to talk about that later we can we",
    "start": "1362279",
    "end": "1367400"
  },
  {
    "text": "can geek out um has limit right like when I was doing",
    "start": "1367400",
    "end": "1373000"
  },
  {
    "text": "my dry runs this is the part where I was like okay I was like the crazy cncf landscape SK me explaining it uh but I'm",
    "start": "1373000",
    "end": "1380600"
  },
  {
    "text": "going to break this down I'm going to try and break this down um so we're going to look at the first part of the",
    "start": "1380600",
    "end": "1385679"
  },
  {
    "text": "condition right uh pr. liit so if I specify a limit it means that I'm going",
    "start": "1385679",
    "end": "1392559"
  },
  {
    "text": "to specify a continuation later and watch cast does not support continuations and it does not support",
    "start": "1392559",
    "end": "1398480"
  },
  {
    "text": "pagination so we're going to send that request to L CD that's the first aspect of",
    "start": "1398480",
    "end": "1403840"
  },
  {
    "text": "it but if no limit is set we can serve the list from the watch cach itself",
    "start": "1403840",
    "end": "1409640"
  },
  {
    "text": "because that doesn't that means that we're not going to support we're not going to uh specify a continuation and",
    "start": "1409640",
    "end": "1416200"
  },
  {
    "text": "watch cash is capable of just serving a list request by itself however if we set a limit and put",
    "start": "1416200",
    "end": "1423120"
  },
  {
    "text": "a resource version um that is equal to zero that's we are just going to",
    "start": "1423120",
    "end": "1429240"
  },
  {
    "text": "disregard limit altogether and we're going to say I don't care what you said to me I'm going to serve the list",
    "start": "1429240",
    "end": "1435000"
  },
  {
    "text": "request from the watch cash no matter what so that's what what it says so when I specify resource version zero I'm",
    "start": "1435000",
    "end": "1440640"
  },
  {
    "text": "disregarding the limit entirely and that condition is going to evaluate to false uh well so trying to understand",
    "start": "1440640",
    "end": "1449480"
  },
  {
    "text": "why that's the case so the first thing is resource version zero is any data semantics I don't care about consistency",
    "start": "1449480",
    "end": "1455279"
  },
  {
    "text": "so serving it from the watch cach makes sense but why do we do this why do we disregard limit all together um more",
    "start": "1455279",
    "end": "1463000"
  },
  {
    "text": "importantly it allows us to support requests which we know are going to be extremely large in size and these are",
    "start": "1463000",
    "end": "1470000"
  },
  {
    "text": "requests that are so large in size that LCD will face significant load if loaded from that so what I mean by that is in a",
    "start": "1470000",
    "end": "1478240"
  },
  {
    "text": "moderately large okay in a large cluster we can have nodes in the order of thousands and each node can have pods in",
    "start": "1478240",
    "end": "1485399"
  },
  {
    "text": "the order of hundreds now if a if if the cube blade or the stateful set controller were to list all these pods",
    "start": "1485399",
    "end": "1491880"
  },
  {
    "text": "it's a huge amount of load on NCD so what we do is whenever the initial the",
    "start": "1491880",
    "end": "1497840"
  },
  {
    "text": "first list request is issued we ensure that we put the resource version at zero so that always",
    "start": "1497840",
    "end": "1504360"
  },
  {
    "text": "always watch cach is the one that serves it it does not go to HD so that we don't overload hcd we serve it from the watch",
    "start": "1504360",
    "end": "1510600"
  },
  {
    "text": "cach with arbitrarily stale data because we just want an initial idea of what",
    "start": "1510600",
    "end": "1515919"
  },
  {
    "text": "these resource versions are going to be like and we just want an initial idea of what the current state is going to be like later on as and when we get more",
    "start": "1515919",
    "end": "1523000"
  },
  {
    "text": "and more events we can incrementally build up to the actual current state",
    "start": "1523000",
    "end": "1529679"
  },
  {
    "text": "uh so and again you have the unsupported match option watch cash only supports",
    "start": "1530080",
    "end": "1536240"
  },
  {
    "text": "not older than so if you specify exact semantics because you want exact data watch Crash can't guarantee that because",
    "start": "1536240",
    "end": "1542760"
  },
  {
    "text": "at the end of the day it's a caching layer so we end up sending the request to ETD",
    "start": "1542760",
    "end": "1548679"
  },
  {
    "text": "again so the only time we Ser a list request from the wats cash is if we",
    "start": "1549039",
    "end": "1554440"
  },
  {
    "text": "specify a non-empty resource version it is not a paginated list and we specify not older than semantics so if you don't",
    "start": "1554440",
    "end": "1561480"
  },
  {
    "text": "have strong consistency guarantees that that you require always it's always",
    "start": "1561480",
    "end": "1566520"
  },
  {
    "text": "beneficial for you to specify a resource version and do um a non-patented list if",
    "start": "1566520",
    "end": "1572600"
  },
  {
    "text": "you're okay with that and you your cluster can tolerate it because you're going to get really really good performance benefits from",
    "start": "1572600",
    "end": "1578679"
  },
  {
    "text": "there great so there's a few gchas to keep in mind here right because we did so much complexity everything's done we",
    "start": "1578679",
    "end": "1584520"
  },
  {
    "text": "are good to go but no this also has some problems and that's why I'm here to talk about it U when you need consistent",
    "start": "1584520",
    "end": "1591039"
  },
  {
    "text": "lists and the request goes to hcd API server can also see spikes in memory because we saw that we we force a",
    "start": "1591039",
    "end": "1597880"
  },
  {
    "text": "resource version zero we disregard limit all those scalability hacks we've done but we still see spikes in memory and",
    "start": "1597880",
    "end": "1604360"
  },
  {
    "text": "that is because when we get data from ETD it's unmarshal under a lock it take",
    "start": "1604360",
    "end": "1610360"
  },
  {
    "text": "conversions happen we prepare a response under a lock not anymore some parts of it are not prepared under a lock and I",
    "start": "1610360",
    "end": "1616039"
  },
  {
    "text": "have another talk about that uh I can send you the links to that but yeah you you do a lot of work at the API server",
    "start": "1616039",
    "end": "1622600"
  },
  {
    "text": "most of most of it under a log just to serve the data back and this is because you're you're requesting all of this",
    "start": "1622600",
    "end": "1629039"
  },
  {
    "text": "huge amounts of data from ETD and sometimes fasinating responses also will not help if each chunk of the",
    "start": "1629039",
    "end": "1635520"
  },
  {
    "text": "response is huge in itself if I have 100,000 pods and I ask for five if I ask for like let's say thousand uh that's",
    "start": "1635520",
    "end": "1642039"
  },
  {
    "text": "okay but it's going to take me a lot of time so if I ask for bigger and bigger chunks I have bigger and bigger memory Footprints and so so on and so forth so",
    "start": "1642039",
    "end": "1649720"
  },
  {
    "text": "uh there is a cap right now in the kubernetes community uh called watch",
    "start": "1649720",
    "end": "1654760"
  },
  {
    "text": "list uh so this proposes serving list requests but using watch semantics so",
    "start": "1654760",
    "end": "1660320"
  },
  {
    "text": "you can stream list data back you so this gives you predictable memory",
    "start": "1660320",
    "end": "1666519"
  },
  {
    "text": "Footprints and it handles the lack of pagination in the watch cache because now you can serve all the list requests",
    "start": "1666519",
    "end": "1672960"
  },
  {
    "text": "even if it's even if it's just a small chunk but from a in a streaming manner",
    "start": "1672960",
    "end": "1678320"
  },
  {
    "text": "so uh this is in Alpha right now as of 1.28 so if this is something of interest",
    "start": "1678320",
    "end": "1684760"
  },
  {
    "text": "to you please try it out provide feedback to the community it's set to go to Beta in 1.29 which is set to release",
    "start": "1684760",
    "end": "1690919"
  },
  {
    "text": "end of November so uh get list is another time",
    "start": "1690919",
    "end": "1696600"
  },
  {
    "text": "another gcha is time traveling so you need you have something called stale read from the watch",
    "start": "1696600",
    "end": "1701720"
  },
  {
    "text": "cash and this usually happens in an ha setup so you have two kubernetes API",
    "start": "1701720",
    "end": "1706960"
  },
  {
    "text": "servers uh that have watch cach and cachers enabled Each of which can have arbitrarily staled data so whenever you",
    "start": "1706960",
    "end": "1713240"
  },
  {
    "text": "do the first initial list which is now forced with a zero resource version which means give me any data you can get",
    "start": "1713240",
    "end": "1721120"
  },
  {
    "text": "data that you've already seen in the past because one of those API servers might be arbitrarily behind and as soon",
    "start": "1721120",
    "end": "1726840"
  },
  {
    "text": "as you see that data you start reacting to the data and taking wrong actions So You're vulnerable to something called",
    "start": "1726840",
    "end": "1732440"
  },
  {
    "text": "stale reads in in controllers and this happens in h API server",
    "start": "1732440",
    "end": "1739559"
  },
  {
    "text": "setups um externally to externally to kubernetes there are a few tools that",
    "start": "1739559",
    "end": "1745880"
  },
  {
    "text": "have come out from collaboration between industry and Academia two of such tools are C and acto which can automatically",
    "start": "1745880",
    "end": "1753000"
  },
  {
    "text": "detect such issues and more um through automatic reliability testing of your controllers and we all we have uh",
    "start": "1753000",
    "end": "1760399"
  },
  {
    "text": "researchers who do that in the room right now so we have tianan and we also have Trion here so if that this is",
    "start": "1760399",
    "end": "1765960"
  },
  {
    "text": "something of interest to you please go talk to them after this now within kubernetes",
    "start": "1765960",
    "end": "1771200"
  },
  {
    "text": "itself uh there are a couple of caps that are attempting to solve this problem so the first one I talked about",
    "start": "1771200",
    "end": "1776320"
  },
  {
    "text": "is watch list all right so this this is um we are we we are streaming data back",
    "start": "1776320",
    "end": "1783559"
  },
  {
    "text": "into the watch cache because we have a watch open open against ETD and we're",
    "start": "1783559",
    "end": "1789480"
  },
  {
    "text": "using the same semantics to serve list requests so essentially you are serving data as in when it comes you're not",
    "start": "1789480",
    "end": "1795559"
  },
  {
    "text": "waiting for a lock and serving a stale data and then you have another cap",
    "start": "1795559",
    "end": "1801559"
  },
  {
    "text": "called consistent reads from cash so this cap uh argues that data in in the",
    "start": "1801559",
    "end": "1808679"
  },
  {
    "text": "watch cache most times is recent enough we don't need to go tocd all the time so",
    "start": "1808679",
    "end": "1815000"
  },
  {
    "text": "this cap sort of tells you works on the fact that if I provide a specific",
    "start": "1815000",
    "end": "1820120"
  },
  {
    "text": "resource version or if I don't provide a resource version at all it it basically waits for the cash",
    "start": "1820120",
    "end": "1826679"
  },
  {
    "text": "to get popul it back waits for the cash to be consistent enough what enough means is you can read the cap and then",
    "start": "1826679",
    "end": "1833559"
  },
  {
    "text": "after that happens you can serve the you can serve watch requests uh from the watch cash itself",
    "start": "1833559",
    "end": "1838720"
  },
  {
    "text": "instead of going and list request from the watch cash itself instead of going back to HD this is also an alpha right now so if",
    "start": "1838720",
    "end": "1845760"
  },
  {
    "text": "you're interested try it out provide feedback and U hopefully uh we can move",
    "start": "1845760",
    "end": "1852640"
  },
  {
    "text": "this to Beta in the future kubernetes releases you also get some nice performance benefits from these caps so",
    "start": "1852640",
    "end": "1859039"
  },
  {
    "text": "the watch list cap that I talked about uh I don't think the numbers are visible but before this this is CPU on the left",
    "start": "1859039",
    "end": "1866120"
  },
  {
    "text": "is before so that's around averaging around four to five cores and on the on the left on the right it's after that is",
    "start": "1866120",
    "end": "1872840"
  },
  {
    "text": "after watch list is enabled this is memory footprint this is CPU footprint of the API server on the right it's averaging around 1.2 CES of CPU so you",
    "start": "1872840",
    "end": "1880080"
  },
  {
    "text": "get some nice performance benefits and the same goes for memory as well and similarly for the consistent",
    "start": "1880080",
    "end": "1886200"
  },
  {
    "text": "reads from the from the the cash cap you also have some nice pre preliminary uh",
    "start": "1886200",
    "end": "1891519"
  },
  {
    "text": "performance improvements that you see not just for the API server but also Ford now we're done with list finally we",
    "start": "1891519",
    "end": "1900600"
  },
  {
    "text": "can talk about the watch's request nothing to special here we've already talked about most of the things so if",
    "start": "1900600",
    "end": "1906200"
  },
  {
    "text": "it's if it's an empty string we delegate the request Tod as always otherwise we serve it from the watch cach but the way",
    "start": "1906200",
    "end": "1911960"
  },
  {
    "text": "we do it is we create something called the cash Watcher um I'm naming these things so that when you go look at the",
    "start": "1911960",
    "end": "1917720"
  },
  {
    "text": "code it's familiar to you so we create this thing called Cash Watcher which helps you serve these watch requests to",
    "start": "1917720",
    "end": "1923320"
  },
  {
    "text": "each of the clients right and it also helps uh D duplicate some of the objects that you need to serve back so it's it's",
    "start": "1923320",
    "end": "1929440"
  },
  {
    "text": "a pretty nice abstraction but the way the cash Watcher works is it allocates an input buffer",
    "start": "1929440",
    "end": "1936279"
  },
  {
    "text": "statically at start time and the way it allocates this is depending on what type of object is being watched and the sizes",
    "start": "1936279",
    "end": "1944200"
  },
  {
    "text": "of these allocations are sort of determined through huris that we found out from our scale testing in the",
    "start": "1944200",
    "end": "1949480"
  },
  {
    "text": "kubernetes community now as soon as the buffer becomes full we terminate the watch the",
    "start": "1949480",
    "end": "1955639"
  },
  {
    "text": "client gets a 410 we restart or we reestablish a watch and then things continue so on and so forth",
    "start": "1955639",
    "end": "1963320"
  },
  {
    "text": "however the cost of keeping up with watch events the the cost of keeping up with watch events is essentially",
    "start": "1963320",
    "end": "1969600"
  },
  {
    "text": "establishing a new connection because as soon as the buffer is full we stop it and we want to keep up so we establish a",
    "start": "1969600",
    "end": "1975200"
  },
  {
    "text": "new watch and then keep on going but establishing a new watch all those things also have a cost and this",
    "start": "1975200",
    "end": "1981600"
  },
  {
    "text": "wouldn't be happening if for example if you have a lot of events but a static size allocated is very small",
    "start": "1981600",
    "end": "1987720"
  },
  {
    "text": "because heuristics are heuristics at the end of the day they aren't fully accurate",
    "start": "1987720",
    "end": "1992760"
  },
  {
    "text": "so however a slow client slow server or a storm of Rapid updates is is enough to",
    "start": "1992760",
    "end": "1998760"
  },
  {
    "text": "necessitate a new watch connection so there is also some very interesting discussion in the community happening of",
    "start": "1998760",
    "end": "2003960"
  },
  {
    "text": "how we can dynamically allocate the size of this buffer and the issue has some really good analysis of how that can be",
    "start": "2003960",
    "end": "2010120"
  },
  {
    "text": "done in the future he's also the person by the way who implemented the watch list feature",
    "start": "2010120",
    "end": "2015639"
  },
  {
    "text": "so if you get a chance to talk to him you should okay we are done that was a lot uh but",
    "start": "2015639",
    "end": "2024279"
  },
  {
    "text": "in conclusion the list plus watch pattern is the central theme to how kubernetes the kubernetes Machine Works",
    "start": "2024279",
    "end": "2030480"
  },
  {
    "text": "different requests interact differently with each layer of the storage layer of kubernetes depending on the resource",
    "start": "2030480",
    "end": "2037200"
  },
  {
    "text": "version specified the resource version match specified and a combination of those two and specification of these",
    "start": "2037200",
    "end": "2042880"
  },
  {
    "text": "parameters can help you make the trade-off between data consistency latency and that can majorly impact",
    "start": "2042880",
    "end": "2049158"
  },
  {
    "text": "scalability of your cluster now unless you have strict consistency guarantees rely on the watch cache trust it but",
    "start": "2049159",
    "end": "2056839"
  },
  {
    "text": "also be mindful of the scale rates problem thank you for listening and",
    "start": "2056839",
    "end": "2062720"
  },
  {
    "text": "coming for the last session of the day if you want to reach out please fre to this",
    "start": "2062720",
    "end": "2068839"
  },
  {
    "text": "than",
    "start": "2068839",
    "end": "2071839"
  }
]