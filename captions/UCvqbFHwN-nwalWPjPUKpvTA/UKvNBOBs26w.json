[
  {
    "text": "thank you for all being here on the last session of a very busy conference um excited that I'm not speaking to an",
    "start": "120",
    "end": "6080"
  },
  {
    "text": "empty room uh so my name is Kevin Hannon I'm a senior software engineer at red",
    "start": "6080",
    "end": "11840"
  },
  {
    "text": "hat and hello I'm AR at Google uh so can",
    "start": "11840",
    "end": "17760"
  },
  {
    "text": "I get a show of hands who here knows what a working group is in",
    "start": "17760",
    "end": "22840"
  },
  {
    "text": "kubernetes okay so I will go a little I'll explain it so uh as we all know",
    "start": "22840",
    "end": "29560"
  },
  {
    "text": "kubernetes has these things called special interest groups these are essentially the people",
    "start": "29560",
    "end": "35120"
  },
  {
    "text": "that own a lot of the code in kubernetes what's important for batch is like Sig node Sig apps Sig Auto scaling and Sig",
    "start": "35120",
    "end": "43760"
  },
  {
    "text": "scheduling uh and generally it's a kind of difficult to get features into kubernetes cuz sometimes these groups",
    "start": "43760",
    "end": "49280"
  },
  {
    "text": "are a little bit siloed the idea of a working group is to try to say hey we have a major pain point in kubernetes",
    "start": "49280",
    "end": "56120"
  },
  {
    "text": "and back in I think it was 2022 the working group bachor was created to kind of address the problem that running",
    "start": "56120",
    "end": "62440"
  },
  {
    "text": "batch workloads in kubernetes is way too hard so uh generally there's was a large",
    "start": "62440",
    "end": "69280"
  },
  {
    "text": "amount of people uh having their own apis for running batch jobs it was",
    "start": "69280",
    "end": "75560"
  },
  {
    "text": "honestly it was pretty chaotic and there was kind of this we need we we saw this",
    "start": "75560",
    "end": "80880"
  },
  {
    "text": "need to improve the the batch API ecosystem around",
    "start": "80880",
    "end": "86200"
  },
  {
    "text": "kubernetes uh and try to make like a single API workload for actually running",
    "start": "86200",
    "end": "91560"
  },
  {
    "text": "batch jobs uh Martin will Marin will talk about uh Q in the second half of",
    "start": "91560",
    "end": "97600"
  },
  {
    "text": "this talk for the rest of this talk I want to talk about what we're doing to make the job API useful for uh folks",
    "start": "97600",
    "end": "103759"
  },
  {
    "text": "that want to run batch workloads uh and so if you want to get",
    "start": "103759",
    "end": "109200"
  },
  {
    "text": "one takeaway from this talk uh if you're interested in this please join the the batch slack Channel that's where most of",
    "start": "109200",
    "end": "116280"
  },
  {
    "text": "our discussion happens in real time uh follow the and then follow the Sig projects like q",
    "start": "116280",
    "end": "123000"
  },
  {
    "text": "and I'll talk about Job set and this is kind of and you can join the mailing list too there will be some discussion",
    "start": "123000",
    "end": "128520"
  },
  {
    "text": "on there and we also have bi-weekly meetings we have a a once a month",
    "start": "128520",
    "end": "133599"
  },
  {
    "text": "meeting that's friendly for Europeans and once a month meeting that's friendly for uh our Asian co-workers and West",
    "start": "133599",
    "end": "140440"
  },
  {
    "text": "Coast folks and all this is on the QR code has some information about how to join these meetings and everything so",
    "start": "140440",
    "end": "147160"
  },
  {
    "text": "hope to see one of you there so without further Ado what I wanted to talk about",
    "start": "147160",
    "end": "152360"
  },
  {
    "text": "was the job API so U who here has actually used the",
    "start": "152360",
    "end": "157720"
  },
  {
    "text": "job API all right a few uh so generally at a",
    "start": "157720",
    "end": "163720"
  },
  {
    "text": "high level it's it looks very similar to like a deployment we have a pod and we have this thing called parallelisms is a",
    "start": "163720",
    "end": "170480"
  },
  {
    "text": "number of replicas and so this is kind of how many pods are replicated uh and then we have a thing called back off",
    "start": "170480",
    "end": "176760"
  },
  {
    "text": "limit I'll talk a lot about that in this talk so I want to highlight that uh these this is just this idea of how many",
    "start": "176760",
    "end": "181959"
  },
  {
    "text": "retries a pod has before you declare the job is failed uh and so the next step what we",
    "start": "181959",
    "end": "190640"
  },
  {
    "text": "saw one of the major problems we saw in this space was that most folks that are",
    "start": "190640",
    "end": "196879"
  },
  {
    "text": "running jobs they want uh they want communication between their pods and",
    "start": "196879",
    "end": "202159"
  },
  {
    "text": "they also want a stable name for their pod rank so if you're running like an MPI like job you might have like a rank",
    "start": "202159",
    "end": "208640"
  },
  {
    "text": "zero pod and your rank one and two you want to make sure you can communicate to your rank zero and this is kind of this",
    "start": "208640",
    "end": "215239"
  },
  {
    "text": "idea of an index job it's you can have this feature by a completion Mode called an index and then if you couple that",
    "start": "215239",
    "end": "222000"
  },
  {
    "text": "with a headless service you're actually able to get all your pods to communicate to one another it's kind of an",
    "start": "222000",
    "end": "227200"
  },
  {
    "text": "implementation detail but it's actually if you want to build like a a workload that requires communication between your",
    "start": "227200",
    "end": "233319"
  },
  {
    "text": "pods it's necessary to create this sorry it's necessary to create the",
    "start": "233319",
    "end": "238360"
  },
  {
    "text": "Headless service in conjunction with an index job so one of my favorite features",
    "start": "238360",
    "end": "244200"
  },
  {
    "text": "I've found in the kubernetes community is this one called pod failure policy it seems so simple yet it has C it it",
    "start": "244200",
    "end": "251680"
  },
  {
    "text": "really showed a lot of problems in the ecosystem I was not the author of this one but it was fun to follow this one",
    "start": "251680",
    "end": "257440"
  },
  {
    "text": "over the years and all the discussions that were made even around stuff like hey my pod failed to this exit code and",
    "start": "257440",
    "end": "264040"
  },
  {
    "text": "trying to standardize on some exit codes was kind of a a fun little thing but generally the idea",
    "start": "264040",
    "end": "270440"
  },
  {
    "text": "is we have this pod failure policy stanza and yaml and we say hey what do I want to do if I get a certain exit code",
    "start": "270440",
    "end": "276919"
  },
  {
    "text": "of a container and I want to go ahead and fail the job this matters a lot for",
    "start": "276919",
    "end": "282360"
  },
  {
    "text": "you know if you're running a GPU workload and you have some kind of segmentation fault you don't want to",
    "start": "282360",
    "end": "287680"
  },
  {
    "text": "keep retrying I've seen you know some people are paranoid so they'll set their retry limits to like a million just so",
    "start": "287680",
    "end": "294560"
  },
  {
    "text": "they can get their jobs to keep running and so we find that we want to make sure to kick those pods kick those jobs off",
    "start": "294560",
    "end": "302280"
  },
  {
    "text": "if they're failing for an application failure so that's like on the left hand side that's this idea of using uh pod",
    "start": "302280",
    "end": "308360"
  },
  {
    "text": "failure policy for exit codes an area that was really interesting was seeing",
    "start": "308360",
    "end": "313520"
  },
  {
    "text": "how can you do the Counterpoint where how can I not decrement the retry account if I have an an",
    "start": "313520",
    "end": "320360"
  },
  {
    "text": "infrastructure uh thing like pre my my workload was preempted I shouldn't be penalized and I should not say my job",
    "start": "320360",
    "end": "327120"
  },
  {
    "text": "failed that was my stupid Cloud vendor preempted me so why should I be penalized for that well that's kind of",
    "start": "327120",
    "end": "333639"
  },
  {
    "text": "one other use case we have for the Pod failure policy and like when stuff is preempted or other things you might you",
    "start": "333639",
    "end": "339960"
  },
  {
    "text": "would have a special condition called disruption Target that you could use to not fail your",
    "start": "339960",
    "end": "345759"
  },
  {
    "text": "jobs we have uh the author I see is here that create actually created this",
    "start": "345759",
    "end": "351120"
  },
  {
    "text": "feature pod success policy uh but uh this is not this is kind of the Counterpoint of when a pod of a certain",
    "start": "351120",
    "end": "359000"
  },
  {
    "text": "index is succeeds I want to mark my job as successful uh this is only this",
    "start": "359000",
    "end": "366080"
  },
  {
    "text": "highlights a few of the the features that this working group has sponsored over the last like two or three years uh",
    "start": "366080",
    "end": "374039"
  },
  {
    "text": "most of these are available in most kubernetes clusters anywhere now uh my",
    "start": "374039",
    "end": "379960"
  },
  {
    "text": "my first kep was actually uh the Pod replacement policy in kubernetes which this was this idea of making sure that",
    "start": "379960",
    "end": "387199"
  },
  {
    "text": "we're not terminating pod like sometimes you would see a pod go terminating then the job controller would create a",
    "start": "387199",
    "end": "392919"
  },
  {
    "text": "replacement and this would cause some problems with P torch Frameworks cuz they would require the same you would",
    "start": "392919",
    "end": "398479"
  },
  {
    "text": "have like duplicate ranks and things go Haywire so that that's that",
    "start": "398479",
    "end": "404319"
  },
  {
    "text": "one uh by far the most interesting thing in this space is seeing how many people",
    "start": "404319",
    "end": "410000"
  },
  {
    "text": "are starting to actually become more aware of what the kubernetes community is doing uh I have a couple issues where",
    "start": "410000",
    "end": "416039"
  },
  {
    "text": "people are saying oh hey this thing that the batch working group people are doing is pretty cool I would like to build this integration why don't I go ahead",
    "start": "416039",
    "end": "423400"
  },
  {
    "text": "and do that like if you're not familiar airflow workflow engine it was really",
    "start": "423400",
    "end": "428639"
  },
  {
    "text": "high high it would run it would run pods on kubernetes and",
    "start": "428639",
    "end": "434039"
  },
  {
    "text": "there was a request a few years ago to improve to actually have a kubernetes job as a first class support and we kind",
    "start": "434039",
    "end": "440400"
  },
  {
    "text": "of been seeing that over the last year more and more Frameworks are starting to use the job API because it's more useful",
    "start": "440400",
    "end": "446759"
  },
  {
    "text": "so that's great uh so uh the last area that's kind of an",
    "start": "446759",
    "end": "453720"
  },
  {
    "text": "interesting one is seeing the cube flow Community like they when they're one of the first people for running a lot of",
    "start": "453720",
    "end": "460160"
  },
  {
    "text": "distributed ML workloads and they ran they run into a lot of problems where their users want the job API support but",
    "start": "460160",
    "end": "466919"
  },
  {
    "text": "it was actually really difficult to support this with Cube flow because they had pod and services and I'll talk a",
    "start": "466919",
    "end": "472720"
  },
  {
    "text": "little bit about what the approach they have done but these are other features that they have requested from the CU",
    "start": "472720",
    "end": "478000"
  },
  {
    "text": "plow community and yeah so the last area for a workload",
    "start": "478000",
    "end": "484039"
  },
  {
    "text": "that we know was not well served by uh the job API was this concept of a distributed job so this is idea of I",
    "start": "484039",
    "end": "492280"
  },
  {
    "text": "have a I have multiple uh multiple templates so I might have like you know",
    "start": "492280",
    "end": "497400"
  },
  {
    "text": "with popular Nows Ray for serving or for training you might have like uh your ray",
    "start": "497400",
    "end": "503199"
  },
  {
    "text": "pod is a a different template from your sorry your parent Ray pod is a different template from your worker pods and you",
    "start": "503199",
    "end": "509800"
  },
  {
    "text": "kind of want uh failure policies to be smarter to say like if my leader fails",
    "start": "509800",
    "end": "515320"
  },
  {
    "text": "my entire job needs to go down but if my workers succeed I can kill the leader",
    "start": "515320",
    "end": "520719"
  },
  {
    "text": "and I can mark my job as success and a startup policy was a big one like if you're if you're running like an MPI job",
    "start": "520719",
    "end": "528519"
  },
  {
    "text": "you might want the the MPI launcher to run first and set up all the stuff it needs and all your workers are able to",
    "start": "528519",
    "end": "535080"
  },
  {
    "text": "communicate to the leader or the launcher whatever and we want to make sure they start in order and we also",
    "start": "535080",
    "end": "541320"
  },
  {
    "text": "want that implementation detail of like the Headless service and everything to be uh to to be there so that our pods",
    "start": "541320",
    "end": "549600"
  },
  {
    "text": "can all communicate so that's uh the motivation between the project called job Set uh",
    "start": "549600",
    "end": "556320"
  },
  {
    "text": "the idea is surprise surprise it solves all the use cases we mentioned about a distributed job but the idea is we kind",
    "start": "556320",
    "end": "563160"
  },
  {
    "text": "of wanted all these feature requests and job set and start and allow building you",
    "start": "563160",
    "end": "569440"
  },
  {
    "text": "know make sure all these things are set in stone so we can all kind of build off of this so our first use case was saying",
    "start": "569440",
    "end": "576079"
  },
  {
    "text": "can we represent a pie torch job pretty simply and this is uh a pretty simple",
    "start": "576079",
    "end": "582360"
  },
  {
    "text": "example but it actually it works uh and if you try to do this with a with a job",
    "start": "582360",
    "end": "587720"
  },
  {
    "text": "API if you don't have the Headless service it actually won't work because you can't communicate with your pods so",
    "start": "587720",
    "end": "593160"
  },
  {
    "text": "that's like the first step with the the job set we wanted to try was just can we run a simple py torch job",
    "start": "593160",
    "end": "600440"
  },
  {
    "text": "uh the next one is this idea of success policy so can I say if all my workers",
    "start": "600440",
    "end": "606279"
  },
  {
    "text": "are if my let me see uh if my workers finish I want to mark my job set as",
    "start": "606279",
    "end": "612959"
  },
  {
    "text": "complete I don't and then I don't really care what the leader does the leader is only there to coordinate work and then I",
    "start": "612959",
    "end": "619200"
  },
  {
    "text": "don't really need him to finish and so this kind of you know uh it's one of the main use cases for like worker and",
    "start": "619200",
    "end": "626399"
  },
  {
    "text": "leaders uh startup policy was a feature where we wanted to make sure that like",
    "start": "626399",
    "end": "632720"
  },
  {
    "text": "the driver would actually start first and if you're not familiar in kubernetes",
    "start": "632720",
    "end": "637760"
  },
  {
    "text": "we have this concept of Readiness probe so our basis for this is saying a pod is",
    "start": "637760",
    "end": "643120"
  },
  {
    "text": "considered ready if it passes the Readiness probes so the idea is if and a lot of projects will set these and so",
    "start": "643120",
    "end": "650519"
  },
  {
    "text": "once these things are passed your pod is ready to uh can be considered active or",
    "start": "650519",
    "end": "656160"
  },
  {
    "text": "ready actually and then after that stage it goes on to the workers and starts those so that if your driver was had to",
    "start": "656160",
    "end": "663200"
  },
  {
    "text": "be set up and running that's why it's there uh now one of the things that's",
    "start": "663200",
    "end": "669160"
  },
  {
    "text": "been interesting to see with job set is I I view the kubernetes Sig projects as dog fooding for uh for users because we",
    "start": "669160",
    "end": "677279"
  },
  {
    "text": "want to make sure the apis we build are useful and job set one of our main goals was really to use Concepts like the Pod",
    "start": "677279",
    "end": "684399"
  },
  {
    "text": "failure policy of a job and how does that work if you have a higher level controller how do you react to that and",
    "start": "684399",
    "end": "690760"
  },
  {
    "text": "so the idea this is a relatively new feature but for the failure policy it was this idea of saying if my if my",
    "start": "690760",
    "end": "698959"
  },
  {
    "text": "leader fails I want to fail a job set if it fails due to a pod failure policy I",
    "start": "698959",
    "end": "704000"
  },
  {
    "text": "want to go ahead and uh either fail it or restart based off of you know the rules you set in",
    "start": "704000",
    "end": "710680"
  },
  {
    "text": "stone uh an area that I think has been super exciting to see and there was a",
    "start": "710680",
    "end": "715839"
  },
  {
    "text": "talk I think it was yesterday talking about the next rendition of the training operator and Cube flow and if you see in",
    "start": "715839",
    "end": "723079"
  },
  {
    "text": "the top top little icon there job set is playing a crucial component in uh the",
    "start": "723079",
    "end": "728160"
  },
  {
    "text": "next version of the cube flow training operator because they want all the functionality that the job API has",
    "start": "728160",
    "end": "733600"
  },
  {
    "text": "provided over the years and they want to kind of build off of that and start stream and start really thinking more",
    "start": "733600",
    "end": "738920"
  },
  {
    "text": "about how to improve the machine learning life cycle rather than having to figure out all the kubernetes",
    "start": "738920",
    "end": "746000"
  },
  {
    "text": "stuff uh and Mar and without that like the co speaker okay hello everyone uh let's get",
    "start": "746000",
    "end": "754920"
  },
  {
    "text": "to Q you might have seen the Keynotes so you may already have a little bit of",
    "start": "754920",
    "end": "760079"
  },
  {
    "text": "introduction into what Q is about but let me start uh with why we started this",
    "start": "760079",
    "end": "766320"
  },
  {
    "text": "project in the first place so users in kubernetes run into a number of problems",
    "start": "766320",
    "end": "772600"
  },
  {
    "text": "for example which of the 100 taining jobs or data processing pipelines should be running at the given time on Limited",
    "start": "772600",
    "end": "779320"
  },
  {
    "text": "resources kubernetes doesn't handle this well how to ensure that all parts of a",
    "start": "779320",
    "end": "784880"
  },
  {
    "text": "job will quickly scheduled before actually starting the uh this uh the",
    "start": "784880",
    "end": "790000"
  },
  {
    "text": "process how to allow users uh uh as many spot to use as many spot instances as",
    "start": "790000",
    "end": "796959"
  },
  {
    "text": "possible but limit their on demand capacity how to do uh All Above without",
    "start": "796959",
    "end": "802680"
  },
  {
    "text": "replacing lots of stuff in the kubernetes how to set up a resource",
    "start": "802680",
    "end": "807880"
  },
  {
    "text": "quarter so that they can be fairly distributed among the uh teams that are",
    "start": "807880",
    "end": "813000"
  },
  {
    "text": "using them or not using them and in the end how to schedule workload based on their networking needs so Q is a kind of",
    "start": "813000",
    "end": "821959"
  },
  {
    "text": "bat scheduling and admission system that decides which of the jobs run at the given moment and on which of machines it",
    "start": "821959",
    "end": "829279"
  },
  {
    "text": "provides Advanced resource controls like Hardware specific quot so with Q you can",
    "start": "829279",
    "end": "834959"
  },
  {
    "text": "specify the quot for a particular type of machines for example the separate quar for the newest and",
    "start": "834959",
    "end": "841839"
  },
  {
    "text": "greatest gpus and separate quas for Less fancy uh CPUs and gpus there is a",
    "start": "841839",
    "end": "849560"
  },
  {
    "text": "concept of qu sharing and borrowing so that you can set uh qu for a team or",
    "start": "849560",
    "end": "857120"
  },
  {
    "text": "multiple teams and if one of the team is not using them then others may borrow it",
    "start": "857120",
    "end": "863399"
  },
  {
    "text": "the we provide different policies for preemptions and qu reclamations in case of borrow qu we have job level",
    "start": "863399",
    "end": "870320"
  },
  {
    "text": "priorities and Q doesn't replace any of the kubernetes",
    "start": "870320",
    "end": "875880"
  },
  {
    "text": "component Q tries to meet users where they are it provides integration with",
    "start": "875880",
    "end": "882279"
  },
  {
    "text": "kubernetes job with all of the cube portfolio with Ray job with job set with Standalone pods with pods that are uh",
    "start": "882279",
    "end": "890759"
  },
  {
    "text": "grouped via annotation with deployment stateful set and very very soon with",
    "start": "890759",
    "end": "896000"
  },
  {
    "text": "leader worker set okay someone is calling Q please take",
    "start": "896000",
    "end": "902920"
  },
  {
    "text": "it okay so Q has been around for quite a while it's relatively new but also",
    "start": "902920",
    "end": "909839"
  },
  {
    "text": "relatively mature project we have a number of users that are using it and those users are requesting more and more",
    "start": "909839",
    "end": "917519"
  },
  {
    "text": "features one of the uh uh very important features for our user was to be able to",
    "start": "917519",
    "end": "925240"
  },
  {
    "text": "take advantage of uh networking structure so many of ml workloads",
    "start": "925240",
    "end": "933360"
  },
  {
    "text": "require quite uh a lot of communication so POS are talking to each other and the",
    "start": "933360",
    "end": "938680"
  },
  {
    "text": "further the P are the more problems come with the communication the Laten is bigger the bandwidth is smaller and uh",
    "start": "938680",
    "end": "946480"
  },
  {
    "text": "because of that the computation may take longer gpus are super expensive these uh",
    "start": "946480",
    "end": "952800"
  },
  {
    "text": "days and the sooner the computation completes the cheaper the computation is",
    "start": "952800",
    "end": "960000"
  },
  {
    "text": "okay so uh what does the topology of the network",
    "start": "960000",
    "end": "968360"
  },
  {
    "text": "look like so probably you have been running couple of works in the cloud already so the first thing of the",
    "start": "968360",
    "end": "974079"
  },
  {
    "text": "topology is the zone so most of the clouds are have a concept of Zone Zone",
    "start": "974079",
    "end": "981279"
  },
  {
    "text": "basically you can think about it as a uh building in the data center so they have",
    "start": "981279",
    "end": "986759"
  },
  {
    "text": "a data center I don't know somewhere in Oregon and they have three buildings each building is a separate Zone inside",
    "start": "986759",
    "end": "992800"
  },
  {
    "text": "of these buildings they have rooms these rooms may be called differently from cloud provider to cloud provider but",
    "start": "992800",
    "end": "999279"
  },
  {
    "text": "let's use a name block for them inside of these rooms they have big",
    "start": "999279",
    "end": "1004319"
  },
  {
    "text": "rocks uh where they put their servers uh their servers well are in the",
    "start": "1004319",
    "end": "1010519"
  },
  {
    "text": "Rock and there are physical machines each of the physical machine run uh one or more VMS obviously the most bandwidth",
    "start": "1010519",
    "end": "1018600"
  },
  {
    "text": "and the lowest latency is at the VM and physical layer then the the next thing",
    "start": "1018600",
    "end": "1024400"
  },
  {
    "text": "is the rck in R each rack usually have its individual switch and a lot of",
    "start": "1024400",
    "end": "1031079"
  },
  {
    "text": "throughput and very very very small latency latency is bigger when communication goes from one room to",
    "start": "1031079",
    "end": "1037438"
  },
  {
    "text": "another room and even bigger if it needs to uh go from one building to",
    "start": "1037439",
    "end": "1043760"
  },
  {
    "text": "another so uh workloads would like to take advantage for for example a very",
    "start": "1043760",
    "end": "1050039"
  },
  {
    "text": "talkative workload may say hey I require to run on a rock if you don't have a",
    "start": "1050039",
    "end": "1057120"
  },
  {
    "text": "rock available for me and I have 20 pods to to run please please keep me on hold",
    "start": "1057120",
    "end": "1063080"
  },
  {
    "text": "because running this computation uh in a scattered way doesn't make sense at all",
    "start": "1063080",
    "end": "1069400"
  },
  {
    "text": "okay this is one sort of workloads other workloads may be a little bit less stative and say hey I would prefer to be",
    "start": "1069400",
    "end": "1076159"
  },
  {
    "text": "on rack but if you could put me on a block or in this room in the data center",
    "start": "1076159",
    "end": "1082039"
  },
  {
    "text": "that would be good enough so to allow that we introduce two annotations that",
    "start": "1082039",
    "end": "1087960"
  },
  {
    "text": "you can put on uh your workloads you can put either I require topology or I",
    "start": "1087960",
    "end": "1094280"
  },
  {
    "text": "require uh I prefer topology and in the value of this annotation you would need",
    "start": "1094280",
    "end": "1099960"
  },
  {
    "text": "to specify one of these layers that are here these layers are obviously uh",
    "start": "1099960",
    "end": "1105679"
  },
  {
    "text": "specific to Cloud providers or to your onr environment they may look",
    "start": "1105679",
    "end": "1111440"
  },
  {
    "text": "differently on cloud providers they may have different values however Q tries to understand all of them and uh build a",
    "start": "1111440",
    "end": "1118960"
  },
  {
    "text": "three like structure out of them okay how does it look like so the workload",
    "start": "1118960",
    "end": "1124400"
  },
  {
    "text": "puts The annotation it goes to q q checks what is available in the cluster",
    "start": "1124400",
    "end": "1130000"
  },
  {
    "text": "and decides whether the workload can be admitted or not if the workload can be admitted it is admitted the pods are",
    "start": "1130000",
    "end": "1136480"
  },
  {
    "text": "being created and while the pods are being created we catch each of them and assign them the uh right topology via uh",
    "start": "1136480",
    "end": "1144640"
  },
  {
    "text": "not selector okay next big thing is uh in Q",
    "start": "1144640",
    "end": "1152039"
  },
  {
    "text": "is per sharing so as I said you can specify a quot in Q you can specify this",
    "start": "1152039",
    "end": "1159960"
  },
  {
    "text": "quot for different quot for different teams you can put uh teams in a cohort",
    "start": "1159960",
    "end": "1165360"
  },
  {
    "text": "so that one team can borrow unused capacity from the other but there is a",
    "start": "1165360",
    "end": "1170880"
  },
  {
    "text": "problem what to do if suddenly there is a lot of unused capacity what to do it",
    "start": "1170880",
    "end": "1177440"
  },
  {
    "text": "how to split it if we split it into in a first Camp first save manner then the",
    "start": "1177440",
    "end": "1184080"
  },
  {
    "text": "teams that are uh more East uh like on the East Coast would be unfairly",
    "start": "1184080",
    "end": "1190039"
  },
  {
    "text": "advantaged because they are coming to work earlier and they can grab unused resources that's not fair and the team",
    "start": "1190039",
    "end": "1197120"
  },
  {
    "text": "on the west code would complain so to fight with that we are introducing",
    "start": "1197120",
    "end": "1203320"
  },
  {
    "text": "Fair sharing in uh in Q so that the unused capacity can be distributed uh",
    "start": "1203320",
    "end": "1210559"
  },
  {
    "text": "across the team based on their needs if only one team needs uh extra capacity",
    "start": "1210559",
    "end": "1216000"
  },
  {
    "text": "then this team gets it if suddenly some other team starts to need it then the",
    "start": "1216000",
    "end": "1222520"
  },
  {
    "text": "team that got too much capacity gets a couple of preemption and the team that",
    "start": "1222520",
    "end": "1227799"
  },
  {
    "text": "just show up with more needs will get the extra",
    "start": "1227799",
    "end": "1233480"
  },
  {
    "text": "capacity okay so how do you enable first sharing you need to slightly modify Q",
    "start": "1233600",
    "end": "1240480"
  },
  {
    "text": "config each uh cluster queue in cohort can specify some weights uh that can",
    "start": "1240480",
    "end": "1247360"
  },
  {
    "text": "influence for sharing for example one team can or one cluster queue can be three or four times more important than",
    "start": "1247360",
    "end": "1253520"
  },
  {
    "text": "the others and in case of uh for sharing they should get more resources than the others",
    "start": "1253520",
    "end": "1259400"
  },
  {
    "text": "first sharing is done within the cohort so if you have separate cohorts then separate cohorts don't share and there",
    "start": "1259400",
    "end": "1265960"
  },
  {
    "text": "is no first sharing between them and the new feature that is not yet completed",
    "start": "1265960",
    "end": "1271320"
  },
  {
    "text": "but is coming to queue uh very very shortly is here Kar cohorts so couple of",
    "start": "1271320",
    "end": "1278080"
  },
  {
    "text": "our customers want to put their or entire organization inside of que entire",
    "start": "1278080",
    "end": "1284480"
  },
  {
    "text": "organizations have complex needs they have company politics and usually they have three structures right now Q offers",
    "start": "1284480",
    "end": "1292279"
  },
  {
    "text": "you a flat structure so you can have cluster cues or teams whatever you call",
    "start": "1292279",
    "end": "1297559"
  },
  {
    "text": "them and you can put them in a single group but that doesn't match well with",
    "start": "1297559",
    "end": "1304039"
  },
  {
    "text": "complex organization so with the next release of Q will introduce her keyar Fair sharing so that",
    "start": "1304039",
    "end": "1311640"
  },
  {
    "text": "you can build a her here of your organization and then ensure that uh",
    "start": "1311640",
    "end": "1316679"
  },
  {
    "text": "this Fair sharing happens at the appropriate level of uh the organization",
    "start": "1316679",
    "end": "1322159"
  },
  {
    "text": "so that the teams first try to consume and distribute free resources within their",
    "start": "1322159",
    "end": "1329799"
  },
  {
    "text": "uh all small cohorts for example one manager wants to uh distribute first",
    "start": "1329799",
    "end": "1336400"
  },
  {
    "text": "their unus resources to the to his team then maybe borrow it to the Nate boring manager under the same director then",
    "start": "1336400",
    "end": "1343679"
  },
  {
    "text": "this goes upper one director will would like to fulfill the needs of his organization before starting borrowing",
    "start": "1343679",
    "end": "1350000"
  },
  {
    "text": "to some others and so on and so on up to the very top of the organization and this thing is coming in the next release",
    "start": "1350000",
    "end": "1357279"
  },
  {
    "text": "of Q okay what else do we have in Q uh Q",
    "start": "1357279",
    "end": "1363039"
  },
  {
    "text": "is uh run by kubernetes objects all kubernetes objects sits in yaml",
    "start": "1363039",
    "end": "1369320"
  },
  {
    "text": "kubernetes administrators needs to play with yaml and some of them don't really",
    "start": "1369320",
    "end": "1374880"
  },
  {
    "text": "like playing with yaml too much and many things cannot be easily done with yamos",
    "start": "1374880",
    "end": "1380679"
  },
  {
    "text": "and putting some extra automation for common task in yamos can be a little bit cumbersome so to fight with that we are",
    "start": "1380679",
    "end": "1389440"
  },
  {
    "text": "in we introduced actually a half a year ago something that we called qctl",
    "start": "1389440",
    "end": "1395120"
  },
  {
    "text": "QC TL is a cctl plug-in managed by crew and provides you with quite nice",
    "start": "1395120",
    "end": "1402799"
  },
  {
    "text": "functionality to uh manage your Q environment easier so you can create Q",
    "start": "1402799",
    "end": "1409840"
  },
  {
    "text": "with resource flavors uh you can uh list them you can delete them you can list workloads and",
    "start": "1409840",
    "end": "1416480"
  },
  {
    "text": "see where they are in the queue you can pause some admission for workloads you",
    "start": "1416480",
    "end": "1422679"
  },
  {
    "text": "can stop cues you can drain them you can do all of the nice things that otherwise",
    "start": "1422679",
    "end": "1427720"
  },
  {
    "text": "you would have hard time doing with regular uh Cube",
    "start": "1427720",
    "end": "1433559"
  },
  {
    "text": "CTL okay so we have Foundation layer covered we have admins covered but we",
    "start": "1433559",
    "end": "1440600"
  },
  {
    "text": "have researchers that also are not particularly happy with",
    "start": "1440600",
    "end": "1446039"
  },
  {
    "text": "yamas uh and they have a problems they want to have uh they want to start lots",
    "start": "1446039",
    "end": "1451080"
  },
  {
    "text": "of oneoff jobs with complex volume and storage configuration and with very very",
    "start": "1451080",
    "end": "1456559"
  },
  {
    "text": "little differences between each of the run but still with differences they would like to use command line as much",
    "start": "1456559",
    "end": "1462960"
  },
  {
    "text": "as possible because they spent like 20 years running slurm scripts in Academia",
    "start": "1462960",
    "end": "1469720"
  },
  {
    "text": "uh and they also would like to share their configuration with teammates so to make their life easier",
    "start": "1469720",
    "end": "1477799"
  },
  {
    "text": "we are introducing KOB KOB is a tool that",
    "start": "1477799",
    "end": "1483080"
  },
  {
    "text": "provides reusable templates stored on the API API server site with a c CLI that",
    "start": "1483080",
    "end": "1491320"
  },
  {
    "text": "transforms them into yamos and send the API server for execution that uh may sound",
    "start": "1491320",
    "end": "1499159"
  },
  {
    "text": "quite complex but the bottom line is that we want to create a template put in",
    "start": "1499159",
    "end": "1504720"
  },
  {
    "text": "the server and have it created by the system administrator who knows how to set up the volumes how to set up the",
    "start": "1504720",
    "end": "1511600"
  },
  {
    "text": "entire kubernetes environment we want to give this template to researchers whose",
    "start": "1511600",
    "end": "1517120"
  },
  {
    "text": "only task would be to provide couple of missing elements in this template like",
    "start": "1517120",
    "end": "1522159"
  },
  {
    "text": "the command that they want to run maybe the number of PS they run to execute their job on or maybe some some",
    "start": "1522159",
    "end": "1529320"
  },
  {
    "text": "different requests for the PO and very very tiny amount of things that wouldn't require that person",
    "start": "1529320",
    "end": "1536880"
  },
  {
    "text": "to understand all of the nasty details of running bad jobs on kubernetes okay",
    "start": "1536880",
    "end": "1544279"
  },
  {
    "text": "so the flow is very simple ad me set up the template researcher pick ups the template provides some extra data via",
    "start": "1544279",
    "end": "1550520"
  },
  {
    "text": "common line tool the job is submitted and all people are happy uh system administrator because he is not backed",
    "start": "1550520",
    "end": "1557559"
  },
  {
    "text": "by researchers researchers because they have very very similar environment that they used to have for the past 20 years",
    "start": "1557559",
    "end": "1564440"
  },
  {
    "text": "and uh clouds providers are happy because uh lots of computations happens",
    "start": "1564440",
    "end": "1571360"
  },
  {
    "text": "there okay uh one more thing here about K job what is supported so we support",
    "start": "1571360",
    "end": "1578279"
  },
  {
    "text": "job job set we are planning to Pro cover all of the things that uh are covered by",
    "start": "1578279",
    "end": "1584520"
  },
  {
    "text": "Q so all of the API there and one thing that is not mentioned here I forgot to put this slide somehow is that we are",
    "start": "1584520",
    "end": "1591840"
  },
  {
    "text": "trying to mimic slam environment using this thing so we want to provide users",
    "start": "1591840",
    "end": "1598919"
  },
  {
    "text": "with common line tooling common line Flags common line arguments and",
    "start": "1598919",
    "end": "1604399"
  },
  {
    "text": "configuration that tries to uh reflect slur environment as much as possible uh",
    "start": "1604399",
    "end": "1611919"
  },
  {
    "text": "K job currently lives in cure repository but hopefully soon we'll get in dependent repository and much more",
    "start": "1611919",
    "end": "1620440"
  },
  {
    "text": "Spotlight uh so a little bit information on how we view this entire ecosystem so",
    "start": "1620440",
    "end": "1628039"
  },
  {
    "text": "on the bottom we've got kubernetes okay so that's that's given on top of it we",
    "start": "1628039",
    "end": "1633080"
  },
  {
    "text": "put a extra scheduling layer that fills the gaps that are in kubernetes on the",
    "start": "1633080",
    "end": "1640240"
  },
  {
    "text": "top of it there is a qctl tool for administrator and set of apis for",
    "start": "1640240",
    "end": "1647480"
  },
  {
    "text": "researchers to run their job and to make their life easier to allow them to play",
    "start": "1647480",
    "end": "1652919"
  },
  {
    "text": "with common line tools instead of yaml files we introduce K job that encompasses all of the user researcher",
    "start": "1652919",
    "end": "1660840"
  },
  {
    "text": "experience okay uh that's all what we have present prepared for this session",
    "start": "1660840",
    "end": "1666960"
  },
  {
    "text": "now there is a time for questions [Applause]",
    "start": "1666960",
    "end": "1678190"
  },
  {
    "text": "was excellent presentation thank you uh the job set is very exciting my company um we've we've had a pretty hacked up",
    "start": "1683559",
    "end": "1689960"
  },
  {
    "text": "setup for the past like five six years where we use a bunch of crown jobs that",
    "start": "1689960",
    "end": "1695480"
  },
  {
    "text": "kind of do sort of like job set there's one feature that seems like it would be obvious to",
    "start": "1695480",
    "end": "1701600"
  },
  {
    "text": "add which is not only do you run say a set of jobs but the success policy would have like a finalizer job that's",
    "start": "1701600",
    "end": "1708760"
  },
  {
    "text": "something we do a lot we have ml jobs that all run on different data sets and a finalizer that sits there in an init",
    "start": "1708760",
    "end": "1714760"
  },
  {
    "text": "container pending for like days waiting for all those to complete watching for",
    "start": "1714760",
    "end": "1721840"
  },
  {
    "text": "their logs and then kicking off something to upload data or whatever just seems like it we had now that might",
    "start": "1721840",
    "end": "1727640"
  },
  {
    "text": "overlap some other projects I guess like Cube flow might be never actually use",
    "start": "1727640",
    "end": "1733120"
  },
  {
    "text": "Cube FL I'm assuming that's something where you you would do that if this then that sort of jobs but I it's just a",
    "start": "1733120",
    "end": "1739159"
  },
  {
    "text": "thought yeah the uh in Cube flow we are discussing this idea of like expanding",
    "start": "1739159",
    "end": "1745600"
  },
  {
    "text": "the squential execution model uh because in Cube flow V2 they're talking about",
    "start": "1745600",
    "end": "1751640"
  },
  {
    "text": "having like a a model ini initialization step where they kind of pull down the images for the first step I don't know",
    "start": "1751640",
    "end": "1758200"
  },
  {
    "text": "if we've talked much about the like a finalizer job but uh the idea is that",
    "start": "1758200",
    "end": "1764000"
  },
  {
    "text": "like we would kind of have like a bit more uh functionality than the startup policy because for yeah for v2 Cube flow",
    "start": "1764000",
    "end": "1772200"
  },
  {
    "text": "like they do kind of want the initializer to pull and if you have like an NPI job you might have the launcher",
    "start": "1772200",
    "end": "1777640"
  },
  {
    "text": "be ready in there I don't know if uh if that doesn't fit I suggest you could",
    "start": "1777640",
    "end": "1783360"
  },
  {
    "text": "open up an issue on job set and we can discuss",
    "start": "1783360",
    "end": "1787840"
  },
  {
    "text": "there I was wondering if you had any um thoughts towards doing sort of a larger scale uh aspect of the job flows with",
    "start": "1790679",
    "end": "1798279"
  },
  {
    "text": "something like workflows so like be able to specify a directed asy graph of jobs",
    "start": "1798279",
    "end": "1805279"
  },
  {
    "text": "yeah so uh for our use we don't do ml so Cube flow doesn't really fit as well",
    "start": "1805279",
    "end": "1812279"
  },
  {
    "text": "yeah so I actually found out this conference when I was or I found it out a few weeks ago but I got confirmation",
    "start": "1812279",
    "end": "1818799"
  },
  {
    "text": "from a company called outer bounds who uses metaflow where they were actually using Argo workflows with job set and",
    "start": "1818799",
    "end": "1825880"
  },
  {
    "text": "they actually had like we had we did Implement a feature for them to kind of like you can always use something like",
    "start": "1825880",
    "end": "1831240"
  },
  {
    "text": "Argo workflows with uh arbitrary like resources so a job set is possible there",
    "start": "1831240",
    "end": "1836760"
  },
  {
    "text": "to kind of build the pipelines we've kind of and we've had to implement our own",
    "start": "1836760",
    "end": "1842000"
  },
  {
    "text": "but it I was you know wondering if they thought about moving that into batch because it strikes me as a relatively common like for his questions of he",
    "start": "1842000",
    "end": "1849279"
  },
  {
    "text": "wants job a then job B yeah I mean I we've always discussed",
    "start": "1849279",
    "end": "1855720"
  },
  {
    "text": "it it's I think the challenge is there's a lot of workflow engines and we don't",
    "start": "1855720",
    "end": "1861960"
  },
  {
    "text": "really like Argo is a great tool a great project and like we don't really want to",
    "start": "1861960",
    "end": "1867880"
  },
  {
    "text": "take on the burden of trying to create the best workflow engine in the world because everyone else seems to have tried that already so like we were kind",
    "start": "1867880",
    "end": "1875679"
  },
  {
    "text": "of being a little selective on what we're trying to bring into the project thank you thank",
    "start": "1875679",
    "end": "1883799"
  },
  {
    "text": "you so um the K job SE seems like super useful and and could be",
    "start": "1884639",
    "end": "1892480"
  },
  {
    "text": "really useful not just for like one Shop jobs like there's plenty of users who",
    "start": "1892480",
    "end": "1898399"
  },
  {
    "text": "would love to just have a CLI to like create a deployment and a service um but",
    "start": "1898399",
    "end": "1904039"
  },
  {
    "text": "like that starts to look a little bit more like like a Helm registry with like a slightly nicer CLI on top how do you",
    "start": "1904039",
    "end": "1910799"
  },
  {
    "text": "kind of see where that line is Oh I don't know to be honest where",
    "start": "1910799",
    "end": "1918120"
  },
  {
    "text": "this line is so we wanted to solve the primary problem was which was uh uh",
    "start": "1918120",
    "end": "1924880"
  },
  {
    "text": "researchers loving slur but have being forced to run on kubernetes against their will okay so we wanted to create",
    "start": "1924880",
    "end": "1932360"
  },
  {
    "text": "some environment that would gradually transfer them from uh running things on",
    "start": "1932360",
    "end": "1937960"
  },
  {
    "text": "slurm with common line tool into uh uh embracing yamos and loving kubernetes",
    "start": "1937960",
    "end": "1944519"
  },
  {
    "text": "okay and of course we cannot do it in one hop we create created this multihop experience for them that was the primary",
    "start": "1944519",
    "end": "1952039"
  },
  {
    "text": "goal uh can it be generalized further into deployments uh whatever yes it could be",
    "start": "1952039",
    "end": "1961200"
  },
  {
    "text": "uh but I'm not sure if it's uh the right path so uh use cases like I want to",
    "start": "1961200",
    "end": "1969279"
  },
  {
    "text": "create uh 20 different deployments every other day uh are not that common as I",
    "start": "1969279",
    "end": "1976679"
  },
  {
    "text": "want to create 20 jobs so probably if we Tred to generalize it we wouldn't hit",
    "start": "1976679",
    "end": "1983120"
  },
  {
    "text": "this very small uh Target group uh if you belong to this target group and you",
    "start": "1983120",
    "end": "1990960"
  },
  {
    "text": "see the value in expanding KOB in the Direction please please reach out to us",
    "start": "1990960",
    "end": "1997000"
  },
  {
    "text": "and we will be very happy to discuss options right now we are trying to complete the uh this experience for B ml",
    "start": "1997000",
    "end": "2006120"
  },
  {
    "text": "training and HPC type of",
    "start": "2006120",
    "end": "2010760"
  },
  {
    "text": "workloads okay anyone going once going twice okay folks",
    "start": "2017039",
    "end": "2025080"
  },
  {
    "text": "thank you uh for joining us that late have a very very safe uh trip back home",
    "start": "2025080",
    "end": "2031320"
  },
  {
    "text": "and see you soon hopefully in London or next year in us take care bye-bye thank",
    "start": "2031320",
    "end": "2038279"
  },
  {
    "text": "you everyone",
    "start": "2038279",
    "end": "2041159"
  }
]