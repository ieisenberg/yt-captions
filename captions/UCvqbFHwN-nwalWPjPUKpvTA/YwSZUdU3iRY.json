[
  {
    "start": "0",
    "end": "47000"
  },
  {
    "text": "hi hi everyone again my name is abdullah i'm a staff software engineer at google today i'm going to present our open",
    "start": "240",
    "end": "6960"
  },
  {
    "text": "source work uh named queue that's a play on uh queue with the with the queue it's",
    "start": "6960",
    "end": "12880"
  },
  {
    "text": "the kubernetes native um job queueing um uh controller um just i want to see that this is a",
    "start": "12880",
    "end": "19520"
  },
  {
    "text": "community uh um a projects and open source project um and we hopefully uh um it started",
    "start": "19520",
    "end": "27039"
  },
  {
    "text": "like um a few months ago it's it's new we had our first release uh really like a couple weeks ago",
    "start": "27039",
    "end": "33520"
  },
  {
    "text": "um we're here to present the idea the high level idea how we think that curing should work or potentially could work in",
    "start": "33520",
    "end": "40960"
  },
  {
    "text": "one way how it could work on top of kubernetes",
    "start": "40960",
    "end": "45840"
  },
  {
    "text": "so here's the agent i'm just going to quickly describe the problem trying to solve what is our core proposal here of",
    "start": "46640",
    "end": "53039"
  },
  {
    "start": "47000",
    "end": "72000"
  },
  {
    "text": "how we handle job curing within kubernetes the apis that we've introduced",
    "start": "53039",
    "end": "58640"
  },
  {
    "text": "how it operates in general uh how it interacts with different controllers that we have",
    "start": "58640",
    "end": "64239"
  },
  {
    "text": "uh in in kubernetes and finally we're gonna summarize",
    "start": "64239",
    "end": "70600"
  },
  {
    "start": "72000",
    "end": "144000"
  },
  {
    "text": "so just being the first talk in this batch working group maybe we should just define what is the",
    "start": "72159",
    "end": "77600"
  },
  {
    "text": "job um the way we're looking at the definition of a job is that any computation that has a start and a",
    "start": "77600",
    "end": "84240"
  },
  {
    "text": "finish so it's mostly computations that run to completion imagine like with service type workloads you have",
    "start": "84240",
    "end": "91280"
  },
  {
    "text": "a process listening on a port all the time it can be infinitely doing that",
    "start": "91280",
    "end": "96720"
  },
  {
    "text": "receives requests and processing them that's why most applications we have in kubernetes like deployment replica set",
    "start": "96720",
    "end": "104320"
  },
  {
    "text": "state will set if your pod dies or finishes it will directly recreate",
    "start": "104320",
    "end": "110079"
  },
  {
    "text": "because the assumption is that this thing this deployment should always continue to run",
    "start": "110079",
    "end": "116159"
  },
  {
    "text": "in contrast the job api it allows a pod to run to completion",
    "start": "116159",
    "end": "121520"
  },
  {
    "text": "and so it will not restart the part and so we have this concept of that the part runs the completion we count how many",
    "start": "121520",
    "end": "127600"
  },
  {
    "text": "comp how many uh how many points are going to get completed and this is how we define that the job finished or not",
    "start": "127600",
    "end": "133599"
  },
  {
    "text": "job status and so this is a very simple definition of a job um",
    "start": "133599",
    "end": "138879"
  },
  {
    "text": "but we find it fits most types of batch workloads",
    "start": "138879",
    "end": "144400"
  },
  {
    "start": "144000",
    "end": "205000"
  },
  {
    "text": "and this kind of like brings us to the second point is like these types of batch workloads they usually aren't",
    "start": "144400",
    "end": "150480"
  },
  {
    "text": "not not just a single part like if we cast these definitions into kubernetes terminology and using pods as a task",
    "start": "150480",
    "end": "157840"
  },
  {
    "text": "they either run independently like monte carlo simulations processing the frames of an image uh processing the trading",
    "start": "157840",
    "end": "165680"
  },
  {
    "text": "days and like you know when you when you uh if you think about fintech or they collaboratively process um a",
    "start": "165680",
    "end": "173760"
  },
  {
    "text": "task basically there's a ton of communication that happens between a group of tasks think about mpi uh doing",
    "start": "173760",
    "end": "180480"
  },
  {
    "text": "and body simulation for example to uh",
    "start": "180480",
    "end": "185840"
  },
  {
    "text": "for example simulate how particles with gravitational power react over time",
    "start": "185840",
    "end": "192640"
  },
  {
    "text": "i just want to hear mention that the gray color is lightly",
    "start": "192640",
    "end": "197680"
  },
  {
    "text": "whitish on this on this screen so please use your imagination",
    "start": "197680",
    "end": "203920"
  },
  {
    "start": "205000",
    "end": "277000"
  },
  {
    "text": "all right so a very important characteristic of batch workloads that we can capitalize on as",
    "start": "205760",
    "end": "212560"
  },
  {
    "text": "people who are trying to develop batch schedulers or batch orchestrators is that they are often flexible on multiple",
    "start": "212560",
    "end": "218560"
  },
  {
    "text": "dimensions they are flexible on time they can sometimes they don't necessarily need to run right now it's",
    "start": "218560",
    "end": "225519"
  },
  {
    "text": "not like a a pro you know a request that you're serving to an end user sometimes they can they can wait",
    "start": "225519",
    "end": "232799"
  },
  {
    "text": "and run their start time can be pushed further ahead they're sometimes flexible on location",
    "start": "232799",
    "end": "237920"
  },
  {
    "text": "if you think about cloud you have multiple regions multiple zones they don't necessarily all the time need to",
    "start": "237920",
    "end": "244239"
  },
  {
    "text": "run in a specific region they could run in different in a different zone for",
    "start": "244239",
    "end": "250319"
  },
  {
    "text": "example of course there is data locality restrictions but giving but taking that",
    "start": "250319",
    "end": "256560"
  },
  {
    "text": "reconciliation they could still run on different places they are also flexible in types of resources uh they could for",
    "start": "256560",
    "end": "263520"
  },
  {
    "text": "example run different gpu types or different architecture like cpu",
    "start": "263520",
    "end": "270000"
  },
  {
    "text": "architectures intel versus for example amd",
    "start": "270000",
    "end": "276240"
  },
  {
    "start": "277000",
    "end": "296000"
  },
  {
    "text": "um so what is your opinion the problem that we're trying to solve but at the higher the high level it's basically managing a limited pool of resources uh",
    "start": "278320",
    "end": "285840"
  },
  {
    "text": "by multiple tenants so you have multiple users",
    "start": "285840",
    "end": "290759"
  },
  {
    "start": "296000",
    "end": "356000"
  },
  {
    "text": "thank you very much the time it takes to finish this job so",
    "start": "301199",
    "end": "307360"
  },
  {
    "text": "especially in in my experience there's a huge difference",
    "start": "307360",
    "end": "313280"
  },
  {
    "text": "between having some kind of micro batches which running only seconds",
    "start": "313280",
    "end": "318560"
  },
  {
    "text": "and those running maybe for minutes hours and hence i'm here we have like a big",
    "start": "318560",
    "end": "325440"
  },
  {
    "text": "problem with our micro batching jobs and uh",
    "start": "325440",
    "end": "330720"
  },
  {
    "text": "right this is something to consider yeah definitely that's a very good point so you have different type different",
    "start": "330720",
    "end": "336080"
  },
  {
    "text": "running time of uh for for different types of batch jobs that can be taken into consideration when trying to",
    "start": "336080",
    "end": "342080"
  },
  {
    "text": "schedule these jobs uh this is by no means like um summarizing everything",
    "start": "342080",
    "end": "347280"
  },
  {
    "text": "we're just trying to highlight some of the aspects that we're taking into account when solving this problem in the",
    "start": "347280",
    "end": "352960"
  },
  {
    "text": "context of q so as i mentioned um job job uh what do",
    "start": "352960",
    "end": "359440"
  },
  {
    "text": "you mean by job keying here again tries to decide which jobs should wait and which job can start right now",
    "start": "359440",
    "end": "367840"
  },
  {
    "start": "368000",
    "end": "462000"
  },
  {
    "text": "and why do we need job queueing well i mean as i mentioned we in most cases we have lots of jobs that",
    "start": "369199",
    "end": "375280"
  },
  {
    "text": "needs to be processed but we have limited amount of resources so we need to maximize utilization for these resources and if some jobs can't fit",
    "start": "375280",
    "end": "382160"
  },
  {
    "text": "right now we need to decide which ones should start and which ones should wait and we have two types of infrastructure",
    "start": "382160",
    "end": "389039"
  },
  {
    "text": "usually right like we have the on-prem infrastructure and we have the cloud on on-prem",
    "start": "389039",
    "end": "394479"
  },
  {
    "text": "on-prem cluster are are really static typically small in scale and so you have this like limited amount of resources by",
    "start": "394479",
    "end": "401360"
  },
  {
    "text": "design on cloud there's the perception of flexibility and infinite uh",
    "start": "401360",
    "end": "407039"
  },
  {
    "text": "you can infinitely grow but that's not actually always the case because you have discounts for example you buy a",
    "start": "407039",
    "end": "412880"
  },
  {
    "text": "discount on a specific amount of uh cpus or core hours for example so you want to only fit your jobs and your processing",
    "start": "412880",
    "end": "419599"
  },
  {
    "text": "within that discount or you could have spending limits you don't want just like you know as many",
    "start": "419599",
    "end": "425280"
  },
  {
    "text": "jobs as you have you want to continue to buy more and more you have you have some restrictions on how much money you want to spend",
    "start": "425280",
    "end": "431840"
  },
  {
    "text": "um you have also paired tenant limits like different users can't they just spend them as much as they want some users want some tenants could spend more",
    "start": "431840",
    "end": "439120"
  },
  {
    "text": "than than others but also cluster limits like in kubernetes we have cluster limits you",
    "start": "439120",
    "end": "444240"
  },
  {
    "text": "can't have for example like in it by default we have probably i think 5k in gk you could have like 15k but you see",
    "start": "444240",
    "end": "451120"
  },
  {
    "text": "at the end you still have some limits there's a limit to how much the master for example",
    "start": "451120",
    "end": "456400"
  },
  {
    "text": "can scale how many nodes you could have in the in the in the cluster",
    "start": "456400",
    "end": "461440"
  },
  {
    "start": "462000",
    "end": "530000"
  },
  {
    "text": "and so here i'm trying to summarize like what is it that we want to solve with job queuing uh obviously we want a",
    "start": "462160",
    "end": "467360"
  },
  {
    "text": "queueing jobs that don't fit um should uh uh for existing classes should wait the other thing is execution order who",
    "start": "467360",
    "end": "473759"
  },
  {
    "text": "should get to execute now whether it's based on for example the typical fifo",
    "start": "473759",
    "end": "479520"
  },
  {
    "text": "like submission time or priority you need some these knobs that allows users to to could figure how ordering of the",
    "start": "479520",
    "end": "487280"
  },
  {
    "text": "jobs should should happen also for pershing we have a talk later today as well about fair um fair sharing of",
    "start": "487280",
    "end": "493680"
  },
  {
    "text": "ability of resources between tenants different tenants we may want to set some limits how much they can use and",
    "start": "493680",
    "end": "498960"
  },
  {
    "text": "the unused capacity can be shared between them um budgeting again like it's not always the case where you want like a fixed",
    "start": "498960",
    "end": "504800"
  },
  {
    "text": "quota but you want how much resources you can spend over a period of time ability to set policies",
    "start": "504800",
    "end": "511680"
  },
  {
    "text": "uh maybe like on the runtime of the job and finally as i mentioned at the beginning flexible placement you want",
    "start": "511680",
    "end": "516880"
  },
  {
    "text": "the ability to say if i can't run on on demand i want to fall back to for",
    "start": "516880",
    "end": "522959"
  },
  {
    "text": "example to spot vms again like it could also be flexible on time and location and so",
    "start": "522959",
    "end": "530480"
  },
  {
    "start": "530000",
    "end": "594000"
  },
  {
    "text": "why are we trying uh to propose a new controller well with playing kubernetes playing kubernetes continuously tries to",
    "start": "530480",
    "end": "537519"
  },
  {
    "text": "start workloads it can work itself today so every time you create a job the job controller will create the pods and if",
    "start": "537519",
    "end": "544160"
  },
  {
    "text": "you have limited amount of resources these pods will continue to be unscheduled and the control plan will continuously try to schedule them",
    "start": "544160",
    "end": "551440"
  },
  {
    "text": "to no avail right so this is not a desired behavior there's no built-in approach within kubernetes you say okay",
    "start": "551440",
    "end": "556880"
  },
  {
    "text": "let me stop now and try again like later and we have one way of doing that it's",
    "start": "556880",
    "end": "562959"
  },
  {
    "text": "called like kubernetes uh resource quality but the the way that resource courses are built is not really fit the",
    "start": "562959",
    "end": "569360"
  },
  {
    "text": "job queueing uh paradigm it mostly tries to protect the clusters from failing",
    "start": "569360",
    "end": "574880"
  },
  {
    "text": "over and so the way that it applies applies a resource creation whether you get to create the pod in the first place or not so there's no way to say okay let",
    "start": "574880",
    "end": "582000"
  },
  {
    "text": "me wait and then i'll try again it's up to some other external control to keep trying and even then that's not a good",
    "start": "582000",
    "end": "588240"
  },
  {
    "text": "approach you can continue to hammer the api server so it doesn't really fit well the concept of a queue",
    "start": "588240",
    "end": "594880"
  },
  {
    "start": "594000",
    "end": "655000"
  },
  {
    "text": "there are multiple existing uh customer schedulers custom schedulers they all fit really nice in different",
    "start": "594880",
    "end": "601200"
  },
  {
    "text": "use cases but one thing that we wanted to avoid is re-implementing existing functionality we didn't want to do",
    "start": "601200",
    "end": "606480"
  },
  {
    "text": "implement pod to node scheduling for example we want to continue to have the cube scheduler to do that we didn't want",
    "start": "606480",
    "end": "612000"
  },
  {
    "text": "to re-implement the job control drop lifecycle manager uh we wanted to continue to have the core kubernetes job",
    "start": "612000",
    "end": "617600"
  },
  {
    "text": "control to do job management of the of a single job um the second thing is that existing",
    "start": "617600",
    "end": "624000"
  },
  {
    "text": "scheduler didn't have clear integration with auto scaling which is really important characteristic on the cloud and finally last but not least",
    "start": "624000",
    "end": "630880"
  },
  {
    "text": "we found that nobody actually saw the problem of resource flexibility fungibility neither at the api level um",
    "start": "630880",
    "end": "636880"
  },
  {
    "text": "or or mechanically so we want to make that like a first class let's say citizen of job queueing",
    "start": "636880",
    "end": "643519"
  },
  {
    "text": "flexibility because we come from the cloud cloud is massive have different types of resources you want to make sure",
    "start": "643519",
    "end": "649279"
  },
  {
    "text": "that you have really good way of expressing flexibility",
    "start": "649279",
    "end": "654160"
  },
  {
    "start": "655000",
    "end": "782000"
  },
  {
    "text": "so what is our was it like the main design principle for queue is that we don't want to implement anything that",
    "start": "655760",
    "end": "661040"
  },
  {
    "text": "already exists nor implementation of auto scaling pod scheduling lifecycle job lifecycle management even admission",
    "start": "661040",
    "end": "666880"
  },
  {
    "text": "control auto scaling we have cluster order scale we have a bunch of other order scalers out there as well other than the one",
    "start": "666880",
    "end": "672480"
  },
  {
    "text": "hosted by kubernetes cube scheduler hundred spot scheduling job lifecycle management handled by the job uh",
    "start": "672480",
    "end": "678240"
  },
  {
    "text": "controller admission control we have many policy controllers",
    "start": "678240",
    "end": "683600"
  },
  {
    "text": "out there as well like gatekeeper for example and we want to have native support for the batch v1 job api the idea here is",
    "start": "683600",
    "end": "690480"
  },
  {
    "text": "that we want to have this on ramp really easy process of onboarding batch onto kubernetes you want to start by i'm a",
    "start": "690480",
    "end": "696959"
  },
  {
    "text": "user i just create a batch up using v1 job now my needs has grown i want queueing",
    "start": "696959",
    "end": "704240"
  },
  {
    "text": "that's it you just install our controller uh v1 drop now it's become like it's it's the one that will be",
    "start": "704240",
    "end": "709360"
  },
  {
    "text": "supported you're going to be able to queue these jobs etc you don't really need to adapt again to a different job",
    "start": "709360",
    "end": "715279"
  },
  {
    "text": "api so the advantages of this again as i mentioned we reuse significant existing",
    "start": "715279",
    "end": "721920"
  },
  {
    "text": "functionalities no duplication there's no divergence in functionality and and it enforces also",
    "start": "721920",
    "end": "728800"
  },
  {
    "text": "separation of concerns uh we don't want to have two schedules on this on the on the controller we don't have to have two job controllers",
    "start": "728800",
    "end": "735680"
  },
  {
    "text": "uh and what not this comes with some limitations that like the last column here mentioned some some some potential drawbacks that it",
    "start": "735680",
    "end": "742800"
  },
  {
    "text": "creates two levels of resource management if we have a controller at the top that only does job level management",
    "start": "742800",
    "end": "748639"
  },
  {
    "text": "uh it might diverge from like the lower level layer that does pod level management which is most kubernetes",
    "start": "748639",
    "end": "755120"
  },
  {
    "text": "controllers do right they all mostly work at the pod level um it does have",
    "start": "755120",
    "end": "762079"
  },
  {
    "text": "a concern related to starting the job so if you assume that i'm going to manage the job once i decide that the job",
    "start": "762079",
    "end": "767519"
  },
  {
    "text": "should start i'm going to start it but then there is the job controller that have to react to start the pods et cetera so there is that but we think",
    "start": "767519",
    "end": "774560"
  },
  {
    "text": "that these limitations our trade-offs are worth are worth it uh versus the advantage that we're gonna get",
    "start": "774560",
    "end": "781920"
  },
  {
    "start": "782000",
    "end": "814000"
  },
  {
    "text": "um and as i mentioned like we're focusing on the job api the existing drop api is the main vehicle for deploying jobs here i'm just quickly",
    "start": "782320",
    "end": "788800"
  },
  {
    "text": "mentioned that we have tried to fix the job er we understand that uh the job api did not save all use cases we're trying",
    "start": "788800",
    "end": "795279"
  },
  {
    "text": "to do that again as part of the batch working group that we initiated we've fixed a bunch of things we're looking",
    "start": "795279",
    "end": "800720"
  },
  {
    "text": "for other things also to add to it not only to run normal batches also hopefully to use it as a building block",
    "start": "800720",
    "end": "807040"
  },
  {
    "text": "to run for example machine learning workloads or even mpi",
    "start": "807040",
    "end": "814279"
  },
  {
    "text": "um and so the two percent is that uh the queueing controller that we have focused on is uh the batch admin the batch user",
    "start": "814480",
    "end": "821279"
  },
  {
    "text": "the batch admin is the one that is going to set up the tenants set up the queues the usage limits uh and the batch user",
    "start": "821279",
    "end": "827440"
  },
  {
    "text": "their user gene should be simple the same thing they use the v1 job the only thing that they have to do is really simple just select the queue and that's",
    "start": "827440",
    "end": "833839"
  },
  {
    "text": "it okay so let's get to the heart of it i",
    "start": "833839",
    "end": "839360"
  },
  {
    "start": "837000",
    "end": "922000"
  },
  {
    "text": "spoke a lot where is q show me okay so the resource model is really introducing two main resources the queue",
    "start": "839360",
    "end": "846480"
  },
  {
    "text": "and cluster queue namespace is an already existing concept as you know it is the one that",
    "start": "846480",
    "end": "852560"
  },
  {
    "text": "represents the tenant okay you if you have a new tenant for example you have a a new uh group of uh a new team you",
    "start": "852560",
    "end": "859600"
  },
  {
    "text": "create a namespace for them that's their uh working space basically a queue is a namespace resource it",
    "start": "859600",
    "end": "866399"
  },
  {
    "text": "represents like uh a close-rated single tenant jobs um",
    "start": "866399",
    "end": "872000"
  },
  {
    "text": "and there's the cluster eq think of it as raw like the cluster roles and roles like in in in",
    "start": "872000",
    "end": "878000"
  },
  {
    "text": "our back cluster queue is a cluster resource cluster scoped resource",
    "start": "878000",
    "end": "883040"
  },
  {
    "text": "it is where we define the core actual quotas for a specific group of uh group of",
    "start": "883040",
    "end": "889360"
  },
  {
    "text": "tenants and in this context the queue is really the namespace resource is just simply a",
    "start": "889360",
    "end": "895440"
  },
  {
    "text": "pointer to the cluster queue where the resource is going to be consumed a user",
    "start": "895440",
    "end": "900560"
  },
  {
    "text": "within a namespace they usually have access to that namespace",
    "start": "900560",
    "end": "905920"
  },
  {
    "text": "right like they only can list the uh resource exist in that namespace and so",
    "start": "905920",
    "end": "911519"
  },
  {
    "text": "they don't really need to know too much about the cluster queue the only thing they need to know okay what are the keys that are available in my main my",
    "start": "911519",
    "end": "917040"
  },
  {
    "text": "namespace they list them and then they just submit the drop to that queue",
    "start": "917040",
    "end": "921760"
  },
  {
    "start": "922000",
    "end": "951000"
  },
  {
    "text": "and here i want to highlight that multiple different namespaces they can't point to the same cluster queue and that's why like multiple tenants can",
    "start": "922160",
    "end": "928000"
  },
  {
    "text": "share the same amount of resources and the actual queueing in order of execution of the job will happen at the cluster queue level so here you have job",
    "start": "928000",
    "end": "934880"
  },
  {
    "text": "1 and job 3 from the from the robo actions team from the robovirgin team they have drop",
    "start": "934880",
    "end": "941120"
  },
  {
    "text": "two and four the actual starting order is going to be happening like you're going to be married it is going to be",
    "start": "941120",
    "end": "946560"
  },
  {
    "text": "happening at the cluster q level for example for the robotics team pool and",
    "start": "946560",
    "end": "951680"
  },
  {
    "start": "951000",
    "end": "977000"
  },
  {
    "text": "another one last thing here is that you could have multiple cluster queues of course in the cluster for example one for robotics team and",
    "start": "951680",
    "end": "958639"
  },
  {
    "text": "the other one for the uh natural language processing team each one could have their own resource pool we have this concept of a borrowing",
    "start": "958639",
    "end": "965600"
  },
  {
    "text": "cohort like can if you if you see like there's a cohort uh parameter there can define that and basically decides how",
    "start": "965600",
    "end": "972560"
  },
  {
    "text": "far sharing happens who can borrow format i'm going to highlight this in the next few slides in the apis so again i i spoke this",
    "start": "972560",
    "end": "980240"
  },
  {
    "start": "977000",
    "end": "1001000"
  },
  {
    "text": "about this a lot we use the normal job api here um and the only thing that you need to do to use a specific queue is just",
    "start": "980240",
    "end": "986560"
  },
  {
    "text": "basically set the queue annotation we're trying to make that a parameter first class citizen of the of the job api we",
    "start": "986560",
    "end": "992240"
  },
  {
    "text": "want to introduce the queue name uh in the in the in the in the job spec",
    "start": "992240",
    "end": "998399"
  },
  {
    "start": "1001000",
    "end": "1037000"
  },
  {
    "text": "um so as i mentioned the queue api is really simple this is the namespace resource the only like in in simplest",
    "start": "1001600",
    "end": "1008240"
  },
  {
    "text": "form the only thing that has it's a pointer to the cluster queue where the resources will be allocated",
    "start": "1008240",
    "end": "1014480"
  },
  {
    "text": "this is like again um this model is splitting the queue into two different resources the name space one namespace",
    "start": "1014480",
    "end": "1020720"
  },
  {
    "text": "one fits better the like the kubernetes model in general the",
    "start": "1020720",
    "end": "1026558"
  },
  {
    "text": "fact that namespace is the organizing concept for teams and where you do our back etc",
    "start": "1026559",
    "end": "1033038"
  },
  {
    "text": "and so this is why we came up with this with this with this model the more exciting api is like where",
    "start": "1033039",
    "end": "1038880"
  },
  {
    "start": "1037000",
    "end": "1077000"
  },
  {
    "text": "everything lives is the cluster queue api so the first thing i want to highlight here as i mentioned it defines quotas",
    "start": "1038880",
    "end": "1046798"
  },
  {
    "text": "one thing that we've introduced is called like flavors for a specific resource here i'm only mentioning in one",
    "start": "1046799",
    "end": "1052720"
  },
  {
    "text": "resource called is the cpu you could you could define multiple flavors of a cpu here i'm defining two flavors the",
    "start": "1052720",
    "end": "1059200"
  },
  {
    "text": "on-demand one and the spot one you can define different quarters for each one the minimum for example is the minimum",
    "start": "1059200",
    "end": "1065440"
  },
  {
    "text": "amount of resources you're going to get the maximum how much you can you can uh scale up to by borrowing from",
    "start": "1065440",
    "end": "1072400"
  },
  {
    "text": "another cluster queue in the same borrowing cohort so as i mentioned",
    "start": "1072400",
    "end": "1078720"
  },
  {
    "start": "1077000",
    "end": "1121000"
  },
  {
    "text": "dakota is uh basically defines the total amount of resources that can be requested",
    "start": "1078720",
    "end": "1084799"
  },
  {
    "text": "from the jobs that start from this through this cluster queue it does not really guarantee that these pods are",
    "start": "1084799",
    "end": "1090080"
  },
  {
    "text": "actually going to get scheduled so i want to i want you to think about this as a dynamic content manager not as like",
    "start": "1090080",
    "end": "1097760"
  },
  {
    "text": "like really a resource manager per se it doesn't guarantee that the pods are actually going to schedule it but it",
    "start": "1097760",
    "end": "1102960"
  },
  {
    "text": "just says that similar to resource cortez we're going to allow you to start this is the amount of which are going to have we're going to depend on the low",
    "start": "1102960",
    "end": "1109360"
  },
  {
    "text": "level infrastructure to actually provision the resources for you we are also planning here defining only",
    "start": "1109360",
    "end": "1114799"
  },
  {
    "text": "quota we're planning to support budgets in the future basically like over time rather than just on a specific point of time",
    "start": "1114799",
    "end": "1120880"
  },
  {
    "text": "the second thing i want to say here and highlight is that how do you differentiate between flavors",
    "start": "1120880",
    "end": "1126240"
  },
  {
    "text": "we've introduced um two parameters labels and things again like they match how we differentiate between partitions",
    "start": "1126240",
    "end": "1132320"
  },
  {
    "text": "of the of the cluster between nodes right you partition them using labels and and and taints and i want you to",
    "start": "1132320",
    "end": "1138720"
  },
  {
    "text": "think about flavors the same in the same context so again flavors are associated with tins",
    "start": "1138720",
    "end": "1145679"
  },
  {
    "text": "labels when for example when i start a job i assign it a specific flavor for",
    "start": "1145679",
    "end": "1150799"
  },
  {
    "text": "example a specific job when it starts i'm going to say okay i'm going to take 10 cores from on demand how do i force",
    "start": "1150799",
    "end": "1156160"
  },
  {
    "text": "it to go to the in demand i'm going to use that label translate it into a node selector inject it in the job and that's",
    "start": "1156160",
    "end": "1161520"
  },
  {
    "text": "how i would force the job to to only use um that to you to use that that specific",
    "start": "1161520",
    "end": "1166960"
  },
  {
    "text": "um quarter teens is an interesting uh uh parameter here it allows users it's like it's it's",
    "start": "1166960",
    "end": "1174720"
  },
  {
    "text": "an opt out by default mechanism right jobs that don't tolerate for example",
    "start": "1174720",
    "end": "1180240"
  },
  {
    "text": "taint they will not if for example we're going to start from the top to the bottom the key controller",
    "start": "1180240",
    "end": "1186000"
  },
  {
    "text": "will say okay let me try to give it uh some cores from in demand if there is no quota i will fall back to spot but if",
    "start": "1186000",
    "end": "1193039"
  },
  {
    "text": "the job doesn't explicitly tolerate spot then it will it will continue to be",
    "start": "1193039",
    "end": "1198640"
  },
  {
    "text": "pending and so this is the nice thing about this approach it's kind of a kubernetes native the way that you try",
    "start": "1198640",
    "end": "1204240"
  },
  {
    "text": "to assign pods to nodes the same thing here you use node selector and use uh tolerations to",
    "start": "1204240",
    "end": "1211120"
  },
  {
    "text": "say whether i'm opting n2 for example using spot or not or you can also say",
    "start": "1211120",
    "end": "1216799"
  },
  {
    "text": "i always want to use spot and so you could have this is the last point on the job if you say in the node selector i",
    "start": "1216799",
    "end": "1223039"
  },
  {
    "text": "want spot then i'm going to skip the first one and directly assigned to you try to assign to you a quota from from",
    "start": "1223039",
    "end": "1229200"
  },
  {
    "text": "the spot flavor so that that is the those are the two main apis um this is the the main api for",
    "start": "1229200",
    "end": "1236720"
  },
  {
    "start": "1231000",
    "end": "1274000"
  },
  {
    "text": "defining quotas uh one last thing here in the api i want to mention is the borrowing cohort i mentioned that before",
    "start": "1236720",
    "end": "1242000"
  },
  {
    "text": "in the resource model um different cluster cues uh can be grouped",
    "start": "1242000",
    "end": "1247440"
  },
  {
    "text": "into a borrowing cohort so basically this allows you to do um like some sort",
    "start": "1247440",
    "end": "1252640"
  },
  {
    "text": "of like fair sharing or your unused uh using unused quota from",
    "start": "1252640",
    "end": "1258880"
  },
  {
    "text": "different cluster cues um imagine you have a research department you know cohort and",
    "start": "1258880",
    "end": "1265760"
  },
  {
    "text": "robotics team if they are not using all of their quota a different team within the research department can use it",
    "start": "1265760",
    "end": "1273600"
  },
  {
    "start": "1274000",
    "end": "1283000"
  },
  {
    "text": "so how all this uh works um as i mentioned like you start with the batch admin creating the name",
    "start": "1274320",
    "end": "1280000"
  },
  {
    "text": "spaces create the cues create the cluster cues they use the only thing that it does creates the job and assign it to a queue",
    "start": "1280000",
    "end": "1286640"
  },
  {
    "start": "1283000",
    "end": "1299000"
  },
  {
    "text": "one thing we're doing here is that we force the drop to start in a suspended mode no pods are going to get created at the beginning we do have a suspend of a",
    "start": "1286640",
    "end": "1294400"
  },
  {
    "text": "flag now in the job api we can use a web hook to force it to do that the second thing is well our queuing",
    "start": "1294400",
    "end": "1301039"
  },
  {
    "start": "1299000",
    "end": "1331000"
  },
  {
    "text": "controller what it will do is that it will watch for these suspended jobs and based on i mean the cluster cue",
    "start": "1301039",
    "end": "1307679"
  },
  {
    "text": "that's assigned to the flavors that are available the code is available its order and whatnot it will admit the job",
    "start": "1307679",
    "end": "1313440"
  },
  {
    "text": "what that means that it assigns its specific flavor give it a quota and then the way that it",
    "start": "1313440",
    "end": "1318480"
  },
  {
    "text": "does is basically it injects the affinity into the uh into the job and unsuspends the job that's basically all",
    "start": "1318480",
    "end": "1324559"
  },
  {
    "text": "it does it doesn't create pause it doesn't it doesn't do anything else that already an existing controller do",
    "start": "1324559",
    "end": "1331200"
  },
  {
    "start": "1331000",
    "end": "1344000"
  },
  {
    "text": "the race is the same as i mentioned so these red boxes are existing controllers the blue one is the one that we've",
    "start": "1331200",
    "end": "1336880"
  },
  {
    "text": "introduced the job control once the job is unsuspected we create the parts and the scheduler assign them two nodes",
    "start": "1336880",
    "end": "1344000"
  },
  {
    "start": "1344000",
    "end": "1374000"
  },
  {
    "text": "so how do we handle custom workloads not everything is a v1 job and not everything can be modeled as a v1 job",
    "start": "1344159",
    "end": "1349360"
  },
  {
    "text": "um so we do have a hook to do that and the only thing we we require from a custom workload to work in in queue is",
    "start": "1349360",
    "end": "1356400"
  },
  {
    "text": "that it needs to be represented by high level object api like an mpi job that controls creating the pods etc and needs",
    "start": "1356400",
    "end": "1363520"
  },
  {
    "text": "to implement the suspend semantics which is the whole enabler for everything related to",
    "start": "1363520",
    "end": "1368880"
  },
  {
    "text": "queue we need to decide when the job should start and so we need to suspend and suspend the job",
    "start": "1368880",
    "end": "1374960"
  },
  {
    "start": "1374000",
    "end": "1411000"
  },
  {
    "text": "we've introduced a new api uh called workload basically is an abstraction",
    "start": "1374960",
    "end": "1380080"
  },
  {
    "text": "that allows allows us to decouple us from actually linking",
    "start": "1380080",
    "end": "1386640"
  },
  {
    "text": "the mpi job or tf job type workload and the workload api is basically an api",
    "start": "1386640",
    "end": "1392480"
  },
  {
    "text": "that gives queue and like makes you understand how much resources do you want",
    "start": "1392480",
    "end": "1399360"
  },
  {
    "text": "and like in all of this like really it's just the pod set what is just what template how many files do you want out",
    "start": "1400320",
    "end": "1407120"
  },
  {
    "text": "of that and that's it line and the q name so how that works in action",
    "start": "1407120",
    "end": "1413840"
  },
  {
    "start": "1411000",
    "end": "1481000"
  },
  {
    "text": "we envisioned that for each custom workload we need a new workload controller that worker controller does that syncing",
    "start": "1413840",
    "end": "1420640"
  },
  {
    "text": "between the actual custom workload and the workload type that we have in in queue and so the batch user is going to",
    "start": "1420640",
    "end": "1428880"
  },
  {
    "text": "create the mpi job for example in this we will force it to be start in a suspended mode the workload controller",
    "start": "1428880",
    "end": "1435520"
  },
  {
    "text": "will should be watching for these suspended mpi jobs it will create the workload the the key workload type for",
    "start": "1435520",
    "end": "1442480"
  },
  {
    "text": "us q will be watching for the workload type it has it will basically reflect the the",
    "start": "1442480",
    "end": "1448960"
  },
  {
    "text": "uh the assignment on the workload status this is how basically we admitted the workload then the workload controller",
    "start": "1448960",
    "end": "1455360"
  },
  {
    "text": "watching for that uh status once it is assigned it will inject the node affinity based on the",
    "start": "1455360",
    "end": "1461360"
  },
  {
    "text": "selected flavor and unsuspend the job we have actually implemented the job api using this model like it's not really",
    "start": "1461360",
    "end": "1467600"
  },
  {
    "text": "built into queue we have a really tiny job controller that does that syncing between the",
    "start": "1467600",
    "end": "1474799"
  },
  {
    "text": "queue workload type and the actual job api",
    "start": "1474799",
    "end": "1479600"
  },
  {
    "start": "1481000",
    "end": "1555000"
  },
  {
    "text": "one last integration point that i want to highlight is is auto scaling it's really important for us to support auto scaling",
    "start": "1481760",
    "end": "1488400"
  },
  {
    "text": "natively because in the cloud you know uh everything is auto scaled and this is",
    "start": "1488400",
    "end": "1493919"
  },
  {
    "text": "the promise of the cloud is that when you use resources we're gonna scale it up to you when you don't we're gonna",
    "start": "1493919",
    "end": "1499039"
  },
  {
    "text": "scale it down uh but i get i guess i have",
    "start": "1499039",
    "end": "1504240"
  },
  {
    "text": "10 more minutes right uh so how do we envision integration with",
    "start": "1504240",
    "end": "1509919"
  },
  {
    "text": "with auto scalers well one way is to say okay i'm going just to create the pods some cluster auto scalers like the",
    "start": "1509919",
    "end": "1516960"
  },
  {
    "text": "normal cluster autoscaler basically reacts to unscheduled posts and will provide the resources when they see an",
    "start": "1516960",
    "end": "1522400"
  },
  {
    "text": "uh a port that is unscheduled but we can do better than that we envision having a research for",
    "start": "1522400",
    "end": "1528480"
  },
  {
    "text": "virginia object api let's name it capacity request that research that cluster auto scalers should be",
    "start": "1528480",
    "end": "1533600"
  },
  {
    "text": "supporting instead of just reacting to pods we want an explicit api to request",
    "start": "1533600",
    "end": "1540159"
  },
  {
    "text": "a specific amount of resources from the cluster auto scaler and so the idea here is that",
    "start": "1540159",
    "end": "1545919"
  },
  {
    "text": "the cluster order scaler would be watching for this cluster request for capacity request and potentially scale",
    "start": "1545919",
    "end": "1551279"
  },
  {
    "text": "up the cluster to satisfy that request and why do we need to enable this well",
    "start": "1551279",
    "end": "1558240"
  },
  {
    "start": "1555000",
    "end": "1642000"
  },
  {
    "text": "it uh like this approach it's it's an enabler for location flexibility before i",
    "start": "1558240",
    "end": "1563919"
  },
  {
    "text": "we won't have the cluster or scale aware of a group of parts like when i want to schedule resources for a for for a job i",
    "start": "1563919",
    "end": "1571279"
  },
  {
    "text": "want to say okay i want 10 cores of each for each for 10 pods one core for each",
    "start": "1571279",
    "end": "1576960"
  },
  {
    "text": "but i want all of them to be in this region but i want all of them to be in one zone",
    "start": "1576960",
    "end": "1582559"
  },
  {
    "text": "but don't spread them across zones i want any single zone type of semantics and so",
    "start": "1582559",
    "end": "1588559"
  },
  {
    "text": "well i want an api where i can express this uh to to cluster autoscaler it also allows",
    "start": "1588559",
    "end": "1594880"
  },
  {
    "text": "us to do better handling of stock outs it see that when i before i actually unsuspend the job i'm going to create a",
    "start": "1594880",
    "end": "1600960"
  },
  {
    "text": "cluster capacity request do i actually have 10 cores to to to provision or not",
    "start": "1600960",
    "end": "1607279"
  },
  {
    "text": "um in cloud again it does have this like promise of infinite scalability but we do face",
    "start": "1607279",
    "end": "1613679"
  },
  {
    "text": "stock outs in some cases on black friday or cyber monday or in specific you know",
    "start": "1613679",
    "end": "1619840"
  },
  {
    "text": "regions that have like smaller clusters etc",
    "start": "1619840",
    "end": "1625039"
  },
  {
    "text": "and again and last but not least it allows us to handle scale up and when to use provision capacity it's not always",
    "start": "1625120",
    "end": "1630960"
  },
  {
    "text": "the case where you want to actually scale up i want cluster oscar to be aware of existing provision resources and actually trigger a scale up all the",
    "start": "1630960",
    "end": "1638080"
  },
  {
    "text": "time if capacity is already already provisioned and the way that this we envision this",
    "start": "1638080",
    "end": "1644480"
  },
  {
    "start": "1642000",
    "end": "1658000"
  },
  {
    "text": "being uh integrated with q is that after the job is created q is not always",
    "start": "1644480",
    "end": "1650240"
  },
  {
    "text": "going to just simply assign a flavor and unsuspend the job before it unsuspends the job it will create a capacity",
    "start": "1650240",
    "end": "1656399"
  },
  {
    "text": "request we see that cluster order scaler watching for these capacity requests",
    "start": "1656399",
    "end": "1662640"
  },
  {
    "start": "1658000",
    "end": "1687000"
  },
  {
    "text": "fulfill it and then queue watches for this capacity request once it's actually satisfied it will inject the known",
    "start": "1662640",
    "end": "1669200"
  },
  {
    "text": "affinity based not only on the flavor but also the capacity requests we envision there",
    "start": "1669200",
    "end": "1674720"
  },
  {
    "text": "to be something similar for example if the cluster or scalar selected a specific zone we're going to inject no",
    "start": "1674720",
    "end": "1679760"
  },
  {
    "text": "dfinity to that specific zone and then it unsuspends the job",
    "start": "1679760",
    "end": "1687200"
  },
  {
    "start": "1687000",
    "end": "1731000"
  },
  {
    "text": "and the rest is the same jobs the job control will create the pods the scheduler will place them and",
    "start": "1687840",
    "end": "1694240"
  },
  {
    "text": "here i like one last thing i wanted to highlight and stress is that we are trying to use existing",
    "start": "1694240",
    "end": "1700000"
  },
  {
    "text": "controllers as they are we're trying to use the primitives that they give us to control when to schedule what to",
    "start": "1700000",
    "end": "1705440"
  },
  {
    "text": "schedule and where we're using a lot not affinity not selector teens we envision",
    "start": "1705440",
    "end": "1710960"
  },
  {
    "text": "we see those as really powerful genetic uh you know features that allows us to",
    "start": "1710960",
    "end": "1716159"
  },
  {
    "text": "do higher level you know types of decisions uh while delegating the low level pod",
    "start": "1716159",
    "end": "1721840"
  },
  {
    "text": "management to the lower level controllers and so far so good it worked us it",
    "start": "1721840",
    "end": "1727279"
  },
  {
    "text": "worked for us really well for the job api um and and this is like my summary so um",
    "start": "1727279",
    "end": "1734080"
  },
  {
    "text": "it's um what is q so as i mentioned q is is a",
    "start": "1734080",
    "end": "1740399"
  },
  {
    "text": "queuing controller you can think of it also as an elastic curtain manager um it does no implementation existing",
    "start": "1740399",
    "end": "1747520"
  },
  {
    "text": "functionality as i mentioned it worked really well for us for the v1 job api so",
    "start": "1747520",
    "end": "1752559"
  },
  {
    "text": "far we have an api to integrate with custom workloads",
    "start": "1752559",
    "end": "1758159"
  },
  {
    "text": "and it does support fair sharing and resource flexibility using uh flavors",
    "start": "1758159",
    "end": "1764240"
  },
  {
    "start": "1763000",
    "end": "1780000"
  },
  {
    "text": "i'm sorry about the again the uh the font here um but the start is that",
    "start": "1764240",
    "end": "1769360"
  },
  {
    "text": "we've released our first version version zero point um 0.1",
    "start": "1769360",
    "end": "1775279"
  },
  {
    "text": "includes everything that i discussed apart from the cluster autoscaler integration what's next again we want to prove that",
    "start": "1775279",
    "end": "1782000"
  },
  {
    "text": "this also will work for custom workloads we want to start with integration with common operators like spark kubeflow we",
    "start": "1782000",
    "end": "1789440"
  },
  {
    "text": "want to implement job preemption think about the suspend again a really powerful simple concept",
    "start": "1789440",
    "end": "1795120"
  },
  {
    "text": "uh when you're unsuspended you're starting the job when you suspend you think of it as job preemption so we want",
    "start": "1795120",
    "end": "1800159"
  },
  {
    "text": "to use that to do more advanced you know uh um",
    "start": "1800159",
    "end": "1805200"
  },
  {
    "text": "job management maybe based on job runtime budgets multi-cluster as well on our",
    "start": "1805200",
    "end": "1811600"
  },
  {
    "text": "roadmap i want to thank all the contributors uh to to queue so",
    "start": "1811600",
    "end": "1818640"
  },
  {
    "text": "far again this it's a really it's in very first days",
    "start": "1818640",
    "end": "1824000"
  },
  {
    "start": "1824000",
    "end": "1924000"
  },
  {
    "text": "and that's it you can visit our repo at q dot sh and thank you i'm happy",
    "start": "1824000",
    "end": "1829840"
  },
  {
    "text": "to receive [Applause]",
    "start": "1829840",
    "end": "1836480"
  },
  {
    "text": "i'm gonna open four questions first here",
    "start": "1836480",
    "end": "1840398"
  },
  {
    "text": "uh question so with jobs uh like reports of for jobs where created",
    "start": "1842000",
    "end": "1848240"
  },
  {
    "text": "in the where the queue was originated or in some different namespace they will be",
    "start": "1848240",
    "end": "1853919"
  },
  {
    "text": "created in the namespace where the job is created okay",
    "start": "1853919",
    "end": "1859600"
  },
  {
    "text": "i'm going to jump to a question from slack yeah um with regular hpc i'm able to launch 1000",
    "start": "1859600",
    "end": "1868320"
  },
  {
    "text": "of jobs in parallel but kubernetes limits the number of bots to 110 per",
    "start": "1868320",
    "end": "1873760"
  },
  {
    "text": "node how could we accommodate a regular number of jobs from hpc in a kubernetes",
    "start": "1873760",
    "end": "1879279"
  },
  {
    "text": "cluster i'm guessing this is like not really related to the job job curing per se but",
    "start": "1879279",
    "end": "1885840"
  },
  {
    "text": "in general i think the 110 limit is not like is i don't think it's fixed you could",
    "start": "1885840",
    "end": "1891600"
  },
  {
    "text": "you could adjust it basically to increase the number of pods that you can have in a node it's basically how many ips i think that's the main limiting",
    "start": "1891600",
    "end": "1898000"
  },
  {
    "text": "factor there of course in addition to other resources but the main limiting factor is typically we assign i think uh slash 24",
    "start": "1898000",
    "end": "1907519"
  },
  {
    "text": "subnet that's why you get 256 they divide it by two because when you do um",
    "start": "1907519",
    "end": "1914320"
  },
  {
    "text": "you can't reuse the ip quickly i think that's why it's it's 110 but generally it's something configurable",
    "start": "1914320",
    "end": "1921840"
  },
  {
    "start": "1924000",
    "end": "2092000"
  },
  {
    "text": "hey thanks for your talk um i was wondering how does the queue interact with index jobs does it only launch",
    "start": "1924399",
    "end": "1931760"
  },
  {
    "text": "the pods when all of the parts are ready to go or when a subset of 8 right so right now the",
    "start": "1931760",
    "end": "1937760"
  },
  {
    "text": "assumption is that we we don't we don't really know about the semantics of the job how these pods are being created we",
    "start": "1937760",
    "end": "1943360"
  },
  {
    "text": "only manage the whole job like as a single unit so it's pretty",
    "start": "1943360",
    "end": "1950000"
  },
  {
    "text": "much you can think of it as all or nothing scheduling but it's not really scheduling all or nothing quota management so i have quota for the whole",
    "start": "1950000",
    "end": "1956399"
  },
  {
    "text": "index job i'm gonna start it and here we're looking only at the parallelism field not the completions right because",
    "start": "1956399",
    "end": "1962960"
  },
  {
    "text": "this is what dictates how many parts the drip controller is actually going to create",
    "start": "1962960",
    "end": "1968000"
  },
  {
    "text": "i'm going to jump to slack then how does q handle workloads",
    "start": "1968000",
    "end": "1974240"
  },
  {
    "text": "such as apache spark and other frameworks that might need to dynamically ask for more resources",
    "start": "1974240",
    "end": "1980480"
  },
  {
    "text": "that's a very good question it is on our uh roadmap we don't right now have that explicitly",
    "start": "1980480",
    "end": "1986880"
  },
  {
    "text": "but we've discussed this in the community how to handle dynamic resource management we think of it as kind of",
    "start": "1986880",
    "end": "1993039"
  },
  {
    "text": "similar to vertical part other scale i don't know if some of you were following that in the community how you adjust the",
    "start": "1993039",
    "end": "1999120"
  },
  {
    "text": "resources for a pod we're kind of leaning towards a solution in the same point",
    "start": "1999120",
    "end": "2006240"
  },
  {
    "text": "i think you may have just answered my question i was going to ask about right sizing but that might be",
    "start": "2007200",
    "end": "2012880"
  },
  {
    "text": "vertical scaling right sizing for the part of the job the job like how do you pick this right",
    "start": "2012880",
    "end": "2018320"
  },
  {
    "text": "so it is on our road map we we were having some discussions on how we can uh dynamically",
    "start": "2018320",
    "end": "2025120"
  },
  {
    "text": "change the uh decided like for example you could want to start with one pod",
    "start": "2025120",
    "end": "2030960"
  },
  {
    "text": "but then but then you want to change it right okay",
    "start": "2030960",
    "end": "2036320"
  },
  {
    "text": "okay yeah oh there's one",
    "start": "2037360",
    "end": "2042000"
  },
  {
    "text": "just order also plans for priority and queues so like if if job comes into one queue with the",
    "start": "2044640",
    "end": "2051358"
  },
  {
    "text": "priority xy set and then the job comes into another with a higher priority would you then reorder the the cluster queue yeah so",
    "start": "2051359",
    "end": "2058560"
  },
  {
    "text": "when you have uh job uh job level uh uh priority not q priority",
    "start": "2058560",
    "end": "2065440"
  },
  {
    "text": "um so basically the jobs um uh basically are ordered based on priority and then based on uh uh",
    "start": "2065440",
    "end": "2072398"
  },
  {
    "text": "submission time um we do have like a revision like different you know ordering uh",
    "start": "2072399",
    "end": "2079679"
  },
  {
    "text": "and it is a parameter on the on the cluster fuel level that um with that uh we'll conclude this",
    "start": "2079679",
    "end": "2085679"
  },
  {
    "text": "session um thank you abdullah [Applause]",
    "start": "2085679",
    "end": "2094510"
  }
]