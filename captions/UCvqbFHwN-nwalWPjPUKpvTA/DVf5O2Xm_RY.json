[
  {
    "text": "good afternoon um my name is Aron uh I am at Intel I'm part of the open",
    "start": "240",
    "end": "6839"
  },
  {
    "text": "ecosystem team hello my name is iik Lanza and I'm I an AI open source",
    "start": "6839",
    "end": "12080"
  },
  {
    "text": "evangelist yeah so today we're going to talk about um how do you deploy llms",
    "start": "12080",
    "end": "17640"
  },
  {
    "text": "using Cloud native with Lang chain and what is the use case for that",
    "start": "17640",
    "end": "23680"
  },
  {
    "text": "specifically we all want we have always wanted assistant you know I want assistant that can cook food for me I",
    "start": "23680",
    "end": "30800"
  },
  {
    "text": "can get that with AI but at least in my job how can I get an assistant that could make me more productive whether is",
    "start": "30800",
    "end": "38280"
  },
  {
    "text": "writing my emails whether it's summarizing my meeting minutes simple assistance you know I mean bouncing",
    "start": "38280",
    "end": "44640"
  },
  {
    "text": "ideas back and forth conversational chat BS all sorts of assistance could be very very helpful actually um and if you put",
    "start": "44640",
    "end": "52239"
  },
  {
    "text": "that in a business context you know in a company there are different business units so to say there is a finance there",
    "start": "52239",
    "end": "58160"
  },
  {
    "text": "is it you know there is is legal there is engineering and their needs are very",
    "start": "58160",
    "end": "63559"
  },
  {
    "text": "different their jurisdiction is very different uh how much amount of data can",
    "start": "63559",
    "end": "68759"
  },
  {
    "text": "they keep within the business unit can they stand outside the business unit",
    "start": "68759",
    "end": "73880"
  },
  {
    "text": "should it stay inside the company can it go outside the company so each business",
    "start": "73880",
    "end": "79080"
  },
  {
    "text": "unit in that sense if they want to explore how do you leverage an llm their",
    "start": "79080",
    "end": "84280"
  },
  {
    "text": "needs are very different and very unique in that sense so that's sort of where we",
    "start": "84280",
    "end": "89560"
  },
  {
    "text": "are going to to talk about there are many ways by which customers are deploying llms uh in Cloud native so",
    "start": "89560",
    "end": "97240"
  },
  {
    "text": "what we're going to talk about is one specific Way by which you know you could deploy um llms in Cloud native exactly",
    "start": "97240",
    "end": "104799"
  },
  {
    "text": "yes and this our is one way to do it so basically we Define in five main steps",
    "start": "104799",
    "end": "110119"
  },
  {
    "text": "the first one is the model the definition you need to Define your model the second one is the API the third is",
    "start": "110119",
    "end": "116360"
  },
  {
    "text": "the packaging because you need to put everything in one package you need to containerize that and you need to use kubernetes of course so let's go for we",
    "start": "116360",
    "end": "124240"
  },
  {
    "text": "will go deep in all of them so let's go right let's go for the model definition",
    "start": "124240",
    "end": "130720"
  },
  {
    "text": "which is probably the most challenging part right because we need to know what is our problem so we need to pick which",
    "start": "130720",
    "end": "136519"
  },
  {
    "text": "model we would like to use and what is the use case do we do we want to build a conversational chatbot a text imization",
    "start": "136519",
    "end": "143080"
  },
  {
    "text": "you know that llms we can use it for multiple things so classification question answering and so on so the",
    "start": "143080",
    "end": "149120"
  },
  {
    "text": "first thing to find is a problem the second one is the the is the model we have tons of models as you may hear Lama",
    "start": "149120",
    "end": "155879"
  },
  {
    "text": "Falcon open AI everything you have tons of models and within then you have Foundation models fine-tune models do do",
    "start": "155879",
    "end": "162560"
  },
  {
    "text": "you want to use a generic llm do you want to find tun it for your context do",
    "start": "162560",
    "end": "167720"
  },
  {
    "text": "do you want to use it a rack like a retrieval augmented and the last point",
    "start": "167720",
    "end": "173239"
  },
  {
    "text": "is you need to Define your tools so you define your model your strategy and you need to define the tools that you have",
    "start": "173239",
    "end": "178440"
  },
  {
    "text": "available to use it like haging pH Lang chain and and so on in our case we will",
    "start": "178440",
    "end": "184440"
  },
  {
    "text": "be using a question answering application with a foundation model which will be a 7 billion parameters and",
    "start": "184440",
    "end": "191159"
  },
  {
    "text": "we will be using Lang chain and Hing facee and we will be explaining the benefits that you can take when you're",
    "start": "191159",
    "end": "196200"
  },
  {
    "text": "are using Lang chain and also Hing face as well so the first challenge is you",
    "start": "196200",
    "end": "201799"
  },
  {
    "text": "need to define the model how can you make or how can you do the decision to use a um to pick a model so we have main",
    "start": "201799",
    "end": "210319"
  },
  {
    "text": "free free things the first one is the leaderboard which is basically how your model behaves Against The Benchmark",
    "start": "210319",
    "end": "217000"
  },
  {
    "text": "which is a public Benchmark so you go to the Hing face leaderboard and you see okay I would like to pick a model that",
    "start": "217000",
    "end": "222200"
  },
  {
    "text": "it's acceptable between the benchmarks but it's not just performance of course",
    "start": "222200",
    "end": "227519"
  },
  {
    "text": "you need to see if this model is used by people if there is a community around it",
    "start": "227519",
    "end": "233200"
  },
  {
    "text": "if there is a tutorials if there are forums the people is using you know that these models they are they behave in a",
    "start": "233200",
    "end": "240680"
  },
  {
    "text": "mysterious ways right so most people and the feedback of the people is very important and the last one is the",
    "start": "240680",
    "end": "246439"
  },
  {
    "text": "ethical considerations right so you need to know if your model is biased because you need to probably mitigate that bias",
    "start": "246439",
    "end": "253040"
  },
  {
    "text": "or do something and you probably should know the consider the training data so",
    "start": "253040",
    "end": "258160"
  },
  {
    "text": "well so these are the first part right so once you know the model and let's",
    "start": "258160",
    "end": "263800"
  },
  {
    "text": "suppose that we Define a model and now we need to consume it so on that model definition you know if you think about",
    "start": "263800",
    "end": "269400"
  },
  {
    "text": "it hugging phase as they say is a GitHub for llms so very much when you go to",
    "start": "269400",
    "end": "274440"
  },
  {
    "text": "GitHub when you're looking at an open source project you know you don't want to use a project that was created 3",
    "start": "274440",
    "end": "279960"
  },
  {
    "text": "years ago never been maintained potentially has cves and you saw the impact that it could cause you know if a",
    "start": "279960",
    "end": "286680"
  },
  {
    "text": "model has not been maintained for a long time it could be malicious you know it could be you know having vulnerabilities",
    "start": "286680",
    "end": "293479"
  },
  {
    "text": "so very much the same philosophy that you apply in terms of picking a project from a GitHub repo is what you're",
    "start": "293479",
    "end": "300360"
  },
  {
    "text": "applying to hugging face as well absolutely so we need to consume the",
    "start": "300360",
    "end": "305600"
  },
  {
    "text": "model so now we need to consume the model you know in a business unit again as we talked about if you are in a legal",
    "start": "305600",
    "end": "312000"
  },
  {
    "text": "business unit then maybe you can send the data outside of your business unit you want to keep some data inside your",
    "start": "312000",
    "end": "317600"
  },
  {
    "text": "business unit and maybe in certain cases like if you're in engineering you're more okay to you know send the data out",
    "start": "317600",
    "end": "324960"
  },
  {
    "text": "maybe use open AI as a back end which is open only in name but at least you can",
    "start": "324960",
    "end": "330199"
  },
  {
    "text": "call it using an AP API so local that means the model is within your",
    "start": "330199",
    "end": "336000"
  },
  {
    "text": "jurisdiction versus using an external model that means it's hosted somewhere externally so those are sort of the two",
    "start": "336000",
    "end": "342240"
  },
  {
    "text": "models essentially that we are looking at and if you think about the considerations what should be the",
    "start": "342240",
    "end": "348000"
  },
  {
    "text": "considerations between whether you want to pick a local model versus external model right you know local model is is",
    "start": "348000",
    "end": "355080"
  },
  {
    "text": "within your app part of your app you know you want data privacy you want offline usage because external model is",
    "start": "355080",
    "end": "362639"
  },
  {
    "text": "hosted somewhere else you know it's you don't have control over it cost could be a big issue because when you're using an",
    "start": "362639",
    "end": "368759"
  },
  {
    "text": "external model really you're paying cost per outbound token or inbound token and",
    "start": "368759",
    "end": "375120"
  },
  {
    "text": "we'll talk about that cost in a in a bit with a local model you have a better ability to customize you know you could",
    "start": "375120",
    "end": "381199"
  },
  {
    "text": "feed the data back into the model and then you know continue improving it and",
    "start": "381199",
    "end": "386400"
  },
  {
    "text": "then when you're looking at external model what are the pros and the cons that that well uh in an external model",
    "start": "386400",
    "end": "393000"
  },
  {
    "text": "setting up a local model could take time with external model is ready to go you know you just make a API call you get",
    "start": "393000",
    "end": "399360"
  },
  {
    "text": "the data back you get the scalability because scalability is no longer your concern you know is the external model",
    "start": "399360",
    "end": "405000"
  },
  {
    "text": "concerned it's less complex setup essentially you get a token and you're ready to call invoking it elasticity and",
    "start": "405000",
    "end": "411360"
  },
  {
    "text": "high availability and the way we look about look at this is if you see the comparison local model versus external",
    "start": "411360",
    "end": "418160"
  },
  {
    "text": "model is very much like hey do you want to run a private data center versus you going to be on a cloud so those are the",
    "start": "418160",
    "end": "424000"
  },
  {
    "text": "similar considerations between using a local model versus external",
    "start": "424000",
    "end": "429560"
  },
  {
    "text": "model so now we are using a local model for example if you want to use a local",
    "start": "429560",
    "end": "434599"
  },
  {
    "text": "model say you download the model from hugging face if you're doing local inference then there are two elements",
    "start": "434599",
    "end": "440680"
  },
  {
    "text": "you got to think about one is the storage because the model you know if it's a 7 billion parameter model you",
    "start": "440680",
    "end": "446479"
  },
  {
    "text": "know it's 30 GB big it got to be stored on an NFS server or EFS or some sort of a storage internally then when you're",
    "start": "446479",
    "end": "454280"
  },
  {
    "text": "running the model as part of your compute then you need that compute capacity essentially it's a CPU it's a",
    "start": "454280",
    "end": "461599"
  },
  {
    "text": "GPU and of course your app is running over there as well so that's the combination you got to think about it",
    "start": "461599",
    "end": "466800"
  },
  {
    "text": "and really um you could be running like a 7 billion parameter which is about 26 gbyte or you could use a 7 billion",
    "start": "466800",
    "end": "473240"
  },
  {
    "text": "parameter model which is optimized and potentially still 7 GB model so all of",
    "start": "473240",
    "end": "479280"
  },
  {
    "text": "that storage runtime capacity something you got to plan for and how do you scale it all comes into the action um as",
    "start": "479280",
    "end": "487000"
  },
  {
    "text": "opposed to a external inference because then you're focused only on your app and",
    "start": "487000",
    "end": "493000"
  },
  {
    "text": "then all you're doing is in order to engage with an llm you just making an API that external model could be hosted",
    "start": "493000",
    "end": "499680"
  },
  {
    "text": "by open AI or gemini or whatever that model is you get your token and whether",
    "start": "499680",
    "end": "505000"
  },
  {
    "text": "you go local or external is a call that you will need to make",
    "start": "505000",
    "end": "510479"
  },
  {
    "text": "now this is a um uh wonderful tool that uh we found a couple of weeks ago so really what you can do is you can put",
    "start": "510479",
    "end": "517120"
  },
  {
    "text": "your number of input tokens so at the very top you see there are 100,000 input tokens then there are output tokens that",
    "start": "517120",
    "end": "524440"
  },
  {
    "text": "okay I'm going to put 100,000 input token and for that let's say I'm getting back 300,000 output token and roughly if",
    "start": "524440",
    "end": "532160"
  },
  {
    "text": "you think about 100,000 output token which estimated maybe you're making 8,000 API calls so in terms of your llm",
    "start": "532160",
    "end": "539560"
  },
  {
    "text": "pricing you're putting all the relevant information and what you're getting is if that's sort of the input output API",
    "start": "539560",
    "end": "546560"
  },
  {
    "text": "combination you're using is giving you the idea of a approximate price that is",
    "start": "546560",
    "end": "551839"
  },
  {
    "text": "going to cost for that model So based upon your usage I would say on the right column is the price So based upon that",
    "start": "551839",
    "end": "559120"
  },
  {
    "text": "user you should think about it is that price giving me the value or is that the way I should maybe look at maybe a local",
    "start": "559120",
    "end": "565600"
  },
  {
    "text": "inference in that case all right yes let's let's go now",
    "start": "565600",
    "end": "571440"
  },
  {
    "text": "with a with a package because we we have the model we Define if it's local or external and we need to find a way to",
    "start": "571440",
    "end": "578440"
  },
  {
    "text": "make it a package right so but recapping to the first slid we need to find",
    "start": "578440",
    "end": "584519"
  },
  {
    "text": "something to meet multiple business needs right so we have multiple models so you make the definition for instance",
    "start": "584519",
    "end": "590720"
  },
  {
    "text": "that you need to use multiple models and all all of those models commonly have different ways to be accessed different",
    "start": "590720",
    "end": "598160"
  },
  {
    "text": "comput or storage requirements because you probably may like to use a big model and you would like to use an optimized",
    "start": "598160",
    "end": "604839"
  },
  {
    "text": "model or a fine tune model so you have multiple different kind of models to use",
    "start": "604839",
    "end": "610000"
  },
  {
    "text": "and the requirements Al are also different because you need to meet the business needs of course so what you",
    "start": "610000",
    "end": "615600"
  },
  {
    "text": "need as a it or Tech or the technical part so you need to provide um a unified",
    "start": "615600",
    "end": "622120"
  },
  {
    "text": "way to interact with those mod so we need to find a way or tool or something that will be showcasing in the the next",
    "start": "622120",
    "end": "629600"
  },
  {
    "text": "slides that can allow us to use in a unified way basically so and that's tool",
    "start": "629600",
    "end": "636240"
  },
  {
    "text": "is L Ching I mean we are not sponsored by by L chain I don't know so okay good so basically L chain is an open source",
    "start": "636240",
    "end": "642639"
  },
  {
    "text": "project which basically a framework that you can use to make easy when you would like to build an llm that could be",
    "start": "642639",
    "end": "649639"
  },
  {
    "text": "chatbot or a conversation on chatbot and so on H it's based on Python javascripts",
    "start": "649639",
    "end": "655120"
  },
  {
    "text": "and also but the important thing is that it has support for 0 Plus the llm open",
    "start": "655120",
    "end": "660519"
  },
  {
    "text": "source models so that's the beauty because you can",
    "start": "660519",
    "end": "666040"
  },
  {
    "text": "work with multiple models just using the same tool you how to worry about is the",
    "start": "666040",
    "end": "671240"
  },
  {
    "text": "model configure for something or whatever so you have everything in one simple tool and the other one is that it",
    "start": "671240",
    "end": "677639"
  },
  {
    "text": "supports rag I mean most people is talking now about rag which is provided the context to the model but you can",
    "start": "677639",
    "end": "683720"
  },
  {
    "text": "also do it if you are using Lang chain so one point that I want to say on that slide as well is you know you're",
    "start": "683720",
    "end": "691079"
  },
  {
    "text": "really using learning the lag chain concept that what that abstraction looks like and then the plugable model fits in",
    "start": "691079",
    "end": "697800"
  },
  {
    "text": "very well and something else that we missed out on this one there's a project called as Lang chain for J so if you're",
    "start": "697800",
    "end": "703560"
  },
  {
    "text": "a Java developer for example which is used quite heavily in the Enterprises there is a very similar abstraction that",
    "start": "703560",
    "end": "709480"
  },
  {
    "text": "is done in the Lang chain for jand like using Lang chain so you can start interacting with that project if you",
    "start": "709480",
    "end": "715839"
  },
  {
    "text": "want to bring in those plugable llms into your Java project yes yes great great so how can we how is the logic now",
    "start": "715839",
    "end": "725120"
  },
  {
    "text": "if we would like to use it so we will start with a very complicated question which is tell me about cuetes so I don't",
    "start": "725120",
    "end": "732519"
  },
  {
    "text": "know if someone can answer about that but it's that would be the question that would be what will be making the",
    "start": "732519",
    "end": "737639"
  },
  {
    "text": "question to the llm and the first tool that we will use is is the prom template you know the challenges that the",
    "start": "737639",
    "end": "744120"
  },
  {
    "text": "challenges that we have when you are prompting to char PT or to a model so you need to know how to make the",
    "start": "744120",
    "end": "749600"
  },
  {
    "text": "question because they are probably not so smart as we think so they have something a tool called prom template",
    "start": "749600",
    "end": "756399"
  },
  {
    "text": "which is basically it helps you on your customer engineering which is basically instructing in a different way the model",
    "start": "756399",
    "end": "763120"
  },
  {
    "text": "so it's saying you are a very smart and educated assistant and so on and don't say if you don't know something and so",
    "start": "763120",
    "end": "770040"
  },
  {
    "text": "on and it adds the question in that part this is very important because how you make the question to the model you need",
    "start": "770040",
    "end": "776519"
  },
  {
    "text": "to at least provide the most context or given some instructions on how to behave",
    "start": "776519",
    "end": "782440"
  },
  {
    "text": "and the user is not doing that and you are not doing that when you are talking to a model of course this is why the",
    "start": "782440",
    "end": "788000"
  },
  {
    "text": "prompt template is very important there is the other part that is the model we need to use a model remember that this",
    "start": "788000",
    "end": "793680"
  },
  {
    "text": "is a local deployment in that part in that example so we need to download the model from The Hub which is Hing face",
    "start": "793680",
    "end": "801199"
  },
  {
    "text": "mainly if you if you are familiar with Hing face it's where all the models live basically so if you like to download the",
    "start": "801199",
    "end": "807720"
  },
  {
    "text": "model you need to download the from Hing face so this is basically how you download it and you need to put your",
    "start": "807720",
    "end": "814120"
  },
  {
    "text": "model in a pipeline basically the pipeline is think that a model is not",
    "start": "814120",
    "end": "819560"
  },
  {
    "text": "just the model is the model is the parameters there's the tokenizer and",
    "start": "819560",
    "end": "824600"
  },
  {
    "text": "multiple configurations that you can do when you are using an a model so you need to put everything in something",
    "start": "824600",
    "end": "830120"
  },
  {
    "text": "that's called a pipeline which is provided by hiding face and the beauty is that if you pick",
    "start": "830120",
    "end": "836440"
  },
  {
    "text": "the prom template and if you pick the pipeline L chain provides an API that is called chain which is a chain basically",
    "start": "836440",
    "end": "844040"
  },
  {
    "text": "which puts together the prompt and puts together the pipeline and how you use it",
    "start": "844040",
    "end": "849519"
  },
  {
    "text": "basically chain. invoke invoke is one method they have multiple methods but you as a developer the way that you have",
    "start": "849519",
    "end": "856279"
  },
  {
    "text": "to interact with the model is a method that is chain. invoke and you send your question basically and you got the",
    "start": "856279",
    "end": "863360"
  },
  {
    "text": "answer so hopefully the answer is correct yes and but now remember as as Arun said",
    "start": "863360",
    "end": "871120"
  },
  {
    "text": "at the beginning that we have optimized models and we have normal models right what happens if we have if we need to",
    "start": "871120",
    "end": "877320"
  },
  {
    "text": "make it fit in a CPU or in a low uh resource device well the chain will be",
    "start": "877320",
    "end": "883959"
  },
  {
    "text": "exactly the same right so we will be using the promp template same as usual but you need to do an nextra step right",
    "start": "883959",
    "end": "890680"
  },
  {
    "text": "inel provides Intel extension for Transformers which is basically make a quantization I mean optimization methods",
    "start": "890680",
    "end": "897959"
  },
  {
    "text": "that it's more like a a compressing if you're not familiar with that it's a kind of compressing which allows us to",
    "start": "897959",
    "end": "904720"
  },
  {
    "text": "start from a model that weighs 26 gigabits to a model that weighs 7",
    "start": "904720",
    "end": "910320"
  },
  {
    "text": "gigabits and this is the tool that runs the process so you can go to the GitHub",
    "start": "910320",
    "end": "916000"
  },
  {
    "text": "and you can run it and once you have that model you use it in the same way as we did it before right you put it on the",
    "start": "916000",
    "end": "922920"
  },
  {
    "text": "pipeline and you the pipeline is integrated with the chain and you interact in the same way with chain",
    "start": "922920",
    "end": "929519"
  },
  {
    "text": "invoke and you send the question and you get the same answer of course right so this is basically how",
    "start": "929519",
    "end": "936480"
  },
  {
    "text": "you move a model from a very weight model to a light model but you can also",
    "start": "936480",
    "end": "942360"
  },
  {
    "text": "use the same thing when you're doing something external let's suppose that you would like to use the external API",
    "start": "942360",
    "end": "947480"
  },
  {
    "text": "the open AI for for example and this is something that Lon has it has a an API",
    "start": "947480",
    "end": "952959"
  },
  {
    "text": "ready to be used when you are using something externally right so we have the chat open AI the only thing that you",
    "start": "952959",
    "end": "958720"
  },
  {
    "text": "need to create is the connection like the server which is the model client basically and this model client has the",
    "start": "958720",
    "end": "966560"
  },
  {
    "text": "key that connects with the open Ai and you interact in the same way you have the same chain and you do chain invoke",
    "start": "966560",
    "end": "973759"
  },
  {
    "text": "and you get the answer now we need to containerize how",
    "start": "973759",
    "end": "979120"
  },
  {
    "text": "we put it together yeah so the the whole idea that we saw showed in the three previous slides is you know you can use",
    "start": "979120",
    "end": "985920"
  },
  {
    "text": "whatever model you want whether it's a local model whether it's a quantize model whether it's an external model but",
    "start": "985920",
    "end": "992399"
  },
  {
    "text": "Lang chain is the abstraction that you are learning but let's think about what is containerization",
    "start": "992399",
    "end": "999560"
  },
  {
    "text": "mean first of all let's take a look at why Cloud native is the platform of choice for deploying llms um last year",
    "start": "1001079",
    "end": "1009639"
  },
  {
    "text": "um I live in the San Francisco Bay area we did the Ted AI uh is a is a Ted event",
    "start": "1009639",
    "end": "1015800"
  },
  {
    "text": "and we were talking to several folks over there that how are you deploying your llms kubernetes that was the answer",
    "start": "1015800",
    "end": "1022959"
  },
  {
    "text": "like of course kubernetes is a defacto compute platform but let's take a look at it why it is a platform as we talked",
    "start": "1022959",
    "end": "1029360"
  },
  {
    "text": "about it you know one of the key elements over there is how do if you",
    "start": "1029360",
    "end": "1034520"
  },
  {
    "text": "think about model by itself it is several components in there if you think about your app you know how you can",
    "start": "1034520",
    "end": "1040720"
  },
  {
    "text": "containerize all of that package that as a doer file and that dependency module",
    "start": "1040720",
    "end": "1047959"
  },
  {
    "text": "all packaged up to together it moves as a single unit is a big deal um scalability is a big part of it as we",
    "start": "1047959",
    "end": "1054559"
  },
  {
    "text": "saw in the demo this morning how you can use that in a kind cluster on your",
    "start": "1054559",
    "end": "1060080"
  },
  {
    "text": "desktop and the same concepts are done very very applicable when you're going",
    "start": "1060080",
    "end": "1065600"
  },
  {
    "text": "into a production you can scale it on eks or you can scale it on Azure or gke",
    "start": "1065600",
    "end": "1071200"
  },
  {
    "text": "or Intel developer Cloud it doesn't matter because once you have done the experimentation using your local desktop",
    "start": "1071200",
    "end": "1077679"
  },
  {
    "text": "they just scale out there another important part is the resource management these are again the concepts",
    "start": "1077679",
    "end": "1083440"
  },
  {
    "text": "that we know very well in the cloud native landscape you know models are memory hungry you know you don't want to",
    "start": "1083440",
    "end": "1089039"
  },
  {
    "text": "call NY neighbor so that's where you can start putting CPU memory limits on that",
    "start": "1089039",
    "end": "1094799"
  },
  {
    "text": "this is what it takes and know you can start putting priority classes that Which models are getting priority so all",
    "start": "1094799",
    "end": "1100320"
  },
  {
    "text": "of those usual execution Concepts that you would need for your model to run in",
    "start": "1100320",
    "end": "1105640"
  },
  {
    "text": "a business context are available in Cloud native you don't need to learn a new language uh portability is a big",
    "start": "1105640",
    "end": "1111840"
  },
  {
    "text": "deal like you know you have got a model up and running now um in your data",
    "start": "1111840",
    "end": "1117000"
  },
  {
    "text": "center now you are scaling it you want to go to a cloud provider now you want to run the model on an edge now you want to run the model maybe on your client",
    "start": "1117000",
    "end": "1123919"
  },
  {
    "text": "desktop so lots of different ways but kubernetes being the sort of the defao compute platform it really brings that",
    "start": "1123919",
    "end": "1130440"
  },
  {
    "text": "all along and a big part of models are because models are typically long",
    "start": "1130440",
    "end": "1135679"
  },
  {
    "text": "running they need a lot of observability and um open Telemetry based observability groups are really engaging",
    "start": "1135679",
    "end": "1142760"
  },
  {
    "text": "more and more um I was learning about a new project called as open llm metry",
    "start": "1142760",
    "end": "1148559"
  },
  {
    "text": "which is basically you know based upon open Telemetry but they are kind of looking into how they can provide more",
    "start": "1148559",
    "end": "1154240"
  },
  {
    "text": "observability into llms running in Cloud native some of the benefits on why Cloud",
    "start": "1154240",
    "end": "1159559"
  },
  {
    "text": "native is a platform of choice for deploying llms nice and now we need to put everything together in a in a",
    "start": "1159559",
    "end": "1165880"
  },
  {
    "text": "container right so the same thing as we showed before like the chain the huging face part the pipeline and the prom",
    "start": "1165880",
    "end": "1171840"
  },
  {
    "text": "template Lang chain provides also but is a is basically an API that exposes that",
    "start": "1171840",
    "end": "1177480"
  },
  {
    "text": "method like do invoke for instance and exposes so the way to interact with this",
    "start": "1177480",
    "end": "1183159"
  },
  {
    "text": "model or with is minimal part is via a post an API basically so you send the",
    "start": "1183159",
    "end": "1188480"
  },
  {
    "text": "question to that and what I wanted to highlight internally is that the two pipelines there are two pipelines here",
    "start": "1188480",
    "end": "1195240"
  },
  {
    "text": "but mainly the the idea is to Showcase that you can use either local or",
    "start": "1195240",
    "end": "1200760"
  },
  {
    "text": "external if you use in local you will not be storing your model locally when you're are building the container",
    "start": "1200760",
    "end": "1206919"
  },
  {
    "text": "basically so you're just creating the space and you're just putting the the direction of the file server because",
    "start": "1206919",
    "end": "1214240"
  },
  {
    "text": "that will be downloaded when it's launched and the pipeline of course for the external is a connection that you",
    "start": "1214240",
    "end": "1220679"
  },
  {
    "text": "will have to do with the connection with the external",
    "start": "1220679",
    "end": "1225840"
  },
  {
    "text": "API oh this is the animation that we love yes so when it starts working for",
    "start": "1227640",
    "end": "1233760"
  },
  {
    "text": "instance let's suppose that we have your models and your in your file server that could be EFS file server whatever so",
    "start": "1233760",
    "end": "1241000"
  },
  {
    "text": "basically when you are running the container or when the container launch what happens",
    "start": "1241000",
    "end": "1246200"
  },
  {
    "text": "is I love it it's the model is downloaded from the Lama from the file",
    "start": "1246200",
    "end": "1251600"
  },
  {
    "text": "server to The Container so the the model is leaving on the container of course what you need is the container is a pot",
    "start": "1251600",
    "end": "1258799"
  },
  {
    "text": "of course you need the surveys but you need the PVC and the PV the the",
    "start": "1258799",
    "end": "1263840"
  },
  {
    "text": "persistent volume claim and the persistent volume which is basically the way that you can connect to the file",
    "start": "1263840",
    "end": "1269120"
  },
  {
    "text": "server but what what I wanted to highlight here is that the model is downloaded every time the Pod launches",
    "start": "1269120",
    "end": "1275120"
  },
  {
    "text": "right so it doesn't leave there so you need to be aware as we talk in the section two I think so we need to know",
    "start": "1275120",
    "end": "1282120"
  },
  {
    "text": "that hey you need to have a file server so you need to think on the space where that that will be stored and you also",
    "start": "1282120",
    "end": "1288200"
  },
  {
    "text": "need to thing that you need to create that connection between your pod and the file server",
    "start": "1288200",
    "end": "1295000"
  },
  {
    "text": "basically sering multiple models so now we have our pots our containers and we",
    "start": "1295000",
    "end": "1300279"
  },
  {
    "text": "need to put them all together so now think about this you",
    "start": "1300279",
    "end": "1306200"
  },
  {
    "text": "know think about your intranet like where all of your you know workday pay uh legal notices Etc come",
    "start": "1306200",
    "end": "1315000"
  },
  {
    "text": "inside the company so this is the model that you want to think about that think about UI as your inet like your like at",
    "start": "1315000",
    "end": "1321440"
  },
  {
    "text": "Intel we have Intel circuit so we go to circuit. intel.com internally and that's",
    "start": "1321440",
    "end": "1326480"
  },
  {
    "text": "where my pay is available um all my um my workday is available and those could",
    "start": "1326480",
    "end": "1333559"
  },
  {
    "text": "go through a LM proxy and then that llm proxy is where you could do model",
    "start": "1333559",
    "end": "1339760"
  },
  {
    "text": "provisioning model governance you know how much cost allocation all of that",
    "start": "1339760",
    "end": "1344960"
  },
  {
    "text": "could be done at that proxy layer then from there it goes down to the actual",
    "start": "1344960",
    "end": "1350200"
  },
  {
    "text": "llm that is hosted down into the business unit so that proxy layer is where you could do a lot of control and",
    "start": "1350200",
    "end": "1356559"
  },
  {
    "text": "governance essentially and really unite all the elements over",
    "start": "1356559",
    "end": "1362440"
  },
  {
    "text": "there and if you are deploying all of that together in like a kubernetes",
    "start": "1365039",
    "end": "1370600"
  },
  {
    "text": "architecture know the llm proxy where we'll probably have a llm proxy adapter",
    "start": "1370600",
    "end": "1375919"
  },
  {
    "text": "running as a pod that could be like a horizontal scale that could scale really good and at the back end typically when you are running",
    "start": "1375919",
    "end": "1382640"
  },
  {
    "text": "the llm backend worker those are generally vertically scaled as opposed to horizontally scaled because they need",
    "start": "1382640",
    "end": "1389400"
  },
  {
    "text": "more M memory and CPU and GPU and all so that's sort of the way to think about it now there are products available in the",
    "start": "1389400",
    "end": "1396760"
  },
  {
    "text": "market for this already so take a look at it for example one of the companies that I advise for is carton Nemo and",
    "start": "1396760",
    "end": "1403720"
  },
  {
    "text": "they're exactly building like a llm proxy adapter by which they can allow you to do NLP based you know um going to",
    "start": "1403720",
    "end": "1411520"
  },
  {
    "text": "the right model essentially yeah and basically if you go just one basically the challenging uh with using llm proxy",
    "start": "1411520",
    "end": "1419720"
  },
  {
    "text": "is because you need to use a filter or something has to take the decision or when you receive the question or the",
    "start": "1419720",
    "end": "1426559"
  },
  {
    "text": "user is prompting something if you have something smart enough in the middle that's not just a proxy let's suppose",
    "start": "1426559",
    "end": "1431880"
  },
  {
    "text": "that in the proxy you have another model that is guessing or is uh predicting which is the topic of I mean of the",
    "start": "1431880",
    "end": "1438960"
  },
  {
    "text": "question probably so you can add external additional intelligence when you are using the llm proxy uh which is",
    "start": "1438960",
    "end": "1446720"
  },
  {
    "text": "basically I mean you know that the models I mean we need to to use something in the middle to interact with all the models basically um yes and now",
    "start": "1446720",
    "end": "1456640"
  },
  {
    "text": "um how we can build that in a c in a cluster in kubernetes because this is cubec Con and we need to to explain how",
    "start": "1456640",
    "end": "1463640"
  },
  {
    "text": "we build that on on on the kubernetes basically we deployed and the demo we",
    "start": "1463640",
    "end": "1469840"
  },
  {
    "text": "deployed an engine aing basically and we expose the front end and we expose the",
    "start": "1469840",
    "end": "1475640"
  },
  {
    "text": "proxy the front end uh is built on react and we need to expose the proxy of",
    "start": "1475640",
    "end": "1482279"
  },
  {
    "text": "course because the browser will be talking to the to the proxy but all the connection underneath like the local",
    "start": "1482279",
    "end": "1489159"
  },
  {
    "text": "models the optimize the external apis and everything they are parts that lives internally of course in the in the",
    "start": "1489159",
    "end": "1496520"
  },
  {
    "text": "cluster and of course you need a container registry because once you are launching you need to go to The Container registry and you need a PVC",
    "start": "1496520",
    "end": "1503279"
  },
  {
    "text": "and a PV all of those configurations they are on the GitHub so the step by",
    "start": "1503279",
    "end": "1508320"
  },
  {
    "text": "step on how to build it and they are all explained with detail in the in the",
    "start": "1508320",
    "end": "1514880"
  },
  {
    "text": "GitHub so let's do some recap H before going to the short demo what we what you",
    "start": "1514880",
    "end": "1521120"
  },
  {
    "text": "will see is you see a front end which is basically as I said in in react the",
    "start": "1521120",
    "end": "1527080"
  },
  {
    "text": "front end will be send in the the post apis the post calls to the proxy and the proxy has a connections of course with",
    "start": "1527080",
    "end": "1533640"
  },
  {
    "text": "multiple LMS models and there's a detail like re the API or the name of the API",
    "start": "1533640",
    "end": "1539919"
  },
  {
    "text": "which is basically if you see that on the GitHub you can see how is the flow",
    "start": "1539919",
    "end": "1544960"
  },
  {
    "text": "basically and has a fast API which is what exposes of course to the external",
    "start": "1544960",
    "end": "1550360"
  },
  {
    "text": "word and at the end you have the each",
    "start": "1550360",
    "end": "1555440"
  },
  {
    "text": "particular LM and they will be sending the answer of course to the front end and the user one detail of course that",
    "start": "1555440",
    "end": "1561440"
  },
  {
    "text": "we have at the in the in the foot is that the model is loaded when each",
    "start": "1561440",
    "end": "1567360"
  },
  {
    "text": "container is launched which is something that we talked before and let's go to the demo it will be very",
    "start": "1567360",
    "end": "1576320"
  },
  {
    "text": "short which is this right so this demo how I have it is built on internet Intel",
    "start": "1578760",
    "end": "1587559"
  },
  {
    "text": "kubernetes service which is the developer cloud service provided by Intel so it was recently launched so we",
    "start": "1587559",
    "end": "1595760"
  },
  {
    "text": "are using that and we are proposing to the users also to use it but it's basically a platform where you can",
    "start": "1595760",
    "end": "1601520"
  },
  {
    "text": "access to the newest Hardware that Intel is launching so you have to wait for instance for six months or five or five",
    "start": "1601520",
    "end": "1607880"
  },
  {
    "text": "months or whatever that the CSP is adopted uh you can use it as soon as it's lunch you have it there so and this",
    "start": "1607880",
    "end": "1615600"
  },
  {
    "text": "is basically the same as if you are familiar with of course with eks or something we have the ik which is the",
    "start": "1615600",
    "end": "1622600"
  },
  {
    "text": "Intel kubernetes service basically I have here the my cluster which is very",
    "start": "1622600",
    "end": "1628480"
  },
  {
    "text": "easy to create and also you can use the nodes in our case we'll be using two",
    "start": "1628480",
    "end": "1634360"
  },
  {
    "text": "nodes basically this small node which is this node that has the front end has the llm proxy and has the external open AI",
    "start": "1634360",
    "end": "1643799"
  },
  {
    "text": "connection which is basically we we decided to divide that because in terms of the processing needed and also the",
    "start": "1643799",
    "end": "1650159"
  },
  {
    "text": "storage needed so we have the small node for that and the other one is a big node",
    "start": "1650159",
    "end": "1656039"
  },
  {
    "text": "a Excel node using the laser generation of the DM processors and just to yes and this is",
    "start": "1656039",
    "end": "1664640"
  },
  {
    "text": "just to show you real quick if you would like to create for instance a node it's very simple you go to the noes",
    "start": "1664640",
    "end": "1671440"
  },
  {
    "text": "and if you would like to create a node group and here you can select all the instances that you can use for instance",
    "start": "1671440",
    "end": "1677000"
  },
  {
    "text": "you can use a small LGE or something and this is what I have",
    "start": "1677000",
    "end": "1682880"
  },
  {
    "text": "here if I go to my environment it's okay def fonts yes so first of",
    "start": "1682880",
    "end": "1690640"
  },
  {
    "text": "all this is the GitHub right so this is the same thing that we were talking",
    "start": "1690640",
    "end": "1696440"
  },
  {
    "text": "before but basically you have the four files which are how you deploy each",
    "start": "1696440",
    "end": "1701799"
  },
  {
    "text": "particular container so this is a recipe this not something that you do people installing it's working it's more an",
    "start": "1701799",
    "end": "1707440"
  },
  {
    "text": "educational GitHub so you have to download it you have to create your own containers you have to upload your",
    "start": "1707440",
    "end": "1713000"
  },
  {
    "text": "containers to your container registry and you have to deploy your kubernetes cluster everything is explained just to",
    "start": "1713000",
    "end": "1720399"
  },
  {
    "text": "go a bit in detail for for example if you would like to explore the deployment file here's the deployment file when you",
    "start": "1720399",
    "end": "1726720"
  },
  {
    "text": "create the deployment for the F for the Pod the service we create vertical part",
    "start": "1726720",
    "end": "1732600"
  },
  {
    "text": "outo scaler and also horizontal part outo scaler which is something that we showed in the in the previous slide but",
    "start": "1732600",
    "end": "1738399"
  },
  {
    "text": "this is how you created basically uh ding which is how you expose what we seen before like the low",
    "start": "1738399",
    "end": "1746399"
  },
  {
    "text": "the local Lama the proxy and everything how you expose it using the engine X and",
    "start": "1746399",
    "end": "1752240"
  },
  {
    "text": "how you expose the front end and the PVC of course that will be different if you are as in eks or any other provider but",
    "start": "1752240",
    "end": "1759760"
  },
  {
    "text": "you can you can use it so basically this is what I have deployed here just to",
    "start": "1759760",
    "end": "1765039"
  },
  {
    "text": "showcase for instance Let Go cube control get",
    "start": "1765039",
    "end": "1770039"
  },
  {
    "text": "pods we'll see that we have the four four pods running we have the front end",
    "start": "1770080",
    "end": "1775960"
  },
  {
    "text": "we have the Lama 7 billion nonoptimized the 7 billion optimize the proxy and the",
    "start": "1775960",
    "end": "1781240"
  },
  {
    "text": "open AI model just one thing that I think that is very important to mention",
    "start": "1781240",
    "end": "1787600"
  },
  {
    "text": "is that we talked about the the the weight of those models right so if we",
    "start": "1787600",
    "end": "1792960"
  },
  {
    "text": "see the Lama 7 billion nonoptimized it weighs 26",
    "start": "1792960",
    "end": "1798600"
  },
  {
    "text": "gigabits and sometimes it's very weird how to use that but gigabits gigabytes",
    "start": "1798600",
    "end": "1804279"
  },
  {
    "text": "and the optimize weights 3 gigb right so the story that's because the model is",
    "start": "1804279",
    "end": "1810440"
  },
  {
    "text": "running there so instead of using 20 26 we using just three gigb bytes so",
    "start": "1810440",
    "end": "1818760"
  },
  {
    "text": "let's how I how we worked on the on the demo we do a a forwarding of course just",
    "start": "1818760",
    "end": "1824760"
  },
  {
    "text": "to forward what we are doing and this is because I already have it so we",
    "start": "1824760",
    "end": "1832120"
  },
  {
    "text": "need to kill process that's a live demo right so this",
    "start": "1832120",
    "end": "1840278"
  },
  {
    "text": "is here we go that's it now let's do the",
    "start": "1840519",
    "end": "1845840"
  },
  {
    "text": "forwarding and it should be working so if we go to our browser and we go to the Local Host",
    "start": "1845840",
    "end": "1853240"
  },
  {
    "text": "we'll see it's not a real fancy interface to be honest but we use it react because most people would like to",
    "start": "1853240",
    "end": "1859679"
  },
  {
    "text": "use react because of the interactions that you can you can use we didn't want to use gradio something that you cannot",
    "start": "1859679",
    "end": "1865399"
  },
  {
    "text": "make a real use case so we use it react Des because of that I'm not a react developer so I did my best",
    "start": "1865399",
    "end": "1873320"
  },
  {
    "text": "so and it gives you the three options right it gives you the non-optimized version the optimized version and the",
    "start": "1873320",
    "end": "1878919"
  },
  {
    "text": "external version so the question will be the same question right tell me about",
    "start": "1878919",
    "end": "1885120"
  },
  {
    "text": "kubernetes and this is running in the local non-optimized model so it will take probably 10 seconds 8 seconds to",
    "start": "1885120",
    "end": "1892639"
  },
  {
    "text": "give you the answer because it's a non-optimized model and you see the weight that you have in that model that",
    "start": "1892639",
    "end": "1898559"
  },
  {
    "text": "is very i 26 gbits right so it's a lot of space of running that so it took 10",
    "start": "1898559",
    "end": "1906159"
  },
  {
    "text": "seconds 12 seconds and you can do start for instance and now we will like to use",
    "start": "1906159",
    "end": "1912399"
  },
  {
    "text": "to the optimize model and we will make the same question tell about kubernetes so the the optim",
    "start": "1912399",
    "end": "1920320"
  },
  {
    "text": "the optimization that we used is not to accelerate together it faster just because we didn't want it to to use that",
    "start": "1920320",
    "end": "1926519"
  },
  {
    "text": "so it was just an option we wanted to make it light so we have the same results a bit",
    "start": "1926519",
    "end": "1932360"
  },
  {
    "text": "faster so about half the time about half the time H but we are using this three G",
    "start": "1932360",
    "end": "1937840"
  },
  {
    "text": "gigabytes and of course if we go to the if we start again and if we would like to talk to the external",
    "start": "1937840",
    "end": "1944240"
  },
  {
    "text": "model which in this case is open AI you see tell me about",
    "start": "1944240",
    "end": "1950000"
  },
  {
    "text": "humanties and you will get the answer in probably 2 seconds 2 seconds 30 but of",
    "start": "1950000",
    "end": "1955519"
  },
  {
    "text": "course it it's faster because we using an exteral API so we are not we're not trying to compete against an exteral API",
    "start": "1955519",
    "end": "1962279"
  },
  {
    "text": "of course but basically what we are showing is that with a very light model",
    "start": "1962279",
    "end": "1967440"
  },
  {
    "text": "you can have the same results or similar results at least accessable the optimizations can go further and further",
    "start": "1967440",
    "end": "1973360"
  },
  {
    "text": "you can do even more faster you can be very close to the same result",
    "start": "1973360",
    "end": "1978399"
  },
  {
    "text": "results I'm not doing anything is that an indication our time",
    "start": "1978399",
    "end": "1985120"
  },
  {
    "text": "is up we got two more minutes okay we can go if if you don't like it and I don't know have to let's go to",
    "start": "1985120",
    "end": "1993279"
  },
  {
    "text": "the conclusion slide yeah let's go to the we'll talk through that one go to the slides yeah let's go to",
    "start": "1993279",
    "end": "2001080"
  },
  {
    "text": "the conclusion SL we'll talk through that one oh here we go okay it's working it's working right okay so let's go to",
    "start": "2001080",
    "end": "2007000"
  },
  {
    "text": "the conclusions yeah so in terms of conclusion you know",
    "start": "2007000",
    "end": "2013080"
  },
  {
    "text": "really the choice of the model that you want to use really depends upon your business need but the whole idea here is",
    "start": "2013080",
    "end": "2019440"
  },
  {
    "text": "you know langin makes it simple to switch between multiple models and we talked about why Cloud native is the",
    "start": "2019440",
    "end": "2025639"
  },
  {
    "text": "choice of running your llm and optimizations as you noticed you know plays a significant role it probably",
    "start": "2025639",
    "end": "2033159"
  },
  {
    "text": "ties you to a hardware but then you know depending upon what architecture you're using I I think it's an important",
    "start": "2033159",
    "end": "2040480"
  },
  {
    "text": "consideration yes and do you have the all the all the QR codes uh the Clone",
    "start": "2041639",
    "end": "2048560"
  },
  {
    "text": "you can go to to the geub you can clone we have our site we just open at in and we also have a podcast which is very",
    "start": "2048560",
    "end": "2055118"
  },
  {
    "text": "interesting to listen and so these are the links so thank you thank you for your time and thank you I think we are",
    "start": "2055119",
    "end": "2061919"
  },
  {
    "text": "just out of time",
    "start": "2061919",
    "end": "2066679"
  }
]