[
  {
    "text": "okay um it's been about another minute so I think I'll go ahead and get going",
    "start": "520",
    "end": "5759"
  },
  {
    "text": "um first of all thank you all for coming I know it's like 5:30 pretty close to dinner time uh if youall are hungry I um",
    "start": "5759",
    "end": "13519"
  },
  {
    "text": "I won't feel offended if you got to duck out because you just got to eat some food um I'm feeling pretty hungry myself",
    "start": "13519",
    "end": "18800"
  },
  {
    "text": "soon so looking forward to eating after this um I won't try to keep you",
    "start": "18800",
    "end": "24560"
  },
  {
    "text": "forever uh so this is going to be a fun topic I",
    "start": "24560",
    "end": "30039"
  },
  {
    "text": "think at least fun for me um because this is squarely at the intersection of two talk tracks I guess uh there's the",
    "start": "30039",
    "end": "38079"
  },
  {
    "text": "ML and AI track and there's the observability track and the two are very firmly intertwined in in this talk so if",
    "start": "38079",
    "end": "45680"
  },
  {
    "text": "you're familiar with one but not the other then hopefully you'll find some stuff that's useful here if you're not familiar with either uh you might be",
    "start": "45680",
    "end": "52559"
  },
  {
    "text": "able to bookmark some of the stuff after you go learn a little bit more about um some of the others but regardless um",
    "start": "52559",
    "end": "58120"
  },
  {
    "text": "let's get going so I want to talk about I call it observing an LM in",
    "start": "58120",
    "end": "63160"
  },
  {
    "text": "production but this is really about ways that you can make large language models more reliable in",
    "start": "63160",
    "end": "70360"
  },
  {
    "text": "production and um the impetus behind this is is you are likely aware pretty",
    "start": "70360",
    "end": "77000"
  },
  {
    "text": "much every organization in the planet right now is looking to build with large linguage modules to some extent and",
    "start": "77000",
    "end": "82320"
  },
  {
    "text": "they're really really powerful but they're kind of hard uh it's not magic",
    "start": "82320",
    "end": "87640"
  },
  {
    "text": "there's a lot of problems that they bring and a lot of challenges is that a lot of teams may not have necessarily",
    "start": "87640",
    "end": "92960"
  },
  {
    "text": "been prepared to handle uh we were one of those and so I want to walk through some of our case study at honeycomb",
    "start": "92960",
    "end": "98799"
  },
  {
    "text": "where we learned a whole lot um but I want to spend a little bit of time on why large language models are hard in",
    "start": "98799",
    "end": "105840"
  },
  {
    "text": "production so I would posit that your average",
    "start": "105840",
    "end": "112040"
  },
  {
    "text": "engineer who knows a thing or two about the product that you're working on if given an afternoon could probably cough",
    "start": "112040",
    "end": "119079"
  },
  {
    "text": "up some sort of prot protype that does something pretty useful with an llm that's that's some of the incredible",
    "start": "119079",
    "end": "124479"
  },
  {
    "text": "power and I think it I just want to underscore how amazing it is that we have literally the world's most powerful",
    "start": "124479",
    "end": "129800"
  },
  {
    "text": "machine learning models just available from a single API call like I could never have dreamed that we had this kind",
    "start": "129800",
    "end": "135440"
  },
  {
    "text": "of power uh available to us but with that comes a whole set of problems that we may not have knew that we would have",
    "start": "135440",
    "end": "142239"
  },
  {
    "text": "um and to extend that I would say it's actually fairly easy to take a feature",
    "start": "142239",
    "end": "147560"
  },
  {
    "text": "that uses a large language model to market for you know something that's experimental or Alpha quality or",
    "start": "147560",
    "end": "153040"
  },
  {
    "text": "something like that um if you give a team about a month they could probably put something together that integrates",
    "start": "153040",
    "end": "158959"
  },
  {
    "text": "with the west of your product solves a specific problem and does a pretty decent job at it but the problem is not",
    "start": "158959",
    "end": "165120"
  },
  {
    "text": "getting something to Market it's keeping it to Market because that initial launch that you might have you can create a",
    "start": "165120",
    "end": "171239"
  },
  {
    "text": "nice little marketing moment you can have a nice Splash maybe solve a problem or two and then your users are going to start getting used to whatever it's",
    "start": "171239",
    "end": "177640"
  },
  {
    "text": "doing they're going to expect it to do more they're going to want to do things that you could have never possibly predicted and they're going to want it",
    "start": "177640",
    "end": "184360"
  },
  {
    "text": "all to be Fairly reliable or at least what they think reliable means now",
    "start": "184360",
    "end": "189560"
  },
  {
    "text": "unfortunately you just put a non-deterministic UND debuggable black box into production and so that kind of",
    "start": "189560",
    "end": "195040"
  },
  {
    "text": "goes against the whole idea of trying to make software more reliable well it's it's not really an either or and I want",
    "start": "195040",
    "end": "201120"
  },
  {
    "text": "to talk a little bit about that um it's my hypothesis that it's very hard for",
    "start": "201120",
    "end": "207680"
  },
  {
    "text": "your typical software engineering team to test and debug llms in a way that",
    "start": "207680",
    "end": "213080"
  },
  {
    "text": "they would normal software and that's for a couple reasons I think the first and probably the most important one is",
    "start": "213080",
    "end": "218799"
  },
  {
    "text": "that when you give your end users a blank box and you tell them type whatever you want and we're going to try to make this work in our product that",
    "start": "218799",
    "end": "226120"
  },
  {
    "text": "opens up your product to inputs that you cannot possibly predict uh these are",
    "start": "226120",
    "end": "231680"
  },
  {
    "text": "modalities that your users have never used before that you've never tested before that you know you the English",
    "start": "231680",
    "end": "237720"
  },
  {
    "text": "language for is basically infinitely expressible and so they're going to infinitely Express as much as they can",
    "start": "237720",
    "end": "242879"
  },
  {
    "text": "inside of your product so you can't necessarily predict what the bounds of that is going to be can't write unit test proactively that are going to be as",
    "start": "242879",
    "end": "250200"
  },
  {
    "text": "good as they can be now you know maybe given enough time if you spent an entire year building something you could",
    "start": "250200",
    "end": "255560"
  },
  {
    "text": "approximate something that your users are going to do but a year in development is nonsense and no business",
    "start": "255560",
    "end": "260600"
  },
  {
    "text": "owner is going to actually accept that as a proposition for building with large language models um once you're actually",
    "start": "260600",
    "end": "267919"
  },
  {
    "text": "there though like it's not just about user inputs because these models uh depending on what you're doing can",
    "start": "267919",
    "end": "273000"
  },
  {
    "text": "sometimes be non-deterministic now there's a bit of nuance to that like if you're using open AI like the majority",
    "start": "273000",
    "end": "278240"
  },
  {
    "text": "of us they are literally non-deterministic but there are open source models where you can set the temperature value to zero and that can",
    "start": "278240",
    "end": "284720"
  },
  {
    "text": "actually be deterministic but um that's a piece that may change in time uh but",
    "start": "284720",
    "end": "290720"
  },
  {
    "text": "regardless there's infinite expressibility on the inputs and not",
    "start": "290720",
    "end": "296080"
  },
  {
    "text": "necessarily reliability on what the outputs are going to be with respect to that same in inut you could have somebody input the same thing twice and",
    "start": "296080",
    "end": "301720"
  },
  {
    "text": "get two different answers uh and that's by Design that's that's literally why we use this um there's the phrase that",
    "start": "301720",
    "end": "309280"
  },
  {
    "text": "large language models hallucinate is something that you'll see quite a bit uh",
    "start": "309280",
    "end": "314320"
  },
  {
    "text": "I don't really agree with that characterization I would say that every single output from a large language model is itself a hallucination and it's",
    "start": "314320",
    "end": "320880"
  },
  {
    "text": "really just a matter of tuning the hallucinations that you get every single time to be useful to the use case that",
    "start": "320880",
    "end": "326759"
  },
  {
    "text": "you actually have in mind uh and if you kind of make that mental shift you can sort of accept that okay this thing is",
    "start": "326759",
    "end": "333000"
  },
  {
    "text": "kind of inherently unreliable but we can reain it in a little bit and uh the last",
    "start": "333000",
    "end": "338840"
  },
  {
    "text": "piece of that uh which is there in in in text you can pretty much read it but um my own experiences with prompt",
    "start": "338840",
    "end": "345120"
  },
  {
    "text": "engineering with something that's live in production for the better part of the year it's extremely easy to cause a",
    "start": "345120",
    "end": "350440"
  },
  {
    "text": "regression uh because kind of going back to that first point your users are doing things that you may not necessarily",
    "start": "350440",
    "end": "356039"
  },
  {
    "text": "predict and you're going to have things that are actually working that you're really not aware of yet and if you fix",
    "start": "356039",
    "end": "361840"
  },
  {
    "text": "something that somebody said it's not doing the right thing you may have caused a regression with something that was already working and because you",
    "start": "361840",
    "end": "367880"
  },
  {
    "text": "weren't aware that it was working you've now fixed one problem and cause another uh this is extremely common in prompt",
    "start": "367880",
    "end": "373800"
  },
  {
    "text": "engineering this is something that anybody who's in production right now with this stuff probably knows and feels quite a bit and I want to tell a little",
    "start": "373800",
    "end": "380800"
  },
  {
    "text": "bit of the honeycomb story and how we managed to make our own feature uh at least our first feature that that use",
    "start": "380800",
    "end": "387080"
  },
  {
    "text": "large language models be good enough uh it's not perfect but it's it's pretty",
    "start": "387080",
    "end": "392560"
  },
  {
    "text": "good so in May of 2023 uh honeycomb is an observability product so if you're",
    "start": "392560",
    "end": "398960"
  },
  {
    "text": "not familiar with observability the idea is that you have a bunch of data from your systems sent to our backend and if",
    "start": "398960",
    "end": "405680"
  },
  {
    "text": "there's something going wrong or if there's something that's particularly slow you can query that data and you can say okay you know is this if I'm running",
    "start": "405680",
    "end": "413199"
  },
  {
    "text": "any Ecommerce site is my checkout service the one that's particularly slow or something like that there's all sorts of questions that you can come come to",
    "start": "413199",
    "end": "419960"
  },
  {
    "text": "our tool with and what we found with our new users who were coming in is they had heard honeycomb is a great observability",
    "start": "419960",
    "end": "426080"
  },
  {
    "text": "tool ah all right cool but I don't know how to use it I they they would come in",
    "start": "426080",
    "end": "431280"
  },
  {
    "text": "and they would know what they want to ask but we had a UI that didn't let them actually Express that in the first place",
    "start": "431280",
    "end": "438879"
  },
  {
    "text": "and they would walk away and say okay I don't know how to use your UI so one of our hypotheses was well what if we put a text box in there have a large language",
    "start": "438879",
    "end": "445599"
  },
  {
    "text": "model do its best to translate that into something that we can get structured output in the form of a Json object and",
    "start": "445599",
    "end": "452360"
  },
  {
    "text": "parse and validate that thing and then turn it into a query that we execute on behalf of the user um we launched uh it",
    "start": "452360",
    "end": "460360"
  },
  {
    "text": "was it was a great marketing moment uh I definitely do want to highlight that if if you have the opportunity to build",
    "start": "460360",
    "end": "465960"
  },
  {
    "text": "something with language models right now your marketing team is going to be very happy with you and that will provide",
    "start": "465960",
    "end": "471240"
  },
  {
    "text": "good business to the company that you work for so I recommend doing it um but more importantly we iterated uh a ton uh",
    "start": "471240",
    "end": "479520"
  },
  {
    "text": "uh after after we we released and we really truly stumbled our way into what",
    "start": "479520",
    "end": "485400"
  },
  {
    "text": "might be some best practices uh there were there were frankly no documented bre practices out there prompt",
    "start": "485400",
    "end": "490599"
  },
  {
    "text": "engineering is still kind of the wild west but it was even more the wild west earlier that year and we think we we",
    "start": "490599",
    "end": "496400"
  },
  {
    "text": "have a methodology and the beginnings of a tool set that can be used to range some of this in to make something not",
    "start": "496400",
    "end": "504440"
  },
  {
    "text": "just a good marketing moment that your marketing team is going to be happy with but actually turn it into Reliable Software that your end users are going",
    "start": "504440",
    "end": "510840"
  },
  {
    "text": "to enjoy using in the long run rather than just a fun little experiment that you eventually have to shut down because",
    "start": "510840",
    "end": "516839"
  },
  {
    "text": "it's too janky and so specifically um I didn't",
    "start": "516839",
    "end": "522440"
  },
  {
    "text": "add the numbers in here I completely forgot to do that uh but we did see actually some good results from doing",
    "start": "522440",
    "end": "528279"
  },
  {
    "text": "from doing this in particular with net new users who come into the product we see a lot more retention um from them",
    "start": "528279",
    "end": "534120"
  },
  {
    "text": "using the if if they use the natural language quering feature they end up quering their data a lot more in the",
    "start": "534120",
    "end": "539279"
  },
  {
    "text": "long run and ultimately converting into paid users more than people who don't and in particular our Enterprise sales",
    "start": "539279",
    "end": "546200"
  },
  {
    "text": "folks really love this feature because instead of having to handhold someone through the process of learning how to",
    "start": "546200",
    "end": "552240"
  },
  {
    "text": "use our product they can just say see that text box just plug in your question",
    "start": "552240",
    "end": "557320"
  },
  {
    "text": "and see what comes out and it'll like create a honeycomb query for you and then you can like manipulate that thing",
    "start": "557320",
    "end": "562560"
  },
  {
    "text": "afterwards and like in in a way even if it doesn't do what you want it kind of teaches you how to use the product such",
    "start": "562560",
    "end": "567720"
  },
  {
    "text": "that you can um be successful and and and our sales folks have basically said this is great",
    "start": "567720",
    "end": "573440"
  },
  {
    "text": "we we love it this has basically shortened the the cycle that it takes to get somebody over that first hump in in",
    "start": "573440",
    "end": "579079"
  },
  {
    "text": "using the thing so how did we get there we practiced observability and if",
    "start": "579079",
    "end": "586800"
  },
  {
    "text": "you're thinking that a representative from an observability company got up on stage and told you that they did some",
    "start": "586800",
    "end": "592279"
  },
  {
    "text": "observability for a feature and it worked really well um try not to be too cynical about that uh it it actually is",
    "start": "592279",
    "end": "600440"
  },
  {
    "text": "is legit and when I say observability to the max I really truly mean it so",
    "start": "600440",
    "end": "605800"
  },
  {
    "text": "whenever honeycomb releases a new feature we practice observability for it and see how it goes but we really don't",
    "start": "605800",
    "end": "612880"
  },
  {
    "text": "do it to the extent that we did for this feature not even remotely close uh very few times in in fact are we leaning on",
    "start": "612880",
    "end": "619959"
  },
  {
    "text": "observability as sort of like the primary way to improve a given product right you know very often frankly a lot",
    "start": "619959",
    "end": "626240"
  },
  {
    "text": "of our changes are front- end changes and we can just directly act the ask user like hey did this chart thing improve stuff and they say yes or no and",
    "start": "626240",
    "end": "632560"
  },
  {
    "text": "they sort of say hey this would be a little bit better um but with large language models you kind of can't do",
    "start": "632560",
    "end": "637800"
  },
  {
    "text": "that because uh especially we found that uh users are just going to use the ever",
    "start": "637800",
    "end": "643680"
  },
  {
    "text": "living crap out of this stuff because you give them a blank text box and they're really just going to go for it and um it would have been way too much",
    "start": "643680",
    "end": "650560"
  },
  {
    "text": "effort for us to like actually try to talk to all the different people and let alone understand if we had a accurate",
    "start": "650560",
    "end": "656040"
  },
  {
    "text": "sampling of like you know their experiences or our or what everybody wants so we had to use observability so",
    "start": "656040",
    "end": "662839"
  },
  {
    "text": "starting from scratch we captured everything as tracing data using open Telemetry so if you're not familiar with",
    "start": "662839",
    "end": "669600"
  },
  {
    "text": "a trace it's actually it's kind distributed Trace it's fancy word it's a $25 word for a 10-cent concept right",
    "start": "669600",
    "end": "676160"
  },
  {
    "text": "it's a bunch of structured logs that are just ordered and correlated with an ID and all that stuff kind of happens to",
    "start": "676160",
    "end": "682120"
  },
  {
    "text": "you automatically the idea is that if you have a given transaction across a system you can say Okay user input this",
    "start": "682120",
    "end": "688079"
  },
  {
    "text": "thing it pops through you know whatever subsystems are there maybe it calls another service maybe it calls a database eventually it sort of finishes",
    "start": "688079",
    "end": "694880"
  },
  {
    "text": "off and the user gets a result so there's sort of this pathway and what happened along that pathway that's what",
    "start": "694880",
    "end": "701839"
  },
  {
    "text": "tracing enables we capture absolutely as much contextual information as we can",
    "start": "701839",
    "end": "707480"
  },
  {
    "text": "between the point from which a user hits the get query button and they see a",
    "start": "707480",
    "end": "712720"
  },
  {
    "text": "query that has actually been executed on their behalf um specifically it's actually when we enue an object to run",
    "start": "712720",
    "end": "718399"
  },
  {
    "text": "against the query engine but it's pretty much the same thing um we do retrieval augmented generation or rag as it's",
    "start": "718399",
    "end": "725519"
  },
  {
    "text": "often called another $25 word for 10-cent concept um but it's a large",
    "start": "725519",
    "end": "731200"
  },
  {
    "text": "multi-step process and at every single one of those steps we capture relevant information about the decision that we",
    "start": "731200",
    "end": "736880"
  },
  {
    "text": "made and the data that we gathered that we ultimately contextualize into a prompt that finally makes its way to the",
    "start": "736880",
    "end": "742199"
  },
  {
    "text": "large language model and then also on the end of that",
    "start": "742199",
    "end": "747279"
  },
  {
    "text": "when we actually get a request we parse that request we we parse the the value",
    "start": "747279",
    "end": "752440"
  },
  {
    "text": "in this case is a Json object we turn it into what's called a honeycomb query specification that then gets validated",
    "start": "752440",
    "end": "759040"
  },
  {
    "text": "against a set of different rules and there's a couple other things that we do programmatically to fix things up sometimes depending on if they need to",
    "start": "759040",
    "end": "765480"
  },
  {
    "text": "be fixed up and all the way until a query gets executed and there's a little feedback mechanism and somebody can say",
    "start": "765480",
    "end": "771839"
  },
  {
    "text": "you know thumbs up or thumbs down or not sure on the result that we gave um it's",
    "start": "771839",
    "end": "778480"
  },
  {
    "text": "a lot of of data but it's also really important because every single one of these things are a part of the end user",
    "start": "778480",
    "end": "785480"
  },
  {
    "text": "experience and especially on the input side every one of those things directly influences the behavior of the large",
    "start": "785480",
    "end": "791880"
  },
  {
    "text": "language model and so if we want to systematically understand how this model is actually doing out in the real world",
    "start": "791880",
    "end": "797920"
  },
  {
    "text": "we have to capture this kind of data otherwise we're going to be blind as to why it's actually doing what it's",
    "start": "797920",
    "end": "804760"
  },
  {
    "text": "doing and so in particular I'll have a visual of this a little bit later um but there's I I say that there's",
    "start": "804760",
    "end": "810760"
  },
  {
    "text": "pretty much four primary things the user clicks the get query button there's a pipeline of context Gathering via a rag",
    "start": "810760",
    "end": "816399"
  },
  {
    "text": "pipeline that happens it's fully instrumented we make a request in this case to open Ai and then we parse that",
    "start": "816399",
    "end": "823120"
  },
  {
    "text": "the data and validate it against a set of rules and then we submit to our quering engine and that whole thing is 48 spans inside of a trace so 48",
    "start": "823120",
    "end": "829920"
  },
  {
    "text": "structured logs Each of which representing a particular piece of the end user experience and critically each",
    "start": "829920",
    "end": "837759"
  },
  {
    "text": "of those representing something that could potentially go wrong and we want to make sure that we understand okay if",
    "start": "837759",
    "end": "844800"
  },
  {
    "text": "something is going wrong is that because the model is at fault or are we making the wrong decisions When We Gather",
    "start": "844800",
    "end": "850959"
  },
  {
    "text": "context did we validate something incorrectly that actually happened",
    "start": "850959",
    "end": "856040"
  },
  {
    "text": "there's all kinds of things that can happen and then we also monitor this",
    "start": "856040",
    "end": "861759"
  },
  {
    "text": "thing with SOS or service level objectives so if you're not in the SR space a service level objective is",
    "start": "861759",
    "end": "869320"
  },
  {
    "text": "arguably the best way to monitor software today you're not monitoring things like you know my CPU level is",
    "start": "869320",
    "end": "875279"
  },
  {
    "text": "good or you know my memory usage isn't blowing up you're monitoring an end user experience in this case you're saying okay what matters here in this case we",
    "start": "875279",
    "end": "883480"
  },
  {
    "text": "want to make sure that the user hits a get query button and a query gets submitted to our querying engine that",
    "start": "883480",
    "end": "889720"
  },
  {
    "text": "needs to succeed like that that whole thing needs to actually happen and it should happen within a certain amount of",
    "start": "889720",
    "end": "895399"
  },
  {
    "text": "time because if it takes like a whole minute to do this then the user's probably just going to walk away in frustration anyway so doesn't even",
    "start": "895399",
    "end": "901079"
  },
  {
    "text": "matter if we were correct and so these are actually two service level objectives where we measure first of all",
    "start": "901079",
    "end": "906639"
  },
  {
    "text": "that entire time that entire time slice and we say it needs to take uh less than 10",
    "start": "906639",
    "end": "912440"
  },
  {
    "text": "seconds and then we call this an error SLO it's really kind of like an availability or success SLO maybe the",
    "start": "912440",
    "end": "919120"
  },
  {
    "text": "idea is that regardless of what the user input we should for the vast majority of",
    "start": "919120",
    "end": "924600"
  },
  {
    "text": "times always have a query that gets submitted against our querying engine that can ultimately result in you know",
    "start": "924600",
    "end": "930319"
  },
  {
    "text": "query results that somebody can look at that it did their job now of course the",
    "start": "930319",
    "end": "935639"
  },
  {
    "text": "the key thing with serviceable objectives they bake in a budget of failures so if somebody enters in like",
    "start": "935639",
    "end": "942279"
  },
  {
    "text": "my favorite color is blue well we can't really produce a honeycomb query based off of that that's kind of nonsense and",
    "start": "942279",
    "end": "949319"
  },
  {
    "text": "so that's fine the large majority of your users are not going to be inputting nonsense they're going to try to get some actual value out of this tool and",
    "start": "949319",
    "end": "956160"
  },
  {
    "text": "so that's what service level objectives allow you to do is they allow you to establish what your success rate over a",
    "start": "956160",
    "end": "961440"
  },
  {
    "text": "Target um period of time is and then um what your budget for failures are and",
    "start": "961440",
    "end": "967040"
  },
  {
    "text": "critically it measures every one of those things what I said those 48 uh pieces those 48 spans each of those can",
    "start": "967040",
    "end": "972759"
  },
  {
    "text": "represent an event that could potentially fail uh this service level objective and we want every single one",
    "start": "972759",
    "end": "978920"
  },
  {
    "text": "of those to to succeed so every step needs to happen and it needs to occur in less than 10",
    "start": "978920",
    "end": "986440"
  },
  {
    "text": "seconds and the other piece to this which um I think is not really talked",
    "start": "987079",
    "end": "993720"
  },
  {
    "text": "about in the llm space is if you want to iterate with the stuff in production you",
    "start": "993720",
    "end": "1000160"
  },
  {
    "text": "need to be able to deploy your code very rapidly um I would say that anybody who's worked at Big Tech probably",
    "start": "1000160",
    "end": "1006000"
  },
  {
    "text": "doesn't have this problem but the majority of Enterprise organizations out there are constantly trying to reduce",
    "start": "1006000",
    "end": "1011519"
  },
  {
    "text": "their cycle times because um regardless of if you're using LMS or not you want to be able to ship bug fixes pretty",
    "start": "1011519",
    "end": "1017279"
  },
  {
    "text": "quickly um this really dials that up if you're using large language models uh in",
    "start": "1017279",
    "end": "1023160"
  },
  {
    "text": "particular you want to do things like monitor that end user experience are we doing the job and are we doing it in the",
    "start": "1023160",
    "end": "1028520"
  },
  {
    "text": "time that we said we want to do it okay but what if we failed what if there's a case where something is not",
    "start": "1028520",
    "end": "1034839"
  },
  {
    "text": "very good okay well that's fine um we can fix that but we want to be able to deploy that and immediately look at the",
    "start": "1034839",
    "end": "1040000"
  },
  {
    "text": "past 24 hours and make sure that okay we're still doing the job that we want to do but we've also fixed the problem",
    "start": "1040000",
    "end": "1045400"
  },
  {
    "text": "that we thought we just fixed without introducing something new that could have gone wrong right and that's why",
    "start": "1045400",
    "end": "1050520"
  },
  {
    "text": "those slos are really important because those allow you to make sure that you're guarding against those regressions over time and kind of as I said before this",
    "start": "1050520",
    "end": "1058160"
  },
  {
    "text": "is something that you will find is extremely challenging to put into a unit test Suite you need to use an",
    "start": "1058160",
    "end": "1063799"
  },
  {
    "text": "observability tool that's monitoring all of this data that you've captured as traces to at a very high level of",
    "start": "1063799",
    "end": "1071200"
  },
  {
    "text": "granularity and once you can set up sort of this this flywheel where you're successfully sort of getting into this",
    "start": "1071200",
    "end": "1076840"
  },
  {
    "text": "mode where you are prompt engineering every day and fixing very specific",
    "start": "1076840",
    "end": "1082039"
  },
  {
    "text": "problems you're identifying a specific thing you ship that fix you monitor over the next 24 hours okay did this actually",
    "start": "1082039",
    "end": "1088799"
  },
  {
    "text": "do it rinse and repeat again and again and again this is exactly what we did with our feature uh um from May 3rd all",
    "start": "1088799",
    "end": "1096360"
  },
  {
    "text": "the way up until July I believe yeah all the way up until July and we literally shipped pretty much every single day we",
    "start": "1096360",
    "end": "1103559"
  },
  {
    "text": "would isolate a very specific problem and we would be constantly looking at these slos and looking at the Trace",
    "start": "1103559",
    "end": "1110000"
  },
  {
    "text": "data so that was a bit abstract there was a lot of stuff in there uh I want to",
    "start": "1110000",
    "end": "1115360"
  },
  {
    "text": "give you a walk through based off of uh this feature and actually how it's doing",
    "start": "1115360",
    "end": "1121320"
  },
  {
    "text": "live as of last week so the screenshots that I'm going to have in here are actually live from uh you know what's",
    "start": "1121320",
    "end": "1128200"
  },
  {
    "text": "actually happening right now with our query system feature so what you're looking at here",
    "start": "1128200",
    "end": "1135600"
  },
  {
    "text": "is our SLO page so there's a bit going on here um I want to make sure you can",
    "start": "1135600",
    "end": "1142080"
  },
  {
    "text": "actually have some time to absorb it it's called natural language query generation availability so we're saying",
    "start": "1142080",
    "end": "1148919"
  },
  {
    "text": "that the proportion of requests to generate natural language queries that complete with without an error and",
    "start": "1148919",
    "end": "1155400"
  },
  {
    "text": "so we're basically identifying this success criteria where we're saying okay that entire 48 step process was able to",
    "start": "1155400",
    "end": "1161880"
  },
  {
    "text": "succeed and if it failed for any reason at all doesn't matter what the error is it could be something completely",
    "start": "1161880",
    "end": "1168520"
  },
  {
    "text": "unrelated to the large language model it could be that we expected a Json object and we didn't get back Json it could be",
    "start": "1168520",
    "end": "1174760"
  },
  {
    "text": "that we just had a bug unrelated to any of this stuff that was somehow affecting this feature doesn't matter we want to",
    "start": "1174760",
    "end": "1180280"
  },
  {
    "text": "capture any of those reasons why something could possibly fail and so you can see that we have",
    "start": "1180280",
    "end": "1185799"
  },
  {
    "text": "about a 95.6 about 96% or so historical compliance with the ceso meaning that",
    "start": "1185799",
    "end": "1191679"
  },
  {
    "text": "about 96 out of 100 times somebody hits get query they actually get a query um",
    "start": "1191679",
    "end": "1197240"
  },
  {
    "text": "and then we have a budget burn down here you can see we have about 78% of our allocated error rates and this is something that's calculated",
    "start": "1197240",
    "end": "1202760"
  },
  {
    "text": "automatically by us saying that we want 80% of our requests that we make or 80%",
    "start": "1202760",
    "end": "1210400"
  },
  {
    "text": "of that you know people hitting that get query button succeeding over a period of 7 days so this is just sort of an overview",
    "start": "1210400",
    "end": "1218159"
  },
  {
    "text": "page but it's too much to sort of fit on a screen which is why I split this into two screenshots here and so the second",
    "start": "1218159",
    "end": "1224159"
  },
  {
    "text": "one is where it gets a little bit more interesting so this is Honeycomb SLO tool but you there are other service",
    "start": "1224159",
    "end": "1231000"
  },
  {
    "text": "level objective tools out there that can show you similar kinds of things this is starting to get into very specific",
    "start": "1231000",
    "end": "1236919"
  },
  {
    "text": "reasons why certain things may fail so what I'm highlighting here uh in in the",
    "start": "1236919",
    "end": "1242640"
  },
  {
    "text": "screenshot kind of in the bottom bottom left here is a particular error called",
    "start": "1242640",
    "end": "1248200"
  },
  {
    "text": "ml response does not contain valid Json so what that meant is when we got a",
    "start": "1248200",
    "end": "1254720"
  },
  {
    "text": "response back from openi we expected a Json object but we didn't actually get one back so we kind of can't create a",
    "start": "1254720",
    "end": "1261600"
  },
  {
    "text": "query when we don't have the object that represents what the query should be oops",
    "start": "1261600",
    "end": "1268080"
  },
  {
    "text": "okay well let's dig into that a little bit so what we can do is go directly",
    "start": "1268080",
    "end": "1274840"
  },
  {
    "text": "from one of those failures uh into a querying interface in this case this is Honeycombs querying interface where",
    "start": "1274840",
    "end": "1281320"
  },
  {
    "text": "we're we're scoping everything that we're looking at down to exactly those times when that error occurred when we",
    "start": "1281320",
    "end": "1288679"
  },
  {
    "text": "did not get a Json object back and we've also gone ahead and grouped by the user",
    "start": "1288679",
    "end": "1294840"
  },
  {
    "text": "input and the response so we can see okay sort of in in these in these three cases in this case over this over this",
    "start": "1294840",
    "end": "1300600"
  },
  {
    "text": "time span here um we got this is the user input and this is the output that",
    "start": "1300600",
    "end": "1305960"
  },
  {
    "text": "that resulted in that error and so indeed we can see there are three such cases where somebody entered something",
    "start": "1305960",
    "end": "1312559"
  },
  {
    "text": "and there was no response given back by open AI at all well that's not a Json object that sounds about right",
    "start": "1312559",
    "end": "1318840"
  },
  {
    "text": "but that last one is really bothering me because somebody input you know number",
    "start": "1318840",
    "end": "1324159"
  },
  {
    "text": "of requests where HTTP route contains this thing and this thing and this thing",
    "start": "1324159",
    "end": "1329640"
  },
  {
    "text": "it seems like a pretty valid input you know that I feel like that should result in a query and indeed it looks like I",
    "start": "1329640",
    "end": "1336360"
  },
  {
    "text": "have a Json object there and the in the response that kind of looks like it's it's a Json object so why did this",
    "start": "1336360",
    "end": "1344559"
  },
  {
    "text": "fail okay well that's something that I might want to invest so what comes next is digging into a",
    "start": "1344559",
    "end": "1352919"
  },
  {
    "text": "specific request going into each of those 48 steps and being able to understand exactly what happened when we",
    "start": "1352919",
    "end": "1360120"
  },
  {
    "text": "got there and so this is part of that um again because it's 48 pieces it's a",
    "start": "1360120",
    "end": "1367000"
  },
  {
    "text": "little hard to throw up on a screen there um and in fact I've collapsed over 20 of these spans but you can see that",
    "start": "1367000",
    "end": "1374000"
  },
  {
    "text": "there's a lot of operations that we're performing they're named in very particular ways such as find all",
    "start": "1374000",
    "end": "1380320"
  },
  {
    "text": "suggested in queries for data set most relevant columns fetching some embeddings from an embedding model that",
    "start": "1380320",
    "end": "1386200"
  },
  {
    "text": "we have create chat prompt all that kind of stuff all of these things are are relevant here um but in particular in",
    "start": "1386200",
    "end": "1393640"
  },
  {
    "text": "honeycom UI when when you select one of these things this is one of those spans in that Trace spans contain a bunch of",
    "start": "1393640",
    "end": "1399320"
  },
  {
    "text": "key value pairs that have Rich information that you can capture at each point and so in particular uh I actually",
    "start": "1399320",
    "end": "1407240"
  },
  {
    "text": "highlighted the response itself the the actual raw data there because I noticed that it didn't fit in my table View and",
    "start": "1407240",
    "end": "1414159"
  },
  {
    "text": "indeed that does look like it's a Json object but it doesn't look like it's",
    "start": "1414159",
    "end": "1420159"
  },
  {
    "text": "finished there's a order. external uncore and then it's it's done um well",
    "start": "1420159",
    "end": "1428520"
  },
  {
    "text": " uh that sounds like a bug uh we should probably fix that one um I'm the",
    "start": "1428520",
    "end": "1434080"
  },
  {
    "text": "owner of this feature so when I say we I mean probably should be me um but basically what I want to highlight",
    "start": "1434080",
    "end": "1441159"
  },
  {
    "text": "about this is I didn't start by knowing that there was this case where sometimes",
    "start": "1441159",
    "end": "1447120"
  },
  {
    "text": "it produces like part of a Json object um I started with a very very",
    "start": "1447120",
    "end": "1454039"
  },
  {
    "text": "high level these requests should succeed over a period of time and it started to",
    "start": "1454039",
    "end": "1459720"
  },
  {
    "text": "narrow down okay what are the what are the ones that actually fail all right let's now dig into that let's look at",
    "start": "1459720",
    "end": "1465679"
  },
  {
    "text": "inputs and outputs and just sort of see what's going on on oh crap we saw something indeed let's dig into that",
    "start": "1465679",
    "end": "1471720"
  },
  {
    "text": "Trace sort of walk through each of those spans and yes I actually did walk through each of these manually just to",
    "start": "1471720",
    "end": "1477960"
  },
  {
    "text": "sort of make sure everything was going on it took me a few minutes and then I found one of these and I said oh all",
    "start": "1477960",
    "end": "1483120"
  },
  {
    "text": "right well there we go it's an incomplete Json object can't parse that",
    "start": "1483120",
    "end": "1488320"
  },
  {
    "text": "oops that's a bug and kind of did the job there now I can take this into",
    "start": "1488320",
    "end": "1493600"
  },
  {
    "text": "prompt engineering and say yes we have we have a very specific problem that we can go and fix like you know this wasn't",
    "start": "1493600",
    "end": "1500200"
  },
  {
    "text": "nonsense that somebody input and in fact the model seemed like it was trying to uh to do something useful I mean it it",
    "start": "1500200",
    "end": "1507000"
  },
  {
    "text": "pulls up a bunch of columns here that look like they're they're relevant it's maybe pulling up too many columns that might be the reason why um but like this",
    "start": "1507000",
    "end": "1513919"
  },
  {
    "text": "now I can start forming hypothesis about why is the model not doing what it's doing and critically I have the direct",
    "start": "1513919",
    "end": "1520960"
  },
  {
    "text": "user input and then I have all data and every single piece of context that was assembled as a um as result of of that",
    "start": "1520960",
    "end": "1528799"
  },
  {
    "text": "tracing in fact um one of the fields on here is something that we call full prompt where where after we've sort of",
    "start": "1528799",
    "end": "1535520"
  },
  {
    "text": "gathered all context and parameterize everything into the prompt just literally that full string itself is actually available there and so what",
    "start": "1535520",
    "end": "1541840"
  },
  {
    "text": "that allows me to do is just quite literally copy paste that field go into prompt engineering and just replay that",
    "start": "1541840",
    "end": "1548039"
  },
  {
    "text": "request and be like okay is this reproduced like what is actually happening here do it enough times so",
    "start": "1548039",
    "end": "1553120"
  },
  {
    "text": "that so that I sort of have my my head in the right space and the Insight that",
    "start": "1553120",
    "end": "1558440"
  },
  {
    "text": "I sort of gleaned from here and when I say I need to fix this problem I mean I literally do need to fix this problem",
    "start": "1558440",
    "end": "1563520"
  },
  {
    "text": "because I've not submitted a bug fix for this yet um is due to a variety of",
    "start": "1563520",
    "end": "1569440"
  },
  {
    "text": "reasons we limit the response that we get back from open AI uh it would be a",
    "start": "1569440",
    "end": "1574480"
  },
  {
    "text": "whole other talk to uh describe why you want to do this kind of stuff why things like rate limiting matter why things",
    "start": "1574480",
    "end": "1580200"
  },
  {
    "text": "like tokens perm minut it matter and and various ways that you can guard against prompt prompt injection attacks and that",
    "start": "1580200",
    "end": "1585760"
  },
  {
    "text": "kind of stuff uh limiting response is one of your mitigations against a variety of problems that can go wrong",
    "start": "1585760",
    "end": "1591399"
  },
  {
    "text": "when you're building with large language models and in particular we say 150 tokens is sort of the max we assumed",
    "start": "1591399",
    "end": "1598159"
  },
  {
    "text": "that that would probably be enough for most responses that we get back and that's true but now we're at the point",
    "start": "1598159",
    "end": "1605720"
  },
  {
    "text": "where I actually do want to fix this thing and 150 is actually not enough um furthermore we've been in production",
    "start": "1605720",
    "end": "1611640"
  },
  {
    "text": "long enough that I can probably be I can probably be confident that we're not going to have this be the attack Vector",
    "start": "1611640",
    "end": "1618320"
  },
  {
    "text": "that finally takes us down um so maybe I can think about bumping that up or maybe",
    "start": "1618320",
    "end": "1624080"
  },
  {
    "text": "I can instead be okay well what if that output should not have been more than 150 tokens what if it should have been",
    "start": "1624080",
    "end": "1630880"
  },
  {
    "text": "less all right well that's another path that I can explore there and I think I really want to highlight that this is",
    "start": "1630880",
    "end": "1636440"
  },
  {
    "text": "based off of real user data and real interactions that we're getting in our product this is not something I had to",
    "start": "1636440",
    "end": "1642840"
  },
  {
    "text": "Divine from somewhere and and you know hope that it actually represented what users we're",
    "start": "1642840",
    "end": "1649760"
  },
  {
    "text": "doing so the last thing that I want to highlight is what I sort of showed you is there's",
    "start": "1650000",
    "end": "1657039"
  },
  {
    "text": "really nothing magical about this there's really nothing terribly special about it either you can do all of this today all of this is powered by open",
    "start": "1657039",
    "end": "1663760"
  },
  {
    "text": "Telemetry um if you're not familiar with otel I highly recommend checking it out it's quite a bit so set aside some time",
    "start": "1663760",
    "end": "1670080"
  },
  {
    "text": "to make sure you you you really uh um Can can understand what it gives you but it's an extremely powerful",
    "start": "1670080",
    "end": "1676360"
  },
  {
    "text": "instrumentation framework that that is also an open standard that works with pretty much any tool so you don't have to use honeycomb you can use pretty much",
    "start": "1676360",
    "end": "1682720"
  },
  {
    "text": "any observability tool out there they all support open Telemetry and you can do this today whatever language you're",
    "start": "1682720",
    "end": "1687799"
  },
  {
    "text": "using that's actually running your application you can just pick the SDK that supports it right now otel has stable support for 11 different",
    "start": "1687799",
    "end": "1693720"
  },
  {
    "text": "languages so chances are what you're using is already supported you can add what's called Auto instrumentation to",
    "start": "1693720",
    "end": "1699399"
  },
  {
    "text": "your whole system and you start with that first and the idea is that that picks up things like you know request",
    "start": "1699399",
    "end": "1705640"
  },
  {
    "text": "incoming response outgoing across your entire system so that especially if you're doing something like retrieval",
    "start": "1705640",
    "end": "1711399"
  },
  {
    "text": "where maybe you hit a database maybe you call another service you want to make sure you're actually connecting all of",
    "start": "1711399",
    "end": "1716480"
  },
  {
    "text": "those pieces in that pathway that that a a user's request goes in that that gets",
    "start": "1716480",
    "end": "1721880"
  },
  {
    "text": "instrumented for you automatically so you can really just focus in the context of your own application okay when we're",
    "start": "1721880",
    "end": "1727440"
  },
  {
    "text": "doing retrieval this is the operation that we're performing we're going to create a span we're going to capture a",
    "start": "1727440",
    "end": "1732960"
  },
  {
    "text": "bunch of data at that point and that's just going to be a part of this Trace that's been created for us or rather the",
    "start": "1732960",
    "end": "1738080"
  },
  {
    "text": "skeleton of that has been created for us automatically by open Telemetry and then you can just have your Dev team in the",
    "start": "1738080",
    "end": "1743360"
  },
  {
    "text": "course of I'd say maybe a day maybe a little bit longer kind of depends on um",
    "start": "1743360",
    "end": "1748880"
  },
  {
    "text": "otel setup um get open Telemetry setup and start identifying these are the",
    "start": "1748880",
    "end": "1754840"
  },
  {
    "text": "relevant operations that we want to instrument and instrument those things um I've seen this many times lots of",
    "start": "1754840",
    "end": "1761000"
  },
  {
    "text": "Engineers uh like a senior engineer on a team somewhere could probably get this stuff wired up within a day and start to",
    "start": "1761000",
    "end": "1767000"
  },
  {
    "text": "give you even just the most basic of insights about what your users are actually doing lastly I think you can expect the",
    "start": "1767000",
    "end": "1774760"
  },
  {
    "text": "way that you accomplish observability with llms to get a lot better over time um right now there's a lot of really",
    "start": "1774760",
    "end": "1782480"
  },
  {
    "text": "rich automatic instrumentation for all kinds of backend Technologies those backend technologies have been around",
    "start": "1782480",
    "end": "1788000"
  },
  {
    "text": "for a lot longer than the uh constellation of technologies that are sort of emerging when building things",
    "start": "1788000",
    "end": "1793720"
  },
  {
    "text": "with large language models that's going to change uh we are sort of end route I guess you could say to building Auto",
    "start": "1793720",
    "end": "1800399"
  },
  {
    "text": "instrumentation packages for things like requests to llm Providers uh Frameworks",
    "start": "1800399",
    "end": "1805480"
  },
  {
    "text": "like Lang chain and llama index and all the different C of different Vector databases that you might be using and",
    "start": "1805480",
    "end": "1811559"
  },
  {
    "text": "just making sure that automatic instrumentation that captures these are the requests these are the inputs and",
    "start": "1811559",
    "end": "1816919"
  },
  {
    "text": "these are the outputs that came back can get wired up for you um similarly there are standards being developed right now",
    "start": "1816919",
    "end": "1823559"
  },
  {
    "text": "in the open Telemetry uh side of things um come talk to me I'm the person who's",
    "start": "1823559",
    "end": "1828679"
  },
  {
    "text": "proposing them uh and uh we're basically identifying naming conventions for",
    "start": "1828679",
    "end": "1834320"
  },
  {
    "text": "things so that for example if you want to capture a prompt the name of that should probably be like lm. prompt or",
    "start": "1834320",
    "end": "1839960"
  },
  {
    "text": "something like that and what that does is that provides a a specification that additional libraries can be built up",
    "start": "1839960",
    "end": "1846440"
  },
  {
    "text": "against so that eventually we can have a pretty rich ecosystem of automatic instrumentation sort of at every level",
    "start": "1846440",
    "end": "1852679"
  },
  {
    "text": "of application that uses large language models um and finally um observability",
    "start": "1852679",
    "end": "1858279"
  },
  {
    "text": "tools are you know we're identifying this is like this is a legit use case for observability tools period and so",
    "start": "1858279",
    "end": "1864840"
  },
  {
    "text": "honeycom is one I know data dog is another there there are other vendors out there that are starting to build out better support within their own product",
    "start": "1864840",
    "end": "1870639"
  },
  {
    "text": "once you have that data to support this use case quite a bit better so that's all I got and uh we got just a",
    "start": "1870639",
    "end": "1878600"
  },
  {
    "text": "few more minutes but it's also near uh dinner time uh feel free to ask me any questions you have I'll stay here as long as you want me to um but if you",
    "start": "1878600",
    "end": "1885120"
  },
  {
    "text": "leave to go grab food I will not be offended",
    "start": "1885120",
    "end": "1889159"
  },
  {
    "text": "than",
    "start": "1893760",
    "end": "1896760"
  }
]