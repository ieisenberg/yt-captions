[
  {
    "start": "0",
    "end": "150000"
  },
  {
    "text": "okay i want to thank everyone for joining us welcome to today's cncf webinar what's new in linkard i'm libby schultz",
    "start": "1920",
    "end": "9760"
  },
  {
    "text": "i'll be moderating today's webinar and we would like to welcome our presenter today oliver gould linkard",
    "start": "9760",
    "end": "15599"
  },
  {
    "text": "ceo excuse me a few housekeeping items before we get",
    "start": "15599",
    "end": "20720"
  },
  {
    "text": "started during the webinar you're not able to talk as an attendee there's a q a box at the bottom of your",
    "start": "20720",
    "end": "26320"
  },
  {
    "text": "screen please feel free to add your questions there not the chat the q a and we'll get as many in as we can at",
    "start": "26320",
    "end": "33040"
  },
  {
    "text": "the end this is an official webinar of the cncf and as such is subject to the cncf code",
    "start": "33040",
    "end": "38960"
  },
  {
    "text": "of conduct please do not add anything to the chat or questions that would be in violation of that code of conduct",
    "start": "38960",
    "end": "44960"
  },
  {
    "text": "basically just please be respectful of your fellow participants and presenters also please note the recording and",
    "start": "44960",
    "end": "51120"
  },
  {
    "text": "slides will be posted later on the cncf webinar page at www dot cncf dot io slash webinars",
    "start": "51120",
    "end": "58719"
  },
  {
    "text": "and with that i will hand it over to oliver thanks lindy um so before i dive into",
    "start": "58719",
    "end": "65518"
  },
  {
    "text": "this if you can figure out how to give me a reaction and i just want to see if you can give me a thumbs up in the in zoom before i",
    "start": "65519",
    "end": "73920"
  },
  {
    "text": "go on are you alive there are you able to direct yeah we get some hands there thank you great okay so before",
    "start": "73920",
    "end": "80880"
  },
  {
    "text": "i dive in how many of you are actually running kubernetes in production give me",
    "start": "80880",
    "end": "86479"
  },
  {
    "text": "a hand if you are cool we got a few there oh excellent um",
    "start": "86479",
    "end": "92320"
  },
  {
    "text": "how many of you carry a pager for kubernetes in production where you're actually responsible for",
    "start": "92320",
    "end": "98320"
  },
  {
    "text": "things running and we got okay we got a few um anyone running",
    "start": "98320",
    "end": "103680"
  },
  {
    "text": "linkery in production okay great we still got a few excellent",
    "start": "103680",
    "end": "109280"
  },
  {
    "text": "that's what i like to see well my hope today is that uh if you haven't used linkedin before that this",
    "start": "109280",
    "end": "115119"
  },
  {
    "text": "talk gives you enough to get started and understand what problems it solved and how it works and if you have used linkedin before i",
    "start": "115119",
    "end": "121119"
  },
  {
    "text": "hope this will be an invite to get more involved in the community and contribute uh in one of many ways um",
    "start": "121119",
    "end": "128000"
  },
  {
    "text": "so with that my name is oliver gould i'm the creator of linker d i've been working on this for the past",
    "start": "128000",
    "end": "133200"
  },
  {
    "text": "few years and uh the talk will basically start with a brief overview of linkedin",
    "start": "133200",
    "end": "138879"
  },
  {
    "text": "service meshes and what they do i'll take you through a tour of link reduce features and then i will get to",
    "start": "138879",
    "end": "144319"
  },
  {
    "text": "the good stuff we'll get to a demo and then have some questions after that great so linker d",
    "start": "144319",
    "end": "151120"
  },
  {
    "start": "150000",
    "end": "196000"
  },
  {
    "text": "has been around we've been working on it since 2015 i believe",
    "start": "151120",
    "end": "156400"
  },
  {
    "text": "um and it's been production for over four years now we've got a really active community many of you",
    "start": "156400",
    "end": "162000"
  },
  {
    "text": "already in the slack that are already dealing with us in github and we really appreciate that and we've been in production with a",
    "start": "162000",
    "end": "168400"
  },
  {
    "text": "whole uh you know wealth of different types of companies very small very large",
    "start": "168400",
    "end": "173519"
  },
  {
    "text": "uh big traffic small traffic everything and we've been part of cncf since early 2016",
    "start": "173519",
    "end": "180159"
  },
  {
    "text": "um if i recall correctly but we've been in cncf for a long time and that means we're committed to open governance this",
    "start": "180159",
    "end": "185920"
  },
  {
    "text": "isn't a corporate project this is a community project even though buoyant where i work does a",
    "start": "185920",
    "end": "193840"
  },
  {
    "text": "lot of the work on the project so lingerie comes out of my experience",
    "start": "193840",
    "end": "199519"
  },
  {
    "start": "196000",
    "end": "275000"
  },
  {
    "text": "or our experience uh working at twitter i was a production operations engineer at twitter from 2010",
    "start": "199519",
    "end": "206879"
  },
  {
    "text": "to 2015 and that's where we really saw what was one of the you know first modern microservices of",
    "start": "206879",
    "end": "213519"
  },
  {
    "text": "all um so this is of course with mesos because kubernetes was around yet but a lot of the same",
    "start": "213519",
    "end": "219360"
  },
  {
    "text": "problems and primitives was there and i was on call for service discovery and traffic management and a lot of the kind",
    "start": "219360",
    "end": "225680"
  },
  {
    "text": "of core concepts that link these uh deals with and then in 2016 we we",
    "start": "225680",
    "end": "231200"
  },
  {
    "text": "launched linker d um 1.0 which was a jvm based proxy and it",
    "start": "231200",
    "end": "237280"
  },
  {
    "text": "wasn't kubernetes specific it's super super generic can be tied together mesos and",
    "start": "237280",
    "end": "242480"
  },
  {
    "text": "console and kubernetes and all sorts of things but over time we learned that the that kind of",
    "start": "242480",
    "end": "247920"
  },
  {
    "text": "heavyweight flexibility um wasn't really a good suit for kubernetes and so we",
    "start": "247920",
    "end": "254080"
  },
  {
    "text": "created a new version of linkedin too which is really lightweight meant to just get up",
    "start": "254080",
    "end": "259759"
  },
  {
    "text": "and running and really tied to kubernetes tightly and that's really what i want to talk about today is that that new version of linkery",
    "start": "259759",
    "end": "265919"
  },
  {
    "text": "which has been around since oh i don't know 2018 is when it became 32. we've been",
    "start": "265919",
    "end": "272240"
  },
  {
    "text": "working on this since probably 2017 sometime lingerie out of the box give you three",
    "start": "272240",
    "end": "279199"
  },
  {
    "start": "275000",
    "end": "331000"
  },
  {
    "text": "kind of core uh you know polls of value uh first being observability it can be really",
    "start": "279199",
    "end": "285520"
  },
  {
    "text": "hard to understand what's going on in your cluster um cube cuddle is great but it can tell",
    "start": "285520",
    "end": "290800"
  },
  {
    "text": "you whether things are up or down they can't tell you if things are slow or fast or if they're",
    "start": "290800",
    "end": "296000"
  },
  {
    "text": "failing requests that's kind of very decoupled from the kubernetes ecosystem",
    "start": "296000",
    "end": "301120"
  },
  {
    "text": "and we also have support for tracing and i'll get into more of that later the other big thing we do is",
    "start": "301120",
    "end": "306479"
  },
  {
    "text": "connectivity and so load balancing timeouts routing connecting clusters",
    "start": "306479",
    "end": "312479"
  },
  {
    "text": "uh making sure that a pod can talk to another pod or another service is a big part of what he does and we do",
    "start": "312479",
    "end": "319680"
  },
  {
    "text": "that securely we do that with mtls by default for everything and we integrate with projects like cert",
    "start": "319680",
    "end": "325520"
  },
  {
    "text": "manager which is just admitted as a cncf project uh congrats to manager and uh i'll get into more of these",
    "start": "325520",
    "end": "332800"
  },
  {
    "start": "331000",
    "end": "364000"
  },
  {
    "text": "details later let's start though with microservices",
    "start": "332800",
    "end": "338240"
  },
  {
    "text": "i assume most of you are basically familiar with microservices but just to kind of lay the groundwork",
    "start": "338240",
    "end": "344800"
  },
  {
    "text": "microservices are you know contrast with what we had before then they're kind of the lamp",
    "start": "344800",
    "end": "350240"
  },
  {
    "text": "stack and that we've taken what used to be a bunch of library interactions and this",
    "start": "350240",
    "end": "355280"
  },
  {
    "text": "kind of linking and put that over the network and so now we can separate business logic over the network and things call each",
    "start": "355280",
    "end": "361600"
  },
  {
    "text": "other via apis the service mesh is a way to",
    "start": "361600",
    "end": "368080"
  },
  {
    "start": "364000",
    "end": "396000"
  },
  {
    "text": "add rich operability the things we're just talking about as a side car to this so you don't have",
    "start": "368080",
    "end": "374560"
  },
  {
    "text": "to embed this in a library in your code of course you can but it's about having",
    "start": "374560",
    "end": "379600"
  },
  {
    "text": "a proxy that's able to do take on a lot of these concerns outside of your application so you can have it",
    "start": "379600",
    "end": "384720"
  },
  {
    "text": "uniformly depending on you know regardless of what application stack you're using whether",
    "start": "384720",
    "end": "390479"
  },
  {
    "text": "the homogenous or heterogeneous a service mesh can make that you know uniform and work well",
    "start": "390479",
    "end": "397440"
  },
  {
    "start": "396000",
    "end": "426000"
  },
  {
    "text": "this is all powered by a control plane and so you have a kubernetes control plane which is the",
    "start": "397440",
    "end": "402960"
  },
  {
    "text": "kubernetes api server and its extensions and linkery provides a set of control",
    "start": "402960",
    "end": "409120"
  },
  {
    "text": "plane apis that are interact closely with the groupenetics apis to power the proxies",
    "start": "409120",
    "end": "414160"
  },
  {
    "text": "so in linkerid itself the proxy is basically unaware of anything having to do with",
    "start": "414160",
    "end": "419520"
  },
  {
    "text": "kubernetes it only knows about the control plane api the control plane itself is very tightly coupled to kubernetes",
    "start": "419520",
    "end": "427360"
  },
  {
    "start": "426000",
    "end": "475000"
  },
  {
    "text": "this kind of looks like this right we have these proxies are ejected sidecar so it",
    "start": "427599",
    "end": "433840"
  },
  {
    "text": "within your pod we add a container to your pod which is the proxy",
    "start": "433840",
    "end": "439199"
  },
  {
    "text": "that's done by a control plane web hook called the proxy injector also in the control plane we have a",
    "start": "439199",
    "end": "445039"
  },
  {
    "text": "certificate authority which is the proxy uses to establish identities we'll get into more of those details",
    "start": "445039",
    "end": "450720"
  },
  {
    "text": "later and then we've we've built the proxy in rust we've built most of the control plane in go uh we ship",
    "start": "450720",
    "end": "457039"
  },
  {
    "text": "up an instance of prometheus and grafana by default uh we have great helm charts we support smi",
    "start": "457039",
    "end": "462880"
  },
  {
    "text": "we're a big part of the you know we're big fans of the cncf ecosystem and so we really try to leverage as much",
    "start": "462880",
    "end": "468960"
  },
  {
    "text": "of that ecosystem as you can that's not in our core competency",
    "start": "468960",
    "end": "473919"
  },
  {
    "start": "475000",
    "end": "555000"
  },
  {
    "text": "our goal is to do this without adding complexity so kubernetes itself is",
    "start": "475759",
    "end": "481120"
  },
  {
    "text": "complex and that's really what we find no one should want to adopt a service",
    "start": "481120",
    "end": "486160"
  },
  {
    "text": "mesh most people need are having enough trouble just to adopt kubernetes and linguity is meant to just kind of",
    "start": "486160",
    "end": "493440"
  },
  {
    "text": "get installed get out of your way and then grow with you as you have more problems that you're trying to solve we",
    "start": "493440",
    "end": "498720"
  },
  {
    "text": "really don't want you to try to solve all of the traffic problems on day one",
    "start": "498720",
    "end": "503840"
  },
  {
    "text": "that should be an incremental path if you're going to be successful and so to do that we make it you know it",
    "start": "503840",
    "end": "509360"
  },
  {
    "text": "installs you add it to your application almost no configuration necessary to get started",
    "start": "509360",
    "end": "514880"
  },
  {
    "text": "there we really focus on minimizing the resource requirements of linguity and",
    "start": "514880",
    "end": "521680"
  },
  {
    "text": "i'll show you some examples of that later part of our goals for simplicity are",
    "start": "521680",
    "end": "527839"
  },
  {
    "text": "that we don't try to invent new abstractions that are specific to linkedin we really embrace kubernetes primitives and",
    "start": "527839",
    "end": "535440"
  },
  {
    "text": "sometimes to a fault i would argue but we really don't want you to have to introduce new types of",
    "start": "535440",
    "end": "540880"
  },
  {
    "text": "abstractions that you have to think about and manage we really try to use things that are already well understood and well supported",
    "start": "540880",
    "end": "547519"
  },
  {
    "text": "and we try to do that securely by default um and we'll we'll show more examples of",
    "start": "547519",
    "end": "552959"
  },
  {
    "text": "this later uh we do mtls uh by well we add security by",
    "start": "552959",
    "end": "562000"
  },
  {
    "start": "555000",
    "end": "615000"
  },
  {
    "text": "supporting mtls in the proxy that's mutual identity where every proxy gets its own uh",
    "start": "562000",
    "end": "568000"
  },
  {
    "text": "generates its own private keys those private keys never leave the pod",
    "start": "568000",
    "end": "574399"
  },
  {
    "text": "and then we automatically secure everything we can and we don't currently break anything that we can't",
    "start": "574399",
    "end": "579519"
  },
  {
    "text": "secure and this is really important because a lot of the kubernetes core api or core traffic",
    "start": "579519",
    "end": "584880"
  },
  {
    "text": "like uh health checks readiness probes etc can't be secured by default today and so",
    "start": "584880",
    "end": "591440"
  },
  {
    "text": "we really need to take an incremental approach to improving things and making it auditable we've really focused on secure",
    "start": "591440",
    "end": "597600"
  },
  {
    "text": "foundations and so our proxy is written in rust which is a memory safe native language show more examples of",
    "start": "597600",
    "end": "604480"
  },
  {
    "text": "that later and again built on kubernetes we want the goal for linkedin is to be able to install it",
    "start": "604480",
    "end": "610959"
  },
  {
    "text": "add tls add policy and get going",
    "start": "610959",
    "end": "616000"
  },
  {
    "text": "the proxy is to uh one of the biggest questions we get is does linker to use envoy and the answer",
    "start": "616480",
    "end": "622399"
  },
  {
    "text": "is no um i know we really have tailored our proxy to the service mesh use case it's not a",
    "start": "622399",
    "end": "628640"
  },
  {
    "text": "generally configurable proxy there's no config file that you can go change how the proxy works",
    "start": "628640",
    "end": "634480"
  },
  {
    "text": "or add plugins to it today and that's one for simplicity two for for minimizing resource overhead",
    "start": "634480",
    "end": "641680"
  },
  {
    "text": "and three for security um we find that the more configurable something in this",
    "start": "641680",
    "end": "647360"
  },
  {
    "text": "kind of critical part of the stack is the harder it is to audit the harder it is to gain",
    "start": "647360",
    "end": "653120"
  },
  {
    "text": "confidence about being secure so we really try to minimize the flexibility of the proxy and really",
    "start": "653120",
    "end": "658160"
  },
  {
    "text": "tailor it for this use case for these reasons and we've had this um audited a few",
    "start": "658160",
    "end": "664560"
  },
  {
    "text": "times now we basically do security audits once a year and things have been going really well as far as that we've had a few",
    "start": "664560",
    "end": "670480"
  },
  {
    "text": "very minor issues but nothing nothing scary um and this is all built heavily on the",
    "start": "670480",
    "end": "676640"
  },
  {
    "text": "rust networking ecosystem and if you haven't played with the rust stack of",
    "start": "676640",
    "end": "682000"
  },
  {
    "text": "tokyo hyper tonic tower uh these projects are really exciting and we've invested",
    "start": "682000",
    "end": "687920"
  },
  {
    "text": "heavily in them in fact a lot of the proxy code has been forked out of the proxy moved upstream into these projects",
    "start": "687920",
    "end": "694480"
  },
  {
    "text": "and so we're we're big fans of the open source ecosystem and and making things better there",
    "start": "694480",
    "end": "702079"
  },
  {
    "start": "700000",
    "end": "710000"
  },
  {
    "text": "okay so that's just setting the stage and i want to go into a little bit more detail about the actual proxies or link these",
    "start": "702079",
    "end": "708640"
  },
  {
    "text": "features in general so the single sharpest tool in our tool",
    "start": "708640",
    "end": "714160"
  },
  {
    "text": "belt is little load balancer and so every proxy implements what's called a peak yuma",
    "start": "714160",
    "end": "720880"
  },
  {
    "text": "load balancer yuma is a exponentially weighted moving average and we use the latency of individual",
    "start": "720880",
    "end": "727519"
  },
  {
    "text": "requests to inform the load balancer so there's no centralized load balancer state there's no single load bounce you go to",
    "start": "727519",
    "end": "733680"
  },
  {
    "text": "every s every pod has its own little load balancer embedded in that proxy that's making its own",
    "start": "733680",
    "end": "739279"
  },
  {
    "text": "latency decisions in its own routing decisions um this is all really tightly coupled to",
    "start": "739279",
    "end": "746079"
  },
  {
    "text": "kubernetes services so we don't again we don't add new primitives there we use services for service discovery which",
    "start": "746079",
    "end": "752959"
  },
  {
    "text": "means we benefit from things like service topology and i think i saw mate in the the list",
    "start": "752959",
    "end": "758079"
  },
  {
    "text": "here mate is one of our interns this summer and who implemented service topology support so service topologies",
    "start": "758079",
    "end": "764000"
  },
  {
    "text": "are a new kubernetes feature i think they've just been available since 118 or 119. and",
    "start": "764000",
    "end": "770720"
  },
  {
    "text": "it allows you to express kind of node affinity and things like that or you know failure zones and make",
    "start": "770720",
    "end": "777760"
  },
  {
    "text": "kubernetes services aware of that and now linkerity honors that stuff as well this means that we can bypass cube proxy",
    "start": "777760",
    "end": "783839"
  },
  {
    "text": "and again the big big goal here is that you can just add this proxy to your application",
    "start": "783839",
    "end": "789839"
  },
  {
    "text": "you don't have to reconfigure your application to work all that differently and it can take that it can benefit from",
    "start": "789839",
    "end": "795680"
  },
  {
    "text": "all this logic um here's a really good example i don't",
    "start": "795680",
    "end": "801839"
  },
  {
    "text": "spend too much time on here but just to show what a load balancing algorithm can actually do for reliability um if we use something",
    "start": "801839",
    "end": "809920"
  },
  {
    "text": "really naive like around robin balancer and we let's say we're this is actually a test",
    "start": "809920",
    "end": "815040"
  },
  {
    "text": "we ran where there's ten instances and one instance is slow it has a two second latency",
    "start": "815040",
    "end": "820480"
  },
  {
    "text": "everything else is quite fast you know about 100 milliseconds and what we see is that if we set a one",
    "start": "820480",
    "end": "826720"
  },
  {
    "text": "second timeout a round robin load balancer means we'd have about a 95",
    "start": "826720",
    "end": "832639"
  },
  {
    "text": "success rate and just by changing the load balancing out until he's loaded we can push that up over 99",
    "start": "832639",
    "end": "839600"
  },
  {
    "text": "and with a yuma load balancer which is latency aware we can get better than three nines and that in at",
    "start": "839600",
    "end": "845839"
  },
  {
    "text": "least my experience can be a real difference between being woken up and being able to sleep",
    "start": "845839",
    "end": "850959"
  },
  {
    "text": "through the night if your pager goes off and so i really do think that the the load balancing and",
    "start": "850959",
    "end": "856880"
  },
  {
    "text": "liquidity is maybe under touted but really important uh thing that it does",
    "start": "856880",
    "end": "863760"
  },
  {
    "start": "863000",
    "end": "962000"
  },
  {
    "text": "one of the the probably the most important thing that lincry does though is add mutual tls to all measure",
    "start": "864320",
    "end": "871199"
  },
  {
    "text": "connections so when we say that we mean if there's a proxy on two pods and those pops",
    "start": "871199",
    "end": "876800"
  },
  {
    "text": "communicate we'll establish mpls for that mutual tls were both both pods identified identify themselves",
    "start": "876800",
    "end": "884240"
  },
  {
    "text": "to each other and we have a secure connection there this is all bootstrap off of kubernetes",
    "start": "884240",
    "end": "889680"
  },
  {
    "text": "service accounts again we don't want to provide any new uh requirements in terms of api",
    "start": "889680",
    "end": "895600"
  },
  {
    "text": "service area so every pod generally has a service account we take that token and use that to",
    "start": "895600",
    "end": "901839"
  },
  {
    "text": "authenticate ourselves to the ca which gives us short-lived certificates that we rotate throughout the lifetime",
    "start": "901839",
    "end": "907120"
  },
  {
    "text": "of the pod the private key never leaves the memory of the pod and this really is a important um",
    "start": "907120",
    "end": "915120"
  },
  {
    "text": "important tool in a zero trust architecture you can use things like cert manager to",
    "start": "915120",
    "end": "920880"
  },
  {
    "text": "bootstrap this and help manage that ca and also if you already have tls and for your infrastructure",
    "start": "920880",
    "end": "927600"
  },
  {
    "text": "linkerdy will work with that transparently we won't add a new layer of tls to that necessarily but we",
    "start": "927600",
    "end": "935600"
  },
  {
    "text": "will let that pass through as tcp traffic and up until two nine so linker d",
    "start": "935600",
    "end": "941920"
  },
  {
    "text": "two eight for instance only supported mtls for http communication i was two nine we've added",
    "start": "941920",
    "end": "948000"
  },
  {
    "text": "support for both the load balancer and mutual tls for almost all tcp traffic there's some",
    "start": "948000",
    "end": "954000"
  },
  {
    "text": "classes of pcp traffic that doesn't apply to yet but we'll be fixing that in 210 and that's really exciting actually that's",
    "start": "954000",
    "end": "960240"
  },
  {
    "text": "the big thing we did in two nights here's my favorite linguity feature",
    "start": "960240",
    "end": "967440"
  },
  {
    "start": "962000",
    "end": "1057000"
  },
  {
    "text": "and i don't think this is any in any other service mesh you'll have to check with them i haven't",
    "start": "967440",
    "end": "972959"
  },
  {
    "text": "really done my research there but this is something we specially designed in delinquity and that we do protocol",
    "start": "972959",
    "end": "978959"
  },
  {
    "text": "upgrades between proxies and so this is a hand-drawn diagram i did earlier today",
    "start": "978959",
    "end": "984399"
  },
  {
    "text": "but if you have h21 communication and you're doing many requests at once let's say you're doing",
    "start": "984399",
    "end": "989680"
  },
  {
    "text": "a thousand concurrent requests from a pod to another set of pods that'll be a thousand",
    "start": "989680",
    "end": "995600"
  },
  {
    "text": "tcp connections uh one per you know request that's active and with",
    "start": "995600",
    "end": "1002240"
  },
  {
    "text": "hp2 multiplexing what we do is proxies only establish one connection between",
    "start": "1002240",
    "end": "1007680"
  },
  {
    "text": "each other at most that gets mtls once and then we multiplex all of those requests across that single connection and then",
    "start": "1007680",
    "end": "1014720"
  },
  {
    "text": "we you know de-multiplex it on the other end so that we don't have to change your application semantics in any way",
    "start": "1014720",
    "end": "1022560"
  },
  {
    "text": "testing i was doing just in the last few weeks shows that this really really really reduces the memory requirements for these proxies",
    "start": "1022560",
    "end": "1029120"
  },
  {
    "text": "uh connections can cost quite a lot in terms of buffering and things like that and so by only having one connection we",
    "start": "1029120",
    "end": "1035360"
  },
  {
    "text": "really bring the proxy's memory usage down and this again kind of speaks to why we've chosen to",
    "start": "1035360",
    "end": "1041760"
  },
  {
    "text": "implement our own proxy here as we can be really special about the types of smart things we do",
    "start": "1041760",
    "end": "1047839"
  },
  {
    "text": "within the scope of the proxy again no application changes just works",
    "start": "1047839",
    "end": "1053360"
  },
  {
    "text": "out of the box most people don't even know about this",
    "start": "1053360",
    "end": "1057679"
  },
  {
    "start": "1057000",
    "end": "1104000"
  },
  {
    "text": "another feature we implemented i think probably about a year ago now is traffic splitting and this is",
    "start": "1059200",
    "end": "1064960"
  },
  {
    "text": "basically stochastic weighted uh routing and um a popular tool flagger",
    "start": "1064960",
    "end": "1072000"
  },
  {
    "text": "can be used to drive this basically says if you have multiple services that you want to send",
    "start": "1072000",
    "end": "1077360"
  },
  {
    "text": "traffic to well you can wait between them and say i want twenty percent traffic here and eighty percent of the traffic there",
    "start": "1077360",
    "end": "1083360"
  },
  {
    "text": "and the link candy will on to that and i can show this off in a demo um again this was something that only",
    "start": "1083360",
    "end": "1088880"
  },
  {
    "text": "worked with http traffic until two nine and as of the two nine release we now support this for tcp as well",
    "start": "1088880",
    "end": "1094960"
  },
  {
    "text": "now it's at the connection level we can't be any smarter than that but it is um it's really a convenient tool",
    "start": "1094960",
    "end": "1101600"
  },
  {
    "text": "especially for multi-cluster routing um this the service mesh interface is a",
    "start": "1101600",
    "end": "1107679"
  },
  {
    "start": "1104000",
    "end": "1151000"
  },
  {
    "text": "project that was initially sponsored by uh folks at azure and we've been heavily involved in",
    "start": "1107679",
    "end": "1112799"
  },
  {
    "text": "it and most of the other service meshes have been heavily involved in it and it provides basically three core apis",
    "start": "1112799",
    "end": "1118799"
  },
  {
    "text": "one is this traffic split that i just showed another is a telemetry api to",
    "start": "1118799",
    "end": "1126480"
  },
  {
    "text": "smi metrics which is really just a uniform set of metrics that will work with any service mesh",
    "start": "1126480",
    "end": "1132080"
  },
  {
    "text": "and then there's a policy api as well and this is another crd that'll let you just",
    "start": "1132080",
    "end": "1137120"
  },
  {
    "text": "uh express policy linkery doesn't support the policy api yet um and we're working on revving that api",
    "start": "1137120",
    "end": "1143120"
  },
  {
    "text": "with those folks uh so that we can support it in a better way that's probably 211 so not in the next release but the",
    "start": "1143120",
    "end": "1149440"
  },
  {
    "text": "following in two seven i think and maybe two eight",
    "start": "1149440",
    "end": "1155600"
  },
  {
    "start": "1151000",
    "end": "1218000"
  },
  {
    "text": "in the last release we started adding a multi-cluster capabilities and this lets you basically",
    "start": "1155600",
    "end": "1163200"
  },
  {
    "text": "sync services across clusters and route them through a gateway uh or an ingress on that",
    "start": "1163200",
    "end": "1168960"
  },
  {
    "text": "uh other cluster this is really cool and it works really well with traffic split so",
    "start": "1168960",
    "end": "1174240"
  },
  {
    "text": "the application doesn't have to know about where a service is hosted you just say i want to talk to the foo service",
    "start": "1174240",
    "end": "1179360"
  },
  {
    "text": "and if the proof service is actually an east cluster you can make that all totally transparent to the app and you can even kind of do weighted",
    "start": "1179360",
    "end": "1186160"
  },
  {
    "text": "shifting and things like that this is great because there's no single point of failure here we're not going through any kind of centralized load",
    "start": "1186160",
    "end": "1192320"
  },
  {
    "text": "balancer there's no special network requirements you don't have to have a flat network where everything's addressable",
    "start": "1192320",
    "end": "1197679"
  },
  {
    "text": "um so we try to maximize for flexibility and this is really the first link of the",
    "start": "1197679",
    "end": "1203120"
  },
  {
    "text": "extension where most of the core linquity experience doesn't know anything about multi-clusters it's kind of a pure add-on which leverages",
    "start": "1203120",
    "end": "1210240"
  },
  {
    "text": "the kubernetes api and a little bit of linkery service discovery implementation to make this really cool",
    "start": "1210240",
    "end": "1220000"
  },
  {
    "start": "1218000",
    "end": "1307000"
  },
  {
    "text": "another reason that we focused on implementing our own proxy was uh that we really wanted to have",
    "start": "1220000",
    "end": "1226400"
  },
  {
    "text": "an excellent prometheus experience and i think it's gotten better over the last year or so but for a long time on voice",
    "start": "1226400",
    "end": "1232720"
  },
  {
    "text": "from atheist instrumentation was really is difficult um envoy started with a statsd push-based model and we've",
    "start": "1232720",
    "end": "1240640"
  },
  {
    "text": "taken another approach which is to use a lot of the kubernetes metadata so the deployment you're",
    "start": "1240640",
    "end": "1246320"
  },
  {
    "text": "talking to the surface account but that's part of the um",
    "start": "1246320",
    "end": "1251360"
  },
  {
    "text": "the pod name all sorts of labels for that pod all of those things get hydrated onto the uh linkedin metrics so we can",
    "start": "1251360",
    "end": "1258559"
  },
  {
    "text": "give you really you know rich dashboards and create queryability",
    "start": "1258559",
    "end": "1263840"
  },
  {
    "text": "this also we've extended to work with open api or swagger specs as well as",
    "start": "1263840",
    "end": "1268880"
  },
  {
    "text": "your pc so you can take those route definitions and import them into linkedin and have all that information show up in your",
    "start": "1268880",
    "end": "1275440"
  },
  {
    "text": "linkedin metrics as well again there's no configuration necessary those swagger and protobuf",
    "start": "1275440",
    "end": "1282080"
  },
  {
    "text": "enhancements do take some configuration but we just try to make this work out of the box you don't have to do any",
    "start": "1282080",
    "end": "1288320"
  },
  {
    "text": "configuration we ship from prometheus by default and you can get run up and running",
    "start": "1288320",
    "end": "1294320"
  },
  {
    "text": "in two nine you're also able to bring your own prometheus so if you already have a prometheus in your cluster you don't have to use linkrd's as well",
    "start": "1294320",
    "end": "1300880"
  },
  {
    "text": "you can configure link or do to talk to the main prometheus and since they all work great",
    "start": "1300880",
    "end": "1307600"
  },
  {
    "start": "1307000",
    "end": "1340000"
  },
  {
    "text": "we get a lot of folks asking for distributed tracing with open census or wind telemetry",
    "start": "1308720",
    "end": "1313919"
  },
  {
    "text": "and linguity can work in that world as well but the dirty secret about distributed",
    "start": "1313919",
    "end": "1319120"
  },
  {
    "text": "tracing is that you actually have to change your application integrate with this there's no way that we can do this totally transparent to the application",
    "start": "1319120",
    "end": "1325679"
  },
  {
    "text": "so if you have tracing set up in your ecosystem linguity will emit spans and tell you",
    "start": "1325679",
    "end": "1331120"
  },
  {
    "text": "you know where you're hitting hops in linker d um in the service mesh but this isn't something that'll just",
    "start": "1331120",
    "end": "1336480"
  },
  {
    "text": "work out of the box but we have a good integration there as well what will work out of the box is uh",
    "start": "1336480",
    "end": "1342799"
  },
  {
    "start": "1340000",
    "end": "1385000"
  },
  {
    "text": "linkery's tap functionality and this is what i call ad hoc tracing where you're able to",
    "start": "1342799",
    "end": "1348640"
  },
  {
    "text": "at runtime without prior configuration query proxies directly and say show me",
    "start": "1348640",
    "end": "1354240"
  },
  {
    "text": "metadata about the traffic that's going through you so this isn't aggregated in the metrics form where you already know what you're looking for",
    "start": "1354240",
    "end": "1360000"
  },
  {
    "text": "this can be done kind of in the discovery or exploratory mode um that does mean that your your headers",
    "start": "1360000",
    "end": "1366880"
  },
  {
    "text": "and all sorts of private information can be exposed by this api and so this is all locked down with our back you can",
    "start": "1366880",
    "end": "1372640"
  },
  {
    "text": "set role bindings to make this much more locked down if you need to or you can turn it off entirely and",
    "start": "1372640",
    "end": "1378559"
  },
  {
    "text": "factor in 210 will be making tap and optional components so you don't even need to run it by default",
    "start": "1378559",
    "end": "1384960"
  },
  {
    "start": "1385000",
    "end": "1639000"
  },
  {
    "text": "okay then there's a few more things in q9 that i haven't even talked about yet uh one of the bigger ones was a summer",
    "start": "1385679",
    "end": "1391840"
  },
  {
    "text": "of code project that added multicharge builds so now you can use linkedin on your raspberry pi clusters or your",
    "start": "1391840",
    "end": "1398080"
  },
  {
    "text": "gravitron clusters if you're in aws so with arm taking on a lot more",
    "start": "1398080",
    "end": "1403360"
  },
  {
    "text": "importance in the world and being much more power efficient what i can tell uh it's really exciting that linguity can",
    "start": "1403360",
    "end": "1409840"
  },
  {
    "text": "work in those environments out of the box today and so no configuration changes you just if you have a mixed cluster with some",
    "start": "1409840",
    "end": "1416320"
  },
  {
    "text": "arm nodes and some uh amd nodes they'll just pull the right images and",
    "start": "1416320",
    "end": "1422480"
  },
  {
    "text": "it works transparently as i mentioned mckay added service topology support this also means that we",
    "start": "1422480",
    "end": "1427840"
  },
  {
    "text": "now support the new kubernetes endpoint slices api the endpoint slices api is designed to",
    "start": "1427840",
    "end": "1432880"
  },
  {
    "text": "allow your clusters to have much larger services the kubernetes api itself could get very slower on large",
    "start": "1432880",
    "end": "1438799"
  },
  {
    "text": "services and large deployments and so we support those new apis that allow those things to be more efficient",
    "start": "1438799",
    "end": "1445120"
  },
  {
    "text": "as i mentioned just a second ago uh we allow you to bring your own prometheus and grafana so if you already have those things no reason",
    "start": "1445120",
    "end": "1451600"
  },
  {
    "text": "to install a second copy of them and in 210 we'll be making a lot of these components very optional",
    "start": "1451600",
    "end": "1456960"
  },
  {
    "text": "where they get installed separately and then where i spend most of my time",
    "start": "1456960",
    "end": "1462000"
  },
  {
    "text": "is molinkini proxy we had a ton of changes between two eight and two nine um we basically rewrote the entire services",
    "start": "1462000",
    "end": "1469600"
  },
  {
    "text": "discovery mechanism so now instead of looking at post headers and headers of the requests we defined we do",
    "start": "1469600",
    "end": "1476000"
  },
  {
    "text": "all discovery based on the ips we're talking to um so if it's an ip we know it's a service ip and we can do load balancing",
    "start": "1476000",
    "end": "1481520"
  },
  {
    "text": "based on that or if you're talking directly to a pod uh we'll just forward it to the pod without doing load balancing",
    "start": "1481520",
    "end": "1487200"
  },
  {
    "text": "and this means that we can work well with ingresses so if you're using envoys and ingress for instance",
    "start": "1487200",
    "end": "1492720"
  },
  {
    "text": "i'll we'll let you configure session stickiness or any of those things in envoy and link that you will honor without interfering",
    "start": "1492720",
    "end": "1500559"
  },
  {
    "text": "we've done all this work basically to support mtls for tcp communication so most tcp protocols will now be secure",
    "start": "1500559",
    "end": "1508799"
  },
  {
    "text": "and load balanced and routed with via traffic split out of the box the protocols that doesn't match are server first protocols so things like my",
    "start": "1508799",
    "end": "1515520"
  },
  {
    "text": "sql or smtp where the where the server speaks first to the proxy um or to the client and that's because",
    "start": "1515520",
    "end": "1523440"
  },
  {
    "text": "we do protocol detection there's no configuration needed to tell us what protocol you're speaking for the most part so we look at the first few bytes",
    "start": "1523440",
    "end": "1529840"
  },
  {
    "text": "of the connection we say oh this is hdp-1 this is hb2 and we forward it and 210 will support uh mtls for those",
    "start": "1529840",
    "end": "1539039"
  },
  {
    "text": "protocols as well and we'll also support it uh and also support multi-cluster for tcp",
    "start": "1539039",
    "end": "1544880"
  },
  {
    "text": "connections um there's a we as we've been getting",
    "start": "1544880",
    "end": "1549919"
  },
  {
    "text": "more and more production users we had some people doing failure testing and realizing that it could take linguity a while to reconnect after a",
    "start": "1549919",
    "end": "1556880"
  },
  {
    "text": "node outage and things like that and this was actually turns out because of coup proxy because coup proxy itself",
    "start": "1556880",
    "end": "1562720"
  },
  {
    "text": "which is the eip tables based service discovery and load balancing scheme which",
    "start": "1562720",
    "end": "1567760"
  },
  {
    "text": "kubernetes ships with by default can actually be quite slow to update in some environments and so now we bypass that entirely when",
    "start": "1567760",
    "end": "1574559"
  },
  {
    "text": "we talk to the control plane we load balance requests over all of the the pods in the control",
    "start": "1574559",
    "end": "1579600"
  },
  {
    "text": "plane and it's much more resilient to those types of failures",
    "start": "1579600",
    "end": "1584720"
  },
  {
    "text": "we also adopted a new runtime in the proxy so previously the proxy would only use at",
    "start": "1584720",
    "end": "1590240"
  },
  {
    "text": "most one core we only ran one thread for the proxy and this is great for most",
    "start": "1590240",
    "end": "1595919"
  },
  {
    "text": "applications where you just want linkedin to be small and get out of the way but some folks want to push",
    "start": "1595919",
    "end": "1600960"
  },
  {
    "text": "50 000 requests a second through a pod and that's going to be really hard to do on one thread and so now we support scaling this up it",
    "start": "1600960",
    "end": "1607760"
  },
  {
    "text": "on there's a an annotation or if you set cpu limits via a",
    "start": "1607760",
    "end": "1614400"
  },
  {
    "text": "a mutating web book or something like that the proxy will just pick up those settings and limit itself to uh the correct number of",
    "start": "1615760",
    "end": "1622400"
  },
  {
    "text": "cpus that you can figure and all this work has enabled us to reduce latency reduce",
    "start": "1622400",
    "end": "1627760"
  },
  {
    "text": "the cpu usage and reduce the memory usage of the proxies which is really one of the big",
    "start": "1627760",
    "end": "1633200"
  },
  {
    "text": "motivators for us is to have the lightest footprint we can especially in the proxy",
    "start": "1633200",
    "end": "1640159"
  },
  {
    "start": "1639000",
    "end": "1645000"
  },
  {
    "text": "okay i finally got to the fun time i have enough time to get through it i think",
    "start": "1640159",
    "end": "1645919"
  },
  {
    "start": "1645000",
    "end": "1705000"
  },
  {
    "text": "so um i instead of giving you like a flashy micro service demo where like we've done",
    "start": "1645919",
    "end": "1651919"
  },
  {
    "text": "emoji photo in the past and i think in the 2 8 webinar i did we did multi-cluster and things like that",
    "start": "1651919",
    "end": "1657360"
  },
  {
    "text": "i'm actually just going to show you the dev environment that i use for testing a lot of the proxy changes",
    "start": "1657360",
    "end": "1662880"
  },
  {
    "text": "and lengthy changes in general i'm a big fan of k3d it's much like kind",
    "start": "1662880",
    "end": "1670240"
  },
  {
    "text": "in that you can just run it in docker on your host no build kubernetes setup i'm going to be",
    "start": "1670240",
    "end": "1676320"
  },
  {
    "text": "using the latest lingerie stable release which came out on monday and i'm going to be using this",
    "start": "1676320",
    "end": "1682480"
  },
  {
    "text": "uh load test tool i wrote called uh ort might change its name but uh it's",
    "start": "1682480",
    "end": "1688880"
  },
  {
    "text": "really just a a client which generates lots of requests those can be grpc issue one or tcp and we have a",
    "start": "1688880",
    "end": "1695840"
  },
  {
    "text": "server that supports those this is what we use for testing or reproducing bugs or things like that",
    "start": "1695840",
    "end": "1701360"
  },
  {
    "text": "and so without further ado let's see if i can show you what's going on and if",
    "start": "1701360",
    "end": "1707039"
  },
  {
    "start": "1705000",
    "end": "1750000"
  },
  {
    "text": "you can't if if this responds too small just shut it and then chat and i'll try",
    "start": "1707039",
    "end": "1713600"
  },
  {
    "text": "to fix it so right instead of showing you how to set up a k3d cluster i've already set",
    "start": "1713600",
    "end": "1719520"
  },
  {
    "text": "that up and i've already deployed the app to it um this is you know we we have i think",
    "start": "1719520",
    "end": "1725679"
  },
  {
    "text": "about three like nine twelve pods running something like that in the app",
    "start": "1725679",
    "end": "1731679"
  },
  {
    "text": "key 3d itself takes about uh 500 megs or so in this environment and you know i've",
    "start": "1731679",
    "end": "1739120"
  },
  {
    "text": "been running these load tests no lingerie installed yet so this is just the load generator",
    "start": "1739120",
    "end": "1744399"
  },
  {
    "text": "running on a host under my desk in my office which i have not been to a long time",
    "start": "1744399",
    "end": "1750720"
  },
  {
    "start": "1750000",
    "end": "1839000"
  },
  {
    "text": "um so let's install linkerd and uh i'll just take you through that experience really quickly so before we",
    "start": "1750720",
    "end": "1757279"
  },
  {
    "text": "actually install link we're gonna check and make sure the cluster is ready um",
    "start": "1757279",
    "end": "1762320"
  },
  {
    "text": "you know if i had to kind of if i installed link it previously and not uninstalled properly or if there's a",
    "start": "1762320",
    "end": "1768720"
  },
  {
    "text": "clock skew we do a bunch of checks just to make sure link can be installed",
    "start": "1768720",
    "end": "1774720"
  },
  {
    "text": "great that worked and um so now just install it linkedin will",
    "start": "1776880",
    "end": "1783679"
  },
  {
    "text": "just generate a lot of yaml um",
    "start": "1783679",
    "end": "1788960"
  },
  {
    "text": "and this will take a few seconds um",
    "start": "1788960",
    "end": "1796799"
  },
  {
    "text": "we see that linker d is one two three four five six seven eight nine ten ten components",
    "start": "1797679",
    "end": "1804000"
  },
  {
    "text": "right now um all of them pretty small you can run these in aha mode where they run multiple replicas of each and as i",
    "start": "1804000",
    "end": "1810880"
  },
  {
    "text": "mentioned we now support turning off prometheus and grafana we'll make this kind of slimmer in the next release even",
    "start": "1810880",
    "end": "1816320"
  },
  {
    "text": "so that the web component is optional we'll give this a second to start up uh",
    "start": "1816320",
    "end": "1821919"
  },
  {
    "text": "but that won't actually um do very much for us we've installed linkedin but the the",
    "start": "1821919",
    "end": "1828799"
  },
  {
    "text": "application isn't running on grid yet and in fact we see that here where um there's only one",
    "start": "1828799",
    "end": "1834960"
  },
  {
    "text": "container in each of those pods in order to validate the lingerie's",
    "start": "1834960",
    "end": "1840880"
  },
  {
    "start": "1839000",
    "end": "1919000"
  },
  {
    "text": "coming up correctly we can use the lengthy check command and what lengthy check does is it again runs through a whole bunch of common",
    "start": "1840880",
    "end": "1846640"
  },
  {
    "text": "problems that can happen during an install and validate so they they aren't there",
    "start": "1846640",
    "end": "1852080"
  },
  {
    "text": "one of the biggest things we have to make sure all the pods come up so we're going to wait for all the pods to initialize this will take",
    "start": "1852080",
    "end": "1857360"
  },
  {
    "text": "hopefully not too long a couple seconds",
    "start": "1857360",
    "end": "1865840"
  },
  {
    "text": "much more impatient about this when i have people watching okay it looks like we're almost there",
    "start": "1867679",
    "end": "1879840"
  },
  {
    "text": "so i don't know if you notice but before we installed linkery the this host was",
    "start": "1889120",
    "end": "1896399"
  },
  {
    "text": "about two gigs usage and that was mostly uh k3d and docker we'll see now installing",
    "start": "1896399",
    "end": "1904000"
  },
  {
    "text": "linker d the largest part of linguity memory wise is the prometheus instance which is",
    "start": "1904000",
    "end": "1909600"
  },
  {
    "text": "going to run us around 100 megs or so in the stiff environment",
    "start": "1909600",
    "end": "1914720"
  },
  {
    "text": "and then if we can also look at the proxies here we see the proxies all",
    "start": "1914880",
    "end": "1921120"
  },
  {
    "start": "1919000",
    "end": "2001000"
  },
  {
    "text": "running all of them you know most of them well under 20 megs",
    "start": "1921120",
    "end": "1926159"
  },
  {
    "text": "uh the permeate the one that's running with the prometheus which talks to all of the pods in the cluster is a little bit higher there running",
    "start": "1926159",
    "end": "1932080"
  },
  {
    "text": "around 20 megs okay so i've got that all set up and now",
    "start": "1932080",
    "end": "1937360"
  },
  {
    "text": "i'm gonna upgrade my test so don't worry too much about this this is",
    "start": "1937360",
    "end": "1943679"
  },
  {
    "text": "a helm chart i use for configuring different topologies to test um let's see we're going to run at most",
    "start": "1943679",
    "end": "1951039"
  },
  {
    "text": "three requests at once per load generator",
    "start": "1951039",
    "end": "1956559"
  },
  {
    "text": "we're going to inject linguity and so this injector linker sitting there what it really does is",
    "start": "1956880",
    "end": "1962159"
  },
  {
    "text": "just add a single annotation to the the pods or to the manifest",
    "start": "1962159",
    "end": "1968240"
  },
  {
    "text": "so that the linguity controller can know to inject these things so by default we won't add link created things you can",
    "start": "1968399",
    "end": "1973600"
  },
  {
    "text": "either annotate name spaces or the workloads themselves to get things running and so that",
    "start": "1973600",
    "end": "1980399"
  },
  {
    "text": "will we're going to roll our test environment",
    "start": "1980399",
    "end": "1986320"
  },
  {
    "text": "there so we see those pods restarting and coming up now is two containers",
    "start": "1986320",
    "end": "1993120"
  },
  {
    "text": "let's see if this works",
    "start": "1994320",
    "end": "1997518"
  },
  {
    "start": "2001000",
    "end": "2284000"
  },
  {
    "text": "there's this check proxy command which will uh again kind of look at the health",
    "start": "2002159",
    "end": "2008640"
  },
  {
    "text": "and make sure the proxies have all started they have service accounts in those pods et cetera so they can actually run",
    "start": "2008640",
    "end": "2014880"
  },
  {
    "text": "and now we actually have linkedin running with this environment so let me",
    "start": "2014880",
    "end": "2022000"
  },
  {
    "text": "open a dashboard this is really what we get out of the",
    "start": "2022000",
    "end": "2028480"
  },
  {
    "text": "box of linkedin so we immediately have a whole bunch of metrics about",
    "start": "2028480",
    "end": "2033760"
  },
  {
    "text": "uh what's going on in the cluster so we can take a high level look at the name spaces we",
    "start": "2033760",
    "end": "2039919"
  },
  {
    "text": "see that the linker do name space and the oort name space are the only ones injected doing around 2.5 000 requests a second",
    "start": "2039919",
    "end": "2048240"
  },
  {
    "text": "in there which makes some sense",
    "start": "2048240",
    "end": "2052158"
  },
  {
    "text": "and the other thing i've done is i've installed a traffic split here which we'll see here and so uh now the load generator is talking to",
    "start": "2053839",
    "end": "2060960"
  },
  {
    "text": "a single service called server and that's being split equally across three separate services service",
    "start": "2060960",
    "end": "2066638"
  },
  {
    "text": "server one zero one and two and they're all doing about the you know",
    "start": "2066639",
    "end": "2071919"
  },
  {
    "text": "they're all the same way and they're all doing about the same amount of traffic",
    "start": "2071919",
    "end": "2078800"
  },
  {
    "text": "i've handcrafted some prometheus queries here to show the tcp load generator we haven't wired",
    "start": "2079520",
    "end": "2085679"
  },
  {
    "text": "through many of the tcp oriented metrics into the dashboard yet that'd be a great place if folks want to",
    "start": "2085679",
    "end": "2092320"
  },
  {
    "text": "help us there and in the 210 release but we see you know about the same amount of traffic being",
    "start": "2092320",
    "end": "2098800"
  },
  {
    "text": "spread equally across all of those connections and now what we can do is we can actually",
    "start": "2098800",
    "end": "2106000"
  },
  {
    "text": "modify the traffic uh just by adding this resource so i'm going to open this",
    "start": "2106000",
    "end": "2111119"
  },
  {
    "text": "traffic split resource and i can put say",
    "start": "2111119",
    "end": "2117839"
  },
  {
    "text": "do eighty percent to server row two and we'll do the remaining ten percentage to the other ones",
    "start": "2118960",
    "end": "2130320"
  },
  {
    "text": "this will take a couple seconds especially for prometheus to do some scrapes so you know prometheus scrapes every 10",
    "start": "2130320",
    "end": "2135839"
  },
  {
    "text": "seconds the actual change will be instantaneous but it'll take us a little a minute to see this in the",
    "start": "2135839",
    "end": "2142839"
  },
  {
    "text": "dashboard oh one of the other interesting things that we see here is",
    "start": "2142839",
    "end": "2148240"
  },
  {
    "text": "that um we get a topology out of the box right and so we haven't had to configure",
    "start": "2148240",
    "end": "2153920"
  },
  {
    "text": "anything we're not using tracing but just from the metrics being involved here we can actually get",
    "start": "2153920",
    "end": "2159119"
  },
  {
    "text": "a kind of call graph for your system out of the box if we look at for",
    "start": "2159119",
    "end": "2166160"
  },
  {
    "text": "instance one of the server deployments we can see all the deployments that talk to it as i mentioned we also have a tcp load",
    "start": "2166160",
    "end": "2172400"
  },
  {
    "text": "generator that's talking to it but it's not showing up because we need some enhanced metrics there or",
    "start": "2172400",
    "end": "2177839"
  },
  {
    "text": "we need some ui work to show those metrics",
    "start": "2177839",
    "end": "2181519"
  },
  {
    "text": "and as we look at for instance the the grpc load generator what we see are that we are actually doing um",
    "start": "2184880",
    "end": "2192160"
  },
  {
    "text": "the dashboard is doing tap and so it's actually looking and counting live requests here and this shows us that like you know in",
    "start": "2192160",
    "end": "2198320"
  },
  {
    "text": "the last 10 20 seconds that we've been looking at this we've been doing you know a lot more requests to the the heavily",
    "start": "2198320",
    "end": "2205440"
  },
  {
    "text": "weighted server and very much fewer to the others we also get that from our metrics here",
    "start": "2205440",
    "end": "2215839"
  },
  {
    "text": "and if we want to go even further we can take all the load off of one of them",
    "start": "2218720",
    "end": "2225119"
  },
  {
    "text": "and in a few seconds we'll see i think server one just drop off the map there",
    "start": "2225119",
    "end": "2231440"
  },
  {
    "text": "as i mentioned we shipped a grephon instance by default and so we can actually just explore some of these things directly from here",
    "start": "2238079",
    "end": "2245359"
  },
  {
    "text": "yeah and we see just in the this request top level request rate graph that this is started at equal traffic",
    "start": "2249359",
    "end": "2256560"
  },
  {
    "text": "reduced to about ten percent of the traffic and now it's at zero percent of the traffic",
    "start": "2256560",
    "end": "2262480"
  },
  {
    "text": "two you see the opposite",
    "start": "2262480",
    "end": "2271839"
  },
  {
    "text": "all right that's enough fiddling around with lingerie i think",
    "start": "2272000",
    "end": "2285838"
  },
  {
    "start": "2284000",
    "end": "2577000"
  },
  {
    "text": "so looking forward uh this is a lot on the roadmap and the community is working on a bunch of different things",
    "start": "2286560",
    "end": "2292560"
  },
  {
    "text": "not all of it uh at point um the one of the bigger features we're",
    "start": "2292560",
    "end": "2298880"
  },
  {
    "text": "working on for 210 is really focusing on minimizing the core control plane so in order to get",
    "start": "2298880",
    "end": "2304640"
  },
  {
    "text": "tls service discovery and load balancing and proxy configuration which are the really",
    "start": "2304640",
    "end": "2309839"
  },
  {
    "text": "what you need to run the surface mesh we want to minimize stolen premium space too so only those core concerns",
    "start": "2309839",
    "end": "2315839"
  },
  {
    "text": "and then things like prometheus and grafana and tap are going to become an extension that you",
    "start": "2315839",
    "end": "2320960"
  },
  {
    "text": "can add on delinquity with a single command again but are not part of that kind of core control plane um this",
    "start": "2320960",
    "end": "2327520"
  },
  {
    "text": "again should hopefully make it easier to upgrade the viz components without having to upgrade the core components",
    "start": "2327520",
    "end": "2332800"
  },
  {
    "text": "and ultimately we want that core set of functionality to change much less frequently to become boring is",
    "start": "2332800",
    "end": "2339280"
  },
  {
    "text": "the goal uh today we don't do we multi-cluster",
    "start": "2339280",
    "end": "2344880"
  },
  {
    "text": "only supports http traffic and so the big thing we're gonna do for um",
    "start": "2344880",
    "end": "2351599"
  },
  {
    "text": "for that in for multi-cluster in 210 is start supporting all tcp traffic including",
    "start": "2351599",
    "end": "2358000"
  },
  {
    "text": "server first protocols and that means we'll start doing mtls for those protocols as well that will require some configuration to",
    "start": "2358000",
    "end": "2364079"
  },
  {
    "text": "enable but uh it's not it's like an annotation it's not that heavy",
    "start": "2364079",
    "end": "2370320"
  },
  {
    "text": "as a as we kind of saw in this demo some of the tcp metrics uh the door tcp metrics haven't been wired",
    "start": "2370320",
    "end": "2377119"
  },
  {
    "text": "through into the uis um i guess i missed one of the better guys to show where t-symmetrics are used but we want to validate",
    "start": "2377119",
    "end": "2385599"
  },
  {
    "text": "that",
    "start": "2385599",
    "end": "2387839"
  },
  {
    "text": "we want to validate that tls is actually working we have this edges command which lists all of the you know in this case",
    "start": "2391359",
    "end": "2398000"
  },
  {
    "text": "deployees that are talking to each other we can also do this by pod you get a lot more there and then the cases where",
    "start": "2398000",
    "end": "2404480"
  },
  {
    "text": "this is from probably when we were doing the restart we didn't have service discovery on some of those connections so",
    "start": "2404480",
    "end": "2410480"
  },
  {
    "text": "we didn't have identities for them so they weren't provided but we want to keep enhancing a lot of",
    "start": "2410480",
    "end": "2416400"
  },
  {
    "text": "this functionality with uh with the new tcp data that we have there",
    "start": "2416400",
    "end": "2421680"
  },
  {
    "text": "there's a new feature a newer feature in kubernetes that's still not widely supported called bounded service accounts and so",
    "start": "2421680",
    "end": "2428160"
  },
  {
    "text": "today every pod has a single service account that it uses for everything it's going to use whether it's",
    "start": "2428160",
    "end": "2433920"
  },
  {
    "text": "you know modifying kubernetes resources or doing identity provisioning",
    "start": "2433920",
    "end": "2438960"
  },
  {
    "text": "and there's a new beta feature that allows us to bound service account tokens to different uses",
    "start": "2438960",
    "end": "2444400"
  },
  {
    "text": "so what we'd really like to do is have short-lived service account tokens that are only",
    "start": "2444400",
    "end": "2449599"
  },
  {
    "text": "used for provisioning identity so they can't be confused with other things that you might you know if you give a service account token to something else you can't",
    "start": "2449599",
    "end": "2456560"
  },
  {
    "text": "use that to get linkedin identity traffic policy as i mentioned is a big concern",
    "start": "2456560",
    "end": "2461920"
  },
  {
    "text": "that we have we've been being pretty incremental about that the goal is to first get everything to",
    "start": "2461920",
    "end": "2468079"
  },
  {
    "text": "be tlsed get that to be multi-cluster and then once we have these kind of core connectivity concerns we can start to",
    "start": "2468079",
    "end": "2474480"
  },
  {
    "text": "think about policy in a more serious way um that's a big part of what i think differentiates linkedin from other",
    "start": "2474480",
    "end": "2479760"
  },
  {
    "text": "service meshes especially istio is we haven't started with all the features and tried to productionize them all",
    "start": "2479760",
    "end": "2485200"
  },
  {
    "text": "slowly we've really focused on being production ready and incremental in our approach there so that's what",
    "start": "2485200",
    "end": "2490560"
  },
  {
    "text": "we're going to do for traffic policy as well there's a group of folks outside of buoyant that are very eager to implement",
    "start": "2490560",
    "end": "2497119"
  },
  {
    "text": "pips 140-2 for this basically means for government so if you want to use linguity",
    "start": "2497119",
    "end": "2503839"
  },
  {
    "text": "and government applications today our rts tls implementations are not validated by",
    "start": "2503839",
    "end": "2509680"
  },
  {
    "text": "the standard and so there's some folks working on allowing you to swap out the tls mutations for ones that are",
    "start": "2509680",
    "end": "2515760"
  },
  {
    "text": "validated i'm really excited about that as well and right now linguity only works within",
    "start": "2515760",
    "end": "2521599"
  },
  {
    "text": "kubernetes clusters you can of course talk to resources that aren't in the kubernetes clusters but those aren't",
    "start": "2521599",
    "end": "2526640"
  },
  {
    "text": "secured and managed in the same way and so we're really eager to uh allow you to add proxies to things",
    "start": "2526640",
    "end": "2533359"
  },
  {
    "text": "that are not in kubernetes so that we can support that use case as well and finally there's a new wasm web web",
    "start": "2533359",
    "end": "2540720"
  },
  {
    "text": "assembly which is neither webinar assembly um api called proxy wasm which is",
    "start": "2540720",
    "end": "2546079"
  },
  {
    "text": "supported by envoy and we started to experiment with what it would take to support that sort of thing in a proxy",
    "start": "2546079",
    "end": "2551520"
  },
  {
    "text": "i'm still a little you know conservative about adopting leading edge technologies like that of",
    "start": "2551520",
    "end": "2558400"
  },
  {
    "text": "course i think from a security point of view there may be some risks there but ultimately this means that you might",
    "start": "2558400",
    "end": "2563440"
  },
  {
    "text": "be able to write plugins in any number of languages uh javascript go rust and have them just",
    "start": "2563440",
    "end": "2569280"
  },
  {
    "text": "dropped from the proxy and work without having to change proxy code which is really promising but don't let that stop you from getting",
    "start": "2569280",
    "end": "2575040"
  },
  {
    "text": "involved in writing proxy code it's important to call out that not all",
    "start": "2575040",
    "end": "2581440"
  },
  {
    "start": "2577000",
    "end": "2619000"
  },
  {
    "text": "contributions to linkedin have to be code contributions we have a community anchor program where we've just started over the past couple",
    "start": "2581440",
    "end": "2587359"
  },
  {
    "text": "months where we really want to get folks in the community who are getting value at linkedin solving your problems with it",
    "start": "2587359",
    "end": "2593680"
  },
  {
    "text": "we want you to be talking about that we want you to be uh you know on the stage at kubecon with",
    "start": "2593680",
    "end": "2599520"
  },
  {
    "text": "telling your story and um and also telling telling the good and the bad of what it's like to use liquidity",
    "start": "2599520",
    "end": "2606560"
  },
  {
    "text": "and so if that's something that's interesting to you uh go to that url and be happy to help you set up with uh",
    "start": "2606560",
    "end": "2613440"
  },
  {
    "text": "both the material and the opportunity to go through those talks and in those blog posts",
    "start": "2613440",
    "end": "2620880"
  },
  {
    "start": "2619000",
    "end": "2687000"
  },
  {
    "text": "and finally it's it's a community project right i've said this many times today um and it's the real value of linkedin",
    "start": "2620880",
    "end": "2628240"
  },
  {
    "text": "is that we've got a great set of folks who are always adding new things to it who are helping us find bugs before we",
    "start": "2628240",
    "end": "2635760"
  },
  {
    "text": "get to stable releases and so you know folks testing edge releases we do edge releases basically",
    "start": "2635760",
    "end": "2641359"
  },
  {
    "text": "every week and stable releases about every two months or so this last one is a little bit longer",
    "start": "2641359",
    "end": "2646560"
  },
  {
    "text": "but we'd love for you to get involved on github there's a help wanted tag where it's great to just you know chime in that you",
    "start": "2646560",
    "end": "2652480"
  },
  {
    "text": "want to help us and and start working um we love code contributions i also love it when people help people",
    "start": "2652480",
    "end": "2658880"
  },
  {
    "text": "debug each other debug things on slack we have a lot of new people coming in and asking questions and it's great",
    "start": "2658880",
    "end": "2664000"
  },
  {
    "text": "to have people who've already been there to answer those questions so it's not just a few of us all the time we've got",
    "start": "2664000",
    "end": "2671119"
  },
  {
    "text": "mailing lists we've got monthly community calls we get security audits we're we love the scenes",
    "start": "2671119",
    "end": "2676720"
  },
  {
    "text": "except ecosystem and what it affords us so with that um i would love to hear what questions you",
    "start": "2676720",
    "end": "2683520"
  },
  {
    "text": "have if any yep let's go ahead and drop all the q",
    "start": "2683520",
    "end": "2689200"
  },
  {
    "start": "2687000",
    "end": "3147000"
  },
  {
    "text": "and a's into the q a box and um oliver you can take",
    "start": "2689200",
    "end": "2694240"
  },
  {
    "text": "it away and we have about 10 minutes left so we'll get as many done as we have time for",
    "start": "2694240",
    "end": "2700160"
  },
  {
    "text": "okay is my q a box",
    "start": "2700160",
    "end": "2705838"
  },
  {
    "text": "nope i got it just catching my breath um so the first question from sanjay is",
    "start": "2714000",
    "end": "2720160"
  },
  {
    "text": "what's the alternate to strict and permissive mtls mode and likely from paradistia and so i kind of answered this earlier",
    "start": "2720160",
    "end": "2726880"
  },
  {
    "text": "but we don't have a strict mode today um there's a number of gotchas",
    "start": "2726880",
    "end": "2732800"
  },
  {
    "text": "in the way of that and so we're working out in policy i want to get the kind of first pieces of that into the 211 release but today uh",
    "start": "2732800",
    "end": "2741280"
  },
  {
    "text": "we're opportunistic about adding tls and so we're kind of in the permissive mode",
    "start": "2741280",
    "end": "2746960"
  },
  {
    "text": "and we provide auditing tools to let you debug and alert on that yourself um the next question is",
    "start": "2746960",
    "end": "2755119"
  },
  {
    "text": "uh with the t9 release has linked you been optimized for smaller footprint arm devices like raspberry pi",
    "start": "2755119",
    "end": "2760800"
  },
  {
    "text": "it's a great question uh printing um i wouldn't say it's been totally uh optimized we haven't spent a lot of",
    "start": "2760800",
    "end": "2769119"
  },
  {
    "text": "time in the arm environment yet it's kind of experimental at this point but our focus in general has been really",
    "start": "2769119",
    "end": "2774720"
  },
  {
    "text": "lightweight and so you know we're focusing on minimizing the control plane so you have a very",
    "start": "2774720",
    "end": "2779760"
  },
  {
    "text": "lightweight stall footprint there uh and also the proxies themselves you know are quite small if we go back to this",
    "start": "2779760",
    "end": "2786079"
  },
  {
    "text": "environment which is running our load tests i we started around two gigs and",
    "start": "2786079",
    "end": "2791680"
  },
  {
    "text": "we're still under three gigs and most of that is prometheus right all of",
    "start": "2791680",
    "end": "2797200"
  },
  {
    "text": "these the largest proxies prometheus proxy at 28 megs everything else is 20",
    "start": "2797200",
    "end": "2802319"
  },
  {
    "text": "let megs or lower which is really really lightweight especially if you compare to some of the other service meshes",
    "start": "2802319",
    "end": "2809440"
  },
  {
    "text": "so i think we'll be better on raspberry pi than some of the others are but there's probably more work to do",
    "start": "2809440",
    "end": "2814480"
  },
  {
    "text": "there we'd love help and sanjay also asked what's the plan to",
    "start": "2814480",
    "end": "2821200"
  },
  {
    "text": "implement authorization rules in liquidity um so",
    "start": "2821200",
    "end": "2826400"
  },
  {
    "text": "uh we're targeting something like the smi traffic",
    "start": "2826400",
    "end": "2832800"
  },
  {
    "text": "policy apis and so smi has a traffic policy crd",
    "start": "2832800",
    "end": "2837920"
  },
  {
    "text": "it doesn't quite fit our model of the world and so we probably probably gonna submit another",
    "start": "2837920",
    "end": "2843599"
  },
  {
    "text": "revision and work with them to rev that the design is probably kicking off in",
    "start": "2843599",
    "end": "2849760"
  },
  {
    "text": "december in earnest we need to get to kubecon and get some of the 210 work in flight but we'll start",
    "start": "2849760",
    "end": "2855839"
  },
  {
    "text": "designing that towards the end of the year and uh once 210 goes out we're going to start",
    "start": "2855839",
    "end": "2861119"
  },
  {
    "text": "working on that full time and that'll be the kind of big feature in 211. so i'm really looking forward to that and",
    "start": "2861119",
    "end": "2866880"
  },
  {
    "text": "uh i would love input if you have pain",
    "start": "2866880",
    "end": "2872079"
  },
  {
    "text": "points that you've seen in the existing implementations in either smi or istio we'd love to hear what's worked well for",
    "start": "2872079",
    "end": "2877520"
  },
  {
    "text": "you what hasn't worked well john said that i said that",
    "start": "2877520",
    "end": "2884480"
  },
  {
    "text": "multiplexing computer reduces network load i think um this is reduced the amount of ports used in a cluster maybe running port",
    "start": "2884480",
    "end": "2890800"
  },
  {
    "text": "exhaustion it definitely can reduce the number of raw sockets and file descriptors and epoxy",
    "start": "2890800",
    "end": "2895920"
  },
  {
    "text": "equipment right so if you imagine because we're proxying tcp connections we really double if you're if we're not",
    "start": "2895920",
    "end": "2901760"
  },
  {
    "text": "doing anything smart here we really double the number of uh connections in a pod right because",
    "start": "2901760",
    "end": "2906880"
  },
  {
    "text": "we have to terminate the connection when we accept it and we have to establish new connections on the way out and so this is the multiplexing really",
    "start": "2906880",
    "end": "2913359"
  },
  {
    "text": "lets us work around that cost it means one that we have you know we use far",
    "start": "2913359",
    "end": "2918480"
  },
  {
    "text": "fewer femoral ports on the outbound side and on the inbound side we you know not really doing a poor",
    "start": "2918480",
    "end": "2924640"
  },
  {
    "text": "exhaustion on the inbound side but it's a similar problem there where we try to reduce the number of sockets the",
    "start": "2924640",
    "end": "2930079"
  },
  {
    "text": "number of files to boost the proxy use and as we've seen in testing that can really have an impact on memory",
    "start": "2930079",
    "end": "2935280"
  },
  {
    "text": "consumption so uh yeah we're our goal is to minimize the the operating system resources we need",
    "start": "2935280",
    "end": "2940720"
  },
  {
    "text": "to do these things",
    "start": "2940720",
    "end": "2944160"
  },
  {
    "text": "any other questions i've gotten through them too quickly i think",
    "start": "2945920",
    "end": "2951359"
  },
  {
    "text": "did great anybody else have anything there we go does lingerie support",
    "start": "2953119",
    "end": "2960240"
  },
  {
    "text": "networked policies similar to calico or should one just use calico uh today i would say use both so i think",
    "start": "2960240",
    "end": "2967440"
  },
  {
    "text": "um you know security and depth is a better approach than just having one layer",
    "start": "2967440",
    "end": "2973119"
  },
  {
    "text": "and so it's good to use network policies where you really want to enforce where traffic can",
    "start": "2973119",
    "end": "2979359"
  },
  {
    "text": "go but i think it's also important to use things like mtls uh for for kind of the zero trust aspects of securing um",
    "start": "2979359",
    "end": "2986160"
  },
  {
    "text": "you know communication and flight and things like that cms can we segregate traffic between",
    "start": "2986160",
    "end": "2992000"
  },
  {
    "text": "namespaces for example all namespaces can talk to linkedin insert manager but not others ia multi-tenant uh this is a",
    "start": "2992000",
    "end": "2998559"
  },
  {
    "text": "great feature request and i think it's definitely something we're going to uh approach like that's definitely a use",
    "start": "2998559",
    "end": "3005119"
  },
  {
    "text": "case for us to consider as we implement traffic policies uh this is you know an obvious type of",
    "start": "3005119",
    "end": "3010720"
  },
  {
    "text": "policy you may want to express um i think today the way to achieve that",
    "start": "3010720",
    "end": "3016160"
  },
  {
    "text": "would be through a layered approach so something like network policy is layered with linguity",
    "start": "3016160",
    "end": "3021440"
  },
  {
    "text": "it's on the roadmap it's not really there as a fully linked feature today",
    "start": "3021440",
    "end": "3027838"
  },
  {
    "text": "excellent all right you have time for one more question if someone wants to",
    "start": "3031040",
    "end": "3040720"
  },
  {
    "text": "all right well you can also jump into our slack um happy to answer questions there i'll",
    "start": "3040720",
    "end": "3046960"
  },
  {
    "text": "be around for some of the afternoon one more pop-up you want to answer one more question okay are there plans to",
    "start": "3046960",
    "end": "3053680"
  },
  {
    "text": "have beginner and intermediate coursework great question conrad i believe that we",
    "start": "3053680",
    "end": "3059440"
  },
  {
    "text": "just had a free cncf course just go live um so i i don't have dylan candy but yes there is",
    "start": "3059440",
    "end": "3066559"
  },
  {
    "text": "coursework it's available through cncf i believe it's free and it should be",
    "start": "3066559",
    "end": "3071839"
  },
  {
    "text": "focused on beginner intermediate for sure a good question from jindong about is",
    "start": "3071839",
    "end": "3078480"
  },
  {
    "text": "there a downside to hp2 multiplexing and so it can add latency and cpu usage",
    "start": "3078480",
    "end": "3084559"
  },
  {
    "text": "in some cases so it's really a trade-off between the kind of memory footprint and the socket overlook head there",
    "start": "3084559",
    "end": "3092240"
  },
  {
    "text": "versus some cpu and a little bit of latency we see higher tail latencies in the h21",
    "start": "3092240",
    "end": "3099200"
  },
  {
    "text": "case but kind of higher slightly higher average latencies or p50 latencies in",
    "start": "3099200",
    "end": "3105839"
  },
  {
    "text": "the hp2 case so there is a little bit of a trade-off there that's a really good question but we think hb2 is a better default for",
    "start": "3105839",
    "end": "3112319"
  },
  {
    "text": "kind of the lightweight approach",
    "start": "3112319",
    "end": "3115359"
  },
  {
    "text": "okay thanks so much oliver for a great discussion um and a great presentation and q a",
    "start": "3118800",
    "end": "3127280"
  },
  {
    "text": "that's all the time we have for today everyone and thanks for joining us the webinar recording and slides will be",
    "start": "3127280",
    "end": "3133040"
  },
  {
    "text": "online later today and we are looking forward to seeing you at a future cncf webinar",
    "start": "3133040",
    "end": "3138559"
  },
  {
    "text": "everybody have a great day",
    "start": "3138559",
    "end": "3143359"
  },
  {
    "text": "bye",
    "start": "3145960",
    "end": "3148960"
  }
]