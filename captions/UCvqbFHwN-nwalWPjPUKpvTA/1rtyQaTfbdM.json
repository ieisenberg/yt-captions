[
  {
    "text": "good afternoon thank you very much for coming to my session today i know it's one of the last session of cubecorn it's",
    "start": "480",
    "end": "6080"
  },
  {
    "text": "friday we're all tired and everything but you know if you're here you probably want to know something or at least",
    "start": "6080",
    "end": "11440"
  },
  {
    "text": "you're curious about how we do wildfire prevention with kubernetes and ai my",
    "start": "11440",
    "end": "17279"
  },
  {
    "text": "name is andre jardini i'm a sens ambassador i work as an sre for overstory and in my free time i also uh",
    "start": "17279",
    "end": "24800"
  },
  {
    "text": "organize community events and yeah community conferences around europe",
    "start": "24800",
    "end": "29920"
  },
  {
    "text": "the title of my talk is kubernetes and ai to protect our forest a cloudnative infrastructure for wildfire",
    "start": "29920",
    "end": "38440"
  },
  {
    "text": "prevention like me a couple of years ago before i started working for overtory i",
    "start": "38440",
    "end": "44160"
  },
  {
    "text": "didn't actually know much about wildfires wildfires are events that in",
    "start": "44160",
    "end": "50239"
  },
  {
    "text": "europe are a little bit more rare compared to the us just for because of the density because of the geography of",
    "start": "50239",
    "end": "55600"
  },
  {
    "text": "the us territory but there are events that are extremely destructive for people lives community building",
    "start": "55600",
    "end": "62559"
  },
  {
    "text": "businesses and so on these events are often very sudden they happen",
    "start": "62559",
    "end": "68119"
  },
  {
    "text": "unpredictably and they bring a lot of destruction to our to our people just to give you an idea just a",
    "start": "68119",
    "end": "75360"
  },
  {
    "text": "couple of months ago i'm sure you're aware i'm sure you're following the news a big wildfire spread out in southern",
    "start": "75360",
    "end": "81439"
  },
  {
    "text": "california this was the second biggest wildfire that happened in california over 29 people died over 200,000 people",
    "start": "81439",
    "end": "89119"
  },
  {
    "text": "got evacuated 18,000 homes destroyed and over 57,000 acres of land burned down",
    "start": "89119",
    "end": "97680"
  },
  {
    "text": "just to give you an idea of what 57,000 acres of land means this is the area",
    "start": "97680",
    "end": "103200"
  },
  {
    "text": "that would have burned in london that represents 57,000 acres and this was not",
    "start": "103200",
    "end": "108880"
  },
  {
    "text": "even the largest wildfire that happened in california the 2018",
    "start": "108880",
    "end": "115079"
  },
  {
    "text": "campfire was uh spread over an area that was almost three times as large",
    "start": "115079",
    "end": "121479"
  },
  {
    "text": "153,000 acres another thing that i didn't know about wildfire is that in california",
    "start": "121479",
    "end": "128720"
  },
  {
    "text": "specifically um over 60% of the most destructive wildfires have been caused",
    "start": "128720",
    "end": "135360"
  },
  {
    "text": "by defective power lines i know it sounds curious i know that we expect you know fire to be you know wildfires to be",
    "start": "135360",
    "end": "143360"
  },
  {
    "text": "um caused by you know humans most of the time just you know a cigarette that is not turned down correctly or things like",
    "start": "143360",
    "end": "149680"
  },
  {
    "text": "this but power lines are actually a big responsible of this kind of wildfire especially in",
    "start": "149680",
    "end": "156680"
  },
  {
    "text": "california and wildfire prevention is not an easy task for a multitude of",
    "start": "156680",
    "end": "162680"
  },
  {
    "text": "reason the first one in respect to power lines is that the power infrastructure",
    "start": "162680",
    "end": "168000"
  },
  {
    "text": "is very vast it takes a lot of space it's very complex it has a big number of",
    "start": "168000",
    "end": "173599"
  },
  {
    "text": "different substation and the infrastructure is just really really big to handle and to maintain and if you",
    "start": "173599",
    "end": "179680"
  },
  {
    "text": "think about it power lines are bringing us electricity 365 days a year so it's",
    "start": "179680",
    "end": "185120"
  },
  {
    "text": "the kind of infrastructure that needs to be you know as close as possible as a two a 100%",
    "start": "185120",
    "end": "192120"
  },
  {
    "text": "uptime the second cause why wildfire prevention is important but is also difficult at the same time is that",
    "start": "192120",
    "end": "198560"
  },
  {
    "text": "failures are always sudden and catastrophic you might have heard that just a couple of weeks ago a power",
    "start": "198560",
    "end": "205200"
  },
  {
    "text": "substation near the ephro airport failed and that caused the whole ephrop airport",
    "start": "205200",
    "end": "211280"
  },
  {
    "text": "to be shut down for over 24 hours one over 1,000 flights got disrupted over",
    "start": "211280",
    "end": "217680"
  },
  {
    "text": "300,000 passengers got affected and this was just a small power substation you",
    "start": "217680",
    "end": "223599"
  },
  {
    "text": "see how the impact of this can be sudden and catastrophic and can cause a lot of disruption",
    "start": "223599",
    "end": "230840"
  },
  {
    "text": "finally if you think about it much of these infrastructure much of these power lines and network are always subject to",
    "start": "230840",
    "end": "238640"
  },
  {
    "text": "atmospheric agent so they can get damaged by all kind of things let it be vegetation or a storm thunders and so on",
    "start": "238640",
    "end": "247519"
  },
  {
    "text": "inspecting them is also not easy very often those power lines they cross for example private properties so if we want",
    "start": "247519",
    "end": "253760"
  },
  {
    "text": "to go and inspect them visually we need to ask permission for these people to go in their private property and inspect",
    "start": "253760",
    "end": "260479"
  },
  {
    "text": "their power lines this is extremely time consuming and extremely costly for uh",
    "start": "260479",
    "end": "266720"
  },
  {
    "text": "power utility companies and this is why overtory has",
    "start": "266720",
    "end": "272000"
  },
  {
    "text": "started to think in a different approach what if instead of instead of going and",
    "start": "272000",
    "end": "277759"
  },
  {
    "text": "inspecting those power line visually what if we do it through satellite images specifically very high resolution",
    "start": "277759",
    "end": "284800"
  },
  {
    "text": "satellite imagery and machine learning so that we can get insights from the sky",
    "start": "284800",
    "end": "289919"
  },
  {
    "text": "pretty much on how our infrastructure is doing if it needs maintenance if there is a damaged line for",
    "start": "289919",
    "end": "296919"
  },
  {
    "text": "example we do that with a simple process well simple it will be simple because",
    "start": "296919",
    "end": "302080"
  },
  {
    "text": "i'm explaining to you in these three different steps but actually the whole process is a lot more complicated the",
    "start": "302080",
    "end": "308240"
  },
  {
    "text": "idea is first of all that we get vegetation data from different sources some of those are satellite images some",
    "start": "308240",
    "end": "314400"
  },
  {
    "text": "of those are aerial images so we have providers that we query and they give us high resolution satellite images think",
    "start": "314400",
    "end": "321840"
  },
  {
    "text": "that the highest resolution that we use is 15 cm then we combine this data with",
    "start": "321840",
    "end": "329520"
  },
  {
    "text": "information that we get from the energy provider so they give us all the coordinates of the poles the lines the",
    "start": "329520",
    "end": "336800"
  },
  {
    "text": "kind of territory where their infrastructure is standing on and so on",
    "start": "336800",
    "end": "341919"
  },
  {
    "text": "and basically what we do is that we combine these two different types of data to create a map a risk map so that",
    "start": "341919",
    "end": "349440"
  },
  {
    "text": "the utility company can go and see where there is an elevated risk of wildfire the idea is that whenever the vegetation",
    "start": "349440",
    "end": "356560"
  },
  {
    "text": "gets too close to the power lines there is an elevated risk of wildfire and in this case the provider can go and trim",
    "start": "356560",
    "end": "363199"
  },
  {
    "text": "the vegetation before a wildfire spreads out so we make it easy for those",
    "start": "363199",
    "end": "368639"
  },
  {
    "text": "utilities to look at the map to look at our product at the results of our scan and basically understand where they need",
    "start": "368639",
    "end": "376160"
  },
  {
    "text": "to go and act faster in order to prevent the next",
    "start": "376160",
    "end": "381198"
  },
  {
    "text": "wildfire but how are we doing that and how are we doing that using kubernetes and cloudnative solutions before we move",
    "start": "382520",
    "end": "389840"
  },
  {
    "text": "into what we currently have right now i want to take a step back and think about how we got started we are a startup when",
    "start": "389840",
    "end": "397919"
  },
  {
    "text": "i joined over story we were around 20 people right now we're over 100 when we",
    "start": "397919",
    "end": "403280"
  },
  {
    "text": "got started we need to break things and moves fast and move fast this is you know what a lot of people say about",
    "start": "403280",
    "end": "409360"
  },
  {
    "text": "startup we need to figure out things you know on right on the ground so when we",
    "start": "409360",
    "end": "414639"
  },
  {
    "text": "started we actually had a stack that was extremely simple we just used kubernetes because it was providing us the right",
    "start": "414639",
    "end": "421599"
  },
  {
    "text": "balance between flexibility and stability you can think that our workload can take anything from one cpu",
    "start": "421599",
    "end": "428800"
  },
  {
    "text": "and 4 gigs of memory to 72 cpus and alpha terabyte of memory and stability",
    "start": "428800",
    "end": "435520"
  },
  {
    "text": "so we had that with kubernetes and most of our processing was happening with jupyter hub i'm sure",
    "start": "435520",
    "end": "442479"
  },
  {
    "text": "you're familiar already with jupyter notebooks jupyter hub is just a tool that allows you to create different",
    "start": "442479",
    "end": "447759"
  },
  {
    "text": "jupyter notebooks using different types of resources and in the beginning that was all we had we started with this and",
    "start": "447759",
    "end": "455680"
  },
  {
    "text": "everything was a jupyter notebook so we would create all our processing pipelines we would create all our data",
    "start": "455680",
    "end": "462000"
  },
  {
    "text": "analysis in jupyter notebook we built some custom integration to it so that people could go and select the container",
    "start": "462000",
    "end": "469120"
  },
  {
    "text": "image that they wanted to spin select the machine where they wanted to run it and basically they would have the possibility of running their jupyter",
    "start": "469120",
    "end": "475440"
  },
  {
    "text": "notebook on a machine which no much higher power compared to their laptop eventually a bigger gpu or a large",
    "start": "475440",
    "end": "481840"
  },
  {
    "text": "amount of memory or cpu so data scientists could just go on this platform select the machine that",
    "start": "481840",
    "end": "488879"
  },
  {
    "text": "they wanted select the image that they wanted and run this experiment in a jupyter notebook as you can imagine",
    "start": "488879",
    "end": "494639"
  },
  {
    "text": "jupyter notebooks are great for experimenting but this became a",
    "start": "494639",
    "end": "499759"
  },
  {
    "text": "nightmare pretty early reputability and tracking in particular was extremely difficult and as we have to repeat these",
    "start": "499759",
    "end": "506560"
  },
  {
    "text": "kinds over time every time we had this kind of messages all over zlack hey do",
    "start": "506560",
    "end": "511840"
  },
  {
    "text": "you remember which version of pandas we used a year ago the new one breaks everything and the whole process in",
    "start": "511840",
    "end": "518320"
  },
  {
    "text": "general was just very manual it was really we always needed to assign a data scientist with every delivery that we",
    "start": "518320",
    "end": "525839"
  },
  {
    "text": "were doing for a client and is the data scientist would have to go through the jupyter notebook step by step fix the",
    "start": "525839",
    "end": "532080"
  },
  {
    "text": "problem that they would encounter on the way and pretty much deliver the results to the final client it was a very",
    "start": "532080",
    "end": "538160"
  },
  {
    "text": "onetoone binding one request from the client one data scientist assigned to",
    "start": "538160",
    "end": "544760"
  },
  {
    "text": "it so after a while it was just time to automate and move forward",
    "start": "544760",
    "end": "550120"
  },
  {
    "text": "we set to ourself two main objectives the first one getting rid of jupyter",
    "start": "550120",
    "end": "556000"
  },
  {
    "text": "notebook we tried different combination of things we tried using a tool called nbde that allows you to take your",
    "start": "556000",
    "end": "562240"
  },
  {
    "text": "jupyter notebook and convert it into a python package we were not very successful with that i have to say in",
    "start": "562240",
    "end": "569360"
  },
  {
    "text": "general what we experienced that jupyter notebook are difficult to maintain difficult to test and tracking with git",
    "start": "569360",
    "end": "574800"
  },
  {
    "text": "was very complicated if you don't know about it a jupyter notebook is pretty much a giant json file so you can",
    "start": "574800",
    "end": "580959"
  },
  {
    "text": "imagine what that means whenever you need to track that in git and the second and probably the most important",
    "start": "580959",
    "end": "586240"
  },
  {
    "text": "objective was that we wanted to automate those workflow runs we wanted to break this relationship between the number of",
    "start": "586240",
    "end": "592399"
  },
  {
    "text": "data scientists we have and the number of projects that we were running so we needed for a data and workflow",
    "start": "592399",
    "end": "600200"
  },
  {
    "text": "orchestrator in here we stumbled upon a workflow called daxter this is a project",
    "start": "600200",
    "end": "605279"
  },
  {
    "text": "i'm a fan of i've been contributed to the open source project for quite some time we started using it since version 0",
    "start": "605279",
    "end": "612080"
  },
  {
    "text": "something and now there are like 1.10 i personally like daxter because",
    "start": "612080",
    "end": "617200"
  },
  {
    "text": "first of all it's an open source project is backed by a company in the us with their own cloud offering but we've been",
    "start": "617200",
    "end": "622320"
  },
  {
    "text": "using their open source version very successfully it gave us immediately a couple of things that we like first of",
    "start": "622320",
    "end": "629040"
  },
  {
    "text": "first of all fast development if you have experience with data and workflow orchestrator you know that just",
    "start": "629040",
    "end": "635000"
  },
  {
    "text": "understanding what is the right way of setting up your python project can be a pain daxter makes it very easy daxter",
    "start": "635000",
    "end": "642160"
  },
  {
    "text": "scaffold project name and you get the project structure out of the box with all the best use cases from their",
    "start": "642160",
    "end": "648200"
  },
  {
    "text": "maintainers it was very easy to test locally it was very easy to get started really really fast it all works with",
    "start": "648200",
    "end": "655200"
  },
  {
    "text": "python modules which our data scientists and data engineers like and this is",
    "start": "655200",
    "end": "660240"
  },
  {
    "text": "something that where i'm a little bit biased as a cloud engineer it's very cloud native you can really see that",
    "start": "660240",
    "end": "665519"
  },
  {
    "text": "people built this tool with cloud native in mind so what we do at our store is",
    "start": "665519",
    "end": "670640"
  },
  {
    "text": "that we run daxter in our kubernetes cluster and the setup if you look at this slide it's a classical micros",
    "start": "670640",
    "end": "678240"
  },
  {
    "text": "service architecture setup so our user that you can see on the right side of the slide accesses the web interface of",
    "start": "678240",
    "end": "685519"
  },
  {
    "text": "dagster which is called dagit you know as a normal web application basically what they get is an overview of the",
    "start": "685519",
    "end": "691920"
  },
  {
    "text": "system which pipelines are failing which pipelines are successful which pipelines need attention",
    "start": "691920",
    "end": "697839"
  },
  {
    "text": "then there is another component called the dster demon which takes care of running the pipelines based on a certain",
    "start": "697839",
    "end": "704000"
  },
  {
    "text": "schedule or on certain condition and monitor pipelines while they're running so that if there is a step that needs to",
    "start": "704000",
    "end": "710399"
  },
  {
    "text": "be retrieded it gets retrieded and if the pipeline fails it notifies the final user and all of these all of these boxes",
    "start": "710399",
    "end": "717279"
  },
  {
    "text": "that you see they are all deployments all this they are all stateless",
    "start": "717279",
    "end": "722440"
  },
  {
    "text": "application and then you can see in the lower part of the slide a set of boxes that i called the deployment ment daxter",
    "start": "722440",
    "end": "728639"
  },
  {
    "text": "calls them code locations basically each one of those is a grpc server it's a",
    "start": "728639",
    "end": "734399"
  },
  {
    "text": "grpc server that exposes basically advertise to the dster demon and the web interface which pipelines are present in",
    "start": "734399",
    "end": "741600"
  },
  {
    "text": "that code location which schedules sensor assets and so on so each one of",
    "start": "741600",
    "end": "748079"
  },
  {
    "text": "those deployments is independent meaning that each team that we have inside of our story is responsible for one of the",
    "start": "748079",
    "end": "755279"
  },
  {
    "text": "or more of these code locations and they can do whatever they want with them they can create a new pipeline remove it edit",
    "start": "755279",
    "end": "761680"
  },
  {
    "text": "it modify it and so on so on and the big advantage here is that whenever a team",
    "start": "761680",
    "end": "767600"
  },
  {
    "text": "breaks one of these deployment all the other deployments are not affected we there is very high isolation between",
    "start": "767600",
    "end": "774399"
  },
  {
    "text": "them and that's extremely important as the company started to grow",
    "start": "774399",
    "end": "779639"
  },
  {
    "text": "up when i said before that daxter is really something that has been built with cloud native in mind i wanted to",
    "start": "779639",
    "end": "786240"
  },
  {
    "text": "bring you a couple of example as well one of the feature that i like the most about daxter and that's something where",
    "start": "786240",
    "end": "791760"
  },
  {
    "text": "i had experience with other tools and had a lot of trouble with others tools is the idea behind io managers in other",
    "start": "791760",
    "end": "798720"
  },
  {
    "text": "tools whenever you need to build a pipeline that needs to run locally and in the cloud i always found it a little",
    "start": "798720",
    "end": "804959"
  },
  {
    "text": "bit complicated very often you need to put some logic in the code that you've write that says if i'm running in the",
    "start": "804959",
    "end": "811120"
  },
  {
    "text": "cloud i want my data to be saved in s3 if i'm running it locally i want my data to be saved locally you don't need to do",
    "start": "811120",
    "end": "818000"
  },
  {
    "text": "that with daxter daxter framework does this for you so you can focus on the logic of your data of your application",
    "start": "818000",
    "end": "824720"
  },
  {
    "text": "of your workflow in your python code and daxter abstracts all the rest in this",
    "start": "824720",
    "end": "830560"
  },
  {
    "text": "way the project stay the same the code stays the same but simply based on the environment if daxter is running in",
    "start": "830560",
    "end": "836560"
  },
  {
    "text": "kubernetes it will save its output in s3 on google cloud storage if it's running locally it will run on your daxter demon",
    "start": "836560",
    "end": "844000"
  },
  {
    "text": "and save the file locally which is a really neat feature because it allows us to really you know have a clear cut",
    "start": "844000",
    "end": "851600"
  },
  {
    "text": "between the data part of things and the implementation part",
    "start": "851600",
    "end": "858199"
  },
  {
    "text": "another task that daxter does really well is that in the age of cloud data",
    "start": "858240",
    "end": "865680"
  },
  {
    "text": "has many different forms right when we talk about data processing in the past",
    "start": "865680",
    "end": "870720"
  },
  {
    "text": "usually you had a database a couple of files here and there the data that you were processing was more or less uniform",
    "start": "870720",
    "end": "877199"
  },
  {
    "text": "but now we live in the cloud right data can be a big query table can be a possql",
    "start": "877199",
    "end": "883279"
  },
  {
    "text": "database or your snowflake data store or a file on gcs data can have a lot of",
    "start": "883279",
    "end": "888880"
  },
  {
    "text": "different formats and tools very often they focus on how data is built no not",
    "start": "888880",
    "end": "894959"
  },
  {
    "text": "on how data is connected with each other it is more and more difficult with with",
    "start": "894959",
    "end": "900000"
  },
  {
    "text": "other tools to understand what is the relationship within between my postsql",
    "start": "900000",
    "end": "905199"
  },
  {
    "text": "database table and my big query table and that's because the tool focuses on how data is built not on how it's",
    "start": "905199",
    "end": "911720"
  },
  {
    "text": "connected a really neat concept that daxter introduced on top of the usual jobs and pipelines and step is the idea",
    "start": "911720",
    "end": "919279"
  },
  {
    "text": "behind assets an asset in daxter can be anything can",
    "start": "919279",
    "end": "925199"
  },
  {
    "text": "be any data any type of data again bigquery table it can be an asset a",
    "start": "925199",
    "end": "931120"
  },
  {
    "text": "database can be an asset a file can be an asset and what daxter is very good at is",
    "start": "931120",
    "end": "938160"
  },
  {
    "text": "making explicit where the connection between those asset is which assets depends on each other and so on in here",
    "start": "938160",
    "end": "945519"
  },
  {
    "text": "i just put a couple of example i pulled them from their documentation you can see very clearly that there are two air",
    "start": "945519",
    "end": "951440"
  },
  {
    "text": "bite ingestion pipelines that then gets processed with you know three different dbt projects and finally i'm running my",
    "start": "951440",
    "end": "958560"
  },
  {
    "text": "forecast using python simple easy to visualize i know where the data is going",
    "start": "958560",
    "end": "963839"
  },
  {
    "text": "i know where how the data depends on each other example here on the right side ingestion is done using five trend",
    "start": "963839",
    "end": "970720"
  },
  {
    "text": "i'm then using dbt to modify the data and finally i'm predicting the new order",
    "start": "970720",
    "end": "975759"
  },
  {
    "text": "using my tensorflow model it's easy to understand how the data flows through the system because we're focusing on how",
    "start": "975759",
    "end": "982480"
  },
  {
    "text": "data is related not on how data is built as i mentioned in daxter they",
    "start": "982480",
    "end": "989680"
  },
  {
    "text": "don't want you to go all in on this approach it's a kind of optin situation you can even mix them but you still have",
    "start": "989680",
    "end": "996480"
  },
  {
    "text": "all the possibilities of running pipelines you know in the traditional way with jobs pipelines steps and so on",
    "start": "996480",
    "end": "1003839"
  },
  {
    "text": "over time as we started to get more and more experience with daxter we also started to build our own libraries in",
    "start": "1003839",
    "end": "1010480"
  },
  {
    "text": "particular we have a library internal library called maple you know we're a company that cares about trees so our",
    "start": "1010480",
    "end": "1016320"
  },
  {
    "text": "cluster libraries and projects very often have name of trees in this case in particular i'm showing you a maple op",
    "start": "1016320",
    "end": "1023759"
  },
  {
    "text": "you can think of op as a pipeline steps think of an op as a step pretty much",
    "start": "1023759",
    "end": "1030079"
  },
  {
    "text": "what i'm putting doing here is adding a decorator and saying okay this function is not an actual fi p python function",
    "start": "1030079",
    "end": "1037120"
  },
  {
    "text": "this one is a step of my pipeline and i can customize this step however i want",
    "start": "1037120",
    "end": "1042720"
  },
  {
    "text": "in this case i'm specifying that i want this function this particular step to run on a node that has a gpu and this",
    "start": "1042720",
    "end": "1050320"
  },
  {
    "text": "are high memory node a node that has at least 32 cpus available and around 390",
    "start": "1050320",
    "end": "1056320"
  },
  {
    "text": "gigs of memory and the last two lines are also very important this is a feature of kubernetes that is relatively",
    "start": "1056320",
    "end": "1062400"
  },
  {
    "text": "recent but we use extensively this is called ephemeral volumes basically there are volumes that can be created",
    "start": "1062400",
    "end": "1068320"
  },
  {
    "text": "dynamically and have the same lifetime as the pod so as the pod gets created a",
    "start": "1068320",
    "end": "1074160"
  },
  {
    "text": "volume gets attached to it it stays connected to the pod for the whole lifetime of the pod and when the pod",
    "start": "1074160",
    "end": "1080160"
  },
  {
    "text": "gets terminated the volume also gets deleted and this is very useful for scratch space so what i'm specifying in",
    "start": "1080160",
    "end": "1086720"
  },
  {
    "text": "this maple op is that i also want a scratch space i want 150 gb of volume",
    "start": "1086720",
    "end": "1092640"
  },
  {
    "text": "that is temporary to the lifetime of the pod and this is is very used for example to store temporary",
    "start": "1092640",
    "end": "1099160"
  },
  {
    "text": "files right now this is the situation that we have with maple we have plans to",
    "start": "1099160",
    "end": "1104880"
  },
  {
    "text": "improve it because i feel like this is still too kubernetes specific like our developers need to know too much about",
    "start": "1104880",
    "end": "1111120"
  },
  {
    "text": "kubernetes to know about this so in the new version of maple the idea is to hide that even",
    "start": "1111120",
    "end": "1117640"
  },
  {
    "text": "more as daxter step can be mapped into different pods daxter gives us the",
    "start": "1117640",
    "end": "1123679"
  },
  {
    "text": "possibility of playing with it in different ways in particular in two ways that i find quite",
    "start": "1123679",
    "end": "1129640"
  },
  {
    "text": "interesting the first one is that we can have a pipeline where we have a single pod and a single node so i can have a",
    "start": "1129640",
    "end": "1135840"
  },
  {
    "text": "pipeline with multiple steps but all of them will be run in one node with the",
    "start": "1135840",
    "end": "1141280"
  },
  {
    "text": "same amount of allocated resources so at the beginning of my pipeline i can say well this whole pipeline is going to",
    "start": "1141280",
    "end": "1147520"
  },
  {
    "text": "take four cpus and gigs of memory and daxter will run that on a single pod so",
    "start": "1147520",
    "end": "1153200"
  },
  {
    "text": "all the step will will be run as part of a single pod or i can go the other way",
    "start": "1153200",
    "end": "1158400"
  },
  {
    "text": "around and say look instead of having a single pod let's do it in multiple pods",
    "start": "1158400",
    "end": "1164080"
  },
  {
    "text": "where every step has a sing different amount of cpus and resources and so for example i can have a multiple pods",
    "start": "1164080",
    "end": "1171440"
  },
  {
    "text": "multiple nodes pipeline where every step has a different type of requirements in",
    "start": "1171440",
    "end": "1177039"
  },
  {
    "text": "the example below the first one takes four cpu and 8 gigs of memory the second one requires a lot more cpu a lot more",
    "start": "1177039",
    "end": "1184080"
  },
  {
    "text": "gpu and a lot more memory and finally the third one uses ephemeral storage",
    "start": "1184080",
    "end": "1189760"
  },
  {
    "text": "this allows us to not reserve a gpu for the whole duration of a pipeline but",
    "start": "1189760",
    "end": "1195200"
  },
  {
    "text": "just for the steps that uses that particular feature if you have a pipeline that lasts eight hours you",
    "start": "1195200",
    "end": "1201200"
  },
  {
    "text": "don't want to reserve a gpu for eight hours and use it just for one you want to reserve a you want to reserve the gpu",
    "start": "1201200",
    "end": "1208240"
  },
  {
    "text": "just for the hour where you're using it right as gpus are really expensive",
    "start": "1208240",
    "end": "1214840"
  },
  {
    "text": "what is happening now so at this point we already became pretty you know dster",
    "start": "1216400",
    "end": "1221840"
  },
  {
    "text": "you know good users we were open we were contributing back to the open source project we were working with it very",
    "start": "1221840",
    "end": "1228159"
  },
  {
    "text": "extensively we started to build more and more pipelines and more and more you know assets into it but now we wanted to",
    "start": "1228159",
    "end": "1235360"
  },
  {
    "text": "take the next step one thing we noticed with daxter one",
    "start": "1235360",
    "end": "1240960"
  },
  {
    "text": "thing we struggle a little bit with um is observability in particular the",
    "start": "1240960",
    "end": "1246720"
  },
  {
    "text": "daxter web interface is very self-explanatory by opening it i can get like a gist immediately of how my",
    "start": "1246720",
    "end": "1253039"
  },
  {
    "text": "pipelines are doing and how my system is going but the problem is that it's very difficult to get aggregate statistic if",
    "start": "1253039",
    "end": "1259840"
  },
  {
    "text": "i want to know something like what is the failure rate of this pipeline over the past 30 days that's something very",
    "start": "1259840",
    "end": "1267360"
  },
  {
    "text": "difficult to get also some of the dependencies that we have while we build this process are",
    "start": "1267360",
    "end": "1273520"
  },
  {
    "text": "really difficult to model to model using daxter native uh the using the daxter",
    "start": "1273520",
    "end": "1279600"
  },
  {
    "text": "model so what we decided to do is to build a platform on top of it meaning that daxter will continue to stay our",
    "start": "1279600",
    "end": "1286559"
  },
  {
    "text": "workflow engine but the logic on how we trigger those pipelines the parameters and the configuration will happen",
    "start": "1286559",
    "end": "1293120"
  },
  {
    "text": "externally so we built an interface to daxter that talks with a daxter api and",
    "start": "1293120",
    "end": "1298320"
  },
  {
    "text": "that triggers jobs in a certain order or with a certain settings depending on the deliver that we need to do depending on",
    "start": "1298320",
    "end": "1304799"
  },
  {
    "text": "the data that we need to analyze and this is how it looks like so in the lower part of this image you can",
    "start": "1304799",
    "end": "1311280"
  },
  {
    "text": "see the usual gke cluster and we have daxter installed in there and that's all and good on the top right side we see",
    "start": "1311280",
    "end": "1319200"
  },
  {
    "text": "what we call the platform the delivery platform these are different microservices starter router trigger and",
    "start": "1319200",
    "end": "1325760"
  },
  {
    "text": "watcher that are all running in google cloudron we're also big fans of google cloud and all their communication happen",
    "start": "1325760",
    "end": "1333039"
  },
  {
    "text": "using google popsub so a classic like messaging queuing system basically what",
    "start": "1333039",
    "end": "1338880"
  },
  {
    "text": "happens is that whenever a new event gets triggered in the starter the router the trigger and the watcher basically",
    "start": "1338880",
    "end": "1345919"
  },
  {
    "text": "keep an eye on their execution in daxter and make sure to trigger jobs in a certain order and make sure to trigger",
    "start": "1345919",
    "end": "1352400"
  },
  {
    "text": "them with the right setting so the trigger and the washer are not doing nothing more than interacting with the",
    "start": "1352400",
    "end": "1358640"
  },
  {
    "text": "dx api and making sure the flows uh happens correctly and then what we get",
    "start": "1358640",
    "end": "1364559"
  },
  {
    "text": "as an output is the possibility of seeing a delivery end to end this whole process that you see at the beginning",
    "start": "1364559",
    "end": "1370559"
  },
  {
    "text": "from image acquisition to like uh infrastructure you know getting the post online from the customer up to the final",
    "start": "1370559",
    "end": "1377039"
  },
  {
    "text": "delivery it's something that happens all in our platform since the communication happens",
    "start": "1377039",
    "end": "1383280"
  },
  {
    "text": "via pubsub between all these different microservices we are able to export all those messages to be query we have open",
    "start": "1383280",
    "end": "1390320"
  },
  {
    "text": "telemetry that exports data as well to google cloud metrics and then we visualize it using graphana graphana is",
    "start": "1390320",
    "end": "1397280"
  },
  {
    "text": "a great tool it has multiple different data sources so we can pull data from bigquery we can pull data from google",
    "start": "1397280",
    "end": "1403520"
  },
  {
    "text": "cloud metrics and display them in one unique dashboard to give full visibility to our team and this is what it looks",
    "start": "1403520",
    "end": "1411039"
  },
  {
    "text": "like our orchestration platform dashboard allows us to see something like this where for every deliver that",
    "start": "1411039",
    "end": "1416720"
  },
  {
    "text": "we have we can see step by step how it's going if there are some steps that need attention if there are some steps that",
    "start": "1416720",
    "end": "1422480"
  },
  {
    "text": "are failing more than usual and if some steps need manual intervention that unfortunately still that still happens",
    "start": "1422480",
    "end": "1431039"
  },
  {
    "text": "sometime so how do we connect all the pieces how did what are we doing right now and what is the current state of",
    "start": "1432520",
    "end": "1439200"
  },
  {
    "text": "things well this is a screenshot that i took just uh a couple of weeks ago for",
    "start": "1439200",
    "end": "1444400"
  },
  {
    "text": "the first time we managed to run our first delivery end to end zero touch in 30 minutes this was not a new delivery",
    "start": "1444400",
    "end": "1451840"
  },
  {
    "text": "this was a delivery that we are already done in the past that we were recomputing and comparing the outputs",
    "start": "1451840",
    "end": "1458240"
  },
  {
    "text": "just to make sure the outputs were the same and this was a great result this was the culmination of almost a year of",
    "start": "1458240",
    "end": "1465120"
  },
  {
    "text": "work pretty much building our platform from the ground up this a team that is",
    "start": "1465120",
    "end": "1470880"
  },
  {
    "text": "separate from mine so i'm part of the s team this one is the platform team but we have worked very closely over the",
    "start": "1470880",
    "end": "1476320"
  },
  {
    "text": "past couple of over the past months to make sure that this project was",
    "start": "1476320",
    "end": "1482559"
  },
  {
    "text": "successful what's next well the first target is always to deliver results to",
    "start": "1482760",
    "end": "1488400"
  },
  {
    "text": "our customer as hands off as possible we want to avoid as much as possible manual",
    "start": "1488400",
    "end": "1493760"
  },
  {
    "text": "intervention we want to avoid doing these results manually because whenever that happens the risk of mistakes gets",
    "start": "1493760",
    "end": "1500720"
  },
  {
    "text": "much much higher we also plan on building on newer daxter feature to make the pipeline more reliable to changes we",
    "start": "1500720",
    "end": "1507840"
  },
  {
    "text": "are using daxter extensively but they improve very very fast it's a startup they also need to move fast right we",
    "start": "1507840",
    "end": "1515200"
  },
  {
    "text": "would like to break the one step one node or one job per node assumption it would be very nice to have multiple jobs",
    "start": "1515200",
    "end": "1521760"
  },
  {
    "text": "or steps per node and we are looking into both gke node poolool",
    "start": "1521760",
    "end": "1527320"
  },
  {
    "text": "autoprovisioning but also kubernetes dynamic resource allocation which is something that has been yeah has been",
    "start": "1527320",
    "end": "1533440"
  },
  {
    "text": "talked about several times this week and finally this one is one of the trickiest one i have to say avoiding as",
    "start": "1533440",
    "end": "1540960"
  },
  {
    "text": "much as possible underprovisioning or overprovisioning resources the nature of",
    "start": "1540960",
    "end": "1546080"
  },
  {
    "text": "our work and the data we use is that depending on the input image that we get",
    "start": "1546080",
    "end": "1553360"
  },
  {
    "text": "the same process can take you know very different amount of cpus and memories",
    "start": "1553360",
    "end": "1558880"
  },
  {
    "text": "and this is very difficult to predict in advance we would like first of all to",
    "start": "1558880",
    "end": "1564240"
  },
  {
    "text": "make the maple api more straightforward to use so hide even more details from the users even more you know abstract",
    "start": "1564240",
    "end": "1571279"
  },
  {
    "text": "kubernetes even more and definitely work on improvements on monitoring and",
    "start": "1571279",
    "end": "1577400"
  },
  {
    "text": "observability with all the three of this we hope we will manage in the future to make our delivery more reliable our",
    "start": "1577400",
    "end": "1583039"
  },
  {
    "text": "client happier and hopefully prevent the next big wildfire let's put it this way",
    "start": "1583039",
    "end": "1588720"
  },
  {
    "text": "thank you very much for the attention this was my talk i'm jardini work for over story in case you're interested in",
    "start": "1588720",
    "end": "1595200"
  },
  {
    "text": "what i talk today in case if you're curious to hear more about it our website is overstory.com and yeah there",
    "start": "1595200",
    "end": "1600960"
  },
  {
    "text": "is also a career page we're hiring actively people all over europe and the us thank you very much",
    "start": "1600960",
    "end": "1608080"
  },
  {
    "text": "[Applause] folks any questions",
    "start": "1610000",
    "end": "1617720"
  },
  {
    "text": "new daxter feature okay yeah rob thank you rob thank you rob asked me about new",
    "start": "1625279",
    "end": "1632799"
  },
  {
    "text": "daxter feature that we are looking to implement daxter has a new feature that has been well now it's been released",
    "start": "1632799",
    "end": "1638559"
  },
  {
    "text": "like six months ago or something like this called asset checks basically i",
    "start": "1638559",
    "end": "1643679"
  },
  {
    "text": "talked about asset briefly but the idea with asset checks is that every time your asset is materialized so it's",
    "start": "1643679",
    "end": "1650799"
  },
  {
    "text": "refreshed it will run some checks on your asset to make sure that you're not breaking the api downstream so if you're",
    "start": "1650799",
    "end": "1658720"
  },
  {
    "text": "building yeah a file with some let's let's call it a csv file it will make",
    "start": "1658720",
    "end": "1664080"
  },
  {
    "text": "sure for example that all the rows are in there so that you're not breaking all the all the all the pipelines that are",
    "start": "1664080",
    "end": "1670240"
  },
  {
    "text": "downstream this is a feature that we don't actively use we have coded some logic inside our pipelines which is",
    "start": "1670240",
    "end": "1676720"
  },
  {
    "text": "because we didn't have the feature yet but it's something that i feel can greatly improve the reliability of our",
    "start": "1676720",
    "end": "1682240"
  },
  {
    "text": "pipelines because you know like sometime you remove um you remove a column to a",
    "start": "1682240",
    "end": "1687279"
  },
  {
    "text": "file just because you think nobody's using it but you never know how many customers you have how many you know how",
    "start": "1687279",
    "end": "1692640"
  },
  {
    "text": "many users you have that are pulling data from you so that's one of the feature i'm i'm looking forward",
    "start": "1692640",
    "end": "1699600"
  },
  {
    "text": "to any other question",
    "start": "1699640",
    "end": "1704080"
  },
  {
    "text": "yeah yeah",
    "start": "1707720",
    "end": "1711720"
  },
  {
    "text": "that's that's a very good call so there are basically the question was we have",
    "start": "1730559",
    "end": "1735679"
  },
  {
    "text": "daxter running into our impro into our infrastructure and daxter already provides certain feature like triggers",
    "start": "1735679",
    "end": "1742000"
  },
  {
    "text": "and schedulers and so on that are able to do a lot of this automation for you and the question was why did you decide",
    "start": "1742000",
    "end": "1748720"
  },
  {
    "text": "to reimplement all this outside of daxer using uh the platform as you correctly",
    "start": "1748720",
    "end": "1754320"
  },
  {
    "text": "said the part that you were seeing running in cloudr run this is all custom developed there's a little bit more of",
    "start": "1754320",
    "end": "1760520"
  },
  {
    "text": "a use case for overstory let's say than a wide use case the problem that we have",
    "start": "1760520",
    "end": "1766080"
  },
  {
    "text": "is that our pipeline take a really long time because from image acquisition to final delivery you know they can be days",
    "start": "1766080",
    "end": "1773360"
  },
  {
    "text": "pretty much and also the decision that we make are are difficult to model",
    "start": "1773360",
    "end": "1779039"
  },
  {
    "text": "natively in daxter and so at a certain point we gave it a try to try to implement that in daxter but it was just",
    "start": "1779039",
    "end": "1785440"
  },
  {
    "text": "too complicated and too custom let's put it this way to use just you know daxter",
    "start": "1785440",
    "end": "1790640"
  },
  {
    "text": "native concept and this is why we decided to build this system that as you correctly say runs outside runs in",
    "start": "1790640",
    "end": "1796000"
  },
  {
    "text": "google cloud platform and those components are all customly built pretty much i hope it answers your",
    "start": "1796000",
    "end": "1804080"
  },
  {
    "text": "questions anyone else yeah",
    "start": "1804679",
    "end": "1813480"
  },
  {
    "text": "ah yeah good question so what i've shown you before is that you can allocate different resources depending on the",
    "start": "1824559",
    "end": "1830799"
  },
  {
    "text": "task or you can define the resource like pipeline wise basically how and the",
    "start": "1830799",
    "end": "1837760"
  },
  {
    "text": "question is how can i do one or the other you can do that basically always with always with u decorators so just",
    "start": "1837760",
    "end": "1845679"
  },
  {
    "text": "use python decorators if you put the decorator on the pipeline the resources",
    "start": "1845679",
    "end": "1850799"
  },
  {
    "text": "are being allocated for the whole pipeline if you put the decorator step by step if you select these resources",
    "start": "1850799",
    "end": "1857200"
  },
  {
    "text": "step by step then each step will have its own resources that's this idea yeah",
    "start": "1857200",
    "end": "1863039"
  },
  {
    "text": "with uh with code location you can define some defaults so that you make sure that you know you always have at least a minimum amount of resources that",
    "start": "1863039",
    "end": "1869520"
  },
  {
    "text": "are allocated to every step but yeah you can customize it always using decorators which i find it really nice because you",
    "start": "1869520",
    "end": "1874720"
  },
  {
    "text": "know you have everything in code you don't have to yeah you're not doing anything manual any point and",
    "start": "1874720",
    "end": "1880279"
  },
  {
    "text": "click yeah",
    "start": "1880279",
    "end": "1884279"
  },
  {
    "text": "yeah yeah that's true it's a new feature of daxter i haven't talked about it's called daxter pipes i don't have a lot",
    "start": "1906960",
    "end": "1913039"
  },
  {
    "text": "of experience with it i have to say but it's also one of the new features that we are we want to look into because it",
    "start": "1913039",
    "end": "1918880"
  },
  {
    "text": "looks really promising okay thank you very much for",
    "start": "1918880",
    "end": "1924480"
  },
  {
    "text": "the talk will be around in case you have any other question thank you very much",
    "start": "1924480",
    "end": "1930679"
  }
]