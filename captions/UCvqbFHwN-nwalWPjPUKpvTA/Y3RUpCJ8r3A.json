[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "text": "okay thanks for coming so the title of my talk is be processing billions of",
    "start": "709",
    "end": "7290"
  },
  {
    "text": "events in real time using the newly released open source software called a Twitter henan twitter has parent has",
    "start": "7290",
    "end": "14280"
  },
  {
    "text": "been in production with him to death for more than two one half years and we just got it open source like a few months ago",
    "start": "14280",
    "end": "20850"
  },
  {
    "text": "and already we're hitting around probably thousand pull request so far I",
    "start": "20850",
    "end": "25920"
  },
  {
    "text": "think we should be crossing at any time next week I guess so anyway so to give a",
    "start": "25920",
    "end": "31920"
  },
  {
    "text": "context I manage the real-time infrastructure group at Twitter that comprises of all the streaming and I'll",
    "start": "31920",
    "end": "38610"
  },
  {
    "text": "takes as well as interactive query analytics and real-time anomaly detection signed a few more things like",
    "start": "38610",
    "end": "44250"
  },
  {
    "text": "that and so I also teach at Berkeley some of the undergrads es courses in",
    "start": "44250",
    "end": "51899"
  },
  {
    "text": "addition to bring some little bit of research but other than that like so then before coming to",
    "start": "51899",
    "end": "58969"
  },
  {
    "text": "tutor I was running my own company that got scooped like the scooped up by Twitter and we have been I've been doing",
    "start": "58969",
    "end": "64978"
  },
  {
    "text": "real time for probably around eight eight years on now okay so yeah with",
    "start": "64979",
    "end": "70680"
  },
  {
    "start": "70000",
    "end": "108000"
  },
  {
    "text": "that introduction let me go into the talk so i will give you a first-hand overview what it means or what it looks",
    "start": "70680",
    "end": "76770"
  },
  {
    "text": "like and some idea about how high speed Heron is at this point and also talk",
    "start": "76770",
    "end": "82799"
  },
  {
    "text": "about a couple of issues that we encounter in the field the in practical experience something called they hadn't",
    "start": "82799",
    "end": "89670"
  },
  {
    "text": "backpressure how it does and there's some kind of a load shedding where when",
    "start": "89670",
    "end": "94680"
  },
  {
    "text": "the data is coming at a real velocity and you want to be at the top of the real time always when it starts lagging",
    "start": "94680",
    "end": "101189"
  },
  {
    "text": "behind because of certain issues in the system how do you go to the top of the queue always then I will follow it by",
    "start": "101189",
    "end": "106530"
  },
  {
    "text": "conclusion so to give a heron road view how many of you have known storm oh",
    "start": "106530",
    "end": "114380"
  },
  {
    "text": "nice so so essentially we can think about her own as a next hydration of storm accept the fact that architecture",
    "start": "114380",
    "end": "121560"
  },
  {
    "text": "Lee it has been changed drastically underneath and but the terminology and",
    "start": "121560",
    "end": "127050"
  },
  {
    "text": "the API we have kept the same but the flexibility is that we can add a new API",
    "start": "127050",
    "end": "132270"
  },
  {
    "text": "is on top very quickly if we need to but the architecture is sound enough that the code does not have to change but",
    "start": "132270",
    "end": "138780"
  },
  {
    "text": "anyway I will come to that later so to give us terminology about the storm / heron so your topology is it's a real",
    "start": "138780",
    "end": "146580"
  },
  {
    "start": "141000",
    "end": "219000"
  },
  {
    "text": "time job and it's nothing but that dag and there are two type of vertices in the dag and there are as one",
    "start": "146580",
    "end": "154740"
  },
  {
    "text": "daya one type of what is called the spout so the other one is called bowls and these vertices represent some kind",
    "start": "154740",
    "end": "160050"
  },
  {
    "text": "of computations on the edges within the diag represents the streams of data that is flowing between these vertices and",
    "start": "160050",
    "end": "167310"
  },
  {
    "text": "the spout essentially is a type of vertices which kind of taps into the",
    "start": "167310",
    "end": "172320"
  },
  {
    "text": "data source and injects the data into the topology situationally the real time job and the bolt essentially are",
    "start": "172320",
    "end": "179010"
  },
  {
    "text": "processing elements in the sense like it takes incoming topple transforms into whatever way it needs to be done and",
    "start": "179010",
    "end": "184890"
  },
  {
    "text": "emits an outgoing trouble and some examples include filtering aggregation some kind of even",
    "start": "184890",
    "end": "191850"
  },
  {
    "text": "streaming joins or steam table joins or steam service joints and you can even do an arbitrary function where for example",
    "start": "191850",
    "end": "198090"
  },
  {
    "text": "if i want to write my own machine learning algorithm where I wanted to classify a set of",
    "start": "198090",
    "end": "204500"
  },
  {
    "text": "pictures based on the features that I have extracted out of it right so you can write an arbitrary function better",
    "start": "204500",
    "end": "209850"
  },
  {
    "text": "when a picture comes in extracts the features and after that on the next stage you can push the features out and",
    "start": "209850",
    "end": "215850"
  },
  {
    "text": "it we classify what the picture is going to do so with this terminology like let",
    "start": "215850",
    "end": "221790"
  },
  {
    "start": "219000",
    "end": "276000"
  },
  {
    "text": "us look how a dag looks like so this is a typical dag so you have the couple of",
    "start": "221790",
    "end": "227550"
  },
  {
    "text": "spouts which in turn feeling into a next stage bowls which are like a three of them both one through both three and",
    "start": "227550",
    "end": "234320"
  },
  {
    "text": "once they're done with the part of the Cross thing then it goes to the next stage bowls which is the bold for and",
    "start": "234320",
    "end": "240090"
  },
  {
    "text": "bold five under the end stage bowls also capable of writing the results into another system so that's a so one idea",
    "start": "240090",
    "end": "247890"
  },
  {
    "text": "was that we could have introduced the notion of a sinks like instead of a bowl and we could have spout bolt and a sink",
    "start": "247890",
    "end": "254850"
  },
  {
    "text": "but say we wanted to kind of reduce the amount of new term knowledge that we use so instead we just decide to keep the",
    "start": "254850",
    "end": "261500"
  },
  {
    "text": "bold term but the bolt can do some kind of arbitrary processing so which means they can overlap processing as well as",
    "start": "261500",
    "end": "268850"
  },
  {
    "text": "storage the results into third party system simultaneously if that is would like to do",
    "start": "268850",
    "end": "275830"
  },
  {
    "text": "so to give a concrete example let us take a word count topology and in this",
    "start": "275830",
    "end": "281990"
  },
  {
    "start": "276000",
    "end": "372000"
  },
  {
    "text": "case what you are trying to do is you're going to count the number of distinct words on a live stream of real-time",
    "start": "281990",
    "end": "288410"
  },
  {
    "text": "tweets that are coming at you so in order to do that you need to have some kind of a tweet spout that in turn taps",
    "start": "288410",
    "end": "295550"
  },
  {
    "text": "into the to terrify holes where you can use the Twitter API to grab those tweets and once that is the tweets have been",
    "start": "295550",
    "end": "303020"
  },
  {
    "text": "tapped so the tweet has to be broken up into words somewhere so that's why you introduce this parts to eat bold where",
    "start": "303020",
    "end": "309260"
  },
  {
    "text": "the tweet coming out of the spout is given to the first tweet bowl which in turn breaks up into words then you after",
    "start": "309260",
    "end": "317330"
  },
  {
    "text": "the words are broken the words have to be given to there was some kind of a bolt called vodka bowl which actually",
    "start": "317330",
    "end": "322940"
  },
  {
    "text": "constitutes a logical client and so this is essentially a dag in the database",
    "start": "322940",
    "end": "328880"
  },
  {
    "text": "parlance I mean how many of you taken the intro databases or grad level database courses there's a notion of a",
    "start": "328880",
    "end": "335060"
  },
  {
    "text": "logical plan in the physical plant before the plan gets executed in the physical system so we essentially claim",
    "start": "335060",
    "end": "341030"
  },
  {
    "text": "this as a more of a logical plan but thing is like because of the sheer size of data that you might be getting you",
    "start": "341030",
    "end": "347060"
  },
  {
    "text": "may not be able to handle that in a single process let us say like the tweet spout and a part street board and a word",
    "start": "347060",
    "end": "352550"
  },
  {
    "text": "count bold or a single process of why each then you may not be able to handle the brunt of data in a single process",
    "start": "352550",
    "end": "359450"
  },
  {
    "text": "that's one possibility another possibility even if you have multiple process within a single machine they may",
    "start": "359450",
    "end": "364580"
  },
  {
    "text": "not be able to handle all the data that you get as well so which means your physical execution has to be very different than what you look at in the",
    "start": "364580",
    "end": "370910"
  },
  {
    "text": "logical plan so so in reality like the physical plant looks like this where you",
    "start": "370910",
    "end": "376880"
  },
  {
    "start": "372000",
    "end": "440000"
  },
  {
    "text": "have multiple number of tweets pout task and multiple number of parts to eat both task and the multiple number of word",
    "start": "376880",
    "end": "383060"
  },
  {
    "text": "count both task and this is what we call is the parallelism so you can give any number of parallelism for each one of",
    "start": "383060",
    "end": "389210"
  },
  {
    "text": "those stages so you can say that I want like 15 since of spout to eat spout and I want hundred instances of power sweet",
    "start": "389210",
    "end": "396680"
  },
  {
    "text": "bold and 200 instance of word count both depending upon how much CPU how much memory they need and all the various",
    "start": "396680",
    "end": "402229"
  },
  {
    "text": "things right so this is Ashley worth of what you call the physical plan when it gets realized in the execution stage now",
    "start": "402229",
    "end": "409300"
  },
  {
    "text": "this individual tasks are packed in some base under distributed into a big",
    "start": "409300",
    "end": "415160"
  },
  {
    "text": "cluster so that they can discover each other and start exchanging data from not",
    "start": "415160",
    "end": "420410"
  },
  {
    "text": "only from a within a single machine but across machines as well so I will go into those things a little bit detail so",
    "start": "420410",
    "end": "426050"
  },
  {
    "text": "now one of the challenges here is like given apart apart Street bolts image said a data item or a tuple which of the",
    "start": "426050",
    "end": "434060"
  },
  {
    "text": "word count bolt it should go to because you have multiple tasks right so that is a how do we do it so like there is",
    "start": "434060",
    "end": "441740"
  },
  {
    "start": "440000",
    "end": "495000"
  },
  {
    "text": "something called notion of body callers stream groupings and the first is called the shuffle grouping where it can pick",
    "start": "441740",
    "end": "449120"
  },
  {
    "text": "randomly one of those bowls downstream bowls and send it to it then",
    "start": "449120",
    "end": "455349"
  },
  {
    "text": "there is something called feels grouping where you take the trouble and examine",
    "start": "455349",
    "end": "460490"
  },
  {
    "text": "some of the attributes of the topple and based on the contents of those attributes either you do some kind of a",
    "start": "460490",
    "end": "465949"
  },
  {
    "text": "hash function or whatever it might be and accordingly direct the couple to that appropriate downstream bolt then",
    "start": "465949",
    "end": "473210"
  },
  {
    "text": "the third one is something called all grouping where you can replicate to everybody and similarly like you have",
    "start": "473210",
    "end": "479719"
  },
  {
    "text": "something called global grouping where you have multiple tasks feeding off to one single guys this is used for",
    "start": "479719",
    "end": "486340"
  },
  {
    "text": "debugging and printing and all the various stuff but ninety-nine percent of the time you have a combination of",
    "start": "486340",
    "end": "491659"
  },
  {
    "text": "shuffle grouping and fields grouping is what we end up using so so now going",
    "start": "491659",
    "end": "497120"
  },
  {
    "text": "back to the word count apology you can say that by the way like a tweet spout if it has to send the data to bar sweet",
    "start": "497120",
    "end": "504050"
  },
  {
    "text": "Bowl shuffle grouping will suffice because anybody can parse that break that tweet into words but on the other",
    "start": "504050",
    "end": "510169"
  },
  {
    "text": "hand when you go from afar street ball to a word count board you need to do feels grouping because you want the",
    "start": "510169",
    "end": "516050"
  },
  {
    "text": "words of same words to go to the same ish are the same task so that you can compute the words accurately right so",
    "start": "516050",
    "end": "522349"
  },
  {
    "text": "that's so these shuffle and fields combination achieves the results that you liked it",
    "start": "522349",
    "end": "527690"
  },
  {
    "text": "so now so I cannot use Tom so like we had a lot of issues with storm and we",
    "start": "527690",
    "end": "534030"
  },
  {
    "start": "528000",
    "end": "768000"
  },
  {
    "text": "kind of summarize around 19 or 20 issues based on a running storm at scale and",
    "start": "534030",
    "end": "540320"
  },
  {
    "text": "around for like a couple of years and that's one of the reason why we decide",
    "start": "540320",
    "end": "545370"
  },
  {
    "text": "to do heron and so some of the highlights are it performance predictability one of the various things",
    "start": "545370",
    "end": "551880"
  },
  {
    "text": "was like storm was having a cluster of its own and it had a scheduler of its",
    "start": "551880",
    "end": "557040"
  },
  {
    "text": "own and because of that like those even though it was keep track of how much",
    "start": "557040",
    "end": "563190"
  },
  {
    "text": "resources are available on each one of those machines but it never enforces some kind of resource reservation in the",
    "start": "563190",
    "end": "569310"
  },
  {
    "text": "sense like a by the way if we are given X amount of CPU and why amount of memory that's what you're supposed to use you",
    "start": "569310",
    "end": "575430"
  },
  {
    "text": "cannot go beyond that right so it couldn't enforce that and because of that like you used to Kapali multiple",
    "start": "575430",
    "end": "581730"
  },
  {
    "text": "topologies that were running and sharing some missions they used to trample all over the place and that leads to a lot",
    "start": "581730",
    "end": "587970"
  },
  {
    "text": "of challenging issues that used to happen because like doing especially the events time I",
    "start": "587970",
    "end": "594420"
  },
  {
    "text": "mean tutor is known for real time so during events time like Oscar and Super Bowl so those events there are traffic",
    "start": "594420",
    "end": "601080"
  },
  {
    "text": "spikes up quite a bit and because of that like everybody starts occupying more cpus so then when they don't have a",
    "start": "601080",
    "end": "608010"
  },
  {
    "text": "check on the amount of CPUs that we they use then they start trampling into the",
    "start": "608010",
    "end": "613110"
  },
  {
    "text": "other leading to all kind of performance issues so that is the performance predictability so next one is a improve",
    "start": "613110",
    "end": "619470"
  },
  {
    "text": "developer productivity so the debugging ability of a storm was terrible in the",
    "start": "619470",
    "end": "624870"
  },
  {
    "text": "sense like when the logs coming out of a single process which consists of several threads it's difficult to find out which",
    "start": "624870",
    "end": "631860"
  },
  {
    "text": "that actually admitted that the message unless you go through all the logs and especially if some threads are spewing",
    "start": "631860",
    "end": "637650"
  },
  {
    "text": "more logs than the others it's very hard to debug what heck is going on so that is and second aspect is like you cannot",
    "start": "637650",
    "end": "645450"
  },
  {
    "text": "even do a heap dump or some kind of what do you call the attaching your kid",
    "start": "645450",
    "end": "651150"
  },
  {
    "text": "kind of tool so that we can debug order performance profile that spout or the bolt or whatever so that you can",
    "start": "651150",
    "end": "657540"
  },
  {
    "text": "identify your perform bottleneck so then the third one is the use of manageability so we wanted to make sure",
    "start": "657540",
    "end": "663629"
  },
  {
    "text": "that the instead of running storm on a separate cluster on its own we wanted to",
    "start": "663629",
    "end": "668670"
  },
  {
    "text": "go to multi-ton cluster because as a small team we have to support the entire Twitter's needs right that means like",
    "start": "668670",
    "end": "675209"
  },
  {
    "text": "around 30 or 40 teams who are using the software or the infra so what we have to do is to make sure that we don't have to",
    "start": "675209",
    "end": "682559"
  },
  {
    "text": "maintain a separate cluster and oh the overhead that goes along with it in fact like when we were maintaining the same",
    "start": "682559",
    "end": "688529"
  },
  {
    "text": "cluster the it took around the two to three weeks to take a topology that was",
    "start": "688529",
    "end": "693899"
  },
  {
    "text": "in development going into production because of the fact that we are to go through some kind of a cap planning to",
    "start": "693899",
    "end": "699480"
  },
  {
    "text": "find out how much those missions are required for the topology and we ought to kind of send it to the cap planning",
    "start": "699480",
    "end": "705569"
  },
  {
    "text": "or the hardware department and which in turn released the hardware then we or do then somebody has to manually add those",
    "start": "705569",
    "end": "710819"
  },
  {
    "text": "missions into the cluster or data center then it has to be then some kind of entry has to be put in so that when the",
    "start": "710819",
    "end": "716999"
  },
  {
    "text": "topology kind of a describes order tells ok this is the topology ok you're",
    "start": "716999",
    "end": "722249"
  },
  {
    "text": "entitled to only do this number of machines right so because of that like it took like three weeks to two weeks in",
    "start": "722249",
    "end": "728939"
  },
  {
    "text": "order to get a topology to into production which is unacceptable so that so so what we did is like how to make it",
    "start": "728939",
    "end": "736589"
  },
  {
    "text": "easier and run it in a multi-time cluster so because Twitter maintains a huge multi-ton cluster which runs on top",
    "start": "736589",
    "end": "743220"
  },
  {
    "text": "of me shows and we have the Aurora scheduler also which I'm assuming everybody's familiar with",
    "start": "743220",
    "end": "749100"
  },
  {
    "text": "it so like which is probably around the 60 200 thousand machines and and all the",
    "start": "749100",
    "end": "755639"
  },
  {
    "text": "critical services are running on top of those clusters so if we can run it on",
    "start": "755639",
    "end": "760679"
  },
  {
    "text": "alongside of those services then we don't have to manage anything because there's one team that is managing the",
    "start": "760679",
    "end": "766829"
  },
  {
    "text": "old show so some of the design decisions for haren was it should be fully APA",
    "start": "766829",
    "end": "772259"
  },
  {
    "start": "768000",
    "end": "867000"
  },
  {
    "text": "compatible with stomp for obvious reasons because tutor a huge investment in storm and all its API and all the lot",
    "start": "772259",
    "end": "778589"
  },
  {
    "text": "of programs and the top Ollie's have been determined top of it there are a couple of frameworks that are and on top of it which in turn generates those",
    "start": "778589",
    "end": "784199"
  },
  {
    "text": "topologies automatically so the whole machinery we can lift it by changing the API so the is given is you have a AP a",
    "start": "784199",
    "end": "792300"
  },
  {
    "text": "compatible the second aspect is like a task isolation as I said we want isolation at all levels one is at the",
    "start": "792300",
    "end": "798920"
  },
  {
    "text": "individual task level remember the past week bolt and the word count ask each",
    "start": "798920",
    "end": "804120"
  },
  {
    "text": "one should be having isolation of its own and similarly like when you pack a bunch of these processes into on",
    "start": "804120",
    "end": "810410"
  },
  {
    "text": "container kind of stuff then we need isolation of the container level then the individual topology should have",
    "start": "810410",
    "end": "816180"
  },
  {
    "text": "isolation so that they don't share any kind of state with the they're more so",
    "start": "816180",
    "end": "821220"
  },
  {
    "text": "that it is even killing off a topology does not affect anybody else so then finally like we decided like we",
    "start": "821220",
    "end": "829620"
  },
  {
    "text": "used mainstream languages like C++ Java and Python compared to some of the functional languages that storm was",
    "start": "829620",
    "end": "835620"
  },
  {
    "text": "employing in well they wrote the initial version which is something called closure I don't know that you have we",
    "start": "835620",
    "end": "841410"
  },
  {
    "text": "heard about the language closure so it was written enclosure and the closure and interaction with JVM leads to lot of",
    "start": "841410",
    "end": "847890"
  },
  {
    "text": "j GC shoes and then we decide what k we under the tools for closure was not that",
    "start": "847890",
    "end": "854190"
  },
  {
    "text": "great so which means as a developer productivity is not that fast enough to",
    "start": "854190",
    "end": "859320"
  },
  {
    "text": "deliver any new stuff and even fixing bugs and all so instead we'll stick with mainstream languages so that we can get",
    "start": "859320",
    "end": "864630"
  },
  {
    "text": "enough programmers so so with that are all those decent",
    "start": "864630",
    "end": "869700"
  },
  {
    "start": "867000",
    "end": "924000"
  },
  {
    "text": "ations the heron architecture looks very simple I mean the one of the systems principle is keep it simple so that's",
    "start": "869700",
    "end": "875820"
  },
  {
    "text": "what we did like so we got away from the fact that a we don't want anymore schedulers because already the lot of",
    "start": "875820",
    "end": "882600"
  },
  {
    "text": "communities among run in schedulers that is working Miso's yawn and Cuban attics so there is enough momentum behind the",
    "start": "882600",
    "end": "889740"
  },
  {
    "text": "scheduler coming it is so it reinventing the wheel for doing another scheduler just not in our doesn't make any sense",
    "start": "889740",
    "end": "896790"
  },
  {
    "text": "so instead we piggyback on existing scheduler that could you be scheduled it could be anything and as long as some",
    "start": "896790",
    "end": "903240"
  },
  {
    "text": "that are some mapping is needed to map hair and into that scheduler then we are",
    "start": "903240",
    "end": "908579"
  },
  {
    "text": "okay so which means when you submit a topology it looks like at another job to the scheduler and the scheduler in turn",
    "start": "908579",
    "end": "914730"
  },
  {
    "text": "schedules the various topology based on the resource requirements of that apology so that's what it is so now",
    "start": "914730",
    "end": "920910"
  },
  {
    "text": "let's drill down detail into more how they knew your topology looks like so we",
    "start": "920910",
    "end": "927390"
  },
  {
    "start": "924000",
    "end": "978000"
  },
  {
    "text": "so right from the start we exploited the container it's quite about the to the container level we isolated several",
    "start": "927390",
    "end": "934680"
  },
  {
    "text": "containers and the at the high level there are two different kind of containers one is the master container",
    "start": "934680",
    "end": "940020"
  },
  {
    "text": "and another one is called data containers so the ones that you see in the bottom or data containers and the",
    "start": "940020",
    "end": "946470"
  },
  {
    "text": "one at the top where the topology master is running is called the master container so right from the start we",
    "start": "946470",
    "end": "952080"
  },
  {
    "text": "took advantage of the container architecture today we use the C group but there have been couple of PR request",
    "start": "952080",
    "end": "957990"
  },
  {
    "text": "to move to docker kind of containers and that is in the works and",
    "start": "957990",
    "end": "963560"
  },
  {
    "text": "so soon we will have a way by which you can package this as a docker container and the it will be dispatched and these",
    "start": "963560",
    "end": "971220"
  },
  {
    "text": "takut containers can discover each I itself in the mechanism that I'll describe just in a minute then",
    "start": "971220",
    "end": "976350"
  },
  {
    "text": "automatically everybody should start exchanging data so when a topology gets launched in the first place the master",
    "start": "976350",
    "end": "982200"
  },
  {
    "start": "978000",
    "end": "1469000"
  },
  {
    "text": "container is the first container that gets started and the master container contains something called the topology",
    "start": "982200",
    "end": "987570"
  },
  {
    "text": "master which is the first process that comes up and it looks at the dag and how",
    "start": "987570",
    "end": "992730"
  },
  {
    "text": "much it sources it needs and everything and also runs something called packing algorithm so which means like hey I need",
    "start": "992730",
    "end": "999660"
  },
  {
    "text": "total of ten containers how do I pack these parts treat both tasks and the word count bowl task and the tweet spout",
    "start": "999660",
    "end": "1007160"
  },
  {
    "text": "pass into one into multiple containers and so that everybody knows where that",
    "start": "1007160",
    "end": "1013610"
  },
  {
    "text": "container they are running on so so that is called the packing algorithm so once the packing is done then the topology",
    "start": "1013610",
    "end": "1020180"
  },
  {
    "text": "master in turn will request the scheduler hey by the way I needed 10 containers and each of the container should be of four cpus and probably like",
    "start": "1020180",
    "end": "1027770"
  },
  {
    "text": "say a couple of gig of memory so please give me those content containers and automatically scheduler will tell you",
    "start": "1027770",
    "end": "1033110"
  },
  {
    "text": "because these are the machines on which the containers will be launched and and the schedule also launches those",
    "start": "1033110",
    "end": "1040610"
  },
  {
    "text": "containers and once it launches those containers then what happens is like the first process that comes up is what we",
    "start": "1040610",
    "end": "1047630"
  },
  {
    "text": "call this a stream manager and within each data container and so the topology master before launches those containers",
    "start": "1047630",
    "end": "1054679"
  },
  {
    "text": "of the are even giving the requested scheduler it advertises itself saying that hey by the way if anybody up my",
    "start": "1054679",
    "end": "1061429"
  },
  {
    "text": "topology wants to reach out to me this is the host and port that I am listening on so that location is a returning the",
    "start": "1061429",
    "end": "1068120"
  },
  {
    "text": "zookeeper cluster which acts as a synchronization mechanism then once everybody the data containers and the",
    "start": "1068120",
    "end": "1074240"
  },
  {
    "text": "corresponding stream manager process comes up the stream manager goes and looks in the zookeeper cluster hey by",
    "start": "1074240",
    "end": "1080120"
  },
  {
    "text": "the way various met apology master so the moment it discovers where my topology master is it contacts the",
    "start": "1080120",
    "end": "1085880"
  },
  {
    "text": "topology master hey by the way I am container X and I'm ready so like a kind",
    "start": "1085880",
    "end": "1091730"
  },
  {
    "text": "of a hollow message and registers itself and so the topology master waits until every container that has been spawned",
    "start": "1091730",
    "end": "1098870"
  },
  {
    "text": "has been reported in and once everybody reports in then it constitutes something called a physical plant I mean the",
    "start": "1098870",
    "end": "1104510"
  },
  {
    "text": "logical plan is the diag which is already available on the zookeeper then once every container reports in they",
    "start": "1104510",
    "end": "1110029"
  },
  {
    "text": "constitute something called a physical plan and that physical plan consists of all the hosts and the port information",
    "start": "1110029",
    "end": "1117740"
  },
  {
    "text": "where the stream manager is listening because you have to have a way by which the stream managers on different data",
    "start": "1117740",
    "end": "1125149"
  },
  {
    "text": "containers how to discover each other so that the data exchange can happen because it's all the data is flowing",
    "start": "1125149",
    "end": "1130730"
  },
  {
    "text": "through this topology or the diag right so which means some parts of the depaul you'll be running on a data container",
    "start": "1130730",
    "end": "1136580"
  },
  {
    "text": "one and some parts will be running on data container two and so on and because of that like the data has to be moved",
    "start": "1136580",
    "end": "1142100"
  },
  {
    "text": "around and in order to move around the stream managers have to discover each other you can think about team manager",
    "start": "1142100",
    "end": "1147110"
  },
  {
    "text": "is more of a sophisticated data router info on some form where it has to route",
    "start": "1147110",
    "end": "1152299"
  },
  {
    "text": "the data to appropriate containers so once the",
    "start": "1152299",
    "end": "1157720"
  },
  {
    "text": "steam and it has to discover each other so that's why it forms the top olly murs reforms a physical plant and the moment",
    "start": "1157720",
    "end": "1163909"
  },
  {
    "text": "the physical plant is formed and it's towards the zookeeper claw that physical plant into the zookeeper cluster as well",
    "start": "1163909",
    "end": "1169880"
  },
  {
    "text": "and then it kind of broadcast that physical plant to all the data",
    "start": "1169880",
    "end": "1175880"
  },
  {
    "text": "containers are the stream managers so once everybody gets third physical plan they know the roles in the sense like",
    "start": "1175880",
    "end": "1182179"
  },
  {
    "text": "remember the the stream manager will connect to each other right and also the instances so which is officially the",
    "start": "1182179",
    "end": "1188870"
  },
  {
    "text": "very where task is running like for example a part Street bolt and what you call the twitter-like word count bolts",
    "start": "1188870",
    "end": "1197150"
  },
  {
    "text": "and all the various bold are the individual tasks who are running us running as a single process this is",
    "start": "1197150",
    "end": "1202870"
  },
  {
    "text": "there is a huge distinction between storm and the Heron here is storm used to run all these individual tasks as a",
    "start": "1202870",
    "end": "1210100"
  },
  {
    "text": "thread within a worker within a context of a single worker process and that led",
    "start": "1210100",
    "end": "1215750"
  },
  {
    "text": "to a lot of issues and because that we wanted our isolation and various task level contains a level as well as",
    "start": "1215750",
    "end": "1221360"
  },
  {
    "text": "topology level so we made it each one the task as an individual process on",
    "start": "1221360",
    "end": "1226640"
  },
  {
    "text": "each one so that you can connect to it and query its memory or you can keep",
    "start": "1226640",
    "end": "1231650"
  },
  {
    "text": "dump it or you can even do a performance profiling of the task without affecting anybody else and also you can do fine",
    "start": "1231650",
    "end": "1237650"
  },
  {
    "text": "grained resource allocation as well because when you have a one single process how much resource you want to",
    "start": "1237650",
    "end": "1243080"
  },
  {
    "text": "allocate it is unknown to you unless you know what are the threads and other things are mapping which is controlled",
    "start": "1243080",
    "end": "1248690"
  },
  {
    "text": "by packing algorithms in a here you can tell like it this process requires the smudge and depending upon how the",
    "start": "1248690",
    "end": "1255590"
  },
  {
    "text": "packing is goes up you do light up to the overall container resource limited resource that you need for the container",
    "start": "1255590",
    "end": "1261260"
  },
  {
    "text": "and so so so now once the physical plan is given to all the stream managers and",
    "start": "1261260",
    "end": "1268130"
  },
  {
    "text": "they know how to connect to each other so they will form a what do you call a fully connected graph so that they can start exchanging data so so that's how",
    "start": "1268130",
    "end": "1275300"
  },
  {
    "text": "it starts executing now let us look at the physic failure scenarios because it's all running in a distributed system",
    "start": "1275300",
    "end": "1280480"
  },
  {
    "text": "process failure machine failures are pretty common so now let's take the example of the master container failure",
    "start": "1280480",
    "end": "1286640"
  },
  {
    "text": "so when the master container fails there's nothing surf acting because",
    "start": "1286640",
    "end": "1292640"
  },
  {
    "text": "master container is more on the control path not in the data path so when the master container dies so the data will",
    "start": "1292640",
    "end": "1298700"
  },
  {
    "text": "continue the exchange on the data containers accept the fact that like the topology master will be",
    "start": "1298700",
    "end": "1306620"
  },
  {
    "text": "rescheduled in a different node by the scheduler because schedule it comes to know if that container died so I have to",
    "start": "1306620",
    "end": "1313010"
  },
  {
    "text": "give a different machine and different in a container but of course is the same container ID so moment the topology",
    "start": "1313010",
    "end": "1318380"
  },
  {
    "text": "master comes in a different container or a different machine so the first order of business it will do is look for in",
    "start": "1318380",
    "end": "1326390"
  },
  {
    "text": "the zookeeper cluster where my physical plant logical plan everything is so think about the zookeeper incessantly",
    "start": "1326390",
    "end": "1331910"
  },
  {
    "text": "keeping the state of the Martian container so that when the master container fails and comes back it can",
    "start": "1331910",
    "end": "1337160"
  },
  {
    "text": "rediscover its state and that state includes the logical plan physical plant and the execution state that consists of",
    "start": "1337160",
    "end": "1342980"
  },
  {
    "text": "who launches to the poly what time they launched and what role they launched for tracking purposes and so that takes care",
    "start": "1342980",
    "end": "1349130"
  },
  {
    "text": "of the master the container failure I mean another option is you could run multiple must master containers with the",
    "start": "1349130",
    "end": "1355790"
  },
  {
    "text": "kind of a master-slave relationship which can be elected by a zookeeper also what thing is in practice that we found",
    "start": "1355790",
    "end": "1361340"
  },
  {
    "text": "out as long as we can quickly scheduler another container you're fine I mean in",
    "start": "1361340",
    "end": "1366740"
  },
  {
    "text": "practice we have not seen any moisture content of failure affecting and cost you a lot of incidents so so we",
    "start": "1366740",
    "end": "1374000"
  },
  {
    "text": "kind of stuck with it so now let us look at the failure of a data container so whenever data",
    "start": "1374000",
    "end": "1380630"
  },
  {
    "text": "container fails to paly Marshall will come to know because there's a keepalive messages that goes from the stream",
    "start": "1380630",
    "end": "1386150"
  },
  {
    "text": "manager to the topology master so the moment that the connection goes down or the keeper lives do not come any more",
    "start": "1386150",
    "end": "1392030"
  },
  {
    "text": "then tow poly master knows that the container is dead so then another",
    "start": "1392030",
    "end": "1397490"
  },
  {
    "text": "container gets spawned off by the scheduler or based on the request form topology master so then that container",
    "start": "1397490",
    "end": "1402920"
  },
  {
    "text": "will come up and the first order of business is rediscover where my topology master is and send my host and port then",
    "start": "1402920",
    "end": "1410270"
  },
  {
    "text": "once the poly must forget said it constitutes a new physical plant because one guy has changed because everybody has to connect to this guy then it saves",
    "start": "1410270",
    "end": "1417440"
  },
  {
    "text": "it into the again the zookeeper was a physical plant state then all the got all the guys who fail the connection to",
    "start": "1417440",
    "end": "1425060"
  },
  {
    "text": "that the world guy who died we connect to the new guy again data will start",
    "start": "1425060",
    "end": "1430160"
  },
  {
    "text": "exchange and during the time there is some amount of buffering going on because if the container is",
    "start": "1430160",
    "end": "1435400"
  },
  {
    "text": "average-sized because in a 24 core mission the average size is somewhere around 4 260 so if you do ask after",
    "start": "1435400",
    "end": "1442490"
  },
  {
    "text": "mission order three quarter or a full machine it's hard for the scheduler to get that mission quickly because the",
    "start": "1442490",
    "end": "1449390"
  },
  {
    "text": "time to allocate becomes longer the more the resources that he asked for an upper container basis if it's an average sized",
    "start": "1449390",
    "end": "1455150"
  },
  {
    "text": "container we won't even see a glitch in the failure in the recomp and the coming of back of the container and all the",
    "start": "1455150",
    "end": "1460850"
  },
  {
    "text": "physical plan and all the reasoning it's pretty quick we probably will clocked it with a less than 20 seconds or so so any",
    "start": "1460850",
    "end": "1467630"
  },
  {
    "text": "questions so far on this topology architecture yes mm-hmm",
    "start": "1467630",
    "end": "1473650"
  },
  {
    "start": "1469000",
    "end": "1632000"
  },
  {
    "text": "so it is the stream manager has a direct connection to the poly master using a TCP connection and that connection is a",
    "start": "1473650",
    "end": "1480290"
  },
  {
    "text": "control pod connection and then periodically it sends a hello messages that's it",
    "start": "1480290",
    "end": "1486549"
  },
  {
    "text": "that is hardly like because it doesn't every few seconds so that is a",
    "start": "1486640",
    "end": "1492200"
  },
  {
    "text": "configurable element you can control it how much you want to because of you awfully obviously if the more the",
    "start": "1492200",
    "end": "1497870"
  },
  {
    "text": "interval that you configure the more that failure deduction will go so it's a thing I trade off the do to play with",
    "start": "1497870",
    "end": "1503660"
  },
  {
    "text": "but that traffic is very small so yes",
    "start": "1503660",
    "end": "1508930"
  },
  {
    "text": "service discovery so the zookeeper is a tionally uses a service discovery for example for everybody I all the",
    "start": "1509920",
    "end": "1516140"
  },
  {
    "text": "containers want to discover the topology master where it is running the topology masters advertise to itself on the",
    "start": "1516140",
    "end": "1521180"
  },
  {
    "text": "zookeeper and the zookeeper us a tree like hierarchy right and when one of the",
    "start": "1521180",
    "end": "1526220"
  },
  {
    "text": "hierarchy it will have a topology name since everybody knows what apology name that they are running on you keen on",
    "start": "1526220",
    "end": "1531620"
  },
  {
    "text": "that part then you know whether to poly master is yes yes mm-hmm",
    "start": "1531620",
    "end": "1539990"
  },
  {
    "text": "so like what happens is like go in the data contained a ghost in like the tuples are being buffered right so you",
    "start": "1544020",
    "end": "1551140"
  },
  {
    "text": "have to kind of semantic so nice or at most once which is the best effort like",
    "start": "1551140",
    "end": "1556450"
  },
  {
    "text": "a regular IP kind of this thing we keep pushing the data if some failure happens",
    "start": "1556450",
    "end": "1561790"
  },
  {
    "text": "you're responsible for whatever you are doing then there is another semantics got at least once so which means if some",
    "start": "1561790",
    "end": "1568210"
  },
  {
    "text": "failure happens and the acknowledgement did not come back on time then the tuples will be replayed so that the computation happens again right",
    "start": "1568210",
    "end": "1576630"
  },
  {
    "text": "so so the hell as I said Heron has been running in production for more than 200",
    "start": "1576630",
    "end": "1581650"
  },
  {
    "text": "years and these are some of the topology shapes and that we run I mean it is",
    "start": "1581650",
    "end": "1586660"
  },
  {
    "text": "ranges from something very simple to all the way something complex like the simple topologies are typically people",
    "start": "1586660",
    "end": "1593200"
  },
  {
    "text": "write it directly on the storm API but the complex topologies like this are being generated by a higher level",
    "start": "1593200",
    "end": "1600190"
  },
  {
    "text": "framework that we use called something bird which is you write it in dsl and that dsl spits out a",
    "start": "1600190",
    "end": "1605940"
  },
  {
    "text": "topology and some of the topologies look like this so and as the largest topology",
    "start": "1605940",
    "end": "1610990"
  },
  {
    "text": "runs around eight hundred containers or so so that is equivalent to probably probably like 140 missions I believe so",
    "start": "1610990",
    "end": "1619240"
  },
  {
    "text": "the single job runs on that but since it is running on a multi-ton cluster it could be running on Aiden admission",
    "start": "1619240",
    "end": "1624880"
  },
  {
    "text": "assuming each container maps into one single machine so so the it has been tested and scale also it just runs so",
    "start": "1624880",
    "end": "1631950"
  },
  {
    "text": "so officially like has been production for more than two year to one half years actually then I cannot give out the",
    "start": "1631950",
    "end": "1638740"
  },
  {
    "text": "actual numbers but yeah like some of the jobs varies from one stage to two stage 10 stages the one thing that we did is",
    "start": "1638740",
    "end": "1645610"
  },
  {
    "text": "after moving from storm to hair and one of the biggest and things that we got is we got a 3x reduction in course and",
    "start": "1645610",
    "end": "1651130"
  },
  {
    "text": "memory so we made the system so efficient in terms of moving data and",
    "start": "1651130",
    "end": "1656140"
  },
  {
    "text": "all the various talk so we saw a huge reduction in course I mean we were",
    "start": "1656140",
    "end": "1661840"
  },
  {
    "text": "running around around 3500 nodes or something around 4000 nodes and storm we",
    "start": "1661840",
    "end": "1667300"
  },
  {
    "text": "brought it down to less than thousand five nodes like so that way like it's a huge savings from capex as well as the",
    "start": "1667300",
    "end": "1674470"
  },
  {
    "text": "operational costs of it so so that was one of the highlights of this thing and",
    "start": "1674470",
    "end": "1680990"
  },
  {
    "text": "so some of the use cases involves broadly like a real time etl I mean for",
    "start": "1680990",
    "end": "1687780"
  },
  {
    "start": "1681000",
    "end": "1799000"
  },
  {
    "text": "extract transform and load so where the data is transformed and loaded into some other destination systems the real time",
    "start": "1687780",
    "end": "1694530"
  },
  {
    "text": "bi I mean VI is recently like where you have one of the application is like when",
    "start": "1694530",
    "end": "1699540"
  },
  {
    "text": "people are interacting with Twitter from a website or even mobile phones or",
    "start": "1699540",
    "end": "1704730"
  },
  {
    "text": "different countries so you can continuously view them at a second granularity where the traffic is spiking",
    "start": "1704730",
    "end": "1711420"
  },
  {
    "text": "we said coming from UK or it's coming from Belarus and all the various things and also broken down by what operating",
    "start": "1711420",
    "end": "1717780"
  },
  {
    "text": "system people use an iphone or vs android based on region so you can see",
    "start": "1717780",
    "end": "1722790"
  },
  {
    "text": "the entire breakdown of these various dimensions we used as a twitter command center where you keep wanting monitoring",
    "start": "1722790",
    "end": "1728910"
  },
  {
    "text": "all these crafts so that's pretty cool and we also do a lot of spam detection product abuse all the various things",
    "start": "1728910",
    "end": "1735510"
  },
  {
    "text": "also on top on top of her and where they build some kind of a machine learning model and all the various things and the",
    "start": "1735510",
    "end": "1742220"
  },
  {
    "text": "acid data coming in they used to check against that model whether there is any",
    "start": "1742220",
    "end": "1747450"
  },
  {
    "text": "violation or not so that is all runs on top of her and the trend computations the trending hashtag trends and all the",
    "start": "1747450",
    "end": "1753600"
  },
  {
    "text": "various trends that suckers they're all competed on top of her own as well and in addition to that some of the ML model",
    "start": "1753600",
    "end": "1760230"
  },
  {
    "text": "buildings and model enhancements and model all the things also learn on top of heaven and as I said media like",
    "start": "1760230",
    "end": "1767550"
  },
  {
    "text": "videos and pictures if they want to classify based on certain things they",
    "start": "1767550",
    "end": "1772620"
  },
  {
    "text": "also pick up those media and extract the features and",
    "start": "1772620",
    "end": "1778650"
  },
  {
    "text": "classify those features based on whatever the algorithms that you want to do and third one is a it also being",
    "start": "1778650",
    "end": "1784200"
  },
  {
    "text": "heavily used for ops where the data from the missions and the bigger clusters are continuously being collected and we have",
    "start": "1784200",
    "end": "1790470"
  },
  {
    "text": "proactively look for if there is an emission failure is out the impending machine failures or so these are the",
    "start": "1790470",
    "end": "1795930"
  },
  {
    "text": "broad some in categories of uses so like I wanted to highlight a couple",
    "start": "1795930",
    "end": "1802350"
  },
  {
    "start": "1799000",
    "end": "1847000"
  },
  {
    "text": "of things that we did the hair and reasonably right so as you all know the",
    "start": "1802350",
    "end": "1807750"
  },
  {
    "text": "big data system is content really evolving in the sense like a tomorrow the new scheduler comes up and then the",
    "start": "1807750",
    "end": "1814450"
  },
  {
    "text": "comment that community goes away then another scheduler comes up similarly like today how to boost in fashion and",
    "start": "1814450",
    "end": "1820690"
  },
  {
    "text": "tomorrow something else will be in fashion because how do please being rewritten by Facebook in C++ because",
    "start": "1820690",
    "end": "1826480"
  },
  {
    "text": "they want a high performance or whatever right if they're open sources that becomes stranded so it's in the",
    "start": "1826480",
    "end": "1831640"
  },
  {
    "text": "constantly changing environment of several components how do you expect the code to be in tag and we can move a cord",
    "start": "1831640",
    "end": "1839260"
  },
  {
    "text": "on top of these guys very quickly and easy enough so one of the thing is like so what we did is like in order for",
    "start": "1839260",
    "end": "1847780"
  },
  {
    "start": "1847000",
    "end": "1880000"
  },
  {
    "text": "inspiration we looked at micro kernels how many of you know Mike recalls awesome okay so like we I bet that the",
    "start": "1847780",
    "end": "1856180"
  },
  {
    "text": "original paper and con mock I mean if you remember this from it used to be from CMU from early 90s so I said like",
    "start": "1856180",
    "end": "1863770"
  },
  {
    "text": "they took the monolithic kernel operating system and stripped it down and say that okay this is the basic",
    "start": "1863770",
    "end": "1869230"
  },
  {
    "text": "thing that we need the rest of them can be run as a process like even the pages and the virtual memory was running as",
    "start": "1869230",
    "end": "1874660"
  },
  {
    "text": "pages as long as the colonel has the basic IPC and the scheduling and other functionality so I said like why can't",
    "start": "1874660",
    "end": "1881920"
  },
  {
    "start": "1880000",
    "end": "2080000"
  },
  {
    "text": "we do the same thing here so if you look at the streaming systems they are",
    "start": "1881920",
    "end": "1886930"
  },
  {
    "text": "nothing but data movers I mean they move data quickly as fast as can and a scheduler computation and this",
    "start": "1886930",
    "end": "1892630"
  },
  {
    "text": "computation is moved to the next stages so which means the basic in inter our",
    "start": "1892630",
    "end": "1897820"
  },
  {
    "text": "infra IPC is a constant is needed and also like what regular scheduler that",
    "start": "1897820",
    "end": "1905020"
  },
  {
    "text": "you are drawn on because schedule is also changing continuously how's the resource requested and the scheduling and monitoring those containers and all",
    "start": "1905020",
    "end": "1911800"
  },
  {
    "text": "the various things so the abstract allowed to do what we call as a service provider interval like a regular API we",
    "start": "1911800",
    "end": "1918280"
  },
  {
    "text": "invented something called spi with the service provider interface and similarly the state manager which is the state",
    "start": "1918280",
    "end": "1924610"
  },
  {
    "text": "synchronization point similar to zookeeper that also be is pluggable because today you might be using zookeeper tomorrow you've somebody will",
    "start": "1924610",
    "end": "1931390"
  },
  {
    "text": "write based on something draft then they might have a different consensus algorithm so we want her to abstract",
    "start": "1931390",
    "end": "1937690"
  },
  {
    "text": "that or not that also out so so we have a state manager spi under scheduler spi",
    "start": "1937690",
    "end": "1942970"
  },
  {
    "text": "under the basic thing so now all of them became a process on top of it so which means I well established protocol using",
    "start": "1942970",
    "end": "1949900"
  },
  {
    "text": "TCP and some kind of product of based protocols everybody is interacting in a",
    "start": "1949900",
    "end": "1955450"
  },
  {
    "text": "different way I mean through thy pcs so if the body must interacts with team manager there's a well established protocol as long as you confirm with",
    "start": "1955450",
    "end": "1962020"
  },
  {
    "text": "this protocol you're fine so the solo also allows a lot of flexibility where the polyesters are able to map quickly",
    "start": "1962020",
    "end": "1968830"
  },
  {
    "text": "to me so slasher order which is our production environment and we are able to quickly max what was able to move",
    "start": "1968830",
    "end": "1975730"
  },
  {
    "text": "that to run on yon very quickly within just two or three weeks and then HPC community which is on a high",
    "start": "1975730",
    "end": "1983230"
  },
  {
    "text": "performance community they use a scheduler called the slurm scheduler they were able to move in a couple of weeks as well and then after that the",
    "start": "1983230",
    "end": "1990070"
  },
  {
    "text": "some people asked for me so / marathon okay that's also is fine so so like the scheduler writing became so easy so and",
    "start": "1990070",
    "end": "1997390"
  },
  {
    "text": "it kind of proved itself based on various scheduler mappings and so then also like going to a more process like",
    "start": "1997390",
    "end": "2004380"
  },
  {
    "text": "architecture so for example the instance that I mentioned where your tasks are actually running so what happens if we",
    "start": "2004380",
    "end": "2011250"
  },
  {
    "text": "want to do tomorrow right the your topologies in Python right so then i can",
    "start": "2011250",
    "end": "2017490"
  },
  {
    "text": "write a Python instant that goes along with the API that instance will none the",
    "start": "2017490",
    "end": "2022740"
  },
  {
    "text": "native Python code in a Python vm so we can write the instance in Python without",
    "start": "2022740",
    "end": "2028380"
  },
  {
    "text": "affecting anybody else that's what we did for byte an API and some guys at Adobe wanted to write a C++ topology I",
    "start": "2028380",
    "end": "2036480"
  },
  {
    "text": "said go for it as long as he conformed to this protocol of instant then you can run into your native because the reason",
    "start": "2036480",
    "end": "2041520"
  },
  {
    "text": "why they wanted to go for a C++ was they had some legacy code which they wanted to wrap it up on in addition to that",
    "start": "2041520",
    "end": "2048000"
  },
  {
    "text": "they also wanted our non GC environment where it's running predictively right so because of those two",
    "start": "2048000",
    "end": "2054230"
  },
  {
    "text": "main reason they want to write it in C++ but so the architecture allows it and tomorrow if I wanted to change the",
    "start": "2054230",
    "end": "2060179"
  },
  {
    "text": "stream manager to work on top of InfiniBand or even working on a shared memory kind of architecture yeah you can",
    "start": "2060179",
    "end": "2067050"
  },
  {
    "text": "just spin up write a difference team manager and register it then automatically will spin it up so that",
    "start": "2067050",
    "end": "2072419"
  },
  {
    "text": "way this gave a very flexible architecture and the environment a kind of surrounding it",
    "start": "2072419",
    "end": "2078790"
  },
  {
    "text": "is continuously changing so so we published we are finished a paper on",
    "start": "2078790",
    "end": "2084128"
  },
  {
    "text": "this and submitted to ICD international conference on data engineering it should be available anytime like probably early",
    "start": "2084129",
    "end": "2091599"
  },
  {
    "text": "in the first quarter of so anyway so the plug and play component as environment changes code does not change and so",
    "start": "2091599",
    "end": "2099160"
  },
  {
    "text": "multi-language instances as I mentioned you can write Python in C++ and all the various stuff and you can change the",
    "start": "2099160",
    "end": "2104380"
  },
  {
    "text": "stream manager depending upon the transport or even crossing semantics at least once at most one's exactly ones",
    "start": "2104380",
    "end": "2109990"
  },
  {
    "text": "all kind of semantics and finally this one of the another important thing is it buys us a lot of",
    "start": "2109990",
    "end": "2115660"
  },
  {
    "text": "velocity like the so the guy who's developing stream manager word says the guy who's developing instant they can",
    "start": "2115660",
    "end": "2120880"
  },
  {
    "text": "run off independently and go out the faster velocity and they contested in their own environment and everything",
    "start": "2120880",
    "end": "2126220"
  },
  {
    "text": "before they decide okay this is ready to go out so there's no highly intertwined this thing so that helps a lot even if",
    "start": "2126220",
    "end": "2133480"
  },
  {
    "text": "you break one thing doesn't mean that the other thing is broken right so so I mean like so it actually runs on",
    "start": "2133480",
    "end": "2141070"
  },
  {
    "start": "2137000",
    "end": "2187000"
  },
  {
    "text": "their our development environment we have this notion of a local scheduler which we wrote so that it can run on a",
    "start": "2141070",
    "end": "2146530"
  },
  {
    "text": "single laptop and so we have the notion of an uploader where the jars and other job execution code has to be uploaded",
    "start": "2146530",
    "end": "2153670"
  },
  {
    "text": "right so that also uses a local file system and that's our primary development environment and the miso /",
    "start": "2153670",
    "end": "2159280"
  },
  {
    "text": "otherwise testing also we use zookeeper and we tested with the HDFS uploaders",
    "start": "2159280",
    "end": "2164530"
  },
  {
    "text": "very upload the job test sheer face then we have this the production environment is purely in Aurora the zookeeper is the",
    "start": "2164530",
    "end": "2171790"
  },
  {
    "text": "state manager and the packet uploader which allows you to do versioning management where you hear a job they are",
    "start": "2171790",
    "end": "2178599"
  },
  {
    "text": "uploading several times it keeps a very a version of it so that if something happens in the new deploy you can roll",
    "start": "2178599",
    "end": "2184420"
  },
  {
    "text": "back so those kind of things you can do so now I'm going to go into what we call",
    "start": "2184420",
    "end": "2191140"
  },
  {
    "text": "like so even though it is returning this micro kernel like a kind of architecture",
    "start": "2191140",
    "end": "2196839"
  },
  {
    "text": "did we sacrifice performance or what so that is why I wanted to bring this up like even though we went to a more of a",
    "start": "2196839",
    "end": "2204010"
  },
  {
    "text": "process model as compared to a thread model did we lose performance because of that or we are able to gain so this is",
    "start": "2204010",
    "end": "2210910"
  },
  {
    "text": "what it gives them more insight about what actually the Costas so in order to",
    "start": "2210910",
    "end": "2215920"
  },
  {
    "text": "do this experiment we had a even spout which is kind of tapping into one of the biggest data sources within Twitter and",
    "start": "2215920",
    "end": "2222870"
  },
  {
    "text": "then it sends it to the aggregation board where events are aggregated so the way this thing happening is the intake",
    "start": "2222870",
    "end": "2229540"
  },
  {
    "text": "is around 60 to 100 million tuples per minute and then it filters down to eight to 12 million tuples per minute and then",
    "start": "2229540",
    "end": "2237100"
  },
  {
    "text": "after that there is a flat map operation where breaks are couples into like exponent like 5x so which means like the",
    "start": "2237100",
    "end": "2243640"
  },
  {
    "text": "8 to 12 becomes 40 to 60 million then that's what emitted out of the even spouts the spout is doing all this logic",
    "start": "2243640",
    "end": "2250420"
  },
  {
    "text": "of absorbing later filtering as well as expanding the topple and sending it out and",
    "start": "2250420",
    "end": "2255870"
  },
  {
    "text": "the aggregate both takes the incoming tuples and it aggregates based",
    "start": "2255870",
    "end": "2262540"
  },
  {
    "text": "on every second then outputs into red is and that our rate is around 25 to 40 2",
    "start": "2262540",
    "end": "2268450"
  },
  {
    "text": "million topple per minute and so this is the overall the resource",
    "start": "2268450",
    "end": "2274270"
  },
  {
    "text": "consumed was like a radius we dedicated a machine for reading which is 24 course and but the actual quotes that were used",
    "start": "2274270",
    "end": "2280300"
  },
  {
    "text": "was around two to four and of course it admission content of 48 gb now with",
    "start": "2280300",
    "end": "2286240"
  },
  {
    "text": "Haren which is running on the multi-tenant cluster the we requested course of 120 codes that is",
    "start": "2286240",
    "end": "2292110"
  },
  {
    "text": "approximately around five machines and the actual you say that we found was",
    "start": "2292110",
    "end": "2297490"
  },
  {
    "text": "around 30 to 50 cores and the memory requested was 200 but actually we use",
    "start": "2297490",
    "end": "2302590"
  },
  {
    "text": "just only 180 gb now when you break down where the heck the whole usage of the",
    "start": "2302590",
    "end": "2308980"
  },
  {
    "text": "course is the course are going so we found out like this was kind of an intuitive the spout instances where the",
    "start": "2308980",
    "end": "2316090"
  },
  {
    "text": "guys were taking a lot of time like around eighty four percent of the whole CPUs out of the 30 to 50 core usage it's",
    "start": "2316090",
    "end": "2322870"
  },
  {
    "text": "eighty-four percent went into spouts and the bolts just took around nine percent",
    "start": "2322870",
    "end": "2328390"
  },
  {
    "text": "and the Heron is overhead it's just seven percent which is hardly like probably a few codes out of the 30 to 50",
    "start": "2328390",
    "end": "2335620"
  },
  {
    "text": "codes which is not much so we were curious to find out like why the source powder is taking too long so we broke",
    "start": "2335620",
    "end": "2342550"
  },
  {
    "text": "down the whole thing we figure out like the DC relation cost is sixty-three percent so essentially",
    "start": "2342550",
    "end": "2348579"
  },
  {
    "text": "the data taking out of something like a Kafka or a distributed log that we have",
    "start": "2348579",
    "end": "2353640"
  },
  {
    "text": "taking the data out and the deserialising because we use thrift to do represent the data and deserializing",
    "start": "2353640",
    "end": "2360789"
  },
  {
    "text": "takes the whole amount of cost and partly because we found out our data is more nested I mean it's deeply nested",
    "start": "2360789",
    "end": "2367240"
  },
  {
    "text": "from around six to seven levels of nesting and of course we need to have or some kind of a way for some kind of data",
    "start": "2367240",
    "end": "2373660"
  },
  {
    "text": "evolution or a schema the evolution so that we don't break down all the application that were returned with the",
    "start": "2373660",
    "end": "2378730"
  },
  {
    "text": "older schema right so we that is a necessary thing but so the beers kind of",
    "start": "2378730",
    "end": "2385329"
  },
  {
    "text": "appalled to see that the deceleration cost is the major component of it the next aspect is like getting there",
    "start": "2385329",
    "end": "2392789"
  },
  {
    "text": "they're out of Kafka it's all that took another twenty five percent of the time because of the way they have written the",
    "start": "2392789",
    "end": "2398289"
  },
  {
    "text": "I traitors and all the various thing and of course the pot and filtering which is the user logic and the mapping and a few",
    "start": "2398289",
    "end": "2405369"
  },
  {
    "text": "things are not that much actually miss Kirsten noise then we broke down this poultice bowls",
    "start": "2405369",
    "end": "2410950"
  },
  {
    "text": "also even though bowls don't have much of the overall usage and so it again the",
    "start": "2410950",
    "end": "2416680"
  },
  {
    "text": "sixth writing data into our external system like that is took up more chords rather than anything else and similarly",
    "start": "2416680",
    "end": "2423789"
  },
  {
    "text": "like now we clobber everything into something like a fetching data versus use of logic vs",
    "start": "2423789",
    "end": "2430089"
  },
  {
    "text": "Karen usage and the breakdown is as you can see they had and usage is minimal noise and",
    "start": "2430089",
    "end": "2435869"
  },
  {
    "text": "so one more thing is like we recently did some more optimizations on heron and",
    "start": "2435869",
    "end": "2441690"
  },
  {
    "text": "our throughput is increased by another 5x under the latency has dropped by another fifty to sixty percent so which",
    "start": "2441690",
    "end": "2448989"
  },
  {
    "text": "means you can even have a higher throughput with Lola mode of CPUs so the next problem that I wanted to do",
    "start": "2448989",
    "end": "2456880"
  },
  {
    "start": "2453000",
    "end": "2697000"
  },
  {
    "text": "is like how what do you do especially remember you're running the whole dag in a distributed fashion right",
    "start": "2456880",
    "end": "2463349"
  },
  {
    "text": "what happens if one of the guys going slow I mean so this is what typically we",
    "start": "2463349",
    "end": "2469150"
  },
  {
    "text": "call the stragglers so the stragglers are a norm in the distributed system we got to deal with it because they can",
    "start": "2469150",
    "end": "2476289"
  },
  {
    "text": "account for different reasons and the first reason is a bad host I mean in a hundred thousand cluster like some bad",
    "start": "2476289",
    "end": "2482800"
  },
  {
    "text": "apples will be always coming up and going so but the thing is since the streaming is something that you cannot",
    "start": "2482800",
    "end": "2488020"
  },
  {
    "text": "stop it keeps on going continuously because remember to the topology once you launch it there's no way to stop it",
    "start": "2488020",
    "end": "2494140"
  },
  {
    "text": "unless you kill it so it's so it's not like any interactive query system very type a query and the query finishes and",
    "start": "2494140",
    "end": "2501040"
  },
  {
    "text": "the results come through whereas the here results are continuously being generated as long as the data is they have to be generated but it's the ball",
    "start": "2501040",
    "end": "2507730"
  },
  {
    "text": "you will still run if there is no data as well so the first problem is a bad host so if you're unfortunate that you",
    "start": "2507730",
    "end": "2514660"
  },
  {
    "text": "got scheduled in a bad host what do you do then the second one is executions q I mean twitter is known for ask you",
    "start": "2514660",
    "end": "2520780"
  },
  {
    "text": "because of the fact that the asymmetric nature of the user and the followers",
    "start": "2520780",
    "end": "2525940"
  },
  {
    "text": "right so what do you do with the executions q then if the topology is not adequately provision then again",
    "start": "2525940",
    "end": "2532350"
  },
  {
    "text": "stragglers could come in because you're not able to process then coming data rate right so so let's look at what",
    "start": "2532350",
    "end": "2539620"
  },
  {
    "text": "other approaches that we do in order to handle strugglers so one is by the way",
    "start": "2539620",
    "end": "2544750"
  },
  {
    "text": "the guy is not receiving as fast as I am sending okay so I'll just drop it so",
    "start": "2544750",
    "end": "2549940"
  },
  {
    "text": "that is rationally send it to a straggler drop data so so then another hand the second approach",
    "start": "2549940",
    "end": "2557650"
  },
  {
    "text": "is hey I will slow down to the pace of what the standards are absorbing the data from me so everybody slows down so",
    "start": "2557650",
    "end": "2564520"
  },
  {
    "text": "that is the second approach and this is very typical of a tcp/ip where you have senders and receivers and when Sanders",
    "start": "2564520",
    "end": "2572140"
  },
  {
    "text": "starts with very fast then if the moment the rate at which the receivers are sending a problem and then the readies a",
    "start": "2572140",
    "end": "2577930"
  },
  {
    "text": "window size like it tcp slow start and all the various things right and they said alone on a speed",
    "start": "2577930",
    "end": "2583890"
  },
  {
    "text": "and the third one is detective ease ask stragglers and reschedule them so that's the third",
    "start": "2583890",
    "end": "2590980"
  },
  {
    "text": "approach so like what are the pros and cons of that so drop data strategy one",
    "start": "2590980",
    "end": "2596950"
  },
  {
    "text": "of the biggest issues unpredictable you have no idea where the data is being dropped especially in a complex drag of like running on 500 600 containers right",
    "start": "2596950",
    "end": "2604120"
  },
  {
    "text": "so you don't have enough it affects accuracy because since you don't know where is being dropped hey by the way",
    "start": "2604120",
    "end": "2610510"
  },
  {
    "text": "I'm supposed to a get a count of like a couple of million but suddenly I'm getting something like a 500,000 which",
    "start": "2610510",
    "end": "2617259"
  },
  {
    "text": "is not correct but why it is not correct so which means we don't have visibility into what is causing the issue right",
    "start": "2617259",
    "end": "2623140"
  },
  {
    "text": "because a lot of things could be happening so on the other hand the slowdown Center strategy it's kind of provides",
    "start": "2623140",
    "end": "2629619"
  },
  {
    "text": "predictability because as everybody slows down for the slowest guy the whole thing just put gets pushed into the",
    "start": "2629619",
    "end": "2637180"
  },
  {
    "text": "source that source is start lagging where the starting point of the pointer at which you write the data versus the",
    "start": "2637180",
    "end": "2643719"
  },
  {
    "text": "pointer that you're reading the data it starts diverging because of the fact that they are not absorbing the data as",
    "start": "2643719",
    "end": "2649599"
  },
  {
    "text": "fast as I should so but when it comes back up it reduces the recovery time and",
    "start": "2649599",
    "end": "2654849"
  },
  {
    "text": "process data because moment if you identify that faulty host and reschedule it quickly then we'll go really fast to",
    "start": "2654849",
    "end": "2661329"
  },
  {
    "text": "absorb the stock and it's also like handles temporary spikes because since",
    "start": "2661329",
    "end": "2666579"
  },
  {
    "text": "the source is acting as a cushion of the buffer it can handle spikes like because Twitter is known for spikes especially",
    "start": "2666579",
    "end": "2673359"
  },
  {
    "text": "in the context of a super bowl at all when a touchdown happens boom like a little like 34 exit let's go in spite",
    "start": "2673359",
    "end": "2679869"
  },
  {
    "text": "then again will come down like its last for a few seconds that's it then again like some bad call of vampire everybody",
    "start": "2679869",
    "end": "2685989"
  },
  {
    "text": "boom will go up again come down so like I since Lee like the spikes are a kind of norm within the to test data sets",
    "start": "2685989",
    "end": "2693160"
  },
  {
    "text": "that you get and so this one handle spikes also nicely so let's add lemon so",
    "start": "2693160",
    "end": "2699609"
  },
  {
    "text": "we event went with the slowdown center strategy and that we started implementing that and so the way",
    "start": "2699609",
    "end": "2706479"
  },
  {
    "text": "implement it is like let me give an example an illustrated by an example so",
    "start": "2706479",
    "end": "2711489"
  },
  {
    "text": "let us take this linear topology where yo a single spout and that in turn goes to a bolt b2 which in turns feeds to a",
    "start": "2711489",
    "end": "2718930"
  },
  {
    "text": "bolt b3 and finally it goes to bowl p for which does some additional processing and storing the results so so",
    "start": "2718930",
    "end": "2726009"
  },
  {
    "text": "this is the how the assuming three instance of each one of them each one of the vertices in the node",
    "start": "2726009",
    "end": "2732059"
  },
  {
    "text": "let us say it spread around four containers as you know like all the stream managers are there fully",
    "start": "2732059",
    "end": "2737410"
  },
  {
    "text": "connected graph right under exchanging data now let us say take the example of b2 going slow because v2 is not",
    "start": "2737410",
    "end": "2744130"
  },
  {
    "text": "absorbing data from the other guys and I mean from s ones and other things as fast as it should so what happens is the",
    "start": "2744130",
    "end": "2750730"
  },
  {
    "text": "stream manager on that container will realize because that is a guy who's pushing the data to the guy be too it",
    "start": "2750730",
    "end": "2755740"
  },
  {
    "text": "will realize a bother with this guy is not going as fast as it because my buffers are overflowing because team manager is the one which manages the",
    "start": "2755740",
    "end": "2761559"
  },
  {
    "text": "buffer and so then it sends a message hey by the way some guy in my manager in in my",
    "start": "2761559",
    "end": "2768760"
  },
  {
    "text": "container is experiencing back pressure so I'm going to initiate a back pressure protocol saying that hey by the way",
    "start": "2768760",
    "end": "2775119"
  },
  {
    "text": "everybody initiate whatever you need to do for back pressure please go ahead and do it so once all this team managers",
    "start": "2775119",
    "end": "2781990"
  },
  {
    "text": "receives odd message the first thing that they do is clamp down on the spot because if you look at in the whole",
    "start": "2781990",
    "end": "2787630"
  },
  {
    "text": "topology the spouts are the sources of the data by not absorbing data from the",
    "start": "2787630",
    "end": "2793390"
  },
  {
    "text": "spout which is that translate intentionally slowing down the topology in some fashion because all you are",
    "start": "2793390",
    "end": "2799089"
  },
  {
    "text": "doing that the data in transit they are only allowed to flow through and we are waiting for the buffer to drain out of",
    "start": "2799089",
    "end": "2805180"
  },
  {
    "text": "it so once the buffers in the stream energy drain of below a certain threshold then we again sender what he",
    "start": "2805180",
    "end": "2810910"
  },
  {
    "text": "called really back pressure then the spouts are open and keep going so so that is a very simple mechanism and",
    "start": "2810910",
    "end": "2817049"
  },
  {
    "text": "which all which allows you to go in to initiate and relieve back pressure quickly and also there is a cushion in",
    "start": "2817049",
    "end": "2824049"
  },
  {
    "text": "the sense like you have these buffers in the stream manager and you can pick up the threshold on the Lolich what you",
    "start": "2824049",
    "end": "2830710"
  },
  {
    "text": "call a low-water mark and high water mark and accordingly said those values and those values in turn will govern",
    "start": "2830710",
    "end": "2836530"
  },
  {
    "text": "when the initiate back pressure occurs and the lead back pressure occurs so if there is a huge gap between the two the",
    "start": "2836530",
    "end": "2842859"
  },
  {
    "text": "initiate and the leave will settle down those smooth curve rather than having something like like initiate and relieve initiate and",
    "start": "2842859",
    "end": "2850270"
  },
  {
    "text": "the live like a flip-flop kind of thing right so so yes a",
    "start": "2850270",
    "end": "2856740"
  },
  {
    "text": "deep in the sense like how big so they typically sounded Meg yes I don't Meg",
    "start": "2856799",
    "end": "2863650"
  },
  {
    "text": "but that's again configurable so depending upon your application requirements everything you can you can configure on a per topology level also",
    "start": "2863650",
    "end": "2870220"
  },
  {
    "text": "so so so that seems to work well in practice as you can i mean if we look at",
    "start": "2870220",
    "end": "2876940"
  },
  {
    "text": "the lot so logical topology this is like 15 containers if one container is experiencing back pressure the the",
    "start": "2876940",
    "end": "2883880"
  },
  {
    "text": "source will start lagging because your read pointer will and write pointer will start lagging quite a bit and at that",
    "start": "2883880",
    "end": "2891920"
  },
  {
    "text": "point what we do is you can manually restart a container so we have a special command at the command line tool where",
    "start": "2891920",
    "end": "2898180"
  },
  {
    "text": "you shoot down the container and then automatically removes that container the moment it knows that that container is a",
    "start": "2898180",
    "end": "2903770"
  },
  {
    "text": "cause of the root cause of the bad pressure but sometimes the back pressure will most of the time the back pressure",
    "start": "2903770",
    "end": "2909170"
  },
  {
    "text": "will recover itself automatically and there are scenarios in which it doesn't recover at all because of hi GC",
    "start": "2909170",
    "end": "2914660"
  },
  {
    "text": "something is happening so then manual intervention will be required so we are working on some kind of anomaly",
    "start": "2914660",
    "end": "2921260"
  },
  {
    "text": "detection algorithms where so automatically we look at all these various pout and other instances and the",
    "start": "2921260",
    "end": "2927710"
  },
  {
    "text": "moment something is behaving anomalous and we look at the correlations and all the various things then if there is high",
    "start": "2927710",
    "end": "2933920"
  },
  {
    "text": "degree of confidence that that container is experiencing slow down then automatically you should on the container more to the next host or",
    "start": "2933920",
    "end": "2940700"
  },
  {
    "text": "whatever it is so like the back pressure in practice most scenarios it requires as I've mentioned without any manual",
    "start": "2940700",
    "end": "2946850"
  },
  {
    "text": "intervention which means no pager incidents so the sustained back pressure occurs in some places irrecoverable GC",
    "start": "2946850",
    "end": "2953000"
  },
  {
    "text": "cycles or a bad or faulty hose then finally like sometimes use a preferred",
    "start": "2953000",
    "end": "2958310"
  },
  {
    "text": "dropping of data the care orderly the latest data so we also run what we call",
    "start": "2958310",
    "end": "2964790"
  },
  {
    "text": "for detecting and bad and faulty hose we also run a separate heron topology which takes the data and continuously does",
    "start": "2964790",
    "end": "2971660"
  },
  {
    "text": "some kind of simple machine learning to figure out what is going on and now the we are not",
    "start": "2971660",
    "end": "2976750"
  },
  {
    "text": "automatically taking those hosts out of a scheduler instead it goes through a manual process where it's",
    "start": "2976750",
    "end": "2983200"
  },
  {
    "text": "given to the appropriate administrator they even toned go manually remove it but we will make it more automatic also",
    "start": "2983200",
    "end": "2990820"
  },
  {
    "text": "so finally like this is a load shedding so sampling based approaches but doesn't",
    "start": "2990820",
    "end": "2997760"
  },
  {
    "start": "2991000",
    "end": "3056000"
  },
  {
    "text": "work very well because it's very hard to reason about sampling across uniform distributed spots where everybody's",
    "start": "2997760",
    "end": "3003310"
  },
  {
    "text": "reading in a different rates and everything right and we cannot up sample the results and the opp sampling results",
    "start": "3003310",
    "end": "3008590"
  },
  {
    "text": "might not work for something like a median mode and all the staff too so we simply went with the drop based",
    "start": "3008590",
    "end": "3013780"
  },
  {
    "text": "approaches simply drop the older data the spout lacks a threshold and takes a",
    "start": "3013780",
    "end": "3019480"
  },
  {
    "text": "lag threshold and the Lark adjustments and this seems to work in practice because here people in this case is",
    "start": "3019480",
    "end": "3025570"
  },
  {
    "text": "people don't care about accuracy instead they want to be on top of the real time so because they might be trending",
    "start": "3025570",
    "end": "3030880"
  },
  {
    "text": "computing some trends or whatever it is so even if for example in the cases where we have seen some rock outages",
    "start": "3030880",
    "end": "3037119"
  },
  {
    "text": "right so which mean the data does not come for four hours so when the data comes after four hours then all the",
    "start": "3037119",
    "end": "3042730"
  },
  {
    "text": "topologies who are want to process the data they will process of course but they are out apologies where hey I don't",
    "start": "3042730",
    "end": "3048280"
  },
  {
    "text": "care about what has happened in the last for us I want to know what is right now so move my pointers directly do then we",
    "start": "3048280",
    "end": "3053980"
  },
  {
    "text": "have the latest data is coming from so that works so if you want like there we",
    "start": "3053980",
    "end": "3059200"
  },
  {
    "start": "3056000",
    "end": "3144000"
  },
  {
    "text": "have these three papers returned already and it's all available on the web and if you wanted to learn",
    "start": "3059200",
    "end": "3066070"
  },
  {
    "text": "about it feel free to learn about it and if you have any questions please post them in herrin users mailing list so",
    "start": "3066070",
    "end": "3074109"
  },
  {
    "text": "we'll be happy to answer so if an earnest open-sourced if you guys want to",
    "start": "3074109",
    "end": "3079150"
  },
  {
    "text": "use it go for it one thing that we Canada is reliability which is because we are after hair into production the",
    "start": "3079150",
    "end": "3086050"
  },
  {
    "text": "next reduction number of incidents so the total number of incidents that we got in the 21 of years is just for so so",
    "start": "3086050",
    "end": "3094240"
  },
  {
    "text": "like and you can follow an inherence streaming so the new active work that we are just wrapping up is what you call a",
    "start": "3094240",
    "end": "3101050"
  },
  {
    "text": "scaling automatically the topology can scale and we are pretty much done at various and testing and initially the",
    "start": "3101050",
    "end": "3107440"
  },
  {
    "text": "rollout will be on manual scaling where you do a hair and update command then we'll automatically expand the job from",
    "start": "3107440",
    "end": "3114010"
  },
  {
    "text": "like a 10 containers 200 containers without pulling down the existing job and that will become more automatic in",
    "start": "3114010",
    "end": "3119680"
  },
  {
    "text": "q1 so yes that's all I had any questions",
    "start": "3119680",
    "end": "3126960"
  },
  {
    "text": "thank you [Applause]",
    "start": "3126960",
    "end": "3134540"
  },
  {
    "text": "swoozie everything clear I get him okay cool",
    "start": "3134540",
    "end": "3140809"
  },
  {
    "text": "thank you",
    "start": "3140870",
    "end": "3144020"
  }
]