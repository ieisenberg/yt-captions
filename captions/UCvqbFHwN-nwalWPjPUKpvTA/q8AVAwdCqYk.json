[
  {
    "text": "hello everyone welcome to my session U my name is W who and I'm one of the U",
    "start": "120",
    "end": "6080"
  },
  {
    "text": "maintainer of the cncf was my project um hello everyone my name is",
    "start": "6080",
    "end": "11719"
  },
  {
    "text": "Michael Yan and uh um I'm also uh the maintainer of the wage",
    "start": "11719",
    "end": "17199"
  },
  {
    "text": "project uh so today Mike and I will talk about uh creating cognitive apps uh we",
    "start": "17199",
    "end": "23560"
  },
  {
    "text": "will use uh Lage it's uh lightweight and posable runtime based on the world M run",
    "start": "23560",
    "end": "30119"
  },
  {
    "text": "time um so let's get started so uh the",
    "start": "30119",
    "end": "35239"
  },
  {
    "text": "current Tex stack for our applications is dominated by python apparently we use",
    "start": "35239",
    "end": "41480"
  },
  {
    "text": "pyto and python to do erence and if you want to um build some LM agents when you",
    "start": "41480",
    "end": "49280"
  },
  {
    "text": "where we use launching and python however python has its problems U",
    "start": "49280",
    "end": "55559"
  },
  {
    "text": "first it's heavyweight because there are lots of complex dependencies with python",
    "start": "55559",
    "end": "62039"
  },
  {
    "text": "um the IM the picture here is the official Pyon doer image U from the U",
    "start": "62039",
    "end": "68640"
  },
  {
    "text": "from this picture we can see that um uh py doer image can be 4 gabes it's almost",
    "start": "68640",
    "end": "75720"
  },
  {
    "text": "the same size of a light language model like lamb 27b so it's very huge and um um one more",
    "start": "75720",
    "end": "83680"
  },
  {
    "text": "thing is that the uh uh the the python based umm application is not posable um",
    "start": "83680",
    "end": "93399"
  },
  {
    "text": "uh different uh GPU drivers requires different Docker images also from this",
    "start": "93399",
    "end": "99680"
  },
  {
    "text": "picture we can see that um um C 11 and C 12 uh",
    "start": "99680",
    "end": "105399"
  },
  {
    "text": "sh uh uh the do image for k 11 and 12 is a is a is different do images so this is",
    "start": "105399",
    "end": "114399"
  },
  {
    "text": "the pro the problem of python so how about we use a native comp",
    "start": "114399",
    "end": "121200"
  },
  {
    "text": "applications such like native Russ or C applications yes they are much smaller",
    "start": "121200",
    "end": "128479"
  },
  {
    "text": "than python but um those native applications are not pable U if we want",
    "start": "128479",
    "end": "133959"
  },
  {
    "text": "to run the same application on MacBook and an N media GPU then we need to recom",
    "start": "133959",
    "end": "140400"
  },
  {
    "text": "recompare these applications on the device but um um kubernetes can only",
    "start": "140400",
    "end": "147599"
  },
  {
    "text": "exract binary artifacts so uh that's the problem with python and um Native",
    "start": "147599",
    "end": "153800"
  },
  {
    "text": "applications how can we achieve the how can we solve this",
    "start": "153800",
    "end": "159319"
  },
  {
    "text": "problems um all right yeah so um I I think this is U one of the common thread",
    "start": "159319",
    "end": "165400"
  },
  {
    "text": "in this on cucon is that how do we make kuber kubernetes work better with uh",
    "start": "165400",
    "end": "170800"
  },
  {
    "text": "machine learning and uh large language model workloads um as we have seen there's uh um you know as we have shown",
    "start": "170800",
    "end": "178000"
  },
  {
    "text": "those P pytho images are gigabytes measured in gigabytes right you know it's uh it's very heavy even in the in",
    "start": "178000",
    "end": "185280"
  },
  {
    "text": "in a cloud environment and uh um the native applications apparently uh they",
    "start": "185280",
    "end": "190599"
  },
  {
    "text": "are not portable it's not not just the problem with kubernetes we are going all the way back to the problem where if I develop something on my laptop I can",
    "start": "190599",
    "end": "197400"
  },
  {
    "text": "never be guaranteed this thing going to run on the server or on or on another device because they may have a different",
    "start": "197400",
    "end": "203280"
  },
  {
    "text": "GPU right you know it's not just recompiling because the the code may be different um you know the code uh you",
    "start": "203280",
    "end": "211120"
  },
  {
    "text": "know the the Mac metal framework and the the Nvidia Cuda framework are very you know require different code the source",
    "start": "211120",
    "end": "217200"
  },
  {
    "text": "code to begin with so you have to recompile so we thought about um how",
    "start": "217200",
    "end": "223080"
  },
  {
    "text": "this problem can be solved you know that's a um it um it occurs to us about",
    "start": "223080",
    "end": "229080"
  },
  {
    "text": "I think a year ago is that we can use the same approach that Java has solved",
    "start": "229080",
    "end": "235159"
  },
  {
    "text": "um right once wrong anywhere problem but expand that to the GPU setting using a",
    "start": "235159",
    "end": "241599"
  },
  {
    "text": "lightweight virtual machine our case using the the um using web assembly",
    "start": "241599",
    "end": "247959"
  },
  {
    "text": "right so the idea really is that to provide abstraction layer between the",
    "start": "247959",
    "end": "253519"
  },
  {
    "text": "application and the underlying drivew and the hardware right using the virtual",
    "start": "253519",
    "end": "258840"
  },
  {
    "text": "machine to provide that abstraction so that application developers only need to write towards a single unified API and",
    "start": "258840",
    "end": "267040"
  },
  {
    "text": "then compile the application to W them and then distribute the wasm bite code to anywhere the application might wrong",
    "start": "267040",
    "end": "274080"
  },
  {
    "text": "and then the wasm wrong time takes over at wrong time to figure out how to dispatch how to translate and how to",
    "start": "274080",
    "end": "280919"
  },
  {
    "text": "route those um wasm high level wasm function cost into lowlevel say Cuda",
    "start": "280919",
    "end": "286400"
  },
  {
    "text": "function cost or metal function cost you know so essentially it's uh um is to",
    "start": "286400",
    "end": "292039"
  },
  {
    "text": "provide um a virtual instruction set that is uh um that is abstract for",
    "start": "292039",
    "end": "297639"
  },
  {
    "text": "developers so that we can um you know so that we can provide the the compatibility",
    "start": "297639",
    "end": "304080"
  },
  {
    "text": "so the the key specification that's that we use here is something called the wasi",
    "start": "304080",
    "end": "309960"
  },
  {
    "text": "it's uh uh if you are familiar with web assembly it's that you know web assembly uh started in the browser but uh wasi",
    "start": "309960",
    "end": "316160"
  },
  {
    "text": "means we some system interface meaning if it's runs outside of the browser you need to have um you know system",
    "start": "316160",
    "end": "323120"
  },
  {
    "text": "functions available to it you know things like the socket networking socket or you know file system and all that",
    "start": "323120",
    "end": "328400"
  },
  {
    "text": "stuff so GPU access can be abstracted the same way right so the wasi NN is uh",
    "start": "328400",
    "end": "334520"
  },
  {
    "text": "is a WC WCC standard that's uh uh that abstract uh a set of neural network or",
    "start": "334520",
    "end": "341560"
  },
  {
    "text": "inference um a function cost um to uh through the uh through the web B code",
    "start": "341560",
    "end": "347800"
  },
  {
    "text": "specification right so that allows us to build different language sdks for instance we have a uh was SDK for rust",
    "start": "347800",
    "end": "354960"
  },
  {
    "text": "was SDK for JavaScript was um um SD CK for C C++ you know all those web",
    "start": "354960",
    "end": "362520"
  },
  {
    "text": "languages you write towards wasi earn specification makes their function cost and then compile that to the um wasm",
    "start": "362520",
    "end": "371039"
  },
  {
    "text": "bite code then a wasm round time like like wasm Edge um like I said translate",
    "start": "371039",
    "end": "376080"
  },
  {
    "text": "and routes those those wasm API CS to the to the native to the underlying AI libraries and drivers and and hardware",
    "start": "376080",
    "end": "382919"
  },
  {
    "text": "and all that so many gpus drivers and inflence Frameworks are already supported you know so we have a um you",
    "start": "382919",
    "end": "389680"
  },
  {
    "text": "know so in the wasm age Community we have over 100 uh over 100 contributors at any given time we have four or five",
    "start": "389680",
    "end": "396360"
  },
  {
    "text": "graduate students working through the uh Linux Foundation mentorship program to",
    "start": "396360",
    "end": "402560"
  },
  {
    "text": "uh to help build those pry and connections uh between the the the wasm round time and the underlying framework",
    "start": "402560",
    "end": "408919"
  },
  {
    "text": "so today we already support calling say py to functions it's not the python py",
    "start": "408919",
    "end": "414800"
  },
  {
    "text": "toch but the C based py toch right py to functions from wasam tensorflow functions from wasm open w function from",
    "start": "414800",
    "end": "421120"
  },
  {
    "text": "WM the gml is What U is something you know maybe more commonly known as ll.",
    "start": "421120",
    "end": "426240"
  },
  {
    "text": "CPP is one of the most the popular ways to run the large language model right and on top of that we have um um uh GPU",
    "start": "426240",
    "end": "434280"
  },
  {
    "text": "accelerated connections to um you know to to c-based functions like open CV",
    "start": "434280",
    "end": "439479"
  },
  {
    "text": "ffmpeg and the result is that we can run the whole Suite of llama 2 models large",
    "start": "439479",
    "end": "444800"
  },
  {
    "text": "language models we can run things like the YOLO 5 for object detection on the edge we we can run the Google media pipe",
    "start": "444800",
    "end": "451360"
  },
  {
    "text": "which is a series of tensorflow models which that does object detection and you know things like",
    "start": "451360",
    "end": "457520"
  },
  {
    "text": "that so what do we get we get uh very lightweight inference applications you",
    "start": "457520",
    "end": "463599"
  },
  {
    "text": "know that's the application plus WR time in a in a in a complete image it would",
    "start": "463599",
    "end": "469039"
  },
  {
    "text": "be only measured in megabytes in 20 megabytes 30 megabytes although you know",
    "start": "469039",
    "end": "474800"
  },
  {
    "text": "one thing I would like to say is that you know although the py toor stalker image is 4 gabt the pytorch C library",
    "start": "474800",
    "end": "482360"
  },
  {
    "text": "that perform the task is only 2 megabytes you know so it's only um you know less than 1 thousand of that darker",
    "start": "482360",
    "end": "488720"
  },
  {
    "text": "image the rest of this stuff is complex python dependency that or the the the",
    "start": "488720",
    "end": "493960"
  },
  {
    "text": "native dependenc that they have to add to it right by by extracting the the C",
    "start": "493960",
    "end": "500000"
  },
  {
    "text": "library out of the uh the py to environment and the hook it up with the wasm environment we produce significant",
    "start": "500000",
    "end": "506879"
  },
  {
    "text": "cost savings so we are talking again we are talking about application measured in megabytes and not in gigabytes and uh",
    "start": "506879",
    "end": "513959"
  },
  {
    "text": "um when the application runs on the devices you know that's on the Mac or on Nvidia Jetson device or on server you",
    "start": "513959",
    "end": "521200"
  },
  {
    "text": "know that's U we can have native and GPU accelerated performance we support many",
    "start": "521200",
    "end": "526519"
  },
  {
    "text": "GPU and Hardware accelerators you know um again this is through our mentorship program we have a lot uh we have uh you",
    "start": "526519",
    "end": "533240"
  },
  {
    "text": "know a lot of graduate students working on those those things and you can see from that that diagram you know so um",
    "start": "533240",
    "end": "539000"
  },
  {
    "text": "even within the Nvidia ecosystem we support Q 11 Q 12 T RT T RTM you know",
    "start": "539000",
    "end": "545839"
  },
  {
    "text": "different ways to accelerate the hardware right on the um uh on the Intel side we are we are working on to support",
    "start": "545839",
    "end": "552160"
  },
  {
    "text": "the Intel CPUs to do inference with uh with a special library that come from Intel right and uh on the Mac you know",
    "start": "552160",
    "end": "558800"
  },
  {
    "text": "there's the traditional metal framework and also there's the newer mlx framework right you know so there's um you know",
    "start": "558800",
    "end": "565279"
  },
  {
    "text": "just large number of devices and larger number of inference libraries that uh that we can um that we can um uh hook up",
    "start": "565279",
    "end": "572760"
  },
  {
    "text": "with the wasch uh uh wasi API so developers only need to know what in an",
    "start": "572760",
    "end": "578160"
  },
  {
    "text": "API they don't need to worry about Metal they don't need to worry about Cuda much less Cuda 11 versus Cuda 12 you know so",
    "start": "578160",
    "end": "585120"
  },
  {
    "text": "in they just write their application compiled and then let us figure out how to run on those devices right so and we",
    "start": "585120",
    "end": "592440"
  },
  {
    "text": "also has because we support those um those Hardwares and those uh those wrong",
    "start": "592440",
    "end": "597480"
  },
  {
    "text": "times we support a wide selection of a and models that I've just shown you know from Vision to audio to uh to large",
    "start": "597480",
    "end": "603959"
  },
  {
    "text": "language models uh so we have learned W can can",
    "start": "603959",
    "end": "612000"
  },
  {
    "text": "run apps better so I will do the first demo uh I will U build and run an open a",
    "start": "612000",
    "end": "619079"
  },
  {
    "text": "compatible API server using the Lama Edge SDK so um I provide the The Voice the",
    "start": "619079",
    "end": "628079"
  },
  {
    "text": "the naration while weing doing the demo yeah uh if you have a computer on your hand you can follow us on you can visit",
    "start": "628079",
    "end": "636079"
  },
  {
    "text": "l.com and then we will copy and paste uh this one command line to our",
    "start": "636079",
    "end": "643959"
  },
  {
    "text": "terminal so this command to will uh download and install all the necessary",
    "start": "647399",
    "end": "653880"
  },
  {
    "text": "softwares we need to run and run on openi uh API uh server",
    "start": "653880",
    "end": "660639"
  },
  {
    "text": "yeah so what we are showing here so um what is wasm ede what is llama ede so wasm ede is a wasm wrong time so llama",
    "start": "660639",
    "end": "667000"
  },
  {
    "text": "ede is the a large language model is uh inference application we build on top of",
    "start": "667000",
    "end": "672639"
  },
  {
    "text": "was Ed right you know so you know because was Ed you can do a lot of things with web assembly including microservices Serv functions and all",
    "start": "672639",
    "end": "678760"
  },
  {
    "text": "that stuff but very specifically if you want to do um you know open source large language model this is one of um this is",
    "start": "678760",
    "end": "685920"
  },
  {
    "text": "a way to do it you know that's U um because it provide a very compact environment and it's so it's a fully",
    "start": "685920",
    "end": "691240"
  },
  {
    "text": "programmable environment you can you can um you can write application on top of it and this first application just one",
    "start": "691240",
    "end": "698240"
  },
  {
    "text": "of the application that we wrote the source code is completely available it's under um the GitHub Lama AG um um you",
    "start": "698240",
    "end": "704800"
  },
  {
    "text": "know rep uh repository and uh what it implements is a API server now it has",
    "start": "704800",
    "end": "710600"
  },
  {
    "text": "started so um we do you want to go back and see yeah please so uh this command",
    "start": "710600",
    "end": "718200"
  },
  {
    "text": "to will uh download the was runtime with the wasm plugins like mro just Mission",
    "start": "718200",
    "end": "725720"
  },
  {
    "text": "and they will also download the uh gamma 2B U model um by default uh gamma 2B is",
    "start": "725720",
    "end": "733279"
  },
  {
    "text": "uh recently released by Google and then it will download the portable WM app",
    "start": "733279",
    "end": "739199"
  },
  {
    "text": "it's called Lama API server. wasm this uh this app will create an open an API Z",
    "start": "739199",
    "end": "746000"
  },
  {
    "text": "for the gamma Tob model and then it will download the uh the user interface the",
    "start": "746000",
    "end": "751720"
  },
  {
    "text": "chatbox webui and then it will uh finally it will run the this command",
    "start": "751720",
    "end": "757600"
  },
  {
    "text": "line to um to run the uh API server so the the server is started Let's",
    "start": "757600",
    "end": "765560"
  },
  {
    "text": "uh let's go to the Local Host 8 port to check with the gamma Tob",
    "start": "765560",
    "end": "773360"
  },
  {
    "text": "model",
    "start": "778120",
    "end": "781120"
  },
  {
    "text": "okay let's create a new chat so let's ask a simple",
    "start": "789279",
    "end": "795639"
  },
  {
    "text": "question what is the capital of",
    "start": "795639",
    "end": "803160"
  },
  {
    "text": "France so this is completely working on her laptop so she has has a fairly",
    "start": "803839",
    "end": "809600"
  },
  {
    "text": "low-end Mac laptop and it's already running a large language model right you know so it's uh the the gam to be model",
    "start": "809600",
    "end": "817320"
  },
  {
    "text": "released by Google right yeah so I can ask a follow-up question plan me a two a",
    "start": "817320",
    "end": "825639"
  },
  {
    "text": "one day",
    "start": "825639",
    "end": "828079"
  },
  {
    "text": "trip so yeah you see it follows the conversation right you know if you just tell it PL me W trip there you know it",
    "start": "833519",
    "end": "839800"
  },
  {
    "text": "would not know what you are talking about you know where right but in the previous the the previous question asked",
    "start": "839800",
    "end": "846000"
  },
  {
    "text": "about Paris so it knows it needs to plan a one day trip to Paris right so this is one of the um a very small large",
    "start": "846000",
    "end": "853639"
  },
  {
    "text": "language model and you can see it runs um really fast on her laptop because it",
    "start": "853639",
    "end": "858959"
  },
  {
    "text": "use the GPU you know it's use um um uh the apples I think it's M2 device right",
    "start": "858959",
    "end": "864040"
  },
  {
    "text": "you know that's M2 yeah so it's a M2 Macbook so you know it's uh it's",
    "start": "864040",
    "end": "869079"
  },
  {
    "text": "certainly speaks faster than human can speak I think you know that's uh so um it's complete local right you know and",
    "start": "869079",
    "end": "877079"
  },
  {
    "text": "the the web server and the large language model are both running inside wasam so we compiled uh application",
    "start": "877079",
    "end": "884040"
  },
  {
    "text": "that's stand up web server serve this HTML web page and then serve API end",
    "start": "884040",
    "end": "890199"
  },
  {
    "text": "points that that points to the to the um to the large language model so that you",
    "start": "890199",
    "end": "895480"
  },
  {
    "text": "can chatab that here right so this entire application so that's uh Lama API server. wasm is uh is application that",
    "start": "895480",
    "end": "903199"
  },
  {
    "text": "compiled from rust and then you know um and then we demonstrated here yeah so",
    "start": "903199",
    "end": "911680"
  },
  {
    "text": "let's uh so this is the first EXO we just create um open a compatible AP for",
    "start": "913600",
    "end": "920000"
  },
  {
    "text": "the gamma Tob model and um as M mentioned before um with W Mage with",
    "start": "920000",
    "end": "925560"
  },
  {
    "text": "Lage we can write it once and run it anywhere so uh the next EXO I will copy",
    "start": "925560",
    "end": "930880"
  },
  {
    "text": "and deploy the Lama apsl worm app to uh",
    "start": "930880",
    "end": "936120"
  },
  {
    "text": "to a Invidia uh GPU device so let's go back to my",
    "start": "936120",
    "end": "941839"
  },
  {
    "text": "terminal uh I will stop the current process and then I will use as uh CP",
    "start": "941839",
    "end": "949160"
  },
  {
    "text": "command to transfer the uh lamb API server. wasm to the immediate device so",
    "start": "949160",
    "end": "957360"
  },
  {
    "text": "you can see we are copying that wasm file right you know just a single wasm file there's no Docker image there's no",
    "start": "957360",
    "end": "964440"
  },
  {
    "text": "recompiling anything so it's just the binary file it's very much like a Java bite code application right to a server",
    "start": "964440",
    "end": "970839"
  },
  {
    "text": "that is uh located halfway across the world in our off in our engineering office in Tai Bay and uh there we have a",
    "start": "970839",
    "end": "978480"
  },
  {
    "text": "Nvidia Jetson device it's about this big it has a a fairly large CPU and GPU in",
    "start": "978480",
    "end": "984120"
  },
  {
    "text": "it it's about it cost about $2,000 we use it in our office as a coding assistant so we run the uh uh code Lama",
    "start": "984120",
    "end": "991639"
  },
  {
    "text": "uh model on that on that device to hook up with all the visual studio idees that we have in that office so that our",
    "start": "991639",
    "end": "998079"
  },
  {
    "text": "developers can ask questions about it about their daily work you know one of the most interesting question that we",
    "start": "998079",
    "end": "1003639"
  },
  {
    "text": "ask is uh because we build a lot of the Python stuff into web assembly so often",
    "start": "1003639",
    "end": "1008920"
  },
  {
    "text": "times we need to translate Python program to rust in the past you you need someone who understand both Python and",
    "start": "1008920",
    "end": "1014440"
  },
  {
    "text": "Russ but today we just send the Python program to the uh code Lama chat and you",
    "start": "1014440",
    "end": "1019680"
  },
  {
    "text": "know interface and say translate rust and it g um you know six out of 10 times it's going to do it very well you know",
    "start": "1019680",
    "end": "1025959"
  },
  {
    "text": "so that's a so that's how we use it but um like we have just shown we copied this file to the um to the Jetson or",
    "start": "1025959",
    "end": "1033280"
  },
  {
    "text": "device right so now you can see the file on the jet device and the time stamp is today so it's a 5 megabytes program it's",
    "start": "1033280",
    "end": "1039918"
  },
  {
    "text": "not a 5 gigabytes program like py toor would have you so it's a 5 megabytes program that's being copied to that",
    "start": "1039919",
    "end": "1046720"
  },
  {
    "text": "device now please so I will U Move the was file to uh the model uh directory",
    "start": "1046720",
    "end": "1053280"
  },
  {
    "text": "because I have lots of models",
    "start": "1053280",
    "end": "1057799"
  },
  {
    "text": "here so in the uh in the model folder I have lots of I light langu models I",
    "start": "1062760",
    "end": "1069320"
  },
  {
    "text": "already have the I already have the gamma 2B model",
    "start": "1069320",
    "end": "1076720"
  },
  {
    "text": "and um this is the lama lama apsr wasn't fil we just moved",
    "start": "1076720",
    "end": "1083720"
  },
  {
    "text": "so I can use the same command line to um to run the gamma Tob model let's go back",
    "start": "1083720",
    "end": "1090080"
  },
  {
    "text": "to find the command line so the interesting is that all the models she has downloaded on that machine can works",
    "start": "1090080",
    "end": "1098360"
  },
  {
    "text": "with this single wasm file so that's what we call was it's a it's a portable WM file not only it's runs across",
    "start": "1098360",
    "end": "1105039"
  },
  {
    "text": "different GPU architectures but it runs across different models if you go to hugging phas and search for llama 2",
    "start": "1105039",
    "end": "1111679"
  },
  {
    "text": "model find llama 2 models there are were literally thousands of them you know people are find thousands of uh you know",
    "start": "1111679",
    "end": "1118120"
  },
  {
    "text": "different versions of with different expertise different styles of of was models right",
    "start": "1118120",
    "end": "1124400"
  },
  {
    "text": "so yeah that's um uh so okay so she started the application server now and",
    "start": "1124400",
    "end": "1132080"
  },
  {
    "text": "uh because this is behind the firewall so she going to do the SS SS tunneling to this local device right no I'm will",
    "start": "1132080",
    "end": "1139200"
  },
  {
    "text": "log into the I will log into the device in another okay okay so I log into the",
    "start": "1139200",
    "end": "1144679"
  },
  {
    "text": "device another terminal yeah so um next I will use a call to uh",
    "start": "1144679",
    "end": "1151559"
  },
  {
    "text": "send API request to the uh server so we demonstrate another way of",
    "start": "1151559",
    "end": "1158280"
  },
  {
    "text": "using that API server it's called API server but we showed a a chat interface before right so but it's uh it also",
    "start": "1158280",
    "end": "1164760"
  },
  {
    "text": "responds to open eye style um uh uh uh rest queries right so this uh if you are",
    "start": "1164760",
    "end": "1171520"
  },
  {
    "text": "familiar with the Open Eye stuff it's it looks fairly familiar it's just the Local Host replace Local Host with api.",
    "start": "1171520",
    "end": "1178039"
  },
  {
    "text": "open.com right you know and uh you know you you send it in U um in a Json",
    "start": "1178039",
    "end": "1184799"
  },
  {
    "text": "message about you know what's so the question you want to ask yeah yeah the question is uh the same question what is",
    "start": "1184799",
    "end": "1191919"
  },
  {
    "text": "the capital of",
    "start": "1191919",
    "end": "1194679"
  },
  {
    "text": "fr I think there's something you are type",
    "start": "1197640",
    "end": "1203799"
  },
  {
    "text": "something wrong because you",
    "start": "1203799",
    "end": "1206679"
  },
  {
    "text": "should",
    "start": "1214039",
    "end": "1217039"
  },
  {
    "text": "what what yeah you know that's uh um it's not",
    "start": "1223840",
    "end": "1228919"
  },
  {
    "text": "properly escaped the previous time but now you can see it's uh Capital France is Paris it's come up with a slightly",
    "start": "1228919",
    "end": "1235679"
  },
  {
    "text": "different answer every time because you know it has a it has what do they call the temperature setting right you know",
    "start": "1235679",
    "end": "1240760"
  },
  {
    "text": "so the model um you know doesn't come up with the same questioning every single time so but it's both correct right so",
    "start": "1240760",
    "end": "1248679"
  },
  {
    "text": "do you want to ask another question yeah before we ask another",
    "start": "1248679",
    "end": "1253919"
  },
  {
    "text": "question we can um conclude that with uh L magic we can create",
    "start": "1253919",
    "end": "1259400"
  },
  {
    "text": "uh an open AI compatible a s for any open source light language models it's",
    "start": "1259400",
    "end": "1264640"
  },
  {
    "text": "pable and it's lightweight um because the aps Ser is compatible with open ey",
    "start": "1264640",
    "end": "1271000"
  },
  {
    "text": "it follows the open ey Spike so we can we can make the apsr work with any tool",
    "start": "1271000",
    "end": "1276840"
  },
  {
    "text": "Ching in the open ey system U for example launching and L de but I want to",
    "start": "1276840",
    "end": "1282159"
  },
  {
    "text": "uh recommend you use the FL Network because it's also a sess platform in R and web simp and you can use FL to build",
    "start": "1282159",
    "end": "1289679"
  },
  {
    "text": "uhm agents and disc or teleg Bo so and",
    "start": "1289679",
    "end": "1294799"
  },
  {
    "text": "this is the open AP server but uh there are uh but it's not enough I will uh ask",
    "start": "1294799",
    "end": "1302279"
  },
  {
    "text": "the model another question so we ask a question that the",
    "start": "1302279",
    "end": "1307720"
  },
  {
    "text": "model is likely to get wrong and then show you how to fix",
    "start": "1307720",
    "end": "1312840"
  },
  {
    "text": "it",
    "start": "1317520",
    "end": "1320520"
  },
  {
    "text": "the question is how to install w m on mic yeah as you can imagine the model",
    "start": "1335600",
    "end": "1341480"
  },
  {
    "text": "may not have that information because um it's a 2B model so it's a severely compressed um you know um knowledge of",
    "start": "1341480",
    "end": "1348679"
  },
  {
    "text": "the internet it takes longer because it's not doing the streaming return does not have it have to generate the whole",
    "start": "1348679",
    "end": "1354640"
  },
  {
    "text": "answer before it come back which is what open I does right you know so it's uh um but it's uh it going to generate a",
    "start": "1354640",
    "end": "1360880"
  },
  {
    "text": "manual to install BM Edge and uh because you know truth to be told was match is not say Linux you know whatever right",
    "start": "1360880",
    "end": "1367880"
  },
  {
    "text": "you know so it would uh give you some very generic answers right you know so it",
    "start": "1367880",
    "end": "1373480"
  },
  {
    "text": "says um yeah it's a installation steps",
    "start": "1373480",
    "end": "1378520"
  },
  {
    "text": "download the wrong time where to download the wrong time it doesn't know and uh uh you know and select the latest",
    "start": "1378520",
    "end": "1384039"
  },
  {
    "text": "Mac version from wrong time you know that's you know in general those are very uh you know uh uh generic steps yes",
    "start": "1384039",
    "end": "1392679"
  },
  {
    "text": "the uh if we want to install and apple on on Mac uh on your MacBook um go to",
    "start": "1392679",
    "end": "1400080"
  },
  {
    "text": "the go to the website and download the the release ass set this is the general",
    "start": "1400080",
    "end": "1405840"
  },
  {
    "text": "step but it's it's wrong it can't work with W",
    "start": "1405840",
    "end": "1411840"
  },
  {
    "text": "magic uh here is the right answer we can install W magic with only one command",
    "start": "1411840",
    "end": "1418640"
  },
  {
    "text": "line but how can we make um uh like Lang model can answer such",
    "start": "1418640",
    "end": "1425960"
  },
  {
    "text": "questions so that's where um I think the the um the L Edge the the ability to",
    "start": "1429960",
    "end": "1437880"
  },
  {
    "text": "develop uh applications on Lama AG really shines because I think there's lots of choices where give you a model",
    "start": "1437880",
    "end": "1444240"
  },
  {
    "text": "and uh stand up uh API server we have seen this from the Keynotes we have seen this from many many places where you",
    "start": "1444240",
    "end": "1451240"
  },
  {
    "text": "know even ll. CPP has one it's called ll. CPP serve right you know um",
    "start": "1451240",
    "end": "1456640"
  },
  {
    "text": "but a lot of times you need to do more than that you need to so for instance there's a very common technique called",
    "start": "1456640",
    "end": "1462400"
  },
  {
    "text": "rag right you know meaning that you vectorize a external data source and then put that into a vector data",
    "start": "1462400",
    "end": "1468919"
  },
  {
    "text": "and with each prompt you search that Vector database for relevant information uh your own knowledge base and then put",
    "start": "1468919",
    "end": "1474480"
  },
  {
    "text": "that into the prompt and ask language model to reply based on that prompt right for things like that if you just",
    "start": "1474480",
    "end": "1480760"
  },
  {
    "text": "have a API server you would have to set up another two chain outside of it typically in line chain or Lama index",
    "start": "1480760",
    "end": "1487799"
  },
  {
    "text": "and uh because you often times you need multiple models to act together you know one model detects what the question is",
    "start": "1487799",
    "end": "1493799"
  },
  {
    "text": "about the other model actually ansers the question so you end up um having a fairly complex m m container setup",
    "start": "1493799",
    "end": "1500840"
  },
  {
    "text": "that's uh that has um you know uh uh you know a python everywhere you know that's",
    "start": "1500840",
    "end": "1506120"
  },
  {
    "text": "so it is uh it is complex and it is um uh difficult to manage right so however",
    "start": "1506120",
    "end": "1512279"
  },
  {
    "text": "with uh with um with a programming environment with a development environment like wasm Edge and llama",
    "start": "1512279",
    "end": "1518000"
  },
  {
    "text": "Edge you are be able to build all this logic into a single application and compile them into a single binary and",
    "start": "1518000",
    "end": "1523559"
  },
  {
    "text": "then distribute that single binary right you know so there are lots of things you can do with this single binary you can can by having all the elements of the uh",
    "start": "1523559",
    "end": "1531279"
  },
  {
    "text": "apis and and everything being programmable you you are be able to um say build R rag functions directly into",
    "start": "1531279",
    "end": "1538600"
  },
  {
    "text": "the API server so that from the outside the user doesn't need to know there's a rag function just ask some question and",
    "start": "1538600",
    "end": "1544440"
  },
  {
    "text": "it would magically give the correct answer you configure the knowledge based on the back end you know instead of having the user have to know what's the",
    "start": "1544440",
    "end": "1550960"
  },
  {
    "text": "knowledge come from right you can do State Management in the API server you can c u provide answers based on the",
    "start": "1550960",
    "end": "1557039"
  },
  {
    "text": "previous history you know and uh you can do integrated call function callings by",
    "start": "1557039",
    "end": "1562799"
  },
  {
    "text": "in the a structure response that is actually a fairly um you know uh a hot research area where you know you have",
    "start": "1562799",
    "end": "1569000"
  },
  {
    "text": "you have prompts and you have fine tuning to make sure that the L always reply in certain Json formats and uh",
    "start": "1569000",
    "end": "1575559"
  },
  {
    "text": "because it's re applying certain Jon formats they can can be consumed by another application we we are seeing a lot of those type of projects in the",
    "start": "1575559",
    "end": "1581880"
  },
  {
    "text": "show FL today um you know in this cucon as well so a lot of those agent applications use especially fine-tuned",
    "start": "1581880",
    "end": "1587720"
  },
  {
    "text": "and especially prompt RMS to come up with structure data and then use another Runner to run the structure data all",
    "start": "1587720",
    "end": "1594200"
  },
  {
    "text": "those needs a different application to work together right you know and uh you have ability to uh call external",
    "start": "1594200",
    "end": "1600440"
  },
  {
    "text": "services for instance if there's a the problem is say refers to some URL you need to go out to get the URL first get",
    "start": "1600440",
    "end": "1607320"
  },
  {
    "text": "download the content first and then try to analyze it with large language model you know so you can also do multiple",
    "start": "1607320",
    "end": "1613279"
  },
  {
    "text": "models in a single service you know so that um you know maybe some some questions are answered by Model A some",
    "start": "1613279",
    "end": "1618720"
  },
  {
    "text": "questions and answers by model B or you can have Model A and model B each come of questions and have model C to",
    "start": "1618720",
    "end": "1624080"
  },
  {
    "text": "summarize the answer or to pick the answer you know architecture similar toe right so there are lots of things you",
    "start": "1624080",
    "end": "1629399"
  },
  {
    "text": "can do and uh um again today you can do that with python or you can do that with by having a lot of glue code to glue",
    "start": "1629399",
    "end": "1636159"
  },
  {
    "text": "them together but with the um but with the Llama Edge that's you will be able to build a single application that's uh",
    "start": "1636159",
    "end": "1644000"
  },
  {
    "text": "using rust or using JavaScript or using go and then compile it down to wasm and then just distribute that wasm um um uh",
    "start": "1644000",
    "end": "1651799"
  },
  {
    "text": "um that wasm file into any uh GPU device that you may have so in the next demo",
    "start": "1651799",
    "end": "1658080"
  },
  {
    "text": "we're going to show one of the rack application that's one of our uh end",
    "start": "1658080",
    "end": "1663320"
  },
  {
    "text": "users in our community build right it's it's it's uh yeah in the next Dem I will uh",
    "start": "1663320",
    "end": "1669960"
  },
  {
    "text": "show you how to build a single binary rug is from our and user is called",
    "start": "1669960",
    "end": "1676360"
  },
  {
    "text": "ganet so I we go back to",
    "start": "1676360",
    "end": "1681158"
  },
  {
    "text": "the",
    "start": "1707120",
    "end": "1710120"
  },
  {
    "text": "uh this is a very draft D demo because they uh haven't been launched",
    "start": "1714240",
    "end": "1720039"
  },
  {
    "text": "sucessfully so it's very very rough so uh in this config J we need to uh config",
    "start": "1720039",
    "end": "1726159"
  },
  {
    "text": "some uh some information that we needed um for example um uh chat is for the uh",
    "start": "1726159",
    "end": "1733279"
  },
  {
    "text": "uh chck here we use the Lama 27b model to chck with uh to chck with the model",
    "start": "1733279",
    "end": "1738799"
  },
  {
    "text": "and we also will use um uh a special uh embedding model to create embeddings and",
    "start": "1738799",
    "end": "1745360"
  },
  {
    "text": "the DAT and the document shows data source uh this is the uh the uh the W",
    "start": "1745360",
    "end": "1752399"
  },
  {
    "text": "mag uh documentation for uh installing um so this is the uh data source install",
    "start": "1752399",
    "end": "1760240"
  },
  {
    "text": "was mat on different o so uh after we have set up the U",
    "start": "1760240",
    "end": "1766519"
  },
  {
    "text": "configurations we can go back to to uh create some Bings for the",
    "start": "1766519",
    "end": "1773200"
  },
  {
    "text": "document ganet also provides uh some sh script to KCK side so I will just use",
    "start": "1776679",
    "end": "1783320"
  },
  {
    "text": "one sh",
    "start": "1783320",
    "end": "1785840"
  },
  {
    "text": "scripts so I will use the uh unit script to uh create uh",
    "start": "1791440",
    "end": "1797039"
  },
  {
    "text": "embeddings uh from the returning log you can see",
    "start": "1797039",
    "end": "1802360"
  },
  {
    "text": "that it will oh sorry uh it will download the all the necessary softwares",
    "start": "1802360",
    "end": "1808279"
  },
  {
    "text": "uh like was my run time and it will install the Kant it's uh Vector DB is",
    "start": "1808279",
    "end": "1815480"
  },
  {
    "text": "written in R and uh it will also download the chat model Lama 27b and it",
    "start": "1815480",
    "end": "1821120"
  },
  {
    "text": "will also download the embedding model to create uh to compute embeddings it's",
    "start": "1821120",
    "end": "1826919"
  },
  {
    "text": "called all mini time and it will also download the uh API server it's a pable",
    "start": "1826919",
    "end": "1832120"
  },
  {
    "text": "one file and it will also download the uh user interface uh then it will create um um",
    "start": "1832120",
    "end": "1839679"
  },
  {
    "text": "collection a Kon collection to u to uh St the data and uh then it will start",
    "start": "1839679",
    "end": "1846600"
  },
  {
    "text": "the uh Lama API server so the server will download the uh the document from",
    "start": "1846600",
    "end": "1852559"
  },
  {
    "text": "the link I provide in the config.js and then it will upload to the uh the",
    "start": "1852559",
    "end": "1858760"
  },
  {
    "text": "document to the L API server and then the uh uh the L API server will uh will",
    "start": "1858760",
    "end": "1865960"
  },
  {
    "text": "chunk the document so a larg uh a large articles will become uh smaller uh",
    "start": "1865960",
    "end": "1872840"
  },
  {
    "text": "chunks and then U the aps ER will call the embedding model to compute the",
    "start": "1872840",
    "end": "1879720"
  },
  {
    "text": "embedding space on the chunks and after the embedding is done it will upload the",
    "start": "1879720",
    "end": "1885399"
  },
  {
    "text": "embeddings to the Kant uh Vector DB and um this is the uh step to create",
    "start": "1885399",
    "end": "1894880"
  },
  {
    "text": "UHS so we already have a many then we can check with the uh CH with lamb to 7B",
    "start": "1898960",
    "end": "1906399"
  },
  {
    "text": "with the amings so I will use another another uh sh",
    "start": "1906399",
    "end": "1912440"
  },
  {
    "text": "script",
    "start": "1916840",
    "end": "1919840"
  },
  {
    "text": "this sh squ will start the K uh instance and then start the Lage again and this s",
    "start": "1926519",
    "end": "1934039"
  },
  {
    "text": "is for uh for run for build an API Ser for the Lama 2B uh for the Lama 2B model",
    "start": "1934039",
    "end": "1941279"
  },
  {
    "text": "so we can U go to uh loc host to chat with the",
    "start": "1941279",
    "end": "1946840"
  },
  {
    "text": "model",
    "start": "1946840",
    "end": "1949840"
  },
  {
    "text": "so uh we will ask the same question how to install W magic because we have given",
    "start": "1963480",
    "end": "1968760"
  },
  {
    "text": "the model um the was magic U documentation for",
    "start": "1968760",
    "end": "1974840"
  },
  {
    "text": "installing",
    "start": "1976840",
    "end": "1979840"
  },
  {
    "text": "so this is a 7B model so it's run slower than the 2B model especially at startup because it needs to load all five",
    "start": "1992840",
    "end": "1999000"
  },
  {
    "text": "gigabytes of uh the model content into the into the memory right but now it's come",
    "start": "1999000",
    "end": "2005200"
  },
  {
    "text": "back based on the provided contexts you can install was time on Mac OS by",
    "start": "2005200",
    "end": "2010559"
  },
  {
    "text": "running the following command and that's exactly the command that's appears in",
    "start": "2010559",
    "end": "2016320"
  },
  {
    "text": "our documentation right you know that's uh the Lama 27b model doesn't have this",
    "start": "2016320",
    "end": "2022399"
  },
  {
    "text": "knowledge it's uh it's a documentation doc that we provided through the um",
    "start": "2022399",
    "end": "2028799"
  },
  {
    "text": "through the vector database right yeah so uh uh this time the model",
    "start": "2028799",
    "end": "2036080"
  },
  {
    "text": "given the uh the right answer so with Lage we can create complex",
    "start": "2036080",
    "end": "2044519"
  },
  {
    "text": "complex applications uh this this is all the demos today but we still have one",
    "start": "2044519",
    "end": "2050679"
  },
  {
    "text": "more thing yeah so so with all this you know",
    "start": "2050679",
    "end": "2056118"
  },
  {
    "text": "we at a cubec you know we have demonstrated command lines we have not have time to show you the source code you know all the source code available",
    "start": "2056119",
    "end": "2062280"
  },
  {
    "text": "you know at the end of this uh the next slides you know you can see the source R source code for the API server for the",
    "start": "2062280",
    "end": "2068040"
  },
  {
    "text": "rack API server and you know things like that but all this runs in kubernetes you know um we have been working with um",
    "start": "2068040",
    "end": "2075679"
  },
  {
    "text": "kubernetes distribution in the community for the past two three years to support um wasam as the first class um uh uh",
    "start": "2075679",
    "end": "2085040"
  },
  {
    "text": "container or artifact in the kubernetes cluster right so we can push wasm applications to dockerhub and label it",
    "start": "2085040",
    "end": "2092158"
  },
  {
    "text": "wasm so when cron or container D PS down that image and sees it's not x8x it's",
    "start": "2092159",
    "end": "2097800"
  },
  {
    "text": "not on 64 but wasm it knows to use a wasm round time like wasm Edge to start",
    "start": "2097800",
    "end": "2103400"
  },
  {
    "text": "it so with that we would be able to run those large language model application we also have Demos in our our website to",
    "start": "2103400",
    "end": "2109920"
  },
  {
    "text": "uh to distribute those um large language models in the uh device agnostic way",
    "start": "2109920",
    "end": "2115320"
  },
  {
    "text": "through a kubernetes cluster right as you can imagine there's lots of things that you can do now with uh um with",
    "start": "2115320",
    "end": "2120880"
  },
  {
    "text": "devices on edge with devices on the server so that's uh so u i I'd really encourage you to to try it out although",
    "start": "2120880",
    "end": "2126760"
  },
  {
    "text": "we don't have time to show a full kubernetes demo we we only have time to show the command line demo yeah so those",
    "start": "2126760",
    "end": "2133119"
  },
  {
    "text": "are the resources of the things that we talked about you know we including all the source code that you see that's to",
    "start": "2133119",
    "end": "2138599"
  },
  {
    "text": "build the regular API PL vanilla API server and the rag API server and uh um",
    "start": "2138599",
    "end": "2144680"
  },
  {
    "text": "you know and uh uh how to run those in in with container tools so I think yeah",
    "start": "2144680",
    "end": "2152920"
  },
  {
    "text": "so that's it thank you thank you",
    "start": "2152920",
    "end": "2159640"
  },
  {
    "text": "so I think our time is up but you know we can stay hang around for a couple minutes so if you have time if you have questions come up come up and ask",
    "start": "2162720",
    "end": "2171119"
  },
  {
    "text": "her",
    "start": "2171119",
    "end": "2174119"
  }
]