[
  {
    "text": "we're going to get started here so this is building massive scale Jer AI services with kubernetes and open source",
    "start": "280",
    "end": "7080"
  },
  {
    "text": "my name is John McBride um I was the head of infrastructure and the lead AI developer at Open sauce which was a",
    "start": "7080",
    "end": "13599"
  },
  {
    "text": "small company recently acquired by the Linux Foundation um and I want to do a quick poll um who has built rag",
    "start": "13599",
    "end": "20680"
  },
  {
    "text": "applications before or any kind of rag thing okay um who's built like a big",
    "start": "20680",
    "end": "26439"
  },
  {
    "text": "kubernetes like tons of gpus a big platform for developers okay a couple people great um",
    "start": "26439",
    "end": "33480"
  },
  {
    "text": "it's funny coming to this talk cuz or I guess coming to this cubec Con cuz in Chicago last year this is the talk I",
    "start": "33480",
    "end": "39440"
  },
  {
    "text": "wish I had uh I built a bunch of this stuff and designed a bunch of this stuff and our our rag applications inside of",
    "start": "39440",
    "end": "45960"
  },
  {
    "text": "open sace um and it was really like bad like this is not going to be the kind of",
    "start": "45960",
    "end": "51480"
  },
  {
    "text": "stuff that you would want to inevitably ship to your Enterprises this was like startup mode we're shipping AI we're",
    "start": "51480",
    "end": "58480"
  },
  {
    "text": "trying to find market fit uh we're going to do what we can to save money um and hit some kind of scale that will be",
    "start": "58480",
    "end": "64799"
  },
  {
    "text": "impressive to the market um our potential investors so on and so forth so um the thing that we ended up",
    "start": "64799",
    "end": "71439"
  },
  {
    "text": "building at open sa is something called Star Search and what it really is is kind of you we called it a co-pilot for",
    "start": "71439",
    "end": "78280"
  },
  {
    "text": "your git history but really the idea was to derive unique insights and understandings off of get uh GitHub PLL",
    "start": "78280",
    "end": "85799"
  },
  {
    "text": "requests and issues and try to go a little deeper than something like GitHub co-pilot um this thing you could you",
    "start": "85799",
    "end": "92560"
  },
  {
    "text": "know get in the chat interface we've all seen these kind of chat interfaces before you could ask a unique question so this question here in this example I",
    "start": "92560",
    "end": "99439"
  },
  {
    "text": "asked you know who are the best developers that know tailwind and are also interested in Rust and then through",
    "start": "99439",
    "end": "104799"
  },
  {
    "text": "poll requests issues a bunch of the metrics and data that we consumed we could then give some interesting answers",
    "start": "104799",
    "end": "111520"
  },
  {
    "text": "um when I first built and stubbed this thing out like many of us I used a a",
    "start": "111520",
    "end": "117560"
  },
  {
    "text": "service just an inference service like anthropic or open Ai and our initial",
    "start": "117560",
    "end": "124360"
  },
  {
    "text": "bill was Absolut absolutely astronomic you can't really see it on the slide here but um this was just like 4 days",
    "start": "124360",
    "end": "132080"
  },
  {
    "text": "and it was already like $4,000 plus Dollar in spend um I did some quick back",
    "start": "132080",
    "end": "137920"
  },
  {
    "text": "of the napkin math and looking at it it was going to be you know something like let me see I got to actually read these",
    "start": "137920",
    "end": "144280"
  },
  {
    "text": "um using gp4 it was going to be like $30 per 1 million tokens input and $60 per 1",
    "start": "144280",
    "end": "150120"
  },
  {
    "text": "million output tokens and again remember this is all like last year when I designed All This and like we were",
    "start": "150120",
    "end": "155519"
  },
  {
    "text": "trying to ship all this so we did not have the benefit of you know a bunch of the great platform engineering things",
    "start": "155519",
    "end": "160959"
  },
  {
    "text": "that people have been talking about uh this cubec Con uh we were going to Target about 40,000 plus GitHub",
    "start": "160959",
    "end": "167400"
  },
  {
    "text": "repositories which was kind of like big enough scale that we could have you know the breadth but also the depth of the",
    "start": "167400",
    "end": "173480"
  },
  {
    "text": "kind of insites we were targeting um we were assuming this back of the napkin math that was going to be about 190",
    "start": "173480",
    "end": "179280"
  },
  {
    "text": "words per day in issues and pull requests which is going to be roughly about $600 per day and was going to hit",
    "start": "179280",
    "end": "184879"
  },
  {
    "text": "us at about $200,000 plus per year and that was quite a lot for our small eight",
    "start": "184879",
    "end": "191799"
  },
  {
    "text": "person team at this you know startup um so finding any way that we could actually reduce that spend was going to",
    "start": "191799",
    "end": "197239"
  },
  {
    "text": "be huge um and talking to my colleagues and even in this talk you know practicing it and going through the",
    "start": "197239",
    "end": "202959"
  },
  {
    "text": "slides I feel a little crazy being like we're going to engineer all this stuff on kubernetes to save money and it's going to be great um but ultimately what",
    "start": "202959",
    "end": "209400"
  },
  {
    "text": "we did is we used rag pipelines on top of kubernetes um again what I would call",
    "start": "209400",
    "end": "215560"
  },
  {
    "text": "maybe an arbitrary approach but what really worked for us to hit that big scale that 40,000 plus GitHub",
    "start": "215560",
    "end": "221439"
  },
  {
    "text": "repositories chugging through terabytes of data every day um giving us the kind of product feature that we were looking",
    "start": "221439",
    "end": "227519"
  },
  {
    "text": "for so in a big broad summary the way this thing works is it consumes the",
    "start": "227519",
    "end": "233200"
  },
  {
    "text": "GitHub events feed uh via this thing that we would like to call the fire hose and you can look at it today it's at",
    "start": "233200",
    "end": "239000"
  },
  {
    "text": "api. github.com events and each time you hit that it gives you a little bit of Time series",
    "start": "239000",
    "end": "244799"
  },
  {
    "text": "data excuse me um each time you go to that API endpoint and you get you know",
    "start": "244799",
    "end": "250000"
  },
  {
    "text": "just kind of a consistent feed of events always happening on GitHub um if anybody's familiar with the GitHub",
    "start": "250000",
    "end": "255560"
  },
  {
    "text": "archive that's what that's based on so we consume that feed U we use kubernetes to orchestrate um the inference engine",
    "start": "255560",
    "end": "263199"
  },
  {
    "text": "on top of a couple of gpus um and then we use time scale uh and PG vector to",
    "start": "263199",
    "end": "270120"
  },
  {
    "text": "give us the embeddings to actually do Vector search um and then open AI on the tail end of all that so um I want to",
    "start": "270120",
    "end": "276800"
  },
  {
    "text": "dive deep into how this actually worked uh how we hacked a bunch of this together what it looks like today um and",
    "start": "276800",
    "end": "282199"
  },
  {
    "text": "some lessons and learnings from this whole product offering so again on the very front end is this thing that we",
    "start": "282199",
    "end": "288199"
  },
  {
    "text": "would call the the GitHub events fire hose and really it's just this like constant stream of events always coming",
    "start": "288199",
    "end": "295240"
  },
  {
    "text": "in and we built this little micros service called It's Pizza time um everything at SAU was a pizza pun so get",
    "start": "295240",
    "end": "301880"
  },
  {
    "text": "ready for more and more pizza puns in this talk uh but its Pizza Time was essentially a nozzle around this fire",
    "start": "301880",
    "end": "308160"
  },
  {
    "text": "hose that we could consume all of these GitHub events um and all of this time series data to give us the kind of uh",
    "start": "308160",
    "end": "314680"
  },
  {
    "text": "constant flow of what's happening on GitHub all the time um yes it's a pun",
    "start": "314680",
    "end": "320080"
  },
  {
    "text": "from the Spider-Man movie but uh what do data ends up looking like when we when",
    "start": "320080",
    "end": "326000"
  },
  {
    "text": "it actually lands in its Pizza time um is it has a time stamp it has has a specific type like PLL request opened or",
    "start": "326000",
    "end": "332960"
  },
  {
    "text": "issue opened or issue closed pushes uh all different kinds of stuff and then a",
    "start": "332960",
    "end": "338319"
  },
  {
    "text": "bunch of metadata and a bunch of content inside of that metadata uh it's Pizza time will process it just a little bit",
    "start": "338319",
    "end": "344600"
  },
  {
    "text": "um and then send it into time scale which is a postgress extension for time series database stuff um and then",
    "start": "344600",
    "end": "351560"
  },
  {
    "text": "ultimately that's that's what gives us open SAU as it was before all this AI stuff was the histograms and the charts",
    "start": "351560",
    "end": "358720"
  },
  {
    "text": "and you know anybody who's familiar with like the graphon product offerings and Prometheus and that whole time series",
    "start": "358720",
    "end": "364800"
  },
  {
    "text": "database stuff you know this this is pretty par for the course as far as uh time series things go but we wanted to",
    "start": "364800",
    "end": "370400"
  },
  {
    "text": "take it a little bit further and we wanted to take that time series data and all that content from poll requests and",
    "start": "370400",
    "end": "375880"
  },
  {
    "text": "issues and try to derive some interesting things off of uh generative AI so um using all of that stuff we",
    "start": "375880",
    "end": "382599"
  },
  {
    "text": "built something called the star search EMB better and going to a bunch of the talks this week this is basically C flow",
    "start": "382599",
    "end": "390639"
  },
  {
    "text": "or q but like a tiny bad go microservice that I built again at the time we didn't",
    "start": "390639",
    "end": "396440"
  },
  {
    "text": "really have the benefit of these things being ubiquitous so um learn from my learnings and you know take something",
    "start": "396440",
    "end": "402000"
  },
  {
    "text": "off the shelf don't build your own thing but what star Surge and better basically does is it looks for some content inside",
    "start": "402000",
    "end": "407680"
  },
  {
    "text": "of the time series database um based on some filters based on some criteria it grabs that content and then it uses our",
    "start": "407680",
    "end": "414919"
  },
  {
    "text": "kubernetes platform um with a bunch of the gpus and the inference service um to",
    "start": "414919",
    "end": "420120"
  },
  {
    "text": "do a few things primarily uh what we're concerned with at this step is actually creating a a summary of that content and",
    "start": "420120",
    "end": "428720"
  },
  {
    "text": "why we wanted to do that was because the content was very very messy if you've ever looked at like the raw body of a",
    "start": "428720",
    "end": "435000"
  },
  {
    "text": "poll request or an issue it's got tags it's got a bunch of like markdown Cru it's got like all kinds of things that",
    "start": "435000",
    "end": "441759"
  },
  {
    "text": "honestly will just con we just uh confuse an llm it'll look at that and just be like oh like what is this person",
    "start": "441759",
    "end": "448039"
  },
  {
    "text": "doing there's a bunch of markdown a bunch of random code and stuff so we needed a specific service to actually",
    "start": "448039",
    "end": "453520"
  },
  {
    "text": "summarize that content um that we could then use in embeddings to get better Vector search and we'll look at that as",
    "start": "453520",
    "end": "459400"
  },
  {
    "text": "we kind of go through this flow um but before we go too deep into you know the rest of the flow oh yes generate a",
    "start": "459400",
    "end": "465720"
  },
  {
    "text": "summary for the content that's all that stuff um I do want to take a deep dive on the kubernetes stuff specifically and",
    "start": "465720",
    "end": "472599"
  },
  {
    "text": "this small inference platform that we built so in the end the way that you know I kind of like to think about",
    "start": "472599",
    "end": "478440"
  },
  {
    "text": "kubernetes is you know it's just a way to like get some compute um it's all networked together you can build some",
    "start": "478440",
    "end": "484720"
  },
  {
    "text": "services on top of it and ultimately you know uh bring some value to your customers or to your end Engineers doing",
    "start": "484720",
    "end": "490639"
  },
  {
    "text": "inference and all this stuff so these uh nodes on this kubernetes cluster are a couple of T4 gpus on Azure and we used",
    "start": "490639",
    "end": "498400"
  },
  {
    "text": "manage service off of AKs um to essentially give us kind of that base layer of compute um I just went to the",
    "start": "498400",
    "end": "505440"
  },
  {
    "text": "last talk about Dr and like I just it blew my mind because I had to do a bunch of this by hand to actually get the gpus",
    "start": "505440",
    "end": "512279"
  },
  {
    "text": "label them correctly get the drivers on all that stuff um so the way that we actually put the drivers on these is",
    "start": "512279",
    "end": "518399"
  },
  {
    "text": "installing the Nvidia device plug-in Damon Set uh which you can use the node taints and the node selectors to",
    "start": "518399",
    "end": "524159"
  },
  {
    "text": "actually make that happen um and shout out to the AKs team for making that pretty easy to follow along with but um",
    "start": "524159",
    "end": "530399"
  },
  {
    "text": "was a pretty high touch solution um definitely check out Dr for some of the awesome stuff happening there with uh",
    "start": "530399",
    "end": "536680"
  },
  {
    "text": "acquiring devices for these things so um once we have the gpus and once we have",
    "start": "536680",
    "end": "542480"
  },
  {
    "text": "kind of that whole node pool set up um then we can install something called VM",
    "start": "542480",
    "end": "547720"
  },
  {
    "text": "and VM is an awesome piece of software for essentially serving your your llm um",
    "start": "547720",
    "end": "555120"
  },
  {
    "text": "and at the time was necessary for us because uh again at the time it was the",
    "start": "555120",
    "end": "560920"
  },
  {
    "text": "only one that seemed to support concurrent clients uh concurrent clients",
    "start": "560920",
    "end": "566279"
  },
  {
    "text": "well um I think AMA does this now and I think ll. P does this now but we were essentially like kind of backed into a",
    "start": "566279",
    "end": "572360"
  },
  {
    "text": "corner was like we want to like use open large language models but like how are we going to do this without you know",
    "start": "572360",
    "end": "578560"
  },
  {
    "text": "spending a ton of money on open AI anthropic etc etc so uh VM is great um",
    "start": "578560",
    "end": "584240"
  },
  {
    "text": "the way I sort of think about it it's like you know you got your llm it's a bucket it's serving it out of the bucket",
    "start": "584240",
    "end": "589920"
  },
  {
    "text": "um and what makes VM also really interesting is it uses something called paged attention memory which uh allows",
    "start": "589920",
    "end": "596600"
  },
  {
    "text": "it to effectively use the memory on your gpus for concurrent clients um my",
    "start": "596600",
    "end": "603000"
  },
  {
    "text": "background is not in data science or like in Python really at all but uh it",
    "start": "603000",
    "end": "608480"
  },
  {
    "text": "works really well definitely recommend it go check it out uh what else did I put on here so yeah it serves an open",
    "start": "608480",
    "end": "613720"
  },
  {
    "text": "large language model that you can get off of hugging face pretty easily using VM um and then serve that concurrent um",
    "start": "613720",
    "end": "620440"
  },
  {
    "text": "open AI compatible API yes that's a very important part of this as well um VM uses an Opa an open AI API that looks",
    "start": "620440",
    "end": "628959"
  },
  {
    "text": "just like like it like it it acts just like it um we could very easily lift and shift our clients to use VM instead of",
    "start": "628959",
    "end": "637279"
  },
  {
    "text": "open API open AI excuse me um okay moving on so the open large language",
    "start": "637279",
    "end": "643880"
  },
  {
    "text": "model going even deeper is another important part about this um there's just a bunch of these out there we know",
    "start": "643880",
    "end": "649440"
  },
  {
    "text": "about the mixt ones we know about the Llama ones um but there's even like really good Community ones that are very",
    "start": "649440",
    "end": "655920"
  },
  {
    "text": "small like 7 billion parameters that enable us to use them um without having",
    "start": "655920",
    "end": "661360"
  },
  {
    "text": "to you know have these ginormous images that then you know slow down node startup time because we have the gpus if",
    "start": "661360",
    "end": "668240"
  },
  {
    "text": "we're bringing up like a bunch of spot gpus or something uh so some of the benefits I see of these small open large",
    "start": "668240",
    "end": "674920"
  },
  {
    "text": "language models come on um they're usually easily and freely accessible um the kind of idea around what quote",
    "start": "674920",
    "end": "681720"
  },
  {
    "text": "unquote open source large language models mean is still kind of you know maybe out for the count but um the oci",
    "start": "681720",
    "end": "688600"
  },
  {
    "text": "has recently some good distinctions around what they would classify as open source large language models but usually",
    "start": "688600",
    "end": "694480"
  },
  {
    "text": "you know for the startup you know startup life you can just go get them on hugging face and they're usually easily",
    "start": "694480",
    "end": "699519"
  },
  {
    "text": "and freely available um they often are more permissively licensed I am not a",
    "start": "699519",
    "end": "705480"
  },
  {
    "text": "lawyer so please check with your legal department but usually you can kind of modify them you can requ quantize them",
    "start": "705480",
    "end": "711079"
  },
  {
    "text": "you can kind of tweak them you can usually easily find- tune them um but again check with your legal department",
    "start": "711079",
    "end": "716760"
  },
  {
    "text": "before putting those into production uh they're very small they're very small and very efficient for most use cases",
    "start": "716760",
    "end": "722639"
  },
  {
    "text": "and especially for something that revolves around uh natural language processing like in our case generating a",
    "start": "722639",
    "end": "728920"
  },
  {
    "text": "summary um this was huge it was just so simple to plug and play with small models that could just give us a a",
    "start": "728920",
    "end": "735920"
  },
  {
    "text": "pretty good summary um okay so rolling all that back back into the cluster we have the Damon",
    "start": "735920",
    "end": "741839"
  },
  {
    "text": "sets we have the LM uh VM kind of does its thing and gets the model for it's off hugging face um and then at this",
    "start": "741839",
    "end": "748959"
  },
  {
    "text": "point point we can actually enable a service um a good oldfashioned kubernetes service um and for more",
    "start": "748959",
    "end": "755000"
  },
  {
    "text": "advanced cases this could be something that you know maybe is you know we just put a a normal old kubernetes service up",
    "start": "755000",
    "end": "761399"
  },
  {
    "text": "there but maybe you want to use some sort of internal service mesh or something but for our cases uh kubernetes service this enabled us to",
    "start": "761399",
    "end": "768519"
  },
  {
    "text": "use that open AI compatible API as if it was open AI internal to our cluster for",
    "start": "768519",
    "end": "774600"
  },
  {
    "text": "those other services what this looks like is you know again good old fashioned kubernetes DNS VM service. VM",
    "start": "774600",
    "end": "782399"
  },
  {
    "text": "namespace SVC cluster. loal and then uh we can use that openai compatible API at",
    "start": "782399",
    "end": "789240"
  },
  {
    "text": "V1 chat completions so how do we manage all this",
    "start": "789240",
    "end": "794760"
  },
  {
    "text": "um good oldfashioned devops and GE Ops nothing special here and I I think that",
    "start": "794760",
    "end": "800360"
  },
  {
    "text": "you know just worked for us really really well we use pumi uh with infrastructure as code for managing and",
    "start": "800360",
    "end": "805720"
  },
  {
    "text": "bootstrapping all those clusters we have several different environments for testing validation which mostly just end",
    "start": "805720",
    "end": "811880"
  },
  {
    "text": "up being their own monolithic clusters um we use GitHub actions for automatic container builds and deployments and",
    "start": "811880",
    "end": "817959"
  },
  {
    "text": "pushing those up to azure's container registry um and then Graff alerting on call and basic validation with some of",
    "start": "817959",
    "end": "824560"
  },
  {
    "text": "their semantic alerts so nothing fancy like again this time last year I think",
    "start": "824560",
    "end": "829959"
  },
  {
    "text": "the sort of how do we do ml Ops was still an open question um so I think",
    "start": "829959",
    "end": "835440"
  },
  {
    "text": "there's some great offerings from some companies now um that make deeper validation of your services uh much",
    "start": "835440",
    "end": "841759"
  },
  {
    "text": "better so definitely check them out um a quick demo so what I want to show here is actually uh you know this is the part",
    "start": "841759",
    "end": "849079"
  },
  {
    "text": "of the demo we all get to look at yaml and stuff but I'm actually show kind of how this would work from inside of the",
    "start": "849079",
    "end": "854120"
  },
  {
    "text": "cluster as well um so first going into the Nvidia device plug-in Damon set um",
    "start": "854120",
    "end": "859959"
  },
  {
    "text": "is this font size okay bigger smaller all right seems good um so the Nvidia device plugin Damon set that gives us",
    "start": "859959",
    "end": "867399"
  },
  {
    "text": "the actual ability to use those deves on cluster uh we have the VM namespace we",
    "start": "867399",
    "end": "874279"
  },
  {
    "text": "have uh a VM secret which specifically for vlm gives us the hugging face token",
    "start": "874279",
    "end": "879399"
  },
  {
    "text": "so that we can actually pull a model from uh hugging face and if people aren't familiar with hugging face it's",
    "start": "879399",
    "end": "885240"
  },
  {
    "text": "kind of the GitHub of of models and you know being able to see these things inspect these things modify these things",
    "start": "885240",
    "end": "892199"
  },
  {
    "text": "um the vlon Damon set is interesting where we have the specific notes selector and then the actual toleration",
    "start": "892199",
    "end": "899279"
  },
  {
    "text": "for the skew being GPU um and this was kind of our just approach to making sure",
    "start": "899279",
    "end": "905160"
  },
  {
    "text": "that every single uh every single node inside of the GPU node pool um could",
    "start": "905160",
    "end": "910880"
  },
  {
    "text": "have that and I think there's yeah just going to be so many better ways to do this in the future in kubernetes besides",
    "start": "910880",
    "end": "916399"
  },
  {
    "text": "having to essentially handroll selection for those Damon sets on those nodes um",
    "start": "916399",
    "end": "921680"
  },
  {
    "text": "inside of VM we just grab the VM uh open AI pod from their Upstream",
    "start": "921680",
    "end": "929720"
  },
  {
    "text": "um and then some arguments and these are kind of interesting we grab uh this model from the bloke who was kind of a",
    "start": "929720",
    "end": "936160"
  },
  {
    "text": "pfic guy uh quantizing a bunch of models at the time so this is just a mistal 7 billion parameter model um it uses a",
    "start": "936160",
    "end": "943440"
  },
  {
    "text": "specific uh quantization U called awq and then we also defined that it should",
    "start": "943440",
    "end": "949120"
  },
  {
    "text": "use 95% of the gpu's memory and this was some parameters we had to tweak around",
    "start": "949120",
    "end": "955440"
  },
  {
    "text": "with and try to you kind of play with to ensure we were getting the most usage out of just the few gpus we had cuz you",
    "start": "955440",
    "end": "961600"
  },
  {
    "text": "know again we're still trying to save money even with gpus that we're using on kubernetes um but it also was low enough",
    "start": "961600",
    "end": "968279"
  },
  {
    "text": "that we weren't seeing Hardware fault tolerations I think if we were you know a bigger engineering department you know",
    "start": "968279",
    "end": "974279"
  },
  {
    "text": "not of one person we would have gone a little deeper into that tried to find out you know like what are ways we could",
    "start": "974279",
    "end": "980240"
  },
  {
    "text": "optimize maybe start using spot gpus more often um in the cases we have load",
    "start": "980240",
    "end": "985360"
  },
  {
    "text": "etc etc okay um on the cluster I also have this this simple pod this is just an auntu pod let me flip over to that",
    "start": "985360",
    "end": "992880"
  },
  {
    "text": "real quick and on this pod you know it's just good old fashioned auntu it's got curl",
    "start": "992880",
    "end": "999480"
  },
  {
    "text": "it's got dig um let me go to my notes real quick and grab",
    "start": "999480",
    "end": "1004560"
  },
  {
    "text": "this real quick so this dig is just looking at VM service. vlm namespace SVC",
    "start": "1004560",
    "end": "1010959"
  },
  {
    "text": "etc etc and we can just validate real quick that we have an IP good old fashioned kubernetes service nothing",
    "start": "1010959",
    "end": "1016839"
  },
  {
    "text": "fancy there um then down here here we can actually use that service I'm going to put this in here and this is using",
    "start": "1016839",
    "end": "1023519"
  },
  {
    "text": "that open AI compatible API for V1 models if I go to that that sends that back right away because this is running",
    "start": "1023519",
    "end": "1029839"
  },
  {
    "text": "on that same node um so almost instantaneous and we can see that that model the mistal 7 billion parameter one",
    "start": "1029839",
    "end": "1035760"
  },
  {
    "text": "is running um or available I guess on this service so the next thing that we",
    "start": "1035760",
    "end": "1041199"
  },
  {
    "text": "would want to do is some inference to actually get some text generated so I'm going to use Curl here put this in here",
    "start": "1041199",
    "end": "1049080"
  },
  {
    "text": "and what we get is we're using uh we're specifying the model with the mistal 7 billion parameter one we have a message",
    "start": "1049080",
    "end": "1054960"
  },
  {
    "text": "with the role user and some content where we're asking who won the World Series in 2020 we send that away this",
    "start": "1054960",
    "end": "1060679"
  },
  {
    "text": "will take just a hot second and it said the Tampa Bay Rays won the World Series in 2020 I don't",
    "start": "1060679",
    "end": "1067520"
  },
  {
    "text": "actually watch baseball so great Go Tampa Ray uh but in essence that is all",
    "start": "1067520",
    "end": "1073960"
  },
  {
    "text": "on our cluster um on our gpus running through VM running through that Mr model",
    "start": "1073960",
    "end": "1079520"
  },
  {
    "text": "um to do some inference for you know this demo pod now replace demo pod with anything else that might be running on",
    "start": "1079520",
    "end": "1085080"
  },
  {
    "text": "your cluster to actually do inference so rolling all the way back now um we can",
    "start": "1085080",
    "end": "1091039"
  },
  {
    "text": "go back to the star search and better which you know replace this with Q replace this with one of your own microservices doing inference uh with uh",
    "start": "1091039",
    "end": "1099280"
  },
  {
    "text": "Cube flow any of that stuff and now we can actually get those summaries from our GitHub content which at this point",
    "start": "1099280",
    "end": "1106480"
  },
  {
    "text": "are very very well cleaned up um and again giving you kind of the idea of scale some 40,000 GitHub repositories",
    "start": "1106480",
    "end": "1113039"
  },
  {
    "text": "the way we filtered for those were the top 40,000 repositories starred on GitHub so things like kubernetes",
    "start": "1113039",
    "end": "1118840"
  },
  {
    "text": "kubernetes um things like home assistant etc etc so this was a huge sort of win",
    "start": "1118840",
    "end": "1124320"
  },
  {
    "text": "for us um the cost Factor here instead of using open AI was like a couple",
    "start": "1124320",
    "end": "1131120"
  },
  {
    "text": "thousand a month or something uh I think around 1,500 a month depending on our spot GPU usage so this was a huge win",
    "start": "1131120",
    "end": "1138200"
  },
  {
    "text": "from just a C optimization standpoint with an engineering department of one um",
    "start": "1138200",
    "end": "1143320"
  },
  {
    "text": "at a small company that was you know trying to find uh and bring some sort of AI inference product to Market um so",
    "start": "1143320",
    "end": "1150720"
  },
  {
    "text": "continuing through the flow we have that piece of content that's been generated uh that summary um and we want to use a",
    "start": "1150720",
    "end": "1156480"
  },
  {
    "text": "text embedding model at this point to get a vector for that content and in",
    "start": "1156480",
    "end": "1162280"
  },
  {
    "text": "this case we were continuing to use open AIS of text embedding models which was",
    "start": "1162280",
    "end": "1168080"
  },
  {
    "text": "was great you know you know still very cheap text embedding is not expensive um and we didn't want to lift and shift too many things all at once because that",
    "start": "1168080",
    "end": "1174799"
  },
  {
    "text": "would have just been unsustainable what the text embedding model gives us is a vector which is just a list of numbers",
    "start": "1174799",
    "end": "1180520"
  },
  {
    "text": "um really the way I sort of like to think about it is kind of this model's quote unquote understanding of that text",
    "start": "1180520",
    "end": "1187280"
  },
  {
    "text": "content content that you give it uh which then you can use you know in Vector space to find out the",
    "start": "1187280",
    "end": "1193320"
  },
  {
    "text": "differentiation between other vectors in Vector space it's really just a way to do Vector search which is just a way in the end to do",
    "start": "1193320",
    "end": "1199400"
  },
  {
    "text": "uh uh nearest neighbor search uh with other content so uh we take that Vector",
    "start": "1199400",
    "end": "1204640"
  },
  {
    "text": "we store it in a vector store uh PG Vector in this case because we were like all in on postgress at this point we",
    "start": "1204640",
    "end": "1210159"
  },
  {
    "text": "didn't want to adopt too many technologies that were going to be too messy for our small engineering team um",
    "start": "1210159",
    "end": "1215520"
  },
  {
    "text": "and all of those vectors inside a PG Vector um star searching better but is gone thanks star searching better uh we",
    "start": "1215520",
    "end": "1223159"
  },
  {
    "text": "take all that content and all those vectors and all those references to that content and in inevitably that's what",
    "start": "1223159",
    "end": "1229600"
  },
  {
    "text": "gives us the capability to do a cosine similarity search that nearest neighbor search uh which is really uh yeah a way",
    "start": "1229600",
    "end": "1237400"
  },
  {
    "text": "for us to take content figure out what kind of quote unquote meaning is from those text embedding models etc etc uh",
    "start": "1237400",
    "end": "1244960"
  },
  {
    "text": "we used an index called hnsw which is typically recommended for text type",
    "start": "1244960",
    "end": "1250400"
  },
  {
    "text": "content um but you do trade off some accuracy so using hnsw in your cosign",
    "start": "1250400",
    "end": "1256919"
  },
  {
    "text": "similarity Vector search you're not always going to get the same result every single time and for most chat rag",
    "start": "1256919",
    "end": "1263360"
  },
  {
    "text": "type applications that's okay that's even kind of desired where you don't always want the bot to be like saying",
    "start": "1263360",
    "end": "1268760"
  },
  {
    "text": "the same thing every single time but you do you do trade off some uh uh some accuracy so we we did do some additional",
    "start": "1268760",
    "end": "1276480"
  },
  {
    "text": "steps with PG Vector of kind of separating some of the tables and trying to Shard those in interesting ways but I",
    "start": "1276480",
    "end": "1281520"
  },
  {
    "text": "won't get too into that um that helped us a bit with performance uh but PG Vector even in the last year has had",
    "start": "1281520",
    "end": "1287840"
  },
  {
    "text": "massive Str Ides in performance and capability so I'm a huge fan of PG Vector um I think it's an awesome",
    "start": "1287840",
    "end": "1293480"
  },
  {
    "text": "Community project so that's kind of the whole like backend system we've like embedded a bunch of stuff um we have a",
    "start": "1293480",
    "end": "1299360"
  },
  {
    "text": "bunch of like AI text summaries that can help us you know keep things clean um and now we can actually look at star",
    "start": "1299360",
    "end": "1305799"
  },
  {
    "text": "search uh which is kind of the more product frontend offering and really Star Search is like three or four agents",
    "start": "1305799",
    "end": "1313640"
  },
  {
    "text": "all in a trench coat um and when I say agents you know I think a lot of people love to throw around the word agents",
    "start": "1313640",
    "end": "1319279"
  },
  {
    "text": "it's really just this idea of function calling inside of your code um and we'll look at how that specifically works but",
    "start": "1319279",
    "end": "1325760"
  },
  {
    "text": "most models today um support function calling open AI was one to really sort",
    "start": "1325760",
    "end": "1330919"
  },
  {
    "text": "of pioneer this um and inevitably decisions we made last year we did use",
    "start": "1330919",
    "end": "1336480"
  },
  {
    "text": "open AI function callings with uh gp4 I think it was just gp4 at the time um so",
    "start": "1336480",
    "end": "1342559"
  },
  {
    "text": "let's get into that um the way this works is this is kind of a multi-agent approach again open AI function calling",
    "start": "1342559",
    "end": "1348960"
  },
  {
    "text": "um using our existing Services architecture in our API in our backend so we have a bunch of the stuff sort of",
    "start": "1348960",
    "end": "1355679"
  },
  {
    "text": "split up with specific uh database calls and specific services that these these",
    "start": "1355679",
    "end": "1361720"
  },
  {
    "text": "AI agents can essentially use inside of our backend so the very first AI agent that sort of receives a query from a",
    "start": "1361720",
    "end": "1368880"
  },
  {
    "text": "user is one that we call the pre-processor agent and the pre-processor agent has its toolkit",
    "start": "1368880",
    "end": "1374200"
  },
  {
    "text": "that's a bunch of the functions that it has inside of inside of the function calling stuff and its main job is to",
    "start": "1374200",
    "end": "1380279"
  },
  {
    "text": "detect prompt injection attacks clean up users prompts um and attempt to correct any like obvious spelling mistakes that",
    "start": "1380279",
    "end": "1387320"
  },
  {
    "text": "somebody might make um so if you misspell kubernetes it can usually catch that uh at that point once it's sort of",
    "start": "1387320",
    "end": "1393840"
  },
  {
    "text": "done its job and cleaned up a bunch of stuff and looked at the prompt and make sure that there's something wonky going on um it will then hand it off to the",
    "start": "1393840",
    "end": "1400799"
  },
  {
    "text": "manager agent and this is kind of the central agent that decides which of the",
    "start": "1400799",
    "end": "1405960"
  },
  {
    "text": "other agents to hand off questions to um the manager agent will look at the query",
    "start": "1405960",
    "end": "1411120"
  },
  {
    "text": "it it has its toolkit of a bunch of other agents that it can call uh the",
    "start": "1411120",
    "end": "1416200"
  },
  {
    "text": "Bing search agent the poll request agent uh the issues agent uh we even have a",
    "start": "1416200",
    "end": "1421520"
  },
  {
    "text": "release agent that we built that could tell you about new releases that are happening on GitHub and in certain projects um so on and so forth this was",
    "start": "1421520",
    "end": "1428440"
  },
  {
    "text": "extremely extensible and we would have continued to build more and more capabilities into these agents um but",
    "start": "1428440",
    "end": "1434640"
  },
  {
    "text": "let's say in this example it decided to use the poll request agent maybe somebody's asked asking about recent",
    "start": "1434640",
    "end": "1440120"
  },
  {
    "text": "poll requests in kubernetes kubernetes it would decide you know obviously based on asking about poll requests to use",
    "start": "1440120",
    "end": "1447000"
  },
  {
    "text": "this agent so the manager agent is going to call up the poll request agent it's going to hand off the prompt to that",
    "start": "1447000",
    "end": "1452679"
  },
  {
    "text": "agent again it keeps going uh this agent has its toolkit of a bunch of stuff that it can do U like performing Vector",
    "start": "1452679",
    "end": "1459400"
  },
  {
    "text": "search and summarization of those results from Vector search um doing inference uh from a bunch of these other",
    "start": "1459400",
    "end": "1465600"
  },
  {
    "text": "parallel things in its toolkit um but ultimately again this is using open AI function calling um to ultimately do uh",
    "start": "1465600",
    "end": "1473279"
  },
  {
    "text": "to grab relevant content from our our Vector storage uh so in this case again",
    "start": "1473279",
    "end": "1478720"
  },
  {
    "text": "in this example it's going to use Vector search um and I'll fly through this real quick because this kind of gets into more of like how does rag work and most",
    "start": "1478720",
    "end": "1485760"
  },
  {
    "text": "people kind of understand that that SCS so we're going to use the text embedding model uh we're going to grab our Vector",
    "start": "1485760",
    "end": "1492120"
  },
  {
    "text": "which again is just a list of numbers and again this is just the list of numbers the quote unquote understanding",
    "start": "1492120",
    "end": "1498600"
  },
  {
    "text": "uh of that question and that question then we can compare using a cosign",
    "start": "1498600",
    "end": "1504360"
  },
  {
    "text": "similarity search nearest neighbor uh to actually grab the most relevant content the nearest neighbor specifically and",
    "start": "1504360",
    "end": "1511159"
  },
  {
    "text": "get that stuff um this then gets into kind of the prompt engineering part of",
    "start": "1511159",
    "end": "1516760"
  },
  {
    "text": "the talk uh and really the way I like to think about prompt engineering is really a preamble the context and then the",
    "start": "1516760",
    "end": "1523880"
  },
  {
    "text": "question itself um the Preamble is just going to be your system message which is something like you are a helpful agent",
    "start": "1523880",
    "end": "1529799"
  },
  {
    "text": "you know about PLL requests you know this about GitHub you know more or less basic stuff um that relevant content is",
    "start": "1529799",
    "end": "1537320"
  },
  {
    "text": "then going to go inside of the context part of your prompt um you sort of inject that in and then the question",
    "start": "1537320",
    "end": "1543000"
  },
  {
    "text": "really is just the user prompt ultimately uh which you can then kind of give it uh a preamble even inside of the",
    "start": "1543000",
    "end": "1549760"
  },
  {
    "text": "system message to tell it like hey you're going to see a question about poll requests your job is to ABC XYZ so",
    "start": "1549760",
    "end": "1556320"
  },
  {
    "text": "um that's sort of how I like to think about prompt engineering but this space moves so fast there's there's so much",
    "start": "1556320",
    "end": "1561760"
  },
  {
    "text": "happening so quickly so I'm sure even that has changed so at this point um we've sort of done the rag um open a AI",
    "start": "1561760",
    "end": "1569360"
  },
  {
    "text": "function calling Loop grabs all of that um it generates a sort of answer to all",
    "start": "1569360",
    "end": "1574480"
  },
  {
    "text": "of that it hands it all the way back up the stack to the manager agent and then the manager agent looks at that and",
    "start": "1574480",
    "end": "1579960"
  },
  {
    "text": "returns it to the user to ultimately give a context aware answer so that's you know basically rag 101 with our own",
    "start": "1579960",
    "end": "1586799"
  },
  {
    "text": "kubernetes platform on top of it um all of that sat on top our API uh",
    "start": "1586799",
    "end": "1593360"
  },
  {
    "text": "which sat on api. open sauce. pizza which was all within sort of the kubernetes stuff um and you might see",
    "start": "1593360",
    "end": "1599200"
  },
  {
    "text": "where I'm going with this kind of thinking to the Future um some practical lessons and tips here um using llms to",
    "start": "1599200",
    "end": "1606360"
  },
  {
    "text": "pre-bake those summaries was huge for us um we saw massive problems with like ungroomed text um especially stuff that",
    "start": "1606360",
    "end": "1614720"
  },
  {
    "text": "people could just like slap anything in you would not believe the kind of crap that people put on GitHub that would",
    "start": "1614720",
    "end": "1620960"
  },
  {
    "text": "confuse these llms to every degree so just having a service that could be like hey clean this up please was huge um",
    "start": "1620960",
    "end": "1628279"
  },
  {
    "text": "using indexes like hnsw to trade off for some accuracy was big um that's a big performance upgrade in our Vector search",
    "start": "1628279",
    "end": "1635279"
  },
  {
    "text": "um again huge huge cost savings using small llm inference um just a small 7",
    "start": "1635279",
    "end": "1640679"
  },
  {
    "text": "billion parameter mod model was good enough for us and I would argue um is good enough for most natural language",
    "start": "1640679",
    "end": "1647720"
  },
  {
    "text": "tasks there's not a lot that I think ginormous models are needed for usually",
    "start": "1647720",
    "end": "1653520"
  },
  {
    "text": "uh don't quote me on that uh challenges um again I think the like mlops",
    "start": "1653520",
    "end": "1659760"
  },
  {
    "text": "observability validation question still seems to be unanswered it was definitely unanswered this time last year when you",
    "start": "1659760",
    "end": "1666399"
  },
  {
    "text": "know I was first conceptualizing and architecting a bunch of this so um I'd love to see more standards I know that open Telemetry is definitely looking at",
    "start": "1666399",
    "end": "1673440"
  },
  {
    "text": "this so um I think there's some exciting stuff to come in this space soon um getting dedicated gpus even tiny t4s was",
    "start": "1673440",
    "end": "1681360"
  },
  {
    "text": "still hard uh the I mean even today like I I went to like 50 talks this",
    "start": "1681360",
    "end": "1686480"
  },
  {
    "text": "conference about how hard it is to get gpus on the public Cloud um I don't have a solution for that um using those open",
    "start": "1686480",
    "end": "1693519"
  },
  {
    "text": "models on VM trained for function calling so again like I said we used",
    "start": "1693519",
    "end": "1698600"
  },
  {
    "text": "open ai's function calling Loops um and I think there's an opportunity to use one of the public models within our kind",
    "start": "1698600",
    "end": "1706600"
  },
  {
    "text": "of uh kubernetes platform with our gpus to really bring it all inside of our own",
    "start": "1706600",
    "end": "1711799"
  },
  {
    "text": "platform um the kind of two things that were left on top of open aai were sort",
    "start": "1711799",
    "end": "1717399"
  },
  {
    "text": "of the final the final inference result H and the well and text embedding uh and",
    "start": "1717399",
    "end": "1723679"
  },
  {
    "text": "the actual uh function calling stuff so um huge opportunity for us to move it",
    "start": "1723679",
    "end": "1729039"
  },
  {
    "text": "all in house and do even more cost savings if we wanted uh ultimately I",
    "start": "1729039",
    "end": "1734320"
  },
  {
    "text": "think the product scope for this should have been a little narrower I'm going to put on my product person hat right now and you know targeting 40,000 plus",
    "start": "1734320",
    "end": "1741240"
  },
  {
    "text": "GitHub repositories was a fun challenge to bring to Market uh but I think in",
    "start": "1741240",
    "end": "1746640"
  },
  {
    "text": "hindsight I think we would have had more success um and more validation in the market if we had targeted like the top",
    "start": "1746640",
    "end": "1753360"
  },
  {
    "text": "5,000 or something like there's also a lot of crud on GitHub um a lot of crud",
    "start": "1753360",
    "end": "1758640"
  },
  {
    "text": "that we consume um you know like everybody's CS 101 homework is on there uh and that's just a lot of data that",
    "start": "1758640",
    "end": "1764840"
  },
  {
    "text": "ends up coming in data we weren't capturing inside of Star Search But ultimately I think yeah there's a",
    "start": "1764840",
    "end": "1770480"
  },
  {
    "text": "there's a balance in there in hindsight and you know with everything hindsight is 2020 um again I'm very very excited",
    "start": "1770480",
    "end": "1776120"
  },
  {
    "text": "about uh Dr which is this way for uh pods and applications on your cluster to",
    "start": "1776120",
    "end": "1782600"
  },
  {
    "text": "uh request and allocate those Hardware resources like gpus and stuff it's in Alpha currently but um yeah exciting",
    "start": "1782600",
    "end": "1789440"
  },
  {
    "text": "stuff to come um and that's it the show notes for this are ub.com",
    "start": "1789440",
    "end": "1794880"
  },
  {
    "text": "jpmcb cucon N24 I'd love to take any questions that you",
    "start": "1794880",
    "end": "1800190"
  },
  {
    "text": "[Applause]",
    "start": "1800190",
    "end": "1804238"
  },
  {
    "text": "have well I'll hang around if anybody is around oh we got one yeah go for it how",
    "start": "1813880",
    "end": "1820600"
  },
  {
    "text": "did you handin",
    "start": "1820600",
    "end": "1826080"
  },
  {
    "text": "yeah did you happen or was there anything you could do with that yeah uh",
    "start": "1826080",
    "end": "1831240"
  },
  {
    "text": "so the question for those who couldn't hear it was how did we handle hallucinations you know through this whole chain um it it was it was",
    "start": "1831240",
    "end": "1839159"
  },
  {
    "text": "challenging um I think you know we did evaluate some products that would allow us to kind of do you know some crazy",
    "start": "1839159",
    "end": "1846399"
  },
  {
    "text": "like statistical analysis around like oh you tweaked your your system prompt here we're going to throw like a thousand",
    "start": "1846399",
    "end": "1853000"
  },
  {
    "text": "prompts at this new system message and this new model that you maybe put into the whole Pipeline and see like how you",
    "start": "1853000",
    "end": "1859440"
  },
  {
    "text": "know how it fits with the whole product offering I guess uh we never we didn't get very far with actually doing that",
    "start": "1859440",
    "end": "1865960"
  },
  {
    "text": "again startup life so we just kind of prayed for the best um and I think",
    "start": "1865960",
    "end": "1871240"
  },
  {
    "text": "because we you know the summarization part of that was huge we we had such a",
    "start": "1871240",
    "end": "1876360"
  },
  {
    "text": "problem before that excuse me uh yeah it's kind of an open question excuse me",
    "start": "1876360",
    "end": "1886320"
  },
  {
    "text": "yeah um as a part two or phase two are",
    "start": "1888519",
    "end": "1893760"
  },
  {
    "text": "you planning to move away from device plugin and embrace the GPU operator and other stuff",
    "start": "1893760",
    "end": "1900600"
  },
  {
    "text": "or are you planning to as phase two to embrace GPU operator so that you don't",
    "start": "1900600",
    "end": "1906159"
  },
  {
    "text": "have to do the device plug-in and all those things as phase two good question oh I'm losing my voice",
    "start": "1906159",
    "end": "1913279"
  },
  {
    "text": "now uh yes we we would want to build out the",
    "start": "1913279",
    "end": "1919120"
  },
  {
    "text": "platform so that it was less excuse me me hand rolling you know",
    "start": "1919120",
    "end": "1925159"
  },
  {
    "text": "a bunch of the note selectors and the GPU pools and all that we would want we would want to eventually be able to you",
    "start": "1925159",
    "end": "1931840"
  },
  {
    "text": "know use excuse me oh my gosh uh we'd want to be able to use you know what we",
    "start": "1931840",
    "end": "1937519"
  },
  {
    "text": "could from the community spot gpus scale up scale down with the need because like in the middle of the night nobody's",
    "start": "1937519",
    "end": "1942679"
  },
  {
    "text": "using this at all so any way we could like utilize an operate that would scale",
    "start": "1942679",
    "end": "1948559"
  },
  {
    "text": "that down we evaluated Carpenter at some point um but yeah um the the big lesson",
    "start": "1948559",
    "end": "1954679"
  },
  {
    "text": "that I think everybody should take away from this is that like it's possible with like a small small company you know at reduced cost to build this kind of",
    "start": "1954679",
    "end": "1961760"
  },
  {
    "text": "platform um be it as cruy and startup be as it was um but I think there's so many",
    "start": "1961760",
    "end": "1966799"
  },
  {
    "text": "better Solutions uh today versus yeah versus like what we built this time last year so thank you yeah yeah go ahead",
    "start": "1966799",
    "end": "1974960"
  },
  {
    "text": "kind of in a similar vein you mentioned that you chose VM because at the time it",
    "start": "1974960",
    "end": "1980480"
  },
  {
    "text": "had it was like the only thing that had that feature that you wanted um would you still do so",
    "start": "1980480",
    "end": "1987919"
  },
  {
    "text": "today that's a that's a good question um probably um I I've really liked VM um",
    "start": "1988120",
    "end": "1997039"
  },
  {
    "text": "I haven't been as involved in like evaluating the community recently um I really like oama as well you know mostly",
    "start": "1997039",
    "end": "2003000"
  },
  {
    "text": "because I could run it on my Mac VM doesn't work very well on on you know MacBook stuff but",
    "start": "2003000",
    "end": "2008320"
  },
  {
    "text": "um yeah I would definitely want to evaluate like performance features um VM",
    "start": "2008320",
    "end": "2013559"
  },
  {
    "text": "also supported like ad hoc quantization really well uh which I don't know if you",
    "start": "2013559",
    "end": "2018600"
  },
  {
    "text": "noticed but there was that whole like argument I could just pass to pass it the the D type which was like just cut",
    "start": "2018600",
    "end": "2023960"
  },
  {
    "text": "the uh quantisation in half which is very cool so um yeah I think if I had",
    "start": "2023960",
    "end": "2030360"
  },
  {
    "text": "more resources if I had more time I would have evaluated more today um you know this is the trade-off tradeoff we",
    "start": "2030360",
    "end": "2037600"
  },
  {
    "text": "had to make at the time and just kind of executed but yeah good question I'd love if you have any insights on like better",
    "start": "2037600",
    "end": "2043440"
  },
  {
    "text": "serving inferences these days I'd love that yeah yeah go ahead uh on your challenges slide you've described that",
    "start": "2043440",
    "end": "2050480"
  },
  {
    "text": "um it's hard to get gpus for you is was that a matter can you describe that a bit more is it a matter of scal scaling",
    "start": "2050480",
    "end": "2056200"
  },
  {
    "text": "out or time of the day I don't know price yeah that's a good question the",
    "start": "2056200",
    "end": "2061440"
  },
  {
    "text": "first sort of implementation of that I was like I was like oh spot gpus are so cheap I'm just going to use spot gpus I",
    "start": "2061440",
    "end": "2067480"
  },
  {
    "text": "don't I don't need to like allocate them like at myself and that did not work that was bad uh we maybe got a few hours",
    "start": "2067480",
    "end": "2074839"
  },
  {
    "text": "a day of spot gpus which you know that's the whole that's the whole point I guess",
    "start": "2074839",
    "end": "2080240"
  },
  {
    "text": "um so we do still have some capabilities to like scale up spot GP gpus I would have to go into the plumey um node pool",
    "start": "2080240",
    "end": "2086679"
  },
  {
    "text": "to be like GPU you know spot pool six or something um but it was surprising to me",
    "start": "2086679",
    "end": "2093599"
  },
  {
    "text": "that there was still even a shortage of like t4s at one point AER was like oh experiencing like Mass shortages please",
    "start": "2093599",
    "end": "2099960"
  },
  {
    "text": "you know like deallocate if if possible um I don't think I did cuz we had like 10 which couldn't be any more than like",
    "start": "2099960",
    "end": "2107280"
  },
  {
    "text": "you know Microsoft had themselves right so yeah it's uh it's it's very wild to",
    "start": "2107280",
    "end": "2113079"
  },
  {
    "text": "me that there's still shortage so but you would have been looking for like a kubernetes cluster with nodes with gpus",
    "start": "2113079",
    "end": "2119359"
  },
  {
    "text": "that would have been enough for what do you mean I mean would that need to be on",
    "start": "2119359",
    "end": "2124720"
  },
  {
    "text": "the public cloud or could that be somewhere else you wouldn't care as long as it's a Kate cluster is it like that I",
    "start": "2124720",
    "end": "2130119"
  },
  {
    "text": "see yeah we we had also thought about trying to find a partnership with like a company like Lambda or cor weave or",
    "start": "2130119",
    "end": "2135560"
  },
  {
    "text": "something who has Hardware like in a Data Center and that's their like Core Business um and maybe get you know",
    "start": "2135560",
    "end": "2140839"
  },
  {
    "text": "credits and all this stuff candidly we used aure because they gave us a ton of credits um to partner with them and then",
    "start": "2140839",
    "end": "2147760"
  },
  {
    "text": "you know that was even more cost reduction efforts so like it was hard for us to stomach like going multicloud",
    "start": "2147760",
    "end": "2153200"
  },
  {
    "text": "or like going in any direction you know again startup life um regardless of",
    "start": "2153200",
    "end": "2158560"
  },
  {
    "text": "potential partnership and stuff so um I think I got to finish it up there U but thank you everybody for coming here I'll",
    "start": "2158560",
    "end": "2164760"
  },
  {
    "text": "definitely hang around and yeah would love to chat thank you everybody",
    "start": "2164760",
    "end": "2171078"
  }
]