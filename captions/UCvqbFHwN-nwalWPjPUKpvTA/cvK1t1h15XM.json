[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "hello I think we can start thank you for",
    "start": "30",
    "end": "6480"
  },
  {
    "text": "attending this talk my name is sting I'm Google engineer working in community is storage",
    "start": "6480",
    "end": "12030"
  },
  {
    "text": "lifecycle team today I'll talk about resource management in kubernetes focusing on local ephemeral storage",
    "start": "12030",
    "end": "21199"
  },
  {
    "text": "strange thing is the the words that the bush ad hope you don't bother you as",
    "start": "21529",
    "end": "27330"
  },
  {
    "start": "25000",
    "end": "25000"
  },
  {
    "text": "much so this is the agenda so first we want to make it clear right why it's",
    "start": "27330",
    "end": "33210"
  },
  {
    "text": "important for locally former storage resource management what problems we are",
    "start": "33210",
    "end": "38700"
  },
  {
    "text": "trying to solve and I will give quick overview of resource management in",
    "start": "38700",
    "end": "44460"
  },
  {
    "text": "kubernetes and the general resource model supported in kinase and then focused on the storage and how we manage",
    "start": "44460",
    "end": "52980"
  },
  {
    "text": "the locally for storage at different layers and how we solve the different problems at last give a few directions",
    "start": "52980",
    "end": "61590"
  },
  {
    "text": "of our future work I know today's the last day and almost lunchtime",
    "start": "61590",
    "end": "67170"
  },
  {
    "text": "I'll try to make it on time so in case you have more questions and want further",
    "start": "67170",
    "end": "72869"
  },
  {
    "text": "discussions these are my contacts so my kid hub idea is a little bit different",
    "start": "72869",
    "end": "79939"
  },
  {
    "start": "79000",
    "end": "79000"
  },
  {
    "text": "so first as we know no communities is a solution or ox trading containerized so",
    "start": "79939",
    "end": "86700"
  },
  {
    "text": "were closed so when you have application you're closed a scheduler will pick a node to run the containers for",
    "start": "86700",
    "end": "93689"
  },
  {
    "text": "applications over time right as a user you might experience something not very",
    "start": "93689",
    "end": "99450"
  },
  {
    "text": "present like suddenly your container just are killed terminated or you notice",
    "start": "99450",
    "end": "106649"
  },
  {
    "text": "your service running in a container just getting slower and slower or even worse",
    "start": "106649",
    "end": "111750"
  },
  {
    "text": "the whole machine just keep rushing and all your workloads running they're just in trouble",
    "start": "111750",
    "end": "117350"
  },
  {
    "text": "so why what's going on keep in mind that resources in cloud",
    "start": "117350",
    "end": "123270"
  },
  {
    "text": "environment are shared containers are running together side by side in the",
    "start": "123270",
    "end": "129420"
  },
  {
    "text": "same house and they compete for the resources what if our container had suddenly use a",
    "start": "129420",
    "end": "136650"
  },
  {
    "text": "lot of CPU and memory and cause other containers that run very slow and also",
    "start": "136650",
    "end": "142140"
  },
  {
    "text": "even get killed because out of memory or a container just produce a lots of data",
    "start": "142140",
    "end": "147840"
  },
  {
    "text": "in general lots of files and cause out of disk error so we need to solve these",
    "start": "147840",
    "end": "154740"
  },
  {
    "text": "problems with proper resource management so typically we have different resource",
    "start": "154740",
    "end": "160709"
  },
  {
    "text": "manage goes to address different aspects first the efficient allocation of",
    "start": "160709",
    "end": "167010"
  },
  {
    "start": "161000",
    "end": "161000"
  },
  {
    "text": "resource so we want to allocate just enough resources for applications and so",
    "start": "167010",
    "end": "173730"
  },
  {
    "text": "that you can have some performance guaranteed and also we don't want to",
    "start": "173730",
    "end": "178950"
  },
  {
    "text": "like I don't care too much - with our money and same time we want to avoid",
    "start": "178950",
    "end": "187110"
  },
  {
    "text": "like over commit resources because it will cost failures downtime and hurt your performance the",
    "start": "187110",
    "end": "194940"
  },
  {
    "text": "second aspect the second goal is since containers are competing for resources and we want to have some isolation among",
    "start": "194940",
    "end": "202470"
  },
  {
    "text": "them so that we don't want to have some workloads use up all the resources and",
    "start": "202470",
    "end": "209600"
  },
  {
    "text": "the third aspect is for kubernetes right we have some critical system processes",
    "start": "209600",
    "end": "216720"
  },
  {
    "text": "that cool it is running as a daemon on each node and it managed or were closed",
    "start": "216720",
    "end": "222600"
  },
  {
    "text": "and those critical system processes also need resources and to make sure systems",
    "start": "222600",
    "end": "229500"
  },
  {
    "text": "the parity we won't have those critical processes also have enough resources",
    "start": "229500",
    "end": "236600"
  },
  {
    "start": "237000",
    "end": "237000"
  },
  {
    "text": "so in his resource that definition is something that can be requested can be",
    "start": "238920",
    "end": "245910"
  },
  {
    "text": "allocated and can be consumed and typical example of such resources CPU",
    "start": "245910",
    "end": "252569"
  },
  {
    "text": "memory we call CPU compressible because a CPU can be started as needed but",
    "start": "252569",
    "end": "259769"
  },
  {
    "text": "memory cannot and in plenteous we support a simple resource model so allow",
    "start": "259769",
    "end": "267330"
  },
  {
    "text": "user to spice by resource request and limit so request speedily the amount of",
    "start": "267330",
    "end": "274680"
  },
  {
    "text": "resources you want to allocate for your application file container and how much",
    "start": "274680",
    "end": "279840"
  },
  {
    "text": "you need and the limit set upper bound that how much resources you want you can",
    "start": "279840",
    "end": "285630"
  },
  {
    "text": "be consumed by application and we call those specifications as they they are",
    "start": "285630",
    "end": "291300"
  },
  {
    "text": "state because it's described how system should behave right how much resources",
    "start": "291300",
    "end": "296330"
  },
  {
    "text": "you want to allocate and consumed and the actual resource usage we called",
    "start": "296330",
    "end": "301950"
  },
  {
    "text": "extra state and in Cabrini's we have control up to drive the actual state",
    "start": "301950",
    "end": "307650"
  },
  {
    "text": "always towards to the desire states as close as possible and that figure shows",
    "start": "307650",
    "end": "314970"
  },
  {
    "text": "for second request an image we allow different values the reason for that is",
    "start": "314970",
    "end": "321150"
  },
  {
    "text": "because in many cases some workloads are first of all and so most of time they",
    "start": "321150",
    "end": "326729"
  },
  {
    "text": "only consume certain amount of resources but a kidney they might have big spike",
    "start": "326729",
    "end": "332090"
  },
  {
    "text": "so we want to allocate the resources that these application take reuse so to",
    "start": "332090",
    "end": "339720"
  },
  {
    "text": "have better utilization we are resource but we also want allow it to like",
    "start": "339720",
    "end": "345979"
  },
  {
    "text": "conversed occasionally and the actual usage might be different right it can be",
    "start": "345979",
    "end": "354110"
  },
  {
    "text": "below the request or can be more than request but it should be below the limit",
    "start": "354110",
    "end": "360810"
  },
  {
    "text": "always with this simple resource model",
    "start": "360810",
    "end": "366090"
  },
  {
    "start": "364000",
    "end": "364000"
  },
  {
    "text": "like let's see how we allocate the resource in a solid resource so first each part will",
    "start": "366090",
    "end": "373120"
  },
  {
    "text": "said request and the scheduler will check all the knows not the available",
    "start": "373120",
    "end": "378850"
  },
  {
    "text": "capacity whether it can satisfy the request and if there are multiple candidate notes can all satisfy the",
    "start": "378850",
    "end": "385600"
  },
  {
    "text": "request it will rank or the nose based on the scheduler policy and pick the",
    "start": "385600",
    "end": "391360"
  },
  {
    "text": "best one so we support it for in scheduler policy some like more to us - even in",
    "start": "391360",
    "end": "398380"
  },
  {
    "text": "distribute or close some more like what was healthy pack they were closed and by",
    "start": "398380",
    "end": "404590"
  },
  {
    "text": "allocating enough resources as you request we kind of reduce the chance of",
    "start": "404590",
    "end": "409720"
  },
  {
    "text": "our committee resources but it might still happen we'll see a little in later slides and",
    "start": "409720",
    "end": "417060"
  },
  {
    "text": "also you can set resource limit to make sure the actual usage under limit so the",
    "start": "417060",
    "end": "423490"
  },
  {
    "text": "system will keep monitoring the actual usage and take actions if it is see any",
    "start": "423490",
    "end": "428590"
  },
  {
    "text": "violations so for CPU it will start all the CPU so the CPU users should never go",
    "start": "428590",
    "end": "435670"
  },
  {
    "text": "above the limit and for memory it will kill the container so the memory will be",
    "start": "435670",
    "end": "441520"
  },
  {
    "text": "freed up and in a pot as we know when container is cured the power will",
    "start": "441520",
    "end": "446950"
  },
  {
    "text": "automatically start a new container so continue the work so so far we talked",
    "start": "446950",
    "end": "456700"
  },
  {
    "text": "about the resource management for CPU memory how about storage and in the following",
    "start": "456700",
    "end": "462790"
  },
  {
    "text": "slides we'll talk in more detail about the storage management so for storage",
    "start": "462790",
    "end": "472240"
  },
  {
    "start": "468000",
    "end": "468000"
  },
  {
    "text": "company support many different type of storage in container that you can write",
    "start": "472240",
    "end": "479080"
  },
  {
    "text": "read data and those data are are temporary so when the King dies finish",
    "start": "479080",
    "end": "485530"
  },
  {
    "text": "so the data will be gone and those data are typically stored in the local disk",
    "start": "485530",
    "end": "492100"
  },
  {
    "text": "and no problem is the data in one container cannot be shared to other",
    "start": "492100",
    "end": "497950"
  },
  {
    "text": "containers since part consists of number of containers working together we need a way to share",
    "start": "497950",
    "end": "504009"
  },
  {
    "text": "those Dana so we have a volume concept",
    "start": "504009",
    "end": "509439"
  },
  {
    "text": "in communities so a part can have different volumes and each volume mounted to different containers and",
    "start": "509439",
    "end": "517599"
  },
  {
    "text": "continuer can read right to the volume and share data so one type of volume we",
    "start": "517599",
    "end": "524980"
  },
  {
    "text": "called am reader so this tab volume is also considered as ephemeral",
    "start": "524980",
    "end": "530980"
  },
  {
    "text": "because when the pod finish that terminated the data in that empty row",
    "start": "530980",
    "end": "537670"
  },
  {
    "text": "volume will also be clean up so it has the same life cycle as their pod",
    "start": "537670",
    "end": "543750"
  },
  {
    "text": "typically empty volume are used for store some temporary data for caching and Postgres page or you want to check",
    "start": "543750",
    "end": "551709"
  },
  {
    "text": "pointing or long like running computation and this data when Polly",
    "start": "551709",
    "end": "559240"
  },
  {
    "text": "finished that is okay to clean them up there are a few other types of volumes like secrets configure map",
    "start": "559240",
    "end": "565899"
  },
  {
    "text": "they are basically wrapped around to empty their volume so they have they are opening to the ephemeral volume but if",
    "start": "565899",
    "end": "573880"
  },
  {
    "text": "are they will be back up to my local storage too and we also a support",
    "start": "573880",
    "end": "579940"
  },
  {
    "text": "persist ammonium because people need to persist their data so for percent volume",
    "start": "579940",
    "end": "585399"
  },
  {
    "text": "typically they are backed by dedicated disks and it could either be remote like",
    "start": "585399",
    "end": "592829"
  },
  {
    "text": "gcpd either live it address EPS or recently we also support dedicated local",
    "start": "592829",
    "end": "599529"
  },
  {
    "text": "disk and there will be a talk in afternoon Jericho by Michael dicker and",
    "start": "599529",
    "end": "605370"
  },
  {
    "text": "Michelle talking about support a total precision disk and for such persistent",
    "start": "605370",
    "end": "613029"
  },
  {
    "text": "volume it will have a life cycle totally different from their pause when",
    "start": "613029",
    "end": "618399"
  },
  {
    "text": "the pod finish that the volume is still there that they have will be persisted and normally we use called persistent",
    "start": "618399",
    "end": "626829"
  },
  {
    "text": "modem API object to represent it if a PHA need to use that persist data it need to",
    "start": "626829",
    "end": "632890"
  },
  {
    "text": "refer claimed recalled PVC and PVC Pines to",
    "start": "632890",
    "end": "637930"
  },
  {
    "text": "one of the PV and the part can use that precision model service rating we can",
    "start": "637930",
    "end": "645370"
  },
  {
    "text": "see a former storage red are shared between containers among containers and",
    "start": "645370",
    "end": "651880"
  },
  {
    "start": "647000",
    "end": "647000"
  },
  {
    "text": "empty the volumes so these results are shared we also need some management for",
    "start": "651880",
    "end": "657070"
  },
  {
    "text": "their shared resource and for persistent imodium typically they are dedicated discs so we don't need to worry about",
    "start": "657070",
    "end": "663040"
  },
  {
    "text": "too much so for locally for more storage measurements we also want to achieve",
    "start": "663040",
    "end": "668650"
  },
  {
    "text": "these different management goals and I will talk about in more detail how we",
    "start": "668650",
    "end": "675160"
  },
  {
    "text": "achieve it at different layers so just",
    "start": "675160",
    "end": "682060"
  },
  {
    "start": "680000",
    "end": "680000"
  },
  {
    "text": "like CPU and the memory first we want to support a former storage also as",
    "start": "682060",
    "end": "687640"
  },
  {
    "text": "first-class resource so that user can specify in the container specification",
    "start": "687640",
    "end": "695010"
  },
  {
    "text": "how much you want to request how much you want to limit for a former storage usage and here we have just as example",
    "start": "695010",
    "end": "703690"
  },
  {
    "text": "this part has two containers each one requests different amount of resource and different limits and for request the",
    "start": "703690",
    "end": "713290"
  },
  {
    "text": "scheduler will check whether the node has enough capacity to schedule the the container in the pot and for limit its",
    "start": "713290",
    "end": "721240"
  },
  {
    "text": "monitoring the actual usage of the container and if they see it exceed the",
    "start": "721240",
    "end": "728050"
  },
  {
    "text": "limit it will evict part by eviction we mean your terminus departs gracefully so",
    "start": "728050",
    "end": "735130"
  },
  {
    "text": "that the emitter bottom like data or the continual data will be opening up and",
    "start": "735130",
    "end": "740860"
  },
  {
    "text": "the disk storage will be freed",
    "start": "740860",
    "end": "745290"
  },
  {
    "text": "now considering as a part right we just mentioned the part can have like volumes",
    "start": "748760",
    "end": "753930"
  },
  {
    "text": "and besides we want to constrain how much resources used by container we also",
    "start": "753930",
    "end": "759480"
  },
  {
    "text": "want to constrain how much resources should be used by the volumes so we add",
    "start": "759480",
    "end": "764850"
  },
  {
    "text": "a field the size limit to visit a gave upper bound like how much resources can",
    "start": "764850",
    "end": "771960"
  },
  {
    "text": "be consumed that is volume and similarly if the monitoring like detect the bottom",
    "start": "771960",
    "end": "780360"
  },
  {
    "text": "usage exceed so it will be picked apart and by setting the limits for container",
    "start": "780360",
    "end": "786060"
  },
  {
    "text": "and also volume read we have good isolation among different containers and also the past so one thing is right now",
    "start": "786060",
    "end": "798300"
  },
  {
    "text": "we don't support explicitly pod level like resource specification resource",
    "start": "798300",
    "end": "805680"
  },
  {
    "start": "799000",
    "end": "799000"
  },
  {
    "text": "requirements but internally like in system actually calculate the power",
    "start": "805680",
    "end": "812730"
  },
  {
    "text": "level resource it is basically the sum of all other containers so here in this",
    "start": "812730",
    "end": "821160"
  },
  {
    "text": "example we have two containers so at the pub level the request would be the total",
    "start": "821160",
    "end": "826200"
  },
  {
    "text": "like twelve gigabytes and the limit is 14 gigabytes and the usage for this part",
    "start": "826200",
    "end": "832710"
  },
  {
    "text": "will be the total usage for come from auto containers and also volumes and",
    "start": "832710",
    "end": "838260"
  },
  {
    "text": "meter volumes and we want to make sure at pod level the limit should be",
    "start": "838260",
    "end": "845960"
  },
  {
    "text": "enforced so the reason we also care about power level resources pad level",
    "start": "845960",
    "end": "854160"
  },
  {
    "text": "resource consumption is not simple ages aggregation container strength it also has some pod level overhead in the",
    "start": "854160",
    "end": "863580"
  },
  {
    "text": "future we might want to support like is pasilla pod level specification resource",
    "start": "863580",
    "end": "868950"
  },
  {
    "text": "specification and by citing request limit values we",
    "start": "868950",
    "end": "876990"
  },
  {
    "start": "871000",
    "end": "871000"
  },
  {
    "text": "can also cast by the path to different cue as classes and the first one if you",
    "start": "876990",
    "end": "886950"
  },
  {
    "text": "set your request as the same as your limit we call them guaranteed pass so",
    "start": "886950",
    "end": "893190"
  },
  {
    "text": "those paths right guarantee have this much resource and also they should not use more than they request so those part",
    "start": "893190",
    "end": "901170"
  },
  {
    "text": "will not be cute in case of like resource contention only if the usage",
    "start": "901170",
    "end": "906450"
  },
  {
    "text": "exceed their limit and also as we mentioned we want to support like a",
    "start": "906450",
    "end": "911880"
  },
  {
    "text": "portable workload so when you request is smaller than the limit or at limit is",
    "start": "911880",
    "end": "917580"
  },
  {
    "text": "higher so we call them first of all workload parts they can use more",
    "start": "917580",
    "end": "924030"
  },
  {
    "text": "resources then they request but they are more likely to be cute compared to guaranteed and the last one",
    "start": "924030",
    "end": "932310"
  },
  {
    "text": "if you don't specify anything for your part we call best effort so those part",
    "start": "932310",
    "end": "937650"
  },
  {
    "text": "can fit anywhere so easily they can use any available resources but if you need",
    "start": "937650",
    "end": "943560"
  },
  {
    "text": "to even pass to reduce the contention then the best effort part will be the",
    "start": "943560",
    "end": "949290"
  },
  {
    "text": "first target",
    "start": "949290",
    "end": "951889"
  },
  {
    "text": "so now we covered a container level pod level resource management and are",
    "start": "954850",
    "end": "961340"
  },
  {
    "text": "considering know the level rent although we have like limit for container and past but we might still",
    "start": "961340",
    "end": "968090"
  },
  {
    "text": "have issue for know the level resource management so here use that as example",
    "start": "968090",
    "end": "974570"
  },
  {
    "text": "first based on a capacity we schedule Sri pause and one is guaranteed ones",
    "start": "974570",
    "end": "981710"
  },
  {
    "text": "worst of all ones best effort so based on the requests that they can perfectly fit into that node however and worst of",
    "start": "981710",
    "end": "992630"
  },
  {
    "text": "all and best effort part can use more resources than then they request when",
    "start": "992630",
    "end": "998870"
  },
  {
    "text": "that happened we can see definite the capacity is not sufficient anymore and",
    "start": "998870",
    "end": "1004300"
  },
  {
    "text": "we he'll have like out of disk problem to solve this we have an eviction",
    "start": "1004300",
    "end": "1011080"
  },
  {
    "text": "manager to detect whether there is this pressure and take actions and also it",
    "start": "1011080",
    "end": "1019150"
  },
  {
    "text": "allows you to sell some like eviction threshold so it monitors okay at node if",
    "start": "1019150",
    "end": "1025150"
  },
  {
    "text": "the available resource available disk space is smaller than some amount like",
    "start": "1025150",
    "end": "1030640"
  },
  {
    "text": "one gigabyte it will start eviction action before in bigger pot it will try",
    "start": "1030640",
    "end": "1037480"
  },
  {
    "text": "to reclaim disk resource by TB kingdom and use images or delete dead parts if",
    "start": "1037480",
    "end": "1043770"
  },
  {
    "text": "it's still not enough then we will try to choose some part to evict the order",
    "start": "1043770",
    "end": "1051730"
  },
  {
    "text": "of choosing will be in the order of the queue as we just mentioned and here for",
    "start": "1051730",
    "end": "1058330"
  },
  {
    "text": "eviction start hold it can be solved or hard-hard means you take eviction action immediately when there's a violation",
    "start": "1058330",
    "end": "1065820"
  },
  {
    "text": "soft means you allow like a certain pure time for Chris pyridine",
    "start": "1065820",
    "end": "1072419"
  },
  {
    "text": "so with the immigrants our code which seems to like have some protection for",
    "start": "1076350",
    "end": "1083340"
  },
  {
    "text": "node level resource however we still have issues so can steering here example",
    "start": "1083340",
    "end": "1089880"
  },
  {
    "text": "and we have this capacity and we allocate we scheduled three paths all of",
    "start": "1089880",
    "end": "1095909"
  },
  {
    "text": "them are guaranteed paths right so we those paths should not use more than",
    "start": "1095909",
    "end": "1102149"
  },
  {
    "text": "they request so right now it's perfect fit but remember we mentioned there are",
    "start": "1102149",
    "end": "1107940"
  },
  {
    "text": "critical system processes running to and they also need resources so they will",
    "start": "1107940",
    "end": "1114149"
  },
  {
    "text": "like compete the same resources like disk space with all the other parts and",
    "start": "1114149",
    "end": "1121409"
  },
  {
    "text": "when this happened right we also have this resource contention issue and to",
    "start": "1121409",
    "end": "1127860"
  },
  {
    "text": "solve this we have a concept of allocatable and so basically it allows system Amin",
    "start": "1127860",
    "end": "1136710"
  },
  {
    "text": "to reserve certain amount of resources for these critical system processes and",
    "start": "1136710",
    "end": "1145039"
  },
  {
    "text": "after you reserved that much so we have a local resources speak today the",
    "start": "1145039",
    "end": "1151320"
  },
  {
    "text": "capacity - those reserved and for users part they can only use the allocatable",
    "start": "1151320",
    "end": "1159269"
  },
  {
    "text": "part and you may wonder like how much I should reserve for my part and we need",
    "start": "1159269",
    "end": "1167129"
  },
  {
    "text": "to make monetary in the note and checked the total usage and note level and",
    "start": "1167129",
    "end": "1175309"
  },
  {
    "text": "managed the total usage of users part then this would be the system overhead",
    "start": "1175309",
    "end": "1180960"
  },
  {
    "text": "and we can roughly estimate that over",
    "start": "1180960",
    "end": "1186059"
  },
  {
    "text": "here should be proportional to the capacity because the bigger capacity you not have you expect have more closed and",
    "start": "1186059",
    "end": "1193379"
  },
  {
    "text": "the system overhead is roughly like proportional to the were close let's say",
    "start": "1193379",
    "end": "1200340"
  },
  {
    "text": "cubed for example kübra usage will be roughly proportional to the number of",
    "start": "1200340",
    "end": "1206190"
  },
  {
    "text": "parts you're scheduled to that node so after that we have this allocatable",
    "start": "1206190",
    "end": "1214490"
  },
  {
    "text": "results you can see the Sur part p3 no longer fit and we can only schedule 2",
    "start": "1214490",
    "end": "1220790"
  },
  {
    "text": "parts based on the allocatable resource",
    "start": "1220790",
    "end": "1225160"
  },
  {
    "start": "1225000",
    "end": "1225000"
  },
  {
    "text": "and we see right for scheduling part we make sure we have a schedule pass based",
    "start": "1227770",
    "end": "1234410"
  },
  {
    "text": "on the eligible and we make sure the system demons will have enough reserve",
    "start": "1234410",
    "end": "1239450"
  },
  {
    "text": "resources however again that scheduler part is based on the request and part",
    "start": "1239450",
    "end": "1246830"
  },
  {
    "text": "can always use more than they request when that happened right we don't have",
    "start": "1246830",
    "end": "1252740"
  },
  {
    "text": "enough like we cannot guarantee the system a process have enough resources",
    "start": "1252740",
    "end": "1257890"
  },
  {
    "text": "so after para scheduled running we also keep monitoring how much each users part",
    "start": "1257890",
    "end": "1266120"
  },
  {
    "text": "are consumed if the total usage will exceed there at a capable part the",
    "start": "1266120",
    "end": "1271790"
  },
  {
    "text": "eviction manager will also take actions to evict parts",
    "start": "1271790",
    "end": "1276820"
  },
  {
    "start": "1278000",
    "end": "1278000"
  },
  {
    "text": "and we mention the rats the eviction pics ranked their part based on the Q s",
    "start": "1280100",
    "end": "1285649"
  },
  {
    "text": "but Q s catification is only based on the setting of request and limit and",
    "start": "1285649",
    "end": "1292340"
  },
  {
    "text": "whether the request is bigger than limit or not it's not very flexible how about",
    "start": "1292340",
    "end": "1297649"
  },
  {
    "text": "you have some first of all were close that very important you don't want to be victim first so since released white",
    "start": "1297649",
    "end": "1306340"
  },
  {
    "text": "community is support alpha feature or parts pure ret so the priority will",
    "start": "1306340",
    "end": "1313220"
  },
  {
    "text": "indicate how important of your path compared to others and the eviction",
    "start": "1313220",
    "end": "1320389"
  },
  {
    "text": "policy right now will incorporate the part priority - so first it will target",
    "start": "1320389",
    "end": "1326269"
  },
  {
    "text": "on a part that the usage is more than they request and they will rank their",
    "start": "1326269",
    "end": "1332599"
  },
  {
    "text": "paws based on the priority if there is there is a time then it further will run",
    "start": "1332599",
    "end": "1338419"
  },
  {
    "text": "cause based on the difference between their usage and the request and here we",
    "start": "1338419",
    "end": "1344690"
  },
  {
    "text": "show example how you specify the priority for pot so you have a priority",
    "start": "1344690",
    "end": "1350749"
  },
  {
    "text": "class if you object and you can have different priority here is in this",
    "start": "1350749",
    "end": "1357080"
  },
  {
    "text": "example we have high priority and in the pots back you need to give what is your",
    "start": "1357080",
    "end": "1362570"
  },
  {
    "text": "priority class name to indicate how important your pot is yes",
    "start": "1362570",
    "end": "1371679"
  },
  {
    "text": "what happened to the containers into the pot so the Kenya we are all so cute but",
    "start": "1377120",
    "end": "1383840"
  },
  {
    "text": "if your part is managed by like some other controller like suitable size or deployment replica set and those",
    "start": "1383840",
    "end": "1391040"
  },
  {
    "text": "controller will start a new part at a different place to make sure you have",
    "start": "1391040",
    "end": "1397120"
  },
  {
    "text": "passed always running but if your part is not managed by any controller that",
    "start": "1397120",
    "end": "1403160"
  },
  {
    "text": "and after eviction your part base today will be terminated and there's no work yeah again the data so if you use",
    "start": "1403160",
    "end": "1414140"
  },
  {
    "text": "persistent volume like I said so even part is terminated the data will be still in the volume and theta will not",
    "start": "1414140",
    "end": "1422450"
  },
  {
    "text": "be clean up but only the cathedral volume that is recalled a former or like",
    "start": "1422450",
    "end": "1427730"
  },
  {
    "text": "modem and the data will be coming up and if you have other parts it will it will",
    "start": "1427730",
    "end": "1434510"
  },
  {
    "text": "can continue to use the new part when they started it can continue use those",
    "start": "1434510",
    "end": "1439970"
  },
  {
    "text": "per system all you so the data will be still there you can still access those data",
    "start": "1439970",
    "end": "1446200"
  },
  {
    "text": "right so yeah for persistence basically we are",
    "start": "1459260",
    "end": "1466320"
  },
  {
    "text": "saying you use some dedicated disk right it's not shared so basically you can",
    "start": "1466320",
    "end": "1472890"
  },
  {
    "text": "kind of make sure your testicle size is enough for and also we plan to have we",
    "start": "1472890",
    "end": "1478740"
  },
  {
    "text": "are currently working on some feature like a resize possess the volume so it can be done like resized this volume on",
    "start": "1478740",
    "end": "1485220"
  },
  {
    "text": "time then dynamically yes there's a new feature that we plan to support yes",
    "start": "1485220",
    "end": "1495919"
  },
  {
    "text": "[Music]",
    "start": "1496480",
    "end": "1499630"
  },
  {
    "text": "it's kind of nuts really it's kind of mixed first it will check whether the",
    "start": "1501820",
    "end": "1507450"
  },
  {
    "text": "usage above request right so in other case we are more targeting on portable",
    "start": "1507450",
    "end": "1513670"
  },
  {
    "text": "and best-effort were closed for guaranteed right issue not the actualization not above the usage",
    "start": "1513670",
    "end": "1522450"
  },
  {
    "text": "yes yes current policy is yes it's kind of debatable yes but the behavior is",
    "start": "1532500",
    "end": "1538980"
  },
  {
    "text": "like that yeah okay we covered like",
    "start": "1538980",
    "end": "1547679"
  },
  {
    "text": "resource management at continued level Kitchener pod level node level and we",
    "start": "1547679",
    "end": "1554100"
  },
  {
    "text": "seem he'll provide some allocation and isolation bar resources but for like in",
    "start": "1554100",
    "end": "1559830"
  },
  {
    "text": "cloud and often we have different group of people different teams share resources and how we can make petitioner",
    "start": "1559830",
    "end": "1567870"
  },
  {
    "text": "allocate resources among different teams and in CUDA is we have namespace and",
    "start": "1567870",
    "end": "1574580"
  },
  {
    "text": "namespace pezzi they allow you to partition resources among different group of people and so for namespace it",
    "start": "1574580",
    "end": "1585179"
  },
  {
    "text": "allowed to specify resource requirements for each group there is a just example",
    "start": "1585179",
    "end": "1591210"
  },
  {
    "text": "like you create three different namespaces and when the part is created you specify which namespace this part",
    "start": "1591210",
    "end": "1599280"
  },
  {
    "text": "belongs to and if you don't have any like home now create any namespace by",
    "start": "1599280",
    "end": "1605159"
  },
  {
    "text": "default the part will assign to a default namespace and how to like",
    "start": "1605159",
    "end": "1614070"
  },
  {
    "start": "1612000",
    "end": "1612000"
  },
  {
    "text": "specify resource requirement in in space we have a code objects so in connecting",
    "start": "1614070",
    "end": "1619950"
  },
  {
    "text": "you you probably know des right everything is a pair projects and so if",
    "start": "1619950",
    "end": "1625110"
  },
  {
    "text": "you create a resource code on the object in one specific namespace you can in a",
    "start": "1625110",
    "end": "1631289"
  },
  {
    "text": "spec specify ok how much it is required how much limit and that it is specified",
    "start": "1631289",
    "end": "1639750"
  },
  {
    "text": "like how much parts in this namespace can request and how much is the limit",
    "start": "1639750",
    "end": "1645840"
  },
  {
    "text": "you want to set for all the paths in this namespace and when part is created",
    "start": "1645840",
    "end": "1654179"
  },
  {
    "text": "in this namespace and the system will check whether it violates the code huh",
    "start": "1654179",
    "end": "1659820"
  },
  {
    "text": "assigned for this namespace",
    "start": "1659820",
    "end": "1664279"
  },
  {
    "text": "so by I have this restriction right we basically kind of partition the",
    "start": "1665429",
    "end": "1671590"
  },
  {
    "text": "resources among different namespaces and also if you have a resource code on in a",
    "start": "1671590",
    "end": "1678519"
  },
  {
    "text": "namespace then all the parts in that part required to have resource limit and",
    "start": "1678519",
    "end": "1685600"
  },
  {
    "text": "requests specified so to help user to have some default value we have another",
    "start": "1685600",
    "end": "1691120"
  },
  {
    "text": "API project called limit range and you can in this limit range you specify some",
    "start": "1691120",
    "end": "1696490"
  },
  {
    "text": "default value for a former storage here as example and when the part is created",
    "start": "1696490",
    "end": "1704289"
  },
  {
    "text": "in this namespace and if you don't specify anything ready for they will apply to those default values",
    "start": "1704289",
    "end": "1712378"
  },
  {
    "start": "1715000",
    "end": "1715000"
  },
  {
    "text": "okay in summary we talked about like how we support locally for storage as for",
    "start": "1715980",
    "end": "1723190"
  },
  {
    "text": "scarce resource and we talked about how it support the management at container",
    "start": "1723190",
    "end": "1729340"
  },
  {
    "text": "part level so you can have like resource allocation and limitation note level",
    "start": "1729340",
    "end": "1735519"
  },
  {
    "text": "read by a loop by a low category concept we can ensure the system stability and a",
    "start": "1735519",
    "end": "1742179"
  },
  {
    "text": "namespace level right have a partition the resources among different groups and",
    "start": "1742179",
    "end": "1750669"
  },
  {
    "start": "1749000",
    "end": "1749000"
  },
  {
    "text": "for future work first so far we more talked about the disk space allocation",
    "start": "1750669",
    "end": "1757440"
  },
  {
    "text": "and this guy open way is also a resource shared by other containers and so it's",
    "start": "1757440",
    "end": "1766330"
  },
  {
    "text": "really very nice to be able to pass it yes to allocate resources or have some",
    "start": "1766330",
    "end": "1773559"
  },
  {
    "text": "kind of resource management for disk i/o it's very challenging we know and we are",
    "start": "1773559",
    "end": "1779320"
  },
  {
    "text": "still discussing and whether we can support it and how we can support it and now since we add this locally for more",
    "start": "1779320",
    "end": "1787059"
  },
  {
    "text": "storage resource management will extend our magic API so to allow user to like",
    "start": "1787059",
    "end": "1794200"
  },
  {
    "text": "very easy to more how resources are allocated and how resources are consumed and as I",
    "start": "1794200",
    "end": "1802540"
  },
  {
    "text": "mentioned earlier we want to also support the pot level limit limit and",
    "start": "1802540",
    "end": "1808930"
  },
  {
    "text": "request a setting because it's more convenient for users just consider as",
    "start": "1808930",
    "end": "1814300"
  },
  {
    "text": "potable and also it allowed the containers instead of how to share",
    "start": "1814300",
    "end": "1819460"
  },
  {
    "text": "resources instead of like a fire agree of isolation among each individual",
    "start": "1819460",
    "end": "1825280"
  },
  {
    "text": "container and so far we talked about",
    "start": "1825280",
    "end": "1830650"
  },
  {
    "text": "resource management way also we only set the requirement aesthetically right",
    "start": "1830650",
    "end": "1836530"
  },
  {
    "text": "before pot great you set a value and when the pot is running you cannot change that value but in many cases the",
    "start": "1836530",
    "end": "1845260"
  },
  {
    "text": "workload stretch just keep changing over time the value set may not appropriate",
    "start": "1845260",
    "end": "1851740"
  },
  {
    "text": "anymore then you will lose the benefit of those resource management and we",
    "start": "1851740",
    "end": "1858700"
  },
  {
    "text": "should consider at how we can dynamic manage resources so we allow you that I",
    "start": "1858700",
    "end": "1864610"
  },
  {
    "text": "may change those request citing and based on the application behavior and",
    "start": "1864610",
    "end": "1871980"
  },
  {
    "text": "also for system processes range if they consume resources and they may also",
    "start": "1871980",
    "end": "1878740"
  },
  {
    "text": "change over time rather bit smaller clothes come in the system overhead will become bigger the the initial setting",
    "start": "1878740",
    "end": "1885850"
  },
  {
    "text": "may not enough good enough so we want to like based on the system behavior how to",
    "start": "1885850",
    "end": "1892570"
  },
  {
    "text": "dynamic is that the reserve resources and a low category sauce",
    "start": "1892570",
    "end": "1900090"
  },
  {
    "text": "and this work is a team work I want to acknowledge our communities hew members",
    "start": "1901110",
    "end": "1908010"
  },
  {
    "start": "1902000",
    "end": "1902000"
  },
  {
    "text": "and also community contributors so when it is open right if anyone inches you",
    "start": "1908010",
    "end": "1915750"
  },
  {
    "text": "can also contribute okay that's about my",
    "start": "1915750",
    "end": "1921120"
  },
  {
    "text": "talk any questions yes",
    "start": "1921120",
    "end": "1926870"
  },
  {
    "text": "yes yes so right now bananas itself does not provide any analytical tool for this",
    "start": "1938570",
    "end": "1945600"
  },
  {
    "text": "purpose but I think during some talks I noticed there are some people like",
    "start": "1945600",
    "end": "1951950"
  },
  {
    "text": "working on some this project to analyze the resource usage and to give you",
    "start": "1951950",
    "end": "1957210"
  },
  {
    "text": "better estimation of your application and also we plan to support some dynamic",
    "start": "1957210",
    "end": "1963149"
  },
  {
    "text": "sighting and in this way use that someone Erika - all right you can better",
    "start": "1963149",
    "end": "1969450"
  },
  {
    "text": "estimates the resource usage so renauldi hakuna itself does not support anything yet yes okay",
    "start": "1969450",
    "end": "1981980"
  },
  {
    "text": "disks so right now we only focusing on",
    "start": "1985919",
    "end": "1996570"
  },
  {
    "text": "the root filesystem right and pay today we only monitor how much available in",
    "start": "1996570",
    "end": "2002149"
  },
  {
    "text": "for the root filesystem and we don't consider other disks if you do have that",
    "start": "2002149",
    "end": "2008809"
  },
  {
    "text": "and you can kind of have to manage yourself to make sure you have enough",
    "start": "2008809",
    "end": "2014690"
  },
  {
    "text": "capacity yeah it because in in the typical setting right the local former",
    "start": "2014690",
    "end": "2020239"
  },
  {
    "text": "storage is backed up by the root filesystem yeah yes",
    "start": "2020239",
    "end": "2027789"
  },
  {
    "text": "so busily like say we have example like",
    "start": "2034340",
    "end": "2039870"
  },
  {
    "text": "specify employer right in the path spec you directly specify that volume and by",
    "start": "2039870",
    "end": "2046080"
  },
  {
    "text": "default those volumes are backed by their local disk and persistent volumes",
    "start": "2046080",
    "end": "2052200"
  },
  {
    "text": "those are typically remote disks like enclosure gcpd it as EBS and we",
    "start": "2052200",
    "end": "2061350"
  },
  {
    "text": "currently also start supporting local disks but there should be in a separate dedicated disk and this how a like",
    "start": "2061350",
    "end": "2070560"
  },
  {
    "text": "managed persistent volumes and they separate from theirs locally former",
    "start": "2070560",
    "end": "2075868"
  },
  {
    "text": "storage yeah",
    "start": "2075869",
    "end": "2078470"
  },
  {
    "text": "work on the like I can't say I began the",
    "start": "2086290",
    "end": "2098220"
  },
  {
    "text": "[Music] temporary Jing I think provisioning no",
    "start": "2098960",
    "end": "2108910"
  },
  {
    "text": "right now we don't have any work done for the thing provisioning I think yeah",
    "start": "2108910",
    "end": "2116160"
  },
  {
    "text": "unfortunately or like static posting or like dynamic resizing like dynamic static conditioning for example the plan",
    "start": "2127300",
    "end": "2135350"
  },
  {
    "text": "is too late I think the plan is flee",
    "start": "2135350",
    "end": "2141590"
  },
  {
    "text": "keep it completely separate so we probably see people like building operators for different sets of",
    "start": "2141590",
    "end": "2148640"
  },
  {
    "text": "constraints because there's no one storage solution that people like so it's gonna like keep it separate I see",
    "start": "2148640",
    "end": "2157550"
  },
  {
    "text": "someone in the back have questions oh yeah",
    "start": "2157550",
    "end": "2162730"
  },
  {
    "text": "any other questions yes okay so the",
    "start": "2162730",
    "end": "2202880"
  },
  {
    "text": "question was instead of using do you to measure the storage usage of a pod or container",
    "start": "2202880",
    "end": "2208369"
  },
  {
    "text": "have you ever considered using projects quotas in XFS or extension for is in XFS",
    "start": "2208369",
    "end": "2218450"
  },
  {
    "text": "i thought was intrigued yeah before I extension for my dilatory but for exercise I think yeah short answer to",
    "start": "2218450",
    "end": "2232790"
  },
  {
    "text": "that is like yes we have been thinking about it for two years I took a while for ext4 support tool and it wasn't for",
    "start": "2232790",
    "end": "2238850"
  },
  {
    "text": "ten I think and even then like the the user space tooling isn't complete yet like all the",
    "start": "2238850",
    "end": "2244070"
  },
  {
    "text": "cooler tools it's about the plan is to have like part level like you basically need multiple",
    "start": "2244070",
    "end": "2250269"
  },
  {
    "text": "project IDs for a given pod so we want to get that but there's like you wanna",
    "start": "2250269",
    "end": "2256569"
  },
  {
    "text": "enable people who are like have them do what they can do with the level of tooling and access today before trying",
    "start": "2256569",
    "end": "2262239"
  },
  {
    "text": "to go to ask Roger code up there but just like if you want to get that get up to you know we don't want to go the ldm",
    "start": "2262239",
    "end": "2274959"
  },
  {
    "text": "road because it's like just totally all the way to the bottom line stay as close",
    "start": "2274959",
    "end": "2280089"
  },
  {
    "text": "to correct if he I agree you doing that",
    "start": "2280089",
    "end": "2295449"
  },
  {
    "text": "like if he had a separate file system that it would be great but there's also like scoffs so there's no single storage",
    "start": "2295449",
    "end": "2307199"
  },
  {
    "text": "you know can you make ephemeral storage you know driver plugging pluggable so I",
    "start": "2310920",
    "end": "2316359"
  },
  {
    "text": "can use LVM as someone may implement project waters my short answer to that",
    "start": "2316359",
    "end": "2323859"
  },
  {
    "text": "is pilot issue [Laughter]",
    "start": "2323859",
    "end": "2329430"
  },
  {
    "text": "correct but then you don't get you don't get the you know you don't get all the",
    "start": "2330029",
    "end": "2336569"
  },
  {
    "text": "addictions and like you don't get resource tracking and liberation that way you get this under control",
    "start": "2336569",
    "end": "2342659"
  },
  {
    "text": "hang on you know separated at post level",
    "start": "2342659",
    "end": "2353869"
  },
  {
    "text": "yes so now today evaporates at the first level it's a rule so the fundamental",
    "start": "2353869",
    "end": "2365219"
  },
  {
    "text": "philosophy so let you not leading this to explore the design philosophy is like",
    "start": "2365219",
    "end": "2374699"
  },
  {
    "text": "over on your network for in general like so you have storage separate to compute",
    "start": "2374699",
    "end": "2381319"
  },
  {
    "text": "and you use crash only when you absolutely need it and most of your apps might not need it they use it for",
    "start": "2381319",
    "end": "2387449"
  },
  {
    "text": "caching or like logging yeah that's",
    "start": "2387449",
    "end": "2398869"
  },
  {
    "text": "that's the problem we want to address yes thank you thank you",
    "start": "2398869",
    "end": "2410280"
  },
  {
    "text": "[Applause]",
    "start": "2410280",
    "end": "2412789"
  }
]