[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "hi there I'm Bob this is Sean hi we we",
    "start": "1050",
    "end": "7290"
  },
  {
    "text": "both do a lot of work on SIG's scalability as part of the kubernetes community so we we did a session like",
    "start": "7290",
    "end": "15509"
  },
  {
    "text": "this at coop con austin the last one and",
    "start": "15509",
    "end": "21180"
  },
  {
    "text": "we came in prepared to do more of a developer discussion and we had a whole",
    "start": "21180",
    "end": "26640"
  },
  {
    "text": "deck and just stuff we were gonna talk about and what happened was a lot of people who were not really like core",
    "start": "26640",
    "end": "33239"
  },
  {
    "text": "developers wanting to talk about scalability showed up a lot of people interested in building and running large clusters showed up so we decided that we",
    "start": "33239",
    "end": "40230"
  },
  {
    "text": "would take the lesson and kind of shift the discussion today to be more in that direction but let me go ahead here so",
    "start": "40230",
    "end": "51390"
  },
  {
    "start": "50000",
    "end": "86000"
  },
  {
    "text": "the idea for the session was to kind of introduce SIG's scalability provide some",
    "start": "51390",
    "end": "58350"
  },
  {
    "text": "general education around how the project thinks about big clusters how we do",
    "start": "58350",
    "end": "64018"
  },
  {
    "text": "testing really want to kind of be oriented towards operator concerns and questions we'll try to keep it a little",
    "start": "64019",
    "end": "71100"
  },
  {
    "text": "bit on the short side so if there's QA last time we discovered there was lots of QA people wanted to talk about then we ended up getting thrown out of the",
    "start": "71100",
    "end": "77250"
  },
  {
    "text": "room because people wanted to hang around and talk about big cluster operations experience so with that we'll",
    "start": "77250",
    "end": "83369"
  },
  {
    "text": "move along okay so just a update on six",
    "start": "83369",
    "end": "89040"
  },
  {
    "start": "86000",
    "end": "181000"
  },
  {
    "text": "scalability so if you're interested in big clusters six scalability is a great place to show up and talk about things",
    "start": "89040",
    "end": "95430"
  },
  {
    "text": "and ask so this is the working home by the way the six scalability is the first",
    "start": "95430",
    "end": "100950"
  },
  {
    "text": "was the first grenade' sig formed has been going back continuously yeah I",
    "start": "100950",
    "end": "107100"
  },
  {
    "text": "didn't know that yeah Joe beta and I were the first thing we created in fact it was a sake before we started calling",
    "start": "107100",
    "end": "112710"
  },
  {
    "text": "them six and then we had to rename it okay so you know one of the really key things is setting SLO s like well of",
    "start": "112710",
    "end": "121920"
  },
  {
    "text": "course as a cluster gets bigger and more stuff on it it may get a bit slower and slower like what's what's to slow so I",
    "start": "121920",
    "end": "129780"
  },
  {
    "text": "think Shawn was going to talk about that in a bit primarily concerned with control plane design configuration so",
    "start": "129780",
    "end": "137660"
  },
  {
    "text": "not nodes right so there's different you could think about really big nodes and",
    "start": "137660",
    "end": "142800"
  },
  {
    "text": "nodes starting containers is a performance issue but six scalability is more concerned with how do you build the",
    "start": "142800",
    "end": "150090"
  },
  {
    "text": "control plane to run these kinds of clusters so the concerns are number of nodes pod services really not concerns",
    "start": "150090",
    "end": "157470"
  },
  {
    "text": "or things like nodes and multi cluster as you might expect there's a very heavy",
    "start": "157470",
    "end": "162630"
  },
  {
    "text": "crossover between six scalability and sync testing and of course practically",
    "start": "162630",
    "end": "168690"
  },
  {
    "text": "speaking there's some you know very critical systems design things within a",
    "start": "168690",
    "end": "173910"
  },
  {
    "text": "kubernetes system like how is your at CD cluster set up and configured so just",
    "start": "173910",
    "end": "183450"
  },
  {
    "start": "181000",
    "end": "210000"
  },
  {
    "text": "logistics if you're interested in joining the meetings so we meet bi-weekly thursdays this is essentially just before every two weeks just before",
    "start": "183450",
    "end": "190230"
  },
  {
    "text": "the main community meeting links here you can always find me afterwards as well this is all in the kubernetes",
    "start": "190230",
    "end": "196500"
  },
  {
    "text": "community project as well so if you want to go and look at this list of SIG's and",
    "start": "196500",
    "end": "201660"
  },
  {
    "text": "also there is a group but most of the actions on slack in terms of discussion",
    "start": "201660",
    "end": "207540"
  },
  {
    "text": "between people so alright with that I",
    "start": "207540",
    "end": "214470"
  },
  {
    "start": "210000",
    "end": "393000"
  },
  {
    "text": "will hand it off the show yeah so hi I'm Sean and I'm one of the developers very",
    "start": "214470",
    "end": "219510"
  },
  {
    "text": "closely working with six scalability and I'm a engineer at Google before going",
    "start": "219510",
    "end": "225690"
  },
  {
    "text": "this actually I wanted to add a couple of points here like one of the things which often happens and like I see",
    "start": "225690",
    "end": "232859"
  },
  {
    "text": "happening is like auto scaling is also not a concern like auto scaling doesn't",
    "start": "232859",
    "end": "238590"
  },
  {
    "text": "really fall into the purview of scalability and there a lot of questions and of people and people looking for",
    "start": "238590",
    "end": "244500"
  },
  {
    "text": "auto scaling related questions so this is not really going to address those",
    "start": "244500",
    "end": "250380"
  },
  {
    "text": "that stuff around that and I also want to highlight the fact that scalability",
    "start": "250380",
    "end": "256620"
  },
  {
    "text": "as I see it is more of a horizontal effort so we are having a very big",
    "start": "256620",
    "end": "264330"
  },
  {
    "text": "crossover with other things because we are trying to like monitor and",
    "start": "264330",
    "end": "269940"
  },
  {
    "text": "improve performance of various components and like test this stuff out",
    "start": "269940",
    "end": "275700"
  },
  {
    "text": "across different companies in the cluster there are verticals and so for example",
    "start": "275700",
    "end": "284490"
  },
  {
    "text": "if you're looking for something like performance of different Network plugins",
    "start": "284490",
    "end": "289560"
  },
  {
    "text": "or trying out or trying to understand the performance of let's say custom",
    "start": "289560",
    "end": "294810"
  },
  {
    "text": "schedulers or trying out different container runtimes and these kind of fall under separate verticals and",
    "start": "294810",
    "end": "301500"
  },
  {
    "text": "usually different SIG's we are trying to facilitate them with scale testing and",
    "start": "301500",
    "end": "307220"
  },
  {
    "text": "often consult them with performance related stuff but we are not actively testing out different configurations we",
    "start": "307220",
    "end": "313380"
  },
  {
    "text": "are usually testing out a kind of a core plain vanilla Kuban it is offering so I",
    "start": "313380",
    "end": "320990"
  },
  {
    "text": "probably start by explaining what is our",
    "start": "320990",
    "end": "326190"
  },
  {
    "text": "role like one of our roles is basically to perform scalability testing more so",
    "start": "326190",
    "end": "333990"
  },
  {
    "text": "as part of CI in the Kuban this project and every release we are running a set",
    "start": "333990",
    "end": "339330"
  },
  {
    "text": "of tests and we run tests at different scales starting from hundred nodes all the way till five thousand nodes and",
    "start": "339330",
    "end": "345000"
  },
  {
    "text": "across different providers it's not a very exhaustive list but we are right",
    "start": "345000",
    "end": "350490"
  },
  {
    "text": "now running our tests mostly on GCE and some 1gk and there's also a framework called cube mark that I will come come",
    "start": "350490",
    "end": "358320"
  },
  {
    "text": "back to later it's kind of a cloud provider agnostic way of let's say",
    "start": "358320",
    "end": "364200"
  },
  {
    "text": "running scale tests using simulated clusters they are lightweight but they offer a lot of signal on the control",
    "start": "364200",
    "end": "372030"
  },
  {
    "text": "plane scalability and yeah so these tests are really important they are",
    "start": "372030",
    "end": "377190"
  },
  {
    "text": "blocking the release the Kuban it is release and usually we kind of face some",
    "start": "377190",
    "end": "383130"
  },
  {
    "text": "major issues every release with some some aspect of scalability screwed up and we are trying to fix it and make",
    "start": "383130",
    "end": "390240"
  },
  {
    "text": "things really production-ready in the project so yesterday while I was talking to Bob of I I kind of was",
    "start": "390240",
    "end": "399870"
  },
  {
    "start": "393000",
    "end": "588000"
  },
  {
    "text": "talking about this mental model the name performance envelope was coined by Bob I was",
    "start": "399870",
    "end": "406290"
  },
  {
    "text": "thinking more along something like a scalability space or something so there",
    "start": "406290",
    "end": "412110"
  },
  {
    "text": "are different dimensions in which you can try to run the class like try to",
    "start": "412110",
    "end": "417210"
  },
  {
    "text": "push the cluster to towards its limits so for example you have you have the",
    "start": "417210",
    "end": "424230"
  },
  {
    "text": "number of namespaces the amount of churn you can create in the cluster the density of parts you can run in and and",
    "start": "424230",
    "end": "432180"
  },
  {
    "text": "different stuff so the way it works is like a lot of these limits are you can",
    "start": "432180",
    "end": "439020"
  },
  {
    "text": "see think of them as absolute limits in terms of like like if you had to keep",
    "start": "439020",
    "end": "446330"
  },
  {
    "text": "kind of like if you had to keep other things under control then you can tweak this to its maximum but you cannot have",
    "start": "446330",
    "end": "453570"
  },
  {
    "text": "all the maximum at the same time so it's kind of like a sub space in which you in",
    "start": "453570",
    "end": "459300"
  },
  {
    "text": "which you can you have the guarantees of the of our SLS I I'll go through what",
    "start": "459300",
    "end": "466140"
  },
  {
    "text": "VSS we currently have later on but yeah for example we say we support 5000 node",
    "start": "466140",
    "end": "472590"
  },
  {
    "text": "clusters and we also say we support 30 parts per no 100 pots per node like",
    "start": "472590",
    "end": "477930"
  },
  {
    "text": "officially but we don't support both of them simultaneously which is like 5000 nodes and 100 pods so for example the",
    "start": "477930",
    "end": "485370"
  },
  {
    "text": "photonic no stress tests that we are running have like 30 pots running per node and which and the reason is that",
    "start": "485370",
    "end": "493620"
  },
  {
    "text": "like simultaneously stressing out on all the different aspects can like effect",
    "start": "493620",
    "end": "500850"
  },
  {
    "text": "the let's say the performance and the control plane significantly another",
    "start": "500850",
    "end": "507090"
  },
  {
    "text": "example is like we say that we support close to 10,000 services and then we",
    "start": "507090",
    "end": "513240"
  },
  {
    "text": "also say we support 1,000 endpoints our back ends in a single service but if you try to run like 110 thousand services",
    "start": "513240",
    "end": "519930"
  },
  {
    "text": "each with 1000 backends the control plane will most likely not be able to take this load so yeah so these are a",
    "start": "519930",
    "end": "527580"
  },
  {
    "text": "few examples so yeah that's pretty much what I wanted to",
    "start": "527580",
    "end": "533400"
  },
  {
    "text": "say here and this is what I was saying saying earlier that if you want to like really skew towards one particular limit",
    "start": "533400",
    "end": "540060"
  },
  {
    "text": "you probably will be more limited on other friends so you heard when you're",
    "start": "540060",
    "end": "545550"
  },
  {
    "text": "designing your own cluster as a user if you want to like really start a big cluster you need to carefully consider",
    "start": "545550",
    "end": "553200"
  },
  {
    "text": "these factors like what are the things that you're going to most use what is the what is the kind of churn you're",
    "start": "553200",
    "end": "558480"
  },
  {
    "text": "going to create what are the most how big are your nodes and like what is the",
    "start": "558480",
    "end": "564990"
  },
  {
    "text": "density you want to have pots with and things like these and we are usually we are usually consulting people on like",
    "start": "564990",
    "end": "573030"
  },
  {
    "text": "what are the things that they will need to keep in mind when doing such things and the limits and stuff and if if you",
    "start": "573030",
    "end": "578580"
  },
  {
    "text": "were looking for creating large clusters you probably and and have some questions you probably should contact us on slack",
    "start": "578580",
    "end": "586050"
  },
  {
    "text": "channel we have a six scalable distraction yeah and yeah this is more",
    "start": "586050",
    "end": "593040"
  },
  {
    "start": "588000",
    "end": "882000"
  },
  {
    "text": "about the testing that we are performing we have a test grid which is kind of",
    "start": "593040",
    "end": "598410"
  },
  {
    "text": "like for now it's mostly like a very limited knowledge only limited set of",
    "start": "598410",
    "end": "604650"
  },
  {
    "text": "people know what exactly we are testing and we probably should one of the things that we want to do is like make this more open and make this more visible to",
    "start": "604650",
    "end": "612270"
  },
  {
    "text": "people like what are the configurations we are testing and what are the different things that we're that you're measuring and and part of that is like",
    "start": "612270",
    "end": "620840"
  },
  {
    "text": "the scalability release validation process that you introduced like a couple of releases back where we are for",
    "start": "620840",
    "end": "627120"
  },
  {
    "text": "each release we are giving a spec of what is the configuration we test it out and it's like I was saying earlier it's",
    "start": "627120",
    "end": "633150"
  },
  {
    "text": "mostly a plain vanilla configuration because you cannot really test out on all combinations across each verticals",
    "start": "633150",
    "end": "639600"
  },
  {
    "text": "because then you'll have a combinatorial explosion of cases to test but if you",
    "start": "639600",
    "end": "645210"
  },
  {
    "text": "are interested in testing some particular feature or like you're",
    "start": "645210",
    "end": "650700"
  },
  {
    "text": "interested in looking like how does a particular configuration perform we",
    "start": "650700",
    "end": "656580"
  },
  {
    "text": "might be able to assist you on the slack channel or - or mailing list",
    "start": "656580",
    "end": "663440"
  },
  {
    "text": "yeah so the testicle has it's mostly border tests but maybe I can quickly show you the performance dashboard we",
    "start": "663440",
    "end": "669680"
  },
  {
    "text": "display all the results of our performance runs so this is just an",
    "start": "669680",
    "end": "676819"
  },
  {
    "text": "example graph where we are kind of running benchmark tests against the shed Euler and and like seeing what is the",
    "start": "676819",
    "end": "684350"
  },
  {
    "text": "latency of the shedule operations and there is a array of different",
    "start": "684350",
    "end": "689829"
  },
  {
    "text": "combinations like of tests are you running like like I was Aria saying we run on GC mostly on Google Cloud",
    "start": "689829",
    "end": "695660"
  },
  {
    "text": "actually pretty much almost completely on Google Cloud right now all the tests and we hopefully will be",
    "start": "695660",
    "end": "704629"
  },
  {
    "text": "having these soon on other clouds as well so yeah so this is for example let",
    "start": "704629",
    "end": "713269"
  },
  {
    "text": "me show maybe yeah so this is for",
    "start": "713269",
    "end": "720709"
  },
  {
    "text": "example the port necessary yeah this is",
    "start": "720709",
    "end": "726949"
  },
  {
    "text": "the hundred node cluster test where we're running our density test we primarily have two tests one is the",
    "start": "726949",
    "end": "733490"
  },
  {
    "text": "density and the other is the load which is not so exhaustive as you can imagine and there are probably more stuff that",
    "start": "733490",
    "end": "738889"
  },
  {
    "text": "we can test out so the density test is basically about creating a big cluster",
    "start": "738889",
    "end": "744560"
  },
  {
    "text": "in densely packing it with pods and like basically 30 parts per node and then",
    "start": "744560",
    "end": "750740"
  },
  {
    "text": "creating additional pods and measuring how quickly what is the latency of pods start up and that's where our first SLO",
    "start": "750740",
    "end": "757639"
  },
  {
    "text": "comes which is the pods start of latency a silhouette so in cuba nightosphere officially supporting a pod a 99",
    "start": "757639",
    "end": "764660"
  },
  {
    "text": "percentile of less than 5 seconds of port startup and this does not include the time to pull the image because that",
    "start": "764660",
    "end": "770269"
  },
  {
    "text": "is image variant this is across runs so",
    "start": "770269",
    "end": "776630"
  },
  {
    "text": "we are part of CI so we're continuously running it against the head of the cuban interest master branch and even other",
    "start": "776630",
    "end": "782839"
  },
  {
    "text": "release branches and yeah so this is where you can find quite some useful",
    "start": "782839",
    "end": "788839"
  },
  {
    "text": "information for example you can also look at what is the let's say the",
    "start": "788839",
    "end": "796770"
  },
  {
    "text": "maybe it's City yeah that's the wrong it",
    "start": "796770",
    "end": "801839"
  },
  {
    "text": "city yeah so for example you can look at the resource usage of control in",
    "start": "801839",
    "end": "807959"
  },
  {
    "text": "components for example this is how the HCV memory and CPU looks like so if you plan to start 100 not cluster and kind",
    "start": "807959",
    "end": "816209"
  },
  {
    "text": "of how this much of density and then you probably can expect the memory usage of",
    "start": "816209",
    "end": "822779"
  },
  {
    "text": "HDD to be within this range of course it can vary on several other factors like",
    "start": "822779",
    "end": "828170"
  },
  {
    "text": "what is the kind of requests you make and like how frequently you do stuff and things like these so yeah and and and",
    "start": "828170",
    "end": "848250"
  },
  {
    "text": "the other thing is we are also collecting performance profiles of various kind of things and this is this",
    "start": "848250",
    "end": "854250"
  },
  {
    "text": "is part of our bigger effort to develop more tools to be able to measure and",
    "start": "854250",
    "end": "859560"
  },
  {
    "text": "understand performance in like we have a better understanding of how these",
    "start": "859560",
    "end": "864660"
  },
  {
    "text": "components are working and like part of that is collecting let's say we're collecting the the CPU and memory",
    "start": "864660",
    "end": "870089"
  },
  {
    "text": "profiles of the API server and we some plan to have just more extensively",
    "start": "870089",
    "end": "875370"
  },
  {
    "text": "covered yeah so this is the these are",
    "start": "875370",
    "end": "886890"
  },
  {
    "start": "882000",
    "end": "912000"
  },
  {
    "text": "tips for cluster of so if you're actually planning to start a really big cluster then there are a few things that",
    "start": "886890",
    "end": "892589"
  },
  {
    "text": "you need to keep in mind this is not close to being exhaust exhaustive but",
    "start": "892589",
    "end": "897930"
  },
  {
    "text": "these are just few things that we came up with while creating the slides and",
    "start": "897930",
    "end": "903290"
  },
  {
    "text": "you probably would want to read scalability good practices talk which has more details about what are the",
    "start": "903290",
    "end": "910950"
  },
  {
    "text": "things you should keep in mind so a few examples are let's say this is actually",
    "start": "910950",
    "end": "917880"
  },
  {
    "start": "912000",
    "end": "1078000"
  },
  {
    "text": "a this the first one is based on a real-life incident I think which we saw",
    "start": "917880",
    "end": "923250"
  },
  {
    "text": "a few days earlier so if you're creating a lot of jobs and the jobs are like",
    "start": "923250",
    "end": "928560"
  },
  {
    "text": "frequently getting finished and there is the huge throughput of jobs creating and they're finishing the these job objects",
    "start": "928560",
    "end": "934680"
  },
  {
    "text": "don't naturally get deleted which is kind of a problem that is specific to jobs because they usually run to",
    "start": "934680",
    "end": "941160"
  },
  {
    "text": "completion and all time these jobs start occupying a space of your HCD and this",
    "start": "941160",
    "end": "947430"
  },
  {
    "text": "can eventually cause memory outage issues for HCD so you probably want to",
    "start": "947430",
    "end": "952650"
  },
  {
    "text": "like clean them up yeah and and another",
    "start": "952650",
    "end": "958950"
  },
  {
    "text": "thing which is which is also kind of happens from time to time is like",
    "start": "958950",
    "end": "964410"
  },
  {
    "text": "spamming api server with a lot of calls or some particularly heavy calls so for",
    "start": "964410",
    "end": "972720"
  },
  {
    "text": "example if you're trying to list all the events from a cluster events are like in huge numbers and they're committed by",
    "start": "972720",
    "end": "979620"
  },
  {
    "text": "almost all the control being component component set at a reasonable rate so if you try to list all events this can be",
    "start": "979620",
    "end": "986430"
  },
  {
    "text": "really heavy for the api server because they're just many of them and like i",
    "start": "986430",
    "end": "992670"
  },
  {
    "text": "have seen for example calls where like lists or events last for as much as 45 seconds and you need to remember that",
    "start": "992670",
    "end": "998760"
  },
  {
    "text": "when such long calls are happening you you were you're kind of like taking away",
    "start": "998760",
    "end": "1006980"
  },
  {
    "text": "a part of the api service resources more specs more specifically the daps error buffer for requests it has a limited",
    "start": "1006980",
    "end": "1014810"
  },
  {
    "text": "buffer and it can only take have as many as those requests in flight at any given point so such long calls can actually",
    "start": "1014810",
    "end": "1022640"
  },
  {
    "text": "take up the buffer so as a user when you're operating the cluster or even you",
    "start": "1022640",
    "end": "1029329"
  },
  {
    "text": "have some components which are talking to the api server you usually want to be careful about not creating too much",
    "start": "1029330",
    "end": "1036040"
  },
  {
    "text": "pressure on the api server with calls so these are things that are potential",
    "start": "1036040",
    "end": "1041449"
  },
  {
    "text": "areas of works for example one of the things is we would probably want to have some kind of rate limiting for calls for",
    "start": "1041450",
    "end": "1047689"
  },
  {
    "text": "api calls so that people are not able to like one single client or maybe as a set",
    "start": "1047690",
    "end": "1054290"
  },
  {
    "text": "of clients cannot alone overwhelm the api server and for example the first one they were like we are creating a lot of",
    "start": "1054290",
    "end": "1061610"
  },
  {
    "text": "objects of a particular type one of the things we probably would one of the ways to solve",
    "start": "1061610",
    "end": "1066680"
  },
  {
    "text": "it is have proper quotas in place to be not able to create too many objects in",
    "start": "1066680",
    "end": "1075310"
  },
  {
    "text": "your cluster so yeah and yeah the the",
    "start": "1075310",
    "end": "1082990"
  },
  {
    "start": "1078000",
    "end": "1177000"
  },
  {
    "text": "more important part in this slide that I want to talk about is cue mark so we",
    "start": "1082990",
    "end": "1088970"
  },
  {
    "text": "have this framework called cue mark which is basically our way to perform large cluster tests without actually",
    "start": "1088970",
    "end": "1095570"
  },
  {
    "text": "starting large clusters so this this is like a simulated cluster where you start",
    "start": "1095570",
    "end": "1100640"
  },
  {
    "text": "a cuban it is master separately but you don't really create real nodes but you",
    "start": "1100640",
    "end": "1106310"
  },
  {
    "text": "actually fake them so you create what we call as hollow nodes which are basically",
    "start": "1106310",
    "end": "1111470"
  },
  {
    "text": "pods on top of another cluster and each pod has a hollow version of cube light",
    "start": "1111470",
    "end": "1117500"
  },
  {
    "text": "cube proxy candy and basically the node agents hollow versions of those which",
    "start": "1117500",
    "end": "1123080"
  },
  {
    "text": "actually don't do real work but they still talk to the API server as if as if they're real components doing real work",
    "start": "1123080",
    "end": "1129500"
  },
  {
    "text": "so so by this we're able to actually start really big clusters which is as much as five 5,000 node clusters on like",
    "start": "1129500",
    "end": "1136450"
  },
  {
    "text": "any an 8080 it's something like on 700 course so if you're a user trying to",
    "start": "1136450",
    "end": "1145550"
  },
  {
    "text": "test performance of something and you don't have access to too many too much",
    "start": "1145550",
    "end": "1152360"
  },
  {
    "text": "code are too many computer resources this is something you will probably want to try out and people are I have seen",
    "start": "1152360",
    "end": "1159800"
  },
  {
    "text": "people increasingly showing interest in this there there so initially we had this just it was possible to run this",
    "start": "1159800",
    "end": "1167180"
  },
  {
    "text": "only on GC and then we made it kind of cloud provider independent and then there are there are a couple of people",
    "start": "1167180",
    "end": "1172700"
  },
  {
    "text": "who even try to do this on on Prem yeah",
    "start": "1172700",
    "end": "1179300"
  },
  {
    "start": "1177000",
    "end": "1273000"
  },
  {
    "text": "so so yeah the first dog has a few more important pointers things to keep in",
    "start": "1179300",
    "end": "1186170"
  },
  {
    "text": "mind when you're actually trying to build a scalable cluster and the second",
    "start": "1186170",
    "end": "1194330"
  },
  {
    "text": "one is a doc I recently wrote it is basically kind of composition of small stories",
    "start": "1194330",
    "end": "1201830"
  },
  {
    "text": "around whatever the kind of scalability regressions that we saw in the past in",
    "start": "1201830",
    "end": "1207530"
  },
  {
    "text": "the project and scalability like you're saying is a is a very cross-cutting area and a change in any particular component",
    "start": "1207530",
    "end": "1215480"
  },
  {
    "text": "of the system can potentially break the scalability of the of the entire cluster",
    "start": "1215480",
    "end": "1220580"
  },
  {
    "text": "and scalability in that sense is actually a very sensitive characteristic",
    "start": "1220580",
    "end": "1225650"
  },
  {
    "text": "so for example let's say if if the cubelet starts making one extra call",
    "start": "1225650",
    "end": "1233630"
  },
  {
    "text": "let's say every one minute on a five thousand node cluster this is like five thousand calls every one when it which",
    "start": "1233630",
    "end": "1239060"
  },
  {
    "text": "is like you know close to 180 QPS or or whatever so you are adding a whole lot",
    "start": "1239060",
    "end": "1244370"
  },
  {
    "text": "of QPS on the ApS order and yeah so in general there are very interesting cases",
    "start": "1244370",
    "end": "1251690"
  },
  {
    "text": "there that you might want to look at and that that also kind of demonstrates like how six scalability is kind of",
    "start": "1251690",
    "end": "1259100"
  },
  {
    "text": "collaborating with other six on on areas of work especially around performance and like trying to point out to other",
    "start": "1259100",
    "end": "1265730"
  },
  {
    "text": "six are okay this is causing an issue this particular thing doesn't really scale well and so yeah and the other",
    "start": "1265730",
    "end": "1275930"
  },
  {
    "text": "thing that bob initially mentioned which we are quite closely associated with and",
    "start": "1275930",
    "end": "1281120"
  },
  {
    "text": "we rack we actually own the SL ice and SL O's for the for the for the Cuban",
    "start": "1281120",
    "end": "1287570"
  },
  {
    "text": "itis and like the first SLO that I already mentioned mentioned earlier was to have a 99 percentile of POD start-up",
    "start": "1287570",
    "end": "1294170"
  },
  {
    "text": "latency less than five seconds and the other main SLO that we have is having an",
    "start": "1294170",
    "end": "1300440"
  },
  {
    "text": "API called latency 99 percentile of API call agencies across called types to be",
    "start": "1300440",
    "end": "1308210"
  },
  {
    "text": "less than one second so for each kind of call let's say get bored so like get secret so all these",
    "start": "1308210",
    "end": "1315170"
  },
  {
    "text": "all so 99 percentile of each kind of call should be less than one second and this is the other test that you are",
    "start": "1315170",
    "end": "1321110"
  },
  {
    "text": "performing which is called the load test and yeah so those are basically the two",
    "start": "1321110",
    "end": "1328220"
  },
  {
    "text": "sls we have in general we realized that these are solos not completely informative and in most",
    "start": "1328220",
    "end": "1336170"
  },
  {
    "text": "cases are not enough because there are some aspects of the system which are not completely covered or that are left out",
    "start": "1336170",
    "end": "1343430"
  },
  {
    "text": "by these s ellos and it's possible there it's possible to have scenarios where the oscilloscope tup but still there is",
    "start": "1343430",
    "end": "1350030"
  },
  {
    "text": "something going bad in the cluster and something really bad so one of the efforts one of the fronts that we are",
    "start": "1350030",
    "end": "1355940"
  },
  {
    "text": "working on is to trying to define new SL s and s ellos which are more fine-grained and cover different",
    "start": "1355940",
    "end": "1363820"
  },
  {
    "text": "interactions in the system and voytek who is our other sig lead beside bob is",
    "start": "1363820",
    "end": "1370700"
  },
  {
    "text": "like doing quite some work on this and he's come up with a proposal for a set",
    "start": "1370700",
    "end": "1376220"
  },
  {
    "text": "of a slice and this dog so if you're actually trying to know what are the",
    "start": "1376220",
    "end": "1382210"
  },
  {
    "text": "limits of various kinds of objects we can have in in Cuba notice then this",
    "start": "1382210",
    "end": "1389780"
  },
  {
    "text": "talk is kind of as far as I know the one main source of truth so like I've said",
    "start": "1389780",
    "end": "1397070"
  },
  {
    "text": "this again is kind of like this basically shows the limits along each",
    "start": "1397070",
    "end": "1402500"
  },
  {
    "text": "front but not like a combination of all the limits so yeah you might want to",
    "start": "1402500",
    "end": "1408590"
  },
  {
    "text": "take a look at this if you're trying to like use really big clusters yeah so",
    "start": "1408590",
    "end": "1416540"
  },
  {
    "start": "1413000",
    "end": "1545000"
  },
  {
    "text": "this is about what do we plan to do in future some high level areas of work so",
    "start": "1416540",
    "end": "1423590"
  },
  {
    "text": "one of one of the things that we are keen to look at is like having tests",
    "start": "1423590",
    "end": "1429200"
  },
  {
    "text": "that are running real workloads as opposed to like simple tests that you",
    "start": "1429200",
    "end": "1434450"
  },
  {
    "text": "are running right now which is just starting passports and and not really like measuring performance within in",
    "start": "1434450",
    "end": "1443780"
  },
  {
    "text": "more realistic production like setups we're actually trying to up the game by",
    "start": "1443780",
    "end": "1449120"
  },
  {
    "text": "not by by not doing this ourselves but actually by trying to create a tool",
    "start": "1449120",
    "end": "1455500"
  },
  {
    "text": "there's this tool called cluster loader that we are trying to work on by which we want to like empower normal",
    "start": "1455500",
    "end": "1462320"
  },
  {
    "text": "developers to be able to start there on workload to perform their own load",
    "start": "1462320",
    "end": "1468740"
  },
  {
    "text": "tests be able to configure what is the kind of things that you like what is the kind of tests you want to do what",
    "start": "1468740",
    "end": "1475460"
  },
  {
    "text": "exactly do you want to load tests so if if you're really interested then interested in load testing some",
    "start": "1475460",
    "end": "1481760"
  },
  {
    "text": "particular setups watch out for this this is still kind of in the design phase and like we're still thinking out",
    "start": "1481760",
    "end": "1487850"
  },
  {
    "text": "about the design yeah and the other thing I've already mentioned is having",
    "start": "1487850",
    "end": "1493520"
  },
  {
    "text": "more improved and better SLI xerneas loss at the third main area is",
    "start": "1493520",
    "end": "1498590"
  },
  {
    "text": "developing tools and frameworks which basically help us understand how",
    "start": "1498590",
    "end": "1509020"
  },
  {
    "text": "understanded a better detail the tiny aspects of performance and like be able",
    "start": "1509020",
    "end": "1514670"
  },
  {
    "text": "to understand what exactly is happening at different places and the and the",
    "start": "1514670",
    "end": "1525530"
  },
  {
    "text": "other thing okay I can't remember that anyway so yeah so we want to have better",
    "start": "1525530",
    "end": "1533330"
  },
  {
    "text": "tools and better frameworks to be able to measure things and do scale testing",
    "start": "1533330",
    "end": "1538820"
  },
  {
    "text": "better and yeah Bob would want to talk about so know why because I have close",
    "start": "1538820",
    "end": "1544400"
  },
  {
    "text": "to zero idea oh so this is solder Bui was something that the the hep tio folks",
    "start": "1544400",
    "end": "1550820"
  },
  {
    "start": "1545000",
    "end": "1699000"
  },
  {
    "text": "have been working on for a long time it's the in long time in kubernetes years Sanne Bui is the framework that's",
    "start": "1550820",
    "end": "1557900"
  },
  {
    "text": "used for conformance testing but it's really a general-purpose a framework that can be used for different kinds of",
    "start": "1557900",
    "end": "1563150"
  },
  {
    "text": "testing and so one of the things we were talking about this week even was being able to use sana bui with with a focus",
    "start": "1563150",
    "end": "1571280"
  },
  {
    "text": "density argument to run these kinds of scale tests so that you could have an",
    "start": "1571280",
    "end": "1577040"
  },
  {
    "text": "existing cluster and just like you could run conformance against that cluster and get conformance results you could do the",
    "start": "1577040",
    "end": "1582230"
  },
  {
    "text": "same for a performance result and the idea here would be this is kind of along the line of tools and frameworks for",
    "start": "1582230",
    "end": "1587300"
  },
  {
    "text": "empowering users to figure out what's going on in their own clusters with regard specifically to these kinds of",
    "start": "1587300",
    "end": "1593210"
  },
  {
    "text": "scaling tests so I think that was a our",
    "start": "1593210",
    "end": "1598280"
  },
  {
    "text": "last slide I think so okay yeah that's pretty much the last slide and more or less a",
    "start": "1598280",
    "end": "1603590"
  },
  {
    "text": "description of what you've been working on and what we were planning to work on so yeah I guess it's time for questions",
    "start": "1603590",
    "end": "1610250"
  },
  {
    "text": "if you have some questions or concerns about scalability or performance",
    "start": "1610250",
    "end": "1617408"
  },
  {
    "text": "oh we do have longer tests as well so so okay so maybe I should rip yes yeah",
    "start": "1635960",
    "end": "1663190"
  },
  {
    "text": "now that's a good one for you there",
    "start": "1691710",
    "end": "1700320"
  },
  {
    "text": "isn't really good story for rightly meeting now the only right Freight",
    "start": "1700320",
    "end": "1706049"
  },
  {
    "text": "limited right limiting that we are currently doing is like client site right GPS limits and number of in-flight",
    "start": "1706049",
    "end": "1713850"
  },
  {
    "text": "requests in API server we would like to do something like rightly meeting based",
    "start": "1713850",
    "end": "1721529"
  },
  {
    "text": "on like use it or amount of resources that API server has but that's not even",
    "start": "1721529",
    "end": "1727980"
  },
  {
    "text": "yet designed so I don't expect it will happen anytime soon ish that's the food that's the plan that",
    "start": "1727980",
    "end": "1737039"
  },
  {
    "text": "we have like medium long term but I can't promise it will happen when when it will happen exactly so we still like",
    "start": "1737039",
    "end": "1745250"
  },
  {
    "text": "don't have a very good solution for that",
    "start": "1745250",
    "end": "1749720"
  },
  {
    "text": "yes I think that at least short-term it's it sounds reasonable more",
    "start": "1798779",
    "end": "1804640"
  },
  {
    "text": "longer-term we would we would like to have at least things like fairness with respect to API calls we would like to",
    "start": "1804640",
    "end": "1811929"
  },
  {
    "text": "have priorities of API calls for example like note heartbeats should be should",
    "start": "1811929",
    "end": "1818200"
  },
  {
    "text": "should have some higher priority because like if they if we if note our cubelets",
    "start": "1818200",
    "end": "1823330"
  },
  {
    "text": "are not able to send heartbeats then it can for a longer time it may result in",
    "start": "1823330",
    "end": "1828610"
  },
  {
    "text": "pot evictions which is like obviously a bad experience so there are a couple couple things in the like area of rate",
    "start": "1828610",
    "end": "1837880"
  },
  {
    "text": "limiting or whatever we call it that we we have in mind but we don't really have",
    "start": "1837880",
    "end": "1844240"
  },
  {
    "text": "capacity to work on",
    "start": "1844240",
    "end": "1847799"
  },
  {
    "start": "1854000",
    "end": "1906000"
  },
  {
    "text": "that's what I was just sitting here thinking that that would probably be a good idea so I think I think there are",
    "start": "1855019",
    "end": "1860750"
  },
  {
    "text": "some ways here to improve that the other",
    "start": "1860750",
    "end": "1868129"
  },
  {
    "text": "thing that you probably noticed on the screen is that most of the test grid is presently presently g ki g ke or g SE e",
    "start": "1868129",
    "end": "1876110"
  },
  {
    "text": "and that's another area that we're working on improving video questions how",
    "start": "1876110",
    "end": "1899570"
  },
  {
    "text": "are you so like in general and there are",
    "start": "1899570",
    "end": "1911120"
  },
  {
    "start": "1906000",
    "end": "1996000"
  },
  {
    "text": "a lot of pods that are coming up there are things that are associated the",
    "start": "1911120",
    "end": "1916279"
  },
  {
    "text": "operations that the control plane does which kind of like i would say the primary aspect is like one main aspect",
    "start": "1916279",
    "end": "1923809"
  },
  {
    "text": "of it is the kind of calls that are emitted as part of this process of starting pods and some of them are for",
    "start": "1923809",
    "end": "1929419"
  },
  {
    "text": "example cubelet posting statuses ipod statuses and like emitting events for",
    "start": "1929419",
    "end": "1936080"
  },
  {
    "text": "different parts of the pod startup lifecycle and like if you have a really",
    "start": "1936080",
    "end": "1941149"
  },
  {
    "text": "huge churn then there can be like many cubelets from across the cluster they were trying to simultaneously send a lot",
    "start": "1941149",
    "end": "1947840"
  },
  {
    "text": "of this and actually like from my experience i I've seen a case where like",
    "start": "1947840",
    "end": "1954850"
  },
  {
    "text": "someone was trying to like upgrade a deployment and a really huge deployment",
    "start": "1954850",
    "end": "1960379"
  },
  {
    "text": "and they kind of upgraded it without",
    "start": "1960379",
    "end": "1965419"
  },
  {
    "text": "having any limits on max available or max unavailable overall which basically",
    "start": "1965419",
    "end": "1971210"
  },
  {
    "text": "led to a situation that a huge number of those pods are being deleted and created",
    "start": "1971210",
    "end": "1976399"
  },
  {
    "text": "which kind of like hurt the API server and I think more specifically the hcd",
    "start": "1976399",
    "end": "1981919"
  },
  {
    "text": "hcd couldn't cope up with so many requests so like yeah we really kind of suggested them to",
    "start": "1981919",
    "end": "1988890"
  },
  {
    "text": "Harlem so so in a healthy situation I",
    "start": "1988890",
    "end": "1998490"
  },
  {
    "start": "1996000",
    "end": "2047000"
  },
  {
    "text": "would say that like there is you are kind of limited naturally because for example like there's like ups limit in",
    "start": "1998490",
    "end": "2005840"
  },
  {
    "text": "scheduler of set to 20 which means that you won't be able to to start more than",
    "start": "2005840",
    "end": "2012679"
  },
  {
    "text": "20 pots per second if they are like scheduled by the system obviously like you if you create manually pods that are",
    "start": "2012679",
    "end": "2018920"
  },
  {
    "text": "already like signed to some notes then you will be but in majority of healthy",
    "start": "2018920",
    "end": "2025640"
  },
  {
    "text": "situations the throughput that like kubernetes supports is currently at the",
    "start": "2025640",
    "end": "2031880"
  },
  {
    "text": "level of 20 by default because those QPS limits are like tunable so",
    "start": "2031880",
    "end": "2040330"
  },
  {
    "start": "2047000",
    "end": "2099000"
  },
  {
    "text": "that is a good question we are pushing that 200 in our 5,000 node tests but for",
    "start": "2048190",
    "end": "2057069"
  },
  {
    "text": "production there are there may be some cases that we still have not explored",
    "start": "2057069",
    "end": "2062500"
  },
  {
    "text": "enough to be able to confidently promise that 100 will work always so we are kind of bumping the ups 200 in both",
    "start": "2062500",
    "end": "2069398"
  },
  {
    "text": "controller manager and she do that - basically we quickly create parts and quickly schedule them but yeah but",
    "start": "2069399",
    "end": "2076599"
  },
  {
    "text": "that's a because those are paused containers as well it's kind of a it's a synthetic case so in every in a real",
    "start": "2076599",
    "end": "2082780"
  },
  {
    "text": "system you would expect that you're not gonna actually be able to start the containers just like if you're gonna",
    "start": "2082780",
    "end": "2087878"
  },
  {
    "text": "pull down beep if you're gonna be pulling down container images that you your total system is probably gonna be",
    "start": "2087879",
    "end": "2093599"
  },
  {
    "text": "dominated by that maybe not but it's odd",
    "start": "2093599",
    "end": "2100510"
  },
  {
    "text": "to do this like there is there isn't any one single universal answer to this",
    "start": "2100510",
    "end": "2105549"
  },
  {
    "text": "because it really depends like on how big the cluster is like how big your",
    "start": "2105549",
    "end": "2110890"
  },
  {
    "text": "master machine is or whatever machine where your control plane is running if throwing more resources helps with many",
    "start": "2110890",
    "end": "2118690"
  },
  {
    "text": "different components here so I it I would I would say that proudly for like",
    "start": "2118690",
    "end": "2125950"
  },
  {
    "text": "relatively small clusters we can go even higher than 100 even now if you throw",
    "start": "2125950",
    "end": "2132760"
  },
  {
    "text": "enough resources to control plane",
    "start": "2132760",
    "end": "2137579"
  },
  {
    "text": "well you know we had this on the slides to begin with because this one came up last time as well and I think this",
    "start": "2144220",
    "end": "2150710"
  },
  {
    "start": "2145000",
    "end": "2209000"
  },
  {
    "text": "inevitably leads to discussions of well what is the big enough cluster and I think this is something we want to",
    "start": "2150710",
    "end": "2156260"
  },
  {
    "text": "continue to get feedback from the user community on at the 5,000 ode cluster",
    "start": "2156260",
    "end": "2161750"
  },
  {
    "text": "level you're you're starting to trip across to think to factors so one factor",
    "start": "2161750",
    "end": "2167360"
  },
  {
    "text": "is the number of companies or groups that are actually interested in running at that scale and the other is you're",
    "start": "2167360",
    "end": "2176180"
  },
  {
    "text": "starting to trip across like failure domain considerations you know you're talking about a lot of eggs in one basket and you're probably starting to",
    "start": "2176180",
    "end": "2182780"
  },
  {
    "text": "get into the zone where you should be thinking about a Multi cluster design and really this like though this whole",
    "start": "2182780",
    "end": "2188720"
  },
  {
    "text": "conversation really at that kind of scale you should probably be testing",
    "start": "2188720",
    "end": "2194030"
  },
  {
    "text": "like understand exactly what your profile is in terms of container starts and notes and these other",
    "start": "2194030",
    "end": "2199790"
  },
  {
    "text": "characteristics so that you're able to qualify the configuration you're using to ensure that your production system is",
    "start": "2199790",
    "end": "2205940"
  },
  {
    "text": "working properly but this comes up I mean we have to be I think in the last",
    "start": "2205940",
    "end": "2211820"
  },
  {
    "start": "2209000",
    "end": "2287000"
  },
  {
    "text": "session and the last session in Austin we had people asking about you know 25",
    "start": "2211820",
    "end": "2216950"
  },
  {
    "text": "30 thousand node kinds of system yeah I think we even had a quick kinds of like one big cluster of many small clusters",
    "start": "2216950",
    "end": "2224060"
  },
  {
    "text": "and you remember like it was cute towards one of them and yeah I don't think it was one should we do the same",
    "start": "2224060",
    "end": "2230720"
  },
  {
    "text": "so like how many people here wanna run think they want to run more than 5,000 nodes in a cluster or something of the",
    "start": "2230720",
    "end": "2237740"
  },
  {
    "text": "order for thousands like that well let's strike but let's go to a thousand thousand nodes",
    "start": "2237740",
    "end": "2244690"
  },
  {
    "text": "looks like 750 hundred nodes how many",
    "start": "2244690",
    "end": "2249950"
  },
  {
    "text": "people yeah so we've got I mean even in this room right I mean we have very steep dropoff but that said it is a good",
    "start": "2249950",
    "end": "2259100"
  },
  {
    "text": "point right I mean there are definitely operational advantages you get by running fewer bigger clusters so that",
    "start": "2259100",
    "end": "2264530"
  },
  {
    "text": "does tend to skew things maybe in the bigger direction over time are we",
    "start": "2264530",
    "end": "2271900"
  },
  {
    "text": "right yeah Josh are you a question for long yes that's a very good question and",
    "start": "2272430",
    "end": "2290190"
  },
  {
    "start": "2287000",
    "end": "2411000"
  },
  {
    "text": "this is a discussion we've been having like but maybe we can take this offline",
    "start": "2290190",
    "end": "2297340"
  },
  {
    "text": "Josh if this is like not if this is like query developer centric because this is a long discussion in there is there is",
    "start": "2297340",
    "end": "2303610"
  },
  {
    "text": "no well-defined like a to well-defined process right now we just have like scalable to release blockers and just",
    "start": "2303610",
    "end": "2310210"
  },
  {
    "text": "some time closed before the release with like trying to like put all our energy into it and fix stuff which is not the",
    "start": "2310210",
    "end": "2316270"
  },
  {
    "text": "best way to do it so there are processes we are introducing - like adding scalability pre supplements and be",
    "start": "2316270",
    "end": "2322540"
  },
  {
    "text": "flaking our post submits and there are a few efforts along those lines but yeah they should probably talk about it later",
    "start": "2322540",
    "end": "2328420"
  },
  {
    "text": "I think the scalability presa myths but you're not going to run it down like 5,000 oh I don't oh yeah yeah those are",
    "start": "2328420",
    "end": "2334210"
  },
  {
    "text": "the skin already pre summits cannot be like 5,000 order because you can't ever say on them fast and at that rate they still should probably catch so yeah",
    "start": "2334210",
    "end": "2342850"
  },
  {
    "text": "yeah so we have like 100 node tests and 100 node scalability presubmit and they're already running now by the",
    "start": "2342850",
    "end": "2348490"
  },
  {
    "text": "way they're just not blocking the submit queue but yes they should catch a good number of regressions of course I think",
    "start": "2348490",
    "end": "2356140"
  },
  {
    "text": "I think that generally the problem is that our tests are super super flaky it's not that they are consistently",
    "start": "2356140",
    "end": "2361960"
  },
  {
    "text": "broken which would be easier to fix probably it's flakiness ratio and like unflagging test is like one of the",
    "start": "2361960",
    "end": "2368640"
  },
  {
    "text": "priorities that like we have yes all",
    "start": "2368640",
    "end": "2374950"
  },
  {
    "text": "right I think we're at a time or don't mean quick double check here maybe one more question",
    "start": "2374950",
    "end": "2381690"
  },
  {
    "text": "we have a recommended provider conflict but I don't know if that's super updated",
    "start": "2410609",
    "end": "2416469"
  },
  {
    "start": "2411000",
    "end": "2449000"
  },
  {
    "text": "but in its general in general actually we don't test too many configurations",
    "start": "2416469",
    "end": "2422199"
  },
  {
    "text": "but maybe in general we are running everything on a single machine yeah or like either like single master machine",
    "start": "2422199",
    "end": "2430869"
  },
  {
    "text": "or like couple masters but then still all the master all the control plane components are are running on this",
    "start": "2430869",
    "end": "2437559"
  },
  {
    "text": "single machine I mean we are not splitting for example at CD to separate machines or like schedulers or or other",
    "start": "2437559",
    "end": "2444789"
  },
  {
    "text": "things like it really depends on the",
    "start": "2444789",
    "end": "2451209"
  },
  {
    "start": "2449000",
    "end": "2501000"
  },
  {
    "text": "environment like if if for example your eye ops limits are bounded to - to the",
    "start": "2451209",
    "end": "2460359"
  },
  {
    "text": "machine itself then potentially yes like we have since we are doing this on GCE",
    "start": "2460359",
    "end": "2466390"
  },
  {
    "text": "we we can tweak some of those at least independently like for example I ops are",
    "start": "2466390",
    "end": "2471910"
  },
  {
    "text": "strictly correlated with the disk size which is independent from the CPUs so we",
    "start": "2471910",
    "end": "2477279"
  },
  {
    "text": "can throw more I ops by increasing disk size and we we did we we do such stuff",
    "start": "2477279",
    "end": "2482829"
  },
  {
    "text": "and like so and it really depends like what are you constraints of your",
    "start": "2482829",
    "end": "2487959"
  },
  {
    "text": "environment so in general yes it's in some environment in some environments it",
    "start": "2487959",
    "end": "2493779"
  },
  {
    "text": "makes sense to displayed some controllers it's to split some some control buying components in two",
    "start": "2493779",
    "end": "2500319"
  },
  {
    "text": "separate machines if you look at the provider dock that we linked to there a",
    "start": "2500319",
    "end": "2506529"
  },
  {
    "start": "2501000",
    "end": "2550000"
  },
  {
    "text": "lot of the construction of the provider dock was based on getting a bunch of",
    "start": "2506529",
    "end": "2512140"
  },
  {
    "text": "feedback from people wanting big clusters and in that case the imitation is to run a separate a TD",
    "start": "2512140",
    "end": "2517630"
  },
  {
    "text": "plane but some of that is really when you're starting to run clusters that size some of your considerations are you",
    "start": "2517630",
    "end": "2526420"
  },
  {
    "text": "know fault tolerance as opposed to necessarily strict performance so yeah",
    "start": "2526420",
    "end": "2532809"
  },
  {
    "text": "in general for resources around scalability like your one-stop shop would probably be like the six",
    "start": "2532809",
    "end": "2539890"
  },
  {
    "text": "scalability Docs in the community oh yeah thanks thanks all right thanks",
    "start": "2539890",
    "end": "2547840"
  },
  {
    "text": "[Applause]",
    "start": "2547840",
    "end": "2551849"
  }
]