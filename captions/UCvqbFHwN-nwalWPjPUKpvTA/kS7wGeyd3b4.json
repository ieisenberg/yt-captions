[
  {
    "text": "good morning everyone uh it's so excited",
    "start": "1240",
    "end": "3600"
  },
  {
    "text": "to be here I'm Chen Wong from IBM",
    "start": "3600",
    "end": "5839"
  },
  {
    "text": "research I'm a Staff research scientist",
    "start": "5839",
    "end": "8760"
  },
  {
    "text": "uh working on cloud native AI platform",
    "start": "8760",
    "end": "11320"
  },
  {
    "text": "and I have been working on kuber netics",
    "start": "11320",
    "end": "13000"
  },
  {
    "text": "for five more years this is",
    "start": "13000",
    "end": "16000"
  },
  {
    "text": "abishek hello folks um I'm a senior",
    "start": "16000",
    "end": "18760"
  },
  {
    "text": "software engineer here at IBM and",
    "start": "18760",
    "end": "21199"
  },
  {
    "text": "excited to be",
    "start": "21199",
    "end": "23000"
  },
  {
    "text": "here so uh today uh we are going to",
    "start": "23000",
    "end": "26119"
  },
  {
    "text": "introduce some of our experience and",
    "start": "26119",
    "end": "28240"
  },
  {
    "text": "best practices on um serving uh so",
    "start": "28240",
    "end": "31359"
  },
  {
    "text": "before that we all know um has been",
    "start": "31359",
    "end": "34559"
  },
  {
    "text": "attracting a lot of attentions and then",
    "start": "34559",
    "end": "37280"
  },
  {
    "text": "serving RMS uh will help a lot in um",
    "start": "37280",
    "end": "41440"
  },
  {
    "text": "modernize the the the existing business",
    "start": "41440",
    "end": "44320"
  },
  {
    "text": "use cases and applications so however",
    "start": "44320",
    "end": "48039"
  },
  {
    "text": "serving uh large language models is very",
    "start": "48039",
    "end": "50680"
  },
  {
    "text": "expensive so first it needs to run on",
    "start": "50680",
    "end": "53640"
  },
  {
    "text": "highend GPU accelerator such as a100 or",
    "start": "53640",
    "end": "57680"
  },
  {
    "text": "even h100 and then the sequential nature",
    "start": "57680",
    "end": "61280"
  },
  {
    "text": "of large language models uh makes the",
    "start": "61280",
    "end": "64000"
  },
  {
    "text": "inference uh processing time very long",
    "start": "64000",
    "end": "66760"
  },
  {
    "text": "so for one a100 uh accelerator if you",
    "start": "66760",
    "end": "70040"
  },
  {
    "text": "want to precise and request is less than",
    "start": "70040",
    "end": "73000"
  },
  {
    "text": "one request per second so if you are",
    "start": "73000",
    "end": "75320"
  },
  {
    "text": "thinking about production use cases or",
    "start": "75320",
    "end": "77600"
  },
  {
    "text": "our business use cases you may need tons",
    "start": "77600",
    "end": "80040"
  },
  {
    "text": "of those request inference requests",
    "start": "80040",
    "end": "82000"
  },
  {
    "text": "going through and then it will need tons",
    "start": "82000",
    "end": "84280"
  },
  {
    "text": "of gpus which is apparently very costly",
    "start": "84280",
    "end": "88240"
  },
  {
    "text": "and then um the the the diagram just",
    "start": "88240",
    "end": "90600"
  },
  {
    "text": "shows um a simple toy example of how is",
    "start": "90600",
    "end": "94640"
  },
  {
    "text": "making the inference",
    "start": "94640",
    "end": "97079"
  },
  {
    "text": "iteratively and um so is generating",
    "start": "97079",
    "end": "100960"
  },
  {
    "text": "token per token and every time it",
    "start": "100960",
    "end": "102840"
  },
  {
    "text": "generates a new token which is a word it",
    "start": "102840",
    "end": "105920"
  },
  {
    "text": "needs to cash all the previous tokens in",
    "start": "105920",
    "end": "108560"
  },
  {
    "text": "your cash so this sequential nature",
    "start": "108560",
    "end": "111240"
  },
  {
    "text": "makes it very uh like takes a lot of",
    "start": "111240",
    "end": "115240"
  },
  {
    "text": "resources and makes the um inference",
    "start": "115240",
    "end": "118079"
  },
  {
    "text": "very slow",
    "start": "118079",
    "end": "120000"
  },
  {
    "text": "so uh that's why in production use cases",
    "start": "120000",
    "end": "123000"
  },
  {
    "text": "we really want to uh have some",
    "start": "123000",
    "end": "126119"
  },
  {
    "text": "techniques to improve the serut and",
    "start": "126119",
    "end": "128759"
  },
  {
    "text": "performance of um inference and then",
    "start": "128759",
    "end": "131440"
  },
  {
    "text": "there are two popular techniques uh in",
    "start": "131440",
    "end": "134360"
  },
  {
    "text": "the open source of a wider Academia",
    "start": "134360",
    "end": "137200"
  },
  {
    "text": "Community to improve the LM serving uh",
    "start": "137200",
    "end": "140879"
  },
  {
    "text": "one is called continuous batching and",
    "start": "140879",
    "end": "143560"
  },
  {
    "text": "the other is patient attention so for",
    "start": "143560",
    "end": "146120"
  },
  {
    "text": "the batching part continuous batching is",
    "start": "146120",
    "end": "148519"
  },
  {
    "text": "basically derived from the",
    "start": "148519",
    "end": "150400"
  },
  {
    "text": "static batching which is utilize more",
    "start": "150400",
    "end": "152720"
  },
  {
    "text": "memory to batch more request to use more",
    "start": "152720",
    "end": "155959"
  },
  {
    "text": "uh GPU more efficiently and then",
    "start": "155959",
    "end": "158120"
  },
  {
    "text": "continuous batching is just continuously",
    "start": "158120",
    "end": "160840"
  },
  {
    "text": "understanding how many requests are",
    "start": "160840",
    "end": "162280"
  },
  {
    "text": "coming in and then utilize the memory",
    "start": "162280",
    "end": "164920"
  },
  {
    "text": "better to continuous put request uh",
    "start": "164920",
    "end": "168840"
  },
  {
    "text": "candidate request to the previous one so",
    "start": "168840",
    "end": "171720"
  },
  {
    "text": "uh you maximize your memory utilization",
    "start": "171720",
    "end": "174560"
  },
  {
    "text": "as well as your GPU utilization uh so",
    "start": "174560",
    "end": "177239"
  },
  {
    "text": "the page attention kernel technique is",
    "start": "177239",
    "end": "179440"
  },
  {
    "text": "similar is tried to M The Logical blocks",
    "start": "179440",
    "end": "183280"
  },
  {
    "text": "of KV cache uh which is necessary to",
    "start": "183280",
    "end": "186360"
  },
  {
    "text": "generate the next token to uh the",
    "start": "186360",
    "end": "188840"
  },
  {
    "text": "physical KV cach blocks so you can have",
    "start": "188840",
    "end": "192040"
  },
  {
    "text": "more efficient uh utilization of memory",
    "start": "192040",
    "end": "195480"
  },
  {
    "text": "and reduce the resource fragmentations",
    "start": "195480",
    "end": "197599"
  },
  {
    "text": "in the uh memory space",
    "start": "197599",
    "end": "200519"
  },
  {
    "text": "allocation uh so um in our case we have",
    "start": "200519",
    "end": "204239"
  },
  {
    "text": "research clusters and we want to serve a",
    "start": "204239",
    "end": "206879"
  },
  {
    "text": "lot of GPU models for uh The Wider uh",
    "start": "206879",
    "end": "210720"
  },
  {
    "text": "users in our research lab and then um uh",
    "start": "210720",
    "end": "214560"
  },
  {
    "text": "we found out if we want serve a wider",
    "start": "214560",
    "end": "216959"
  },
  {
    "text": "range of models and then some models are",
    "start": "216959",
    "end": "220120"
  },
  {
    "text": "very popular and some models may be",
    "start": "220120",
    "end": "222799"
  },
  {
    "text": "idoling for uh a long time but",
    "start": "222799",
    "end": "226040"
  },
  {
    "text": "researchers has still want to use those",
    "start": "226040",
    "end": "228959"
  },
  {
    "text": "and in this case for example we if we",
    "start": "228959",
    "end": "231239"
  },
  {
    "text": "want to serve 50 models and fund out 30",
    "start": "231239",
    "end": "234280"
  },
  {
    "text": "of those are uh unpopular models but",
    "start": "234280",
    "end": "237560"
  },
  {
    "text": "necessary to serve and then if we use",
    "start": "237560",
    "end": "240120"
  },
  {
    "text": "one GPU to serve those um popular models",
    "start": "240120",
    "end": "243879"
  },
  {
    "text": "and then we will find out we still have",
    "start": "243879",
    "end": "245760"
  },
  {
    "text": "a very long longtail GPU",
    "start": "245760",
    "end": "249439"
  },
  {
    "text": "underutilized so how can we solve this",
    "start": "249439",
    "end": "252159"
  },
  {
    "text": "problem and uh we are thinking about",
    "start": "252159",
    "end": "255040"
  },
  {
    "text": "like packing more unpopular models in",
    "start": "255040",
    "end": "258400"
  },
  {
    "text": "fewer number of gpus and then the",
    "start": "258400",
    "end": "261320"
  },
  {
    "text": "available techniques of GPU sharing um",
    "start": "261320",
    "end": "264440"
  },
  {
    "text": "available nowadays can be time sharing",
    "start": "264440",
    "end": "267680"
  },
  {
    "text": "MPS and uh MI",
    "start": "267680",
    "end": "270320"
  },
  {
    "text": "and then due to the nature of the",
    "start": "270320",
    "end": "272280"
  },
  {
    "text": "batching and the the the memory of page",
    "start": "272280",
    "end": "275720"
  },
  {
    "text": "attention kernel or Flash attention",
    "start": "275720",
    "end": "277520"
  },
  {
    "text": "kernals uh memory optimizations in those",
    "start": "277520",
    "end": "281240"
  },
  {
    "text": "servers uh so we really think if you",
    "start": "281240",
    "end": "284440"
  },
  {
    "text": "limit the memory allocation you're not",
    "start": "284440",
    "end": "287160"
  },
  {
    "text": "using the uh compute efficiently enough",
    "start": "287160",
    "end": "290919"
  },
  {
    "text": "so because MPS and time sharing really",
    "start": "290919",
    "end": "294039"
  },
  {
    "text": "uh uh targeting the dynamic sharing of",
    "start": "294039",
    "end": "296560"
  },
  {
    "text": "hbm space and those unpredictable memory",
    "start": "296560",
    "end": "300240"
  },
  {
    "text": "allocation may lead to exceptions easily",
    "start": "300240",
    "end": "304400"
  },
  {
    "text": "uh when there's there are a birthday",
    "start": "304400",
    "end": "306880"
  },
  {
    "text": "requests are coming in so then we want",
    "start": "306880",
    "end": "309840"
  },
  {
    "text": "to try the make partition uh initially",
    "start": "309840",
    "end": "313560"
  },
  {
    "text": "which is static partitioning the uh",
    "start": "313560",
    "end": "316000"
  },
  {
    "text": "memory",
    "start": "316000",
    "end": "317440"
  },
  {
    "text": "space so uh we did did some simple",
    "start": "317440",
    "end": "321000"
  },
  {
    "text": "experiments on uh benchmarking the MST",
    "start": "321000",
    "end": "324080"
  },
  {
    "text": "model uh on different uh varying size of",
    "start": "324080",
    "end": "327840"
  },
  {
    "text": "um make potions and then if we set the",
    "start": "327840",
    "end": "331039"
  },
  {
    "text": "per token uh generation Laten is to 50",
    "start": "331039",
    "end": "334240"
  },
  {
    "text": "millisecond per token and then we found",
    "start": "334240",
    "end": "337280"
  },
  {
    "text": "out like uh if the load is low enough",
    "start": "337280",
    "end": "341440"
  },
  {
    "text": "like there's less uh fewer than 32",
    "start": "341440",
    "end": "344960"
  },
  {
    "text": "concurrent users of uh sending request",
    "start": "344960",
    "end": "348280"
  },
  {
    "text": "then we can guarantee the latency pretty",
    "start": "348280",
    "end": "350560"
  },
  {
    "text": "well using smaller mix sizes like 4G",
    "start": "350560",
    "end": "354680"
  },
  {
    "text": "40gb uh however uh in our practice we",
    "start": "354680",
    "end": "358280"
  },
  {
    "text": "find out if we use Nvidia uh default GPU",
    "start": "358280",
    "end": "361840"
  },
  {
    "text": "operator to enable Meg uh every time we",
    "start": "361840",
    "end": "365000"
  },
  {
    "text": "want to reconfigure the me partitions we",
    "start": "365000",
    "end": "367599"
  },
  {
    "text": "need to evict all the uh workflows on",
    "start": "367599",
    "end": "370599"
  },
  {
    "text": "lgps on a server uh so uh this but but",
    "start": "370599",
    "end": "374800"
  },
  {
    "text": "from because the optimal make potation",
    "start": "374800",
    "end": "378080"
  },
  {
    "text": "we need uh for serving the model really",
    "start": "378080",
    "end": "380560"
  },
  {
    "text": "change over time over very load so we",
    "start": "380560",
    "end": "384360"
  },
  {
    "text": "want a dynamic way to create make",
    "start": "384360",
    "end": "387880"
  },
  {
    "text": "potations so that's why uh abish we'll",
    "start": "387880",
    "end": "391560"
  },
  {
    "text": "talk more about how we use",
    "start": "391560",
    "end": "394880"
  },
  {
    "text": "Dr thank you Chen for uh working us",
    "start": "394880",
    "end": "398039"
  },
  {
    "text": "through the importance of using mix",
    "start": "398039",
    "end": "400000"
  },
  {
    "text": "slices for uh inference workload um",
    "start": "400000",
    "end": "402880"
  },
  {
    "text": "let's quickly uh dive into Dr so Dr",
    "start": "402880",
    "end": "406000"
  },
  {
    "text": "stands for dynamic resource allocation",
    "start": "406000",
    "end": "408880"
  },
  {
    "text": "and it provides two new apis uh",
    "start": "408880",
    "end": "411240"
  },
  {
    "text": "basically resource claim and and",
    "start": "411240",
    "end": "413160"
  },
  {
    "text": "resource class uh to request um GPU",
    "start": "413160",
    "end": "417280"
  },
  {
    "text": "resources and while Dr is a blanket",
    "start": "417280",
    "end": "420440"
  },
  {
    "text": "statement but it solves a very important",
    "start": "420440",
    "end": "422759"
  },
  {
    "text": "use case for us uh which is uh the",
    "start": "422759",
    "end": "425560"
  },
  {
    "text": "ability to have incremental mix lices on",
    "start": "425560",
    "end": "429039"
  },
  {
    "text": "um some of the vendor",
    "start": "429039",
    "end": "431440"
  },
  {
    "text": "gpus so as we see that uh in the Dr",
    "start": "431440",
    "end": "434639"
  },
  {
    "text": "World there is a quite a lot of setup",
    "start": "434639",
    "end": "436639"
  },
  {
    "text": "that is needed to enable uh GPU sharing",
    "start": "436639",
    "end": "439639"
  },
  {
    "text": "so on the right hand side of the screen",
    "start": "439639",
    "end": "442759"
  },
  {
    "text": "in the middle we see that uh there are",
    "start": "442759",
    "end": "445000"
  },
  {
    "text": "different resource claims like GPU claim",
    "start": "445000",
    "end": "447199"
  },
  {
    "text": "parameters and make device claim",
    "start": "447199",
    "end": "448919"
  },
  {
    "text": "parameters",
    "start": "448919",
    "end": "450080"
  },
  {
    "text": "uh that are needed to be set up um and",
    "start": "450080",
    "end": "453120"
  },
  {
    "text": "those um claim parameters are then uh",
    "start": "453120",
    "end": "456680"
  },
  {
    "text": "called on or referen into this OPD model",
    "start": "456680",
    "end": "460120"
  },
  {
    "text": "workload that we see over",
    "start": "460120",
    "end": "462080"
  },
  {
    "text": "here let's quickly uh dive into uh the",
    "start": "462080",
    "end": "466280"
  },
  {
    "text": "demo here so what we do here is uh we",
    "start": "466280",
    "end": "470400"
  },
  {
    "text": "submit the same workload uh that we just",
    "start": "470400",
    "end": "472879"
  },
  {
    "text": "saw on the previous slide um as we",
    "start": "472879",
    "end": "476680"
  },
  {
    "text": "submit this workload we see few uh um",
    "start": "476680",
    "end": "480039"
  },
  {
    "text": "resources that are created but notable",
    "start": "480039",
    "end": "482520"
  },
  {
    "text": "resources here are the resource claims",
    "start": "482520",
    "end": "484520"
  },
  {
    "text": "that are created um once uh the desired",
    "start": "484520",
    "end": "489000"
  },
  {
    "text": "resources are created then uh the",
    "start": "489000",
    "end": "491720"
  },
  {
    "text": "container uh comes up and inside the",
    "start": "491720",
    "end": "494199"
  },
  {
    "text": "container what we have is the VM",
    "start": "494199",
    "end": "497680"
  },
  {
    "text": "server we wait for some time for the VM",
    "start": "497680",
    "end": "501080"
  },
  {
    "text": "server to come up and enable port",
    "start": "501080",
    "end": "503000"
  },
  {
    "text": "forwarding uh to interact with it now uh",
    "start": "503000",
    "end": "507000"
  },
  {
    "text": "we send a prompt or a sentence",
    "start": "507000",
    "end": "508919"
  },
  {
    "text": "completion request to the model uh",
    "start": "508919",
    "end": "511199"
  },
  {
    "text": "saying San Francisco is",
    "start": "511199",
    "end": "513279"
  },
  {
    "text": "a and and finally we do get a response",
    "start": "513279",
    "end": "516518"
  },
  {
    "text": "that uh it's a great place to to live in",
    "start": "516519",
    "end": "519839"
  },
  {
    "text": "uh by the OPD model workload here",
    "start": "519839",
    "end": "523719"
  },
  {
    "text": "uh thank you for uh watching this demo",
    "start": "523720",
    "end": "526880"
  },
  {
    "text": "if you want to learn a bit more about Dr",
    "start": "526880",
    "end": "529519"
  },
  {
    "text": "uh we do have another full talk here um",
    "start": "529519",
    "end": "532800"
  },
  {
    "text": "on on Thursday um questions",
    "start": "532800",
    "end": "537920"
  },
  {
    "text": "[Applause]",
    "start": "538950",
    "end": "545200"
  },
  {
    "text": "thank you thank you okay we do have the",
    "start": "555000",
    "end": "557880"
  },
  {
    "text": "uh the barcode of the talk and also the",
    "start": "557880",
    "end": "560480"
  },
  {
    "text": "other tutorial on how we deploy uh V",
    "start": "560480",
    "end": "563800"
  },
  {
    "text": "server uh using Dr uh in the previous",
    "start": "563800",
    "end": "567399"
  },
  {
    "text": "slides so and the demo link is also",
    "start": "567399",
    "end": "570200"
  },
  {
    "text": "available",
    "start": "570200",
    "end": "573200"
  },
  {
    "text": "[Applause]",
    "start": "573550",
    "end": "578669"
  }
]