[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "hello everyone my name is Anand swaminathan I woke on a team called as flight at left flight a cloud native",
    "start": "510",
    "end": "9990"
  },
  {
    "text": "machine learning and batch processing system as a part of flight we were exposed to kubernetes and operators",
    "start": "9990",
    "end": "15740"
  },
  {
    "text": "fling kubernetes operator was built as a collaboration between flight and the streaming platform so for today's agenda",
    "start": "15740",
    "end": "23490"
  },
  {
    "start": "22000",
    "end": "49000"
  },
  {
    "text": "we are going to look at the flink operator how it has helped us run applications on kubernetes but before",
    "start": "23490",
    "end": "30060"
  },
  {
    "text": "just looking into the solution i want to show you how streaming is very critical to lifts business and some of the",
    "start": "30060",
    "end": "36000"
  },
  {
    "text": "problems that we faced with a legacy deployment on the non kubernetes world",
    "start": "36000",
    "end": "41100"
  },
  {
    "text": "then i will also show you how kubernetes has solved these problems we will end a talk with the demo and also leave some",
    "start": "41100",
    "end": "47610"
  },
  {
    "text": "time for questions so lift business",
    "start": "47610",
    "end": "52829"
  },
  {
    "text": "involves providing transportation as a service everything is real time you open the app you can request bike scooters",
    "start": "52829",
    "end": "59460"
  },
  {
    "text": "cars you can also look at trans information in real time so what this makes is that this makes streaming one",
    "start": "59460",
    "end": "66060"
  },
  {
    "text": "of the most critical components for our business if you look at this picture you can actually see so many components that",
    "start": "66060",
    "end": "72810"
  },
  {
    "text": "has streaming involved for example it goes from pricing to eta2 notifications",
    "start": "72810",
    "end": "79170"
  },
  {
    "text": "to coupons to messaging and so on I just want to give you a very explain a",
    "start": "79170",
    "end": "84990"
  },
  {
    "text": "particular use case that will be familiar so if you open the lift app you can see the nearby cars and the way we",
    "start": "84990",
    "end": "91500"
  },
  {
    "text": "do it is every car once a driver opens the app and joins live we start sending",
    "start": "91500",
    "end": "98220"
  },
  {
    "text": "events from the app towards the system and this is where the ingestion into a",
    "start": "98220",
    "end": "103290"
  },
  {
    "text": "streaming platform begins and from there we help you show the real-time location of the driver the closest estimate of",
    "start": "103290",
    "end": "111090"
  },
  {
    "text": "the time the driver would take to arrive you and once you enter the destination you will also see the time it takes to",
    "start": "111090",
    "end": "117210"
  },
  {
    "text": "reach your destination now just start to maintain high levels of service we have pricing involved a",
    "start": "117210",
    "end": "123780"
  },
  {
    "text": "real-time pricing involved and this is directly dependent on the amount of drivers that are near you and so on",
    "start": "123780",
    "end": "130470"
  },
  {
    "text": "so both pricing and eta is one of the most components of her business and they are",
    "start": "130470",
    "end": "136410"
  },
  {
    "text": "powered by the streaming platform so as I had shown you the use cases it makes",
    "start": "136410",
    "end": "144000"
  },
  {
    "start": "139000",
    "end": "276000"
  },
  {
    "text": "sure that and I want to convey that how streaming compute framework is very important if we rewind the clock two",
    "start": "144000",
    "end": "149460"
  },
  {
    "text": "years back we just are streaming involved just kinases and AWS kinases",
    "start": "149460",
    "end": "156120"
  },
  {
    "text": "client library so the way we did it was view build applications by writing it on",
    "start": "156120",
    "end": "161160"
  },
  {
    "text": "Python and we used a doubles KCl with Multi multi Python multi-line daemon to",
    "start": "161160",
    "end": "168540"
  },
  {
    "text": "build the application but the problem was KCl was not very powerful if you",
    "start": "168540",
    "end": "173940"
  },
  {
    "text": "wanted to do something like aggregation or batching or grouping you had to build all these complex systems on top of KCl",
    "start": "173940",
    "end": "180330"
  },
  {
    "text": "and for one example is that the way we did this was we did it was we would read the data from kinases using a CC CL you",
    "start": "180330",
    "end": "188280"
  },
  {
    "text": "would write to Redis and then we will have a separate feat of workers sharded by know certain regions or a mate hashed",
    "start": "188280",
    "end": "194670"
  },
  {
    "text": "who would read from these Redis and then process further so as you can see like",
    "start": "194670",
    "end": "199800"
  },
  {
    "text": "first doing a simple group by operation we had to build these complex systems with you know having multiple systems in",
    "start": "199800",
    "end": "206790"
  },
  {
    "text": "wall and this was really affecting our reliability so two years back we introduced flink into into our ecosystem",
    "start": "206790",
    "end": "212370"
  },
  {
    "text": "and it has done wonders flink is excellent because it has comes up with a rich feature support it has",
    "start": "212370",
    "end": "218100"
  },
  {
    "text": "amazing functional api support for writing simple analytical applications and you can simply do like group by map",
    "start": "218100",
    "end": "225110"
  },
  {
    "text": "order by very easily in just using simple functions it has amazing sequel",
    "start": "225110",
    "end": "231480"
  },
  {
    "text": "support which we critically needed I said I already mentioned almost all our applications were written on Python so",
    "start": "231480",
    "end": "237660"
  },
  {
    "text": "we did not want to rewrite the application so beam support was very important to us so we would instead just",
    "start": "237660",
    "end": "243270"
  },
  {
    "text": "write beam pipelines which would just convert then convert into a fling job and then run run it pretty easily so",
    "start": "243270",
    "end": "250200"
  },
  {
    "text": "these kind of features made filling pretty attractive but it came with the downside the downside is that Flynn",
    "start": "250200",
    "end": "258030"
  },
  {
    "text": "Flinx deployment model is a bit complex unlike KCl which is like stateless you",
    "start": "258030",
    "end": "264150"
  },
  {
    "text": "can deploy it like any of the stateless service and you'll automatically a charge to your workers flink had a",
    "start": "264150",
    "end": "269910"
  },
  {
    "text": "topology and that makes the deployment model complex I would talk about more about it in the next few slides so how",
    "start": "269910",
    "end": "277860"
  },
  {
    "start": "276000",
    "end": "331000"
  },
  {
    "text": "do we run flink at lift we have beans which right flink application directly",
    "start": "277860",
    "end": "283800"
  },
  {
    "text": "on Java like ETA and these Java applications in turn convert or fling",
    "start": "283800",
    "end": "289050"
  },
  {
    "text": "job and run there are also people who write beam pipelines in Python",
    "start": "289050",
    "end": "294449"
  },
  {
    "text": "these beam pipelines gets converted to fling job basing a fling runner and they also they're being used by the pricing",
    "start": "294449",
    "end": "301710"
  },
  {
    "text": "team at left and on top we have a team called as drift their use case is that",
    "start": "301710",
    "end": "307500"
  },
  {
    "text": "they take sequel queries from research scientists and data engineers who we",
    "start": "307500",
    "end": "312810"
  },
  {
    "text": "then convert these sequel queries into a corresponding fling job and you know make sure their processing is pretty",
    "start": "312810",
    "end": "318720"
  },
  {
    "text": "simple as you can see that all of this is powered by the streaming platform under needs so any investment or any",
    "start": "318720",
    "end": "325560"
  },
  {
    "text": "improvement that you provide to the streaming platform directly reflects on all the use cases I had mentioned to you",
    "start": "325560",
    "end": "335400"
  },
  {
    "start": "331000",
    "end": "393000"
  },
  {
    "text": "about the complexity in the deployment model of link so this is the flink",
    "start": "335400",
    "end": "340530"
  },
  {
    "text": "architecture flink has a job manager and a task manager the role of the job",
    "start": "340530",
    "end": "346889"
  },
  {
    "text": "manager is to act as a coordinator and the rule of the task manager is to is to actually do the work the way you do it",
    "start": "346889",
    "end": "353310"
  },
  {
    "text": "is you provide the job - the job managers the job man that does the work of coordinating the execution by",
    "start": "353310",
    "end": "359849"
  },
  {
    "text": "converting the job graphs into an execution graph it handles coordinating checkpointing recovery of task managers",
    "start": "359849",
    "end": "366659"
  },
  {
    "text": "and so on the task managers in turn have these tasks slots which perform a unit of work so if you want to deploy flink",
    "start": "366659",
    "end": "375330"
  },
  {
    "text": "application you have to build this sort of topology of having a job manager and",
    "start": "375330",
    "end": "380819"
  },
  {
    "text": "a task manager and basically making sure that you know the task manager can talk",
    "start": "380819",
    "end": "385949"
  },
  {
    "text": "to the job medicine and so on so I will show you how we deployed flink in a non",
    "start": "385949",
    "end": "392159"
  },
  {
    "text": "cognitive world first so --lifts deployment was through saltstack",
    "start": "392159",
    "end": "397770"
  },
  {
    "start": "393000",
    "end": "513000"
  },
  {
    "text": "saltstack is the configuration management system that we used to deploy all of the stateless",
    "start": "397770",
    "end": "403530"
  },
  {
    "text": "services this could be like any sense like you know authentication service or a right service which is non flink even all stateless services was deployed",
    "start": "403530",
    "end": "410370"
  },
  {
    "text": "using Saul's tag where we would directly say the resources that we want and those",
    "start": "410370",
    "end": "415979"
  },
  {
    "text": "would end up being created in AWS so what we did was we wrote a wrapper or a layer in between the support flink",
    "start": "415979",
    "end": "423000"
  },
  {
    "text": "application so and how it appeared is that if you write a flink application we",
    "start": "423000",
    "end": "428340"
  },
  {
    "text": "would create three things one is you will create an auto scaling group in AWS",
    "start": "428340",
    "end": "433349"
  },
  {
    "text": "for the job manager of size one then we would create an auto scaling group for",
    "start": "433349",
    "end": "438629"
  },
  {
    "text": "your task manager for the size that you that you need and then we will also create an elastic load balancer which",
    "start": "438629",
    "end": "445710"
  },
  {
    "text": "helps the task manager discover the job manager so which means only if the job",
    "start": "445710",
    "end": "451080"
  },
  {
    "text": "answer goes down you need an a static IP for all the task manager to talk to the job manager so as you can clearly see",
    "start": "451080",
    "end": "457979"
  },
  {
    "text": "that for just running one job you need an ASE for job manager and a C for a",
    "start": "457979",
    "end": "463139"
  },
  {
    "text": "task manager and an elastic load balancer and on top of it the way our",
    "start": "463139",
    "end": "468659"
  },
  {
    "text": "deployment work was beautiful ship the bits to these ac's machines like how we",
    "start": "468659",
    "end": "474930"
  },
  {
    "text": "do for any of the stateless service and then we start the service but for in our case first we deploy the binaries to the",
    "start": "474930",
    "end": "481169"
  },
  {
    "text": "machines and then what we would be doing this we would log on to the job manager and make sure to start the job if you",
    "start": "481169",
    "end": "488880"
  },
  {
    "text": "want to deploy a new code what we end up doing is you have a bunch of scripts where we make sure that we say point the",
    "start": "488880",
    "end": "496020"
  },
  {
    "text": "current job and start the new jobs from the same from this previous save point",
    "start": "496020",
    "end": "501300"
  },
  {
    "text": "on the new code so all of this was pretty manual it was highly error-prone and that sort of things what we wanted",
    "start": "501300",
    "end": "508259"
  },
  {
    "text": "to fix when you when you build the flink operator to move to kubernetes so just",
    "start": "508259",
    "end": "515279"
  },
  {
    "start": "513000",
    "end": "674000"
  },
  {
    "text": "talking more about the problems that we face in our vc deployment as I had mentioned every single fling job",
    "start": "515279",
    "end": "521459"
  },
  {
    "text": "requires you to spin up a filling cluster witches to auto scaling groups and an elastic load balancer but on top",
    "start": "521459",
    "end": "528390"
  },
  {
    "text": "of it if you had multiple versions of the flinging job for example one for staging and one for production you would",
    "start": "528390",
    "end": "533550"
  },
  {
    "text": "now have two groups have you know two job two auto-scaling groups each and an elastic",
    "start": "533550",
    "end": "539130"
  },
  {
    "text": "load balancer so naturally you can see that you can infinitely create or a scaling group there's a clear limit and",
    "start": "539130",
    "end": "545870"
  },
  {
    "text": "what we ended up doing is that each team ended up creating just one cluster and",
    "start": "545870",
    "end": "551339"
  },
  {
    "text": "started running multiple jobs inside the same cluster now you might think that this kind of works because you know the",
    "start": "551339",
    "end": "558630"
  },
  {
    "text": "flink does can run multiple jobs and they all run in the corresponding task managers but the problem is that you are",
    "start": "558630",
    "end": "565139"
  },
  {
    "text": "you might end up running like you know one high priority job which is very critical to your business one low priority job which is not so critical",
    "start": "565139",
    "end": "571500"
  },
  {
    "text": "which can have a downtime and your low priority job might end up affecting your high priority job there could be a",
    "start": "571500",
    "end": "577230"
  },
  {
    "text": "memory leak in one job which would end up killing the entire task manager and that would end up affecting all the",
    "start": "577230",
    "end": "583589"
  },
  {
    "text": "other jobs in the cluster the second biggest problem that we faced was since",
    "start": "583589",
    "end": "590220"
  },
  {
    "text": "we are running on AWS anytime a node goes down it takes roughly about 10",
    "start": "590220",
    "end": "596490"
  },
  {
    "text": "minutes for a new node to come up for the bootstrap process to happen and for the task manager to join the job manager",
    "start": "596490",
    "end": "603480"
  },
  {
    "text": "which means that any time if there is a node failure which happens like we know once a month or no but it or more often",
    "start": "603480",
    "end": "609779"
  },
  {
    "text": "depending on which region you are in you we actually saw more than 10 minutes for",
    "start": "609779",
    "end": "615600"
  },
  {
    "text": "recovery so that is not acceptable for our business and on top of it you can",
    "start": "615600",
    "end": "621240"
  },
  {
    "text": "also see that when we want to scale let's say at 5 p.m. if you want to automatically scale and if you want to",
    "start": "621240",
    "end": "627029"
  },
  {
    "text": "run if you want to add new task manager that also takes more than 10 minutes for",
    "start": "627029",
    "end": "632160"
  },
  {
    "text": "us to scale which might not be acceptable when you know the traffic is suddenly spiky and you want to scale the",
    "start": "632160",
    "end": "638310"
  },
  {
    "text": "other biggest biggest problem was I had mentioned the manual error-prone a manual process involved in deployment",
    "start": "638310",
    "end": "644939"
  },
  {
    "text": "where we had to go to the job manager run the script to switch jobs and so on",
    "start": "644939",
    "end": "650639"
  },
  {
    "text": "so this process was slick error-prone our configuration files were all over",
    "start": "650639",
    "end": "656399"
  },
  {
    "text": "the place and also some other things like you know we want to do an automated rollback like for example let's say I deploy a bad code and the code is not",
    "start": "656399",
    "end": "663360"
  },
  {
    "text": "running I want something to happen so that my old code comes back and the so is just not down so all kinds of stuff",
    "start": "663360",
    "end": "670980"
  },
  {
    "text": "like these we're not supported in our legacy system so let's go look at a",
    "start": "670980",
    "end": "676410"
  },
  {
    "start": "674000",
    "end": "795000"
  },
  {
    "text": "solution the solution is fling kubernetes operator but before you look",
    "start": "676410",
    "end": "681450"
  },
  {
    "text": "at the flame kubernetes operator if you want to use if you want to run flink application on kubernetes using the",
    "start": "681450",
    "end": "687060"
  },
  {
    "text": "fling kubernetes operator the way to do it is by creating the flink application customer resource so the flink",
    "start": "687060",
    "end": "695730"
  },
  {
    "text": "application custom resource is the specification that you provide to run",
    "start": "695730",
    "end": "701100"
  },
  {
    "text": "your filling job so constraint that we have is each flink application custom",
    "start": "701100",
    "end": "707010"
  },
  {
    "text": "resource will create one fling cluster and we'll only run one fling job in it",
    "start": "707010",
    "end": "712230"
  },
  {
    "text": "so if you want to see a run ten fling jobs the way you do it is by creating ten flink application customer resources",
    "start": "712230",
    "end": "719640"
  },
  {
    "text": "and the flink operator would from there go on and do all the heavy lifting for",
    "start": "719640",
    "end": "725220"
  },
  {
    "text": "you there is one particular specific field in the specification that I want to highlight that is the image so when",
    "start": "725220",
    "end": "732630"
  },
  {
    "text": "you create a specification or a spec in the flink application customer resource",
    "start": "732630",
    "end": "738089"
  },
  {
    "text": "the image that we expect has to be runnable what I mean by that is we",
    "start": "738089",
    "end": "744870"
  },
  {
    "text": "should be able to start the job manager and the task manager from the image",
    "start": "744870",
    "end": "750140"
  },
  {
    "text": "since fling kubernetes operator is open source we also have a bunch of examples inside our repositories for to help you",
    "start": "750140",
    "end": "757199"
  },
  {
    "text": "build these runnable images but that is one of the most critical component that you have to do in order to use flink",
    "start": "757199",
    "end": "765300"
  },
  {
    "text": "application on kubernetes so while creating the flink application customer",
    "start": "765300",
    "end": "771449"
  },
  {
    "text": "resource you would provide a specification and that becomes the",
    "start": "771449",
    "end": "776839"
  },
  {
    "text": "desired state that you expect the cluster to be and what flink operator",
    "start": "776839",
    "end": "782190"
  },
  {
    "text": "does is once from the resource it will start populating the status field the status field is actually the current",
    "start": "782190",
    "end": "788940"
  },
  {
    "text": "state of the cluster and I would talk more about how the status is filled in the next slides so when it comes to the",
    "start": "788940",
    "end": "797370"
  },
  {
    "start": "795000",
    "end": "900000"
  },
  {
    "text": "architecture of filling kubernetes operator this diagram might look very similar to a general kubernetes",
    "start": "797370",
    "end": "804070"
  },
  {
    "text": "diagram there is a control plane and there is an execution plane what flink operated appears is as an",
    "start": "804070",
    "end": "811930"
  },
  {
    "text": "extension to the kubernetes control plane so let me take the deployment exam let's say if you were to create a",
    "start": "811930",
    "end": "817839"
  },
  {
    "text": "deployment and kubernetes the way you do it is you would pass in the deployment object to the API server and the",
    "start": "817839",
    "end": "824949"
  },
  {
    "text": "deployment object is stored in that CD and once the object gets stored in HCD",
    "start": "824949",
    "end": "830260"
  },
  {
    "text": "your deployment controller picks up the object and we'll create corresponding replica sets the same happens in in",
    "start": "830260",
    "end": "837550"
  },
  {
    "text": "Frink world as well so the moment you create the flink application custom resource the flink operator fetches the",
    "start": "837550",
    "end": "847269"
  },
  {
    "text": "resource from its CD using the api server and it will create the corresponding fling cluster so what is a",
    "start": "847269",
    "end": "854680"
  },
  {
    "text": "fling cluster in the kubernetes world the flink russian kubernetes world consists of the following it has a job",
    "start": "854680",
    "end": "860649"
  },
  {
    "text": "manager deployment it has a task manager deployment a job manager service which",
    "start": "860649",
    "end": "867160"
  },
  {
    "text": "the task manager can use to talk to the job manager and also an ingress for the",
    "start": "867160",
    "end": "872529"
  },
  {
    "text": "flinky wide so you can you want to be able to access the UI from your laptop and so on so for you to make any",
    "start": "872529",
    "end": "878579"
  },
  {
    "text": "requests to the job manager from extend from outside the kubernetes cluster we have created an ingress for you so this",
    "start": "878579",
    "end": "885339"
  },
  {
    "text": "group of job manager task 1 is the deployment the job manager service and",
    "start": "885339",
    "end": "891250"
  },
  {
    "text": "the ingress together is what we call as a flink cluster in kubernetes now let us",
    "start": "891250",
    "end": "897010"
  },
  {
    "text": "see how the flink operator does all of this but before that what is the",
    "start": "897010",
    "end": "902110"
  },
  {
    "start": "900000",
    "end": "975000"
  },
  {
    "text": "fundamental concept that is behind this operator so I want to talk about control",
    "start": "902110",
    "end": "907389"
  },
  {
    "text": "loop because this is the fundamental building block of any controller this could be a deployment control and",
    "start": "907389",
    "end": "912639"
  },
  {
    "text": "controller or a replication controller and this fundamental building block is the same for flink operator the way it",
    "start": "912639",
    "end": "919480"
  },
  {
    "text": "works is the controller looks at the current state of the world and from the",
    "start": "919480",
    "end": "925600"
  },
  {
    "text": "spec it creates a desired state of the world so what it then does is it does a",
    "start": "925600",
    "end": "931930"
  },
  {
    "text": "diff between the current state of the world and the desired state of the and takes an action to move the current",
    "start": "931930",
    "end": "939550"
  },
  {
    "text": "state of the world to the desired state so like for example let's say if we first create a custom resource or flink",
    "start": "939550",
    "end": "945970"
  },
  {
    "text": "application custom resource your expectation there is a fling cluster with a job running in it but in the",
    "start": "945970",
    "end": "952420"
  },
  {
    "text": "current world there is no filling cluster or anything of that sort so the operator states that hey there is",
    "start": "952420",
    "end": "957700"
  },
  {
    "text": "no corresponding cluster that is existing in the kubernetes world so it goes and creates a cluster I want to go",
    "start": "957700",
    "end": "966610"
  },
  {
    "text": "through all the steps involved in the operator - to explain more but this act",
    "start": "966610",
    "end": "971800"
  },
  {
    "text": "of control loop is one of the fundamental building block of this controller so and the core of the flink",
    "start": "971800",
    "end": "981810"
  },
  {
    "start": "975000",
    "end": "1164000"
  },
  {
    "text": "operator is a state machine when you the way this operator works it transitions",
    "start": "981810",
    "end": "989410"
  },
  {
    "text": "the custom resource from one state to another so when you first create the flink",
    "start": "989410",
    "end": "995410"
  },
  {
    "text": "application custom resource what happens is you are actually in the new state now",
    "start": "995410",
    "end": "1000660"
  },
  {
    "text": "there is no cluster for you so what the operator does it first moves the custom",
    "start": "1000660",
    "end": "1006000"
  },
  {
    "text": "resource to the create clusters step where the job manager the task managers are all created once the resources are",
    "start": "1006000",
    "end": "1013320"
  },
  {
    "text": "created the flink operator then goes to wait for the all the parts to come up so this a time duration between when you",
    "start": "1013320",
    "end": "1020970"
  },
  {
    "text": "create all the resource to when all the path pods start coming up and once all",
    "start": "1020970",
    "end": "1027390"
  },
  {
    "text": "the pods are up the operator realizes that hey now it can go and start a job",
    "start": "1027390",
    "end": "1033270"
  },
  {
    "text": "in the job manager and it does this does it by you know transitioning to the submit job state in the submit job state",
    "start": "1033270",
    "end": "1039449"
  },
  {
    "text": "it starts a job in the job manager and if everything goes well the application",
    "start": "1039449",
    "end": "1047160"
  },
  {
    "text": "goes to running so the running here is the desired state that we want the",
    "start": "1047160",
    "end": "1052770"
  },
  {
    "text": "custom resource to be in there is one more path that I want to explain before",
    "start": "1052770",
    "end": "1058320"
  },
  {
    "text": "we go to a demo I will be showing all of this in the demo as well so let's say you have a fling cluster that is up and",
    "start": "1058320",
    "end": "1064500"
  },
  {
    "text": "running and now you make a code change in Java and then you build an image you want to dip",
    "start": "1064500",
    "end": "1069660"
  },
  {
    "text": "your code now to a running cluster so the way you do it is just by simply",
    "start": "1069660",
    "end": "1075510"
  },
  {
    "text": "updating the flink application custom resource so once you update the resource the operator now sees that hey my",
    "start": "1075510",
    "end": "1083580"
  },
  {
    "text": "desired state that is a spec does not match with the current cluster that I have created so what happens is it",
    "start": "1083580",
    "end": "1090540"
  },
  {
    "text": "changes the application resource from running to updating now once it moves to",
    "start": "1090540",
    "end": "1096210"
  },
  {
    "text": "updating what the operated does is it brings up a second cluster for the new",
    "start": "1096210",
    "end": "1101850"
  },
  {
    "text": "spec that that was just updated so which means that now in the updating state you",
    "start": "1101850",
    "end": "1109260"
  },
  {
    "text": "have two clusters running one cluster is the one that is actually doing the work the second cluster is just coming up",
    "start": "1109260",
    "end": "1115590"
  },
  {
    "text": "once the second cluster is up successfully which means if there's a bug and the second cluster does not come",
    "start": "1115590",
    "end": "1121740"
  },
  {
    "text": "up there is no impact your application at all so once the second cluster is up and",
    "start": "1121740",
    "end": "1127080"
  },
  {
    "text": "running we cancel the job in the first cluster by creating at the save point",
    "start": "1127080",
    "end": "1132450"
  },
  {
    "text": "and then start the job completely in the second cluster from the save point so as",
    "start": "1132450",
    "end": "1139620"
  },
  {
    "text": "you can clearly see that if something is going wrong in your code is the second person not coming up or if let's say you",
    "start": "1139620",
    "end": "1146700"
  },
  {
    "text": "know once a cancels job and I start the job in the second cluster if the job submission is failing we automatically",
    "start": "1146700",
    "end": "1153270"
  },
  {
    "text": "roll back to the first cluster we do not delete the first cluster until the job",
    "start": "1153270",
    "end": "1159750"
  },
  {
    "text": "is running successfully in the second cluster so this is roughly the working",
    "start": "1159750",
    "end": "1167310"
  },
  {
    "start": "1164000",
    "end": "1530000"
  },
  {
    "text": "of a state machine I want to just show you a demo of everything that I explained so far so for the demo this is",
    "start": "1167310",
    "end": "1176550"
  },
  {
    "text": "the blink application custom resource that I would be creating here is the image all of this is available and",
    "start": "1176550",
    "end": "1184250"
  },
  {
    "text": "filling kubernetes operator open source repo so you can all try it out you know",
    "start": "1184250",
    "end": "1189330"
  },
  {
    "text": "whenever so this image is runnable as in like you know from this image I can",
    "start": "1189330",
    "end": "1195240"
  },
  {
    "text": "start the job manager and the task manager there are a few other fields like you know job manager config task",
    "start": "1195240",
    "end": "1201930"
  },
  {
    "text": "manager config since these are just not indicating that how many CPU that I want for the job manager",
    "start": "1201930",
    "end": "1207690"
  },
  {
    "text": "how many you know memory that I want for a task manager and so on once I create",
    "start": "1207690",
    "end": "1212730"
  },
  {
    "text": "the spec I would show you that I'll start the operator and we will see what happens",
    "start": "1212730",
    "end": "1219530"
  },
  {
    "text": "so currently there is no application and there is no pods corresponding to the",
    "start": "1225960",
    "end": "1232860"
  },
  {
    "text": "application let me first run the flame kubernetes operators all of this is happening in my",
    "start": "1232860",
    "end": "1239100"
  },
  {
    "text": "docker desktop on my local Copeland is running in my local so I am now creating",
    "start": "1239100",
    "end": "1250650"
  },
  {
    "text": "the custom resource blink application so that I just showed you and let's see what's happening behind the scenes so",
    "start": "1250650",
    "end": "1258240"
  },
  {
    "text": "currently what's happening is that the operator let me just show you slowly so",
    "start": "1258240",
    "end": "1264030"
  },
  {
    "text": "the moment I created the custom resource the operator picked it up it realized",
    "start": "1264030",
    "end": "1269280"
  },
  {
    "text": "that the current faces a new face and it ended up creating the cluster and once",
    "start": "1269280",
    "end": "1275520"
  },
  {
    "text": "the cluster was successfully created what happened was the operated transition the custom resource to a",
    "start": "1275520",
    "end": "1281490"
  },
  {
    "text": "cluster starting state what this means that as I had explained in the cluster",
    "start": "1281490",
    "end": "1286680"
  },
  {
    "text": "starting state we are just waiting for all the pods to come up once all the",
    "start": "1286680",
    "end": "1291930"
  },
  {
    "text": "pods are up the operator then goes to submit the job as you can clearly see",
    "start": "1291930",
    "end": "1298470"
  },
  {
    "text": "here that once all the pods are up for",
    "start": "1298470",
    "end": "1303900"
  },
  {
    "text": "the cluster the operator moves to a submitting job State and it submits the",
    "start": "1303900",
    "end": "1309000"
  },
  {
    "text": "job in the job manager that was just created and from there you can see that",
    "start": "1309000",
    "end": "1314940"
  },
  {
    "text": "once the job was started it moved to a running State the second thing is even",
    "start": "1314940",
    "end": "1320820"
  },
  {
    "text": "though the cluster is currently running the operator is continuously monitoring the existing cluster so let's see if the",
    "start": "1320820",
    "end": "1327360"
  },
  {
    "text": "task manager is down of there is any issues with the with the filling cluster the operator continuously monitors the",
    "start": "1327360",
    "end": "1334260"
  },
  {
    "text": "existing cluster and updates the status field of the flink application resource",
    "start": "1334260",
    "end": "1340140"
  },
  {
    "text": "so let me just show you what I exactly mean",
    "start": "1340140",
    "end": "1345019"
  },
  {
    "text": "so as you can clearly see that for every application that you create we we pose",
    "start": "1353100",
    "end": "1360299"
  },
  {
    "text": "the corresponding filling cluster and notify the status of the job how many check points number of healthy",
    "start": "1360299",
    "end": "1366990"
  },
  {
    "text": "tasks measure and a bunch of things that we monitor from the existing cluster and we store it in the status field of your",
    "start": "1366990",
    "end": "1375029"
  },
  {
    "text": "application which means that if you want to monitor your application you can literally just use read from the status",
    "start": "1375029",
    "end": "1381570"
  },
  {
    "text": "and see what's exactly happening and on top of it we also made matrix for all your failing applications let's see if",
    "start": "1381570",
    "end": "1387029"
  },
  {
    "text": "their application suddenly started failing you we have matrix emitted for all of these which means that you you",
    "start": "1387029",
    "end": "1392129"
  },
  {
    "text": "get alarms on top of matrix automatically now this is one more thing",
    "start": "1392129",
    "end": "1398100"
  },
  {
    "text": "that I wanted to show you and that is what will happen if I go and update my",
    "start": "1398100",
    "end": "1404389"
  },
  {
    "text": "application resource so for that let me just start the",
    "start": "1404389",
    "end": "1409409"
  },
  {
    "text": "threads these are just pulling and the resources the operator is still running so now what I'm doing is I am going and",
    "start": "1409409",
    "end": "1416820"
  },
  {
    "text": "updating the docker image part of it so my Amal so my Amal is exactly the",
    "start": "1416820",
    "end": "1429120"
  },
  {
    "text": "same just that the image is different meaning let's see if you do if you update a code you build a new image and",
    "start": "1429120",
    "end": "1434879"
  },
  {
    "text": "this would ha what happened if you were to do a deployment of your new code",
    "start": "1434879",
    "end": "1440330"
  },
  {
    "text": "yeah let it let it go through let me just really explain what's happening so when I went and updated the existing",
    "start": "1453390",
    "end": "1462060"
  },
  {
    "text": "customer resource the operator found out that the current customer Russell's just not managed does not match the existing",
    "start": "1462060",
    "end": "1468660"
  },
  {
    "text": "cluster so what it does is it brings up a bunch a new cluster in place and waits",
    "start": "1468660",
    "end": "1476070"
  },
  {
    "text": "for a new cluster to come up once the new cluster is up as you can clearly see",
    "start": "1476070",
    "end": "1481380"
  },
  {
    "text": "see that once the new cluster is up it moves from a cluster starting state to a same pointing state where we cancel the",
    "start": "1481380",
    "end": "1489210"
  },
  {
    "text": "job from the existing cluster and try to start the job in the new cluster and as",
    "start": "1489210",
    "end": "1495960"
  },
  {
    "text": "you can clearly see that the same pointing has succeeded we have started the job in the new cluster and once the",
    "start": "1495960",
    "end": "1502380"
  },
  {
    "text": "job is started in the new cluster you can see that the existing cluster is automatically terminated so the first",
    "start": "1502380",
    "end": "1509460"
  },
  {
    "text": "question that was running the job is now terminated because we were able to successfully launch the job in the",
    "start": "1509460",
    "end": "1516330"
  },
  {
    "text": "second cluster so this is an intimate example of how fling kubernetes operator",
    "start": "1516330",
    "end": "1522270"
  },
  {
    "text": "was able to you know take all the updates from your application resource and translate that to a corresponding",
    "start": "1522270",
    "end": "1528840"
  },
  {
    "text": "cluster so what is the impact of all of",
    "start": "1528840",
    "end": "1535950"
  },
  {
    "start": "1530000",
    "end": "1681000"
  },
  {
    "text": "this the the moment we moved from our legacy system to running flink",
    "start": "1535950",
    "end": "1541920"
  },
  {
    "text": "application on kubernetes as you can clearly see we have one fling job for purfling cluster there is no sort of",
    "start": "1541920",
    "end": "1549420"
  },
  {
    "text": "interference all high priority jobs run separately all low priority job run",
    "start": "1549420",
    "end": "1554610"
  },
  {
    "text": "separately scaling the superfast and anytime a pod goes down or goes down",
    "start": "1554610",
    "end": "1560510"
  },
  {
    "text": "kubernetes does a great job of bringing up a new pod pretty soon so we get the",
    "start": "1560510",
    "end": "1566490"
  },
  {
    "text": "advantage of just migrating from an on couple de swert to kubernetes world",
    "start": "1566490",
    "end": "1572120"
  },
  {
    "text": "automatically by just transitioning by just using the fling kubernetes operator",
    "start": "1572120",
    "end": "1577140"
  },
  {
    "text": "the second biggest thing that we noticed is that during a deployment there was no",
    "start": "1577140",
    "end": "1582660"
  },
  {
    "text": "downtime at all meaning that whenever if someone is deploying a bug bug code or a code where you know the",
    "start": "1582660",
    "end": "1589590"
  },
  {
    "text": "permissions were badly configured this never happened and the main recent worst of the auto rollback support any time we",
    "start": "1589590",
    "end": "1596880"
  },
  {
    "text": "were not able to bring up the second cluster or if we were not able to start the job in the second cluster we would",
    "start": "1596880",
    "end": "1603000"
  },
  {
    "text": "automatically roll back to the first cluster and what we saw was there was actually no downtime in many cases but",
    "start": "1603000",
    "end": "1609390"
  },
  {
    "text": "even the downtime in some rare cases was pretty low compared to what we have been seeing in the previous legacy system the",
    "start": "1609390",
    "end": "1617280"
  },
  {
    "text": "third was the configuration became much easier anything that we want to deploy we had to create a corresponding flink",
    "start": "1617280",
    "end": "1624360"
  },
  {
    "text": "application resource so we wrote wrappers on top of our existing flink application resource to set these",
    "start": "1624360",
    "end": "1630150"
  },
  {
    "text": "default values for task manager for the number of task slots for task manager so",
    "start": "1630150",
    "end": "1635490"
  },
  {
    "text": "any user who wants to run a fling job just had to write like three to four lines of code and he would have the",
    "start": "1635490",
    "end": "1641610"
  },
  {
    "text": "flink application running in kubernetes lastly we saw a huge cost saving because of all of this",
    "start": "1641610",
    "end": "1647190"
  },
  {
    "text": "I said mentioned in the previous world we had one fling cluster that was highly",
    "start": "1647190",
    "end": "1653610"
  },
  {
    "text": "provision because we did not know how many jobs you would end up running and we did not want to roll the cluster",
    "start": "1653610",
    "end": "1658970"
  },
  {
    "text": "frequently so we created like these big these big machines or ec2 machines to",
    "start": "1658970",
    "end": "1664290"
  },
  {
    "text": "run these monsters filling clusters but in the New World each fling cluster is just running one",
    "start": "1664290",
    "end": "1670710"
  },
  {
    "text": "job so since you know the job that you're going to run you can provision the resources that you exactly need and",
    "start": "1670710",
    "end": "1677430"
  },
  {
    "text": "this helped us save a lot of cost so what is the current state of the project",
    "start": "1677430",
    "end": "1683880"
  },
  {
    "start": "1681000",
    "end": "1735000"
  },
  {
    "text": "so the all of this is open source any of you guys can just go to github fling",
    "start": "1683880",
    "end": "1689730"
  },
  {
    "text": "kubernetes operator spat the operator in your in your in your for your business or your use cases immediately we call",
    "start": "1689730",
    "end": "1697290"
  },
  {
    "text": "the project status as beta because it helps us do some backward incompatible change in case if something is needed we",
    "start": "1697290",
    "end": "1704370"
  },
  {
    "text": "have had excellent support from the open source community we have like five contributions from outside people outside lyft there there are teams there",
    "start": "1704370",
    "end": "1712500"
  },
  {
    "text": "are companies like trade desk light Bend who are actually using the operator and evaluating the use cases today lastly if",
    "start": "1712500",
    "end": "1719340"
  },
  {
    "text": "you have any questions or if you to use flink abilities operator please join the slack channel",
    "start": "1719340",
    "end": "1724409"
  },
  {
    "text": "this is a public slack channel we have like 40 members they might answer questions or limit you can post any of",
    "start": "1724409",
    "end": "1730679"
  },
  {
    "text": "your doubts or your specific use cases and we would be happy to help lastly my team at flight is giving more",
    "start": "1730679",
    "end": "1739980"
  },
  {
    "start": "1735000",
    "end": "1765000"
  },
  {
    "text": "talks tomorrow there is one tomorrow morning at 10:55 and one at 5:20 in the",
    "start": "1739980",
    "end": "1746789"
  },
  {
    "text": "evening if you are interested in running like large scale machine learning jobs or large-scale batch processing job I",
    "start": "1746789",
    "end": "1753779"
  },
  {
    "text": "would highly recommend you guys to come check it out also flight is also open sourced we open-source flight a month",
    "start": "1753779",
    "end": "1759779"
  },
  {
    "text": "back so it's also something that you can immediately adopt and start using",
    "start": "1759779",
    "end": "1764809"
  },
  {
    "text": "there's also happy hour today evening if you guys are interested and want to talk",
    "start": "1764809",
    "end": "1769980"
  },
  {
    "start": "1765000",
    "end": "1800000"
  },
  {
    "text": "to other engineers like lyft let's meet at thorne barrio Logan I think it's a",
    "start": "1769980",
    "end": "1775830"
  },
  {
    "text": "beer spot but yeah why not thank you so much happy to answer questions yes any",
    "start": "1775830",
    "end": "1794460"
  },
  {
    "text": "questions feel free to hey guys just one",
    "start": "1794460",
    "end": "1801090"
  },
  {
    "start": "1800000",
    "end": "1842000"
  },
  {
    "text": "thing we have flight stickers right here so if you guys want flight stickers feel",
    "start": "1801090",
    "end": "1806759"
  },
  {
    "text": "free to come pick it up thank you so much yeah go ahead so you talked about the the ingress that",
    "start": "1806759",
    "end": "1812429"
  },
  {
    "text": "is generic per cluster so how do you avoid conflict between the universe's sorry how do you avoid the conflict",
    "start": "1812429",
    "end": "1819690"
  },
  {
    "text": "between the ingress is between the different clusters so each fling cluster",
    "start": "1819690",
    "end": "1826019"
  },
  {
    "text": "will have a separate service and a separate ingress and the way we do",
    "start": "1826019",
    "end": "1832019"
  },
  {
    "text": "ingress is by a host path so let me at least show you a demo that's the such a",
    "start": "1832019",
    "end": "1838230"
  },
  {
    "text": "great question was part of my demo but I missed it so this is a fling application",
    "start": "1838230",
    "end": "1845429"
  },
  {
    "text": "that is currently running in order for kubernetes cluster and you can actually see the top part of the URL so",
    "start": "1845429",
    "end": "1853600"
  },
  {
    "text": "what we have done it is when we create and ingress we specify the application",
    "start": "1853600",
    "end": "1858940"
  },
  {
    "text": "name in the host path so which means that each host path will no point of corresponding ingress which will then",
    "start": "1858940",
    "end": "1865960"
  },
  {
    "text": "route the request to the corresponding job manager so that's what we have done and also its considerable so what we",
    "start": "1865960",
    "end": "1871600"
  },
  {
    "text": "have done it is if you want to use an operator link of kubernetes operatory you can specify your own custom ingress",
    "start": "1871600",
    "end": "1877720"
  },
  {
    "text": "path for you to run in your kubernetes world no it's a part of the config map",
    "start": "1877720",
    "end": "1887830"
  },
  {
    "text": "so essentially what we do is you specify like the rejects of how you want to",
    "start": "1887830",
    "end": "1893590"
  },
  {
    "text": "create the ingress URL and viewed while creating the ingress automatically said",
    "start": "1893590",
    "end": "1898900"
  },
  {
    "text": "that particular path in ingress thank you my questions",
    "start": "1898900",
    "end": "1905399"
  },
  {
    "start": "1908000",
    "end": "1955000"
  },
  {
    "text": "sorry what is that your save point in your safe you mentioned saving state or",
    "start": "1908549",
    "end": "1914070"
  },
  {
    "text": "yes what are you saving in where are you saving it so that's a great question so",
    "start": "1914070",
    "end": "1919429"
  },
  {
    "text": "inflamed what happens is you have a fling cluster running there's like a bunch of checkpoints each task manager",
    "start": "1919429",
    "end": "1925529"
  },
  {
    "text": "creates a one checkpoint of the of the entire graph so what happens is that when we want to change code from one",
    "start": "1925529",
    "end": "1931289"
  },
  {
    "text": "cluster to other the very transition is by creating this safe points a point is",
    "start": "1931289",
    "end": "1937229"
  },
  {
    "text": "a concept in fling we store all our save points in s3 and the path of the save",
    "start": "1937229",
    "end": "1943349"
  },
  {
    "text": "point is stored in the flink custom resource so for example",
    "start": "1943349",
    "end": "1949129"
  },
  {
    "start": "1955000",
    "end": "2000000"
  },
  {
    "text": "so you can actually see that okay we should have done it during the yeah",
    "start": "1955120",
    "end": "1960920"
  },
  {
    "text": "right here so if you look at the restored path so for my example I am using a local store but what we do is we",
    "start": "1960920",
    "end": "1967550"
  },
  {
    "text": "store all the save points in s3 and the path of the s3 would be available here",
    "start": "1967550",
    "end": "1972590"
  },
  {
    "text": "which means that when we start the new filling cluster we say hey start the new fling cluster from this particular save",
    "start": "1972590",
    "end": "1979760"
  },
  {
    "text": "point more questions",
    "start": "1979760",
    "end": "1985960"
  },
  {
    "text": "where is that",
    "start": "1993130",
    "end": "1996309"
  },
  {
    "start": "2000000",
    "end": "2030000"
  },
  {
    "text": "that she's asking if he was cute builder or have used some other framework yeah so we started off with using operator",
    "start": "2000460",
    "end": "2007460"
  },
  {
    "text": "SDK to bring build the fling kubernetes operator but we wanted to do some lower",
    "start": "2007460",
    "end": "2012950"
  },
  {
    "text": "level things as well so we ended up directly using controller runtime so controller runtime is also open source",
    "start": "2012950",
    "end": "2020119"
  },
  {
    "text": "is something that we used directly for building and running google dis operator",
    "start": "2020119",
    "end": "2025489"
  },
  {
    "text": "what I mean is yep this is what I mean",
    "start": "2025489",
    "end": "2030759"
  },
  {
    "text": "so we use this particular package the coolest controller runtime to build",
    "start": "2030759",
    "end": "2036470"
  },
  {
    "text": "Plinko business operator we use the operator SDK before we move to this but since you want to support like multi",
    "start": "2036470",
    "end": "2042169"
  },
  {
    "text": "namespace we want the operator to read only from specific namespaces so for building like these sort of like complex",
    "start": "2042169",
    "end": "2048378"
  },
  {
    "text": "use cases we had to use controller runtime directly",
    "start": "2048379",
    "end": "2053048"
  },
  {
    "start": "2058000",
    "end": "2076000"
  },
  {
    "text": "thank you hi um since I using this operator it seems like each job will",
    "start": "2058000",
    "end": "2064730"
  },
  {
    "text": "have its own clusters so do you have like a plan to provide a centralized",
    "start": "2064730",
    "end": "2069879"
  },
  {
    "text": "user interface for all the jobs when they are running in independent blank",
    "start": "2069880",
    "end": "2076280"
  },
  {
    "start": "2076000",
    "end": "2129000"
  },
  {
    "text": "clusters oh that's a very good question as a part of this project it's not but",
    "start": "2076280",
    "end": "2083750"
  },
  {
    "text": "that is something that we have been talking for a while since that now you have all the jobs running separately you",
    "start": "2083750",
    "end": "2090530"
  },
  {
    "text": "want a UI which would show the aggregate of all the jobs not directly but one",
    "start": "2090530",
    "end": "2098390"
  },
  {
    "text": "thing that we are trying to do solve that is using integrations with flight so what we will be so think of it this",
    "start": "2098390",
    "end": "2105560"
  },
  {
    "text": "way apart from flink we also run spark and sparkles it is similarly so you want to",
    "start": "2105560",
    "end": "2110660"
  },
  {
    "text": "just have an overview of not just fling just but spark jobs or any other batch",
    "start": "2110660",
    "end": "2116030"
  },
  {
    "text": "processing jobs so that's running in the cluster so as a part of this work it's not but we are trying to solve some of",
    "start": "2116030",
    "end": "2123830"
  },
  {
    "text": "that using our integration with flight another part of it so yeah yeah I have",
    "start": "2123830",
    "end": "2129890"
  },
  {
    "start": "2129000",
    "end": "2232000"
  },
  {
    "text": "another question yes so if one job will use wine fling cluster on it might be",
    "start": "2129890",
    "end": "2136610"
  },
  {
    "text": "expensive so what is the overhead for running the main kind of service for a",
    "start": "2136610",
    "end": "2143030"
  },
  {
    "text": "flank that's a very good question so if you actually think of it the overhead is",
    "start": "2143030",
    "end": "2148280"
  },
  {
    "text": "actually loads just what I mentioned from the costume that we had noticed so if you were to run let's say if you were",
    "start": "2148280",
    "end": "2155540"
  },
  {
    "text": "to run two jobs in a single fling cluster you now have to see that both",
    "start": "2155540",
    "end": "2162140"
  },
  {
    "text": "the jobs the memory of both the jobs has to be taken into consideration which means that if let's say one jobs high",
    "start": "2162140",
    "end": "2167810"
  },
  {
    "text": "memory low CPU and the other guys like high CPU low memory you will have to like do some sort of math to make sure",
    "start": "2167810",
    "end": "2174650"
  },
  {
    "text": "that both of them are running reliably but if you were to split it up into separate jobs you can just tune the",
    "start": "2174650",
    "end": "2182090"
  },
  {
    "text": "exact amount of resources that you would end up creating the only overhead on top of that is that we are running multiple",
    "start": "2182090",
    "end": "2189080"
  },
  {
    "text": "job managers so for example each instead of having one job manager I'd be running two job",
    "start": "2189080",
    "end": "2194630"
  },
  {
    "text": "managers purfling cluster so it's like times two but if you actually see that the amount of CPU and memory for a job",
    "start": "2194630",
    "end": "2201050"
  },
  {
    "text": "manager is actually not that high so we run it with like 6 CPUs and like 450 GB",
    "start": "2201050",
    "end": "2207380"
  },
  {
    "text": "memory so if you actually do the math at a scale for the reliability benefit that we get it's like hundreds of thousands",
    "start": "2207380",
    "end": "2214250"
  },
  {
    "text": "of dollars a year which is not that much if you were pretty much out of time",
    "start": "2214250",
    "end": "2221360"
  },
  {
    "text": "I think there's one more question do you want to just come up I think yeah so",
    "start": "2221360",
    "end": "2233090"
  },
  {
    "text": "when you were talking about your like like it's the infrastructure you had like your auto scaling capabilities took",
    "start": "2233090",
    "end": "2240020"
  },
  {
    "text": "a long time when you switched over to this kubernetes based deployment model",
    "start": "2240020",
    "end": "2245510"
  },
  {
    "text": "did you allow for like horizontal node scaling or did you say okay now if",
    "start": "2245510",
    "end": "2251300"
  },
  {
    "text": "you're starting to you know get to limited your clusters capacity you add a",
    "start": "2251300",
    "end": "2256850"
  },
  {
    "text": "node or like what how did you guys deal with that and then if it was automated like how did that reduce your note",
    "start": "2256850",
    "end": "2263930"
  },
  {
    "text": "scaling time with that that's a very good question so currently there is no",
    "start": "2263930",
    "end": "2271330"
  },
  {
    "start": "2265000",
    "end": "2380000"
  },
  {
    "text": "trigger based auto scaling in plink so what I mean by that is let's say that you know you have a rough filling",
    "start": "2271330",
    "end": "2277280"
  },
  {
    "text": "cluster that is currently running and suddenly say at five pm that traffic is higher and you see that this is like a",
    "start": "2277280",
    "end": "2284240"
  },
  {
    "text": "curve of increase in CP and memory we don't automatically have like triggers to say that hey when the CPU is",
    "start": "2284240",
    "end": "2291980"
  },
  {
    "text": "more than 70% automatically add new task managers so we we do it at lyft is that",
    "start": "2291980",
    "end": "2297980"
  },
  {
    "text": "by updating the parallelism of the customer resource so if you actually",
    "start": "2297980",
    "end": "2303500"
  },
  {
    "text": "look at the spec right we have a field called as parallelism that is the spot",
    "start": "2303500",
    "end": "2309800"
  },
  {
    "text": "that needs to be updated so today what we have been doing is we have a script where in ignore we update the",
    "start": "2309800",
    "end": "2315920"
  },
  {
    "text": "parallelism so essentially what parallelism does is based on the parallelism and the number",
    "start": "2315920",
    "end": "2321650"
  },
  {
    "text": "of tasks slot manager decides the number of task managers so by just updating the",
    "start": "2321650",
    "end": "2328219"
  },
  {
    "text": "parallelism we go through this path of you know adding more pods the difference is that the previous Willie had to add",
    "start": "2328219",
    "end": "2333799"
  },
  {
    "text": "machines now we just have to add new purchases which was the huge improvement but to answer the question trigger based",
    "start": "2333799",
    "end": "2341239"
  },
  {
    "text": "auto scaling is what we are planning to build in the operator itself so that you",
    "start": "2341239",
    "end": "2346999"
  },
  {
    "text": "know you can have certain triggers there which would make the fling cluster scale",
    "start": "2346999",
    "end": "2352099"
  },
  {
    "text": "automatically but I believe flink is also doing work in this regard to",
    "start": "2352099",
    "end": "2358190"
  },
  {
    "text": "automatically scale horizontally but it's also in the roadmap of filling kubernetes operator yeah and I have one",
    "start": "2358190",
    "end": "2364609"
  },
  {
    "text": "more question in fact at the cost discussion so in terms of like the ratio of cost to savings that you have did you",
    "start": "2364609",
    "end": "2371119"
  },
  {
    "text": "go from you know 100 to like 50 percent of your overall initial cost or what",
    "start": "2371119",
    "end": "2376449"
  },
  {
    "text": "what kind of proportional so that's very good question so the cost let me state",
    "start": "2376449",
    "end": "2386479"
  },
  {
    "text": "this so we have like more than 100 applications and that we are running in the non-communist world we have migrated",
    "start": "2386479",
    "end": "2393650"
  },
  {
    "text": "roughly like 30 percent mainly the ones that are roll loader labs for example drift runs 40 flink jobs in a single",
    "start": "2393650",
    "end": "2401749"
  },
  {
    "text": "single single fling cluster and it will go down always like every two weeks so when we migrated 40 percent we saw",
    "start": "2401749",
    "end": "2408880"
  },
  {
    "text": "roughly 20 to 30 percent reduction in cost but again this number depends on",
    "start": "2408880",
    "end": "2415999"
  },
  {
    "text": "how well you tuned the job manager in the task whereas a conflict in our case what happened will - it just came - like",
    "start": "2415999",
    "end": "2421999"
  },
  {
    "text": "five different teams and they did a good job of setting the right values but if you end up setting like extremely high",
    "start": "2421999",
    "end": "2429650"
  },
  {
    "text": "job manager what task manager resources you might not end up seeing the Cosman",
    "start": "2429650",
    "end": "2434839"
  },
  {
    "text": "fit but it comes down to how you accurately you set the resource for your",
    "start": "2434839",
    "end": "2440269"
  },
  {
    "text": "job yeah that goes back to kind of a higher level view of across your jobs in like",
    "start": "2440269",
    "end": "2446239"
  },
  {
    "text": "what's your utilization right like actually measuring your utilization to manage the cost oh yeah yeah that's true",
    "start": "2446239",
    "end": "2452799"
  },
  {
    "start": "2452000",
    "end": "2493000"
  },
  {
    "text": "actually one more thing on the cost side our sentiment there is we have an another way of doing",
    "start": "2452799",
    "end": "2458859"
  },
  {
    "text": "cost the way we do it is we have is something we have a service called as infraspinatus another coupe con talk on",
    "start": "2458859",
    "end": "2465080"
  },
  {
    "text": "it as well that guy would take metrics from all the pods and you know drill",
    "start": "2465080",
    "end": "2473270"
  },
  {
    "text": "down by namespace and give us a view for the cost it doesn't address the fact that you know it can't tell you how many",
    "start": "2473270",
    "end": "2480410"
  },
  {
    "text": "flink jobs are running in the cluster but it will definitely tell you the cost per namespace so that's one way for us",
    "start": "2480410",
    "end": "2486619"
  },
  {
    "text": "to look at the cost awesome thank you guys [Applause]",
    "start": "2486619",
    "end": "2495360"
  }
]