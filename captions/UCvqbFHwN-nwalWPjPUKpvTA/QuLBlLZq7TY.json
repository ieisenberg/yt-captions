[
  {
    "text": "okay welcome back to the waterfall room and to our next session today so we've",
    "start": "1000",
    "end": "6799"
  },
  {
    "text": "got modernizing new Banks Kafka platform for the next 10 years so I'm joined here",
    "start": "6799",
    "end": "12840"
  },
  {
    "text": "by Julio Tera and Ronnie Silva who going to be giving us this session and",
    "start": "12840",
    "end": "19160"
  },
  {
    "text": "throughout the session if you have any questions do pop them in the Q&A in Bevy and then at the end of the session the",
    "start": "19160",
    "end": "25400"
  },
  {
    "text": "speakers will answer over in the Q&A there as well so with that I'll hand",
    "start": "25400",
    "end": "31400"
  },
  {
    "text": "over to you thanks Kate good morning uh or afternoon everyone thanks for joining",
    "start": "31400",
    "end": "37320"
  },
  {
    "text": "in uh I'm Julia ter I'm a software engineer at newbank mostly focused on SRE I'm currently in s Paulo",
    "start": "37320",
    "end": "46718"
  },
  {
    "text": "Brazil hi everyone my name is Ronnie Silva I'm also a soft engineer inki I've",
    "start": "47320",
    "end": "52600"
  },
  {
    "text": "been working with his and C the current C infrastructure for the last",
    "start": "52600",
    "end": "58160"
  },
  {
    "text": "year um today we're going to walk through the evolution of c new bank how",
    "start": "58160",
    "end": "63399"
  },
  {
    "text": "it works from 2014 to today and how try was selected against multiple other",
    "start": "63399",
    "end": "68799"
  },
  {
    "text": "options for the new cka platform at new bank wel could to the the Str infrastructure how relieves most of our",
    "start": "68799",
    "end": "76439"
  },
  {
    "text": "operational burden and how we are rolling it out to an infrastructure with",
    "start": "76439",
    "end": "82720"
  },
  {
    "text": "tens of thousands of microservices with no downtime in the end we are going to",
    "start": "82720",
    "end": "88240"
  },
  {
    "text": "discuss the results and the future of massaging at newbank so 10 years of kfka",
    "start": "88240",
    "end": "94200"
  },
  {
    "text": "at newbank uh for those who don't know which usually is most most of people",
    "start": "94200",
    "end": "99640"
  },
  {
    "text": "outside Latin America uh new bank is uh one of the biggest fxs in the world we",
    "start": "99640",
    "end": "106320"
  },
  {
    "text": "have over 100 million customers uh and uh we started with credit cards now we",
    "start": "106320",
    "end": "111920"
  },
  {
    "text": "have checking accounts Investments Insurance landing and we started in",
    "start": "111920",
    "end": "118560"
  },
  {
    "text": "2013 t was deployed in 2013 as the backbone of interservice communication",
    "start": "118560",
    "end": "124640"
  },
  {
    "text": "back then we had like one cluster maybe one for staging and uh it was kind of a niche",
    "start": "124640",
    "end": "130879"
  },
  {
    "text": "software back then and it seems it turned out it was a good choice for",
    "start": "130879",
    "end": "136680"
  },
  {
    "text": "us uh the company grew a lot so when I joined we had about 1.6 million",
    "start": "136680",
    "end": "142200"
  },
  {
    "text": "customers and we've been growing steadily and sometimes getting",
    "start": "142200",
    "end": "147800"
  },
  {
    "text": "faster uh from in the in the last six six seven",
    "start": "147800",
    "end": "153280"
  },
  {
    "text": "years so in around 2019 we also went to",
    "start": "153280",
    "end": "158440"
  },
  {
    "text": "Mexico uh we in 2020 21 we started Colombia operations",
    "start": "158440",
    "end": "165440"
  },
  {
    "text": "and back then we had about 20 kfka clusters and we kept growing and right",
    "start": "165440",
    "end": "171360"
  },
  {
    "text": "now we are at 100 million customers back in 2020 the Brazilian",
    "start": "171360",
    "end": "177239"
  },
  {
    "text": "Central Bank created a new instant transfer modality that's called PX and",
    "start": "177239",
    "end": "184319"
  },
  {
    "text": "uh pigs became very popular basically every month we have about five billion",
    "start": "184319",
    "end": "191200"
  },
  {
    "text": "pix transactions pix transactions are realtime transactions between financial",
    "start": "191200",
    "end": "196239"
  },
  {
    "text": "institutions that you can transfer money to everyone or people persons or businesses across uh the Brazilian uh",
    "start": "196239",
    "end": "203599"
  },
  {
    "text": "Financial system and new bank is a participant in from 30 to 40% % of these",
    "start": "203599",
    "end": "210400"
  },
  {
    "text": "transactions as a receiving or sending institution and this means that 40% 30",
    "start": "210400",
    "end": "219760"
  },
  {
    "text": "to 40% of the Brazilian Financial system goes through the cfus that uh we manage",
    "start": "219760",
    "end": "228920"
  },
  {
    "text": "we have an inn latency of 7 Seconds to to answer for a fix transaction and this",
    "start": "228920",
    "end": "236239"
  },
  {
    "text": "latency has we we we share budgets with team responsible for frauds for ledgers",
    "start": "236239",
    "end": "242959"
  },
  {
    "text": "for transactions Etc uh you so those budgets is met so we keep a persistent",
    "start": "242959",
    "end": "250200"
  },
  {
    "text": "transaction to the central bank and in HP and the new bank border servers send",
    "start": "250200",
    "end": "255959"
  },
  {
    "text": "messages through Kafka which send messages to dozens of services to answer if the transaction can be settled or",
    "start": "255959",
    "end": "263680"
  },
  {
    "text": "not that's only Brazil we also have Colombia and Mexico with their own",
    "start": "263680",
    "end": "269360"
  },
  {
    "text": "specific ities and their old systems um our infrastructure handles",
    "start": "269360",
    "end": "277440"
  },
  {
    "text": "currently handles around 250 billion messages per week and as you can see in",
    "start": "277440",
    "end": "283759"
  },
  {
    "text": "this graph uh we uh had a 2X growth over",
    "start": "283759",
    "end": "288880"
  },
  {
    "text": "the last year and this growth U continues and our infrastructure has to",
    "start": "288880",
    "end": "294919"
  },
  {
    "text": "be prepared to keep up with this demand wegan of course we do not have a",
    "start": "294919",
    "end": "302080"
  },
  {
    "text": "single C cluster for that right uh we organize new bank in an organizational",
    "start": "302080",
    "end": "307919"
  },
  {
    "text": "environment and what we call shards uh structure in the organizational level we",
    "start": "307919",
    "end": "313199"
  },
  {
    "text": "have the countries and some internal organizations like data and internal shared tools and uh for each",
    "start": "313199",
    "end": "319400"
  },
  {
    "text": "organization we have environments like production staging and for each environments usually just proection we",
    "start": "319400",
    "end": "325520"
  },
  {
    "text": "have water CL chards which are copies of new bank infrastructure that uh handle a few million customers",
    "start": "325520",
    "end": "333919"
  },
  {
    "text": "and whenever we have to scale up the the new bank system we create",
    "start": "333919",
    "end": "341080"
  },
  {
    "text": "charts and how we got here um basically we got here with automation that created",
    "start": "341759",
    "end": "347800"
  },
  {
    "text": "cloudformation Stacks that created all scaling groups that manage that had user data",
    "start": "347800",
    "end": "355520"
  },
  {
    "text": "system D Etc that ran Docker that ran C",
    "start": "355520",
    "end": "361440"
  },
  {
    "text": "what's the problem of this with this set well many right but uh cloud",
    "start": "361440",
    "end": "369039"
  },
  {
    "text": "formation is nothing about CU so whenever you change a cuff conversion",
    "start": "369039",
    "end": "374479"
  },
  {
    "text": "there is no safe way to apply the C conversion down there whenever we scale up there is no save of scale scale up",
    "start": "374479",
    "end": "382599"
  },
  {
    "text": "the system is not aware of the application that's running and when a blast terminates an",
    "start": "382599",
    "end": "388520"
  },
  {
    "text": "instance well it's basically a KT keep the lights on uh effort to to make sure",
    "start": "388520",
    "end": "396360"
  },
  {
    "text": "that things are running smoothly and sometimes we have to jump in and fix",
    "start": "396360",
    "end": "401759"
  },
  {
    "text": "stuff by hand but this has a reason we when we had just a few copies",
    "start": "401759",
    "end": "408479"
  },
  {
    "text": "of newbang few shards a few clusters of Kafka we used to do something that we",
    "start": "408479",
    "end": "415680"
  },
  {
    "text": "called a immutable infrastructure we just created a new copy of newbank we",
    "start": "415680",
    "end": "422280"
  },
  {
    "text": "created a new copy with services with cfas with zookeepers Etc and then we",
    "start": "422280",
    "end": "428599"
  },
  {
    "text": "took the DNS and rolled it out to the to the new copy that provided us an opportunity to upgrade",
    "start": "428599",
    "end": "435440"
  },
  {
    "text": "infrastructure and uh scale to whatever we needed scale down sometimes and uh",
    "start": "435440",
    "end": "442280"
  },
  {
    "text": "that worked fine then we deleted the previous copy and it was upgraded",
    "start": "442280",
    "end": "449680"
  },
  {
    "text": "but when we got to 5050 copies of newbank um 50 c clusters thousand of of",
    "start": "449680",
    "end": "456879"
  },
  {
    "text": "ECU instances it wasn't so easy for us to just deploy everything because then",
    "start": "456879",
    "end": "464000"
  },
  {
    "text": "we would have a twice the amount on our AWS Bill and it would be very time",
    "start": "464000",
    "end": "472479"
  },
  {
    "text": "consuming so this couldn't we couldn't keep up",
    "start": "472479",
    "end": "477720"
  },
  {
    "text": "with this form of making upgrades not only that but the the fact",
    "start": "477720",
    "end": "484080"
  },
  {
    "text": "that we had a single shared cka cluster for shard meant that different workloads",
    "start": "484080",
    "end": "493319"
  },
  {
    "text": "shared the same resources creating a Noisy Neighbor",
    "start": "493319",
    "end": "498440"
  },
  {
    "text": "problem so the PX transactions are small and require low latency we don't have we",
    "start": "498440",
    "end": "504879"
  },
  {
    "text": "don't need like 10 megabytes per message for that and even to it's another",
    "start": "504879",
    "end": "510960"
  },
  {
    "text": "internal topic um has huge messages with a lot of information click string data",
    "start": "510960",
    "end": "518640"
  },
  {
    "text": "and um we couldn't uh we when we share the same Brokers with for for diff this",
    "start": "518640",
    "end": "526959"
  },
  {
    "text": "very different workloads we uh ended up with higher amplitudes of",
    "start": "526959",
    "end": "533959"
  },
  {
    "text": "latencies so we set out to define the future of c new bank",
    "start": "533959",
    "end": "540959"
  },
  {
    "text": "and we have some options we could use self hosted C like",
    "start": "540959",
    "end": "547440"
  },
  {
    "text": "we did we could use some public Cloud offerings there are some companies",
    "start": "547440",
    "end": "552880"
  },
  {
    "text": "dedicated to Kafka hosting they companies that created Kafka Kafka like",
    "start": "552880",
    "end": "558360"
  },
  {
    "text": "Alternatives compatibles or IBM mq of course we chose IB mmq and that's",
    "start": "558360",
    "end": "565720"
  },
  {
    "text": "the end no so with IB mmq crossed out we started",
    "start": "565720",
    "end": "575560"
  },
  {
    "text": "to to think about the requirements the bank is a regulated",
    "start": "575560",
    "end": "580800"
  },
  {
    "text": "entity so this means that we have constraints that are not defined by us but by central banks for",
    "start": "580800",
    "end": "588160"
  },
  {
    "text": "example and uh there are a lot of rules on where our data can reside and there",
    "start": "588160",
    "end": "594720"
  },
  {
    "text": "are some Disaster Recovery plan tasks that we have to execute every year in uh",
    "start": "594720",
    "end": "601440"
  },
  {
    "text": "so we can prove to Regulators that our infrastructure is very reliable so when",
    "start": "601440",
    "end": "606800"
  },
  {
    "text": "we have this we really uh decided that new Bank's customer data should live in",
    "start": "606800",
    "end": "614160"
  },
  {
    "text": "a cloud provider account that is Sound by newbank and that also has uh other",
    "start": "614160",
    "end": "619920"
  },
  {
    "text": "benefits newbank has a strong IM uh system with roles authorization for",
    "start": "619920",
    "end": "625880"
  },
  {
    "text": "persons and for systems uh we have some infos intral detections uh we have",
    "start": "625880",
    "end": "632640"
  },
  {
    "text": "logging Matrix AIT Trail and these are",
    "start": "632640",
    "end": "637800"
  },
  {
    "text": "all integrated with a broader stack that allows us to create reports to identify",
    "start": "637800",
    "end": "644200"
  },
  {
    "text": "who is using each uh AWS IM action and uh building all of that from scratch",
    "start": "644200",
    "end": "652279"
  },
  {
    "text": "with another provider is a challenge so we thought okay let's go to",
    "start": "652279",
    "end": "658120"
  },
  {
    "text": "bring our own cloud uh but again bring your own cloud",
    "start": "658120",
    "end": "663279"
  },
  {
    "text": "require you to install demon that keeps a persistent connection to a third party and this demon has root",
    "start": "663279",
    "end": "670440"
  },
  {
    "text": "like access uh the third party Engineers can move change your infrastructure and",
    "start": "670440",
    "end": "677959"
  },
  {
    "text": "run updates uh and things that are not very nice like I am creates rolls in the in",
    "start": "677959",
    "end": "686839"
  },
  {
    "text": "the in the demon so we thought okay let's go for cloud offering in the cloud",
    "start": "686839",
    "end": "694519"
  },
  {
    "text": "offering it seems that you scale a cluster and then your promises that it",
    "start": "694519",
    "end": "699959"
  },
  {
    "text": "will take from six to over 24 hours cool down periods depending on the size of the cluster and turns out the Clusters",
    "start": "699959",
    "end": "707120"
  },
  {
    "text": "are big so uh and you couldn't update the Clusters again until it's finished",
    "start": "707120",
    "end": "713480"
  },
  {
    "text": "it and we have this internal and adal evidence uh somebody just because",
    "start": "713480",
    "end": "720000"
  },
  {
    "text": "outscale triggered but the time for outscale cool down period to finish was so much that it wouldn't keep up with",
    "start": "720000",
    "end": "726200"
  },
  {
    "text": "the growth the new size wouldn't be enough and we couldn't do anything uh so we were not super",
    "start": "726200",
    "end": "732720"
  },
  {
    "text": "comfortable with the lack of flexibility that this solution would give",
    "start": "732720",
    "end": "737959"
  },
  {
    "text": "us so we really choose selfhosted selfhosted allows us to M meet our",
    "start": "737959",
    "end": "746600"
  },
  {
    "text": "security requirements it reduces thir party control and we have flexibility in",
    "start": "746600",
    "end": "751839"
  },
  {
    "text": "config storage instances networking one thing about bring around Cloud was that",
    "start": "751839",
    "end": "758839"
  },
  {
    "text": "um all the infrastructure created uh we wouldn't have like the IDS for the vpcs",
    "start": "758839",
    "end": "766680"
  },
  {
    "text": "ETC and we would have to go to data Blas account fetch all this data to then",
    "start": "766680",
    "end": "771839"
  },
  {
    "text": "create Integrations with our system for example uh creating the demon for ring",
    "start": "771839",
    "end": "777920"
  },
  {
    "text": "on cloud would create a VPC per cluster then we would have to get the VPC get",
    "start": "777920",
    "end": "783360"
  },
  {
    "text": "the CER wrench and create a VPC peering to our infastructure which would not be",
    "start": "783360",
    "end": "789839"
  },
  {
    "text": "super nice and the self host it has all the Integrations to the bank platform that I",
    "start": "789839",
    "end": "796600"
  },
  {
    "text": "mentioned and it was obvious to us that kubernets would be a good uh buing block",
    "start": "796600",
    "end": "801639"
  },
  {
    "text": "for that because well kubernets has a lot of automated mechanisms that make",
    "start": "801639",
    "end": "806959"
  },
  {
    "text": "our life easier right uh just mentioned a few uh deploying uh operators that",
    "start": "806959",
    "end": "813800"
  },
  {
    "text": "manage uh load balancers deploying operators that manage",
    "start": "813800",
    "end": "818920"
  },
  {
    "text": "storage uh managing DNS entries all of this can be automated that it's a good",
    "start": "818920",
    "end": "824800"
  },
  {
    "text": "framework for that and also nank has a solid kubernets",
    "start": "824800",
    "end": "831720"
  },
  {
    "text": "um Team um we have many uh I have a lot of clusters we are",
    "start": "831720",
    "end": "839320"
  },
  {
    "text": "very uh good at upgrading them so we could just use the infrastructure that",
    "start": "839320",
    "end": "845920"
  },
  {
    "text": "our teams are super reliable on and build on top of them so we ended up with",
    "start": "845920",
    "end": "854759"
  },
  {
    "text": "streamy and a competitor operator and when we compared both we",
    "start": "854759",
    "end": "860160"
  },
  {
    "text": "realized uh they were very different in materi basically Str had solid CR",
    "start": "860160",
    "end": "865720"
  },
  {
    "text": "schemas crg schemas uh but the comparator had documentation mixing different crg versions We had a full J",
    "start": "865720",
    "end": "873440"
  },
  {
    "text": "body and EVS support and the competitor required feral instance",
    "start": "873440",
    "end": "879399"
  },
  {
    "text": "storage Str automates most failure scenarios um and we couldn't have",
    "start": "879399",
    "end": "885680"
  },
  {
    "text": "recovery for instance laws in the competitor and well that was mostly because there was no reattachment of EBS",
    "start": "885680",
    "end": "893199"
  },
  {
    "text": "discs so you had to rebalance the data and that was not super nice and has a solid community and uh",
    "start": "893199",
    "end": "903959"
  },
  {
    "text": "cncf we would be relying on a small team and also uh stream has consistent",
    "start": "903959",
    "end": "909800"
  },
  {
    "text": "release cycles and we what we noted was that the the competitor had breaking",
    "start": "909800",
    "end": "916240"
  },
  {
    "text": "changes in the last year which makes the lives of everyone managing softare very",
    "start": "916240",
    "end": "922560"
  },
  {
    "text": "hard so honey is going to jump in for uh some some information about the massach",
    "start": "922560",
    "end": "929240"
  },
  {
    "text": "platform go ahead so let's walk through the decisions we made so far to build",
    "start": "929240",
    "end": "935519"
  },
  {
    "text": "the mintion platform uh one of the first decision was to uh get the Kafka",
    "start": "935519",
    "end": "942440"
  },
  {
    "text": "infrastructure that was deployed in one account to split that in many accounts that's in part context so we have one",
    "start": "942440",
    "end": "949959"
  },
  {
    "text": "account for Aran and Country and also that's split by by the environment next",
    "start": "949959",
    "end": "955160"
  },
  {
    "text": "slide please and for sure because of that we had to",
    "start": "955160",
    "end": "962120"
  },
  {
    "text": "to deal with some decisions like using VPC pering to connecting some of our",
    "start": "962120",
    "end": "968600"
  },
  {
    "text": "workloads uh we decided to use VPC pering for connection that high load and sensitive to Laten but the connection",
    "start": "968600",
    "end": "974959"
  },
  {
    "text": "that not depend so it's not so safety sensitive to Laten with use it Transit Gateway next slide",
    "start": "974959",
    "end": "983160"
  },
  {
    "text": "please and another decision we had to do is since we are adding a new component",
    "start": "985120",
    "end": "991000"
  },
  {
    "text": "in the infrastructure that's the AKs clusters we have to decide how to distribute the Kafka between those AKs",
    "start": "991000",
    "end": "999560"
  },
  {
    "text": "clusters and we thought that would be too much to have one AKs for all the",
    "start": "999560",
    "end": "1005519"
  },
  {
    "text": "Kafka cluster because that increased a lot our blast radius that was not was uh",
    "start": "1005519",
    "end": "1011600"
  },
  {
    "text": "good at the the pass infrastructure but having WS for each Kafka would be too much uh professional bur from the team",
    "start": "1011600",
    "end": "1020519"
  },
  {
    "text": "so we came up with a solution that's failer domains and failer domains is a AKs classes that we can deploy some cfas",
    "start": "1020519",
    "end": "1028839"
  },
  {
    "text": "on then and it's pren depending on the",
    "start": "1028839",
    "end": "1034079"
  },
  {
    "text": "the criticality of the CFA that's running inks um next slide",
    "start": "1034079",
    "end": "1041319"
  },
  {
    "text": "please and as you can as we thought uh",
    "start": "1041760",
    "end": "1047280"
  },
  {
    "text": "as we mentioned before we have some dimensions in in new bank that's the",
    "start": "1047280",
    "end": "1052840"
  },
  {
    "text": "organization the environment also the chart and in the past we had only uh one",
    "start": "1052840",
    "end": "1058919"
  },
  {
    "text": "Kafka cluster perad but uh as a a evolution of our infrastructure we",
    "start": "1058919",
    "end": "1065840"
  },
  {
    "text": "would like to have more CFA cluster for specific postol in inside the each chart",
    "start": "1065840",
    "end": "1072600"
  },
  {
    "text": "so we came up with the solution to create a new dimension that we are calling Kafka flavor and that we hand",
    "start": "1072600",
    "end": "1079400"
  },
  {
    "text": "the message for specific loads inside new bank uh for example the PX one a px",
    "start": "1079400",
    "end": "1086240"
  },
  {
    "text": "CFA to handle the P flow a valun t the the data streaming uh flow and so forth",
    "start": "1086240",
    "end": "1093080"
  },
  {
    "text": "next please and on top of it as we uh added",
    "start": "1093080",
    "end": "1101400"
  },
  {
    "text": "many automation so on we were able to add some gr HS to protect since we have",
    "start": "1101400",
    "end": "1107120"
  },
  {
    "text": "lot of automation we have to protect from uh some kind of risk changes we",
    "start": "1107120",
    "end": "1112720"
  },
  {
    "text": "added a protection in SCP in the organization level WEP protecting any",
    "start": "1112720",
    "end": "1118919"
  },
  {
    "text": "EBS to be delet deleted uh by a a wrong operation we also now have more strong",
    "start": "1118919",
    "end": "1126799"
  },
  {
    "text": "isolation uh of permission since we have different accounts for each",
    "start": "1126799",
    "end": "1132320"
  },
  {
    "text": "environment and also we we create some pipelines using a canary methodology op",
    "start": "1132320",
    "end": "1140240"
  },
  {
    "text": "dating first the staging and the last criticality clusters uh and integrating",
    "start": "1140240",
    "end": "1145840"
  },
  {
    "text": "over this deploy using smoke test to make sure the changes are secure to to",
    "start": "1145840",
    "end": "1151159"
  },
  {
    "text": "go to the new clusters next",
    "start": "1151159",
    "end": "1155840"
  },
  {
    "text": "please uh so now we have click please there animation this this slide",
    "start": "1158280",
    "end": "1165480"
  },
  {
    "text": "so yeah oh thank you so we have have uh a modern C lot of many new classes",
    "start": "1165480",
    "end": "1172760"
  },
  {
    "text": "that's modern but with no deployments on there with no data thr flowing through",
    "start": "1172760",
    "end": "1177880"
  },
  {
    "text": "the the Clusters and we need to do a a rolling",
    "start": "1177880",
    "end": "1182960"
  },
  {
    "text": "prade for that can you next slide please so now let's discuss how we decide to",
    "start": "1182960",
    "end": "1191000"
  },
  {
    "text": "migrate the data from the old infrastructure to the new",
    "start": "1191000",
    "end": "1196120"
  },
  {
    "text": "one next so at the moment we had three options me maker that we found out",
    "start": "1196640",
    "end": "1203840"
  },
  {
    "text": "that's really well documented is supported by stringy uh but for our case we had some",
    "start": "1203840",
    "end": "1211440"
  },
  {
    "text": "challenges there because we would like to have a easy H back uh depending of the situation",
    "start": "1211440",
    "end": "1217600"
  },
  {
    "text": "also uh to we have like some sensitive Laten as we mentioned for paks and so on",
    "start": "1217600",
    "end": "1223000"
  },
  {
    "text": "and for both case we found out that would would not be easy to hold back because we need to keep up",
    "start": "1223000",
    "end": "1229480"
  },
  {
    "text": "uh upgrade in the cluster the new cluster in the old cluster so it would",
    "start": "1229480",
    "end": "1234919"
  },
  {
    "text": "change the the we need to require us to have two two top names on the D clusters and",
    "start": "1234919",
    "end": "1243440"
  },
  {
    "text": "uh that's not so easy for us to maintain the second option was to funy",
    "start": "1243440",
    "end": "1248679"
  },
  {
    "text": "clusters uh that's a um a methodology that we already use in the past the in a",
    "start": "1248679",
    "end": "1254880"
  },
  {
    "text": "few words it's the idea to make both closer using the same keeper then making",
    "start": "1254880",
    "end": "1260640"
  },
  {
    "text": "the Brokers know each other and then starting using the Cru CR to balance to",
    "start": "1260640",
    "end": "1266200"
  },
  {
    "text": "move the data from the old cluster to the new one the commission the old Brokers so that's also hard for us to",
    "start": "1266200",
    "end": "1273679"
  },
  {
    "text": "hold back this because all the control will be on cruise control uh to manage",
    "start": "1273679",
    "end": "1279120"
  },
  {
    "text": "this this this whole out and it also require us to expose the zookeeping",
    "start": "1279120",
    "end": "1285320"
  },
  {
    "text": "inside uhy that's not something we would like to do and it's also hard to control the",
    "start": "1285320",
    "end": "1293400"
  },
  {
    "text": "Laten so it's option we did not felt we felt that's not the a good option for us",
    "start": "1293400",
    "end": "1300400"
  },
  {
    "text": "and the third option is using a feature that we are calling concr",
    "start": "1300400",
    "end": "1307080"
  },
  {
    "text": "cluster yeah so newbank has an advantage that",
    "start": "1307080",
    "end": "1313159"
  },
  {
    "text": "our 1,800 microservices have all the same",
    "start": "1313159",
    "end": "1319919"
  },
  {
    "text": "framework and we use we use closure and all of them use what we call a internal",
    "start": "1319919",
    "end": "1327440"
  },
  {
    "text": "Library called Kon Kafka and because of that we have some control on how we can",
    "start": "1327440",
    "end": "1335960"
  },
  {
    "text": "deploy new features to all the microservices and",
    "start": "1335960",
    "end": "1342559"
  },
  {
    "text": "uh I think one one thing to notice is that we don't have anything between the microservices and C AFA so uh and we",
    "start": "1342559",
    "end": "1351000"
  },
  {
    "text": "decided to stay that way because uh we thought it would be better to keep the",
    "start": "1351000",
    "end": "1356600"
  },
  {
    "text": "same guarantees that that the direct connection from Kafka to the service uh",
    "start": "1356600",
    "end": "1364200"
  },
  {
    "text": "allows and the the the concurrent closer feature we're going to explain it but it's basically uh based on the fact that",
    "start": "1364200",
    "end": "1372200"
  },
  {
    "text": "we can lose some message ordering uh from uh changing the producer to from",
    "start": "1372200",
    "end": "1380000"
  },
  {
    "text": "the previous cluster to the new cluster and that happens in three steps where we",
    "start": "1380000",
    "end": "1386080"
  },
  {
    "text": "first start consuming from two clusters then we move the producer from the",
    "start": "1386080",
    "end": "1392200"
  },
  {
    "text": "previous cluster to the secondary cluster after we run some checks and in",
    "start": "1392200",
    "end": "1397360"
  },
  {
    "text": "the end we disabled the the consumer from the previous cluster so the top",
    "start": "1397360",
    "end": "1402720"
  },
  {
    "text": "keeps uh running on the new cluster and uh that's possible because",
    "start": "1402720",
    "end": "1408799"
  },
  {
    "text": "we have some topic control mechanisms and we have a service just to control topics and honey is going to walk you",
    "start": "1408799",
    "end": "1416279"
  },
  {
    "text": "through the solution so here are some of the components involved uh",
    "start": "1416279",
    "end": "1424320"
  },
  {
    "text": "solution uh the first one is the Legacy Kafka we the old Kafka Legacy the new",
    "start": "1424320",
    "end": "1429760"
  },
  {
    "text": "Kafka and also the let's call us",
    "start": "1429760",
    "end": "1435440"
  },
  {
    "text": "consumer ex the the producer the consumer and also a sey that's control",
    "start": "1435440",
    "end": "1442679"
  },
  {
    "text": "this stop configuration that's called France and also uh nucle uh nucle that's",
    "start": "1442679",
    "end": "1449480"
  },
  {
    "text": "a CLI solution we have inside newbank uh this is the current the",
    "start": "1449480",
    "end": "1455720"
  },
  {
    "text": "current state the producer is producing to a topic and the consumer is connected",
    "start": "1455720",
    "end": "1462919"
  },
  {
    "text": "it and next slide please we have the first step uh that",
    "start": "1462919",
    "end": "1469440"
  },
  {
    "text": "nucle will send theom to France H saying to copy the config from the Legacy Kafka",
    "start": "1469440",
    "end": "1476240"
  },
  {
    "text": "and creating a top with the same config in the new C to that will make sure that",
    "start": "1476240",
    "end": "1481399"
  },
  {
    "text": "the new top will be able to handle all the load that the would topic was",
    "start": "1481399",
    "end": "1488200"
  },
  {
    "text": "handled uh next slide please so we run some checks here we basically have to",
    "start": "1488200",
    "end": "1495000"
  },
  {
    "text": "understand if the previous the topic in the Legacy cluster fits in the new cluster so we check if we have the",
    "start": "1495000",
    "end": "1501240"
  },
  {
    "text": "capacity to to handle the load in The Next Step uh nuclear will",
    "start": "1501240",
    "end": "1509559"
  },
  {
    "text": "send a message to France to enable the secondary",
    "start": "1509559",
    "end": "1514519"
  },
  {
    "text": "consumer and then France will send a commod to the consumer that's this this",
    "start": "1515080",
    "end": "1520679"
  },
  {
    "text": "example is Diablo and then Diablo will create a connection to the new top key",
    "start": "1520679",
    "end": "1526760"
  },
  {
    "text": "and starting consumer from the this topic it's in this moment the topic doesn't have any message but the",
    "start": "1526760",
    "end": "1533039"
  },
  {
    "text": "connection to this consumer is already working this configuration of updates is",
    "start": "1533039",
    "end": "1539399"
  },
  {
    "text": "done through a broadcast topic which is a topic that you can send a message and all instances are um are listening so we",
    "start": "1539399",
    "end": "1548919"
  },
  {
    "text": "are able to broadcast and information to the entire fleet uh using",
    "start": "1548919",
    "end": "1555600"
  },
  {
    "text": "C um the next step uh nucle will send a",
    "start": "1556240",
    "end": "1561399"
  },
  {
    "text": "commment to France uh send them to swap the production from the old cluster to",
    "start": "1561399",
    "end": "1569320"
  },
  {
    "text": "the new one so the PX receiver that's the producing this example we start",
    "start": "1569320",
    "end": "1575679"
  },
  {
    "text": "producing to the top K but we also have some validation there to avoid",
    "start": "1575679",
    "end": "1581720"
  },
  {
    "text": "some uh a huge amount of uh mess being duplicated being or been processed uh",
    "start": "1581720",
    "end": "1589480"
  },
  {
    "text": "out of order uh on this case so we just do this swap if this limit is on certain",
    "start": "1589480",
    "end": "1598279"
  },
  {
    "text": "levels yeah there is no duplication but uh if uh we have a lot of lag in the",
    "start": "1598279",
    "end": "1603799"
  },
  {
    "text": "customer groups uh and we and we change the producer it's very possible that the",
    "start": "1603799",
    "end": "1610960"
  },
  {
    "text": "new message is being produced to the new cluster arve earlier than the messages",
    "start": "1610960",
    "end": "1616399"
  },
  {
    "text": "uh that were in the Legacy C this is not a real problem because all",
    "start": "1616399",
    "end": "1622000"
  },
  {
    "text": "infrastructure is made to recover from uh the loss of of ordering but it can",
    "start": "1622000",
    "end": "1629520"
  },
  {
    "text": "sometimes result in a burst of exceptions and when an exception happens",
    "start": "1629520",
    "end": "1636360"
  },
  {
    "text": "another system which is the dead lad system uh handles those exceptions so we",
    "start": "1636360",
    "end": "1642600"
  },
  {
    "text": "made some checks to avoid that we overload the exception",
    "start": "1642600",
    "end": "1650158"
  },
  {
    "text": "system and the last step is to is a the clean up uh and it follow this the same",
    "start": "1651399",
    "end": "1658120"
  },
  {
    "text": "uh logic that nule will send a Comm to France and then ask the consumer to stop",
    "start": "1658120",
    "end": "1664840"
  },
  {
    "text": "consuming from the old cluster and then do some validation to",
    "start": "1664840",
    "end": "1670760"
  },
  {
    "text": "make sure there is no lag or message on the topic to delete this stop and free",
    "start": "1670760",
    "end": "1676799"
  },
  {
    "text": "up some resource in the leg",
    "start": "1676799",
    "end": "1680120"
  },
  {
    "text": "and this is the final state of the the migration so yeah that's only for one",
    "start": "1682039",
    "end": "1688360"
  },
  {
    "text": "topic but Noank has 160,000 of topics across out the the",
    "start": "1688360",
    "end": "1695279"
  },
  {
    "text": "Clusters and we found out if if we",
    "start": "1695279",
    "end": "1701640"
  },
  {
    "text": "migrate per day it would take 63 years to finish the migration if we migrate",
    "start": "1701640",
    "end": "1707960"
  },
  {
    "text": "one 100 topics per day it would take six years to finish the migration and if we",
    "start": "1707960",
    "end": "1713480"
  },
  {
    "text": "migrated 100 topics per day we can finish the migration less than one year",
    "start": "1713480",
    "end": "1718600"
  },
  {
    "text": "and that's our Target and for that we we built a solution for batch",
    "start": "1718600",
    "end": "1725679"
  },
  {
    "text": "migration that talks uh that's using the the feature we just talked about and uh with a a batch",
    "start": "1725679",
    "end": "1734760"
  },
  {
    "text": "manner we are with a list of uh with send a list of top to to France saying",
    "start": "1734760",
    "end": "1741200"
  },
  {
    "text": "to migrate from the Legos class to the new one can you click this slide please and all the top in this batch will be in",
    "start": "1741200",
    "end": "1748799"
  },
  {
    "text": "the state of migrating and then on the the states of our topics are okay",
    "start": "1748799",
    "end": "1756279"
  },
  {
    "text": "and migrated the migration is finished it and here are some hiss we have so far",
    "start": "1756279",
    "end": "1764760"
  },
  {
    "text": "can you go to the next slide uh from the the 153 classes we have now uh",
    "start": "1764760",
    "end": "1775360"
  },
  {
    "text": "we we mentioned uh 15 of those closes already running the new new infra and",
    "start": "1775360",
    "end": "1782720"
  },
  {
    "text": "there are some results we we are see in the new infer next slide",
    "start": "1782720",
    "end": "1787960"
  },
  {
    "text": "please uh the first thing is uh can you click in this like this",
    "start": "1787960",
    "end": "1796240"
  },
  {
    "text": "uh we we were with some instance station burden because we need to take some",
    "start": "1796240",
    "end": "1802240"
  },
  {
    "text": "actions in the past the Legacy infra and that's no more a burden for the team",
    "start": "1802240",
    "end": "1807360"
  },
  {
    "text": "next please uh we are creating a class in one hour instead of one week that was taking",
    "start": "1807360",
    "end": "1814000"
  },
  {
    "text": "the last the Legacy infra next uh we take advantage for of the op",
    "start": "1814000",
    "end": "1821320"
  },
  {
    "text": "grade to the migration to prr the Kafka version so new Kafka are with new",
    "start": "1821320",
    "end": "1828120"
  },
  {
    "text": "version next pleas uh and using G ups bring us many",
    "start": "1828120",
    "end": "1834360"
  },
  {
    "text": "benefits like it's easier to track changes it's also resizing dis is much simpler and",
    "start": "1834360",
    "end": "1841880"
  },
  {
    "text": "also scale up the class is much simple now uh and also all this these",
    "start": "1841880",
    "end": "1850399"
  },
  {
    "text": "mechanisms this mechanism is enable the team to contribute and",
    "start": "1850399",
    "end": "1856080"
  },
  {
    "text": "create clusters with the team so that's leverage some uh initiatives between the",
    "start": "1856080",
    "end": "1863720"
  },
  {
    "text": "new bank uh organizations this is one of our",
    "start": "1863720",
    "end": "1869279"
  },
  {
    "text": "clusters that's the data string cluster that have picks off uh 80 megabytes per",
    "start": "1869279",
    "end": "1876559"
  },
  {
    "text": "second and this flows could be on the Legacy infra and it would be for sure a",
    "start": "1876559",
    "end": "1883720"
  },
  {
    "text": "nice neighborhood for that topic and making some uh making making hard to do",
    "start": "1883720",
    "end": "1890440"
  },
  {
    "text": "some uh hair balancing the Clusters because there's a lot of data there and now it's using the new infra that's a",
    "start": "1890440",
    "end": "1897440"
  },
  {
    "text": "good result we had next",
    "start": "1897440",
    "end": "1901799"
  },
  {
    "text": "please okay so quick recap uh no Bank cfus transport around 3",
    "start": "1903399",
    "end": "1910120"
  },
  {
    "text": "mes per month and we have a high percentage of the Brazilian Financial",
    "start": "1910120",
    "end": "1915799"
  },
  {
    "text": "system going through them uh the architecture of the G infrastructure was devised in",
    "start": "1915799",
    "end": "1922360"
  },
  {
    "text": "2014 and was very hard to manage at scale we designed an new infrastructure",
    "start": "1922360",
    "end": "1928720"
  },
  {
    "text": "around streamy using multiplys accounts uh connectivity with this account and",
    "start": "1928720",
    "end": "1933880"
  },
  {
    "text": "the details you can ask in the chat uh and we are rolling out those",
    "start": "1933880",
    "end": "1940799"
  },
  {
    "text": "clusters using a custom approach that we develop using our topic management",
    "start": "1940799",
    "end": "1946840"
  },
  {
    "text": "system which the concurrent clusters uh one one thing to notice is",
    "start": "1946840",
    "end": "1952600"
  },
  {
    "text": "that the concurrent clusters also allows us to move topics uh to Dedicated single",
    "start": "1952600",
    "end": "1960760"
  },
  {
    "text": "tenant clusters so we can leverage this feature",
    "start": "1960760",
    "end": "1966960"
  },
  {
    "text": "to move topics from the Legacy cluster to the new clusters but we also can move",
    "start": "1966960",
    "end": "1974559"
  },
  {
    "text": "from the new clusters to a dedicated cluster whenever it's possible are required by product teams and the idea",
    "start": "1974559",
    "end": "1980799"
  },
  {
    "text": "of building it as a service is to make sure that product teams can uh do that",
    "start": "1980799",
    "end": "1987639"
  },
  {
    "text": "operation and that the the guard rail and checks will be automated so they can",
    "start": "1987639",
    "end": "1994639"
  },
  {
    "text": "move topics without even U telling the the team responsible for",
    "start": "1994639",
    "end": "2001880"
  },
  {
    "text": "c for the future uh we expect to stream line the",
    "start": "2001880",
    "end": "2008720"
  },
  {
    "text": "the Cadence of upgrades you keep up with stringy and CF releases uh basically upgrading us to",
    "start": "2008720",
    "end": "2016360"
  },
  {
    "text": "the business class every year at least uh they're going to distribute the",
    "start": "2016360",
    "end": "2023480"
  },
  {
    "text": "topics in the different C of flavors in a way that we can provide uh strict slas",
    "start": "2023480",
    "end": "2030159"
  },
  {
    "text": "for topics that are low latency for like pics uh and keep the heavy lifting",
    "start": "2030159",
    "end": "2037880"
  },
  {
    "text": "topics with tens of megabytes per message uh in separated",
    "start": "2037880",
    "end": "2043080"
  },
  {
    "text": "places uh we plan on starting using the operator to rebalance things uh we are",
    "start": "2043080",
    "end": "2049000"
  },
  {
    "text": "currently using cruise control direct through the UI uh and our topic management system it",
    "start": "2049000",
    "end": "2057158"
  },
  {
    "text": "doesn't go through the topic operator so we expect that we we use the the admin",
    "start": "2057159",
    "end": "2063158"
  },
  {
    "text": "API for that but we expect to to add does interaction layer where we the top",
    "start": "2063159",
    "end": "2071040"
  },
  {
    "text": "our top control system call generates crds and then uh the operator can work",
    "start": "2071040",
    "end": "2078398"
  },
  {
    "text": "uh from there the roll out of the platform is is",
    "start": "2078399",
    "end": "2083919"
  },
  {
    "text": "is going well the new clusters are always being created in the new infrastructure uh but we yet to move a",
    "start": "2083919",
    "end": "2090480"
  },
  {
    "text": "topic using the the system and we deployed a few clusters without craft and we plan on upgrading",
    "start": "2090480",
    "end": "2097920"
  },
  {
    "text": "them to craft and use tiered storage whenever it's",
    "start": "2097920",
    "end": "2104359"
  },
  {
    "text": "usable uh thank you very much uh we like to to do a special thanks to the team uh",
    "start": "2104359",
    "end": "2112599"
  },
  {
    "text": "we have people from the team that handles messages here and from all over",
    "start": "2112599",
    "end": "2118320"
  },
  {
    "text": "newbank which helped us a lot Danielle marus Maru Mor B jiku Bruno Marcela",
    "start": "2118320",
    "end": "2126680"
  },
  {
    "text": "Paula uh Andre na thank you very much uh and we are",
    "start": "2126680",
    "end": "2133880"
  },
  {
    "text": "going to answer the questions in the que and",
    "start": "2133880",
    "end": "2138000"
  },
  {
    "text": "me thank you so much that was a really interesting session um yeah have a look",
    "start": "2139480",
    "end": "2145079"
  },
  {
    "text": "over there's a few questions in the Q&A so Julia and one will answer them there but yeah thank you very much for coming",
    "start": "2145079",
    "end": "2150960"
  },
  {
    "text": "and speaking at stry con and the next session will be at 4:40 and it'll be on",
    "start": "2150960",
    "end": "2157680"
  },
  {
    "text": "partial multi-tenancy on Kafka using stsy at little horse",
    "start": "2157680",
    "end": "2164160"
  }
]