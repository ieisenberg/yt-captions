[
  {
    "text": "hello Hey everyone Thank you very much for uh staying with us up until the very last",
    "start": "280",
    "end": "6400"
  },
  {
    "text": "session of the day Highly appreciate it Um so my name is Maxim Vizeno I'm here",
    "start": "6400",
    "end": "12559"
  },
  {
    "text": "today alongside Laurai We are both working for data dog as part of the infrastructure engineering group And if",
    "start": "12559",
    "end": "19680"
  },
  {
    "text": "we're here with you today is to talk to you about uh our story on how we operate our infrastructure over not only one but",
    "start": "19680",
    "end": "26960"
  },
  {
    "text": "three cloud providers now and uh how Kubernetes helped us uh in that journey",
    "start": "26960",
    "end": "33360"
  },
  {
    "text": "Uh we'll be happy to take questions after the talk I'm not sure we'll have enough time but happy to take them offline Otherwise you can always find us",
    "start": "33360",
    "end": "39680"
  },
  {
    "text": "on the Kubernetes Slack And in case you do not know about data dog we are a software as a service obser",
    "start": "39680",
    "end": "46160"
  },
  {
    "text": "observability platform uh that help our and we aim to help our customers obtain better visibility into their",
    "start": "46160",
    "end": "52399"
  },
  {
    "text": "infrastructure and applications And to give you a bit of sense of scale of uh what we operate at",
    "start": "52399",
    "end": "59280"
  },
  {
    "text": "we have about 800 integration as of today and we handle more than 10 trillions events on a daily",
    "start": "59280",
    "end": "66119"
  },
  {
    "text": "basis Uh if we're here today is to talk about Kubernetes So on that front uh we",
    "start": "66119",
    "end": "71200"
  },
  {
    "text": "have hundreds of clusters uh tens of thousands of nodes and hundreds of thousands of pods and uh they are all uh",
    "start": "71200",
    "end": "79040"
  },
  {
    "text": "living in the cloud uh among three of uh three providers public cloud providers namely Azure AWS and",
    "start": "79040",
    "end": "86920"
  },
  {
    "text": "GCP and if we did that uh spoiler alert uh it wasn't because it was fun uh",
    "start": "86920",
    "end": "92759"
  },
  {
    "text": "surprisingly perhaps but we had some reasons to do so first uh our customers",
    "start": "92759",
    "end": "97920"
  },
  {
    "text": "were looking for us to be uh close to them Uh there is also this uh interesting philosophy that we have at",
    "start": "97920",
    "end": "103840"
  },
  {
    "text": "data dog that we call dog fooding which is basically looking to experiment uh as",
    "start": "103840",
    "end": "109040"
  },
  {
    "text": "much as we can in order to potentially provide solutions for that are um more pertinent for our customers if it solves",
    "start": "109040",
    "end": "116079"
  },
  {
    "text": "our problem potentially it solves theirs and of course uh more locations to operate from and also a good starting",
    "start": "116079",
    "end": "122640"
  },
  {
    "text": "point to establish partnerships with those providers And uh so to get started",
    "start": "122640",
    "end": "128319"
  },
  {
    "text": "I'll hand it over to Lauron who will dig into will get you into where we started and uh where we are",
    "start": "128319",
    "end": "135400"
  },
  {
    "text": "today So let's start with a bit of history Um until 2018 dead dog was",
    "start": "135400",
    "end": "142959"
  },
  {
    "text": "running on a single region uh in the US on AWS uh what we could refer to as classic and everything was operated with",
    "start": "142959",
    "end": "149920"
  },
  {
    "text": "virtual machines managed by chef and application were deployed with capistrano In 2018 we decided to expand",
    "start": "149920",
    "end": "156400"
  },
  {
    "text": "and we created our first region outside of the US on GCP in Europe And over the",
    "start": "156400",
    "end": "161440"
  },
  {
    "text": "years we added more and more regions So we started diversification by adding uh edge data centers which we were from",
    "start": "161440",
    "end": "167840"
  },
  {
    "text": "which we run synthetics and browser tests uh we had our first certifications and in 2021 we had a big",
    "start": "167840",
    "end": "175760"
  },
  {
    "text": "expansion where we created uh three new regions one uh for government agency in the US and we continued last year I mean",
    "start": "175760",
    "end": "184239"
  },
  {
    "text": "two years ago we opened our first region in Asia in Tokyo and we continue added uh edge",
    "start": "184239",
    "end": "190239"
  },
  {
    "text": "regions and this is where we stand today as of the first quarter of 2025",
    "start": "190239",
    "end": "197200"
  },
  {
    "text": "To summarize this in in a few numbers uh we currently operate over three different cloud providers Uh we have six",
    "start": "197200",
    "end": "203200"
  },
  {
    "text": "main uh large regions and we have 29 edge locations To make that work we we had to",
    "start": "203200",
    "end": "210319"
  },
  {
    "text": "find a solution right And of course as Maxim was saying one of the reason we're sharing it here today is because we're using Kubernetes to make it",
    "start": "210319",
    "end": "216760"
  },
  {
    "text": "work And as a reminder uh this is how we started at data dog right Everything was running on AWS It was simple enough uh",
    "start": "216760",
    "end": "224319"
  },
  {
    "text": "when we wanted to deploy applications we were using a set of tools terraform chef capistrano some packer that was easy",
    "start": "224319",
    "end": "232159"
  },
  {
    "text": "when we added our first region outside of uh AWS uh we decided to pick GCP in",
    "start": "232159",
    "end": "237519"
  },
  {
    "text": "Europe and we had to decide what to do next right so were we going to rewrite everything we had written for AWS or",
    "start": "237519",
    "end": "244000"
  },
  {
    "text": "where we going to try and find something that was more cloud agnostic and the big question is can we",
    "start": "244000",
    "end": "249680"
  },
  {
    "text": "have an infrastructure abstraction so we can consider different regions um in in the same way and of course I mean I'm",
    "start": "249680",
    "end": "256239"
  },
  {
    "text": "sure you know the answer the answer felt uh very clear at the time it was it was Kubernetes right and the idea is that",
    "start": "256239",
    "end": "264000"
  },
  {
    "text": "Kubernetes offers us a unified way to interact with infra right you can software packaging in a unique way with",
    "start": "264000",
    "end": "270240"
  },
  {
    "text": "containers uh you have a unique way to define application rollout strategies and updates and you have a way to define",
    "start": "270240",
    "end": "276560"
  },
  {
    "text": "resources that will be god agnostic and of course uh this was working on all the providers we were considering at the",
    "start": "276560",
    "end": "284400"
  },
  {
    "text": "So so far I mean our applications were running in in in nodes and what we",
    "start": "284880",
    "end": "289919"
  },
  {
    "text": "wanted to move to is to move for to applications uh running in containers in in",
    "start": "289919",
    "end": "295800"
  },
  {
    "text": "Kubernetes and there are additional reasons right to move to Kubernetes As I was saying uh abstracting the infra was",
    "start": "295800",
    "end": "303520"
  },
  {
    "text": "uh the first one and the most important one for us But something that was important to very similar to using",
    "start": "303520",
    "end": "308560"
  },
  {
    "text": "multiple clouds is that our customers were running were starting to run Kubernetes in 2018 and we were not and",
    "start": "308560",
    "end": "315280"
  },
  {
    "text": "so uh in order to make our integration and our support for communities better it made a lot of sense for us to also",
    "start": "315280",
    "end": "320720"
  },
  {
    "text": "run on Kubernetes In addition the community was uh pretty big at the time It's even bigger now which means like we",
    "start": "320720",
    "end": "326880"
  },
  {
    "text": "have a lot of people to talk to when we have challenges or issues And finally and it's important too right",
    "start": "326880",
    "end": "332720"
  },
  {
    "text": "It was much easier to hire people in the infraspace when we were running on communities at the time So now we have",
    "start": "332720",
    "end": "340000"
  },
  {
    "text": "the answer Uh let's go right However now that we've said that we want to do",
    "start": "340000",
    "end": "345759"
  },
  {
    "text": "Kubernetes we need to figure out how we're going to do it And I'm sure you remember Kubernetes the hard way from KC",
    "start": "345759",
    "end": "352880"
  },
  {
    "text": "High Tower but it can be a bit overwhelming at the beginning right you have a lot of concept you need to um to",
    "start": "352880",
    "end": "359360"
  },
  {
    "text": "to care about In 2018 when we started of course we",
    "start": "359360",
    "end": "364960"
  },
  {
    "text": "considered using managed Kubernetes right uh all cloud providers had offerings so EKS G and AKS however",
    "start": "364960",
    "end": "373080"
  },
  {
    "text": "remember we were early 2018 right and at that time the landscape was pretty",
    "start": "373080",
    "end": "378639"
  },
  {
    "text": "different um EKS and AKS were very young and still",
    "start": "378639",
    "end": "384880"
  },
  {
    "text": "in beta and while G was uh generally available it was limited to 500 not",
    "start": "384880",
    "end": "391120"
  },
  {
    "text": "cluster And at the time we already knew we needed much bigger",
    "start": "391120",
    "end": "396199"
  },
  {
    "text": "clusters In addition um manage Kubernetes clusters uh come with a lot",
    "start": "396199",
    "end": "402240"
  },
  {
    "text": "of things that are that a cloud provider put in them right They're very opinionated uh because they each cloud",
    "start": "402240",
    "end": "409120"
  },
  {
    "text": "provider has its own way of implementing things They're very similar but they're different enough that it's a bit painful to build an abstraction on top of them",
    "start": "409120",
    "end": "415680"
  },
  {
    "text": "And we put a few example there like networking is different Uh operating systems are different Supported",
    "start": "415680",
    "end": "420800"
  },
  {
    "text": "Kubernetes versions can be different And something we also cared about and and we still care about is um this manage",
    "start": "420800",
    "end": "428160"
  },
  {
    "text": "offering offer very limited observability on the control plane And if you've run at scale I'm sure you know",
    "start": "428160",
    "end": "434080"
  },
  {
    "text": "that uh when you have an issue you need to understand what's happening with API servers at CD which you can't see when",
    "start": "434080",
    "end": "439840"
  },
  {
    "text": "you have a manager offering So we decided to do it ourselves",
    "start": "439840",
    "end": "445400"
  },
  {
    "text": "Over the years we went through multiple iterations So we started pretty simple",
    "start": "445400",
    "end": "451599"
  },
  {
    "text": "right uh we deployed the control plane of our cluster in virtual machines and all the worker nodes were also virtual",
    "start": "451599",
    "end": "457440"
  },
  {
    "text": "machines You can recognize here the main components of the control plane and we managed all this with with",
    "start": "457440",
    "end": "463759"
  },
  {
    "text": "glue Uh I mean we were familiar with terraform and civilian packer and of course shellcript So we we had a lot of",
    "start": "463759",
    "end": "469720"
  },
  {
    "text": "those and now that we had clusters we could deploy applications but remember I mean we we",
    "start": "469720",
    "end": "478400"
  },
  {
    "text": "had already we started we were starting to have two regions of the time and we knew and we knew we're soon going to",
    "start": "478400",
    "end": "483840"
  },
  {
    "text": "have more than more than two and we need to provide a way for applications to deploy to all these regions to make things even more complex",
    "start": "483840",
    "end": "491440"
  },
  {
    "text": "clusters have limit right I mentioned 500 nodes before it turns out we're trying to limit our clusters to 5,000",
    "start": "491440",
    "end": "496879"
  },
  {
    "text": "nodes but even with 5000 nodes In some regions we need more than one And if we want the schema to be more diagram to be",
    "start": "496879",
    "end": "503599"
  },
  {
    "text": "more complete uh regions actually have each have multiple clusters which makes the life of",
    "start": "503599",
    "end": "510560"
  },
  {
    "text": "application developers a bit tricky right Because uh they want to deploy their application in all the regions we",
    "start": "510560",
    "end": "515919"
  },
  {
    "text": "support at data dog but it's starting to be difficult because they need to figure out which cluster they deploy to They",
    "start": "515919",
    "end": "521680"
  },
  {
    "text": "need to figure out if regions are slightly different from one another And and also in terms of workflow right If",
    "start": "521680",
    "end": "526959"
  },
  {
    "text": "you do a deployment on a region it breaks maybe you want to stop and not continue to another region So we had to",
    "start": "526959",
    "end": "532959"
  },
  {
    "text": "find a way to make this easy for for application And the we actually implemented a solution uh that is that",
    "start": "532959",
    "end": "538800"
  },
  {
    "text": "we call the software uh delivery platform And here is a quick illustration of how it works right So",
    "start": "538800",
    "end": "546240"
  },
  {
    "text": "when uh application developers want to deploy their application what they do is they use the platform And in this",
    "start": "546240",
    "end": "551920"
  },
  {
    "text": "example here uh we're going to update application A from version blue to version green And what happens is well",
    "start": "551920",
    "end": "558480"
  },
  {
    "text": "the application team is going to call platform and tell update it to this version And what's going to happen is",
    "start": "558480",
    "end": "564160"
  },
  {
    "text": "actually this platform is going to start a temporal workflow where we encode all the logic to deploy And to give you an example of a",
    "start": "564160",
    "end": "571519"
  },
  {
    "text": "typical workflow what's going to happen is going to start deploying to first region here Asia in the US And then",
    "start": "571519",
    "end": "578560"
  },
  {
    "text": "we're going to check that everything is working fine by looking at monitors and also metrics And if everything is okay",
    "start": "578560",
    "end": "585440"
  },
  {
    "text": "sometimes we also want to wait for some time to make sure that things are are stable enough If everything is fine we",
    "start": "585440",
    "end": "591279"
  },
  {
    "text": "proceed and we continue to the second region Right So in that example here we've now updated uh the application A",
    "start": "591279",
    "end": "597200"
  },
  {
    "text": "on GCP And finally we do the same thing where we verify everything and if it's okay we proceed with AWS last check And",
    "start": "597200",
    "end": "605040"
  },
  {
    "text": "now we're happy right the um application has been updated So we now have a good story for",
    "start": "605040",
    "end": "611680"
  },
  {
    "text": "applications right But for infrastructure components we we don't have a good story there right Because as",
    "start": "611680",
    "end": "617760"
  },
  {
    "text": "I was saying we're uh application team tend to care about a single cluster in each region But as an infra team we",
    "start": "617760",
    "end": "623920"
  },
  {
    "text": "actually have to care about all the clusters which means updates are pretty painful right Because imagine when you have dozens or hundreds of clusters And",
    "start": "623920",
    "end": "630959"
  },
  {
    "text": "so our operating model is slightly different from application team because in in our case uh we're using terraform",
    "start": "630959",
    "end": "637600"
  },
  {
    "text": "using enable and so updating the control plane means running terapform content updates and this is one of the big shift",
    "start": "637600",
    "end": "644079"
  },
  {
    "text": "we we did a few years back where we decided to run command in a different way So if you remember uh the way our to",
    "start": "644079",
    "end": "651920"
  },
  {
    "text": "were looking like right everything was working was running in VM and managed with as I was saying terraform on",
    "start": "651920",
    "end": "658279"
  },
  {
    "text": "packer and what we realized and and we're not the only ones to do this right is that instead of running application",
    "start": "658279",
    "end": "664480"
  },
  {
    "text": "in this cluster we could also actually run control pen components if you can see here at the bottom of the slide",
    "start": "664480",
    "end": "669680"
  },
  {
    "text": "instead of having application A application B we know we now have pods running Kubernetes control pen",
    "start": "669680",
    "end": "675440"
  },
  {
    "text": "components And what's interesting there is now if",
    "start": "675440",
    "end": "681040"
  },
  {
    "text": "we want to update a control plane we can actually use the same primitive we use to deploy application which is we can just use helm and just update and roll",
    "start": "681040",
    "end": "688160"
  },
  {
    "text": "out our control plane Of course in this example we have a single cluster but you",
    "start": "688160",
    "end": "693680"
  },
  {
    "text": "can actually have multiple ones right So just to summarize this slide on the top you have the control plane of a parent",
    "start": "693680",
    "end": "699600"
  },
  {
    "text": "cluster which we still have to manage with virtual machines because we have to start somewhere And just lower you have",
    "start": "699600",
    "end": "705920"
  },
  {
    "text": "the worker node from this cluster where we actually run the control planes of the child clusters right And all this is",
    "start": "705920",
    "end": "711440"
  },
  {
    "text": "running as pods and we have multiple we can run control plane for multiple clusters in the same parents And finally",
    "start": "711440",
    "end": "717440"
  },
  {
    "text": "on the left hand side of the slide you have the actual worker nodes for all these",
    "start": "717440",
    "end": "722399"
  },
  {
    "text": "clusters And this is where we are today And as you can see we still have to manage some Terraform and some anible",
    "start": "723000",
    "end": "728800"
  },
  {
    "text": "code But in terms of orders of magnitude we manage dozens of clusters uh with Kubernetes now hundreds even but only a",
    "start": "728800",
    "end": "736480"
  },
  {
    "text": "few have to be managed with Terraform and Anible Still we want to make this better and",
    "start": "736480",
    "end": "741920"
  },
  {
    "text": "one of one of the thing we have in mind is maybe we can run the parent control plane in manage Kubernetes in which case",
    "start": "741920",
    "end": "747680"
  },
  {
    "text": "everything would be manage the same We would just have to manage our cluster with with Kubernetes So if you remember I said",
    "start": "747680",
    "end": "754480"
  },
  {
    "text": "earlier that manage Kubernetes offerings uh were pretty different and hard to and hard to use This is a bit of a specific",
    "start": "754480",
    "end": "760880"
  },
  {
    "text": "case right because it's only a single team that has to care about it the team that is managing Kubernetes and also",
    "start": "760880",
    "end": "766160"
  },
  {
    "text": "it's a very specific single use case and so it's easier to adapt to to to the different needs So it is much better to",
    "start": "766160",
    "end": "772959"
  },
  {
    "text": "what we had before right So instead of running terapform and now what we can do is we can just have this ugly for loop",
    "start": "772959",
    "end": "779200"
  },
  {
    "text": "here where we just updating with helm or our cluster It's good but it's not ideal But",
    "start": "779200",
    "end": "784959"
  },
  {
    "text": "if you remember we actually have a tool that is allowing us to deploy application across multiple region with",
    "start": "784959",
    "end": "790079"
  },
  {
    "text": "logic to deploy it It turned out if instead of deploying application we try and deploy uh control",
    "start": "790079",
    "end": "797279"
  },
  {
    "text": "pen components What we can do here and I'm going to take the example of ATD is we can use exactly the same type of",
    "start": "797279",
    "end": "802800"
  },
  {
    "text": "workflow to update control pen components and here we're going to see how we update ATCD from blue to green",
    "start": "802800",
    "end": "808480"
  },
  {
    "text": "and you're going to see that it's exactly what we're using before for applications So we can leverage the tooling that we've built for other teams",
    "start": "808480",
    "end": "814959"
  },
  {
    "text": "and this is very similar right and first we're going to update in a region then we're going to move to a second one",
    "start": "814959",
    "end": "820079"
  },
  {
    "text": "where slightly differently we can iterate over multiple clusters in a region and and then define a region",
    "start": "820079",
    "end": "825200"
  },
  {
    "text": "where we have three clusters So let's recap right we now have a way to deploy our applications in",
    "start": "825200",
    "end": "832160"
  },
  {
    "text": "multiple regions and and and cluster and different providers and we have a good way to abstract our application uh using",
    "start": "832160",
    "end": "838560"
  },
  {
    "text": "Kubernetes However one big question remains which is how good is this abstraction and Maxim is gonna talk",
    "start": "838560",
    "end": "844880"
  },
  {
    "text": "about it Thank you Laur So yes so as you can see uh as Lauren stated we have a know a",
    "start": "844880",
    "end": "852480"
  },
  {
    "text": "way to help us unify how we run and deploy cub application across uh multiple cloud providers But one",
    "start": "852480",
    "end": "858800"
  },
  {
    "text": "question remains indeed like how far does this abstraction actually goes and where does it start leaking and so as",
    "start": "858800",
    "end": "865600"
  },
  {
    "text": "under kubernetes we are still heavily relying under on cloud native primitives such as compute storage or network This",
    "start": "865600",
    "end": "872959"
  },
  {
    "text": "is where things starts to gets tricky when you look into making that work over multiple providers because while",
    "start": "872959",
    "end": "879040"
  },
  {
    "text": "Kubernetes gives us a consistent API that all sales infrastructure group and",
    "start": "879040",
    "end": "884639"
  },
  {
    "text": "application teams can leverage uh in order to interact with uh the underlying infrastructure remains widely different",
    "start": "884639",
    "end": "891199"
  },
  {
    "text": "from one provider to the other So let's dig into uh each one of them a bit and",
    "start": "891199",
    "end": "896320"
  },
  {
    "text": "let's start with compute So at the compute layer there are two key resources that we care about mainly pods",
    "start": "896320",
    "end": "902320"
  },
  {
    "text": "and nodes Pods are are a new way to um uh deploy our application whilst uh the",
    "start": "902320",
    "end": "908720"
  },
  {
    "text": "nodes represents the actual underlying compute resources on which pods are being scheduled on Hopefully I'm not",
    "start": "908720",
    "end": "915360"
  },
  {
    "text": "making learning you anything there But if you recall beforehand uh before we were using Kubernetes every app we were",
    "start": "915360",
    "end": "922320"
  },
  {
    "text": "operating had uh a native implementation over dedicated instances that were",
    "start": "922320",
    "end": "927680"
  },
  {
    "text": "dynamically scaled through uh well in the case of AWS autoscaling groups And",
    "start": "927680",
    "end": "933440"
  },
  {
    "text": "as uh everyone in the engineering group at data log was very familiar with this approach Uh we decided to get uh the",
    "start": "933440",
    "end": "940160"
  },
  {
    "text": "migration much easier on everyone hands to start uh with a very similar one by",
    "start": "940160",
    "end": "945519"
  },
  {
    "text": "having a single pod per node And the main uh advantage of that well of course",
    "start": "945519",
    "end": "952079"
  },
  {
    "text": "being able to migrate uh a bit more quickly but also preventing ourselves with any kind of upcoming uh quirks that",
    "start": "952079",
    "end": "958560"
  },
  {
    "text": "Kubernetes brings along the way such as like uh having applications competing for the same resources over the same",
    "start": "958560",
    "end": "964000"
  },
  {
    "text": "node and potentially also keeping the possibilities that user were already",
    "start": "964000",
    "end": "969199"
  },
  {
    "text": "used to such as having local disks or any kind of accelerators that needs to uh be solely accessed by a single",
    "start": "969199",
    "end": "975680"
  },
  {
    "text": "application And here as well similarly as before our general idea was to continue relying on",
    "start": "975680",
    "end": "982079"
  },
  {
    "text": "the same underlying uh primitives that well services that the providers was offering us such as autoscaling groups",
    "start": "982079",
    "end": "989680"
  },
  {
    "text": "But from a mentability perspective though uh we had to find a way to efficiently manage all those scaling",
    "start": "989680",
    "end": "996079"
  },
  {
    "text": "groups uh among those providers And this is where Kubernetes can come quite handy",
    "start": "996079",
    "end": "1002160"
  },
  {
    "text": "because if you are missing an abstraction well you can build it yourself uh thanks to custom resource",
    "start": "1002160",
    "end": "1007279"
  },
  {
    "text": "definitions and this is what uh by heavily inspiring ourselves from what the cloud providers do for their",
    "start": "1007279",
    "end": "1014079"
  },
  {
    "text": "respective uh managed Kubernetes offerings uh decided to do as well So we introduced this concept of node",
    "start": "1014079",
    "end": "1019920"
  },
  {
    "text": "groupoups that uh we therefore offered to our users and that way they could have a way to uh declare the uh uh",
    "start": "1019920",
    "end": "1027600"
  },
  {
    "text": "instances that they were expecting to have to run their workloads in a unified fashion and the idea there was to really",
    "start": "1027600",
    "end": "1034319"
  },
  {
    "text": "keep it simple for them Uh they declare what they need and we take care of uh implementing then implementing it on our",
    "start": "1034319",
    "end": "1041280"
  },
  {
    "text": "hand So from there on when uh users were looking to deploy their applications they would start by uh defining node",
    "start": "1041280",
    "end": "1048079"
  },
  {
    "text": "groups uh that represents what they need and uh ourselves we would be we operate",
    "start": "1048079",
    "end": "1053520"
  },
  {
    "text": "a controller on each uh cluster uh that can reconcile uh those configured node",
    "start": "1053520",
    "end": "1059760"
  },
  {
    "text": "groups uh basically making the necessary API calls to the underlying provider uh",
    "start": "1059760",
    "end": "1065600"
  },
  {
    "text": "in the case of AWS for example creating uh the associated autoscaling groups and finally uh we decided to leverage the",
    "start": "1065600",
    "end": "1072559"
  },
  {
    "text": "cluster autoscaler in order to determine how many nodes for we need for each of within each of these groups according to",
    "start": "1072559",
    "end": "1079520"
  },
  {
    "text": "the amount of pods that the user was expecting to have to schedule And uh the",
    "start": "1079520",
    "end": "1086000"
  },
  {
    "text": "cool thing with that is that well the cluster autoscaler came up out of the box with support for all the providers",
    "start": "1086000",
    "end": "1091600"
  },
  {
    "text": "we were aiming to operate on and uh we had to make uh our controller do the",
    "start": "1091600",
    "end": "1097440"
  },
  {
    "text": "same But as long as we have that we can technically support uh any kind of provider while having a a unified",
    "start": "1097440",
    "end": "1103760"
  },
  {
    "text": "abstraction for the users And thanks to this approach but most importantly thanks to the huge",
    "start": "1103760",
    "end": "1109440"
  },
  {
    "text": "effort from the entire engineering group at data dog at some point we eventually managed to uh migrate uh the entireness",
    "start": "1109440",
    "end": "1116400"
  },
  {
    "text": "of our applications over Kubernetes And as you can imagine though",
    "start": "1116400",
    "end": "1122559"
  },
  {
    "text": "uh this solution started uh to quickly show its limits Uh of course uh the",
    "start": "1122559",
    "end": "1128000"
  },
  {
    "text": "abstraction uh was quite uh leaky as uh users would still have to uh configure",
    "start": "1128000",
    "end": "1134799"
  },
  {
    "text": "their uh instance types Figuring out on which cloud providers they reside on and",
    "start": "1134799",
    "end": "1139840"
  },
  {
    "text": "also in terms of efficiency Uh figuring out what instance types you need for your workload is a hard task to do and",
    "start": "1139840",
    "end": "1145600"
  },
  {
    "text": "if you well uh especially on one well especially on three providers where uh",
    "start": "1145600",
    "end": "1150799"
  },
  {
    "text": "you may not have the equivalence uh from one to another And uh last but not least",
    "start": "1150799",
    "end": "1156559"
  },
  {
    "text": "in terms of scalability as well it p uh quite uh some challenges as uh for uh",
    "start": "1156559",
    "end": "1162320"
  },
  {
    "text": "any kind of pod that you need to schedule you need a new node So it adds up in terms of latency and when you",
    "start": "1162320",
    "end": "1168160"
  },
  {
    "text": "start managing hundreds or thousands of uh autoscaler uh within your AWS or GCP",
    "start": "1168160",
    "end": "1174320"
  },
  {
    "text": "or uh Asia accounts uh it starts to quickly put pressure over the API uh the",
    "start": "1174320",
    "end": "1180240"
  },
  {
    "text": "cloud provider APIs And so for those who started with Kubernetes more recently this might seem",
    "start": "1180240",
    "end": "1186080"
  },
  {
    "text": "obvious But of course we want our users to focus on pods rather than nodes right",
    "start": "1186080",
    "end": "1191440"
  },
  {
    "text": "Uh so in order to move from where we started to a more future proof approach uh we had to start somewhere And it",
    "start": "1191440",
    "end": "1197840"
  },
  {
    "text": "began by ensuring that every pod uh defined uh were uh every pod specs were",
    "start": "1197840",
    "end": "1204640"
  },
  {
    "text": "embracing this u uh habit of specifying me resources requests and limits And as",
    "start": "1204640",
    "end": "1211200"
  },
  {
    "text": "you can see here as well and here as well I'm not learning you anything but uh this is a much more abstracted way to",
    "start": "1211200",
    "end": "1217679"
  },
  {
    "text": "represent what uh our application needs in terms of resources rather than specifying a particular instance type",
    "start": "1217679",
    "end": "1224160"
  },
  {
    "text": "And so once we had consistent pot specs being set uh among u uh our apps we were",
    "start": "1224160",
    "end": "1230400"
  },
  {
    "text": "able to start flipping the model and start managing node groups on behalf of our users So application teams that were",
    "start": "1230400",
    "end": "1237200"
  },
  {
    "text": "ready to make the move uh having updated their node spec we could happily onboard them onto those set of management groups",
    "start": "1237200",
    "end": "1244080"
  },
  {
    "text": "that uh infrastructure team would be looking after Whilst others who still needed to have like a strong ownership",
    "start": "1244080",
    "end": "1250000"
  },
  {
    "text": "of the instance they are um operating on and willing to maintain that themselves could remain uh with that approach But",
    "start": "1250000",
    "end": "1257600"
  },
  {
    "text": "aside from the maintenability on both side uh what it brought us as well the",
    "start": "1257600",
    "end": "1263120"
  },
  {
    "text": "the fact that we could manage those n those groups as part of the infrastructure uh group Uh it uh allowed",
    "start": "1263120",
    "end": "1269760"
  },
  {
    "text": "us to start introducing new instance categories for instance such as varying CPU architecture or instances with",
    "start": "1269760",
    "end": "1275960"
  },
  {
    "text": "accelerators And whilst nothing was technically preventing uh well users of",
    "start": "1275960",
    "end": "1281039"
  },
  {
    "text": "dedicated note groups from doing so these option were being made accessible without any kind of effort on their own",
    "start": "1281039",
    "end": "1287600"
  },
  {
    "text": "which made uh the overall approach uh quite appealing for everyone And on top of that if we were to go even deeper",
    "start": "1287600",
    "end": "1294400"
  },
  {
    "text": "than this we could example start using uh different hardware generation that allows us to figure out what's the most",
    "start": "1294400",
    "end": "1300240"
  },
  {
    "text": "cost effective instance types given a particular need right And all of that without interfering uh with the",
    "start": "1300240",
    "end": "1306559"
  },
  {
    "text": "application team's workflows which leads us to our third and last step uh in that process which",
    "start": "1306559",
    "end": "1313200"
  },
  {
    "text": "was we were now being able to focus on to how can we efficiently size our instance fleet and I'm not going to risk",
    "start": "1313200",
    "end": "1320480"
  },
  {
    "text": "myself at paraphrasing what our colleague Rahul and Ben did yesterday uh",
    "start": "1320480",
    "end": "1326000"
  },
  {
    "text": "in this exact room actually uh so I hope that the recording will be available soon but they dig into that uh",
    "start": "1326000",
    "end": "1332000"
  },
  {
    "text": "particular uh topic in quite some depth and of course there is much more",
    "start": "1332000",
    "end": "1337840"
  },
  {
    "text": "things to say about compute but uh we also have stuff to say about the other things so I'll continue with storage now",
    "start": "1337840",
    "end": "1345120"
  },
  {
    "text": "similarly here uh there is a few kubernetes primitives that we can relate to uh when we talk about storage uh as",
    "start": "1345120",
    "end": "1351280"
  },
  {
    "text": "an abstraction in kubernetes what we are mainly referring to usually is how can we provide uh our reports with access to",
    "start": "1351280",
    "end": "1358000"
  },
  {
    "text": "block uh to block devices And when you operate in the cloud there are many two ways you can have those uh block devices",
    "start": "1358000",
    "end": "1364960"
  },
  {
    "text": "Either they are local to the instances that the pods are being scheduled on or they are remotely managed by the cloud",
    "start": "1364960",
    "end": "1370400"
  },
  {
    "text": "providers and that way they can follow up the life cycle of your application if the pod moves from one node to the other",
    "start": "1370400",
    "end": "1377360"
  },
  {
    "text": "Thankfully here containers have been around for a while now and the community has been looking into ways to standardize how can we uh approach this",
    "start": "1377360",
    "end": "1384480"
  },
  {
    "text": "from an even broader perspective than kubernetes actually and thanks to the container storage interface now we can",
    "start": "1384480",
    "end": "1391120"
  },
  {
    "text": "uh rely on providers to implement drivers that satisfies the spec of uh the standardization and the great thing",
    "start": "1391120",
    "end": "1398480"
  },
  {
    "text": "with that is that uh we get out of the box support for most of the well all of the providers that in our case we were",
    "start": "1398480",
    "end": "1404320"
  },
  {
    "text": "looking for but as interestingly as as well We have more cap operational capabilities such as well thanks to",
    "start": "1404320",
    "end": "1410559"
  },
  {
    "text": "additional uh abstractions uh we can uh potentially snapshot uh the",
    "start": "1410559",
    "end": "1416200"
  },
  {
    "text": "volumes On the other hand uh what we've discovered along the way is that managing block devices among multiple",
    "start": "1416200",
    "end": "1422400"
  },
  {
    "text": "cloud provider is uh quite challenges Uh it has a whole set of challenges First",
    "start": "1422400",
    "end": "1427600"
  },
  {
    "text": "of all the offering is uh often quite inconsistent from one another uh which means that uh it's very hard to offer a",
    "start": "1427600",
    "end": "1434880"
  },
  {
    "text": "unified user experience uh among them all Um on top of that uh as I was saying",
    "start": "1434880",
    "end": "1441120"
  },
  {
    "text": "each uh provider is building their own is maintaining their own driver So maturity can largely differ from one",
    "start": "1441120",
    "end": "1447919"
  },
  {
    "text": "another So same here in terms of scalability we do not necessarily have the same experience from one provider to",
    "start": "1447919",
    "end": "1453840"
  },
  {
    "text": "the other And last but not least uh migrating or any kind of migration effort is often an afterthought uh",
    "start": "1453840",
    "end": "1460080"
  },
  {
    "text": "whenever there is a new feature coming up or whenever there is an an update that needs to go through uh quite often",
    "start": "1460080",
    "end": "1465840"
  },
  {
    "text": "it involves a lot of manual effort to uh to get it through On top of that uh if",
    "start": "1465840",
    "end": "1471679"
  },
  {
    "text": "we jump back early uh 2018 uh CSI support in Kubernetes was uh not a thing yet So that means that back then",
    "start": "1471679",
    "end": "1479520"
  },
  {
    "text": "Kubernetes was uh implementing those abstraction on its own way uh which eventually led us to have to migrate",
    "start": "1479520",
    "end": "1486159"
  },
  {
    "text": "towards the CSI way uh a bit later And here as well if you're interested into learning more about this our colleagues",
    "start": "1486159",
    "end": "1493039"
  },
  {
    "text": "Antoan and Batis did an amazing talk last year at CubeCon where they talk about the intricacies of uh this uh",
    "start": "1493039",
    "end": "1501000"
  },
  {
    "text": "migration And finally uh what about network uh here as well multiple facets",
    "start": "1501000",
    "end": "1506720"
  },
  {
    "text": "to comprehend but most importantly when we talk about networking in communities uh at least at data log what we were",
    "start": "1506720",
    "end": "1512799"
  },
  {
    "text": "looking for was to oop sorry to have our pods being able to communicate with each other whether they were uh being",
    "start": "1512799",
    "end": "1518880"
  },
  {
    "text": "scheduled on the same cluster or among different clusters and in more generally",
    "start": "1518880",
    "end": "1524880"
  },
  {
    "text": "uh we were looking for our ports to have the possibility to access any kind of uh entities on our network So at the time",
    "start": "1524880",
    "end": "1533919"
  },
  {
    "text": "it was very common to use overlay networks to do so Uh but uh ourselves at",
    "start": "1533919",
    "end": "1539200"
  },
  {
    "text": "data dog uh we decided to not go that towards that path and we went straight away with addressing our pods uh",
    "start": "1539200",
    "end": "1545440"
  },
  {
    "text": "natively over uh each cloud provider's uh respective software defined networks",
    "start": "1545440",
    "end": "1550880"
  },
  {
    "text": "and the main reason behind this is that we were looking to avoid uh the performance overhead that uh we would",
    "start": "1550880",
    "end": "1557120"
  },
  {
    "text": "have got by having our packets packets flowing through a tunnel uh we also felt",
    "start": "1557120",
    "end": "1562960"
  },
  {
    "text": "that uh in general it would be more reliable because there would be less moving pieces in our data path and last",
    "start": "1562960",
    "end": "1569039"
  },
  {
    "text": "but not least uh as I mentioned before we were looking to have connectivity between two pods from one cluster to",
    "start": "1569039",
    "end": "1575279"
  },
  {
    "text": "another and that's what we could get as uh every single pod had an IP on the underlying network they could have cross",
    "start": "1575279",
    "end": "1581919"
  },
  {
    "text": "network connectivity on the challenging part though uh something that we've been",
    "start": "1581919",
    "end": "1587600"
  },
  {
    "text": "facing from the beginning and that we've been uh continuously uh oh sorry no",
    "start": "1587600",
    "end": "1593919"
  },
  {
    "text": "getting ahead of myself Uh the uh main challenge when we implemented that part was that we had to deeply uh integrate",
    "start": "1593919",
    "end": "1600480"
  },
  {
    "text": "with each of the um cloud providers uh SDN implementation and back then uh the",
    "start": "1600480",
    "end": "1607279"
  },
  {
    "text": "ecosystem in 2018 in terms of uh network plugins was uh fairly limited compared",
    "start": "1607279",
    "end": "1612799"
  },
  {
    "text": "to today uh and so we had to use two different approaches uh and plugins",
    "start": "1612799",
    "end": "1618000"
  },
  {
    "text": "depending on the provider we were uh running on and this is why uh when we started looking into working onto Asia",
    "start": "1618000",
    "end": "1624720"
  },
  {
    "text": "we decided to shift towards a more unified approach and we picked Celium for that purpose what it brought us here",
    "start": "1624720",
    "end": "1631919"
  },
  {
    "text": "as well was the out of the box support for all the providers we were looking for more capabilities such as network",
    "start": "1631919",
    "end": "1637360"
  },
  {
    "text": "policy enforcement better routing uh thanks to the BPF perks and the replacement of cube proxy and also the",
    "start": "1637360",
    "end": "1643440"
  },
  {
    "text": "possibility to collaborate with an amazing open source community and this is what I was looking to get to before",
    "start": "1643440",
    "end": "1649679"
  },
  {
    "text": "Uh the challenge back then and that we are still looking into today mostly is the scalability one Uh the scale at",
    "start": "1649679",
    "end": "1656320"
  },
  {
    "text": "which we operate really leads us towards uh yeah the intricacies of the solutions So yeah whether it's within a cluster or",
    "start": "1656320",
    "end": "1662000"
  },
  {
    "text": "among clusters that's uh what we aim to solve And if you are here as well interested into the matter uh we have uh",
    "start": "1662000",
    "end": "1668480"
  },
  {
    "text": "made a talk with Amos and myself a couple of years ago at CDMCON where we uh dug into the matter and with that",
    "start": "1668480",
    "end": "1674960"
  },
  {
    "text": "I'll leave it back to R So um Maxim told us about um how we",
    "start": "1674960",
    "end": "1682640"
  },
  {
    "text": "could abstract compute storage and and networking um and the challenges associ",
    "start": "1682640",
    "end": "1688640"
  },
  {
    "text": "associated with that But of course uh we we need we also need higher level primitives right and cloud providers",
    "start": "1688640",
    "end": "1694559"
  },
  {
    "text": "have a lot of high level offerings high level services and we have a few on this slides However because we wanted to run",
    "start": "1694559",
    "end": "1701200"
  },
  {
    "text": "on multiple providers we had additional constraints right um we only could we could only consider uh high level",
    "start": "1701200",
    "end": "1708159"
  },
  {
    "text": "services with common API across providers posgress being uh one for instance It had to support our scale and",
    "start": "1708159",
    "end": "1715279"
  },
  {
    "text": "also last but not least it has to be it had to be efficient in terms of cost because we run a pretty significant footprint",
    "start": "1715279",
    "end": "1722480"
  },
  {
    "text": "Based on this constraint uh we picked a few a few services right we picked RDS for posgress elasticasheach for radius",
    "start": "1722480",
    "end": "1729840"
  },
  {
    "text": "uh and load balances and and object storage however and very similar to the",
    "start": "1729840",
    "end": "1735120"
  },
  {
    "text": "other lower level offering um there are there are similar offerings between providers but they are slightly",
    "start": "1735120",
    "end": "1741880"
  },
  {
    "text": "different which means after a few years uh our decisions have evolved on on this",
    "start": "1741880",
    "end": "1746960"
  },
  {
    "text": "topic so the first one is in terms of object storage uh we use S3 GC s and",
    "start": "1746960",
    "end": "1752399"
  },
  {
    "text": "blob storage and we're very happy with all of them right I mean they are very similar they have very similar",
    "start": "1752399",
    "end": "1757840"
  },
  {
    "text": "interfaces and behaviors and we're we're very happy with them and we keep using",
    "start": "1757840",
    "end": "1763000"
  },
  {
    "text": "them in terms of load balances uh it would seem that load balancer are similar enough but it turns out if you",
    "start": "1763000",
    "end": "1769600"
  },
  {
    "text": "look at L7 balancing there are actually a lot of differences between providers right and that's why over the years",
    "start": "1769600",
    "end": "1775840"
  },
  {
    "text": "we've actually uh went down a layer and we're now only considering layer four load balancing and Anything that has to",
    "start": "1775840",
    "end": "1782080"
  },
  {
    "text": "do with L7 properties we actually manage ourself with with envoy and and finally for database and",
    "start": "1782080",
    "end": "1788960"
  },
  {
    "text": "and I'm saying I'm posgress and ready here um we started with manage offering we starting with RDS and cloud SQL and",
    "start": "1788960",
    "end": "1796080"
  },
  {
    "text": "and it was working right but in the end uh after some time we had too many uh too many small issues where the",
    "start": "1796080",
    "end": "1802559"
  },
  {
    "text": "behaviors of the different providers were um different enough that it was starting to be painful because while in",
    "start": "1802559",
    "end": "1809679"
  },
  {
    "text": "p in in in theory It was supposed to be similar between providers and we provide an abstraction to our users All of them",
    "start": "1809679",
    "end": "1816080"
  },
  {
    "text": "had to know the exact differences between all the providers and the constraints of all the providers So because it became too painful we",
    "start": "1816080",
    "end": "1822399"
  },
  {
    "text": "actually implemented the ser the services ourself as a at a platform level We we learned a lot of of lessons",
    "start": "1822399",
    "end": "1830240"
  },
  {
    "text": "uh along the way right And one of the thing we want to say is that the challenges we face we talked a lot about the technical challenges but the",
    "start": "1830240",
    "end": "1836640"
  },
  {
    "text": "challenges are not only technical Um the first thing is well uh if you",
    "start": "1836640",
    "end": "1841919"
  },
  {
    "text": "want to run Kubernetes in multiple cloud providers at scale uh you have to develop expertise you have to understand",
    "start": "1841919",
    "end": "1847200"
  },
  {
    "text": "exactly how Kubernetes works and and these intricacies of the small issues you can get but you also have to",
    "start": "1847200",
    "end": "1852720"
  },
  {
    "text": "understand how the cloud providers work In addition uh you also very likely need",
    "start": "1852720",
    "end": "1858880"
  },
  {
    "text": "to build strong relationship with providers because well if you have if you encounter a performance issue or an",
    "start": "1858880",
    "end": "1863919"
  },
  {
    "text": "API issue you need to be able to to work with them And of course as you're offering a",
    "start": "1863919",
    "end": "1869440"
  },
  {
    "text": "platform to the rest of your company you need to be able to help them and train them to use the platform best",
    "start": "1869440",
    "end": "1875440"
  },
  {
    "text": "And and finally as always when you're building a platform uh we do the pretty good job at addressing uh 80% of the use",
    "start": "1875440",
    "end": "1881679"
  },
  {
    "text": "cases but we still have team with very specific use cases where the platform we offer is is not perfect yet but we we're",
    "start": "1881679",
    "end": "1887679"
  },
  {
    "text": "working on it and we're trying to iterate to support more and more use cases And yes I mean Kubernetes offer",
    "start": "1887679",
    "end": "1894320"
  },
  {
    "text": "pretty compelling abstraction but it has limits Uh it's a great way to describe",
    "start": "1894320",
    "end": "1900000"
  },
  {
    "text": "application deployments but it requires a lot of effort to make it work",
    "start": "1900000",
    "end": "1906480"
  },
  {
    "text": "And and remember I mean if you run on uh on on a single even on a single cloud but even it's even more true on multiple",
    "start": "1906480",
    "end": "1911919"
  },
  {
    "text": "cloud is as soon as you start pushing the limits the underlying uh the",
    "start": "1911919",
    "end": "1917039"
  },
  {
    "text": "underlying implementation is going to leak and you will have to understand it Ideally you'd have only a few people uh",
    "start": "1917039",
    "end": "1922799"
  },
  {
    "text": "that have to understand it but you you still have to understand it And and finally something that's",
    "start": "1922799",
    "end": "1928480"
  },
  {
    "text": "important too is while managed services are getting more and more mature what's happening is providers are trying to",
    "start": "1928480",
    "end": "1934640"
  },
  {
    "text": "make their offering the best And so a lot of the things that happening there are are not shared with the community than and is happening internally And",
    "start": "1934640",
    "end": "1941360"
  },
  {
    "text": "it's much better But it mean that uh the behavior is becoming slightly more and more different between different",
    "start": "1941360",
    "end": "1946919"
  },
  {
    "text": "providers In terms of perspective there are a few things where uh we're thinking about uh we mentioned using it manage",
    "start": "1946919",
    "end": "1953200"
  },
  {
    "text": "kubernetes clusters for uh control plane We have a few other use cases that where it's the use cases are specific enough",
    "start": "1953200",
    "end": "1959760"
  },
  {
    "text": "and unique enough where it makes sense for us to make the effort to adapt applications to multiple manage offerings So we're looking into that",
    "start": "1959760",
    "end": "1966159"
  },
  {
    "text": "We're also looking at new abstraction and capabilities Uh Maxi mentioned GPU earlier of course and GPU is a topic and",
    "start": "1966159",
    "end": "1972080"
  },
  {
    "text": "we have to support GPU for our users and and finally I mean we also have to",
    "start": "1972080",
    "end": "1977760"
  },
  {
    "text": "improve our efficiency because we need to control our costs Final thoughts I mean we we talked a lot",
    "start": "1977760",
    "end": "1984399"
  },
  {
    "text": "about how it was painful with Kubernetes and it's suddenly challenging but without it it would definitely had been",
    "start": "1984399",
    "end": "1990240"
  },
  {
    "text": "uh much harder and and we learned a lot along the way And and on this uh on this slide uh",
    "start": "1990240",
    "end": "1997360"
  },
  {
    "text": "uh thank we want to to thank you all and of course we won't have time for question because we're already a bit late but we'll be around if you have",
    "start": "1997360",
    "end": "2004000"
  },
  {
    "text": "questions and we're and you can reach uh us on on on Slack too Thank you",
    "start": "2004000",
    "end": "2010440"
  }
]