[
  {
    "text": "hey everyone uh so we're going to go ahead and get started so if you can uh find a seat or if you're coming in uh we",
    "start": "80",
    "end": "6359"
  },
  {
    "text": "we only have uh 35 minutes here so we've got five panelists and uh going to try",
    "start": "6359",
    "end": "12120"
  },
  {
    "text": "to get through this quickly so first we're going to do a round of introductions um and then we're going to",
    "start": "12120",
    "end": "18039"
  },
  {
    "text": "get into the panel so if anyone wants to take a minute to introduce themselves hello everyone I'm tanim Ibrahim I'm a",
    "start": "18039",
    "end": "24000"
  },
  {
    "text": "senior engineering manager at Red Hat open shift AI engineering team My Team Works in uh kerf Community keep",
    "start": "24000",
    "end": "30199"
  },
  {
    "text": "Community uh we also work in truste Ai and model registry so those are the aspects uh we working pretty closely",
    "start": "30199",
    "end": "37680"
  },
  {
    "text": "pass it on to Tess hi everyone I'm Tessa Fam I'm a",
    "start": "37680",
    "end": "44160"
  },
  {
    "text": "senior Sol engineer on the AI infant team at Bloomberg and I'm also a contributor of kerf um my team works on",
    "start": "44160",
    "end": "52760"
  },
  {
    "text": "building out and maintaining the INF platform for Bloomberg's data scientists and engineers and our TL our team lead",
    "start": "52760",
    "end": "61399"
  },
  {
    "text": "Dan Sun uh co-founded kaser back in 2019 and since then as a team we have made",
    "start": "61399",
    "end": "67560"
  },
  {
    "text": "contributions to kerve as well as integrated it into the inference platform um and other internal projects",
    "start": "67560",
    "end": "75400"
  },
  {
    "text": "and um yeah that's it for me hi everyone I'm Jon George uh",
    "start": "75400",
    "end": "83360"
  },
  {
    "text": "technical director in new tanic AI or um so uh I wear two hats in new tanic",
    "start": "83360",
    "end": "90600"
  },
  {
    "text": "uh day-to-day life I lead all AI activities within nutanix in open source",
    "start": "90600",
    "end": "98200"
  },
  {
    "text": "um I lead a couple of ml communities uh qlow ml Commons uh if you're not aware",
    "start": "98200",
    "end": "104520"
  },
  {
    "text": "qlow is mlops platform on kubernetes uh I am part of the qlow steering committee",
    "start": "104520",
    "end": "110200"
  },
  {
    "text": "uh in ml Commons we co-founded uh ml common storage working group which is looking at the storage impact of ml work",
    "start": "110200",
    "end": "117479"
  },
  {
    "text": "ml workl during ml training thank you I have a mic so I'm Andre monu",
    "start": "117479",
    "end": "122520"
  },
  {
    "text": "I'm AML product manager at canonical the publisher of Ubuntu and I I work I look",
    "start": "122520",
    "end": "127880"
  },
  {
    "text": "after our ml portfolio which includes cow kerve that has already been mentioned as well as a couple of other",
    "start": "127880",
    "end": "133680"
  },
  {
    "text": "tools such as ml flow for example as well as data science stock that's designed to help developers get started",
    "start": "133680",
    "end": "141080"
  },
  {
    "text": "very quickly on the nuun to workstation Y and uh I'm Adam telman I'm a principal product architect from",
    "start": "141080",
    "end": "147519"
  },
  {
    "text": "Nvidia uh I haven't haven't contributed to nearly as many projects as the other folks on the panel here but i' I've been",
    "start": "147519",
    "end": "153760"
  },
  {
    "text": "using all of them for many many years now so I I use Cube flow I use kerve and",
    "start": "153760",
    "end": "159280"
  },
  {
    "text": "all of the dependent projects on it and the reason we're doing this panel is because recently Nvidia launched a new",
    "start": "159280",
    "end": "165680"
  },
  {
    "text": "AI application product and I thought that the best way to get it to Market and get it in front of people was to",
    "start": "165680",
    "end": "171760"
  },
  {
    "text": "make sure it worked really well on kerve and the open source project so I've been working with with everyone on here very",
    "start": "171760",
    "end": "177599"
  },
  {
    "text": "closely to make that happen and uh this panel is sort of just a journey of what is it like to do things in the open",
    "start": "177599",
    "end": "183840"
  },
  {
    "text": "source with the community uh so the first thing we want to talk about in this panel uh sort of",
    "start": "183840",
    "end": "191040"
  },
  {
    "text": "going off of that is is what what is the current state and uh the the question I think we're going to go uh this way here",
    "start": "191040",
    "end": "197480"
  },
  {
    "text": "so the question for you Andrea uh what what has your experience been uh working with me and working with",
    "start": "197480",
    "end": "203920"
  },
  {
    "text": "the community what what challenges have you faced what accomplishments have you faced and particularly in the realm of",
    "start": "203920",
    "end": "210959"
  },
  {
    "text": "AI llms gen a lot of this new stuff coming out what what has the benefit been to you your company and the the",
    "start": "210959",
    "end": "217640"
  },
  {
    "text": "customers that you work with I I've been active in the open source uh for quite",
    "start": "217640",
    "end": "223040"
  },
  {
    "text": "some time now and if I look back the biggest challenge that I had initially was how to get started especially if you",
    "start": "223040",
    "end": "229360"
  },
  {
    "text": "want to contribute how do you actually contribute it feels that there are a lot of very good developers but everyone has",
    "start": "229360",
    "end": "236200"
  },
  {
    "text": "and knows what they need to do and how to do things um um so that was the initial Challenge on how to get started",
    "start": "236200",
    "end": "242720"
  },
  {
    "text": "to be a contributor within one community and that's something that I've noticed a lot of the cube flow of or the new",
    "start": "242720",
    "end": "249280"
  },
  {
    "text": "joiners in cupo struggle with as well especially since it's it's a large platform it's it's not there very easy",
    "start": "249280",
    "end": "255760"
  },
  {
    "text": "to get started with initially so it's often intimidating um and that's where I think",
    "start": "255760",
    "end": "262960"
  },
  {
    "text": "I've enjoyed working with other developers and I enjoyed working especially with people who are getting",
    "start": "262960",
    "end": "268919"
  },
  {
    "text": "started in the ml space as well as in the open source space to lower the barar entry to help them really feel",
    "start": "268919",
    "end": "275240"
  },
  {
    "text": "comfortable raising bugs solving issues asking questions improving documentation all the small things that you can do and",
    "start": "275240",
    "end": "280919"
  },
  {
    "text": "furthermore also writing projects I think that having such a powerful platform as cube flow without projects",
    "start": "280919",
    "end": "287039"
  },
  {
    "text": "that can help you get started build your first model find you an llm or run names",
    "start": "287039",
    "end": "293440"
  },
  {
    "text": "together with kerve and qflow those are the things that are just lowering the Brier engine helps people get started",
    "start": "293440",
    "end": "301199"
  },
  {
    "text": "right myself I was uh part of queso from the very very start um so I'm so glad to",
    "start": "302039",
    "end": "308960"
  },
  {
    "text": "see this community grow from um very few folks to this scale",
    "start": "308960",
    "end": "315840"
  },
  {
    "text": "right and U thanks to all developers it's all started with training tensorflow models um in Q flow to",
    "start": "315840",
    "end": "323840"
  },
  {
    "text": "serving same T of law model using TF serving so from that it has grown to a",
    "start": "323840",
    "end": "329319"
  },
  {
    "text": "big framework which can do all these large language models at scale uh in any",
    "start": "329319",
    "end": "334960"
  },
  {
    "text": "infra right so um I'm um recently we announced new tchic Enterprise AI which",
    "start": "334960",
    "end": "341800"
  },
  {
    "text": "has um Cas of at it score uh to to provide a oneclick deployment of large",
    "start": "341800",
    "end": "349240"
  },
  {
    "text": "language models of your choice right at scale without knowing anything about underlying assistant infra so this is",
    "start": "349240",
    "end": "355280"
  },
  {
    "text": "not possible without a project like this where it it is a group work that we have done as part of the ques of community so",
    "start": "355280",
    "end": "362479"
  },
  {
    "text": "if you really think about ques of it is a it is a it's a group of Lego blocks together right like many things coming",
    "start": "362479",
    "end": "369440"
  },
  {
    "text": "together people contribute different items and then it gets into a single",
    "start": "369440",
    "end": "374479"
  },
  {
    "text": "project so the all the different blocks if you really see the front facing uh",
    "start": "374479",
    "end": "380520"
  },
  {
    "text": "API which is part of the open INF frence protocol so we for uh reason we have put",
    "start": "380520",
    "end": "387039"
  },
  {
    "text": "that outside case Ser because we wanted inference protocol to be used by other communities as well so for example WR a",
    "start": "387039",
    "end": "393840"
  },
  {
    "text": "to Ser uh Nvidia TR and everyone uses the same inference protocol right and Cas of implements exactly the same right",
    "start": "393840",
    "end": "401479"
  },
  {
    "text": "now the next na is the about the inference run times where users can bring any runtime of your choice and",
    "start": "401479",
    "end": "408680"
  },
  {
    "text": "plug it into ker without uh any code change right be it Nvidia Nim DGI VM of",
    "start": "408680",
    "end": "416879"
  },
  {
    "text": "or any runtime of your choice without just buy having a yaml you have your uh",
    "start": "416879",
    "end": "422319"
  },
  {
    "text": "runtime added 2K so and the at the end the same the whole stack can be run on",
    "start": "422319",
    "end": "427360"
  },
  {
    "text": "any hardware right be it gpus be it uh CPUs AMX accelerated or AMD gpus",
    "start": "427360",
    "end": "434400"
  },
  {
    "text": "whatever it is right so in a way it is it is a group project it is it's a community product which can bring all",
    "start": "434400",
    "end": "441120"
  },
  {
    "text": "these Lego blocks together which is a core strength of K of oh thanks John um I would like to",
    "start": "441120",
    "end": "448919"
  },
  {
    "text": "start off by just speaking from my personal experience so I made my first contribution to kerve back in 2022 when",
    "start": "448919",
    "end": "456599"
  },
  {
    "text": "I shortly after I joined the infant team at Bloomberg and what I found from",
    "start": "456599",
    "end": "461800"
  },
  {
    "text": "multiple PRS that I've contributed to kerve is that the whole process is",
    "start": "461800",
    "end": "467039"
  },
  {
    "text": "pleasantly I was pleasantly surprised by how easy it was to make contributions and for the contribution to count",
    "start": "467039",
    "end": "473520"
  },
  {
    "text": "towards um like the work that I do internally and also help other people at",
    "start": "473520",
    "end": "478960"
  },
  {
    "text": "other companies who are facing the same problems um and it's super easy to just",
    "start": "478960",
    "end": "484520"
  },
  {
    "text": "identify issues because we all are here and we all know that there are a lot of common challenges and we're all try to",
    "start": "484520",
    "end": "491560"
  },
  {
    "text": "tackle um and it's very easy to raise issues and come up with your own Solutions propose it to the wider kerve",
    "start": "491560",
    "end": "498759"
  },
  {
    "text": "community and get your PR uh reviewed uh get feedback and get it merged and then",
    "start": "498759",
    "end": "505240"
  },
  {
    "text": "the moment that your change is in then that will solve the issue for your your Enterprise Products solve the issue for",
    "start": "505240",
    "end": "512360"
  },
  {
    "text": "many people out there that you don't even know so I think that is the strength of the open source Community um",
    "start": "512360",
    "end": "519159"
  },
  {
    "text": "that I have experienced like personally um and talking about the the unique",
    "start": "519159",
    "end": "525120"
  },
  {
    "text": "benefits of being in the open source Community there are countless and youve",
    "start": "525120",
    "end": "530640"
  },
  {
    "text": "got to start first by uh recognizing that all of this comes to us for free",
    "start": "530640",
    "end": "536200"
  },
  {
    "text": "and this we are getting the quickest crowdsourced solution at no cost um and",
    "start": "536200",
    "end": "543680"
  },
  {
    "text": "from The Wider community and that's like the very first benefit that you don't get from an Enterprise product where you",
    "start": "543680",
    "end": "549560"
  },
  {
    "text": "have to pay and it's",
    "start": "549560",
    "end": "553360"
  },
  {
    "text": "vendorization going on a lot of Innovations every day people coming up with Solutions people coming together to",
    "start": "557640",
    "end": "564880"
  },
  {
    "text": "discuss things and what's even cooler is that you are collaborating with people",
    "start": "564880",
    "end": "570120"
  },
  {
    "text": "from across companies that you have never met before and you don't get to CH you don't get the chance to work with if",
    "start": "570120",
    "end": "576160"
  },
  {
    "text": "you just work within your company um and uh finally just leveraging the resource",
    "start": "576160",
    "end": "583800"
  },
  {
    "text": "of The Wider community and the strength of uh people coming together for nothing",
    "start": "583800",
    "end": "589760"
  },
  {
    "text": "else but wanting to find solutions to the problems that they are facing and so other people are facing um you will find",
    "start": "589760",
    "end": "596440"
  },
  {
    "text": "much um quicker Solutions and more efficient um and yeah as you give back",
    "start": "596440",
    "end": "603560"
  },
  {
    "text": "to the community you also get back the same um so that those are the uh very unique um benefits that anyone would get",
    "start": "603560",
    "end": "611399"
  },
  {
    "text": "from open source you know be it like inference AI or anything open source um",
    "start": "611399",
    "end": "617279"
  },
  {
    "text": "but at the same time we also have have to recognize the challenges um in this and I want to talk about challenges that",
    "start": "617279",
    "end": "623920"
  },
  {
    "text": "are specific to AI so aside from all the challenges that we all face here um with",
    "start": "623920",
    "end": "629120"
  },
  {
    "text": "the the boom of Ls in the recent years and the models is getting bigger and bigger we all want to optimize resources",
    "start": "629120",
    "end": "636680"
  },
  {
    "text": "and um and time model startup time and all of that um and uh as a community we",
    "start": "636680",
    "end": "643920"
  },
  {
    "text": "all work together and are working on Solutions and designing solutions to tackle those problems but as uh for kerf",
    "start": "643920",
    "end": "652680"
  },
  {
    "text": "specifically um in order to make the project great we actually have to tackle all of these areas all together and each",
    "start": "652680",
    "end": "659680"
  },
  {
    "text": "of this area requires a lot of deep expertise um and that's why every day",
    "start": "659680",
    "end": "665000"
  },
  {
    "text": "we're working with different companies different partners and other projects to make these Solutions um happen so yeah",
    "start": "665000",
    "end": "673360"
  },
  {
    "text": "that's my experience thank you Tess So speaking of",
    "start": "673360",
    "end": "678639"
  },
  {
    "text": "like Tess was alluding to community collaboration power of source right so if you think about the WG serving",
    "start": "678639",
    "end": "685720"
  },
  {
    "text": "community that we have started recently uh one of the big recent success is more",
    "start": "685720",
    "end": "690959"
  },
  {
    "text": "than 250 members have joined that community so this is umbrella project to tackle the the challenges that llm",
    "start": "690959",
    "end": "698079"
  },
  {
    "text": "inferencing brings and how do we solve it in a cloud native way in criminalities working across many many",
    "start": "698079",
    "end": "704600"
  },
  {
    "text": "many companies right so it's a big big happy family um another interesting uh",
    "start": "704600",
    "end": "710040"
  },
  {
    "text": "area out of that is uh is the Gateway project that is um our friends from Bloomberg and others have been uh",
    "start": "710040",
    "end": "716519"
  },
  {
    "text": "contributing closely because one of the challenges we fa with large models is also uh being able to Route them",
    "start": "716519",
    "end": "723320"
  },
  {
    "text": "efficiently how do you know which model you want to route to sometimes you can have slos like cost you may some models",
    "start": "723320",
    "end": "730200"
  },
  {
    "text": "may work really well with inferencing some mod may work really well with CPU some models may work really well with",
    "start": "730200",
    "end": "735680"
  },
  {
    "text": "gpus and so you have different types of um accelerators that you need to know and the llm instance Gateway project",
    "start": "735680",
    "end": "742519"
  },
  {
    "text": "that uh we are working closely with kerve Community together is one way to help solve that right um there's a all",
    "start": "742519",
    "end": "749279"
  },
  {
    "text": "kinds of newer metrics that you have to think for for language models so if you come from traditional predictive ml model life cycle normally you have CPU",
    "start": "749279",
    "end": "756480"
  },
  {
    "text": "request uh you have U how many threads are being hit those are the common metrics you look at but when it comes to",
    "start": "756480",
    "end": "762480"
  },
  {
    "text": "language models you have to think of like KV cache utilization you have to think of batch size number of tokens",
    "start": "762480",
    "end": "767839"
  },
  {
    "text": "being cued all those type of metrics are new to the space for model serving or inferencing workload right so we're",
    "start": "767839",
    "end": "774680"
  },
  {
    "text": "using the same set of tools that are already there in ku's ecosystem but we",
    "start": "774680",
    "end": "779720"
  },
  {
    "text": "are implementing them for a new type of workload uh and doing it at scale um and",
    "start": "779720",
    "end": "785360"
  },
  {
    "text": "I would also say like another really cool success story recently was one of the projects uh we have readed had have",
    "start": "785360",
    "end": "790839"
  },
  {
    "text": "been working closely on is Trusty Ai and responsible AI Concepts so kerf has an",
    "start": "790839",
    "end": "796720"
  },
  {
    "text": "implementation called explain so where you can actually hook up a explainer model to a kerf endpoint and actually be",
    "start": "796720",
    "end": "803800"
  },
  {
    "text": "able to run various um responsible algorithms like lime sha counterfactuals to EXP explain that uh so we are working",
    "start": "803800",
    "end": "811839"
  },
  {
    "text": "on adding those types of uh um algorithms for large language models as",
    "start": "811839",
    "end": "817760"
  },
  {
    "text": "well as for different types of evaluation like lmol so that you can actually evaluate your model really well",
    "start": "817760",
    "end": "823880"
  },
  {
    "text": "so currently that works with VM with kerf uh together uh so that has been a really",
    "start": "823880",
    "end": "829720"
  },
  {
    "text": "interesting project to work on because for example if you if the models are getting bigger and bigger but how do you",
    "start": "829720",
    "end": "835040"
  },
  {
    "text": "know if your model is actually doing a good job like just because you have a 4 and5 billion parameter model doesn't",
    "start": "835040",
    "end": "840160"
  },
  {
    "text": "mean that the model is going to perform what you need for your work your work may require very domain specific task",
    "start": "840160",
    "end": "846560"
  },
  {
    "text": "maybe you are a Service Company who has support cases right and you want to do a case summarization uh and you may not",
    "start": "846560",
    "end": "853000"
  },
  {
    "text": "need a model that size so LM eval like tools can help you evaluate those models for you your specific use cases using LM",
    "start": "853000",
    "end": "860000"
  },
  {
    "text": "harness toolkits so that has been a really good success story um I would also say like the recently the community",
    "start": "860000",
    "end": "865480"
  },
  {
    "text": "got together on um how do we support really really large models with uh",
    "start": "865480",
    "end": "870600"
  },
  {
    "text": "stateful uh pods um and and and and working across uh the various",
    "start": "870600",
    "end": "876240"
  },
  {
    "text": "implementations we have looked at leader work set we have looked at stateless deployments we have looked at saful pod sets and how do we actually make sure",
    "start": "876240",
    "end": "883320"
  },
  {
    "text": "that these models can be deployed across multiple nodes because some of these large models obviously doesn't fit in",
    "start": "883320",
    "end": "888720"
  },
  {
    "text": "one node anymore uh that is has been a really interesting challenge that I um the community coll collaboratively",
    "start": "888720",
    "end": "894759"
  },
  {
    "text": "worked on to solve and the pr recently got marged as well",
    "start": "894759",
    "end": "900360"
  },
  {
    "text": "thank you for that yeah and I I uh kind of to to your uh Point Tess I I remember",
    "start": "900360",
    "end": "905680"
  },
  {
    "text": "um before I was really deeply in the community I was uh using using tools using Cube flow and it must have been F",
    "start": "905680",
    "end": "912079"
  },
  {
    "text": "five or six years ago uh when I was trying to teach people how to to use djx and do a training of a big Bert model",
    "start": "912079",
    "end": "920040"
  },
  {
    "text": "and Jupiter wasn't enough because I needed to do pipelines and having Cube flow a thing I could just go to and it",
    "start": "920040",
    "end": "926680"
  },
  {
    "text": "sort of did everything and end I was able to make a workshop in like a week that showed people how to train an llm",
    "start": "926680",
    "end": "931959"
  },
  {
    "text": "that that was back back then when you know we llms weren't quite as big then",
    "start": "931959",
    "end": "937519"
  },
  {
    "text": "and so and sort of on the personal note it's it getting into this like getting to know all of you it's been fun it's",
    "start": "937519",
    "end": "943279"
  },
  {
    "text": "been fun for me I'm not just uh working with the folks from my company but I'm working with the folks from a lot of",
    "start": "943279",
    "end": "948519"
  },
  {
    "text": "other companies and that that there's some fun to that and then there's there's sort of a sharing of ideas right",
    "start": "948519",
    "end": "953920"
  },
  {
    "text": "so like I I see my world and what I'm doing but but when I'm in a meeting with with redhead or canonical or nanic or",
    "start": "953920",
    "end": "961319"
  },
  {
    "text": "Bloomberg I'm learning that you see the world in a slightly different way and all of those ways are valid and sort of",
    "start": "961319",
    "end": "966959"
  },
  {
    "text": "sharing of ideas we end up addressing kind of I think you guys were both alluding to there's a whole lot of use",
    "start": "966959",
    "end": "972880"
  },
  {
    "text": "cases out there and we need to solve all of them for someone and some of them I didn't realize they were problems until",
    "start": "972880",
    "end": "978600"
  },
  {
    "text": "you you you showed them to me when when you were going through the solutions and then I realized oh yeah caching at this",
    "start": "978600",
    "end": "984040"
  },
  {
    "text": "layer is going to be hugely important for me in 3 months or 6 months for me but but you're 2 months in the solving",
    "start": "984040",
    "end": "989880"
  },
  {
    "text": "and I sort of the piggy back off of it uh so so it's been a big win there I",
    "start": "989880",
    "end": "995519"
  },
  {
    "text": "think on the product side right uh so Nvidia came out with Nvidia Nim on like March and it was like our new AI product",
    "start": "995519",
    "end": "1002959"
  },
  {
    "text": "and uh when we were first running it it was a Docker run and then someone realized we needed to deploy it into production and so we built a Helm chart",
    "start": "1002959",
    "end": "1009920"
  },
  {
    "text": "for it and then when we go to go to our customers they're like that's great this is a Helm chart but I'm using this",
    "start": "1009920",
    "end": "1015279"
  },
  {
    "text": "platform or I'm using this platform or I'm using this platform and and that Helm chart isn't production",
    "start": "1015279",
    "end": "1020959"
  },
  {
    "text": "ready I care about more than just the application I care about all of these platform level features and me like the",
    "start": "1020959",
    "end": "1026720"
  },
  {
    "text": "thing I'm working on is an application I don't want to have to care about those platform features those I want I want to",
    "start": "1026720",
    "end": "1032438"
  },
  {
    "text": "plug into them but I don't want to to implement all of them and then lucky for me there's there's a whole community of people who are implementing them in",
    "start": "1032439",
    "end": "1038640"
  },
  {
    "text": "different ways and my customers are already your customers so so going into the kerve community working with you to",
    "start": "1038640",
    "end": "1045038"
  },
  {
    "text": "implement API standards around llms to implement um security and some of the caching things we've been working on and",
    "start": "1045039",
    "end": "1051480"
  },
  {
    "text": "then uh I did I did some work with red hat initially we're doing the initial PC",
    "start": "1051480",
    "end": "1056919"
  },
  {
    "text": "to get uh Nim working on kerve and and open shift Ai and then uh it worked on",
    "start": "1056919",
    "end": "1063000"
  },
  {
    "text": "nanx AI and then the charm Charmed kubernetes and all of the other platforms and so it it sort of just",
    "start": "1063000",
    "end": "1068720"
  },
  {
    "text": "worked now we're all benefiting from that I'm benefiting you're benefiting all of our joint customers are benefiting and so that kind of doing it",
    "start": "1068720",
    "end": "1075679"
  },
  {
    "text": "with the open source Community saved me a whole lot of time and then ended up being well fun and win-win-win for for",
    "start": "1075679",
    "end": "1082240"
  },
  {
    "text": "for for everyone so I've really enjoyed it I've really appreciated it I think uh",
    "start": "1082240",
    "end": "1087480"
  },
  {
    "text": "it it it's been a good it's been a good thing and just want to add all these",
    "start": "1087480",
    "end": "1093159"
  },
  {
    "text": "building blocks that Adam was talking about were defined by Community themselves right it's all Community ask",
    "start": "1093159",
    "end": "1099840"
  },
  {
    "text": "and Community features they it's contributed by Community yeah yeah it's all all in GitHub and um the the fun",
    "start": "1099840",
    "end": "1107240"
  },
  {
    "text": "thing is we're not done right so you you talked about work group serving we have that now and uh I looked at some of the",
    "start": "1107240",
    "end": "1114520"
  },
  {
    "text": "meeting notes and there's a big list of AI there's all of these new projects that are coming out but these projects have a new list of feature requests we",
    "start": "1114520",
    "end": "1120600"
  },
  {
    "text": "we have a lot of problems to solve and as a community I think we need to come",
    "start": "1120600",
    "end": "1126000"
  },
  {
    "text": "together because just looking around in cucon I hear people talking about how they're solving the same thing over and",
    "start": "1126000",
    "end": "1131720"
  },
  {
    "text": "over and over again in different ways and everyone wants to come together and solve at once so we can all benefit so",
    "start": "1131720",
    "end": "1137039"
  },
  {
    "text": "sort of with that in mind uh maybe going the the other direction what",
    "start": "1137039",
    "end": "1142600"
  },
  {
    "text": "um what future things uh are you excited about in open source are there any specific road map items that you'd like",
    "start": "1142600",
    "end": "1149280"
  },
  {
    "text": "to talk about any challenges that you're you're excited and anxious about and and",
    "start": "1149280",
    "end": "1154320"
  },
  {
    "text": "anything uh maybe in like the llm geni space that that you like to talk about and more importantly um how can people",
    "start": "1154320",
    "end": "1161080"
  },
  {
    "text": "get involved how can we help because we're we're all the community here so everyone in this room can can be on the panel next year to talk about how how",
    "start": "1161080",
    "end": "1167520"
  },
  {
    "text": "they had a big win that that helped everyone else awesome thanks Adam it's a great question so uh talking about WG",
    "start": "1167520",
    "end": "1174000"
  },
  {
    "text": "serving we need people uh there we have a lot of projects undertaking if you uh",
    "start": "1174000",
    "end": "1180559"
  },
  {
    "text": "if you're on the cncf slack which I hope you all are there's a WG serving Channel join the channel the moment you join the",
    "start": "1180559",
    "end": "1186400"
  },
  {
    "text": "channel you'll you'll see a bunch of uh sub working groups under the working groups uh from topics like LM instance",
    "start": "1186400",
    "end": "1194120"
  },
  {
    "text": "Gateway prompt c um LM cach project which is basically caching of prom s uh",
    "start": "1194120",
    "end": "1199520"
  },
  {
    "text": "there is a project around uh performance benchmarks so standardizing how you um",
    "start": "1199520",
    "end": "1204840"
  },
  {
    "text": "standardize the performance benchmarks for different llms and different run times that supports those llm model deployments and there's a subcommittee",
    "start": "1204840",
    "end": "1211799"
  },
  {
    "text": "around security if you're interested in that area there's a all kinds of fun exciting things that are happening and it can only happen back to your news",
    "start": "1211799",
    "end": "1217840"
  },
  {
    "text": "point if we get community to help us there's only a really there's so much",
    "start": "1217840",
    "end": "1223080"
  },
  {
    "text": "work there to do and so few people involved and so few and so scaling is going to be critical for us to do this",
    "start": "1223080",
    "end": "1229640"
  },
  {
    "text": "like what I want to see personally maybe next year we're talking not about how we",
    "start": "1229640",
    "end": "1235240"
  },
  {
    "text": "make inferencing work with kerve we're talking a layer above how we are applying agentic llm Frameworks",
    "start": "1235240",
    "end": "1241440"
  },
  {
    "text": "orchestration with kerve right uh because inferencing INF optimization maybe we have gotten that problem solved",
    "start": "1241440",
    "end": "1248520"
  },
  {
    "text": "to an extent where that is no longer a concern that we have to deal with right so in order to get there we need a lot",
    "start": "1248520",
    "end": "1254679"
  },
  {
    "text": "of support a lot of help from everyone in the room and spread the word get to WG serving Channel get to the K serve",
    "start": "1254679",
    "end": "1261039"
  },
  {
    "text": "Channel help us um join the community uh so in terms of what I'm excited about red road map wise so um I'm really",
    "start": "1261039",
    "end": "1269200"
  },
  {
    "text": "excited about uh uh another project which is not exactly directly under under WG serving it's under WG batch /wg",
    "start": "1269200",
    "end": "1276640"
  },
  {
    "text": "device is the Dr project which is a dynamic resource allocator uh so if you're not familiar it just allows you",
    "start": "1276640",
    "end": "1283279"
  },
  {
    "text": "to uh allocate gpus for your pods in a claim way like like how you do PVC",
    "start": "1283279",
    "end": "1289840"
  },
  {
    "text": "claims for storage think of that kind of same concept but for gpus now the fun",
    "start": "1289840",
    "end": "1295360"
  },
  {
    "text": "thing or any accelerator really um and and the fun part about that is that you can actually utilize fractional gpus so",
    "start": "1295360",
    "end": "1302200"
  },
  {
    "text": "maybe your model is like a 2 billion parameter model or 1 billion parameter model because it's a very specialized model and you may not need the entire",
    "start": "1302200",
    "end": "1308919"
  },
  {
    "text": "GPU to be consumed for that right so Dr allows you to slice the um gpus in a way",
    "start": "1308919",
    "end": "1316080"
  },
  {
    "text": "where you can actually allocate based on what do you need the fraction you need right and then on top of that there",
    "start": "1316080",
    "end": "1321960"
  },
  {
    "text": "another project that has been recently launched is the insta slice which allows you to do Justin Time GPU slicing um on",
    "start": "1321960",
    "end": "1329480"
  },
  {
    "text": "top of the Dr so you have Dr underneath insta slies on top makes that uh",
    "start": "1329480",
    "end": "1334919"
  },
  {
    "text": "provisioning for Dr much much simpler and easier so I'm really excited to see that because I think once we have that",
    "start": "1334919",
    "end": "1341440"
  },
  {
    "text": "project uh get to stable status um and then we have kerve on top of it and we",
    "start": "1341440",
    "end": "1346760"
  },
  {
    "text": "can work with the VM Community close closely to see how we can allow the",
    "start": "1346760",
    "end": "1352440"
  },
  {
    "text": "developers who are using kerf today to allocate um SL those fractional gpus for",
    "start": "1352440",
    "end": "1357559"
  },
  {
    "text": "their models they're deploying through kerf uh that that is a really exciting project I'm looking forward to and I'm",
    "start": "1357559",
    "end": "1362600"
  },
  {
    "text": "also looking forward to how the LM Gateway project revolves around I think it's going to be a massive um topic for",
    "start": "1362600",
    "end": "1369200"
  },
  {
    "text": "all of us um once we get that figured out and how that routing Works through",
    "start": "1369200",
    "end": "1374640"
  },
  {
    "text": "kerve and have a abstraction layer on top that would also make the inference",
    "start": "1374640",
    "end": "1380360"
  },
  {
    "text": "optimization process uh really really uh efficient pass it on to Tess yeah uh",
    "start": "1380360",
    "end": "1387520"
  },
  {
    "text": "thanks tanim I think tanim really set the right theme for uh for this question",
    "start": "1387520",
    "end": "1393400"
  },
  {
    "text": "um and speaking more specifically about kerve um the common theme right now that",
    "start": "1393400",
    "end": "1399640"
  },
  {
    "text": "we're tackling is how to reduce cost how to reduce serving cost and make it more",
    "start": "1399640",
    "end": "1406480"
  },
  {
    "text": "efficient and that's what all the infant servers are um L infant servers are",
    "start": "1406480",
    "end": "1412039"
  },
  {
    "text": "trying to tackle right now and there's all these buz around GPU fraction um uh",
    "start": "1412039",
    "end": "1417840"
  },
  {
    "text": "and uh GB time slicing people are trying to find different solutions right and uh on the ker side uh we are we do have uh",
    "start": "1417840",
    "end": "1426960"
  },
  {
    "text": "multiple solutions that are currently underway or in progress uh or in design",
    "start": "1426960",
    "end": "1433440"
  },
  {
    "text": "um one of which is um I I can name a few so that we can uh get excited for um and",
    "start": "1433440",
    "end": "1439880"
  },
  {
    "text": "those are new features that we'll be introducing to kerve in within probably the next year um the first thing is uh",
    "start": "1439880",
    "end": "1447400"
  },
  {
    "text": "we want to as these models are all the a models are getting bigger and bigger we",
    "start": "1447400",
    "end": "1452799"
  },
  {
    "text": "need to reduce the model start time we need to reduce the latency and uh for that we have uh a design proposed for",
    "start": "1452799",
    "end": "1460400"
  },
  {
    "text": "model caching and um and it is currently in progress right now um we have a",
    "start": "1460400",
    "end": "1467480"
  },
  {
    "text": "couple team members from blo Bloomberg as well as uh other companies working on this um and Autos scaling is another",
    "start": "1467480",
    "end": "1474840"
  },
  {
    "text": "feature that uh the kaser community has been working very closely with kada the",
    "start": "1474840",
    "end": "1480159"
  },
  {
    "text": "kada community um kubernetes based uh event driven autoscaler so this is how",
    "start": "1480159",
    "end": "1486840"
  },
  {
    "text": "we are leveraging all the expertise from other areas uh other communities to um",
    "start": "1486840",
    "end": "1493039"
  },
  {
    "text": "come together and uh provide with the users out there with the best solution",
    "start": "1493039",
    "end": "1498760"
  },
  {
    "text": "um but yeah so kerve is uh working very closely with kada to um implement this",
    "start": "1498760",
    "end": "1504399"
  },
  {
    "text": "uh Autos skill feature um and uh yeah and another thing uh too is if you were",
    "start": "1504399",
    "end": "1511240"
  },
  {
    "text": "at the keynote uh this morning um and also tenim just mentioned uh is the AI",
    "start": "1511240",
    "end": "1516279"
  },
  {
    "text": "Gateway and um it's also an exciting feature that we look forward to um that",
    "start": "1516279",
    "end": "1521679"
  },
  {
    "text": "we want to uh have uh the a Gateway that is more efficient for MML for LM uh",
    "start": "1521679",
    "end": "1529240"
  },
  {
    "text": "workloads um yeah so those are some projects that are currently underway and",
    "start": "1529240",
    "end": "1535000"
  },
  {
    "text": "we really do need people um it's very exciting you will be working with um",
    "start": "1535000",
    "end": "1540200"
  },
  {
    "text": "experts from multiple different fields and uh yeah and it's it's an exciting",
    "start": "1540200",
    "end": "1545640"
  },
  {
    "text": "time to be working on infant stuff right now um and looking forward to um we are",
    "start": "1545640",
    "end": "1551039"
  },
  {
    "text": "also we want to tackle the challenge of prom caching um a lot of people are talking about prom caching uh at this",
    "start": "1551039",
    "end": "1557799"
  },
  {
    "text": "conference as well and uh that is something that we are starting to design a solution for um and uh yeah and",
    "start": "1557799",
    "end": "1565760"
  },
  {
    "text": "overall uh for the kerf community so those are just uh some exciting features and enhancements to kerf and infant",
    "start": "1565760",
    "end": "1572679"
  },
  {
    "text": "services that we want to give you a sneak peek uh sneak peek of um but overall uh as a community we really hope",
    "start": "1572679",
    "end": "1580399"
  },
  {
    "text": "to get more people involved in the community um again it is a growing",
    "start": "1580399",
    "end": "1586679"
  },
  {
    "text": "Community we have so many pick companies are getting involved right now um we want to get even more companies involved",
    "start": "1586679",
    "end": "1594320"
  },
  {
    "text": "in order to align on protocols together and all the companies to adopt these",
    "start": "1594320",
    "end": "1599360"
  },
  {
    "text": "common protocols so that um yeah so that everything can work for everyone um and",
    "start": "1599360",
    "end": "1606799"
  },
  {
    "text": "uh yeah just obviously we also uh face challenges um for example as different",
    "start": "1606799",
    "end": "1614320"
  },
  {
    "text": "companies come together we all have different priorities and if if we just show up to these Community meetings and",
    "start": "1614320",
    "end": "1620600"
  },
  {
    "text": "talk about uh all the needs and requirements from um yeah from everyone then we can find the right resources to",
    "start": "1620600",
    "end": "1627440"
  },
  {
    "text": "work on these uh different work streams um yeah and yeah and finally just uh",
    "start": "1627440",
    "end": "1634720"
  },
  {
    "text": "yeah just try to get involved in open source um and",
    "start": "1634720",
    "end": "1639840"
  },
  {
    "text": "uh yeah and I know we we have a lot of exciting projects um to to work on so",
    "start": "1639840",
    "end": "1647720"
  },
  {
    "text": "lot of project it's a recurring theme here yes um so uh about the future so",
    "start": "1647720",
    "end": "1655720"
  },
  {
    "text": "Cas of is really good at all traditional predictive models so in the for the last",
    "start": "1655720",
    "end": "1660880"
  },
  {
    "text": "one year we are trying to Pivot to generate model space right so U if you",
    "start": "1660880",
    "end": "1667080"
  },
  {
    "text": "really look at all the traditional models like it was all working with the request based autoscaling and request be",
    "start": "1667080",
    "end": "1672600"
  },
  {
    "text": "nrixs uh it works really well for the traditional models but in large language models due to its size scale and",
    "start": "1672600",
    "end": "1679399"
  },
  {
    "text": "architecture it is not that easy um because of many reasons right um so if",
    "start": "1679399",
    "end": "1684960"
  },
  {
    "text": "you really look at in the large language models like you get uh a request of 100",
    "start": "1684960",
    "end": "1690640"
  },
  {
    "text": "tokens now in the next instant you can get a request of 10,000 tokens right so",
    "start": "1690640",
    "end": "1695799"
  },
  {
    "text": "uh GPU can take like uh 10 milliseconds for the first request the other one can actually take seconds to minutes so the",
    "start": "1695799",
    "end": "1702320"
  },
  {
    "text": "auto scaling and uh the other uh the factors which are deciding Auto scaling",
    "start": "1702320",
    "end": "1708960"
  },
  {
    "text": "uh are not that easy in the llm world right specifically in the generative AI world so in other words we need to have",
    "start": "1708960",
    "end": "1715840"
  },
  {
    "text": "tokens as the first class citizen for the large language models in the newer space so we are we are discussing",
    "start": "1715840",
    "end": "1722840"
  },
  {
    "text": "internally as well like how can we integrate token based autoscaling and token based metrics um for having much",
    "start": "1722840",
    "end": "1730120"
  },
  {
    "text": "more uh guaranteed SLA is for throughput and latency in Cas of and that's one major focus and similar to the other",
    "start": "1730120",
    "end": "1737120"
  },
  {
    "text": "standards that we are looking for again for a a common unified metric interface",
    "start": "1737120",
    "end": "1742200"
  },
  {
    "text": "right like for different run times provide different metrics uh be it name VM so we need a unified metric interface",
    "start": "1742200",
    "end": "1750760"
  },
  {
    "text": "similar to how we have in inference protocol uh which can decide what's the performance of runtime and how can we",
    "start": "1750760",
    "end": "1758200"
  },
  {
    "text": "decide Auto scaling based on that right case of community as a community we can come together and decide what are the",
    "start": "1758200",
    "end": "1764919"
  },
  {
    "text": "right abstractions that we need to do even from the metrics and auto scaling point and that can work with all different run",
    "start": "1764919",
    "end": "1771159"
  },
  {
    "text": "times underneath so from a user perspective he doesn't need to know anything from he just uses the standard",
    "start": "1771159",
    "end": "1776519"
  },
  {
    "text": "open ey protocol um uh sorry the open inference protocol and rest all things are handled by the cas of uh",
    "start": "1776519",
    "end": "1785120"
  },
  {
    "text": "internally thank you I think a lot of the interesting projects and features have been already mentioned but adding",
    "start": "1785440",
    "end": "1791240"
  },
  {
    "text": "to what the name said also it's worth mentioning that it's important for us as a community to share your experience",
    "start": "1791240",
    "end": "1797720"
  },
  {
    "text": "with the project or with a product if you've been using ker K flow or any other tool share it with us how it has",
    "start": "1797720",
    "end": "1803600"
  },
  {
    "text": "been what were the blockers that you faced as well because that's how we end up coming with features with ideas with",
    "start": "1803600",
    "end": "1809360"
  },
  {
    "text": "projects with road maps at the end of the day and one of the reasons why personally I love coming to cubec con is",
    "start": "1809360",
    "end": "1815799"
  },
  {
    "text": "whether we talk about charm Cube flow or kerve or cube flow the Upstream project I really enjoy hearing how people use it",
    "start": "1815799",
    "end": "1822399"
  },
  {
    "text": "what are the challenges that they have because it gives us food for thought on how can we make it better now looking at",
    "start": "1822399",
    "end": "1828760"
  },
  {
    "text": "the future there are think two things that were not necessarily mentioned yet on one hand we are going to need to",
    "start": "1828760",
    "end": "1834440"
  },
  {
    "text": "focus more on security whether we talk about the security of the packages that are being used uh in Cas or in general",
    "start": "1834440",
    "end": "1840799"
  },
  {
    "text": "in in the projects um that I used for data science and machine learning is one",
    "start": "1840799",
    "end": "1847279"
  },
  {
    "text": "aspect that needs to be F prioritized in order to be able to roll out a lot of",
    "start": "1847279",
    "end": "1852399"
  },
  {
    "text": "the projects in production and then on the other hand improving the user experience and lowering the year entry",
    "start": "1852399",
    "end": "1858919"
  },
  {
    "text": "to run gen models uh gen projects and llms is going to be another one last but",
    "start": "1858919",
    "end": "1864120"
  },
  {
    "text": "not least if I go back to the security idea uh what excites me very much is to",
    "start": "1864120",
    "end": "1869320"
  },
  {
    "text": "run CU flow and then probably K serve as well at some point in confidential Computing VMS is something that is going",
    "start": "1869320",
    "end": "1875080"
  },
  {
    "text": "to enable collaboration at a whole different scale between companies within the same industry which I think is",
    "start": "1875080",
    "end": "1880919"
  },
  {
    "text": "exciting as well Adam I think you still have some minutes as well to add some things yeah uh so so yeah so sort of",
    "start": "1880919",
    "end": "1888320"
  },
  {
    "text": "pull that all together I know you guys mentioned a lot of the the API gateways",
    "start": "1888320",
    "end": "1893480"
  },
  {
    "text": "uh new features like these are all new things in road maps I'm excited for so sort of to add to that a little bit uh",
    "start": "1893480",
    "end": "1901120"
  },
  {
    "text": "maybe roll it back uh I I think I'm up here because we had a win and one of the",
    "start": "1901120",
    "end": "1906279"
  },
  {
    "text": "the things getting here that was really difficult that completely different topic convincing people internally that",
    "start": "1906279",
    "end": "1912159"
  },
  {
    "text": "open source was a good path I really had to Champion kerve as a useful platform I",
    "start": "1912159",
    "end": "1918519"
  },
  {
    "text": "really had to pitch to product people and Engineering leaders I need to go to them and say hey this is important we",
    "start": "1918519",
    "end": "1923960"
  },
  {
    "text": "need to put resources on it and I'm really looking to the future seeing that we're all here uh seeing that people are",
    "start": "1923960",
    "end": "1931000"
  },
  {
    "text": "are receptive and that now my customers have choice of platform choice of models",
    "start": "1931000",
    "end": "1936320"
  },
  {
    "text": "choice of AI and it's it's just giving more to the community I I think that's a win that will make it easier for me to",
    "start": "1936320",
    "end": "1942080"
  },
  {
    "text": "go back to Upper leadership and saying hey let's do this more it worked last time uh we all won together let's let's",
    "start": "1942080",
    "end": "1948399"
  },
  {
    "text": "do it again and and so I'm I'm looking forward to seeing more engagements I know Chris lamb gave a a a keynote uh",
    "start": "1948399",
    "end": "1956000"
  },
  {
    "text": "yesterday and I'm hoping to get more folks from my teams involved in more projects uh which is good because the",
    "start": "1956000",
    "end": "1961519"
  },
  {
    "text": "other thing I wanted to mention is there's a lot of new stuff that that we need to get into the open source you",
    "start": "1961519",
    "end": "1966559"
  },
  {
    "text": "mentioned confidential containers uh that's that's a hardware dependent thing and it's going to require a lot of",
    "start": "1966559",
    "end": "1971880"
  },
  {
    "text": "software to make work uh I think you mentioned multi-node inferencing we have",
    "start": "1971880",
    "end": "1977039"
  },
  {
    "text": "the first p of that out now but that's another Hardware dependent things and there's a lot of things that my my teams",
    "start": "1977039",
    "end": "1984000"
  },
  {
    "text": "can work on around high performance networking and high performance uh fabric that we can make the multi-node",
    "start": "1984000",
    "end": "1991080"
  },
  {
    "text": "stuff go faster we can we can make that the time slice stuff go faster so I'm excited to see and Dr right that that's",
    "start": "1991080",
    "end": "1998320"
  },
  {
    "text": "a part of that story so I'm excited to see a lot of that stuff come together that the applications which are getting",
    "start": "1998320",
    "end": "2004000"
  },
  {
    "text": "more complex can get simpler to manage and more efficient and so so sort of the",
    "start": "2004000",
    "end": "2009919"
  },
  {
    "text": "last thing uh generative AI applications are getting more complex is all of these pieces we were here right with the",
    "start": "2009919",
    "end": "2015880"
  },
  {
    "text": "platform where we have a single inference server doing a single model and then model mesh so we could do multiple models and mix it and now we",
    "start": "2015880",
    "end": "2022039"
  },
  {
    "text": "need something completely new because now we have these generative AI applications with Rag and we we have to we're defining Telemetry standards and",
    "start": "2022039",
    "end": "2028679"
  },
  {
    "text": "some working groups and we're defining these new gateways and other working groups and so there's really uh this",
    "start": "2028679",
    "end": "2034000"
  },
  {
    "text": "community the working group serving Community is really across a bunch of different projects and and we're all",
    "start": "2034000",
    "end": "2039919"
  },
  {
    "text": "trying to figure out what the problems are right now and and this is a great time to get involved because I think we we're just getting a grasp of all of the",
    "start": "2039919",
    "end": "2046320"
  },
  {
    "text": "problems and we're starting to put Solutions together um so so I think we have time for one question do we want to",
    "start": "2046320",
    "end": "2052480"
  },
  {
    "text": "take a or does anyone on the panel there is one we got one one uh what kind",
    "start": "2052480",
    "end": "2060520"
  },
  {
    "text": "of okay",
    "start": "2061639",
    "end": "2065638"
  },
  {
    "text": "sure sure y yep so so the question was what",
    "start": "2073159",
    "end": "2079320"
  },
  {
    "text": "sort of skill set do you need to get involved in this sort of work mlops engineering python Cuda good at one",
    "start": "2079320",
    "end": "2084520"
  },
  {
    "text": "thing bad at another what do you need um if I may start um I think it really",
    "start": "2084520",
    "end": "2090158"
  },
  {
    "text": "depends but for example in the CU flow Community I see a lot of students who",
    "start": "2090159",
    "end": "2095480"
  },
  {
    "text": "are just very good python Engineers very eastic very curious um and they just get",
    "start": "2095480",
    "end": "2100760"
  },
  {
    "text": "started they they learn as they go and that can be challenging but at the same time it gives you a lot of satisfaction",
    "start": "2100760",
    "end": "2106599"
  },
  {
    "text": "at the same time I think understanding kubernetes is a foundational piece that is going to lower your barer entry when",
    "start": "2106599",
    "end": "2112079"
  },
  {
    "text": "it comes to kerve and it's going to accelerate the way you can",
    "start": "2112079",
    "end": "2117560"
  },
  {
    "text": "contribute I see tanim is is agreeing I'm not sure would you like to add something no no no go for it sorry I",
    "start": "2117560",
    "end": "2124880"
  },
  {
    "text": "just want to take one example some my team members were completely new to",
    "start": "2124880",
    "end": "2129960"
  },
  {
    "text": "cuber antis and ker two years ago they are currently reviewers of ker project",
    "start": "2129960",
    "end": "2135880"
  },
  {
    "text": "right so it's all driven by interest and the amount of effort which are putting in and K of community is welcoming um",
    "start": "2135880",
    "end": "2143760"
  },
  {
    "text": "everyone each and everyone I would say having technical",
    "start": "2143760",
    "end": "2149160"
  },
  {
    "text": "skills is obviously really helpful to contribute whether you're a python developer go developer or your mlops",
    "start": "2149160",
    "end": "2154760"
  },
  {
    "text": "engineer but there's a lot of help with we need even documentation updating if you go to the K Ser website there are a",
    "start": "2154760",
    "end": "2161560"
  },
  {
    "text": "lot of new features that we have not been able to add to the documents right and that's one the first place where new",
    "start": "2161560",
    "end": "2166880"
  },
  {
    "text": "new user would come and use K serve that's where they're going to go so even if you can contribute by updating a",
    "start": "2166880",
    "end": "2172800"
  },
  {
    "text": "documentation that is a massive contribution so I would say like it's it's all about working in the community",
    "start": "2172800",
    "end": "2178160"
  },
  {
    "text": "it's all about putting the time in uh and and that's how you contribute and help so if you don't feel like you I",
    "start": "2178160",
    "end": "2184560"
  },
  {
    "text": "don't know go I don't know ml I mean how do I go contribute it really doesn't matter uh you can contribute in all",
    "start": "2184560",
    "end": "2190240"
  },
  {
    "text": "kinds of ways we welcome all kinds of",
    "start": "2190240",
    "end": "2195079"
  },
  {
    "text": "contributions yeah I agree with that um I think kubernetes is definitely the the base uh as long as you're familiar with",
    "start": "2195440",
    "end": "2201960"
  },
  {
    "text": "pots and containers I think you're pretty much set to go um and yeah it doesn't matter what programming language",
    "start": "2201960",
    "end": "2207800"
  },
  {
    "text": "you know um and a lot of people from like come from you know and users of of all these Technologies to become uh",
    "start": "2207800",
    "end": "2215119"
  },
  {
    "text": "contributors of Open Source projects so you really can't start from anywhere as long as you know the uh what the what",
    "start": "2215119",
    "end": "2221440"
  },
  {
    "text": "problem we're trying to solve and or yeah just even just raise an issue that's a contribution so yeah yeah yeah",
    "start": "2221440",
    "end": "2228040"
  },
  {
    "text": "so I think you need a passion to to want to help and we'll help you learn the rest is what I just",
    "start": "2228040",
    "end": "2234359"
  },
  {
    "text": "heard I think I think we're we're at time here so uh thank you everyone for attending uh I hope you have a good",
    "start": "2235200",
    "end": "2241119"
  },
  {
    "text": "conference",
    "start": "2241119",
    "end": "2244119"
  }
]