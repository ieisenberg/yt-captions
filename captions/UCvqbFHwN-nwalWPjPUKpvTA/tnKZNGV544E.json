[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "as more and more engineers other microservice architectures and cloud native technologies understanding the",
    "start": "4640",
    "end": "10719"
  },
  {
    "text": "behavior and failure patterns of our systems is key to ensure they are performing and delivering a great",
    "start": "10719",
    "end": "16160"
  },
  {
    "text": "customer experience yet it is really hard because those environments are highly dynamic with all",
    "start": "16160",
    "end": "22960"
  },
  {
    "text": "the scaling and frequent deployments of new versions of our applications i'm ramon gu",
    "start": "22960",
    "end": "29119"
  },
  {
    "text": "vp of observability products at time scale and in this cncf webinar i will",
    "start": "29119",
    "end": "34480"
  },
  {
    "text": "show you how you can use three open source projects open telemetry grafana and prom scale to get insights about",
    "start": "34480",
    "end": "41680"
  },
  {
    "text": "your systems that will help you deeply understand how they behave or misbehave",
    "start": "41680",
    "end": "47200"
  },
  {
    "text": "so you can improve them and deliver a better experience to your users",
    "start": "47200",
    "end": "52878"
  },
  {
    "start": "52000",
    "end": "120000"
  },
  {
    "text": "this is the agenda first i'll do a very quick introduction to open telemetry and distributed",
    "start": "54480",
    "end": "59920"
  },
  {
    "text": "tracing then i'll talk about primskill which is a free open source observability back-end that runs on top of timescale",
    "start": "59920",
    "end": "65920"
  },
  {
    "text": "db and postgresql and finally i'll show how you can use open telemetry prompt scale grafana and",
    "start": "65920",
    "end": "74400"
  },
  {
    "text": "sql to better understand your distributed systems by using a demo environment we've created that you can",
    "start": "74400",
    "end": "80880"
  },
  {
    "text": "get up and running on your computer in just a few minutes",
    "start": "80880",
    "end": "85960"
  },
  {
    "text": "the demo environment we will use is available on github at the url that you see at the top",
    "start": "86560",
    "end": "94078"
  },
  {
    "text": "and we've also written a detailed blog post that covers everything about that",
    "start": "94479",
    "end": "99520"
  },
  {
    "text": "demo environment how to set it up how to instrument your code with open telemetry",
    "start": "99520",
    "end": "105280"
  },
  {
    "text": "and how to query the data and build grafana dashboards i'll be covering some parts of that in",
    "start": "105280",
    "end": "110799"
  },
  {
    "text": "this session if you want to dig deeper i recommend you check that blog post",
    "start": "110799",
    "end": "117200"
  },
  {
    "text": "you ready let's get started for those that are not familiar with it",
    "start": "117200",
    "end": "124320"
  },
  {
    "start": "120000",
    "end": "338000"
  },
  {
    "text": "i'm going to take a few minutes to introduce open telemetry and distributed tracing",
    "start": "124320",
    "end": "130720"
  },
  {
    "text": "open telemetry is a new standard for instrumentation that is hosted by the cloud native computing foundation",
    "start": "131039",
    "end": "139280"
  },
  {
    "text": "it was born after two other open source projects joined forces",
    "start": "139280",
    "end": "144720"
  },
  {
    "text": "open tracing and open sensors in the three years since the joint",
    "start": "144720",
    "end": "150720"
  },
  {
    "text": "effort was announced open telemetry has become the second most active project as",
    "start": "150720",
    "end": "156480"
  },
  {
    "text": "well as the second with most contributors among all cncf projects",
    "start": "156480",
    "end": "162000"
  },
  {
    "text": "only after kubernetes paguela both prometheus and other very popular projects",
    "start": "162000",
    "end": "168319"
  },
  {
    "text": "most observability vendors including us time skill and all major cloud providers are",
    "start": "168319",
    "end": "173760"
  },
  {
    "text": "contributing to the project why is there so much interest in it",
    "start": "173760",
    "end": "180080"
  },
  {
    "text": "well first it's vendor agnostic instrument once and send telemetry",
    "start": "180080",
    "end": "185440"
  },
  {
    "text": "anywhere so your investment is future proof",
    "start": "185440",
    "end": "190480"
  },
  {
    "text": "and behind are the days where you had to re-instrument your systems when adopting a new observability tool",
    "start": "190480",
    "end": "197760"
  },
  {
    "text": "it also opens the door for engineers creating libraries frameworks and tools that other developers use to build their",
    "start": "197760",
    "end": "204400"
  },
  {
    "text": "applications to add instrumentation into the source code for example",
    "start": "204400",
    "end": "210720"
  },
  {
    "text": "kubernetes has started adding open telemetry instrumentation into their",
    "start": "210720",
    "end": "216080"
  },
  {
    "text": "code second it's a standard that includes the three key telemetry signals",
    "start": "216080",
    "end": "223440"
  },
  {
    "text": "metrics locks and traces that share the metadata and tags so you can more easily",
    "start": "223440",
    "end": "230720"
  },
  {
    "text": "correlate them it also defines align protocol and",
    "start": "230720",
    "end": "236319"
  },
  {
    "text": "semantic conventions making interoperability between open telemetry and other tools much easier",
    "start": "236319",
    "end": "243760"
  },
  {
    "text": "and finally it provides libraries that do automatic code instrumentation dramatically reducing the effort",
    "start": "243760",
    "end": "250879"
  },
  {
    "text": "required to instrument your code in today's session we will focus on open",
    "start": "250879",
    "end": "257280"
  },
  {
    "text": "telemetry traces since they hold a lot of valuable data to understand distributed systems that metrics and",
    "start": "257280",
    "end": "263759"
  },
  {
    "text": "logs cannot provide but what is a trace a trace is a",
    "start": "263759",
    "end": "268960"
  },
  {
    "text": "connected representation of the sequence of operations that were performed across all microservices involved in order to",
    "start": "268960",
    "end": "276720"
  },
  {
    "text": "fulfill an individual request for example if you open an article from a new site",
    "start": "276720",
    "end": "284080"
  },
  {
    "text": "in your browser there would be multiple operations saved by different microservices",
    "start": "284080",
    "end": "290000"
  },
  {
    "text": "read the article read the comments for the article and request ads to display",
    "start": "290000",
    "end": "295360"
  },
  {
    "text": "with that article each of those operations are represented by a span with their own subspans",
    "start": "295360",
    "end": "302000"
  },
  {
    "text": "a span can have 0 or multiple children all spans have just one parent except",
    "start": "302000",
    "end": "307600"
  },
  {
    "text": "the initial span in a trace called the root span which has no pattern",
    "start": "307600",
    "end": "313759"
  },
  {
    "text": "some of you may be familiar with yaga a popular open source distributed",
    "start": "317039",
    "end": "322560"
  },
  {
    "text": "tracing product this shows a screenshot of the jaeger ui",
    "start": "322560",
    "end": "329280"
  },
  {
    "text": "which includes an individual trace with all its span and their parent-child",
    "start": "329280",
    "end": "335120"
  },
  {
    "text": "relationships in the demo we will make heavy use of",
    "start": "335120",
    "end": "341199"
  },
  {
    "start": "338000",
    "end": "418000"
  },
  {
    "text": "pram scale and its capabilities prom scale is an open source",
    "start": "341199",
    "end": "346880"
  },
  {
    "text": "observability backend for metrics and traces powered by sql as i mentioned",
    "start": "346880",
    "end": "352720"
  },
  {
    "text": "it's built on top of the proven rock solid foundation of time scale db which is a time series database built on top",
    "start": "352720",
    "end": "359840"
  },
  {
    "text": "of post sql and as a result it lets you query the data using full sql",
    "start": "359840",
    "end": "366400"
  },
  {
    "text": "we will use sql extensively to derive insights from traces in our demo",
    "start": "366400",
    "end": "371759"
  },
  {
    "text": "on top of open telemetry promscale also integrates with prometheus for long-term",
    "start": "371759",
    "end": "377280"
  },
  {
    "text": "metric storage and analysis with grafana to visualize the data and also their",
    "start": "377280",
    "end": "382880"
  },
  {
    "text": "tools like jager which we saw before",
    "start": "382880",
    "end": "387840"
  },
  {
    "text": "this is just a high level architecture where you see prom scale using time scale db to store the data and",
    "start": "388400",
    "end": "394639"
  },
  {
    "text": "integrations with prometheus open telemetry grafana jager",
    "start": "394639",
    "end": "400319"
  },
  {
    "text": "and any tool that speaks sql as a node time scale db is positive sql",
    "start": "400319",
    "end": "406160"
  },
  {
    "text": "with time series super powers technically it's a postgres extension",
    "start": "406160",
    "end": "411759"
  },
  {
    "text": "and so also get access to all the capabilities postgres provides",
    "start": "411759",
    "end": "418080"
  },
  {
    "start": "418000",
    "end": "545000"
  },
  {
    "text": "enough of an introduction let's start playing with open telemetry prompt skill graphana and sql",
    "start": "418960",
    "end": "426319"
  },
  {
    "text": "you can get this demo up and running on your own computer you need to have docker and docker",
    "start": "427360",
    "end": "432800"
  },
  {
    "text": "compose installed and then download the github repo and run docker compose app",
    "start": "432800",
    "end": "440080"
  },
  {
    "text": "this is what the three commands in this slide do i'm going to copy and paste them into a terminal so you see what",
    "start": "441280",
    "end": "447599"
  },
  {
    "text": "happens",
    "start": "447599",
    "end": "450599"
  },
  {
    "text": "so as you can see this has called the repo and then i just run docker compose",
    "start": "458479",
    "end": "465120"
  },
  {
    "text": "which will download all the different uh images build them and um",
    "start": "465120",
    "end": "471039"
  },
  {
    "text": "and then get the environment up and running this will take a few minutes so we're not gonna see all of this now",
    "start": "471039",
    "end": "477520"
  },
  {
    "text": "but you can do it on your laptop and we've tested with mac os linux and",
    "start": "477520",
    "end": "483199"
  },
  {
    "text": "windows",
    "start": "483199",
    "end": "486199"
  },
  {
    "text": "so going back to the slides this is the high level architecture of our demo system",
    "start": "493919",
    "end": "499840"
  },
  {
    "text": "it's a password generator that has been over designed as a micro services",
    "start": "499840",
    "end": "505599"
  },
  {
    "text": "application connected to an open telemetry observability stack it has",
    "start": "505599",
    "end": "511440"
  },
  {
    "text": "a load generator that makes requests to the generator service then the generator service calls the upper lower digit and",
    "start": "511440",
    "end": "519360"
  },
  {
    "text": "special services to get random uppercase lowercase digits and special characters",
    "start": "519360",
    "end": "525440"
  },
  {
    "text": "to build a password the local server the lower service is written in ruby and the rest in python",
    "start": "525440",
    "end": "532480"
  },
  {
    "text": "all services have been instrumented with open telemetry traces and send those traces to prompt scale",
    "start": "532480",
    "end": "539519"
  },
  {
    "text": "as we saw before you know you can get this demo up and running on your own laptop",
    "start": "539519",
    "end": "546000"
  },
  {
    "start": "545000",
    "end": "1022000"
  },
  {
    "text": "okay now now let's go into grafana and let's check uh all the different dashboards that",
    "start": "547200",
    "end": "553440"
  },
  {
    "text": "we've built that will show how to use sql to derive insights from tracing",
    "start": "553440",
    "end": "560920"
  },
  {
    "text": "so here i'm already logged in and i have a demo environment that has been running for uh quite a bit",
    "start": "564160",
    "end": "569680"
  },
  {
    "text": "uh by default the demo environment comes with these six dashboards uh and we will be looking at them now",
    "start": "569680",
    "end": "576959"
  },
  {
    "text": "one thing to keep in mind is that the first time you try to log in into grafana it will ask for your login and",
    "start": "576959",
    "end": "582320"
  },
  {
    "text": "password and those are admin admin okay these are the defaults that are set you can change",
    "start": "582320",
    "end": "587760"
  },
  {
    "text": "them if you want but since this is for demo purposes you know that's not as important",
    "start": "587760",
    "end": "593839"
  },
  {
    "text": "so we're going to start by looking at the request rate dashboard",
    "start": "594240",
    "end": "599839"
  },
  {
    "text": "the request rate dashboard is just simply showing",
    "start": "599839",
    "end": "605279"
  },
  {
    "text": "the number of requests per second that are happening uh across you know the uh the micro",
    "start": "605279",
    "end": "612320"
  },
  {
    "text": "services since this is a um since uh the architecture if you check",
    "start": "612320",
    "end": "617920"
  },
  {
    "text": "the architecture of the uh of if we check the architecture of the um application",
    "start": "617920",
    "end": "624959"
  },
  {
    "text": "we'll see that there is basically one entry point which is the generator okay so this is basically measuring",
    "start": "624959",
    "end": "631440"
  },
  {
    "text": "the request throughput for the generator",
    "start": "631440",
    "end": "637839"
  },
  {
    "text": "the throughput you know request per second is one of the golden metrics",
    "start": "637920",
    "end": "643360"
  },
  {
    "text": "when measuring application performance the other two are error rate and latency which we will be",
    "start": "643360",
    "end": "648640"
  },
  {
    "text": "looking at next but here let's take a look at how these dashboards are built so let's we can take any of those two",
    "start": "648640",
    "end": "654959"
  },
  {
    "text": "let's take the the one at the bottom so you'll see",
    "start": "654959",
    "end": "660560"
  },
  {
    "text": "this is a standard time series component from grafana and what we're doing is we're doing this",
    "start": "660560",
    "end": "667040"
  },
  {
    "text": "sql query okay so you see you recognize the select from",
    "start": "667040",
    "end": "673120"
  },
  {
    "text": "work close in the select what we're adding is we use the time",
    "start": "673120",
    "end": "679920"
  },
  {
    "text": "scale db time bucket function this creates buckets to be displayed so it aggregates that we we then",
    "start": "679920",
    "end": "686399"
  },
  {
    "text": "group by the time bucket so this basically aggregates data on a per second",
    "start": "686399",
    "end": "692480"
  },
  {
    "text": "bucket and then we're doing counting everything that is happening you know within the bucket so that gives",
    "start": "692480",
    "end": "699279"
  },
  {
    "text": "us the number uh of requests so we're counting all the expands uh since that's",
    "start": "699279",
    "end": "704480"
  },
  {
    "text": "what we have in the from close we're we're querying expands counter star is giving us all the span",
    "start": "704480",
    "end": "710800"
  },
  {
    "text": "that meet this requirement you know where you know parent span id is now so",
    "start": "710800",
    "end": "715920"
  },
  {
    "text": "these are you know entry requests into the system which as i mentioned are basically requests to the generator",
    "start": "715920",
    "end": "722560"
  },
  {
    "text": "service and so this is the uh this is the throughput that we see you know it comes in",
    "start": "722560",
    "end": "728079"
  },
  {
    "text": "in sort of waves and we see you know max is getting to uh 11 requests per second",
    "start": "728079",
    "end": "733120"
  },
  {
    "text": "but we also see in some buckets there are no requests at all",
    "start": "733120",
    "end": "739000"
  },
  {
    "text": "okay now we're going to take a look at",
    "start": "745760",
    "end": "750399"
  },
  {
    "text": "we'll take a look at another dashboard we'll look at the error rate dashboard",
    "start": "750959",
    "end": "757120"
  },
  {
    "text": "so this dashboard i think it gets a little bit more interesting than the other one so the other one was obviously showing the",
    "start": "757120",
    "end": "762959"
  },
  {
    "text": "evolution of throughput over time but this one is give us uh is giving us more detail this one for example if we look",
    "start": "762959",
    "end": "769839"
  },
  {
    "text": "at let's focus on this table this table is telling us for each service and operation",
    "start": "769839",
    "end": "776560"
  },
  {
    "text": "what's the error rate you know how many what's the percentage of errors you know across",
    "start": "776560",
    "end": "782959"
  },
  {
    "text": "all these operations that happen in the system in the last uh in this case 30 minutes",
    "start": "782959",
    "end": "790000"
  },
  {
    "text": "so let's take a look at what this query looks like",
    "start": "790320",
    "end": "795360"
  },
  {
    "text": "so this is what this query does it looks at you know it retrieves from",
    "start": "795360",
    "end": "800480"
  },
  {
    "text": "um so it's using a sub query again this is an interesting thing you know it's something that is available in sql but not necessarily",
    "start": "800480",
    "end": "807600"
  },
  {
    "text": "you know in other query languages for or you know the observability tools offer",
    "start": "807600",
    "end": "813519"
  },
  {
    "text": "and so in this case we have a sub query we have an initial query that is doing a select",
    "start": "813519",
    "end": "819279"
  },
  {
    "text": "again on the span view this is a view that we the prime scale exposes but you can think of it as",
    "start": "819279",
    "end": "825680"
  },
  {
    "text": "if it was a you know regular table doesn't matter that much for for the purposes of explaining the",
    "start": "825680",
    "end": "831600"
  },
  {
    "text": "sql that we use so we're querying this pan view",
    "start": "831600",
    "end": "836639"
  },
  {
    "text": "and in the spam view we have a service name which is you know again name of the service plate the explanatory span name",
    "start": "836639",
    "end": "843199"
  },
  {
    "text": "span name is typically the name of the operation okay is the name of this spam but what it indicates is the name of",
    "start": "843199",
    "end": "848720"
  },
  {
    "text": "that specific operation then we have we're counting",
    "start": "848720",
    "end": "854240"
  },
  {
    "text": "how many of those spans have a status code of error",
    "start": "854240",
    "end": "859519"
  },
  {
    "text": "and we're also counting the total number of spans",
    "start": "859519",
    "end": "864800"
  },
  {
    "text": "and we're grouping by one and two that means that you know we're grouping by service",
    "start": "865519",
    "end": "871920"
  },
  {
    "text": "name and span name so these these two statistics are calculated group by service name and span name but that's",
    "start": "871920",
    "end": "877760"
  },
  {
    "text": "why we see you know this in what we see in this table",
    "start": "877760",
    "end": "882480"
  },
  {
    "text": "and uh and then we also we're using you know two variables",
    "start": "882800",
    "end": "889199"
  },
  {
    "text": "as filters okay so if we go up because we as i said this is a subquery",
    "start": "889199",
    "end": "894560"
  },
  {
    "text": "so we have the we have these results and then the only thing this all the query is doing is just taking service name",
    "start": "894560",
    "end": "900560"
  },
  {
    "text": "span name and then calculating the rate the error rate okay we could have done actually everything within the",
    "start": "900560",
    "end": "906639"
  },
  {
    "text": "same query but you know to make it easier to read we just used a the subquery",
    "start": "906639",
    "end": "914000"
  },
  {
    "text": "and finally we're ordering ordering by error rate descendants so we show those uh operations",
    "start": "916560",
    "end": "923519"
  },
  {
    "text": "that have a higher error rate at the top okay so with this information very quickly we can see okay",
    "start": "923519",
    "end": "929759"
  },
  {
    "text": "so generator generate is the one that has a higher error rate but that is the top level",
    "start": "929759",
    "end": "936000"
  },
  {
    "text": "operation so let's focus on you know next level and next level we'll see uh",
    "start": "936000",
    "end": "942240"
  },
  {
    "text": "process extra extra process upper is the other one that has a very higher rate there are some other",
    "start": "942240",
    "end": "949120"
  },
  {
    "text": "operations that have some errors but they are the the error rate for",
    "start": "949120",
    "end": "955680"
  },
  {
    "text": "those is much lower so probably you know we should go and focus in this you know check this method and see what's going",
    "start": "955680",
    "end": "961120"
  },
  {
    "text": "on why we have such a high error rate",
    "start": "961120",
    "end": "965279"
  },
  {
    "text": "um as uh as we mentioned you know you you have here at the top if you wanted you",
    "start": "966480",
    "end": "971759"
  },
  {
    "text": "could actually filter down to some to some specific uh service or uh or or",
    "start": "971759",
    "end": "978000"
  },
  {
    "text": "operation in here the other thing we're doing here is that we're looking at",
    "start": "978000",
    "end": "983920"
  },
  {
    "text": "the evolution you know this is a similar to this but this is looking at the evolution",
    "start": "983920",
    "end": "989759"
  },
  {
    "text": "over time so if you if we open this query we'll see that the query is pretty much",
    "start": "989759",
    "end": "995519"
  },
  {
    "text": "the same the main difference is that we're introducing a time uh variable here a time",
    "start": "995519",
    "end": "1003680"
  },
  {
    "text": "projection in the select that is the time bucket you know so we're calculating this stat the the",
    "start": "1004800",
    "end": "1010639"
  },
  {
    "text": "error rate per service and operation uh in a you know on a per minute basis",
    "start": "1010639",
    "end": "1017759"
  },
  {
    "text": "and we're plotting it here over time okay",
    "start": "1017759",
    "end": "1023120"
  },
  {
    "start": "1022000",
    "end": "1122000"
  },
  {
    "text": "let's move to the next one let the next one is latency okay request durations this is",
    "start": "1023120",
    "end": "1028880"
  },
  {
    "text": "the third golden synonym so as i said there are three so we have uh throughput",
    "start": "1028880",
    "end": "1034480"
  },
  {
    "text": "uh error rate that we've already seen and then latency and here we can see",
    "start": "1034480",
    "end": "1040400"
  },
  {
    "text": "let's look at this chart here this chart is showing the evolution of duration over time",
    "start": "1040400",
    "end": "1047120"
  },
  {
    "text": "but we're not looking at average we're actually looking at percentiles okay so we're computing percentiles so how does",
    "start": "1047120",
    "end": "1052160"
  },
  {
    "text": "this work well again let's take a look at and see how the query works so here again we're using this time",
    "start": "1052160",
    "end": "1057840"
  },
  {
    "text": "bucket function that time scale db provides to group the data in brackets of one minute",
    "start": "1057840",
    "end": "1065200"
  },
  {
    "text": "so then it shows you know the group by close and then what we're doing is that we're looking",
    "start": "1065200",
    "end": "1072640"
  },
  {
    "text": "at the uh percentiles for uh 99 percentile 95th percentile",
    "start": "1072640",
    "end": "1080000"
  },
  {
    "text": "19th percent percentile in the median or 50th percentile and to do that we're using",
    "start": "1080000",
    "end": "1086400"
  },
  {
    "text": "the approx percentile function provided by time scale db which looks like the we use this",
    "start": "1086400",
    "end": "1092480"
  },
  {
    "text": "percentile act function as well which calculates a sketch on the duration",
    "start": "1092480",
    "end": "1098080"
  },
  {
    "text": "millisecond which is a data structure that then allows us to compute an approximate percentile on top of it",
    "start": "1098080",
    "end": "1104640"
  },
  {
    "text": "in a way that is you know more performant and then you we're just plotting all of",
    "start": "1104640",
    "end": "1109840"
  },
  {
    "text": "those here so again you know we can use the power of sql and then scale db to compute those percentiles and we could",
    "start": "1109840",
    "end": "1115120"
  },
  {
    "text": "use any we compute any percentile that we wanted here",
    "start": "1115120",
    "end": "1120760"
  },
  {
    "start": "1122000",
    "end": "1266000"
  },
  {
    "text": "another thing that is interesting is this histogram of durations okay so if we look at this this is showing us",
    "start": "1122880",
    "end": "1129120"
  },
  {
    "text": "the distribution of latency for uh for request again",
    "start": "1129120",
    "end": "1135120"
  },
  {
    "text": "because all requests go through the generator you know this is for you know all generator requests there is just one",
    "start": "1135120",
    "end": "1141919"
  },
  {
    "text": "um entry point into this micro services environment",
    "start": "1141919",
    "end": "1148480"
  },
  {
    "text": "and what we see is that while the majority of the uh",
    "start": "1148480",
    "end": "1153679"
  },
  {
    "text": "requests are processed in let's say maybe let's say two seconds or less there are some of those that are",
    "start": "1154160",
    "end": "1160799"
  },
  {
    "text": "extremely slow you even have requests that took you know 30 seconds that's that's a lot of time",
    "start": "1160799",
    "end": "1167120"
  },
  {
    "text": "what may be going on there okay so here at the bottom we have another interesting thing here we're listing",
    "start": "1167120",
    "end": "1173840"
  },
  {
    "text": "individual traces again a trace maps to our request and how it flew how you know",
    "start": "1173840",
    "end": "1179360"
  },
  {
    "text": "it went through the system so we're looking at individual traces",
    "start": "1179360",
    "end": "1185200"
  },
  {
    "text": "when they happened and how long they took okay and this query is actually showing the slowest",
    "start": "1185200",
    "end": "1190640"
  },
  {
    "text": "one so let's take a look at this so if we look at it we'll see that we have",
    "start": "1190640",
    "end": "1196720"
  },
  {
    "text": "a number of uh traces you know the start time duration as we saw in the uh",
    "start": "1196720",
    "end": "1203840"
  },
  {
    "text": "in the panel in the dashboard and this is what we're doing so we're talking we're displaying",
    "start": "1203840",
    "end": "1209840"
  },
  {
    "text": "the trace id okay and we're doing this replace text that i'll i'll explain why we're doing this",
    "start": "1209840",
    "end": "1217120"
  },
  {
    "text": "we have at the start time and the duration that we're projecting and the only thing we're doing is just sorting",
    "start": "1217120",
    "end": "1223120"
  },
  {
    "text": "okay so we are using again span id null which means this is you know the root span and basically maps",
    "start": "1223120",
    "end": "1230559"
  },
  {
    "text": "to a trace a full trace and um and the only thing we're doing is we're just sorting okay",
    "start": "1230559",
    "end": "1236720"
  },
  {
    "text": "so it's a very simple query we're just searching for root spans",
    "start": "1236720",
    "end": "1243280"
  },
  {
    "text": "and uh we're getting the top 10 that the slowest one right because we're",
    "start": "1243280",
    "end": "1248720"
  },
  {
    "text": "sorting by duration descendant so we're doing this replace thing why",
    "start": "1248720",
    "end": "1254000"
  },
  {
    "text": "are we doing this so the trace ids when they get stored in prompt",
    "start": "1254000",
    "end": "1260080"
  },
  {
    "text": "scale they have they use a uuid format so they have dashes in them",
    "start": "1260080",
    "end": "1266399"
  },
  {
    "start": "1266000",
    "end": "1353000"
  },
  {
    "text": "but you'll notice that this trace id here is underlined this is because this is a link we've made this a link and so",
    "start": "1266640",
    "end": "1273760"
  },
  {
    "text": "we'll actually if you click on it and any of those traces will open",
    "start": "1273760",
    "end": "1279440"
  },
  {
    "text": "the grafana ui to show the district an individual distributor trace which is similar it",
    "start": "1279440",
    "end": "1285039"
  },
  {
    "text": "basically reduces the code from jager and so with this you know you don't need to copy and paste the trace id you can",
    "start": "1285039",
    "end": "1291200"
  },
  {
    "text": "actually just use this linking smart thing that we use thanks to you the amazing capabilities that grafana",
    "start": "1291200",
    "end": "1296640"
  },
  {
    "text": "provides that are very flexible you can jump straight into that slow trace and you can check and try to",
    "start": "1296640",
    "end": "1303440"
  },
  {
    "text": "understand what's going on as you see you know there are a lot of those spans that are very quick but they're always",
    "start": "1303440",
    "end": "1308720"
  },
  {
    "text": "you know a few of them that are slow and if you check closely you'll see that those that are slow",
    "start": "1308720",
    "end": "1314960"
  },
  {
    "text": "actually belong to this um digit and actually it's the random digit",
    "start": "1314960",
    "end": "1321520"
  },
  {
    "text": "function that it's slow okay you can see it you know very quickly here so you could actually go back to your code",
    "start": "1321520",
    "end": "1328960"
  },
  {
    "text": "the random digit method or function in your code and check you know try to understand you",
    "start": "1328960",
    "end": "1334480"
  },
  {
    "text": "know why that is a slow okay so very quickly we've nailed down that the problem is related",
    "start": "1334480",
    "end": "1340000"
  },
  {
    "text": "to this specific function at least in this trace you know we could look at other traces and see maybe the",
    "start": "1340000",
    "end": "1345600"
  },
  {
    "text": "problems would be different but in this case you know that is that is the problem that is causing this trace to be slow",
    "start": "1345600",
    "end": "1352639"
  },
  {
    "start": "1353000",
    "end": "1566000"
  },
  {
    "text": "okay let's go back to our dashboard and now let's take a look at something even more",
    "start": "1354080",
    "end": "1359679"
  },
  {
    "text": "interesting service dependencies",
    "start": "1359679",
    "end": "1365280"
  },
  {
    "text": "and so this is a service map and it helps us to quickly understand how our different services interact",
    "start": "1368799",
    "end": "1376559"
  },
  {
    "text": "for this dependency map we're using grafana's node graph",
    "start": "1376559",
    "end": "1381679"
  },
  {
    "text": "which at the time of this webinar is still in beta so let's take a look as you can see",
    "start": "1381679",
    "end": "1387919"
  },
  {
    "text": "we're using here the node graph and the node graph",
    "start": "1387919",
    "end": "1394880"
  },
  {
    "text": "panel from grafana expects two queries one to retrieve the nodes in the graph",
    "start": "1394880",
    "end": "1400640"
  },
  {
    "text": "the first query here and that is the notes in the graphics the circles and another one to retrieve the edges in the",
    "start": "1400640",
    "end": "1407360"
  },
  {
    "text": "graph and that is the arrows and so this is the query to get the list",
    "start": "1407360",
    "end": "1414240"
  },
  {
    "text": "of nodes and basically we just retrieve all service names that",
    "start": "1414240",
    "end": "1419440"
  },
  {
    "text": "appeared in spans in the currently selecting selected time window in grafana",
    "start": "1419440",
    "end": "1426720"
  },
  {
    "text": "id and title are two parameters the node graph expects",
    "start": "1426720",
    "end": "1431919"
  },
  {
    "text": "id uniquely identifies a node and title is the label",
    "start": "1431919",
    "end": "1437440"
  },
  {
    "text": "that is assigned to the node and so we're using service name in both cases",
    "start": "1437440",
    "end": "1444080"
  },
  {
    "text": "the second query is more interesting and it has to return the arrows so the",
    "start": "1444080",
    "end": "1449360"
  },
  {
    "text": "relationships in between the services",
    "start": "1449360",
    "end": "1454000"
  },
  {
    "text": "this is actually something that is typically or usually impossible with the limited query language that other",
    "start": "1454640",
    "end": "1461200"
  },
  {
    "text": "observability backends provide but because we can leverage the full capabilities of sql provided by password",
    "start": "1461200",
    "end": "1467600"
  },
  {
    "text": "sql we can do joins and in this case",
    "start": "1467600",
    "end": "1473200"
  },
  {
    "text": "we join the spine view the span view with itself to identify paren and child",
    "start": "1473200",
    "end": "1479760"
  },
  {
    "text": "we're using k here for kid so identify parent and child spans",
    "start": "1479760",
    "end": "1486799"
  },
  {
    "text": "that are related to each other to do that we have to check",
    "start": "1486799",
    "end": "1492000"
  },
  {
    "text": "that the span id of the parent is the same",
    "start": "1492000",
    "end": "1498720"
  },
  {
    "text": "as the parent span id of the child span",
    "start": "1498720",
    "end": "1504080"
  },
  {
    "text": "we add two additional conditions so the first one is not strictly needed",
    "start": "1504720",
    "end": "1512720"
  },
  {
    "text": "but what we're doing here is we're ensuring that both the parent and the child span",
    "start": "1513520",
    "end": "1519840"
  },
  {
    "text": "are part of the same trace id and this would only make sense in cases where there are two spans that were",
    "start": "1519840",
    "end": "1526080"
  },
  {
    "text": "assigned the same span id which is very unlikely to happen the other condition the one at the",
    "start": "1526080",
    "end": "1532400"
  },
  {
    "text": "bottom is actually very important because it ensures that we only look at parent-child relationships across",
    "start": "1532400",
    "end": "1540840"
  },
  {
    "text": "services that is a service operation calling an operation in another service",
    "start": "1540840",
    "end": "1547520"
  },
  {
    "text": "and so we remove inter-service relationships that is an operation in a service calling another",
    "start": "1547520",
    "end": "1553760"
  },
  {
    "text": "operation in the same service because we don't want to show those in this map where we're interested in",
    "start": "1553760",
    "end": "1560480"
  },
  {
    "text": "cross-service dependencies",
    "start": "1560480",
    "end": "1564480"
  },
  {
    "start": "1566000",
    "end": "1681000"
  },
  {
    "text": "this table here shows the same relationships as the service map but in a table in a table format with",
    "start": "1568000",
    "end": "1574240"
  },
  {
    "text": "some additional stats so you can see we have you know number of calls total execution time and",
    "start": "1574240",
    "end": "1580960"
  },
  {
    "text": "average execution time if we look at the query behind",
    "start": "1580960",
    "end": "1587039"
  },
  {
    "text": "it's the same so in this case as i said this is a table we're showing a table so this is a table panel uh grafana's table",
    "start": "1587039",
    "end": "1593039"
  },
  {
    "text": "panel and the query pretty much uses the same join okay so it's very similar uses the",
    "start": "1593039",
    "end": "1598400"
  },
  {
    "text": "same join but we're showing you know a set of different stats so we're grouping by uh",
    "start": "1598400",
    "end": "1604320"
  },
  {
    "text": "source target and uh span name so that that's the grouping that we're using and then we're showing how many",
    "start": "1604320",
    "end": "1610640"
  },
  {
    "text": "calls are happening from the source to this source service to this target service and",
    "start": "1610640",
    "end": "1615919"
  },
  {
    "text": "operation and um the total execution time of um",
    "start": "1615919",
    "end": "1621919"
  },
  {
    "text": "you know that was spent so we just some spans and we just compute how much time",
    "start": "1621919",
    "end": "1627760"
  },
  {
    "text": "has been spent in this specific operation across all spans within the selected time window and then the",
    "start": "1627760",
    "end": "1632960"
  },
  {
    "text": "average execution of that of that span",
    "start": "1632960",
    "end": "1639279"
  },
  {
    "text": "and so here very quickly we can see that you know most of the time is actually spent",
    "start": "1639279",
    "end": "1644960"
  },
  {
    "text": "uh in the generator calling the lower service the lower service calling the digit service and the generator calling",
    "start": "1644960",
    "end": "1650559"
  },
  {
    "text": "the digit service so i mean it seems to be that the problem is actually in the digit service that's the service that is very slow",
    "start": "1650559",
    "end": "1656799"
  },
  {
    "text": "and um i think we already saw that you know when we looked at the um traces of the specific trace and we saw",
    "start": "1656799",
    "end": "1663120"
  },
  {
    "text": "that a lot of time was suspended digit service so this is just you know reinforcing that and that is not just an individual",
    "start": "1663120",
    "end": "1669120"
  },
  {
    "text": "current most likely but this is happening consistently across or over time and",
    "start": "1669120",
    "end": "1674720"
  },
  {
    "text": "across multiple requests",
    "start": "1674720",
    "end": "1678679"
  },
  {
    "text": "so we're now going to take a look at an other way to explore and visualize",
    "start": "1682880",
    "end": "1690000"
  },
  {
    "text": "trace data so again we're going to be using the node graph panel",
    "start": "1690000",
    "end": "1697200"
  },
  {
    "text": "but in this case we're trying to solve a different problem",
    "start": "1697200",
    "end": "1702960"
  },
  {
    "text": "imagine then that one of your services is unexpectedly going through a high increasing load",
    "start": "1703440",
    "end": "1710960"
  },
  {
    "text": "and understanding where that load is coming from in a micro services environment is not easy because you",
    "start": "1710960",
    "end": "1716960"
  },
  {
    "text": "would need to check all the different option services that end up calling the service under",
    "start": "1716960",
    "end": "1722840"
  },
  {
    "text": "pressure so let's select a different service here let's go for example for the digit service",
    "start": "1722840",
    "end": "1729360"
  },
  {
    "text": "and so if we look at the digit service and the you know the the slash which is the entry point operation which is",
    "start": "1729360",
    "end": "1735600"
  },
  {
    "text": "here we see in this tree we see that you know this is being called by the generator um",
    "start": "1735600",
    "end": "1743679"
  },
  {
    "text": "it's called by the generator through it you know you get is a http get request to the service but it's also called by",
    "start": "1743679",
    "end": "1749200"
  },
  {
    "text": "the lowest service and we see that uh this is you know that there is a",
    "start": "1749200",
    "end": "1755520"
  },
  {
    "text": "you know digit operation in the lower service that is uh end ups calling digit which is i mean we already saw in the",
    "start": "1755520",
    "end": "1761840"
  },
  {
    "text": "service map that is probably wrong but the thing that is interesting as well is there is quite a bit of load going",
    "start": "1761840",
    "end": "1767360"
  },
  {
    "text": "to that service through this path okay so it's you know close to half of the load is generated via this path and half",
    "start": "1767360",
    "end": "1774960"
  },
  {
    "text": "of the rest of the load you know will be generated by this path which is the correct one so we see that this digit server is probably under",
    "start": "1774960",
    "end": "1781360"
  },
  {
    "text": "pressure we're doubling the amount of work it needs to do uh because we have you know so there's",
    "start": "1781360",
    "end": "1787360"
  },
  {
    "text": "some something wrong in our code in this case and again i mean we could have you know a lot of other hopes you know in",
    "start": "1787360",
    "end": "1793919"
  },
  {
    "text": "the uh tree of spans or operations until we hit this service and we could use this visualization",
    "start": "1793919",
    "end": "1800320"
  },
  {
    "text": "to quickly spot um where most of the requests are coming from again the number here inside what we're",
    "start": "1800320",
    "end": "1807200"
  },
  {
    "text": "doing is showing the total account of spans for that specific operation or",
    "start": "1807200",
    "end": "1814000"
  },
  {
    "text": "what is the same saying the same thing is the total number of times that operation has been executed",
    "start": "1814000",
    "end": "1819600"
  },
  {
    "text": "in the selected time window which in this case is 15 minutes",
    "start": "1819600",
    "end": "1824480"
  },
  {
    "start": "1825000",
    "end": "2191000"
  },
  {
    "text": "and so again as i said we're using the node graph panel and we have again the two queries since those two queries are",
    "start": "1826720",
    "end": "1832000"
  },
  {
    "text": "a bit long i'm going to move to a text editor to review them",
    "start": "1832000",
    "end": "1838480"
  },
  {
    "text": "so the first thing to note is that you know doing this kind of thing like",
    "start": "1840159",
    "end": "1845200"
  },
  {
    "text": "going up you know in the chain of calls is something that would be very tedious",
    "start": "1845200",
    "end": "1850720"
  },
  {
    "text": "you know if you had to do this without a powerful query language because basically we need to recursively",
    "start": "1850720",
    "end": "1856000"
  },
  {
    "text": "traverse the tree of spans up across all traces that involve our problematic",
    "start": "1856000",
    "end": "1863840"
  },
  {
    "text": "and so you know luckily you know we can leverage the power of sql again and in this case",
    "start": "1863840",
    "end": "1870399"
  },
  {
    "text": "what we use is a recursive query okay so we use this construct our recursive query and the way it works is that there",
    "start": "1870399",
    "end": "1877440"
  },
  {
    "text": "is an initial query that will get executed which is this one and we see you know service and operation are the",
    "start": "1877440",
    "end": "1883039"
  },
  {
    "text": "ones that you selected from the drop downs in grafana",
    "start": "1883039",
    "end": "1888399"
  },
  {
    "text": "and we it runs this query which is retrieving basically all spans you know",
    "start": "1888399",
    "end": "1893840"
  },
  {
    "text": "i have you know some data for all expands that match this specific service and operation",
    "start": "1893840",
    "end": "1899360"
  },
  {
    "text": "and then it runs the results so x you know our are the results from this initial query",
    "start": "1899360",
    "end": "1906799"
  },
  {
    "text": "it runs them through this other query and basically what this is doing is",
    "start": "1906799",
    "end": "1912480"
  },
  {
    "text": "a join where it checks that x so the uh",
    "start": "1912480",
    "end": "1918080"
  },
  {
    "text": "the results from the the it looks at the results from the",
    "start": "1918080",
    "end": "1923519"
  },
  {
    "text": "original query reads the parent span id and then it checks for you know this new table that we're",
    "start": "1923519",
    "end": "1929120"
  },
  {
    "text": "joining again which is again the same you know span view um uh table if you want",
    "start": "1929120",
    "end": "1935200"
  },
  {
    "text": "it collects it it looks at uh comparing and ensure that we retrieve the parents okay so basically s in this case will",
    "start": "1935200",
    "end": "1941440"
  },
  {
    "text": "represent the parent of x so we're going up one level and we're projecting all these different values",
    "start": "1941440",
    "end": "1948399"
  },
  {
    "text": "from the parent span and because this is recursive it will do the same thing again so it will take the",
    "start": "1948399",
    "end": "1953760"
  },
  {
    "text": "results that we just got this results here and run this scoring again",
    "start": "1953760",
    "end": "1959120"
  },
  {
    "text": "against it again so it will do again the recursive thing it will check okay so i have to look at this uh the values",
    "start": "1959120",
    "end": "1966080"
  },
  {
    "text": "that i have i inject them you know i inject them into x here and again we look for okay for each of",
    "start": "1966080",
    "end": "1972720"
  },
  {
    "text": "these spans that were returned here let's look for the parents okay and let's retrieve the parent spans and we",
    "start": "1972720",
    "end": "1978559"
  },
  {
    "text": "do that again and again and again until there are no results returned okay so this is how this recursiveness works and",
    "start": "1978559",
    "end": "1985679"
  },
  {
    "text": "once it has built you know that table because you see this union all is just appending all those results the results",
    "start": "1985679",
    "end": "1991120"
  },
  {
    "text": "from the first query and all the subsequent queries that is that navigating um upstream you know through the spans",
    "start": "1991120",
    "end": "1998720"
  },
  {
    "text": "it runs on those results it runs this query okay which is doing okay uh returning",
    "start": "1998720",
    "end": "2004480"
  },
  {
    "text": "uh using a span is the service name and the span name the operation to generate an id so this is will be we're",
    "start": "2004480",
    "end": "2011039"
  },
  {
    "text": "generating one node for each service name and span name something important to notice here is that we're not",
    "start": "2011039",
    "end": "2016880"
  },
  {
    "text": "excluding inter-service operations because we're actually interested in seeing them in case",
    "start": "2016880",
    "end": "2022080"
  },
  {
    "text": "the increase in calls was coming from an internal operation within the service and not",
    "start": "2022080",
    "end": "2027279"
  },
  {
    "text": "generated from something outside it could be maybe something wrong you know a new deployment that we made and maybe cause",
    "start": "2027279",
    "end": "2032960"
  },
  {
    "text": "that problem so so we're not excluding and we're actually including inter service",
    "start": "2032960",
    "end": "2038399"
  },
  {
    "text": "operations as well and then we add service name as a subtitle you know of the node",
    "start": "2038399",
    "end": "2045440"
  },
  {
    "text": "so we had a span name as a title of the node service name as a subtitle of the node and then we're counting the number",
    "start": "2045440",
    "end": "2051200"
  },
  {
    "text": "of spans the number and we use this thing just in case the the you know a span id for some reason during this",
    "start": "2051200",
    "end": "2056480"
  },
  {
    "text": "recursive operation created some duplicates in theory i don't think that's necessarily needed",
    "start": "2056480",
    "end": "2062240"
  },
  {
    "text": "but just in case we use this thing so that we remove any any potential duplicates there and we have an accurate",
    "start": "2062240",
    "end": "2068398"
  },
  {
    "text": "count and uh and then so we and then what we do is we're grouping you know those",
    "start": "2068399",
    "end": "2074560"
  },
  {
    "text": "results by service name and span name so that basically grouping by node",
    "start": "2074560",
    "end": "2080560"
  },
  {
    "text": "so these are this is the query for the nodes and the edges uses a very similar",
    "start": "2080560",
    "end": "2086000"
  },
  {
    "text": "query but so again you see this join here what is traversing app you know",
    "start": "2086000",
    "end": "2091118"
  },
  {
    "text": "from the current set of results let's get the parents and project them but it also adds a bunch of additional",
    "start": "2091119",
    "end": "2097440"
  },
  {
    "text": "information because in here we're interested in the edges so we're project projecting",
    "start": "2097440",
    "end": "2104000"
  },
  {
    "text": "the um the id for the relationship which is you",
    "start": "2104000",
    "end": "2109280"
  },
  {
    "text": "know service name span name from the source",
    "start": "2109280",
    "end": "2114880"
  },
  {
    "text": "uh to the uh service name and the spine name of the child so that is you know the",
    "start": "2114880",
    "end": "2121359"
  },
  {
    "text": "relationship between two notes essentially in the um graph that we're displaying",
    "start": "2121359",
    "end": "2128640"
  },
  {
    "text": "and then we also are doing the um uh target and source",
    "start": "2128640",
    "end": "2137599"
  },
  {
    "text": "we're using uh the md yeah okay we're doing an md5 on the service name and a span name again to compute ids for those",
    "start": "2137599",
    "end": "2146480"
  },
  {
    "text": "and then we just project here again the same thing where we're projecting is the target",
    "start": "2146480",
    "end": "2152079"
  },
  {
    "text": "uh the id the target and the source you know so the the node panel can actually",
    "start": "2152079",
    "end": "2157599"
  },
  {
    "text": "connect the dots between the uh the services so they need to we need to use the same",
    "start": "2157599",
    "end": "2164000"
  },
  {
    "text": "id in here for targeted source we it's constructed as you can see the same way it was constructed in the node so that",
    "start": "2164000",
    "end": "2170560"
  },
  {
    "text": "you know again the node graph panel can identify those",
    "start": "2170560",
    "end": "2175599"
  },
  {
    "text": "those nodes and make the connection with an arrow",
    "start": "2175599",
    "end": "2181000"
  },
  {
    "text": "okay so we saw we've seen how we can troubleshoot scenarios where you know we",
    "start": "2194079",
    "end": "2199680"
  },
  {
    "text": "have a service that is having some issues you know we can actually navigate up",
    "start": "2199680",
    "end": "2204800"
  },
  {
    "text": "through the stream of uh through the sequence of spans in all across all the different traces to understand",
    "start": "2204800",
    "end": "2212400"
  },
  {
    "text": "the uh how did how this service is being called you know what's the impact of things happening upstream into the",
    "start": "2212400",
    "end": "2218079"
  },
  {
    "text": "service we're uh looking at we can do something similar",
    "start": "2218079",
    "end": "2223760"
  },
  {
    "text": "but in this case using downstream expanse okay so let me let me make this",
    "start": "2223760",
    "end": "2230240"
  },
  {
    "text": "bigger and this is showing again uh here i have selected uh generator and http get uh so",
    "start": "2230240",
    "end": "2237040"
  },
  {
    "text": "let's actually select generator and the generate uh",
    "start": "2237040",
    "end": "2243280"
  },
  {
    "text": "uh operation because that is the entry point and so this is showing an entire",
    "start": "2243280",
    "end": "2248400"
  },
  {
    "text": "map of all the requests you know that go through the service what are all the different services and operations that",
    "start": "2248400",
    "end": "2254079"
  },
  {
    "text": "are being called you know and how often they are etc and we're using the same technique",
    "start": "2254079",
    "end": "2260480"
  },
  {
    "text": "that we use in the upstream spanner action dependencies dashboard",
    "start": "2260480",
    "end": "2268320"
  },
  {
    "text": "the only difference is that in this case the join is the other way around okay so we're looking",
    "start": "2268320",
    "end": "2275200"
  },
  {
    "text": "before we had the x pattern span id equals s span id",
    "start": "2275200",
    "end": "2280880"
  },
  {
    "text": "so here is is the other way around so we're looking for xs pan id",
    "start": "2280880",
    "end": "2286079"
  },
  {
    "text": "being the same as the uh pattern spanning so we're just going you know downstream",
    "start": "2286320",
    "end": "2292240"
  },
  {
    "text": "okay and then we project the children and then again we do the same same operation here so it's a very very",
    "start": "2292240",
    "end": "2298320"
  },
  {
    "text": "similar thing so i will not explain it in detail but you can just show me you can navigate upstream but you could also",
    "start": "2298320",
    "end": "2304079"
  },
  {
    "text": "navigate downstream and you know this gives you a very interesting map of all the different calls that happen in",
    "start": "2304079",
    "end": "2310800"
  },
  {
    "text": "the service across you know the you know in this case the last 50 minutes",
    "start": "2310800",
    "end": "2316079"
  },
  {
    "text": "once uh you know for for all the requests to the generator service",
    "start": "2316079",
    "end": "2322240"
  },
  {
    "text": "so it helps you understand in detail at the end of the day what's happening you know how how are the different requests being processed",
    "start": "2322240",
    "end": "2329519"
  },
  {
    "text": "another thing that i'll explain here in this dashboard that is interesting as well is this one and this one is looking",
    "start": "2333440",
    "end": "2339119"
  },
  {
    "text": "at the total execution time but operation but it's not doing this just by blindly",
    "start": "2339119",
    "end": "2346320"
  },
  {
    "text": "adding up the duration of all spans for that specific operation",
    "start": "2346320",
    "end": "2352800"
  },
  {
    "text": "is actually looking at time actually spent in the code of that operation that is it",
    "start": "2352800",
    "end": "2358560"
  },
  {
    "text": "is subtracting the time spent in child spans",
    "start": "2358560",
    "end": "2364160"
  },
  {
    "text": "okay so you have an operation it has some code and then it makes request to other operations to have their you know",
    "start": "2364160",
    "end": "2369280"
  },
  {
    "text": "that are tracking their own spans so instead of telling you that you know the",
    "start": "2369280",
    "end": "2375680"
  },
  {
    "text": "the high level span is the one that is taking the longest no it's actually looking at how much time is spent within",
    "start": "2375680",
    "end": "2381680"
  },
  {
    "text": "that the code of that span and so that you can identify where the",
    "start": "2381680",
    "end": "2387599"
  },
  {
    "text": "bottleneck is because otherwise it will always the one that is at the top of the hierarchy will be the one that shows us",
    "start": "2387599",
    "end": "2393440"
  },
  {
    "text": "being the slowest but here it's not the case you know if you see this query the slowest one where most of the time is",
    "start": "2393440",
    "end": "2399119"
  },
  {
    "text": "spent and we already have seen this you know over the course of this presentation is the digit random digit",
    "start": "2399119",
    "end": "2406480"
  },
  {
    "text": "method or function is where most of the times i spent 88 percent of the time the spender so definitely this is the first",
    "start": "2406480",
    "end": "2412240"
  },
  {
    "text": "place we should go to optimize the performance of our service if we didn't be subtracting",
    "start": "2412240",
    "end": "2418720"
  },
  {
    "text": "uh time from child spans the one at the top would have been the generate",
    "start": "2418720",
    "end": "2424880"
  },
  {
    "text": "password from the generator service because that's the top level one and so all the time is adding up into the",
    "start": "2424880",
    "end": "2431119"
  },
  {
    "text": "duration of that span so how do we do that this is actually really important you know it's this idea of okay i'm looking at where specifically in which",
    "start": "2431119",
    "end": "2438560"
  },
  {
    "text": "code is specifically the time spent and by doing this subtraction of okay the parent span",
    "start": "2438560",
    "end": "2444240"
  },
  {
    "text": "duration i re i subtract the time span in the children then i get",
    "start": "2444240",
    "end": "2449599"
  },
  {
    "text": "the actual uh execution time within that that specific uh code it's really",
    "start": "2449599",
    "end": "2456160"
  },
  {
    "text": "helpful to understand bottlenecks and the way we do it is again we use is the same thing again we use a recursive",
    "start": "2456160",
    "end": "2461280"
  },
  {
    "text": "query to traverse all spans and assign time to the different um actual time",
    "start": "2461280",
    "end": "2468560"
  },
  {
    "text": "spent you know in the different spans and what we do is this thing that you see here this is the key",
    "start": "2468560",
    "end": "2473599"
  },
  {
    "text": "this is the key thing and what it's doing is subtracting to the parent span duration is",
    "start": "2473599",
    "end": "2480960"
  },
  {
    "text": "subtracting the sum of the duration of all the children okay so it's looking",
    "start": "2480960",
    "end": "2486720"
  },
  {
    "text": "where um the span id you know this this span here is the parent okay so so it's",
    "start": "2486720",
    "end": "2493280"
  },
  {
    "text": "subtracting all this time and coalesce what it's doing is if this return null so no data it just says it's zero",
    "start": "2493280",
    "end": "2500240"
  },
  {
    "text": "so it basically doesn't there is there is no number you don't need to subtract any time to the duration it's this is only",
    "start": "2500240",
    "end": "2506400"
  },
  {
    "text": "useful for leaf spans you know they don't have any children",
    "start": "2506400",
    "end": "2511960"
  },
  {
    "text": "all right so we've reached the end of the this webinar",
    "start": "2529760",
    "end": "2535040"
  },
  {
    "text": "i hope that you enjoyed it uh we showed that with open telemetry prom skill and grafana you can get insights you didn't",
    "start": "2535280",
    "end": "2542560"
  },
  {
    "text": "think i don't think you would think you thought were possible thanks to the power of full sequel",
    "start": "2542560",
    "end": "2548160"
  },
  {
    "text": "i encourage all of you to download the open telemetry demo today all the software we've shown here",
    "start": "2548160",
    "end": "2554240"
  },
  {
    "text": "is available on github and it's free to use and if you have questions about prompt scale or the demo environment",
    "start": "2554240",
    "end": "2560640"
  },
  {
    "text": "we're available in the pram scale channel in our slack community that we see you see here i just wanted to take",
    "start": "2560640",
    "end": "2567680"
  },
  {
    "text": "the time to thank you for uh for watching this webinar and i hope to see",
    "start": "2567680",
    "end": "2572720"
  },
  {
    "text": "you on our slack community soon",
    "start": "2572720",
    "end": "2577480"
  }
]