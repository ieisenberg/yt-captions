[
  {
    "start": "0",
    "end": "28000"
  },
  {
    "text": "all right hi everybody my name is flavio castelli i'm distinguished engineer at susem",
    "start": "799",
    "end": "7200"
  },
  {
    "text": "and i'm here today to talk with you about the implication of sharing large cluster",
    "start": "7200",
    "end": "13200"
  },
  {
    "text": "versus having lots of smaller kubernetes clusters and especially how to actually share",
    "start": "13200",
    "end": "18800"
  },
  {
    "text": "large cluster with uh with many users what are the implications the advantages",
    "start": "18800",
    "end": "24080"
  },
  {
    "text": "disadvantages with drawbacks so the common question is",
    "start": "24080",
    "end": "30160"
  },
  {
    "start": "28000",
    "end": "28000"
  },
  {
    "text": "i'm an operator within an organization and i have multiple teams that want to",
    "start": "30160",
    "end": "35920"
  },
  {
    "text": "have their kubernetes cluster so should i go ahead and create many small clusters maybe one per team",
    "start": "35920",
    "end": "42719"
  },
  {
    "text": "or should i just create a large kubernetes cluster but then i'm going to share with uh with different uh users",
    "start": "42719",
    "end": "50239"
  },
  {
    "text": "different groups different tenants and especially if i decide to go in this direction",
    "start": "50239",
    "end": "55600"
  },
  {
    "text": "how safe is going to be this sharing of a single cluster so the the short answer to to that is",
    "start": "55600",
    "end": "62239"
  },
  {
    "start": "62000",
    "end": "62000"
  },
  {
    "text": "that there is no right or wrong approach there are limitations and benefits with uh both ways",
    "start": "62239",
    "end": "68159"
  },
  {
    "text": "it's it's really up to what are your requirements what are your constraints what are",
    "start": "68159",
    "end": "73280"
  },
  {
    "text": "uh the the problems that you're going to solve so today i'm going to try to",
    "start": "73280",
    "end": "78479"
  },
  {
    "text": "highlight to you um the different uh outcomes the different uh um advantages",
    "start": "78479",
    "end": "85040"
  },
  {
    "text": "and disadvantages so that you can then take a more educated decision on on which way",
    "start": "85040",
    "end": "90240"
  },
  {
    "text": "to proceed so to to to provide a really quick overview if you're going to deploy many",
    "start": "90240",
    "end": "96159"
  },
  {
    "start": "92000",
    "end": "92000"
  },
  {
    "text": "small clusters uh you are going to have the strongest isolation",
    "start": "96159",
    "end": "101200"
  },
  {
    "text": "possible between the different tenants this is the best uh one you can get you're also going to be uh able to pick",
    "start": "101200",
    "end": "107920"
  },
  {
    "text": "up a different version of kubernetes of different version of the kubernetes add-ons that are going to run on top of this cluster",
    "start": "107920",
    "end": "114799"
  },
  {
    "text": "and you're going to also allow the the tenants of this cluster to have certain",
    "start": "114799",
    "end": "120000"
  },
  {
    "text": "level of administration privileges over over this cluster so they're going to be able to",
    "start": "120000",
    "end": "125840"
  },
  {
    "text": "to deploy a almost any kind of workloads it all depends on how much freedom you want to give them",
    "start": "125840",
    "end": "132160"
  },
  {
    "text": "on the on the drawback side you will also have to to face higher maintenance because",
    "start": "132160",
    "end": "137840"
  },
  {
    "text": "someone is going to have to look after all these clusters you will have to enforce policies of",
    "start": "137840",
    "end": "143840"
  },
  {
    "text": "your company across all this different cluster you have to make sure that all the policies are enforced and kept in sync",
    "start": "143840",
    "end": "150480"
  },
  {
    "text": "across all of these places and depending on how you are going to deploy which infrastructure you're going to",
    "start": "150480",
    "end": "156560"
  },
  {
    "text": "deploy you might end up having very hard organization higher costs so these are all things",
    "start": "156560",
    "end": "162400"
  },
  {
    "text": "that you have to keep in mind on the other end if you're going to share large cluster between",
    "start": "162400",
    "end": "167599"
  },
  {
    "start": "165000",
    "end": "165000"
  },
  {
    "text": "different tenants you're going of course to reduce the maintenance because you have a single infrastructure to look after you have",
    "start": "167599",
    "end": "174560"
  },
  {
    "text": "also going to have an easier time in enforcing and keeping up to date and synchronize the company",
    "start": "174560",
    "end": "180560"
  },
  {
    "text": "policies that you might have in place you're definitely going to have a better hardware utilization on the other end",
    "start": "180560",
    "end": "186879"
  },
  {
    "text": "uh you're going to to to trade for some security and we'll see more about that um in the next slides in",
    "start": "186879",
    "end": "194159"
  },
  {
    "text": "the rest of the presentation to be to be fair and then you're going to also trade some flexibility for the end user",
    "start": "194159",
    "end": "200640"
  },
  {
    "text": "that will have to stick with the same version of kubernetes that is powering the underlying cluster they",
    "start": "200640",
    "end": "206640"
  },
  {
    "text": "might have to stick with the same monitoring logging that is provided by by the wall uh",
    "start": "206640",
    "end": "212159"
  },
  {
    "text": "platform if you want to consolidate it around that and you will have also to face",
    "start": "212159",
    "end": "217280"
  },
  {
    "text": "more requests coming from them because they want to be able to deploy certain kind of",
    "start": "217280",
    "end": "222720"
  },
  {
    "text": "of of workloads without uh without opening a tickets and have someone with the right privileges to do that",
    "start": "222720",
    "end": "229760"
  },
  {
    "text": "so um today i'm going to to show you to focus more on on the",
    "start": "229760",
    "end": "235040"
  },
  {
    "start": "231000",
    "end": "231000"
  },
  {
    "text": "problem of sharing a cluster because we all know what are the implication of having a dedicated cluster but how how much",
    "start": "235040",
    "end": "241840"
  },
  {
    "text": "secure can i make this this cluster if i share that between uh different tenants",
    "start": "241840",
    "end": "247120"
  },
  {
    "text": "along this journey we will discover that kubernetes has already many features built into inside",
    "start": "247120",
    "end": "252720"
  },
  {
    "text": "of it that can help us to achieve that and where kubernetes by by its own by the batteries included",
    "start": "252720",
    "end": "259199"
  },
  {
    "text": "can do that we can look for help inside of a huge kubernetes ecosystem most important of all we will see what",
    "start": "259199",
    "end": "266240"
  },
  {
    "text": "are the limits how much we can go with with isolation so that by knowing these limits we can",
    "start": "266240",
    "end": "272960"
  },
  {
    "text": "make a more educated decision about which path to choose between sharing or having multiple",
    "start": "272960",
    "end": "278639"
  },
  {
    "text": "dedicated clusters so today's journeys personas are the cluster operators which are the",
    "start": "278639",
    "end": "284800"
  },
  {
    "text": "owner of this large kubernetes cluster they have ultimate access to it and they are the ones who are onboarding",
    "start": "284800",
    "end": "290960"
  },
  {
    "text": "the cluster tenants and setting policies uh so that they can uh",
    "start": "290960",
    "end": "296000"
  },
  {
    "text": "stay in place and they can uh they can actually share in a more secure way this cluster",
    "start": "296000",
    "end": "302720"
  },
  {
    "text": "the foundation block of everything comes straight from kubernetes and is a",
    "start": "302720",
    "end": "308240"
  },
  {
    "start": "303000",
    "end": "303000"
  },
  {
    "text": "kubernetes namespace object and it's interesting to point out that by looking straight into kubernetes",
    "start": "308240",
    "end": "314400"
  },
  {
    "text": "documentation you can see that the namespace is is pointed out as a way to create virtual clusters",
    "start": "314400",
    "end": "322320"
  },
  {
    "text": "for for different users on top of an existing kubernetes cluster we're talking about virtual cluster",
    "start": "322320",
    "end": "327840"
  },
  {
    "text": "we're not talking about cluster running inside of the virtual machines we're talking about creating some",
    "start": "327840",
    "end": "333520"
  },
  {
    "text": "sandboxes uh some dedicated space inside of our an already existing",
    "start": "333520",
    "end": "338560"
  },
  {
    "text": "kubernetes cluster the the kubernetes namespace object is embedded into many other",
    "start": "338560",
    "end": "345199"
  },
  {
    "start": "341000",
    "end": "341000"
  },
  {
    "text": "kubernetes resources the majority of kubernetes resources",
    "start": "345199",
    "end": "350400"
  },
  {
    "text": "are namespace bound which means they belong to a namespace there are also some resources that are",
    "start": "350400",
    "end": "356639"
  },
  {
    "text": "cluster-wide and this is some implication that we will see later on the namespace as pointed by the",
    "start": "356639",
    "end": "363759"
  },
  {
    "text": "kubernetes documentation can be used to create this playground but",
    "start": "363759",
    "end": "369280"
  },
  {
    "text": "it's not enough we want to put some fences around this playground this dedicated space for for each tenant to make sure that the",
    "start": "369280",
    "end": "375919"
  },
  {
    "text": "tenant can't go out out of it and that nobody can can get inside of it and and mess around with",
    "start": "375919",
    "end": "382160"
  },
  {
    "text": "with the workloads of the other tenants so how are we going to do that well first of all regardless of having",
    "start": "382160",
    "end": "389440"
  },
  {
    "start": "387000",
    "end": "387000"
  },
  {
    "text": "many cluster or having a single one of course you must have authentication in place the main goal of authentication",
    "start": "389440",
    "end": "395840"
  },
  {
    "text": "is to prevent unauthenticated users from doing actions against your cluster but also it's about to know who wants to",
    "start": "395840",
    "end": "403280"
  },
  {
    "text": "perform something inside of a cluster so an authenticated user will tell kubernetes api",
    "start": "403280",
    "end": "409759"
  },
  {
    "text": "uh from which user this request is coming from and to which groups this user belongs to",
    "start": "409759",
    "end": "415599"
  },
  {
    "text": "and this is important because the next step that we want to to perform is to patch some",
    "start": "415599",
    "end": "420880"
  },
  {
    "start": "417000",
    "end": "417000"
  },
  {
    "text": "to put some boundaries there and we do that by leveraging our back policies which",
    "start": "420880",
    "end": "426160"
  },
  {
    "text": "is uh another feature built into kubernetes which basically allows us to target",
    "start": "426160",
    "end": "431680"
  },
  {
    "text": "resources through the role or classroom objects so we can define a policy that",
    "start": "431680",
    "end": "436800"
  },
  {
    "text": "is affecting the kind of operation that the user can for example do against the pod against the service",
    "start": "436800",
    "end": "442479"
  },
  {
    "text": "can he create them can he list them can he delete them it's really really granular and then once we define these roles we",
    "start": "442479",
    "end": "449360"
  },
  {
    "text": "can bind these roles against a specific user or a group of user by either using role",
    "start": "449360",
    "end": "454479"
  },
  {
    "text": "binding or across the role binding what is really interesting is that kubernetes has",
    "start": "454479",
    "end": "459840"
  },
  {
    "start": "459000",
    "end": "459000"
  },
  {
    "text": "some built-in roles that are pretty good the most useful one for what we are going to to create are",
    "start": "459840",
    "end": "466080"
  },
  {
    "text": "the admin role which once bound to a namespace allow a user or a set of users",
    "start": "466080",
    "end": "472080"
  },
  {
    "text": "to perform basically read and write operation against many of the kubernetes resources but",
    "start": "472080",
    "end": "478080"
  },
  {
    "text": "also includes defining new outback policies for that namespace and dealing with the secrets that are",
    "start": "478080",
    "end": "484160"
  },
  {
    "text": "defined inside of this name space you also have another useful role which",
    "start": "484160",
    "end": "489199"
  },
  {
    "text": "is the edit role which is the most common one uh that you're going to end out to the tenants",
    "start": "489199",
    "end": "495039"
  },
  {
    "text": "of of uh of a namespace uh with this role they can create uh and manage the resources inside of it",
    "start": "495039",
    "end": "502080"
  },
  {
    "text": "all of them except for our back policies so that they can't let's say escalate they can also handle",
    "start": "502080",
    "end": "508080"
  },
  {
    "text": "secrets because this is useful to deploy their own workloads last but not least you have the view",
    "start": "508080",
    "end": "514719"
  },
  {
    "text": "role which provides free only access but doesn't give access to neither the pod security policies nor most important",
    "start": "514719",
    "end": "521518"
  },
  {
    "text": "of all the secrets this is a really interesting role that you can use to implement some auditing",
    "start": "521519",
    "end": "527360"
  },
  {
    "text": "access to to the namespaces so once you have defined the roles",
    "start": "527360",
    "end": "534080"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "you can use them to build these fence so that you can set a a series",
    "start": "534080",
    "end": "540959"
  },
  {
    "text": "of roles that are applying to to groups of users and you can set also some specific ones",
    "start": "540959",
    "end": "547839"
  },
  {
    "text": "that are targeting either specific resources or specific user or groups to grant them more or",
    "start": "547839",
    "end": "554160"
  },
  {
    "text": "less privileges this is however not enough the next isolation step that we have to perform",
    "start": "554160",
    "end": "560399"
  },
  {
    "text": "is to to isolate the workloads from a networking perspective because by default as shown on this picture",
    "start": "560399",
    "end": "567279"
  },
  {
    "text": "uh the workloads the parts that are scheduled inside of the namespaces of kubernetes they can talk with each other regardless",
    "start": "567279",
    "end": "573839"
  },
  {
    "text": "of where the pod is located and this is something that",
    "start": "573839",
    "end": "579120"
  },
  {
    "text": "in our case most of the time we want to prevent we want the pods of the of a tenant that are",
    "start": "579120",
    "end": "584800"
  },
  {
    "text": "living inside of a tenant namespace to be isolated from the others again luckily kubernetes provide us a",
    "start": "584800",
    "end": "590160"
  },
  {
    "start": "590000",
    "end": "590000"
  },
  {
    "text": "way to do that and it's an easy way to do it and it's by using a network policy the network policy that",
    "start": "590160",
    "end": "597440"
  },
  {
    "text": "you see on the right side of the screen it is actually isolating the pods that are living inside of a namespace called",
    "start": "597440",
    "end": "603680"
  },
  {
    "text": "tenant a it's isolating them from the other name spaces the pod to part networking inside of the",
    "start": "603680",
    "end": "610560"
  },
  {
    "text": "namespace is unaffected as it is unaffected the ability of the pods inside of his namespace to reach",
    "start": "610560",
    "end": "616560"
  },
  {
    "text": "out of the name space they can go to the internet they can also go to other parts inside of inside of ironing spaces",
    "start": "616560",
    "end": "624160"
  },
  {
    "text": "of a cluster but parts that are living outside of this namespace they can't reach out to",
    "start": "624160",
    "end": "630079"
  },
  {
    "text": "them so let's think a bit about the implication of this policy so on the on the good side of it",
    "start": "630079",
    "end": "637440"
  },
  {
    "start": "635000",
    "end": "635000"
  },
  {
    "text": "this is a policy that scales really well because it doesn't matter how many tenants we add to the to the",
    "start": "637440",
    "end": "643200"
  },
  {
    "text": "to the cluster uh once you define that namespace is isolated it is isolated from all the namespaces",
    "start": "643200",
    "end": "650000"
  },
  {
    "text": "all the tenants that you are going to later on board on the cluster on the current side there is definitely",
    "start": "650000",
    "end": "656240"
  },
  {
    "text": "some ingress traffic that you want to allow because out of the box this policy is too strict",
    "start": "656240",
    "end": "661760"
  },
  {
    "text": "if the if a green pod is exposing that you can see here the picture is exposing a service like",
    "start": "661760",
    "end": "667600"
  },
  {
    "text": "through an ingress or through a load balancer type of service you definitely want",
    "start": "667600",
    "end": "673040"
  },
  {
    "text": "everybody to be able to reach it but with the policy that i've just shown to you this is not doable so what you have to do",
    "start": "673040",
    "end": "679760"
  },
  {
    "text": "is to add a new policy on top of the one that i've shown to you before which is basically saying um i will",
    "start": "679760",
    "end": "686640"
  },
  {
    "start": "682000",
    "end": "682000"
  },
  {
    "text": "allow some special traffic to come into the namespace and this special traffic has",
    "start": "686640",
    "end": "692000"
  },
  {
    "text": "to come from a namespaces which have in this case a specific label so here i'm assuming",
    "start": "692000",
    "end": "699040"
  },
  {
    "text": "that the operator of a cluster has deployed like the ingress controller inside of the namespace which has a",
    "start": "699040",
    "end": "705839"
  },
  {
    "text": "specific label with a specific key that you can see over there and by doing that we restore the ability to expose",
    "start": "705839",
    "end": "713279"
  },
  {
    "text": "services from an isolated namespace this is enough from a networking point",
    "start": "713279",
    "end": "719839"
  },
  {
    "text": "of view what we have to take care about is also to preserve the resources of a",
    "start": "719839",
    "end": "725440"
  },
  {
    "start": "725000",
    "end": "725000"
  },
  {
    "text": "cluster we don't want to have one tenant starting the others",
    "start": "725440",
    "end": "730560"
  },
  {
    "text": "we don't want to we want to equally or in a more granular way distribute the resources of our tenants",
    "start": "730560",
    "end": "736959"
  },
  {
    "text": "so what we're going to do is to put limits on the amount of resources that each namespace can can can use and this",
    "start": "736959",
    "end": "744720"
  },
  {
    "text": "is done again by using something that is provided by kubernetes out of the box which is called resource coders resource",
    "start": "744720",
    "end": "751839"
  },
  {
    "start": "746000",
    "end": "746000"
  },
  {
    "text": "code as objects are name space so they are bound to names and they are going to affect",
    "start": "751839",
    "end": "757360"
  },
  {
    "text": "computer resources that can be used by the namespace so cpu memory they can also affect the kind of",
    "start": "757360",
    "end": "763519"
  },
  {
    "text": "extended resources that can be used like it's gpus cards but they can also influence the",
    "start": "763519",
    "end": "769600"
  },
  {
    "text": "amount of storage that can be consumed and the number of kubernetes objects then can be created inside of of the",
    "start": "769600",
    "end": "776160"
  },
  {
    "text": "namespace so let's dig a bit into the the storage because it's interesting here to point out that",
    "start": "776160",
    "end": "782079"
  },
  {
    "start": "779000",
    "end": "779000"
  },
  {
    "text": "as you know persistent storage in kubernetes is done by using persistent volumes which are objects that are cluster-wide",
    "start": "782079",
    "end": "789279"
  },
  {
    "text": "and by persistent volume claims which on the other end are namespaced and given that we have to operate on the",
    "start": "789279",
    "end": "795120"
  },
  {
    "text": "name at the namespace level what we can do with storage quotas is to put a limit on the persistent volume",
    "start": "795120",
    "end": "801200"
  },
  {
    "text": "claims and by doing that we're really flexible we can set a limit of a global amount of storage that can",
    "start": "801200",
    "end": "807040"
  },
  {
    "text": "be consumed or we can be really granular and say i want this namespace to be able to use 10 gigabytes",
    "start": "807040",
    "end": "813120"
  },
  {
    "text": "top of this really expensive storage type but it can use up to 40 gigabytes of",
    "start": "813120",
    "end": "818720"
  },
  {
    "text": "this less expensive storage type so again something built into kubernetes which is pretty flexible",
    "start": "818720",
    "end": "825920"
  },
  {
    "text": "last but not least you can put as i was mentioning before you can put limits",
    "start": "825920",
    "end": "830959"
  },
  {
    "text": "on the number of objects that can be created inside of a namespace and while at first glance this might not",
    "start": "830959",
    "end": "836720"
  },
  {
    "text": "be useful for example to say i'm going to put a limit on the number of parts inside of a namespace because",
    "start": "836720",
    "end": "842160"
  },
  {
    "text": "you you you're going to operate on them on another way like with uh putting limits on the conver",
    "start": "842160",
    "end": "848160"
  },
  {
    "text": "computational resources this is really useful if you want to limit",
    "start": "848160",
    "end": "853360"
  },
  {
    "text": "objects like services of type load bouncer when every time a user declares one of",
    "start": "853360",
    "end": "859199"
  },
  {
    "text": "them they're going to actually allocate a load balancer that you're going to pay for in a public cloud",
    "start": "859199",
    "end": "864880"
  },
  {
    "text": "or they're going to use a floating ip address on on top of your private infrastructure as a service so",
    "start": "864880",
    "end": "871839"
  },
  {
    "text": "you don't want to run out of these limited amount of resources or you don't want to to pay too much for for resources",
    "start": "871839",
    "end": "878560"
  },
  {
    "text": "provision on demand of the cloud and this is a really convenient and built-in way to limit that",
    "start": "878560",
    "end": "885600"
  },
  {
    "text": "now let's change again the topic and let's focus on something more",
    "start": "885600",
    "end": "891120"
  },
  {
    "text": "critical which is uh the security of the workloads that are running on top of the kubernetes",
    "start": "891120",
    "end": "896639"
  },
  {
    "text": "cluster so kubernetes is is designed main purpose is to run use end user workloads which are",
    "start": "896639",
    "end": "903680"
  },
  {
    "text": "scheduled by using pods and pods are used in linux containers and linux containers by definition",
    "start": "903680",
    "end": "910000"
  },
  {
    "text": "they're providing isolation of the workloads but the way they are designed the containerized application is sharing",
    "start": "910000",
    "end": "916800"
  },
  {
    "text": "the same kernel of the host and this is always posing some security implications some security threats",
    "start": "916800",
    "end": "923600"
  },
  {
    "text": "but these implications are even bigger and more important when you are sharing the infrastructure",
    "start": "923600",
    "end": "928800"
  },
  {
    "text": "with other tenants because you will have workloads coming from different tenants sharing the same nodes sharing the same",
    "start": "928800",
    "end": "935600"
  },
  {
    "text": "kernel host and so you you have a higher attack surface in this case so what can we do to make things more",
    "start": "935600",
    "end": "943120"
  },
  {
    "text": "secure to to raise the bar in terms of the security so this is a really condensed slide of a",
    "start": "943120",
    "end": "950720"
  },
  {
    "start": "949000",
    "end": "949000"
  },
  {
    "text": "really huge topic which is a good security habits for linux containers this is not exhaustive this is just a",
    "start": "950720",
    "end": "958560"
  },
  {
    "text": "brief overview about things that you you should definitely think carefully about doing like should",
    "start": "958560",
    "end": "963600"
  },
  {
    "text": "i allow my users to run privileged containers please know unless there is a really good reason uh you should also",
    "start": "963600",
    "end": "970399"
  },
  {
    "text": "aim to provide uh to have uh up armor profiles or se linux profiles for all",
    "start": "970399",
    "end": "976240"
  },
  {
    "text": "the workloads on your on your cluster in the medial world you would have a tailor-made profiles for your",
    "start": "976240",
    "end": "982320"
  },
  {
    "text": "application but if you can do that and it's understandable at least resort",
    "start": "982320",
    "end": "987360"
  },
  {
    "text": "to a generic one that is provided by your container runtime and the same applies to second profiles",
    "start": "987360",
    "end": "992880"
  },
  {
    "text": "and to other things that are mentioned over there so how can we enforce this good security",
    "start": "992880",
    "end": "998399"
  },
  {
    "text": "habits to all the tenants of our cluster again kubernetes provides something out of a",
    "start": "998399",
    "end": "1004320"
  },
  {
    "text": "box for us which are pod security policies these allow customer to sorry this allow",
    "start": "1004320",
    "end": "1011600"
  },
  {
    "text": "an operator to define different profiles that are going to to tune all these knobs and and even more than the ones",
    "start": "1011600",
    "end": "1018560"
  },
  {
    "text": "i've shown before and enforce some good defaults like if the user is not providing",
    "start": "1018560",
    "end": "1024079"
  },
  {
    "text": "a second profile then you will enforce the usage of a generic one if a user is trying to do something that",
    "start": "1024079",
    "end": "1030480"
  },
  {
    "text": "you consider unacceptable like running a privileged container then the the pod security policy is going to",
    "start": "1030480",
    "end": "1037038"
  },
  {
    "text": "reject the requests made by the user and this workload will never ever be scheduled on top",
    "start": "1037039",
    "end": "1042319"
  },
  {
    "text": "of your cluster so it's a really powerful mechanism the way we work is that pod security",
    "start": "1042319",
    "end": "1048799"
  },
  {
    "text": "policies are cluster wide objects that are then bound to a",
    "start": "1048799",
    "end": "1053840"
  },
  {
    "text": "user or a group by using a cluster role binding or a role binding the same way as our back policies work so",
    "start": "1053840",
    "end": "1061520"
  },
  {
    "text": "what we can do in our case is that the operator can define a baseline a generic pod security policy that",
    "start": "1061520",
    "end": "1068000"
  },
  {
    "start": "1062000",
    "end": "1062000"
  },
  {
    "text": "applies to all the user then it can out of demand from tenants when it can create more relaxed ones",
    "start": "1068000",
    "end": "1075520"
  },
  {
    "text": "that have to be used for specific tenants or even for specific workloads",
    "start": "1075520",
    "end": "1081039"
  },
  {
    "text": "but this is something that can be further improved and and we can do that",
    "start": "1081039",
    "end": "1087200"
  },
  {
    "text": "by leveraging some some standards inside of the container ecosystem so the majority of the kubernetes",
    "start": "1087200",
    "end": "1094240"
  },
  {
    "start": "1088000",
    "end": "1088000"
  },
  {
    "text": "clusters are running their workloads using run c which is the reference implementation",
    "start": "1094240",
    "end": "1099760"
  },
  {
    "text": "of the ocr runtime specification this is a specification that describes how containers are going",
    "start": "1099760",
    "end": "1105200"
  },
  {
    "text": "to be run and this is what allowed to have multiple implementation of of container runtimes",
    "start": "1105200",
    "end": "1112320"
  },
  {
    "text": "so we can for example swap out run c and use another ocr runtime a runtime",
    "start": "1112320",
    "end": "1119520"
  },
  {
    "text": "underneath and this allows us for example to leverage some work that has been done",
    "start": "1119520",
    "end": "1125840"
  },
  {
    "start": "1124000",
    "end": "1124000"
  },
  {
    "text": "in the areas of ocr runtimes there are some of them that are more focused on security i'm",
    "start": "1125840",
    "end": "1132799"
  },
  {
    "text": "talking just about cotton containers and g-visor here but there are also other ones and these runtimes they take a different",
    "start": "1132799",
    "end": "1139039"
  },
  {
    "text": "approach in running containers so for example qatar containers is providing a stronger isolation and it",
    "start": "1139039",
    "end": "1145200"
  },
  {
    "text": "does that by taking your containerized application and wrapping it up instead of inside of the virtual machine",
    "start": "1145200",
    "end": "1151039"
  },
  {
    "text": "it's going to introduce a smaller overhead this is a lightweight vm but on the other end this is going to",
    "start": "1151039",
    "end": "1157039"
  },
  {
    "text": "provide an isolation that is based on more traditional uh virtual machine",
    "start": "1157039",
    "end": "1162080"
  },
  {
    "text": "isolation techniques on the other end g-visor take another approach to solve the same problem running",
    "start": "1162080",
    "end": "1168880"
  },
  {
    "text": "containers in a more secure way and it does that by not sharing uh the kernel of the host with the",
    "start": "1168880",
    "end": "1174960"
  },
  {
    "text": "containerized applications but instead exposing to them an application kernel",
    "start": "1174960",
    "end": "1180400"
  },
  {
    "text": "which offers a reduced attack surface so in these cases in both cases",
    "start": "1180400",
    "end": "1186640"
  },
  {
    "text": "you can just swap run c use one of these containers and you don't have to rebuild",
    "start": "1186640",
    "end": "1191919"
  },
  {
    "text": "your images or push your images to another place and how can we leverage",
    "start": "1191919",
    "end": "1197360"
  },
  {
    "text": "that inside of kubernetes well we can do that by using another feature that is built",
    "start": "1197360",
    "end": "1204400"
  },
  {
    "text": "into kubernetes which is called runtime class this is an abstraction that kubernetes offers so",
    "start": "1204400",
    "end": "1209919"
  },
  {
    "text": "that cubelet can talk with a container runtime interface and this container runtime interface",
    "start": "1209919",
    "end": "1215679"
  },
  {
    "text": "can then be configured to talk with ranzi or with kata or divisor or whatever else",
    "start": "1215679",
    "end": "1221840"
  },
  {
    "text": "so we can come up to a scenario where we have trusted workloads that are using",
    "start": "1221840",
    "end": "1226960"
  },
  {
    "text": "run c for example and we have untrusted workloads that instead are forced to use",
    "start": "1226960",
    "end": "1232159"
  },
  {
    "text": "something like kata or divisor or something something different again so how do we",
    "start": "1232159",
    "end": "1237760"
  },
  {
    "text": "use how do we leverage that well the first two steps are up to the",
    "start": "1237760",
    "end": "1242880"
  },
  {
    "start": "1240000",
    "end": "1240000"
  },
  {
    "text": "operator of a cluster the operator has to define an object cluster-wide object called runtime class",
    "start": "1242880",
    "end": "1249440"
  },
  {
    "text": "which makes kubernetes aware of this additional runtime that is uh",
    "start": "1249440",
    "end": "1254720"
  },
  {
    "text": "it is usable consumable when the operator has to install the stack of obvious",
    "start": "1254720",
    "end": "1261600"
  },
  {
    "text": "runtime on all the worker nodes of the cluster and it has to configure either continuously or cry to make them aware",
    "start": "1261600",
    "end": "1268240"
  },
  {
    "text": "of existence of this catastack g-visor stack and finally the end user",
    "start": "1268240",
    "end": "1274159"
  },
  {
    "text": "has to specify which runtime class has to be used when scheduling is workloads",
    "start": "1274159",
    "end": "1280000"
  },
  {
    "text": "and this is an issue because uh it's up to the user to specify which runtime to use and this of course",
    "start": "1280000",
    "end": "1286559"
  },
  {
    "text": "is is not optimal from a security perspective but we will talk more about that at the end of the presentation",
    "start": "1286559",
    "end": "1293679"
  },
  {
    "text": "so at this point we have all the container workloads wrapped up",
    "start": "1293679",
    "end": "1300400"
  },
  {
    "start": "1296000",
    "end": "1296000"
  },
  {
    "text": "in a more secure runtime but still all these workloads",
    "start": "1300400",
    "end": "1305520"
  },
  {
    "text": "are scheduled on the same set of nodes and they are all mixed together",
    "start": "1305520",
    "end": "1310640"
  },
  {
    "text": "so in certain cases in certain scenarios this is a reasonable compromise but",
    "start": "1310640",
    "end": "1317679"
  },
  {
    "text": "for other scenarios this is unacceptable so we might want to force",
    "start": "1317679",
    "end": "1324960"
  },
  {
    "text": "the tenants to have dedicated nodes or we might want to have some workloads from a specific tenant to not",
    "start": "1324960",
    "end": "1332640"
  },
  {
    "text": "be run with kata or divisor but use run c and hence we want it to be located",
    "start": "1332640",
    "end": "1338159"
  },
  {
    "text": "somewhere else or maybe we want to isolate some some workloads that are dealing with more sensitive data",
    "start": "1338159",
    "end": "1344640"
  },
  {
    "text": "so that no other tenants can can can can jump into these workloads if",
    "start": "1344640",
    "end": "1350000"
  },
  {
    "text": "they manage to escape from their containers so how can we achieve that well we can do that",
    "start": "1350000",
    "end": "1355760"
  },
  {
    "text": "by starting to to play around with the kubernetes schedule scheduler by starting to influence that",
    "start": "1355760",
    "end": "1361760"
  },
  {
    "start": "1356000",
    "end": "1356000"
  },
  {
    "text": "in the picture here you can see that we can come up with a scenario where",
    "start": "1361760",
    "end": "1367200"
  },
  {
    "text": "we have some notes where the different tenants workloads are mixed together we have three tenants",
    "start": "1367200",
    "end": "1372559"
  },
  {
    "text": "here green yellow and purple and at the same time instead of the same cluster we have uh",
    "start": "1372559",
    "end": "1377919"
  },
  {
    "text": "one or more nodes that are reserved only to a specific tenant so how did we achieve that well that's",
    "start": "1377919",
    "end": "1383919"
  },
  {
    "text": "easy so we leverage again some features that are built into con into kubernetes",
    "start": "1383919",
    "end": "1389039"
  },
  {
    "text": "the first one is node tainting so we put some things on on specific nodes the nodes that we",
    "start": "1389039",
    "end": "1394880"
  },
  {
    "text": "want to reserve to to uh to a tenant by putting a taint on these nodes the scheduler won't uh want to schedule",
    "start": "1394880",
    "end": "1402400"
  },
  {
    "text": "regular workloads on top of this node only the workloads that have a specific toleration",
    "start": "1402400",
    "end": "1408720"
  },
  {
    "text": "inside of their definition are going to be scheduled on these nodes so in this case it's up to the the",
    "start": "1408720",
    "end": "1416320"
  },
  {
    "text": "purple uh the purple workloads add this toleration and then the scheduler decided to",
    "start": "1416320",
    "end": "1423760"
  },
  {
    "text": "allocate some of them on on the shared nodes and allocate others on the reserved node",
    "start": "1423760",
    "end": "1429200"
  },
  {
    "text": "the key point is no green or yellow workloads can end up on the reserve node",
    "start": "1429200",
    "end": "1435120"
  },
  {
    "text": "for purple we can bring this concept further by",
    "start": "1435120",
    "end": "1440480"
  },
  {
    "text": "introducing yet another feature built into kubernetes which is",
    "start": "1440480",
    "end": "1445679"
  },
  {
    "text": "called node selector so here in this case we took it to the extreme and we have a",
    "start": "1445679",
    "end": "1451840"
  },
  {
    "text": "set of nodes that are dedicated to to to tenants so we have a node where only green",
    "start": "1451840",
    "end": "1458080"
  },
  {
    "text": "workloads are running another for yellow another for pupple so we we did that by building on top of",
    "start": "1458080",
    "end": "1464080"
  },
  {
    "text": "what has shown before so we put tanes tenant-based stains on the nodes we put",
    "start": "1464080",
    "end": "1469600"
  },
  {
    "text": "toleration inside of a workload definition and then finally we also added extra constraint to the",
    "start": "1469600",
    "end": "1476159"
  },
  {
    "text": "workload definition that is the node selector the node selector constrain",
    "start": "1476159",
    "end": "1482080"
  },
  {
    "text": "is going to place a workload on a node only on nodes that are matching a specific set of",
    "start": "1482080",
    "end": "1488640"
  },
  {
    "text": "a specific set of key and values instead of the labels of a node",
    "start": "1488640",
    "end": "1493679"
  },
  {
    "text": "so as an operator of a cluster on top of putting paints of the nodes i also put some",
    "start": "1493679",
    "end": "1499520"
  },
  {
    "text": "carefully designed labels on the nodes and these labels are going to be used by the node selector",
    "start": "1499520",
    "end": "1505360"
  },
  {
    "text": "so you can end up having completely isolated tenants or you can fine-tune what",
    "start": "1505360",
    "end": "1513200"
  },
  {
    "text": "we have seen here and you can say that for example you have this dedicated node for purple",
    "start": "1513200",
    "end": "1520480"
  },
  {
    "text": "where you're going to force some uh workload of purple to end up so that",
    "start": "1520480",
    "end": "1525919"
  },
  {
    "text": "they don't get scheduled on these nodes so you can do that by just putting the node selector",
    "start": "1525919",
    "end": "1531679"
  },
  {
    "text": "on on on some workloads so that they are always 100 landing on these reserved nodes",
    "start": "1531679",
    "end": "1539360"
  },
  {
    "text": "so as i mentioned uh before there are some problems because uh when we're defining quotas where when",
    "start": "1539360",
    "end": "1547120"
  },
  {
    "start": "1541000",
    "end": "1541000"
  },
  {
    "text": "defining uh secure container runtimes or when we're playing with a scheduler in all cases we rely on the end user on",
    "start": "1547120",
    "end": "1554720"
  },
  {
    "text": "the tenant to put some information or and more important of all to put the right",
    "start": "1554720",
    "end": "1560640"
  },
  {
    "text": "information so that uh all our",
    "start": "1560640",
    "end": "1565679"
  },
  {
    "text": "tricks they they fall in place and we end up with a scenario that makes us happy but what if the user",
    "start": "1565679",
    "end": "1571840"
  },
  {
    "text": "forgets to put the data or what if we have an evil user who is deliberately putting wrong data",
    "start": "1571840",
    "end": "1578320"
  },
  {
    "text": "like he's forcing himself to to get his workload run with less secure runtime or with or",
    "start": "1578320",
    "end": "1584880"
  },
  {
    "text": "have his workload scheduled on on a node that belongs to another tenant",
    "start": "1584880",
    "end": "1590640"
  },
  {
    "text": "so what we have to do is to perform some validation and some sanitization of the input that",
    "start": "1590640",
    "end": "1596960"
  },
  {
    "text": "is provided by the user and again kubernetes can help us to do that by using a built-in feature called",
    "start": "1596960",
    "end": "1603600"
  },
  {
    "start": "1600000",
    "end": "1600000"
  },
  {
    "text": "kubernetes admission controllers so these are components that are intercepting all the requests that are",
    "start": "1603600",
    "end": "1609360"
  },
  {
    "text": "made against the kubernetes api the request has to be already has to",
    "start": "1609360",
    "end": "1614400"
  },
  {
    "text": "come from an authenticated user and it has to pass the defined airbag policies",
    "start": "1614400",
    "end": "1620240"
  },
  {
    "text": "once that happens the request has been given to the admission controllers that can be of",
    "start": "1620240",
    "end": "1625440"
  },
  {
    "text": "two types there is the first time which is validation validation one validating admission controllers which",
    "start": "1625440",
    "end": "1632480"
  },
  {
    "text": "are looking at the request and they are either accepting or rejecting that if a request is",
    "start": "1632480",
    "end": "1638080"
  },
  {
    "text": "rejected the workload or the operation will never ever be performed",
    "start": "1638080",
    "end": "1644080"
  },
  {
    "text": "and then the other type is a mutating controller which on top of being able to accept or",
    "start": "1644080",
    "end": "1649840"
  },
  {
    "text": "reject the request is also able to accept a modified version of the original",
    "start": "1649840",
    "end": "1656159"
  },
  {
    "text": "request and this is pretty important to us and pretty flexible so kubernetes comes",
    "start": "1656159",
    "end": "1663360"
  },
  {
    "start": "1661000",
    "end": "1661000"
  },
  {
    "text": "with some pre-built admission controllers and we're going to",
    "start": "1663520",
    "end": "1669200"
  },
  {
    "text": "use them uh to fix the issues that i've been talking before to make sure that for example a tenant",
    "start": "1669200",
    "end": "1676320"
  },
  {
    "text": "is scheduling workloads on his uh reserve machine to make sure that once i enforce quotas",
    "start": "1676320",
    "end": "1682080"
  },
  {
    "text": "on a namespace if a user forgets to specify the resource limits of this workload",
    "start": "1682080",
    "end": "1687919"
  },
  {
    "text": "we're going to put some defaults in there so that the workload is schedulable and we're going to enforce",
    "start": "1687919",
    "end": "1693919"
  },
  {
    "text": "the container runtime to be used so how to do that what",
    "start": "1693919",
    "end": "1699520"
  },
  {
    "start": "1699000",
    "end": "1699000"
  },
  {
    "text": "we will we will leverage some of the admission controllers that",
    "start": "1699520",
    "end": "1705039"
  },
  {
    "text": "are already compiled into kubernetes compile them because these admission controllers are part of the api server",
    "start": "1705039",
    "end": "1711919"
  },
  {
    "text": "process so they're always running always available and this is really important as we will see pretty soon",
    "start": "1711919",
    "end": "1717600"
  },
  {
    "text": "and we will leverage some of these uh controllers like limit ranger which solves problem of quotas by",
    "start": "1717600",
    "end": "1724720"
  },
  {
    "text": "providing the default values for the user forget to specify them we're going to use the",
    "start": "1724720",
    "end": "1730320"
  },
  {
    "text": "with the node restriction uh controller which can make uh the wall node selector",
    "start": "1730320",
    "end": "1738240"
  },
  {
    "text": "pattern more secure because it prevents the user from modifying the labels used",
    "start": "1738240",
    "end": "1744320"
  },
  {
    "text": "by by the node selector to look up for for where to to schedule the workloads and last but not least the",
    "start": "1744320",
    "end": "1752159"
  },
  {
    "text": "pod security policies that i was mentioning before they are actually implemented using admission controllers",
    "start": "1752159",
    "end": "1758399"
  },
  {
    "text": "so this is a really powerful primitive provided by kubernetes there are however",
    "start": "1758399",
    "end": "1766399"
  },
  {
    "start": "1765000",
    "end": "1765000"
  },
  {
    "text": "some tasks that we have to perform that are not uh doable with the compiled in",
    "start": "1766399",
    "end": "1772799"
  },
  {
    "text": "admission controller so we have to write our own admission controllers to to to put some some patches",
    "start": "1772799",
    "end": "1780000"
  },
  {
    "text": "over what i've over the problems that i've shown before so again luckily kubernetes as a way for",
    "start": "1780000",
    "end": "1786960"
  },
  {
    "text": "us to create custom mission controllers and these are called dynamic admission controller",
    "start": "1786960",
    "end": "1793520"
  },
  {
    "text": "and there are two types of them there are validating mission web book which are validating controllers and",
    "start": "1793520",
    "end": "1800159"
  },
  {
    "start": "1794000",
    "end": "1794000"
  },
  {
    "text": "then you have mutating ones in both cases the user has to write his own validation logic and it has to",
    "start": "1800159",
    "end": "1808240"
  },
  {
    "text": "expose that through a web interface that is then called by the api server every time a request",
    "start": "1808240",
    "end": "1815760"
  },
  {
    "text": "comes in so uh the operator of the cluster will register this external admission controller",
    "start": "1815760",
    "end": "1823039"
  },
  {
    "text": "this external admission controller is living inside of uh inside of most of the times inside of a",
    "start": "1823039",
    "end": "1828880"
  },
  {
    "text": "containerized application on top of the same kubernetes cluster and then the api server is just going to",
    "start": "1828880",
    "end": "1835039"
  },
  {
    "text": "reach out to it and and get a response whether to accept reject uh an incoming request the important thing",
    "start": "1835039",
    "end": "1842240"
  },
  {
    "text": "when talking about mutating admission control is that if your uh mission controller",
    "start": "1842240",
    "end": "1848399"
  },
  {
    "text": "changes the incoming request then the api server is going to re-evaluate",
    "start": "1848399",
    "end": "1853760"
  },
  {
    "text": "the change at one make sure that this new request is still passing all the our back policy checks",
    "start": "1853760",
    "end": "1860240"
  },
  {
    "text": "that are in place so again more safety nets around our wall",
    "start": "1860240",
    "end": "1865440"
  },
  {
    "text": "uh concept uh there are some issues with uh",
    "start": "1865440",
    "end": "1870720"
  },
  {
    "start": "1869000",
    "end": "1869000"
  },
  {
    "text": "admission controllers the external ones so the biggest one is you can't uh take for granted",
    "start": "1870720",
    "end": "1876320"
  },
  {
    "text": "that they're always reachable always working the way you expect so what is kubernetes supposed to do if uh",
    "start": "1876320",
    "end": "1883840"
  },
  {
    "text": "if these external admission controllers are not behaving the way we expect so kubernetes allows you to",
    "start": "1883840",
    "end": "1890640"
  },
  {
    "text": "define a failure policy for each admission controller that is living outside is an external one so",
    "start": "1890640",
    "end": "1898559"
  },
  {
    "text": "the first policy is the ignore one which basically says i don't know what this external admission controller might",
    "start": "1898559",
    "end": "1905039"
  },
  {
    "text": "think about this request but given it's not responding i'm going to accept it as valid so i'm",
    "start": "1905039",
    "end": "1911039"
  },
  {
    "text": "going to allow this with request to come through the other one is a more strict policy which is the",
    "start": "1911039",
    "end": "1917760"
  },
  {
    "text": "failed one which says given that i don't get any kind of uh response i will reject this request even",
    "start": "1917760",
    "end": "1924880"
  },
  {
    "text": "though this might be a good one a valid one so you might be tempted to use the fail",
    "start": "1924880",
    "end": "1930960"
  },
  {
    "text": "policy because it is more strict but you have to be careful because if one of your admission",
    "start": "1930960",
    "end": "1936799"
  },
  {
    "text": "controllers is is down you could turn the world cluster in something unstable where nothing can",
    "start": "1936799",
    "end": "1942799"
  },
  {
    "text": "be can be can be done so be careful about what you're doing with that",
    "start": "1942799",
    "end": "1948000"
  },
  {
    "text": "the next question i have for you is uh do you really want to write your own admission controller because on",
    "start": "1948000",
    "end": "1954640"
  },
  {
    "text": "top of writing the actual business logic of the policy there is a a lot of other",
    "start": "1954640",
    "end": "1960880"
  },
  {
    "text": "things that have to be done and my suggestion to you is instead of reinventing the wheel",
    "start": "1960880",
    "end": "1966000"
  },
  {
    "text": "uh go ahead and leverage something done by others so i want to briefly talk about",
    "start": "1966000",
    "end": "1972799"
  },
  {
    "start": "1971000",
    "end": "1971000"
  },
  {
    "text": "a cncf project called open policy agent oppan which is currently inside of the incubating stage and is currently going",
    "start": "1972799",
    "end": "1979760"
  },
  {
    "text": "through some architectural change so i will just focus on on what is going to be the next generation architecture",
    "start": "1979760",
    "end": "1985679"
  },
  {
    "text": "which is already available and you can already experiment with so the main goal of oppa",
    "start": "1985679",
    "end": "1992399"
  },
  {
    "start": "1991000",
    "end": "1991000"
  },
  {
    "text": "is to allow the end user to focus on what really matters which is writing policies",
    "start": "1992399",
    "end": "1997600"
  },
  {
    "text": "so you're going to just write these policies you're going to do that using a high level language which is",
    "start": "1997600",
    "end": "2002880"
  },
  {
    "text": "called rego and oppa is also providing you a way to test these policies which is really",
    "start": "2002880",
    "end": "2008559"
  },
  {
    "text": "important because you want to be sure that they are behaving the exact way that you expect them to behave",
    "start": "2008559",
    "end": "2014159"
  },
  {
    "text": "once you're done with your policies you can load them into kubernetes and you do that by creating",
    "start": "2014159",
    "end": "2019760"
  },
  {
    "text": "a custom resource object that is provided to you by op once these policies are loaded then",
    "start": "2019760",
    "end": "2025919"
  },
  {
    "text": "there is another component of oppa that that comes into into",
    "start": "2025919",
    "end": "2031039"
  },
  {
    "start": "2030000",
    "end": "2030000"
  },
  {
    "text": "into the scene comes into action and this is called gatekeeper gatekeeper is a it's an admission",
    "start": "2031039",
    "end": "2036559"
  },
  {
    "text": "controller that is provided by oppa that looks at all the policies that you defined",
    "start": "2036559",
    "end": "2042159"
  },
  {
    "text": "and enforces them so right now at this time as i was mentioning oppa is going",
    "start": "2042159",
    "end": "2047519"
  },
  {
    "text": "through a change of architecture so gatekeeper is a new component of it and right now it can do just a",
    "start": "2047519",
    "end": "2054158"
  },
  {
    "text": "validation of incoming requests it can't mutate them but there are plans to extend that so",
    "start": "2054159",
    "end": "2059200"
  },
  {
    "text": "that it can also mutate incoming requests now i want to uh to",
    "start": "2059200",
    "end": "2064800"
  },
  {
    "text": "make you think about a problem that is not specific to oppa but is uh",
    "start": "2064800",
    "end": "2070000"
  },
  {
    "start": "2070000",
    "end": "2070000"
  },
  {
    "text": "specific to all uh external admission controllers so your policies are not set in stone",
    "start": "2070000",
    "end": "2077760"
  },
  {
    "text": "they're going to change every time you're going to introduce new one you're going to drop other ones",
    "start": "2077760",
    "end": "2083520"
  },
  {
    "text": "and that leads to an interesting question an interesting problem which is uh what was yesterday",
    "start": "2083520",
    "end": "2090398"
  },
  {
    "text": "considered as a valid request maybe today due to a policy uh new policy be introduced or to",
    "start": "2090399",
    "end": "2097118"
  },
  {
    "text": "a policy an existing one being changed maybe today is would be rejected and",
    "start": "2097119",
    "end": "2104320"
  },
  {
    "text": "the other problem is let's say that you want to play safe and you want to set the failure policy of your external",
    "start": "2104320",
    "end": "2111839"
  },
  {
    "text": "admission controller to be ignored then what about uh things that might slip into the cluster",
    "start": "2111839",
    "end": "2119520"
  },
  {
    "text": "while the external admission controller was down so how can i solve this problem then really nice thing",
    "start": "2119520",
    "end": "2126079"
  },
  {
    "text": "about oppa is that with the new architecture it introduces an auditing feature with",
    "start": "2126079",
    "end": "2132000"
  },
  {
    "start": "2129000",
    "end": "2129000"
  },
  {
    "text": "this auditing feature oppa is going to periodically re-evaluate all the policies that are",
    "start": "2132000",
    "end": "2137200"
  },
  {
    "text": "defined inside of the cluster and find all the objects that are violating them and is going to keep a",
    "start": "2137200",
    "end": "2142400"
  },
  {
    "text": "log of all the violations that happen and then from there it's up to you to you know revisit that how to make",
    "start": "2142400",
    "end": "2148320"
  },
  {
    "text": "something out of it but this is a really powerful feature that opel provides that solves a really concrete problem",
    "start": "2148320",
    "end": "2155040"
  },
  {
    "text": "so it has been quite a quite a tour i think it's time to to wrap it up so as uh",
    "start": "2155040",
    "end": "2162560"
  },
  {
    "start": "2162000",
    "end": "2162000"
  },
  {
    "text": "as we have seen it is actually possible to take a large cluster and share that with different tenants",
    "start": "2162560",
    "end": "2169520"
  },
  {
    "text": "this is doable because kubernetes has many built-in features inside of it",
    "start": "2169520",
    "end": "2174720"
  },
  {
    "text": "that allow us to create dedicated space one per tenant or even more than one per tenant",
    "start": "2174720",
    "end": "2181040"
  },
  {
    "text": "if we want all these spaces can be isolated and can be secured",
    "start": "2181040",
    "end": "2186960"
  },
  {
    "text": "by using some primitives that are built into kubernetes and also by using some some other",
    "start": "2186960",
    "end": "2193520"
  },
  {
    "text": "features that are brought in by uh the kubernetes ecosystem",
    "start": "2193520",
    "end": "2199280"
  },
  {
    "text": "so if you're going to go for sharing a single cluster will end up",
    "start": "2199280",
    "end": "2205359"
  },
  {
    "text": "having less infrastructure to maintain you will end up having a single place where to enforce all the our back",
    "start": "2205359",
    "end": "2212000"
  },
  {
    "text": "policies all these oppa policies you're you're going to be able for example to",
    "start": "2212000",
    "end": "2217119"
  },
  {
    "text": "to consolidate around providing a shared monitoring login tracing service mesh uh",
    "start": "2217119",
    "end": "2223839"
  },
  {
    "text": "whatever to all the tenants and you will have to just keep this single instance up running",
    "start": "2223839",
    "end": "2229200"
  },
  {
    "text": "you will go uh you will have also a better hardware utilization unless you start to play heavily with a",
    "start": "2229200",
    "end": "2235599"
  },
  {
    "text": "kubernetes scheduler where you're going to reserve some nodes for for the tenants and maybe sometimes",
    "start": "2235599",
    "end": "2240800"
  },
  {
    "text": "it's not going to make a usage of all the resources that you you dedicate to them on the on the",
    "start": "2240800",
    "end": "2248160"
  },
  {
    "text": "disadvantages side uh as we have seen we can make things secure but there is uh there is a limit",
    "start": "2248160",
    "end": "2254800"
  },
  {
    "start": "2249000",
    "end": "2249000"
  },
  {
    "text": "there is a something where we where we can go ahead and for example the control planes of",
    "start": "2254800",
    "end": "2261440"
  },
  {
    "text": "these kubernetes cluster is shared among all the different tenants and while this can be considered",
    "start": "2261440",
    "end": "2267040"
  },
  {
    "text": "acceptable in some cases in other cases this could be not doable because of thai security",
    "start": "2267040",
    "end": "2274000"
  },
  {
    "text": "requirements you also have to put some trust into kubernetes and into all the different components that",
    "start": "2274000",
    "end": "2280400"
  },
  {
    "text": "you're putting together to create these sandboxes be sure that you as an",
    "start": "2280400",
    "end": "2286160"
  },
  {
    "text": "operator of this cluster are looking out for all the corner cases are writing good policies that don't",
    "start": "2286160",
    "end": "2291520"
  },
  {
    "text": "have bugs you're keeping up with with security vulnerabilities within all the different pieces",
    "start": "2291520",
    "end": "2297200"
  },
  {
    "text": "as always of course but there are many things that are put together the more moving parts you have more trust you must have with with them",
    "start": "2297200",
    "end": "2304720"
  },
  {
    "text": "uh one of the biggest disadvantages that can impact some some end users is the",
    "start": "2304720",
    "end": "2310160"
  },
  {
    "text": "fact that you have to use the very same version of kubernetes for all the different tenants",
    "start": "2310160",
    "end": "2315520"
  },
  {
    "text": "and in some cases this can be a problem because you have some tenants that want the latest and greatest while you have others that want a really",
    "start": "2315520",
    "end": "2321839"
  },
  {
    "text": "specific maybe slightly dated version of kubernetes because they have this special workload that is",
    "start": "2321839",
    "end": "2327440"
  },
  {
    "text": "certified to be working only on this a specific version of kubernetes",
    "start": "2327440",
    "end": "2332480"
  },
  {
    "text": "and last but not least you have to you're going to be bothered by by the",
    "start": "2332480",
    "end": "2338079"
  },
  {
    "text": "end users of the cluster because certain operation due to the security uh fences that we put in place are not",
    "start": "2338079",
    "end": "2344960"
  },
  {
    "text": "going to be doable for them like if they want to install this operator that is cluster-wide is not name space-based so you have to",
    "start": "2344960",
    "end": "2352240"
  },
  {
    "text": "to to to actually do that for them because they don't have the rights and then you have to go ahead and write",
    "start": "2352240",
    "end": "2357920"
  },
  {
    "text": "a new our back policies for for objects that are defined by these operators for example",
    "start": "2357920",
    "end": "2363760"
  },
  {
    "text": "uh i also want to to leave here a brief note to the virtual cluster project that is",
    "start": "2363760",
    "end": "2370400"
  },
  {
    "text": "currently being worked inside of a sick multi-tenancy group this is a really interesting project that has another",
    "start": "2370400",
    "end": "2376000"
  },
  {
    "text": "take on on sharing a cluster with different tenants it's too big to fit into this uh already",
    "start": "2376000",
    "end": "2383440"
  },
  {
    "text": "really crowded presentation uh finally let's take a brief look at uh",
    "start": "2383440",
    "end": "2389920"
  },
  {
    "text": "the advantages of having many smaller clusters so in this way you can have the the strongest isolation you can aim for",
    "start": "2389920",
    "end": "2397440"
  },
  {
    "text": "you can have the flexibility to pick the kubernetes version that each tenants want to have",
    "start": "2397440",
    "end": "2402960"
  },
  {
    "text": "and you can also end them out uh more freedom of this class so they don't bother you they need to",
    "start": "2402960",
    "end": "2409200"
  },
  {
    "text": "deploy a cluster-wide operator for example on the on the dark side you are going to",
    "start": "2409200",
    "end": "2415920"
  },
  {
    "text": "have more machines to keep up to date if you start to have multiple kubernetes versions",
    "start": "2415920",
    "end": "2421280"
  },
  {
    "text": "deployed you have to keep track of all the different security bulletins that come up and make sure",
    "start": "2421280",
    "end": "2426480"
  },
  {
    "text": "that um the the these kubernetes versions are not affected by them",
    "start": "2426480",
    "end": "2431920"
  },
  {
    "text": "that you are it's something that you always have to do of course but of course the more versions of kubernetes",
    "start": "2431920",
    "end": "2437440"
  },
  {
    "text": "you have to support the more work ends up on your plate you also have to make sure that the corporate policies",
    "start": "2437440",
    "end": "2443920"
  },
  {
    "text": "that you have in place are enforced everywhere they are synchronized everywhere and you might run into inferior hardware",
    "start": "2443920",
    "end": "2450319"
  },
  {
    "text": "utilization and higher cost because just to make an example you have to",
    "start": "2450319",
    "end": "2455440"
  },
  {
    "text": "to have multiple control planes nodes for each kubernetes cluster which might be",
    "start": "2455440",
    "end": "2461680"
  },
  {
    "text": "not the case if you're running on top of a public cloud that is offering you managed kubernetes",
    "start": "2461680",
    "end": "2466800"
  },
  {
    "text": "but in other cases it might be a factor for you so that's really all uh",
    "start": "2466800",
    "end": "2471920"
  },
  {
    "text": "i hope that i provided you a good overview i hope this is going to be useful to you",
    "start": "2471920",
    "end": "2477599"
  },
  {
    "text": "and feel free to reach out to you to me not just during this q a section but also during",
    "start": "2477599",
    "end": "2482800"
  },
  {
    "text": "kubecon europe during this event i will be reachable and thanks a lot",
    "start": "2482800",
    "end": "2489200"
  },
  {
    "text": "great thank you flavio for great presentation um just going to get to a couple of",
    "start": "2489200",
    "end": "2495280"
  },
  {
    "text": "questions here um sanjay singh would like to know how to implement access controls",
    "start": "2495280",
    "end": "2500560"
  },
  {
    "text": "using network policy between clusters so here when we talk between clusters",
    "start": "2500560",
    "end": "2507680"
  },
  {
    "text": "i guess we are talking about uh sharing one large cluster between",
    "start": "2507680",
    "end": "2512720"
  },
  {
    "text": "between different tenants so of uh in this case the the cluster",
    "start": "2512720",
    "end": "2517839"
  },
  {
    "text": "that we end out of attendance are actually name spaces and so what we are going to do to limit",
    "start": "2517839",
    "end": "2522880"
  },
  {
    "text": "the access to them is by defining the network policies and the the one that i've shown to you",
    "start": "2522880",
    "end": "2528960"
  },
  {
    "text": "is already a really good starting point uh because it isolate uh a namespace a",
    "start": "2528960",
    "end": "2534400"
  },
  {
    "text": "tenant from the other ones you can then just leverage traditional uh",
    "start": "2534400",
    "end": "2539920"
  },
  {
    "text": "network policies that you would use in a in a single cluster use case to have more glandular access",
    "start": "2539920",
    "end": "2546079"
  },
  {
    "text": "and say for example i want this uh i want the parts that are exposing the service database inside of",
    "start": "2546079",
    "end": "2552240"
  },
  {
    "text": "this namespace to be reachable by all the tenants so that for example you have a central learning",
    "start": "2552240",
    "end": "2557680"
  },
  {
    "text": "database that has been accessed by the different teams of your organization if you want to do that so it's regular",
    "start": "2557680",
    "end": "2564560"
  },
  {
    "text": "uh it's regular namespace behavior",
    "start": "2564560",
    "end": "2570079"
  },
  {
    "text": "even i see that there was for recommending that in a scenario where there are two different kubernetes",
    "start": "2570079",
    "end": "2576640"
  },
  {
    "text": "clusters so if you have two different kubernetes class or like two separate kubernetes",
    "start": "2576640",
    "end": "2582079"
  },
  {
    "text": "cluster then what you have to do is to define policies define clearly define what you want to",
    "start": "2582079",
    "end": "2589040"
  },
  {
    "text": "expose and limit what is being exposed uh through ingresses or through load",
    "start": "2589040",
    "end": "2595520"
  },
  {
    "text": "balancers type of services and then you have to resort to a usual mechanism to to limit the access to that",
    "start": "2595520",
    "end": "2602640"
  },
  {
    "text": "um where it's i would say it's a traditional problem",
    "start": "2602640",
    "end": "2607839"
  },
  {
    "text": "in this case david sharp would like to know beyond",
    "start": "2607839",
    "end": "2613920"
  },
  {
    "text": "validation and sanitation mutating and mission controllers can be used to apply these policies",
    "start": "2613920",
    "end": "2619520"
  },
  {
    "text": "i believe opa could be used to enforce and apply policies like all ns slash purple pods tolerate and",
    "start": "2619520",
    "end": "2626240"
  },
  {
    "text": "select purple nodes do you have experience with that and what kind of policies can",
    "start": "2626240",
    "end": "2631680"
  },
  {
    "text": "or should be applied automatically versus some user cooperation so um",
    "start": "2631680",
    "end": "2639200"
  },
  {
    "text": "i think that uh gangway right now can't do mutation i don't know how this will",
    "start": "2639200",
    "end": "2646319"
  },
  {
    "text": "be implemented with a final design of it so right now what you can do is",
    "start": "2646319",
    "end": "2653440"
  },
  {
    "text": "expect some user corporation for this purple scenario that you're mentioning",
    "start": "2653440",
    "end": "2659359"
  },
  {
    "text": "so expect the user to provide uh the right set of toleration and then with uh with",
    "start": "2659359",
    "end": "2665359"
  },
  {
    "text": "the current gangway with current oba policies um you can go and and make sure that",
    "start": "2665359",
    "end": "2671040"
  },
  {
    "text": "first of all the user is actually specifying the a node selector and second is specifying",
    "start": "2671040",
    "end": "2677839"
  },
  {
    "text": "the the node selector that you expect him to be to be writing if he is not doing that then you can",
    "start": "2677839",
    "end": "2683839"
  },
  {
    "text": "just deny the the workload to be scheduled the user will get um nice explanation about that",
    "start": "2683839",
    "end": "2692079"
  },
  {
    "text": "and then we'll hopefully fix that or reach out to you but right now with the current design there is no mutation available with a",
    "start": "2692079",
    "end": "2698480"
  },
  {
    "text": "new interface",
    "start": "2698480",
    "end": "2705200"
  },
  {
    "text": "so network cni is cluster-wide calicon flannel can use at the same time",
    "start": "2705200",
    "end": "2711680"
  },
  {
    "text": "so um calico is built on top of flannel no sorry sorry i'm confusing with canal",
    "start": "2711680",
    "end": "2718000"
  },
  {
    "text": "canalis is to is calico and flannel together so um the network cni has to be used",
    "start": "2718000",
    "end": "2724800"
  },
  {
    "text": "cluster wide so it has to be the same network cni but has to be used by all the different",
    "start": "2724800",
    "end": "2729839"
  },
  {
    "text": "uh tenants running on the same cluster and these cni must provide you",
    "start": "2729839",
    "end": "2737200"
  },
  {
    "text": "network policies this is important so for example flannel wouldn't be useful to you because",
    "start": "2737200",
    "end": "2742880"
  },
  {
    "text": "flannel doesn't offer network policies it can't implement them so you would have to use",
    "start": "2742880",
    "end": "2749359"
  },
  {
    "text": "calico for that or canal or psyllium for example one of these cni's",
    "start": "2749359",
    "end": "2760240"
  },
  {
    "text": "okay do we have any more questions at all",
    "start": "2760240",
    "end": "2766000"
  },
  {
    "text": "doesn't look it okay well thank you again flavio for a great presentation",
    "start": "2771440",
    "end": "2777200"
  },
  {
    "text": "and q a facilitation um thank you all for joining us today the",
    "start": "2777200",
    "end": "2782319"
  },
  {
    "text": "webinar uh recording and slides will be online later today and we're looking forward to seeing you",
    "start": "2782319",
    "end": "2787359"
  },
  {
    "text": "at the next cncf webinar thank you again everyone have a great day thanks",
    "start": "2787359",
    "end": "2803359"
  }
]