[
  {
    "text": "thank you so i'm here to talk to you about how we are optimizing resources in kubernetes",
    "start": "880",
    "end": "7120"
  },
  {
    "text": "uh how can you reduce costs how can you improve utilization on your kubernetes",
    "start": "7120",
    "end": "13679"
  },
  {
    "text": "clusters and i'll show you what you can do with open source kubernetes i'll show you",
    "start": "13679",
    "end": "19359"
  },
  {
    "text": "what we have built on top further business use cases and hopefully you'll get some ideas that you can go back and",
    "start": "19359",
    "end": "25359"
  },
  {
    "text": "use i used yourself so that's what uh you already said i",
    "start": "25359",
    "end": "32960"
  },
  {
    "text": "one thing that people know me for for good or bad things is i restarted the junkies kubernetes plugin",
    "start": "32960",
    "end": "39920"
  },
  {
    "text": "and but i'm happy that you all made it here right after lunch instead of going and",
    "start": "39920",
    "end": "45680"
  },
  {
    "text": "take a siesta which would be the proper thing i hope you got coffee because we're going to be here for the next two hours",
    "start": "45680",
    "end": "52960"
  },
  {
    "text": "just kidding um so i work at one team at adobe we use",
    "start": "52960",
    "end": "58320"
  },
  {
    "text": "kubernetes at adobe a lot of people ask me oh i didn't know adobe use is kubernetes yes we do use a lot of",
    "start": "58320",
    "end": "63920"
  },
  {
    "text": "kubernetes stuff and we're hiring by the way and i work at the experience manager",
    "start": "63920",
    "end": "69840"
  },
  {
    "text": "product uh is a bit of an introduction so you understand what type of application we",
    "start": "69840",
    "end": "75520"
  },
  {
    "text": "run it's an existing java osdi application with all the things that happen with",
    "start": "75520",
    "end": "81920"
  },
  {
    "text": "java and the jvm it was already distributed before running on kubernetes",
    "start": "81920",
    "end": "87759"
  },
  {
    "text": "it uses a lot of open source components from the apache software foundation we use a lot of open source and we",
    "start": "87759",
    "end": "92960"
  },
  {
    "text": "contribute back it has a huge marker for extensions developers that write their own",
    "start": "92960",
    "end": "98720"
  },
  {
    "text": "extensions plugin well components that run on adobe experience manager",
    "start": "98720",
    "end": "104880"
  },
  {
    "text": "and they write modules that uh that run in process on am so this is an interesting use case because we run",
    "start": "104880",
    "end": "111439"
  },
  {
    "text": "customer code in our multi-tenant clusters so on kubernetes on the cloud service am",
    "start": "111439",
    "end": "119040"
  },
  {
    "text": "cloud service specifically there's also on-prem and managed services but on",
    "start": "119040",
    "end": "125280"
  },
  {
    "text": "on on the cloud service we have more than 25 clusters and we keep growing every",
    "start": "125280",
    "end": "130720"
  },
  {
    "text": "month because this is a content management system people want to run this in multiple",
    "start": "130720",
    "end": "137040"
  },
  {
    "text": "regions so they want to run close to their customers so we're we have u.s",
    "start": "137040",
    "end": "142560"
  },
  {
    "text": "europe australia japan and we keep adding new regions as needed",
    "start": "142560",
    "end": "148720"
  },
  {
    "text": "and because customers can run their own code we also have limited permissions",
    "start": "148720",
    "end": "154640"
  },
  {
    "text": "for security reasons we use namespace to provide the scope",
    "start": "154640",
    "end": "159920"
  },
  {
    "text": "so different customers cannot see each other's data networking and all that so network",
    "start": "159920",
    "end": "166800"
  },
  {
    "text": "isolation we use make use of quotas and permissions to to isolate tenants",
    "start": "166800",
    "end": "173760"
  },
  {
    "text": "i like to refer it as a micro monolith because it's not our use case is not the",
    "start": "174160",
    "end": "179680"
  },
  {
    "text": "the typical use case i have one deployment and i scale up to 200 pods or",
    "start": "179680",
    "end": "184959"
  },
  {
    "text": "something like that we have thousands of deployments that are very similar",
    "start": "184959",
    "end": "190640"
  },
  {
    "text": "but they also scale but it's not one",
    "start": "190640",
    "end": "195760"
  },
  {
    "text": "deployment that scales a lot is all thousands of deployments that are smaller",
    "start": "195760",
    "end": "201920"
  },
  {
    "text": "we have multiple teams building services on top of kubernetes and on the application level",
    "start": "201920",
    "end": "207840"
  },
  {
    "text": "and this is also important because we want a way to to make these",
    "start": "207840",
    "end": "213720"
  },
  {
    "text": "optimizations and these ways to to scale that are like orthogonal to the developer teams if we don't want to",
    "start": "213720",
    "end": "220799"
  },
  {
    "text": "chase people in each team to do something sometimes it is way better to",
    "start": "220799",
    "end": "225920"
  },
  {
    "text": "have a way that applies to the whole cluster or or we",
    "start": "225920",
    "end": "232159"
  },
  {
    "text": "or we don't have to uh require each team to do something we do that for them so",
    "start": "232159",
    "end": "237680"
  },
  {
    "text": "those solutions are way better we",
    "start": "237680",
    "end": "242959"
  },
  {
    "text": "we use extensive extensive usage of resource requests and",
    "start": "242959",
    "end": "248400"
  },
  {
    "text": "limits so if for those of you that are newer kubernetes requests how many",
    "start": "248400",
    "end": "254959"
  },
  {
    "text": "resources are guaranteed limit this how limit is how many resources can be consumed over those",
    "start": "254959",
    "end": "261600"
  },
  {
    "text": "number of requests and we play with this a lot to make sure that we can scale while keeping the",
    "start": "261600",
    "end": "268240"
  },
  {
    "text": "cluster stable and we apply them well",
    "start": "268240",
    "end": "273280"
  },
  {
    "text": "and you can apply them to cpu if you are as uh if you have more cpu usage than your",
    "start": "273280",
    "end": "280400"
  },
  {
    "text": "request that you may end with cpu throttling if you have uh for memory uh the limit is enforced uh",
    "start": "280400",
    "end": "288720"
  },
  {
    "text": "if you try to use more memory than the limit the cloud the cloud the kernel is just",
    "start": "288720",
    "end": "295040"
  },
  {
    "text": "going to kill your your workloads and for ephemeral storage synchronous",
    "start": "295040",
    "end": "300080"
  },
  {
    "text": "that's also the request unlimited you can set and if you have spots that use more",
    "start": "300080",
    "end": "306639"
  },
  {
    "text": "ephemeral storage than they then the limit was set you're gonna get both evictions so you have to play with",
    "start": "306639",
    "end": "313280"
  },
  {
    "text": "all these uh resources and you have to play with requests and limits in a way",
    "start": "313280",
    "end": "318400"
  },
  {
    "text": "that you don't need a huge cluster to run and",
    "start": "318400",
    "end": "324880"
  },
  {
    "text": "your workload is not going to crash all the time",
    "start": "324880",
    "end": "330080"
  },
  {
    "text": "on am this is a specific case for java applications anybody here running java kubernetes",
    "start": "331680",
    "end": "337120"
  },
  {
    "text": "yes a lot of people so if you probably know java the jvm is",
    "start": "337120",
    "end": "342960"
  },
  {
    "text": "going to take all the memory on a startup and manage it so you set the heap sizes",
    "start": "342960",
    "end": "348000"
  },
  {
    "text": "and if you say i don't know 75 percent of the memory as a hip size and you have",
    "start": "348000",
    "end": "353039"
  },
  {
    "text": "one gig it's gonna take those 750 megs and use it all the time and kubernetes doesn't",
    "start": "353039",
    "end": "358960"
  },
  {
    "text": "have any visibility on what's actually used what's not the jvm just takes it",
    "start": "358960",
    "end": "366080"
  },
  {
    "text": "and so that kind of makes it a bit harder to get visibility",
    "start": "366080",
    "end": "372080"
  },
  {
    "text": "on what happened on the jd case over 11 the jvm will detect how much",
    "start": "372080",
    "end": "379199"
  },
  {
    "text": "how many how much limits is set on the memory level and the jvm is never",
    "start": "379199",
    "end": "385039"
  },
  {
    "text": "going to try to use math ma more than that in the previous versions it was just using by default it would take the",
    "start": "385039",
    "end": "391440"
  },
  {
    "text": "host memory and that would always typically cause crashes",
    "start": "391440",
    "end": "398800"
  },
  {
    "text": "so to start on the things you can do to improve usage",
    "start": "399680",
    "end": "404960"
  },
  {
    "text": "uh first thing obviously kubernetes cluster autoscaler who is using the cluster router scaler",
    "start": "404960",
    "end": "410400"
  },
  {
    "text": "everybody is using clutter scaler unless you're running on bare metal you're gonna be use the cluster autoscaler to",
    "start": "410400",
    "end": "417039"
  },
  {
    "text": "increase and reduce the the cluster size you can",
    "start": "417039",
    "end": "423039"
  },
  {
    "text": "base it on cpu a memory request we always leave room for the spikes",
    "start": "423039",
    "end": "428800"
  },
  {
    "text": "because you don't want new paths to require notes and the time it takes for a node to come up and so on",
    "start": "428800",
    "end": "436319"
  },
  {
    "text": "and we have multiple scale sets for different regions for different reasons specifically we want a different",
    "start": "436319",
    "end": "442960"
  },
  {
    "text": "availability zones we have multiple worker tiers",
    "start": "442960",
    "end": "449520"
  },
  {
    "text": "kubernetes nodes as node groups on azure",
    "start": "449520",
    "end": "456240"
  },
  {
    "text": "we have the maximum number of nodes that the cluster can have defined we don't want to scale past the limits",
    "start": "456240",
    "end": "462880"
  },
  {
    "text": "we know are safe and we use the least waste scaling strategy to",
    "start": "462880",
    "end": "469360"
  },
  {
    "text": "that will minimize the the idle cpu i put a rough number there 30 50. i mean",
    "start": "469360",
    "end": "476400"
  },
  {
    "text": "i don't think anybody on their same mind would not use the cluster autoscaler",
    "start": "476400",
    "end": "482560"
  },
  {
    "text": "maybe some very specific use cases but if we had to run clusters at full",
    "start": "482560",
    "end": "488479"
  },
  {
    "text": "capacity all the time the amount of money we would be wasting would be",
    "start": "488479",
    "end": "493520"
  },
  {
    "text": "crazy so things that you can see these are examples uh real examples so",
    "start": "493520",
    "end": "500960"
  },
  {
    "text": "you can see the cluster of the scalar going up and down in the number of nodes in a cluster",
    "start": "500960",
    "end": "506800"
  },
  {
    "text": "some spikes that could be i don't know maybe it's a day of the week or maybe",
    "start": "506800",
    "end": "511919"
  },
  {
    "text": "it's business hours or something like that for our customers and we have these typical",
    "start": "511919",
    "end": "518560"
  },
  {
    "text": "patterns sometimes you will see other patterns that are a bit more scary right so this",
    "start": "518560",
    "end": "525920"
  },
  {
    "text": "going to the to the limit of this cluster size and this was because of a bug",
    "start": "525920",
    "end": "532480"
  },
  {
    "text": "that triggered out the scalar to to scale up so you see how at some point the two",
    "start": "532480",
    "end": "537600"
  },
  {
    "text": "scalar went crazy and uh because we have the limit set up uh this didn't keep going",
    "start": "537600",
    "end": "544720"
  },
  {
    "text": "but you see once we figure out what was happening the the number of machines just started",
    "start": "544720",
    "end": "552320"
  },
  {
    "text": "stabilizing and going down",
    "start": "552320",
    "end": "555760"
  },
  {
    "text": "the other typical option you're going to have is the horizontal polarity scale who's using our horizontal button scaler",
    "start": "557360",
    "end": "564880"
  },
  {
    "text": "a lot of people using so you this is basically creating more parts of a deployment when you need them",
    "start": "564880",
    "end": "571760"
  },
  {
    "text": "and we have uh [Music] two ways to metric set up to the to the",
    "start": "571760",
    "end": "578080"
  },
  {
    "text": "hpa of the scale on cpu and http requests per minute",
    "start": "578080",
    "end": "583760"
  },
  {
    "text": "for cpu is a bit problematic because you could have periodic tasks or startup",
    "start": "583760",
    "end": "589360"
  },
  {
    "text": "tasks that spike the cpu so imagine somebody makes a mistake or a customer",
    "start": "589360",
    "end": "595680"
  },
  {
    "text": "makes a mistake and only start up the cpu spikes especially with java",
    "start": "595680",
    "end": "602000"
  },
  {
    "text": "maybe the garbage collection things like that this would cause a cascading effect so",
    "start": "602000",
    "end": "608000"
  },
  {
    "text": "if every time a pot starts there's a spike on cpu that would trigger another pot to start up that will also spike on",
    "start": "608000",
    "end": "613600"
  },
  {
    "text": "cpu and so on so it's something that it's gonna you gotta",
    "start": "613600",
    "end": "618880"
  },
  {
    "text": "be careful about um am specifically also needs to be warm up",
    "start": "618880",
    "end": "625600"
  },
  {
    "text": "on a startup um because we are serving content we want caching to be worn up and a bunch of",
    "start": "625600",
    "end": "633120"
  },
  {
    "text": "other reasons and for us like a request based auto scaling is",
    "start": "633120",
    "end": "638560"
  },
  {
    "text": "better suited as long as customers don't have expensive requests that you have one",
    "start": "638560",
    "end": "644399"
  },
  {
    "text": "request that takes a lot of resources then the number of requests is not an indicator",
    "start": "644399",
    "end": "649839"
  },
  {
    "text": "to scale up and this probably save us like 50 to 75 percent of uh then running at full scale",
    "start": "649839",
    "end": "659120"
  },
  {
    "text": "eight balls temples or whatever for each customer right so this allows us to run from two parts",
    "start": "659120",
    "end": "665040"
  },
  {
    "text": "for we use two parts for ha for production environments",
    "start": "665040",
    "end": "671200"
  },
  {
    "text": "we have one pot for other environments but we don't we don't go to the",
    "start": "671200",
    "end": "676399"
  },
  {
    "text": "to the run on the limit all the time and here it's another example um",
    "start": "676399",
    "end": "682480"
  },
  {
    "text": "how you see that the number of spots more or less matches the requests per minute that we get and the request per",
    "start": "682480",
    "end": "689360"
  },
  {
    "text": "minute is again it's a very typical example of business hours night",
    "start": "689360",
    "end": "697120"
  },
  {
    "text": "uh number of requests that you get and the pods just match",
    "start": "697120",
    "end": "703680"
  },
  {
    "text": "another option you have on kubernetes is the vertical polarity scaler anybody using the vertical scaler",
    "start": "703680",
    "end": "710800"
  },
  {
    "text": "less people okay so this is a basically increase and decrease the number of resources used for each spot",
    "start": "710800",
    "end": "717279"
  },
  {
    "text": "or each deployment so you don't scale with motherboards you just take one scale",
    "start": "717279",
    "end": "723519"
  },
  {
    "text": "increase or decrease the number of resources something that is problematic is that it",
    "start": "723519",
    "end": "730399"
  },
  {
    "text": "requires the the restart of the pots so if you have something that is very fast",
    "start": "730399",
    "end": "735920"
  },
  {
    "text": "to start that may not be a problem if you have something that takes a long time to start then that's a problem this",
    "start": "735920",
    "end": "741920"
  },
  {
    "text": "could be said automatically or on the next start so you don't are you're not killing pots continuously you could just",
    "start": "741920",
    "end": "748720"
  },
  {
    "text": "say oh if this spot gets rescheduled or restarts or something apply the changes",
    "start": "748720",
    "end": "754639"
  },
  {
    "text": "then but this also makes it uh slow to respond and it can exhaust um",
    "start": "754639",
    "end": "762720"
  },
  {
    "text": "if if you do a scale if all of the parts that you have running on the same node they scale up at the same time you",
    "start": "762720",
    "end": "769200"
  },
  {
    "text": "can also exhaust the resources of the node so we only use it on our case on",
    "start": "769200",
    "end": "774720"
  },
  {
    "text": "developer environments and we only used it to scale down if needed",
    "start": "774720",
    "end": "780639"
  },
  {
    "text": "and only for some containers because",
    "start": "780639",
    "end": "786480"
  },
  {
    "text": "because of all the reasons is not very good for for our production uses",
    "start": "786480",
    "end": "791519"
  },
  {
    "text": "so this saves us a bit a small percentage five to 15 percent or something like that of the resources for",
    "start": "791519",
    "end": "798720"
  },
  {
    "text": "developer environments now",
    "start": "798720",
    "end": "803839"
  },
  {
    "text": "some things that you that are outside of kubernetes that we build ourselves for a use case",
    "start": "803839",
    "end": "809760"
  },
  {
    "text": "the first one is hibernation and is very similar to the scale to zero",
    "start": "809760",
    "end": "816000"
  },
  {
    "text": "problem that uh you can solve today with the um with the horizontal pull out the",
    "start": "816000",
    "end": "821360"
  },
  {
    "text": "scalar and custom matrix you could do a scale down to zero or if you use k native or something like",
    "start": "821360",
    "end": "827760"
  },
  {
    "text": "that or functions then it is something like that but this has a twist uh",
    "start": "827760",
    "end": "834639"
  },
  {
    "text": "we don't first our pods take a while to start some minutes to start so it's not like",
    "start": "834639",
    "end": "841600"
  },
  {
    "text": "you can just bring down to zero and scale up again",
    "start": "841600",
    "end": "847120"
  },
  {
    "text": "and we not only scale down a deployment because most of the things we are going to find in kubernetes here",
    "start": "847279",
    "end": "853360"
  },
  {
    "text": "are deployment specific so this is more of a business concept of a customer environment",
    "start": "853360",
    "end": "860399"
  },
  {
    "text": "so when a customer environment doesn't have any resources coming in for x amount of hours we just scale it down",
    "start": "860399",
    "end": "867519"
  },
  {
    "text": "and we scale the whole environment which is several deployments and we also delete",
    "start": "867519",
    "end": "874079"
  },
  {
    "text": "ingress routes and other objects that may limit cluster scale so",
    "start": "874079",
    "end": "879839"
  },
  {
    "text": "we hit a limit where we have a lot because we have a lot of environments running",
    "start": "879839",
    "end": "885199"
  },
  {
    "text": "thousands of environments in each cluster we have thousands of ingresses and at some point that becomes a problem",
    "start": "885199",
    "end": "892079"
  },
  {
    "text": "for reprogramming the ingress controller so on hibernation we also delete those",
    "start": "892079",
    "end": "899199"
  },
  {
    "text": "ingresses this is implemented very easily",
    "start": "899199",
    "end": "906079"
  },
  {
    "text": "very simple it's a cron job that just goes to prometheus checks the number of requests in the last n hours",
    "start": "906079",
    "end": "913839"
  },
  {
    "text": "and if if it was not accessed it just scales it hibernates it",
    "start": "913839",
    "end": "919279"
  },
  {
    "text": "and for the hibernation because we we change the ingress route that the customer with you or the user would use",
    "start": "919279",
    "end": "926560"
  },
  {
    "text": "and just point we point them to an to a website where they can click a button and be hibernated",
    "start": "926560",
    "end": "933680"
  },
  {
    "text": "and this we are playing between 16 80 percent we do it for only some environments with",
    "start": "933680",
    "end": "940560"
  },
  {
    "text": "what we call sandboxes so it's like development environments or developers customer developers different",
    "start": "940560",
    "end": "948000"
  },
  {
    "text": "more like playing playgrounds and the savings we get",
    "start": "948000",
    "end": "954399"
  },
  {
    "text": "as i said is 60 80 percent so this is huge for us because it allows",
    "start": "954399",
    "end": "960079"
  },
  {
    "text": "us to to pack a lot of things in the same cluster",
    "start": "960079",
    "end": "965360"
  },
  {
    "text": "and it's very stable in our case",
    "start": "965360",
    "end": "970560"
  },
  {
    "text": "and then at some point what we're gonna do is if you haven't used your environment for x amount of months",
    "start": "970560",
    "end": "977680"
  },
  {
    "text": "well just gonna garbage clean it delete it garbage collect it",
    "start": "977680",
    "end": "983839"
  },
  {
    "text": "whatever whatever you want",
    "start": "984240",
    "end": "987759"
  },
  {
    "text": "another thing that we've built in collaboration with other team at adobe is a project called arc",
    "start": "992079",
    "end": "1000240"
  },
  {
    "text": "which is automatic resource configuration so",
    "start": "1000240",
    "end": "1006800"
  },
  {
    "text": "one thing we noticed and we analyzed is that a lot of services request more memory",
    "start": "1006800",
    "end": "1012880"
  },
  {
    "text": "and cpu that they are actually using so arc can transparently",
    "start": "1012880",
    "end": "1019680"
  },
  {
    "text": "reduce this cpu and memory requirements so if we see that uh",
    "start": "1019680",
    "end": "1027038"
  },
  {
    "text": "one cluster has a utilization rate very low like five ten percent or",
    "start": "1027039",
    "end": "1032400"
  },
  {
    "text": "whatever on like cpu we just apply a percentage to the whole",
    "start": "1032400",
    "end": "1038400"
  },
  {
    "text": "cluster or to a specific namespaces this way what i was telling you before",
    "start": "1038400",
    "end": "1043760"
  },
  {
    "text": "we have different teams doing different applications so instead of telling them",
    "start": "1043760",
    "end": "1048880"
  },
  {
    "text": "you go and analyze what you usage and do this and that we can we just apply this all across all",
    "start": "1048880",
    "end": "1056559"
  },
  {
    "text": "the namespaces specifically for like sandbox stage clusters",
    "start": "1056559",
    "end": "1062000"
  },
  {
    "text": "and non-production we just go and say everything everything is goes down",
    "start": "1062000",
    "end": "1069039"
  },
  {
    "text": "it doesn't touch us touch the limits so the side effects are a bit limited",
    "start": "1069039",
    "end": "1075120"
  },
  {
    "text": "most likely you're not going to trigger on java the out of memory and the kernel kill in",
    "start": "1075120",
    "end": "1080720"
  },
  {
    "text": "your pots uh obviously if if for whatever reason [Music]",
    "start": "1080720",
    "end": "1087840"
  },
  {
    "text": "many pods that use a lot more resources than their requests happen to be in the",
    "start": "1087840",
    "end": "1093600"
  },
  {
    "text": "same node then you could get cpu throttling on some side effects",
    "start": "1093600",
    "end": "1099440"
  },
  {
    "text": "but then that's what i mean what we analyzed right we look at the utilization and what are the chances",
    "start": "1099440",
    "end": "1107120"
  },
  {
    "text": "of high resource using paths being in the same node at the same time",
    "start": "1107120",
    "end": "1115280"
  },
  {
    "text": "so that's risk benefit thing",
    "start": "1115280",
    "end": "1119760"
  },
  {
    "text": "arc also has the recommender part that leverages historical metrics at the",
    "start": "1121760",
    "end": "1127919"
  },
  {
    "text": "deployment level so it will give you recommendations on annotations",
    "start": "1127919",
    "end": "1133760"
  },
  {
    "text": "um about the optimization of of the deployment and how much is being used",
    "start": "1133760",
    "end": "1139520"
  },
  {
    "text": "so it's a bit like vpa it's a bit like vpa",
    "start": "1139520",
    "end": "1146720"
  },
  {
    "text": "but it gives you historical data and also applies to the whole cluster namespace all that",
    "start": "1146720",
    "end": "1153840"
  },
  {
    "text": "and people don't have to know about vpa or having to create a new",
    "start": "1153840",
    "end": "1159280"
  },
  {
    "text": "crd for vpa or anything like that so it's automatic for them",
    "start": "1159280",
    "end": "1165840"
  },
  {
    "text": "so we we can dial down at the i mentioned requests at the cluster or the namespace",
    "start": "1166799",
    "end": "1172960"
  },
  {
    "text": "level and these kind of give us a 10 15",
    "start": "1172960",
    "end": "1178480"
  },
  {
    "text": "reduction so in this graph for instance at the",
    "start": "1179039",
    "end": "1185360"
  },
  {
    "text": "we have the number of requests the blue line and we see that it's very consistent",
    "start": "1185360",
    "end": "1190720"
  },
  {
    "text": "that the utilization is very low for this you gotta think also that our workloads",
    "start": "1190720",
    "end": "1196720"
  },
  {
    "text": "are very specific um we have we are serving content if there's no users coming to the website",
    "start": "1196720",
    "end": "1203760"
  },
  {
    "text": "there's no activity but you cannot just shut it down so we cannot scale it to zero in",
    "start": "1203760",
    "end": "1209679"
  },
  {
    "text": "production but then we rely on hpa to pick up the base if",
    "start": "1209679",
    "end": "1216720"
  },
  {
    "text": "needed but we always have to have like at least two pods running all the time even if",
    "start": "1216720",
    "end": "1222799"
  },
  {
    "text": "there's no traffic because at any point in time there could be right so we have the utilization pretty low in",
    "start": "1222799",
    "end": "1230480"
  },
  {
    "text": "this cluster uh we have the requests and we have the original request here",
    "start": "1230480",
    "end": "1237039"
  },
  {
    "text": "and you see that the actual request is a percentage lower than the than the",
    "start": "1237039",
    "end": "1242240"
  },
  {
    "text": "requested the original request and this is what we are saving",
    "start": "1242240",
    "end": "1247600"
  },
  {
    "text": "and then we have the limits which we always put pretty high",
    "start": "1247600",
    "end": "1254000"
  },
  {
    "text": "just in case we have spikes so we don't have to use hpa we",
    "start": "1254000",
    "end": "1260480"
  },
  {
    "text": "have a spikes temporary spikes and yeah the only risk is having very",
    "start": "1260480",
    "end": "1266240"
  },
  {
    "text": "noisy neighbors in the same node",
    "start": "1266240",
    "end": "1269919"
  },
  {
    "text": "some questions that i anticipate is why do you use arc another vpa recommender",
    "start": "1271440",
    "end": "1278640"
  },
  {
    "text": "and this is a team that where joe is working so the arc allows them to to do the full",
    "start": "1278640",
    "end": "1285280"
  },
  {
    "text": "control of the recommendation engine and in the implementation as i said it",
    "start": "1285280",
    "end": "1290320"
  },
  {
    "text": "was at the cluster level so you don't have to deal with specific deployments it just applies to everything running in",
    "start": "1290320",
    "end": "1297280"
  },
  {
    "text": "the cluster",
    "start": "1297280",
    "end": "1299919"
  },
  {
    "text": "okay so to sum it up",
    "start": "1302559",
    "end": "1307600"
  },
  {
    "text": "we have uh a few optimizations uh i didn't mention phenox in the whole",
    "start": "1307600",
    "end": "1313120"
  },
  {
    "text": "talk maybe i should or i shouldn't because that's all the trend now",
    "start": "1313120",
    "end": "1319360"
  },
  {
    "text": "so we have from the kubernetes ecosystem we use the cluster of the scalar hpa vpa and there's some new things coming um",
    "start": "1319360",
    "end": "1327440"
  },
  {
    "text": "like the hpa down to zero that was very interesting that was already out there",
    "start": "1327440",
    "end": "1332880"
  },
  {
    "text": "some releases ago vpa i don't know if it's ready or not but i think there's the possibility or",
    "start": "1332880",
    "end": "1339440"
  },
  {
    "text": "will be the possibility of changing the requests on",
    "start": "1339440",
    "end": "1344480"
  },
  {
    "text": "life without having to restart the pots i i read about that i don't know where what's the status",
    "start": "1344480",
    "end": "1350799"
  },
  {
    "text": "right now internally we built this hibernation very simple hibernation",
    "start": "1350799",
    "end": "1357200"
  },
  {
    "text": "more business case oriented and arc which is not so business case",
    "start": "1357200",
    "end": "1363200"
  },
  {
    "text": "oriented but it's more like kubernetes cluster thing and they apply at different levels",
    "start": "1363200",
    "end": "1369760"
  },
  {
    "text": "application level and an infrastructure level and hopefully a combination of",
    "start": "1369760",
    "end": "1375280"
  },
  {
    "text": "them will help you optimize and resource and reduce the resources you need",
    "start": "1375280",
    "end": "1381919"
  },
  {
    "text": "so i hope this will be useful to you and you can go and apply some ideas on your use case",
    "start": "1381919",
    "end": "1390080"
  },
  {
    "text": "and now i'll take some questions",
    "start": "1390080",
    "end": "1394240"
  },
  {
    "text": "thank you thank you carlos we have one question",
    "start": "1396640",
    "end": "1402320"
  },
  {
    "text": "here from from the online audience they want to know more about ark if it's open sourced and if you have any plans to",
    "start": "1402320",
    "end": "1408400"
  },
  {
    "text": "let the community access it no it's it's not open source um",
    "start": "1408400",
    "end": "1414799"
  },
  {
    "text": "i don't know what the plans is this is uh from another team um",
    "start": "1414799",
    "end": "1420799"
  },
  {
    "text": "do you have anything to say joe it is in the works we are we just",
    "start": "1420799",
    "end": "1427840"
  },
  {
    "text": "about about a month ago there was a you know talk about this um stay tuned we should hopefully have something coming up in the summer",
    "start": "1427840",
    "end": "1434880"
  },
  {
    "text": "great thank you i think there's another microphone a little bit further down as well so you're going to line up and ask questions there and i'll pass",
    "start": "1434880",
    "end": "1440480"
  },
  {
    "text": "this around up here hello thank you for the talk",
    "start": "1440480",
    "end": "1445840"
  },
  {
    "text": "i want to know how like after deployments scale down how do you deal with",
    "start": "1445840",
    "end": "1452240"
  },
  {
    "text": "resource fragmentation to scale down the nodes itself so typically we have",
    "start": "1452240",
    "end": "1459440"
  },
  {
    "text": "the workloads distributed across multiple nodes so the cluster of the scalar will not scale down the nodes but",
    "start": "1459440",
    "end": "1465840"
  },
  {
    "text": "if you like could move somewhere close to someone you could scale them down",
    "start": "1465840",
    "end": "1471840"
  },
  {
    "text": "i think that's all tunable at the autism how to cluster out the scaler if you want to do",
    "start": "1471840",
    "end": "1477279"
  },
  {
    "text": "that or not we have a lot of things that come and go so we don't have very long-lived pods so",
    "start": "1477279",
    "end": "1484640"
  },
  {
    "text": "we don't have that problem that much we i don't know",
    "start": "1484640",
    "end": "1489840"
  },
  {
    "text": "probably we don't have anything more than a few days old so it's continuously we never had any",
    "start": "1489840",
    "end": "1496000"
  },
  {
    "text": "issue with that and we also applied more or less frequently",
    "start": "1496000",
    "end": "1502000"
  },
  {
    "text": "which may trigger not restarts and evictions to happen and so on but we have enough",
    "start": "1502000",
    "end": "1508159"
  },
  {
    "text": "movement of workloads that we never had to worry about that too much",
    "start": "1508159",
    "end": "1513600"
  },
  {
    "text": "thank you so there's a microphone installed please",
    "start": "1513600",
    "end": "1519200"
  },
  {
    "text": "like there's a microphone like halfway down the room so if you want to line up down there so i don't have to run across the whole room",
    "start": "1519200",
    "end": "1525840"
  },
  {
    "text": "hi hi here i have a small question about",
    "start": "1525840",
    "end": "1532960"
  },
  {
    "text": "how do you know how much resource to give to each spot in the request cpu and the memory",
    "start": "1532960",
    "end": "1539840"
  },
  {
    "text": "i don't speak about auto scaling about static number",
    "start": "1539840",
    "end": "1546240"
  },
  {
    "text": "yeah that was just like a trial and error experience over time",
    "start": "1546240",
    "end": "1552880"
  },
  {
    "text": "we started with some numbers the different teams building applications they come up with those numbers and we",
    "start": "1552880",
    "end": "1558480"
  },
  {
    "text": "just help them see what's happening and have providing them their grafana",
    "start": "1558480",
    "end": "1564000"
  },
  {
    "text": "dashboards and things like that and each team looks at it and says you know well i'm using more i'm using",
    "start": "1564000",
    "end": "1571279"
  },
  {
    "text": "less i need more a little less and they keep refining it the initial numbers",
    "start": "1571279",
    "end": "1577039"
  },
  {
    "text": "did you thought about to build a system or platform that will say to its",
    "start": "1577039",
    "end": "1582480"
  },
  {
    "text": "deployment what are the optimal cpu and memory that it needs",
    "start": "1582480",
    "end": "1588240"
  },
  {
    "text": "yeah so our recommender would do that and put set annotations at the pod",
    "start": "1588240",
    "end": "1593600"
  },
  {
    "text": "levels and tell them what's the like usage real usage",
    "start": "1593600",
    "end": "1599760"
  },
  {
    "text": "if they want to use that we we go with some to some teams we go and tell them you know",
    "start": "1599760",
    "end": "1606640"
  },
  {
    "text": "look at this because it's wasting a lot of cpus and we can let them know",
    "start": "1606640",
    "end": "1612400"
  },
  {
    "text": "but we my my whole goal is that each team is vertically independent so we",
    "start": "1612400",
    "end": "1618320"
  },
  {
    "text": "provide tools and they are they own it so we just tell them",
    "start": "1618320",
    "end": "1624400"
  },
  {
    "text": "unless there's a problem we just let them know you know uh there's you should tweak it or not and it's up",
    "start": "1624400",
    "end": "1631679"
  },
  {
    "text": "to them to do it yes do you instruct them to put",
    "start": "1631679",
    "end": "1638159"
  },
  {
    "text": "um requests and limits for both memory and cpu we enforce that on the pull",
    "start": "1638159",
    "end": "1644880"
  },
  {
    "text": "request level uh we use regal policies with conf test",
    "start": "1644880",
    "end": "1650480"
  },
  {
    "text": "and we have a set of policies typical ones like security policies",
    "start": "1650480",
    "end": "1656080"
  },
  {
    "text": "um like you should not mount secrets as environment variables you should not uh",
    "start": "1656080",
    "end": "1662159"
  },
  {
    "text": "do some things mount things from the host and then you have to put requests and",
    "start": "1662159",
    "end": "1667600"
  },
  {
    "text": "limits [Music] hi um here yes thank you for your time uh",
    "start": "1667600",
    "end": "1674000"
  },
  {
    "text": "you mentioned a couple of times that uh the risk of maintaining your require is quite low but the limit is quite high if",
    "start": "1674000",
    "end": "1680640"
  },
  {
    "text": "you've got noisy neighborhoods do you have any recommendation for that type of scenario",
    "start": "1680640",
    "end": "1686559"
  },
  {
    "text": "yeah it depends on your use case in our case on our um",
    "start": "1686559",
    "end": "1691679"
  },
  {
    "text": "statistically we don't have that much of that problem",
    "start": "1691679",
    "end": "1697320"
  },
  {
    "text": "but if it yeah it depends on your business case the only solution is to raise the",
    "start": "1697440",
    "end": "1703360"
  },
  {
    "text": "request and make sure that they all are using some sort of like",
    "start": "1703360",
    "end": "1709840"
  },
  {
    "text": "tying things to nodes and then having nodes that you know they're going to be very busy and use labels",
    "start": "1709840",
    "end": "1716399"
  },
  {
    "text": "to get them scheduled on those nodes and then having nodes that are not so busy and having other",
    "start": "1716399",
    "end": "1722080"
  },
  {
    "text": "and you could have workloads that have less resources and more spiky things and",
    "start": "1722080",
    "end": "1728080"
  },
  {
    "text": "one node and more resources and more if you use like machine learning workloads",
    "start": "1728080",
    "end": "1733520"
  },
  {
    "text": "right those are going to be like 90 95 cpu all the time otherwise you're just",
    "start": "1733520",
    "end": "1739039"
  },
  {
    "text": "wasting money uh you want them to be up there in our case we",
    "start": "1739039",
    "end": "1745039"
  },
  {
    "text": "we cannot because it doesn't depend on us it depends on how much traffic we're getting we cannot make them be there all",
    "start": "1745039",
    "end": "1751440"
  },
  {
    "text": "the time and because it has to handle the spikes yes",
    "start": "1751440",
    "end": "1757039"
  },
  {
    "text": "is there any recommendation of the instance types that you're using and are you using spotting i don't know",
    "start": "1757039",
    "end": "1762240"
  },
  {
    "text": "if you're using aws but are you using spot or we use a and we don't use a spot instances",
    "start": "1762240",
    "end": "1768720"
  },
  {
    "text": "because [Music] pricing reasons or whatever",
    "start": "1768720",
    "end": "1773919"
  },
  {
    "text": "is not worth i guess but i've done it in the past just use spot",
    "start": "1773919",
    "end": "1781039"
  },
  {
    "text": "instances for things uh i don't know like jenkins builds you can just on kubernetes just",
    "start": "1781039",
    "end": "1787279"
  },
  {
    "text": "use spot instances if you don't care if sometimes a bill fails because there's no spot instances you can wait a bit",
    "start": "1787279",
    "end": "1793520"
  },
  {
    "text": "more then that's fine and the types of nodes",
    "start": "1793520",
    "end": "1799120"
  },
  {
    "text": "we use standard ones we look at that we look at the proportion basically what we measure is",
    "start": "1799840",
    "end": "1806480"
  },
  {
    "text": "the proportion between cpu and memory that our application uses and then we went and looked at",
    "start": "1806480",
    "end": "1813200"
  },
  {
    "text": "vms that have that proportion because horse is pretty",
    "start": "1813200",
    "end": "1819440"
  },
  {
    "text": "normal i said but yeah if you have like high cpu usage then you would go with a hype cpu",
    "start": "1819440",
    "end": "1826399"
  },
  {
    "text": "to range ratio cpu to memory ratio and so so it depends on your use case and also",
    "start": "1826399",
    "end": "1832159"
  },
  {
    "text": "depends if whether you mix workloads or not but",
    "start": "1832159",
    "end": "1837360"
  },
  {
    "text": "would you prefer like large nodes or too many smaller i know we prefer large null because there's also a limit on how many",
    "start": "1837360",
    "end": "1843120"
  },
  {
    "text": "nodes you can have so the larger the better typically",
    "start": "1843120",
    "end": "1850760"
  },
  {
    "text": "so i'm wondering if you could go into more detail about your jvm configuration",
    "start": "1851279",
    "end": "1856559"
  },
  {
    "text": "and the memory usage i think the newer right the newer garbage collectors are able to release",
    "start": "1856559",
    "end": "1862240"
  },
  {
    "text": "memory to the operating system yeah we didn't have a really good experience with that so i'm wondering if you could",
    "start": "1862240",
    "end": "1867279"
  },
  {
    "text": "share your insights so one thing so we don't let the",
    "start": "1867279",
    "end": "1873200"
  },
  {
    "text": "the default jvm algorithm to decide how much heap we are said it i think is 75",
    "start": "1873200",
    "end": "1879840"
  },
  {
    "text": "of the available of the requests um because that was we don't have anything",
    "start": "1879840",
    "end": "1886960"
  },
  {
    "text": "off heap not much of hip memory um 75 was like the safe high",
    "start": "1886960",
    "end": "1895120"
  },
  {
    "text": "number you also have to consider the jvm",
    "start": "1895360",
    "end": "1900399"
  },
  {
    "text": "changes the defaults based on how much memory and cpu is available so i",
    "start": "1900399",
    "end": "1906559"
  },
  {
    "text": "wouldn't recommend sticking with the defaults because then you're gonna get surprises",
    "start": "1906559",
    "end": "1911840"
  },
  {
    "text": "maybe you're using pods with less memory and then now the jvm suddenly changes your garbage collection algorithm it",
    "start": "1911840",
    "end": "1918960"
  },
  {
    "text": "changes your your amount of hip and things like that so it's better if you always set it",
    "start": "1918960",
    "end": "1925360"
  },
  {
    "text": "explicitly to what you need and yeah it was mostly memory",
    "start": "1925360",
    "end": "1932559"
  },
  {
    "text": "setting up the right memory in this application case we want the heap to be always the same amount we",
    "start": "1932559",
    "end": "1939840"
  },
  {
    "text": "don't want it to go up and down uh you could also set that you could say minimum maximum",
    "start": "1939840",
    "end": "1945679"
  },
  {
    "text": "but that also uh yeah is the garbage collection working and things like that",
    "start": "1945679",
    "end": "1952240"
  },
  {
    "text": "have a question over here",
    "start": "1955120",
    "end": "1958399"
  },
  {
    "text": "thank you so much for the presentation um at my company we came with similar solutions although probably a bit more",
    "start": "1960240",
    "end": "1966399"
  },
  {
    "text": "crude so my question is um what's next um do you have any ideas of how to",
    "start": "1966399",
    "end": "1972960"
  },
  {
    "text": "be more make a better use of resources [Music]",
    "start": "1972960",
    "end": "1979360"
  },
  {
    "text": "yes so [Music] we want to improve hibernation a bit",
    "start": "1979360",
    "end": "1985039"
  },
  {
    "text": "more [Music] and we are looking",
    "start": "1985039",
    "end": "1990399"
  },
  {
    "text": "at i'm trying to figure out which one i can tell you about",
    "start": "1990399",
    "end": "1997840"
  },
  {
    "text": "uh we were looking at the different vm sizes but we kind of left it",
    "start": "1997919",
    "end": "2004399"
  },
  {
    "text": "not for now it looks good enough for us as we start mixing maybe different",
    "start": "2004399",
    "end": "2010000"
  },
  {
    "text": "workloads we may have to revisit that and probably increasing the cpu usage making",
    "start": "2010000",
    "end": "2017519"
  },
  {
    "text": "sure that people because our cpu usage is very low in average then we have",
    "start": "2017519",
    "end": "2024720"
  },
  {
    "text": "times where there's a lot of traffic black friday whatever and then",
    "start": "2024720",
    "end": "2030559"
  },
  {
    "text": "yeah i mean we're looking now more at other costs that come",
    "start": "2030559",
    "end": "2036080"
  },
  {
    "text": "on is not so much about the infrastructure level",
    "start": "2036080",
    "end": "2041200"
  },
  {
    "text": "but it's on how how can we pack more customers into the same clusters",
    "start": "2041200",
    "end": "2048480"
  },
  {
    "text": "because not not only increasing the usage but also having",
    "start": "2048480",
    "end": "2053839"
  },
  {
    "text": "bigger clusters means less maintenance less work to do so it's not directly on",
    "start": "2053839",
    "end": "2059679"
  },
  {
    "text": "resource utilization but it's also in how to reduce the cost of operating the whole thing",
    "start": "2059679",
    "end": "2065679"
  },
  {
    "text": "less overhead i think we have time for one last",
    "start": "2065679",
    "end": "2071599"
  },
  {
    "text": "question down there um one question over here um carlos so i've been reading",
    "start": "2071599",
    "end": "2077200"
  },
  {
    "text": "over here the back i've been reading a lot about um not setting cpu limits i mean i've been",
    "start": "2077200",
    "end": "2083440"
  },
  {
    "text": "reading about people not recommending setting cpu limits on pods yep we've been trying to implement that at our",
    "start": "2083440",
    "end": "2090240"
  },
  {
    "text": "company um we just did that recently what's your opinion actually about that yeah i was looking at that recently and",
    "start": "2090240",
    "end": "2097599"
  },
  {
    "text": "so what happens if you don't set cpu limits your your workloads",
    "start": "2097599",
    "end": "2103920"
  },
  {
    "text": "basically are going to share based on how much their request so you have to both requesting",
    "start": "2103920",
    "end": "2109599"
  },
  {
    "text": "one cpu they are going to share the cpu uh 50 of the time",
    "start": "2109599",
    "end": "2115440"
  },
  {
    "text": "the problem is when you have pods that request a huge amount of mem of cpu",
    "start": "2115440",
    "end": "2122079"
  },
  {
    "text": "like things for customers so they have i don't know four cpus or hcps whatever but then you have cluster",
    "start": "2122079",
    "end": "2128880"
  },
  {
    "text": "services that you want always running that require but that only leads like",
    "start": "2128880",
    "end": "2134720"
  },
  {
    "text": "operators or something that only need 0.5 cpus so now suddenly",
    "start": "2134720",
    "end": "2140400"
  },
  {
    "text": "this workload can take a lot of the resources of the node and this other workload which may be critical for you",
    "start": "2140400",
    "end": "2148400"
  },
  {
    "text": "is starved so that's the balance uh why it's not",
    "start": "2148400",
    "end": "2153599"
  },
  {
    "text": "clear for us whether we want to remove the limits or not thank you",
    "start": "2153599",
    "end": "2159040"
  },
  {
    "text": "excellent i think that concludes the qa section if you have more questions for carlos please take it outside and",
    "start": "2159040",
    "end": "2165200"
  },
  {
    "text": "don't forget to rate the session in the in the app afterwards thanks everyone",
    "start": "2165200",
    "end": "2170690"
  },
  {
    "text": "[Applause]",
    "start": "2170690",
    "end": "2173539"
  }
]