[
  {
    "text": "hello everyone I'm happy to be here with you and thank you for joining our",
    "start": "30",
    "end": "6210"
  },
  {
    "text": "session about democratizing my sequel cloud managed to Burnett is managed my",
    "start": "6210",
    "end": "13920"
  },
  {
    "text": "name is Flavius major and I'm going to talk about the story of building press",
    "start": "13920",
    "end": "19949"
  },
  {
    "text": "loves operator for my sequel this is a joint session with a platform 9",
    "start": "19949",
    "end": "27420"
  },
  {
    "text": "searching month attack unfortunately he is not present today here but he",
    "start": "27420",
    "end": "33239"
  },
  {
    "text": "prepared a video for you and his colleague Daniel is here to answer your",
    "start": "33239",
    "end": "39120"
  },
  {
    "text": "questions he will talk about a case study how they migrated from cloud",
    "start": "39120",
    "end": "45960"
  },
  {
    "text": "managed to Burnett has managed SQL first",
    "start": "45960",
    "end": "52350"
  },
  {
    "text": "of all I want to start representing my Mexico operator and I will go through",
    "start": "52350",
    "end": "57390"
  },
  {
    "text": "these five main topics the context in which the operator was born the needs",
    "start": "57390",
    "end": "63930"
  },
  {
    "text": "that we had in mind also what have changed and what we achieved also some",
    "start": "63930",
    "end": "73770"
  },
  {
    "text": "challenges that we've encountered during development and finally the project",
    "start": "73770",
    "end": "79020"
  },
  {
    "text": "status and some future plans first let me introduce the company that I work for",
    "start": "79020",
    "end": "84180"
  },
  {
    "text": "and the context in which the operator was born we are a managed wordpress",
    "start": "84180",
    "end": "90450"
  },
  {
    "text": "hosting company doing business for more than 10 years we started as a WordPress",
    "start": "90450",
    "end": "95820"
  },
  {
    "text": "development agency then we pivoted towards the hosting business after",
    "start": "95820",
    "end": "101700"
  },
  {
    "text": "serving both publisher and enterprise clients for several years we came to",
    "start": "101700",
    "end": "108270"
  },
  {
    "text": "realize that all companies in the global top tire Enterprise were doing the same",
    "start": "108270",
    "end": "114119"
  },
  {
    "text": "thing including us so that's when we started thinking about",
    "start": "114119",
    "end": "119390"
  },
  {
    "text": "open source infrastructure that could become the WordPress hosting standard",
    "start": "119390",
    "end": "126140"
  },
  {
    "text": "name rest of stack so it's not just about that stack it's about all commit",
    "start": "126140",
    "end": "132420"
  },
  {
    "text": "to our mission to democratize WordPress hosting infrastructure and to share all",
    "start": "132420",
    "end": "137910"
  },
  {
    "text": "accumulated knowledge as part of our mission have two key objectives we are",
    "start": "137910",
    "end": "145410"
  },
  {
    "text": "currently building an open infrastructure using kubernetes to run and operate WordPress press lab stack",
    "start": "145410",
    "end": "154110"
  },
  {
    "text": "and the other is building the a magical operator for WordPress you may know that",
    "start": "154110",
    "end": "163019"
  },
  {
    "text": "half of WordPress hosting means hosting my sequel so why we just burn at ease",
    "start": "163019",
    "end": "169950"
  },
  {
    "text": "first it's run several from dev laptops public clouds private data centers also",
    "start": "169950",
    "end": "178530"
  },
  {
    "text": "it's open source we had experience before containers with containers our core services already",
    "start": "178530",
    "end": "186569"
  },
  {
    "text": "runs on kubernetes and it provides a lot of support for a lot of integrations",
    "start": "186569",
    "end": "194310"
  },
  {
    "text": "like search management nginx Prometheus for monitoring and so on so as part of",
    "start": "194310",
    "end": "202079"
  },
  {
    "text": "the stack we needed a way to automate certain operations such as deploying",
    "start": "202079",
    "end": "207560"
  },
  {
    "text": "scaling maintaining and backups for my sequel for that we've identified several",
    "start": "207560",
    "end": "214650"
  },
  {
    "text": "key requirements to focus on first we wanted something that it's easy to",
    "start": "214650",
    "end": "221340"
  },
  {
    "text": "operate something that doesn't get in your way second we need an elastic service to",
    "start": "221340",
    "end": "229140"
  },
  {
    "text": "help us scale with the demand and you all know that in hosting service uptime",
    "start": "229140",
    "end": "236280"
  },
  {
    "text": "is paramount so we had to maximize the",
    "start": "236280",
    "end": "241920"
  },
  {
    "text": "availability for service also no one wants to lose data especially when it",
    "start": "241920",
    "end": "248310"
  },
  {
    "text": "comes to someone else's data and in order to reliably operate the service",
    "start": "248310",
    "end": "254660"
  },
  {
    "text": "our system we need a method to observe what's happening from the top down to",
    "start": "254660",
    "end": "260940"
  },
  {
    "text": "the request level so with this in mind a mind we checked",
    "start": "260940",
    "end": "266190"
  },
  {
    "text": "some of the available solution and conclude that they were not suitable for",
    "start": "266190",
    "end": "271290"
  },
  {
    "text": "us for example both Oracle and Percona operator performs group replication",
    "start": "271290",
    "end": "278120"
  },
  {
    "text": "which implies that they requires more nodes to operate at least three and this",
    "start": "278120",
    "end": "285390"
  },
  {
    "text": "will increase the cost so as great engineers do we've ended up building a",
    "start": "285390",
    "end": "291450"
  },
  {
    "text": "solution ourself a kubernetes operator for managing magical cluster with",
    "start": "291450",
    "end": "298340"
  },
  {
    "text": "asynchronous or semi synchronous replication so in the past ten years",
    "start": "298340",
    "end": "303390"
  },
  {
    "text": "we've identified several must-have features which have been integrated into",
    "start": "303390",
    "end": "310140"
  },
  {
    "text": "the operator to fulfill our basic needs needs that I just presented",
    "start": "310140",
    "end": "315660"
  },
  {
    "text": "so first is self-healing clusters without this feature the operator",
    "start": "315660",
    "end": "320820"
  },
  {
    "text": "doesn't make sense and the operator has to continuously reconcile and solve for",
    "start": "320820",
    "end": "325980"
  },
  {
    "text": "application issues also high highly available reads this is true and more",
    "start": "325980",
    "end": "333480"
  },
  {
    "text": "more nodes are available also virtual highly available writes this provides us",
    "start": "333480",
    "end": "339660"
  },
  {
    "text": "minimum of downtime due to fast failover we'll see how replication lag detection",
    "start": "339660",
    "end": "346080"
  },
  {
    "text": "and mitigation the operator takes takes",
    "start": "346080",
    "end": "351300"
  },
  {
    "text": "lag node out of rotation or unhealthy nodes and resource abuse control this is",
    "start": "351300",
    "end": "359610"
  },
  {
    "text": "very useful because it can limit noisy queries that can slow down your cluster",
    "start": "359610",
    "end": "364800"
  },
  {
    "text": "also automate backups and restore this",
    "start": "364800",
    "end": "369840"
  },
  {
    "text": "one speaks for itself so all of this feature have proven to be very helpful",
    "start": "369840",
    "end": "375620"
  },
  {
    "text": "compared with our setup now let's move on to the practical aspects of building",
    "start": "375620",
    "end": "382380"
  },
  {
    "text": "the operator in these figures presented the entire system overview the heat",
    "start": "382380",
    "end": "387600"
  },
  {
    "text": "actually split into three main parts the control plane data plane and monitoring",
    "start": "387600",
    "end": "392810"
  },
  {
    "text": "the control plane consists of the operator and its components",
    "start": "392810",
    "end": "399120"
  },
  {
    "text": "and are deployed using count using usually in a dedicated namespace here we",
    "start": "399120",
    "end": "406350"
  },
  {
    "text": "have the controller itself and your Orchestrator a high availability and",
    "start": "406350",
    "end": "413610"
  },
  {
    "text": "application management tool I'll come back to it later in the talk now the",
    "start": "413610",
    "end": "420210"
  },
  {
    "text": "data plane represents the Mexico actual Mexico deployment and it's made of basic",
    "start": "420210",
    "end": "426150"
  },
  {
    "text": "components resources like pods services and so on which can be spread across multiple namespaces and last but not",
    "start": "426150",
    "end": "433290"
  },
  {
    "text": "least we have monitoring which is performed by parameters the standard kubernetes monitoring system going",
    "start": "433290",
    "end": "441150"
  },
  {
    "text": "deeper into the data plane we can see that the my cycle cluster has multiple",
    "start": "441150",
    "end": "447000"
  },
  {
    "text": "components for example the state who said that's the main component and is",
    "start": "447000",
    "end": "454680"
  },
  {
    "text": "responsible for provisioning pods and persistent volumes for each mexico node",
    "start": "454680",
    "end": "461870"
  },
  {
    "text": "also there are two services a master service that always points to the master",
    "start": "461870",
    "end": "468560"
  },
  {
    "text": "master node and a healthy node service that points to all nodes that are",
    "start": "468560",
    "end": "475680"
  },
  {
    "text": "considered healthy by the operator the selection are made based on information",
    "start": "475680",
    "end": "481220"
  },
  {
    "text": "gathered from Orchestrator so your application will",
    "start": "481220",
    "end": "486690"
  },
  {
    "text": "interact with those two services for rights and for reads and is the",
    "start": "486690",
    "end": "493770"
  },
  {
    "text": "application responsibility to split them by using specific logic or some",
    "start": "493770",
    "end": "500160"
  },
  {
    "text": "dedicated software like proxy sequel ok so internally and not consists of",
    "start": "500160",
    "end": "507419"
  },
  {
    "text": "several components some init containers that are used for mexico initialization",
    "start": "507419",
    "end": "513330"
  },
  {
    "text": "and configuration a main container which",
    "start": "513330",
    "end": "518430"
  },
  {
    "text": "is a per corner server for mexico which is per corner because it's a battle-tested in enterprise environments",
    "start": "518430",
    "end": "524430"
  },
  {
    "text": "and it's a drop-in replacement for my sequel and also there are some site",
    "start": "524430",
    "end": "530010"
  },
  {
    "text": "cards some of them are based on Percona toolkit which are",
    "start": "530010",
    "end": "536040"
  },
  {
    "text": "responsible for serving several action like lag detection monitoring resource",
    "start": "536040",
    "end": "544199"
  },
  {
    "text": "limit policy enforcement and there is more one more container that provides an",
    "start": "544199",
    "end": "549929"
  },
  {
    "text": "endpoint for backups and for other nodes to initialize from so now I want to",
    "start": "549929",
    "end": "559709"
  },
  {
    "text": "mention some specific challenges that we had during the implementation of the",
    "start": "559709",
    "end": "566459"
  },
  {
    "text": "operator such of Orchestrator integration how we integrate",
    "start": "566459",
    "end": "573689"
  },
  {
    "text": "Orchestrator a third-party tool so we don't have to reinvent the wheel",
    "start": "573689",
    "end": "580220"
  },
  {
    "text": "persistent volume cleanup we have to manage persistent volumes or self",
    "start": "580220",
    "end": "586079"
  },
  {
    "text": "because the Wake Burnett is does it is not fit suitable for mass equal operator",
    "start": "586079",
    "end": "594149"
  },
  {
    "text": "upgrades and deploy which is a common problem between the operators because",
    "start": "594149",
    "end": "599429"
  },
  {
    "text": "and provides very modest CRD management and my sequel operate this is a specific",
    "start": "599429",
    "end": "609269"
  },
  {
    "text": "problem because usually it's done by humans and it's very hard to automate so",
    "start": "609269",
    "end": "615899"
  },
  {
    "text": "let me start by presenting the orchestrator integration Orchestrator is",
    "start": "615899",
    "end": "622529"
  },
  {
    "text": "a sub component of the entire operator and it's the tool that handles my sequel",
    "start": "622529",
    "end": "629629"
  },
  {
    "text": "fan overs and topology but it's not",
    "start": "629629",
    "end": "635129"
  },
  {
    "text": "meant to be stateless as operator usual usually are so kubernetes has a state",
    "start": "635129",
    "end": "642889"
  },
  {
    "text": "Orchestrator has a state and the operator doesn't know which one to listen to here I'm talking about an",
    "start": "642889",
    "end": "650399"
  },
  {
    "text": "information flow conflict to fix this we choose to implement a reconciliation",
    "start": "650399",
    "end": "655799"
  },
  {
    "text": "loop between Orchestrator and communities which every few seconds",
    "start": "655799",
    "end": "662299"
  },
  {
    "text": "reconcile reconciles the state between them so on one hand Orchestrator",
    "start": "662299",
    "end": "669369"
  },
  {
    "text": "is responsible for updating replication topology in emergency situation in case",
    "start": "669369",
    "end": "675850"
  },
  {
    "text": "of downtime and to observe the current status of the cluster on the other hand",
    "start": "675850",
    "end": "684040"
  },
  {
    "text": "the operator reconciles the desired topology into the orchestrator and",
    "start": "684040",
    "end": "689220"
  },
  {
    "text": "provide service discovery for Orchestrator so even if the data from",
    "start": "689220",
    "end": "695439"
  },
  {
    "text": "Orchestrator is lost the operator is able to restore that data so as a",
    "start": "695439",
    "end": "703240"
  },
  {
    "text": "conclusion the operator takes decision based on information found in kubernetes",
    "start": "703240",
    "end": "712319"
  },
  {
    "text": "which is always up-to-date because of the reconciliation though another",
    "start": "712709",
    "end": "719319"
  },
  {
    "text": "challenge was how Burnett has managed persistent volumes the magical data is",
    "start": "719319",
    "end": "726429"
  },
  {
    "text": "stored in persistent volumes managed by the state Fassett but this implies that",
    "start": "726429",
    "end": "732459"
  },
  {
    "text": "when a cluster is scaled down the volumes are not deleted so after a while",
    "start": "732459",
    "end": "737740"
  },
  {
    "text": "when the cluster is scaled back up the replication may fail to fix this we",
    "start": "737740",
    "end": "745809"
  },
  {
    "text": "choose to implement we choose to implement cleaner ideal a worker that",
    "start": "745809",
    "end": "752559"
  },
  {
    "text": "deal its persistent volumes when the cluster scaled down except for node 0",
    "start": "752559",
    "end": "758259"
  },
  {
    "text": "which is a special case because that data should be kept as longer as longer",
    "start": "758259",
    "end": "764589"
  },
  {
    "text": "as the cluster exists to preserve data to preserve losing data another",
    "start": "764589",
    "end": "771579"
  },
  {
    "text": "challenge is a common problem in the world of operators because how share D",
    "start": "771579",
    "end": "777549"
  },
  {
    "text": "management is still very painful currently the de facto standard for",
    "start": "777549",
    "end": "782559"
  },
  {
    "text": "packaging application is helm if you are a hand user you probably know that CRD",
    "start": "782559",
    "end": "790119"
  },
  {
    "text": "management still still very painful and ham doesn't know a dozen provides and",
    "start": "790119",
    "end": "796060"
  },
  {
    "text": "obligated path for series what is more my sickle operator is still in",
    "start": "796060",
    "end": "801550"
  },
  {
    "text": "development and share this specification are subject to change this made us to",
    "start": "801550",
    "end": "809980"
  },
  {
    "text": "install er this without validation to minimize user intervention at upgrades",
    "start": "809980",
    "end": "815220"
  },
  {
    "text": "however we hope we hope that this is a temporary fix solution until hands",
    "start": "815220",
    "end": "822040"
  },
  {
    "text": "improve its support for CR DS now a",
    "start": "822040",
    "end": "827700"
  },
  {
    "text": "specific challenge is how my sequel upgrades are performed kubernetes",
    "start": "827700",
    "end": "834550"
  },
  {
    "text": "already provides us some upgrade policies like rolling up gates for stateful set but this is not exactly",
    "start": "834550",
    "end": "842200"
  },
  {
    "text": "gentle for my sequel because it can choose to update first the master which",
    "start": "842200",
    "end": "849070"
  },
  {
    "text": "will trigger a fellow to the replica then when the replica is updated will",
    "start": "849070",
    "end": "856870"
  },
  {
    "text": "trigger another failover which is unnecessary and can be avoid if the",
    "start": "856870",
    "end": "862990"
  },
  {
    "text": "master is the last one that is updated so a contributor came up with an idea to",
    "start": "862990",
    "end": "871540"
  },
  {
    "text": "use on delete policy which fits bad batter on needs because the operator can",
    "start": "871540",
    "end": "878380"
  },
  {
    "text": "choose which node to update therefore we can control the order in which pods are",
    "start": "878380",
    "end": "885580"
  },
  {
    "text": "upgraded we try also other techniques like pod finalizer",
    "start": "885580",
    "end": "891580"
  },
  {
    "text": "to block the pod deletion until the failover is done but we hit a dead end",
    "start": "891580",
    "end": "897820"
  },
  {
    "text": "because we misunderstood how kubernetes finalizer work it happens also we try",
    "start": "897820",
    "end": "905380"
  },
  {
    "text": "using container lifecycle hooks but proved to be too complicated to",
    "start": "905380",
    "end": "911290"
  },
  {
    "text": "implement so we choose to implement on delete policy which is still",
    "start": "911290",
    "end": "916510"
  },
  {
    "text": "work-in-progress now I want to share with you the current",
    "start": "916510",
    "end": "921640"
  },
  {
    "text": "status of the project and some future plans so we plan to integrate with",
    "start": "921640",
    "end": "927730"
  },
  {
    "text": "marketplaces like Google cloud marketplace operator hub and AWS marketplace to make easier",
    "start": "927730",
    "end": "936250"
  },
  {
    "text": "for and user to install it also we'd like to finish what we started so we would like to introduce your the",
    "start": "936250",
    "end": "942430"
  },
  {
    "text": "validation and well folks and also",
    "start": "942430",
    "end": "947490"
  },
  {
    "text": "multiple backups policy for granular control over backups and to make it",
    "start": "947490",
    "end": "955750"
  },
  {
    "text": "easier for your application to connect to the cluster we'd like to integrate proxy Sequoia",
    "start": "955750",
    "end": "962220"
  },
  {
    "text": "instead of using that to services that I just presented the your application",
    "start": "962220",
    "end": "968620"
  },
  {
    "text": "connects to the proxy which will do the routing for you for rides and for it now",
    "start": "968620",
    "end": "976270"
  },
  {
    "text": "the operator is still in alpha version and we are really close really close to",
    "start": "976270",
    "end": "982300"
  },
  {
    "text": "two beta we have a good feedback from community so far and some major",
    "start": "982300",
    "end": "988089"
  },
  {
    "text": "platforms already actively use it and contribute to it so I would like to",
    "start": "988089",
    "end": "995440"
  },
  {
    "text": "invite you to visit the project page on github and for any questions to join on",
    "start": "995440",
    "end": "1002100"
  },
  {
    "text": "slack Channel my sequel operator slack channel coming up next I will play a",
    "start": "1002100",
    "end": "1011459"
  },
  {
    "text": "video or recorded video with the second part of the presentation where session talked about a case study how they",
    "start": "1011459",
    "end": "1019650"
  },
  {
    "text": "migrate from cloud manage sequel to govern others manage so that our session",
    "start": "1019650",
    "end": "1028400"
  },
  {
    "text": "that's hello my name is Sachin I work as",
    "start": "1028400",
    "end": "1035130"
  },
  {
    "text": "an engineer at platform 9 sorry I couldn't be there at Q Khan in person but hopefully this video would give you",
    "start": "1035130",
    "end": "1042959"
  },
  {
    "text": "you enough insight into how we have been using this operator and what your our",
    "start": "1042959",
    "end": "1048540"
  },
  {
    "text": "clients are with this operator and I'll be around to answer questions over phone",
    "start": "1048540",
    "end": "1054570"
  },
  {
    "text": "as well so just a quick intro to platform 9 we are a company which offers",
    "start": "1054570",
    "end": "1060620"
  },
  {
    "text": "open cloud services store customs and we have an offering which is based on OpenStack as well as",
    "start": "1060620",
    "end": "1067710"
  },
  {
    "text": "kubernetes this is a managed offering of the cloud services and we have around",
    "start": "1067710",
    "end": "1074010"
  },
  {
    "text": "300 cloud regions around management today and we are managing quite",
    "start": "1074010",
    "end": "1079350"
  },
  {
    "text": "substantial amount of capital so it's like 500,000 cores or computer and Arad",
    "start": "1079350",
    "end": "1084690"
  },
  {
    "text": "management so what is how does this try",
    "start": "1084690",
    "end": "1091050"
  },
  {
    "text": "to the my Sequoia operator we started back in 2014 and as well startup we",
    "start": "1091050",
    "end": "1097080"
  },
  {
    "text": "adopted the public cloud model so using public cloud services without investing",
    "start": "1097080",
    "end": "1103380"
  },
  {
    "text": "too much into our IT infrastructure be equal to grow our customer base of fairly quickly but at the current scale",
    "start": "1103380",
    "end": "1111860"
  },
  {
    "text": "the cost of public cloud is quite significant just our database bills",
    "start": "1111860",
    "end": "1118980"
  },
  {
    "text": "amount to tens of thousands of dollars and that's a significant drag for a small sized company like us so as I",
    "start": "1118980",
    "end": "1128520"
  },
  {
    "text": "explained before the 2014-2016 years were mainly based on public cloud",
    "start": "1128520",
    "end": "1135390"
  },
  {
    "text": "services using exclusively RDS or all our diplomatic work needs we also used",
    "start": "1135390",
    "end": "1142080"
  },
  {
    "text": "other services like the database DNS load balancer etc there are two Phillip",
    "start": "1142080",
    "end": "1149700"
  },
  {
    "text": "gaps then in 2017 we decided to move",
    "start": "1149700",
    "end": "1154800"
  },
  {
    "text": "completely off of the public cloud in order to save on these costs as well as",
    "start": "1154800",
    "end": "1161180"
  },
  {
    "text": "scale according to our requirements so",
    "start": "1161180",
    "end": "1167160"
  },
  {
    "text": "we we already are we started using our own offering the OpenStack offering to",
    "start": "1167160",
    "end": "1173430"
  },
  {
    "text": "run our the instances that we run for our customers and we run this in our our",
    "start": "1173430",
    "end": "1180000"
  },
  {
    "text": "own private cloud in London but at that time we realized that moving off of",
    "start": "1180000",
    "end": "1186030"
  },
  {
    "text": "database wasn't so easy because there was no not quite equivalent service like",
    "start": "1186030",
    "end": "1192060"
  },
  {
    "text": "RDS that you could simply run on a private road infrastructure and that's when we found this my sickle",
    "start": "1192060",
    "end": "1199789"
  },
  {
    "text": "operator created by press labs and tested it validated it and we are",
    "start": "1199789",
    "end": "1205099"
  },
  {
    "text": "rolling it out in production so the plan is that this year we'll be rolling out our production deployments and by end of",
    "start": "1205099",
    "end": "1212479"
  },
  {
    "text": "2020 we'll have all our production deployments on this our database so what",
    "start": "1212479",
    "end": "1219559"
  },
  {
    "text": "were the hurdles to to running our own database as a service and getting out of",
    "start": "1219559",
    "end": "1226069"
  },
  {
    "text": "consuming the the public cloud based databases so first problem was that the",
    "start": "1226069",
    "end": "1231889"
  },
  {
    "text": "our deployment automation is set up with",
    "start": "1231889",
    "end": "1236899"
  },
  {
    "text": "a self-service API based model in mind so the ansible based scripts that we",
    "start": "1236899",
    "end": "1242239"
  },
  {
    "text": "have they exercise the RDS api is to create scale delete resize databases and",
    "start": "1242239",
    "end": "1249579"
  },
  {
    "text": "we needed a very similar self-service api experience in our own translation we",
    "start": "1249579",
    "end": "1255709"
  },
  {
    "text": "also relied on snapshotting and backups that RDS profiles and again this this",
    "start": "1255709",
    "end": "1262069"
  },
  {
    "text": "process was also automated using AWS api's and we needed a replacement for",
    "start": "1262069",
    "end": "1268669"
  },
  {
    "text": "that so because of these two points we couldn't really adopt a solution where",
    "start": "1268669",
    "end": "1273739"
  },
  {
    "text": "we created we create a database in a VM or on a bare metal silver and then ssh",
    "start": "1273739",
    "end": "1280729"
  },
  {
    "text": "into it to make changes that that would have broken our automation another",
    "start": "1280729",
    "end": "1287329"
  },
  {
    "text": "problem was that by relying exclusively on a hosted My Secret Service me we",
    "start": "1287329",
    "end": "1293029"
  },
  {
    "text": "didn't really have database admins to save us from situations where our database backup goes wrong or database",
    "start": "1293029",
    "end": "1300949"
  },
  {
    "text": "current gets corrupted or the service keeps crashing divison bugs we also",
    "start": "1300949",
    "end": "1307189"
  },
  {
    "text": "require comparable performance with audience because we we wanted to sort of",
    "start": "1307189",
    "end": "1313939"
  },
  {
    "text": "do a lift and shift and we wanted to make sure that when we move a customer",
    "start": "1313939",
    "end": "1319069"
  },
  {
    "text": "to an on-prem database they shouldn't see any performance degradation in",
    "start": "1319069",
    "end": "1325369"
  },
  {
    "text": "addition all our services were designed with expecting the my sequel as the",
    "start": "1325369",
    "end": "1331039"
  },
  {
    "text": "backend so when we switch databases we wanted a drop-in replacement to avoid",
    "start": "1331039",
    "end": "1337009"
  },
  {
    "text": "any unforeseen bugs and impact on our production environments so with these",
    "start": "1337009",
    "end": "1343579"
  },
  {
    "text": "goals in mind we we realized that using a simple VM",
    "start": "1343579",
    "end": "1349009"
  },
  {
    "text": "based approach might not be a good thing for us and we started looking at a",
    "start": "1349009",
    "end": "1354799"
  },
  {
    "text": "selfing platforms like kubernetes to run our databases that's what we found out",
    "start": "1354799",
    "end": "1360499"
  },
  {
    "text": "about the my sickle operator and understood various features that it provides and it basically ticked all our",
    "start": "1360499",
    "end": "1367549"
  },
  {
    "text": "check boxes so this is how our comparison per our requirement looks",
    "start": "1367549",
    "end": "1374119"
  },
  {
    "text": "like looks like we wanted a self-service simple open API and that is provided by",
    "start": "1374119",
    "end": "1380119"
  },
  {
    "text": "the CR DS that could that this operator exposes and it hooks nicely into our",
    "start": "1380119",
    "end": "1386929"
  },
  {
    "text": "automation we wanted a drop-in replacement for my sequel and since this",
    "start": "1386929",
    "end": "1392659"
  },
  {
    "text": "implementation was a spark owner which is a blue one my secure database it is we found it to be quite compatible we",
    "start": "1392659",
    "end": "1400279"
  },
  {
    "text": "haven't really discovered any breakages or bugs in our services to do switch in",
    "start": "1400279",
    "end": "1405919"
  },
  {
    "text": "the database itself we wanted fairly automated API driven backup and recovery",
    "start": "1405919",
    "end": "1411799"
  },
  {
    "text": "process and that is provided by the database backup CR that my secure",
    "start": "1411799",
    "end": "1418759"
  },
  {
    "text": "operator provides and we were able to switch our automation to use this API and take the backup to s3 as well as",
    "start": "1418759",
    "end": "1426229"
  },
  {
    "text": "recover from s3 backups automatically since we are trying to do this at scale",
    "start": "1426229",
    "end": "1434209"
  },
  {
    "text": "so we will be will have around 300 databases at least to manage when you go",
    "start": "1434209",
    "end": "1439699"
  },
  {
    "text": "to production we wanted a fairly automated and loaded solution when it comes to handling failures and we found",
    "start": "1439699",
    "end": "1447949"
  },
  {
    "text": "that having the highly available database using Orchestrator it's quite",
    "start": "1447949",
    "end": "1453440"
  },
  {
    "text": "appropriate for our requirements and we so we tested out straighter in the wild and found that is",
    "start": "1453440",
    "end": "1461030"
  },
  {
    "text": "able to recover from TB master failures in a quiet automated and seamless manner",
    "start": "1461030",
    "end": "1469810"
  },
  {
    "text": "given our experience with an existing cloud service we wanted to avoid the vendor lock-in and we wanted that",
    "start": "1469810",
    "end": "1476990"
  },
  {
    "text": "ability and flexibility to switch the underlying cloud platform without disrupting our database as a service",
    "start": "1476990",
    "end": "1483110"
  },
  {
    "text": "offering and the the underlying kubernetes layer provides sufficient",
    "start": "1483110",
    "end": "1490520"
  },
  {
    "text": "abstraction for us and we have enough confidence that we'll be able to run this database not only on our private in",
    "start": "1490520",
    "end": "1497930"
  },
  {
    "text": "transaction but any other cloud that supports kubernetes and last but not",
    "start": "1497930",
    "end": "1503870"
  },
  {
    "text": "least we will be relied on the built-in monitoring that the databases service provided we regularly check and generate",
    "start": "1503870",
    "end": "1512300"
  },
  {
    "text": "alerts based on already seconds and certain thresholds and the witness",
    "start": "1512300",
    "end": "1520550"
  },
  {
    "text": "matrix that my sickle operator provides we found them to be quite adequate for this purpose in fact they are more",
    "start": "1520550",
    "end": "1527390"
  },
  {
    "text": "extensive than water date manage database and this provides so this",
    "start": "1527390",
    "end": "1533300"
  },
  {
    "text": "database checked all the boxes that we needed and we started implementing it on",
    "start": "1533300",
    "end": "1540260"
  },
  {
    "text": "thread and this is how our infrastructure looked like so as I explained before we had an open stat",
    "start": "1540260",
    "end": "1547190"
  },
  {
    "text": "cloud which provided multiple availability zones each availability",
    "start": "1547190",
    "end": "1552530"
  },
  {
    "text": "zone is essentially a failure domain for compute storage as well as networking and although this diagram doesn't show",
    "start": "1552530",
    "end": "1560210"
  },
  {
    "text": "it our storage is also multiple easy aware and for example the failures in",
    "start": "1560210",
    "end": "1566720"
  },
  {
    "text": "easy to doesn't impact the storage in easy parking so we have these three as",
    "start": "1566720",
    "end": "1572240"
  },
  {
    "text": "is provided by underlying OpenStack clock and for kubernetes week we use the",
    "start": "1572240",
    "end": "1578180"
  },
  {
    "text": "multi master our deployment where each master runs in its own availability zone",
    "start": "1578180",
    "end": "1584330"
  },
  {
    "text": "and we have a load balancer in front two rows will be logging the three motors",
    "start": "1584330",
    "end": "1590789"
  },
  {
    "text": "to deploy databases we also leverage this AZ concept so the availability zone",
    "start": "1590789",
    "end": "1597759"
  },
  {
    "text": "labels are bubbled up as work on all the labels in kubernetes and the deployment",
    "start": "1597759",
    "end": "1604299"
  },
  {
    "text": "of any database always has at least three instances so one master and two",
    "start": "1604299",
    "end": "1611230"
  },
  {
    "text": "replicas and each one of them come up in a different abilities in addition the orchestrator piece also runs with a Z",
    "start": "1611230",
    "end": "1620940"
  },
  {
    "text": "multi a-z in mind so there is one copy of Orchestrator in djz so single easy",
    "start": "1620940",
    "end": "1626559"
  },
  {
    "text": "failure does not impart all the layers including the kubernetes Orchestrator as",
    "start": "1626559",
    "end": "1633639"
  },
  {
    "text": "well as the database in addition we take backups of this database to s3 and",
    "start": "1633639",
    "end": "1642059"
  },
  {
    "text": "recover from a stream so that's the layer of cloud infrastructure which",
    "start": "1642059",
    "end": "1647110"
  },
  {
    "text": "still using so the current state is that we have validated and it works pretty",
    "start": "1647110",
    "end": "1653950"
  },
  {
    "text": "well for audience case and all our internal Devon test environments are use",
    "start": "1653950",
    "end": "1659440"
  },
  {
    "text": "this database today to rule out the operator and to test the newer version",
    "start": "1659440",
    "end": "1666970"
  },
  {
    "text": "of operators we have set up three clusters the dev cluster is used to",
    "start": "1666970",
    "end": "1672249"
  },
  {
    "text": "validate changes in operator and show that there is no breaking change after",
    "start": "1672249",
    "end": "1678220"
  },
  {
    "text": "validation and testing it's promote gets promoted to staging where in that area",
    "start": "1678220",
    "end": "1684490"
  },
  {
    "text": "we manage our internal dev and test environments once it gets going there it",
    "start": "1684490",
    "end": "1690879"
  },
  {
    "text": "gets promoted to production to run actual personal clothes so our plan is",
    "start": "1690879",
    "end": "1696789"
  },
  {
    "text": "to do 100% deployments of databases managed by a bicycle operator and",
    "start": "1696789",
    "end": "1703019"
  },
  {
    "text": "another nice thing out of this exercise is that we we stumbled across the",
    "start": "1703019",
    "end": "1711129"
  },
  {
    "text": "operator paradigm and the CR DS which is very powerful for managing your",
    "start": "1711129",
    "end": "1716259"
  },
  {
    "text": "infrastructure in a fairly automated map so we plan to use the same model for",
    "start": "1716259",
    "end": "1723410"
  },
  {
    "text": "Oh Matias based monitoring drug collection API gateways and so with that",
    "start": "1723410",
    "end": "1729500"
  },
  {
    "text": "I'm end of at the end of my presentation thank you and I'll be around for",
    "start": "1729500",
    "end": "1735020"
  },
  {
    "text": "questions thank you this action can you",
    "start": "1735020",
    "end": "1752420"
  },
  {
    "text": "hear us so you have questions yeah I can",
    "start": "1752420",
    "end": "1758120"
  },
  {
    "text": "hear you please ask hi I'm Daniel I work",
    "start": "1758120",
    "end": "1763160"
  },
  {
    "text": "with Sachin and unfortunately you couldn't be here today in person but we",
    "start": "1763160",
    "end": "1768950"
  },
  {
    "text": "went from on-premise Sachin to cloud-based Hodgins so sort of in Reverse but yeah well we'll relay",
    "start": "1768950",
    "end": "1775910"
  },
  {
    "text": "questions that you have to him",
    "start": "1775910",
    "end": "1780640"
  },
  {
    "text": "yeah I would like to know what you think about the current production readiness",
    "start": "1788860",
    "end": "1794450"
  },
  {
    "text": "of all of this I mean you said that it's alpha-beta thing so at which point are you at the",
    "start": "1794450",
    "end": "1801080"
  },
  {
    "text": "point where you think it's ready for production use we already use it in",
    "start": "1801080",
    "end": "1808340"
  },
  {
    "text": "production not critical workloads we",
    "start": "1808340",
    "end": "1815600"
  },
  {
    "text": "have finally did I rewrite something we change and it's a release candidate and",
    "start": "1815600",
    "end": "1823309"
  },
  {
    "text": "we have to improve some as I said orchestrate our integration and to",
    "start": "1823309",
    "end": "1829100"
  },
  {
    "text": "finish on delete policy implementation and after that almost ready from our",
    "start": "1829100",
    "end": "1839630"
  },
  {
    "text": "point of view of course there's still",
    "start": "1839630",
    "end": "1844720"
  },
  {
    "text": "need for a lot of tests but we already",
    "start": "1844720",
    "end": "1850160"
  },
  {
    "text": "use it in production and such guys from platform lines already test and used the",
    "start": "1850160",
    "end": "1858020"
  },
  {
    "text": "operator in I think in production not sure for some of them workloads session",
    "start": "1858020",
    "end": "1865669"
  },
  {
    "text": "did you did you hear the question it was you know when to you when do we consider",
    "start": "1865669",
    "end": "1874070"
  },
  {
    "text": "this you know alpha where where we where",
    "start": "1874070",
    "end": "1881390"
  },
  {
    "text": "we along in the journey what's important for us when we go from alpha to beta the ga eventually oh thank",
    "start": "1881390",
    "end": "1891530"
  },
  {
    "text": "you thank you muted anyone ever muted on",
    "start": "1891530",
    "end": "1896720"
  },
  {
    "text": "the zoom call one person can you hear me",
    "start": "1896720",
    "end": "1906130"
  },
  {
    "text": "[Laughter]",
    "start": "1909350",
    "end": "1913770"
  },
  {
    "text": "and other questions for me can you hear me all right",
    "start": "1922120",
    "end": "1932120"
  },
  {
    "text": "I'll tell you one that oh yeah so here for almost a year now and it's been",
    "start": "1932120",
    "end": "1938780"
  },
  {
    "text": "pretty solid in replacing audiences this week : tortillas that gives us a lot of",
    "start": "1938780",
    "end": "1945940"
  },
  {
    "text": "data that are this provides we can monitor it pretty well most of the",
    "start": "1945940",
    "end": "1951140"
  },
  {
    "text": "problems we we had we have not actually because of the operator but because the way our content from kubernetes was and",
    "start": "1951140",
    "end": "1963460"
  },
  {
    "text": "so we think it's Russian ready and as I said in the talk we have we hope that",
    "start": "1963460",
    "end": "1972130"
  },
  {
    "text": "we'll be able to roll it out completely",
    "start": "1972429",
    "end": "1977110"
  },
  {
    "text": "thank you very much for the talk I have a question so first one is how do you",
    "start": "1979419",
    "end": "1987470"
  },
  {
    "text": "deal with growth when you have to change the precision volume claim or does that",
    "start": "1987470",
    "end": "1992510"
  },
  {
    "text": "work with the operator and the second one is since you delete the persistent",
    "start": "1992510",
    "end": "1997970"
  },
  {
    "text": "volume when you scale down when you go up again you have to replicate all the",
    "start": "1997970",
    "end": "2004000"
  },
  {
    "text": "data and what's the latency there",
    "start": "2004000",
    "end": "2009020"
  },
  {
    "text": "I'm sorry the second questions depends",
    "start": "2009020",
    "end": "2015660"
  },
  {
    "text": "if we use extra backup for backups and we stream that back up to the other node",
    "start": "2015660",
    "end": "2020790"
  },
  {
    "text": "so if we delete that persistent volume one is recreated for small databases",
    "start": "2020790",
    "end": "2027950"
  },
  {
    "text": "takes you a few seconds one minute for",
    "start": "2027950",
    "end": "2033780"
  },
  {
    "text": "large databases takes more and sorry if you can repeat the first questions yeah",
    "start": "2033780",
    "end": "2051750"
  },
  {
    "text": "you can resize the persistent volumes the operator doesn't do that so you have",
    "start": "2051750",
    "end": "2058620"
  },
  {
    "text": "to do it manually it's Cooper necessary it provides you Matt to resize yeah okay I wonder if you",
    "start": "2058620",
    "end": "2073889"
  },
  {
    "text": "could elaborate on why they will be interested in moving to proxy executors and still instead of having two services",
    "start": "2073890",
    "end": "2082100"
  },
  {
    "text": "to make easier to to make to make it",
    "start": "2082460",
    "end": "2089429"
  },
  {
    "text": "easier for people for application to connect to it it's easier to have a one endpoint",
    "start": "2089429",
    "end": "2094879"
  },
  {
    "text": "instead of using some I don't know specific publication logic usually",
    "start": "2094880",
    "end": "2101010"
  },
  {
    "text": "applications use only one endpoint for region from writes and now not all of",
    "start": "2101010",
    "end": "2109470"
  },
  {
    "text": "them have this little logic that's why but we will provide both of them both",
    "start": "2109470",
    "end": "2115200"
  },
  {
    "text": "solution because we prefer having in",
    "start": "2115200",
    "end": "2120570"
  },
  {
    "text": "this way yeah I was going to ask you use",
    "start": "2120570",
    "end": "2127960"
  },
  {
    "text": "Orchestrator yeah that I think service itself yeah and I guess as you were",
    "start": "2127960",
    "end": "2136540"
  },
  {
    "text": "saying that the source of truth for Orchestrator is the cover Nidhi's yeah",
    "start": "2136540",
    "end": "2142210"
  },
  {
    "text": "and you have some sort of sink what happens when Orchestrator goes down and",
    "start": "2142210",
    "end": "2147820"
  },
  {
    "text": "you have a failover situation I mean how long does Orchestrator take to recover",
    "start": "2147820",
    "end": "2154680"
  },
  {
    "text": "itself yeah Orchestrator is highly available it",
    "start": "2154680",
    "end": "2160630"
  },
  {
    "text": "can be deployed in multiple clusters so it can be highly available it's used raft communicate and so on if it's",
    "start": "2160630",
    "end": "2167710"
  },
  {
    "text": "highly available then it's always up hopefully if not that's for the moment",
    "start": "2167710",
    "end": "2177760"
  },
  {
    "text": "it's an undefined behavior it we still do tests to see what's happened it may",
    "start": "2177760",
    "end": "2187030"
  },
  {
    "text": "split that cluster into two surprise separate server so the partitions I mean",
    "start": "2187030",
    "end": "2194290"
  },
  {
    "text": "so you can configure in the operator how many orchestrators do you have for",
    "start": "2194290",
    "end": "2201670"
  },
  {
    "text": "availability I mean yeah for no one but usually three registry entry to",
    "start": "2201670",
    "end": "2208630"
  },
  {
    "text": "Orchestrator and three operators so",
    "start": "2208630",
    "end": "2215670"
  },
  {
    "text": "another question",
    "start": "2215670",
    "end": "2219119"
  },
  {
    "text": "such and you you want to talk about you know how how we've been dealing with the",
    "start": "2224620",
    "end": "2232010"
  },
  {
    "text": "growth of you know the database and how we resize persistent volumes what's our",
    "start": "2232010",
    "end": "2238100"
  },
  {
    "text": "experience been like and then also discuss what our experience has been like with operator",
    "start": "2238100",
    "end": "2245599"
  },
  {
    "text": "sorry the orchestrator and you know what what our plans are you know when when",
    "start": "2245599",
    "end": "2252119"
  },
  {
    "text": "Orchestrator goes down and there's an issue or for the my single operator to",
    "start": "2252119",
    "end": "2259260"
  },
  {
    "text": "actually access yes sure so if usually",
    "start": "2259260",
    "end": "2267420"
  },
  {
    "text": "when we resize the database we we do have some interval of time where we can",
    "start": "2267420",
    "end": "2272580"
  },
  {
    "text": "take the environment offline resize the resources or the database and bring it",
    "start": "2272580",
    "end": "2278520"
  },
  {
    "text": "back up and it works pretty well with the operator in our case or the",
    "start": "2278520",
    "end": "2285060"
  },
  {
    "text": "orchestrator piece as I mentioned we have multiple availability zones and",
    "start": "2285060",
    "end": "2290250"
  },
  {
    "text": "each replicas in the schedule site of Orchestrator is spun off in its own",
    "start": "2290250",
    "end": "2296400"
  },
  {
    "text": "availability zone so we are really banking on the fact that failure one",
    "start": "2296400",
    "end": "2301830"
  },
  {
    "text": "availability zone does not impact either the database or the orchestrator",
    "start": "2301830",
    "end": "2307730"
  },
  {
    "text": "Orchestrator workflow we have had issues in the past where our networking went",
    "start": "2307730",
    "end": "2315690"
  },
  {
    "text": "down and as a result one of the issues was unavailable and we we were able to",
    "start": "2315690",
    "end": "2321690"
  },
  {
    "text": "successfully have the orchestrator revert the master replica master to a",
    "start": "2321690",
    "end": "2328020"
  },
  {
    "text": "replica of the database and this this depends on the time outs certain the",
    "start": "2328020",
    "end": "2334230"
  },
  {
    "text": "orchestration and the setting we currently have is about it waits for about three minutes before changing the",
    "start": "2334230",
    "end": "2342630"
  },
  {
    "text": "master of replicas so we do have had a glitch in availability but it is it is",
    "start": "2342630",
    "end": "2352140"
  },
  {
    "text": "acceptable for the workloads we are running with this database other",
    "start": "2352140",
    "end": "2361050"
  },
  {
    "text": "questions",
    "start": "2361050",
    "end": "2363619"
  },
  {
    "text": "it's like so this makes it easy to Horizonte scale",
    "start": "2370000",
    "end": "2378610"
  },
  {
    "text": "the cluster but how do you deal with vertical scaling yeah we don't do that",
    "start": "2378610",
    "end": "2389370"
  },
  {
    "text": "yeah you have to change specification and that will trigger a rolling upgrade",
    "start": "2389370",
    "end": "2395980"
  },
  {
    "text": "and update update if you change the",
    "start": "2395980",
    "end": "2401500"
  },
  {
    "text": "requirements limits and so on on pods you have accessed them Mike on my sequel",
    "start": "2401500",
    "end": "2406930"
  },
  {
    "text": "cluster CRD and you can modify them and that will trigger the deploy it's not",
    "start": "2406930",
    "end": "2413710"
  },
  {
    "text": "online yeah like a breeze other",
    "start": "2413710",
    "end": "2426460"
  },
  {
    "text": "questions",
    "start": "2426460",
    "end": "2429030"
  },
  {
    "text": "yeah we we hope yeah we hope in the next few months we in the new release we have",
    "start": "2435760",
    "end": "2445310"
  },
  {
    "text": "some same bugs that we have to handle and after that we want to integrate them",
    "start": "2445310",
    "end": "2451850"
  },
  {
    "text": "in the operator operator hub and see the",
    "start": "2451850",
    "end": "2457100"
  },
  {
    "text": "feedback from some users yeah that's it",
    "start": "2457100",
    "end": "2464720"
  },
  {
    "text": "I think we already used we you hope to to bring this to our clients in at the",
    "start": "2464720",
    "end": "2471530"
  },
  {
    "text": "end of this year so it has to be ready",
    "start": "2471530",
    "end": "2477860"
  },
  {
    "text": "because our clients depends on it so",
    "start": "2477860",
    "end": "2484220"
  },
  {
    "text": "thank you thank you for your time [Applause]",
    "start": "2484220",
    "end": "2490249"
  }
]