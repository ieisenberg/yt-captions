[
  {
    "text": "hi everybody I'm Sophie Watson and I'm here today with my colleague will Benton",
    "start": "380",
    "end": "6180"
  },
  {
    "text": "and we're going to talk to you about why data scientists love kubernetes so you",
    "start": "6180",
    "end": "12509"
  },
  {
    "text": "know everything there is to know about us since like all well-prepared conference audiences you read our BIOS next to the abstract so let's talk about",
    "start": "12509",
    "end": "19500"
  },
  {
    "text": "you I talk is titled why data scientist love kubernetes but we appreciate that",
    "start": "19500",
    "end": "24840"
  },
  {
    "text": "there's a few possible audiences for this talk so who here is a software developer and okay what about anybody",
    "start": "24840",
    "end": "33840"
  },
  {
    "text": "who manages infrastructure okay and does anyone here consider themselves a data",
    "start": "33840",
    "end": "40649"
  },
  {
    "text": "scientist or an analyst or a machine learning engineer less but still good",
    "start": "40649",
    "end": "47520"
  },
  {
    "text": "any other roles anyone want to tell me what they do okay",
    "start": "47520",
    "end": "53250"
  },
  {
    "text": "so hopefully we'll be able to tell everybody here something new we're going to explain what a data scientist",
    "start": "53250",
    "end": "58500"
  },
  {
    "text": "actually does and we hope you'll see how it resembles what you're doing with kubernetes right now if you're a",
    "start": "58500",
    "end": "64290"
  },
  {
    "text": "developer or an operator you learn how kubernetes can support collaboration with date two scientists and how it can",
    "start": "64290",
    "end": "70710"
  },
  {
    "text": "help them to incorporate machine learning into production within intelligent applications and those of",
    "start": "70710",
    "end": "77340"
  },
  {
    "text": "you who are dear to scientists you learn how Tsukuba Nettie's can support your work even if you don't want to dive deep",
    "start": "77340",
    "end": "83490"
  },
  {
    "text": "and become a cube expert so in order to do that we'll start by talking about a",
    "start": "83490",
    "end": "89820"
  },
  {
    "text": "data scientist workflow typically when you hear about a new application of",
    "start": "89820",
    "end": "95310"
  },
  {
    "text": "artificial intelligence for business it's arisen because a data scientist has bridged that gap between domain experts",
    "start": "95310",
    "end": "103470"
  },
  {
    "text": "and machine learning techniques will focus on how they do and what their workflow is for the business centric",
    "start": "103470",
    "end": "109799"
  },
  {
    "text": "example of customer churn from there we'll see the analogies between the",
    "start": "109799",
    "end": "116369"
  },
  {
    "text": "workflows we use to develop software in containers and the workflows that data",
    "start": "116369",
    "end": "121560"
  },
  {
    "text": "scientists use to develop models to make predictions from data you'll see how kubernetes is an autumn flip for data",
    "start": "121560",
    "end": "128340"
  },
  {
    "text": "scientists and we'll see that the things that you know about like continuous integration and continuous deployment can support a",
    "start": "128340",
    "end": "135100"
  },
  {
    "text": "data scientists work in the same way that they can support an application developers will also look at how qts",
    "start": "135100",
    "end": "143260"
  },
  {
    "text": "provides the foundation for truly reproducible research so having reproducible research is of",
    "start": "143260",
    "end": "149080"
  },
  {
    "text": "key importance in good science and in any regulated industry and Cuba nighties",
    "start": "149080",
    "end": "154270"
  },
  {
    "text": "really aids this to begin we're going to look at the day in a life of a data",
    "start": "154270",
    "end": "160630"
  },
  {
    "text": "scientist and we'll see how container workflows can be translated into something useful for them will is going",
    "start": "160630",
    "end": "167350"
  },
  {
    "text": "to talk about how data scientists can use kubernetes without becoming Yammer lovers or cube experts and he'll tell us",
    "start": "167350",
    "end": "174459"
  },
  {
    "text": "about some great communities that you and your data scientists should be aware of if you haven't already heard a lot",
    "start": "174459",
    "end": "179740"
  },
  {
    "text": "about them today some definitions of what a data scientist is very greatly",
    "start": "179740",
    "end": "186190"
  },
  {
    "text": "some people say that a data scientist job includes scalable computing their engineering software development machine",
    "start": "186190",
    "end": "193840"
  },
  {
    "text": "learning statistics and more other people think that a data scientist is",
    "start": "193840",
    "end": "199060"
  },
  {
    "text": "certainly someone who uses machine learning to solve problems instead of committing to a single definition of",
    "start": "199060",
    "end": "205840"
  },
  {
    "text": "what a scientist is we're going to talk about what data scientist does and we're going to do this in the context of one",
    "start": "205840",
    "end": "211989"
  },
  {
    "text": "specific example the example we're going to talk about is customer churn so say",
    "start": "211989",
    "end": "218170"
  },
  {
    "text": "we're a large enterprise with many customers who subscribe to our products this is a common business model in",
    "start": "218170",
    "end": "224970"
  },
  {
    "text": "telecommunications media and software among other industries within such",
    "start": "224970",
    "end": "231700"
  },
  {
    "text": "companies it's the data scientists one of the data scientists jobs to predict",
    "start": "231700",
    "end": "236860"
  },
  {
    "text": "which customers will not renew their contract so the data scientist also has to find a way to identify which",
    "start": "236860",
    "end": "243370"
  },
  {
    "text": "engagements are most likely to retain customers and find a way to prioritize customers whilst considering that impact",
    "start": "243370",
    "end": "250120"
  },
  {
    "text": "on the business and the past and trade-offs of any interventions so",
    "start": "250120",
    "end": "255880"
  },
  {
    "text": "hopefully we've got a good idea what customer churn is but in order to make progress with their task the data scientist has to create a",
    "start": "255880",
    "end": "262169"
  },
  {
    "text": "formal definition of the problem they want to solve for us this is going to be",
    "start": "262169",
    "end": "267449"
  },
  {
    "text": "given a customer's behavior over the last 18 months what is the likelihood that the customer leaves within the next",
    "start": "267449",
    "end": "274560"
  },
  {
    "text": "six as well as defining the problem they also want to quantify the amount of loss",
    "start": "274560",
    "end": "281330"
  },
  {
    "text": "associated with churn and tie back to underlying business metrics not all",
    "start": "281330",
    "end": "286680"
  },
  {
    "text": "customers are equal so for a mobile phone provider a customer with a fancy plan giving up six of their 10 lines may",
    "start": "286680",
    "end": "294060"
  },
  {
    "text": "or may not be worse than a customer with two lines giving up both of them similarly in software if you have a",
    "start": "294060",
    "end": "300539"
  },
  {
    "text": "customer who has 10,000 seats and they give up half of them that's arguably worse than a customer with 1,000 seats",
    "start": "300539",
    "end": "306780"
  },
  {
    "text": "giving up all of them so we can't just think about the impact of row numbers or",
    "start": "306780",
    "end": "312750"
  },
  {
    "text": "percentages we need to think about what's happening to the business so the",
    "start": "312750",
    "end": "318180"
  },
  {
    "text": "data science probably begins by trying to visualize the data by trying to find a way to map from the data we have about",
    "start": "318180",
    "end": "324750"
  },
  {
    "text": "customers to points in space we can try and find some structure so maybe we come across a visualization that structures",
    "start": "324750",
    "end": "331650"
  },
  {
    "text": "some structure like this one here so here we can see that the churning customers shown in reds cluster in that",
    "start": "331650",
    "end": "337950"
  },
  {
    "text": "top left-hand corner fortunately there's structure in this data and we found it but we need to find a way to capture it",
    "start": "337950",
    "end": "344700"
  },
  {
    "text": "and encored it in a nice useful way this aspect of a data scientists work is",
    "start": "344700",
    "end": "351090"
  },
  {
    "text": "known as feature engineering so we need to encode relevant data about the customer in a numeric vector and we'll",
    "start": "351090",
    "end": "357840"
  },
  {
    "text": "use that data to predict customer churn in practice choosing this numeric vector",
    "start": "357840",
    "end": "364199"
  },
  {
    "text": "is complicated we can use our investigation from the visualizations stage to help but it's not always",
    "start": "364199",
    "end": "370020"
  },
  {
    "text": "obvious how to translate the data into useful numeric vector and we want to",
    "start": "370020",
    "end": "375900"
  },
  {
    "text": "make sure that the structure we want to expose with our model is actually present in the vector itself furthermore",
    "start": "375900",
    "end": "383130"
  },
  {
    "text": "there's not really a 1 numeric summary fits all approach so for each different",
    "start": "383130",
    "end": "388440"
  },
  {
    "text": "business problem you likely need a different set of numbers simply cause of the day you have access to and",
    "start": "388440",
    "end": "395099"
  },
  {
    "text": "going forward at this point you probably already have to be thinking about which models or which algorithms you might",
    "start": "395099",
    "end": "400900"
  },
  {
    "text": "want to implement because different algorithms take in different feature vectors once we have our feature vectors",
    "start": "400900",
    "end": "408580"
  },
  {
    "text": "we can take some label data that we know about and use it to train a model so by label data",
    "start": "408580",
    "end": "414039"
  },
  {
    "text": "I mean data where we know the outcome so in the customer churn case this is gonna be a dataset covering two years of data",
    "start": "414039",
    "end": "420879"
  },
  {
    "text": "it'll contain some customers who turned some who did not and will know which is which",
    "start": "420879",
    "end": "426419"
  },
  {
    "text": "we take that data and we train a model train and a model is essentially running",
    "start": "426419",
    "end": "432069"
  },
  {
    "text": "an algorithm to identify good trade-offs while summarizing the dear we can control some aspects of the algorithm",
    "start": "432069",
    "end": "439210"
  },
  {
    "text": "through parameters but we're going to have to see which parameter values work best",
    "start": "439210",
    "end": "444870"
  },
  {
    "text": "once we've trained our model we've got our way to distinguish between customers who were likely to churn and those who",
    "start": "444870",
    "end": "450969"
  },
  {
    "text": "are not likely to so in general you can think I've trained in a model as",
    "start": "450969",
    "end": "456789"
  },
  {
    "text": "allowing the model alone that underlying structures and pattern in the data we",
    "start": "456789",
    "end": "462009"
  },
  {
    "text": "could get perfect performance on a training set simply by memorizing that training set but that wouldn't do us any",
    "start": "462009",
    "end": "468639"
  },
  {
    "text": "good in real life a model which simply memorizes the training set is descriptive but not predictive if it",
    "start": "468639",
    "end": "475870"
  },
  {
    "text": "doesn't learn those underlying patterns then it's not going to be able to make good predictions when it sees any new",
    "start": "475870",
    "end": "481120"
  },
  {
    "text": "data so in practice to prevent the model from spitting out what it's just seen",
    "start": "481120",
    "end": "486520"
  },
  {
    "text": "before we use the training and a validation method we take our data that we know the truth about and we hold some",
    "start": "486520",
    "end": "493029"
  },
  {
    "text": "back we train our model on the rest of the data and from there we can look at",
    "start": "493029",
    "end": "499509"
  },
  {
    "text": "how the model performs on those two data sets through some error metric so in",
    "start": "499509",
    "end": "505509"
  },
  {
    "text": "this case we have a curve and the area under the curve represents how well the",
    "start": "505509",
    "end": "510550"
  },
  {
    "text": "model is performing the bigger the area the better and that dotted line represents the results we'd get if we",
    "start": "510550",
    "end": "516849"
  },
  {
    "text": "simply used a point us to estimate whether or not someone is going to so we've got pretty good performance on",
    "start": "516849",
    "end": "523760"
  },
  {
    "text": "both subsets of Thor's data but you can see that the area under the curve at the top is better that's on the training",
    "start": "523760",
    "end": "529700"
  },
  {
    "text": "data so perhaps we've overfit our data a bit and we haven't truly generalized for",
    "start": "529700",
    "end": "535130"
  },
  {
    "text": "for it so we might want to go back to the drawing board at this point and try a bit harder once your model is built",
    "start": "535130",
    "end": "543260"
  },
  {
    "text": "tuned and trained you can go ahead and put it into production as a service running on kubernetes that's part of the",
    "start": "543260",
    "end": "549350"
  },
  {
    "text": "larger system or application but the data scientists can't wash the hands of it yet",
    "start": "549350",
    "end": "554420"
  },
  {
    "text": "progress of the model needs to be tracked over times often data changes the time and so that can make the model",
    "start": "554420",
    "end": "561470"
  },
  {
    "text": "poorly tuned perhaps you need to retune the parameters through something like a be testing we're also going to want to",
    "start": "561470",
    "end": "568970"
  },
  {
    "text": "start tracking new metrics that are related to business there's many cases where a model which performs perfectly",
    "start": "568970",
    "end": "575500"
  },
  {
    "text": "initially doesn't really solve your business problem so for example say we",
    "start": "575500",
    "end": "581150"
  },
  {
    "text": "put a great model into production and are able to recommend early customer",
    "start": "581150",
    "end": "588050"
  },
  {
    "text": "interventions to prevent any churn no customers turn now and that's great but",
    "start": "588050",
    "end": "593540"
  },
  {
    "text": "actually your sales team was so busy preventing customers from churning that they didn't sign any new deals this year",
    "start": "593540",
    "end": "598610"
  },
  {
    "text": "so you didn't meet your growth targets similarly he could get a nearly perfect",
    "start": "598610",
    "end": "604220"
  },
  {
    "text": "model by putting your top ten data scientists on the problem for the next few years but if they can't work on",
    "start": "604220",
    "end": "610610"
  },
  {
    "text": "anything else in the next few years is an early perfect model really that much better than an early nearly perfect",
    "start": "610610",
    "end": "616670"
  },
  {
    "text": "model we've got to think about things like this we've talked about what a data",
    "start": "616670",
    "end": "622730"
  },
  {
    "text": "scientist does but were they actually doing this work in practice most area scientists now start out by working in",
    "start": "622730",
    "end": "629480"
  },
  {
    "text": "interactive notebooks so this notebook is implementing a bloom filter you can",
    "start": "629480",
    "end": "635240"
  },
  {
    "text": "see that the notebook contains both text and code you can define functions run",
    "start": "635240",
    "end": "641840"
  },
  {
    "text": "them in line see the output immediately and you can go ahead and change functions and change things in there and",
    "start": "641840",
    "end": "648170"
  },
  {
    "text": "the output updates similarly you can plot graphs in there so those graphs",
    "start": "648170",
    "end": "654049"
  },
  {
    "text": "support in all of the main notebook providers notebook sorry I'm very",
    "start": "654049",
    "end": "659299"
  },
  {
    "text": "flexible but they have a couple of serious downsides that is Portability and reproducibility sharing a data",
    "start": "659299",
    "end": "667730"
  },
  {
    "text": "scientist notebook is like sharing a physical notebook there's good stuff in there but you've got to do some",
    "start": "667730",
    "end": "672980"
  },
  {
    "text": "archaeology to figure out where it is you're essentially getting an uncreated snapshot of someone else's desk and they",
    "start": "672980",
    "end": "681799"
  },
  {
    "text": "these notebooks often have dependencies and they're built in bespoke environments so sending a notebook to a",
    "start": "681799",
    "end": "687980"
  },
  {
    "text": "colleague often one of two things will happen either they will get different results or the notebook will just crash we'll",
    "start": "687980",
    "end": "696799"
  },
  {
    "text": "revisit this problem later so hope the analogies between some of the things",
    "start": "696799",
    "end": "702799"
  },
  {
    "text": "that a data scientist does and some of the things that are contemporary software developer does are starting to",
    "start": "702799",
    "end": "708439"
  },
  {
    "text": "become clear you may already be thinking of ways that kubernetes can improve the data scientist workflow but let's start",
    "start": "708439",
    "end": "715720"
  },
  {
    "text": "make some starting points and make these connections a little bit more concrete the first advantage brought by",
    "start": "715720",
    "end": "722839"
  },
  {
    "text": "kubernetes is a workflow advantage so say you do all of your work on your personal computer this is convenient but",
    "start": "722839",
    "end": "728989"
  },
  {
    "text": "it has some limitations you can't get access to sensitive data you're limited",
    "start": "728989",
    "end": "734629"
  },
  {
    "text": "by the speed of your computer CPU and your computer probably doesn't have as",
    "start": "734629",
    "end": "740149"
  },
  {
    "text": "much storage as you'd like it to so you can't hold all the data you want to work with at once",
    "start": "740149",
    "end": "746439"
  },
  {
    "text": "similarly model training is often very maths heavy and hardware acceleration",
    "start": "746499",
    "end": "751519"
  },
  {
    "text": "can really help but not every data scientist laptop has a GPU that machine",
    "start": "751519",
    "end": "756949"
  },
  {
    "text": "learning workloads can exploit this so both will and I work on Macs and their GPUs enable great multimedia performance",
    "start": "756949",
    "end": "763610"
  },
  {
    "text": "but they won't accelerate models ruining declarative deployment configurations",
    "start": "763610",
    "end": "769549"
  },
  {
    "text": "which specify which services are connected to which but not a particular way to set them up make it possible to",
    "start": "769549",
    "end": "775970"
  },
  {
    "text": "define the environment that you want to use in a portable way so you can take that environment that",
    "start": "775970",
    "end": "781449"
  },
  {
    "text": "you've got on your laptop and transparently move it somewhere else when you want to scale up or scale out this figure shows how we've described",
    "start": "781449",
    "end": "788290"
  },
  {
    "text": "the pods we won and the service interfaces they expose on a laptop but we can take this and rerun it anywhere",
    "start": "788290",
    "end": "794259"
  },
  {
    "text": "so we can start prototyping a technique anywhere in the world and then reproduce it inside we produce that environment",
    "start": "794259",
    "end": "801220"
  },
  {
    "text": "inside your company's firewall where you can scale out with more CPUs up with more GPUs and you can access that data",
    "start": "801220",
    "end": "808870"
  },
  {
    "text": "that you probably shouldn't keep on your personal computer another advantage of",
    "start": "808870",
    "end": "814509"
  },
  {
    "text": "using repeatable deployments in kubernetes is scale up processing for model training sometimes the technique",
    "start": "814509",
    "end": "822399"
  },
  {
    "text": "itself scales out so there's frameworks like Apache spark and tensor flow which support training models across multiple",
    "start": "822399",
    "end": "829120"
  },
  {
    "text": "cores by distributing the training job and even in cases where a single training job doesn't scale out we can",
    "start": "829120",
    "end": "835389"
  },
  {
    "text": "still benefit from parallelism so when we train the model we might want to do like the parameter sweeping or so on and",
    "start": "835389",
    "end": "841959"
  },
  {
    "text": "we can scale these experiments out by using a templated declarative",
    "start": "841959",
    "end": "846999"
  },
  {
    "text": "configuration to run all of our training and evaluation jobs in kubernetes and then we can collect the results as",
    "start": "846999",
    "end": "852910"
  },
  {
    "text": "metrics in a single place a third",
    "start": "852910",
    "end": "858249"
  },
  {
    "text": "advantage is service written for experiment design so kubernetes allows",
    "start": "858249",
    "end": "863829"
  },
  {
    "text": "you to route traffic to two or more different versions of the same service you might have training and scoring",
    "start": "863829",
    "end": "869829"
  },
  {
    "text": "pipelines that run continuously as part of your application or you might have two different ways to model the problem",
    "start": "869829",
    "end": "876069"
  },
  {
    "text": "that you care about but prior to implementing the models you don't know which is better in the real world being",
    "start": "876069",
    "end": "882670"
  },
  {
    "text": "able to run multiple instances in parallel and monitor them in parallel allows these models to be updated and",
    "start": "882670",
    "end": "888910"
  },
  {
    "text": "evaluated in real time it also gives you a lovely failsafe",
    "start": "888910",
    "end": "894069"
  },
  {
    "text": "so accidentally putting a model into production which has problems that you don't know about until you put it into",
    "start": "894069",
    "end": "900939"
  },
  {
    "text": "production is tragically common of course this isn't a real problem in real",
    "start": "900939",
    "end": "906610"
  },
  {
    "text": "software development right so in software development we use Green deployment to roll out any new",
    "start": "906610",
    "end": "913350"
  },
  {
    "text": "changes and similarly we could do that here if we have a new model or a service we want to deploy then we can use Cuban",
    "start": "913350",
    "end": "920970"
  },
  {
    "text": "IPS and route only say one percent of the traffic to the new model and increase that as we're going once the",
    "start": "920970",
    "end": "926820"
  },
  {
    "text": "northern model is working well we talked earlier about the reproducible notebook",
    "start": "926820",
    "end": "932820"
  },
  {
    "text": "problem many data scientists still don't use a lot of software engineering practices like source control or",
    "start": "932820",
    "end": "939030"
  },
  {
    "text": "continuous deployment but this is starting to change if your data scientist is running their notebooks and",
    "start": "939030",
    "end": "944520"
  },
  {
    "text": "gates you can use cuban at ease to go from a notebook to a notebook that we",
    "start": "944520",
    "end": "949590"
  },
  {
    "text": "know runs with all of its dependencies and passes any relevant tests if it passes those tests we can set it up to",
    "start": "949590",
    "end": "957060"
  },
  {
    "text": "automatically build a notebook image that can then be deployed anywhere or even be deployed straight into",
    "start": "957060",
    "end": "962940"
  },
  {
    "text": "production finally when we train in a model we need a repeatable way to go",
    "start": "962940",
    "end": "969240"
  },
  {
    "text": "from raw data to clean data and feature vectors to our trained model which we can then ultimately evaluate and deploy",
    "start": "969240",
    "end": "976800"
  },
  {
    "text": "into production these sorts of pipelines have the same structure and requirements",
    "start": "976800",
    "end": "982080"
  },
  {
    "text": "as the pipelines we use to build repeatable software artifacts we draw on",
    "start": "982080",
    "end": "987270"
  },
  {
    "text": "code and external libraries and we run jobs to build images and run separate audited jobs to test and deploy these",
    "start": "987270",
    "end": "994110"
  },
  {
    "text": "whilst we're tracking the results as we're going so the same orchestration capabilities that make kubernetes great",
    "start": "994110",
    "end": "1000560"
  },
  {
    "text": "for repeatable software builds make it great for repeatable data pipelines I hope it's becoming clearer how the",
    "start": "1000560",
    "end": "1007670"
  },
  {
    "text": "data science workflow can benefit from many of these same kubernetes features that enable our contemporary software",
    "start": "1007670",
    "end": "1015440"
  },
  {
    "text": "engineering workflow but I'm going to pass over to will who's going to introduce some concrete and high-level tools to make it easy for data",
    "start": "1015440",
    "end": "1022430"
  },
  {
    "text": "scientists to benefit from this power thanks Rafi so I hope you're convinced",
    "start": "1022430",
    "end": "1028970"
  },
  {
    "text": "that containers can work for data science workflows in the same way that containers are great for a developer workflows and you might have some ideas",
    "start": "1028970",
    "end": "1035689"
  },
  {
    "text": "for concrete problems so you can apply them to but in the rest of the talk I'm going to show you a couple of concrete ways that data scientists can take",
    "start": "1035690",
    "end": "1042319"
  },
  {
    "text": "advantage of this stuff right now and tell you we're good more so a lot of people have been",
    "start": "1042320",
    "end": "1048050"
  },
  {
    "text": "excited about containers for data science for a long time but a lot of the material you can find if you search like how can I use",
    "start": "1048050",
    "end": "1054710"
  },
  {
    "text": "containers for data science starts with the assumption that the only thing missing from a data scientists life is",
    "start": "1054710",
    "end": "1060230"
  },
  {
    "text": "becoming a packaging expert so I work for a company that makes money packaging",
    "start": "1060230",
    "end": "1065840"
  },
  {
    "text": "open-source software I'm a sponsor on the Fedora project and I don't want to be a packaging expert almost all of the",
    "start": "1065840",
    "end": "1072200"
  },
  {
    "text": "time right so you go into one of these and things and since I hate to get started using containers for data science you need to go through all of",
    "start": "1072200",
    "end": "1078740"
  },
  {
    "text": "these command line invitations you need to go through all this metadata and here are three pages of a mole that will get",
    "start": "1078740",
    "end": "1084110"
  },
  {
    "text": "you going and those details are great for people who want to dive deep into infrastructure and a lot of data scientists are interested in really",
    "start": "1084110",
    "end": "1090440"
  },
  {
    "text": "understanding how the infrastructure works but not everyone does and I think we can have a better story from the",
    "start": "1090440",
    "end": "1095780"
  },
  {
    "text": "kubernetes community for it at a scientist if we start with thinking about high level tools that make their",
    "start": "1095780",
    "end": "1100910"
  },
  {
    "text": "lives easier and then sort of using that as motivation to learn more about sort of the pages of vml and command-line",
    "start": "1100910",
    "end": "1107570"
  },
  {
    "text": "invitations so let's talk about a totally easy way to get started on a problem so if you mentioned which is",
    "start": "1107570",
    "end": "1113450"
  },
  {
    "text": "that reproducible notebook problem remember that you want to have a notebook that's packaged up with all of",
    "start": "1113450",
    "end": "1118460"
  },
  {
    "text": "its dependencies that you're sure that it you someone else will get the same result that you did my binder org is a",
    "start": "1118460",
    "end": "1124340"
  },
  {
    "text": "hosted service sponsored by project Jupiter you don't need to do anything to use this other than give it a URL to a",
    "start": "1124340",
    "end": "1132740"
  },
  {
    "text": "git repository you can also run it on Prem let's see how it works right now so",
    "start": "1132740",
    "end": "1137960"
  },
  {
    "text": "I'm going to go to a git repository I have that has a Jupiter notebook in it and you can see this is just a regular",
    "start": "1137960",
    "end": "1144650"
  },
  {
    "text": "git repository with a jupiter notebook and our Python requirements file I'm going to copy the URL of that and pass",
    "start": "1144650",
    "end": "1152120"
  },
  {
    "text": "it into binder what binder is gonna do is it's gonna build a container and serve that in a kubernetes cluster with",
    "start": "1152120",
    "end": "1158900"
  },
  {
    "text": "a route published to the world so that I can interact with this and anyone else can get their own copy of this spun up",
    "start": "1158900",
    "end": "1165650"
  },
  {
    "text": "and run it run and we can see I click on my notebook I'm gonna run it here this is a recorded",
    "start": "1165650",
    "end": "1171500"
  },
  {
    "text": "demo so this is gonna obviously be a reproducible notebook because we recorded it at once and play it back as",
    "start": "1171500",
    "end": "1176900"
  },
  {
    "text": "many times as you want to see it but but you could go into this notebook too and get the same result right and we're just",
    "start": "1176900",
    "end": "1183019"
  },
  {
    "text": "going to go through some examples there and that's that's great binder is a really great service I think",
    "start": "1183019",
    "end": "1189589"
  },
  {
    "text": "that a really powerful thing we can do to sort of go beyond individual services though is the idea of source to image",
    "start": "1189589",
    "end": "1195079"
  },
  {
    "text": "workflows so you've probably how many people in here views at a high level source to image builder tool to make",
    "start": "1195079",
    "end": "1200869"
  },
  {
    "text": "applications so any people use s to I draft something else yeah so I mean a",
    "start": "1200869",
    "end": "1210469"
  },
  {
    "text": "lot of people have used these tools but they're but they're more general than just doing applications right so I'll talk about the s2i tool these principles",
    "start": "1210469",
    "end": "1217609"
  },
  {
    "text": "are really more general I don't care what tool you use s2i is just the one that I know but the idea is that we have",
    "start": "1217609",
    "end": "1223609"
  },
  {
    "text": "a builder image that knows how to turn a git repository of something source code",
    "start": "1223609",
    "end": "1229249"
  },
  {
    "text": "or whatever may be a model object may be a notebook into a container image that we can run as a service on kubernetes in",
    "start": "1229249",
    "end": "1235609"
  },
  {
    "text": "this example I'm going to use a builder image that my colleague Graham developed and this is something that knows how to take a repository of notebooks and",
    "start": "1235609",
    "end": "1241969"
  },
  {
    "text": "turned it into a notebook server container image so I have a way to sort of build an image that works exactly",
    "start": "1241969",
    "end": "1247099"
  },
  {
    "text": "like that binder service that we saw earlier and by using this I can give this Builder image to my data scientists",
    "start": "1247099",
    "end": "1254119"
  },
  {
    "text": "and say this is how you package up your notebooks in a container without becoming a packaging expert let's see",
    "start": "1254119",
    "end": "1259549"
  },
  {
    "text": "that in action so you can use the s2i command-line tool to build images no matter what kubernetes distribution",
    "start": "1259549",
    "end": "1265729"
  },
  {
    "text": "you're using it's just a tool that will build an image from the Builder image we tell it the Builder image and the git",
    "start": "1265729",
    "end": "1271579"
  },
  {
    "text": "repo the advantage there is that you can push that image to a container registry to skip a step in this demo we're just",
    "start": "1271579",
    "end": "1277190"
  },
  {
    "text": "going to use a build strategy here with openshift to sort of see how we can publish this notebook we're specifying",
    "start": "1277190",
    "end": "1284629"
  },
  {
    "text": "an environment there we'll watch the logs here now and we can see that we're gonna clone that git repository and",
    "start": "1284629",
    "end": "1291289"
  },
  {
    "text": "because we want to have a reproducible environment we're gonna start by downloading most of the universe so we can actually run this Jupiter notebook",
    "start": "1291289",
    "end": "1298190"
  },
  {
    "text": "somewhere else as this goes let's think a little bit about why having a sort of",
    "start": "1298190",
    "end": "1304789"
  },
  {
    "text": "framework this sort of source to image workflow having a framework to build services is really flexible I mean there",
    "start": "1304789",
    "end": "1310039"
  },
  {
    "text": "are a lot of things I can do to custom this without becoming a packaging expert I mean how many how many of you have",
    "start": "1310039",
    "end": "1315220"
  },
  {
    "text": "taken a container recipe file and modified it without totally understanding what it does even if",
    "start": "1315220",
    "end": "1320500"
  },
  {
    "text": "you're a packaging expert you do this right so here we have a we have this service running now I'm gonna log in and",
    "start": "1320500",
    "end": "1328059"
  },
  {
    "text": "we see that we get basically the same results we did on binder but I also could have pushed this image to a registry or done whatever I wanted with",
    "start": "1328059",
    "end": "1334960"
  },
  {
    "text": "it right so that's uh I think that's a really powerful thing you can do with",
    "start": "1334960",
    "end": "1341919"
  },
  {
    "text": "source to image workflows but they're really more general than that any task that involves generating a container",
    "start": "1341919",
    "end": "1348520"
  },
  {
    "text": "image from source files can be implemented as a source image builder so one of the calls to action I have for",
    "start": "1348520",
    "end": "1354010"
  },
  {
    "text": "you from this talk is to be creative about what kinds of things you could put in a source repository and what kinds of",
    "start": "1354010",
    "end": "1359649"
  },
  {
    "text": "things you could do to those to turn them into kubernetes services the Selden",
    "start": "1359649",
    "end": "1364690"
  },
  {
    "text": "project has a source to image builder that turns a serialized Python object into a model service we can actually go",
    "start": "1364690",
    "end": "1373059"
  },
  {
    "text": "further than that and say if I have a notebook that knows how to train a model I could turn that into a model service",
    "start": "1373059",
    "end": "1380020"
  },
  {
    "text": "as well so we'll see how a builder would do something more involved like that I'm going to show you a source to image",
    "start": "1380020",
    "end": "1386260"
  },
  {
    "text": "builder that operates on a notebook that trains a model the kind of model isn't important it's it's general enough to",
    "start": "1386260",
    "end": "1392049"
  },
  {
    "text": "work with just about anything but what it's going to do is it's going to load that get repository containing the notebook that knows how to train the",
    "start": "1392049",
    "end": "1397510"
  },
  {
    "text": "model it's gonna post process the notebook to identify its dependencies post process the notebook to extract a",
    "start": "1397510",
    "end": "1404860"
  },
  {
    "text": "Python script to train the model run that script in the environment that the",
    "start": "1404860",
    "end": "1410440"
  },
  {
    "text": "notebook expects and save that model and then create an image that has the environment the notebook expects and",
    "start": "1410440",
    "end": "1417059"
  },
  {
    "text": "creates a rest endpoint to serve that model so I hope you wrote that all down we're gonna go through it now and and",
    "start": "1417059",
    "end": "1423490"
  },
  {
    "text": "see what it does so again I'm gonna create create an image using using it a",
    "start": "1423490",
    "end": "1431260"
  },
  {
    "text": "build strategy and OpenShift this is basically just firing off a source to image build saying use this builder",
    "start": "1431260",
    "end": "1437559"
  },
  {
    "text": "image that I have on quay with this repository while we run through the logs while this build runs let's see what the",
    "start": "1437559",
    "end": "1444010"
  },
  {
    "text": "actual notebook like to see the conventions that this builder is expecting this is a example model s2i notebook you can visit that on",
    "start": "1444010",
    "end": "1450840"
  },
  {
    "text": "github the first thing is we specify the Python requirements the library requirements in a distinguished variable",
    "start": "1450840",
    "end": "1456420"
  },
  {
    "text": "so that the tool that post processes this notebook can identify that then we're training the model this is just a",
    "start": "1456420",
    "end": "1461880"
  },
  {
    "text": "k-means clustering model so it's going to map from a two-dimensional vector in this case to a small integer don't need",
    "start": "1461880",
    "end": "1468000"
  },
  {
    "text": "to worry about what it's doing and that's just what we should expect and then I have two functions one called validate which tells me that I've given",
    "start": "1468000",
    "end": "1474840"
  },
  {
    "text": "this model the right kind of input in this case a two-dimensional vector and one called predict that actually makes",
    "start": "1474840",
    "end": "1479850"
  },
  {
    "text": "the prediction so we've looked at the notebook now the build is done over here and I'm gonna expose a route to that and",
    "start": "1479850",
    "end": "1487230"
  },
  {
    "text": "then I'm gonna go to the insomnia rest client and start interacting with this notebook we can see how I give it a",
    "start": "1487230",
    "end": "1492900"
  },
  {
    "text": "prediction as a JSON array we're just passing in that vector and I'll get a",
    "start": "1492900",
    "end": "1498720"
  },
  {
    "text": "result back if I give it a different vector I get a different result now what",
    "start": "1498720",
    "end": "1506130"
  },
  {
    "text": "should happen if I give it a three dimensional vector well let's see mm-hmm",
    "start": "1506130",
    "end": "1513500"
  },
  {
    "text": "so I get an error a descriptive error so this is a way that you can use these workflows not only to publish work and",
    "start": "1513590",
    "end": "1519930"
  },
  {
    "text": "to enable collaboration but also to sort of enforce the kinds of practices you want to have in these services anyway if",
    "start": "1519930",
    "end": "1526350"
  },
  {
    "text": "someone's rolling their own service it's up to them if you build a tool to build a service from something higher-level you can really impose policies on this",
    "start": "1526350",
    "end": "1533610"
  },
  {
    "text": "in any way so frameworks like source to image make it easy for developers to",
    "start": "1533610",
    "end": "1538710"
  },
  {
    "text": "publish applications kubernetes but as we've just seen they're not just for applications they also let infrastructure experts make kubernetes",
    "start": "1538710",
    "end": "1544710"
  },
  {
    "text": "accessible to people who are domain experts but not infrastructure experts but all the techniques we've seen so far",
    "start": "1544710",
    "end": "1551370"
  },
  {
    "text": "have focused on developing analytic techniques on that sort of first part of the data science workflow you know when",
    "start": "1551370",
    "end": "1557310"
  },
  {
    "text": "Sophie said they can't wash their hands of it just yet the point before you would want to wash your hands anyway",
    "start": "1557310",
    "end": "1562860"
  },
  {
    "text": "right to talk about the concerns that you have actually taking this into production we're talk a little bit about",
    "start": "1562860",
    "end": "1568530"
  },
  {
    "text": "what it means to put machine learning into production today machine learning isn't always just a separate workload",
    "start": "1568530",
    "end": "1574590"
  },
  {
    "text": "that we run as a batch job you know once per quarter or once per night use the results to inform our business today we often put machine learning into",
    "start": "1574590",
    "end": "1581260"
  },
  {
    "text": "production as a component of intelligent applications which learned from data to provide improved functionality as people",
    "start": "1581260",
    "end": "1587470"
  },
  {
    "text": "continue to use them so these applications need to collect data from various sources and transform it federated in order to continuously train",
    "start": "1587470",
    "end": "1594670"
  },
  {
    "text": "and serve models to support key application functionality the machine learning is not on the side it's in the",
    "start": "1594670",
    "end": "1600250"
  },
  {
    "text": "heart of these applications and these data scientists aren't going to work in a vacuum they're gonna work in a team with data engineers who are gonna",
    "start": "1600250",
    "end": "1607060"
  },
  {
    "text": "publish those data services that they're going to use the data scientists are going to interact with those services",
    "start": "1607060",
    "end": "1612550"
  },
  {
    "text": "and do their prototyping and identify patterns and develop techniques to train models and finally application",
    "start": "1612550",
    "end": "1619510"
  },
  {
    "text": "developers are gonna worry about the second 90% of the problem after we have the first 90% they're gonna take all of",
    "start": "1619510",
    "end": "1624910"
  },
  {
    "text": "this and put it into production they're gonna make it scalable they're gonna make it accessible and they're gonna make it robust some teams even have",
    "start": "1624910",
    "end": "1632170"
  },
  {
    "text": "machine learning engineers who work with application developers and whose job it is to manage these machine learning",
    "start": "1632170",
    "end": "1637270"
  },
  {
    "text": "pipelines at scale whether we're scaling out with more cores or scaling up with",
    "start": "1637270",
    "end": "1643660"
  },
  {
    "text": "specialized hardware and machine learning engineers often also develop tools to make it easier to manage models in production so in the past people have",
    "start": "1643660",
    "end": "1651580"
  },
  {
    "text": "thought hey I've applications that depend on machine learning I need to figure out some way to coordinate that with my compute cluster and this has led",
    "start": "1651580",
    "end": "1659140"
  },
  {
    "text": "to a lot of clever workarounds because if you have a cluster that's really great for just training machine learning models it's probably not what you want",
    "start": "1659140",
    "end": "1665560"
  },
  {
    "text": "to develop your and deploy your applications against and since we're at coop con I I have a pretty good idea of",
    "start": "1665560",
    "end": "1671470"
  },
  {
    "text": "what people actually want to be developing and deploying their applications against yeah fortunately",
    "start": "1671470",
    "end": "1676720"
  },
  {
    "text": "kubernetes as we've seen a lot is flexible enough to say I have the primitive you know flight communities",
    "start": "1676720",
    "end": "1681940"
  },
  {
    "text": "has the primitives to support both application components and serious compute and data intensive machine",
    "start": "1681940",
    "end": "1688360"
  },
  {
    "text": "learning pipelines and it makes it thus simpler to manage and deploy intelligent applications we can actually take the",
    "start": "1688360",
    "end": "1695650"
  },
  {
    "text": "kinds of workflows we've talked about and make it even simpler the same pattern that we've seen making data",
    "start": "1695650",
    "end": "1701200"
  },
  {
    "text": "science development work easier can also make putting these things into production easier a source to image",
    "start": "1701200",
    "end": "1706390"
  },
  {
    "text": "builder can take the source code for an application and produce not just an image deployment configuration for a scale-out",
    "start": "1706390",
    "end": "1713270"
  },
  {
    "text": "compute cluster that runs in the same kubernetes namespace as the application that depends on it so we're gonna close",
    "start": "1713270",
    "end": "1719809"
  },
  {
    "text": "by talking about two community efforts behind projects you can use in production and the first one has actually a sourced image builder that",
    "start": "1719809",
    "end": "1726230"
  },
  {
    "text": "does just that read analytics Co is a community we sponsored at Red Hat for the last two and a half years or so it's",
    "start": "1726230",
    "end": "1733490"
  },
  {
    "text": "targeted at application developers and provides tooling and a downstream distribution to make it easy to build intelligent applications on kubernetes",
    "start": "1733490",
    "end": "1740480"
  },
  {
    "text": "from prototype to production there's a containerized Apache spark distribution for skaila data processing and compute a",
    "start": "1740480",
    "end": "1746860"
  },
  {
    "text": "spark operator and a spark management interface and Jupiter notebook images",
    "start": "1746860",
    "end": "1751940"
  },
  {
    "text": "and source damage builders for Jupiter we have tensor flow training and surveying as well but as I mentioned a",
    "start": "1751940",
    "end": "1758240"
  },
  {
    "text": "really cool thing that read analytics has is a source to image builder that takes an application as a git repository",
    "start": "1758240",
    "end": "1763760"
  },
  {
    "text": "and turns it into a deployment of that application and the cluster in in",
    "start": "1763760",
    "end": "1770090"
  },
  {
    "text": "kubernetes the second community I want to tell you about is coop flow if you've been in",
    "start": "1770090",
    "end": "1775250"
  },
  {
    "text": "this room all day you've already heard a lot about coop flow if you didn't see David and Jays talk immediately before this one catch the video there have been",
    "start": "1775250",
    "end": "1782570"
  },
  {
    "text": "a lot of great talks about coop flow at coop con so far I will encourage you to watch the videos um if you want the",
    "start": "1782570",
    "end": "1789110"
  },
  {
    "text": "quick overview coop flow is is really I think targeted at machine learning engineers and provides a wide range of",
    "start": "1789110",
    "end": "1794510"
  },
  {
    "text": "libraries and tools for making these workloads and making this development part of the process really portable",
    "start": "1794510",
    "end": "1799690"
  },
  {
    "text": "tensorflow jupiter hub Seldon and pi torch are some of the things that coop flow has made it",
    "start": "1799690",
    "end": "1805610"
  },
  {
    "text": "really easy to deploy on kubernetes and a real strength is that it makes it easy",
    "start": "1805610",
    "end": "1811429"
  },
  {
    "text": "to stand up these reproducible environments for machine learning workloads in in any kubernetes environment so again I hope you've",
    "start": "1811429",
    "end": "1818659"
  },
  {
    "text": "caught those talks and and we'll catch that ketchup on the ones you've missed I'll call out my friend Pete MacKinnon's",
    "start": "1818659",
    "end": "1824299"
  },
  {
    "text": "talk on the coop flow community tomorrow at 2:35 but I'd say if you see anything on the schedule that has coop flow in it it's",
    "start": "1824299",
    "end": "1830120"
  },
  {
    "text": "going to be worth watching so let's close just sort of by talking about what",
    "start": "1830120",
    "end": "1835340"
  },
  {
    "text": "we've covered today and and where we're going from here so if he introduced the data science workflow a pattern of",
    "start": "1835340",
    "end": "1840710"
  },
  {
    "text": "techniques to identify and exploit patterns and we saw analogies to contemporary",
    "start": "1840710",
    "end": "1845850"
  },
  {
    "text": "software development workflows and we hope you're already thinking about how you can help data scientists or if",
    "start": "1845850",
    "end": "1850860"
  },
  {
    "text": "you're a data scientist how you can use your infrastructure more effectively to to do this workflow we also saw some of",
    "start": "1850860",
    "end": "1859800"
  },
  {
    "text": "the advantages of kubernetes for data science portable environments via declarative deployments reproducible research scaling out application and",
    "start": "1859800",
    "end": "1867090"
  },
  {
    "text": "training components elastically and template and data processing pipelines since data scientists shouldn't have to",
    "start": "1867090",
    "end": "1873720"
  },
  {
    "text": "become infrastructure experts to get their jobs done just like no one should have to become an infrastructure expert",
    "start": "1873720",
    "end": "1878730"
  },
  {
    "text": "to get their job done unless they're an infrastructure expert we saw some high",
    "start": "1878730",
    "end": "1883830"
  },
  {
    "text": "level tools based on image building pipelines to facilitate making notebooks reproducible publishing models from",
    "start": "1883830",
    "end": "1889320"
  },
  {
    "text": "notebooks and putting machine intelligence into production and I hope you have some ideas for building tools",
    "start": "1889320",
    "end": "1894480"
  },
  {
    "text": "like this using your kubernetes expertise to make it accessible to data scientists as well after seeing the",
    "start": "1894480",
    "end": "1900690"
  },
  {
    "text": "advantages of kubernetes for data science development we looked at two communities that make it easier to put machine learning and intelligent",
    "start": "1900690",
    "end": "1906810"
  },
  {
    "text": "applications into production we looked at the read analytics community and at the coop flow community so with that",
    "start": "1906810",
    "end": "1913470"
  },
  {
    "text": "here's how you can get in touch with us you know we'd love to hear about your ideas for taking kubernetes and using it",
    "start": "1913470",
    "end": "1919500"
  },
  {
    "text": "to for machine learning and data science those are our email addresses and both github and twitter handles for both of",
    "start": "1919500",
    "end": "1926370"
  },
  {
    "text": "us and that's the link for the read analytics community that we mentioned earlier",
    "start": "1926370",
    "end": "1932420"
  },
  {
    "text": "[Applause] [Music]",
    "start": "1933390",
    "end": "1938840"
  },
  {
    "text": "[Applause]",
    "start": "1938840",
    "end": "1943140"
  },
  {
    "text": "yes",
    "start": "1943930",
    "end": "1946930"
  },
  {
    "text": "so the question is have we seen friction you know as a sort of data science team at a company that does a lot of stuff",
    "start": "1963400",
    "end": "1970220"
  },
  {
    "text": "with containers you know just for context in using containers for the whole data science workflow even though",
    "start": "1970220",
    "end": "1977780"
  },
  {
    "text": "that leads to sort of a longer build cycle and it's not as immediate as just sort of setting up your notebook and",
    "start": "1977780",
    "end": "1983090"
  },
  {
    "text": "going from there on you know we've both sort of done things the easy way right",
    "start": "1983090",
    "end": "1988160"
  },
  {
    "text": "at times and it's I think a real advantage for me is always the",
    "start": "1988160",
    "end": "1993590"
  },
  {
    "text": "reproducibility even Portability and I think once someone has been bitten by that you know of having a notebook that",
    "start": "1993590",
    "end": "1999770"
  },
  {
    "text": "you then have to take extra steps to make it reproducible when you put it in a container it's much easier to sell",
    "start": "1999770",
    "end": "2005880"
  },
  {
    "text": "start in the container from experience there but it's a reasonable point for",
    "start": "2005880",
    "end": "2013870"
  },
  {
    "text": "sure",
    "start": "2013870",
    "end": "2016020"
  },
  {
    "text": "so the so the question is if if if a notebook depends on SPARC how do we spin",
    "start": "2029590",
    "end": "2035270"
  },
  {
    "text": "it up and spin it out so in notebooks is we should we should talk send me an email we should talk about this offline because notebooks and SPARC is actually",
    "start": "2035270",
    "end": "2041540"
  },
  {
    "text": "an interesting special case the application source to image builder that I mentioned will spin up the spark",
    "start": "2041540",
    "end": "2048050"
  },
  {
    "text": "cluster and when the application terminates it will tear down the spark cluster because a notebook doesn't have",
    "start": "2048050",
    "end": "2053510"
  },
  {
    "text": "a natural teardown point you could think of a notebook as an application but you",
    "start": "2053510",
    "end": "2058550"
  },
  {
    "text": "know I'll refer you to Eric Aronson actually who's a spark at notebook expert here yeah absolutely",
    "start": "2058550",
    "end": "2069409"
  },
  {
    "text": "absolutely we'll be at Red Hat comm and",
    "start": "2069410",
    "end": "2074450"
  },
  {
    "text": "so if he had Red Hat doc",
    "start": "2074450",
    "end": "2077619"
  },
  {
    "text": "it's not like secret so for example if I got",
    "start": "2085410",
    "end": "2091050"
  },
  {
    "text": "right so so the workflow we would use is",
    "start": "2102960",
    "end": "2108630"
  },
  {
    "text": "is sort of leaving that as a whole in the published image and then when you deploy it you you mount the secrets you",
    "start": "2108630",
    "end": "2114690"
  },
  {
    "text": "know I mean that's I'd sorry the question was how do we manage secrets",
    "start": "2114690",
    "end": "2120089"
  },
  {
    "text": "and so on in in in a source to image pipeline yes",
    "start": "2120089",
    "end": "2129290"
  },
  {
    "text": "yeah okay so I'm not you know I'm not",
    "start": "2134660",
    "end": "2141270"
  },
  {
    "text": "gonna repeat the whole question but the question is that with to get started with Kubb flow you have to use interact with case on it which which not everyone",
    "start": "2141270",
    "end": "2147960"
  },
  {
    "text": "is yeah so so that is I think I think",
    "start": "2147960",
    "end": "2153030"
  },
  {
    "text": "the thing is David and Jays talk that was immediately before this that I that I've recommended already the coop flow",
    "start": "2153030",
    "end": "2159420"
  },
  {
    "text": "community has done a lot to make it easier to use so that you actually can spin up coop flow without interacting",
    "start": "2159420",
    "end": "2165090"
  },
  {
    "text": "with case on it so if you're if you if you like case on it it's there if you don't like kiss on it you can still use",
    "start": "2165090",
    "end": "2171240"
  },
  {
    "text": "coop flow that first demo showed how you can spin up keep flow just at the click of a button rather than getting involved",
    "start": "2171240",
    "end": "2178350"
  },
  {
    "text": "so",
    "start": "2178350",
    "end": "2180650"
  },
  {
    "text": "thanks so much everyone for joining us for this late afternoon session enjoy the rest of the conferences",
    "start": "2185420",
    "end": "2191860"
  },
  {
    "text": "[Applause]",
    "start": "2191860",
    "end": "2195919"
  }
]