[
  {
    "start": "0",
    "end": "89000"
  },
  {
    "text": "hello everyone thanks for coming my name is Pablo I'm one of the founders",
    "start": "179",
    "end": "5880"
  },
  {
    "text": "and the CDO adotedo is a development platform to deploy remote development",
    "start": "5880",
    "end": "11400"
  },
  {
    "text": "environments and kubernetes and we will talk about that a little bit later",
    "start": "11400",
    "end": "17940"
  },
  {
    "text": "and hi I'm vinay I work for future Bay Technologies it's the research armor for parent company Huawei and I've been",
    "start": "17940",
    "end": "25320"
  },
  {
    "text": "working on kubernetes my interests are in kubernetes network and compute and lately ebpf",
    "start": "25320",
    "end": "30480"
  },
  {
    "text": "and we hope to have a good talk with you today thank you cool so this is the agenda of the talk",
    "start": "30480",
    "end": "38399"
  },
  {
    "text": "um we are going to introduce the idea of cloud native development environments uh the problems that they solve and also",
    "start": "38399",
    "end": "45719"
  },
  {
    "text": "some of their challenges one of them is to make them cost effective",
    "start": "45719",
    "end": "51079"
  },
  {
    "text": "so we will analyze the different challenges there and then we will introduce a feature coming soon it's in",
    "start": "51079",
    "end": "57360"
  },
  {
    "text": "place but resize and the idea with this feature is that you can modify that request and the limits of a running Pod",
    "start": "57360",
    "end": "64320"
  },
  {
    "text": "without restarting the bottom uh after that we will see a demo using in place",
    "start": "64320",
    "end": "71460"
  },
  {
    "text": "battery size and evpf to optimize the infrastructure",
    "start": "71460",
    "end": "76580"
  },
  {
    "text": "utilization of cloud native development environments and we will finalize with",
    "start": "76580",
    "end": "82020"
  },
  {
    "text": "some takeaways okay so let's talk about Cloud native development environments and first let's",
    "start": "82020",
    "end": "89280"
  },
  {
    "start": "89000",
    "end": "89000"
  },
  {
    "text": "see the current state of the Earth so most companies are moving to",
    "start": "89280",
    "end": "95040"
  },
  {
    "text": "kubernetes and microservices for the library",
    "start": "95040",
    "end": "100040"
  },
  {
    "text": "actually",
    "start": "100079",
    "end": "102500"
  },
  {
    "text": "okay so most companies are moving to kubernetes and microservices for the",
    "start": "112079",
    "end": "117479"
  },
  {
    "text": "right research right they solve many problems in production environments but they also come with some challenges",
    "start": "117479",
    "end": "124320"
  },
  {
    "text": "and one of them is that they make make it harder to mimic your",
    "start": "124320",
    "end": "129420"
  },
  {
    "text": "production environment in your local deaf environment um even with tools like Docker local",
    "start": "129420",
    "end": "136260"
  },
  {
    "text": "kubernetes distributions like mini Cube you need to install this software run",
    "start": "136260",
    "end": "142200"
  },
  {
    "text": "all these micro services so there are several issues there one of them is that you may run out of CPU and memory in",
    "start": "142200",
    "end": "149040"
  },
  {
    "text": "your laptop so things go very slow or even stop working um",
    "start": "149040",
    "end": "154379"
  },
  {
    "text": "you need to maintain local configuration so if there is something wrong in your",
    "start": "154379",
    "end": "159480"
  },
  {
    "text": "Dev environment it's very difficult to replicate the same problem in other laptop or it's",
    "start": "159480",
    "end": "166200"
  },
  {
    "text": "very hard for anyone else to troubleshoot what is the issue and if you are using for example multi-repo",
    "start": "166200",
    "end": "171860"
  },
  {
    "text": "it's not trivial to orchestrate the build pools and deploy of all these",
    "start": "171860",
    "end": "178440"
  },
  {
    "text": "containers running locally so what most people do is to assume an",
    "start": "178440",
    "end": "184560"
  },
  {
    "text": "environment disparity between deaf environments and production and I think this is wrong",
    "start": "184560",
    "end": "190680"
  },
  {
    "text": "because at the end what you are doing is Shifting right your testing efforts ideally a developer should be able to",
    "start": "190680",
    "end": "198060"
  },
  {
    "text": "deploy a deaf environment and test end-to-end all the changes that they are doing before even sending a bull request",
    "start": "198060",
    "end": "205879"
  },
  {
    "text": "but if you are not able to do that you need to send a PR wait for the",
    "start": "205879",
    "end": "212220"
  },
  {
    "text": "continuous integration job to validate your changes or even worse you need to merge and wait for an staging or",
    "start": "212220",
    "end": "218940"
  },
  {
    "text": "intelligent environment to be a credit and then do the final end-to-end tester and if there is an issue you need to",
    "start": "218940",
    "end": "225659"
  },
  {
    "text": "start again work on your local deaf environments and a PR and all those things so all this",
    "start": "225659",
    "end": "232379"
  },
  {
    "text": "cycle reduces the developer productivity a lot and also the developer happiness",
    "start": "232379",
    "end": "239040"
  },
  {
    "start": "239000",
    "end": "239000"
  },
  {
    "text": "so the solution that we propose is cloud native development environments and this",
    "start": "239040",
    "end": "244319"
  },
  {
    "text": "is a high level view of this methodology the idea is to have a single kubernetes",
    "start": "244319",
    "end": "250439"
  },
  {
    "text": "cluster served by all the Developers and every developer is working on a",
    "start": "250439",
    "end": "256320"
  },
  {
    "text": "different name space and you can have as much isolation you need between them spaces using the",
    "start": "256320",
    "end": "262979"
  },
  {
    "text": "standard kubernetes objects so the idea is that on each name space",
    "start": "262979",
    "end": "268560"
  },
  {
    "text": "the developer is able to deploy a full replica of the application which is much more realistic because",
    "start": "268560",
    "end": "275400"
  },
  {
    "text": "this is running in kubernetes with your hand chats using your network configuration",
    "start": "275400",
    "end": "281280"
  },
  {
    "text": "security policies the same thing that you do in production and it's fully replicable because it",
    "start": "281280",
    "end": "289139"
  },
  {
    "text": "doesn't depend on your local configuration you should be able to deploy any commit from any git repository and everything",
    "start": "289139",
    "end": "296160"
  },
  {
    "text": "happens in the cluster in the cloud so it doesn't depend on your local configuration and anyone in your team",
    "start": "296160",
    "end": "301800"
  },
  {
    "text": "can go there and check it out because it's available for everyone um so in order to",
    "start": "301800",
    "end": "308300"
  },
  {
    "text": "adopt this methodology is very important that for the developer the deaf experience is the same and to do that",
    "start": "308300",
    "end": "315240"
  },
  {
    "text": "there are several open source projects like potato the presence deal garden and",
    "start": "315240",
    "end": "321180"
  },
  {
    "text": "many more and more or less the goal of these Dev tools is to provide this",
    "start": "321180",
    "end": "326460"
  },
  {
    "text": "experience so the developer keeps working locally on their IDE",
    "start": "326460",
    "end": "331620"
  },
  {
    "text": "and the application is hot reloading on remote immediately and you can even set",
    "start": "331620",
    "end": "336800"
  },
  {
    "text": "configure your debater set breakpoints and all those things so that's key for developer adoption if you need to change",
    "start": "336800",
    "end": "344160"
  },
  {
    "text": "the developer workflow people listen are not going to adopt it but um with these",
    "start": "344160",
    "end": "350940"
  },
  {
    "text": "tools you can have a realistic applicable and and fast iteration remote",
    "start": "350940",
    "end": "358740"
  },
  {
    "text": "Dev environments so that's the solution and I'm going to",
    "start": "358740",
    "end": "363780"
  },
  {
    "text": "talk about one of the problems with this approach and is that you need to run all these environments",
    "start": "363780",
    "end": "369840"
  },
  {
    "start": "367000",
    "end": "367000"
  },
  {
    "text": "in the cloud and it could be an impact in your Cloud wheel right which is skd",
    "start": "369840",
    "end": "375800"
  },
  {
    "text": "but good news is that kubernetes is very good to optimize resource allocation",
    "start": "375800",
    "end": "381479"
  },
  {
    "text": "that means that all the containers running in the same node they may serve",
    "start": "381479",
    "end": "388080"
  },
  {
    "text": "the CPU and the memory available in the node so for example if you are building your application and you need a spike",
    "start": "388080",
    "end": "395039"
  },
  {
    "text": "for CPU and memory the memory and the CPU of the node is available for your",
    "start": "395039",
    "end": "400259"
  },
  {
    "text": "container and when you are done the same CBR memory is available for other containers so that is very helpful to optimize your",
    "start": "400259",
    "end": "407639"
  },
  {
    "text": "infra utilization and also if you need more nodes in your cluster the cluster",
    "start": "407639",
    "end": "413220"
  },
  {
    "text": "will allow the scale up and down as needed automatically so that's good news but let's analyze",
    "start": "413220",
    "end": "420660"
  },
  {
    "text": "different use cases for development environments to see if this is really helpful in our use case",
    "start": "420660",
    "end": "428639"
  },
  {
    "text": "so this is a workload in production and it's more or less using two CPUs all",
    "start": "428639",
    "end": "436560"
  },
  {
    "text": "the time so we are able to set the CPU request to two CPUs",
    "start": "436560",
    "end": "442500"
  },
  {
    "text": "and in this case it's working very good if there are more incoming requests the deployment will scale horizontally",
    "start": "442500",
    "end": "449819"
  },
  {
    "text": "so that's good but for development is a little bit different what usually happens is that",
    "start": "449819",
    "end": "456180"
  },
  {
    "start": "452000",
    "end": "452000"
  },
  {
    "text": "when the application is booting you need more CPU and memory to boot your to put",
    "start": "456180",
    "end": "461639"
  },
  {
    "text": "a strap your your application and then after a while in development you don't usually have",
    "start": "461639",
    "end": "467520"
  },
  {
    "text": "that many incoming requests right so after a while you need less CPU and",
    "start": "467520",
    "end": "473039"
  },
  {
    "text": "memory to keep your application running um so in this case there is already the",
    "start": "473039",
    "end": "479400"
  },
  {
    "text": "vertical pod of the scalar in the kubernetes community and the idea is that the vertical product or scalar",
    "start": "479400",
    "end": "485220"
  },
  {
    "text": "monitors your container CPU and memory usage and then it will update the",
    "start": "485220",
    "end": "491460"
  },
  {
    "text": "requests and limits of your containers based on the real usage of your container",
    "start": "491460",
    "end": "496740"
  },
  {
    "text": "here I'm assuming that in place battery size is available because otherwise",
    "start": "496740",
    "end": "502979"
  },
  {
    "text": "here when the requests are updated the container will restart and then you",
    "start": "502979",
    "end": "508259"
  },
  {
    "text": "would have the booting time building CPU again but let's assume that in place but",
    "start": "508259",
    "end": "514140"
  },
  {
    "text": "precise is available but even with that when you are",
    "start": "514140",
    "end": "519240"
  },
  {
    "start": "516000",
    "end": "516000"
  },
  {
    "text": "developing they are random spikes and they tend to be short in time for example you are",
    "start": "519240",
    "end": "525899"
  },
  {
    "text": "working on your application editing your code you don't need too much CPU to do",
    "start": "525899",
    "end": "531240"
  },
  {
    "text": "that but then you need to build your application with for example a make Command",
    "start": "531240",
    "end": "537000"
  },
  {
    "text": "so for the make Command you need more CPU but the vertical product of the scalar",
    "start": "537000",
    "end": "542580"
  },
  {
    "text": "is reactive so there is a delay between you start using more CPU and until the",
    "start": "542580",
    "end": "548580"
  },
  {
    "text": "BB and the vertical product was killer updates the the request of the pot so in",
    "start": "548580",
    "end": "553620"
  },
  {
    "text": "this case um I don't have in in a",
    "start": "553620",
    "end": "558660"
  },
  {
    "text": "in enough requests here to run my main command then the requests have updated",
    "start": "558660",
    "end": "564779"
  },
  {
    "text": "but I'm not using this CPU here then the vertical product Skillet update the",
    "start": "564779",
    "end": "570000"
  },
  {
    "text": "request again and I don't have two CPUs available for the main command so there is a delay so it's not really working",
    "start": "570000",
    "end": "575279"
  },
  {
    "text": "for for this scenario ideally we we would like something like",
    "start": "575279",
    "end": "580620"
  },
  {
    "start": "578000",
    "end": "578000"
  },
  {
    "text": "this something that is proactive and updates the CPU request of my",
    "start": "580620",
    "end": "585959"
  },
  {
    "text": "container in real time basically or milliseconds",
    "start": "585959",
    "end": "591240"
  },
  {
    "text": "so in our case when we are started we basically run a dedicated cluster for",
    "start": "591240",
    "end": "596899"
  },
  {
    "text": "every customer and when we are started the tether we were using a standard kubernetes",
    "start": "596899",
    "end": "602580"
  },
  {
    "text": "requests and limits and um using that we were able to run a pots",
    "start": "602580",
    "end": "608700"
  },
  {
    "text": "per node um which is not cool because",
    "start": "608700",
    "end": "614940"
  },
  {
    "text": "um we are using VMS with four CPUs and 32 gigas of",
    "start": "614940",
    "end": "620519"
  },
  {
    "text": "memory and we were not utilizing all the resources so what we did is an adult",
    "start": "620519",
    "end": "626459"
  },
  {
    "text": "solution for this is a custom scheduler basically and with that solution with I don't have time to talk about but with",
    "start": "626459",
    "end": "632940"
  },
  {
    "text": "that solution we were able to run AP boards per node without affecting the",
    "start": "632940",
    "end": "638040"
  },
  {
    "text": "developer experience so this is huge it's 10x infra savings and and there is a lot of potential to",
    "start": "638040",
    "end": "645899"
  },
  {
    "text": "solve this problem properly our other solution is not ideal it requires a lot",
    "start": "645899",
    "end": "651660"
  },
  {
    "text": "of effort to maintain the solution and to configure the solution for different customers uh but it shows us the potential of",
    "start": "651660",
    "end": "659940"
  },
  {
    "text": "solving this problem so the question is if there is a better way to solve this",
    "start": "659940",
    "end": "665640"
  },
  {
    "text": "and with that in mind I'm gonna pass to Rene to talk about in place for resize and PDF",
    "start": "665640",
    "end": "671339"
  },
  {
    "text": "thank you Pablo that was great so in place pod resize can I get a show",
    "start": "671339",
    "end": "676740"
  },
  {
    "text": "of hands how many people here are aware that this feature is in the works",
    "start": "676740",
    "end": "681839"
  },
  {
    "text": "a few okay then those few probably know that this PR is moving at light speeds",
    "start": "681839",
    "end": "689300"
  },
  {
    "text": "okay well not exactly light speeds we're being extra cautious and there are good",
    "start": "690200",
    "end": "695399"
  },
  {
    "text": "reasons for that the pr is big and it touches critical components and kubernetes and mistakes can be costly so",
    "start": "695399",
    "end": "701339"
  },
  {
    "text": "it's imperative that we stage it in a responsible way what really matters is that we get",
    "start": "701339",
    "end": "706560"
  },
  {
    "text": "across that finish line hopefully in 126 I don't know",
    "start": "706560",
    "end": "712680"
  },
  {
    "text": "we just released the continuity 169 which is needed for this and so this PR",
    "start": "712680",
    "end": "717839"
  },
  {
    "text": "could merge any day now in the next few years",
    "start": "717839",
    "end": "722420"
  },
  {
    "text": "if you guys are really nice to me I will leave this PR to you in my will okay enough grief for my PR so let's",
    "start": "723000",
    "end": "730140"
  },
  {
    "start": "729000",
    "end": "729000"
  },
  {
    "text": "take a look at what really changed the first thing we did was we made you have the container spec and that is the",
    "start": "730140",
    "end": "735779"
  },
  {
    "text": "resources field we made it mutable for CPU and memory when you do that what it",
    "start": "735779",
    "end": "741000"
  },
  {
    "text": "lets you do is it lets you update send a patch to your pod spec saying I want the container I started out with one CPU but",
    "start": "741000",
    "end": "746820"
  },
  {
    "text": "I want two thus you're expressing desired resources for the Pod and then kubernetes goes to work doing what it",
    "start": "746820",
    "end": "753060"
  },
  {
    "text": "does best which is you know get the actual State equal to the desired state so with that with the ability to specify",
    "start": "753060",
    "end": "761339"
  },
  {
    "text": "a desired state of resources we need a way to Signal the user who's specified",
    "start": "761339",
    "end": "767399"
  },
  {
    "text": "the request what's the status of their request and for that we introduced a new field called resize in the Pod status",
    "start": "767399",
    "end": "774899"
  },
  {
    "text": "this status holds one of these four values when you have a pending request for resize the interesting one is in",
    "start": "774899",
    "end": "782160"
  },
  {
    "text": "progress where which should be the default case that means that the kublet was able to allocate the memory or CPU",
    "start": "782160",
    "end": "787740"
  },
  {
    "text": "that you wanted and it's working on it with the runtime to make it happen proposed as the initial state where when",
    "start": "787740",
    "end": "794639"
  },
  {
    "text": "your API server looks at your request it makes sure that the request is valid you're not exceeding like your requests are not exceeding limits and stuff like",
    "start": "794639",
    "end": "800940"
  },
  {
    "text": "that and infeasible is the user may not know okay there there is a node that has four",
    "start": "800940",
    "end": "806940"
  },
  {
    "text": "CPUs they may ask okay my part needs five it's never gonna happen so that's a",
    "start": "806940",
    "end": "812160"
  },
  {
    "text": "signal that you may need to evict your pod and ask the scheduler to schedule a new instance to another node where you",
    "start": "812160",
    "end": "817920"
  },
  {
    "text": "can get five CPUs and deferred is the case where the node has six CPUs but another pod is using two CPUs there and",
    "start": "817920",
    "end": "825240"
  },
  {
    "text": "it's possible but just not now it's your choice do you want to wait or do you want to evict and get five CPUs",
    "start": "825240",
    "end": "830880"
  },
  {
    "text": "elsewhere we added a couple of more Fields the resources allocated field in the",
    "start": "830880",
    "end": "837240"
  },
  {
    "text": "container status it's a persistent way of telling what is it that the in the in",
    "start": "837240",
    "end": "842279"
  },
  {
    "text": "progress pod status is driving towards and the resources field in the container",
    "start": "842279",
    "end": "848279"
  },
  {
    "text": "status I know there are a lot of these fields in here but the resources field in the container status tells you the",
    "start": "848279",
    "end": "853440"
  },
  {
    "text": "actual State as reported To Us by the runtime so that's what is actually on the container on your containers",
    "start": "853440",
    "end": "859560"
  },
  {
    "text": "lastly we have a new field called resize policy this was introduced for one",
    "start": "859560",
    "end": "865200"
  },
  {
    "text": "reason there are some Legacy applications where like Java applications which are using the xmx",
    "start": "865200",
    "end": "870480"
  },
  {
    "text": "flag they're not able to take advantage of increased capacity of the Pod without",
    "start": "870480",
    "end": "876120"
  },
  {
    "text": "restarting it's just the those applications need them so you have we want to give the users a way to specify",
    "start": "876120",
    "end": "882180"
  },
  {
    "text": "hey this is my application needs to be restarted the default is restart not required",
    "start": "882180",
    "end": "888899"
  },
  {
    "text": "where we will try to we will try to resize your Pod without restarting it doesn't guarantee it's not a guarantee",
    "start": "888899",
    "end": "895199"
  },
  {
    "text": "but kubernetes will try its best so what does it really involve I touched",
    "start": "895199",
    "end": "901440"
  },
  {
    "start": "899000",
    "end": "899000"
  },
  {
    "text": "upon this earlier we have changes to the API server cubelet scheduler and runtime there is a lot to go into here but I",
    "start": "901440",
    "end": "909180"
  },
  {
    "text": "will focus on one thing the kublet so cubelet admission of a resize request is",
    "start": "909180",
    "end": "915899"
  },
  {
    "text": "interesting here because this PR introduces a new race condition where",
    "start": "915899",
    "end": "921720"
  },
  {
    "text": "the resize request is racing with a pod that may just have been scheduled to that node and kublet is the ultimate",
    "start": "921720",
    "end": "929160"
  },
  {
    "text": "Authority and it will it's a gatekeeper it checks to make sure that at any point of time the requested resources is not",
    "start": "929160",
    "end": "936240"
  },
  {
    "text": "exceeding what's available so one of the two requests might fail if there is a contention",
    "start": "936240",
    "end": "942500"
  },
  {
    "text": "going to one of the best ways to really look at how this works is to go through",
    "start": "943560",
    "end": "948660"
  },
  {
    "start": "944000",
    "end": "944000"
  },
  {
    "text": "it step by step so let's do that now consider this example here what we have",
    "start": "948660",
    "end": "954360"
  },
  {
    "text": "here is a node where you have a pod with 40 Milli CPUs allocated to it now you",
    "start": "954360",
    "end": "961260"
  },
  {
    "text": "desire to give it 80 Milli CPUs so it starts with a patch to the Pod spec the API server validates this spot spec the",
    "start": "961260",
    "end": "968880"
  },
  {
    "text": "patch and then updates the object store in this case we update the hcd now the",
    "start": "968880",
    "end": "974040"
  },
  {
    "text": "next step is watch is triggered for the scheduler and kubelet the scheduler takes that looks at that watch and",
    "start": "974040",
    "end": "980820"
  },
  {
    "text": "updates its podcast and uses that to do the do the max of desired and actual so",
    "start": "980820",
    "end": "986820"
  },
  {
    "text": "that it doesn't compete with resize that's going that's in progress and in this case let's assume that the",
    "start": "986820",
    "end": "992940"
  },
  {
    "text": "kublet was able to successfully allocate the the resources that was requested 80 Milli CPUs so it does the admit pod",
    "start": "992940",
    "end": "999839"
  },
  {
    "text": "resize it succeeds and it immediately patches the Pod status with the port",
    "start": "999839",
    "end": "1005600"
  },
  {
    "text": "status resize field saying I was able to give you 80 Milli CPUs and the pro the",
    "start": "1005600",
    "end": "1011240"
  },
  {
    "text": "status of your resize is in progress and I'm going to be working with the runtime to make it happen",
    "start": "1011240",
    "end": "1017240"
  },
  {
    "text": "now in this case we are increasing the CPU so the Pod C Group C group settings",
    "start": "1017240",
    "end": "1023540"
  },
  {
    "text": "are initial are set first and then it goes and talks to container D the",
    "start": "1023540",
    "end": "1029600"
  },
  {
    "text": "runtime via the update container resources CRI API asking containerdy to",
    "start": "1029600",
    "end": "1034640"
  },
  {
    "text": "allocate 80 Milli CPUs for the pods containers and now the container d goes to work it",
    "start": "1034640",
    "end": "1041660"
  },
  {
    "text": "updates the configuration for the Pod and next time a container status CRI API",
    "start": "1041660",
    "end": "1047540"
  },
  {
    "text": "comes in it reports back to the group let's say yes your pod its containers",
    "start": "1047540",
    "end": "1053240"
  },
  {
    "text": "have 80 Milli CPUs that triggers a generation of update status update to",
    "start": "1053240",
    "end": "1058580"
  },
  {
    "text": "the Pod where it patches the power status saying the resize is now complete the Pod has 80 Milli CPUs and we have",
    "start": "1058580",
    "end": "1066080"
  },
  {
    "text": "come full circle so that is the happy golden path now",
    "start": "1066080",
    "end": "1072020"
  },
  {
    "text": "one there is a Nuance to this that I want to touch upon your pod may have more than one",
    "start": "1072020",
    "end": "1077480"
  },
  {
    "start": "1074000",
    "end": "1074000"
  },
  {
    "text": "container and you may be resizing more than one container at the same time you",
    "start": "1077480",
    "end": "1082760"
  },
  {
    "text": "may be giving more resources to some and taking more taking resources away from other containers",
    "start": "1082760",
    "end": "1088340"
  },
  {
    "text": "in all these cases what we do is we order the resize such that the decreases",
    "start": "1088340",
    "end": "1093679"
  },
  {
    "text": "for container resize are done before the increases are invoked and if there is a",
    "start": "1093679",
    "end": "1099500"
  },
  {
    "text": "net increase to the Pod C group values due to the resize then the part c groups",
    "start": "1099500",
    "end": "1104840"
  },
  {
    "text": "are ordered first and then the containers are resized and vice versa why is this important let's take the",
    "start": "1104840",
    "end": "1111500"
  },
  {
    "text": "scenario you have a system where you have two gig of memory available to the pods and you have one single pod running",
    "start": "1111500",
    "end": "1117799"
  },
  {
    "text": "on that system it has two containers there they take one gig each and we desire to give C1",
    "start": "1117799",
    "end": "1125600"
  },
  {
    "text": ".5 gig and cut memory for C2 make it 0.5 gig now the Pod hasn't changed the pods",
    "start": "1125600",
    "end": "1133340"
  },
  {
    "text": "resources are the same but if we did C1 and then C2 we are oversubscribed and this request will fail and the Pod will",
    "start": "1133340",
    "end": "1140000"
  },
  {
    "text": "end up in a bad state for this reason that we want the runtimes to not only",
    "start": "1140000",
    "end": "1145220"
  },
  {
    "start": "1143000",
    "end": "1143000"
  },
  {
    "text": "support update container resources CRI API but do so in a synchronous and",
    "start": "1145220",
    "end": "1150440"
  },
  {
    "text": "transactional manner if we don't do this then the downstream request to update",
    "start": "1150440",
    "end": "1156080"
  },
  {
    "text": "the policy groups might fail and the Pod will be in a bad State and when I say synchronous and transactional what I",
    "start": "1156080",
    "end": "1161360"
  },
  {
    "text": "mean here is that don't queue a task and say okay I will apply this later we need that a or nay whether it's succeeded or",
    "start": "1161360",
    "end": "1168980"
  },
  {
    "text": "it failed and if it failed what's the reason for failure in the context of that update container resources CRI API",
    "start": "1168980",
    "end": "1174860"
  },
  {
    "text": "call uh one other thing that got introduced",
    "start": "1174860",
    "end": "1180740"
  },
  {
    "text": "to the to The Container status CRI API is a resources field this is what allows the",
    "start": "1180740",
    "end": "1185840"
  },
  {
    "text": "container runtime to tell us to tell kublet what resources are actually configured on the pod",
    "start": "1185840",
    "end": "1192500"
  },
  {
    "text": "I guess you're starting to see why this feature is a little complex right and lastly there is C group V2 it's here",
    "start": "1192500",
    "end": "1202360"
  },
  {
    "text": "more and more OSS are shifting to C group V2 there are a bunch of desirable features to this",
    "start": "1202360",
    "end": "1207919"
  },
  {
    "text": "for us in particular we are interested in the ability to specify memory requests at the container level we did",
    "start": "1207919",
    "end": "1214400"
  },
  {
    "text": "not have that in C group V1 there is another talk I believe it's on Friday it's by David Porter and Ronald Patel",
    "start": "1214400",
    "end": "1220520"
  },
  {
    "text": "about C group V2 I highly recommend attending that if you can",
    "start": "1220520",
    "end": "1226120"
  },
  {
    "start": "1226000",
    "end": "1226000"
  },
  {
    "text": "okay so this is the fun stuff we what can we do with ebpf for in place",
    "start": "1226700",
    "end": "1233120"
  },
  {
    "text": "boundary size as Pablo has laid out we have a problem here we have uh the use",
    "start": "1233120",
    "end": "1240559"
  },
  {
    "text": "case here with the remote Dev environments there is this workload where there are spikes and the reactive",
    "start": "1240559",
    "end": "1246140"
  },
  {
    "text": "approach that we have with vpa is not good enough so",
    "start": "1246140",
    "end": "1251900"
  },
  {
    "text": "how do we how can ebpf help let's take a look consider this part here it's called",
    "start": "1251900",
    "end": "1258020"
  },
  {
    "text": "could build pod with one container Google container and as you might guess we use this spot to exec into the Pod",
    "start": "1258020",
    "end": "1266299"
  },
  {
    "text": "and edit code and build code it's a pretty good example that mimics the remote Dev",
    "start": "1266299",
    "end": "1272900"
  },
  {
    "text": "environment except that in the remote div you might do rsync of local code to remote",
    "start": "1272900",
    "end": "1278059"
  },
  {
    "text": "what's interesting here is that when we look at the requests the resources that we have requested we're requesting four",
    "start": "1278059",
    "end": "1283880"
  },
  {
    "text": "CPUs which is good enough to you know edit code and build but we have curated and cherry-picked a",
    "start": "1283880",
    "end": "1290840"
  },
  {
    "text": "value of 50 meg for memory that is sufficient to edit code and edit code",
    "start": "1290840",
    "end": "1296120"
  },
  {
    "text": "with a lightweight editor like VI but not enough to build if you try to hit",
    "start": "1296120",
    "end": "1301880"
  },
  {
    "text": "the make Command with this much of memory the ohm killer will come along and take care of things it's going to",
    "start": "1301880",
    "end": "1308600"
  },
  {
    "text": "kill your process or it's going to run really slow so well what can evpf do well let's take a",
    "start": "1308600",
    "end": "1315500"
  },
  {
    "text": "look what you're looking at here is this 20",
    "start": "1315500",
    "end": "1320960"
  },
  {
    "text": "odd lines of python code that's all we need that's it you go to the node on which the Pod is",
    "start": "1320960",
    "end": "1327799"
  },
  {
    "text": "running and tell python hey run this for me and you're set looking more closely there are two parts",
    "start": "1327799",
    "end": "1334520"
  },
  {
    "text": "to it the first is the abpf code itself this code attaches to the exact exec ve",
    "start": "1334520",
    "end": "1341059"
  },
  {
    "text": "system call that's the system call through which all the commands in the system that are executed go through you",
    "start": "1341059",
    "end": "1346460"
  },
  {
    "text": "do LS it goes for exec you do make it goes through LS it goes through exactly",
    "start": "1346460",
    "end": "1351919"
  },
  {
    "text": "and the second part is well when this when this ebpf program sees those",
    "start": "1351919",
    "end": "1357740"
  },
  {
    "text": "commands it traces it it traces it to a trace file and the second part to this is that we watch the trace file and if",
    "start": "1357740",
    "end": "1365120"
  },
  {
    "text": "we see make in the trace file then we resize our pod to 5 gig",
    "start": "1365120",
    "end": "1370460"
  },
  {
    "text": "that's it it's very simple isn't it well not so much this the only good thing about this code",
    "start": "1370460",
    "end": "1377360"
  },
  {
    "text": "is that it fits on one PowerPoint slide it's it's very inefficient everything that",
    "start": "1377360",
    "end": "1383539"
  },
  {
    "text": "you're doing in the system is getting traced to a trace file I actually tried it it's really slow and it's of limited",
    "start": "1383539",
    "end": "1389000"
  },
  {
    "text": "use so you're in your container you're happily writing codes somebody else doesn't make another container and you",
    "start": "1389000",
    "end": "1394340"
  },
  {
    "text": "resize that's not great right so let's do something a little less dumb for the demo so BPF also offers this",
    "start": "1394340",
    "end": "1402320"
  },
  {
    "start": "1399000",
    "end": "1399000"
  },
  {
    "text": "facility of maps which is a way for uh talking to the appf program and having it be more configurable than this what",
    "start": "1402320",
    "end": "1409640"
  },
  {
    "text": "we're going to do is we're going to use the BPF maps to tell the abpf program to focus on only specific containers and in",
    "start": "1409640",
    "end": "1416600"
  },
  {
    "text": "that container focus on specific commands how do we do that the containers have C group ID so we",
    "start": "1416600",
    "end": "1423380"
  },
  {
    "text": "specify the C group ID as a key in the bbp in the ebpf maps and in the value we",
    "start": "1423380",
    "end": "1429440"
  },
  {
    "text": "specify the list of commands that we are interested in tracing this will let us only trace the commands from the",
    "start": "1429440",
    "end": "1435740"
  },
  {
    "text": "containers that we are interested in that's good but how does the user tell this to us well",
    "start": "1435740",
    "end": "1442940"
  },
  {
    "text": "we resort to the good old annotations we have defined annotation called abpf resize which contains cname the",
    "start": "1442940",
    "end": "1449960"
  },
  {
    "text": "container name that we're interested in and the commands that we're interested in in this case make and of course what",
    "start": "1449960",
    "end": "1456679"
  },
  {
    "text": "do you want to resize to so we specify these three things with these three things the Pod Watcher thread or process",
    "start": "1456679",
    "end": "1462860"
  },
  {
    "text": "can configure the BPF maps and the program can trace exactly what we need",
    "start": "1462860",
    "end": "1467960"
  },
  {
    "text": "with that all we need to do is initiate the resize when we see the trace",
    "start": "1467960",
    "end": "1474260"
  },
  {
    "text": "and of course we are lazy kubernetes people we are going to go to each node and say hey python run this code hey python run",
    "start": "1474260",
    "end": "1480320"
  },
  {
    "text": "this code we're going to ask kubernetes do it we added a Daemon set and kubernetes does the hard work for us",
    "start": "1480320",
    "end": "1486740"
  },
  {
    "text": "I have code up on GitHub it's demo code only so don't run it in production",
    "start": "1486740",
    "end": "1492980"
  },
  {
    "text": "so you can take a look at it and see how this works it's a prototype",
    "start": "1492980",
    "end": "1498519"
  },
  {
    "text": "and wow it brings us to the show time",
    "start": "1498559",
    "end": "1504320"
  },
  {
    "text": "okay I'm gonna switch to the the screen here and this",
    "start": "1504320",
    "end": "1510460"
  },
  {
    "text": "can everyone see the the terminal screens okay all the way in the back this is the hardest part to get okay",
    "start": "1512299",
    "end": "1517700"
  },
  {
    "text": "great okay what you're looking at here is uh is a local all these terminals are sshed",
    "start": "1517700",
    "end": "1524659"
  },
  {
    "text": "into a local VM called abpf resize and we're running a local kubernetes",
    "start": "1524659",
    "end": "1530419"
  },
  {
    "text": "cluster a local cluster here with In-Place bodary size feature gate enabled that means we can resize our",
    "start": "1530419",
    "end": "1535940"
  },
  {
    "text": "pods without restarting the power or containers the first step is to of course deploy the build pod and I'm doing that here",
    "start": "1535940",
    "end": "1543740"
  },
  {
    "text": "with this yaml deployed it's going to take a moment there it is so we have the Pod running here the good",
    "start": "1543740",
    "end": "1551240"
  },
  {
    "text": "build pod we can exec into this part edit code and build code and we have this uh window up here shows the stats",
    "start": "1551240",
    "end": "1558500"
  },
  {
    "text": "it's using it's using 600 600 KB it's using under 1B MB of memory",
    "start": "1558500",
    "end": "1566179"
  },
  {
    "text": "and then you have we can do one more thing here we get the cube build pod in Json format",
    "start": "1566179",
    "end": "1573260"
  },
  {
    "text": "and query the status resources allocated the new field that we introduced for this feature that tells us what the",
    "start": "1573260",
    "end": "1579679"
  },
  {
    "text": "cubelet has allocated for your pods pod and its containers in this case we are focusing on the build container that's",
    "start": "1579679",
    "end": "1584720"
  },
  {
    "text": "container zero now let's take a look at",
    "start": "1584720",
    "end": "1589820"
  },
  {
    "text": "let's edit some code so I'm going to exec into the build part",
    "start": "1589820",
    "end": "1595640"
  },
  {
    "text": "and LS this is the kubernetes 125 release branch that I've gotten the Pod",
    "start": "1595640",
    "end": "1601580"
  },
  {
    "text": "for this demo get status shows us that if you're looking at the memory values we are at",
    "start": "1601580",
    "end": "1607940"
  },
  {
    "text": "42 Meg it's under what we so we have 50 we're under that and we use the lightweight editor like",
    "start": "1607940",
    "end": "1613880"
  },
  {
    "text": "vim and edit some code",
    "start": "1613880",
    "end": "1618580"
  },
  {
    "text": "so this does not look right kubernetes also known as k8s we need to fix this",
    "start": "1619340",
    "end": "1624740"
  },
  {
    "text": "so let's fix it all right",
    "start": "1624740",
    "end": "1631360"
  },
  {
    "text": "that looks much better isn't it okay so the memory usage is 30 Meg we're",
    "start": "1632000",
    "end": "1638000"
  },
  {
    "text": "good so what happens when you try to build this let's try there's one way to find out",
    "start": "1638000",
    "end": "1644960"
  },
  {
    "text": "so it's going to run really oh so it got killed well that was fast",
    "start": "1644960",
    "end": "1650240"
  },
  {
    "text": "this is expected right we have 50 meg and make is trying to allocate more than 50 and we just don't have it and the ohm",
    "start": "1650240",
    "end": "1656900"
  },
  {
    "text": "killer came along came along and pretty much told us to go kick rocks so what can we do we have choices we",
    "start": "1656900",
    "end": "1664640"
  },
  {
    "text": "have options we can use we can schedule the Pod with five gig and you won't have to worry about it of",
    "start": "1664640",
    "end": "1670520"
  },
  {
    "text": "course your bank account won't be happy and you could use you could rely on vpa when",
    "start": "1670520",
    "end": "1676580"
  },
  {
    "text": "vpa sees this um events out of memory events it will resize the Pod for you but it's not great developer experience",
    "start": "1676580",
    "end": "1684919"
  },
  {
    "text": "so there is another way fortunately let's take a look at what ebpf can do for us",
    "start": "1684919",
    "end": "1690620"
  },
  {
    "text": "vpf tool map list just shows that map entries I'm going to use this later but first is to deploy the Daemon set",
    "start": "1690620",
    "end": "1698559"
  },
  {
    "text": "so with this command I'm going to deploy the Daemon set it's going to show up here it takes a moment to initialize now",
    "start": "1701240",
    "end": "1706640"
  },
  {
    "text": "it's running we can know that the program is up and running by looking using the BPF tool to list the map",
    "start": "1706640",
    "end": "1712159"
  },
  {
    "text": "entries so I'm going to run that again so there is a map entry here now we can get into the details of this by doing a",
    "start": "1712159",
    "end": "1718580"
  },
  {
    "text": "dump of this map entry 67.",
    "start": "1718580",
    "end": "1725419"
  },
  {
    "text": "so what we see here is a key one four seven five two that is the C group ID of",
    "start": "1725419",
    "end": "1731960"
  },
  {
    "text": "this container in which I just executed the make Command which you know got killed by ohm killer and it's telling",
    "start": "1731960",
    "end": "1738620"
  },
  {
    "text": "the BPF program hey if you see this container execute a make Command then please trace it",
    "start": "1738620",
    "end": "1744559"
  },
  {
    "text": "so now let's try this again let's see if it works",
    "start": "1744559",
    "end": "1750039"
  },
  {
    "text": "oh keep an eye on this window the top left top right corner window with the 15 Meg",
    "start": "1752900",
    "end": "1759140"
  },
  {
    "text": "value that's being watched as I hit the make Command here we go",
    "start": "1759140",
    "end": "1764659"
  },
  {
    "text": "so we're at five gig and the container seems to be happy it's allocating",
    "start": "1764659",
    "end": "1770240"
  },
  {
    "text": "whatever it needs and the make is making much better progress than it did before",
    "start": "1770240",
    "end": "1776600"
  },
  {
    "text": "so that my friends is the magic of ebpf for you",
    "start": "1776600",
    "end": "1781600"
  },
  {
    "text": "thank you [Applause]",
    "start": "1782299",
    "end": "1790880"
  },
  {
    "text": "okay let me take a moment to thank the demo Gods here for saving the surprises for another day",
    "start": "1790880",
    "end": "1798460"
  },
  {
    "text": "and uh so what did we what did we see today we saw that uh we have this use case as",
    "start": "1798559",
    "end": "1805820"
  },
  {
    "text": "Pablo described where make spikes that happen in the development environment they need a much more responsive resize",
    "start": "1805820",
    "end": "1813200"
  },
  {
    "text": "and that's not always possible with what with what we currently have and we saw",
    "start": "1813200",
    "end": "1818600"
  },
  {
    "text": "that ebpf programs can help here by almost instantaneously resizing the Pod for you to your needs and uh so to recap",
    "start": "1818600",
    "end": "1829340"
  },
  {
    "start": "1829000",
    "end": "1829000"
  },
  {
    "text": "we feel that cloud native development environments are the future and because they're cost effective they're uh they",
    "start": "1829340",
    "end": "1836059"
  },
  {
    "text": "give you a production like environment and all teams work on the same config which saves on testing costs and production issues",
    "start": "1836059",
    "end": "1842840"
  },
  {
    "text": "we we want the run times to be able to support uh in place resize if you're",
    "start": "1842840",
    "end": "1849200"
  },
  {
    "text": "working with runtime or if you're a maintainer please consider adding support for In-Place resize to your runtime we don't want the user to have",
    "start": "1849200",
    "end": "1855380"
  },
  {
    "text": "to worry about whether whether this works with my runtime or not we want it",
    "start": "1855380",
    "end": "1860960"
  },
  {
    "text": "to just work it shouldn't even be something that people think about that's where we want to be",
    "start": "1860960",
    "end": "1866960"
  },
  {
    "text": "and uh try this feature gate out when this makes it in please turn on the feature",
    "start": "1866960",
    "end": "1873140"
  },
  {
    "text": "gate hammer on it beat up on the feature we want to find those Corner case issues it's important to find it's important to",
    "start": "1873140",
    "end": "1879740"
  },
  {
    "text": "handle the use case as well but it's also important to gracefully handle the abuse cases",
    "start": "1879740",
    "end": "1885440"
  },
  {
    "text": "and lastly try out eppf look I'm I'm no evpf expert not even",
    "start": "1885440",
    "end": "1892820"
  },
  {
    "text": "close but we managed to Cobble together this little Improvement in a short time and this Improvement got us from being",
    "start": "1892820",
    "end": "1901059"
  },
  {
    "text": "reactive to being proactive for this use case it took us from you know tens or",
    "start": "1901059",
    "end": "1907279"
  },
  {
    "text": "hundreds of seconds of response time to like a sub second response time and that little Improvement is actually",
    "start": "1907279",
    "end": "1915620"
  },
  {
    "text": "a paradigm shift now and it's great when small improvements lead to Amplified gains so yeah try out ebpf it just might",
    "start": "1915620",
    "end": "1922220"
  },
  {
    "text": "solve problems for you we have one more slide to go and",
    "start": "1922220",
    "end": "1927620"
  },
  {
    "text": "it's the one that everyone is waiting for the title says it all right so please scan this QR code",
    "start": "1927620",
    "end": "1935600"
  },
  {
    "text": "it will take you to a place where you can live you can leave feedback for us",
    "start": "1935600",
    "end": "1941000"
  },
  {
    "text": "and please tell us what you found useful and more importantly what we could do better",
    "start": "1941000",
    "end": "1946539"
  },
  {
    "text": "and uh we love to hear from you we love the feedback so and thank you for being here thank",
    "start": "1946539",
    "end": "1953000"
  },
  {
    "text": "you for being such a great audience uh it's been a pleasure with that I will conclude this talk",
    "start": "1953000",
    "end": "1960080"
  },
  {
    "text": "[Applause]",
    "start": "1960080",
    "end": "1967359"
  }
]