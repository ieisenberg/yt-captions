[
  {
    "start": "0",
    "end": "58000"
  },
  {
    "text": "all right so yeah today today we're here to talk about how we scale open source",
    "start": "160",
    "end": "5200"
  },
  {
    "text": "machine learning and how we use communities to deliver food to millions of people i'm stephen buttiful i'm a machine",
    "start": "5200",
    "end": "11440"
  },
  {
    "text": "learning engineer at walt and i'm here with i'm edgy i'm head of developer relations at company called selden",
    "start": "11440",
    "end": "18240"
  },
  {
    "text": "so yeah so maybe for the people that don't know walt we are we started as a food",
    "start": "18240",
    "end": "23519"
  },
  {
    "text": "delivery company in helsinki in 2016 and we are now in 23 countries going from norway to japan and then we have",
    "start": "23519",
    "end": "30640"
  },
  {
    "text": "millions of users that's like the boring part but the fun part is that we have machine",
    "start": "30640",
    "end": "35840"
  },
  {
    "text": "learning and we have a lot of different use cases the first one is supply and demand forecasting the second one is",
    "start": "35840",
    "end": "41440"
  },
  {
    "text": "recommended systems then we have logistic optimization then we also have fraud detection and",
    "start": "41440",
    "end": "47120"
  },
  {
    "text": "the last one with the longest title situation monitoring inbox prioritization",
    "start": "47120",
    "end": "52320"
  },
  {
    "text": "so with a lot of different machine learning use cases we have a lot of different needs and we have to address those needs",
    "start": "52320",
    "end": "59520"
  },
  {
    "text": "and the first one the biggest need that you can have usually when you train machine learning is data access how do you access your",
    "start": "59520",
    "end": "66640"
  },
  {
    "text": "data how do you make sure that you access it in a secure way but it has to be simple as well we don't want our data",
    "start": "66640",
    "end": "71840"
  },
  {
    "text": "scientists to be struggling and to be like okay i need access to that table or that table or that database but",
    "start": "71840",
    "end": "77520"
  },
  {
    "text": "struggling for like i don't know days and days and we also don't want them to have access to the whole",
    "start": "77520",
    "end": "82880"
  },
  {
    "text": "database that we have and all the all the tables that we have so yeah we want to have that simple and",
    "start": "82880",
    "end": "88560"
  },
  {
    "text": "yet secure and then infrastructure as well has it has been told you know scaling is",
    "start": "88560",
    "end": "94079"
  },
  {
    "text": "quite hard and then you need infrastructure to scale usually and when you train a model you need some",
    "start": "94079",
    "end": "100000"
  },
  {
    "text": "resources you need some computer resources so you're gonna need cpu usually memory as well and then you",
    "start": "100000",
    "end": "105200"
  },
  {
    "text": "might also need gpus and you might need one gpu or five gpus or even more so how do you have access",
    "start": "105200",
    "end": "111600"
  },
  {
    "text": "to those i'm a big believer that if you need something it should be easy to have access to it so if you need one gpu you",
    "start": "111600",
    "end": "118159"
  },
  {
    "text": "should just say okay i want one gpu you should not have to think about all the drivers and everything so yeah we have a",
    "start": "118159",
    "end": "124320"
  },
  {
    "text": "whole infrastructure for that and then one part that is very important is fast",
    "start": "124320",
    "end": "129920"
  },
  {
    "text": "deployment you know you machine learning is very iterative and if you make it slow for",
    "start": "129920",
    "end": "135360"
  },
  {
    "text": "your data scientist to deploy something then usually the whole process is becoming very slow and then instead of",
    "start": "135360",
    "end": "141280"
  },
  {
    "text": "taking three months to deploy model to production for example you take six months a year or even you don't even",
    "start": "141280",
    "end": "147120"
  },
  {
    "text": "deploy it you know we know like a lot of models that are you train your model you're like very happy with it but then",
    "start": "147120",
    "end": "152560"
  },
  {
    "text": "you never deployed to production so that's why yeah we have fast deployment is very important and so we have a lot",
    "start": "152560",
    "end": "158000"
  },
  {
    "text": "of ci cd pipelines and a lot of different templates and the last one is standardized",
    "start": "158000",
    "end": "163760"
  },
  {
    "text": "monitoring so we have data centers the trainer model they're very happy they deploy it",
    "start": "163760",
    "end": "169440"
  },
  {
    "text": "but you know i don't want them to have to go on graffana and be like okay i'm going to create my new dashboard there because most of them",
    "start": "169440",
    "end": "175920"
  },
  {
    "text": "they just want to use graphana they don't really care and they don't want to create dashboards and also you train your model so you're",
    "start": "175920",
    "end": "182080"
  },
  {
    "text": "going to have logging and we want it to be the same for everyone at least for the basic you know you can have i don't",
    "start": "182080",
    "end": "187440"
  },
  {
    "text": "know like cpu usage memory usage and all those things they shouldn't have to think about it",
    "start": "187440",
    "end": "192640"
  },
  {
    "text": "we should just yeah provide it for them so that's those are the things and the different needs that we have",
    "start": "192640",
    "end": "200400"
  },
  {
    "start": "199000",
    "end": "273000"
  },
  {
    "text": "and yeah maybe a quick reminder of the typical life cycle of a machine learning say you have your your idea you have a",
    "start": "200720",
    "end": "206879"
  },
  {
    "text": "problem usually so you're going to collect some data and then you're going to create one model usually the first model is going",
    "start": "206879",
    "end": "212799"
  },
  {
    "text": "to be terrible then you're going to evaluate the model and you create a new model and then you collect more data and everything and",
    "start": "212799",
    "end": "218560"
  },
  {
    "text": "then you have a whole life cycle here and then after after a bit of time you're like okay i'm pretty happy with",
    "start": "218560",
    "end": "224400"
  },
  {
    "text": "my model here it looks looks decent so what you're going to do usually is that you want to deploy it to production",
    "start": "224400",
    "end": "232080"
  },
  {
    "text": "so you deploy to production you have your first model deployed you're very happy but then you need to monitor it because",
    "start": "232080",
    "end": "238640"
  },
  {
    "text": "you know you have a model but then if you don't know the latency for example of your model if you're doing it online inference",
    "start": "238640",
    "end": "244480"
  },
  {
    "text": "then it's pretty bad and it's the same you also have to monitor the quality of your model",
    "start": "244480",
    "end": "249920"
  },
  {
    "text": "you trade it on data from the past usually so you have now new data that is coming and so you also need to make sure",
    "start": "249920",
    "end": "255599"
  },
  {
    "text": "that you know your model is the same quality it's like yeah maybe you deploy something and the data is very different",
    "start": "255599",
    "end": "260799"
  },
  {
    "text": "now that what you're training off so you really have to be careful with that and then yeah iterate again you go back",
    "start": "260799",
    "end": "267360"
  },
  {
    "text": "to the problem you go back to collecting data prototyping models and deploying again",
    "start": "267360",
    "end": "274080"
  },
  {
    "text": "and that's like the thing we really wanted to focus on making iterative work uh quick and easy that's our main thing",
    "start": "274080",
    "end": "280880"
  },
  {
    "text": "um and we had different challenges when we started the the whole ml platform the",
    "start": "280880",
    "end": "288160"
  },
  {
    "start": "282000",
    "end": "399000"
  },
  {
    "text": "first one is that we had projects that have been running for a long time at world and we we've",
    "start": "288160",
    "end": "294160"
  },
  {
    "text": "been using measuring for i think five years but we only had a couple of projects that were running and it was very hard",
    "start": "294160",
    "end": "300400"
  },
  {
    "text": "to create new projects we had a lot of people we have some data scientists and they have amazing ideas",
    "start": "300400",
    "end": "306400"
  },
  {
    "text": "but sometimes it was very hard to create a new project and be like okay i want to work on that but then you don't have",
    "start": "306400",
    "end": "312000"
  },
  {
    "text": "access to the data or then you don't know exactly how to deploy something so that was quite hard",
    "start": "312000",
    "end": "317680"
  },
  {
    "text": "and then the second one is that we had a lot of different tools we had we're using airflow then we're using also some",
    "start": "317680",
    "end": "323680"
  },
  {
    "text": "cron job to train your model but also yeah some save maker and then sometimes some custom code you know like someone",
    "start": "323680",
    "end": "329840"
  },
  {
    "text": "wrote three years ago and then no one knows exactly what it's doing so that's that was like a big problem that we had",
    "start": "329840",
    "end": "336560"
  },
  {
    "text": "and the last one is really the impact because we didn't know exactly what was running or if it was running well it's",
    "start": "336560",
    "end": "343680"
  },
  {
    "text": "also very hard to monitor the impact of your models and be like okay my model is pretty good you know when i order food",
    "start": "343680",
    "end": "349520"
  },
  {
    "text": "it arrives on time or not and so that's those are models that are very important for us and we couldn't really monitor it",
    "start": "349520",
    "end": "356639"
  },
  {
    "text": "and one thing that usually people don't really think about is like we had a disconnect between match learning",
    "start": "356639",
    "end": "362720"
  },
  {
    "text": "metrics and business metrics so you arrive to a meeting you're a data scientist you're very happy your mean",
    "start": "362720",
    "end": "368400"
  },
  {
    "text": "squad error for example is lower but then we didn't exactly know what it was uh what does it mean for the business",
    "start": "368400",
    "end": "374560"
  },
  {
    "text": "and so it was like a big big struggle to be like okay so when you start a new project you have some metrics that are",
    "start": "374560",
    "end": "380800"
  },
  {
    "text": "data metrics and then you have some business metrics so you want to optimize for both and then when you have meetings then",
    "start": "380800",
    "end": "386560"
  },
  {
    "text": "it's a bit easier to be like okay my my error is lower therefore the the business should",
    "start": "386560",
    "end": "392160"
  },
  {
    "text": "increase or the retention should increase and so those were the main challenges that we had",
    "start": "392160",
    "end": "399280"
  },
  {
    "start": "399000",
    "end": "556000"
  },
  {
    "text": "and if our clicker works yes so what do we want exactly for the",
    "start": "399520",
    "end": "404720"
  },
  {
    "text": "machine learning platform well the first one is easy to launch and iterate i said that already a couple of times but",
    "start": "404720",
    "end": "410720"
  },
  {
    "text": "that's the main thing uh and then really a driving force for new endeavors you know you're like you",
    "start": "410720",
    "end": "415759"
  },
  {
    "text": "create a new project and you're like yeah we have a whole ml platform and it's pretty easy to train a model it's pretty easy to deploy your model and",
    "start": "415759",
    "end": "422000"
  },
  {
    "text": "then we also we have monitoring we have logging so you know we can we can create it and we're pretty confident that we",
    "start": "422000",
    "end": "427680"
  },
  {
    "text": "can make it an end-to-end model actually and focus on platform velocity that's a",
    "start": "427680",
    "end": "433360"
  },
  {
    "text": "big thing as well and then tooling so common best practices",
    "start": "433360",
    "end": "438400"
  },
  {
    "text": "you know i have like we have a lot of tests we write a lot of tests for data scientists and then they can use them",
    "start": "438400",
    "end": "443680"
  },
  {
    "text": "but then we also have different tooling like we have an in-house tool that is doing yaml to terraform",
    "start": "443680",
    "end": "450400"
  },
  {
    "text": "for example so our data scientist when they want to access some uh bucket on s3",
    "start": "450400",
    "end": "455599"
  },
  {
    "text": "instead of writing all the terraform and everything then we have we write the yamufl the yamo with the s3 bucket",
    "start": "455599",
    "end": "462639"
  },
  {
    "text": "and then it will create the whole terraform without you having to do anything so that's pretty handy for a data scientist because yeah they didn't",
    "start": "462639",
    "end": "469199"
  },
  {
    "text": "sign up to do some terraform i didn't sign up to do some terraform so pretty happy when i don't have to",
    "start": "469199",
    "end": "474800"
  },
  {
    "text": "uh and then we also have different templates so you you can train your new model and",
    "start": "474800",
    "end": "480720"
  },
  {
    "text": "with the different tool that we use and i'm going to talk about it later you need to write a docker file if you want",
    "start": "480720",
    "end": "485759"
  },
  {
    "text": "to train your model and for that you know you you might not want to run your docker file as root or you know",
    "start": "485759",
    "end": "491039"
  },
  {
    "text": "different things that are related to security so we have those templates so that people can just use them and then",
    "start": "491039",
    "end": "497440"
  },
  {
    "text": "they replace they replace the template with their code but then most of their work is already done so they don't",
    "start": "497440",
    "end": "502560"
  },
  {
    "text": "really have to think about those things and therefore it's deployed in a secure way i can say",
    "start": "502560",
    "end": "508800"
  },
  {
    "text": "and also a lot of automation that's something very important you work on your feature branch you",
    "start": "508800",
    "end": "515120"
  },
  {
    "text": "merge it to the main branch and then we have a lot of ci cd running we have automatic deployment as well running if",
    "start": "515120",
    "end": "520880"
  },
  {
    "text": "you image but i'll come back to that a bit later",
    "start": "520880",
    "end": "526160"
  },
  {
    "text": "and the impact yeah we want logging continuous monitoring by default for everything so for the model that you're",
    "start": "526160",
    "end": "532800"
  },
  {
    "text": "training for the model that you're deploying and we want that by default for everything without having the data sensors to do anything",
    "start": "532800",
    "end": "539440"
  },
  {
    "text": "and yeah make machine learning a core business component it's been used for years",
    "start": "539440",
    "end": "545200"
  },
  {
    "text": "but we're still not there and i think if we can really have a good ml platform then",
    "start": "545200",
    "end": "550880"
  },
  {
    "text": "we'll be able to to make it great and a core business component",
    "start": "550880",
    "end": "556800"
  },
  {
    "start": "556000",
    "end": "608000"
  },
  {
    "text": "so yeah and how did we start then started to create value very quickly for",
    "start": "557440",
    "end": "564000"
  },
  {
    "text": "a data scientist our data scientist our customers uh i guess most of you work with customers usually you have to",
    "start": "564000",
    "end": "569839"
  },
  {
    "text": "create some values for them so they're happy but it's the same for us we had to create values for them quickly",
    "start": "569839",
    "end": "575440"
  },
  {
    "text": "so that means yeah first iteration within automatic monitoring but for example being able to run a",
    "start": "575440",
    "end": "581279"
  },
  {
    "text": "model in shadow mode you have a model running in production you want to run a new one in shadow mode so you can make",
    "start": "581279",
    "end": "586480"
  },
  {
    "text": "sure that you know it's actually good and the latency is not too high in comparison to the one that is running in production where we can you can do that",
    "start": "586480",
    "end": "593279"
  },
  {
    "text": "easily now and rolling up data you you deploy your model and you're like okay",
    "start": "593279",
    "end": "599200"
  },
  {
    "text": "maybe if i if it's if it's broken sorry uh then you know you're not going to update everything and then you have the",
    "start": "599200",
    "end": "604480"
  },
  {
    "text": "whole what communities allows you to to have with running updates",
    "start": "604480",
    "end": "609519"
  },
  {
    "start": "608000",
    "end": "638000"
  },
  {
    "text": "and it had to be easy to deploy on kubernetes as well because when i joined the company i",
    "start": "609519",
    "end": "615040"
  },
  {
    "text": "joined a year and a half ago um you know we already had like 14 data centers and i was the only one uh so you",
    "start": "615040",
    "end": "621920"
  },
  {
    "text": "did you i had to earn trust of the data scientists and the different stakeholders you know i didn't want to be to arrive there and be like okay now",
    "start": "621920",
    "end": "628880"
  },
  {
    "text": "everyone trust me you know have no background in building an ml platform so i had to earn their trust and to do that",
    "start": "628880",
    "end": "635360"
  },
  {
    "text": "we had to deploy something that was quick to deploy on communities and the last one",
    "start": "635360",
    "end": "641200"
  },
  {
    "start": "638000",
    "end": "682000"
  },
  {
    "text": "uh we have a motto at walt which is to focus and so for that we also wanted to focus",
    "start": "641200",
    "end": "646720"
  },
  {
    "text": "on only one component meaning that we didn't want to build the whole ml platform you know from the",
    "start": "646720",
    "end": "651839"
  },
  {
    "text": "first month we had to only one component that creates value quickly and we can",
    "start": "651839",
    "end": "657279"
  },
  {
    "text": "have a quick impact so that we don't spend you know i don't know six months eight months working on something",
    "start": "657279",
    "end": "662560"
  },
  {
    "text": "something that can be amazing but then your data scientists will be okay i'm not gonna use it because i don't need it",
    "start": "662560",
    "end": "667680"
  },
  {
    "text": "you know we want quick feedback as well from them we expect quick iteration from them but in return we want quick feedback from them so you can be like",
    "start": "667680",
    "end": "674399"
  },
  {
    "text": "okay what you did is pretty cool you have to improve that all that so there's",
    "start": "674399",
    "end": "679680"
  },
  {
    "text": "like that's how we started and that's what it looks like now today",
    "start": "679680",
    "end": "686160"
  },
  {
    "start": "682000",
    "end": "746000"
  },
  {
    "text": "so we have a whole ml platform now uh and we have flights here which which is",
    "start": "686160",
    "end": "691680"
  },
  {
    "text": "what we use to train and orchestrate our different workflows so that's running on kubernetes as well and it's fully",
    "start": "691680",
    "end": "697360"
  },
  {
    "text": "distributed it's fully scalable and it's running on dedicated kubernetes clusters that we have",
    "start": "697360",
    "end": "702959"
  },
  {
    "text": "and then we use ml flow for experiment tracking well it's very famous but yeah you can",
    "start": "702959",
    "end": "708000"
  },
  {
    "text": "track metrics and different things related to your models and then we have some python services",
    "start": "708000",
    "end": "713760"
  },
  {
    "text": "that are here to provide automatic updating for your model and different things that we need",
    "start": "713760",
    "end": "719600"
  },
  {
    "text": "and the one that i mentioned here as well which is the yamal to terraform it's also something that we have here",
    "start": "719600",
    "end": "725279"
  },
  {
    "text": "and the last one it's seldom core so it's what we started with it's our deployment service",
    "start": "725279",
    "end": "731120"
  },
  {
    "text": "and it basically allows you to put models into production microservices and to have automatic logging automatic",
    "start": "731120",
    "end": "737440"
  },
  {
    "text": "monitoring shadow mode a b test and so that's what we have now and that's what we're going to build on in the future",
    "start": "737440",
    "end": "744800"
  },
  {
    "text": "so yeah go for it great uh thanks stephen um so yeah just",
    "start": "744800",
    "end": "750880"
  },
  {
    "start": "746000",
    "end": "828000"
  },
  {
    "text": "a quick show of hands if you've heard of or used ml flow before okay cool so for i don't know people",
    "start": "750880",
    "end": "757440"
  },
  {
    "text": "joining online or watching the recording that was maybe half the run um i'll do a very quick recap and then um i'll show a",
    "start": "757440",
    "end": "765040"
  },
  {
    "text": "little bit of the kind of what seldom core offers and i'm going to jump into a demo and show some stuff live but i'll try and go through things very quickly",
    "start": "765040",
    "end": "771680"
  },
  {
    "text": "okay so the uh ml flow is basically made up of four components um there's projects which",
    "start": "771680",
    "end": "778160"
  },
  {
    "text": "allows you to kind of create this reproducible environment you can define the kind of model interface etc set all",
    "start": "778160",
    "end": "785200"
  },
  {
    "text": "of that up um there's the tracking api right and this is the bit that will use very heavily",
    "start": "785200",
    "end": "791440"
  },
  {
    "text": "this is what allows you to to track every experiment that you run in every training run the parameters that we use",
    "start": "791440",
    "end": "798240"
  },
  {
    "text": "the data and the versions of um configuration and so on uh you know and you'll see some of that again when i",
    "start": "798240",
    "end": "804480"
  },
  {
    "text": "show that in a second it has its own model format which is really good for making",
    "start": "804480",
    "end": "810000"
  },
  {
    "text": "your models particularly different frameworks reusable in the same way",
    "start": "810000",
    "end": "815440"
  },
  {
    "text": "and then it also has a model registry which is a way of handling the life cycle of a you know machine learning",
    "start": "815440",
    "end": "821760"
  },
  {
    "text": "model particularly if you're using mlflow to track you know the training runs and stuff like that um",
    "start": "821760",
    "end": "828399"
  },
  {
    "start": "828000",
    "end": "889000"
  },
  {
    "text": "okay so what's sold and core sultan core um steven mentioned a bit already it's but it's an open source",
    "start": "828399",
    "end": "834320"
  },
  {
    "text": "project that allows you to deploy and monitor models on kubernetes that's kind of it at the highest level",
    "start": "834320",
    "end": "840639"
  },
  {
    "text": "if we dig a little bit deeper what it actually does is it creates these containerized microservices with a rest",
    "start": "840639",
    "end": "846399"
  },
  {
    "text": "and grpc interface um and then if we're digging in even further right it actually provides these",
    "start": "846399",
    "end": "852880"
  },
  {
    "text": "like highly optimized inference servers um that you can then use to to execute",
    "start": "852880",
    "end": "858959"
  },
  {
    "text": "your model's code on when at you know at runtime when you're you're doing inference um and if you have a look on",
    "start": "858959",
    "end": "864959"
  },
  {
    "text": "the right there that's an example of a very simple selden deployment crd",
    "start": "864959",
    "end": "871360"
  },
  {
    "text": "um and they i don't know if this point is gonna work but you can see for example i've got like this implementation",
    "start": "871360",
    "end": "876959"
  },
  {
    "text": "uh key value pair and then i've got you know sklearn server so it's using the scikit-learn server",
    "start": "876959",
    "end": "882079"
  },
  {
    "text": "and i've given it this model uri so i can go and pull my model down and deploy it on a on an optimized scikit-learn",
    "start": "882079",
    "end": "887839"
  },
  {
    "text": "server um cool uh one of the things i really really like about solon as well is is",
    "start": "887839",
    "end": "893360"
  },
  {
    "start": "889000",
    "end": "971000"
  },
  {
    "text": "this ability to do complex inference graphs um so we appreciate that like not every",
    "start": "893360",
    "end": "898800"
  },
  {
    "text": "model just runs on its own standalone often you know you have input transformers output transformers you",
    "start": "898800",
    "end": "904320"
  },
  {
    "text": "might have uh you might want to chain models together um i've given a kind of example up here of how you might do",
    "start": "904320",
    "end": "912079"
  },
  {
    "text": "maybe two models running in tandem uh if i kind of think of an example for this maybe you know that if you look down the",
    "start": "912079",
    "end": "918800"
  },
  {
    "text": "left-hand side here this model two might be a you know some sort of like image",
    "start": "918800",
    "end": "923839"
  },
  {
    "text": "classification where i'm doing some you know transformation on on it beforehand",
    "start": "923839",
    "end": "929519"
  },
  {
    "text": "and then some output particularly nlp we see a lot of use cases for that you know converting to word embeddings running a",
    "start": "929519",
    "end": "934880"
  },
  {
    "text": "model and then you know converting back to some sort of text on the on the output um and then maybe model one here",
    "start": "934880",
    "end": "941199"
  },
  {
    "text": "just takes standard tabular data as in numerical inputs uh and then you can have things like combiners right which",
    "start": "941199",
    "end": "947440"
  },
  {
    "text": "take the outputs of multiple models and then feed those back so it allows you to define the whole",
    "start": "947440",
    "end": "955839"
  },
  {
    "text": "machine learning like almost business logic within your deployment which means your end users only have to hit one api",
    "start": "956000",
    "end": "963120"
  },
  {
    "text": "and they get all the results they're expecting and there's i've got a kind of example of how that you'd build that up within",
    "start": "963120",
    "end": "969360"
  },
  {
    "text": "the crd um a couple other things so it integrates",
    "start": "969360",
    "end": "974560"
  },
  {
    "start": "971000",
    "end": "1012000"
  },
  {
    "text": "really nicely with everything in the kubernetes stack um you know we use a lot for you know request",
    "start": "974560",
    "end": "980160"
  },
  {
    "text": "logging response logging etcetera we use elasticsearch for that but you you could plug and play different databases um you",
    "start": "980160",
    "end": "986720"
  },
  {
    "text": "can do distributed tracing with jaeger um it automatically will scrape metrics from your models and um and the",
    "start": "986720",
    "end": "993839"
  },
  {
    "text": "containers underneath pass them to a prometheus server um we do stream processing integrations",
    "start": "993839",
    "end": "999759"
  },
  {
    "text": "with k native and kafka and then you can integrate with batch workflow",
    "start": "999759",
    "end": "1005920"
  },
  {
    "text": "managers like argo or we actually have a cli tool that allows you to kind of run batch processing workloads too um okay",
    "start": "1005920",
    "end": "1014000"
  },
  {
    "start": "1012000",
    "end": "1073000"
  },
  {
    "text": "so that's enough talking i'm going to show you something a bit of a demo um i'm going to have to squat a little bit",
    "start": "1014000",
    "end": "1019680"
  },
  {
    "text": "here because i'm too tall for this lectern so maybe maybe i'll go for a power stance",
    "start": "1019680",
    "end": "1024959"
  },
  {
    "text": "um okay uh just before i start i'll talk about the the use case so obviously i",
    "start": "1024959",
    "end": "1030079"
  },
  {
    "text": "can't give you any of walt's actual machine learning models and their their data and kind of show it live um so i've",
    "start": "1030079",
    "end": "1036400"
  },
  {
    "text": "tried to pick something that's kind of similar so we're gonna do some food classification with tensorflow um",
    "start": "1036400",
    "end": "1041520"
  },
  {
    "text": "they're basically a bunch of images that have been taken at different restaurants by people's phones and so on so you'll see that the quality varies quite a lot",
    "start": "1041520",
    "end": "1048880"
  },
  {
    "text": "and maybe the motivation for doing this is that if you can classify every image that",
    "start": "1048880",
    "end": "1054799"
  },
  {
    "text": "you're using on you know like a food ordering app right and have the metadata attached to it you",
    "start": "1054799",
    "end": "1060240"
  },
  {
    "text": "can then build up a profile of a user you know as i click on certain images and go oh yeah that looks nice right et",
    "start": "1060240",
    "end": "1065760"
  },
  {
    "text": "cetera i can be storing that metadata and use it for recommender systems later on um so let's dive into that right and",
    "start": "1065760",
    "end": "1074000"
  },
  {
    "text": "i'll see can you can someone at the back tell me if you can see this all right do i need to make a bit bigger okay let's i don't know what this means",
    "start": "1074000",
    "end": "1082080"
  },
  {
    "text": "thumbs up i think big bigger okay cool right let's go bigger uh",
    "start": "1082080",
    "end": "1087200"
  },
  {
    "text": "all right fine so i'm gonna do i'm just gonna get something training first so i've got this again i'll make",
    "start": "1087200",
    "end": "1093440"
  },
  {
    "text": "this one bigger too presumably that's okay okay great um this is again a simple kind of",
    "start": "1093440",
    "end": "1100160"
  },
  {
    "text": "convolutional neural net um the things to highlight in this code rather than going through it in detail",
    "start": "1100160",
    "end": "1105760"
  },
  {
    "text": "is this line here is really cool right so if you import ml flow you can you can actually just run one line of code there",
    "start": "1105760",
    "end": "1112080"
  },
  {
    "text": "this mlflow dot keras to auto log um it has this auto log feature for a bunch of frameworks and that will then",
    "start": "1112080",
    "end": "1118080"
  },
  {
    "text": "automatically track a bunch of parameters um metrics around your model and then store your model into one of",
    "start": "1118080",
    "end": "1124640"
  },
  {
    "text": "those reusable ml flow model formats as well um so that's enough right to do",
    "start": "1124640",
    "end": "1130000"
  },
  {
    "text": "kind of 80 of what you want to do you can also do things like you know log",
    "start": "1130000",
    "end": "1135200"
  },
  {
    "text": "maybe manually log some parameters in there so i'm going to do batch size epochs drop out et cetera",
    "start": "1135200",
    "end": "1141360"
  },
  {
    "text": "and then what i'm going to do actually is i'll just run this so",
    "start": "1141360",
    "end": "1146960"
  },
  {
    "text": "okay and that might that i think how many epochs did i do only three so we're good that should take a little bit of",
    "start": "1147919",
    "end": "1154240"
  },
  {
    "start": "1154000",
    "end": "1170000"
  },
  {
    "text": "time um okay and you can see the kind of examples of images that are being classified by this um you know some are",
    "start": "1154240",
    "end": "1161200"
  },
  {
    "text": "good some are a little bit more ropey right you know down here this pizza image i mean that kind of could be",
    "start": "1161200",
    "end": "1166640"
  },
  {
    "text": "anything maybe if you look closely you can see it's it's a pizza",
    "start": "1166640",
    "end": "1171679"
  },
  {
    "start": "1170000",
    "end": "1628000"
  },
  {
    "text": "cool so we'll let that run and while we do i'm going to jump in and show you",
    "start": "1171760",
    "end": "1176880"
  },
  {
    "text": "the ml flow ui so if i refresh this um you should see",
    "start": "1176880",
    "end": "1182559"
  },
  {
    "text": "that hang on any second is that running cool yeah it's already picked up my new",
    "start": "1182559",
    "end": "1188799"
  },
  {
    "text": "training run even though it hasn't finished so if i jump back down you can see it's still training but it's picked up some of those",
    "start": "1188799",
    "end": "1194480"
  },
  {
    "text": "parameters i logged already um and then if i jump in and show you something you know from a previous run",
    "start": "1194480",
    "end": "1201360"
  },
  {
    "text": "the cool thing here is i can look at like i can compare metrics across you know over time all of this is",
    "start": "1201360",
    "end": "1207679"
  },
  {
    "text": "automatically logged for me by ml flow tracking so if i want to look at maybe you know the accuracy",
    "start": "1207679",
    "end": "1213280"
  },
  {
    "text": "versus the validation accuracy we can see for this model um let's put this for epochs right",
    "start": "1213280",
    "end": "1219039"
  },
  {
    "text": "rather than time maybe after about 30 epochs i'm just overfitting because you can see",
    "start": "1219039",
    "end": "1224559"
  },
  {
    "text": "the the accuracy of the model's going up but the validation accuracy is not um so that maybe you know that's a good",
    "start": "1224559",
    "end": "1230720"
  },
  {
    "text": "indicator that i probably don't need to train for that long but there's maybe some data enhancements i need to do",
    "start": "1230720",
    "end": "1237200"
  },
  {
    "text": "i can also do cool things like compare runs so if i go in and you know grab two of them",
    "start": "1237200",
    "end": "1242720"
  },
  {
    "text": "let's go compare those and then again you know look at the different accuracy or loss metrics in",
    "start": "1242720",
    "end": "1248559"
  },
  {
    "text": "there you can see so this one that had a larger batch size as we'd expect right kind of converges on a reasonable",
    "start": "1248559",
    "end": "1254960"
  },
  {
    "text": "solution much quicker because the machine learning model has more information about",
    "start": "1254960",
    "end": "1260240"
  },
  {
    "text": "to read with each each batch um cool so let's jump back down here",
    "start": "1260240",
    "end": "1265280"
  },
  {
    "text": "we'll see that my model's completed and actually wait just quickly before i do i'm gonna jump in here and show you that",
    "start": "1265280",
    "end": "1271520"
  },
  {
    "text": "you know again you get all the same information for that run i just completed it's also saved a summary of",
    "start": "1271520",
    "end": "1276720"
  },
  {
    "text": "the model um you know the actual model topology that's been built within keras i'm also i'm just going to copy",
    "start": "1276720",
    "end": "1283120"
  },
  {
    "text": "this run id because then what i want to show you is if i um let's do",
    "start": "1283120",
    "end": "1290000"
  },
  {
    "text": "hang on python upload model and i'm just going to give it this run id so what that's",
    "start": "1290000",
    "end": "1296080"
  },
  {
    "text": "going to do is just saves me a bit of navigating around the file system but that's copying",
    "start": "1296080",
    "end": "1301120"
  },
  {
    "text": "that saved mlflow model into a mineo",
    "start": "1301120",
    "end": "1306159"
  },
  {
    "text": "storage bucket on my cube cluster and then from putting it there what that means is",
    "start": "1306159",
    "end": "1312720"
  },
  {
    "text": "i can now create a seldom deployment and deploy that straight on top of kubernetes so let's have a look at what one of those looks like right so let's",
    "start": "1312720",
    "end": "1319600"
  },
  {
    "text": "go uh hang on i'll play in a second okay so this is",
    "start": "1319600",
    "end": "1326159"
  },
  {
    "text": "uh for the sake of this demo right there's one already running this is my model that's already up and running my",
    "start": "1326159",
    "end": "1332480"
  },
  {
    "text": "food classifier that already can accept predictions and you can see in here you know some of the obvious things i",
    "start": "1332480",
    "end": "1338480"
  },
  {
    "text": "pointed out earlier like i'm using this ml flow server because it's using that ml flow model format",
    "start": "1338480",
    "end": "1344640"
  },
  {
    "text": "and then i've passed it this uh model uri which basically is saying like where where is my model stored um",
    "start": "1344640",
    "end": "1351280"
  },
  {
    "text": "you know go and grab that and then importantly here as well like because that's in an s3 bucket or in this case",
    "start": "1351280",
    "end": "1357039"
  },
  {
    "text": "on mineo running on my cluster how do i actually access that so i have to pass this um secret in as well um but",
    "start": "1357039",
    "end": "1364559"
  },
  {
    "text": "that's basically it in terms of what you need and then you can see actually like in one of the cool things with seldom",
    "start": "1364559",
    "end": "1370320"
  },
  {
    "text": "core is you can um customize like a whole load of the crd underneath so you know if you want to",
    "start": "1370320",
    "end": "1376559"
  },
  {
    "text": "write the full container spec you can do that and in this case i actually needed to because what ml flow does when you",
    "start": "1376559",
    "end": "1383440"
  },
  {
    "text": "deploy an mlf model is it recreates the whole environment from scratch so for this model it's going to go uh in fact",
    "start": "1383440",
    "end": "1390960"
  },
  {
    "text": "i'll show you it in the ui so like the model we just trained you can see these artifacts and there's this",
    "start": "1390960",
    "end": "1397360"
  },
  {
    "text": "like ml model file and there's a condo.yaml it uses this with all my python dependencies to recreate the",
    "start": "1397360",
    "end": "1403440"
  },
  {
    "text": "actual uh environment that the model is going to run in the reason i then need to set timeouts",
    "start": "1403440",
    "end": "1409760"
  },
  {
    "text": "on my liveness probe and my readiness probe is that that can take a while particularly on you know i've got",
    "start": "1409760",
    "end": "1414880"
  },
  {
    "text": "rubbish wi-fi here in this venue and it's installing tensorflow from scratch right that's a pretty big download um so",
    "start": "1414880",
    "end": "1422480"
  },
  {
    "text": "i can do cool things like that what i'm going to do for this one is i'm just going to go",
    "start": "1422480",
    "end": "1428400"
  },
  {
    "text": "show you how maybe for a new one that we've trained which was only three epochs and probably pretty rubbish model",
    "start": "1428400",
    "end": "1433600"
  },
  {
    "text": "let's not roll it straight out into production let's do a shadow deployment instead um so if you look at this one",
    "start": "1433600",
    "end": "1439679"
  },
  {
    "text": "again it's the same as the deployment we had earlier but i've added another component spec down here with the same",
    "start": "1439679",
    "end": "1446320"
  },
  {
    "text": "kind of liveness probe thresholds um and then this time i'm going to let's",
    "start": "1446320",
    "end": "1452080"
  },
  {
    "text": "put in that new run id so this is the new model we just created and just pushed to a bucket",
    "start": "1452080",
    "end": "1459039"
  },
  {
    "text": "and then all i need to do to specify that it's a shadow deployment is just this one",
    "start": "1459039",
    "end": "1464320"
  },
  {
    "text": "little line here so i can do shadow true right and now what's going to happen is all",
    "start": "1464320",
    "end": "1470320"
  },
  {
    "text": "the traffic that goes to the main model is going to be replicated to the other one um but then the output's just going to be discarded so if i'm monitoring it",
    "start": "1470320",
    "end": "1477360"
  },
  {
    "text": "with you know prometheus and looking at latency things like that i can make sure the model performs well before i actually promote it um i could equally",
    "start": "1477360",
    "end": "1484480"
  },
  {
    "text": "you know if i wanted i could do really cool things like um just say you know traffic",
    "start": "1484480",
    "end": "1490320"
  },
  {
    "text": "20 right and then you know go and change the traffic up here as well to be",
    "start": "1490320",
    "end": "1496080"
  },
  {
    "text": "traffic 80 right and so i can i can do kind of a b tests and things like that on my model too",
    "start": "1496159",
    "end": "1501360"
  },
  {
    "text": "let me just just undo that",
    "start": "1501360",
    "end": "1506159"
  },
  {
    "text": "okay so shadow true okay fine and then how do i how do i actually deploy that",
    "start": "1506720",
    "end": "1511840"
  },
  {
    "text": "model so that's just a simple cube ctl apply deploy shadow",
    "start": "1511840",
    "end": "1518400"
  },
  {
    "text": "right and now if we look at my you can see that it's creating so my",
    "start": "1518400",
    "end": "1524000"
  },
  {
    "text": "food classifier defaults already there but it's creating this shadow deployment as well",
    "start": "1524000",
    "end": "1529520"
  },
  {
    "text": "which when it receives requests is going to send to you as well so finally let's just send a request",
    "start": "1529520",
    "end": "1535520"
  },
  {
    "text": "have a bit of fun check if my classifier is any good let's go grab an image",
    "start": "1535520",
    "end": "1540840"
  },
  {
    "text": "um i don't know who here likes tiramisu that was one of the classes of food in",
    "start": "1540840",
    "end": "1546000"
  },
  {
    "text": "there all right this guy does nice um let's grab an image",
    "start": "1546000",
    "end": "1551440"
  },
  {
    "text": "hopefully it's going to get this right but let's copy the image url and then what",
    "start": "1551440",
    "end": "1557200"
  },
  {
    "text": "we're going to do is go predict",
    "start": "1557200",
    "end": "1562559"
  },
  {
    "text": "and i'm just going to give it the url and it's going to pull that image down oh maybe not what have i done wrong oh",
    "start": "1562559",
    "end": "1569520"
  },
  {
    "text": "forbidden it that url doesn't like me okay let's try another one",
    "start": "1569520",
    "end": "1576240"
  },
  {
    "text": "uh actually",
    "start": "1578720",
    "end": "1582039"
  },
  {
    "text": "okay let's try this one see if i can pull that one",
    "start": "1584720",
    "end": "1588640"
  },
  {
    "text": "okay yeah that looks good oh i still didn't like it all right well this is what happens when you do stuff",
    "start": "1590080",
    "end": "1596400"
  },
  {
    "text": "live and try and grab images i can try one final image but then if this doesn't work",
    "start": "1596400",
    "end": "1602480"
  },
  {
    "text": "i did copy the image address yep this doesn't work we'll just move on",
    "start": "1602480",
    "end": "1607919"
  },
  {
    "text": "the reason i want to show this is because the output of the classifier is kind of cool you get to see like live",
    "start": "1610000",
    "end": "1615039"
  },
  {
    "text": "inference requests oh no i've broken something now um oh my that's all",
    "start": "1615039",
    "end": "1620320"
  },
  {
    "text": "okay nevermind i won't fix it now because we don't have time but the issue there is that i haven't pulled forwarded",
    "start": "1620320",
    "end": "1625360"
  },
  {
    "text": "from my cluster to my device um so what i do is i'll hand over to steven so we don't",
    "start": "1625360",
    "end": "1631760"
  },
  {
    "start": "1628000",
    "end": "1716000"
  },
  {
    "text": "run out of time yeah and you can finish off yeah thank you thank you so yeah maybe the very",
    "start": "1631760",
    "end": "1637200"
  },
  {
    "text": "important question on that site is how exactly do we scale the ml platform because that was you know the title of",
    "start": "1637200",
    "end": "1642640"
  },
  {
    "text": "the talk and on our side yeah so our data scientist our customers and what we have now is we have dedicated",
    "start": "1642640",
    "end": "1649440"
  },
  {
    "text": "communities clusters only for machine learning we have ones running in the development environment production",
    "start": "1649440",
    "end": "1654480"
  },
  {
    "text": "environments and then we have other ones as well so then you work on your machine learning and you use a lot of a lot of",
    "start": "1654480",
    "end": "1660720"
  },
  {
    "text": "different data a lot of different nodes well then you want to impact the the rest of the production data so that's",
    "start": "1660720",
    "end": "1666399"
  },
  {
    "text": "pretty good uh yeah common tooling and infrastructure as well as i said our",
    "start": "1666399",
    "end": "1671840"
  },
  {
    "text": "yamal to terraform for example is one of the best examples we have automation yeah you automatically deploy",
    "start": "1671840",
    "end": "1678320"
  },
  {
    "text": "to production automatically deploy to shadow mode as well automatically do a lot of things a lot of automation",
    "start": "1678320",
    "end": "1685039"
  },
  {
    "text": "and the last one is really predicting the future needs of data scientists at the moment we only have one model",
    "start": "1685039",
    "end": "1690720"
  },
  {
    "text": "that is using gpus for example and but maybe in a year we'll have i don't know five five different models",
    "start": "1690720",
    "end": "1696960"
  },
  {
    "text": "and so they're gonna use gpus to train the model but then how do you how do you run inference with gpus do you need gpus",
    "start": "1696960",
    "end": "1703919"
  },
  {
    "text": "to run inference or do you use cpus that's like one thing that you try to predict so then when they arrive and",
    "start": "1703919",
    "end": "1709279"
  },
  {
    "text": "then a year later they're like okay we need that we're like okay we try it out you know you should do it like that so that's something that we really try to",
    "start": "1709279",
    "end": "1715440"
  },
  {
    "text": "have and i clearly don't have time to talk",
    "start": "1715440",
    "end": "1720480"
  },
  {
    "start": "1716000",
    "end": "1726000"
  },
  {
    "text": "about that but that's a very big model we had running and we're totally blind on it",
    "start": "1720480",
    "end": "1726559"
  },
  {
    "start": "1726000",
    "end": "1762000"
  },
  {
    "text": "but what we did now is that it's running on the infra and because of that now we can see like number of requests per",
    "start": "1726559",
    "end": "1732399"
  },
  {
    "text": "second it receives and the latency the memory usage but one thing that is very",
    "start": "1732399",
    "end": "1737600"
  },
  {
    "text": "cool is like we use kafka a lot and now every response you can you can log it to kafka automatically so then",
    "start": "1737600",
    "end": "1743919"
  },
  {
    "text": "you can monitor the quality of your model and then you can we use snowflake behind the scene so then you go to",
    "start": "1743919",
    "end": "1749279"
  },
  {
    "text": "snowflake and you can compare your prediction to the real values and you can do that easily because everything is",
    "start": "1749279",
    "end": "1754399"
  },
  {
    "text": "locked to kafka and yeah you can run shadow mode a b test and different things",
    "start": "1754399",
    "end": "1760000"
  },
  {
    "text": "so that's that's what we have now and yeah future work",
    "start": "1760000",
    "end": "1765039"
  },
  {
    "start": "1762000",
    "end": "1792000"
  },
  {
    "text": "i will finish with that we won't really want to work on the ergonomics of the platform data scientists for now still",
    "start": "1765039",
    "end": "1770320"
  },
  {
    "text": "have to struggle with cube ctl and everything in some cases we want to get rid of that so we want to make",
    "start": "1770320",
    "end": "1775919"
  },
  {
    "text": "communities completely yeah we want to hide it from them if they're not interested and really have an integration with the",
    "start": "1775919",
    "end": "1782399"
  },
  {
    "text": "rest of our tools we have an experimentation platform so we want to have full integration with that so we can run a b test and then monitor the",
    "start": "1782399",
    "end": "1788720"
  },
  {
    "text": "result and everything and i will finish on that because we're out of time so yeah thank you cool",
    "start": "1788720",
    "end": "1795760"
  },
  {
    "start": "1792000",
    "end": "1959000"
  },
  {
    "text": "all right any questions any questions",
    "start": "1795760",
    "end": "1802720"
  },
  {
    "text": "anybody got a question anybody hungry jeremy sounds good too please okay we've got a couple questions here question uh you down here",
    "start": "1802720",
    "end": "1809919"
  },
  {
    "text": "sorry sorry sir michael from scotland hi steven nice to meet you nice to meet",
    "start": "1809919",
    "end": "1814960"
  },
  {
    "text": "you hugo from vmware um quick question on your yamo to terraform tool why are",
    "start": "1814960",
    "end": "1820640"
  },
  {
    "text": "you not open sourcing that that's one thing we want to do but for now it's very very targeted",
    "start": "1820640",
    "end": "1826000"
  },
  {
    "text": "towards our infra but it's one of the goal here to to open source it at one point because i think it's pretty cool",
    "start": "1826000",
    "end": "1831440"
  },
  {
    "text": "for data scientists too",
    "start": "1831440",
    "end": "1835320"
  },
  {
    "text": "hi there um when you put model uri on the spec are you actually fetching the",
    "start": "1836480",
    "end": "1841760"
  },
  {
    "text": "model and then running in your inference server there so you like it will provide for you like the",
    "start": "1841760",
    "end": "1848320"
  },
  {
    "text": "uh intel um interface like http grpc for you yes yes maybe you want to yeah yeah",
    "start": "1848320",
    "end": "1855039"
  },
  {
    "text": "it it pulls that model down right and and that's something that's like mandatory within seldom core is that",
    "start": "1855039",
    "end": "1860880"
  },
  {
    "text": "like you have to provide an external uri to the model because like if your model's huge right you",
    "start": "1860880",
    "end": "1867440"
  },
  {
    "text": "don't want to be like pushing that up with a you know cube manifest right when you do it um so and we have an init",
    "start": "1867440",
    "end": "1873760"
  },
  {
    "text": "container that goes and grabs that down pulls it down make sure you've got all the artifacts before it starts running the classifier",
    "start": "1873760",
    "end": "1879600"
  },
  {
    "text": "so it could be also like a proper s3 not an em menu oh yeah yeah yeah so yeah that i mean that endpoint was one",
    "start": "1879600",
    "end": "1886080"
  },
  {
    "text": "running on my cluster but yeah just swap that for an s3 bucket you know google cloud storage whatever",
    "start": "1886080",
    "end": "1892559"
  },
  {
    "text": "it supported like 40 different file types because we use our clone under the covers",
    "start": "1892559",
    "end": "1898080"
  },
  {
    "text": "any other questions all right",
    "start": "1899600",
    "end": "1903398"
  },
  {
    "text": "hi i'm luca nice to meet you thanks for the talk um have you ever considered using the",
    "start": "1906640",
    "end": "1912480"
  },
  {
    "text": "kubeflow stack i see a lot of overlapping in what you did and if it is why you discarded it",
    "start": "1912480",
    "end": "1921120"
  },
  {
    "text": "yeah so uh when i started we benched smart cube flow and the whole thing but i found it",
    "start": "1921120",
    "end": "1926480"
  },
  {
    "text": "to be quite hard to use especially for data scientists you know having to write all the pipelines and all the dsl and",
    "start": "1926480",
    "end": "1932080"
  },
  {
    "text": "everything related to that i find it to be quite hard even for me and i was interested in the tool so i didn't",
    "start": "1932080",
    "end": "1937440"
  },
  {
    "text": "really wanted to sell it to our data scientist because i think it would have been too completely complicated for them",
    "start": "1937440",
    "end": "1943679"
  },
  {
    "text": "so yeah that's why we didn't go with it okay folks that's unfortunately all the time we got for questions if you want to",
    "start": "1943679",
    "end": "1949679"
  },
  {
    "text": "continue the conversation we can do so outside big round of applause for this amazing talk thank you let's get a photo",
    "start": "1949679",
    "end": "1957840"
  },
  {
    "text": "nice",
    "start": "1957840",
    "end": "1960840"
  }
]