[
  {
    "text": "okay welcome to the talk on cutting climate costs with",
    "start": "400",
    "end": "6200"
  },
  {
    "text": "kubernetes hi I'm Shiva razai I'm Iranian immigrated to the US 11 years",
    "start": "6879",
    "end": "13480"
  },
  {
    "text": "ago and I just graduated with the Cs and psychology degree from Harvard School of",
    "start": "13480",
    "end": "20760"
  },
  {
    "text": "continuing education and I worked in many different Industries but this is my first tech job",
    "start": "20760",
    "end": "29519"
  },
  {
    "text": "as of developer relation for CED Labs so you've been warned just adjust your",
    "start": "29519",
    "end": "37399"
  },
  {
    "text": "expectations uh I am Steve Francis the CEO of sedero labs I graduated with a CS",
    "start": "37399",
    "end": "43760"
  },
  {
    "text": "degree and a law degree a lot longer than Shiva did um in Australia so I've",
    "start": "43760",
    "end": "50320"
  },
  {
    "text": "spent most of my career as an SRE U working for SAS companies and then about",
    "start": "50320",
    "end": "55440"
  },
  {
    "text": "15 years ago I started my own sess company logic monitor.com as data is in a monitoring SAS company um used by",
    "start": "55440",
    "end": "63359"
  },
  {
    "text": "PayPal Netflix ER trade at least it was at the time when I was there and while at logic monitor I worked with Andrew",
    "start": "63359",
    "end": "69439"
  },
  {
    "text": "Reinhard who was doing our migration onto kubernetes eight years ago",
    "start": "69439",
    "end": "74759"
  },
  {
    "text": "something like that um then Andrew went on to start an open source project tals",
    "start": "74759",
    "end": "80759"
  },
  {
    "text": "Linux which makes deploying and managing kubernetes easier for the work he was doing and then he started a company",
    "start": "80759",
    "end": "87320"
  },
  {
    "text": "around that about four years ago and I've been CEO of that for the last three years so now I can confidently say how",
    "start": "87320",
    "end": "93640"
  },
  {
    "text": "little I know about kubernetes but little more every day okay so we're gonna today we're",
    "start": "93640",
    "end": "100479"
  },
  {
    "text": "going to talk about um a little bit about our company and our products and",
    "start": "100479",
    "end": "105640"
  },
  {
    "text": "briefly and then go into how we can cut climate costs with kubernetes we're",
    "start": "105640",
    "end": "112360"
  },
  {
    "text": "going to do a can demo and then take questions after that so sedur lab as Steve just",
    "start": "112360",
    "end": "121360"
  },
  {
    "text": "mentioned was uh founded around Talis Linux which is a Linux distribution",
    "start": "121360",
    "end": "126799"
  },
  {
    "text": "ridden from scratch just for kubernetes and it's like Linux and kubernetes had a",
    "start": "126799",
    "end": "134200"
  },
  {
    "text": "baby and send it to Avenger training and then it came out very fabulous but only",
    "start": "134200",
    "end": "142040"
  },
  {
    "text": "does kubernetes so it is um there's no SSH no bash no um package",
    "start": "142040",
    "end": "150959"
  },
  {
    "text": "manager and it is apid driven and configured declaratively just like",
    "start": "150959",
    "end": "157680"
  },
  {
    "text": "kubernetes it's very minimal and little surface for Tech so um then the other",
    "start": "157680",
    "end": "167640"
  },
  {
    "text": "products that we have we have cedero metal which is cluster API provider for",
    "start": "167640",
    "end": "174280"
  },
  {
    "text": "bare metal creations and management of clusters using cppy the other um this",
    "start": "174280",
    "end": "182920"
  },
  {
    "text": "mic is so loud the other product that we have is s Omni which is a SAS simplified",
    "start": "182920",
    "end": "190519"
  },
  {
    "text": "uh cluster creation it's a SAS that simplifies cluster creations and",
    "start": "190519",
    "end": "196720"
  },
  {
    "text": "management on bare metal Edge devices or Cloud devices and it how it does it is by",
    "start": "196720",
    "end": "205680"
  },
  {
    "text": "booting off isos and Amis and or other image and then it goes uses API or UI to",
    "start": "205680",
    "end": "215560"
  },
  {
    "text": "create the Clusters in few clicks if you have not used it you should try it it's",
    "start": "215560",
    "end": "221640"
  },
  {
    "text": "few click it's much easier so I know from my days when I",
    "start": "221640",
    "end": "227080"
  },
  {
    "text": "worked as an SRE that the easiest way to solve scaling problems is to have a provision especially if you're talking",
    "start": "227080",
    "end": "232879"
  },
  {
    "text": "about bare metal like where you know back in my day it was like well we're going to need more servers so you order",
    "start": "232879",
    "end": "238120"
  },
  {
    "text": "them from Dell and they'll show up in two months and then it takes another two months to get power from your data center and network provision So to avoid",
    "start": "238120",
    "end": "244959"
  },
  {
    "text": "those things you just deploy more than you need at any given point in time um I know when I ran data centers for an",
    "start": "244959",
    "end": "251120"
  },
  {
    "text": "adtech company we could get by like we had Peak demands during you know us East",
    "start": "251120",
    "end": "256600"
  },
  {
    "text": "Coast morning time and so during that time all you know several hundred servers were all almost all busy running",
    "start": "256600",
    "end": "262440"
  },
  {
    "text": "it pretty high CPU load but outside of those times we could have got by with probably a quarter maybe even less of",
    "start": "262440",
    "end": "268199"
  },
  {
    "text": "the servers at any one point in time but we didn't we left all hundreds of servers running on all the time all",
    "start": "268199",
    "end": "274840"
  },
  {
    "text": "consuming energy all generating heat the heat required more cooling which consumed more electricity so not very",
    "start": "274840",
    "end": "280759"
  },
  {
    "text": "efficient um studies show us data centers contribute between 1 to 2% of all us greenhouse gases so that's a",
    "start": "280759",
    "end": "287880"
  },
  {
    "text": "pretty big opportunity to make a sizable Target um so this talk I think in is in",
    "start": "287880",
    "end": "295080"
  },
  {
    "text": "the program is about using um cluster API to solve climate change we changed",
    "start": "295080",
    "end": "300360"
  },
  {
    "text": "that we initially thought this was going to be a cppy talk but because our largest customers like Nokia do in fact",
    "start": "300360",
    "end": "307240"
  },
  {
    "text": "run sedo metal which is a Cappy provider for bare metal and so you know having nuia be able to power down part of their",
    "start": "307240",
    "end": "314520"
  },
  {
    "text": "servers and fleets great idea but we also have a whole bunch of customers that use TS just by itself without of",
    "start": "314520",
    "end": "321319"
  },
  {
    "text": "Cappy to run relatively static clusters and then we have as cha mentioned we have Omni our SAS for kubernetes and",
    "start": "321319",
    "end": "327560"
  },
  {
    "text": "Edge devices we have customers running hundreds of clusters on that so we want on something that could be more universally applicable than just cppy so",
    "start": "327560",
    "end": "335479"
  },
  {
    "text": "we can find a way to shut down exra service we want to do it for everyone so what we came up with is not cppy not",
    "start": "335479",
    "end": "341919"
  },
  {
    "text": "Cappy specific it's not even tallos specific so can you be used by everyone whether they're running sedera metal",
    "start": "341919",
    "end": "347919"
  },
  {
    "text": "Omni tals or even if they're foolish one of these other operating systems that is not t tals and if they're not doing that",
    "start": "347919",
    "end": "354960"
  },
  {
    "text": "you got to ask what are they doing so there there are various systems",
    "start": "354960",
    "end": "361560"
  },
  {
    "text": "in kubernetes that make things U dynamically adjust to the workload",
    "start": "361560",
    "end": "366800"
  },
  {
    "text": "however none of those met our design goal for this specific which is being emissions aware",
    "start": "366800",
    "end": "375280"
  },
  {
    "text": "and um broadly applicable so the options those options are the horizontal and",
    "start": "375280",
    "end": "381960"
  },
  {
    "text": "vertical po Auto scalers and cluster autoscaler which quick review of those",
    "start": "381960",
    "end": "388199"
  },
  {
    "text": "so horizontal pot AO scaler would will add or remove pod replicas in response",
    "start": "388199",
    "end": "393240"
  },
  {
    "text": "to the workload and so if there's not enough it will add if there's too many",
    "start": "393240",
    "end": "399440"
  },
  {
    "text": "um across all pods um then it will",
    "start": "399440",
    "end": "405680"
  },
  {
    "text": "remove uh the vertical pod autoscaler will change the CPU and memory request",
    "start": "405680",
    "end": "412639"
  },
  {
    "text": "and limits of a pot so it doesn't schedule more or less it just changes",
    "start": "412639",
    "end": "419160"
  },
  {
    "text": "that the specific part um and neither of these two will affect the infrastructure",
    "start": "419160",
    "end": "425199"
  },
  {
    "text": "to achieve what we want to do which uh cluster autoscaler does kind of so",
    "start": "425199",
    "end": "433160"
  },
  {
    "text": "cluster autoscaler automatically adds or removes nodes in a cluster based on all",
    "start": "433160",
    "end": "439639"
  },
  {
    "text": "pods request resources it looks for unschedulable",
    "start": "439639",
    "end": "444680"
  },
  {
    "text": "pods and we'll scale up clusters if needed it also tries to consolidate pods",
    "start": "444680",
    "end": "450840"
  },
  {
    "text": "that are currently deployed on only few no and if it can free up um but and this",
    "start": "450840",
    "end": "459960"
  },
  {
    "text": "is a large part of what we are doing here but it does it's not emission aware",
    "start": "459960",
    "end": "466400"
  },
  {
    "text": "so it doesn't check what's happening with what type of energy we're using and",
    "start": "466400",
    "end": "471919"
  },
  {
    "text": "for that we have to um it doesn't consider priorities in scheduling so we",
    "start": "471919",
    "end": "477199"
  },
  {
    "text": "have to consider that and Bar only works with cppy managed clusters like sedero",
    "start": "477199",
    "end": "482639"
  },
  {
    "text": "Metals we wanted something more broadly applicable so that's what we did",
    "start": "482639",
    "end": "491520"
  },
  {
    "text": "and yeah so we would have had to write a scheduler anyway to work with cluster Auto scalers so we decided to make it",
    "start": "491520",
    "end": "497560"
  },
  {
    "text": "simpler and just write an emissions aware Schuler um kubernetes allows you to run multiple schedulers which is a",
    "start": "497560",
    "end": "503400"
  },
  {
    "text": "good thing so you can apply specific schedulers to specific uh",
    "start": "503400",
    "end": "509800"
  },
  {
    "text": "deployments and this is good because it means you can like use the generic kubernetes scheduler to run your critical workloads and your your control",
    "start": "509800",
    "end": "516719"
  },
  {
    "text": "plane and everything like that and then your workloads that you are you know",
    "start": "516719",
    "end": "521800"
  },
  {
    "text": "possibly less critical you can apply a custom schedule for so the default scheduler yeah we're just left alone so",
    "start": "521800",
    "end": "529279"
  },
  {
    "text": "at a high level what this project does is it checks the emissions on the grid",
    "start": "529279",
    "end": "534880"
  },
  {
    "text": "in the area where your data center is and get gets back an answer of uh Z to 100 where zero is the energy all",
    "start": "534880",
    "end": "541760"
  },
  {
    "text": "renewable and clean and sunlight and 100 is the worst it's made by you know burning baby seals and owls and tearing",
    "start": "541760",
    "end": "548640"
  },
  {
    "text": "down some redwoods just for fun so then it compares the emissions that are currently going on to the Pod priority",
    "start": "548640",
    "end": "554200"
  },
  {
    "text": "and only runs pods if they have a higher priority than the than the emissions so the scheduler has three",
    "start": "554200",
    "end": "562720"
  },
  {
    "text": "components uh it has the scheduling logic uh pod manager and note manager",
    "start": "562720",
    "end": "569200"
  },
  {
    "text": "the schedule looks at pod priorities and Emissions we will go through the actual",
    "start": "569200",
    "end": "575720"
  },
  {
    "text": "process in the demo but it will decides whether a pod worth running in the face",
    "start": "575720",
    "end": "581000"
  },
  {
    "text": "of dirty energy if your pod is time critical you'll want to make it sure it",
    "start": "581000",
    "end": "586560"
  },
  {
    "text": "runs even if it costs an owl or two but given lower priority pods could",
    "start": "586560",
    "end": "593160"
  },
  {
    "text": "start when energy is clean then later",
    "start": "593160",
    "end": "598200"
  },
  {
    "text": "the energy changes is and what are you going to do so we need the Pod manager to evict those ones so it will take care",
    "start": "598200",
    "end": "606160"
  },
  {
    "text": "of that and then to save energy the Oh",
    "start": "606160",
    "end": "611440"
  },
  {
    "text": "wrong one to save energy the node manager will go check okay what is happening if it",
    "start": "611440",
    "end": "618959"
  },
  {
    "text": "the nodes are needed it will power them if they're needed they're on but if",
    "start": "618959",
    "end": "624360"
  },
  {
    "text": "they're not and they're idle it will turn them off the way you create a scheduler that",
    "start": "624360",
    "end": "631440"
  },
  {
    "text": "uh behaves differently than a default kubernetes scheduler is to use plugins",
    "start": "631440",
    "end": "636800"
  },
  {
    "text": "that can change the behavior at different extension points um sorting the que pre uh filter filter and so on",
    "start": "636800",
    "end": "645360"
  },
  {
    "text": "so if you don't overwrite the extension by using one of the plugins it will just use the default which will do the",
    "start": "645360",
    "end": "653680"
  },
  {
    "text": "Upstream scheduler code running and that's perfect that's nice that gets what we want so we just affect the ones",
    "start": "653680",
    "end": "660680"
  },
  {
    "text": "that we want and the rest will work perfectly we want to affect the prefilter plugins in this project and",
    "start": "660680",
    "end": "667480"
  },
  {
    "text": "our plug-in filter uh filters whether a pod should be uh run by comparing the",
    "start": "667480",
    "end": "675639"
  },
  {
    "text": "emission and emission level and the priority of the",
    "start": "675639",
    "end": "682639"
  },
  {
    "text": "Pod yeah so just a note about the cluster topology that's in use in this",
    "start": "682760",
    "end": "688120"
  },
  {
    "text": "demo um we're going to show off a tals Linux feature but not not just because we think it's cool but it is but it's",
    "start": "688120",
    "end": "694519"
  },
  {
    "text": "actually applicable in the energy saving use case so the way this schedule works",
    "start": "694519",
    "end": "699760"
  },
  {
    "text": "is it Powers nodes on and off by using um by connecting to the BMC the baseb management controller using",
    "start": "699760",
    "end": "706000"
  },
  {
    "text": "ipmi so in order for that to work wherever theuer is running it needs",
    "start": "706000",
    "end": "711200"
  },
  {
    "text": "network access to the BMC cards of the servers that are going to be powered on and off so normally that's not a problem",
    "start": "711200",
    "end": "716839"
  },
  {
    "text": "because clusters tend to run in the same data Center uh what we often do at sedur Labs is not that we uh Talis Linux has a",
    "start": "716839",
    "end": "724720"
  },
  {
    "text": "function called Cub span which transparently encrypts traffic within a cluster when it's needed to and this is",
    "start": "724720",
    "end": "730920"
  },
  {
    "text": "great because it means you can run your control planes and workers in multiple locations so one thing we often do is",
    "start": "730920",
    "end": "738360"
  },
  {
    "text": "you know if we're running big beefy nodes on like acinic metal um they're large and powerful and can do a whole",
    "start": "738360",
    "end": "744880"
  },
  {
    "text": "lot of cicd stuff very powerfully but they're way over for a pretty small",
    "start": "744880",
    "end": "750360"
  },
  {
    "text": "cluster that doesn't do a lot of dynamic changes so we run our control plane nodes in somewhere cheap where you can",
    "start": "750360",
    "end": "756199"
  },
  {
    "text": "get small virtual machines in Amazon or Azure or in this particular one it's in vulture and then the workers somewhere",
    "start": "756199",
    "end": "761920"
  },
  {
    "text": "else on bare metal but this also gives rise to an issue because the Cub span takes care of all",
    "start": "761920",
    "end": "769120"
  },
  {
    "text": "the traffic within the cluster but the BMC nodes aren't within the cluster so they're not part of the Cub span mesh",
    "start": "769120",
    "end": "775920"
  },
  {
    "text": "the the wire guard overlay so in this case we actually had to to run the the scheduler on a worker node that's in the",
    "start": "775920",
    "end": "782639"
  },
  {
    "text": "same data center so that it was behind the firewall so it can actually attach to the BMC nodes so that's kind of why uh it's not",
    "start": "782639",
    "end": "790839"
  },
  {
    "text": "just a simple deployment on the control plane which you would normally do so we will now deploy our new Schuler",
    "start": "790839",
    "end": "799160"
  },
  {
    "text": "and see what it does um basically these are the steps that you will need to go through to deploy this uh it's all you",
    "start": "799160",
    "end": "805120"
  },
  {
    "text": "know up there on GitHub but and there's a readme and documentation and stuff but this these are the high level steps and",
    "start": "805120",
    "end": "810720"
  },
  {
    "text": "we'll go through all these things kind of um one at a time so you will need to create a what",
    "start": "810720",
    "end": "818000"
  },
  {
    "text": "time account to um access the real time",
    "start": "818000",
    "end": "823800"
  },
  {
    "text": "energy emission in your area by an API uh what time is a nonprofit and right on their homepage",
    "start": "823800",
    "end": "832519"
  },
  {
    "text": "you can enter your zip code and it will give you which we did for Chicago this morning and it will tell you you how it",
    "start": "832519",
    "end": "839680"
  },
  {
    "text": "is and Chicago is not that bad not that great either it's on [Music]",
    "start": "839680",
    "end": "844920"
  },
  {
    "text": "68 and yeah okay um yeah so the next step",
    "start": "844920",
    "end": "850959"
  },
  {
    "text": "is creating priority classes because kubernetes loves complication or abstracting depending on how you look at",
    "start": "850959",
    "end": "856959"
  },
  {
    "text": "it you can't set priority on a pod directly you have to create a priority class and then reference that priority",
    "start": "856959",
    "end": "863600"
  },
  {
    "text": "class in your pod creation manifest so in this case we're just creating two uh",
    "start": "863600",
    "end": "868839"
  },
  {
    "text": "priority classes one is high priority with a value of 100 and one with low priority with a value of zero so the",
    "start": "868839",
    "end": "875360"
  },
  {
    "text": "high priority one is you know going to run basically no matter what the emissions are uh alows be damned and a",
    "start": "875360",
    "end": "881079"
  },
  {
    "text": "priority of zeros like only going to run if the energy is completely renewable and coming from the Sun and wind and",
    "start": "881079",
    "end": "888959"
  },
  {
    "text": "nothing else so we create the priority classes",
    "start": "888959",
    "end": "894120"
  },
  {
    "text": "and then yes and then we create the classes we just defined by the usual",
    "start": "894120",
    "end": "901040"
  },
  {
    "text": "coul apply and that will go",
    "start": "901040",
    "end": "906360"
  },
  {
    "text": "through all right so now we've defined our priority class we created our priority class we now need to say all",
    "start": "906360",
    "end": "912759"
  },
  {
    "text": "right this particular BMC is associated with this particular piece of hardware and the way we do that is we just annotate the nodes um so here we're just",
    "start": "912759",
    "end": "919680"
  },
  {
    "text": "adding in three annotation endpoints for three annotation uh values for each node",
    "start": "919680",
    "end": "925240"
  },
  {
    "text": "so the bmc's IP address and the username and password to access that particular BMC for this particular node so you do",
    "start": "925240",
    "end": "931319"
  },
  {
    "text": "this for each particular node in your cluster that you want the controller to manage we also need to edit the demon",
    "start": "931319",
    "end": "938160"
  },
  {
    "text": "set to configure it with the uh what time credentials which um you'll see here I",
    "start": "938160",
    "end": "946000"
  },
  {
    "text": "don't know if you're blocking it it's down there and then we are going to do I think San Bernandino yes I'm literally",
    "start": "946000",
    "end": "954360"
  },
  {
    "text": "in front of it and",
    "start": "954360",
    "end": "959560"
  },
  {
    "text": "that is done then we'll go to the next step",
    "start": "959560",
    "end": "966959"
  },
  {
    "text": "wait there now we can apply our demon set and we get all the fun output",
    "start": "966959",
    "end": "972839"
  },
  {
    "text": "showing that it is",
    "start": "972839",
    "end": "977839"
  },
  {
    "text": "configured okay so now we've deployed it what what happens so uh we've got our",
    "start": "978600",
    "end": "984519"
  },
  {
    "text": "Schuler um as we've said the Schuler just checks the current emissions from what time for the area you've said your",
    "start": "984519",
    "end": "990240"
  },
  {
    "text": "data center is in if there are pods that are in the scheduling queue with a priority higher than the current",
    "start": "990240",
    "end": "996079"
  },
  {
    "text": "emissions they'll pass the pre-filter and continue on with the scheduling process if there are pods that are less",
    "start": "996079",
    "end": "1003639"
  },
  {
    "text": "than the current emissions they will not pass the pre-filter um so that's all the",
    "start": "1003639",
    "end": "1009040"
  },
  {
    "text": "scheduler does and then the Pod manager basically checks any pods that are running now if they are less than the",
    "start": "1009040",
    "end": "1014519"
  },
  {
    "text": "current emissions they'll get evicted and then the node manager does does one of two things it turns nodes on or it",
    "start": "1014519",
    "end": "1020440"
  },
  {
    "text": "turns nodes off it will turn a node on if there are pods that have passed theuer and waiting to be run but there's",
    "start": "1020440",
    "end": "1026720"
  },
  {
    "text": "nowhere to run them if that's the case it'll say all right great schedule has decided these pods are worth running given the current emissions level I",
    "start": "1026720",
    "end": "1032678"
  },
  {
    "text": "should turn on another machine and it'll do that and then it'll also say okay if the Pod manager has decided all the",
    "start": "1032679",
    "end": "1038558"
  },
  {
    "text": "nodes on this machine are worthy of being evicted I can shut down that",
    "start": "1038559",
    "end": "1043319"
  },
  {
    "text": "machine to apply this Schuler to deployment we just need to select the",
    "start": "1044319",
    "end": "1049360"
  },
  {
    "text": "scheduler and uh set the PO priority in the deployment yo file",
    "start": "1049360",
    "end": "1056360"
  },
  {
    "text": "which okay let's play it",
    "start": "1056360",
    "end": "1060400"
  },
  {
    "text": "again y so we're just adding a schule name and a priority class name the schule name is the schule we defined",
    "start": "1065640",
    "end": "1072120"
  },
  {
    "text": "before and the priority class is in this case high priority okay then when we deploy the high",
    "start": "1072120",
    "end": "1078960"
  },
  {
    "text": "priority will cause it to be scheduled regardless of emissions and if there is",
    "start": "1078960",
    "end": "1084120"
  },
  {
    "text": "nowhere to run it will cause the node manager to power on another node and",
    "start": "1084120",
    "end": "1089440"
  },
  {
    "text": "this exactly what we just explained um we can see that one of the",
    "start": "1089440",
    "end": "1095720"
  },
  {
    "text": "nodes at the bottom I don't know if you can see but we see right down here it's um it switched",
    "start": "1095720",
    "end": "1103960"
  },
  {
    "text": "from not ready scheduling disabled to um ready and in response to the need to run",
    "start": "1103960",
    "end": "1110320"
  },
  {
    "text": "the spot okay another one so um yet a low",
    "start": "1110320",
    "end": "1119799"
  },
  {
    "text": "priority pod which just got deployed the workload was created and then now we look at the the Pod",
    "start": "1119799",
    "end": "1126559"
  },
  {
    "text": "status uh we will see the high priority one is running the low priority workload is still pending so now if we describe",
    "start": "1126559",
    "end": "1133360"
  },
  {
    "text": "that low priority pod why is it still pending you may ask and zip on to the next",
    "start": "1133360",
    "end": "1140600"
  },
  {
    "text": "one okay so we can see it it failed scheduling that's why it's stuck in pending because zero of the nodes are",
    "start": "1140600",
    "end": "1147720"
  },
  {
    "text": "available because the Pod priority of zero is lower than the emissions index of 44 so in this case had the Pod",
    "start": "1147720",
    "end": "1154360"
  },
  {
    "text": "priority had a priority of 50 it would have run if it had anything less than 44 it uh doesn't pass scheduling pre-filter",
    "start": "1154360",
    "end": "1161440"
  },
  {
    "text": "so it uh fail scheduling and here we see if each",
    "start": "1161440",
    "end": "1168320"
  },
  {
    "text": "change the priority of the running deployment to low priority it is",
    "start": "1168320",
    "end": "1175520"
  },
  {
    "text": "evicted low priority and get uh shows",
    "start": "1175520",
    "end": "1181320"
  },
  {
    "text": "that think we're doing get y now and we'll show both pods are now",
    "start": "1181320",
    "end": "1188480"
  },
  {
    "text": "pending and because that's now freed up a node after a few moments it will um the",
    "start": "1188480",
    "end": "1197039"
  },
  {
    "text": "node is powered down and it is now idle then notes switches from ready to not",
    "start": "1197039",
    "end": "1202720"
  },
  {
    "text": "ready with scheduling disabled we go it just changed here so that pod that node no",
    "start": "1202720",
    "end": "1211200"
  },
  {
    "text": "longer had any anything running on it so the node manager said great I can shut you down save",
    "start": "1211200",
    "end": "1217559"
  },
  {
    "text": "energy and okay so we kind of this is a",
    "start": "1217760",
    "end": "1223120"
  },
  {
    "text": "part of the project that still needs work um we used a poor man's bin packing system uh we take advantage of two",
    "start": "1223120",
    "end": "1229520"
  },
  {
    "text": "different attributes of the kubernetes scheduler here the default Q sort and a different option for node resources fit",
    "start": "1229520",
    "end": "1236159"
  },
  {
    "text": "attributes so our scheduler uses the most allocated option for node resources fit uh this basically means it's it's",
    "start": "1236159",
    "end": "1244080"
  },
  {
    "text": "sticks things on machines that already have other things on them normally by default kubernetes will spread out the",
    "start": "1244080",
    "end": "1249400"
  },
  {
    "text": "workload amongst all the the workers in the in the cluster the most allocated option says if you've got whatever say",
    "start": "1249400",
    "end": "1255880"
  },
  {
    "text": "five nodes and one of them has a drob running on on it and you get another deployment it'll put it on the same",
    "start": "1255880",
    "end": "1261400"
  },
  {
    "text": "machine if it'll fit so it tries to put things where they're most allocated",
    "start": "1261400",
    "end": "1267280"
  },
  {
    "text": "um and the other attribute is the default scheduling system schedules high",
    "start": "1267280",
    "end": "1272720"
  },
  {
    "text": "priority Jobs first that's just we didn't do anything for that that's just default kubernetes",
    "start": "1272720",
    "end": "1277840"
  },
  {
    "text": "default priority sort so basically if you turn on a cluster and there's a variety of high priority jobs they'll",
    "start": "1277840",
    "end": "1283240"
  },
  {
    "text": "all get scheduled first and with the most allocated they'll all get scheduled first on the same note",
    "start": "1283240",
    "end": "1288799"
  },
  {
    "text": "this means you'll get some nodes that have all the high priority jobs the medium priority jobs and the low priority jobs and then when the low",
    "start": "1288799",
    "end": "1294559"
  },
  {
    "text": "priority jobs get evicted that will tend to free up an entire machine um so that's kind of like a poor",
    "start": "1294559",
    "end": "1301039"
  },
  {
    "text": "man spin packing it's clearly not perfect it's not Dynamic if you you know it doesn't take account of taints and",
    "start": "1301039",
    "end": "1307840"
  },
  {
    "text": "tolerations and resource constraints and some things having gpus and different machine classes but you know it's a it",
    "start": "1307840",
    "end": "1314080"
  },
  {
    "text": "works for kind of if your workloads are fairly consistent your clust is consistent it'll it'll work so it's",
    "start": "1314080",
    "end": "1321200"
  },
  {
    "text": "definitely not perfect so that's um kind of it things to",
    "start": "1321200",
    "end": "1326960"
  },
  {
    "text": "improve everything bin packing yes certainly um we don't have full",
    "start": "1326960",
    "end": "1332960"
  },
  {
    "text": "integration with Omni at the moment uh or cluster API or the cluster Auto scaler but these are things we can can",
    "start": "1332960",
    "end": "1339960"
  },
  {
    "text": "add in we and we plan on doing things like this certainly and we'd love to get ideas from the",
    "start": "1339960",
    "end": "1346760"
  },
  {
    "text": "community um um yeah so good use cases for this would be generally any bare",
    "start": "1346760",
    "end": "1353320"
  },
  {
    "text": "metal workload that has a periodic demand uh things that can be time shifted things that aren't time critical",
    "start": "1353320",
    "end": "1360039"
  },
  {
    "text": "um not things that are dealing with interaction with humans because they",
    "start": "1360039",
    "end": "1366480"
  },
  {
    "text": "tend to need to happen when the humans need it but anything that's like a batch job um Bank financing that happens after",
    "start": "1366480",
    "end": "1372440"
  },
  {
    "text": "hours if it needs to happen in a you know 12-hour window you'd rather schedule it in that 12 hours",
    "start": "1372440",
    "end": "1378480"
  },
  {
    "text": "in California that's usually in the daytime that's when solar energy is online and not kind of late at night um",
    "start": "1378480",
    "end": "1385520"
  },
  {
    "text": "that's what we got and then now we can take questions we change the slide",
    "start": "1385520",
    "end": "1391080"
  },
  {
    "text": "to the well you we will have access all of this uh links and we have a happy",
    "start": "1391080",
    "end": "1397799"
  },
  {
    "text": "hour after but we'll take questions now yeah the the GitHub repository is there that's where the project is we would",
    "start": "1397799",
    "end": "1403240"
  },
  {
    "text": "love you to rate our session highly but you can rate it lowly too um",
    "start": "1403240",
    "end": "1409640"
  },
  {
    "text": "higher would be better um and we are doing a customer mixer custom customer",
    "start": "1409640",
    "end": "1415840"
  },
  {
    "text": "mixer after this at Simone's Bar in an hour um which is not far away but we would also love to open it up and hear",
    "start": "1415840",
    "end": "1421360"
  },
  {
    "text": "questions feedback discoveries thoughts on uh the talk we just gave yes I don't",
    "start": "1421360",
    "end": "1427480"
  },
  {
    "text": "know how the microphones work do you go to a microphone or do the microphones come to you there two one over there one over",
    "start": "1427480",
    "end": "1434440"
  },
  {
    "text": "here um does your current implementation have any any sort of uh mechanism for ensuring a maximum weight time like if",
    "start": "1434559",
    "end": "1442279"
  },
  {
    "text": "the local grid goes down okay well if the local grid goes down then your data Center's offline but",
    "start": "1442279",
    "end": "1448279"
  },
  {
    "text": "the clean energy grid okay yeah no no it doesn't that's actually that is exactly a point of improvement I had in there",
    "start": "1448279",
    "end": "1456440"
  },
  {
    "text": "um but I took it out because I thought it was a bit complicated but uh apparently it's a real use case yes no",
    "start": "1456440",
    "end": "1461880"
  },
  {
    "text": "what what what we were thinking is having the priority of a job kind of increment over time so if you schedule a",
    "start": "1461880",
    "end": "1467559"
  },
  {
    "text": "job 10 it'll only work when the energyy is clean but if the energy grid never",
    "start": "1467559",
    "end": "1472679"
  },
  {
    "text": "gets that clean your job's never going to get scheduled so you may want a process that says jobs that are using",
    "start": "1472679",
    "end": "1478399"
  },
  {
    "text": "this scheduler that have a low priority every 10 minutes their priority goes up by one and eventually it'll get run but",
    "start": "1478399",
    "end": "1485760"
  },
  {
    "text": "that that's not implemented at the moment yeah thank you oh I should look at the slack too",
    "start": "1485760",
    "end": "1491679"
  },
  {
    "text": "what uh what sort of audit Trail exists so I can see you know what the impact",
    "start": "1491679",
    "end": "1497559"
  },
  {
    "text": "was and sort of try to understand hey we were able to save this much energy or",
    "start": "1497559",
    "end": "1504480"
  },
  {
    "text": "some Metric that I can show back that says Yeah we actually did something and it met and it worked yeah um that's a",
    "start": "1504480",
    "end": "1511600"
  },
  {
    "text": "great idea that's a great idea I don't know that we would know that information from within the kubernetes space but you",
    "start": "1511600",
    "end": "1518559"
  },
  {
    "text": "know if you're monitoring your power drawer within your data center sure you can definitely do it from that side um",
    "start": "1518559",
    "end": "1523919"
  },
  {
    "text": "we we the the system can report out you know this number of nodes were powered on at this time but it doesn't know if it's a you know 10,000 watt server or a",
    "start": "1523919",
    "end": "1531440"
  },
  {
    "text": "500 watt server",
    "start": "1531440",
    "end": "1534720"
  },
  {
    "text": "so uh we're using it's not polling it's using",
    "start": "1537000",
    "end": "1543600"
  },
  {
    "text": "the yes yeah we",
    "start": "1544919",
    "end": "1548600"
  },
  {
    "text": "do yes yeah so the the question was that if we're using logging events then those metrics are things we can use yeah",
    "start": "1550600",
    "end": "1558960"
  },
  {
    "text": "hi um did you consider using Carpenter um as the whole pod life cycle like um",
    "start": "1558960",
    "end": "1565200"
  },
  {
    "text": "shutting them down and bring up new notes uh no but I can't tell you why I I",
    "start": "1565200",
    "end": "1572440"
  },
  {
    "text": "just said Ask Quest take questions we didn't say we're going to answer it sounds like there's a huge",
    "start": "1572440",
    "end": "1578520"
  },
  {
    "text": "overlap with like the whole um consolidation um it's an AWS thing yes",
    "start": "1578520",
    "end": "1584480"
  },
  {
    "text": "and they take money as the kind of input parameter so make it cheap you make it",
    "start": "1584480",
    "end": "1591120"
  },
  {
    "text": "green yeah yeah I mean there you know like we said there's overlaps with the cluster Auto Scala from Google there's",
    "start": "1591120",
    "end": "1596760"
  },
  {
    "text": "overlaps with Carpenter there's overlaps with um Gardner um but this",
    "start": "1596760",
    "end": "1602520"
  },
  {
    "text": "was we tried to avoid adding in excess complications um so this is kind of like",
    "start": "1602520",
    "end": "1608440"
  },
  {
    "text": "just looking at emissions and scheduling in that way and so there's certainly room to integrate it into these other",
    "start": "1608440",
    "end": "1614559"
  },
  {
    "text": "systems I think that would actually be really helpful but at the moment it is as kind of a standalone project yeah can",
    "start": "1614559",
    "end": "1620840"
  },
  {
    "text": "can you sorry oh I'm on the other mic I I oh hi sorry I jumped over here because",
    "start": "1620840",
    "end": "1626720"
  },
  {
    "text": "it was easier I was also going to say with Carpenter um I work on the carpenter team and there's not actually hooks available for something like a",
    "start": "1626720",
    "end": "1631799"
  },
  {
    "text": "data center uh like API inpoint so you'd have to have something like Cappy there to be able to call out for a fleet of",
    "start": "1631799",
    "end": "1637960"
  },
  {
    "text": "machines so you You' need both extensions of that available before you integrated with Carpenter uh my question",
    "start": "1637960",
    "end": "1643200"
  },
  {
    "text": "with the shutdowns was is there any notion of like a rack where I could tell like say I actually have you know 50",
    "start": "1643200",
    "end": "1649760"
  },
  {
    "text": "machines that can go away um and there is overhead available like I would love to be able to shut down a rack in the data center so I can turn off AC to",
    "start": "1649760",
    "end": "1656640"
  },
  {
    "text": "portions of the building because that extends how much power I'm saving for something like a batch job if I'm only",
    "start": "1656640",
    "end": "1662320"
  },
  {
    "text": "using the batch jobs for you know 50% of the data center at a time um is is any",
    "start": "1662320",
    "end": "1668600"
  },
  {
    "text": "of that also like collocation of servers available in there um not as it is but that that would be doable if you",
    "start": "1668600",
    "end": "1675200"
  },
  {
    "text": "annotated your servers with rack location and and then you had your your node manager",
    "start": "1675200",
    "end": "1680760"
  },
  {
    "text": "prioritize common rack locations first yeah right I was thinking rack and and Heights of servers cuz like the AC costs",
    "start": "1680760",
    "end": "1687480"
  },
  {
    "text": "of lower versus higher in the rack would also be valuable to say hey let's let's turn off the high ones first because",
    "start": "1687480",
    "end": "1692559"
  },
  {
    "text": "they take more to cool great idea that would be the Talk number two for the next North America or we all meet in",
    "start": "1692559",
    "end": "1698519"
  },
  {
    "text": "Paris in six months and we'll do that give us some",
    "start": "1698519",
    "end": "1704200"
  },
  {
    "text": "time are there data sources that support energy outside of North America I was",
    "start": "1704200",
    "end": "1710159"
  },
  {
    "text": "messing with the uh with it and I threw in some UK zip codes just for Fu went no",
    "start": "1710159",
    "end": "1716200"
  },
  {
    "text": "what time doesn't um I don't know if there's a equivalent API outside of the US well because also probably each",
    "start": "1716200",
    "end": "1723080"
  },
  {
    "text": "country and continent has their own metrics of and different of things so they probably it is possible to find",
    "start": "1723080",
    "end": "1730600"
  },
  {
    "text": "each individual ones and then put all right I would actually think it's probably easier in European countries",
    "start": "1730600",
    "end": "1736760"
  },
  {
    "text": "because they they have the countries tend to have unified power grids I suppose to the US which has yeah",
    "start": "1736760",
    "end": "1742080"
  },
  {
    "text": "fractionated and crazy and private Enterprise power grids yeah I think maybe green pixie uh in the UK might be",
    "start": "1742080",
    "end": "1749399"
  },
  {
    "text": "the the supplier there okay and some probably don't even have any anywhere to",
    "start": "1749399",
    "end": "1756600"
  },
  {
    "text": "report so right you you you don't get information if you're in Texas they're just",
    "start": "1756600",
    "end": "1762120"
  },
  {
    "text": "like I was thinking Iran but let's not make it political anyone",
    "start": "1762120",
    "end": "1770240"
  },
  {
    "text": "else okay well uh I would like to say who uses Talis Linux now show",
    "start": "1771039",
    "end": "1777799"
  },
  {
    "text": "fans some people good all right what are the rest of you doing you're doing it",
    "start": "1777799",
    "end": "1783519"
  },
  {
    "text": "wrong um well I guess that's it you get uh five minutes of your day back and you",
    "start": "1783519",
    "end": "1788760"
  },
  {
    "text": "can use that time to commute over to Simone's Bar or wherever your next event happens to be so but if you are with us",
    "start": "1788760",
    "end": "1794600"
  },
  {
    "text": "Simone SP at 6:30 yes thank you for coming thank you very much and thank you",
    "start": "1794600",
    "end": "1802398"
  }
]