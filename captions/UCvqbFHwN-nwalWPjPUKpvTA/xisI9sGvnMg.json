[
  {
    "text": "hello everyone welcome to this live migration production workloads from apache missus",
    "start": "80",
    "end": "5839"
  },
  {
    "text": "pass to kubernetes session um my name is maria and my name is",
    "start": "5839",
    "end": "10960"
  },
  {
    "text": "guffron and we work for nokia and nokia is a",
    "start": "10960",
    "end": "16480"
  },
  {
    "text": "producer of network equipment and software and services for our communication service providers",
    "start": "16480",
    "end": "23600"
  },
  {
    "text": "yes and as you you all know that nokia driven by its award-winning bell labs nokia is a",
    "start": "23600",
    "end": "30160"
  },
  {
    "text": "leader in the development and deployment of 5g networks um so 5g you know is going to be the",
    "start": "30160",
    "end": "36559"
  },
  {
    "text": "core of connecting people connecting world in the future or already happening actually so we're still connecting people yes we",
    "start": "36559",
    "end": "43280"
  },
  {
    "text": "do okay um this is uh the story so in in the",
    "start": "43280",
    "end": "48800"
  },
  {
    "text": "today's session what uh actually we're gonna talk about we're gonna talk about a migration story",
    "start": "48800",
    "end": "54719"
  },
  {
    "text": "our migration journey from um past to kubernetes so um",
    "start": "54719",
    "end": "61199"
  },
  {
    "text": "just imagine as you can see already also on the slide there is a gif which is showing uh that two trains",
    "start": "61199",
    "end": "67520"
  },
  {
    "text": "moving side by side first one definitely the message something that we started working on in the beginning of the project",
    "start": "67520",
    "end": "73680"
  },
  {
    "text": "so um message running all the workloads providing all the services and so on and then kubernetes comes in",
    "start": "73680",
    "end": "80799"
  },
  {
    "text": "and sharing the same platform resources right you know moving side by side and",
    "start": "80799",
    "end": "86720"
  },
  {
    "text": "then the workloads are um migrated from one platform to one train to the other one without interrupting the",
    "start": "86720",
    "end": "93920"
  },
  {
    "text": "existing services without interrupting the users so a seamless migration on a shared platform we're going to talk about this",
    "start": "93920",
    "end": "100560"
  },
  {
    "text": "today in this session our journey of messages pass to kubernetes right yeah",
    "start": "100560",
    "end": "107840"
  },
  {
    "text": "right so a little uh about our project what was it all about so the",
    "start": "107840",
    "end": "115280"
  },
  {
    "text": "mesos is only the orchestrator or kubernetes later on on top of the orchestrator what we have",
    "start": "115280",
    "end": "122079"
  },
  {
    "text": "is a big data analytics platform we provide a set of services",
    "start": "122079",
    "end": "129119"
  },
  {
    "text": "for end users who bring network specific data into our platform",
    "start": "129119",
    "end": "136480"
  },
  {
    "text": "they store this data they use several tools that the platform provides for analytics they",
    "start": "136480",
    "end": "142879"
  },
  {
    "text": "use a bi tool for presentations so this is a never stopping process of",
    "start": "142879",
    "end": "150400"
  },
  {
    "text": "different data sources coming in hundreds of users hundreds of data",
    "start": "150400",
    "end": "158000"
  },
  {
    "text": "analysts working um on the clock to provide analytics for our customers",
    "start": "158000",
    "end": "163599"
  },
  {
    "text": "and on the screen you can see the high level architecture of of this platform you can recognize",
    "start": "163599",
    "end": "169760"
  },
  {
    "text": "probably the logos of several open source components so we concentrated on using",
    "start": "169760",
    "end": "175040"
  },
  {
    "text": "open source so this is apache mesos we are not using mesosphere",
    "start": "175040",
    "end": "181519"
  },
  {
    "text": "um right so the the goal was to switch keep the platform as it is keep",
    "start": "181519",
    "end": "188480"
  },
  {
    "text": "our users uh working uh as as they were but in the in the lower level move",
    "start": "188480",
    "end": "196239"
  },
  {
    "text": "slowly from mesos seamlessly seamlessly yes from mesos to kubernetes",
    "start": "196239",
    "end": "203280"
  },
  {
    "text": "as orchestrate yeah and uh just to mention few things so this is a big data platform",
    "start": "203280",
    "end": "208560"
  },
  {
    "text": "so uh it's it's we had around five uh five production instances across four",
    "start": "208560",
    "end": "215280"
  },
  {
    "text": "different continents and thousands of users uh you know this this uh platform has",
    "start": "215280",
    "end": "220879"
  },
  {
    "text": "been also used to do things like such as mdt minimization of drive tests you know predictive analysis and and",
    "start": "220879",
    "end": "227599"
  },
  {
    "text": "process you know terabytes of uh network element data you know performance data and so on and",
    "start": "227599",
    "end": "235680"
  },
  {
    "text": "do some analysis store in the database and visualize it there are also many different successful",
    "start": "235680",
    "end": "241519"
  },
  {
    "text": "even customer cases as well where this has been this platform has been used actually for",
    "start": "241519",
    "end": "247040"
  },
  {
    "text": "doing those things um uh i think we started this project around 2015 or",
    "start": "247040",
    "end": "253519"
  },
  {
    "text": "that's correct time frame uh but by 2018 um we noticed that you know kubernetes was",
    "start": "253519",
    "end": "260799"
  },
  {
    "text": "getting a lot of industry attention and industry leaders were starting to use it it was starting to become",
    "start": "260799",
    "end": "266560"
  },
  {
    "text": "industries darling and we also wanted to adopt it for many different reasons many",
    "start": "266560",
    "end": "272560"
  },
  {
    "text": "different reasons and uh you know kubernetes was getting uh becoming the center of a growing",
    "start": "272560",
    "end": "278320"
  },
  {
    "text": "community quickly uh reaching production with maturity and um uh there were more and more features uh",
    "start": "278320",
    "end": "285520"
  },
  {
    "text": "being promised to kubernetes and while we actually noticed at the same time that messes was getting",
    "start": "285520",
    "end": "291120"
  },
  {
    "text": "less and less work done less and less features coming to messages but uh kubernetes was getting more and more",
    "start": "291120",
    "end": "296560"
  },
  {
    "text": "people working on it and also one of the reasons could be that message was at that time uh you know um",
    "start": "296560",
    "end": "304240"
  },
  {
    "text": "productized or there was an impressed version and there were less and less people working a message but also from",
    "start": "304240",
    "end": "310080"
  },
  {
    "text": "kubernetes there are things such as you know daymond sets the deployments um you know",
    "start": "310080",
    "end": "315440"
  },
  {
    "text": "deploying your application with healthcare huge amounts of penalties benefits that we saw although there were",
    "start": "315440",
    "end": "320800"
  },
  {
    "text": "some lackings on the uh but you met smart side that you couldn't run spark natively in",
    "start": "320800",
    "end": "326080"
  },
  {
    "text": "kubernetes you know managing this local persistent volumes at restaurant but the benefits were much much more",
    "start": "326080",
    "end": "332560"
  },
  {
    "text": "than you know some of those things robux yeah and uh since we did a spike",
    "start": "332560",
    "end": "338240"
  },
  {
    "text": "where we identified how we would like to migrate so uh those were not really any uh issue for",
    "start": "338240",
    "end": "343919"
  },
  {
    "text": "us because we were planning to uh run both of the platforms side by side",
    "start": "343919",
    "end": "350840"
  },
  {
    "text": "seamlessly migrate components one by one in a you know release driven manner",
    "start": "350840",
    "end": "357199"
  },
  {
    "text": "prioritized and also there were rollbacks and and um features like you know",
    "start": "357199",
    "end": "364319"
  },
  {
    "text": "since they're sharing the same platform services developers will also be able to use those things so um",
    "start": "364319",
    "end": "372160"
  },
  {
    "text": "we started uh moving to kubernetes right and yeah a little bit of a message",
    "start": "372160",
    "end": "377280"
  },
  {
    "text": "over a little bit about mesos exactly so we can take a look at the high level architecture",
    "start": "377280",
    "end": "382960"
  },
  {
    "text": "as you can see on the left side of the screen we have the basic highly available",
    "start": "382960",
    "end": "389199"
  },
  {
    "text": "cluster three master nodes two and standby one acting as leader zookeeper in the back",
    "start": "389199",
    "end": "396400"
  },
  {
    "text": "end keeping the leadership on and the worker knows who actually hosts",
    "start": "396400",
    "end": "401520"
  },
  {
    "text": "the workloads so how the workloads get there",
    "start": "401520",
    "end": "406960"
  },
  {
    "text": "all the worker nodes provide their availability uh and the resource situation to the",
    "start": "406960",
    "end": "412960"
  },
  {
    "text": "master the master hosts the application definitions through this application",
    "start": "412960",
    "end": "418080"
  },
  {
    "text": "model and when the application also called framework in mesos world is ready to launch",
    "start": "418080",
    "end": "426800"
  },
  {
    "text": "a job a task it uses a schedule so selecting the available node uh based",
    "start": "426800",
    "end": "434080"
  },
  {
    "text": "on the available resources the schedule launches an executor and the executor",
    "start": "434080",
    "end": "439280"
  },
  {
    "text": "hosts the tasks yes these tasks are actually the the application the the",
    "start": "439280",
    "end": "446000"
  },
  {
    "text": "if we take for example spark apache spark as an example so apache spark as a framework doesn't do",
    "start": "446000",
    "end": "452720"
  },
  {
    "text": "anything when when it's ready to process data through mesos master it selects the",
    "start": "452720",
    "end": "460080"
  },
  {
    "text": "appropriate worker or workers and launches the executors who actually do the data",
    "start": "460080",
    "end": "466000"
  },
  {
    "text": "analysis and on the other hand for example you could mention the marathon framework that you have been using it's one of the popular frameworks",
    "start": "466000",
    "end": "472319"
  },
  {
    "text": "for messes to run you know containers you don't necessarily have to be container you can run some tasks or even some scripts",
    "start": "472319",
    "end": "478400"
  },
  {
    "text": "using it so uh in methods this page networks that you've got different frameworks and where you start",
    "start": "478400",
    "end": "484160"
  },
  {
    "text": "to message they're getting you know resources uh from the resource performances and",
    "start": "484160",
    "end": "489759"
  },
  {
    "text": "working based on this you know dynamic resource fairness scheduling policy um yeah so that's",
    "start": "489759",
    "end": "497199"
  },
  {
    "text": "the message of review and if we look at this uh kubernetes overview here these pictures as you can see basically",
    "start": "497199",
    "end": "503599"
  },
  {
    "text": "the same they're doing this almost the same things of course you know both of them uh have are sharing ancestry of googleborg",
    "start": "503599",
    "end": "511440"
  },
  {
    "text": "uh but uh also in kubernetes you can see in the architecture there is like a three master notes or you know there",
    "start": "511440",
    "end": "517200"
  },
  {
    "text": "is even like elections going on and later and you've got this api server scheduler control measure all this stuff",
    "start": "517200",
    "end": "523360"
  },
  {
    "text": "and the worker notes you can see a keyblade which is basically uh you know making sure that the containers are running a port uh q proxy",
    "start": "523360",
    "end": "531040"
  },
  {
    "text": "taking care of the networking etc but in the messa side the message slave or mrs agent",
    "start": "531040",
    "end": "538000"
  },
  {
    "text": "actually processed that was taking care of similar things so they are very much",
    "start": "538000",
    "end": "543440"
  },
  {
    "text": "similar in terms of you know how they are working um the sketching on",
    "start": "543440",
    "end": "549279"
  },
  {
    "text": "also you can see here on the right basically similar to messes um",
    "start": "549279",
    "end": "554720"
  },
  {
    "text": "then this is a picture together like the marriage of messes and kubernetes yes that is true",
    "start": "554720",
    "end": "560720"
  },
  {
    "text": "so um the the way we selected to go is to host them together not on so",
    "start": "560720",
    "end": "568800"
  },
  {
    "text": "two clusters working together in the same pool of resources we we didn't want to have separate",
    "start": "568800",
    "end": "576000"
  },
  {
    "text": "clusters separate deployments what we wanted to do is to keep the existing production",
    "start": "576000",
    "end": "581680"
  },
  {
    "text": "platforms up and running and basically hook kubernetes into this setup",
    "start": "581680",
    "end": "591040"
  },
  {
    "text": "they would share resources on them on the control plane and also on the workload management",
    "start": "591040",
    "end": "597680"
  },
  {
    "text": "they would be completely invisible to the end users and the applications that were already",
    "start": "597680",
    "end": "604079"
  },
  {
    "text": "working on mesos will continue to do so and the new obligations or the migrated",
    "start": "604079",
    "end": "610079"
  },
  {
    "text": "applications would be hosted on the kubernetes cluster running",
    "start": "610079",
    "end": "615680"
  },
  {
    "text": "in exactly the same place so as you can see masternodes same",
    "start": "615680",
    "end": "623519"
  },
  {
    "text": "three for mesos and kubernetes and worker nodes all of them sharing both mesos and",
    "start": "623519",
    "end": "631440"
  },
  {
    "text": "kubernetes workloads this there is a small gray box in the in the worker nodes",
    "start": "631440",
    "end": "638800"
  },
  {
    "text": "which represents our magical script which is called cubelet right yes so we will talk",
    "start": "638800",
    "end": "647200"
  },
  {
    "text": "more about the cubelet wrapper but this was just one more mesos application that was",
    "start": "647200",
    "end": "653440"
  },
  {
    "text": "running in the backend no user interface but it was taking care",
    "start": "653440",
    "end": "659680"
  },
  {
    "text": "that the kubernetes workers were up and running and were",
    "start": "659680",
    "end": "664720"
  },
  {
    "text": "able to consume some resources from the host and hence host the loads yes so",
    "start": "664720",
    "end": "671279"
  },
  {
    "text": "basically when you run the kubernetes marathon boom you've got this cover in this cluster yes you know and",
    "start": "671279",
    "end": "678560"
  },
  {
    "text": "interesting thing is that they are not fighting with each other's resources as you can see here in this cupid",
    "start": "678560",
    "end": "684640"
  },
  {
    "text": "preparing marathon in this slide you can see um how kibler paper is running in marathon",
    "start": "684640",
    "end": "691440"
  },
  {
    "text": "so basically in marathon you do not your application doesn't necessarily need to be have a container",
    "start": "691440",
    "end": "697279"
  },
  {
    "text": "it can you can just write any script scripts or tasks you know there is message containers as well",
    "start": "697279",
    "end": "702399"
  },
  {
    "text": "and so we can see here um a keyboard wrapper running in marathon",
    "start": "702399",
    "end": "707839"
  },
  {
    "text": "and with this marathon job what we can do is we can control how many nodes from the cluster will be",
    "start": "707839",
    "end": "716320"
  },
  {
    "text": "you know sharing resources and become yeah also become kubernetes nodes yes and also we can control the resources so",
    "start": "716320",
    "end": "722720"
  },
  {
    "text": "how much cpus and maps they will have and you know um with this you know",
    "start": "722720",
    "end": "728399"
  },
  {
    "text": "keyboard system reserved uh parameters then uh kubernetes will not uh be confused",
    "start": "728399",
    "end": "734720"
  },
  {
    "text": "with uh you know mrs used resources and so basically all the resources that are used by messages is seen to cover it as a system",
    "start": "734720",
    "end": "742560"
  },
  {
    "text": "reserved and you know then there will not be any other subscription or you know",
    "start": "742560",
    "end": "747600"
  },
  {
    "text": "any issues with the you know limitations of resources or fighting with the resources and",
    "start": "747600",
    "end": "753040"
  },
  {
    "text": "uh mesos also on the other side is able to see how much resources kubernetes is using",
    "start": "753040",
    "end": "758639"
  },
  {
    "text": "from particular node and so on you can actually even run kubelet with no resources assigned at all so",
    "start": "758639",
    "end": "764079"
  },
  {
    "text": "basically that is uh that could be used for body eviction and stuff like that and and so you know even a even a node is",
    "start": "764079",
    "end": "772480"
  },
  {
    "text": "running cubelet doesn't necessarily mean that it is it has resources we can control those from this so this is really",
    "start": "772480",
    "end": "778560"
  },
  {
    "text": "a very nice magical script okay it looks very simple but we we try to",
    "start": "778560",
    "end": "784639"
  },
  {
    "text": "follow you know this keys principle keep it simple stupid you know systems work best when they're kept simple and we also are a",
    "start": "784639",
    "end": "791839"
  },
  {
    "text": "bit lazy we try to do less and gain a little bit more but of course all these are coming from spike which we started",
    "start": "791839",
    "end": "798000"
  },
  {
    "text": "you know when we started thinking to move to kubernetes but um uh now that we have",
    "start": "798000",
    "end": "804800"
  },
  {
    "text": "seen the picture of a cube light rapper in marathon maria would like to say something how we actually package it",
    "start": "804800",
    "end": "810720"
  },
  {
    "text": "yeah so how it all worked um on the platform uh major components",
    "start": "810720",
    "end": "818399"
  },
  {
    "text": "were actually packaged into a vm image and this vm image was transported to the target cloud where",
    "start": "818399",
    "end": "825760"
  },
  {
    "text": "the platform was supposed to be deployed on yes and this gave us the possibility to deploy fairly on any cloud",
    "start": "825760",
    "end": "833199"
  },
  {
    "text": "in customer premises or in our internal nokia cloud which was the actual environment where",
    "start": "833199",
    "end": "840560"
  },
  {
    "text": "we had a production clusters runnings so from the list on the screen you can",
    "start": "840560",
    "end": "846160"
  },
  {
    "text": "see all the components you can see mesos you can see marathon kubernetes and so on so these are the",
    "start": "846160",
    "end": "854399"
  },
  {
    "text": "open source applications that we would uh deploy as part of the platform and as part on",
    "start": "854399",
    "end": "860880"
  },
  {
    "text": "the little uh box on the right you can see that as part of the kubernetes",
    "start": "860880",
    "end": "866399"
  },
  {
    "text": "role or when this folder is where we have the wrapper so basically you know is using packer basically we're just",
    "start": "866399",
    "end": "872800"
  },
  {
    "text": "creating the images and all the necessary binaries and scripts uh you know that we would like to let's say",
    "start": "872800",
    "end": "879519"
  },
  {
    "text": "carry offline in certain offline cases which was actually one of our major requirements as well",
    "start": "879519",
    "end": "884720"
  },
  {
    "text": "that we would be able to deploy it offline and so you basically you know pack up everything in your laptop or in",
    "start": "884720",
    "end": "890800"
  },
  {
    "text": "a usb stick and deploy it in a custom premise or in your environment case where you don't have access to internet",
    "start": "890800",
    "end": "897120"
  },
  {
    "text": "uh so packaging the vm image and now that we have it in the vm image",
    "start": "897120",
    "end": "903360"
  },
  {
    "text": "um we launch it from marathon yes so we are",
    "start": "903360",
    "end": "908399"
  },
  {
    "text": "ready to deploy with the image and yeah and uh this you can see here is",
    "start": "908399",
    "end": "914399"
  },
  {
    "text": "just some app definition in marathon even the keyblade wrapper itself is also deployed in a",
    "start": "914399",
    "end": "919920"
  },
  {
    "text": "metadata event deployment way which also we had identified during our skips as",
    "start": "919920",
    "end": "925920"
  },
  {
    "text": "spike sorry durian spike that um we can leverage the existing way of",
    "start": "925920",
    "end": "932079"
  },
  {
    "text": "deployment uh so that it's much easier for the developer teams to you know migrate their applications",
    "start": "932079",
    "end": "940320"
  },
  {
    "text": "so as you can see here um this app type helm so in app.yaml the",
    "start": "940320",
    "end": "946880"
  },
  {
    "text": "developers would put a type helm after creating their hand chart of course for their application and then our command line",
    "start": "946880",
    "end": "954720"
  },
  {
    "text": "tool will just we just identify it as an you know helm application and it will just install",
    "start": "954720",
    "end": "960000"
  },
  {
    "text": "uh with helm and if they didn't provide it which means that they didn't need to make any changes",
    "start": "960000",
    "end": "969680"
  },
  {
    "text": "you know applications that are running and messes then you need to make any changes so the existing way was also working and we",
    "start": "969680",
    "end": "976399"
  },
  {
    "text": "also were able to gradually move to kubernetes you know based on priorities and release um",
    "start": "976399",
    "end": "982959"
  },
  {
    "text": "cycles and so on so on the bottom side you can see there is an example of couch tv",
    "start": "982959",
    "end": "988240"
  },
  {
    "text": "which don't have this app time help so this was actually a marathon application yeah so that's",
    "start": "988240",
    "end": "994480"
  },
  {
    "text": "how it's been deployed and although it leaks all these things looks very fairly simple",
    "start": "994480",
    "end": "1000480"
  },
  {
    "text": "you know definitely there are many other things underneath but these are simplifying for the presentation",
    "start": "1000480",
    "end": "1005519"
  },
  {
    "text": "simplifying for the presentation yes here is a picture of",
    "start": "1005519",
    "end": "1011120"
  },
  {
    "text": "little things what we have learned from this project sharing also although there are a lot more a lot more yes little big things so",
    "start": "1011120",
    "end": "1018880"
  },
  {
    "text": "we kind of um found we in practice and while implementing the",
    "start": "1018880",
    "end": "1026079"
  },
  {
    "text": "the the side by side deployment of the both clusters using them side by side and",
    "start": "1026079",
    "end": "1032000"
  },
  {
    "text": "troubleshooting of course as as usual you have issues on the way so um we found out that",
    "start": "1032000",
    "end": "1040240"
  },
  {
    "text": "part of the of the winning kind of conditions come from the correct",
    "start": "1040240",
    "end": "1046079"
  },
  {
    "text": "strategy so and the points in the achieving the best strategy is to study",
    "start": "1046079",
    "end": "1053039"
  },
  {
    "text": "first so we did a spike as uh mentioned uh implementing the side-by-side deployment",
    "start": "1053039",
    "end": "1059600"
  },
  {
    "text": "uh hosting workloads and making sure that the resource consumption was um",
    "start": "1059600",
    "end": "1067679"
  },
  {
    "text": "controllable for both clusters following the keys principle keep it",
    "start": "1067679",
    "end": "1072880"
  },
  {
    "text": "simple right yes um and achieve more with less",
    "start": "1072880",
    "end": "1078080"
  },
  {
    "text": "so we don't need more glass complex in our complex system systems yes what we",
    "start": "1078080",
    "end": "1084080"
  },
  {
    "text": "need is to keep running as we did yes but introduce uh",
    "start": "1084080",
    "end": "1089360"
  },
  {
    "text": "this parallel cluster and a parallel continues yeah other thing also here we",
    "start": "1089360",
    "end": "1095440"
  },
  {
    "text": "can see we're clearly saving resources you know we're clearly saving some different complexities like",
    "start": "1095440",
    "end": "1101280"
  },
  {
    "text": "if you would like to have another kubernetes cluster deployed you know with dedicated hosts and stuff like this",
    "start": "1101280",
    "end": "1107440"
  },
  {
    "text": "then you know backup and resources you need even more resources and it gets even more complicated and so on",
    "start": "1107440",
    "end": "1112799"
  },
  {
    "text": "so hooking hooking the the new cluster into an existing platform we took advantage of",
    "start": "1112799",
    "end": "1119039"
  },
  {
    "text": "the logging management of the",
    "start": "1119039",
    "end": "1124960"
  },
  {
    "text": "metrics management the telemetry system where access and engineering went yes uh we didn't move them to kubernetes",
    "start": "1124960",
    "end": "1130960"
  },
  {
    "text": "so you know they were being shared between the both platforms and since we used the same overlay",
    "start": "1130960",
    "end": "1136080"
  },
  {
    "text": "network all the logs",
    "start": "1136080",
    "end": "1140960"
  },
  {
    "text": "every container that was running there was collected by the let's call it old system so everything",
    "start": "1142480",
    "end": "1148720"
  },
  {
    "text": "was working as expected with a slight difference that some applications were hosted on",
    "start": "1148720",
    "end": "1154960"
  },
  {
    "text": "kubernetes yes and one immediate measure is",
    "start": "1154960",
    "end": "1160240"
  },
  {
    "text": "because kubernetes manages the demo sets you know very smoothly it's very nice and also",
    "start": "1160240",
    "end": "1166880"
  },
  {
    "text": "um the deployment for the helm applications you know the deployment was very nice although",
    "start": "1166880",
    "end": "1172480"
  },
  {
    "text": "in the beginning i remember there was this helm too and there were some issues with you know security yes management",
    "start": "1172480",
    "end": "1179760"
  },
  {
    "text": "yes and uh but with command this helm three it's been resolved yeah so um",
    "start": "1179760",
    "end": "1187679"
  },
  {
    "text": "a metadata deployment way was really helpful for us and cooperatives and methods both sharing the same resource",
    "start": "1187679",
    "end": "1194480"
  },
  {
    "text": "pools you know no extra resources needed and of course we need to imagine our cluster properly",
    "start": "1194480",
    "end": "1200960"
  },
  {
    "text": "including the system resources because if you are not dimensioning the cluster properly you may have issues let's say you know",
    "start": "1200960",
    "end": "1206480"
  },
  {
    "text": "the uh the application is requiring certain resource that none of your and nodes",
    "start": "1206480",
    "end": "1212960"
  },
  {
    "text": "or what i know is actually providing and it can happen even for spark jobs as well yes one one thing that now that you",
    "start": "1212960",
    "end": "1219520"
  },
  {
    "text": "mentioned the dimensioning topic is very important because it did happen to us at least in the beginning where",
    "start": "1219520",
    "end": "1225440"
  },
  {
    "text": "development teams were getting used to the fact that if they are going to deploy the",
    "start": "1225440",
    "end": "1231280"
  },
  {
    "text": "application not on mesos anymore but on kubernetes they would need to tune yeah the",
    "start": "1231280",
    "end": "1238880"
  },
  {
    "text": "resource assigned to kubernetes through the cubelet wrapper so the magical",
    "start": "1238880",
    "end": "1244320"
  },
  {
    "text": "script was not so magical anymore if you didn't provide the right information too exactly",
    "start": "1244320",
    "end": "1249440"
  },
  {
    "text": "yes yes so there was some effort needed from everybody to kind of keep in sync between resource",
    "start": "1249440",
    "end": "1256480"
  },
  {
    "text": "consumption and migrated applications yes so basically we also didn't have any",
    "start": "1256480",
    "end": "1263280"
  },
  {
    "text": "duplicates within the within the shared platform so we were sharing or for example you know the telemetry",
    "start": "1263280",
    "end": "1270640"
  },
  {
    "text": "systems and even the fcd same http was being shared you know so we were also saving",
    "start": "1270640",
    "end": "1277600"
  },
  {
    "text": "resources from that point of view that you know you have you don't need necessarily to have multiple services you know",
    "start": "1277600",
    "end": "1283280"
  },
  {
    "text": "one for each yeah dedicated for each yeah so that's basically it uh there are",
    "start": "1283280",
    "end": "1289200"
  },
  {
    "text": "a lot more feel free to ping us we will definitely share and uh here is a bonus",
    "start": "1289200",
    "end": "1298080"
  },
  {
    "text": "uh just a picture of uh some of the tools that we have used and loved yes yeah and maria has and myself has",
    "start": "1298080",
    "end": "1304159"
  },
  {
    "text": "mentioned during this session already some of these you know that probably you're already using and",
    "start": "1304159",
    "end": "1309679"
  },
  {
    "text": "kubernetes is there bigger because let's stay in this darling and our new baby exactly yeah or",
    "start": "1309679",
    "end": "1318240"
  },
  {
    "text": "it's not you anymore but yeah hi so this is the end of our session",
    "start": "1318240",
    "end": "1325200"
  },
  {
    "text": "thank you very much for joining and we are ready for your questions we have some had some already",
    "start": "1325200",
    "end": "1333120"
  },
  {
    "text": "in in the chat we'll have time for more so please write your questions",
    "start": "1333120",
    "end": "1341280"
  },
  {
    "text": "meanwhile we can maybe share what questions were asked one of the questions were what is marathon is it an",
    "start": "1342400",
    "end": "1347440"
  },
  {
    "text": "app yes it's an app actually um second one was how did we",
    "start": "1347440",
    "end": "1353840"
  },
  {
    "text": "um handle the player for load balancing where the uh applications from message side uh",
    "start": "1353840",
    "end": "1360640"
  },
  {
    "text": "accessible from the kubernetes side yes that was actually the original goal to be able to share you know",
    "start": "1360640",
    "end": "1366640"
  },
  {
    "text": "similarities like that all the resources and all the services within the both of the uh platforms to be shared",
    "start": "1366640",
    "end": "1374720"
  },
  {
    "text": "so for example telemetry systems uh monitoring and enlarging all these things they were being shared before even being",
    "start": "1374720",
    "end": "1381200"
  },
  {
    "text": "moved to kubernetes and while we gradually moved from one to another we were able to make use of the existing",
    "start": "1381200",
    "end": "1387919"
  },
  {
    "text": "services yeah and maybe it's good to mention and we didn't say it in the presentation that the",
    "start": "1387919",
    "end": "1394080"
  },
  {
    "text": "nginx was one and only for both clusters so there's there is also a question",
    "start": "1394080",
    "end": "1401840"
  },
  {
    "text": "you like to read it out yeah sure so uh kashif is asking how will the whole",
    "start": "1401840",
    "end": "1407200"
  },
  {
    "text": "system behave in case of resource congestion very good question yes",
    "start": "1407200",
    "end": "1413280"
  },
  {
    "text": "yes yeah um so as you know that uh the during the",
    "start": "1413280",
    "end": "1419039"
  },
  {
    "text": "presentation we have mentioned like message is able to know how much is just covering this is using and",
    "start": "1419039",
    "end": "1424799"
  },
  {
    "text": "kubernetes also knows what is its limit and also we have got this um you know telemetry systems monitoring and",
    "start": "1424799",
    "end": "1431279"
  },
  {
    "text": "alerting where we were watching uh like at certain thresholds we get a large in",
    "start": "1431279",
    "end": "1438320"
  },
  {
    "text": "terms of you know cpu or memory condition so definitely we were getting a large before issues happening and our sres",
    "start": "1438320",
    "end": "1444480"
  },
  {
    "text": "were taking care of those but yeah so messes and",
    "start": "1444480",
    "end": "1449679"
  },
  {
    "text": "and kubernetes both the clusters even though they were sharing resources from the same resource",
    "start": "1449679",
    "end": "1454960"
  },
  {
    "text": "a pull from the node you know they were uh not getting like in fight with each",
    "start": "1454960",
    "end": "1462159"
  },
  {
    "text": "other and before even oh there's another question",
    "start": "1462159",
    "end": "1467600"
  },
  {
    "text": "yeah how do you decide uh request limits for cpu and memory do you",
    "start": "1467600",
    "end": "1474080"
  },
  {
    "text": "use the vertical port auto scaler or do you have a smarter algorithm",
    "start": "1474080",
    "end": "1479520"
  },
  {
    "text": "so i think none basically of those options i can",
    "start": "1479520",
    "end": "1486400"
  },
  {
    "text": "answer that this one what we had we had predefined",
    "start": "1486400",
    "end": "1492000"
  },
  {
    "text": "resource requests and limits for those applications that were migrated to kubernetes from mesos",
    "start": "1492000",
    "end": "1498320"
  },
  {
    "text": "and we have estimated based on our experience in production tests and performance tests uh how much",
    "start": "1498320",
    "end": "1505360"
  },
  {
    "text": "resource they would maximum uh be allowed to consume and how much they need to start so we would assign kind of a fixed",
    "start": "1505360",
    "end": "1513840"
  },
  {
    "text": "set of requests and limits for those applications",
    "start": "1513840",
    "end": "1518720"
  },
  {
    "text": "there is is there any more questions not",
    "start": "1524320",
    "end": "1530559"
  },
  {
    "text": "but these are really very nice questions and you know the migration is actually everybody has",
    "start": "1533039",
    "end": "1539600"
  },
  {
    "text": "whoever have used some other clusters before before kubernetes you know they would have a story migration story",
    "start": "1539600",
    "end": "1546559"
  },
  {
    "text": "and probably there are many different stories people are following different approaches for our cases we also wanted",
    "start": "1546559",
    "end": "1552080"
  },
  {
    "text": "to save resources we couldn't just you know go have new um set of resources",
    "start": "1552080",
    "end": "1559760"
  },
  {
    "text": "in a new cloud account and you know deploy the kubernetes cluster separately over there",
    "start": "1559760",
    "end": "1565279"
  },
  {
    "text": "and then migrate you know that would be very costly for us yes but this was really nice because you",
    "start": "1565279",
    "end": "1571120"
  },
  {
    "text": "know we were just not really wasting any resources",
    "start": "1571120",
    "end": "1576158"
  },
  {
    "text": "right where how many how long time do we have",
    "start": "1582720",
    "end": "1587200"
  },
  {
    "text": "conflicts for running mesos and kubernetes master processes on same nodes",
    "start": "1590159",
    "end": "1598320"
  },
  {
    "text": "no we don't have we we had those uh predefined we all the host ports that we had actually",
    "start": "1598320",
    "end": "1604640"
  },
  {
    "text": "they were identified and uh any other ports you know i mean with overland networks you have",
    "start": "1604640",
    "end": "1610960"
  },
  {
    "text": "got this for containers unique uh unique ips so yeah they are",
    "start": "1610960",
    "end": "1616240"
  },
  {
    "text": "not completing with the ports yeah but basically we took uh care of defining the ports and uh specifying",
    "start": "1616240",
    "end": "1623200"
  },
  {
    "text": "the ports uh carefully yes to avoid the the conflicts yeah what platform did you",
    "start": "1623200",
    "end": "1630320"
  },
  {
    "text": "use on premise uh if you refer for platform like the cloud the hosting",
    "start": "1630320",
    "end": "1636480"
  },
  {
    "text": "cloud on-premise our previously yes so on-premise we had",
    "start": "1636480",
    "end": "1642480"
  },
  {
    "text": "openstack so this this platform of ours was running",
    "start": "1642480",
    "end": "1650000"
  },
  {
    "text": "in in nokia premises and we have an openstack cloud yeah",
    "start": "1650000",
    "end": "1655840"
  },
  {
    "text": "nokia has i think it's already known that one of the largest on-prem",
    "start": "1655840",
    "end": "1660960"
  },
  {
    "text": "clouds provided to the internal developers and so on so we were using that and it is built on",
    "start": "1660960",
    "end": "1667120"
  },
  {
    "text": "top of openstack yes um there is one more question have you thought of running kubernetes",
    "start": "1667120",
    "end": "1673679"
  },
  {
    "text": "on top of mesos and not beside it like like a metal framework",
    "start": "1673679",
    "end": "1679120"
  },
  {
    "text": "but yes yeah we did and we decided not to go for that yes and um",
    "start": "1679120",
    "end": "1686840"
  },
  {
    "text": "[Music] basically why because we wanted to get r",
    "start": "1686840",
    "end": "1692399"
  },
  {
    "text": "in the end we wanted to get rid of mesos all together",
    "start": "1692399",
    "end": "1697840"
  },
  {
    "text": "want to go this layer over layer",
    "start": "1699039",
    "end": "1711840"
  },
  {
    "text": "and side by side is actually then you know basically they're independent of each other even though they're sharing",
    "start": "1720640",
    "end": "1726480"
  },
  {
    "text": "resources when you have like one is deployed on top of the other then if you have issue with the one one of",
    "start": "1726480",
    "end": "1732960"
  },
  {
    "text": "them then you have issue with the other one you know yes",
    "start": "1732960",
    "end": "1739840"
  },
  {
    "text": "any other questions we are running out of time counting three minutes yeah but",
    "start": "1742080",
    "end": "1747600"
  },
  {
    "text": "basically this mess of kubernetes on top of mesos was the mesosphere approach and they",
    "start": "1747600",
    "end": "1754240"
  },
  {
    "text": "provided this service as part of the mesosphere which we didn't use",
    "start": "1754240",
    "end": "1760080"
  },
  {
    "text": "yeah we didn't use the enterprise version of messes so we were using the open source versions and you know",
    "start": "1760080",
    "end": "1765600"
  },
  {
    "text": "basically using as much as or was it 90 over 95 percent of the components were",
    "start": "1765600",
    "end": "1771360"
  },
  {
    "text": "open source yes of course we were you know tweaking changing adding stuff",
    "start": "1771360",
    "end": "1777200"
  },
  {
    "text": "i think everybody's doing that based on your requirements and this is serious but yeah any other questions",
    "start": "1777200",
    "end": "1795840"
  },
  {
    "text": "thank you very much for these questions these are very nice questions and was there another one actually no",
    "start": "1796399",
    "end": "1803679"
  },
  {
    "text": "okay so we can end the broadcast whenever yeah yeah um",
    "start": "1803679",
    "end": "1811679"
  },
  {
    "text": "if there are no more questions i think we can we can end the broadcasting thank you very much for joining and hope",
    "start": "1811679",
    "end": "1817679"
  },
  {
    "text": "you enjoy the kubecon yes the virtual one first ever yes thank you we have enjoyed yeah",
    "start": "1817679",
    "end": "1826399"
  }
]