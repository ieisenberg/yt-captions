[
  {
    "text": "right let's go ahead and get started thank you all for coming to hear us uh blather on about Network performance",
    "start": "160",
    "end": "5960"
  },
  {
    "text": "monitoring uh we hope to find that you'll find it an interesting topic I hope you had a great",
    "start": "5960",
    "end": "12440"
  },
  {
    "text": "C who are we so my name is Matt Franklin I'm a senior production engineering manager at Shopify um I joined Shopify",
    "start": "12440",
    "end": "20039"
  },
  {
    "text": "about two years ago joined the observability team and have since then gone through quite a process of moving",
    "start": "20039",
    "end": "27199"
  },
  {
    "text": "off of a a metrics vendor to an in-house back and seeing this process of of",
    "start": "27199",
    "end": "32920"
  },
  {
    "text": "building new observability systems uh come to life I am Sebastian Ros I'm a senior",
    "start": "32920",
    "end": "39239"
  },
  {
    "text": "production engineer at mat's team and I'm probably nearly five years now yeah",
    "start": "39239",
    "end": "44600"
  },
  {
    "text": "and I have previously worked on metrics at Shopify now I work on logs and that's",
    "start": "44600",
    "end": "50480"
  },
  {
    "text": "uh also a project um I worked on recently a little bit of context uh so",
    "start": "50480",
    "end": "57840"
  },
  {
    "text": "at Shopify we used to have a bunch of different signals that we had different vendors for and the observability team",
    "start": "57840",
    "end": "64559"
  },
  {
    "text": "was responsible for kind of just maintaining those those vendors um a coup few years ago the executive team",
    "start": "64559",
    "end": "71080"
  },
  {
    "text": "pushed us to to figure out how we could build a more unified observability platform for Shopify that meets our use",
    "start": "71080",
    "end": "77759"
  },
  {
    "text": "cases um so we inh housed our metric system we started with a migration there and then moved on to uh logs traces",
    "start": "77759",
    "end": "85799"
  },
  {
    "text": "tracing was actually first but as part of that we found that we needed the the ability to kind of look into our event",
    "start": "85799",
    "end": "92399"
  },
  {
    "text": "stores so traces logs and analyze those in New in different ways right so we",
    "start": "92399",
    "end": "97960"
  },
  {
    "text": "built a tool called investigate so as Sebastian and I were looking at this process of hey we need to build a a",
    "start": "97960",
    "end": "104200"
  },
  {
    "text": "network Performance Management tool we started to leverage that that capability um to give you some context",
    "start": "104200",
    "end": "110680"
  },
  {
    "text": "at Shopify you know like I said we had a prior vendor that vendor had a network Performance Management tool that tool",
    "start": "110680",
    "end": "116560"
  },
  {
    "text": "was very useful uh as many of you who are in infrastructure ructure or are responsible for production systems will",
    "start": "116560",
    "end": "122399"
  },
  {
    "text": "know the first question that everybody asks during an incident is is it the Network's",
    "start": "122399",
    "end": "128000"
  },
  {
    "text": "fault fair question we had some ways to answer that after migrating away from our vendor but we didn't have a really",
    "start": "128000",
    "end": "134080"
  },
  {
    "text": "good way to to get quick answers to those questions for additional context uh",
    "start": "134080",
    "end": "140640"
  },
  {
    "text": "Shopify we run hundreds of of kubernetes clusters those clusters run millions of",
    "start": "140640",
    "end": "145920"
  },
  {
    "text": "containers those millions of containers make millions of DNS queries per second and tens of millions of connections per",
    "start": "145920",
    "end": "154000"
  },
  {
    "text": "second so this all left us with okay this Mission we have a little bit of time on our hands after we got done with",
    "start": "154000",
    "end": "160080"
  },
  {
    "text": "our Mig migration we are prepping up for the next you know kind of set of capabilities that we're going to bring",
    "start": "160080",
    "end": "165319"
  },
  {
    "text": "into Shopify um but what can we do to kind of solve this particular use case effectively uh we needed to be able to",
    "start": "165319",
    "end": "172280"
  },
  {
    "text": "to kind of like restore the functionality we took away right we don't want to be a burden to our teams",
    "start": "172280",
    "end": "178000"
  },
  {
    "text": "that are trying to solve problems we want to help them we need to restore that functionality um we need to quickly",
    "start": "178000",
    "end": "183519"
  },
  {
    "text": "be able to debug DNS flows and or DNS requests and network flows we needed uh",
    "start": "183519",
    "end": "190200"
  },
  {
    "text": "minimal impact on our client clusters right so the way we are organized from an infrastructure perspective is we have",
    "start": "190200",
    "end": "196480"
  },
  {
    "text": "all of our kind of core backend stuff where we manage our observability signals and all all bunch of other",
    "start": "196480",
    "end": "201680"
  },
  {
    "text": "different tools um but we also have these client clusters that serve our Merchants right so shopify's job is to",
    "start": "201680",
    "end": "207440"
  },
  {
    "text": "make Merchants successful so we want to make sure we're not impacting uh any of the resources that are there",
    "start": "207440",
    "end": "213280"
  },
  {
    "text": "and we have the last kind of requirement is we have to be able to handle a significant amount of network traffic so Network traffic at Shopify is measured",
    "start": "213280",
    "end": "219760"
  },
  {
    "text": "TI bytes a minute so it's non-trivial and uh thank you Matt for",
    "start": "219760",
    "end": "225959"
  },
  {
    "text": "the introduction and I will give you now an overview over the pipeline we build to get those um I will I will call them",
    "start": "225959",
    "end": "233040"
  },
  {
    "text": "um Network events from the Clusters into our click house and make them queriable",
    "start": "233040",
    "end": "240319"
  },
  {
    "text": "um again I want to reiterate on that it was very important to BU like to keep",
    "start": "240319",
    "end": "246360"
  },
  {
    "text": "the resource consumptions in the client clusters and especially on the noes where we have running uh and kind of",
    "start": "246360",
    "end": "253920"
  },
  {
    "text": "Agents as low as possible but maybe let's start with a a short recap on ebpf and that's all I",
    "start": "253920",
    "end": "261359"
  },
  {
    "text": "will say about eppf in this talk so don't worry um EF extends the kernel",
    "start": "261359",
    "end": "267800"
  },
  {
    "text": "capabilities by running sandbox programs and um that doesn't require to change",
    "start": "267800",
    "end": "273680"
  },
  {
    "text": "the kernel source code or lo lo kernel modules in advance so you can do it at",
    "start": "273680",
    "end": "279919"
  },
  {
    "text": "at the run time ebpf programs are event driven and they run when the kernel or",
    "start": "279919",
    "end": "286600"
  },
  {
    "text": "an application passes a certain hook in our case that would be networking events",
    "start": "286600",
    "end": "292400"
  },
  {
    "text": "uh or DNS events so um they put the collected information into a map and",
    "start": "292400",
    "end": "298720"
  },
  {
    "text": "this map is sh between the kernel and the user space So This is How we get the information out of the kernel and then",
    "start": "298720",
    "end": "304479"
  },
  {
    "text": "you can get this information from the map and um process it this is uh like",
    "start": "304479",
    "end": "310800"
  },
  {
    "text": "for example the the the connection information um we use the ebpf manager",
    "start": "310800",
    "end": "316560"
  },
  {
    "text": "you see this here in this diagram um uh that builds on top of",
    "start": "316560",
    "end": "321680"
  },
  {
    "text": "celium and we use predefined um programs",
    "start": "321680",
    "end": "327319"
  },
  {
    "text": "for um the connections DNS iies and other network related metrics so now let's have a look on how",
    "start": "327319",
    "end": "334800"
  },
  {
    "text": "this um looks on the note I will go from the note to our storage click house so",
    "start": "334800",
    "end": "341560"
  },
  {
    "text": "on each note we have running the system prob exporter as a demon set um and this is the system Pro itself and an exporter",
    "start": "341560",
    "end": "349319"
  },
  {
    "text": "the system proof as I said uses ebpf manager and um programs to capture um",
    "start": "349319",
    "end": "357199"
  },
  {
    "text": "the capture Network events and and it watches connections itself like it's it's this is this is connections uh for",
    "start": "357199",
    "end": "365639"
  },
  {
    "text": "local containers where you get the um process ID the IP the port remote IP and",
    "start": "365639",
    "end": "372759"
  },
  {
    "text": "the remote Port then the NS queries there where you get the process ID again the client IP the resolver IP and the",
    "start": "372759",
    "end": "381000"
  },
  {
    "text": "type and the status and then we also look for TCP Q length um metrics this is",
    "start": "381000",
    "end": "387759"
  },
  {
    "text": "this is the only metric we currently EXP there might be more in the future we we're just starting here um the exporter",
    "start": "387759",
    "end": "394800"
  },
  {
    "text": "then uh scrapes the system probe the system probe has a um it's it's a small",
    "start": "394800",
    "end": "401039"
  },
  {
    "text": "go program it has a um it has a Proto buff um so it scrapes protuff from the",
    "start": "401039",
    "end": "408560"
  },
  {
    "text": "system probe and it for each each scrape it gets the diff compared to the",
    "start": "408560",
    "end": "413599"
  },
  {
    "text": "previous um scrapes or all the DNS queries that happened or all the connection information like how",
    "start": "413599",
    "end": "419599"
  },
  {
    "text": "connection changed and what it also does I was saying that on the on the network",
    "start": "419599",
    "end": "425520"
  },
  {
    "text": "events we only have the process ID but what we want is the container ID because",
    "start": "425520",
    "end": "430560"
  },
  {
    "text": "when we have the container ID we can map all the metadata we have on a container like P deployment region cluster",
    "start": "430560",
    "end": "438000"
  },
  {
    "text": "whatever to this so um the exporter also has a map from process ID to um",
    "start": "438000",
    "end": "444479"
  },
  {
    "text": "container ID and then it tags the networks event accordingly and writes",
    "start": "444479",
    "end": "449720"
  },
  {
    "text": "them to here's only one vector instance but we have multiple Vector instances running in a cluster writes them into a",
    "start": "449720",
    "end": "455479"
  },
  {
    "text": "vector instance for further sending it down the pipeline uh yeah so maybe before we go",
    "start": "455479",
    "end": "464319"
  },
  {
    "text": "on I mean I let's have a look at the connections itself so what is a connection a connection has a p ID which",
    "start": "464319",
    "end": "471759"
  },
  {
    "text": "we will use to tag again the container ID it has a local address it has a remote address it has a type so um UDP",
    "start": "471759",
    "end": "480800"
  },
  {
    "text": "TCP it has bites and packet transmitted and received it has a Direction so",
    "start": "480800",
    "end": "486240"
  },
  {
    "text": "incoming and outgoing which means if there's a connection between two containers within our Fleet like within",
    "start": "486240",
    "end": "492680"
  },
  {
    "text": "our kubernetes Fleet we will have actually we will see two connections right one incoming one outgoing if it's",
    "start": "492680",
    "end": "499120"
  },
  {
    "text": "outside our Fleet we will only have like the the connection part that's in within",
    "start": "499120",
    "end": "504599"
  },
  {
    "text": "our cluster uh in our cluster Fleet and then there are other uh um",
    "start": "504599",
    "end": "510520"
  },
  {
    "text": "uh the there are other fields like TCP errors and so on I I left them here",
    "start": "510520",
    "end": "517760"
  },
  {
    "text": "yeah um so next slide yeah so how does this look like the bigger picture now um",
    "start": "517760",
    "end": "524640"
  },
  {
    "text": "here again you see on the left hand side what I just showed you in the cluster we have the notes we have the exporters",
    "start": "524640",
    "end": "530800"
  },
  {
    "text": "running they send connection DNS queries in TCP Q um event data into the vector",
    "start": "530800",
    "end": "538839"
  },
  {
    "text": "instances running in our clusters and those Vector instances they just batch them up compress them and send them",
    "start": "538839",
    "end": "546360"
  },
  {
    "text": "further down to a kubernetes cluster where we have click house deployed and in front of this click house we have",
    "start": "546360",
    "end": "552240"
  },
  {
    "text": "another set of vector instances they create bigger batches so um for better",
    "start": "552240",
    "end": "557360"
  },
  {
    "text": "insert performance and just write the raw data into click house we we went",
    "start": "557360",
    "end": "562800"
  },
  {
    "text": "with with Vector because it's like um as I as I showed you the the the um the",
    "start": "562800",
    "end": "569560"
  },
  {
    "text": "connections and all the other networks events it's just flat chasing and it's very you can ship it very efficient with",
    "start": "569560",
    "end": "575560"
  },
  {
    "text": "vector and with a minimal configuration only so you don't have to do you you you only care about batching and um",
    "start": "575560",
    "end": "582959"
  },
  {
    "text": "buffering and that's mostly it and it's very it's very efficient yeah like tens of millions uh we ship tens of millions",
    "start": "582959",
    "end": "590360"
  },
  {
    "text": "of connections and other networks events per second through this pipelines yeah uh one thing I want to",
    "start": "590360",
    "end": "598040"
  },
  {
    "text": "mention here when we start when I when I built the first iteration of this pipeline it was just like this and the",
    "start": "598040",
    "end": "604360"
  },
  {
    "text": "distribution like the right requests between the component it wasn't very good so what we did we just spped",
    "start": "604360",
    "end": "611480"
  },
  {
    "text": "NY in in front of each of the components here before vector and in front of Click house and now the distribution is really",
    "start": "611480",
    "end": "618680"
  },
  {
    "text": "nice and HPA works really nice so when there's more load uh the pipeline scales",
    "start": "618680",
    "end": "624000"
  },
  {
    "text": "up and if there's less load it just scales down that works really nicely",
    "start": "624000",
    "end": "629880"
  },
  {
    "text": "yeah next slide so um what's missing metadata is missing we need metadata",
    "start": "629880",
    "end": "636519"
  },
  {
    "text": "right because we only have the container ID for our networking events but for querying we want",
    "start": "636519",
    "end": "643079"
  },
  {
    "text": "all we want we want to to match the container ID with the metadata so we can",
    "start": "643079",
    "end": "650079"
  },
  {
    "text": "um filter group and group by those um attributes so what we do is in clusters",
    "start": "650079",
    "end": "657160"
  },
  {
    "text": "in all our clusters we watch um pods we watch pods using the cube API we have",
    "start": "657160",
    "end": "662880"
  },
  {
    "text": "the so-called metadata exporter and every time there's a new pod we get this information you will see on the next",
    "start": "662880",
    "end": "669760"
  },
  {
    "text": "next slide um what it include like name deployment zone region and so on um we",
    "start": "669760",
    "end": "675839"
  },
  {
    "text": "batch it we send it down to another Vector instance in front of Click house it works mostly the same way like the",
    "start": "675839",
    "end": "681639"
  },
  {
    "text": "other pipeline just much less volume and we write it into click house also into a",
    "start": "681639",
    "end": "687600"
  },
  {
    "text": "separate table and the metadata looks like this so um here we have I mean the",
    "start": "687600",
    "end": "695200"
  },
  {
    "text": "time stamp is when it happens um so so we can set an TTL on it and uh we we",
    "start": "695200",
    "end": "700560"
  },
  {
    "text": "also like relist every 30 minutes um then there's Network project region and",
    "start": "700560",
    "end": "706000"
  },
  {
    "text": "so on so all these attributes um you can later use to query and fil and filter um",
    "start": "706000",
    "end": "711639"
  },
  {
    "text": "our uh Network events by so now how does this work in Click house itself right we",
    "start": "711639",
    "end": "717240"
  },
  {
    "text": "have now in Click house we have the raw data we have the metadata but somehow we want to make this um we want to be able",
    "start": "717240",
    "end": "724839"
  },
  {
    "text": "to query this in a performant way and if you just chin this like let's say if you",
    "start": "724839",
    "end": "730279"
  },
  {
    "text": "if you chin the raw data and aggregate the raw data um uh when you're quering that that",
    "start": "730279",
    "end": "738560"
  },
  {
    "text": "doesn't perform really well it works for the TCP Q Matrix but for everything else the volume is just too high so what we",
    "start": "738560",
    "end": "745720"
  },
  {
    "text": "did we have a pipeline within click house in Click house there's a concept of materialized few and a materialized",
    "start": "745720",
    "end": "752040"
  },
  {
    "text": "VI is a query that's triggered on an insert into a table that it targets and",
    "start": "752040",
    "end": "757480"
  },
  {
    "text": "then it writes the result into another table and you can use this in our",
    "start": "757480",
    "end": "763040"
  },
  {
    "text": "example we use this to join the metadata but also to aggregate um the connections",
    "start": "763040",
    "end": "770600"
  },
  {
    "text": "within a Time window and actually that's not done by the materialized view for",
    "start": "770600",
    "end": "775639"
  },
  {
    "text": "the people who know um click house that's done by an aggregate merge tree",
    "start": "775639",
    "end": "781320"
  },
  {
    "text": "table so um how this looks like is a connection comes in and um now on each",
    "start": "781320",
    "end": "788600"
  },
  {
    "text": "note so our click house setup is uh obviously sharded so we can keep all the load and we have two we have replication",
    "start": "788600",
    "end": "795639"
  },
  {
    "text": "Factor two for each Shard and the raw tables are all distributed over um the",
    "start": "795639",
    "end": "801800"
  },
  {
    "text": "raw data tables are distributed over all the nodes with exception of the metadata but because the metadata we want to Chin",
    "start": "801800",
    "end": "808600"
  },
  {
    "text": "on each insert so the metadata we replicate over all nodes so we have all metadata on all nodes and since this is",
    "start": "808600",
    "end": "815399"
  },
  {
    "text": "only like a million entries that's not a lot for click house we can keep it and um and um replicate it over all notes",
    "start": "815399",
    "end": "822600"
  },
  {
    "text": "and let's say a connection a batch of connections comes in what happens we join the metadata according to the",
    "start": "822600",
    "end": "831240"
  },
  {
    "text": "container ID for local and IP for remote and then uh we aggregate or the",
    "start": "831240",
    "end": "837839"
  },
  {
    "text": "aggregate emergeny aggregated in within a one minute window and then it goes into this aggregated data table the same",
    "start": "837839",
    "end": "845000"
  },
  {
    "text": "happens for um DNS queries obviously there's no remote so um that's only the",
    "start": "845000",
    "end": "850680"
  },
  {
    "text": "container ID um did I forgot yeah and the TCP query uh the TCP Q Matrix they are",
    "start": "850680",
    "end": "857680"
  },
  {
    "text": "really low volume so we can we currently don't do the um materialized view there we just um merge the metadata why we",
    "start": "857680",
    "end": "864639"
  },
  {
    "text": "query them and now talking about querying if I haven't forgot something",
    "start": "864639",
    "end": "869720"
  },
  {
    "text": "no I think I'm fine Matt will take over and will show you the yourui and give you",
    "start": "869720",
    "end": "876560"
  },
  {
    "text": "demo the part of what we wanted to accomplish with the UI is make it simple to find out what's going on right I mean",
    "start": "877839",
    "end": "885000"
  },
  {
    "text": "as I said before the most common use case for using this this tool is during problem so you want to make sure that",
    "start": "885000",
    "end": "890560"
  },
  {
    "text": "whatever you're doing matches the way we think about the network at Shopify but also makes it easy to you know",
    "start": "890560",
    "end": "897199"
  },
  {
    "text": "accomplish your goal little bit of of you know kind of about",
    "start": "897199",
    "end": "903480"
  },
  {
    "text": "our tool observe right so basically all of this Liv as a custom plugin um into",
    "start": "903480",
    "end": "908880"
  },
  {
    "text": "grafana we have a separate like API that we have built that sits on top of Click",
    "start": "908880",
    "end": "914360"
  },
  {
    "text": "house it does query optimization and a bunch of other different additional facilities allows us to query different",
    "start": "914360",
    "end": "919880"
  },
  {
    "text": "data sets different tables um and it builds a nice little abstraction layer so that the developers don't have to",
    "start": "919880",
    "end": "925560"
  },
  {
    "text": "worry about writing a buch of SQL queries um and they can quer a common link it works really well in this case",
    "start": "925560",
    "end": "931560"
  },
  {
    "text": "and and for this particular use case it's great um first thing we want to",
    "start": "931560",
    "end": "937600"
  },
  {
    "text": "have is a network flow view right we want to be able to see what's actually happening with TCP or UDP traffic when",
    "start": "937600",
    "end": "943880"
  },
  {
    "text": "incoming or outgoing trying to accomplish here uh we can data table down at the bottom",
    "start": "943880",
    "end": "950720"
  },
  {
    "text": "basically Group by different attributes different metadata as Sebastian was talking about right so some of the",
    "start": "950720",
    "end": "956079"
  },
  {
    "text": "metadata that we have is actually Shopify specific right so if I want to say by cluster roll right that's not a",
    "start": "956079",
    "end": "962279"
  },
  {
    "text": "that's not a kubernetes thing that's very much a Shopify thing but we can enhance that metadata andrich it with",
    "start": "962279",
    "end": "967680"
  },
  {
    "text": "additional Fields so that we can give our our you know internal customers something a little bit better to work",
    "start": "967680",
    "end": "973920"
  },
  {
    "text": "with Beats their own",
    "start": "973920",
    "end": "979480"
  },
  {
    "text": "criteria for the DNS view a little bit simpler um we're really just trying to understand where failures are happening",
    "start": "979680",
    "end": "985399"
  },
  {
    "text": "and how what those failures look like still same type of filtering very common this particular UI is is very based on",
    "start": "985399",
    "end": "992680"
  },
  {
    "text": "the same componentry that we use for our log and Trace analytics um so it was something that we could actually support",
    "start": "992680",
    "end": "999120"
  },
  {
    "text": "long term without having have a bunch of custom custom code in there um and something that is leverages a bunch of",
    "start": "999120",
    "end": "1004240"
  },
  {
    "text": "common component excellent okay so now I'm gonna unfortunately I don't have a live demo",
    "start": "1004240",
    "end": "1010680"
  },
  {
    "text": "for you I didn't sacrifice to the demo guys today so want to um work",
    "start": "1010680",
    "end": "1019759"
  },
  {
    "text": "but El correctly I'll walk you through go okay so what you see here is our npm",
    "start": "1019759",
    "end": "1026959"
  },
  {
    "text": "investigation tool um first you want to start by narrowing down your local or remote down to",
    "start": "1026959",
    "end": "1033839"
  },
  {
    "text": "something that you're trying to to see the flow between here I'm going to narrow down our our local uh cluster to",
    "start": "1033839",
    "end": "1042400"
  },
  {
    "text": "our promethus clusters which is our internal metric system and I just want to make sure that I'm matching something",
    "start": "1042400",
    "end": "1047760"
  },
  {
    "text": "that is actually has a remote cluster so here we're talking about container to container connections and cluster I'm",
    "start": "1047760",
    "end": "1054240"
  },
  {
    "text": "going to group by a couple different attributes uh name space can Group by cluster region right whatever we want",
    "start": "1054240",
    "end": "1059960"
  },
  {
    "text": "there um so I'm just going through and selecting whatever attributes I want to I want to see so I can see how the",
    "start": "1059960",
    "end": "1065840"
  },
  {
    "text": "traffic flows between these different endpoints um here I think we're doing",
    "start": "1065840",
    "end": "1072000"
  },
  {
    "text": "deployment so basically the deployment name right whether kind of just abstracted the concept there it's not all just only deployments any staple",
    "start": "1072000",
    "end": "1078840"
  },
  {
    "text": "slider that will also um grouping by the remotes I'm just",
    "start": "1078840",
    "end": "1085080"
  },
  {
    "text": "going to focus on TCP traffic here I don't particularly care about the round trip time right now I just want to see packet flow um and go ahead and run",
    "start": "1085080",
    "end": "1092919"
  },
  {
    "text": "query so one of the things you'll notice is when we talked about the volume of data that we are dealing with we're dealing with a ton of network",
    "start": "1092919",
    "end": "1099760"
  },
  {
    "text": "connections the filtering is extremely quick right this gives us back real-time results quickly um I can sort that table",
    "start": "1099760",
    "end": "1107200"
  },
  {
    "text": "by any value that I care about uh here I'm looking local remote volume",
    "start": "1107200",
    "end": "1112480"
  },
  {
    "text": "you have any understanding of Thanos some of these things may actually sound familiar to you that's a different topic",
    "start": "1112480",
    "end": "1118360"
  },
  {
    "text": "though and here in the table you can kind of see that to the right I can keep scrolling and I have additional metrics",
    "start": "1118360",
    "end": "1125240"
  },
  {
    "text": "that I can look at and this just tells us hey here's where we're potentially seeing issues here's where we're seeing",
    "start": "1125240",
    "end": "1130280"
  },
  {
    "text": "know problems if I've got a bunch of PCP retries then probably something's wrong",
    "start": "1130280",
    "end": "1135559"
  },
  {
    "text": "there or if I'm seeing a ton of uh connections established but not closed right then we potentially have something",
    "start": "1135559",
    "end": "1141159"
  },
  {
    "text": "with connection leaks happening there um it's just a kind of a real quick Insight now I'm not a network engineer",
    "start": "1141159",
    "end": "1147520"
  },
  {
    "text": "Sebastian's not a network engineer you know we're observability people but you know our goal is to try to help those",
    "start": "1147520",
    "end": "1152840"
  },
  {
    "text": "Network Engineers figure out what's going on here um and a lot of what we drove off of this is just taking their",
    "start": "1152840",
    "end": "1159159"
  },
  {
    "text": "you know kind of concerns here listening to them and then sure we were able to implement um the cool thing about",
    "start": "1159159",
    "end": "1165000"
  },
  {
    "text": "Network traffic right is I can open a connection here to something over over here and data flows in both directions",
    "start": "1165000",
    "end": "1171559"
  },
  {
    "text": "right so I sometimes I need to actually see what's going on there and where we're pushing and receiving data from so",
    "start": "1171559",
    "end": "1176760"
  },
  {
    "text": "being able to switch in quickly from outgoing to incoming traffic and see how that data is Flowing is key to",
    "start": "1176760",
    "end": "1183159"
  },
  {
    "text": "understanding the health of the network um and kind of there obviously some of these values are decently sized um",
    "start": "1183159",
    "end": "1191120"
  },
  {
    "text": "there's a lot of this is keep in mind this is all within our own internal metrics clusters right now so that's a lot of data we're shipping around but",
    "start": "1191120",
    "end": "1198760"
  },
  {
    "text": "you kind of get the uh I will'll get to questions in just a second yeah um we'll get to you can kind of see here how",
    "start": "1198760",
    "end": "1205679"
  },
  {
    "text": "we're able to to quickly get some insights into what's going on inside of our own system I will say that when",
    "start": "1205679",
    "end": "1210799"
  },
  {
    "text": "Sebastian and I first built this we started seeing the data here we started seeing problems in our own clusters and",
    "start": "1210799",
    "end": "1216600"
  },
  {
    "text": "we were like oh we should probably fix that so um it's been impactful even just for for us as",
    "start": "1216600",
    "end": "1224320"
  },
  {
    "text": "a uh real quick",
    "start": "1225039",
    "end": "1230520"
  },
  {
    "text": "okay obviously I don't think we have a ton of UDP traffic in our clusters you'll see some here but you know at",
    "start": "1230520",
    "end": "1236840"
  },
  {
    "text": "Shopify we use a lot of statsd so if we're trying to debug something in a client cluster right like that statsd",
    "start": "1236840",
    "end": "1242960"
  },
  {
    "text": "you have to see how that UDP traffic is Flowing this is key for us as the observability team in troubleshooting our own system um let me just pop ahead",
    "start": "1242960",
    "end": "1254320"
  },
  {
    "text": "so moving to DNS queries um you can see here I mean ideally what we're trying to",
    "start": "1260120",
    "end": "1266360"
  },
  {
    "text": "understand here is when things are failing right that's where our Network team is most likely engaged is when",
    "start": "1266360",
    "end": "1271880"
  },
  {
    "text": "there's a problem so being able to see where the failure codes are being able to see you know where that's happening",
    "start": "1271880",
    "end": "1278440"
  },
  {
    "text": "and you know we do a lot of if you again know anything about Thanos Thanos does a lot of of taking end points and doing",
    "start": "1278440",
    "end": "1284360"
  },
  {
    "text": "the finding the actual end points for a service and being to solve those there's some ISS there obviously outside of our",
    "start": "1284360",
    "end": "1290240"
  },
  {
    "text": "side that we got to take a look at but you can kind of get a feel for for how this tool can be used to",
    "start": "1290240",
    "end": "1297159"
  },
  {
    "text": "successfully",
    "start": "1297159",
    "end": "1300159"
  },
  {
    "text": "troot okay",
    "start": "1304760",
    "end": "1308760"
  },
  {
    "text": "yeah so let's talk about limitations because um I mentioned this already",
    "start": "1320960",
    "end": "1326360"
  },
  {
    "text": "currently we only everything is running in our kubernetes Fleet and we have more we have V machines we have Google cloud",
    "start": "1326360",
    "end": "1333880"
  },
  {
    "text": "run so all the stuff we are not watching it and also every service that's um",
    "start": "1333880",
    "end": "1339240"
  },
  {
    "text": "outside of Shopify um an external service that we reach out via HTTP or uh",
    "start": "1339240",
    "end": "1345799"
  },
  {
    "text": "manage service like a database we we don't the only thing we know about this is the IP and um and also yeah within",
    "start": "1345799",
    "end": "1354559"
  },
  {
    "text": "the Clusters we don't support the cluster I like services and um host",
    "start": "1354559",
    "end": "1360320"
  },
  {
    "text": "Network yet it's just we don't have implemented it yet it's it's just more effort we have to do it um yeah and and",
    "start": "1360320",
    "end": "1367240"
  },
  {
    "text": "we will do this in the future um as I said filtering and grouping Curr is",
    "start": "1367240",
    "end": "1373080"
  },
  {
    "text": "limited to information within the Shopify kubernetes clusters or IPS",
    "start": "1373080",
    "end": "1379440"
  },
  {
    "text": "and um everything outside we we we don't have information yet yet yet we are",
    "start": "1379440",
    "end": "1385360"
  },
  {
    "text": "working on this but obviously it it needs it we need to extend how this whole metadata type uh part will work so",
    "start": "1385360",
    "end": "1392400"
  },
  {
    "text": "it will be a lot of work I guess and here's yeah here the the Outlook is like adding those external metadata adding",
    "start": "1392400",
    "end": "1400320"
  },
  {
    "text": "node and cluster IP support um adding metadata for non kubernetes workloads",
    "start": "1400320",
    "end": "1407880"
  },
  {
    "text": "like like you for example you could you could get traces from our services and then",
    "start": "1407880",
    "end": "1415240"
  },
  {
    "text": "resolve um uh URLs to IPS and get those metadata so you can you can map external",
    "start": "1415240",
    "end": "1423080"
  },
  {
    "text": "Services um also we are working closely with our um customers or with our like",
    "start": "1423080",
    "end": "1428640"
  },
  {
    "text": "internal users to enhance features um according to the feedback and leverage",
    "start": "1428640",
    "end": "1433799"
  },
  {
    "text": "the underlying platform to surface new insights into network activity um one example here would be um inter zone or",
    "start": "1433799",
    "end": "1441279"
  },
  {
    "text": "inter region traffic because this is usually what drives up your cost for High um traffic services so we might try",
    "start": "1441279",
    "end": "1448000"
  },
  {
    "text": "to make this visible and if it's possible for the users to place related work notes or work notes that talk to",
    "start": "1448000",
    "end": "1454400"
  },
  {
    "text": "each other within the same region or even within the same Zone that can bring",
    "start": "1454400",
    "end": "1460240"
  },
  {
    "text": "down costs um by a lot and yeah that's is that's it like you have any questions",
    "start": "1460240",
    "end": "1467600"
  },
  {
    "text": "and here can also leave feedback so",
    "start": "1467600",
    "end": "1474600"
  },
  {
    "text": "questions so in the future for um alerts",
    "start": "1475480",
    "end": "1480919"
  },
  {
    "text": "thresholds break fix workflows tickets that's one set of questions my other set",
    "start": "1480919",
    "end": "1486279"
  },
  {
    "text": "of question is um considering uh Ai workloads and uh HBC workloads which",
    "start": "1486279",
    "end": "1492919"
  },
  {
    "text": "have a large amount of parallel Nicks and are you do you have any plans for a future RDMA SL Rocky",
    "start": "1492919",
    "end": "1499919"
  },
  {
    "text": "support yeah I mean I think the answer to that is we're thinking about as much of that as we can I mean I think the",
    "start": "1499919",
    "end": "1505640"
  },
  {
    "text": "primary next step for us is to expand the metadata and get something like an alerting pipeline in place we have",
    "start": "1505640",
    "end": "1511760"
  },
  {
    "text": "alerts that fire off of different metrics um right now but that's that you",
    "start": "1511760",
    "end": "1517039"
  },
  {
    "text": "know they they come through a different pipeline but being able to alert on this particular data is something that we are",
    "start": "1517039",
    "end": "1522880"
  },
  {
    "text": "kind of looking at how we can accomplish and are you publishing an API",
    "start": "1522880",
    "end": "1528919"
  },
  {
    "text": "no uh our this API is internal and it's changing so rapidly that I don't think we could actually publish it um but it's",
    "start": "1528919",
    "end": "1535960"
  },
  {
    "text": "uh you know our goal here is to service Shopify primarily right now and then",
    "start": "1535960",
    "end": "1541120"
  },
  {
    "text": "what the future is beyond that is is really kind of outside of our hands okay thank you uh thanks for the talk um I'm",
    "start": "1541120",
    "end": "1548880"
  },
  {
    "text": "curious a little bit about your prober are you all using data dogs open source stuff under the hood or do you have",
    "start": "1548880",
    "end": "1555039"
  },
  {
    "text": "custom stuff built in how do you yeah I mean uh to give some context on this project we built this within two weeks",
    "start": "1555039",
    "end": "1562279"
  },
  {
    "text": "so there was no much space for doing custom stuff and yeah the we we just",
    "start": "1562279",
    "end": "1567320"
  },
  {
    "text": "used the system Pro which works like to get to this point yourself I I think you",
    "start": "1567320",
    "end": "1574679"
  },
  {
    "text": "have to invest really a lot because this thing is super efficient awesome",
    "start": "1574679",
    "end": "1581679"
  },
  {
    "text": "thanks you showed a number of tools uh ebpf tools uh in your architecture",
    "start": "1581760",
    "end": "1587760"
  },
  {
    "text": "diagram uh but then I heard you specifically called out celium is that what's",
    "start": "1587760",
    "end": "1592960"
  },
  {
    "text": "generating the the network metrics for you in this case selum is managing the",
    "start": "1592960",
    "end": "1598679"
  },
  {
    "text": "ebpf like the actual prob themselves so those BPF programs and your prober is",
    "start": "1598679",
    "end": "1604600"
  },
  {
    "text": "reading uh the shared memory maybe to make the like there's this um open",
    "start": "1604600",
    "end": "1609760"
  },
  {
    "text": "Telemetry networking project the new one like the difference to our solution is",
    "start": "1609760",
    "end": "1615039"
  },
  {
    "text": "why we use click house is we don't do metrix we we actually capture the flows",
    "start": "1615039",
    "end": "1620480"
  },
  {
    "text": "itself and adjust them which you can't do I mean you can do it with metric but the cardinality will be will grow so",
    "start": "1620480",
    "end": "1628200"
  },
  {
    "text": "fast that like it would be an insane am you you would to need scale yours",
    "start": "1628200",
    "end": "1633600"
  },
  {
    "text": "whatever metric solution have to an insane size so that's the difference I I also had a question about outlier",
    "start": "1633600",
    "end": "1639840"
  },
  {
    "text": "detection at which layer in your stack here uh would you be able to do",
    "start": "1639840",
    "end": "1646320"
  },
  {
    "text": "outlier I didn't quite hear that outlier detection oh yeah so like anomaly",
    "start": "1646320",
    "end": "1653000"
  },
  {
    "text": "detection yeah where would you have to implement that because it sounds like you end up with an aggregate so it seems",
    "start": "1653000",
    "end": "1661120"
  },
  {
    "text": "like somewhere in the pipeline would have to introduce the logic yeah I mean I think you know as",
    "start": "1661120",
    "end": "1668840"
  },
  {
    "text": "Sebastian said we with this project we we kind of did the 80% in two weeks and then you a little bit more to get it",
    "start": "1668840",
    "end": "1674840"
  },
  {
    "text": "fully we haven't really thought through anomaly we do have some anomaly detection systems inside of Shopify um",
    "start": "1674840",
    "end": "1681080"
  },
  {
    "text": "so there there's you know again we're we're kind of on year three of this journey in total of of in-housing a lot",
    "start": "1681080",
    "end": "1687880"
  },
  {
    "text": "of our observability um so we expect to continue to to build on top of what we",
    "start": "1687880",
    "end": "1694200"
  },
  {
    "text": "built so far something that I mean the idea here would be generate metric like",
    "start": "1694200",
    "end": "1699679"
  },
  {
    "text": "do what open Telemetry networking does but do it from The Click house generate metrics from that and then do anomaly",
    "start": "1699679",
    "end": "1706640"
  },
  {
    "text": "detection on top of that that's I think that would be the way okay SEC a secondary set of metrics that it would",
    "start": "1706640",
    "end": "1712880"
  },
  {
    "text": "generate in the uh in inside of the click house processing yeah okay and",
    "start": "1712880",
    "end": "1718080"
  },
  {
    "text": "then maybe uh uh some postprocessing of that okay thank",
    "start": "1718080",
    "end": "1723320"
  },
  {
    "text": "you uh great presentation thank you um was there any discussion or consideration on the on the enrichment",
    "start": "1723320",
    "end": "1730200"
  },
  {
    "text": "and storage engine used for this or was click house already present in just a great fit yeah I I think that we we are using",
    "start": "1730200",
    "end": "1737720"
  },
  {
    "text": "click in other areas and if you go to um I think it's on our YouTube channel we have we have some of other talks that",
    "start": "1737720",
    "end": "1744679"
  },
  {
    "text": "were given about our observability systems um so it was kind of a natural fit to try it there as we had that that",
    "start": "1744679",
    "end": "1751720"
  },
  {
    "text": "kind of operational capability it was like a playground project a bit for me to try new",
    "start": "1751720",
    "end": "1757559"
  },
  {
    "text": "stuff and unrelated question is shapify doing any accelerated on a accelerated Network traffic like colel bypass and is",
    "start": "1757559",
    "end": "1763799"
  },
  {
    "text": "that considered at all in this monitoring platform currently uh sorry that's not coming",
    "start": "1763799",
    "end": "1768919"
  },
  {
    "text": "through super loud I'm sorry I'm sorry like are you guys doing anything like onload acceleration any kernel bypass in the network stack that would um uh that",
    "start": "1768919",
    "end": "1775279"
  },
  {
    "text": "would require additional steps to capture Network flow data that is something that would have",
    "start": "1775279",
    "end": "1781640"
  },
  {
    "text": "be better served for one of our Network Engineers that I am not one of right thank you um I think here's a question",
    "start": "1781640",
    "end": "1788480"
  },
  {
    "text": "yeah I have a question is uh all the exporters that you're using are those open source are your homegrown the",
    "start": "1788480",
    "end": "1795720"
  },
  {
    "text": "export is homegrown yes but um the system Pro itself you can find uh you",
    "start": "1795720",
    "end": "1801640"
  },
  {
    "text": "can find it uh it's in it's open source and there's a Proto buff so it's pretty",
    "start": "1801640",
    "end": "1807399"
  },
  {
    "text": "easy to write this exporter so what the exporter does is mostly scraping and forwarding and using container D to get",
    "start": "1807399",
    "end": "1815760"
  },
  {
    "text": "the um process ID to container ID mapping and no we don't plan to open",
    "start": "1815760",
    "end": "1821120"
  },
  {
    "text": "source it for now sorry thank you actually to answer the gentleman's question is when you using a cal bypass",
    "start": "1821120",
    "end": "1828039"
  },
  {
    "text": "onload uh ebpf actually picks it up again Sor I didn't get the question what",
    "start": "1828039",
    "end": "1834000"
  },
  {
    "text": "was it no no question it was the answering to him oh yeah that uh if you're using colel bypass using onload",
    "start": "1834000",
    "end": "1840120"
  },
  {
    "text": "or similar product uh ebpf will pick it up even if it's user space EF all will",
    "start": "1840120",
    "end": "1846200"
  },
  {
    "text": "pick up all the transactions thank I'm sorry like for the EF I worked with our Network Engineers they helped me to get",
    "start": "1846200",
    "end": "1852880"
  },
  {
    "text": "this all working hi uh thanks for the talk um so when you say system probe is",
    "start": "1852880",
    "end": "1858600"
  },
  {
    "text": "it uh so data dog agents component is also called system probe is there an overlap here or are you calling your",
    "start": "1858600",
    "end": "1865200"
  },
  {
    "text": "component also system prob is is the is the question if it's the system probe from data do agent yeah yeah it is the",
    "start": "1865200",
    "end": "1871360"
  },
  {
    "text": "system prob from data do agent you can't just run them separately and it's very lightweight process that will you can",
    "start": "1871360",
    "end": "1877480"
  },
  {
    "text": "configure which information to get cool um so the flow related data that you're",
    "start": "1877480",
    "end": "1884919"
  },
  {
    "text": "collecting is from system prob but not from from celium like you could export them from celium Hubble as well because",
    "start": "1884919",
    "end": "1892679"
  },
  {
    "text": "especially when you're talking about uh cluster IP resolution and if you add service mesh on top of it uh who",
    "start": "1892679",
    "end": "1900240"
  },
  {
    "text": "resolves the cluster IP to the endpoint IP depends on whether you're using celium cluster IP resolution or service",
    "start": "1900240",
    "end": "1907080"
  },
  {
    "text": "meses resolution and Hubble kind of sols for",
    "start": "1907080",
    "end": "1912200"
  },
  {
    "text": "that so do you plan on reimplementing some of it or do you use Hubble I mean to to Sebastian's point",
    "start": "1912200",
    "end": "1918639"
  },
  {
    "text": "this the getting up and running was the biggest thing right because this was a gap in functionality so using what we could off",
    "start": "1918639",
    "end": "1925399"
  },
  {
    "text": "the shelf we have looked at psyllium Hubble custom probes things like that that is something that we not us but our",
    "start": "1925399",
    "end": "1933000"
  },
  {
    "text": "Shopify as a whole has looked at um the idea is primarily that that that",
    "start": "1933000",
    "end": "1939120"
  },
  {
    "text": "interface and how we extract the data is less important than getting the data out right now but eventually we'll probably",
    "start": "1939120",
    "end": "1945240"
  },
  {
    "text": "have something more custom there this was just the leg up using an open source component to get a step in the right",
    "start": "1945240",
    "end": "1950279"
  },
  {
    "text": "direction so that then we can figure out what we actually want long term got it yeah we will be expanding on the system",
    "start": "1950279",
    "end": "1955480"
  },
  {
    "text": "for sure cool and what is the UI that you're using it looks great it's grafana",
    "start": "1955480",
    "end": "1961200"
  },
  {
    "text": "with custom plugins yeah everything we showed you was actually custom code that runs inside of a it like grafana is kind",
    "start": "1961200",
    "end": "1967360"
  },
  {
    "text": "of like it's a custom grafana plugin um nothing you saw was actually grafana except for the",
    "start": "1967360",
    "end": "1973360"
  },
  {
    "text": "outside of the everything else is is we wrote internally interesting yeah thank",
    "start": "1973360",
    "end": "1980840"
  },
  {
    "text": "you um I'm just wondering if you capture any ler 7 data in the flows what kind of",
    "start": "1981159",
    "end": "1986760"
  },
  {
    "text": "data uh layer 7 just kind of like like details of HTTP request it's captured",
    "start": "1986760",
    "end": "1992480"
  },
  {
    "text": "but we don't forward it yet I think the system prob can do it but we are not forwarding it yet okay cool",
    "start": "1992480",
    "end": "1999080"
  },
  {
    "text": "thanks any more questions we have two more minutes",
    "start": "1999080",
    "end": "2005480"
  },
  {
    "text": "yeah oh you I can repeat the question okay I was just going to ask is there a reason that you guys went with a custom",
    "start": "2005480",
    "end": "2012399"
  },
  {
    "text": "uh plugin on top of grafana for the visualization as opposed like building dashboards with uh template",
    "start": "2012399",
    "end": "2018320"
  },
  {
    "text": "variables yeah I mean I think that when we started the pro this process right grafana offered a lot of what we you",
    "start": "2018320",
    "end": "2024480"
  },
  {
    "text": "know kind of our primary metric stack is based in the Prometheus world right and",
    "start": "2024480",
    "end": "2029639"
  },
  {
    "text": "gra plays really well with that um so that offered us again that kind of like leg up in the journey that wasn't",
    "start": "2029639",
    "end": "2035159"
  },
  {
    "text": "something we had to build or find a different solution for um that just that getting the custom plugin there for",
    "start": "2035159",
    "end": "2042120"
  },
  {
    "text": "these types of things um you know it allows us to integrate a lot of some of the charts you can see come are are you",
    "start": "2042120",
    "end": "2048599"
  },
  {
    "text": "know default graphon charts but they that we've kind of wrapped with our own custom componentry um but it's again",
    "start": "2048599",
    "end": "2055960"
  },
  {
    "text": "like we're spending time on the things that are most effective for Shopify and we don't need to reinvent the wheels",
    "start": "2055960",
    "end": "2061878"
  },
  {
    "text": "kind of where we started from I think that's it then all right",
    "start": "2061879",
    "end": "2068079"
  },
  {
    "text": "thank you well thank you all for coming appreciate it and hope you enjoy the rest of your day",
    "start": "2068079",
    "end": "2074720"
  }
]