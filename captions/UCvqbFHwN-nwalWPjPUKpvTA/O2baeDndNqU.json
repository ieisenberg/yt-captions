[
  {
    "text": "hello everyone um Welcome to our talk we are here to share our experiences on how",
    "start": "480",
    "end": "7000"
  },
  {
    "text": "we reduced the devops toil on eliminating the patching and up uh",
    "start": "7000",
    "end": "14200"
  },
  {
    "text": "upgrading toil with cluster autoscaler so I'm Shake Israel and I go",
    "start": "14200",
    "end": "22439"
  },
  {
    "text": "by Israel um I've been with Oracle for the last 3 years and I'm I'm just starting",
    "start": "22439",
    "end": "29480"
  },
  {
    "text": "out in the kubernetes world and I've I'm I'm actually really loving it so I'm a",
    "start": "29480",
    "end": "35320"
  },
  {
    "text": "kubernetes Enthusiast and I actually I'm very passionate about solving distributed system problems I am really",
    "start": "35320",
    "end": "42600"
  },
  {
    "text": "enjoying this this is my first conference by the way and I'm very excited to be here to be on so many",
    "start": "42600",
    "end": "48879"
  },
  {
    "text": "talks and meet so many people here so thank you cubec con for arranging",
    "start": "48879",
    "end": "53960"
  },
  {
    "text": "this uh Hey folks uh my name is John Moore most people call me J Mo uh I am",
    "start": "53960",
    "end": "59079"
  },
  {
    "text": "an SRE by trade uh probably got somewhere close to 20 years now in the industry um started off in networking",
    "start": "59079",
    "end": "66159"
  },
  {
    "text": "and stumbl my way through programming eventually got to um basically data",
    "start": "66159",
    "end": "71360"
  },
  {
    "text": "stores I don't know what it is about them but I fell in love with them uh loved mongodb I don't know what it was",
    "start": "71360",
    "end": "76400"
  },
  {
    "text": "about it but document databases really turned the turned the page for me um before I knew it I started playing",
    "start": "76400",
    "end": "82000"
  },
  {
    "text": "around with kubernetes and honestly I'm never going to look back so first of all a little bit about",
    "start": "82000",
    "end": "89600"
  },
  {
    "text": "us and kind of where why we're here um Israel and I we work in oci that's the Oracle Cloud infrastructure uh we",
    "start": "89600",
    "end": "96479"
  },
  {
    "text": "operate the service known as OSS which is the Oracle streaming service so much like Kinesis and AWS um we offer a fully",
    "start": "96479",
    "end": "105520"
  },
  {
    "text": "scalable streaming environment that customers can use for all of their data in Motion in real-time use cases behind",
    "start": "105520",
    "end": "111560"
  },
  {
    "text": "the scenes it's incred incredibly run on top of kubernetes with a lot of staple sets we deal with customers data and we",
    "start": "111560",
    "end": "118399"
  },
  {
    "text": "take that very very serious seriously so whenever we're talking about scale we're usually talking about maintaining uptime",
    "start": "118399",
    "end": "125119"
  },
  {
    "text": "and availability of these backend systems but we're also talking about the sheer number of regions that we offer",
    "start": "125119",
    "end": "130879"
  },
  {
    "text": "our service in so little terminology um cluster autoscaler is",
    "start": "130879",
    "end": "137360"
  },
  {
    "text": "typically called the ca um I call it the cast because sometimes when I hear people say I'm going to go rotate a CA I",
    "start": "137360",
    "end": "143920"
  },
  {
    "text": "just have like a minor cardiac infarction I'm sure some of you folks know what I'm talk this guy's shaking his head he knows what's going on yeah",
    "start": "143920",
    "end": "150400"
  },
  {
    "text": "scares you right uh so I just call the cast it makes explaining stuff to people so much easier U so if I say it and I",
    "start": "150400",
    "end": "156480"
  },
  {
    "text": "sound wrong um I'm just weird uh ok if I drop that that's our version of our our",
    "start": "156480",
    "end": "162400"
  },
  {
    "text": "kubernetes environment um same thing as like AKs eks GK that kind of stuff um",
    "start": "162400",
    "end": "168000"
  },
  {
    "text": "and then node pools uh very similar to like AWS asgs um it's a way for us to templae the different types of worker",
    "start": "168000",
    "end": "173360"
  },
  {
    "text": "nodes that we're going to have in our cluster um and give us a way to kind of facilitate that scale that up and down",
    "start": "173360",
    "end": "179159"
  },
  {
    "text": "that's our that's our interface to doing so so um today we are going to talk uh",
    "start": "179159",
    "end": "185599"
  },
  {
    "text": "about a few things so first we are going to dive into what our patching requirements are then we are going to",
    "start": "185599",
    "end": "193239"
  },
  {
    "text": "talk a little bit about a tool that we developed to automate our patching and",
    "start": "193239",
    "end": "199080"
  },
  {
    "text": "as we grew we grew in a lot of uh so the scale that J was talking about we grew",
    "start": "199080",
    "end": "204120"
  },
  {
    "text": "very much into so we expanded in a lot of regions so that is the scale we are talking about so we had to be present in",
    "start": "204120",
    "end": "211000"
  },
  {
    "text": "a lot of regions um so with it came a new set of problems which which brought",
    "start": "211000",
    "end": "218040"
  },
  {
    "text": "in a new set of requirements then we moved on to figure out okay how do we",
    "start": "218040",
    "end": "223159"
  },
  {
    "text": "fix this automation how do we manage to automate all of our security patching at this scale so we're going to then talk a",
    "start": "223159",
    "end": "231400"
  },
  {
    "text": "little bit about the impact we had with this new solution and then we are going to Deep",
    "start": "231400",
    "end": "236519"
  },
  {
    "text": "dive into how we implemented it",
    "start": "236519",
    "end": "241680"
  },
  {
    "text": "so uh let's talk a little bit about the requirements we had so every month we",
    "start": "241879",
    "end": "247680"
  },
  {
    "text": "have a deadline to patch all of our machines with the latest security monthly image so this image comes with",
    "start": "247680",
    "end": "254120"
  },
  {
    "text": "the latest security fixes that we have to run all of our machines in to meet the compliance",
    "start": "254120",
    "end": "260120"
  },
  {
    "text": "requirements so all these worker notes that we have in our clusters they have",
    "start": "260120",
    "end": "265479"
  },
  {
    "text": "an in-house tool that automatically p uh patches the",
    "start": "265479",
    "end": "270680"
  },
  {
    "text": "machines and we wanted to move into a world that would be that would automate",
    "start": "270680",
    "end": "276479"
  },
  {
    "text": "all of our patching for us so we wanted it to be simple so that it could be",
    "start": "276479",
    "end": "281960"
  },
  {
    "text": "operated in disconnected regions and we wanted it to be hands",
    "start": "281960",
    "end": "287280"
  },
  {
    "text": "off so uh I'm going to talk a little bit about the solution that we first",
    "start": "287280",
    "end": "292600"
  },
  {
    "text": "had one fight so this is a Mortal Combat so um",
    "start": "292600",
    "end": "299639"
  },
  {
    "text": "what we did was we actually created tool that got deployed outside of the kubernetes cluster what this tool did",
    "start": "299639",
    "end": "306720"
  },
  {
    "text": "was it would collect a list of all the nodes that we had in our clusters and",
    "start": "306720",
    "end": "312080"
  },
  {
    "text": "then it would go and sequentially patch all of them now this was deployed as a",
    "start": "312080",
    "end": "317479"
  },
  {
    "text": "privileged pod on the nodes and it would run the in-house tool that we already",
    "start": "317479",
    "end": "322639"
  },
  {
    "text": "have so this this tool would also follow the kubernetes semantics of how you",
    "start": "322639",
    "end": "328759"
  },
  {
    "text": "maintain a node doing maintenance in a node so it would first go drain the node and run the",
    "start": "328759",
    "end": "335440"
  },
  {
    "text": "update this update would automatically reboot the machine so when the machine comes back up it uncor on it and all the",
    "start": "335440",
    "end": "343000"
  },
  {
    "text": "parts that were pending would go back to the node with this tool though what we had",
    "start": "343000",
    "end": "349560"
  },
  {
    "text": "was it would take 25 minutes to patch one node which during which time the",
    "start": "349560",
    "end": "355199"
  },
  {
    "text": "parts running on the on the Node would be unavailable",
    "start": "355199",
    "end": "361319"
  },
  {
    "text": "so as I was talking about the scale oci grew very fast and got like spread very",
    "start": "361319",
    "end": "368120"
  },
  {
    "text": "fast in many regions so we did scale but our deadlines did",
    "start": "368120",
    "end": "373479"
  },
  {
    "text": "not scale right like we had to go uh do our patching within the fixed deadline",
    "start": "373479",
    "end": "378680"
  },
  {
    "text": "so deadlines were actually too close to comfort so at times the other problems",
    "start": "378680",
    "end": "384840"
  },
  {
    "text": "that we saw with this tool was sometimes the nodes would not come back up so it would just be stuck and be waiting for",
    "start": "384840",
    "end": "391560"
  },
  {
    "text": "the node to come back on healthy because it would not be able to uncinate so it",
    "start": "391560",
    "end": "397160"
  },
  {
    "text": "would wait uh on the stuck node and we would have to go and check oh what's the status we would go and look into the",
    "start": "397160",
    "end": "403120"
  },
  {
    "text": "graphs and see oh this thing is stuck because the node did not come up so with",
    "start": "403120",
    "end": "409280"
  },
  {
    "text": "the scale that we grew in we often started to get a lot more compute maintenance notification so we would go",
    "start": "409280",
    "end": "415599"
  },
  {
    "text": "get a notification from comput saying that hey this noce this node needs to be replaced or it has to be rebooted so we",
    "start": "415599",
    "end": "424039"
  },
  {
    "text": "run into we ran into a lot of these kind of cases where we had to go periodically",
    "start": "424039",
    "end": "429319"
  },
  {
    "text": "time to time update our uh nodes like that so one other problem was that with",
    "start": "429319",
    "end": "437599"
  },
  {
    "text": "the compute maintenance that we were doing if we rotated a node out we the node that came out uh the came up would",
    "start": "437599",
    "end": "444400"
  },
  {
    "text": "be on the old version and then we had to go and Patch It Again time to time we also had to deal with a lot of",
    "start": "444400",
    "end": "450599"
  },
  {
    "text": "kubernetes upgrades um and with this scale it became a challenge for",
    "start": "450599",
    "end": "456759"
  },
  {
    "text": "us so my friend James saterfield and I uh we've been sres for quite a while we",
    "start": "458199",
    "end": "464039"
  },
  {
    "text": "tend to sit down every once in a while and just kind of have one of those reflective moments right we ask a lot of",
    "start": "464039",
    "end": "470280"
  },
  {
    "text": "wise when we see something we're like why do we do this why do we put up with this it's like 2023 we should do like",
    "start": "470280",
    "end": "478159"
  },
  {
    "text": "something more modern and at the same time we also realize that trying to IND introduce big changes",
    "start": "478159",
    "end": "484879"
  },
  {
    "text": "can be scary but we don't have to reinvent the wheel you know I might want",
    "start": "484879",
    "end": "490479"
  },
  {
    "text": "to put new tires on my car but I don't want to have to go get like an oval instead of a circle right I want to be",
    "start": "490479",
    "end": "496599"
  },
  {
    "text": "able to use something that other people do I want to be able to go and do things",
    "start": "496599",
    "end": "501759"
  },
  {
    "text": "that if we go and have somebody join our team that it's not just some internal tool that's kind of hard to reason",
    "start": "501759",
    "end": "507560"
  },
  {
    "text": "through and more importantly James and I had actually done something like this in the past but not really for security",
    "start": "507560",
    "end": "513518"
  },
  {
    "text": "patching but when we really saw you know kind of thought about it and sat back we're like wow wouldn't this work here",
    "start": "513519",
    "end": "519360"
  },
  {
    "text": "too and that's where the cast comes in one of the other things that we try",
    "start": "519360",
    "end": "525120"
  },
  {
    "text": "to do here is I think most folks have kind of adopted like a replace versus",
    "start": "525120",
    "end": "530600"
  },
  {
    "text": "repair um it's kind of tough to troubleshoot some issues sometimes and you really don't want to have to keep maintaining these machines over and over",
    "start": "530600",
    "end": "537279"
  },
  {
    "text": "and over again we actually had machines with an an age of like 768 days they had gone through two kuet c rotations uh at",
    "start": "537279",
    "end": "545279"
  },
  {
    "text": "that point in time it's kind of like okay we should probably let this thing go off the greener pastures the other big thing that we",
    "start": "545279",
    "end": "551200"
  },
  {
    "text": "wanted to try to accomplish was we're a big fan of surge we really want to try to favor bringing up something new",
    "start": "551200",
    "end": "556600"
  },
  {
    "text": "before we touch something that's existing and dealing with customer traffic is try to minimize the impact not only to ourselves and our alarms and",
    "start": "556600",
    "end": "563000"
  },
  {
    "text": "our alerting but also to our customers kubernetes upgrades um I'm",
    "start": "563000",
    "end": "568240"
  },
  {
    "text": "sure folks have gone through many of them at this point um I don't know what the oldest kubernetes run time that somebody's run over here but I'm in the",
    "start": "568240",
    "end": "574880"
  },
  {
    "text": "single digit ones you know like 1.2 1.3 something there uh anybody remember pet",
    "start": "574880",
    "end": "580320"
  },
  {
    "text": "sets no y this guy okay minions remember yeah anyways I've been doing kubernetes",
    "start": "580320",
    "end": "587040"
  },
  {
    "text": "for a while and uh upgrades are relatively easy if you can keep them going fast enough and that's one of the",
    "start": "587040",
    "end": "593200"
  },
  {
    "text": "issues we currently had you know that two version bump was a bit of a problem so these no maintenances that Israel",
    "start": "593200",
    "end": "598959"
  },
  {
    "text": "talk talked about they were coming up all the time and we really should be able to handle this if a compute notification comes in why doesn't our",
    "start": "598959",
    "end": "604120"
  },
  {
    "text": "system just like get rid of the note um additionally I think everybody has been trying to focus on cost and we knew that",
    "start": "604120",
    "end": "611000"
  },
  {
    "text": "we were over provisioned in several several situations we had several",
    "start": "611000",
    "end": "617079"
  },
  {
    "text": "machines that kind of had like a little bit of room left and we really could use kubernetes to bin pack a little bit",
    "start": "617079",
    "end": "622240"
  },
  {
    "text": "better we just needed our infrastructure to not be so rigid and most importantly we needed this to remain simple enough",
    "start": "622240",
    "end": "627880"
  },
  {
    "text": "because in those disconnected where we physically cannot see or operate we want to make sure that we are good providers",
    "start": "627880",
    "end": "633839"
  },
  {
    "text": "to our customers there who help us maintain and operate our systems in those disconnected regions so my buddy James and I uh",
    "start": "633839",
    "end": "642040"
  },
  {
    "text": "decided to write up a little dock and propose something we called it project ectasis bonus points if anybody knows",
    "start": "642040",
    "end": "647399"
  },
  {
    "text": "what that means um but basically this was the next stage of the fight round two",
    "start": "647399",
    "end": "654720"
  },
  {
    "text": "fight now we can leverage the cluster autoscaler for all that it's capable of you know the standard it goes ahead and",
    "start": "654720",
    "end": "660959"
  },
  {
    "text": "brings up a machine if you don't have enough places to run a podt if you don't have enough resources defined on on the",
    "start": "660959",
    "end": "666760"
  },
  {
    "text": "existing infrastructure that's there we're huge fans of pdbs we think they're a great feature and we emphasize that",
    "start": "666760",
    "end": "672040"
  },
  {
    "text": "when Engineers want to add new services to our Fleet please think about pod disruption budgets we are going to have",
    "start": "672040",
    "end": "677440"
  },
  {
    "text": "to shoot one of these services in the head but let's maintain availability and consistency with what we're",
    "start": "677440",
    "end": "682600"
  },
  {
    "text": "doing these Replacements of the fleet um it might be an OS image but what if it's",
    "start": "682600",
    "end": "687639"
  },
  {
    "text": "something else what if I want to change the CPU architecture what if I wanted to change the network that it's running on",
    "start": "687639",
    "end": "694320"
  },
  {
    "text": "like a cni type what if I wanted to change basically anything metadata about a node we should be able to turn that",
    "start": "694320",
    "end": "700680"
  },
  {
    "text": "into some sort of trigger kubernetes upgrades that's definitely something we'd have to deal with as well and most",
    "start": "700680",
    "end": "706200"
  },
  {
    "text": "importantly we needed this thing to parallelize because this sequential nature when we really get up to par in",
    "start": "706200",
    "end": "711639"
  },
  {
    "text": "some of our bigger regions it was going to kill us and at most of all I do want to say this I always try to joke at uh",
    "start": "711639",
    "end": "717560"
  },
  {
    "text": "when I talk about this with internal team uh I want to just do like drinks on a beach man that's the kind of stuff that I want to run I want to be able to just",
    "start": "717560",
    "end": "723720"
  },
  {
    "text": "sit back and watch my systems run themselves so when we did this when we",
    "start": "723720",
    "end": "729920"
  },
  {
    "text": "implemented we were kind of shocked at actually how fast it went um using the Custer a the way we did we went from 5",
    "start": "729920",
    "end": "736760"
  },
  {
    "text": "days in our biggest regions with many hundreds of nodes um we went down to a",
    "start": "736760",
    "end": "742560"
  },
  {
    "text": "matter of hours uh in some regions it actually finishes before you could go make a cup of coffee so the Pod pending time is a huge",
    "start": "742560",
    "end": "750360"
  },
  {
    "text": "thing for us we no longer had to touch an existing service we got to basically try it before you buy it with all of our new infrastructure this also opened up",
    "start": "750360",
    "end": "757519"
  },
  {
    "text": "room for us to basically go back and say hey we have an alarm that says if a machine's going to take a while to come",
    "start": "757519",
    "end": "762560"
  },
  {
    "text": "up maybe we could tighten that maybe we can actually get a little bit closer to that kind of realtime infrastructure",
    "start": "762560",
    "end": "768959"
  },
  {
    "text": "workload that we were looking for okay it's I mean we we moved through I don't know 1.18 all the way to 1.25 and",
    "start": "768959",
    "end": "776720"
  },
  {
    "text": "probably what less than a month that was pretty crazy um so and the other thing is the",
    "start": "776720",
    "end": "782120"
  },
  {
    "text": "Adaptive infrastructure by using the cluster Auto scaler we actually gained the ability to scale out as our services",
    "start": "782120",
    "end": "789199"
  },
  {
    "text": "needed to scale themselves up which means we could start doing something like HBA Keta all the fun",
    "start": "789199",
    "end": "795199"
  },
  {
    "text": "stuff the other thing was the migration when we moved to this we introduced a completely different infrastructure",
    "start": "795199",
    "end": "801120"
  },
  {
    "text": "layout but none of our services internally had to deal with it we just changed our node selector nobody even",
    "start": "801120",
    "end": "806880"
  },
  {
    "text": "knew that we moved to a completely different run time so so the next big question was how",
    "start": "806880",
    "end": "815120"
  },
  {
    "text": "did we achieve this so we used uh so we were in a place where we wanted to go to",
    "start": "815120",
    "end": "821079"
  },
  {
    "text": "a Direction Where where we adopted a lot of cloud native uh practices so as J",
    "start": "821079",
    "end": "827480"
  },
  {
    "text": "mentioned earlier we were thinking about implementing cluster autoscaler and we",
    "start": "827480",
    "end": "833079"
  },
  {
    "text": "we also actually Ed the concept of taints and tolerations and we introduced a new",
    "start": "833079",
    "end": "838839"
  },
  {
    "text": "system a new way to version our Hardware by using rotation hash and we're going to talk a little",
    "start": "838839",
    "end": "844880"
  },
  {
    "text": "bit about the infrastructure layout that we had and how the rotation hash is integrated into our infrastructure we",
    "start": "844880",
    "end": "851600"
  },
  {
    "text": "talk we're going to talk about a little bit about the Rotator parts so these parts are basically deployments that we",
    "start": "851600",
    "end": "859120"
  },
  {
    "text": "inject the rotation hash with then we'll touch a little bit about how we use",
    "start": "859120",
    "end": "864480"
  },
  {
    "text": "pre-op hooks to make our service more resilient since we are operating a lot",
    "start": "864480",
    "end": "869639"
  },
  {
    "text": "of stateful sets so so what is the cluster Auto scaler uh",
    "start": "869639",
    "end": "877079"
  },
  {
    "text": "there's three basic things that it does scale up scale in scale down it's as simple as",
    "start": "877079",
    "end": "884120"
  },
  {
    "text": "that so Israel put together a cool little animation here while I'll talk about it so when we scale up it's",
    "start": "884120",
    "end": "889839"
  },
  {
    "text": "typically I make a pod I ask for it to be created and if I don't have the resources to schedule it the coupe scheder is like man I don't know what to",
    "start": "889839",
    "end": "896199"
  },
  {
    "text": "do well the cluster Auto scaler sitting there going like like hm I want every pod to have a home so let me go and find",
    "start": "896199",
    "end": "902639"
  },
  {
    "text": "the right place in my infrastructure to scale this up just curious show a hands who here has run the cluster Auto",
    "start": "902639",
    "end": "908800"
  },
  {
    "text": "scaler awesome it's pretty simple but there's Beauty in that Simplicity do you",
    "start": "908800",
    "end": "914519"
  },
  {
    "text": "really have to overthink every problem like the folks who were behind this they put a lot of time and energy into this",
    "start": "914519",
    "end": "920759"
  },
  {
    "text": "so This scaleup mechanism that we use just going to run through it real quick so you can see it and it's going to go",
    "start": "920759",
    "end": "926120"
  },
  {
    "text": "ahead and prision a new node whenever a pod came be scheduled then it's going to get that",
    "start": "926120",
    "end": "932199"
  },
  {
    "text": "it's going to get that pod ready and then once it's ready your pod is now running on it simple as that so the",
    "start": "932199",
    "end": "938199"
  },
  {
    "text": "scale down discipline's a little bit more fun so first of all when the",
    "start": "938199",
    "end": "944240"
  },
  {
    "text": "cluster aut scaler wants to scale something down typically you're going to deal with something called the Min utilization percentage um which is going",
    "start": "944240",
    "end": "951360"
  },
  {
    "text": "to say how much utilization of this node do I make it eligible for me to get rid of it right I think the default is what",
    "start": "951360",
    "end": "957600"
  },
  {
    "text": "50% I think that's right what if you made it 100 what if every node in the cluster",
    "start": "957600",
    "end": "964480"
  },
  {
    "text": "could be deleted at any point in time well that's the question that we asked and so we try to do it that way so when",
    "start": "964480",
    "end": "970959"
  },
  {
    "text": "the cluster Auto scaler wants to scale down a node it's going to go ahead and look and say hey hypothetically if I",
    "start": "970959",
    "end": "976519"
  },
  {
    "text": "were to get rid of this node can all the pods here go somewhere else and if they can it'll go ahead and start scheduling",
    "start": "976519",
    "end": "983079"
  },
  {
    "text": "them somewhere else utilizing evictions boom checkbox number one got my pdbs in action",
    "start": "983079",
    "end": "990360"
  },
  {
    "text": "so after the cluster Auto scaler goes through this routine that node is going to end up being empty that's the last",
    "start": "990360",
    "end": "996519"
  },
  {
    "text": "one once you have an empty node you really don't need to keep this thing around anymore all",
    "start": "996519",
    "end": "1004399"
  },
  {
    "text": "right pretty slow slide got to work on some animations so cluster autoscaler",
    "start": "1005399",
    "end": "1011040"
  },
  {
    "text": "configurations um there are way more that I want to go over here but the big ones that stand out to me and I want to",
    "start": "1011040",
    "end": "1017839"
  },
  {
    "text": "call it out scan interval as your clusters start to increase and the and the complexity of your scheduling goes",
    "start": "1017839",
    "end": "1023880"
  },
  {
    "text": "up you really need to start looking at that scale interval so far we're at 10 in most of our regions but I've had to",
    "start": "1023880",
    "end": "1029918"
  },
  {
    "text": "start to L and even monitor it to see if we need to go up to 20 one of the big key takeaways there is keep track of how",
    "start": "1029919",
    "end": "1037160"
  },
  {
    "text": "long it takes your clustered autoscaler to run through your Fleet and ask those what if questions the bigger your",
    "start": "1037160",
    "end": "1043079"
  },
  {
    "text": "clusters get you need to pay attention to this feature secondly new pod scale",
    "start": "1043079",
    "end": "1048720"
  },
  {
    "text": "up um how many folks have like done a deployment and maybe one of your pods",
    "start": "1048720",
    "end": "1053760"
  },
  {
    "text": "has a slower than normal shutdown time right something where it just doesn't go",
    "start": "1053760",
    "end": "1059559"
  },
  {
    "text": "away as fast as you wish it did well you have another pod that's going to come and fill on that Noe as soon as that guy",
    "start": "1059559",
    "end": "1064600"
  },
  {
    "text": "goes away maybe due to an anti-affinity or some other resource constraint right well I don't want to have the cluster",
    "start": "1064600",
    "end": "1070000"
  },
  {
    "text": "autoscaler just churn in noes just burning our infrastructure adding removing adding removing right I really",
    "start": "1070000",
    "end": "1075600"
  },
  {
    "text": "recommend this new pod scaleup delay now we use a flag uh at first when we did",
    "start": "1075600",
    "end": "1081200"
  },
  {
    "text": "this but they do support annotations and I highly recommend that as well so using",
    "start": "1081200",
    "end": "1086640"
  },
  {
    "text": "an annotation different pods have different characteristics for whether they trigger a scale up or not the",
    "start": "1086640",
    "end": "1091880"
  },
  {
    "text": "balancing C node groups is a concept that we used heavily here as well because of those staple sets we were",
    "start": "1091880",
    "end": "1097640"
  },
  {
    "text": "able to make sure that when we moved over to have say three node pools that looked all the same we wanted to make",
    "start": "1097640",
    "end": "1103320"
  },
  {
    "text": "sure that they each scaled up individually so we had good striping across availability zones fault domains",
    "start": "1103320",
    "end": "1108520"
  },
  {
    "text": "concept inside oci for single ad regions so we wanted to definitely set that the true but as Isel mentioned we actually",
    "start": "1108520",
    "end": "1116120"
  },
  {
    "text": "ended up using tolerations and taints to do a lot of this cool work so you need to tell the cluster autoscaler this",
    "start": "1116120",
    "end": "1121320"
  },
  {
    "text": "taint you don't need to worry about this one this one should not be factor in when it comes up to scaling your noes",
    "start": "1121320",
    "end": "1127039"
  },
  {
    "text": "balancing ignore labels this one is interesting because whenever you're getting an infrastructure that has labels already applied for you they may",
    "start": "1127039",
    "end": "1133799"
  },
  {
    "text": "not be part of the labels that are already defaulted and were you know acceptable to the cluster Auto scaler when it treats a group as balanceable or",
    "start": "1133799",
    "end": "1140720"
  },
  {
    "text": "something that is of the same group or set so make sure you take a look at those labels in order to set those up",
    "start": "1140720",
    "end": "1146760"
  },
  {
    "text": "scale down uation one right here every node is always is always eligible to be",
    "start": "1146760",
    "end": "1152400"
  },
  {
    "text": "scaled down and the reason we want that is because the simple the simple rotation that we're going to do needs to",
    "start": "1152400",
    "end": "1158159"
  },
  {
    "text": "be able to run on top of nodes that are perfectly B packed by Design the next",
    "start": "1158159",
    "end": "1163320"
  },
  {
    "text": "thing is the scale down un needed or scale down unneeded time that one um we turn this down because we actually got",
    "start": "1163320",
    "end": "1169919"
  },
  {
    "text": "to the point where we trusted this thing so much we kind of want it to run on like like warp speed so we started",
    "start": "1169919",
    "end": "1177280"
  },
  {
    "text": "tuning all of these things down so uh the next thing we are going",
    "start": "1177280",
    "end": "1183200"
  },
  {
    "text": "to talk about is the tains and tolerations um I like pod Affinity it attracts pods to the nodes",
    "start": "1183200",
    "end": "1191400"
  },
  {
    "text": "so taints is the opposite it basically repels the parts from the nodes so this",
    "start": "1191400",
    "end": "1197520"
  },
  {
    "text": "demonstration here here is a little bit uh that says a little bit about how tolerations and tints work so if you can",
    "start": "1197520",
    "end": "1203640"
  },
  {
    "text": "see the Green Pod is only going to uh be scheduled onto the node which has a",
    "start": "1203640",
    "end": "1209320"
  },
  {
    "text": "which it for which the taint it tolerates right like the green one",
    "start": "1209320",
    "end": "1215360"
  },
  {
    "text": "so we we came up with a New Concept we called it uh called it as a rotation",
    "start": "1215360",
    "end": "1221360"
  },
  {
    "text": "hash so this rotation hash we basically thought of it as why don't we try to",
    "start": "1221360",
    "end": "1228559"
  },
  {
    "text": "version of our infrastructure like we want to move from one one version of infrastructure to another version of",
    "start": "1228559",
    "end": "1234799"
  },
  {
    "text": "infrastructure so every month for example we had to go through the oos patching cycle and since the image would",
    "start": "1234799",
    "end": "1241960"
  },
  {
    "text": "be different in every month so that would be a component that told us that hey you need to change your upgrade your",
    "start": "1241960",
    "end": "1249360"
  },
  {
    "text": "infrastructure so we came up with a couple of things couple of parts that we",
    "start": "1249360",
    "end": "1254600"
  },
  {
    "text": "could use as versioning our infrastructure so for example the OS smage the kubernetes version for example",
    "start": "1254600",
    "end": "1260919"
  },
  {
    "text": "if you wanted to move from one kubernetes version to the other one this also required us to like rotate out the",
    "start": "1260919",
    "end": "1266240"
  },
  {
    "text": "whole Fleet so we added it as a part of the rotation hash similarly moving on from one architecture to another",
    "start": "1266240",
    "end": "1271919"
  },
  {
    "text": "architecture similarly from time to time cloud in it and all that so this is a this is a simple",
    "start": "1271919",
    "end": "1278760"
  },
  {
    "text": "example on how we implemented the rotation hash as part of our monthly patching cycle so every month since we",
    "start": "1278760",
    "end": "1286760"
  },
  {
    "text": "have different OS image you can see in line 28 uh line 20 the image is October",
    "start": "1286760",
    "end": "1293279"
  },
  {
    "text": "OS image and it calculates a rotation hash uh as you can see from the rotation",
    "start": "1293279",
    "end": "1300799"
  },
  {
    "text": "hash parts we just calculate a sha out of it and we get a rotation hash out of it and similarly we do this let's try to",
    "start": "1300799",
    "end": "1308760"
  },
  {
    "text": "evaluate and compare it with the month of November so in November the O image changes and I would quickly show you the",
    "start": "1308760",
    "end": "1316640"
  },
  {
    "text": "difference between these two uh months so on the top you can see the rotation",
    "start": "1316640",
    "end": "1322320"
  },
  {
    "text": "hash for October is different to what we have in November but the only field that",
    "start": "1322320",
    "end": "1327520"
  },
  {
    "text": "changed was the OS image part and this",
    "start": "1327520",
    "end": "1332559"
  },
  {
    "text": "basically helped us version our infrastructure and we applied it to our security",
    "start": "1332559",
    "end": "1338640"
  },
  {
    "text": "pature so want to talk a little bit about pre-stop hooks um running a staple set is kind of hard at sometimes like",
    "start": "1338640",
    "end": "1344840"
  },
  {
    "text": "you you have to deal with the kubernetes knowing about the cluster St now we hadn't gotten to the operator SE",
    "start": "1344840",
    "end": "1351159"
  },
  {
    "text": "the section of our of our workloads um due to some unique constraints but what we could do is we",
    "start": "1351159",
    "end": "1357880"
  },
  {
    "text": "could utilize a lot of the kubernetes tooling that's already built in place such as pre-op hooks so what if we could",
    "start": "1357880",
    "end": "1364039"
  },
  {
    "text": "run a script that actually would inform kubernetes that we can't quite stop yet like we're in a state where everything",
    "start": "1364039",
    "end": "1370159"
  },
  {
    "text": "to you looks cool but inside I need a little cleanup or I need to make sure some data is replicated first that",
    "start": "1370159",
    "end": "1376520"
  },
  {
    "text": "three-way replicated data store is one of the things that we use and we noticed that from time to time during certain",
    "start": "1376520",
    "end": "1382840"
  },
  {
    "text": "failure scenarios that we might not want to have more than one node leave the cluster at a time so these pre stop",
    "start": "1382840",
    "end": "1390279"
  },
  {
    "text": "hooks became Mission critical to being able to maintain our up time the second",
    "start": "1390279",
    "end": "1395320"
  },
  {
    "text": "thing you got to take into account though is termination grace period you got to tell kubernetes that it's going to take a little bit for this pod to",
    "start": "1395320",
    "end": "1401279"
  },
  {
    "text": "shut down and then what's unreasonable so in some of our cases we actually started to put this thing up towards six",
    "start": "1401279",
    "end": "1408039"
  },
  {
    "text": "seven maybe eight hours because we had alarming that said hey if you have a pod that's stuck in terminating and you're",
    "start": "1408039",
    "end": "1413600"
  },
  {
    "text": "in the middle of your pre-stop hook for a while which is constantly emitting metrics I should probably get Al get",
    "start": "1413600",
    "end": "1419000"
  },
  {
    "text": "alarmed and page in somebody to take a look at what's going on something is atypical and I'd like to react",
    "start": "1419000",
    "end": "1425799"
  },
  {
    "text": "appropriately so basically we're going to try to put this all together as zero",
    "start": "1425799",
    "end": "1431159"
  },
  {
    "text": "mentioned we go ahead and we calculate this rotation hash now this Rotator pod that he mentioned too that's just the",
    "start": "1431159",
    "end": "1439000"
  },
  {
    "text": "application side of it but the infrastructure side is really key and it's it's governed by terraform and that",
    "start": "1439000",
    "end": "1444360"
  },
  {
    "text": "was the other aspect of this how can we do this without introducing new tools and what's available to us like again",
    "start": "1444360",
    "end": "1451520"
  },
  {
    "text": "there's Beauty in this Simplicity that sha one sha on as side uh is just a",
    "start": "1451520",
    "end": "1456840"
  },
  {
    "text": "simple hash function that will able to tell us that things are changing so we actually passed that to",
    "start": "1456840",
    "end": "1462080"
  },
  {
    "text": "our nodes so that when they come up they'll have an extra ARG that says hey start up with this taint which is going",
    "start": "1462080",
    "end": "1467320"
  },
  {
    "text": "to repel certain pods so there's this Rotator pod",
    "start": "1467320",
    "end": "1472720"
  },
  {
    "text": "deployment well hold on a second how does our pod know about the infrastructure version that it should",
    "start": "1472720",
    "end": "1479000"
  },
  {
    "text": "Target again we utilize terraform we have to use terraform as part of our deployment mechanism well why not just",
    "start": "1479000",
    "end": "1486080"
  },
  {
    "text": "use a data source so each one of our node pools that we have basically is also going to get a sister or companion",
    "start": "1486080",
    "end": "1492880"
  },
  {
    "text": "Rotator POD at the time of deploy it says what is the current infrastructure version that this node pool will spin up",
    "start": "1492880",
    "end": "1500000"
  },
  {
    "text": "if it were to add a new node add that as part of the deployment and make sure that when it goes",
    "start": "1500000",
    "end": "1505039"
  },
  {
    "text": "out it causes that node or that excuse me that pod to not go on any existing nodes it forces the cluster autoscaler",
    "start": "1505039",
    "end": "1511960"
  },
  {
    "text": "to get involved so one of the other things here is that Max surge so I can sit here we",
    "start": "1511960",
    "end": "1519360"
  },
  {
    "text": "don't really care about the rotation pods but we do get that Max surge capability that we're looking for I get to try it before I buy it and we've all",
    "start": "1519360",
    "end": "1526000"
  },
  {
    "text": "been there I'm sure people have tried to make changes to their infrastructure and then everything works in Dev and then",
    "start": "1526000",
    "end": "1532159"
  },
  {
    "text": "you try to bring it up and your just node doesn't join in time your node comes up with something unique we have",
    "start": "1532159",
    "end": "1537640"
  },
  {
    "text": "all been there I'd really like to maintain not touching my existing software and this worked beautifully one",
    "start": "1537640",
    "end": "1544240"
  },
  {
    "text": "big aspect though is this progress deadline so for those who not familiar a deployment will basically time out the",
    "start": "1544240",
    "end": "1550000"
  },
  {
    "text": "deployment controller will be like man this is taking too long I'm going to give up we have to bump this up a little while and that's okay for us because",
    "start": "1550000",
    "end": "1556520"
  },
  {
    "text": "this particular rotator pod is the only one that we put this on everyone else we alarm on if we can't get to that",
    "start": "1556520",
    "end": "1562039"
  },
  {
    "text": "progress deadline in the in the correct amount of time now Helm is one of the",
    "start": "1562039",
    "end": "1567080"
  },
  {
    "text": "ways that we deploy our apps so just as simple again same tools we already use today just a template variable gets",
    "start": "1567080",
    "end": "1574320"
  },
  {
    "text": "thrown as a toleration and these Rotator pods look for the exact match of this",
    "start": "1574320",
    "end": "1579799"
  },
  {
    "text": "particular hash that was what couples our infrastructure to this rotation again super simple what's also cool is",
    "start": "1579799",
    "end": "1586880"
  },
  {
    "text": "all of our existing Fleet is already ignoring that same rotation hash in terms of an application",
    "start": "1586880",
    "end": "1592360"
  },
  {
    "text": "perspective so we don't repel any of our other pods just the rotators so Israel's",
    "start": "1592360",
    "end": "1598600"
  },
  {
    "text": "going to run through a quick little demonstration of this this deployment here but imagine that this is also at",
    "start": "1598600",
    "end": "1603679"
  },
  {
    "text": "the same time also flowing through the fleet and upgrading our infrastructure we're using that Max Surge and we're",
    "start": "1603679",
    "end": "1609080"
  },
  {
    "text": "gaining the ability to have new infrastructure come up that's vetted before we move",
    "start": "1609080",
    "end": "1616600"
  },
  {
    "text": "forward so to bring it all together let's say we",
    "start": "1616600",
    "end": "1621799"
  },
  {
    "text": "have two notes here right and we see that for a particular note pool all the nodes are tainted in",
    "start": "1621799",
    "end": "1628640"
  },
  {
    "text": "yellow now we are trying to do a rotator part deployment So currently the Rotator",
    "start": "1628640",
    "end": "1633679"
  },
  {
    "text": "part deployment is in V1 so as the as we move on to a new",
    "start": "1633679",
    "end": "1640200"
  },
  {
    "text": "version of our infrastructure let's say it's V2 so our so we got we get a new pod that is pending now which cannot go",
    "start": "1640200",
    "end": "1647279"
  },
  {
    "text": "on to any of the nodes because of the Toleration it has because it has a new Toleration that is pink so now the",
    "start": "1647279",
    "end": "1654159"
  },
  {
    "text": "cluster autoscaler sees that oh this part is spending let's find it at home and it brings up a new node and now you",
    "start": "1654159",
    "end": "1660760"
  },
  {
    "text": "see a new node that comes up with a rotation hash pink now the Pod can actually go and settle in",
    "start": "1660760",
    "end": "1668440"
  },
  {
    "text": "there so once the Pod get gets settled in the deployment progresses further up",
    "start": "1668440",
    "end": "1674159"
  },
  {
    "text": "so it removes one of the older version of the Rotator p Bo so now as Jos said",
    "start": "1674159",
    "end": "1682960"
  },
  {
    "text": "earlier the cluster autoscaler comes into action and it starts its second operation the scaled down operation so",
    "start": "1682960",
    "end": "1690200"
  },
  {
    "text": "the cluster autoscaler sees that there are couple of nodes and let's try to figure out if these parts can live",
    "start": "1690200",
    "end": "1695679"
  },
  {
    "text": "somewhere else so it evaluates it and sees that these parts can be moved to another node and since these parts can",
    "start": "1695679",
    "end": "1703440"
  },
  {
    "text": "be moved to another node I do not need this node anymore so that node goes away",
    "start": "1703440",
    "end": "1708760"
  },
  {
    "text": "and it is scaled on by the cluster Auto scaler so the cluster Auto scaler does this for all of our nodes that have that",
    "start": "1708760",
    "end": "1715880"
  },
  {
    "text": "are in the system so it evaluates the previous node this node and it performs this operation in all the",
    "start": "1715880",
    "end": "1721640"
  },
  {
    "text": "nodes so I want to add a little bit more on that in closing most folks probably run the",
    "start": "1721640",
    "end": "1727919"
  },
  {
    "text": "cluster Auto scaler on a dedicated Fleet of notes outside of the cluster or at least outside the purview of its own",
    "start": "1727919",
    "end": "1733720"
  },
  {
    "text": "ownership we tested this heavily and as one of the core requirements that we had we wanted to make sure that not just the",
    "start": "1733720",
    "end": "1739799"
  },
  {
    "text": "nodes that cluster autoscaler was managing could be replaced what about the cluster autoscaler itself running",
    "start": "1739799",
    "end": "1746279"
  },
  {
    "text": "those as a deployment with multiple replicas was was key for us reaching",
    "start": "1746279",
    "end": "1751360"
  },
  {
    "text": "that goal at this point in time there isn't a single node in any of our fleets that isn't governed by the cluster autoscaler or capable of being killed at",
    "start": "1751360",
    "end": "1758799"
  },
  {
    "text": "any point in time so this is our talk and I wanted to thank you all for coming out um really",
    "start": "1758799",
    "end": "1766240"
  },
  {
    "text": "appreciate it and really appreciate the support it is also my first coupon as well so thank you so much for your time",
    "start": "1766240",
    "end": "1771840"
  },
  {
    "text": "I really hope we get some questions too thank",
    "start": "1771840",
    "end": "1778200"
  },
  {
    "text": "[Applause]",
    "start": "1779050",
    "end": "1782290"
  },
  {
    "text": "you thanks for the great presentation so a question that I have is about the",
    "start": "1784600",
    "end": "1790240"
  },
  {
    "text": "testing process that you had for this uh um I assume it's going to be challenging",
    "start": "1790240",
    "end": "1796559"
  },
  {
    "text": "to do that and and I would love to know a little bit about that and also um another question that I had was",
    "start": "1796559",
    "end": "1803360"
  },
  {
    "text": "about you mentioned about OK what would be different in um your deployment",
    "start": "1803360",
    "end": "1809320"
  },
  {
    "text": "process if you wanted to um do everything in Native uh kubernetes okay um so that's a great",
    "start": "1809320",
    "end": "1816960"
  },
  {
    "text": "question so two that I heard there um first of all the testing process um",
    "start": "1816960",
    "end": "1822320"
  },
  {
    "text": "because we didn't actually introduce any new componentry to the system it wasn't that we needed to go through through",
    "start": "1822320",
    "end": "1827679"
  },
  {
    "text": "extensive validation of existing or new tools all of our existing tools basically we could use the same",
    "start": "1827679",
    "end": "1833200"
  },
  {
    "text": "workflows but what we did start doing is we made sure that this thing runs all",
    "start": "1833200",
    "end": "1838600"
  },
  {
    "text": "the time in several of our testing environments we're constantly running just a little pod think of it like a",
    "start": "1838600",
    "end": "1844240"
  },
  {
    "text": "crown job that's going through and changing these tolerations so one one aspect of the the slides you might have",
    "start": "1844240",
    "end": "1849960"
  },
  {
    "text": "missed is we use something called No execute on that Toleration which means that when you have a mismatch we kick",
    "start": "1849960",
    "end": "1856000"
  },
  {
    "text": "off that Rotator pod so that means that as soon as it gets touched and for say for example we just",
    "start": "1856000",
    "end": "1862000"
  },
  {
    "text": "touch it and say replace me we start the process so we're constantly running this",
    "start": "1862000",
    "end": "1867679"
  },
  {
    "text": "thing through its Paces looking for any regressions that introduce into the system we have regions that are being",
    "start": "1867679",
    "end": "1872840"
  },
  {
    "text": "built tens hundreds if not faster times a day so secondly what makes this unique",
    "start": "1872840",
    "end": "1879440"
  },
  {
    "text": "about oci or is it is let me rephrase that question is there a way to use this in any other cloud and the answer is yes",
    "start": "1879440",
    "end": "1886919"
  },
  {
    "text": "so so because we're using just simple terraform which most people are using today in order to manage their infrastructure you have access to the",
    "start": "1886919",
    "end": "1893120"
  },
  {
    "text": "same hashing functions you have access to that same concept if you really think about it there's probably a way that you",
    "start": "1893120",
    "end": "1898880"
  },
  {
    "text": "look at your infrastructure today and say oh there's this class of nodes for these and there's this class of nodes for these think about what those",
    "start": "1898880",
    "end": "1904840"
  },
  {
    "text": "rotation Parts mean to you they're unique to us and we're constantly adding them the other day I think we added what",
    "start": "1904840",
    "end": "1910799"
  },
  {
    "text": "three or four new things for node labels and all sorts of stuff like it's actually pretty simple and that's where",
    "start": "1910799",
    "end": "1917360"
  },
  {
    "text": "the beauty of this is and that's why we wanted to share it you should be you should be able to look at your infrastructure and really break it down",
    "start": "1917360",
    "end": "1923039"
  },
  {
    "text": "into those small component parts that may change from time to time yeah I'd like to add like we got a lot of uh side",
    "start": "1923039",
    "end": "1930919"
  },
  {
    "text": "um additional benefits with this so when we had the system in place it was easier",
    "start": "1930919",
    "end": "1936600"
  },
  {
    "text": "for us to add the rotation hash Parts like we talked about like if we wanted to move our Fleet from one architecture",
    "start": "1936600",
    "end": "1943440"
  },
  {
    "text": "machine to another architecture machine we could do it with one deployment like and it would be done like all of your",
    "start": "1943440",
    "end": "1948960"
  },
  {
    "text": "Fleet would be just migrated from one infrastructure to another infrastructure and it would do it like the max search",
    "start": "1948960",
    "end": "1954840"
  },
  {
    "text": "and like J Mo said very importantly with for stateful sets you would get a machine first and then your it will not",
    "start": "1954840",
    "end": "1961919"
  },
  {
    "text": "touch your stateful set before you get a new machine that's that's very important yeah yeah great question yeah any other",
    "start": "1961919",
    "end": "1969960"
  },
  {
    "text": "questions oh thank you yeah I'm curious about uh your work organization there it sounds like you guys are the you know",
    "start": "1969960",
    "end": "1976679"
  },
  {
    "text": "inov ators the creative thinkers right you're coming up with new new ways of doing things how how many people are on",
    "start": "1976679",
    "end": "1983600"
  },
  {
    "text": "the team that runs this infrastructure and and what kind of support do you have at the like system administrator I mean",
    "start": "1983600",
    "end": "1990080"
  },
  {
    "text": "obviously you have a lot but you know like I I work in an academic setting where I kind of wear all those hats and",
    "start": "1990080",
    "end": "1997120"
  },
  {
    "text": "it can be a little bit difficult to have enough time to do to do all those things like how much of your time do you get to devote to this Innovation and how much",
    "start": "1997120",
    "end": "2004080"
  },
  {
    "text": "do you just have to do the grunt work of watching pods spin up and spin down",
    "start": "2004080",
    "end": "2009360"
  },
  {
    "text": "great question so um as isra mentioned",
    "start": "2009360",
    "end": "2014480"
  },
  {
    "text": "back in the talk we used to have folks who would sit here and have to watch graphs as we would rotate our Fleet with",
    "start": "2014480",
    "end": "2019559"
  },
  {
    "text": "that manual tool 24/7 we had to have somebody there if it was off hours and somebody wanted to go out to eat or",
    "start": "2019559",
    "end": "2025639"
  },
  {
    "text": "something they'd have to turn off the rotation we'd have to slow down we'd lose time right when you have a system",
    "start": "2025639",
    "end": "2031360"
  },
  {
    "text": "like this and hopefully you have things like Prometheus and alert manager various other alarms that are available",
    "start": "2031360",
    "end": "2037279"
  },
  {
    "text": "you can get notified if it gets stuck and the concept of getting stuck at least in this particular case is",
    "start": "2037279",
    "end": "2043880"
  },
  {
    "text": "typically in the situation of I've run out of compute I got some weird transient error from you know my cloud",
    "start": "2043880",
    "end": "2049960"
  },
  {
    "text": "provider that says pool's closed man like that's basically the things that have held us up but at the most excuse",
    "start": "2049960",
    "end": "2056280"
  },
  {
    "text": "me for the most part we don't actually have to watch this anymore tun your alarms would be my biggest thing you",
    "start": "2056280",
    "end": "2062079"
  },
  {
    "text": "probably have alarms given the way that you're currently thinking about your infrastructure today once you truly kind of break yourself of that existing model",
    "start": "2062079",
    "end": "2068638"
  },
  {
    "text": "step back and say what if you can really find out that 3 minutes is your average spin-up time alarm yourself in five you",
    "start": "2068639",
    "end": "2076599"
  },
  {
    "text": "know tailor that closer down to a your workload and B your runtime um as far as the hats um I wear",
    "start": "2076599",
    "end": "2083679"
  },
  {
    "text": "many of them mostly my favorite um but the the real thing is like we're all",
    "start": "2083679",
    "end": "2088960"
  },
  {
    "text": "over the place we don't have that kind of Dev and Ops mindset so we're constantly having the ability to say hey",
    "start": "2088960",
    "end": "2096079"
  },
  {
    "text": "I'm currently right writing software right now that manages my infrastructure one way or manages my this part of my",
    "start": "2096079",
    "end": "2101720"
  },
  {
    "text": "application another way and we have that kind of luxury where we're at to be able to take a step back and kind of think of",
    "start": "2101720",
    "end": "2107040"
  },
  {
    "text": "things from a different lens and I think that's actually one of the things that uh I really enjoyed working with Israel",
    "start": "2107040",
    "end": "2112760"
  },
  {
    "text": "on is uh I don't know if you if you saw it in our opening slides but uh he's a software engineer and I am not a",
    "start": "2112760",
    "end": "2118839"
  },
  {
    "text": "software engineer um so I hope that answer your question good cool thank you",
    "start": "2118839",
    "end": "2124000"
  },
  {
    "text": "so much um we' have to wrap up now but we are happy to take questions on the uh nearby stage did you have one more man I",
    "start": "2124000",
    "end": "2130359"
  },
  {
    "text": "think we got enough time for one more yeah oh excuse me I I couldn't hear you",
    "start": "2130359",
    "end": "2136560"
  },
  {
    "text": "m I just had a question about the taints and tolerations yes so I was a little confused so when you boot up that new",
    "start": "2136560",
    "end": "2142160"
  },
  {
    "text": "node group right you have a taint with a new hash yes and that first pod right it",
    "start": "2142160",
    "end": "2148599"
  },
  {
    "text": "has a toleration for that new hash and therefore the the autoscaler can then",
    "start": "2148599",
    "end": "2153680"
  },
  {
    "text": "put up that new note y but the old workloads they don't have the Toleration for that new taint right how do you get",
    "start": "2153680",
    "end": "2159800"
  },
  {
    "text": "that great question so uh tolerations can be based on key and value or they",
    "start": "2159800",
    "end": "2167440"
  },
  {
    "text": "can be explicitly just key and there's also a third one which is the the type whether it's be like no execute prefer",
    "start": "2167440",
    "end": "2173880"
  },
  {
    "text": "no execute right or you know no schedule stuff like that um so we specifically",
    "start": "2173880",
    "end": "2179079"
  },
  {
    "text": "for all of our apps they already trust that rotation key and they're willing to run on it regardless of the version",
    "start": "2179079",
    "end": "2185000"
  },
  {
    "text": "that's the magic remove that value field from your definition of tolerations and",
    "start": "2185000",
    "end": "2190960"
  },
  {
    "text": "now you just care about whether the it's like a it's like a three-part Tuple now you only care about two of them the ones",
    "start": "2190960",
    "end": "2196640"
  },
  {
    "text": "that are always static I see so in combination with the setting the clust here where you have like 100%",
    "start": "2196640",
    "end": "2202920"
  },
  {
    "text": "utilization yes it it it vict and attracts to the new yes so the cluster",
    "start": "2202920",
    "end": "2208440"
  },
  {
    "text": "Auto scaler um I might have gloss over it a little bit cluster Autos Skiller says can I hypothetically move all pods",
    "start": "2208440",
    "end": "2214599"
  },
  {
    "text": "on this node then I'm eligible for for scale down if one of them can't go away your new nodes never leave you because",
    "start": "2214599",
    "end": "2221079"
  },
  {
    "text": "that new pod can't live on your old infrastructure it doesn't trust that hash gotcha cool does that make sense",
    "start": "2221079",
    "end": "2227480"
  },
  {
    "text": "yeah okay it's I mean it's it's so simple but it actually worked remarkably",
    "start": "2227480",
    "end": "2232560"
  },
  {
    "text": "well for us great question man thank you so much thank you so much all right thanks again",
    "start": "2232560",
    "end": "2239079"
  },
  {
    "text": "folks uh before people go I'm sorry I forgot to uh mention this um we actually have a lot of Swag for every body who's",
    "start": "2239839",
    "end": "2247079"
  },
  {
    "text": "interested at the talk uh some helpers over there are going to be handing out little cards um stop by the Oracle Booth",
    "start": "2247079",
    "end": "2253040"
  },
  {
    "text": "upstairs and you're going to be able to get some you know t-shirts and all that kind of cool stuff for attending our talk we really appreciate your time",
    "start": "2253040",
    "end": "2258560"
  },
  {
    "text": "thank you so much",
    "start": "2258560",
    "end": "2261960"
  }
]