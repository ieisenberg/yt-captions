[
  {
    "text": "all right so uh thank you everybody for coming today uh we have a very exciting",
    "start": "160",
    "end": "6240"
  },
  {
    "text": "and interesting topic uh today we're going to be diving into the topic of accelerating high",
    "start": "6240",
    "end": "11360"
  },
  {
    "text": "performance machine learning at scale in kubernetes so a little bit about myself and my",
    "start": "11360",
    "end": "18640"
  },
  {
    "text": "co-speaker my name is alejandro saucedo i am engineering director at zelda technologies a machine learning",
    "start": "18640",
    "end": "24400"
  },
  {
    "text": "deployment and monitoring startup based in london i'm also chief scientist at the institute for ethical ai and",
    "start": "24400",
    "end": "30160"
  },
  {
    "text": "governing member council at large at the acm my co-speaker couldn't make it today",
    "start": "30160",
    "end": "36399"
  },
  {
    "text": "she's still based in vancouver but she was able to send us some exciting videos of the demos that you will be",
    "start": "36399",
    "end": "42960"
  },
  {
    "text": "able to try out yourselves with the jupiter notebooks and deploy on your own site so elena is a senior cloud architect at",
    "start": "42960",
    "end": "50640"
  },
  {
    "text": "microsoft and today we're going to be able to show you a great interesting collaboration uh productionizing machine",
    "start": "50640",
    "end": "57120"
  },
  {
    "text": "learning uh you know at what is scale we're going to be taking a use case",
    "start": "57120",
    "end": "63039"
  },
  {
    "text": "which in this case is going to be a text generation um you know exciting uh",
    "start": "63039",
    "end": "69760"
  },
  {
    "text": "gpt2 model we're going to be showcasing how to perform optimizations on machine",
    "start": "69760",
    "end": "75200"
  },
  {
    "text": "learning of course this is a kubernetes conference not a machine learning conference so we're going to be actually",
    "start": "75200",
    "end": "80960"
  },
  {
    "text": "covering more about the steps productionizing those models some of the nuances of how",
    "start": "80960",
    "end": "88080"
  },
  {
    "text": "these practicalities change when you're dealing with machine learning as opposed to just normal software and then how",
    "start": "88080",
    "end": "94400"
  },
  {
    "text": "we're going to be deploying and scaling this in a kubernetes cluster finally we're going to be covering some cloud",
    "start": "94400",
    "end": "100479"
  },
  {
    "text": "native best practices things like githubs and operational monitoring that you would introduce in normal",
    "start": "100479",
    "end": "106159"
  },
  {
    "text": "microservices but adapting that into the machine learning space so let's get started let's start with the what",
    "start": "106159",
    "end": "112960"
  },
  {
    "text": "so we're going to be taking this machine learning use case that",
    "start": "112960",
    "end": "118000"
  },
  {
    "text": "some of you may have come across before so this is the gpd2 text generation use case what it basically does is it takes",
    "start": "118000",
    "end": "124960"
  },
  {
    "text": "a text input and it simply generates the next token right and what this allows",
    "start": "124960",
    "end": "130000"
  },
  {
    "text": "you to do is to basically generate human-like um you know uh text right so",
    "start": "130000",
    "end": "135599"
  },
  {
    "text": "here you can see that the input is the tokens a robot may",
    "start": "135599",
    "end": "140800"
  },
  {
    "text": "you can see that the model actually generates that next token so that's basically what we're going to be doing the reason why we're taking this use",
    "start": "140800",
    "end": "147440"
  },
  {
    "text": "case is for a couple of uh various uh perspectives the first one is that it's",
    "start": "147440",
    "end": "152640"
  },
  {
    "text": "quite intuitive you can see some of the exciting value not just in how it actually performs the predictions but",
    "start": "152640",
    "end": "158800"
  },
  {
    "text": "also in the use cases that uh people have seen uh uh deploying it on so there has been",
    "start": "158800",
    "end": "165280"
  },
  {
    "text": "if you may have come across this a dungeon crawler where you can choose your own adventure and interact",
    "start": "165280",
    "end": "171440"
  },
  {
    "text": "with this ai model to say what you want to do as the next action so you can start as a i don't know as a wizard and",
    "start": "171440",
    "end": "177519"
  },
  {
    "text": "say i want to now go and grab the staff and it replies what happens to you",
    "start": "177519",
    "end": "182959"
  },
  {
    "text": "so it's quite interesting but also from the hardware perspective it's very uh you know computationally",
    "start": "182959",
    "end": "188800"
  },
  {
    "text": "intensive so it's also going to allow us to show how to accelerate uh the performance of this model so it takes a",
    "start": "188800",
    "end": "195599"
  },
  {
    "text": "couple of seconds to run so we're going to be able to actually showcase how you know we can we can make it run faster",
    "start": "195599",
    "end": "201440"
  },
  {
    "text": "the actual steps we're going to be carrying out today are the ones outlined here we're going to be fetching the model optimizing the model",
    "start": "201440",
    "end": "208480"
  },
  {
    "text": "running it in a micro in a server locally then deploying it into our kubernetes cluster and then showing how",
    "start": "208480",
    "end": "214879"
  },
  {
    "text": "to um you know make this deployment much more robust through githubs and monitoring",
    "start": "214879",
    "end": "220560"
  },
  {
    "text": "so uh the code and examples you're gonna find them in the resource and i'm going",
    "start": "220560",
    "end": "225760"
  },
  {
    "text": "to actually share the link for the talk so that you can access it uh later on",
    "start": "225760",
    "end": "230799"
  },
  {
    "text": "so let's get started with fetching the model so the first step is actually getting access to the artifact and",
    "start": "230799",
    "end": "236959"
  },
  {
    "text": "because in this talk we're going to be covering the personalization we're going to skip all of the part of",
    "start": "236959",
    "end": "242400"
  },
  {
    "text": "the training the model so we're going to already access a pre-trained artifact",
    "start": "242400",
    "end": "247840"
  },
  {
    "text": "fortunately we are going to be using a collaboration that we have done with the hugging face team so this is basically a",
    "start": "247840",
    "end": "254799"
  },
  {
    "text": "team that has collated and trained a broad range of uh you know machine",
    "start": "254799",
    "end": "260000"
  },
  {
    "text": "learning models in this case they are they have also trained this gpt2 model that we're going to be able to just make",
    "start": "260000",
    "end": "265759"
  },
  {
    "text": "use of uh the way that we're going to be doing this is just using their transformers library and by using the",
    "start": "265759",
    "end": "271919"
  },
  {
    "text": "tokenizer we can actually fetch the preprocessor and the model we're going",
    "start": "271919",
    "end": "277040"
  },
  {
    "text": "to talk about what that happened what what that means and we're going to just be able to build the pipeline this actually just fetches everything from",
    "start": "277040",
    "end": "283680"
  },
  {
    "text": "their model hub and simplifies the python side for us",
    "start": "283680",
    "end": "288880"
  },
  {
    "text": "so what i what happens under the hood just to provide an intuition is we are able to provide a text input",
    "start": "288880",
    "end": "295520"
  },
  {
    "text": "so in this case is a text i love artificial intelligence we have to convert this text into something that",
    "start": "295520",
    "end": "301199"
  },
  {
    "text": "the machine learning model can understand so we're going to tokenize it in this case we're going to convert this",
    "start": "301199",
    "end": "307199"
  },
  {
    "text": "string into a bunch of tokens then we can actually pass this tokens to the model but if you remember the model",
    "start": "307199",
    "end": "313039"
  },
  {
    "text": "actually generates one token so if um we want to actually understand what's the most uh i guess reasonable",
    "start": "313039",
    "end": "320960"
  },
  {
    "text": "prediction we can actually take the most likely next token or we can actually take the most likely series of tokens",
    "start": "320960",
    "end": "329199"
  },
  {
    "text": "right so this is this generate function that we leverage here and then once we actually get the output we decode it",
    "start": "329199",
    "end": "335360"
  },
  {
    "text": "back to a human readable string and then we return it right so this is all happening at the python level so the",
    "start": "335360",
    "end": "341759"
  },
  {
    "text": "internals of the generate function you know as i mentioned it could be just a greedy approach of taking the next most",
    "start": "341759",
    "end": "346880"
  },
  {
    "text": "likely token but in this case we can also use other algorithms like in this case the beam search algorithm that uh",
    "start": "346880",
    "end": "353759"
  },
  {
    "text": "has a look ahead to find the most sort of like uh plausible uh series of tokens",
    "start": "353759",
    "end": "359120"
  },
  {
    "text": "so we're gonna skip you know through all of this and abstract it primarily because we're gonna just interact with",
    "start": "359120",
    "end": "364720"
  },
  {
    "text": "this as a black box so the next step is the optimization right so we have this pi torch model under the hood we can",
    "start": "364720",
    "end": "371360"
  },
  {
    "text": "also fetch a tensorflow model but we can actually export it using this um",
    "start": "371360",
    "end": "376800"
  },
  {
    "text": "serialization format called onyx and for this fortunately we have a library still",
    "start": "376800",
    "end": "382560"
  },
  {
    "text": "with a hugging face framework that we have collaborated with that simplifies this the only thing that",
    "start": "382560",
    "end": "389120"
  },
  {
    "text": "we need to do is to actually use the optimum framework and the optimum class that actually gives us the onyx",
    "start": "389120",
    "end": "396000"
  },
  {
    "text": "quantized models which just basically means it's going to be much more efficient it's going to run significantly faster and we're going to",
    "start": "396000",
    "end": "402319"
  },
  {
    "text": "see what that actually looks like in practice so now that we have an artifact and we",
    "start": "402319",
    "end": "407600"
  },
  {
    "text": "saw how to run it in python the question is now how do we actually deploy it before putting it in our kubernetes",
    "start": "407600",
    "end": "413759"
  },
  {
    "text": "cluster to avoid uh bothering the devops team we want to first make sure that it works right make",
    "start": "413759",
    "end": "419680"
  },
  {
    "text": "sure that it works locally run it and ensure that it performs to what you expect for this we're going to be using",
    "start": "419680",
    "end": "426400"
  },
  {
    "text": "these two tools called ml server and zeldan core and the reason why is because there are a lot of challenges",
    "start": "426400",
    "end": "432080"
  },
  {
    "text": "when it comes to scaling that go well beyond just the challenges that you face with normal software the reason why is",
    "start": "432080",
    "end": "438479"
  },
  {
    "text": "because you have specialized hardware in play right things like gpus or tpus you have complex dependency graphs so it's",
    "start": "438479",
    "end": "444960"
  },
  {
    "text": "not just like a microservice that you consume but it's actually multiple hubs across potential inference pipelines you",
    "start": "444960",
    "end": "450639"
  },
  {
    "text": "have compliance requirements where your model your code your environment has to be reproducible and you have you know",
    "start": "450639",
    "end": "457039"
  },
  {
    "text": "all of the the nuances that may actually require higher level principles that may",
    "start": "457039",
    "end": "462240"
  },
  {
    "text": "be dependent on use cases in various different industry domains right if you have to explain your predictions",
    "start": "462240",
    "end": "468319"
  },
  {
    "text": "if you have to actually keep audit trails etc etc so today we're going to actually just simplify this by",
    "start": "468319",
    "end": "474080"
  },
  {
    "text": "introducing those technologies the first one is seldom core which is this kubernetes cloud native",
    "start": "474080",
    "end": "479440"
  },
  {
    "text": "orchestration tool and seldom core allows you to basically convert those uh models artifacts or",
    "start": "479440",
    "end": "487199"
  },
  {
    "text": "custom code into fully fledged microservices and run them in different run times one of the runtimes can be",
    "start": "487199",
    "end": "494240"
  },
  {
    "text": "triton it can be tf serving but today we're going to be using this python",
    "start": "494240",
    "end": "499520"
  },
  {
    "text": "runtime called mo server right so ml server the only difference is that it provides you with a simple compatibility",
    "start": "499520",
    "end": "506879"
  },
  {
    "text": "with python based libraries and because hogging face is a python based library it is very easy to actually interact and",
    "start": "506879",
    "end": "514640"
  },
  {
    "text": "integrate with that so now that we actually talked a little bit about the tools that",
    "start": "514640",
    "end": "519919"
  },
  {
    "text": "we're going to be using now let's actually talk about how we're going to be using them we're going to take this",
    "start": "519919",
    "end": "525680"
  },
  {
    "text": "gpt2 model and define it in our ml server runtime right we're then going to test",
    "start": "525680",
    "end": "532720"
  },
  {
    "text": "it locally by running that as a microservice but locally so",
    "start": "532720",
    "end": "537760"
  },
  {
    "text": "that we can consume the model as by sending inference predictions and getting the responses and if we're happy",
    "start": "537760",
    "end": "544160"
  },
  {
    "text": "with that we can then just deploy it into our kubernetes cluster with the seldom core scheduler which is going to",
    "start": "544160",
    "end": "551279"
  },
  {
    "text": "be using that same runtime underneath so we're going to be first as i said defining the gpt2 model pipeline this is",
    "start": "551279",
    "end": "558800"
  },
  {
    "text": "actually a programmatic way of defining it if you remember in the previous python example we actually selected what",
    "start": "558800",
    "end": "566320"
  },
  {
    "text": "is the task right so in this case it was the task text generation and it's going to be using a pre-trained model from the",
    "start": "566320",
    "end": "572240"
  },
  {
    "text": "hub which in this case is the digital gpt-2 right it's basically like a smaller gpt2 version because that model",
    "start": "572240",
    "end": "577920"
  },
  {
    "text": "is actually huge and you know if you guys have been using the conference wi-fi we don't want to",
    "start": "577920",
    "end": "583680"
  },
  {
    "text": "download that you know and get stuck for that because we're going to be all day here and we",
    "start": "583680",
    "end": "588720"
  },
  {
    "text": "can actually just activate the optimization by saying optimum model true here we specify the runtime which we're",
    "start": "588720",
    "end": "595680"
  },
  {
    "text": "using the hugging phase runtime if you're familiar with machine learning you can also use things like xg boost",
    "start": "595680",
    "end": "601519"
  },
  {
    "text": "runtime uh or scikit-learn runtime etc etc so now that",
    "start": "601519",
    "end": "607040"
  },
  {
    "text": "we actually have the configuration we can just basically run it we do ml server start with that config file it",
    "start": "607040",
    "end": "613760"
  },
  {
    "text": "runs this fast api server and then we actually just send a call request with",
    "start": "613760",
    "end": "619040"
  },
  {
    "text": "this input selden is very and it returns with this output generated of selden is",
    "start": "619040",
    "end": "625040"
  },
  {
    "text": "very curious about the matter right so it works right so that's basically what we have now fortunately",
    "start": "625040",
    "end": "631760"
  },
  {
    "text": "we do have a demo that uh my co-speaker recorded before coming here so now let's",
    "start": "631760",
    "end": "637600"
  },
  {
    "text": "hope that everything is connected hooked up because i'm gonna press play",
    "start": "637600",
    "end": "643760"
  },
  {
    "text": "model working with ml server and hacking phase optimum library it's rather simple",
    "start": "644160",
    "end": "649200"
  },
  {
    "text": "and fast and we will do it all in github code spaces online no local installation of python or anything",
    "start": "649200",
    "end": "655440"
  },
  {
    "text": "to start using the model we need to create model settings file direct to use hug in phase runtime",
    "start": "655440",
    "end": "662399"
  },
  {
    "text": "we will use text generation task for this example with distil gpt2 model",
    "start": "662399",
    "end": "667600"
  },
  {
    "text": "available from hug in phase and we will enable optimum optimizations",
    "start": "667600",
    "end": "672959"
  },
  {
    "text": "we will start a ml server it will serve model on both http and the",
    "start": "672959",
    "end": "679839"
  },
  {
    "text": "grpc protocols following kf7 v2 protocol it will download retrained model from",
    "start": "679839",
    "end": "685279"
  },
  {
    "text": "plugin phase and apply optimizations once server starts we could explore open api docs",
    "start": "685279",
    "end": "691600"
  },
  {
    "text": "and run inference it will show a structure of expected input and output to help out",
    "start": "691600",
    "end": "697920"
  },
  {
    "text": "and we will submit data investing io love ai let's see what it generates",
    "start": "697920",
    "end": "705040"
  },
  {
    "text": "it looks rather cool for ai generation and all that power without writing a",
    "start": "705360",
    "end": "710880"
  },
  {
    "text": "single line of python code now we could use this model easily from application written in any language",
    "start": "710880",
    "end": "719440"
  },
  {
    "text": "but that's not all many transformer models support different nlp tasks we",
    "start": "719440",
    "end": "724639"
  },
  {
    "text": "saw text generation now let's look at sentiment analysis we will change our model settings",
    "start": "724639",
    "end": "731120"
  },
  {
    "text": "for sentiment analysis tasks and we'll start with server we didn't specify model so downloaded bert model as it",
    "start": "731120",
    "end": "738160"
  },
  {
    "text": "dims at best for the task at hand let's resubmit the data with our i love ai",
    "start": "738160",
    "end": "743920"
  },
  {
    "text": "sentence and see what it generates it generated sentiment positive score",
    "start": "743920",
    "end": "751680"
  },
  {
    "text": "in this demo we saw how simple it is to run various nlp models and tasks on",
    "start": "751680",
    "end": "757600"
  },
  {
    "text": "github codespaces with a mail server and optimum library",
    "start": "757600",
    "end": "763279"
  },
  {
    "text": "awesome that that worked out so that's great we're literally pushing the release of ml server as we speak as we",
    "start": "763279",
    "end": "770880"
  },
  {
    "text": "had to do some uh yeah last minute updates but you can try it out so please do make sure you know you check out some",
    "start": "770880",
    "end": "777200"
  },
  {
    "text": "of the notebooks so now let's actually dive into the next step right so we run it locally now we want to productionize",
    "start": "777200",
    "end": "783600"
  },
  {
    "text": "it and we want to make sure we run this service as a microservice in our kubernetes cluster so we're going to be",
    "start": "783600",
    "end": "790160"
  },
  {
    "text": "using our kubernetes operator for seldon core which provides us with a bunch of",
    "start": "790160",
    "end": "795760"
  },
  {
    "text": "custom resource definitions that abstract those machine learning concepts into uh basically crs right like the",
    "start": "795760",
    "end": "802480"
  },
  {
    "text": "concept of a seldom deployment which allows you to deploy your models the way that you would be able to do that is by",
    "start": "802480",
    "end": "808720"
  },
  {
    "text": "using the uh you know definition if you remember those same parameters that we use them in ml server and in python",
    "start": "808720",
    "end": "815519"
  },
  {
    "text": "there is a one-to-one mapping between those parameters and the ones that are passed downstream so here you can see",
    "start": "815519",
    "end": "820720"
  },
  {
    "text": "that it's the same text generation you can see that you can define your pre-trained model and then you can you",
    "start": "820720",
    "end": "826079"
  },
  {
    "text": "know select the optimization the key thing here is that we are accessing the pre-trained models from the hugging face",
    "start": "826079",
    "end": "832000"
  },
  {
    "text": "hub which means that you can actually download you know all of the you know large range of models that they",
    "start": "832000",
    "end": "837040"
  },
  {
    "text": "actually uh provide you you can now one once you deploy it with cube cuts or apply",
    "start": "837040",
    "end": "842480"
  },
  {
    "text": "you know you can actually see that the actual pods are running successfully and you can actually send a request now the",
    "start": "842480",
    "end": "849279"
  },
  {
    "text": "one thing to mention is that uh you know under the hood we also had to install",
    "start": "849279",
    "end": "855120"
  },
  {
    "text": "in the cluster our gateway controller which is istio which provides you kind of like for the routing uh to be able to",
    "start": "855120",
    "end": "861040"
  },
  {
    "text": "access the models so seldom core integrates with both istio and",
    "start": "861040",
    "end": "866079"
  },
  {
    "text": "ambassador so that's basically the uh you know kubernetes deployment part but if we remember",
    "start": "866079",
    "end": "872000"
  },
  {
    "text": "you know when when we're dealing with kubernetes clusters and with kubernetes deployments it's not just about running",
    "start": "872000",
    "end": "878399"
  },
  {
    "text": "a pod right it's also about the reproducibility and um you know the ability to actually ensure you have uh",
    "start": "878399",
    "end": "885440"
  },
  {
    "text": "you know roll back disaster recovery mechanisms etc etc so for this we're going to now be delving into some best",
    "start": "885440",
    "end": "891839"
  },
  {
    "text": "practices that we can introduce that you know have been covered quite extensively in the general cloud native uh you know",
    "start": "891839",
    "end": "899040"
  },
  {
    "text": "ecosystem things like githubs or operational monitoring but adopted into this machine learning deployment",
    "start": "899040",
    "end": "906399"
  },
  {
    "text": "workflows so the first one that we're going to cover is continuous delivery via githubs right so",
    "start": "906399",
    "end": "913279"
  },
  {
    "text": "so githubs can be summarized as deployment as code through version control right the ability for you to",
    "start": "913279",
    "end": "919920"
  },
  {
    "text": "have a one-to-one mapping between your kubernetes cluster state",
    "start": "919920",
    "end": "925360"
  },
  {
    "text": "and you know the equivalent within a git repo that is versioned one of the benefits of course of git ups is the",
    "start": "925360",
    "end": "932959"
  },
  {
    "text": "ability to be able to roll back that's one of the things that normally gets covered but one important",
    "start": "932959",
    "end": "938639"
  },
  {
    "text": "benefit of githubs is also disaster recovery right like uh if suddenly your cluster gets into an",
    "start": "938639",
    "end": "945440"
  },
  {
    "text": "inconsistent state and you want to you know recreate it somewhere else that gives you a robust disaster recovery",
    "start": "945440",
    "end": "950959"
  },
  {
    "text": "mechanism and similarly for migration you're able to actually replicate the cluster so what does that look like if",
    "start": "950959",
    "end": "957199"
  },
  {
    "text": "we actually have our data scientist our machine learning engineer interacting with the kubernetes cluster",
    "start": "957199",
    "end": "962880"
  },
  {
    "text": "so you would have the data scientists training new models perhaps doing transfer learning pushing it into the",
    "start": "962880",
    "end": "968720"
  },
  {
    "text": "hugging face hub or as we also support pushing it into a google bucket or an s3",
    "start": "968720",
    "end": "974800"
  },
  {
    "text": "bucket etc etc then the machine learning engineer is able to you know programmatically deploy that",
    "start": "974800",
    "end": "981519"
  },
  {
    "text": "model by pushing the specific yaml configuration into the git repository then as you will",
    "start": "981519",
    "end": "987600"
  },
  {
    "text": "see in the next uh you know part of the demo the githubs integration would allow you to actually sync that change in the",
    "start": "987600",
    "end": "995199"
  },
  {
    "text": "github repo and then make sure that the cluster uh recon reconciliates with with those",
    "start": "995199",
    "end": "1001199"
  },
  {
    "text": "changes what that means in practice is what we saw just not with cube cuts will apply is with the ktops workflow that",
    "start": "1001199",
    "end": "1008079"
  },
  {
    "text": "would actually run our machine learning model runtime so seldom core would be the reconciler component that would see",
    "start": "1008079",
    "end": "1015199"
  },
  {
    "text": "hey you requested a seldom deployment with this runtime i am going to run that",
    "start": "1015199",
    "end": "1020560"
  },
  {
    "text": "specific runtime which is in this case ml server and then mo server will fetch",
    "start": "1020560",
    "end": "1026240"
  },
  {
    "text": "the particular pre-trained model that you specified right so so so simple enough uh now actually uh let's",
    "start": "1026240",
    "end": "1033678"
  },
  {
    "text": "let's see what that looks like in practice in order to actually configure it from our site you know we actually have",
    "start": "1033679",
    "end": "1040000"
  },
  {
    "text": "direct integrations with things like argo cd in this example we're using",
    "start": "1040000",
    "end": "1045038"
  },
  {
    "text": "flux for the integration so you can see that the flux config specifies which",
    "start": "1045039",
    "end": "1050080"
  },
  {
    "text": "repo it's going to be syncing into which cluster as well as what are the particular",
    "start": "1050080",
    "end": "1055760"
  },
  {
    "text": "parameters that that we want to uh you know create uh we will then uh make sure",
    "start": "1055760",
    "end": "1060960"
  },
  {
    "text": "that once the model is deployed we are able to leverage some of the i guess",
    "start": "1060960",
    "end": "1066640"
  },
  {
    "text": "observability richness that you would normally get out of the box in kubernetes and of course extending it",
    "start": "1066640",
    "end": "1073200"
  },
  {
    "text": "into the world of machine learning what that means in practice is that with seldom core you get the benefits of",
    "start": "1073200",
    "end": "1079840"
  },
  {
    "text": "ensuring that all of the models that get deployed not only have that you know rest grpc and kafka apis but also you're",
    "start": "1079840",
    "end": "1088320"
  },
  {
    "text": "able to ensure that metrics are exposed operational metrics like requests per second latency but also more advanced",
    "start": "1088320",
    "end": "1095360"
  },
  {
    "text": "metrics like gpu utilization etc etc and this is not only relevant for our use case but also for when you're using more",
    "start": "1095360",
    "end": "1102000"
  },
  {
    "text": "you know advanced runtimes like triton where you want to really get every single sort of like millisecond or",
    "start": "1102000",
    "end": "1108160"
  },
  {
    "text": "nanosecond of latency and similarly you know out of the scope of this talk but um being able to collect the inputs and",
    "start": "1108160",
    "end": "1114640"
  },
  {
    "text": "outputs of the model so that you can actually get insights from what's actually uh being processed in the",
    "start": "1114640",
    "end": "1120799"
  },
  {
    "text": "inference side of your deployments so we're gonna actually see some practical insights that we can",
    "start": "1120799",
    "end": "1127760"
  },
  {
    "text": "extract in the next demo that is gonna basically you know showcase all of the things that i covered so let's now",
    "start": "1127760",
    "end": "1134320"
  },
  {
    "text": "switch back into the next video and hope that it all works and let's give it play",
    "start": "1134320",
    "end": "1140080"
  },
  {
    "text": "now we will see how to deploy scene transformer model to kubernetes with seldom core and following detox approach",
    "start": "1140080",
    "end": "1147280"
  },
  {
    "text": "we have a cast cluster with two node pools one with gpu and one with cpu based",
    "start": "1147280",
    "end": "1153440"
  },
  {
    "text": "computer cluster is enabled with gitobs flux addon and we onboarded our cone repo manifests",
    "start": "1153440",
    "end": "1161280"
  },
  {
    "text": "to be synced with the cluster we have installed seldom core with",
    "start": "1161280",
    "end": "1166640"
  },
  {
    "text": "streamgrass on the cluster and now ready to create seldom deployment service one crd is demonstrating running model",
    "start": "1166640",
    "end": "1173520"
  },
  {
    "text": "with hacking phase runtime on gpu nodes we could see that we set hugging phase",
    "start": "1173520",
    "end": "1179120"
  },
  {
    "text": "server as our runtime we defined task that model would perform",
    "start": "1179120",
    "end": "1186160"
  },
  {
    "text": "to text generation and we will use distil gpt as our pre-trained model we",
    "start": "1186160",
    "end": "1192480"
  },
  {
    "text": "have also defined tolerations and nvidia gpu requests so",
    "start": "1192480",
    "end": "1197840"
  },
  {
    "text": "that model will run on gpu nodes for cpu version we have removed tolerations and it will be running on",
    "start": "1197840",
    "end": "1204960"
  },
  {
    "text": "cpu nodes we have committed manifests to the repo",
    "start": "1204960",
    "end": "1210080"
  },
  {
    "text": "and we see flux controller thinking latest commit with a cluster resources are being deployed and it",
    "start": "1210080",
    "end": "1216880"
  },
  {
    "text": "takes few seconds to get readiness props turn green southern controller processed at cd",
    "start": "1216880",
    "end": "1222880"
  },
  {
    "text": "object and deployed two containers for each model and virtual servers to enable routing",
    "start": "1222880",
    "end": "1230080"
  },
  {
    "text": "for the model for eastern grass our model pod has one container with",
    "start": "1230080",
    "end": "1235679"
  },
  {
    "text": "email server configuration and another is seldom sidecar performing",
    "start": "1235679",
    "end": "1240880"
  },
  {
    "text": "orchestration tasks now we have both models running",
    "start": "1240880",
    "end": "1246400"
  },
  {
    "text": "let's compare how these models deployed on separate nodes performed we will use k6 load testing 2 and define",
    "start": "1246400",
    "end": "1254480"
  },
  {
    "text": "two scenarios running multiple iterations with text prediction payload against both models we see that gpu",
    "start": "1254480",
    "end": "1262880"
  },
  {
    "text": "outperforms cpu but alert by a large degree running hundreds of iterations",
    "start": "1262880",
    "end": "1268400"
  },
  {
    "text": "while cpu processes just 12. once test finishes we see that cpu-based",
    "start": "1268400",
    "end": "1274400"
  },
  {
    "text": "runs takes 2 seconds to run while gpu is 10 times less just 200",
    "start": "1274400",
    "end": "1280640"
  },
  {
    "text": "milliseconds the ml server optimum library abstracted from us complexity of model serving and",
    "start": "1280640",
    "end": "1288480"
  },
  {
    "text": "we were able to utilize underlying gpu infrastructure very efficiently",
    "start": "1288480",
    "end": "1295039"
  },
  {
    "text": "awesome so i think i think one of the things that we can see from from that demo is",
    "start": "1295919",
    "end": "1301280"
  },
  {
    "text": "ultimately the comparison of a slightly more complex machine learning model which would actually take perhaps a",
    "start": "1301280",
    "end": "1307120"
  },
  {
    "text": "couple of seconds to process the input data the interesting thing is that if we",
    "start": "1307120",
    "end": "1312240"
  },
  {
    "text": "actually perform the inference you know one input at a time the cpu and",
    "start": "1312240",
    "end": "1318080"
  },
  {
    "text": "the gpu would perform equally at the same speed the benefit is when we actually do batching right when we send",
    "start": "1318080",
    "end": "1325440"
  },
  {
    "text": "multiple requests batched for those to be processed by the gpu on a single",
    "start": "1325440",
    "end": "1331760"
  },
  {
    "text": "clock cycle and then similarly one of the things that you know is kind of outside of the scope of this session but",
    "start": "1331760",
    "end": "1337520"
  },
  {
    "text": "that you can try out yourselves is how to leverage things like adaptive batching or predictive batching as it's",
    "start": "1337520",
    "end": "1343760"
  },
  {
    "text": "also called the ability to ensure that the actual server itself is doing the batching so you can send like a heavy",
    "start": "1343760",
    "end": "1349679"
  },
  {
    "text": "load and the server would actually take some requests let's say 100 and then actually run them within the gpu and",
    "start": "1349679",
    "end": "1356720"
  },
  {
    "text": "then make sure that they get returned with the relevant you know open connections uh you know accordingly",
    "start": "1356720",
    "end": "1362480"
  },
  {
    "text": "right and that actually makes sure that you still are able to leverage some of the optimizations within the gpu itself",
    "start": "1362480",
    "end": "1368720"
  },
  {
    "text": "so so again you know the great thing about all of these things and as we all love open source is that if you find any",
    "start": "1368720",
    "end": "1375120"
  },
  {
    "text": "issues or if you find something that needs improvement we would love for you to you know open an issue uh or even a",
    "start": "1375120",
    "end": "1382159"
  },
  {
    "text": "pr uh as always uh very much welcome so just to summarize and to make sure that",
    "start": "1382159",
    "end": "1387520"
  },
  {
    "text": "we um take a step back and see what actually happened from the big picture",
    "start": "1387520",
    "end": "1392640"
  },
  {
    "text": "let's see what does the anatomy of production mlops look like right so so we can see all of our",
    "start": "1392640",
    "end": "1398559"
  },
  {
    "text": "persistent areas the training data the artifact store and then we will see also",
    "start": "1398559",
    "end": "1403760"
  },
  {
    "text": "the git repo and the inference store the first step which we actually skipped in this talk is the experimentation right",
    "start": "1403760",
    "end": "1410960"
  },
  {
    "text": "is when data scientists are training machine learning models using data converting into artifacts right and in",
    "start": "1410960",
    "end": "1418240"
  },
  {
    "text": "the case of the hogging phase demo this is basically pushing them into the hogging hugging face hub but you know it",
    "start": "1418240",
    "end": "1424320"
  },
  {
    "text": "can be also into an artifact store s3 you know google bucket uh azure blob etc",
    "start": "1424320",
    "end": "1430559"
  },
  {
    "text": "etc the next step is once you have a model that is ready to be productionized you would be able to either manually or",
    "start": "1430559",
    "end": "1437279"
  },
  {
    "text": "programmatically ensure that it's actually you know pushed into that kubernetes cluster",
    "start": "1437279",
    "end": "1443039"
  },
  {
    "text": "right so in that case we can cover it from the ci cd side programmatically having a ci pipeline or an etl pipeline",
    "start": "1443039",
    "end": "1451120"
  },
  {
    "text": "that is responsible of potentially packaging the model uh potentially you know pushing the runtime if it's",
    "start": "1451120",
    "end": "1457279"
  },
  {
    "text": "actually all encompassed in an image or just pushing it again into the artifact and then actually pushing into the git",
    "start": "1457279",
    "end": "1463679"
  },
  {
    "text": "tops repo this is actually quite important because in the previous slide we were showing how potentially the",
    "start": "1463679",
    "end": "1469279"
  },
  {
    "text": "machine learning engineer would push into the githubs repo but normally from our from our side we tend to discourage",
    "start": "1469279",
    "end": "1475440"
  },
  {
    "text": "that right uh pushing into a github repo is not something that you should do particularly given that github's",
    "start": "1475440",
    "end": "1482400"
  },
  {
    "text": "you know at least in some contexts is seen more as a data store right it's it is the state of the cluster and the",
    "start": "1482400",
    "end": "1489520"
  },
  {
    "text": "ability to you make sure that you can actually you know roll back and if you actually can do that programmatically it",
    "start": "1489520",
    "end": "1495279"
  },
  {
    "text": "provides a extra level of security now as we also saw then that's when flux",
    "start": "1495279",
    "end": "1501360"
  },
  {
    "text": "would come in and be able to perform that reconciliation with a cluster that means that you would have your real-time",
    "start": "1501360",
    "end": "1507440"
  },
  {
    "text": "or batch models that are running in kubernetes and then of course the operational metrics that we were showing",
    "start": "1507440",
    "end": "1514559"
  },
  {
    "text": "the monitoring the observability of course again this is not a monitoring talk uh i have links to resources that",
    "start": "1514559",
    "end": "1521760"
  },
  {
    "text": "cover things like drift detection outlier detection so that you can actually delve into some proper data",
    "start": "1521760",
    "end": "1527520"
  },
  {
    "text": "science monitoring with cloud native architectures but from that same context",
    "start": "1527520",
    "end": "1532960"
  },
  {
    "text": "this gives you an idea of this what we call the anatomy of the mlops lifecycle",
    "start": "1532960",
    "end": "1538799"
  },
  {
    "text": "so yeah that's kind of like the main sort of premise you know taking a step back",
    "start": "1538799",
    "end": "1543919"
  },
  {
    "text": "getting a bit of an intuition and not getting you know to too much in in depth one last thing that i do want to",
    "start": "1543919",
    "end": "1549360"
  },
  {
    "text": "highlight is the step that we showed about running ml server locally right we we see one of",
    "start": "1549360",
    "end": "1555919"
  },
  {
    "text": "the key challenges is that often in the envelopes life cycle the data scientists or machine learning",
    "start": "1555919",
    "end": "1562480"
  },
  {
    "text": "engineers go straight from experimentation to production and the reason why that's a challenge is because",
    "start": "1562480",
    "end": "1567760"
  },
  {
    "text": "if they have like a container crashing that would actually introduce a very inefficient loop between the data",
    "start": "1567760",
    "end": "1573440"
  },
  {
    "text": "scientists and the devops engineer or the platform engineer going hey can you send me the logs hey why is this not",
    "start": "1573440",
    "end": "1578960"
  },
  {
    "text": "working right so that part about being able to run it locally making sure that everything works you know send some",
    "start": "1578960",
    "end": "1584240"
  },
  {
    "text": "requests debug it that's actually quite key in this in this workflow",
    "start": "1584240",
    "end": "1589360"
  },
  {
    "text": "so if you want further resources uh you know we have you know other talks that we've actually given in previous kubecon",
    "start": "1589360",
    "end": "1595120"
  },
  {
    "text": "conferences on ci cd for production machine learning at scale on production machine learning monitoring with explainers drift",
    "start": "1595120",
    "end": "1601440"
  },
  {
    "text": "detectors outlier detectors um the similar one on accelerating mlm versus",
    "start": "1601440",
    "end": "1606640"
  },
  {
    "text": "at scale but with onyx and triton uh machine learning security and then you know the machine learning ecosystem and",
    "start": "1606640",
    "end": "1613279"
  },
  {
    "text": "operations the current state of that space uh the slides you can find them in that bit.ly link at the uh top right uh",
    "start": "1613279",
    "end": "1621760"
  },
  {
    "text": "over there so yeah if you want to access the slide the resources the notebook check it out there",
    "start": "1621760",
    "end": "1627360"
  },
  {
    "text": "so just to summarize again uh today we covered machine learning acceleration at scale",
    "start": "1627360",
    "end": "1632640"
  },
  {
    "text": "how to optimize your models how to run them locally how to deployment to go to kubernetes and how to introduce a",
    "start": "1632640",
    "end": "1638480"
  },
  {
    "text": "production cloud native tooling again thank you so much and thank you for bearing with us you know with this uh",
    "start": "1638480",
    "end": "1645200"
  },
  {
    "text": "juggling of video and presentation i hope you enjoyed it and i'll take questions if anyone has them if not you",
    "start": "1645200",
    "end": "1651919"
  },
  {
    "text": "can grab me for a drink later on for more questions thank you very much [Applause]",
    "start": "1651919",
    "end": "1662640"
  },
  {
    "text": "awesome",
    "start": "1663679",
    "end": "1666679"
  },
  {
    "text": "so any takers for questions",
    "start": "1670159",
    "end": "1673600"
  },
  {
    "text": "any brave ones nice we have one there",
    "start": "1675440",
    "end": "1680559"
  },
  {
    "text": "oh i think uh yeah just just him i think you haven't yeah he hasn't see you oh you have a microphone",
    "start": "1680559",
    "end": "1687279"
  },
  {
    "text": "okay if you tell me i'll repeat the question yeah",
    "start": "1687279",
    "end": "1692279"
  },
  {
    "text": "right right right so the question is uh do we have any methods to share gpus across",
    "start": "1696960",
    "end": "1703200"
  },
  {
    "text": "containers so that's actually interesting we were just chatting about that and the talk right before this was also talking about",
    "start": "1703200",
    "end": "1708960"
  },
  {
    "text": "about that um so seldom core operates at a sort of like you know high level sort of scheduling",
    "start": "1708960",
    "end": "1715919"
  },
  {
    "text": "building kind of like the pots so we would rely on the schedulers themselves there was a very interesting talk uh",
    "start": "1715919",
    "end": "1722000"
  },
  {
    "text": "that one of my colleagues was mentioning from nvidia that was actually talking about how to introduce at the scheduler",
    "start": "1722000",
    "end": "1728480"
  },
  {
    "text": "level the ability to uh specify fractions of gpus and actually you know",
    "start": "1728480",
    "end": "1733679"
  },
  {
    "text": "make container d or the lower level you know magic handle that for you uh not",
    "start": "1733679",
    "end": "1739360"
  },
  {
    "text": "being able to actually deal with that yourself um so you know from our perspective that would be the ideal but",
    "start": "1739360",
    "end": "1744640"
  },
  {
    "text": "of course you can actually leverage some um of the runtime capabilities like",
    "start": "1744640",
    "end": "1750080"
  },
  {
    "text": "triton so i didn't cover how you can use triton instead of ml server but when you deploy your models using triton you",
    "start": "1750080",
    "end": "1757279"
  },
  {
    "text": "actually have access to those low level configuration of course it is at the mercy of your configuration so",
    "start": "1757279",
    "end": "1764559"
  },
  {
    "text": "you're going to have to like specify that at the pod level and handle that on your configurations so it does get a little",
    "start": "1764559",
    "end": "1770080"
  },
  {
    "text": "bit you know complex but there are options um so i think from from our perspective one is actually gpu sharing",
    "start": "1770080",
    "end": "1777440"
  },
  {
    "text": "another one is multi-modal serving right and that's one of the easier ways of actually handling what is actually",
    "start": "1777440",
    "end": "1784559"
  },
  {
    "text": "sharing you know one gpu across multiple models is just actually having one container running multiple models an ml",
    "start": "1784559",
    "end": "1791600"
  },
  {
    "text": "server allows for multi-model serving triton allows for multi-modal serving",
    "start": "1791600",
    "end": "1796720"
  },
  {
    "text": "and we've been actually doing a lot of collaboration across all of these projects to make sure that this the apis are consistent",
    "start": "1796720",
    "end": "1803840"
  },
  {
    "text": "right so so the management apis are the same the inference apis are the same across ml server and triton",
    "start": "1803840",
    "end": "1810000"
  },
  {
    "text": "so it's more of a preference of which one you want to use yeah so that would be the current sort of answer to that which is not very much a",
    "start": "1810000",
    "end": "1816720"
  },
  {
    "text": "full answer um but yeah good question yeah",
    "start": "1816720",
    "end": "1822279"
  },
  {
    "text": "other questions",
    "start": "1822880",
    "end": "1825600"
  },
  {
    "text": "awesome oh i think we have one one over there",
    "start": "1829679",
    "end": "1835240"
  },
  {
    "text": "yeah and if not yeah you can grab me for for for deeper dives and questions um you know later on hi great talk thank",
    "start": "1836960",
    "end": "1842960"
  },
  {
    "text": "you thank you thank you how do you deal with model versioning uh you know i use",
    "start": "1842960",
    "end": "1848320"
  },
  {
    "text": "dvc usually for versioning the models and how does this affect",
    "start": "1848320",
    "end": "1854080"
  },
  {
    "text": "the github section of what should or what should talk about yeah that's that's actually really",
    "start": "1854080",
    "end": "1860480"
  },
  {
    "text": "interesting question and i'm always really really keen on delving into into that context so the dvc team",
    "start": "1860480",
    "end": "1868159"
  },
  {
    "text": "is is awesome we've actually done collaborations with them and we have examples of how to deploy",
    "start": "1868159",
    "end": "1873360"
  },
  {
    "text": "models that have been trained and pipelines that have been using dvc dvc",
    "start": "1873360",
    "end": "1878480"
  },
  {
    "text": "handles the experimentation part right and the reason why i'm pointing this out is",
    "start": "1878480",
    "end": "1883840"
  },
  {
    "text": "because in the experimentation part you may have a hundred experiments with a hundred artifacts",
    "start": "1883840",
    "end": "1889519"
  },
  {
    "text": "when you move to production you choose one artifact you choose one experiment you say i want to productionize this one",
    "start": "1889519",
    "end": "1895840"
  },
  {
    "text": "experiment and now you move into a new realm where the relationship between production models and experiments is",
    "start": "1895840",
    "end": "1902480"
  },
  {
    "text": "different you have a one-to-many relationship where one experiment can have multiple deployments you can have",
    "start": "1902480",
    "end": "1909360"
  },
  {
    "text": "them deployed in a dev environment you can have them deployed in a production environment you can have them deployed",
    "start": "1909360",
    "end": "1914559"
  },
  {
    "text": "across three namespaces etc etc so when it comes to versioning we do keep a um",
    "start": "1914559",
    "end": "1922000"
  },
  {
    "text": "you know sort of principle where the experiments themselves must be",
    "start": "1922000",
    "end": "1927440"
  },
  {
    "text": "consistent right so making sure that the experiment has a unique identifier so whenever you productionize it into",
    "start": "1927440",
    "end": "1933679"
  },
  {
    "text": "your githubs repo there would be a unique identifier in the yaml right so",
    "start": "1933679",
    "end": "1939519"
  },
  {
    "text": "whenever you change the experiment the yaml changes right of course you have you know some servers that allow you to",
    "start": "1939519",
    "end": "1946399"
  },
  {
    "text": "actually just point to a bucket and have the server updating whenever",
    "start": "1946399",
    "end": "1951440"
  },
  {
    "text": "the bucket content changes so that's something that is a no-no for us right we ensure item potency make sure that",
    "start": "1951440",
    "end": "1958880"
  },
  {
    "text": "whenever there is a change in the yaml there is a change in the server and no magic underneath right so so there are",
    "start": "1958880",
    "end": "1965600"
  },
  {
    "text": "some considerations to take into account in summary is that relationship between experiments and production",
    "start": "1965600",
    "end": "1972640"
  },
  {
    "text": "services as well as the ability to ensure item potence on the",
    "start": "1972640",
    "end": "1978399"
  },
  {
    "text": "yamos and github's you know components themselves so that it allows you to actually trace back all",
    "start": "1978399",
    "end": "1984320"
  },
  {
    "text": "the way to the previous steps in the machine learning life cycle yeah awesome",
    "start": "1984320",
    "end": "1992720"
  },
  {
    "text": "another one thanks for the dog um for example if you want to go one step",
    "start": "1993440",
    "end": "2000080"
  },
  {
    "text": "further once you have done the the inference and you detect somewhere thing in a video or",
    "start": "2000080",
    "end": "2008960"
  },
  {
    "text": "or kind of stuff and you want to to trigger any action",
    "start": "2008960",
    "end": "2015679"
  },
  {
    "text": "related to this uh inference does sheldon offer any feature that can",
    "start": "2015679",
    "end": "2021760"
  },
  {
    "text": "be can be used like a pipeline or something like that",
    "start": "2021760",
    "end": "2026799"
  },
  {
    "text": "yeah yeah great question uh so the short answer is yes the long answer is it's complicated",
    "start": "2026799",
    "end": "2033039"
  },
  {
    "text": "right um so so we have multiple different uh interfaces through which you can trigger",
    "start": "2033039",
    "end": "2039679"
  },
  {
    "text": "um i guess events right so one of them being",
    "start": "2039679",
    "end": "2045600"
  },
  {
    "text": "let me see if i can just open it one of them being operational metrics right so this is one to one to your service level",
    "start": "2045600",
    "end": "2052158"
  },
  {
    "text": "agreements you can say i want to set a service level objective i want my uh model to be using this amount of uh uh",
    "start": "2052159",
    "end": "2059040"
  },
  {
    "text": "you know throughput or this this maximum latency and i want to set up alerts",
    "start": "2059040",
    "end": "2064638"
  },
  {
    "text": "through alert manager or something like that right so that's on the operational level on the data science metrics side",
    "start": "2064639",
    "end": "2070720"
  },
  {
    "text": "so i talked a little bit about um drift detectors outlier detectors which we didn't cover in this talk but",
    "start": "2070720",
    "end": "2077599"
  },
  {
    "text": "you will see in some of the resources that i link that there are ways in which you can hook in",
    "start": "2077599",
    "end": "2083919"
  },
  {
    "text": "extra components that would be able to listen through the you know various",
    "start": "2083919",
    "end": "2089919"
  },
  {
    "text": "inputs and outputs of your model to actually perform um i guess advanced",
    "start": "2089919",
    "end": "2096000"
  },
  {
    "text": "checks of the current state and then you're able to trigger respective actions depending on the outputs of",
    "start": "2096000",
    "end": "2102720"
  },
  {
    "text": "those right so so so the answer is seldom would provide you with with the lego blocks that then your platform",
    "start": "2102720",
    "end": "2109599"
  },
  {
    "text": "teams would be able to um i guess arrange accordingly right and and you",
    "start": "2109599",
    "end": "2114800"
  },
  {
    "text": "know as an as an open core company that's kind of like more of where we delve into providing that sort of you",
    "start": "2114800",
    "end": "2120800"
  },
  {
    "text": "know management layer uh but yeah the open source provides you with all of the tooling that you would need and that's",
    "start": "2120800",
    "end": "2126000"
  },
  {
    "text": "why you know we have you know seven million downloads and people kind of like integrating in different ways",
    "start": "2126000",
    "end": "2131440"
  },
  {
    "text": "with with you know whether it's k native or whatever but um yeah so so the answer is yes",
    "start": "2131440",
    "end": "2138319"
  }
]