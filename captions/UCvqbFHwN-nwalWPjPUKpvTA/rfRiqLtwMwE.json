[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "hi kubecon and welcome to building the multi-cluster data layer talk my name is chirag narang",
    "start": "960",
    "end": "7520"
  },
  {
    "text": "i'm a product lead at egobyte in today's session i'm going to show you how to build a multi-cluster data layer",
    "start": "7520",
    "end": "13840"
  },
  {
    "text": "the easy way we'll go over some key requirements for deploying a database across multiple kubernetes clusters",
    "start": "13840",
    "end": "20400"
  },
  {
    "text": "running in different regions and how to use a service mesh like sto to solve for networking",
    "start": "20400",
    "end": "26240"
  },
  {
    "text": "challenges then we'll have the product demo and at the end i'm going to give you an interesting use case for",
    "start": "26240",
    "end": "32320"
  },
  {
    "text": "improving your application performance that you can use on your own so with that let's start",
    "start": "32320",
    "end": "38879"
  },
  {
    "text": "with the ego by db project you could buy db is a fully open source",
    "start": "38879",
    "end": "43920"
  },
  {
    "text": "distributed database which is built for the cloud native world it can be deployed",
    "start": "43920",
    "end": "48960"
  },
  {
    "text": "across private and public clouds including kubernetes it reuses the query layer of postgres",
    "start": "48960",
    "end": "55280"
  },
  {
    "text": "sql so it offers advanced features like triggers stored procedures partial indexes it",
    "start": "55280",
    "end": "60960"
  },
  {
    "text": "allows easy migration from other databases like postgres mysql cassandra",
    "start": "60960",
    "end": "68080"
  },
  {
    "text": "the database offers high resiliency and high availability the multi-node architecture allows you",
    "start": "68080",
    "end": "73840"
  },
  {
    "text": "to survive different failures on the node level zone level region or",
    "start": "73840",
    "end": "78960"
  },
  {
    "text": "entire data center it allows you to do zero downtime upgrades and security patching",
    "start": "78960",
    "end": "85360"
  },
  {
    "text": "unlike traditional databases you can scale yiga by db horizontally to serve high throughput like billions",
    "start": "85360",
    "end": "92720"
  },
  {
    "text": "of operations a day and store hundreds of terabytes of data you can reliably scale out and scale in",
    "start": "92720",
    "end": "98560"
  },
  {
    "text": "on demand with larger data set without really impacting the application performance this ensures that",
    "start": "98560",
    "end": "104479"
  },
  {
    "text": "you can handle peak traffic during black friday and cyber monday so once the holiday season is over you",
    "start": "104479",
    "end": "110000"
  },
  {
    "text": "can always scale back eh you don't need maintenance maintenance window for these operations anymore",
    "start": "110000",
    "end": "116320"
  },
  {
    "text": "all these operations can happen while your database is online you can distribute data across excuse me",
    "start": "116320",
    "end": "123040"
  },
  {
    "text": "across zones regions or clouds with acid consistency so you can move data closer to your",
    "start": "123040",
    "end": "129039"
  },
  {
    "text": "customers and comply with regulations like gdpr",
    "start": "129039",
    "end": "134480"
  },
  {
    "start": "135000",
    "end": "167000"
  },
  {
    "text": "so very quickly let's look at the database design the guiding principle for gigabyte is a layered approach",
    "start": "135599",
    "end": "140959"
  },
  {
    "text": "we built a plugable query engine which preserves the top half query layer of postgres and cassandra but at the storage layer",
    "start": "140959",
    "end": "148560"
  },
  {
    "text": "is changed to use docdb which is common across both apis it is built using a custom integration",
    "start": "148560",
    "end": "155599"
  },
  {
    "text": "of rough replication distributed asset transaction and the roxdb storage engine",
    "start": "155599",
    "end": "161200"
  },
  {
    "text": "which all are based on the google spanner design",
    "start": "161200",
    "end": "166319"
  },
  {
    "start": "167000",
    "end": "269000"
  },
  {
    "text": "so now next let's see what a few benefits of deploying your database and kubernetes",
    "start": "167040",
    "end": "173200"
  },
  {
    "text": "one of the key values of containers over deploying your applications simply on vms is that you",
    "start": "173200",
    "end": "178879"
  },
  {
    "text": "get to package all of the system dependencies that are required for the application to work in your container",
    "start": "178879",
    "end": "184400"
  },
  {
    "text": "that includes your database right everything can travel together so that makes it much easier for you to ensure",
    "start": "184400",
    "end": "190159"
  },
  {
    "text": "that your application alongside database move from development to test to all the way into production",
    "start": "190159",
    "end": "195599"
  },
  {
    "text": "nothing really changes except except maybe some of the secrets you are using to communicate with the databases",
    "start": "195599",
    "end": "200720"
  },
  {
    "text": "this really gives you operation efficiency next it eliminates the single point of failure",
    "start": "200720",
    "end": "205760"
  },
  {
    "text": "now since databases are running in containers all single point of failure can be eliminated completely",
    "start": "205760",
    "end": "210879"
  },
  {
    "text": "database containers can be as highly available as any other container in your ecosystem you can set up multiple kubernetes",
    "start": "210879",
    "end": "218159"
  },
  {
    "text": "multiple redundancies use load balancing to bridge multiple containers and maintain",
    "start": "218159",
    "end": "223519"
  },
  {
    "text": "performance at all times it gives you better resource utilization when databases are deployed as",
    "start": "223519",
    "end": "229760"
  },
  {
    "text": "containers they automatically become on demand as per the rest of the application there is no need to maintain",
    "start": "229760",
    "end": "235200"
  },
  {
    "text": "a monolith database instance to maintain your data instead applications can utilize their own",
    "start": "235200",
    "end": "240640"
  },
  {
    "text": "databases as needed and only when they are needed the next one is declarative conflict",
    "start": "240640",
    "end": "246879"
  },
  {
    "text": "you can use a declarative conflict to specify your database resources so if the content of that file changes",
    "start": "246879",
    "end": "251920"
  },
  {
    "text": "kubernetes can automatically reconfigure a database to match that config this really allows easy scale on demand",
    "start": "251920",
    "end": "258720"
  },
  {
    "text": "during peak traffic and the last one is it gives you the freedom to run anywhere it gives you easy portability between different cloud",
    "start": "258720",
    "end": "265680"
  },
  {
    "text": "and on-premise with that let's see next how we modeled",
    "start": "265680",
    "end": "272800"
  },
  {
    "start": "269000",
    "end": "351000"
  },
  {
    "text": "yoga by db as a workload on kubernetes the database has two distributed services the first one is",
    "start": "272800",
    "end": "279199"
  },
  {
    "text": "yb master which is actually responsible for keeping your system metadata coordinating system-wide operations like",
    "start": "279199",
    "end": "285600"
  },
  {
    "text": "uh create alter and drop table commands and maintaining your operations such as load",
    "start": "285600",
    "end": "291440"
  },
  {
    "text": "balancing the second one is the ybt server",
    "start": "291440",
    "end": "297280"
  },
  {
    "text": "this is the actual data node which is responsible for hosting and serving your user data both",
    "start": "297280",
    "end": "303520"
  },
  {
    "text": "ypmaster and ybt server are modeled as independent independent stateful sets",
    "start": "303520",
    "end": "309199"
  },
  {
    "text": "the yb master deployment needs one stateful set and two services uh one of the one of",
    "start": "309199",
    "end": "315600"
  },
  {
    "text": "these services is actually a headless service that enables the discovery of underlying stateful parts",
    "start": "315600",
    "end": "320960"
  },
  {
    "text": "and other one is a load balancer which is needed for accessing the admin ui",
    "start": "320960",
    "end": "326240"
  },
  {
    "text": "whereas the ybt server needs just one stateful set and one headless service the stateful",
    "start": "326240",
    "end": "332800"
  },
  {
    "text": "set as as i mentioned before is actually a data node so you can easily scale up your cluster up and down",
    "start": "332800",
    "end": "338720"
  },
  {
    "text": "by just changing the replica account and it will trigger a rolling upgrade without any maintenance on time",
    "start": "338720",
    "end": "344160"
  },
  {
    "text": "the headless services is provides a load balancer for your client applications to actually connect to that database",
    "start": "344160",
    "end": "351280"
  },
  {
    "start": "351000",
    "end": "387000"
  },
  {
    "text": "so very quickly let me show you the basics of installing the database on kubernetes then we'll switch over to requirements",
    "start": "351680",
    "end": "357600"
  },
  {
    "text": "and challenges in a multi-cluster deployment model uh deploying on kubernetes is pretty",
    "start": "357600",
    "end": "363199"
  },
  {
    "text": "straightforward we have a helm repo for you to get sorted you can come up with your own set of values",
    "start": "363199",
    "end": "369520"
  },
  {
    "text": "uh what kind of database version you want resource allocation number of uh replicas for the",
    "start": "369520",
    "end": "375120"
  },
  {
    "text": "master and the data node and and if you want to change the storage class so based on your use case and throughput",
    "start": "375120",
    "end": "382080"
  },
  {
    "text": "you can really tailor the configuration of of the database",
    "start": "382080",
    "end": "387759"
  },
  {
    "text": "and then you can go ahead and simply just install it deploying a hem chart is pretty",
    "start": "387759",
    "end": "394240"
  },
  {
    "text": "straightforward in a single cluster architecture so let's let's see behind the scene",
    "start": "394240",
    "end": "400800"
  },
  {
    "text": "what's happening in the cluster and how is networking set up using kealy",
    "start": "400800",
    "end": "407840"
  },
  {
    "text": "at this point let me switch over to the terminal all right so let's first check the",
    "start": "407840",
    "end": "413440"
  },
  {
    "text": "safely set and services that are running inside a single cluster deployment of cable tv",
    "start": "413440",
    "end": "419599"
  },
  {
    "text": "as you can see here i have both t server and master stateful sets deployed and there is one headless service for yb",
    "start": "419599",
    "end": "426880"
  },
  {
    "text": "masters and one headless service for ybt servers so now at this point let me launch the kiali dashboard and let's see",
    "start": "426880",
    "end": "434400"
  },
  {
    "text": "the networking setup behind sitting behind the scenes",
    "start": "434400",
    "end": "438720"
  },
  {
    "text": "okay ah let me switch to the craft view",
    "start": "439919",
    "end": "451840"
  },
  {
    "text": "here it shows how master and t service t server services the headless services are communicating with each other",
    "start": "461599",
    "end": "469120"
  },
  {
    "text": "then we have the ybt server service which is the load balancer for connecting for client applications",
    "start": "469120",
    "end": "474960"
  },
  {
    "text": "to connect to the database and the yb master ui service for any admin operations as you can see in a",
    "start": "474960",
    "end": "482960"
  },
  {
    "text": "single cluster deployment the the the setup is fairly straightforward and easy uh we'll see on how this setup really",
    "start": "482960",
    "end": "489599"
  },
  {
    "text": "changes with the multi-cluster setup so let me go back to my presentation",
    "start": "489599",
    "end": "495680"
  },
  {
    "start": "497000",
    "end": "554000"
  },
  {
    "text": "but modeling a stateful workload for a multi-cluster has its own challenges especially around",
    "start": "498160",
    "end": "504879"
  },
  {
    "text": "networking for deploying gigabyte in a multi-cluster setup we have to satisfy",
    "start": "504879",
    "end": "512159"
  },
  {
    "text": "three high-level requirements now the master and three server pods",
    "start": "512159",
    "end": "519360"
  },
  {
    "text": "will be distributed across multiple clusters so now they should be able to reach and communicate with each other that's the",
    "start": "519360",
    "end": "525120"
  },
  {
    "text": "first requirement the second one is we need consistent global identity across",
    "start": "525120",
    "end": "531600"
  },
  {
    "text": "and within the cluster which requires setting up fully qualified domain name for each master and t server",
    "start": "531600",
    "end": "538800"
  },
  {
    "text": "within the cluster the last one is setting up load balancer for client apps connecting to the",
    "start": "538800",
    "end": "545360"
  },
  {
    "text": "database the data node which is t server and the admin ui for any operational activities",
    "start": "545360",
    "end": "552480"
  },
  {
    "start": "554000",
    "end": "591000"
  },
  {
    "text": "so here it shows what the end of the day the a very simplified view of",
    "start": "554480",
    "end": "561519"
  },
  {
    "text": "what you would expect with a three region setup of yoga by db",
    "start": "561519",
    "end": "566720"
  },
  {
    "text": "the t server and masters are reachable from within the cluster and outside the cluster as you can see",
    "start": "566720",
    "end": "573519"
  },
  {
    "text": "the central t server is actually connecting to its own mouse centers also to west and east as well",
    "start": "573519",
    "end": "581519"
  },
  {
    "text": "this is a really simplified view in few moments we'll actually look at the real view using",
    "start": "581680",
    "end": "587120"
  },
  {
    "text": "using clearly on what's happening under the hood",
    "start": "587120",
    "end": "591839"
  },
  {
    "start": "591000",
    "end": "625000"
  },
  {
    "text": "so next let's see what steps you need to follow to achieve this deployment topology first step is to",
    "start": "592480",
    "end": "600480"
  },
  {
    "text": "set up service discovery then you have to allow cross cluster access and finally you",
    "start": "600480",
    "end": "606640"
  },
  {
    "text": "have to expose services to other regions and clusters for for the first few for the first two",
    "start": "606640",
    "end": "612880"
  },
  {
    "text": "steps of service discovery and cross cluster access i'm going to use sto service mesh and then finally for exposing services",
    "start": "612880",
    "end": "620640"
  },
  {
    "text": "i'll need to make few changes on the on the database side",
    "start": "620640",
    "end": "625360"
  },
  {
    "start": "625000",
    "end": "705000"
  },
  {
    "text": "so let's look at setting up service discovery with this too for service discovery i'm using istio's",
    "start": "625680",
    "end": "633839"
  },
  {
    "text": "multi-primary multi-network setup you can see the instructions are available at the at the bottom of the",
    "start": "633839",
    "end": "639040"
  },
  {
    "text": "screen uh the steps actually require you to install",
    "start": "639040",
    "end": "644240"
  },
  {
    "text": "istio control plane all the clusters that are going to participate in a multi-cluster setup and you have to mark each cluster as a",
    "start": "644240",
    "end": "650800"
  },
  {
    "text": "primary cluster uh with the with the issue one dot with",
    "start": "650800",
    "end": "656480"
  },
  {
    "text": "with 1.8 release of istio they introduce a concept of dns proxy while kubernetes provides dns resolution",
    "start": "656480",
    "end": "663519"
  },
  {
    "text": "for kubernetes services out of the box but any custom service entry is not recognized",
    "start": "663519",
    "end": "668880"
  },
  {
    "text": "with the latest release in 1.8 now service entry addresses can be resolved with the hto site call itself",
    "start": "668880",
    "end": "675040"
  },
  {
    "text": "so you don't require any custom configuration of a dns server uh you'll need to repeat these two steps",
    "start": "675040",
    "end": "682160"
  },
  {
    "text": "of setting up primary clusters and dns processing for each cluster that is going to participate in",
    "start": "682160",
    "end": "687360"
  },
  {
    "text": "this configuration and as you can see on the right uh the cluster one is on the network one",
    "start": "687360",
    "end": "693760"
  },
  {
    "text": "while the cluster two is on the network two so this mean means that at this point there is no",
    "start": "693760",
    "end": "698800"
  },
  {
    "text": "direct connectivity between parts across clusters cluster boundaries yet which",
    "start": "698800",
    "end": "705839"
  },
  {
    "start": "705000",
    "end": "788000"
  },
  {
    "text": "brings us to the next step the next step is to set up connectivity connectivity between paws",
    "start": "705839",
    "end": "711200"
  },
  {
    "text": "so service workloads across cluster boundaries can communicate indirectly via delegated gateways for",
    "start": "711200",
    "end": "716959"
  },
  {
    "text": "e-space traffic uh the gateway in each cluster must be reachable from",
    "start": "716959",
    "end": "722720"
  },
  {
    "text": "from from the other cluster so again you would need to repeat this step of",
    "start": "722720",
    "end": "728560"
  },
  {
    "text": "setting setting up especially for every cluster that that will be participating in this conflict another important point to actually keep",
    "start": "728560",
    "end": "734959"
  },
  {
    "text": "in mind here is that gateway will be uh public on intel on the internet by default so for if you're setting up",
    "start": "734959",
    "end": "740639"
  },
  {
    "text": "anything in production you may need to make some additional adjustments on your firewall rules to not allow public",
    "start": "740639",
    "end": "746320"
  },
  {
    "text": "access",
    "start": "746320",
    "end": "748720"
  },
  {
    "text": "the last one is exposing services outside the cluster so since",
    "start": "751839",
    "end": "758399"
  },
  {
    "text": "these clusters are on two separate networks we need to expose all the services from cluster one",
    "start": "758399",
    "end": "764000"
  },
  {
    "text": "and t cluster two uh while this gateway is public on the",
    "start": "764000",
    "end": "769440"
  },
  {
    "text": "on the internet the services behind it can only be accessed by services with the trusted mutual tls certificate",
    "start": "769440",
    "end": "776079"
  },
  {
    "text": "and the workload id so visually ensures that your data is secured at any given",
    "start": "776079",
    "end": "781120"
  },
  {
    "text": "point of time and you don't really have to worry about all this because sto will handle all this seamlessly behind the scene",
    "start": "781120",
    "end": "788560"
  },
  {
    "start": "788000",
    "end": "850000"
  },
  {
    "text": "with this we are done with the service discovery using seo and the next step is setting up cross",
    "start": "788959",
    "end": "794240"
  },
  {
    "text": "cluster access inside sdu the service discovery won't",
    "start": "794240",
    "end": "800160"
  },
  {
    "text": "alone work without the endpoint discovery so you need to allow api servers say api servers access",
    "start": "800160",
    "end": "806639"
  },
  {
    "text": "across clusters uh you have to install remote opaque secrets",
    "start": "806639",
    "end": "811839"
  },
  {
    "text": "let's say in the cluster 2 that provides access to cluster 1's api server and vice versa",
    "start": "811839",
    "end": "817200"
  },
  {
    "text": "so if you have three clusters participating in this configuration you will have to repeat this step for",
    "start": "817200",
    "end": "822320"
  },
  {
    "text": "every pair so with this i have hto which is installed in all my",
    "start": "822320",
    "end": "828639"
  },
  {
    "text": "clusters i have set up my service discovery and i have also set up remote secrets so",
    "start": "828639",
    "end": "833760"
  },
  {
    "text": "i would really expect this cross cluster access for my master and t server workloads to work but there is a",
    "start": "833760",
    "end": "841120"
  },
  {
    "text": "slight problem which brings us to your next step which is to expose services",
    "start": "841120",
    "end": "847199"
  },
  {
    "start": "850000",
    "end": "942000"
  },
  {
    "text": "let's consider i'm doing a replication factor 3 deployment across",
    "start": "850880",
    "end": "856160"
  },
  {
    "text": "three different clusters uh east cluster and a waste cluster and a center cluster for simplicity sake",
    "start": "856160",
    "end": "863199"
  },
  {
    "text": "i'm not i'm just showing east and west right now what this means is really that there'll",
    "start": "863199",
    "end": "868320"
  },
  {
    "text": "be one master deployment per cluster right one in each cluster the master",
    "start": "868320",
    "end": "874639"
  },
  {
    "text": "part and the east cluster is connecting to the master pod in the west cluster and both of them both of them are",
    "start": "874639",
    "end": "880480"
  },
  {
    "text": "running in different networks in this case everything is working fine because the master server is resolved to a",
    "start": "880480",
    "end": "885519"
  },
  {
    "text": "single bot ip because there is just one port but what happens in case of a in case of",
    "start": "885519",
    "end": "890959"
  },
  {
    "text": "t server which has multiple pods scheduled on the same cluster",
    "start": "890959",
    "end": "896320"
  },
  {
    "text": "the first problem that we faced with the setup was a single t-server headless service which is what we",
    "start": "897040",
    "end": "902560"
  },
  {
    "text": "actually use in a single cluster setup if we recall i showed you uh when we were talking about single",
    "start": "902560",
    "end": "907600"
  },
  {
    "text": "cluster setup it was doing round robin between multiple t server parts so there was no consistent ip address to reach all these",
    "start": "907600",
    "end": "914560"
  },
  {
    "text": "separate parts so if the master from west was trying to connect to t server in east it was not able to get ips of all the t",
    "start": "914560",
    "end": "921519"
  },
  {
    "text": "servers that were actually running in that cluster the second one was when this headless service was",
    "start": "921519",
    "end": "927199"
  },
  {
    "text": "exposed in other clusters using istio it didn't resolve to the individual part",
    "start": "927199",
    "end": "932320"
  },
  {
    "text": "experience so again we had communication challenges between all master and dc reports across",
    "start": "932320",
    "end": "940560"
  },
  {
    "text": "clusters so to solve for these issues we ended up creating separate",
    "start": "940560",
    "end": "947440"
  },
  {
    "start": "942000",
    "end": "1213000"
  },
  {
    "text": "cluster ip services for every t server and master pods in each cluster",
    "start": "947440",
    "end": "953759"
  },
  {
    "text": "so as you can see now we have multiple cluster ip services for t servers",
    "start": "953759",
    "end": "958880"
  },
  {
    "text": "which will finally resolve to individual power ips and they are reachable from masks in",
    "start": "958880",
    "end": "964000"
  },
  {
    "text": "that cluster and also from other clusters i had to repeat this step",
    "start": "964000",
    "end": "969839"
  },
  {
    "text": "for all the clusters that are participating to make sure the t servers and masters are reachable everywhere",
    "start": "969839",
    "end": "975120"
  },
  {
    "text": "so this was this was it this was the only step that i had to do on on top of istio setup to get this",
    "start": "975120",
    "end": "981360"
  },
  {
    "text": "entire architecture working so now we have established what steps we need to follow just let's look at the",
    "start": "981360",
    "end": "987759"
  },
  {
    "text": "actual networking setup behind the scene using curly and i also want to show you",
    "start": "987759",
    "end": "993759"
  },
  {
    "text": "on how the database is set up on my terminal let me switch the window",
    "start": "993759",
    "end": "1002160"
  },
  {
    "text": "so here i have three terminal windows the one on the top left is my east cluster on the right is my west and at the",
    "start": "1002880",
    "end": "1009040"
  },
  {
    "text": "bottom is the center cluster first let's check the workloads and services running in all of them",
    "start": "1009040",
    "end": "1016240"
  },
  {
    "text": "as you can notice i have a lot more services running in each cluster compared to a single cluster setup that we just",
    "start": "1019920",
    "end": "1026558"
  },
  {
    "text": "talked about and we just touched based on the fact that we had to expose extra t",
    "start": "1026559",
    "end": "1033600"
  },
  {
    "text": "server and master services for setting up cross cluster communication and that's why you're seeing these extra services first let me",
    "start": "1033600",
    "end": "1040798"
  },
  {
    "text": "access the admin ui for this database to verify a cluster config and then we'll switch over to qrli to look at the",
    "start": "1040799",
    "end": "1047438"
  },
  {
    "text": "networking setup so for accessing the admin ui i need to access the master ui",
    "start": "1047439",
    "end": "1052720"
  },
  {
    "text": "load balancer ip on port number 7000 so let me bring it up over here",
    "start": "1052720",
    "end": "1066880"
  },
  {
    "text": "okay as you can see the replication factor three and the number of nodes are six and here",
    "start": "1066880",
    "end": "1072960"
  },
  {
    "text": "the masters of master servers which are evenly distributed across",
    "start": "1072960",
    "end": "1078400"
  },
  {
    "text": "uh each cluster the u.s central one has one then the us west ii and us east one now let's verify",
    "start": "1078400",
    "end": "1086400"
  },
  {
    "text": "the t servers so in total you can see there are six d servers which are also evenly",
    "start": "1086400",
    "end": "1091760"
  },
  {
    "text": "distributed across across these clusters so my cluster seems to be sort of fine now",
    "start": "1091760",
    "end": "1099280"
  },
  {
    "text": "let's deep dive into how the networking setup behind the scene using qrli",
    "start": "1099280",
    "end": "1104880"
  },
  {
    "text": "so i'm just going to go back to my terminal again and",
    "start": "1104880",
    "end": "1110160"
  },
  {
    "text": "launch kiali from the center cluster oops",
    "start": "1110160",
    "end": "1116240"
  },
  {
    "text": "let's first look at the mesh as you can see it shows",
    "start": "1121440",
    "end": "1128480"
  },
  {
    "text": "it's a three uh three cluster setup all all the clusters are shown here switch",
    "start": "1128480",
    "end": "1133520"
  },
  {
    "text": "looks fine we talked about that now let's go to the graph view",
    "start": "1133520",
    "end": "1139280"
  },
  {
    "text": "and look at the database setup",
    "start": "1139280",
    "end": "1151840"
  },
  {
    "text": "and let me just clean up a little bit by hiding few unnecessary",
    "start": "1152080",
    "end": "1158080"
  },
  {
    "text": "nodes zoom in",
    "start": "1158080",
    "end": "1165120"
  },
  {
    "text": "oops all right this is better uh as you can see the networking is",
    "start": "1165120",
    "end": "1173280"
  },
  {
    "text": "a lot more complicated compared to the single network setup sorry single cluster setup the",
    "start": "1173280",
    "end": "1178720"
  },
  {
    "text": "master and t server services within the yb central sorry within the central cluster now they're connecting",
    "start": "1178720",
    "end": "1184640"
  },
  {
    "text": "across the west and the east cluster and all this is set up using the same",
    "start": "1184640",
    "end": "1190000"
  },
  {
    "text": "steps that we talked about using issue for setting up cross cluster access and then setting up those extra services for allowing access",
    "start": "1190000",
    "end": "1197600"
  },
  {
    "text": "across clusters so yeah pretty this is it pretty much in setting in terms of setting up your",
    "start": "1197600",
    "end": "1203919"
  },
  {
    "text": "database let me now go back to my presentation",
    "start": "1203919",
    "end": "1209440"
  },
  {
    "start": "1213000",
    "end": "1254000"
  },
  {
    "text": "okay so that now next is the most interesting section of this talk where we're going to have a",
    "start": "1213600",
    "end": "1221200"
  },
  {
    "text": "product demo of fault tolerance i'm going to deploy i have deployed a three node",
    "start": "1221200",
    "end": "1227120"
  },
  {
    "text": "uh three node ego by cluster across three different regions and we're gonna demonstrate fault",
    "start": "1227120",
    "end": "1233039"
  },
  {
    "text": "tolerance by by taking down the entire region this is my cluster spec i've different",
    "start": "1233039",
    "end": "1239360"
  },
  {
    "text": "the databases deployed across three gke clusters and usbs west and central i'm using the default",
    "start": "1239360",
    "end": "1244960"
  },
  {
    "text": "synchronous replication between between our three different nodes and data is secured with both",
    "start": "1244960",
    "end": "1250880"
  },
  {
    "text": "encryption at rest and in flight for the purpose of demo i'm using a",
    "start": "1250880",
    "end": "1256240"
  },
  {
    "text": "classic risk micro services application which is which is also running in kubernetes the",
    "start": "1256240",
    "end": "1261919"
  },
  {
    "text": "ui app is actually outside of the cluster so let me switch my screen and show you",
    "start": "1261919",
    "end": "1269120"
  },
  {
    "text": "the ui application first",
    "start": "1269120",
    "end": "1277840"
  },
  {
    "text": "so here is my pet service ui app uh it's a it's a spring boot app what",
    "start": "1279600",
    "end": "1286240"
  },
  {
    "text": "i'm going to do is i'm going to use this app to create some owners and vet records from the ui and",
    "start": "1286240",
    "end": "1291840"
  },
  {
    "text": "then verify if i'm able to access the data across different clusters so let's first create",
    "start": "1291840",
    "end": "1300480"
  },
  {
    "text": "a new record here contest",
    "start": "1300799",
    "end": "1308640"
  },
  {
    "text": "okay so yeah everything checks out fine on the ui and now",
    "start": "1315280",
    "end": "1323760"
  },
  {
    "text": "let's create one more for the owner",
    "start": "1324080",
    "end": "1333840"
  },
  {
    "text": "okay now this is done let me switch to my terminal to verify if the records have been",
    "start": "1345200",
    "end": "1351919"
  },
  {
    "text": "created and what i'm going to do is from the",
    "start": "1351919",
    "end": "1358480"
  },
  {
    "text": "east cluster i'm going to use the ysql s shell",
    "start": "1358480",
    "end": "1363520"
  },
  {
    "text": "utility to access my database so let me launch that here",
    "start": "1363520",
    "end": "1370080"
  },
  {
    "text": "and what this does is this allows me to use postgres meta commands to",
    "start": "1372480",
    "end": "1379039"
  },
  {
    "text": "access the database so connect to petrinic i'm going to display all the tables",
    "start": "1379039",
    "end": "1388880"
  },
  {
    "text": "and look at the bonus table just let's clear up",
    "start": "1388880",
    "end": "1398480"
  },
  {
    "text": "all right uh here as you can see this was the record that i created from the ui",
    "start": "1399120",
    "end": "1404159"
  },
  {
    "text": "so everything checks are fine in the east cluster for the owners table let's repeat the same",
    "start": "1404159",
    "end": "1411200"
  },
  {
    "text": "exercise on the best cluster",
    "start": "1411200",
    "end": "1417840"
  },
  {
    "text": "connect and",
    "start": "1424559",
    "end": "1431840"
  },
  {
    "text": "okay so the record is available even in the west let's check the records for the vet",
    "start": "1432159",
    "end": "1439360"
  },
  {
    "text": "that we created oops i think the name of the table is bits",
    "start": "1439360",
    "end": "1445600"
  },
  {
    "text": "and so",
    "start": "1446640",
    "end": "1452400"
  },
  {
    "text": "cube contest that was the one i created",
    "start": "1453760",
    "end": "1459840"
  },
  {
    "text": "okay yeah so data seems to be persisted across across different clusters so this",
    "start": "1461679",
    "end": "1469039"
  },
  {
    "text": "verifies that single database database stressed across multiple kubernetes",
    "start": "1469039",
    "end": "1474240"
  },
  {
    "text": "is working fine with with data consistency i mean i don't have to do the test for central we'll we'll look at and",
    "start": "1474240",
    "end": "1481039"
  },
  {
    "text": "in the next in the failover by accessing the data actually through central",
    "start": "1481039",
    "end": "1487039"
  },
  {
    "text": "so at this point let me go back to my presentation",
    "start": "1487039",
    "end": "1491919"
  },
  {
    "start": "1495000",
    "end": "1726000"
  },
  {
    "text": "now let's actually simulate a region level outage by bringing down both the",
    "start": "1496320",
    "end": "1501360"
  },
  {
    "text": "stateless app and my database node what i want to achieve is strong high availability even",
    "start": "1501360",
    "end": "1507600"
  },
  {
    "text": "in this case when the entire region goes down i want to see what would be the impact of on my client application",
    "start": "1507600",
    "end": "1515039"
  },
  {
    "text": "let me go back to the terminal or simulating east region level failure",
    "start": "1515039",
    "end": "1522640"
  },
  {
    "text": "i'm going to scale down the deployment for both stateless and the stateful app",
    "start": "1522640",
    "end": "1528799"
  },
  {
    "text": "in my east cluster so let me get it out get out of this shell utility",
    "start": "1528799",
    "end": "1535360"
  },
  {
    "text": "back here and first let's scale down our master",
    "start": "1535360",
    "end": "1542159"
  },
  {
    "text": "staple set to zero",
    "start": "1542159",
    "end": "1545679"
  },
  {
    "text": "and let's bring down dt server as well",
    "start": "1548640",
    "end": "1557200"
  },
  {
    "text": "and finally the stateless application all right so we can",
    "start": "1557200",
    "end": "1563760"
  },
  {
    "text": "verify databases scale down",
    "start": "1563760",
    "end": "1571840"
  },
  {
    "text": "okay you can see it's terminating so that's which is good and now let's go back to our ui app",
    "start": "1574080",
    "end": "1581440"
  },
  {
    "text": "and try to access the app with some records and let's see what happens on the database side so let me just",
    "start": "1581440",
    "end": "1588960"
  },
  {
    "text": "first refresh this app and try to create a new record",
    "start": "1588960",
    "end": "1594320"
  },
  {
    "text": "for an owner",
    "start": "1594320",
    "end": "1597440"
  },
  {
    "text": "sfo",
    "start": "1603760",
    "end": "1606320"
  },
  {
    "text": "six all right now added a record",
    "start": "1610840",
    "end": "1615919"
  },
  {
    "text": "let's go back to the terminal and try to access this record from the west and the center cluster",
    "start": "1615919",
    "end": "1627840"
  },
  {
    "text": "kubecon region fade over record is available here and let's try to do the same thing",
    "start": "1639760",
    "end": "1646799"
  },
  {
    "text": "from the center cluster",
    "start": "1646799",
    "end": "1655840"
  },
  {
    "text": "connected to the pet cleaning database and let's try to access the record",
    "start": "1661200",
    "end": "1667679"
  },
  {
    "text": "here so as you can see even my even in the case when my ease cluster is",
    "start": "1667679",
    "end": "1673520"
  },
  {
    "text": "not available i'm trying i'm able to access the records i'm able to access the database from the central and the west",
    "start": "1673520",
    "end": "1680840"
  },
  {
    "text": "cluster so this really proves that my cluster is resilient to region level failures as",
    "start": "1680840",
    "end": "1687679"
  },
  {
    "text": "well so this is it i hope you enjoyed this demo and and you notice how easy it was to",
    "start": "1687679",
    "end": "1694960"
  },
  {
    "text": "build a cloud-native fall tolerant and geo-distributed data layer using",
    "start": "1694960",
    "end": "1700399"
  },
  {
    "text": "service mesh and ego by db but at this point few of you might be thinking about the app latency due to",
    "start": "1700399",
    "end": "1707200"
  },
  {
    "text": "regional boundaries and how i can improve the app performance so for that let me switch back to my presentation and we'll cover",
    "start": "1707200",
    "end": "1713600"
  },
  {
    "text": "on how to improve performance for geo distributed applications",
    "start": "1713600",
    "end": "1725840"
  },
  {
    "start": "1726000",
    "end": "1908000"
  },
  {
    "text": "all right so this is an interesting use case that you can try on your own to improve your application performance",
    "start": "1726960",
    "end": "1733200"
  },
  {
    "text": "by default all nodes of eco by db are eligible to have short leaders but what",
    "start": "1733200",
    "end": "1740000"
  },
  {
    "text": "if you want to localize your shard leaders closer to your client application",
    "start": "1740000",
    "end": "1745039"
  },
  {
    "text": "which might be deployed in a particular region for example if the client app is deployed in u.s west so what if you also wanted to have",
    "start": "1745039",
    "end": "1751360"
  },
  {
    "text": "a show leader in that particular region or imagine another use case that your application reads",
    "start": "1751360",
    "end": "1756480"
  },
  {
    "text": "including multi-role joins are coming from a single region then a three region distributed cluster",
    "start": "1756480",
    "end": "1763679"
  },
  {
    "text": "can be configured to have short leaders pinned to that single region",
    "start": "1763679",
    "end": "1769120"
  },
  {
    "text": "in that case you can set a preferred zone which will reduce the number of hops for",
    "start": "1769120",
    "end": "1774640"
  },
  {
    "text": "database to write transactions and it will give you an immediate performance improvement",
    "start": "1774640",
    "end": "1780240"
  },
  {
    "text": "as you can see here you can use yb admin command which takes all master addresses and",
    "start": "1780240",
    "end": "1786960"
  },
  {
    "text": "preferred zone as as an as input variables so try this use case out and let us know how does it",
    "start": "1786960",
    "end": "1792559"
  },
  {
    "text": "improve your application performance",
    "start": "1792559",
    "end": "1795840"
  },
  {
    "text": "so now that we have gone through getting multi-cluster setup the easy way and that's pretty much it we have shown",
    "start": "1797760",
    "end": "1806399"
  },
  {
    "text": "what you need to do to get service service discovery so that you can access access services",
    "start": "1806399",
    "end": "1811840"
  },
  {
    "text": "from one cluster and another uh we have outlined what what it takes to get cross cluster access",
    "start": "1811840",
    "end": "1818399"
  },
  {
    "text": "so this really opens up new possibilities on how you can deploy a distributed sql database to",
    "start": "1818399",
    "end": "1824799"
  },
  {
    "text": "build fault tolerant cloud native applications as you can see the first one is single",
    "start": "1824799",
    "end": "1830399"
  },
  {
    "text": "region multi zone this is a traditional use case that probably most of most of you are doing it today this has the lowest latency",
    "start": "1830399",
    "end": "1837200"
  },
  {
    "text": "because everything is localized within the region but it does not give you any region level failover uh failover resiliency the second one is",
    "start": "1837200",
    "end": "1844640"
  },
  {
    "text": "what we talked about today within a single cloud now you're stretching your single database across multiple regions",
    "start": "1844640",
    "end": "1850559"
  },
  {
    "text": "this gives you region level failover resiliency and the third one is multi-cloud multi-region which is",
    "start": "1850559",
    "end": "1856960"
  },
  {
    "text": "really now you can stretch your database across different clouds right the same",
    "start": "1856960",
    "end": "1862080"
  },
  {
    "text": "constructs the primitives that we talked in a single cloud multi region now can be applied to this setup as well",
    "start": "1862080",
    "end": "1868159"
  },
  {
    "text": "so now you get cloud level failover resiliency for your application as well",
    "start": "1868159",
    "end": "1874398"
  },
  {
    "text": "finally some parting thoughts um we are a fast growing project but we really like to get the community",
    "start": "1877039",
    "end": "1883279"
  },
  {
    "text": "involved uh we would love to know more about your use cases so jump onto",
    "start": "1883279",
    "end": "1889200"
  },
  {
    "text": "onto those links if you are interested in becoming a cloud native database expert",
    "start": "1889200",
    "end": "1894880"
  },
  {
    "text": "and if there is anything that you need or help with come join us slack community uh with that thank you so much",
    "start": "1894880",
    "end": "1902960"
  },
  {
    "text": "and have a great rest of your kubecon thank you",
    "start": "1902960",
    "end": "1910000"
  }
]