[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "and we are recording so I'd like everybody for joining us today",
    "start": "30",
    "end": "5370"
  },
  {
    "text": "welcome to the CN CF webinar stork storage operator for multi cloud deployments I'm Kim McMahon on the CN CF",
    "start": "5370",
    "end": "13019"
  },
  {
    "text": "marketing team and I'll be moderating today's webinar we'd like to welcome our presenter today Dean ashes is rani",
    "start": "13019",
    "end": "20279"
  },
  {
    "text": "principal software engineer at port works a few housekeeping items before we",
    "start": "20279",
    "end": "25410"
  },
  {
    "text": "get started during the webinar you're not able to talk as an attendee there's a Q&A box at the bottom of your screen",
    "start": "25410",
    "end": "31800"
  },
  {
    "text": "so please feel free to drop your questions in there and we'll get to as many as we can Dinesh will be answering them throughout",
    "start": "31800",
    "end": "38250"
  },
  {
    "text": "the webinar the session is being recorded and will be sent out afterward along with a link to the presentation",
    "start": "38250",
    "end": "44399"
  },
  {
    "text": "now with that I'll hand it over to Dinesh to kick off today's presentation",
    "start": "44399",
    "end": "50960"
  },
  {
    "text": "hey thanks thanks for giving us the opportunity to present about stock today",
    "start": "51379",
    "end": "56420"
  },
  {
    "text": "so let's get started so I'm gonna present about an open source project",
    "start": "56420",
    "end": "62489"
  },
  {
    "text": "that we started at port works called stock which is basically a multi cloud operator for managing your storage to",
    "start": "62489",
    "end": "70320"
  },
  {
    "text": "run your stateful applications my name is Denise Rani and I'm a principal software engineer at port works and I've",
    "start": "70320",
    "end": "76470"
  },
  {
    "text": "been at port works for roughly two and a half years now so this is the agenda for",
    "start": "76470",
    "end": "81900"
  },
  {
    "text": "the talks so first I'm going to introduce stock and what it is and what it does and I'm going to talk about what",
    "start": "81900",
    "end": "87450"
  },
  {
    "text": "the motivation for stock is and then we'll go to the different features that stock provides basically how we how you",
    "start": "87450",
    "end": "96180"
  },
  {
    "text": "start to shadow your stateful services more efficiently and how it can be used to monitor your storage storage",
    "start": "96180",
    "end": "102990"
  },
  {
    "text": "solutions then we talked about how to stock and reuse for different types of disaster recoveries basically how you",
    "start": "102990",
    "end": "109380"
  },
  {
    "text": "can take snapshots backups of your applications and how you can migrate migrate your applications between",
    "start": "109380",
    "end": "114840"
  },
  {
    "text": "clusters and if there's time I'll also do a demo and then we'll move on to the Q&A section if there's any more",
    "start": "114840",
    "end": "122100"
  },
  {
    "text": "questions towards the end of the seminar but you can always ask questions while I'm presenting and I answer them",
    "start": "122100",
    "end": "129629"
  },
  {
    "text": "as they come up so so an introduction about stocks so",
    "start": "129629",
    "end": "135120"
  },
  {
    "text": "our stock was started and maintained by port works which is a software-defined solution for communities so it",
    "start": "135120",
    "end": "143700"
  },
  {
    "text": "basically helps you manage your data for container workloads it is an open source",
    "start": "143700",
    "end": "150150"
  },
  {
    "text": "project and it's available on github under Apache 2.0 license so stock was",
    "start": "150150",
    "end": "156270"
  },
  {
    "text": "started in November 2017 and we went GA in January 2018 since then we have had",
    "start": "156270",
    "end": "163260"
  },
  {
    "text": "our 2023 releases minor and major and the next 2.3.4 release is scheduled for",
    "start": "163260",
    "end": "170550"
  },
  {
    "text": "the end of july so these are just some of the adopters that that are actually",
    "start": "170550",
    "end": "178320"
  },
  {
    "text": "using stock right now either in production or in pre-production there",
    "start": "178320",
    "end": "183780"
  },
  {
    "text": "there are few more users but I just wanted to put out some of the users are",
    "start": "183780",
    "end": "190170"
  },
  {
    "text": "actually using them right now so let's talk about the motivation for stock so",
    "start": "190170",
    "end": "197070"
  },
  {
    "start": "193000",
    "end": "193000"
  },
  {
    "text": "when we started off Starke one of our motivations was to basically to help people run the stateful applications",
    "start": "197070",
    "end": "204270"
  },
  {
    "text": "more efficiently on kubernetes so basically the stories subsystem in",
    "start": "204270",
    "end": "210510"
  },
  {
    "text": "kubernetes is built in a very generic way so there is no real way to run",
    "start": "210510",
    "end": "215780"
  },
  {
    "text": "stateful applications hyper-converged if you're running a software defined soft",
    "start": "215780",
    "end": "221100"
  },
  {
    "text": "storage solutions on kubernetes and also there's no real native way to monitor",
    "start": "221100",
    "end": "227250"
  },
  {
    "text": "the state of your stateful apps that are using software-defined storage solutions",
    "start": "227250",
    "end": "232460"
  },
  {
    "text": "we also wanted a way to basically help users manage the entire lifecycle of stateful applications so start helps you",
    "start": "232460",
    "end": "240030"
  },
  {
    "text": "take application consistent snapshots and it also helps you migrate your applications between clusters and we've",
    "start": "240030",
    "end": "246210"
  },
  {
    "text": "also added features to basically pack up your entire data and you're covering",
    "start": "246210",
    "end": "251370"
  },
  {
    "text": "these resources so basically to help you backup your entire applications into an external store and then restore them in",
    "start": "251370",
    "end": "257910"
  },
  {
    "text": "case of a disaster scenario so the also",
    "start": "257910",
    "end": "263270"
  },
  {
    "text": "start is built on a plug-in model so we have support for the port folks storage",
    "start": "263270",
    "end": "269160"
  },
  {
    "text": "driver but the way it is built it can actually be extended to work with any storage driver there is there is already",
    "start": "269160",
    "end": "275580"
  },
  {
    "text": "there is an interface that is used by all the different components and as long as you implement that interface you",
    "start": "275580",
    "end": "281400"
  },
  {
    "text": "should be able to use any of the features of stock so let's move on to",
    "start": "281400",
    "end": "288660"
  },
  {
    "text": "the first feature that stock provides so so when you are running a software-defined storage solution you",
    "start": "288660",
    "end": "295110"
  },
  {
    "text": "want to basically make sure that your apps your pods are scheduled on a node where your data is basically located",
    "start": "295110",
    "end": "302450"
  },
  {
    "text": "locally this is this for multiple reasons first of all this provides",
    "start": "302450",
    "end": "308670"
  },
  {
    "text": "better performance because all your reads and lights will actually be serviced locally instead of going over",
    "start": "308670",
    "end": "313770"
  },
  {
    "text": "the network the second thing is it actually reduces network congestion because if you are using a software",
    "start": "313770",
    "end": "320610"
  },
  {
    "text": "defying solution software-defined storage solution and your data is",
    "start": "320610",
    "end": "325830"
  },
  {
    "text": "actually located on a different node you will actually end up using network bandwidth which could which could have",
    "start": "325830",
    "end": "331590"
  },
  {
    "text": "adverse effects on your applications as well as your storage solution so so how does how do people actually use how do",
    "start": "331590",
    "end": "340650"
  },
  {
    "text": "people do this without stock so the the wide variety of solutions basically use",
    "start": "340650",
    "end": "349250"
  },
  {
    "text": "labels and affinity rules on their applications there are a couple of",
    "start": "349250",
    "end": "354330"
  },
  {
    "text": "issues with this first of all it doesn't really scale you have to make sure that all your applications are actually using",
    "start": "354330",
    "end": "361680"
  },
  {
    "text": "all your applications as well as your PVCs actually have the same labels and you need to make sure that all of them",
    "start": "361680",
    "end": "367770"
  },
  {
    "text": "actually have this - make sure things are getting hyper-converged the other thing is this this obviously error-prone",
    "start": "367770",
    "end": "373800"
  },
  {
    "text": "what if you forget to put these label selectors or affinity rules on your applications in that case everything",
    "start": "373800",
    "end": "380730"
  },
  {
    "text": "will work fine it just won't be hyper-converged another major issue with",
    "start": "380730",
    "end": "387150"
  },
  {
    "text": "this is that it doesn't really work with stateful sets because in stateful sets you are not since you are only able to",
    "start": "387150",
    "end": "394290"
  },
  {
    "text": "specify a PVC PV temp PVC template name you aren't able to associate the PVCs",
    "start": "394290",
    "end": "400740"
  },
  {
    "text": "that automatically created by stateful sets to the pods that we'll be using",
    "start": "400740",
    "end": "405780"
  },
  {
    "text": "those PVCs so you're not able to specify unique labels for both the PVCs and the",
    "start": "405780",
    "end": "411990"
  },
  {
    "text": "pods using stateful sets if you were to use labels and affinity rules so so our",
    "start": "411990",
    "end": "421139"
  },
  {
    "start": "419000",
    "end": "419000"
  },
  {
    "text": "solution was to basically use a scheduler extenders that is built into communities so there was a feature added",
    "start": "421139",
    "end": "428969"
  },
  {
    "text": "into kubernetes to have an external component running as an extender and what happens in that case is every time",
    "start": "428969",
    "end": "436199"
  },
  {
    "text": "apart is to be scheduled it actually makes a few API calls to the extender to help help and make decisions on where",
    "start": "436199",
    "end": "443009"
  },
  {
    "text": "pods should be scheduled so stock has basically there are a couple of verbs that you can basically implement in your",
    "start": "443009",
    "end": "449370"
  },
  {
    "text": "extender and stock has implemented two of these so the first is first of verb",
    "start": "449370",
    "end": "455400"
  },
  {
    "text": "that start as implemented is basically the filter and what this does is it",
    "start": "455400",
    "end": "462120"
  },
  {
    "text": "basically filters out nodes where your storage isn't available so I'll go into detail into what this does in the next",
    "start": "462120",
    "end": "467969"
  },
  {
    "text": "few slides but think of it this way right so if you have attended kubernetes",
    "start": "467969",
    "end": "474330"
  },
  {
    "text": "clusters and you have your storage solution installed only on five nodes now you obviously don't want your pods",
    "start": "474330",
    "end": "480779"
  },
  {
    "text": "to get scheduled on the finals that does not have your storage solution so this filter verb will basically work in in",
    "start": "480779",
    "end": "487289"
  },
  {
    "text": "stock to filter out the nodes west where your storage solution is not installed the other work that has been implemented",
    "start": "487289",
    "end": "495360"
  },
  {
    "text": "here is the prioritize node what what this does is it basically for each part",
    "start": "495360",
    "end": "502289"
  },
  {
    "text": "that is to be scheduled it basically checks where the where the volumes are located for that for that pod basically",
    "start": "502289",
    "end": "510689"
  },
  {
    "text": "where the replicas are located and then depending on that it basically prioritizes the nodes where the pot",
    "start": "510689",
    "end": "516719"
  },
  {
    "text": "should be get should be scheduled so this takes this takes into consideration a couple of things it's not just the",
    "start": "516719",
    "end": "523440"
  },
  {
    "text": "node but it also makes sure in case you don't have resources on the node it then prioritizes nodes on the same rack",
    "start": "523440",
    "end": "530010"
  },
  {
    "text": "and racks on the same region and basically also zones so it basically makes sure that your data is closest",
    "start": "530010",
    "end": "536270"
  },
  {
    "text": "your pods get started closest to where your data lies we have one question the",
    "start": "536270",
    "end": "544650"
  },
  {
    "text": "underlying your underlying container technology matter and and then further",
    "start": "544650",
    "end": "549720"
  },
  {
    "text": "he asks so this is based on the container storage interface specification so I'm not sure about the",
    "start": "549720",
    "end": "559080"
  },
  {
    "text": "first part but so like I mentioned everything is basically an interface so if you mean the storage solution this",
    "start": "559080",
    "end": "564630"
  },
  {
    "text": "can actually be extended to work with any storage solution if you meant whether it works with docker or",
    "start": "564630",
    "end": "569640"
  },
  {
    "text": "continuity or anything it will work with any of the container runtimes about si",
    "start": "569640",
    "end": "575970"
  },
  {
    "text": "si so this does not use the si si api's directly because the si si ApS don't",
    "start": "575970",
    "end": "583080"
  },
  {
    "text": "have support to get all the information over here so what happens is since",
    "start": "583080",
    "end": "588360"
  },
  {
    "text": "everything is implemented as an interface all the different components make API calls to the interface and then",
    "start": "588360",
    "end": "594690"
  },
  {
    "text": "the port works implementation of the interface makes API calls to two port works in this case but we could also",
    "start": "594690",
    "end": "601950"
  },
  {
    "text": "have a si si implementation of the driver as long as si si provides all the information back for for the different",
    "start": "601950",
    "end": "609450"
  },
  {
    "text": "modules to work I hope that answers the question all right so the advantage of",
    "start": "609450",
    "end": "619110"
  },
  {
    "text": "using this scheduler extender is that it's it's it's very simple to use so you",
    "start": "619110",
    "end": "624990"
  },
  {
    "text": "don't have to worry about making sure that your labels on your PVC is match your applications and that you have set",
    "start": "624990",
    "end": "630990"
  },
  {
    "text": "up your and the affinity rules or affinity rules correctly on your applications all you need to do is users",
    "start": "630990",
    "end": "636510"
  },
  {
    "text": "need to configure your scheduler name to be the new instance of the scheduler that you started and actually you can uh",
    "start": "636510",
    "end": "643410"
  },
  {
    "text": "you can also configure your default scheduler to work with the extender so in that case you don't have to make any",
    "start": "643410",
    "end": "648870"
  },
  {
    "text": "changes at all so as long as your scheduler is made is configured to talk to stock you will automatically be able",
    "start": "648870",
    "end": "656040"
  },
  {
    "text": "to provide hyperconvergence for your stateful apps the other way is you can basically start a new instance of the",
    "start": "656040",
    "end": "662880"
  },
  {
    "text": "scheduler and any call it start for instance and then use that in your applications so we had",
    "start": "662880",
    "end": "670270"
  },
  {
    "text": "also added support for for for a feature called initializes and kubernetes what",
    "start": "670270",
    "end": "676870"
  },
  {
    "text": "this would do is every time you would actually create a stateful set or a deployment the initializer would then go",
    "start": "676870",
    "end": "682780"
  },
  {
    "text": "and check if it's using a PVC that is supported by stock basically the",
    "start": "682780",
    "end": "687970"
  },
  {
    "text": "provision is supported by stock if it was it would automatically go and a bid the schedule a name so that you would get hyperconvergence but initializers",
    "start": "687970",
    "end": "696040"
  },
  {
    "text": "actually got deprecated and kubernetes 1.14 so we are looking at other ways to",
    "start": "696040",
    "end": "701680"
  },
  {
    "text": "provide this automatic update for the scheduler name in your deployments and stateful sets so this is basically how",
    "start": "701680",
    "end": "712020"
  },
  {
    "start": "710000",
    "end": "710000"
  },
  {
    "text": "how the filter work would work for first when you're when a part is trying to get",
    "start": "712020",
    "end": "717430"
  },
  {
    "text": "scheduled so for example every time a part is going to be scheduled the",
    "start": "717430",
    "end": "722920"
  },
  {
    "text": "cuvette scheduler would would basically see that a new part has been created in the API server and at that point yeah at",
    "start": "722920",
    "end": "733810"
  },
  {
    "text": "that point it would basically send out a filter request to start now start would basically talk to the storage solution",
    "start": "733810",
    "end": "740620"
  },
  {
    "text": "in this case I've shown an example of how it would work with port works it would basically talk to port works and",
    "start": "740620",
    "end": "746080"
  },
  {
    "text": "figure out on what nodes the storage solution is installed so for example",
    "start": "746080",
    "end": "751840"
  },
  {
    "text": "when a part is to be scheduled it will actually tell stock that I have these five nodes and one to n five and then",
    "start": "751840",
    "end": "758680"
  },
  {
    "text": "start will basically talk to the storage solution and figure out that the solution is installed only on n1 n2 and",
    "start": "758680",
    "end": "764980"
  },
  {
    "text": "n3 and then it would basically filter out and four and n Phi and send back only and went to n3 because those are",
    "start": "764980",
    "end": "771220"
  },
  {
    "text": "the only nodes by the pod can actually get scheduled if it wants to use the storage solution now once the filter",
    "start": "771220",
    "end": "779590"
  },
  {
    "text": "request has has gone through the kubja sherrilyn will then send a prioritize",
    "start": "779590",
    "end": "784600"
  },
  {
    "text": "request to start now since we have filtered out n n4 and n5 it will only",
    "start": "784600",
    "end": "789820"
  },
  {
    "text": "send n 1 to n 3 and it will then tell the extender these are the three nodes",
    "start": "789820",
    "end": "795190"
  },
  {
    "text": "where I want to show you my parts can you be please prioritize the nodes where these should be scheduled now again start will",
    "start": "795190",
    "end": "802629"
  },
  {
    "text": "basically figure out where the replicas for the volumes line so for in this example the part that is to be scheduled",
    "start": "802629",
    "end": "810550"
  },
  {
    "text": "is actually using two volumes v1 and v2 and from the diagram you can see that two replicas these are basically volumes",
    "start": "810550",
    "end": "818439"
  },
  {
    "text": "with two replicas and the replicas of v1 lie on n1 and n2 and the wall replicas",
    "start": "818439",
    "end": "824559"
  },
  {
    "text": "for P to lie on n2 and n3 now obviously over here if you shed",
    "start": "824559",
    "end": "829569"
  },
  {
    "text": "you'll the pod on n2 you will get the best performance because both your volumes v1 and v2 are located locally so",
    "start": "829569",
    "end": "836439"
  },
  {
    "text": "start will basically send an API call to the storage driver and ask for it well",
    "start": "836439",
    "end": "841689"
  },
  {
    "text": "the volume replicas lie once it gets that information then it will basically score the different nodes",
    "start": "841689",
    "end": "847839"
  },
  {
    "text": "are based on how many replicas each each node has so for example in this case",
    "start": "847839",
    "end": "854350"
  },
  {
    "text": "what will happen is it will basically rate n1 will give anyone a score of 100 into a score of 200 since it has both",
    "start": "854350",
    "end": "861730"
  },
  {
    "text": "the replicas and n3 a score of 100 since it has wonderful Acuff for one of the volumes and at this point the",
    "start": "861730",
    "end": "868929"
  },
  {
    "text": "correlation EULA will realize that n2 has the highest priority and start the",
    "start": "868929",
    "end": "874959"
  },
  {
    "text": "part on on node n2 so basically this is",
    "start": "874959",
    "end": "880299"
  },
  {
    "text": "this will end up hypo converging the part with the data and all the i/o that will happen will basically happen",
    "start": "880299",
    "end": "886480"
  },
  {
    "text": "locally on the node can you we have a",
    "start": "886480",
    "end": "893799"
  },
  {
    "text": "couple questions the first one does stork run as a side car so stock itself",
    "start": "893799",
    "end": "901119"
  },
  {
    "text": "runs as a deployment we just start we started so typically we started as a",
    "start": "901119",
    "end": "906999"
  },
  {
    "text": "three replicas deployment and one of them is V learner leader election and one of them is the leader so we run this",
    "start": "906999",
    "end": "913179"
  },
  {
    "text": "in the three erica mode so that it's it feels over quickly in case one of the replicas fails okay and second question",
    "start": "913179",
    "end": "921970"
  },
  {
    "text": "how does this integrate with PVC so",
    "start": "921970",
    "end": "927660"
  },
  {
    "text": "so there are a couple of things right so the stuff that I'm talking about right now where the scheduling is happening so",
    "start": "927660",
    "end": "933250"
  },
  {
    "text": "the PVC has already been created so for example for either port works or any of",
    "start": "933250",
    "end": "938440"
  },
  {
    "text": "the other storage solutions you will either have drivers entry in kubernetes",
    "start": "938440",
    "end": "943570"
  },
  {
    "text": "or you'll basically either use flex volumes or an external provisional or CSI Pro is not to provision the volumes",
    "start": "943570",
    "end": "950320"
  },
  {
    "text": "so there is no provisioning happening over here as such all this is doing is every time a pod comes in it sees what",
    "start": "950320",
    "end": "956980"
  },
  {
    "text": "PVCs are being used by the pod and then it basically talks to the storage driver to get more information about that PVC",
    "start": "956980",
    "end": "963520"
  },
  {
    "text": "and then it basically schedules the pod closer to where the volumes for that PVC",
    "start": "963520",
    "end": "969370"
  },
  {
    "text": "lie so it's using the PVCs but it's not really manipulating any of the PVCs over",
    "start": "969370",
    "end": "974710"
  },
  {
    "text": "here okay and the third question how is it different or similar to Rock so rook",
    "start": "974710",
    "end": "984340"
  },
  {
    "text": "is basically an installer for four different storage solutions so stock",
    "start": "984340",
    "end": "991120"
  },
  {
    "text": "does not actually do any installation of any storage solution it is just orchestrating data and making sure your",
    "start": "991120",
    "end": "997330"
  },
  {
    "text": "stateful apps done more efficiently so it is not doing an actual install of port books or any of the",
    "start": "997330",
    "end": "1003090"
  },
  {
    "text": "storage solutions it's just making API calls to different put to the different for the different features to basically",
    "start": "1003090",
    "end": "1010230"
  },
  {
    "text": "orchestrate all these things basically make sure everything's hyper-converged and based what I'm going to talk about",
    "start": "1010230",
    "end": "1016350"
  },
  {
    "text": "next is monitor the health of your storage solutions and basically D are snapshots and backups okay we have a",
    "start": "1016350",
    "end": "1023880"
  },
  {
    "text": "couple more questions still this works like a tool or like him oh no so he'll",
    "start": "1023880",
    "end": "1031530"
  },
  {
    "text": "again is is the one that we'll be installing the apps so this is not",
    "start": "1031530",
    "end": "1037319"
  },
  {
    "text": "installing any of the apps right so you might actually use help to to install for example a Cassandra chart and at",
    "start": "1037320",
    "end": "1044970"
  },
  {
    "text": "that point it will either install a deployment or a stateful set which will spin up pods now once these pods are",
    "start": "1044970",
    "end": "1051660"
  },
  {
    "text": "going to be scheduled that's when start will actually kick in because the scheduler is then basically",
    "start": "1051660",
    "end": "1057720"
  },
  {
    "text": "going to try to schedule the pods on different nodes and it'll basically make API calls to to",
    "start": "1057720",
    "end": "1063050"
  },
  {
    "text": "start to figure out where it should be scheduled so again this is not installing or installing any of the apps",
    "start": "1063050",
    "end": "1068990"
  },
  {
    "text": "it's just helping you run your apps more efficiently okay what is px on a slide",
    "start": "1068990",
    "end": "1077030"
  },
  {
    "text": "also px is basically port works so I'm just showing an example of of port works",
    "start": "1077030",
    "end": "1083540"
  },
  {
    "text": "running as the storage solution on this cluster okay can a storage be sand or",
    "start": "1083540",
    "end": "1089180"
  },
  {
    "text": "nows yes it can be any storage solution as long as stark and support for that",
    "start": "1089180",
    "end": "1094760"
  },
  {
    "text": "driver okay question from David this is",
    "start": "1094760",
    "end": "1099830"
  },
  {
    "text": "just for pot alignment two volumes for performance ah yes so this feature is",
    "start": "1099830",
    "end": "1106040"
  },
  {
    "text": "basically for performance to make sure that your pods like close to where your data is located okay is stork in the",
    "start": "1106040",
    "end": "1114230"
  },
  {
    "text": "data path no stock is not in the data path it's only in the scheduling path in",
    "start": "1114230",
    "end": "1119810"
  },
  {
    "text": "this in this scenario okay on what basis but and what basis volumes",
    "start": "1119810",
    "end": "1126560"
  },
  {
    "text": "are created on the worker nodes so again stark does not participate in the actual",
    "start": "1126560",
    "end": "1133880"
  },
  {
    "text": "provisioning of the volume so in this case what would happen is you would actually create a PVC and say put box is",
    "start": "1133880",
    "end": "1140480"
  },
  {
    "text": "the provision of or that PVC once you apply it the entry divert for that PVC",
    "start": "1140480",
    "end": "1145490"
  },
  {
    "text": "would go ahead and go ahead and provision the volumes so stock is not in the provisioning path in this case okay",
    "start": "1145490",
    "end": "1154310"
  },
  {
    "text": "and this may have already been answered but in case of stork how the PD gets created in storage",
    "start": "1154310",
    "end": "1159440"
  },
  {
    "text": "every PV needs PVC and for dynamic provisioning we need provisioner and",
    "start": "1159440",
    "end": "1164540"
  },
  {
    "text": "storage class yes so again that's that's outside start but yeah when you create a",
    "start": "1164540",
    "end": "1171740"
  },
  {
    "text": "PVC you would basically first create a storage class and put the provisioner over there you would provide the different options you want for the for",
    "start": "1171740",
    "end": "1178610"
  },
  {
    "text": "the PVC and then refer to that storage class in the PVC once you create the PVC it would basically talk to whoever the",
    "start": "1178610",
    "end": "1186230"
  },
  {
    "text": "provider is so it could be the entry provision or it could be flex volumes or it could be the CSF revisional and at",
    "start": "1186230",
    "end": "1191780"
  },
  {
    "text": "that point we'll basically create will be we talk to the storage solution create a volume and then by create a PV and then",
    "start": "1191780",
    "end": "1198710"
  },
  {
    "text": "bind the PV to the PVC now start kicks in only after all of this has been done and you are actually scheduling a part",
    "start": "1198710",
    "end": "1205100"
  },
  {
    "text": "to use the PVC that has been created okay is there any value if we are not",
    "start": "1205100",
    "end": "1210620"
  },
  {
    "text": "stuck on internal disk there is still",
    "start": "1210620",
    "end": "1217880"
  },
  {
    "text": "value I talked about what other features you can use basically but even if you",
    "start": "1217880",
    "end": "1225590"
  },
  {
    "text": "are not using local disks there is still value in the sense you so you can do a couple of things over here right I mean",
    "start": "1225590",
    "end": "1231680"
  },
  {
    "text": "right now the prioritization is happening only based on where Tara is located but that could be scenarios but",
    "start": "1231680",
    "end": "1237380"
  },
  {
    "text": "you want to make sure that you don't overwhelm even if you're using network attached storage right you don't want to",
    "start": "1237380",
    "end": "1242900"
  },
  {
    "text": "use you don't want to attach a lot of network attached disks on to one one node so you could basically do a couple",
    "start": "1242900",
    "end": "1249140"
  },
  {
    "text": "of things where you try to spread out those pods based on the density of the",
    "start": "1249140",
    "end": "1254860"
  },
  {
    "text": "you can spread out the pods on different nodes depending on how many how many stories how many this you have attached",
    "start": "1254860",
    "end": "1261830"
  },
  {
    "text": "on a particular node ok so another comment here so stork is basically an",
    "start": "1261830",
    "end": "1268040"
  },
  {
    "text": "extension to the kubernetes scheduler that helps it schedule pods properly correct that's right for stateful",
    "start": "1268040",
    "end": "1275000"
  },
  {
    "text": "applications yeah how it start being managed while updating the kubernetes",
    "start": "1275000",
    "end": "1281300"
  },
  {
    "text": "cluster in terms of workload availability so start will not okay I'm",
    "start": "1281300",
    "end": "1290030"
  },
  {
    "text": "not sure what what that question means but if you upgrade your kubernetes cluster that will not affect any of this",
    "start": "1290030",
    "end": "1298970"
  },
  {
    "text": "right so it all depends on how kubernetes actually doing the update right so as long as pods don't get",
    "start": "1298970",
    "end": "1304010"
  },
  {
    "text": "deleted or rescheduled there will not be any impact as such and since stock is at",
    "start": "1304010",
    "end": "1310400"
  },
  {
    "text": "stock itself is running in a three replicas deployment if the node well the",
    "start": "1310400",
    "end": "1316670"
  },
  {
    "text": "leader is running goes down you'll still be able to share your pod since two of their other replicas of stock will still",
    "start": "1316670",
    "end": "1322070"
  },
  {
    "text": "be out okay when we have quite a few questions coming in do you want me to keep ants",
    "start": "1322070",
    "end": "1327350"
  },
  {
    "text": "mm or III have quite a few slides to cover so maybe we can move them out and",
    "start": "1327350",
    "end": "1334640"
  },
  {
    "text": "maybe you have questions about some of the other features as we move yeah I'm wondering if some of the earlier slides",
    "start": "1334640",
    "end": "1340010"
  },
  {
    "text": "might answer some of the questions coming in so we're gonna proceed for a little bit keep the questions coming in",
    "start": "1340010",
    "end": "1345020"
  },
  {
    "text": "and we'll get to them in a couple more slides here okay so I'm gonna move on to",
    "start": "1345020",
    "end": "1350990"
  },
  {
    "text": "basically the next feature of stock so one of the challenges with running",
    "start": "1350990",
    "end": "1356270"
  },
  {
    "start": "1351000",
    "end": "1351000"
  },
  {
    "text": "stateful applications on kubernetes is that there is no current there is no way",
    "start": "1356270",
    "end": "1361280"
  },
  {
    "text": "right now for communities to monitor what this what the health of your storage solution is because each storage",
    "start": "1361280",
    "end": "1368300"
  },
  {
    "text": "solution actually behaves very differently and you don't want to add all of that logic into the kubernetes core right so and all is good when",
    "start": "1368300",
    "end": "1376580"
  },
  {
    "text": "everything is online right so you won't really see any issues but dealing with",
    "start": "1376580",
    "end": "1382430"
  },
  {
    "text": "failures with dealing with failures is actually difficult especially when the state involved because what if your",
    "start": "1382430",
    "end": "1388520"
  },
  {
    "text": "storage naivet goes offline night and this could happen for a couple of reasons right storage degrades as you",
    "start": "1388520",
    "end": "1394340"
  },
  {
    "text": "use it right so basically what if what if your discs go bad and you're not able to perform IO on the disk where the part",
    "start": "1394340",
    "end": "1401150"
  },
  {
    "text": "on the node where the part is actually scheduled also since since the storage solution is actually a soft is a",
    "start": "1401150",
    "end": "1407660"
  },
  {
    "text": "separate component that might actually fail or be crash in light and if you have part said you'll on the node where",
    "start": "1407660",
    "end": "1413090"
  },
  {
    "text": "this is happening what happens in that case so what typically happens for",
    "start": "1413090",
    "end": "1420440"
  },
  {
    "text": "storage solutions or for software-defined storage solutions is communities one really know if if",
    "start": "1420440",
    "end": "1427190"
  },
  {
    "text": "storage has gone bad on that node right so to us kubernetes just monitors the",
    "start": "1427190",
    "end": "1432440"
  },
  {
    "text": "the health of cubelet and some other system level parameters to make sure that there is enough CPU",
    "start": "1432440",
    "end": "1437870"
  },
  {
    "text": "memory and things are not basically going out of whack but as long as cubelet is healthy and all the other",
    "start": "1437870",
    "end": "1443840"
  },
  {
    "text": "resources are available if storage on you on one of your nodes goes down community is not gonna know and it's not",
    "start": "1443840",
    "end": "1449360"
  },
  {
    "text": "going to basically shut you apart so that it basically spins up on some other node what will happen in that case is",
    "start": "1449360",
    "end": "1455120"
  },
  {
    "text": "your pods will actually get stuck and depending on your storage solution either it will not be able to serve iOS",
    "start": "1455120",
    "end": "1460880"
  },
  {
    "text": "or go into a degraded mode and this and and and for some applications even health",
    "start": "1460880",
    "end": "1468080"
  },
  {
    "text": "check doesn't really help because in scenarios where the app is actually up",
    "start": "1468080",
    "end": "1474560"
  },
  {
    "text": "and it's not able to talk to the story solution the health the health check actually returns fine too so what's",
    "start": "1474560",
    "end": "1480500"
  },
  {
    "text": "gonna end up happening is your your stateful application is not able to make any progress but kubernetes is also not",
    "start": "1480500",
    "end": "1487160"
  },
  {
    "text": "able to reschedule the part because it doesn't know that something is actually stuck in the shell in the application",
    "start": "1487160",
    "end": "1493390"
  },
  {
    "text": "and for stateful apps of a stateful sets actually there is there there is there",
    "start": "1493390",
    "end": "1499670"
  },
  {
    "text": "are there are a couple of more issues because stateful sets actually behave differently than deployments stateful",
    "start": "1499670",
    "end": "1506090"
  },
  {
    "text": "such actually if something goes wrong one note it does not automatically reschedule parts because it's got a",
    "start": "1506090",
    "end": "1513410"
  },
  {
    "text": "stickiness towards towards a node so in that case even if something does go wrong for example if cubelet actually",
    "start": "1513410",
    "end": "1519920"
  },
  {
    "text": "crashes on one of your nodes it will not actually shut you in the pod onto another node so so what ends up",
    "start": "1519920",
    "end": "1526820"
  },
  {
    "text": "happening in these scenarios is you actually end up having to manually intervene and reschedule your parts so",
    "start": "1526820",
    "end": "1533090"
  },
  {
    "text": "that your application is is up and running so what start does is it actually",
    "start": "1533090",
    "end": "1539180"
  },
  {
    "text": "monitors the health of storage on all the nodes so periodically it basically makes an API call to the storage",
    "start": "1539180",
    "end": "1545510"
  },
  {
    "text": "solution and checks where the storages are healthy on all the nodes and if it actually goes offline it actually lists",
    "start": "1545510",
    "end": "1552020"
  },
  {
    "text": "out all the pods that are using storage from from that storage solution and then",
    "start": "1552020",
    "end": "1558590"
  },
  {
    "text": "G shields the pods so the advantage of this is for example if you are using a",
    "start": "1558590",
    "end": "1563840"
  },
  {
    "text": "storage solution that is actually doing block level replication across nodes even if even if the storage solution",
    "start": "1563840",
    "end": "1570590"
  },
  {
    "text": "goes down on one node it will automatically figure out that storage is down on this node I have pods running on",
    "start": "1570590",
    "end": "1576620"
  },
  {
    "text": "this node using the storage solution which will likely get stuck at this point so it will delete the pod and make",
    "start": "1576620",
    "end": "1581960"
  },
  {
    "text": "which will basically shuttle the pod onto one of the other nodes so your applications are then basically are",
    "start": "1581960",
    "end": "1588320"
  },
  {
    "text": "available right away Inc in imminent case where things are crashing",
    "start": "1588320",
    "end": "1593680"
  },
  {
    "text": "without this what would happen is pod would either get stuck in pending stains since they're not able to access stories",
    "start": "1593680",
    "end": "1600010"
  },
  {
    "text": "because they won't be able to either right to the right to do this or amount amount the volumes like I mentioned for",
    "start": "1600010",
    "end": "1608890"
  },
  {
    "text": "stateful sets this also deals with scenarios where cubelet actually is offline unalloyed if you don't have this",
    "start": "1608890",
    "end": "1616150"
  },
  {
    "text": "pod will actually just get stuck in basically the KU Bellinis node will go",
    "start": "1616150",
    "end": "1622240"
  },
  {
    "text": "into a node lost state and your pods will also gets stuck and they'll not yet",
    "start": "1622240",
    "end": "1627490"
  },
  {
    "text": "be scheduled because kubernetes will still be waiting for cubelet to come back upon that node so this is this how",
    "start": "1627490",
    "end": "1636240"
  },
  {
    "text": "it works architectural so basically every so",
    "start": "1636240",
    "end": "1642580"
  },
  {
    "text": "stock is basically pulling in this case the port work service to check if put",
    "start": "1642580",
    "end": "1647710"
  },
  {
    "text": "box is healthy on all the nodes and it is basically so so in the previous",
    "start": "1647710",
    "end": "1653680"
  },
  {
    "text": "example we had actually scheduled the pod on node two and in this case if node",
    "start": "1653680",
    "end": "1659950"
  },
  {
    "text": "two actually goes offline it will it will see that px is down on node two and",
    "start": "1659950",
    "end": "1665380"
  },
  {
    "text": "it will delete the part in that case now once the pod gets deleted the",
    "start": "1665380",
    "end": "1670930"
  },
  {
    "text": "controller basically whether it's the replica the deployment controller or the state full set controller will see that",
    "start": "1670930",
    "end": "1678510"
  },
  {
    "text": "the pod needs to get rescheduled and it will again talk to start to figure out where the replicas of the volume lie and",
    "start": "1678510",
    "end": "1684820"
  },
  {
    "text": "try to try to schedule the pod on one of the replicas now as in the previous example since both n1 and n3 just have",
    "start": "1684820",
    "end": "1692800"
  },
  {
    "text": "one replicas of the volumes it will basically start the ball basically start",
    "start": "1692800",
    "end": "1698290"
  },
  {
    "text": "the pod on either n1 or in three since both of them are going to remotely access one of the volumes so in this",
    "start": "1698290",
    "end": "1704590"
  },
  {
    "text": "case the pod got started on node n1 and since v2 is not available locally it's",
    "start": "1704590",
    "end": "1711070"
  },
  {
    "text": "basically doing a remote mount of the volume and using that for the part yeah",
    "start": "1711070",
    "end": "1716890"
  },
  {
    "text": "there's a type of year this actually starts it on n1 and not on into",
    "start": "1716890",
    "end": "1722250"
  },
  {
    "text": "okay I'm gonna move on to the disaster recovery part just I wonder we should",
    "start": "1723809",
    "end": "1731909"
  },
  {
    "text": "jump into a couple of questions is that okay yeah sure yeah let's do I don't know if I'm saying your name correctly",
    "start": "1731909",
    "end": "1736950"
  },
  {
    "text": "but why he he has a couple does this work for kubernetes on VMs or bare-metal or both or does it even matter um it",
    "start": "1736950",
    "end": "1744989"
  },
  {
    "text": "works on any kubernetes deployments so it'll work in the cloud will work on",
    "start": "1744989",
    "end": "1749999"
  },
  {
    "text": "Prem will work in VMs he'll work for any kubernetes district so it's it's all",
    "start": "1749999",
    "end": "1757559"
  },
  {
    "text": "independent of that yeah okay and when he has another question does Robyn systems address the similar use case as",
    "start": "1757559",
    "end": "1768330"
  },
  {
    "text": "far as I'm aware they do not okay yeah and the third question how does scale",
    "start": "1768330",
    "end": "1774929"
  },
  {
    "text": "out handle whis torque scale out of",
    "start": "1774929",
    "end": "1780950"
  },
  {
    "text": "parts supports again so again stock is not involved in scaling out any of the",
    "start": "1780950",
    "end": "1786809"
  },
  {
    "text": "applications or Biscay are doing basically scaling out in case you need",
    "start": "1786809",
    "end": "1792169"
  },
  {
    "text": "basically auto scaling so if your application actually gets auto scale for example if you're using any kind of",
    "start": "1792169",
    "end": "1798960"
  },
  {
    "text": "water scaler and it decides that it needs to spin up more parts at that point start will come into play when the",
    "start": "1798960",
    "end": "1804299"
  },
  {
    "text": "pods are getting scheduled before that stock does not do auto scaling of either",
    "start": "1804299",
    "end": "1809639"
  },
  {
    "text": "pods or your storage okay um from our",
    "start": "1809639",
    "end": "1815159"
  },
  {
    "text": "anonymous attendee if storage is external not in the kubernetes nodes would be the role is stork will there be",
    "start": "1815159",
    "end": "1821549"
  },
  {
    "text": "any benefit um yeah like I mentioned so you can always use stock in cases where",
    "start": "1821549",
    "end": "1827369"
  },
  {
    "text": "you don't you want to make sure that your your network attached storage this",
    "start": "1827369",
    "end": "1832919"
  },
  {
    "text": "are spread out so what you can do is for example if you have five nodes you want to make sure that and your and you",
    "start": "1832919",
    "end": "1839279"
  },
  {
    "text": "started like 15 pods you know you want to make sure that all your 15 parts don't get scheduled on one of the nodes right so you could basically do things",
    "start": "1839279",
    "end": "1845489"
  },
  {
    "text": "like monitor on how many how many amount how many disks have been mounted on one of the nodes and then basically load",
    "start": "1845489",
    "end": "1852359"
  },
  {
    "text": "balance at the mounts on different nodes and I think you should go back a slide for Anthony's question what if n 2 and",
    "start": "1852359",
    "end": "1859669"
  },
  {
    "text": "then 3 nodes go down don't we go by 3 node isn't there a three node approach",
    "start": "1859669",
    "end": "1866360"
  },
  {
    "text": "to avoid data loss uh yes so in this case in this scenario obviously if you",
    "start": "1866360",
    "end": "1872210"
  },
  {
    "text": "have any kind of storage solution you want to make sure that your you have a quorum of nodes up so if in this",
    "start": "1872210",
    "end": "1878509"
  },
  {
    "text": "scenario of two if you of your nodes go down you are you you will basically lose quorum for your storage solution so at",
    "start": "1878509",
    "end": "1884570"
  },
  {
    "text": "that point you will not be able to share you anymore parts ok is this just for",
    "start": "1884570",
    "end": "1890809"
  },
  {
    "text": "pot alignment two volumes for performance yes okay what storage",
    "start": "1890809",
    "end": "1897769"
  },
  {
    "text": "back-end historic already compatible with so right now it's compatible with",
    "start": "1897769",
    "end": "1902809"
  },
  {
    "text": "port works but like I mentioned everything is abstracted out so as long as you implement and into the interface",
    "start": "1902809",
    "end": "1910789"
  },
  {
    "text": "you should be able to use this with any storage solution okay yeah anonymous has",
    "start": "1910789",
    "end": "1916009"
  },
  {
    "text": "a lot of questions you should connect connect with Dinesh when you're done but here's another one if if I have a",
    "start": "1916009",
    "end": "1922340"
  },
  {
    "text": "cluster file system and the PVR created on the cluster notes and not the",
    "start": "1922340",
    "end": "1927590"
  },
  {
    "text": "application notes how how will work or schedule it so you can you be back yeah",
    "start": "1927590",
    "end": "1935889"
  },
  {
    "text": "he's talking about a cluster file system and the PV are created on the cluster",
    "start": "1935889",
    "end": "1942620"
  },
  {
    "text": "nodes and not the application nodes okay how will Stork work and how old schedule",
    "start": "1942620",
    "end": "1949179"
  },
  {
    "text": "so it'll still work in the sense so you might so the other advantage over here",
    "start": "1949179",
    "end": "1954740"
  },
  {
    "text": "is so if you're saying that your cluster nodes are running only storage and they're not doing any you don't want to",
    "start": "1954740",
    "end": "1961850"
  },
  {
    "text": "schedule any pods over that what you can do is if you actually place labels and",
    "start": "1961850",
    "end": "1967129"
  },
  {
    "text": "have information on which slack they are located so what you what start will do is it'll actually prefer nodes it'll",
    "start": "1967129",
    "end": "1974299"
  },
  {
    "text": "actually prefer nodes on the same rack where your cluster FS nodes are so it'll actually provide you better performance",
    "start": "1974299",
    "end": "1979639"
  },
  {
    "text": "in that case so you are actually reducing bandwidth network usage even in that case because you're not sending",
    "start": "1979639",
    "end": "1985399"
  },
  {
    "text": "data across racks so so if you implement a cluster",
    "start": "1985399",
    "end": "1990530"
  },
  {
    "text": "driver over you will be able to take advantage of things like that in your data center okay we have two more here",
    "start": "1990530",
    "end": "1997580"
  },
  {
    "text": "how does stork work when horizontal scaling is enabled since the volumes predefined on certain notes if",
    "start": "1997580",
    "end": "2005890"
  },
  {
    "text": "horizontal scaling is enabled what will happen is actually new pods will be created and new PVCs will also be",
    "start": "2005890",
    "end": "2011110"
  },
  {
    "text": "created so in that case basically things will get spread out so",
    "start": "2011110",
    "end": "2017520"
  },
  {
    "text": "let me talk about how Botox will work in case of port works if you horizontally scale your applications your it is",
    "start": "2017520",
    "end": "2025570"
  },
  {
    "text": "basically going to create new PVCs also right and the PVCs themselves are going to get spread out so by that",
    "start": "2025570",
    "end": "2031870"
  },
  {
    "text": "considerations your your pods also going to get spread out to make sure that your cluster is even used okay do you have to",
    "start": "2031870",
    "end": "2040540"
  },
  {
    "text": "feed the storage and host configuration the store core does a scan and determine that once deployed so you don't need to",
    "start": "2040540",
    "end": "2048940"
  },
  {
    "text": "feed anything the only thing that you need to do here is set the scheduler name is stuck in your deployment or",
    "start": "2048940",
    "end": "2055659"
  },
  {
    "text": "stateful set so there's a field in the spec you just need to set that to stock and then everything happens",
    "start": "2055660",
    "end": "2060879"
  },
  {
    "text": "automatically there are a couple of specs that you have to apply to actually install stock but once that is done all",
    "start": "2060880",
    "end": "2067990"
  },
  {
    "text": "you need to do is set the scheduler name in your applications okay and I believe we're gonna start talking about dr and",
    "start": "2067990",
    "end": "2073540"
  },
  {
    "text": "snapshots and things like that correct yes yes hold on this last question here",
    "start": "2073540",
    "end": "2078730"
  },
  {
    "text": "go ahead okay cool so so what I was talking about previously was basically how you can run applications more",
    "start": "2078730",
    "end": "2086050"
  },
  {
    "text": "efficient stateful applications more efficient you're on kubernetes as well as how you can manage and basically",
    "start": "2086050",
    "end": "2092530"
  },
  {
    "text": "monitor your storage solution so that your apps always running even in case of",
    "start": "2092530",
    "end": "2097840"
  },
  {
    "text": "failures so what I'm going to move to next is basically dr scenarios",
    "start": "2097840",
    "end": "2104200"
  },
  {
    "text": "so all that i talked about previously is good for your day one scenarios right",
    "start": "2104200",
    "end": "2110530"
  },
  {
    "text": "but as you move forward and you move into production of production you want to make sure that your applications are actually product protected and if things",
    "start": "2110530",
    "end": "2118120"
  },
  {
    "text": "go bad you are able to basically recover your applications yeah so so the there was enough when we",
    "start": "2118120",
    "end": "2127720"
  },
  {
    "text": "started this there was a way to basically there was a need to manage lifecycle for storage natively in",
    "start": "2127720",
    "end": "2133540"
  },
  {
    "text": "communities so the storage solutions provide ways to basically take backups but there's no native way of doing this",
    "start": "2133540",
    "end": "2139660"
  },
  {
    "text": "in the sense you can there was no way of basically applying specs in kubernetes to take snapshots or take application",
    "start": "2139660",
    "end": "2147670"
  },
  {
    "text": "backups or do migrations so that is what stock has started building on - so when",
    "start": "2147670",
    "end": "2155710"
  },
  {
    "text": "it was started there was no native support for snapshotting pvcs right now there is there is a CSS snapshot of but",
    "start": "2155710",
    "end": "2162849"
  },
  {
    "text": "again if you are using either flex volumes or any of the entry drivers there is no real way of taking snapshots",
    "start": "2162849",
    "end": "2169630"
  },
  {
    "text": "of PVCs so what we did in start was we will seek the added support for this and",
    "start": "2169630",
    "end": "2174910"
  },
  {
    "text": "this was based on a project that was started in the kubernetes incubator actually there was a there was",
    "start": "2174910",
    "end": "2180910"
  },
  {
    "text": "something in the external external external volume external storage project",
    "start": "2180910",
    "end": "2190000"
  },
  {
    "text": "in the kubernetes incubator that had a snapshot so we basically use that to build snapshotting capabilities in stock",
    "start": "2190000",
    "end": "2199140"
  },
  {
    "text": "so this also works over a group of Phoebe C's so for example if you have a distributed app like a Sonora Kafka",
    "start": "2199140",
    "end": "2205630"
  },
  {
    "text": "MongoDB you can actually give label selectors to select what group of labels you a group of PVCs you want to snapshot",
    "start": "2205630",
    "end": "2212559"
  },
  {
    "text": "together and this actually takes application consistent snapshots I will talk about how that is done at a later",
    "start": "2212559",
    "end": "2218740"
  },
  {
    "text": "point and so right now you can basically take snapshots snapshots of your PVCs",
    "start": "2218740",
    "end": "2227190"
  },
  {
    "text": "but in 2.3 dot over ssin which is releasing end of July we will actually",
    "start": "2227190",
    "end": "2232329"
  },
  {
    "text": "add support to backup your data as well as your application resources to an",
    "start": "2232329",
    "end": "2238540"
  },
  {
    "text": "object store so what would happen in that case is you would specify a backup location which could be any SD compliant",
    "start": "2238540",
    "end": "2245109"
  },
  {
    "text": "object store as your blog or good cloud storage and what that will do is it'll basically back up all your data as well",
    "start": "2245109",
    "end": "2251680"
  },
  {
    "text": "as all your resources to that object store now in case your your goes down you just have to spin up a new",
    "start": "2251680",
    "end": "2258280"
  },
  {
    "text": "cluster create a backup location that is pointing to the same object stored and",
    "start": "2258280",
    "end": "2264339"
  },
  {
    "text": "you'll be able to restore your your application including your data as well",
    "start": "2264339",
    "end": "2269799"
  },
  {
    "text": "as the resources to to a new cluster yeah like I mentioned we support any of",
    "start": "2269799",
    "end": "2276280"
  },
  {
    "text": "these three object stores we have a",
    "start": "2276280",
    "end": "2283569"
  },
  {
    "start": "2283000",
    "end": "2283000"
  },
  {
    "text": "couple couple different questions what are the parallels with Valero for snapshots do they complement so the so a",
    "start": "2283569",
    "end": "2294520"
  },
  {
    "text": "couple of things right so snapshots themselves over here they take snapshots of only the PVCs so the application the",
    "start": "2294520",
    "end": "2302920"
  },
  {
    "text": "new feature that we are talking about in 2.3 dot over which is basically backing up your data and the resources is similar to bolero but what we've kind of",
    "start": "2302920",
    "end": "2312099"
  },
  {
    "text": "paid more attention to over here is to give more access to users so for example",
    "start": "2312099",
    "end": "2318069"
  },
  {
    "text": "in Valero you can only it's more geared towards admins where you can only create backups and resource from one namespace",
    "start": "2318069",
    "end": "2325680"
  },
  {
    "text": "in start what we've tried to do is give more allow the users of the apps to have",
    "start": "2325680",
    "end": "2332200"
  },
  {
    "text": "more control actually so here the way it works is as long as you have access to your namespace you will be able to",
    "start": "2332200",
    "end": "2339520"
  },
  {
    "text": "basically take backups and restores of applications over there you don't really have to talk to a storage admin who's",
    "start": "2339520",
    "end": "2344980"
  },
  {
    "text": "managing your entire cluster to take backups and stuff like that so it is similar but I think we we kind",
    "start": "2344980",
    "end": "2353770"
  },
  {
    "text": "of behave a little more differently to give more more power to users in this case okay and David has a question he",
    "start": "2353770",
    "end": "2361809"
  },
  {
    "text": "thinks this might be outside of historic scope but can you set priorities based on disk type for example are you storage",
    "start": "2361809",
    "end": "2370450"
  },
  {
    "text": "based on SSD HDD Layton sees do the faster disk type IO hence you can",
    "start": "2370450",
    "end": "2375790"
  },
  {
    "text": "prioritize pods - no - because it's all SSD yeah that's right so this is not not",
    "start": "2375790",
    "end": "2383500"
  },
  {
    "text": "a part of start because when start comes into play the volumes have already been provisioned actually so this is more",
    "start": "2383500",
    "end": "2389630"
  },
  {
    "text": "more a thing of the storage solution for example I can talk about how this works in port works in port works if you have",
    "start": "2389630",
    "end": "2396470"
  },
  {
    "text": "different types of this they basically get put into different pools and when you're actually creating PVCs you can",
    "start": "2396470",
    "end": "2402740"
  },
  {
    "text": "actually specify which pool you want to create the PVC air so once that is done once the PVC is created in the in the",
    "start": "2402740",
    "end": "2409010"
  },
  {
    "text": "appropriate pool at that point start will just schedule up schedule the pod on to the node where the pool exists so",
    "start": "2409010",
    "end": "2415820"
  },
  {
    "text": "that that decision of where the data which pools or which type of disks the",
    "start": "2415820",
    "end": "2421760"
  },
  {
    "text": "storage gets allocated on it happens even before stock comes into play okay",
    "start": "2421760",
    "end": "2427070"
  },
  {
    "text": "in a recovery related question how does that work in the case of geo redundancy",
    "start": "2427070",
    "end": "2432800"
  },
  {
    "text": "use cases so I'm not sure so there's",
    "start": "2432800",
    "end": "2441620"
  },
  {
    "text": "there are a couple of CS and are using in case of geo redundancy also right so the way it works I mean you could use",
    "start": "2441620",
    "end": "2448970"
  },
  {
    "text": "this in multiple ways right so for example if you're doing this in the cloud and you have a cluster in AWS say",
    "start": "2448970",
    "end": "2454580"
  },
  {
    "text": "US East one you could basically have all your apps running over there and you basically take a regular backups into s3",
    "start": "2454580",
    "end": "2461600"
  },
  {
    "text": "now once if there is an outage of the entire US East region what you could do",
    "start": "2461600",
    "end": "2468380"
  },
  {
    "text": "is you could spin up your entire cluster in u.s. West and then point it to the same same s3 location and basically do a",
    "start": "2468380",
    "end": "2477170"
  },
  {
    "text": "restore of your entire app over so that is one of the scenarios that you can use application basically backups oh yeah so",
    "start": "2477170",
    "end": "2484730"
  },
  {
    "text": "I'm going to talk about how you can actually do migrations to keep your to keep two clusters across regions in sync",
    "start": "2484730",
    "end": "2490670"
  },
  {
    "text": "to it that's that's in the next slides okay go ahead okay",
    "start": "2490670",
    "end": "2497180"
  },
  {
    "start": "2497000",
    "end": "2497000"
  },
  {
    "text": "so the next kind of disaster recovery is a feature that we provide in storage",
    "start": "2497180",
    "end": "2502520"
  },
  {
    "text": "that we actually have support for multi cloud or multi cluster migration so what you can do in this scenario is you and I",
    "start": "2502520",
    "end": "2509180"
  },
  {
    "text": "actually pay two clusters and then migrate all your applications between two clusters so there are a couple of",
    "start": "2509180",
    "end": "2515900"
  },
  {
    "text": "for you our use cases where you would be where you would use this one of the use cases is where you want to augment your",
    "start": "2515900",
    "end": "2522410"
  },
  {
    "text": "current class so you might have a cluster with some some amount of resources CPU memory",
    "start": "2522410",
    "end": "2528049"
  },
  {
    "text": "storage and you're running a couple of apps over there now as you start onboarding more users you might realize",
    "start": "2528049",
    "end": "2534109"
  },
  {
    "text": "that some of your apps are taking up a lot more resources and you don't have enough resources on the cluster to",
    "start": "2534109",
    "end": "2540589"
  },
  {
    "text": "basically start up more apps and at that point what you might want to do is you might want to migrate a couple of your",
    "start": "2540589",
    "end": "2545779"
  },
  {
    "text": "apps onto another cluster now this is very simple to do with state stateless",
    "start": "2545779",
    "end": "2551059"
  },
  {
    "text": "apps right you basically scale down your scale down your apps on your source cluster and just spin it up on the destination cluster and everything's",
    "start": "2551059",
    "end": "2557210"
  },
  {
    "text": "good but with data there is there's some kind of gravity involved right how do you move your graph how do you move your",
    "start": "2557210",
    "end": "2562759"
  },
  {
    "text": "data as well as your resources together so that that is what that is the support that we've actually added installed the",
    "start": "2562759",
    "end": "2569900"
  },
  {
    "text": "other scenario where you would want to use migration is where you want to do some kind of blue-green testing so for",
    "start": "2569900",
    "end": "2576619"
  },
  {
    "text": "example if your storage solution has a new version release you want to make sure that your apps and everything works",
    "start": "2576619",
    "end": "2581720"
  },
  {
    "text": "fine before you actually upgrade so in that case you would basically have cluster 1 running with v1 you would and",
    "start": "2581720",
    "end": "2588499"
  },
  {
    "text": "a couple of apps running on the cluster 1 right so then you spin up cluster 2 you would install a v2 of the storage",
    "start": "2588499",
    "end": "2594529"
  },
  {
    "text": "solution and then you will migrate all your apps and then make sure everything is fine and then on your production cluster basically upgrade everything",
    "start": "2594529",
    "end": "2601099"
  },
  {
    "text": "once you once you're confident that everything is running fine the third scenario is in cases of dev tests so you",
    "start": "2601099",
    "end": "2608390"
  },
  {
    "text": "might actually hit a bug in production that can be reproduced only with data that's running in production right now",
    "start": "2608390",
    "end": "2614599"
  },
  {
    "text": "before fixing it you might want to run some tests to make sure that the bug is actually fixed so how do you do that so",
    "start": "2614599",
    "end": "2622599"
  },
  {
    "text": "that is one of the use cases over here where you can basically migrate that",
    "start": "2622599",
    "end": "2628130"
  },
  {
    "text": "particular app from your production cluster into a separate cluster run a couple of tests with the updated app",
    "start": "2628130",
    "end": "2633259"
  },
  {
    "text": "make sure everything is fine and then update your production cluster so how",
    "start": "2633259",
    "end": "2640309"
  },
  {
    "text": "does this work so first what you do is you basically you can pair two or more clusters so what this does is it",
    "start": "2640309",
    "end": "2646430"
  },
  {
    "text": "basically pairs the storage solutions on both the clusters as well as the communities are orchestrate on both the",
    "start": "2646430",
    "end": "2653450"
  },
  {
    "text": "clusters and this can actually be any type of clusters right so it can be on-prem it",
    "start": "2653450",
    "end": "2659060"
  },
  {
    "text": "can be running in VMs it can be any kind of - cloud like gke or aks and it can",
    "start": "2659060",
    "end": "2664850"
  },
  {
    "text": "even be any other distribution of communities like cosa p or p ksi any of",
    "start": "2664850",
    "end": "2670430"
  },
  {
    "text": "those distributions yeah",
    "start": "2670430",
    "end": "2675560"
  },
  {
    "text": "so once the so I'm actually gonna show a demo of this if we have time so once once the two clusters are basically paid",
    "start": "2675560",
    "end": "2681740"
  },
  {
    "text": "then you can start migrating applications across this so all you need to do is specify the namespace that you",
    "start": "2681740",
    "end": "2687470"
  },
  {
    "text": "want to migrate and you can actually specify label selectors in case you have multiple apps running in the namespace you can select only a few of the apps",
    "start": "2687470",
    "end": "2694130"
  },
  {
    "text": "that you actually want to migrate between between two clusters and once you apply those specs start will then",
    "start": "2694130",
    "end": "2701480"
  },
  {
    "text": "talk to the storage solution to migrate your data between the two clusters and then it'll basically migrate all your",
    "start": "2701480",
    "end": "2707480"
  },
  {
    "text": "resources between the two clusters so these resources include stuff like your deployment stateful sites config Maps",
    "start": "2707480",
    "end": "2714070"
  },
  {
    "text": "secrets and we also have support for some of the OSI PRC IDs like templates",
    "start": "2714070",
    "end": "2719690"
  },
  {
    "text": "routes and image streams so what this does is it basically gives you an exact",
    "start": "2719690",
    "end": "2725540"
  },
  {
    "text": "copy of your application on another cluster no matter what cloud it is no",
    "start": "2725540",
    "end": "2731000"
  },
  {
    "text": "matter what cloud or kind of kubernetes cluster it is running ok we do have a",
    "start": "2731000",
    "end": "2738680"
  },
  {
    "start": "2738000",
    "end": "2738000"
  },
  {
    "text": "couple questions before you get to the demo sure come on is the port work storage solution appliance based do you",
    "start": "2738680",
    "end": "2745609"
  },
  {
    "text": "need to buy the hardware from port works can you just find a storage software and run it anywhere support hooks is",
    "start": "2745609",
    "end": "2751580"
  },
  {
    "text": "completely software-defined so it's there is no hardware involved so it can",
    "start": "2751580",
    "end": "2756740"
  },
  {
    "text": "basically you can install port works on any hard on any hardware you can actually so we have cloud native in the",
    "start": "2756740",
    "end": "2762200"
  },
  {
    "text": "sense you can actually install us on Prem on VMs or any of the cloud services ok and he had a following question port",
    "start": "2762200",
    "end": "2770359"
  },
  {
    "text": "works as per container only can can he provide block or sorry can they provide",
    "start": "2770359",
    "end": "2775820"
  },
  {
    "text": "block and file to the VMS so port works is built for containers but you can",
    "start": "2775820",
    "end": "2783640"
  },
  {
    "text": "obviously manually orchestrate and so",
    "start": "2783640",
    "end": "2788760"
  },
  {
    "text": "port works by itself is actually providing virtual block storage solution virtual blocks block devices so you can",
    "start": "2788760",
    "end": "2796440"
  },
  {
    "text": "obviously manually orchestrate all of that to provide volumes to your VMs as",
    "start": "2796440",
    "end": "2801750"
  },
  {
    "text": "well okay kids you work with storage OS our stock does not work with storage or",
    "start": "2801750",
    "end": "2808680"
  },
  {
    "text": "us right now so like I mentioned both boxes don't need our that's implemented but you can implement a driver for any",
    "start": "2808680",
    "end": "2814230"
  },
  {
    "text": "storage solution okay new I'm not sure I'm got your question how does this help",
    "start": "2814230",
    "end": "2820650"
  },
  {
    "text": "in disaster recovery warm hot site scheduling historic leaders on a warm",
    "start": "2820650",
    "end": "2826710"
  },
  {
    "text": "hot site yeah I don't understand how I",
    "start": "2826710",
    "end": "2832230"
  },
  {
    "text": "understand the question either but so there are couple yeah so we'll go to",
    "start": "2832230",
    "end": "2838050"
  },
  {
    "text": "Anthony's question how does the data migration work from a non start deployment to a snort deployment so the",
    "start": "2838050",
    "end": "2845850"
  },
  {
    "text": "destination doesn't require stock to be running at all it's just the source cluster so everything is basically being",
    "start": "2845850",
    "end": "2850950"
  },
  {
    "text": "orchestrated from the source cluster so it it basically handles all the migration it basically migrates all your",
    "start": "2850950",
    "end": "2856230"
  },
  {
    "text": "data as well as the resources onto the destination so you don't have need to have start running on the other cluster",
    "start": "2856230",
    "end": "2862530"
  },
  {
    "text": "but obviously if you want your applications to be shared more efficiently you would have stopped anymore excuse me on both the clusters",
    "start": "2862530",
    "end": "2869280"
  },
  {
    "text": "okay so we'll wait for nearly given more clarity to this question if you want to go ahead and go to the demo sure so let",
    "start": "2869280",
    "end": "2878310"
  },
  {
    "text": "me just walk through two more slides really quickly so all of this stuff that you're doing for disaster recovery you",
    "start": "2878310",
    "end": "2884070"
  },
  {
    "text": "obviously don't want to do them manually you want to shadow them on a periodic basis so for example if you are doing",
    "start": "2884070",
    "end": "2889500"
  },
  {
    "text": "migrations or snapshot you want to make sure that they are either done daily when you have some downtime in the",
    "start": "2889500",
    "end": "2894720"
  },
  {
    "text": "cluster or weekly or monthly so we have support for that too we basically have CRE is defined for schedules for each of",
    "start": "2894720",
    "end": "2901710"
  },
  {
    "text": "the different types of disaster recovery so that you can do and the other thing is if you are actually doing any",
    "start": "2901710",
    "end": "2907140"
  },
  {
    "start": "2905000",
    "end": "2905000"
  },
  {
    "text": "performing any of these operations you want to make sure that they are basically application consistent in the sense if you're running distributed apps",
    "start": "2907140",
    "end": "2914070"
  },
  {
    "text": "you wanna make sure that your databases are unlocked or all your for for example",
    "start": "2914070",
    "end": "2919530"
  },
  {
    "text": "for your Cassandra or application you wanna make sure all your data is actually flush to notice before you perform these operations so that you",
    "start": "2919530",
    "end": "2926630"
  },
  {
    "text": "have the latest data available for your when you have a disaster so we actually",
    "start": "2926630",
    "end": "2931640"
  },
  {
    "text": "have support for running rules before and after your before and after the",
    "start": "2931640",
    "end": "2936770"
  },
  {
    "text": "applications for example for MySQL you can actually lock a table flush all your data lock a table and",
    "start": "2936770",
    "end": "2942500"
  },
  {
    "text": "then take a snapshot and then unlock it and for Cassandra for example you can flush all the data from the memory before taking a snapshot and we also",
    "start": "2942500",
    "end": "2950300"
  },
  {
    "text": "have support for so we also have a tool called stock curl to easily manage all the custom resources that we've defined",
    "start": "2950300",
    "end": "2955940"
  },
  {
    "text": "so for example you can actually just create a snapshot simply by just running this command so you can do stock curl",
    "start": "2955940",
    "end": "2962090"
  },
  {
    "text": "create snapshot the name space and the name of the PVC and the name of SAP sure and it creates snapshot and if you do a",
    "start": "2962090",
    "end": "2967850"
  },
  {
    "text": "get on the snapshot you'll basically see the snapshot that has been created and similarly for the cluster pear and",
    "start": "2967850",
    "end": "2973190"
  },
  {
    "text": "migration you you basically get a lot more information if you use stocker so I",
    "start": "2973190",
    "end": "2978619"
  },
  {
    "text": "guess we have eight when it's left I'm just going to switch to my terminal right now so that I can show you a quick",
    "start": "2978619",
    "end": "2986180"
  },
  {
    "text": "demo of the migration so in the interest",
    "start": "2986180",
    "end": "2991700"
  },
  {
    "text": "of time I have actually spun up two clusters and I've so this is this is my",
    "start": "2991700",
    "end": "3000369"
  },
  {
    "text": "destination cluster and this is my this is my source cluster so on a source",
    "start": "3000369",
    "end": "3006730"
  },
  {
    "text": "cluster I basically have a MySQL app running through a deployment as well as",
    "start": "3006730",
    "end": "3011740"
  },
  {
    "text": "a service rate and if I do a get PVC in this you'll see I also have a PVC",
    "start": "3011740",
    "end": "3018820"
  },
  {
    "text": "created that is being used by by my sequel now I've already created a",
    "start": "3018820",
    "end": "3024520"
  },
  {
    "text": "cluster pair between these two nodes so if I do a cube stock I'll get cluster in",
    "start": "3024520",
    "end": "3030130"
  },
  {
    "text": "the MySQL namespace you'll see that I was a cluster period created and storage",
    "start": "3030130",
    "end": "3035290"
  },
  {
    "text": "is ready as well as the scheduler is ready now to migrate so you can use",
    "start": "3035290",
    "end": "3041859"
  },
  {
    "text": "stock cartel to actually spin up the migration to but I'm just going to show you what the CRD looks or the custom",
    "start": "3041859",
    "end": "3048190"
  },
  {
    "text": "resource for this actually looks like so this is the cluster pair that we actually created so that is the name of",
    "start": "3048190",
    "end": "3054250"
  },
  {
    "text": "the cluster that we created is pointing to the other cluster so we are actually telling it",
    "start": "3054250",
    "end": "3059740"
  },
  {
    "text": "that we want to include the resources when we do the migration and have actually initially said this to false",
    "start": "3059740",
    "end": "3066430"
  },
  {
    "text": "but I'm just going to say that once the applications actually spun up on the destination cluster I also want to start",
    "start": "3066430",
    "end": "3071530"
  },
  {
    "text": "the applications now if you said this to false what is going to do is it's actually going to migrate the",
    "start": "3071530",
    "end": "3076960"
  },
  {
    "text": "deployments but it's going to spin down the replicas to zero because I don't",
    "start": "3076960",
    "end": "3082000"
  },
  {
    "text": "know if that that was the question that the person previously asked but you might have scenarios where you actually",
    "start": "3082000",
    "end": "3087220"
  },
  {
    "text": "have active and standby clusters right and you won the East application to be running only on the active cluster so in",
    "start": "3087220",
    "end": "3094270"
  },
  {
    "text": "that case you will actually set this to false that would basically migrate all your data migrate all the resources but",
    "start": "3094270",
    "end": "3099370"
  },
  {
    "text": "spin down the applications and then you can basically in case of a disaster go to your standby cluster and spin up the",
    "start": "3099370",
    "end": "3105820"
  },
  {
    "text": "applications so here I'm actually let me just set this to false so I'm going to",
    "start": "3105820",
    "end": "3111940"
  },
  {
    "text": "say false to say that now you should migrate everything but not spin up the applications and then this basically says the list of namespaces that I want",
    "start": "3111940",
    "end": "3118570"
  },
  {
    "text": "to migrate I'm just saying that I want to migrate just the MySQL namespace so I",
    "start": "3118570",
    "end": "3124270"
  },
  {
    "text": "am just going to apply that spec and I'm",
    "start": "3124270",
    "end": "3130120"
  },
  {
    "text": "then gonna watch I'm just gonna watch",
    "start": "3130120",
    "end": "3135820"
  },
  {
    "text": "the status for the migration so what you want to see happen here is basically it",
    "start": "3135820",
    "end": "3141100"
  },
  {
    "text": "goes through a different state you know different stages or doing the migration so first it's going to migrate all the",
    "start": "3141100",
    "end": "3146200"
  },
  {
    "text": "volumes and then it's going to migrate the application resources so once this",
    "start": "3146200",
    "end": "3151240"
  },
  {
    "text": "kicks in you will see that the stage actually goes into volume and if you see over you're on the destination there's",
    "start": "3151240",
    "end": "3157480"
  },
  {
    "text": "nothing really present and as you can see over here the migrations for the volume actually started up and in a few",
    "start": "3157480",
    "end": "3166930"
  },
  {
    "text": "seconds you're going to see that this is going to complete and then it's going to migrate all the applications application",
    "start": "3166930",
    "end": "3172240"
  },
  {
    "text": "resources so while we're waiting we do a question is there a UI for this so there",
    "start": "3172240",
    "end": "3179410"
  },
  {
    "text": "is not through stock but port works separately has a UI so there is no stark",
    "start": "3179410",
    "end": "3185020"
  },
  {
    "text": "there's no UI in the project itself start but what works as a UI called",
    "start": "3185020",
    "end": "3190930"
  },
  {
    "text": "light house where you can actually trigger it from there okay are we still",
    "start": "3190930",
    "end": "3200410"
  },
  {
    "text": "waiting um yeah see I got a couple more questions then do you have a preference oops",
    "start": "3200410",
    "end": "3206950"
  },
  {
    "text": "do you have a preference for underlying dist 80h HD SSD or flash and DME etc no",
    "start": "3206950",
    "end": "3215410"
  },
  {
    "text": "I love that depends on your application I mean for port works as long as pootabucks gets a block device it will",
    "start": "3215410",
    "end": "3222820"
  },
  {
    "text": "basically form a clustered store clustered virtual storage pool across that so you know the type of dis doesn't",
    "start": "3222820",
    "end": "3230050"
  },
  {
    "text": "really matter as long as it's an unformatted block device that's presented okay so follow-on to that does support",
    "start": "3230050",
    "end": "3235810"
  },
  {
    "text": "persistent storage again as long as it is presented as a block device and you",
    "start": "3235810",
    "end": "3243340"
  },
  {
    "text": "want to make sure that yeah as long as this perp is it's presented as a storage",
    "start": "3243340",
    "end": "3249580"
  },
  {
    "text": "as a block device virtual as a rob block device it will work yes okay yep so as",
    "start": "3249580",
    "end": "3255670"
  },
  {
    "text": "you can see oh yeah it went through so the stages went through really quickly so from volume to applications to find",
    "start": "3255670",
    "end": "3262270"
  },
  {
    "text": "out so here it basically says that it migrated one volume and eight different resources from cluster one to cluster",
    "start": "3262270",
    "end": "3268030"
  },
  {
    "text": "two and if we go to cluster two right now you will see that all the resources actually present over you",
    "start": "3268030",
    "end": "3274090"
  },
  {
    "text": "and if you look at the deployment you'll see that the replicas are actually set to zero that is because we said that we",
    "start": "3274090",
    "end": "3280390"
  },
  {
    "text": "do not want to start the applications on the motor now in case of a dr scenario",
    "start": "3280390",
    "end": "3286360"
  },
  {
    "text": "once your cluster goes down all you need to do is run stock stock curdle activate",
    "start": "3286360",
    "end": "3291400"
  },
  {
    "text": "migrations and give it the the namespace where you want to activate it and it'll",
    "start": "3291400",
    "end": "3297850"
  },
  {
    "text": "basically spin up the replicas count to what was on the source cluster the way this works is if you do a cube curl",
    "start": "3297850",
    "end": "3306690"
  },
  {
    "text": "described on the deployment you see that",
    "start": "3306690",
    "end": "3313660"
  },
  {
    "text": "stock actually put an annotation saying what the number of replicas were on the",
    "start": "3313660",
    "end": "3318670"
  },
  {
    "text": "source so this allows you to basically scale it up once you once your once you are active cluster",
    "start": "3318670",
    "end": "3326900"
  },
  {
    "text": "goes down and if you do a get all on this cluster you will see that the pod is basically up and running or and if I",
    "start": "3326900",
    "end": "3333320"
  },
  {
    "text": "do a cute cuddly get PVC in the MySQL namespace you will see that the same the",
    "start": "3333320",
    "end": "3338570"
  },
  {
    "text": "same PVC and the same P you are actually created on the destination cluster as the source custom yep I think that's",
    "start": "3338570",
    "end": "3350270"
  },
  {
    "text": "about it from me yes no questions coming in so can you go to the slide how people",
    "start": "3350270",
    "end": "3357470"
  },
  {
    "text": "can get ahold of you and I encourage of the questions that we didn't get to that you they contact you directly so to that",
    "start": "3357470",
    "end": "3365000"
  },
  {
    "text": "slide okay yeah so yeah so yes yes link",
    "start": "3365000",
    "end": "3376280"
  },
  {
    "text": "to basically the project like I mentioned we are it's an open source project on Stark and be welcome",
    "start": "3376280",
    "end": "3382550"
  },
  {
    "text": "contributions for any drivers and features there are a couple of blogs that have more technical detail about",
    "start": "3382550",
    "end": "3388840"
  },
  {
    "text": "about the features that I pointed to and my contact information is here if you",
    "start": "3388840",
    "end": "3394700"
  },
  {
    "text": "want to contact me so it's D is re NI at port force calm or my github is D is re",
    "start": "3394700",
    "end": "3402140"
  },
  {
    "text": "and I - px and you can also always care in touch with us on the port works",
    "start": "3402140",
    "end": "3407780"
  },
  {
    "text": "lecture okay well thank you with that we we are out of time there were some",
    "start": "3407780",
    "end": "3413090"
  },
  {
    "text": "questions so I encourage you to get in cut me in contact with Dean ash and thank you all for joining this",
    "start": "3413090",
    "end": "3418580"
  },
  {
    "text": "presentation will be uploaded to YouTube later today and I believe Dinesh is sending me the slides so we'll get those",
    "start": "3418580",
    "end": "3424520"
  },
  {
    "text": "on the CNC F website thank you everybody and have a fabulous day Thanks",
    "start": "3424520",
    "end": "3431890"
  }
]