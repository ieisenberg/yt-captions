[
  {
    "text": "thank you everyone for joining this session I know it's the last one I hope you had a great day here at the cube day",
    "start": "0",
    "end": "6660"
  },
  {
    "text": "Israel I know that I enjoyed most of it and talking to most of you I'm actually really excited to give in",
    "start": "6660",
    "end": "12599"
  },
  {
    "text": "the closing session with all of you today we'll be speaking about how to simplify your multi-cluster management",
    "start": "12599",
    "end": "18660"
  },
  {
    "text": "using a tool called karmada a little bit about myself my name is",
    "start": "18660",
    "end": "24660"
  },
  {
    "text": "Elian bivas I'm the architect for the platform group at apps live I work for",
    "start": "24660",
    "end": "30300"
  },
  {
    "text": "upside for almost four years now I have a real passion for technology so",
    "start": "30300",
    "end": "36360"
  },
  {
    "text": "if you look at me in LinkedIn or any other place you will see that I am a",
    "start": "36360",
    "end": "41640"
  },
  {
    "text": "self-proclaimed Tech Junkie a little bit about apply I let the",
    "start": "41640",
    "end": "47280"
  },
  {
    "text": "numbers speak for themselves but I'll read out some of them uh after is the market leader for for",
    "start": "47280",
    "end": "55320"
  },
  {
    "text": "mobile attribution we operate with 14 000 customers and 65 percent of the",
    "start": "55320",
    "end": "60780"
  },
  {
    "text": "global market share we have about 1500 employees that operate across the globe",
    "start": "60780",
    "end": "68760"
  },
  {
    "text": "a little bit about the engineering organization that I'm part of the engineering group as about roughly 400",
    "start": "68760",
    "end": "75659"
  },
  {
    "text": "Engineers divided into squads that operates around 1200 microservices that",
    "start": "75659",
    "end": "82380"
  },
  {
    "text": "handle roughly three million events per second and we operate an infrastructure",
    "start": "82380",
    "end": "87960"
  },
  {
    "text": "that it's close to 20 250 000 Cloud resources and dozens of such integration",
    "start": "87960",
    "end": "94439"
  },
  {
    "text": "and more the reason it's it's roughly a number because our infrastructure is",
    "start": "94439",
    "end": "99659"
  },
  {
    "text": "with any other large organization is growing all shrinking based on the on",
    "start": "99659",
    "end": "105180"
  },
  {
    "text": "the demand and the customer requirements so a little bit about the agenda that",
    "start": "105180",
    "end": "110399"
  },
  {
    "text": "we're gonna talk about today we'll start about the challenges of working with a large",
    "start": "110399",
    "end": "117500"
  },
  {
    "text": "large-scale clusters and we'll introduce karmada and understand what is",
    "start": "117500",
    "end": "122640"
  },
  {
    "text": "architecture a little bit about its API I'll go over several use cases just to",
    "start": "122640",
    "end": "128940"
  },
  {
    "text": "demonstrate the use of komada and how it should simplify the use of multicluster",
    "start": "128940",
    "end": "134580"
  },
  {
    "text": "management and we'll conclude what we discuss in this talk",
    "start": "134580",
    "end": "140879"
  },
  {
    "text": "so let us begin I I'm pretty sure that all of you are familiar with this logo if not this is kubernetes of course",
    "start": "140879",
    "end": "148860"
  },
  {
    "text": "um and I guess for over the years we'll learn to love this tool level 8 it's uh",
    "start": "148860",
    "end": "154379"
  },
  {
    "text": "it's a matter of perspection um we learned to love it because it has",
    "start": "154379",
    "end": "159420"
  },
  {
    "text": "a decent scheduling mechanism it has a simple yet a very robust API to describe",
    "start": "159420",
    "end": "164580"
  },
  {
    "text": "our requirements yes it has a lot of built-in goodies like service Discovery has node failures and and countless",
    "start": "164580",
    "end": "172080"
  },
  {
    "text": "Integrations that targets specifically kubernetes it's enough to look at the",
    "start": "172080",
    "end": "177480"
  },
  {
    "text": "cncf landscape just to understand how many different organizations or startups",
    "start": "177480",
    "end": "182760"
  },
  {
    "text": "or Frameworks Target kubernetes as their infrastructure operating system",
    "start": "182760",
    "end": "189360"
  },
  {
    "text": "but when you reach a certain scale when operating with kubernetes",
    "start": "189360",
    "end": "194519"
  },
  {
    "text": "it forces you to rethink your entire architecture of how to manage that scale",
    "start": "194519",
    "end": "201180"
  },
  {
    "text": "and when it comes to large scale there are basically two options for scaling your large scale clusters either you can",
    "start": "201180",
    "end": "209220"
  },
  {
    "text": "go with vertical scaling having a very large disco ball that you keep growing and growing and growing oh the other way",
    "start": "209220",
    "end": "215940"
  },
  {
    "text": "is to use many clusters basically try to have as many balls as possible and",
    "start": "215940",
    "end": "222239"
  },
  {
    "text": "juggle with them each approach has of course its advantage and some drawbacks and then try to list some of them in the",
    "start": "222239",
    "end": "229560"
  },
  {
    "text": "following slides first let's start with the vertical scanning the very large disco ball so",
    "start": "229560",
    "end": "238260"
  },
  {
    "text": "having a very large disco ball it's basically a single point of failure in the organization if something happens to",
    "start": "238260",
    "end": "245459"
  },
  {
    "text": "that cluster your entire production might be in Jeopardy so having a single",
    "start": "245459",
    "end": "250500"
  },
  {
    "text": "point of value is a big No-No in most of our own organizations another is",
    "start": "250500",
    "end": "255659"
  },
  {
    "text": "scalability limitation you can't really allow have the ball that big goodness",
    "start": "255659",
    "end": "261959"
  },
  {
    "text": "itself has similitation for its clusters you can't really reach more than 5000",
    "start": "261959",
    "end": "267240"
  },
  {
    "text": "nodes in some cloud provider you can't even reach 3000 nodes there is also",
    "start": "267240",
    "end": "272880"
  },
  {
    "text": "limitation for proper node and other limitations that prevent you from growing even larger and larger",
    "start": "272880",
    "end": "279660"
  },
  {
    "text": "another issue that you will have is reasons us under utilization you're gonna schedule allows walk or walk um",
    "start": "279660",
    "end": "287240"
  },
  {
    "text": "workloads operating alongside smaller ones you get a lot of fragmentation in",
    "start": "287240",
    "end": "292320"
  },
  {
    "text": "your bean packing and the cluster is becoming underutilized",
    "start": "292320",
    "end": "297900"
  },
  {
    "text": "another thing is the complexity of managing a very large cluster just think",
    "start": "297900",
    "end": "303240"
  },
  {
    "text": "of a simple task to if you would like to let's say upgrade the cluster API how",
    "start": "303240",
    "end": "308520"
  },
  {
    "text": "long did it take to to do a rolling upgrade for a 3000 note cluster and it takes quite a while",
    "start": "308520",
    "end": "316919"
  },
  {
    "text": "next we have a horizontal scaling so now instead of managing a single cluster we're going to manage multiple cluster",
    "start": "316919",
    "end": "323660"
  },
  {
    "text": "so just by introducing many clusters we have a complexity of management of",
    "start": "323660",
    "end": "329639"
  },
  {
    "text": "multiple clusters for example at apps live we manage hundreds of kubernetes clusters and we have different flavors",
    "start": "329639",
    "end": "336539"
  },
  {
    "text": "of cluster we have clusters that are targeting Kafka where cost of the targeting aerospike soil airflow spark",
    "start": "336539",
    "end": "345000"
  },
  {
    "text": "services and many many more each require different type of management and different kind of workloads so the",
    "start": "345000",
    "end": "351840"
  },
  {
    "text": "entire complexity of managing them has become an issue resource allocation this is again",
    "start": "351840",
    "end": "357840"
  },
  {
    "text": "another another difficulty that you need to face with basically when you have so many",
    "start": "357840",
    "end": "365100"
  },
  {
    "text": "cluster which workload you are targeting to which cluster how do you manage it so",
    "start": "365100",
    "end": "370860"
  },
  {
    "text": "your resource allocation become an issue that you need to work with",
    "start": "370860",
    "end": "376380"
  },
  {
    "text": "when you walk into any clusters your entire network become a complexity on its own how you do interconnectivity",
    "start": "376380",
    "end": "382560"
  },
  {
    "text": "between cluster which clusters allow to talk to which and this is a very big",
    "start": "382560",
    "end": "388080"
  },
  {
    "text": "challenge if you are facing with service Discovery because now we need to do a",
    "start": "388080",
    "end": "393180"
  },
  {
    "text": "course uh self-discovery across multiple cluster so this is again another challenge and",
    "start": "393180",
    "end": "399539"
  },
  {
    "text": "of course the last drawback when working on multiple cluster is you have a",
    "start": "399539",
    "end": "404819"
  },
  {
    "text": "potential to have an inconsistency of of con of your configuration basically did",
    "start": "404819",
    "end": "411180"
  },
  {
    "text": "we install the right controller in all of our cluster do we have the same API in all of them",
    "start": "411180",
    "end": "416759"
  },
  {
    "text": "have we deployed the right deployment to each of our regions",
    "start": "416759",
    "end": "422039"
  },
  {
    "text": "it's a drawback when working with so many clusters so this is where actually comada comes",
    "start": "422039",
    "end": "428759"
  },
  {
    "text": "into place and I'm going to read out loud what basically which is a short for kubernetes Armada",
    "start": "428759",
    "end": "435120"
  },
  {
    "text": "is a kubernetes management system that enables you to run Cloud native application across multiple kubernetes",
    "start": "435120",
    "end": "442139"
  },
  {
    "text": "clusters and clouds with no changes to your application so just by by this short statement we're",
    "start": "442139",
    "end": "450000"
  },
  {
    "text": "talking about not just multi-cluster we're talking about multiple regions",
    "start": "450000",
    "end": "455160"
  },
  {
    "text": "even running on multiple cloud provider now this is a very strong statement but",
    "start": "455160",
    "end": "460259"
  },
  {
    "text": "basically Kalamata is aiming to manage your entire Global availability of your",
    "start": "460259",
    "end": "465300"
  },
  {
    "text": "production so let's do a short dive into our",
    "start": "465300",
    "end": "470880"
  },
  {
    "text": "mother's architecture Commodus control plane is trying to mimic kubernetes API so you're going to",
    "start": "470880",
    "end": "478860"
  },
  {
    "text": "be familiar with with most of the examples that I'm gonna show you because you're only natively working with",
    "start": "478860",
    "end": "485160"
  },
  {
    "text": "kubernetes so commodity's API server mimics the the kubernetes API server we",
    "start": "485160",
    "end": "491520"
  },
  {
    "text": "have a scheduler but now instead of scheduling two nodes we are scheduling into clusters",
    "start": "491520",
    "end": "497520"
  },
  {
    "text": "and ah there are multiple controllers that consist the entire control plane",
    "start": "497520",
    "end": "503160"
  },
  {
    "text": "one of them for just for an example is a cluster controller so if you're familiar",
    "start": "503160",
    "end": "508199"
  },
  {
    "text": "with how kubernetes itself operates we have a node controller to operate on on the Node level here we are operating on",
    "start": "508199",
    "end": "514820"
  },
  {
    "text": "cluster level there are two options to integrate with the control plane if you have a cluster",
    "start": "514820",
    "end": "521279"
  },
  {
    "text": "that you want karamada to integrate directly meaning a push uh kalmata will",
    "start": "521279",
    "end": "527339"
  },
  {
    "text": "integrate directly with that API server and push the workflows directly to that",
    "start": "527339",
    "end": "532920"
  },
  {
    "text": "cluster other option is of course the pool using an agent the agent will",
    "start": "532920",
    "end": "538800"
  },
  {
    "text": "connect to chromata's a control plane and fetch all the workflows to that",
    "start": "538800",
    "end": "544019"
  },
  {
    "text": "designated cluster a little bit about the primary concept",
    "start": "544019",
    "end": "550740"
  },
  {
    "text": "that Kalamata is trying to align with basically we're trying to work with this",
    "start": "550740",
    "end": "557339"
  },
  {
    "text": "tree and I'll later on demonstrate how they are coming into action first we",
    "start": "557339",
    "end": "562620"
  },
  {
    "text": "have a resource template resource template are the native kubernetes API you're already familiar with if you",
    "start": "562620",
    "end": "568680"
  },
  {
    "text": "don't know how to work with a deployment a service secret config or whatever it's",
    "start": "568680",
    "end": "574860"
  },
  {
    "text": "it will be become available into chromata's API as a resource template any other existing tool that you're",
    "start": "574860",
    "end": "581519"
  },
  {
    "text": "currently working with in kubernetes will become available for you when working with karmada there is no need to",
    "start": "581519",
    "end": "588360"
  },
  {
    "text": "change anything next we have a propagation policy the propagation policy is basically the",
    "start": "588360",
    "end": "595200"
  },
  {
    "text": "multi-class scheduling that allows you to do a one-to-many scheduling",
    "start": "595200",
    "end": "600240"
  },
  {
    "text": "uh having the template propagate 2 any other cluster that you would like and",
    "start": "600240",
    "end": "606060"
  },
  {
    "text": "last we have the override policy and the override policy is a cluster-specific configuration that allows you to change",
    "start": "606060",
    "end": "612839"
  },
  {
    "text": "the propagation into a much more specific and individual for each cluster or a group of clusters",
    "start": "612839",
    "end": "620760"
  },
  {
    "text": "so let's look at the API flow for example the API starts with",
    "start": "620760",
    "end": "626100"
  },
  {
    "text": "a resource template and as I mentioned everything that needs native to kubernetes it's native to incomeada so a",
    "start": "626100",
    "end": "634560"
  },
  {
    "text": "deployment config map and so on you're submitting into karmadas API alongside",
    "start": "634560",
    "end": "640080"
  },
  {
    "text": "with the propagation and the avoid policy everything comes into action Karma that knows how to push that",
    "start": "640080",
    "end": "647420"
  },
  {
    "text": "workflow directly to the designated cluster it internally it creates an",
    "start": "647420",
    "end": "654120"
  },
  {
    "text": "object called walk this walk object is responsible to reconcile or synchronize to to each of specific clusters now it",
    "start": "654120",
    "end": "663120"
  },
  {
    "text": "doesn't schedule on the cluster itself the the cluster knows how to schedule deployment or config map it knows how to",
    "start": "663120",
    "end": "669360"
  },
  {
    "text": "handle the service it just communicate with the designated cluster through the",
    "start": "669360",
    "end": "674579"
  },
  {
    "text": "API and tells it you need to work with that object internally everything uh keeps working as it should",
    "start": "674579",
    "end": "683399"
  },
  {
    "text": "now in most of the diagrams that I show and later on in some of the other examples I",
    "start": "683399",
    "end": "689700"
  },
  {
    "text": "will show when you're looking at diagrams usually in kubernetes",
    "start": "689700",
    "end": "695519"
  },
  {
    "text": "um squares are nodes okay it's pretty simple we see a lot of nodes",
    "start": "695519",
    "end": "701100"
  },
  {
    "text": "a lot of squares in karomata's diagrams each square is actually a cluster so",
    "start": "701100",
    "end": "707760"
  },
  {
    "text": "it's a it's a it's a much larger scale that your accustomed to you see",
    "start": "707760",
    "end": "714360"
  },
  {
    "text": "so let's go over several use cases to demonstrate the multi-cluster API that karamada provide I have to say that due",
    "start": "714360",
    "end": "722700"
  },
  {
    "text": "to some limitation specifically space in a in a slide I wouldn't be able to see",
    "start": "722700",
    "end": "728640"
  },
  {
    "text": "it show you a complete Armada of clusters I'm going to demonstrate with only two",
    "start": "728640",
    "end": "734779"
  },
  {
    "text": "but just for the sense of it think of it if we had multiple clusters in the US in",
    "start": "734779",
    "end": "740880"
  },
  {
    "text": "Europe in Asia and maybe in Africa the entire slide will be a lot of squares so I'm going to show a",
    "start": "740880",
    "end": "748920"
  },
  {
    "text": "very few initial use cases just to demonstrate it let's start with the simplest uh",
    "start": "748920",
    "end": "755579"
  },
  {
    "text": "item that camera supports is of course scheduling how to do a multi-cluster scheduling",
    "start": "755579",
    "end": "761160"
  },
  {
    "text": "and in this example as I show as I mentioned earlier we are working with",
    "start": "761160",
    "end": "766800"
  },
  {
    "text": "Native kubernetes API so this is a deployment I hope you're familiar with it but it's",
    "start": "766800",
    "end": "774240"
  },
  {
    "text": "nothing related to komada it's a simple deployment that we submit to co-mathes",
    "start": "774240",
    "end": "780480"
  },
  {
    "text": "API nothing special but nothing happens when we submit it this is actually a",
    "start": "780480",
    "end": "787019"
  },
  {
    "text": "resource template nothing happens on either of the cluster that we are connected to",
    "start": "787019",
    "end": "793440"
  },
  {
    "text": "so if we look at the next step is the",
    "start": "793440",
    "end": "798480"
  },
  {
    "text": "propagation policy this is a the multi-cluster API that chromatos provide part of the principles that kalmata is",
    "start": "798480",
    "end": "806579"
  },
  {
    "text": "operating with we are defining a propagation policy and just for the sake of the example I use",
    "start": "806579",
    "end": "811800"
  },
  {
    "text": "the static propagation okay just for a short demonstration you usually",
    "start": "811800",
    "end": "817320"
  },
  {
    "text": "wouldn't use static configuration but in this case we have two cluster one in the",
    "start": "817320",
    "end": "823380"
  },
  {
    "text": "EU the other in the US the EU one called EU West one the US One us one is East",
    "start": "823380",
    "end": "830040"
  },
  {
    "text": "one pretty simple in this static configuration I'm gonna say",
    "start": "830040",
    "end": "835500"
  },
  {
    "text": "um propagate that deployment that they defined earlier to EU this is what",
    "start": "835500",
    "end": "841980"
  },
  {
    "text": "Commander will do schedule it in EU West one nothing really happens on U.S simple",
    "start": "841980",
    "end": "849300"
  },
  {
    "text": "and again any square that you see here is a cluster is not a node specifically",
    "start": "849300",
    "end": "855320"
  },
  {
    "text": "kalamata's API is a cluster each one of the EU and the US class cluster are",
    "start": "855320",
    "end": "862380"
  },
  {
    "text": "fully functional clusters next a more advanced scheduling maybe",
    "start": "862380",
    "end": "868380"
  },
  {
    "text": "with affinity against is something that you are familiar with when working with kubernetes I'm gonna add a label for each of my",
    "start": "868380",
    "end": "875639"
  },
  {
    "text": "cluster one label that's called location based on the continent in this case we'll have",
    "start": "875639",
    "end": "882060"
  },
  {
    "text": "two and I'm gonna chain deploy all of my resource templates to the US clusters so",
    "start": "882060",
    "end": "888480"
  },
  {
    "text": "if we'll have multiple clusters not in this example will deploy all of the resource",
    "start": "888480",
    "end": "894240"
  },
  {
    "text": "templates all the resource template to all of our us-based clusters and slightly modified scheduling",
    "start": "894240",
    "end": "902279"
  },
  {
    "text": "mechanism is an API that is using again match expression with labels now I'm",
    "start": "902279",
    "end": "908100"
  },
  {
    "text": "targeting the US and the EU our Asia and Africa cluster will not get",
    "start": "908100",
    "end": "914000"
  },
  {
    "text": "any propagation defined",
    "start": "914000",
    "end": "918920"
  },
  {
    "text": "next let's see how the override policies come into action again this is a multi-cluster API that kamada provides",
    "start": "919620",
    "end": "926540"
  },
  {
    "text": "uh maybe due to due to some privacy issues or other business constraints",
    "start": "926540",
    "end": "932519"
  },
  {
    "text": "that you have you need to change the way that services are deployed to your",
    "start": "932519",
    "end": "938000"
  },
  {
    "text": "us-based clusters you're basically using the same level selector as before and now we are saying",
    "start": "938000",
    "end": "944519"
  },
  {
    "text": "okay for that for that cluster I'm omitting the overriders and what are the possibilities though for example let's",
    "start": "944519",
    "end": "951600"
  },
  {
    "text": "say we're gonna use a different environment variable or an image and now Commander will schedule the same",
    "start": "951600",
    "end": "957300"
  },
  {
    "text": "deployment but with the overriding scheduling of a cluster in the US",
    "start": "957300",
    "end": "965120"
  },
  {
    "text": "available this is another internal mechanism that comes with income others",
    "start": "967079",
    "end": "972360"
  },
  {
    "text": "API um I updated the example a little bit now the deployment has a replica set to",
    "start": "972360",
    "end": "980579"
  },
  {
    "text": "three so meaning that you will schedule it on a regular kubernetes cluster will get three parts right this is the",
    "start": "980579",
    "end": "987600"
  },
  {
    "text": "definition again I'm um deploying it to command as API it's",
    "start": "987600",
    "end": "993240"
  },
  {
    "text": "becoming a resource template and again I'm I'm going to talk about",
    "start": "993240",
    "end": "998279"
  },
  {
    "text": "cluster failover not node failovers node failovers is something that kubernetes",
    "start": "998279",
    "end": "1003560"
  },
  {
    "text": "know to do with outcome others so it doesn't really need to handle it but this is again a",
    "start": "1003560",
    "end": "1010160"
  },
  {
    "text": "fellow mechanism that slightly have a more advanced propagation policy and",
    "start": "1010160",
    "end": "1015259"
  },
  {
    "text": "again I'm using static configuration just for the sake of the example in this case I'm going to use a",
    "start": "1015259",
    "end": "1021440"
  },
  {
    "text": "preferences of a weighted cluster meaning that I want to change the way that everything is split across my",
    "start": "1021440",
    "end": "1029240"
  },
  {
    "text": "Armada I Have a scheduling type it's called divided meaning that I wanna",
    "start": "1029240",
    "end": "1035319"
  },
  {
    "text": "take the entire weight and divide it across my entire clusters I can also use",
    "start": "1035319",
    "end": "1040520"
  },
  {
    "text": "replicated and other strategies again just for the sake of the example I've used a weighted and divided because it's",
    "start": "1040520",
    "end": "1047480"
  },
  {
    "text": "much much easier to understand and I'm gonna use the static that weights 2 to the US one to the EU",
    "start": "1047480",
    "end": "1054559"
  },
  {
    "text": "meaning that two parts go to the U.S one two EU and again Kalamata doesn't",
    "start": "1054559",
    "end": "1061280"
  },
  {
    "text": "schedule the pods themselves commodity scheduling only the deployments and the",
    "start": "1061280",
    "end": "1066440"
  },
  {
    "text": "those deployments have a slight different propagation based on the",
    "start": "1066440",
    "end": "1072620"
  },
  {
    "text": "propagation policy that we Define and when they reach internally into the designated clusters two parts appear in",
    "start": "1072620",
    "end": "1080059"
  },
  {
    "text": "the U.S one pod appeared appear in the U EU",
    "start": "1080059",
    "end": "1085899"
  },
  {
    "text": "maybe an entire region has failed or maybe something that we did that causes",
    "start": "1087340",
    "end": "1092360"
  },
  {
    "text": "our cluster in the EU to shut down in this case Commander will identify it",
    "start": "1092360",
    "end": "1099260"
  },
  {
    "text": "and will simply reschedule the deployment the deployment not the pod",
    "start": "1099260",
    "end": "1105380"
  },
  {
    "text": "and change the way that the number of replicas will represent the desired state that we wanted three replicas now",
    "start": "1105380",
    "end": "1112340"
  },
  {
    "text": "everyone will be in the US",
    "start": "1112340",
    "end": "1116320"
  },
  {
    "text": "another example for Commodus multi-cluster API is service Discovery and now",
    "start": "1118820",
    "end": "1126279"
  },
  {
    "text": "um this is something that I personally find very interesting something I'm contributing to this project I wish to",
    "start": "1128000",
    "end": "1135140"
  },
  {
    "text": "keep contributing to enhance its features um we'll start again with a very simple",
    "start": "1135140",
    "end": "1140900"
  },
  {
    "text": "example we have a deployment and a service I'm gonna omit the propagation policy just for the sake of the example",
    "start": "1140900",
    "end": "1148539"
  },
  {
    "text": "we propagate it through the command API and we have it on the EU West one",
    "start": "1148539",
    "end": "1153860"
  },
  {
    "text": "cluster so now everything runs very simple we have a deployment and a",
    "start": "1153860",
    "end": "1159380"
  },
  {
    "text": "service that connects into it so everyone that access the service locally will reach the deployment right",
    "start": "1159380",
    "end": "1167240"
  },
  {
    "text": "simple still kubernetes when we employ the multi-cluster",
    "start": "1167240",
    "end": "1173059"
  },
  {
    "text": "services API we say that we want to export that service from the US cluster",
    "start": "1173059",
    "end": "1180020"
  },
  {
    "text": "e sorry from the EU cluster to the US cluster we export",
    "start": "1180020",
    "end": "1186559"
  },
  {
    "text": "from one cluster and import to another again I'm omitting the propagation everything else but",
    "start": "1186559",
    "end": "1193340"
  },
  {
    "text": "uh once we'll have everything running um will have a service defined in the",
    "start": "1193340",
    "end": "1200840"
  },
  {
    "text": "U.S there's basically once the services will reach it locally will reach the EU",
    "start": "1200840",
    "end": "1206419"
  },
  {
    "text": "EU cluster without having to know that there is no local deployment now there",
    "start": "1206419",
    "end": "1212720"
  },
  {
    "text": "are other possibilities other strategies for service Discovery you can do failovers you can do other options how",
    "start": "1212720",
    "end": "1219500"
  },
  {
    "text": "to to do uh service cover in multiple clusters but",
    "start": "1219500",
    "end": "1224840"
  },
  {
    "text": "this is this is just the simplest example of having a service that basically does a direct connect to",
    "start": "1224840",
    "end": "1231020"
  },
  {
    "text": "another cluster but we know the thing that I just",
    "start": "1231020",
    "end": "1238280"
  },
  {
    "text": "demonstrated you often understand that all of the examples that I recently showed you possesses some risk okay and",
    "start": "1238280",
    "end": "1246200"
  },
  {
    "text": "the risk is it's if you send the yamls and read it closely you will probably",
    "start": "1246200",
    "end": "1252500"
  },
  {
    "text": "see that the API version was V1 Alpha 1. meaning that while some companies do use",
    "start": "1252500",
    "end": "1259460"
  },
  {
    "text": "commands production you have to understand that this is a cncf Sandbox project as a cncf sandbot",
    "start": "1259460",
    "end": "1266360"
  },
  {
    "text": "project it means that the API is not finalized all should be considered stable",
    "start": "1266360",
    "end": "1271880"
  },
  {
    "text": "okay so if you're planning on using it you should definitely consider the risk",
    "start": "1271880",
    "end": "1276919"
  },
  {
    "text": "of using a Sandbox project and as a Sandbox project or any other",
    "start": "1276919",
    "end": "1282200"
  },
  {
    "text": "open source project it requires a lot of contribution from its audience and users",
    "start": "1282200",
    "end": "1288400"
  },
  {
    "text": "first you need you should access their website in commodus.io there's a lot of",
    "start": "1288400",
    "end": "1293900"
  },
  {
    "text": "documentation that the team has provided a very well documented code and a very",
    "start": "1293900",
    "end": "1300440"
  },
  {
    "text": "well documented feature set you can also open a lot of issues and contribute of course code to the GitHub",
    "start": "1300440",
    "end": "1307820"
  },
  {
    "text": "repository it's highly appreciated and of course join the discussion there",
    "start": "1307820",
    "end": "1313820"
  },
  {
    "text": "are a lot of discussions over the cncf slack over red color models and any",
    "start": "1313820",
    "end": "1320299"
  },
  {
    "text": "contribution will be much helpful so let us conclude a little bit about",
    "start": "1320299",
    "end": "1327200"
  },
  {
    "text": "what discuss in the past 20 so minutes first we discussed about",
    "start": "1327200",
    "end": "1333039"
  },
  {
    "text": "kubernetes and how it used to ease our work our operational work",
    "start": "1333039",
    "end": "1338559"
  },
  {
    "text": "we mentioned that once we reach a certain amount of scale we need to",
    "start": "1338559",
    "end": "1344059"
  },
  {
    "text": "rethink operating strategy we talked about vertical scaling versus",
    "start": "1344059",
    "end": "1349220"
  },
  {
    "text": "horizontal scaling having the disco ball versus number of balls and what are the downside for each of",
    "start": "1349220",
    "end": "1355280"
  },
  {
    "text": "the approaches and last we talked about karmada its architecture and very very",
    "start": "1355280",
    "end": "1361820"
  },
  {
    "text": "initial use cases just to get a glimpse of its multi-cluster API",
    "start": "1361820",
    "end": "1368480"
  },
  {
    "text": "thank you very much thank you very much and we have a few",
    "start": "1368480",
    "end": "1374780"
  },
  {
    "text": "minutes for questions any questions from the audience okay I see a few hands here",
    "start": "1374780",
    "end": "1381140"
  },
  {
    "text": "let's start here so my question is regarding your",
    "start": "1381140",
    "end": "1386240"
  },
  {
    "text": "experience in scaling at AppStar before you uh when you made the decision to scale",
    "start": "1386240",
    "end": "1394220"
  },
  {
    "text": "horizontally did you what was the limit that you discovered practically on",
    "start": "1394220",
    "end": "1400100"
  },
  {
    "text": "vertical scaling sorry on horizontal scaling of a single cluster before you decided that you had to go split",
    "start": "1400100",
    "end": "1407000"
  },
  {
    "text": "workloads to multiple clusters like in upside we decided for that we",
    "start": "1407000",
    "end": "1412220"
  },
  {
    "text": "immediately gonna go with the horizontal scaling because of the the amount of different workflow that we have since we",
    "start": "1412220",
    "end": "1418880"
  },
  {
    "text": "have workflows that targeting Kafka airflow Services they're all different",
    "start": "1418880",
    "end": "1424340"
  },
  {
    "text": "in shapes and sizes so we immediately started with responsible scaling next question okay here we go",
    "start": "1424340",
    "end": "1433779"
  },
  {
    "text": "hi can you please uh say how Kafka is different from ocm open cluster management and why would I choose one",
    "start": "1434240",
    "end": "1441260"
  },
  {
    "text": "over the other I guess this is a topic for another uh for another conversation",
    "start": "1441260",
    "end": "1447520"
  },
  {
    "text": "uh so maybe we could talk yeah let's have it because it's a big subject and I",
    "start": "1447520",
    "end": "1453860"
  },
  {
    "text": "can actually fill in another 30 minutes for it okay uh more questions here we go",
    "start": "1453860",
    "end": "1460360"
  },
  {
    "text": "um um when you deploy an application across multiple questions",
    "start": "1461000",
    "end": "1466340"
  },
  {
    "text": "um and let's let's say it deployed and everything went successfully who's responsible for the deployment if it",
    "start": "1466340",
    "end": "1474140"
  },
  {
    "text": "let's say change somehow someone change it so",
    "start": "1474140",
    "end": "1479480"
  },
  {
    "text": "um it looks on it and check and checks if it change and reverted to the initial state",
    "start": "1479480",
    "end": "1486200"
  },
  {
    "text": "okay so and basically because let's say if I use Argo CD to manage my",
    "start": "1486200",
    "end": "1493100"
  },
  {
    "text": "applications and I want to deploy all the application across multiple cluster I can use both but since algo city is",
    "start": "1493100",
    "end": "1500720"
  },
  {
    "text": "the manager of the application it might change something and if this one will change it back it will go back and forth",
    "start": "1500720",
    "end": "1508460"
  },
  {
    "text": "I think you kind of answer your question uh because the key here is get UPS you",
    "start": "1508460",
    "end": "1514280"
  },
  {
    "text": "basically need to have the entire API propagate with githubs okay so if you",
    "start": "1514280",
    "end": "1519799"
  },
  {
    "text": "are managing today your entire clusters with the Argo CDO or flux to to manage your clusters you basically",
    "start": "1519799",
    "end": "1527059"
  },
  {
    "text": "have the same Notions of githubs everything the source of tool for you is still your git repositories",
    "start": "1527059",
    "end": "1533779"
  },
  {
    "text": "so if some something changing across the cluster it will propagate again and reconcile to your source of tool",
    "start": "1533779",
    "end": "1540980"
  },
  {
    "text": "more questions okay we have here the corner",
    "start": "1540980",
    "end": "1546080"
  },
  {
    "text": "there we go okay a couple of questions about the camera first",
    "start": "1546080",
    "end": "1552260"
  },
  {
    "text": "um does it also handle things like changing of kubernetes versions or it just handles the deployment and",
    "start": "1552260",
    "end": "1559159"
  },
  {
    "text": "following up with this question does camera have gear UPS of its own because if I understand correctly instead of",
    "start": "1559159",
    "end": "1566900"
  },
  {
    "text": "writing infrastructure as code that goes directly to the kubernetes cluster you have to write infrastructures code that",
    "start": "1566900",
    "end": "1573200"
  },
  {
    "text": "goes to carmuta to handle all of it so it should have the same tools of managing code and configuration on its",
    "start": "1573200",
    "end": "1580520"
  },
  {
    "text": "own so I'll start with the second one actually um but no it doesn't have its uh its own",
    "start": "1580520",
    "end": "1588080"
  },
  {
    "text": "gate UPS it relies on the reconcile itself this is why it says call mother's API and you're supposed to bring in all",
    "start": "1588080",
    "end": "1594860"
  },
  {
    "text": "your yamas and configuration from another source in this case maybe I'll go maybe flux to bringing your",
    "start": "1594860",
    "end": "1601760"
  },
  {
    "text": "configuration it's that and so the the second part of your",
    "start": "1601760",
    "end": "1608000"
  },
  {
    "text": "question okay and for the first part I need to",
    "start": "1608000",
    "end": "1613820"
  },
  {
    "text": "I need you to ask it again sorry so you you mentioned a few",
    "start": "1613820",
    "end": "1619179"
  },
  {
    "text": "drawbacks of using multiple clusters like class inconsistencies of configuration between",
    "start": "1619179",
    "end": "1626419"
  },
  {
    "text": "clusters uh how does if it does karmada resolve this those issues",
    "start": "1626419",
    "end": "1632240"
  },
  {
    "text": "it actually doesn't okay this is part of being a Sandbox it doesn't really Target it yet",
    "start": "1632240",
    "end": "1640279"
  },
  {
    "text": "any other questions just a second here we go",
    "start": "1640279",
    "end": "1646360"
  },
  {
    "text": "in the example you gave with deployment with multiple replicas that got divided",
    "start": "1649940",
    "end": "1655279"
  },
  {
    "text": "between two clusters how is POD disruption budget handled in that",
    "start": "1655279",
    "end": "1661220"
  },
  {
    "text": "example you basically if you know how to work with part distribution budget in a single cluster",
    "start": "1661220",
    "end": "1667520"
  },
  {
    "text": "it's the same with the with the in karamata's API is submitted and propagated to the Clusters and we'll",
    "start": "1667520",
    "end": "1674000"
  },
  {
    "text": "keep walking is the same but if my deployment is let's say I want",
    "start": "1674000",
    "end": "1680480"
  },
  {
    "text": "to have a Max of one unavailable but I have one cluster with one",
    "start": "1680480",
    "end": "1686059"
  },
  {
    "text": "deployment with one replica and another cluster with two replicas",
    "start": "1686059",
    "end": "1691299"
  },
  {
    "text": "what would be the pdb that gets set on each of those so I I showed a static",
    "start": "1691340",
    "end": "1696620"
  },
  {
    "text": "configuration so usually wouldn't use that a specific number of PODS this is a",
    "start": "1696620",
    "end": "1703100"
  },
  {
    "text": "contradicts walking with with auto scaling and contradicts budgeting and so on",
    "start": "1703100",
    "end": "1710059"
  },
  {
    "text": "so if you're working with budgeting there is another API for that okay I didn't show it because it's a much more",
    "start": "1710059",
    "end": "1716240"
  },
  {
    "text": "advanced one but karma knows how to handle it this is a different one there's a question on",
    "start": "1716240",
    "end": "1722360"
  },
  {
    "text": "this side of the house yeah here we go",
    "start": "1722360",
    "end": "1726400"
  },
  {
    "text": "uh yes uh first question um crds are they supported or yep okay",
    "start": "1727640",
    "end": "1734900"
  },
  {
    "text": "that's good and the second thing uh some uh infrastructure is good providers like",
    "start": "1734900",
    "end": "1740059"
  },
  {
    "text": "pollumi and terraform rely on the status of the deployments so in this case",
    "start": "1740059",
    "end": "1745880"
  },
  {
    "text": "because one deployment is actually multiple deployments how does the status feel change I'm I'm sure it's not like",
    "start": "1745880",
    "end": "1752960"
  },
  {
    "text": "the same as one single deployment API is the source of tools for you if",
    "start": "1752960",
    "end": "1760820"
  },
  {
    "text": "you usually in in if you deploy with telephone telephone looks as a single cluster right",
    "start": "1760820",
    "end": "1766100"
  },
  {
    "text": "but now we are working on multiple clusters so coromada is your entry point so telephone operates on the garmada API",
    "start": "1766100",
    "end": "1772640"
  },
  {
    "text": "level if you deployment if you apply deployment uh to a cluster in terraform",
    "start": "1772640",
    "end": "1780440"
  },
  {
    "text": "it waits on the deployment status to be complete so in this case what's the status because there's 10 clusters and",
    "start": "1780440",
    "end": "1787760"
  },
  {
    "text": "I'm sure it's a different syntax or do you even have like a status that's yeah it's the native if you'll do Cube cattle",
    "start": "1787760",
    "end": "1795039"
  },
  {
    "text": "uh describe the deployment he will get the status of it as if it was running on",
    "start": "1795039",
    "end": "1800480"
  },
  {
    "text": "the curl models API oh oh so like you the complete is on every single poster",
    "start": "1800480",
    "end": "1806179"
  },
  {
    "text": "combined yeah okay thank you very much aliron great Round",
    "start": "1806179",
    "end": "1811700"
  },
  {
    "text": "of Applause great job",
    "start": "1811700",
    "end": "1814960"
  }
]