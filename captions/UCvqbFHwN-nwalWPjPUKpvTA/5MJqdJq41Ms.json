[
  {
    "text": "hi everybody welcome to turn it up to a",
    "start": "0",
    "end": "2159"
  },
  {
    "text": "million ingesting millions of metrics",
    "start": "2159",
    "end": "3520"
  },
  {
    "text": "with thanos received",
    "start": "3520",
    "end": "4960"
  },
  {
    "text": "i hope that everyone's doing well at",
    "start": "4960",
    "end": "6160"
  },
  {
    "text": "home and i'm happy to be here with you",
    "start": "6160",
    "end": "8639"
  },
  {
    "text": "so i originally proposed this talk",
    "start": "8639",
    "end": "10800"
  },
  {
    "text": "because i'm excited to share some of the",
    "start": "10800",
    "end": "12799"
  },
  {
    "text": "work that we've been doing",
    "start": "12799",
    "end": "13679"
  },
  {
    "text": "inside of red hat on the observability",
    "start": "13679",
    "end": "15519"
  },
  {
    "text": "team and also inside of the thanos",
    "start": "15519",
    "end": "17520"
  },
  {
    "text": "project",
    "start": "17520",
    "end": "18240"
  },
  {
    "text": "around ingesting large amounts of time",
    "start": "18240",
    "end": "20160"
  },
  {
    "text": "series in a horizontally scalable way",
    "start": "20160",
    "end": "23279"
  },
  {
    "text": "my hope is that at the end of this talk",
    "start": "23279",
    "end": "24800"
  },
  {
    "text": "you'll come away with some",
    "start": "24800",
    "end": "26640"
  },
  {
    "text": "concrete ideas around how to build and",
    "start": "26640",
    "end": "28800"
  },
  {
    "text": "also deploy",
    "start": "28800",
    "end": "30080"
  },
  {
    "text": "a scalable metric suggestion stack",
    "start": "30080",
    "end": "33840"
  },
  {
    "text": "so who am i and why am i talking about",
    "start": "33920",
    "end": "36960"
  },
  {
    "text": "this",
    "start": "36960",
    "end": "37680"
  },
  {
    "text": "my name is lucas marin i'm spanish i",
    "start": "37680",
    "end": "40879"
  },
  {
    "text": "am a twin and i love penguins and i'm",
    "start": "40879",
    "end": "44000"
  },
  {
    "text": "also",
    "start": "44000",
    "end": "44480"
  },
  {
    "text": "a principal software engineer at red hat",
    "start": "44480",
    "end": "46239"
  },
  {
    "text": "where i work on the observability",
    "start": "46239",
    "end": "48239"
  },
  {
    "text": "platform team",
    "start": "48239",
    "end": "49520"
  },
  {
    "text": "where we primarily work on building",
    "start": "49520",
    "end": "52239"
  },
  {
    "text": "distributed systems",
    "start": "52239",
    "end": "53680"
  },
  {
    "text": "for monitoring openshift i'm lucky that",
    "start": "53680",
    "end": "56320"
  },
  {
    "text": "at work get to contribute a lot to open",
    "start": "56320",
    "end": "57920"
  },
  {
    "text": "source projects",
    "start": "57920",
    "end": "59039"
  },
  {
    "text": "and i get to work as a maintainer of",
    "start": "59039",
    "end": "60800"
  },
  {
    "text": "thanos the prometheus operator and a",
    "start": "60800",
    "end": "62320"
  },
  {
    "text": "bunch of other things",
    "start": "62320",
    "end": "63760"
  },
  {
    "text": "and you can find me on github as at",
    "start": "63760",
    "end": "65600"
  },
  {
    "text": "squat",
    "start": "65600",
    "end": "67520"
  },
  {
    "text": "okay so this talk assumes some",
    "start": "67520",
    "end": "70400"
  },
  {
    "text": "familiarity with thanos",
    "start": "70400",
    "end": "72159"
  },
  {
    "text": "so i recommend that you take a look at",
    "start": "72159",
    "end": "74400"
  },
  {
    "text": "the talk that my fellow thanos",
    "start": "74400",
    "end": "76080"
  },
  {
    "text": "maintainers",
    "start": "76080",
    "end": "76799"
  },
  {
    "text": "are giving at this year's scoop con",
    "start": "76799",
    "end": "78479"
  },
  {
    "text": "called intro to thanos",
    "start": "78479",
    "end": "81200"
  },
  {
    "text": "we'll also talk a bit about it right now",
    "start": "81200",
    "end": "83920"
  },
  {
    "text": "so",
    "start": "83920",
    "end": "85439"
  },
  {
    "text": "the goal of the thanos project is to",
    "start": "85439",
    "end": "87840"
  },
  {
    "text": "provide",
    "start": "87840",
    "end": "88560"
  },
  {
    "text": "a global query view of all of our data",
    "start": "88560",
    "end": "92400"
  },
  {
    "text": "to be compatible 100 with prometheus",
    "start": "92400",
    "end": "96159"
  },
  {
    "text": "to provide long-term retention so that",
    "start": "96159",
    "end": "98560"
  },
  {
    "text": "we can query data from months",
    "start": "98560",
    "end": "100560"
  },
  {
    "text": "or even years ago and also to provide",
    "start": "100560",
    "end": "103360"
  },
  {
    "text": "compaction down sampling",
    "start": "103360",
    "end": "104960"
  },
  {
    "text": "so we can speed up our queries",
    "start": "104960",
    "end": "108560"
  },
  {
    "text": "thanos does this by providing a set of",
    "start": "108799",
    "end": "112079"
  },
  {
    "text": "components",
    "start": "112079",
    "end": "113119"
  },
  {
    "text": "that we can compose into custom-built",
    "start": "113119",
    "end": "116880"
  },
  {
    "text": "distributed modding distributed",
    "start": "116880",
    "end": "118640"
  },
  {
    "text": "monitoring stacks",
    "start": "118640",
    "end": "120880"
  },
  {
    "text": "that suit our needs for different",
    "start": "120880",
    "end": "122479"
  },
  {
    "text": "scenarios",
    "start": "122479",
    "end": "123920"
  },
  {
    "text": "a really common deployment pattern for",
    "start": "123920",
    "end": "125759"
  },
  {
    "text": "thanos looks like this",
    "start": "125759",
    "end": "128479"
  },
  {
    "text": "we have one central monitoring cluster",
    "start": "128479",
    "end": "131760"
  },
  {
    "text": "with a thanos querier and maybe a",
    "start": "131760",
    "end": "134640"
  },
  {
    "text": "grafana next to it",
    "start": "134640",
    "end": "136080"
  },
  {
    "text": "and then we have a bunch of different",
    "start": "136080",
    "end": "137920"
  },
  {
    "text": "clusters or environments",
    "start": "137920",
    "end": "139360"
  },
  {
    "text": "with prometheus servers that we want to",
    "start": "139360",
    "end": "141840"
  },
  {
    "text": "query so we want to",
    "start": "141840",
    "end": "143200"
  },
  {
    "text": "from one single location query the data",
    "start": "143200",
    "end": "145520"
  },
  {
    "text": "from all these different prometheus",
    "start": "145520",
    "end": "146640"
  },
  {
    "text": "servers",
    "start": "146640",
    "end": "148160"
  },
  {
    "text": "when we go ahead and type a query into",
    "start": "148160",
    "end": "150560"
  },
  {
    "text": "the thanos ui",
    "start": "150560",
    "end": "152480"
  },
  {
    "text": "thanos pulls the data that it needs from",
    "start": "152480",
    "end": "155360"
  },
  {
    "text": "the prometheus servers by making a grpc",
    "start": "155360",
    "end": "157519"
  },
  {
    "text": "request",
    "start": "157519",
    "end": "158480"
  },
  {
    "text": "to the thunderside cars in each cluster",
    "start": "158480",
    "end": "161280"
  },
  {
    "text": "and the thundersidecar in turn",
    "start": "161280",
    "end": "163280"
  },
  {
    "text": "proxy that requested their prometheus",
    "start": "163280",
    "end": "164720"
  },
  {
    "text": "api",
    "start": "164720",
    "end": "166480"
  },
  {
    "text": "all right so far so good but what",
    "start": "166480",
    "end": "168319"
  },
  {
    "text": "happens if we don't have ingress access",
    "start": "168319",
    "end": "169920"
  },
  {
    "text": "to the clusters that we want to query",
    "start": "169920",
    "end": "171280"
  },
  {
    "text": "for example what if the clusters are",
    "start": "171280",
    "end": "173360"
  },
  {
    "text": "owned by customers and they want to",
    "start": "173360",
    "end": "174560"
  },
  {
    "text": "share some metrics with us",
    "start": "174560",
    "end": "176160"
  },
  {
    "text": "but they don't want to expose all of",
    "start": "176160",
    "end": "178480"
  },
  {
    "text": "their data to the internet",
    "start": "178480",
    "end": "179920"
  },
  {
    "text": "or what happens if the customers want to",
    "start": "179920",
    "end": "182239"
  },
  {
    "text": "share some data with us but not",
    "start": "182239",
    "end": "184080"
  },
  {
    "text": "all the metrics one",
    "start": "184080",
    "end": "187200"
  },
  {
    "text": "option we have in this case would be to",
    "start": "187200",
    "end": "188480"
  },
  {
    "text": "switch from a pull-based model to a",
    "start": "188480",
    "end": "190239"
  },
  {
    "text": "push-based approach",
    "start": "190239",
    "end": "191360"
  },
  {
    "text": "it turns out that prometheus has an api",
    "start": "191360",
    "end": "193040"
  },
  {
    "text": "exactly for this use case it's called",
    "start": "193040",
    "end": "194400"
  },
  {
    "text": "the prometheus remote right api",
    "start": "194400",
    "end": "196239"
  },
  {
    "text": "and the implementation is really nice so",
    "start": "196239",
    "end": "198080"
  },
  {
    "text": "when prometheus is configured to remote",
    "start": "198080",
    "end": "199760"
  },
  {
    "text": "right to an endpoint",
    "start": "199760",
    "end": "201200"
  },
  {
    "text": "it's essentially tailing the right ahead",
    "start": "201200",
    "end": "202800"
  },
  {
    "text": "log for the database and sending all of",
    "start": "202800",
    "end": "204799"
  },
  {
    "text": "the time series to this remote endpoint",
    "start": "204799",
    "end": "207280"
  },
  {
    "text": "the api has built in retry logic request",
    "start": "207280",
    "end": "210159"
  },
  {
    "text": "batching",
    "start": "210159",
    "end": "210879"
  },
  {
    "text": "and back offs and a lot of configurable",
    "start": "210879",
    "end": "213120"
  },
  {
    "text": "features that make it really good for",
    "start": "213120",
    "end": "214640"
  },
  {
    "text": "for replicating data so the thanos",
    "start": "214640",
    "end": "217840"
  },
  {
    "text": "receiver essentially",
    "start": "217840",
    "end": "218959"
  },
  {
    "text": "answers the question of how can we build",
    "start": "218959",
    "end": "221200"
  },
  {
    "text": "a thanos component to accept metrics",
    "start": "221200",
    "end": "223680"
  },
  {
    "text": "sent over the prometheus remote right",
    "start": "223680",
    "end": "225120"
  },
  {
    "text": "api this component should be able to",
    "start": "225120",
    "end": "228000"
  },
  {
    "text": "ingest metrics that are sent from lots",
    "start": "228000",
    "end": "230239"
  },
  {
    "text": "of clients into one local tsdb",
    "start": "230239",
    "end": "232640"
  },
  {
    "text": "so far so good but imagine what happens",
    "start": "232640",
    "end": "234640"
  },
  {
    "text": "when instead of having",
    "start": "234640",
    "end": "235680"
  },
  {
    "text": "two or three or four or just a few",
    "start": "235680",
    "end": "237680"
  },
  {
    "text": "clients we go to having an",
    "start": "237680",
    "end": "239519"
  },
  {
    "text": "arbitrary amount of clients so one",
    "start": "239519",
    "end": "242480"
  },
  {
    "text": "single component ingesting metrics from",
    "start": "242480",
    "end": "244239"
  },
  {
    "text": "thousands of clients",
    "start": "244239",
    "end": "245439"
  },
  {
    "text": "is going to eventually break we need a",
    "start": "245439",
    "end": "247599"
  },
  {
    "text": "way to make this",
    "start": "247599",
    "end": "248720"
  },
  {
    "text": "component scalable and that's exactly",
    "start": "248720",
    "end": "250560"
  },
  {
    "text": "what this talk is about",
    "start": "250560",
    "end": "253040"
  },
  {
    "text": "okay so how do we make the thunder's",
    "start": "253040",
    "end": "254879"
  },
  {
    "text": "receiver scale",
    "start": "254879",
    "end": "256239"
  },
  {
    "text": "the approach that we took was to design",
    "start": "256239",
    "end": "257680"
  },
  {
    "text": "the receiver as a hash ring of many",
    "start": "257680",
    "end": "259519"
  },
  {
    "text": "replicas collaborating to ingest data",
    "start": "259519",
    "end": "261280"
  },
  {
    "text": "together",
    "start": "261280",
    "end": "262079"
  },
  {
    "text": "so when prometheus remote writes data",
    "start": "262079",
    "end": "264320"
  },
  {
    "text": "into the hash ring",
    "start": "264320",
    "end": "265680"
  },
  {
    "text": "the request gets load balanced and",
    "start": "265680",
    "end": "267280"
  },
  {
    "text": "initially lands on one of the replicas",
    "start": "267280",
    "end": "269680"
  },
  {
    "text": "this replica hashes the received time",
    "start": "269680",
    "end": "271680"
  },
  {
    "text": "series and their labels",
    "start": "271680",
    "end": "273120"
  },
  {
    "text": "and it finds a corresponding replica",
    "start": "273120",
    "end": "274800"
  },
  {
    "text": "that should handle this request",
    "start": "274800",
    "end": "276800"
  },
  {
    "text": "and it forwards the request to it so",
    "start": "276800",
    "end": "279759"
  },
  {
    "text": "this internal forwarding of requests",
    "start": "279759",
    "end": "281680"
  },
  {
    "text": "is what allows us to distribute load",
    "start": "281680",
    "end": "283360"
  },
  {
    "text": "equally across all of the replicas",
    "start": "283360",
    "end": "285120"
  },
  {
    "text": "and more importantly it also allows us",
    "start": "285120",
    "end": "287759"
  },
  {
    "text": "to scale the ingestion stack",
    "start": "287759",
    "end": "289199"
  },
  {
    "text": "horizontally",
    "start": "289199",
    "end": "291440"
  },
  {
    "text": "the way that we configure the thunder's",
    "start": "291440",
    "end": "292639"
  },
  {
    "text": "receiver allows us to place different",
    "start": "292639",
    "end": "294400"
  },
  {
    "text": "replicas",
    "start": "294400",
    "end": "295120"
  },
  {
    "text": "into distinct hash rings where each",
    "start": "295120",
    "end": "296960"
  },
  {
    "text": "hashing can correspond to some number of",
    "start": "296960",
    "end": "298720"
  },
  {
    "text": "tenants",
    "start": "298720",
    "end": "299360"
  },
  {
    "text": "where for example a tenant might be a",
    "start": "299360",
    "end": "301759"
  },
  {
    "text": "general purpose telemetry system or",
    "start": "301759",
    "end": "303759"
  },
  {
    "text": "maybe",
    "start": "303759",
    "end": "304639"
  },
  {
    "text": "a company-wide metric suggestion service",
    "start": "304639",
    "end": "308800"
  },
  {
    "text": "if we know that one of the hash rings",
    "start": "308800",
    "end": "310639"
  },
  {
    "text": "has to serve a tenant",
    "start": "310639",
    "end": "311840"
  },
  {
    "text": "that has a lot of clients and ingests a",
    "start": "311840",
    "end": "313680"
  },
  {
    "text": "lot of data then we can scale that",
    "start": "313680",
    "end": "315199"
  },
  {
    "text": "hashing independently for example hash",
    "start": "315199",
    "end": "317039"
  },
  {
    "text": "ring b might have to serve",
    "start": "317039",
    "end": "318479"
  },
  {
    "text": "thousands of clients it needs to have",
    "start": "318479",
    "end": "320240"
  },
  {
    "text": "bigger a bigger hash ring",
    "start": "320240",
    "end": "322400"
  },
  {
    "text": "with beefier machines we can accommodate",
    "start": "322400",
    "end": "324720"
  },
  {
    "text": "that on the other hand if we know that",
    "start": "324720",
    "end": "327039"
  },
  {
    "text": "hash ring c only has to serve a few",
    "start": "327039",
    "end": "328800"
  },
  {
    "text": "different tenants that are sending very",
    "start": "328800",
    "end": "331600"
  },
  {
    "text": "few metrics then we can scale that",
    "start": "331600",
    "end": "333360"
  },
  {
    "text": "hashing down",
    "start": "333360",
    "end": "335840"
  },
  {
    "text": "but what happens now if one of the",
    "start": "335840",
    "end": "337440"
  },
  {
    "text": "replicas is experiencing problems",
    "start": "337440",
    "end": "339280"
  },
  {
    "text": "for any reason maybe we're rolling out a",
    "start": "339280",
    "end": "341600"
  },
  {
    "text": "new version",
    "start": "341600",
    "end": "342560"
  },
  {
    "text": "or we're adding more memory to it",
    "start": "342560",
    "end": "344800"
  },
  {
    "text": "whenever this replica is down",
    "start": "344800",
    "end": "346560"
  },
  {
    "text": "the hash ring has an entire set of",
    "start": "346560",
    "end": "350240"
  },
  {
    "text": "hashes so an entire set of time series",
    "start": "350240",
    "end": "353520"
  },
  {
    "text": "and labels",
    "start": "353520",
    "end": "354560"
  },
  {
    "text": "that it can no longer forward anywhere",
    "start": "354560",
    "end": "358240"
  },
  {
    "text": "so this means that the hash ring is",
    "start": "358240",
    "end": "359840"
  },
  {
    "text": "scalable but it's not highly available",
    "start": "359840",
    "end": "363600"
  },
  {
    "text": "we saw this inside of thanos by",
    "start": "363600",
    "end": "365520"
  },
  {
    "text": "providing the option for a kind of",
    "start": "365520",
    "end": "367280"
  },
  {
    "text": "dynamo style replication that means that",
    "start": "367280",
    "end": "370160"
  },
  {
    "text": "for example if we choose to replicate",
    "start": "370160",
    "end": "371680"
  },
  {
    "text": "our data always three times",
    "start": "371680",
    "end": "373280"
  },
  {
    "text": "when we receive a request the initial",
    "start": "373280",
    "end": "376000"
  },
  {
    "text": "replica will",
    "start": "376000",
    "end": "377120"
  },
  {
    "text": "forward this request to three different",
    "start": "377120",
    "end": "378880"
  },
  {
    "text": "nodes in the hash ring",
    "start": "378880",
    "end": "380639"
  },
  {
    "text": "and as long as two of the nodes respond",
    "start": "380639",
    "end": "383199"
  },
  {
    "text": "with yes i was able to write this data",
    "start": "383199",
    "end": "385039"
  },
  {
    "text": "then we return back to the client a 200",
    "start": "385039",
    "end": "387199"
  },
  {
    "text": "and we say this was written successfully",
    "start": "387199",
    "end": "389360"
  },
  {
    "text": "into our tsdb",
    "start": "389360",
    "end": "390960"
  },
  {
    "text": "this now allows us to have both a",
    "start": "390960",
    "end": "393280"
  },
  {
    "text": "scalable",
    "start": "393280",
    "end": "394319"
  },
  {
    "text": "and a highly available ingestion system",
    "start": "394319",
    "end": "398160"
  },
  {
    "text": "this replication scheme means that we",
    "start": "398160",
    "end": "399600"
  },
  {
    "text": "can tolerate having one node done at any",
    "start": "399600",
    "end": "401440"
  },
  {
    "text": "given time",
    "start": "401440",
    "end": "402240"
  },
  {
    "text": "and that means that we can perform a",
    "start": "402240",
    "end": "403759"
  },
  {
    "text": "rolling upgrade of our hash ring",
    "start": "403759",
    "end": "405280"
  },
  {
    "text": "whenever we need to",
    "start": "405280",
    "end": "406240"
  },
  {
    "text": "without having any loss of ingestion",
    "start": "406240",
    "end": "408000"
  },
  {
    "text": "capacity",
    "start": "408000",
    "end": "410080"
  },
  {
    "text": "if we want to tolerate having more nodes",
    "start": "410080",
    "end": "411680"
  },
  {
    "text": "at any given time all we have to do is",
    "start": "411680",
    "end": "413680"
  },
  {
    "text": "increase the replication factor so for",
    "start": "413680",
    "end": "415280"
  },
  {
    "text": "example",
    "start": "415280",
    "end": "415840"
  },
  {
    "text": "if we change the replication factor from",
    "start": "415840",
    "end": "417520"
  },
  {
    "text": "three to five replicas then we can",
    "start": "417520",
    "end": "419759"
  },
  {
    "text": "tolerate having two nodes down at any",
    "start": "419759",
    "end": "421199"
  },
  {
    "text": "given time",
    "start": "421199",
    "end": "423120"
  },
  {
    "text": "now we've designed our thunders receiver",
    "start": "423120",
    "end": "424800"
  },
  {
    "text": "to be both scalable and highly available",
    "start": "424800",
    "end": "427039"
  },
  {
    "text": "we're able to ingest data from thousands",
    "start": "427039",
    "end": "428560"
  },
  {
    "text": "of clients into our tsdb",
    "start": "428560",
    "end": "430960"
  },
  {
    "text": "but since we're ingesting so much data",
    "start": "430960",
    "end": "432479"
  },
  {
    "text": "into our tsdb storage is likely going to",
    "start": "432479",
    "end": "434479"
  },
  {
    "text": "become an issue on these nodes",
    "start": "434479",
    "end": "436000"
  },
  {
    "text": "we're going to be creating so many tsdb",
    "start": "436000",
    "end": "437840"
  },
  {
    "text": "blocks with so many samples and time",
    "start": "437840",
    "end": "439440"
  },
  {
    "text": "series",
    "start": "439440",
    "end": "439919"
  },
  {
    "text": "that it's going to occupy a lot of space",
    "start": "439919",
    "end": "441759"
  },
  {
    "text": "so storage is going to become a",
    "start": "441759",
    "end": "443599"
  },
  {
    "text": "bottleneck",
    "start": "443599",
    "end": "444639"
  },
  {
    "text": "the way that we solve this inside of",
    "start": "444639",
    "end": "445919"
  },
  {
    "text": "thanos is by uploading all",
    "start": "445919",
    "end": "448240"
  },
  {
    "text": "completed tsdb blocks into object",
    "start": "448240",
    "end": "450160"
  },
  {
    "text": "storage object storage is cheap",
    "start": "450160",
    "end": "452400"
  },
  {
    "text": "and essentially infinitely scalable so",
    "start": "452400",
    "end": "454560"
  },
  {
    "text": "we can upload all the data we want there",
    "start": "454560",
    "end": "457039"
  },
  {
    "text": "we later make this data queryable by",
    "start": "457039",
    "end": "460319"
  },
  {
    "text": "using a component called the thanos",
    "start": "460319",
    "end": "462080"
  },
  {
    "text": "store or the thunderstor gateway",
    "start": "462080",
    "end": "464160"
  },
  {
    "text": "this essentially exposes the same grpc",
    "start": "464160",
    "end": "466400"
  },
  {
    "text": "api for querying data",
    "start": "466400",
    "end": "468240"
  },
  {
    "text": "except instead of reading data from disk",
    "start": "468240",
    "end": "470160"
  },
  {
    "text": "or from a local tsdb",
    "start": "470160",
    "end": "471520"
  },
  {
    "text": "is reading it from object storage",
    "start": "471520",
    "end": "475039"
  },
  {
    "text": "until now we've seen the purpose that",
    "start": "475039",
    "end": "476479"
  },
  {
    "text": "the thunders receive component has",
    "start": "476479",
    "end": "478000"
  },
  {
    "text": "inside of the thanos project",
    "start": "478000",
    "end": "479599"
  },
  {
    "text": "we've also seen from a high level how",
    "start": "479599",
    "end": "481919"
  },
  {
    "text": "its hashing design allows us to run it",
    "start": "481919",
    "end": "483919"
  },
  {
    "text": "as a scalable and highly available",
    "start": "483919",
    "end": "485440"
  },
  {
    "text": "metrics ingestion system",
    "start": "485440",
    "end": "486960"
  },
  {
    "text": "but how do we actually run such a system",
    "start": "486960",
    "end": "489599"
  },
  {
    "text": "and how do we automate it so that it",
    "start": "489599",
    "end": "491120"
  },
  {
    "text": "scales horizontally in a platform like",
    "start": "491120",
    "end": "492560"
  },
  {
    "text": "kubernetes",
    "start": "492560",
    "end": "493360"
  },
  {
    "text": "let's take a look at the details we",
    "start": "493360",
    "end": "495440"
  },
  {
    "text": "mentioned earlier that the thunder's",
    "start": "495440",
    "end": "496639"
  },
  {
    "text": "received component can be configured",
    "start": "496639",
    "end": "497840"
  },
  {
    "text": "into hash rings",
    "start": "497840",
    "end": "499039"
  },
  {
    "text": "today this configuration works by using",
    "start": "499039",
    "end": "501280"
  },
  {
    "text": "a json file to explicitly map the",
    "start": "501280",
    "end": "503280"
  },
  {
    "text": "addresses of the different replicas to",
    "start": "503280",
    "end": "504960"
  },
  {
    "text": "distinct hash rings",
    "start": "504960",
    "end": "506639"
  },
  {
    "text": "unfortunately json is not a very",
    "start": "506639",
    "end": "508000"
  },
  {
    "text": "human-friendly language and we",
    "start": "508000",
    "end": "509280"
  },
  {
    "text": "definitely don't want to have to write",
    "start": "509280",
    "end": "510400"
  },
  {
    "text": "json by hand",
    "start": "510400",
    "end": "511520"
  },
  {
    "text": "every single time that we need to scale",
    "start": "511520",
    "end": "513200"
  },
  {
    "text": "our hash ring up or down",
    "start": "513200",
    "end": "514880"
  },
  {
    "text": "however thanos needs this configuration",
    "start": "514880",
    "end": "517039"
  },
  {
    "text": "file so that the replicas can discover",
    "start": "517039",
    "end": "518399"
  },
  {
    "text": "each other",
    "start": "518399",
    "end": "519039"
  },
  {
    "text": "because thanos doesn't know anything",
    "start": "519039",
    "end": "520399"
  },
  {
    "text": "about kubernetes or any other cloud or",
    "start": "520399",
    "end": "522320"
  },
  {
    "text": "container orchestration system",
    "start": "522320",
    "end": "523839"
  },
  {
    "text": "in fact this limitation is intentional",
    "start": "523839",
    "end": "526240"
  },
  {
    "text": "and actually allows us to keep the",
    "start": "526240",
    "end": "527440"
  },
  {
    "text": "thunders project simple and focused",
    "start": "527440",
    "end": "529680"
  },
  {
    "text": "because we don't have to put any",
    "start": "529680",
    "end": "531040"
  },
  {
    "text": "non-metrics domain logic in",
    "start": "531040",
    "end": "532959"
  },
  {
    "text": "the project but it also means that we",
    "start": "532959",
    "end": "535360"
  },
  {
    "text": "have to lean on other tools to automate",
    "start": "535360",
    "end": "536959"
  },
  {
    "text": "configuring our hash rings when found is",
    "start": "536959",
    "end": "538720"
  },
  {
    "text": "deployed onto the platform",
    "start": "538720",
    "end": "540399"
  },
  {
    "text": "fortunately we have an effective pattern",
    "start": "540399",
    "end": "542399"
  },
  {
    "text": "for solving these types of problems in",
    "start": "542399",
    "end": "543600"
  },
  {
    "text": "the kubernetes community namely",
    "start": "543600",
    "end": "545440"
  },
  {
    "text": "controllers and operators at red hat",
    "start": "545440",
    "end": "548560"
  },
  {
    "text": "we've written a small project called the",
    "start": "548560",
    "end": "549920"
  },
  {
    "text": "thunder's received controller",
    "start": "549920",
    "end": "551200"
  },
  {
    "text": "for the sole purpose of automating the",
    "start": "551200",
    "end": "553200"
  },
  {
    "text": "configuration of our hash rings",
    "start": "553200",
    "end": "554959"
  },
  {
    "text": "this kubernetes controller works by",
    "start": "554959",
    "end": "556560"
  },
  {
    "text": "watching all the staple sets that match",
    "start": "556560",
    "end": "558240"
  },
  {
    "text": "a specific label selector",
    "start": "558240",
    "end": "559760"
  },
  {
    "text": "whenever the replica count for one of",
    "start": "559760",
    "end": "561360"
  },
  {
    "text": "these staple sets containing thumbs",
    "start": "561360",
    "end": "562720"
  },
  {
    "text": "receivers",
    "start": "562720",
    "end": "563360"
  },
  {
    "text": "changes the controller regenerates the",
    "start": "563360",
    "end": "565680"
  },
  {
    "text": "hash ring configuration json file",
    "start": "565680",
    "end": "567680"
  },
  {
    "text": "and updates a configmap holding it the",
    "start": "567680",
    "end": "570399"
  },
  {
    "text": "thumbnails receive replicas in turn",
    "start": "570399",
    "end": "572000"
  },
  {
    "text": "watch the file system for changes to",
    "start": "572000",
    "end": "573519"
  },
  {
    "text": "their configuration so when the kubelet",
    "start": "573519",
    "end": "575440"
  },
  {
    "text": "updates the volume out for the",
    "start": "575440",
    "end": "576480"
  },
  {
    "text": "configuration file the thunders receive",
    "start": "576480",
    "end": "578399"
  },
  {
    "text": "replicas reload",
    "start": "578399",
    "end": "579680"
  },
  {
    "text": "and update their in-process",
    "start": "579680",
    "end": "581040"
  },
  {
    "text": "representation of the hash rings",
    "start": "581040",
    "end": "582720"
  },
  {
    "text": "to reflect the new state of the world",
    "start": "582720",
    "end": "585200"
  },
  {
    "text": "with this in mind let's consider the",
    "start": "585200",
    "end": "586800"
  },
  {
    "text": "simplest possible deployment",
    "start": "586800",
    "end": "588240"
  },
  {
    "text": "of authors receive hash ring that's",
    "start": "588240",
    "end": "590160"
  },
  {
    "text": "automated on kubernetes",
    "start": "590160",
    "end": "592080"
  },
  {
    "text": "the deployment would include one",
    "start": "592080",
    "end": "594399"
  },
  {
    "text": "thunders receive",
    "start": "594399",
    "end": "595279"
  },
  {
    "text": "controller replica managing the",
    "start": "595279",
    "end": "597519"
  },
  {
    "text": "configuration",
    "start": "597519",
    "end": "598399"
  },
  {
    "text": "for one single thunder receive hash ring",
    "start": "598399",
    "end": "600800"
  },
  {
    "text": "that contains only",
    "start": "600800",
    "end": "601839"
  },
  {
    "text": "one replica so admittedly this setup",
    "start": "601839",
    "end": "605120"
  },
  {
    "text": "might not seem like very much",
    "start": "605120",
    "end": "606560"
  },
  {
    "text": "but it's actually foundational for any",
    "start": "606560",
    "end": "608399"
  },
  {
    "text": "more complicated and more",
    "start": "608399",
    "end": "609760"
  },
  {
    "text": "capable setups that we'll deploy in the",
    "start": "609760",
    "end": "611680"
  },
  {
    "text": "future",
    "start": "611680",
    "end": "613120"
  },
  {
    "text": "it's also true that the stack probably",
    "start": "613120",
    "end": "614720"
  },
  {
    "text": "won't handle very much load",
    "start": "614720",
    "end": "616399"
  },
  {
    "text": "but with one single indication of a",
    "start": "616399",
    "end": "618640"
  },
  {
    "text": "kubectl scale command",
    "start": "618640",
    "end": "620240"
  },
  {
    "text": "we can take this single replica hash",
    "start": "620240",
    "end": "622160"
  },
  {
    "text": "ring to three or five",
    "start": "622160",
    "end": "623839"
  },
  {
    "text": "or twenty replicas and really handle as",
    "start": "623839",
    "end": "626000"
  },
  {
    "text": "much load as we want to",
    "start": "626000",
    "end": "628959"
  },
  {
    "text": "now let's try to do exactly this in a",
    "start": "630000",
    "end": "631600"
  },
  {
    "text": "quick demo let's deploy this super",
    "start": "631600",
    "end": "633839"
  },
  {
    "text": "simple automated thought and receive",
    "start": "633839",
    "end": "635200"
  },
  {
    "text": "hash ring",
    "start": "635200",
    "end": "635839"
  },
  {
    "text": "onto a kubernetes cluster we'll then run",
    "start": "635839",
    "end": "638320"
  },
  {
    "text": "one process next to this",
    "start": "638320",
    "end": "639839"
  },
  {
    "text": "that produces metrics in the remote",
    "start": "639839",
    "end": "641519"
  },
  {
    "text": "write api format",
    "start": "641519",
    "end": "642959"
  },
  {
    "text": "and sends them to the hash ring for",
    "start": "642959",
    "end": "644720"
  },
  {
    "text": "ingestion by the way",
    "start": "644720",
    "end": "646720"
  },
  {
    "text": "all of the commands instructions and",
    "start": "646720",
    "end": "648240"
  },
  {
    "text": "manifest that i'm using these demos for",
    "start": "648240",
    "end": "649680"
  },
  {
    "text": "my talk",
    "start": "649680",
    "end": "650399"
  },
  {
    "text": "can be found on my github in a repo",
    "start": "650399",
    "end": "652320"
  },
  {
    "text": "called kubecon eu 2020",
    "start": "652320",
    "end": "655920"
  },
  {
    "text": "what we're going to do is deploy authors",
    "start": "656000",
    "end": "657920"
  },
  {
    "text": "receive hash ring and also a thunderous",
    "start": "657920",
    "end": "660000"
  },
  {
    "text": "receive controller",
    "start": "660000",
    "end": "660959"
  },
  {
    "text": "along with it to configure the hash ring",
    "start": "660959",
    "end": "662959"
  },
  {
    "text": "so",
    "start": "662959",
    "end": "664000"
  },
  {
    "text": "let's begin by creating the namespace",
    "start": "664000",
    "end": "665760"
  },
  {
    "text": "for our cluster this is going to be",
    "start": "665760",
    "end": "668800"
  },
  {
    "text": "the founder's namespace and now",
    "start": "668800",
    "end": "672480"
  },
  {
    "text": "we're going to create all the manifest",
    "start": "672480",
    "end": "674160"
  },
  {
    "text": "that we have in the directory for",
    "start": "674160",
    "end": "675920"
  },
  {
    "text": "our first demo",
    "start": "675920",
    "end": "678639"
  },
  {
    "text": "all right great now let's take a look at",
    "start": "684480",
    "end": "686240"
  },
  {
    "text": "what manifests we actually deployed",
    "start": "686240",
    "end": "688000"
  },
  {
    "text": "we have a bunch of manifest",
    "start": "688000",
    "end": "689040"
  },
  {
    "text": "corresponding to the thunder's receive",
    "start": "689040",
    "end": "690399"
  },
  {
    "text": "controller itself",
    "start": "690399",
    "end": "691519"
  },
  {
    "text": "and then some manifest corresponding at",
    "start": "691519",
    "end": "693040"
  },
  {
    "text": "the bottom to",
    "start": "693040",
    "end": "694800"
  },
  {
    "text": "the thunder receive hash ring for our",
    "start": "694800",
    "end": "696800"
  },
  {
    "text": "default hash ring",
    "start": "696800",
    "end": "698320"
  },
  {
    "text": "here we have a staple set with one",
    "start": "698320",
    "end": "700959"
  },
  {
    "text": "single replica so when we take a look at",
    "start": "700959",
    "end": "702640"
  },
  {
    "text": "the pods we have",
    "start": "702640",
    "end": "703760"
  },
  {
    "text": "one single thunder's received pod in the",
    "start": "703760",
    "end": "705760"
  },
  {
    "text": "default",
    "start": "705760",
    "end": "706800"
  },
  {
    "text": "hash ring and we also have the funnels",
    "start": "706800",
    "end": "708560"
  },
  {
    "text": "receive controller pi",
    "start": "708560",
    "end": "710480"
  },
  {
    "text": "let's take a look at the configuration",
    "start": "710480",
    "end": "711680"
  },
  {
    "text": "that the thunders receive controller",
    "start": "711680",
    "end": "713360"
  },
  {
    "text": "generated for us",
    "start": "713360",
    "end": "714560"
  },
  {
    "text": "you can see that from the one pod that",
    "start": "714560",
    "end": "716959"
  },
  {
    "text": "we had in our staple set",
    "start": "716959",
    "end": "718320"
  },
  {
    "text": "we generated one endpoint for our hash",
    "start": "718320",
    "end": "720240"
  },
  {
    "text": "ring perfect",
    "start": "720240",
    "end": "723600"
  },
  {
    "text": "now let's run a second process in",
    "start": "723600",
    "end": "725040"
  },
  {
    "text": "another terminal where",
    "start": "725040",
    "end": "726800"
  },
  {
    "text": "we're actually going to be generating",
    "start": "726800",
    "end": "727839"
  },
  {
    "text": "some metrics to ingest in the hash ring",
    "start": "727839",
    "end": "729440"
  },
  {
    "text": "here we're going to be using",
    "start": "729440",
    "end": "730639"
  },
  {
    "text": "a binary that we wrote at red hat called",
    "start": "730639",
    "end": "732320"
  },
  {
    "text": "up that simply sends",
    "start": "732320",
    "end": "733839"
  },
  {
    "text": "one single metric an up metric to an",
    "start": "733839",
    "end": "736399"
  },
  {
    "text": "endpoint that you specify",
    "start": "736399",
    "end": "737839"
  },
  {
    "text": "via the remote write api",
    "start": "737839",
    "end": "741120"
  },
  {
    "text": "and now let's scale up our hash ring",
    "start": "742560",
    "end": "744720"
  },
  {
    "text": "from one replica to three replicas",
    "start": "744720",
    "end": "748800"
  },
  {
    "text": "let's take a look at the pods and it",
    "start": "748800",
    "end": "751120"
  },
  {
    "text": "seems like everything rolled out",
    "start": "751120",
    "end": "752399"
  },
  {
    "text": "perfectly",
    "start": "752399",
    "end": "754560"
  },
  {
    "text": "and during this rollout process we",
    "start": "754560",
    "end": "756399"
  },
  {
    "text": "didn't drop any remote write requests it",
    "start": "756399",
    "end": "758240"
  },
  {
    "text": "seems like",
    "start": "758240",
    "end": "759839"
  },
  {
    "text": "so now let's look at the hash ring",
    "start": "759839",
    "end": "761040"
  },
  {
    "text": "configuration that was generated by the",
    "start": "761040",
    "end": "762320"
  },
  {
    "text": "receive controller",
    "start": "762320",
    "end": "763680"
  },
  {
    "text": "and as you can see now that we have",
    "start": "763680",
    "end": "765519"
  },
  {
    "text": "three replicas the receive controller",
    "start": "765519",
    "end": "767680"
  },
  {
    "text": "updated our hashing configuration and",
    "start": "767680",
    "end": "769680"
  },
  {
    "text": "our endpoints array now has three",
    "start": "769680",
    "end": "771200"
  },
  {
    "text": "endpoints in it",
    "start": "771200",
    "end": "772480"
  },
  {
    "text": "exactly what we expected and now when we",
    "start": "772480",
    "end": "774480"
  },
  {
    "text": "quit the up binary",
    "start": "774480",
    "end": "775839"
  },
  {
    "text": "we can take a look at some statistics",
    "start": "775839",
    "end": "777440"
  },
  {
    "text": "and we can find that we send 100 remote",
    "start": "777440",
    "end": "779519"
  },
  {
    "text": "write requests we send one request per",
    "start": "779519",
    "end": "781040"
  },
  {
    "text": "second",
    "start": "781040",
    "end": "782240"
  },
  {
    "text": "we just saw how we were able to",
    "start": "782240",
    "end": "783519"
  },
  {
    "text": "effortlessly deploy a hash from onto",
    "start": "783519",
    "end": "784959"
  },
  {
    "text": "kubernetes and then with a single",
    "start": "784959",
    "end": "786160"
  },
  {
    "text": "command",
    "start": "786160",
    "end": "786880"
  },
  {
    "text": "scale that hash ring up for example to",
    "start": "786880",
    "end": "788560"
  },
  {
    "text": "handle increased traffic",
    "start": "788560",
    "end": "791200"
  },
  {
    "text": "i think that these kind of easy wins are",
    "start": "791200",
    "end": "793760"
  },
  {
    "text": "oftentimes underrated",
    "start": "793760",
    "end": "795120"
  },
  {
    "text": "in fact inside a red hat we're running",
    "start": "795120",
    "end": "798000"
  },
  {
    "text": "essentially exactly",
    "start": "798000",
    "end": "799200"
  },
  {
    "text": "this same stack as part of the openshift",
    "start": "799200",
    "end": "801040"
  },
  {
    "text": "monitoring platform",
    "start": "801040",
    "end": "802399"
  },
  {
    "text": "one of the services that we run is a",
    "start": "802399",
    "end": "805519"
  },
  {
    "text": "thunders receive hash stream with six",
    "start": "805519",
    "end": "807120"
  },
  {
    "text": "replicas that ingest metrics from",
    "start": "807120",
    "end": "810240"
  },
  {
    "text": "on the order of ten thousand openshift",
    "start": "810240",
    "end": "812000"
  },
  {
    "text": "clusters these openshift clusters",
    "start": "812000",
    "end": "814240"
  },
  {
    "text": "report data to this platform regularly",
    "start": "814240",
    "end": "817120"
  },
  {
    "text": "throughout the day",
    "start": "817120",
    "end": "817920"
  },
  {
    "text": "so that we can identify and alert if",
    "start": "817920",
    "end": "820720"
  },
  {
    "text": "these",
    "start": "820720",
    "end": "821279"
  },
  {
    "text": "platforms if these clusters are",
    "start": "821279",
    "end": "822959"
  },
  {
    "text": "experiencing any issues for example",
    "start": "822959",
    "end": "825120"
  },
  {
    "text": "something wrong with the control plane",
    "start": "825120",
    "end": "826399"
  },
  {
    "text": "or something going on with the nodes",
    "start": "826399",
    "end": "829040"
  },
  {
    "text": "of course since we're the observability",
    "start": "829040",
    "end": "830639"
  },
  {
    "text": "team we closely monitor all of the",
    "start": "830639",
    "end": "832160"
  },
  {
    "text": "services that we run",
    "start": "832160",
    "end": "833199"
  },
  {
    "text": "so i was pretty easily able to get some",
    "start": "833199",
    "end": "834720"
  },
  {
    "text": "statistics around the amount of data",
    "start": "834720",
    "end": "836880"
  },
  {
    "text": "that we're ingesting",
    "start": "836880",
    "end": "838320"
  },
  {
    "text": "as of late july when i recorded this",
    "start": "838320",
    "end": "839760"
  },
  {
    "text": "talk we're regularly handling around",
    "start": "839760",
    "end": "842320"
  },
  {
    "text": "10 million active time series and we're",
    "start": "842320",
    "end": "845040"
  },
  {
    "text": "ingesting",
    "start": "845040",
    "end": "845839"
  },
  {
    "text": "on the order of 30 000 samples per",
    "start": "845839",
    "end": "848000"
  },
  {
    "text": "second so that's actually pretty low",
    "start": "848000",
    "end": "849920"
  },
  {
    "text": "frequency data for the amount of time",
    "start": "849920",
    "end": "851680"
  },
  {
    "text": "series that we have",
    "start": "851680",
    "end": "852880"
  },
  {
    "text": "and that's because each of the clusters",
    "start": "852880",
    "end": "854720"
  },
  {
    "text": "that reports data to our platform",
    "start": "854720",
    "end": "856720"
  },
  {
    "text": "only sends data about once every five",
    "start": "856720",
    "end": "858560"
  },
  {
    "text": "minutes for a pretty limited",
    "start": "858560",
    "end": "860160"
  },
  {
    "text": "set of time series in the case of this",
    "start": "860160",
    "end": "863839"
  },
  {
    "text": "openshift monitoring production service",
    "start": "863839",
    "end": "865600"
  },
  {
    "text": "that we're running",
    "start": "865600",
    "end": "866720"
  },
  {
    "text": "we're able to pretty accurately estimate",
    "start": "866720",
    "end": "868480"
  },
  {
    "text": "the load that we're going to have this",
    "start": "868480",
    "end": "869600"
  },
  {
    "text": "week next week in a month",
    "start": "869600",
    "end": "870959"
  },
  {
    "text": "or in a quarter and this means that",
    "start": "870959",
    "end": "872800"
  },
  {
    "text": "we're able to plan the capacity",
    "start": "872800",
    "end": "874320"
  },
  {
    "text": "effectively for the size of our",
    "start": "874320",
    "end": "875920"
  },
  {
    "text": "ingestion hash ring appropriately",
    "start": "875920",
    "end": "877839"
  },
  {
    "text": "this is possible because we have a good",
    "start": "877839",
    "end": "879680"
  },
  {
    "text": "idea of how many openshift clusters",
    "start": "879680",
    "end": "881279"
  },
  {
    "text": "there was going to be in the wild",
    "start": "881279",
    "end": "882399"
  },
  {
    "text": "and we know exactly which time series",
    "start": "882399",
    "end": "884160"
  },
  {
    "text": "these clusters will be sending and how",
    "start": "884160",
    "end": "885680"
  },
  {
    "text": "often",
    "start": "885680",
    "end": "886560"
  },
  {
    "text": "but nevertheless every few months we",
    "start": "886560",
    "end": "889040"
  },
  {
    "text": "need to resize our cluster",
    "start": "889040",
    "end": "890800"
  },
  {
    "text": "resize the hash ring in order to",
    "start": "890800",
    "end": "892560"
  },
  {
    "text": "accommodate the increased load",
    "start": "892560",
    "end": "894079"
  },
  {
    "text": "which means that we have unnecessary",
    "start": "894079",
    "end": "895680"
  },
  {
    "text": "toilet for engineers and this problem is",
    "start": "895680",
    "end": "898000"
  },
  {
    "text": "even more pronounced",
    "start": "898000",
    "end": "899600"
  },
  {
    "text": "for workloads that are spikier in nature",
    "start": "899600",
    "end": "901760"
  },
  {
    "text": "than ours are",
    "start": "901760",
    "end": "902720"
  },
  {
    "text": "so imagine that the amount of time",
    "start": "902720",
    "end": "905120"
  },
  {
    "text": "series and samples",
    "start": "905120",
    "end": "906160"
  },
  {
    "text": "that a hashrate needs to ingest are",
    "start": "906160",
    "end": "908720"
  },
  {
    "text": "varying wildly",
    "start": "908720",
    "end": "909680"
  },
  {
    "text": "from peak to trough this kind of",
    "start": "909680",
    "end": "912320"
  },
  {
    "text": "volatility",
    "start": "912320",
    "end": "913279"
  },
  {
    "text": "means that it's going to be really",
    "start": "913279",
    "end": "914560"
  },
  {
    "text": "difficult to appropriately size an",
    "start": "914560",
    "end": "915920"
  },
  {
    "text": "ingestion service",
    "start": "915920",
    "end": "917199"
  },
  {
    "text": "so without good automation in these",
    "start": "917199",
    "end": "919279"
  },
  {
    "text": "cases the best option that we have",
    "start": "919279",
    "end": "921040"
  },
  {
    "text": "is to over provision the service so",
    "start": "921040",
    "end": "922639"
  },
  {
    "text": "that's able to handle the highest peaks",
    "start": "922639",
    "end": "925120"
  },
  {
    "text": "but obviously this has a downside of",
    "start": "925120",
    "end": "926639"
  },
  {
    "text": "causing the service to be really under",
    "start": "926639",
    "end": "928240"
  },
  {
    "text": "utilized",
    "start": "928240",
    "end": "929120"
  },
  {
    "text": "in times of low traffic so um",
    "start": "929120",
    "end": "932399"
  },
  {
    "text": "we're gonna be wasting both resources",
    "start": "932399",
    "end": "934160"
  },
  {
    "text": "and that means we're gonna be wasting",
    "start": "934160",
    "end": "935600"
  },
  {
    "text": "money as well",
    "start": "935600",
    "end": "937199"
  },
  {
    "text": "in the case of the thunders receive",
    "start": "937199",
    "end": "938320"
  },
  {
    "text": "controller we can use the horizontal pod",
    "start": "938320",
    "end": "940240"
  },
  {
    "text": "auto scaler",
    "start": "940240",
    "end": "941040"
  },
  {
    "text": "to scale the hash ring up when the hash",
    "start": "941040",
    "end": "943120"
  },
  {
    "text": "ring is",
    "start": "943120",
    "end": "944160"
  },
  {
    "text": "swamped and using too much cpu and then",
    "start": "944160",
    "end": "946720"
  },
  {
    "text": "we can use the thunder's receipt",
    "start": "946720",
    "end": "948079"
  },
  {
    "text": "controller to automatically regenerate",
    "start": "948079",
    "end": "949839"
  },
  {
    "text": "the configuration for this hash ring",
    "start": "949839",
    "end": "951519"
  },
  {
    "text": "and then have everything working for us",
    "start": "951519",
    "end": "953279"
  },
  {
    "text": "just out of the box",
    "start": "953279",
    "end": "954639"
  },
  {
    "text": "let's try running this exact",
    "start": "954639",
    "end": "955680"
  },
  {
    "text": "configuration of an auto scaling",
    "start": "955680",
    "end": "957199"
  },
  {
    "text": "thunders receive hash ring",
    "start": "957199",
    "end": "958480"
  },
  {
    "text": "in a new demo this is going to look a",
    "start": "958480",
    "end": "960240"
  },
  {
    "text": "lot like the first demo where we had",
    "start": "960240",
    "end": "962240"
  },
  {
    "text": "the thunder receive controller",
    "start": "962240",
    "end": "964160"
  },
  {
    "text": "generating the configuration for",
    "start": "964160",
    "end": "965440"
  },
  {
    "text": "authorized hashing",
    "start": "965440",
    "end": "966720"
  },
  {
    "text": "but with the added change that the hash",
    "start": "966720",
    "end": "969040"
  },
  {
    "text": "ring is going to scale",
    "start": "969040",
    "end": "970000"
  },
  {
    "text": "automatically when we're increasing the",
    "start": "970000",
    "end": "971839"
  },
  {
    "text": "load on it using the horizontal pod",
    "start": "971839",
    "end": "973920"
  },
  {
    "text": "autoscaler",
    "start": "973920",
    "end": "974639"
  },
  {
    "text": "the horizontal pod autoscaler in turn is",
    "start": "974639",
    "end": "976399"
  },
  {
    "text": "going to depend on the availability of",
    "start": "976399",
    "end": "978079"
  },
  {
    "text": "the resource metrics api in the cluster",
    "start": "978079",
    "end": "980160"
  },
  {
    "text": "so in our case we're going to be using",
    "start": "980160",
    "end": "981920"
  },
  {
    "text": "the kubernetes metric server to provide",
    "start": "981920",
    "end": "983519"
  },
  {
    "text": "that",
    "start": "983519",
    "end": "985360"
  },
  {
    "text": "and another thing we're going to be",
    "start": "985360",
    "end": "986959"
  },
  {
    "text": "doing here is we're going to be",
    "start": "986959",
    "end": "988000"
  },
  {
    "text": "generating a lot more load than before",
    "start": "988000",
    "end": "990000"
  },
  {
    "text": "we'll be sending hundreds of thousands",
    "start": "990000",
    "end": "991920"
  },
  {
    "text": "of time series",
    "start": "991920",
    "end": "993040"
  },
  {
    "text": "that are going to be changing um every",
    "start": "993040",
    "end": "995120"
  },
  {
    "text": "few seconds",
    "start": "995120",
    "end": "996959"
  },
  {
    "text": "to really stretch this hash ring a",
    "start": "996959",
    "end": "999440"
  },
  {
    "text": "little bit more to its limit",
    "start": "999440",
    "end": "1000959"
  },
  {
    "text": "and see how it stabilizes and how it",
    "start": "1000959",
    "end": "1003360"
  },
  {
    "text": "performs",
    "start": "1003360",
    "end": "1004079"
  },
  {
    "text": "all right let's begin again by creating",
    "start": "1004079",
    "end": "1005759"
  },
  {
    "text": "the thunder's namespace",
    "start": "1005759",
    "end": "1007600"
  },
  {
    "text": "and then we're going to deploy all the",
    "start": "1007600",
    "end": "1009279"
  },
  {
    "text": "resources in the",
    "start": "1009279",
    "end": "1010959"
  },
  {
    "text": "directory for our second demo and you",
    "start": "1010959",
    "end": "1013680"
  },
  {
    "text": "can see a lot of these look like before",
    "start": "1013680",
    "end": "1015279"
  },
  {
    "text": "but let's take a look at the few things",
    "start": "1015279",
    "end": "1016639"
  },
  {
    "text": "that are different so namely let's look",
    "start": "1016639",
    "end": "1017920"
  },
  {
    "text": "at the thunder receive staple set",
    "start": "1017920",
    "end": "1019600"
  },
  {
    "text": "and one of the things we can see here is",
    "start": "1019600",
    "end": "1020959"
  },
  {
    "text": "that we're deploying a new sidecar",
    "start": "1020959",
    "end": "1022720"
  },
  {
    "text": "inside of the thunder received pod this",
    "start": "1022720",
    "end": "1025038"
  },
  {
    "text": "is a side car called",
    "start": "1025039",
    "end": "1026558"
  },
  {
    "text": "configmap to disk and the purpose of",
    "start": "1026559",
    "end": "1028160"
  },
  {
    "text": "this is to automatically synchronize the",
    "start": "1028160",
    "end": "1029918"
  },
  {
    "text": "configmap that's in the api",
    "start": "1029919",
    "end": "1031520"
  },
  {
    "text": "to a phylon disk instead of avoiding the",
    "start": "1031520",
    "end": "1034558"
  },
  {
    "text": "long resync loop that it might take when",
    "start": "1034559",
    "end": "1036319"
  },
  {
    "text": "the kubelet has to",
    "start": "1036319",
    "end": "1037678"
  },
  {
    "text": "update the volume mount um this",
    "start": "1037679",
    "end": "1039760"
  },
  {
    "text": "sometimes when a",
    "start": "1039760",
    "end": "1040880"
  },
  {
    "text": "big amount changes in the api can take a",
    "start": "1040880",
    "end": "1042480"
  },
  {
    "text": "long time for it to be reflected on disk",
    "start": "1042480",
    "end": "1043760"
  },
  {
    "text": "so here",
    "start": "1043760",
    "end": "1044480"
  },
  {
    "text": "we're just trying to have the",
    "start": "1044480",
    "end": "1045520"
  },
  {
    "text": "hashing.json configuration file update a",
    "start": "1045520",
    "end": "1047360"
  },
  {
    "text": "little bit faster",
    "start": "1047360",
    "end": "1048558"
  },
  {
    "text": "the next thing that's different in this",
    "start": "1048559",
    "end": "1049520"
  },
  {
    "text": "demo is that we're deploying a",
    "start": "1049520",
    "end": "1050640"
  },
  {
    "text": "horizontal pod autoscaler resource",
    "start": "1050640",
    "end": "1052960"
  },
  {
    "text": "and here we're setting the minimum",
    "start": "1052960",
    "end": "1055280"
  },
  {
    "text": "replica count to",
    "start": "1055280",
    "end": "1056400"
  },
  {
    "text": "five replicas this means that we're",
    "start": "1056400",
    "end": "1058160"
  },
  {
    "text": "always going to have at least some",
    "start": "1058160",
    "end": "1059360"
  },
  {
    "text": "ingestion capacity",
    "start": "1059360",
    "end": "1060720"
  },
  {
    "text": "in our hash ring and that means at a",
    "start": "1060720",
    "end": "1062720"
  },
  {
    "text": "minimum five replicas",
    "start": "1062720",
    "end": "1064320"
  },
  {
    "text": "the other thing is notable here is that",
    "start": "1064320",
    "end": "1065520"
  },
  {
    "text": "we're scaling based on the average cpu",
    "start": "1065520",
    "end": "1067280"
  },
  {
    "text": "utilization of the hash ring so",
    "start": "1067280",
    "end": "1069200"
  },
  {
    "text": "when the average utilization of all the",
    "start": "1069200",
    "end": "1070640"
  },
  {
    "text": "pods in the hash ring reaches 50",
    "start": "1070640",
    "end": "1072400"
  },
  {
    "text": "of the cpu that they requested then we",
    "start": "1072400",
    "end": "1074720"
  },
  {
    "text": "scale up",
    "start": "1074720",
    "end": "1075919"
  },
  {
    "text": "let's take a look at all the pods we",
    "start": "1075919",
    "end": "1077120"
  },
  {
    "text": "have and everything rolled out correctly",
    "start": "1077120",
    "end": "1078559"
  },
  {
    "text": "we can also",
    "start": "1078559",
    "end": "1079280"
  },
  {
    "text": "see that the configuration generated by",
    "start": "1079280",
    "end": "1081440"
  },
  {
    "text": "the final receive controller should be",
    "start": "1081440",
    "end": "1083120"
  },
  {
    "text": "there",
    "start": "1083120",
    "end": "1083679"
  },
  {
    "text": "this looks good so let's take a look at",
    "start": "1083679",
    "end": "1084960"
  },
  {
    "text": "the pods all right this looks good",
    "start": "1084960",
    "end": "1087360"
  },
  {
    "text": "so let's run the second terminal of",
    "start": "1087360",
    "end": "1088720"
  },
  {
    "text": "process is just watching",
    "start": "1088720",
    "end": "1090400"
  },
  {
    "text": "the horizontal pod auto scaling resource",
    "start": "1090400",
    "end": "1092080"
  },
  {
    "text": "so we can see the decisions",
    "start": "1092080",
    "end": "1093440"
  },
  {
    "text": "that it's going to be making in real",
    "start": "1093440",
    "end": "1094559"
  },
  {
    "text": "time and now in one last terminal we'll",
    "start": "1094559",
    "end": "1096960"
  },
  {
    "text": "run",
    "start": "1096960",
    "end": "1097520"
  },
  {
    "text": "a process to generate load against our",
    "start": "1097520",
    "end": "1099679"
  },
  {
    "text": "hash ring in this case we're going to be",
    "start": "1099679",
    "end": "1101280"
  },
  {
    "text": "using fresh traxxas",
    "start": "1101280",
    "end": "1102720"
  },
  {
    "text": "avalanche project which is",
    "start": "1102720",
    "end": "1105760"
  },
  {
    "text": "a binary that generates remote write",
    "start": "1105760",
    "end": "1107600"
  },
  {
    "text": "requests",
    "start": "1107600",
    "end": "1108799"
  },
  {
    "text": "against some endpoint that you choose",
    "start": "1108799",
    "end": "1112000"
  },
  {
    "text": "one thing to note here is that we're",
    "start": "1112000",
    "end": "1113520"
  },
  {
    "text": "using a fork that i made",
    "start": "1113520",
    "end": "1115120"
  },
  {
    "text": "that's very tellingly called log errors",
    "start": "1115120",
    "end": "1118799"
  },
  {
    "text": "so the original binary when it reaches",
    "start": "1118799",
    "end": "1121600"
  },
  {
    "text": "20",
    "start": "1121600",
    "end": "1122080"
  },
  {
    "text": "errors it just exits and in this case",
    "start": "1122080",
    "end": "1124240"
  },
  {
    "text": "the um we want to avoid",
    "start": "1124240",
    "end": "1125840"
  },
  {
    "text": "exiting and just keep logging here so",
    "start": "1125840",
    "end": "1127280"
  },
  {
    "text": "that should serve a little bit as an",
    "start": "1127280",
    "end": "1128400"
  },
  {
    "text": "indication",
    "start": "1128400",
    "end": "1129200"
  },
  {
    "text": "of where this demo is going to go",
    "start": "1129200",
    "end": "1131280"
  },
  {
    "text": "another thing to note here is that we're",
    "start": "1131280",
    "end": "1132559"
  },
  {
    "text": "going to be deploying a thousand metrics",
    "start": "1132559",
    "end": "1134480"
  },
  {
    "text": "each with 100 different time series",
    "start": "1134480",
    "end": "1137280"
  },
  {
    "text": "pretty soon the horizontal auto scaler",
    "start": "1137280",
    "end": "1139360"
  },
  {
    "text": "scaled the hash ring up to seven nodes",
    "start": "1139360",
    "end": "1141520"
  },
  {
    "text": "and when this occurred",
    "start": "1141520",
    "end": "1142799"
  },
  {
    "text": "the process is generating the load",
    "start": "1142799",
    "end": "1145280"
  },
  {
    "text": "against our hash ring",
    "start": "1145280",
    "end": "1146320"
  },
  {
    "text": "started logging a lot of errors this",
    "start": "1146320",
    "end": "1147600"
  },
  {
    "text": "means that our hash ring is in some kind",
    "start": "1147600",
    "end": "1149039"
  },
  {
    "text": "of",
    "start": "1149039",
    "end": "1149440"
  },
  {
    "text": "unstable state soon after that",
    "start": "1149440",
    "end": "1152960"
  },
  {
    "text": "the pot auto scaler scales once again",
    "start": "1152960",
    "end": "1155919"
  },
  {
    "text": "the hash ring up to",
    "start": "1155919",
    "end": "1157520"
  },
  {
    "text": "the next 10 minutes of this the next 10",
    "start": "1157520",
    "end": "1159600"
  },
  {
    "text": "minutes of the demo are pretty",
    "start": "1159600",
    "end": "1160480"
  },
  {
    "text": "uneventful",
    "start": "1160480",
    "end": "1161280"
  },
  {
    "text": "the hash ring is super stable at nine",
    "start": "1161280",
    "end": "1163280"
  },
  {
    "text": "pods and we're not generating any errors",
    "start": "1163280",
    "end": "1165520"
  },
  {
    "text": "so let's fast forward a little bit in",
    "start": "1165520",
    "end": "1168400"
  },
  {
    "text": "the top screen we're now running cube",
    "start": "1168400",
    "end": "1169679"
  },
  {
    "text": "ctl top",
    "start": "1169679",
    "end": "1170559"
  },
  {
    "text": "to see in real time how much cpu and",
    "start": "1170559",
    "end": "1172799"
  },
  {
    "text": "memory each of the thunder c pods is",
    "start": "1172799",
    "end": "1174480"
  },
  {
    "text": "using",
    "start": "1174480",
    "end": "1175280"
  },
  {
    "text": "one thing that we can see is that",
    "start": "1175280",
    "end": "1176240"
  },
  {
    "text": "whenever the process that generates load",
    "start": "1176240",
    "end": "1179360"
  },
  {
    "text": "changes the time series that it's",
    "start": "1179360",
    "end": "1181039"
  },
  {
    "text": "sending the characteristics",
    "start": "1181039",
    "end": "1183360"
  },
  {
    "text": "of the cpu utilization change as well",
    "start": "1183360",
    "end": "1185840"
  },
  {
    "text": "the reason for this is because",
    "start": "1185840",
    "end": "1187760"
  },
  {
    "text": "every single time that we change the",
    "start": "1187760",
    "end": "1189760"
  },
  {
    "text": "time stages that we're sending",
    "start": "1189760",
    "end": "1191039"
  },
  {
    "text": "we also change which final",
    "start": "1191039",
    "end": "1194480"
  },
  {
    "text": "thunder receive replicas these hash",
    "start": "1194480",
    "end": "1195840"
  },
  {
    "text": "rings end up on so it means that",
    "start": "1195840",
    "end": "1197520"
  },
  {
    "text": "the characteristics of which pods",
    "start": "1197520",
    "end": "1200640"
  },
  {
    "text": "receive how many metrics",
    "start": "1200640",
    "end": "1202400"
  },
  {
    "text": "and which pods have to forward metrics",
    "start": "1202400",
    "end": "1205039"
  },
  {
    "text": "to which other pods",
    "start": "1205039",
    "end": "1206000"
  },
  {
    "text": "changes so we might in sometimes have a",
    "start": "1206000",
    "end": "1208720"
  },
  {
    "text": "very smooth distribution of metrics and",
    "start": "1208720",
    "end": "1210480"
  },
  {
    "text": "other times we might have",
    "start": "1210480",
    "end": "1211520"
  },
  {
    "text": "very lumpy distribution of metrics where",
    "start": "1211520",
    "end": "1213440"
  },
  {
    "text": "a lot of time series get clumped up on",
    "start": "1213440",
    "end": "1215360"
  },
  {
    "text": "one node now at around 23 minutes",
    "start": "1215360",
    "end": "1218559"
  },
  {
    "text": "the avalanche process reconfigures",
    "start": "1218559",
    "end": "1220400"
  },
  {
    "text": "itself and",
    "start": "1220400",
    "end": "1221840"
  },
  {
    "text": "it seems that the distribution of time",
    "start": "1221840",
    "end": "1223200"
  },
  {
    "text": "series in this case is causing",
    "start": "1223200",
    "end": "1225440"
  },
  {
    "text": "a really uneven distribution of metrics",
    "start": "1225440",
    "end": "1227919"
  },
  {
    "text": "and",
    "start": "1227919",
    "end": "1228880"
  },
  {
    "text": "increasing the cpu utilization of the",
    "start": "1228880",
    "end": "1230640"
  },
  {
    "text": "hash ring this is",
    "start": "1230640",
    "end": "1232080"
  },
  {
    "text": "causing the hashing to become unstable",
    "start": "1232080",
    "end": "1235120"
  },
  {
    "text": "and is going to require a scaling event",
    "start": "1235120",
    "end": "1238240"
  },
  {
    "text": "now when this hash ring scales up we",
    "start": "1238240",
    "end": "1240960"
  },
  {
    "text": "further destabilize",
    "start": "1240960",
    "end": "1242640"
  },
  {
    "text": "the hash ring because some of the nodes",
    "start": "1242640",
    "end": "1245200"
  },
  {
    "text": "are unavailable for some amount of time",
    "start": "1245200",
    "end": "1246880"
  },
  {
    "text": "which causes more errors in the hash",
    "start": "1246880",
    "end": "1248559"
  },
  {
    "text": "ring causing more seat utilization et",
    "start": "1248559",
    "end": "1250400"
  },
  {
    "text": "cetera et cetera",
    "start": "1250400",
    "end": "1252640"
  },
  {
    "text": "now when this occurs we end up scaling",
    "start": "1252640",
    "end": "1255840"
  },
  {
    "text": "the cluster up all the way to 20",
    "start": "1255840",
    "end": "1257200"
  },
  {
    "text": "replicas which is actually the max that",
    "start": "1257200",
    "end": "1258480"
  },
  {
    "text": "we set on our horizontal power autoscale",
    "start": "1258480",
    "end": "1260080"
  },
  {
    "text": "resource",
    "start": "1260080",
    "end": "1261120"
  },
  {
    "text": "and at this point the pods will",
    "start": "1261120",
    "end": "1263200"
  },
  {
    "text": "eventually stabilize",
    "start": "1263200",
    "end": "1264559"
  },
  {
    "text": "and the seep utilization we can expect",
    "start": "1264559",
    "end": "1266080"
  },
  {
    "text": "it to drop",
    "start": "1266080",
    "end": "1268559"
  },
  {
    "text": "around the 31 minute mark the hash ring",
    "start": "1270000",
    "end": "1272240"
  },
  {
    "text": "finally stabilizes",
    "start": "1272240",
    "end": "1273679"
  },
  {
    "text": "and the cpu utilization will begin to",
    "start": "1273679",
    "end": "1275520"
  },
  {
    "text": "drop",
    "start": "1275520",
    "end": "1276960"
  },
  {
    "text": "now the separation will keep dropping so",
    "start": "1276960",
    "end": "1280000"
  },
  {
    "text": "much that the horizontal power auto",
    "start": "1280000",
    "end": "1281200"
  },
  {
    "text": "scaler",
    "start": "1281200",
    "end": "1281919"
  },
  {
    "text": "elects to scale the hash ring all the",
    "start": "1281919",
    "end": "1283280"
  },
  {
    "text": "way down to 12 replicas where the hash",
    "start": "1283280",
    "end": "1285280"
  },
  {
    "text": "ring seems to be pretty stable",
    "start": "1285280",
    "end": "1286960"
  },
  {
    "text": "and at some point we just quit the",
    "start": "1286960",
    "end": "1288400"
  },
  {
    "text": "process because this has been",
    "start": "1288400",
    "end": "1290480"
  },
  {
    "text": "this has given us already a lot to think",
    "start": "1290480",
    "end": "1291760"
  },
  {
    "text": "about compared to the first exercise",
    "start": "1291760",
    "end": "1294640"
  },
  {
    "text": "where everything was rainbows",
    "start": "1294640",
    "end": "1295840"
  },
  {
    "text": "this demo didn't quite go as smoothly as",
    "start": "1295840",
    "end": "1297600"
  },
  {
    "text": "we would have hoped yes we did finally",
    "start": "1297600",
    "end": "1299760"
  },
  {
    "text": "reach",
    "start": "1299760",
    "end": "1300159"
  },
  {
    "text": "some stable state where the hash ring",
    "start": "1300159",
    "end": "1302159"
  },
  {
    "text": "was able to ingest all of the data that",
    "start": "1302159",
    "end": "1303600"
  },
  {
    "text": "we were throwing at it",
    "start": "1303600",
    "end": "1304559"
  },
  {
    "text": "but along the way every time that we had",
    "start": "1304559",
    "end": "1306320"
  },
  {
    "text": "a scaling event",
    "start": "1306320",
    "end": "1307679"
  },
  {
    "text": "we had inconsistent states where we were",
    "start": "1307679",
    "end": "1310720"
  },
  {
    "text": "dropping requests and we couldn't ingest",
    "start": "1310720",
    "end": "1312480"
  },
  {
    "text": "the data that we",
    "start": "1312480",
    "end": "1313440"
  },
  {
    "text": "were being requested to do even though",
    "start": "1313440",
    "end": "1316640"
  },
  {
    "text": "these experiments don't go 100 smoothly",
    "start": "1316640",
    "end": "1318880"
  },
  {
    "text": "they provide a super good insight that",
    "start": "1318880",
    "end": "1320559"
  },
  {
    "text": "helps us understand the systems that",
    "start": "1320559",
    "end": "1322159"
  },
  {
    "text": "we're designing",
    "start": "1322159",
    "end": "1323039"
  },
  {
    "text": "in the case of the thunders receiver we",
    "start": "1323039",
    "end": "1324400"
  },
  {
    "text": "can begin to ask questions like",
    "start": "1324400",
    "end": "1326080"
  },
  {
    "text": "why is this hash ring behaving so",
    "start": "1326080",
    "end": "1328400"
  },
  {
    "text": "erratically or is it so unstable",
    "start": "1328400",
    "end": "1330960"
  },
  {
    "text": "one of the ways we can imagine the",
    "start": "1330960",
    "end": "1332159"
  },
  {
    "text": "system is that we can describe it as",
    "start": "1332159",
    "end": "1334320"
  },
  {
    "text": "being an",
    "start": "1334320",
    "end": "1334799"
  },
  {
    "text": "unstable equilibrium this would be for",
    "start": "1334799",
    "end": "1336720"
  },
  {
    "text": "example if we're balancing",
    "start": "1336720",
    "end": "1338080"
  },
  {
    "text": "a really tall and slim item vertically",
    "start": "1338080",
    "end": "1342080"
  },
  {
    "text": "or if we have some kind of pebble that",
    "start": "1342080",
    "end": "1344080"
  },
  {
    "text": "lands in a local minima",
    "start": "1344080",
    "end": "1346080"
  },
  {
    "text": "any small disturbance might nudge this",
    "start": "1346080",
    "end": "1348400"
  },
  {
    "text": "thing out of its equilibrium state",
    "start": "1348400",
    "end": "1350159"
  },
  {
    "text": "and into some totally chaotic uh state",
    "start": "1350159",
    "end": "1353840"
  },
  {
    "text": "in fact this chaotic state that we're",
    "start": "1353840",
    "end": "1355280"
  },
  {
    "text": "landing in is some kind of positive",
    "start": "1355280",
    "end": "1356640"
  },
  {
    "text": "feedback where we're generating more",
    "start": "1356640",
    "end": "1358240"
  },
  {
    "text": "instability",
    "start": "1358240",
    "end": "1359039"
  },
  {
    "text": "the more unstable that we become to help",
    "start": "1359039",
    "end": "1361440"
  },
  {
    "text": "understand the instability of the system",
    "start": "1361440",
    "end": "1362799"
  },
  {
    "text": "let's think about some of the instances",
    "start": "1362799",
    "end": "1364159"
  },
  {
    "text": "where we had",
    "start": "1364159",
    "end": "1364880"
  },
  {
    "text": "long spans of time with a lot of errors",
    "start": "1364880",
    "end": "1367679"
  },
  {
    "text": "so one of the notable instances",
    "start": "1367679",
    "end": "1369440"
  },
  {
    "text": "was when the horizontal pod auto scaler",
    "start": "1369440",
    "end": "1371919"
  },
  {
    "text": "scaled the hash ring up to 20 pods",
    "start": "1371919",
    "end": "1374480"
  },
  {
    "text": "what was occurring was that there were",
    "start": "1374480",
    "end": "1376559"
  },
  {
    "text": "only 15 pods in the system",
    "start": "1376559",
    "end": "1378559"
  },
  {
    "text": "but the staple set resource had 20",
    "start": "1378559",
    "end": "1380480"
  },
  {
    "text": "replicas in the spec",
    "start": "1380480",
    "end": "1382720"
  },
  {
    "text": "the reason for this was that the stable",
    "start": "1382720",
    "end": "1384640"
  },
  {
    "text": "set controller",
    "start": "1384640",
    "end": "1386159"
  },
  {
    "text": "only created 15 paws in the cluster",
    "start": "1386159",
    "end": "1388159"
  },
  {
    "text": "because we only wanted to have one",
    "start": "1388159",
    "end": "1389679"
  },
  {
    "text": "unready pot at a time the end result is",
    "start": "1389679",
    "end": "1393120"
  },
  {
    "text": "that there was a mismatch between the",
    "start": "1393120",
    "end": "1394640"
  },
  {
    "text": "amount of replicas",
    "start": "1394640",
    "end": "1396000"
  },
  {
    "text": "that we had in the endpoints",
    "start": "1396000",
    "end": "1397760"
  },
  {
    "text": "configuration file",
    "start": "1397760",
    "end": "1399039"
  },
  {
    "text": "for our hash ring and the amount of",
    "start": "1399039",
    "end": "1400559"
  },
  {
    "text": "replicas that were actually available",
    "start": "1400559",
    "end": "1402880"
  },
  {
    "text": "this means that we had tons of requests",
    "start": "1402880",
    "end": "1405200"
  },
  {
    "text": "that were destined",
    "start": "1405200",
    "end": "1406640"
  },
  {
    "text": "to end points that didn't even exist",
    "start": "1406640",
    "end": "1409760"
  },
  {
    "text": "and as a result we had a lot of failures",
    "start": "1409760",
    "end": "1412480"
  },
  {
    "text": "a lot of remote by request failures",
    "start": "1412480",
    "end": "1414559"
  },
  {
    "text": "these remote right request failures end",
    "start": "1414559",
    "end": "1416240"
  },
  {
    "text": "up causing more network load more cpu",
    "start": "1416240",
    "end": "1418880"
  },
  {
    "text": "load",
    "start": "1418880",
    "end": "1419600"
  },
  {
    "text": "and if there hadn't been a hard limit of",
    "start": "1419600",
    "end": "1421760"
  },
  {
    "text": "20 replicas it might have even caused",
    "start": "1421760",
    "end": "1423600"
  },
  {
    "text": "our hashing to be scaled up even higher",
    "start": "1423600",
    "end": "1426320"
  },
  {
    "text": "another issue that we spoke about",
    "start": "1426320",
    "end": "1428080"
  },
  {
    "text": "earlier is that the hashing",
    "start": "1428080",
    "end": "1429679"
  },
  {
    "text": "configuration loading can take a really",
    "start": "1429679",
    "end": "1431440"
  },
  {
    "text": "long time and is non-deterministic so",
    "start": "1431440",
    "end": "1433440"
  },
  {
    "text": "some of the pods might load it before",
    "start": "1433440",
    "end": "1434880"
  },
  {
    "text": "others this means that we can have a",
    "start": "1434880",
    "end": "1436720"
  },
  {
    "text": "split brain",
    "start": "1436720",
    "end": "1438320"
  },
  {
    "text": "in our hash ring where some of the pods",
    "start": "1438320",
    "end": "1440159"
  },
  {
    "text": "have the old hashing configuration",
    "start": "1440159",
    "end": "1442000"
  },
  {
    "text": "and some of the pods have the new",
    "start": "1442000",
    "end": "1443039"
  },
  {
    "text": "hashing configuration the way that we",
    "start": "1443039",
    "end": "1444720"
  },
  {
    "text": "chose to address this",
    "start": "1444720",
    "end": "1446000"
  },
  {
    "text": "was to have a side card that",
    "start": "1446000",
    "end": "1447360"
  },
  {
    "text": "automatically loads the new hashing",
    "start": "1447360",
    "end": "1448960"
  },
  {
    "text": "configuration whenever the configma",
    "start": "1448960",
    "end": "1450640"
  },
  {
    "text": "updates in the api",
    "start": "1450640",
    "end": "1452640"
  },
  {
    "text": "however this has some other downsides to",
    "start": "1452640",
    "end": "1456080"
  },
  {
    "text": "it",
    "start": "1456080",
    "end": "1456960"
  },
  {
    "text": "namely when all of the pods update the",
    "start": "1456960",
    "end": "1459440"
  },
  {
    "text": "configuration at exactly the same time",
    "start": "1459440",
    "end": "1461760"
  },
  {
    "text": "there is an instant of time when none of",
    "start": "1461760",
    "end": "1463679"
  },
  {
    "text": "the pods are available",
    "start": "1463679",
    "end": "1464880"
  },
  {
    "text": "this is because when the hashing",
    "start": "1464880",
    "end": "1466240"
  },
  {
    "text": "configuration changes the thanos",
    "start": "1466240",
    "end": "1467760"
  },
  {
    "text": "receiver",
    "start": "1467760",
    "end": "1468880"
  },
  {
    "text": "takes itself offline temporarily when it",
    "start": "1468880",
    "end": "1471600"
  },
  {
    "text": "flushes",
    "start": "1471600",
    "end": "1472240"
  },
  {
    "text": "its tsdb we flush the tsdb whenever the",
    "start": "1472240",
    "end": "1474799"
  },
  {
    "text": "hashing configuration changes because",
    "start": "1474799",
    "end": "1476559"
  },
  {
    "text": "we don't want data from one tenant to",
    "start": "1476559",
    "end": "1478559"
  },
  {
    "text": "leak into the tsdb",
    "start": "1478559",
    "end": "1480159"
  },
  {
    "text": "block of another tenant because when the",
    "start": "1480159",
    "end": "1482240"
  },
  {
    "text": "hashing configuration changes we might",
    "start": "1482240",
    "end": "1483760"
  },
  {
    "text": "also be changing the tenants that",
    "start": "1483760",
    "end": "1485039"
  },
  {
    "text": "correspond to a pod",
    "start": "1485039",
    "end": "1487919"
  },
  {
    "text": "this downtime can take a lot of time and",
    "start": "1487919",
    "end": "1490000"
  },
  {
    "text": "it means that",
    "start": "1490000",
    "end": "1491600"
  },
  {
    "text": "all 15 of our pods might be down at",
    "start": "1491600",
    "end": "1493600"
  },
  {
    "text": "exactly the same time",
    "start": "1493600",
    "end": "1494880"
  },
  {
    "text": "and this could take a minute and during",
    "start": "1494880",
    "end": "1496799"
  },
  {
    "text": "this entire minute we have no injection",
    "start": "1496799",
    "end": "1498240"
  },
  {
    "text": "capability",
    "start": "1498240",
    "end": "1499440"
  },
  {
    "text": "so even though we have a replication",
    "start": "1499440",
    "end": "1501600"
  },
  {
    "text": "factor of three for example",
    "start": "1501600",
    "end": "1503679"
  },
  {
    "text": "it doesn't really do us any good because",
    "start": "1503679",
    "end": "1505120"
  },
  {
    "text": "all the pods are down",
    "start": "1505120",
    "end": "1506960"
  },
  {
    "text": "another option we could have is to",
    "start": "1506960",
    "end": "1508559"
  },
  {
    "text": "perform a rolling update of the",
    "start": "1508559",
    "end": "1510240"
  },
  {
    "text": "configuration where we only allow",
    "start": "1510240",
    "end": "1512000"
  },
  {
    "text": "one pod at a time to update its hashing",
    "start": "1512000",
    "end": "1513760"
  },
  {
    "text": "configuration however the downside of",
    "start": "1513760",
    "end": "1515760"
  },
  {
    "text": "this is we",
    "start": "1515760",
    "end": "1516320"
  },
  {
    "text": "have a longer amount of time where we",
    "start": "1516320",
    "end": "1518000"
  },
  {
    "text": "have this kind of split brain effect",
    "start": "1518000",
    "end": "1519840"
  },
  {
    "text": "where some of the pods in the beginning",
    "start": "1519840",
    "end": "1521360"
  },
  {
    "text": "have already up to the configuration",
    "start": "1521360",
    "end": "1522880"
  },
  {
    "text": "and the pause at the end haven't at the",
    "start": "1522880",
    "end": "1525919"
  },
  {
    "text": "same time",
    "start": "1525919",
    "end": "1526720"
  },
  {
    "text": "whenever the pods are updating they're",
    "start": "1526720",
    "end": "1528559"
  },
  {
    "text": "still going to be getting traffic",
    "start": "1528559",
    "end": "1530080"
  },
  {
    "text": "forwarded from the other pause in the",
    "start": "1530080",
    "end": "1531600"
  },
  {
    "text": "hash ring this is because",
    "start": "1531600",
    "end": "1533360"
  },
  {
    "text": "the final receive controller doesn't",
    "start": "1533360",
    "end": "1534720"
  },
  {
    "text": "consider the readiness or unreadiness of",
    "start": "1534720",
    "end": "1536960"
  },
  {
    "text": "a pod when it produces the endpoints",
    "start": "1536960",
    "end": "1538400"
  },
  {
    "text": "array",
    "start": "1538400",
    "end": "1539440"
  },
  {
    "text": "even though unready pause are taken out",
    "start": "1539440",
    "end": "1541120"
  },
  {
    "text": "of the load balancing group for",
    "start": "1541120",
    "end": "1542640"
  },
  {
    "text": "requests they're coming at a service ip",
    "start": "1542640",
    "end": "1545360"
  },
  {
    "text": "these pods can still get requests",
    "start": "1545360",
    "end": "1546799"
  },
  {
    "text": "forwarded directly to them",
    "start": "1546799",
    "end": "1548640"
  },
  {
    "text": "from pods in the hash ring so when a pod",
    "start": "1548640",
    "end": "1552000"
  },
  {
    "text": "is temporarily crash looping or maybe",
    "start": "1552000",
    "end": "1554080"
  },
  {
    "text": "it's temporarily offline because it's",
    "start": "1554080",
    "end": "1555760"
  },
  {
    "text": "flushing its tsb",
    "start": "1555760",
    "end": "1557279"
  },
  {
    "text": "the other pods in the hash ring are",
    "start": "1557279",
    "end": "1559360"
  },
  {
    "text": "still going to be making grpc requests",
    "start": "1559360",
    "end": "1560960"
  },
  {
    "text": "to it",
    "start": "1560960",
    "end": "1561679"
  },
  {
    "text": "and causing extra load on this pod and",
    "start": "1561679",
    "end": "1564159"
  },
  {
    "text": "causing more trouble for this pod that's",
    "start": "1564159",
    "end": "1565679"
  },
  {
    "text": "already struggling",
    "start": "1565679",
    "end": "1567039"
  },
  {
    "text": "one way we can address this issue is by",
    "start": "1567039",
    "end": "1568960"
  },
  {
    "text": "using back off logic",
    "start": "1568960",
    "end": "1570720"
  },
  {
    "text": "and in fact we actually already have a",
    "start": "1570720",
    "end": "1573279"
  },
  {
    "text": "pr",
    "start": "1573279",
    "end": "1573760"
  },
  {
    "text": "in the thanos repo for implementing",
    "start": "1573760",
    "end": "1575679"
  },
  {
    "text": "exactly this maybe by the time",
    "start": "1575679",
    "end": "1577279"
  },
  {
    "text": "you see this talk it's already merged a",
    "start": "1577279",
    "end": "1579279"
  },
  {
    "text": "further improvement would maybe be for",
    "start": "1579279",
    "end": "1580720"
  },
  {
    "text": "the thunder receive controller to",
    "start": "1580720",
    "end": "1581919"
  },
  {
    "text": "consider pod readiness when generating",
    "start": "1581919",
    "end": "1583840"
  },
  {
    "text": "the endpoints array",
    "start": "1583840",
    "end": "1585120"
  },
  {
    "text": "finally one big issue that we have in",
    "start": "1585120",
    "end": "1586400"
  },
  {
    "text": "thumbnails receive is the way that we",
    "start": "1586400",
    "end": "1588080"
  },
  {
    "text": "forward requests in the hash ring",
    "start": "1588080",
    "end": "1589760"
  },
  {
    "text": "so normally remote write requests",
    "start": "1589760",
    "end": "1592000"
  },
  {
    "text": "contain a lot of time series",
    "start": "1592000",
    "end": "1593600"
  },
  {
    "text": "and each time series hashes to a unique",
    "start": "1593600",
    "end": "1595440"
  },
  {
    "text": "value that destines it to one node in",
    "start": "1595440",
    "end": "1597840"
  },
  {
    "text": "the hash ring",
    "start": "1597840",
    "end": "1599760"
  },
  {
    "text": "when there's enough time series in the",
    "start": "1599760",
    "end": "1601919"
  },
  {
    "text": "remote write requests statistically it's",
    "start": "1601919",
    "end": "1603600"
  },
  {
    "text": "very likely",
    "start": "1603600",
    "end": "1604559"
  },
  {
    "text": "that some time series will be destined",
    "start": "1604559",
    "end": "1606480"
  },
  {
    "text": "for every node in the hash ring",
    "start": "1606480",
    "end": "1608320"
  },
  {
    "text": "so we might have one incoming request",
    "start": "1608320",
    "end": "1610559"
  },
  {
    "text": "and it can result",
    "start": "1610559",
    "end": "1611440"
  },
  {
    "text": "in 15 outgoing requests inside of the",
    "start": "1611440",
    "end": "1613760"
  },
  {
    "text": "hash ring",
    "start": "1613760",
    "end": "1614799"
  },
  {
    "text": "even though the final receiver batches",
    "start": "1614799",
    "end": "1618240"
  },
  {
    "text": "requests that are destined for the same",
    "start": "1618240",
    "end": "1619600"
  },
  {
    "text": "node all together",
    "start": "1619600",
    "end": "1621279"
  },
  {
    "text": "we still end up having a lot of traffic",
    "start": "1621279",
    "end": "1623120"
  },
  {
    "text": "inside of the hash ring for one single",
    "start": "1623120",
    "end": "1624640"
  },
  {
    "text": "request",
    "start": "1624640",
    "end": "1625600"
  },
  {
    "text": "this means that each incoming request is",
    "start": "1625600",
    "end": "1627600"
  },
  {
    "text": "multiplied by the number of nodes in the",
    "start": "1627600",
    "end": "1629120"
  },
  {
    "text": "hash ring",
    "start": "1629120",
    "end": "1629919"
  },
  {
    "text": "although each request is proportionally",
    "start": "1629919",
    "end": "1631679"
  },
  {
    "text": "smaller minus some gains that you would",
    "start": "1631679",
    "end": "1634000"
  },
  {
    "text": "have had from",
    "start": "1634000",
    "end": "1635440"
  },
  {
    "text": "compression so the larger the hash ring",
    "start": "1635440",
    "end": "1637440"
  },
  {
    "text": "the more intra-hashing traffic is",
    "start": "1637440",
    "end": "1639200"
  },
  {
    "text": "generated for each request",
    "start": "1639200",
    "end": "1640640"
  },
  {
    "text": "right now we're in the process of",
    "start": "1640640",
    "end": "1641760"
  },
  {
    "text": "investigating some different solutions",
    "start": "1641760",
    "end": "1643120"
  },
  {
    "text": "for this and",
    "start": "1643120",
    "end": "1644240"
  },
  {
    "text": "we're specifically looking into having",
    "start": "1644240",
    "end": "1646480"
  },
  {
    "text": "tiered layers of hash rings kind of the",
    "start": "1646480",
    "end": "1648080"
  },
  {
    "text": "same way that",
    "start": "1648080",
    "end": "1649039"
  },
  {
    "text": "the alert manager project has tiered",
    "start": "1649039",
    "end": "1652159"
  },
  {
    "text": "routing trees so what's the deal with",
    "start": "1652159",
    "end": "1654720"
  },
  {
    "text": "the thunder's receiver is it stable or",
    "start": "1654720",
    "end": "1656240"
  },
  {
    "text": "is it not stable can we use the hashing",
    "start": "1656240",
    "end": "1657840"
  },
  {
    "text": "stuff",
    "start": "1657840",
    "end": "1658559"
  },
  {
    "text": "the answer is that yes but it's",
    "start": "1658559",
    "end": "1659760"
  },
  {
    "text": "complicated as we saw earlier",
    "start": "1659760",
    "end": "1661679"
  },
  {
    "text": "when we have a stable sized hash ring we",
    "start": "1661679",
    "end": "1663760"
  },
  {
    "text": "can happily ingest all the data that we",
    "start": "1663760",
    "end": "1665440"
  },
  {
    "text": "send to it",
    "start": "1665440",
    "end": "1666399"
  },
  {
    "text": "but it's when we have a scaling event",
    "start": "1666399",
    "end": "1668399"
  },
  {
    "text": "that we trigger some instability",
    "start": "1668399",
    "end": "1670080"
  },
  {
    "text": "that leads to more instability this",
    "start": "1670080",
    "end": "1672000"
  },
  {
    "text": "means that using automated tools like",
    "start": "1672000",
    "end": "1673520"
  },
  {
    "text": "the horizontal pod autoscaler",
    "start": "1673520",
    "end": "1675200"
  },
  {
    "text": "can lead to unpredictable results yes",
    "start": "1675200",
    "end": "1677600"
  },
  {
    "text": "it'll probably stabilize at the end but",
    "start": "1677600",
    "end": "1679200"
  },
  {
    "text": "we're not exactly sure",
    "start": "1679200",
    "end": "1681360"
  },
  {
    "text": "we can partially mitigate some of these",
    "start": "1681360",
    "end": "1683679"
  },
  {
    "text": "uh uncertainties",
    "start": "1683679",
    "end": "1685120"
  },
  {
    "text": "by implementing policies scaling",
    "start": "1685120",
    "end": "1687840"
  },
  {
    "text": "policies",
    "start": "1687840",
    "end": "1688480"
  },
  {
    "text": "in the horizontal autoscaler which is",
    "start": "1688480",
    "end": "1690240"
  },
  {
    "text": "something that's new in the v2 scaling",
    "start": "1690240",
    "end": "1691760"
  },
  {
    "text": "api",
    "start": "1691760",
    "end": "1693600"
  },
  {
    "text": "another thing we can do is we can start",
    "start": "1693600",
    "end": "1695279"
  },
  {
    "text": "to work on a lot of the improvements",
    "start": "1695279",
    "end": "1697360"
  },
  {
    "text": "that address the problems we identified",
    "start": "1697360",
    "end": "1699360"
  },
  {
    "text": "earlier one of the things that we could",
    "start": "1699360",
    "end": "1701120"
  },
  {
    "text": "do",
    "start": "1701120",
    "end": "1701360"
  },
  {
    "text": "just off the bat is eliminate the need",
    "start": "1701360",
    "end": "1704240"
  },
  {
    "text": "for downtime whenever a hash ring change",
    "start": "1704240",
    "end": "1706000"
  },
  {
    "text": "occurs",
    "start": "1706000",
    "end": "1706880"
  },
  {
    "text": "as we mentioned earlier whenever the",
    "start": "1706880",
    "end": "1708159"
  },
  {
    "text": "hash ring is reconfigured",
    "start": "1708159",
    "end": "1710080"
  },
  {
    "text": "the thunder received pods temporarily",
    "start": "1710080",
    "end": "1712080"
  },
  {
    "text": "take themselves offline while they flush",
    "start": "1712080",
    "end": "1713600"
  },
  {
    "text": "our tsdb",
    "start": "1713600",
    "end": "1714799"
  },
  {
    "text": "one thing we could do is essentially",
    "start": "1714799",
    "end": "1716480"
  },
  {
    "text": "pivot to an alternative tsdb",
    "start": "1716480",
    "end": "1718720"
  },
  {
    "text": "and keep ingesting data while we're",
    "start": "1718720",
    "end": "1720720"
  },
  {
    "text": "flushing the old one",
    "start": "1720720",
    "end": "1722640"
  },
  {
    "text": "yes there's going to be some trade-offs",
    "start": "1722640",
    "end": "1724000"
  },
  {
    "text": "where we have higher cpu and memory",
    "start": "1724000",
    "end": "1725360"
  },
  {
    "text": "utilization",
    "start": "1725360",
    "end": "1726240"
  },
  {
    "text": "but a lot of the recent improvements",
    "start": "1726240",
    "end": "1727600"
  },
  {
    "text": "that um the cortex folks have been",
    "start": "1727600",
    "end": "1729760"
  },
  {
    "text": "making in prometheus and tsdb",
    "start": "1729760",
    "end": "1732000"
  },
  {
    "text": "have improved significantly the resource",
    "start": "1732000",
    "end": "1735200"
  },
  {
    "text": "utilization",
    "start": "1735200",
    "end": "1736080"
  },
  {
    "text": "that might even allow us to make these",
    "start": "1736080",
    "end": "1737440"
  },
  {
    "text": "kind of uh trade-offs",
    "start": "1737440",
    "end": "1740159"
  },
  {
    "text": "another thing we could do would be to",
    "start": "1740159",
    "end": "1741520"
  },
  {
    "text": "use consistent hashing so one of the",
    "start": "1741520",
    "end": "1743520"
  },
  {
    "text": "problems that we see when we have the",
    "start": "1743520",
    "end": "1744720"
  },
  {
    "text": "split brain effect on our hash ring",
    "start": "1744720",
    "end": "1746480"
  },
  {
    "text": "is that one group of pods that receives",
    "start": "1746480",
    "end": "1748080"
  },
  {
    "text": "a request things of the time series",
    "start": "1748080",
    "end": "1750240"
  },
  {
    "text": "contained in it should be forwarded to",
    "start": "1750240",
    "end": "1751919"
  },
  {
    "text": "another set of pods with a different",
    "start": "1751919",
    "end": "1753200"
  },
  {
    "text": "view of the hash ring",
    "start": "1753200",
    "end": "1754640"
  },
  {
    "text": "these paws on the other hand think that",
    "start": "1754640",
    "end": "1756480"
  },
  {
    "text": "the time series should go to the first",
    "start": "1756480",
    "end": "1757520"
  },
  {
    "text": "set of paws and they forward to each",
    "start": "1757520",
    "end": "1758640"
  },
  {
    "text": "other infinitely",
    "start": "1758640",
    "end": "1759840"
  },
  {
    "text": "using consistent hashing will allow us",
    "start": "1759840",
    "end": "1761360"
  },
  {
    "text": "to partially mitigate this because we",
    "start": "1761360",
    "end": "1763120"
  },
  {
    "text": "could reduce",
    "start": "1763120",
    "end": "1764240"
  },
  {
    "text": "how much keys get redistributed when a",
    "start": "1764240",
    "end": "1766559"
  },
  {
    "text": "new node is added to the hash ring",
    "start": "1766559",
    "end": "1769039"
  },
  {
    "text": "all right so that gives us a lot of",
    "start": "1769039",
    "end": "1770240"
  },
  {
    "text": "ideas for ways that we can improve the",
    "start": "1770240",
    "end": "1771679"
  },
  {
    "text": "thanos project so",
    "start": "1771679",
    "end": "1773120"
  },
  {
    "text": "hopefully some of you feel inspired to",
    "start": "1773120",
    "end": "1774880"
  },
  {
    "text": "contribute and i hope to see you all on",
    "start": "1774880",
    "end": "1777120"
  },
  {
    "text": "github",
    "start": "1777120",
    "end": "1778000"
  },
  {
    "text": "um thanks a lot for watching that's what",
    "start": "1778000",
    "end": "1779279"
  },
  {
    "text": "i have for you today and i'll stick",
    "start": "1779279",
    "end": "1780799"
  },
  {
    "text": "around for",
    "start": "1780799",
    "end": "1781440"
  },
  {
    "text": "questions",
    "start": "1781440",
    "end": "1784240"
  }
]