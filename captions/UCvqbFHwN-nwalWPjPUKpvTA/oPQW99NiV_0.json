[
  {
    "start": "0",
    "end": "10000"
  },
  {
    "text": "how's everybody today this is the last Talk of the day thank you for coming",
    "start": "4799",
    "end": "11120"
  },
  {
    "start": "10000",
    "end": "26000"
  },
  {
    "text": "this is bring back production from scratch in under one hour using cops Argo CD and",
    "start": "11340",
    "end": "19380"
  },
  {
    "text": "Valero my name is Andre Marcelo Tanner",
    "start": "19380",
    "end": "25939"
  },
  {
    "start": "26000",
    "end": "67000"
  },
  {
    "text": "a little bit about me I'm a Staff devops engineer which really means like distinguished yaml engineer",
    "start": "26580",
    "end": "33899"
  },
  {
    "text": "you can find me on these slack groups um I have taken the cka and cks exam how",
    "start": "33899",
    "end": "41160"
  },
  {
    "text": "many people here have taken these exams all right awesome for anybody who hasn't I'd really",
    "start": "41160",
    "end": "48660"
  },
  {
    "text": "recommend these exams they're really practical examples that test you under",
    "start": "48660",
    "end": "54000"
  },
  {
    "text": "pressure and how to repair kubernetes clusters and that'll become relevant in the coming slides also I'm originally",
    "start": "54000",
    "end": "60840"
  },
  {
    "text": "from the Philippines so my boys and I now live and work in Canada eh",
    "start": "60840",
    "end": "68159"
  },
  {
    "start": "67000",
    "end": "117000"
  },
  {
    "text": "I work for a company named Ada and we're AI powered customer service automation",
    "start": "68159",
    "end": "74640"
  },
  {
    "text": "company we've been around since 2016. we help Enterprises resolve their customer service inquiries in any language or",
    "start": "74640",
    "end": "81659"
  },
  {
    "text": "Channel you know uh WhatsApp Facebook text email browser",
    "start": "81659",
    "end": "87900"
  },
  {
    "text": "Ada has automated more than 4 billion customer conversations for companies like meta Verizon AirAsia Yeti Square",
    "start": "87900",
    "end": "94259"
  },
  {
    "text": "and we serve companies and their customers across 85 countries how is that relevant so we've been",
    "start": "94259",
    "end": "100500"
  },
  {
    "text": "running kubernetes in production for almost like six years and I've been there just for like four",
    "start": "100500",
    "end": "106979"
  },
  {
    "text": "so I've seen a lot and a lot has changed um",
    "start": "106979",
    "end": "112700"
  },
  {
    "text": "and about a year ago we had an incident that gave birth to this talk",
    "start": "112740",
    "end": "119880"
  },
  {
    "start": "117000",
    "end": "212000"
  },
  {
    "text": "so picture this it's the end of the day you're on call",
    "start": "119880",
    "end": "125460"
  },
  {
    "text": "you're about to you know go home log off turn off your laptop and suddenly one of",
    "start": "125460",
    "end": "130619"
  },
  {
    "text": "the teams contacts you but hey we've got a problem we're trying to deploy our service to",
    "start": "130619",
    "end": "137220"
  },
  {
    "text": "this one production cluster it's not working so I give my typical reply",
    "start": "137220",
    "end": "143879"
  },
  {
    "text": "have you tried running it again and they do that and they come back it's not working",
    "start": "143879",
    "end": "149819"
  },
  {
    "text": "so then I say we'll have you open the ticket because we we don't do work with Algeria",
    "start": "149819",
    "end": "156120"
  },
  {
    "text": "tickets right well no so I started investigating right I go on I go on the cluster I try to see what's going on and",
    "start": "156120",
    "end": "162540"
  },
  {
    "text": "the pods they're not coming up they're pending it seems like it's been this way for a few days which is strange uh so",
    "start": "162540",
    "end": "169620"
  },
  {
    "text": "the current deployment is rather old but it's still running right it's kubernetes it's highly available",
    "start": "169620",
    "end": "176280"
  },
  {
    "text": "and I go and look into our Auto scaling infrastructure okay it's bringing up new nodes that's working and I go check",
    "start": "176280",
    "end": "183480"
  },
  {
    "text": "those out in our Cloud infrastructure ah they come up and then",
    "start": "183480",
    "end": "189599"
  },
  {
    "text": "they go away they're not connecting to the cluster that's strange why is it doing",
    "start": "189599",
    "end": "196080"
  },
  {
    "text": "that not supposed to do that so I go reach out for our cluster",
    "start": "196080",
    "end": "202019"
  },
  {
    "text": "management tool to figure out what's going on and I'm going to talk about each of these tools",
    "start": "202019",
    "end": "207319"
  },
  {
    "text": "and what they are and then how we use them how they're important in this",
    "start": "207319",
    "end": "213480"
  },
  {
    "start": "212000",
    "end": "372000"
  },
  {
    "text": "so our cluster management tool at that time was cops stands for kubernetes",
    "start": "213480",
    "end": "218819"
  },
  {
    "text": "chaos or kubernetes operations um it's a tool that lets you create its",
    "start": "218819",
    "end": "225180"
  },
  {
    "text": "own kubernetes distribution you can create a production cluster in a single command on any public Cloud AWS gcp Azure and",
    "start": "225180",
    "end": "234000"
  },
  {
    "text": "more and it gets it up and running and we did this using we had the",
    "start": "234000",
    "end": "239760"
  },
  {
    "text": "declarative configuration has like a yaml syntax similar to helm or others out there",
    "start": "239760",
    "end": "245400"
  },
  {
    "text": "and so um we'd been using it and we had lots of playbooks and experience using it we",
    "start": "245400",
    "end": "251220"
  },
  {
    "text": "create all our production clusters using it and their copies like we have a source of Truth and then we create our",
    "start": "251220",
    "end": "256560"
  },
  {
    "text": "different clusters so they're either there's there's a lot of commands but there's cops create",
    "start": "256560",
    "end": "262440"
  },
  {
    "text": "cluster cops update cluster so I go in I run the cops update cluster command",
    "start": "262440",
    "end": "268680"
  },
  {
    "text": "which doesn't update things right away it tells me is it everything expect as expected from",
    "start": "268680",
    "end": "275639"
  },
  {
    "text": "my source of Truth the declarative file I run it and it says I want to do all",
    "start": "275639",
    "end": "282720"
  },
  {
    "text": "these changes not something doesn't seem right okay that's strange",
    "start": "282720",
    "end": "288620"
  },
  {
    "text": "so I get another engineer on board with me and we go debug it we like what's what's going on why does it want to",
    "start": "290220",
    "end": "296220"
  },
  {
    "text": "change all these things so we go run it on our other production clusters just to make sure that it's not it's not just us",
    "start": "296220",
    "end": "303060"
  },
  {
    "text": "and it is only this cluster hmm letter clusters they don't want to",
    "start": "303060",
    "end": "308280"
  },
  {
    "text": "change anything but with with this specific cluster it wants to change certain files it wants to recreate Some",
    "start": "308280",
    "end": "314340"
  },
  {
    "text": "Cloud resources so we go debug some more and",
    "start": "314340",
    "end": "319860"
  },
  {
    "text": "we could we need to fix it because eventually if you don't new nodes are not able to join",
    "start": "319860",
    "end": "325800"
  },
  {
    "text": "your cluster then a node may go away and a server that's running in there can never come back so",
    "start": "325800",
    "end": "331680"
  },
  {
    "text": "you need we need to fix it we need to get it running so we made a decision we decided to okay",
    "start": "331680",
    "end": "339240"
  },
  {
    "text": "let's go ahead let's run the update now we we've done this a lot of times before it should just work",
    "start": "339240",
    "end": "347280"
  },
  {
    "text": "um worse that could happen maybe it uh",
    "start": "347280",
    "end": "352860"
  },
  {
    "text": "you know it breaks things but we can get it back running right that's not gonna happen",
    "start": "352860",
    "end": "358698"
  },
  {
    "text": "so I run cops update cluster dash dash apply and it asks me do you want to apply",
    "start": "359220",
    "end": "365759"
  },
  {
    "text": "these changes yes or no I type in yes enter",
    "start": "365759",
    "end": "372320"
  },
  {
    "text": "foreign",
    "start": "374039",
    "end": "377039"
  },
  {
    "text": "process and we have this process in our company where we get on the call we call",
    "start": "384440",
    "end": "389819"
  },
  {
    "text": "it an instant Commander there's Communications there's different people talking to customers we start the process",
    "start": "389819",
    "end": "395940"
  },
  {
    "text": "and then I'm like yeah I'm looking into it I caused this it's me but we're trying to figure it out so",
    "start": "395940",
    "end": "402780"
  },
  {
    "text": "we go debugging it some more and now we're under pressure remember those exams they actually come in handy",
    "start": "402780",
    "end": "409800"
  },
  {
    "text": "so we we look at what's going on and suddenly it's like okay",
    "start": "409800",
    "end": "415259"
  },
  {
    "text": "um our networking layer we're using Calico it's it's just having an error Accord DNS has another area it's not",
    "start": "415259",
    "end": "422039"
  },
  {
    "text": "coming up that doesn't make sense it was working a while ago why does it not work now maybe let's try running it again",
    "start": "422039",
    "end": "428520"
  },
  {
    "text": "so we run cops update cluster supposed to work right well it it",
    "start": "428520",
    "end": "434639"
  },
  {
    "text": "usually works and nothing changes so we're like okay okay what do you do what do you do what do we do",
    "start": "434639",
    "end": "440160"
  },
  {
    "text": "well thankfully we have our disaster recovery process",
    "start": "440160",
    "end": "446599"
  },
  {
    "start": "442000",
    "end": "480000"
  },
  {
    "text": "that's not it but how many people run Disaster Recovery planning on their",
    "start": "447599",
    "end": "452880"
  },
  {
    "text": "kubernetes environments no one okay",
    "start": "452880",
    "end": "458759"
  },
  {
    "text": "maybe you should maybe I'm doing something wrong okay so we have Disaster Recovery plans for in case things happen",
    "start": "458759",
    "end": "465840"
  },
  {
    "text": "we run them once or twice a year we have a certain scenario and we use our tools to bring back our",
    "start": "465840",
    "end": "472560"
  },
  {
    "text": "cluster when they go down so we pull it out we're like okay here's",
    "start": "472560",
    "end": "478500"
  },
  {
    "text": "the guide what do we do what do we do this where this is where Valero comes in",
    "start": "478500",
    "end": "484740"
  },
  {
    "start": "480000",
    "end": "550000"
  },
  {
    "text": "uh Valero is a backup and restorer for the state of kubernetes",
    "start": "484740",
    "end": "490080"
  },
  {
    "text": "it's backed by VMware basically it's like it can regularly",
    "start": "490080",
    "end": "495240"
  },
  {
    "text": "back up your cluster on a schedule including persistent volumes so Cloud volumes EBS volumes or whatever the",
    "start": "495240",
    "end": "501539"
  },
  {
    "text": "volume is in your cloud and then you can install that to another cluster we had used this during our Disaster",
    "start": "501539",
    "end": "507599"
  },
  {
    "text": "Recovery testing to migrate clusters to bring up a new cluster",
    "start": "507599",
    "end": "513300"
  },
  {
    "text": "it's basically like getting uh Cube control getting all the yaml and storing it somewhere it stores it in a object",
    "start": "513300",
    "end": "520320"
  },
  {
    "text": "store so you know this will work this should work right we we take the latest back up",
    "start": "520320",
    "end": "526800"
  },
  {
    "text": "before things go wrong before things went wrong and we we start to restore it",
    "start": "526800",
    "end": "531839"
  },
  {
    "text": "it restores pretty quickly um some things start working but not not",
    "start": "531839",
    "end": "536880"
  },
  {
    "text": "everything like our main services are still down and so we go investigate some more and",
    "start": "536880",
    "end": "542880"
  },
  {
    "text": "more but it didn't get things working",
    "start": "542880",
    "end": "549060"
  },
  {
    "text": "so we had to make a decision like when you're under pressure under Fire",
    "start": "549060",
    "end": "556200"
  },
  {
    "start": "550000",
    "end": "631000"
  },
  {
    "text": "you can't really just stall you gotta talk to your team that's there make a decision move on make a decision",
    "start": "556200",
    "end": "562260"
  },
  {
    "text": "talk about the consequences to trade-offs what are the options and of course we have we have our our",
    "start": "562260",
    "end": "567899"
  },
  {
    "text": "leaders on the call we have people on our teams helping us trying to figure out what's going on",
    "start": "567899",
    "end": "573000"
  },
  {
    "text": "um we did have a backup plan because we did our Disaster Recovery planning we knew",
    "start": "573000",
    "end": "579779"
  },
  {
    "text": "that no matter what if something if worse came to worse we could recreate the entire cluster",
    "start": "579779",
    "end": "585120"
  },
  {
    "text": "of course that might take time right but so we made a decision as a group as a",
    "start": "585120",
    "end": "591120"
  },
  {
    "text": "team all right can't figure this out we don't know what what's going on we don't know how it",
    "start": "591120",
    "end": "596279"
  },
  {
    "text": "started um it's our restore plans are not working",
    "start": "596279",
    "end": "601920"
  },
  {
    "text": "let's just recreate the cluster and get everything at least we know how that works and so we did we pressed the big reset",
    "start": "601920",
    "end": "608399"
  },
  {
    "text": "button we deleted the cluster",
    "start": "608399",
    "end": "613620"
  },
  {
    "text": "on purpose and then we recreated it we recreated uh",
    "start": "613620",
    "end": "619200"
  },
  {
    "text": "using cops cops update cluster it's real quick it brings it all up but it was empty",
    "start": "619200",
    "end": "625440"
  },
  {
    "text": "now what we we did have Valero to restore things but",
    "start": "625440",
    "end": "631560"
  },
  {
    "start": "631000",
    "end": "710000"
  },
  {
    "text": "our next tool is what brings everything together so we use Argo CD we had been migrating",
    "start": "631560",
    "end": "638000"
  },
  {
    "text": "migrating to it and we migrated our core applications to it and so we installed Argo CD on the cluster",
    "start": "638000",
    "end": "645540"
  },
  {
    "text": "and then it installed our main applications and then things came up",
    "start": "645540",
    "end": "652760"
  },
  {
    "text": "how many people here know about Argo CD all right okay quick intro about Argos",
    "start": "654060",
    "end": "660779"
  },
  {
    "text": "Seasons most people know about it um so it's a github's tool that allows you to",
    "start": "660779",
    "end": "666240"
  },
  {
    "text": "deploy your applications from a single source of Truth previously before using Argo CD we were",
    "start": "666240",
    "end": "671459"
  },
  {
    "text": "doing deployments via imperative commands from our CI service or from uh",
    "start": "671459",
    "end": "676680"
  },
  {
    "text": "tool we built in-house like Cube control apply basically but we moved it because we needed to be",
    "start": "676680",
    "end": "683100"
  },
  {
    "text": "more scalable and we need things to be just reliable not someone deployed something here and we don't know how",
    "start": "683100",
    "end": "688920"
  },
  {
    "text": "they deployed it so we deployed Argo CD and it brought up",
    "start": "688920",
    "end": "695700"
  },
  {
    "text": "our main applications and everything was good actually service came back",
    "start": "695700",
    "end": "702540"
  },
  {
    "text": "and so we were okay I saw the job good no but everything was good",
    "start": "702540",
    "end": "711199"
  },
  {
    "text": "and it just worked well not not exactly so our Disaster",
    "start": "711300",
    "end": "718380"
  },
  {
    "text": "Recovery plan wasn't perfect our migration to Argo CD wasn't perfect we had some services our main Services came",
    "start": "718380",
    "end": "725100"
  },
  {
    "text": "up and this restored um for our customers customer facing",
    "start": "725100",
    "end": "730680"
  },
  {
    "text": "services but there were other things in the cluster that weren't there we had outdated guides Disaster Recovery",
    "start": "730680",
    "end": "737100"
  },
  {
    "text": "guides we had outdated processes some things we had to install manually some things we had to go find the gitlab",
    "start": "737100",
    "end": "744300"
  },
  {
    "text": "job or CI job and click a play button and make get that installed so a lot of things we had to do to fix",
    "start": "744300",
    "end": "750720"
  },
  {
    "text": "it after things came up but eventually we got into its working state",
    "start": "750720",
    "end": "756600"
  },
  {
    "text": "and we could stop the incident called so in total",
    "start": "756600",
    "end": "764180"
  },
  {
    "start": "757000",
    "end": "793000"
  },
  {
    "text": "it was like two hours of Total outage it took us about 41 minutes",
    "start": "764639",
    "end": "770579"
  },
  {
    "text": "to delete and bring back the cluster which was by our standards when we do Dr we give it about two hours",
    "start": "770579",
    "end": "778079"
  },
  {
    "text": "there were many interdependent Services we had to figure out what's working what's not working",
    "start": "778079",
    "end": "784320"
  },
  {
    "text": "um but things were good we were back",
    "start": "784320",
    "end": "790100"
  },
  {
    "start": "793000",
    "end": "894000"
  },
  {
    "text": "now what what was actually the problem in case you're wondering so this is what we found out afterwards",
    "start": "794279",
    "end": "801000"
  },
  {
    "text": "is cops has a state store so when you create a cluster it stores the state of",
    "start": "801000",
    "end": "807360"
  },
  {
    "text": "the cluster in a object storage we were using AWS and",
    "start": "807360",
    "end": "813899"
  },
  {
    "text": "and so we created a S3 bucket for that and when we did it we were using a new",
    "start": "813899",
    "end": "819600"
  },
  {
    "text": "process that we hadn't used before well not for this we had a new process to create that object storage and had a",
    "start": "819600",
    "end": "827279"
  },
  {
    "text": "life cycle policy lifecycle policy is after a certain",
    "start": "827279",
    "end": "834300"
  },
  {
    "text": "amount of time it will delete files and so it turns out that",
    "start": "834300",
    "end": "839700"
  },
  {
    "text": "when a new cluster when a new node comes up it accesses this",
    "start": "839700",
    "end": "844800"
  },
  {
    "text": "object storage and gets secrets and things it needs to connect to the cluster certificates",
    "start": "844800",
    "end": "852320"
  },
  {
    "text": "and so when when we created the cluster [Music] um it created up we created with an object",
    "start": "853680",
    "end": "860519"
  },
  {
    "text": "store that had an expiry policy of 90 days guess what happened 90 days ago",
    "start": "860519",
    "end": "866700"
  },
  {
    "text": "that's when we created the cluster so in hindsight we could have fixed this if",
    "start": "866700",
    "end": "874320"
  },
  {
    "text": "we knew where to look if we knew more if we prepared for this problem if we knew how our tool failed",
    "start": "874320",
    "end": "881579"
  },
  {
    "text": "um actually one of the maintainers helped us find this um they probably say it before",
    "start": "881579",
    "end": "888600"
  },
  {
    "text": "and so now we know",
    "start": "888600",
    "end": "891560"
  },
  {
    "start": "894000",
    "end": "1046000"
  },
  {
    "text": "so what did we learn through this process",
    "start": "895260",
    "end": "900540"
  },
  {
    "text": "right well",
    "start": "900540",
    "end": "905480"
  },
  {
    "text": "first thing is to complete your migrations we had several CI CI processes",
    "start": "905760",
    "end": "912180"
  },
  {
    "text": "deployment processes we were figuring out how to move over the Argo CD we were figuring out how to",
    "start": "912180",
    "end": "917880"
  },
  {
    "text": "do things we didn't have enough time or we didn't have enough we had other things that came up",
    "start": "917880",
    "end": "923339"
  },
  {
    "text": "and then we had multiple deployment pipelines this service was deployed via the old way this service was deployed by",
    "start": "923339",
    "end": "928800"
  },
  {
    "text": "the new way this service was I don't know so complete your migrations the second",
    "start": "928800",
    "end": "935639"
  },
  {
    "text": "is be experts in your tooling there's a lot of Open Source tooling",
    "start": "935639",
    "end": "941100"
  },
  {
    "text": "that we all use that's all out on display there and if if you're not paying someone or a vendor to support",
    "start": "941100",
    "end": "947940"
  },
  {
    "text": "that for you your company is paying you to be the maintainer of that software to be the experts",
    "start": "947940",
    "end": "954540"
  },
  {
    "text": "and so when you take on an open source software when we take on an open source software we have to learn how it fails",
    "start": "954540",
    "end": "961019"
  },
  {
    "text": "learn how it works and also learn all the bad things about it have guides have training teach new team",
    "start": "961019",
    "end": "968339"
  },
  {
    "text": "members how to use it and over time we had accumulated a lot of software that",
    "start": "968339",
    "end": "974519"
  },
  {
    "text": "some people knew how to run that were been there a long time and others didn't and so we had to look over it and look",
    "start": "974519",
    "end": "980160"
  },
  {
    "text": "over the critical ones and look at how they worked",
    "start": "980160",
    "end": "986000"
  },
  {
    "text": "um yeah and the third point is to always be",
    "start": "986699",
    "end": "993660"
  },
  {
    "text": "practicing your disaster recovery now we had outdated guides we mostly we did",
    "start": "993660",
    "end": "1002360"
  },
  {
    "text": "them for compliance you know we we went through the exercise we did them but",
    "start": "1002360",
    "end": "1008620"
  },
  {
    "text": "we actually had to use them and that's when we you know found out that they're they're they're not as if they were",
    "start": "1008959",
    "end": "1016399"
  },
  {
    "text": "outdated they they had commands that didn't work and so what if someone else was on call that didn't write the guide",
    "start": "1016399",
    "end": "1022040"
  },
  {
    "text": "or didn't hasn't been there a while so if you're always practicing your",
    "start": "1022040",
    "end": "1027678"
  },
  {
    "text": "processes if you're then you have less chance of them drifting",
    "start": "1027679",
    "end": "1034720"
  },
  {
    "start": "1046000",
    "end": "1136000"
  },
  {
    "text": "so we learn from this and it wasn't quick but",
    "start": "1048740",
    "end": "1054799"
  },
  {
    "text": "we fully migrated over to Argo CD we took the time to do it properly",
    "start": "1054799",
    "end": "1060140"
  },
  {
    "text": "we took the time to learn more how to do it to improve our process and this is sort of what it looks like",
    "start": "1060140",
    "end": "1066500"
  },
  {
    "text": "today maybe we might be doing it wrong but",
    "start": "1066500",
    "end": "1071660"
  },
  {
    "text": "this is what we picked you know um when someone deploys",
    "start": "1071660",
    "end": "1078559"
  },
  {
    "text": "it goes through a abstracted CI pipeline job that goes to our get Ops repo",
    "start": "1078559",
    "end": "1086179"
  },
  {
    "text": "and then every cluster is running Argo CD and they they all deploy whenever they",
    "start": "1086179",
    "end": "1092240"
  },
  {
    "text": "get to deployment for that environment they deploy it and so if something is not on Argo CD it",
    "start": "1092240",
    "end": "1099200"
  },
  {
    "text": "does not exist in our cluster there is no more manual deployments imperative deploys",
    "start": "1099200",
    "end": "1107919"
  },
  {
    "text": "and we were able to move all teams over to this new method it took time we did critical Services",
    "start": "1109820",
    "end": "1115940"
  },
  {
    "text": "first and then we took other teams through it some teams had to migrate themselves",
    "start": "1115940",
    "end": "1121880"
  },
  {
    "text": "but we got there and if there was a service that they didn't migrate well it must not be that important but yeah when",
    "start": "1121880",
    "end": "1128660"
  },
  {
    "text": "we moved over we found some but we fixed those so",
    "start": "1128660",
    "end": "1133780"
  },
  {
    "start": "1136000",
    "end": "1184000"
  },
  {
    "text": "and being experts at our processes so now when we spin up a new cluster",
    "start": "1138200",
    "end": "1144620"
  },
  {
    "text": "we have a single workflow we're actually on eks now but",
    "start": "1144620",
    "end": "1150559"
  },
  {
    "text": "and we use terraform to span up and then after that we installed Argo CD",
    "start": "1150559",
    "end": "1155720"
  },
  {
    "text": "and then Argo CD from there takes all the applications we're using application sets now and label selectors",
    "start": "1155720",
    "end": "1163039"
  },
  {
    "text": "and that allows the cluster our proceed in the cluster to pull in what's right",
    "start": "1163039",
    "end": "1168140"
  },
  {
    "text": "for that environment and deploy it so actually now we can deploy an entire cluster end to end in just a few minutes",
    "start": "1168140",
    "end": "1177760"
  },
  {
    "start": "1184000",
    "end": "1359000"
  },
  {
    "text": "now this is what I mean by Dr is our deployment process we don't do the",
    "start": "1186919",
    "end": "1192559"
  },
  {
    "text": "process once in a while we actually do it quite frequently",
    "start": "1192559",
    "end": "1199280"
  },
  {
    "text": "so now we used to be really bad at upgrading clusters as you know there's a new",
    "start": "1199280",
    "end": "1206179"
  },
  {
    "text": "version of kubernetes all the time the older versions go out of date so you have to get used to upgrading it",
    "start": "1206179",
    "end": "1212780"
  },
  {
    "text": "and with our new process we made sure we got it really good that we could upgrade all the time so we always have like a",
    "start": "1212780",
    "end": "1218720"
  },
  {
    "text": "blue and green cluster maybe in the future we'll have more so we recently just last week like we we",
    "start": "1218720",
    "end": "1225320"
  },
  {
    "text": "went from 1.22 to 1.25 of course we had to figure out the",
    "start": "1225320",
    "end": "1230360"
  },
  {
    "text": "incompatibilities but the Clusters and all the applications that was a breeze we put up the new cluster we called the",
    "start": "1230360",
    "end": "1236600"
  },
  {
    "text": "green and they grab from the same source of Truth",
    "start": "1236600",
    "end": "1242720"
  },
  {
    "text": "in our github's repo Argo CD deploys it and then when we're ready to switch over",
    "start": "1242720",
    "end": "1248120"
  },
  {
    "text": "traffic we switch our load balancers or we do rated weighted load balancing",
    "start": "1248120",
    "end": "1254960"
  },
  {
    "text": "and we've done this already for two different version upgrades and we just keep improving the process",
    "start": "1254960",
    "end": "1263299"
  },
  {
    "text": "so the same process we use for creating clusters for restoring them is is our deployment process",
    "start": "1263299",
    "end": "1271100"
  },
  {
    "text": "we're always doing that there is no other well we still have backups just in",
    "start": "1271100",
    "end": "1276679"
  },
  {
    "text": "case but we hope we don't have to use them",
    "start": "1276679",
    "end": "1283120"
  },
  {
    "text": "now it took a team to get here or a lot of teams a village this wasn't just like",
    "start": "1285440",
    "end": "1290780"
  },
  {
    "text": "one small team this was several teams in our companies three teams working together",
    "start": "1290780",
    "end": "1297260"
  },
  {
    "text": "um over a lot of time and we're not done we're going to keep",
    "start": "1297260",
    "end": "1304159"
  },
  {
    "text": "improving this process you know um",
    "start": "1304159",
    "end": "1309260"
  },
  {
    "text": "so to give you an example we've been running kubernetes in production for almost six years",
    "start": "1309260",
    "end": "1316340"
  },
  {
    "text": "Argo City actually for two years and to get here since our incident like",
    "start": "1316340",
    "end": "1322520"
  },
  {
    "text": "last year it's been almost a year like it took almost a year of work",
    "start": "1322520",
    "end": "1327919"
  },
  {
    "text": "with many people to get there so wherever you are in your journey and",
    "start": "1327919",
    "end": "1333320"
  },
  {
    "text": "we're still on a journey it'll take time but you get there and it's it's it's",
    "start": "1333320",
    "end": "1339500"
  },
  {
    "text": "really good and we're still learning I met someone yesterday Michael from Stockholm and he",
    "start": "1339500",
    "end": "1347240"
  },
  {
    "text": "showed me how he set up his clusters with Argo CD and cluster API and I learned something there that I probably want to go Implement when I get home",
    "start": "1347240",
    "end": "1355480"
  },
  {
    "start": "1359000",
    "end": "1403000"
  },
  {
    "text": "um are we gonna do a demo and with real production um I didn't prepare for this no so",
    "start": "1361520",
    "end": "1368780"
  },
  {
    "text": "oh I do have this Ripple up that I use for",
    "start": "1368780",
    "end": "1374539"
  },
  {
    "text": "a lot of demos of what it's like except Argo CD in one goal on everything",
    "start": "1374539",
    "end": "1379760"
  },
  {
    "text": "um this is using a kind cluster so you can do it on your local laptop in Docker and then it sets up Argo CD managing",
    "start": "1379760",
    "end": "1386960"
  },
  {
    "text": "Argo CD and other infrastructure services and then a sample application it's pretty much the same thing we use",
    "start": "1386960",
    "end": "1393380"
  },
  {
    "text": "just less CI but I encourage you to check it out",
    "start": "1393380",
    "end": "1400658"
  },
  {
    "start": "1403000",
    "end": "1634000"
  },
  {
    "text": "that's all we can go [Applause]",
    "start": "1406280",
    "end": "1413120"
  },
  {
    "text": "[Music] so if anybody wants to take questions",
    "start": "1413120",
    "end": "1420559"
  },
  {
    "text": "please come up to the mic if not I'm available on Slack hello hello",
    "start": "1420559",
    "end": "1427520"
  },
  {
    "text": "I uh I have a question yes um so um looking at this the speed of your",
    "start": "1427520",
    "end": "1433640"
  },
  {
    "text": "like backups and recoveries I'm assuming you didn't have like too many like",
    "start": "1433640",
    "end": "1438740"
  },
  {
    "text": "stateful applications or like large databases um or maybe not I mean maybe you did I",
    "start": "1438740",
    "end": "1444620"
  },
  {
    "text": "don't know but um or how what would you recommend if you're running something a bit you know",
    "start": "1444620",
    "end": "1450200"
  },
  {
    "text": "more stateful like a large database or something like that what would be the way because you know I don't think Argo",
    "start": "1450200",
    "end": "1455539"
  },
  {
    "text": "would be sufficient because it's all it stores is like the Manifest correct so",
    "start": "1455539",
    "end": "1460700"
  },
  {
    "text": "you probably need something like Valero or you know it might be slower I don't know true",
    "start": "1460700",
    "end": "1465799"
  },
  {
    "text": "um so we had stateless and stateful applications but our state was outside the cluster so regardless of what",
    "start": "1465799",
    "end": "1471980"
  },
  {
    "text": "happened to the cluster we can um we can as long as we can connect to our data stores",
    "start": "1471980",
    "end": "1478340"
  },
  {
    "text": "so you can use Valero for stateful but first for depending on how it's stored",
    "start": "1478340",
    "end": "1483559"
  },
  {
    "text": "but it's a different process for every database depending on how that's done",
    "start": "1483559",
    "end": "1488600"
  },
  {
    "text": "um with Valero you can configure hooks like when say when how to how to make",
    "start": "1488600",
    "end": "1494059"
  },
  {
    "text": "Valero backup it's data data stores the PVCs you can configure hooks depending",
    "start": "1494059",
    "end": "1499700"
  },
  {
    "text": "on like say run this Backup Tool first and then like to",
    "start": "1499700",
    "end": "1505100"
  },
  {
    "text": "create the database backup so it does have a lot of those features",
    "start": "1505100",
    "end": "1510559"
  },
  {
    "text": "cool thank you",
    "start": "1510559",
    "end": "1513520"
  },
  {
    "text": "no one else wants to ask a question I do um following up on that",
    "start": "1516919",
    "end": "1522159"
  },
  {
    "text": "given if you wouldn't be able to connect to your stateful person's volumes how",
    "start": "1522159",
    "end": "1530960"
  },
  {
    "text": "would you restore that in terms of using Argo and maybe Valero in combination",
    "start": "1530960",
    "end": "1536419"
  },
  {
    "text": "because what I think about is if as soon as you install Argo and you restore from",
    "start": "1536419",
    "end": "1543140"
  },
  {
    "text": "a backup using Valero you now have the conflict that Valero as well as Argo",
    "start": "1543140",
    "end": "1549620"
  },
  {
    "text": "want to apply their manifests how do you solve this conflict",
    "start": "1549620",
    "end": "1556279"
  },
  {
    "text": "so the question is if you're using both Valero and Argo and you restore them both which one would override or which",
    "start": "1556279",
    "end": "1563539"
  },
  {
    "text": "how would they conflict um so there is settings in Argo to let you",
    "start": "1563539",
    "end": "1570320"
  },
  {
    "text": "backup sorry let you um override if someone something else",
    "start": "1570320",
    "end": "1577400"
  },
  {
    "text": "has deployed it or not override or something is owned by something else but typically Argo would override anything",
    "start": "1577400",
    "end": "1584659"
  },
  {
    "text": "that it it thinks it owns so if we use Valero and then we have Argo in that",
    "start": "1584659",
    "end": "1590900"
  },
  {
    "text": "cluster and they're both like Valero would only run once you only restore it once that's a one-time thing",
    "start": "1590900",
    "end": "1596059"
  },
  {
    "text": "but Argo would constantly be reconciling the state of your cluster based on what's in git so it would override it",
    "start": "1596059",
    "end": "1603620"
  },
  {
    "text": "so you would basically restore from Valero and then install Argo afterwards",
    "start": "1603620",
    "end": "1608659"
  },
  {
    "text": "and let's do its thing yeah if we if we if we had to but for now like",
    "start": "1608659",
    "end": "1614539"
  },
  {
    "text": "because we're stateless we just use our go to get the applications running and then the state is outside the cluster",
    "start": "1614539",
    "end": "1621200"
  },
  {
    "text": "if there's any data okay thanks thank you just a quick question how would you deal",
    "start": "1621200",
    "end": "1628220"
  },
  {
    "text": "with the whole region failure in that scenario a whole region failure and with eks",
    "start": "1628220",
    "end": "1635740"
  },
  {
    "text": "yeah okay so yes so we we have done disaster recovery",
    "start": "1635840",
    "end": "1642620"
  },
  {
    "text": "um testing for like a region failure like failure from one region to another this would probably involve your data",
    "start": "1642620",
    "end": "1649460"
  },
  {
    "text": "stores so um simplest way is for us we do have",
    "start": "1649460",
    "end": "1654919"
  },
  {
    "text": "like our Central data store it's in one region we'd have to make sure that's running in multiple regions if we wanted",
    "start": "1654919",
    "end": "1660799"
  },
  {
    "text": "to cover that use case so it really depends on your business needs for us we've considered it",
    "start": "1660799",
    "end": "1668779"
  },
  {
    "text": "um if it is it worth the downtime versus the cost of constantly maintaining a cross region cluster depends on your",
    "start": "1668779",
    "end": "1675980"
  },
  {
    "text": "data store we use we also use postgres it",
    "start": "1675980",
    "end": "1681559"
  },
  {
    "text": "really like more like a business decision like with your um time to recover I mean time to recover",
    "start": "1681559",
    "end": "1688580"
  },
  {
    "text": "sorry the is that so yeah it depends okay thank you",
    "start": "1688580",
    "end": "1696039"
  },
  {
    "text": "how do you manage Secrets because that's the thing is that if you restore everything but you cannot access your your secrets anymore",
    "start": "1698419",
    "end": "1705679"
  },
  {
    "text": "what type of Secrets like to start the application um yeah for example to access your data",
    "start": "1705679",
    "end": "1711140"
  },
  {
    "text": "store so I guess the applications need to have access to the secrets to access your data source yes true so you would",
    "start": "1711140",
    "end": "1716419"
  },
  {
    "text": "make sure those can be deployed via Argo CD as well we have",
    "start": "1716419",
    "end": "1721880"
  },
  {
    "text": "um one way there is plugins for Helm Helm secrets that you can encrypt your secrets using",
    "start": "1721880",
    "end": "1728240"
  },
  {
    "text": "a tool called Sops and it would encrypt the values file and then decrypt it Argo can decrypt it at runtime like when it",
    "start": "1728240",
    "end": "1735620"
  },
  {
    "text": "templates it there are other tools like external Secrets operator that can fetch from external stores like",
    "start": "1735620",
    "end": "1742940"
  },
  {
    "text": "secret manager or gcp Secret store but as long as you when you deploy those",
    "start": "1742940",
    "end": "1749539"
  },
  {
    "text": "the same all together with your application Argo then it'll it'll pull in the secrets and deploy those so the",
    "start": "1749539",
    "end": "1756440"
  },
  {
    "text": "goal is Argo will deploy everything that it needs to get your application up and running including your secrets if you",
    "start": "1756440",
    "end": "1761960"
  },
  {
    "text": "want to include infrastructure you can do that as well thank you thank you",
    "start": "1761960",
    "end": "1769000"
  },
  {
    "text": "how many notes are in your cluster and how many applications are running on it",
    "start": "1769779",
    "end": "1774799"
  },
  {
    "text": "oh no it's an application so we have several um",
    "start": "1774799",
    "end": "1779960"
  },
  {
    "text": "you have different production clusters of different sizes our biggest ones is like sometimes 300 to 500 nodes we have",
    "start": "1779960",
    "end": "1787340"
  },
  {
    "text": "Auto scaling so it depends applications we have some amount of",
    "start": "1787340",
    "end": "1792440"
  },
  {
    "text": "micro Services some macro services so something under around 50",
    "start": "1792440",
    "end": "1799100"
  },
  {
    "text": "yeah thanks thank you",
    "start": "1799100",
    "end": "1804760"
  },
  {
    "text": "thank you for your talk one question for this diagram here you're not using anything persistent right you don't have",
    "start": "1806360",
    "end": "1811880"
  },
  {
    "text": "persistent volumes sorry could you uh say that again to the mic yeah so in this case for example if",
    "start": "1811880",
    "end": "1818360"
  },
  {
    "text": "you have status how would you handle that oh if we have stateful set and how do we",
    "start": "1818360",
    "end": "1823940"
  },
  {
    "text": "migrate from one to the other um so we do have some services that cannot",
    "start": "1823940",
    "end": "1830120"
  },
  {
    "text": "be run on both clusters at the same time and when we do those migrations well",
    "start": "1830120",
    "end": "1835580"
  },
  {
    "text": "specifically we have those teams it's a different we can't have it running on both so they",
    "start": "1835580",
    "end": "1841580"
  },
  {
    "text": "have to schedule some type of migration period why do they go down or they have a highly available way it really depends",
    "start": "1841580",
    "end": "1849620"
  },
  {
    "text": "on the service but in Argo we would deploy them with selectors we use the",
    "start": "1849620",
    "end": "1856460"
  },
  {
    "text": "cluster generator with with selectors to deploy only to a specific like the degree the blue cluster at the top and",
    "start": "1856460",
    "end": "1863120"
  },
  {
    "text": "then when they're ready to move to the other cluster and that cluster is live they change their selectors to deploy to",
    "start": "1863120",
    "end": "1869720"
  },
  {
    "text": "the other cluster so most of our applications don't have to know what cluster they're running on they just",
    "start": "1869720",
    "end": "1876020"
  },
  {
    "text": "deploy to our deploy through our githubs pipelines some Services we have some Kafka services and other things they do",
    "start": "1876020",
    "end": "1883340"
  },
  {
    "text": "have to know what cluster they're on they have to actually know the name of the cluster no but I mean more about data I mean",
    "start": "1883340",
    "end": "1889460"
  },
  {
    "text": "what do you do with the data to transfer between the two oh the data stores yeah yes I mean yeah I mean volumes that you",
    "start": "1889460",
    "end": "1896539"
  },
  {
    "text": "store some data let's say like oh so so yeah like I mentioned most of our we don't Stores um stateful services with their own PVCs",
    "start": "1896539",
    "end": "1904220"
  },
  {
    "text": "um and we do have some but they're not critical most of our data stores are outside so like they're in within our",
    "start": "1904220",
    "end": "1911000"
  },
  {
    "text": "VPC and AWS and the applications can access them from both clusters so we keep our data stores",
    "start": "1911000",
    "end": "1917779"
  },
  {
    "text": "separate such that we can have multiple clusters accessing the same data store",
    "start": "1917779",
    "end": "1923000"
  },
  {
    "text": "in one environment thank you thank you",
    "start": "1923000",
    "end": "1928360"
  },
  {
    "text": "yeah thanks for the presentation uh do you or does your Disaster Recovery plan also cover for third party dependencies",
    "start": "1928880",
    "end": "1935960"
  },
  {
    "text": "like let's say um cloudflare or sorry sorry could you",
    "start": "1935960",
    "end": "1941480"
  },
  {
    "text": "say it again into the mic it's uh does it does your Disaster Recovery plan also cover for uh Cloud flare or any other",
    "start": "1941480",
    "end": "1949539"
  },
  {
    "text": "external dependencies yeah this is the thing which I'd like to talk more people about disaster recovery",
    "start": "1949539",
    "end": "1955880"
  },
  {
    "text": "but we have a certain like say we have a certain scope of what we would consider a disaster and it may",
    "start": "1955880",
    "end": "1961760"
  },
  {
    "text": "not be the same every time we run it and then we we plan for that of course there is things like well what if our",
    "start": "1961760",
    "end": "1968740"
  },
  {
    "text": "CDN goes down that's a different thing so um",
    "start": "1968740",
    "end": "1973880"
  },
  {
    "text": "we have certain plans some things we have more thoughts about but we yeah",
    "start": "1973880",
    "end": "1979220"
  },
  {
    "text": "we we what we usually consider is like okay if a cluster is down or if a region is down can we move it to somewhere else",
    "start": "1979220",
    "end": "1986899"
  },
  {
    "text": "can we mitigate that um sometimes it's like DNS is down let's",
    "start": "1986899",
    "end": "1992600"
  },
  {
    "text": "just go take a break you know it really depends on on on the business",
    "start": "1992600",
    "end": "1997940"
  },
  {
    "text": "and what's willing to accept um so for us we can accept a certain amount of downtime if",
    "start": "1997940",
    "end": "2004299"
  },
  {
    "text": "you know sometimes our customers are down also so it really yeah thank you",
    "start": "2004299",
    "end": "2010000"
  },
  {
    "text": "thank you so you uh talked about like you moved",
    "start": "2010000",
    "end": "2018640"
  },
  {
    "text": "the load balancer to the new cluster do you do that manually um currently yes so when we're migrating",
    "start": "2018640",
    "end": "2025120"
  },
  {
    "text": "clusters were let's say migrating traffic we would we would do either DNS",
    "start": "2025120",
    "end": "2031000"
  },
  {
    "text": "load balancing um specifically in our in our case we have",
    "start": "2031000",
    "end": "2037659"
  },
  {
    "text": "cdns in front of them so DNS doesn't always um apply or work as easy so",
    "start": "2037659",
    "end": "2045779"
  },
  {
    "text": "there is like other services like there's a global load balancing service we use in front of our cdns but then we",
    "start": "2046120",
    "end": "2052960"
  },
  {
    "text": "shift traffic over from one to the other so basically we we manually shift",
    "start": "2052960",
    "end": "2058179"
  },
  {
    "text": "traffic over but something we do whenever we upgrade a cluster it's and it's like",
    "start": "2058179",
    "end": "2065320"
  },
  {
    "text": "and we make sure okay and then if if we if we see that some when we're migrating traffic that it's not working properly",
    "start": "2065320",
    "end": "2071919"
  },
  {
    "text": "in a new classroom just move back so it's something manual right now but we're fine with that",
    "start": "2071919",
    "end": "2077820"
  },
  {
    "text": "so so the load balancer service do you create like new load balancers in front",
    "start": "2078220",
    "end": "2083440"
  },
  {
    "text": "of eks and then move yeah yeah so we have two different like each one has its own",
    "start": "2083440",
    "end": "2089378"
  },
  {
    "text": "environment like when you when you have Ingress we have a we have an English controller and that creates load",
    "start": "2089379",
    "end": "2095618"
  },
  {
    "text": "balancers in the cloud and then we balance that from either the DNS or the uh",
    "start": "2095619",
    "end": "2101380"
  },
  {
    "text": "a higher Point depending on what what you what we use thank you thank you",
    "start": "2101380",
    "end": "2108780"
  },
  {
    "start": "2106000",
    "end": "2192000"
  },
  {
    "text": "um thank you about the github's report so is it like one God Ripple with all",
    "start": "2109599",
    "end": "2114880"
  },
  {
    "text": "your deployment into it or is it multiple ones yeah so this is something where we we were learning from or",
    "start": "2114880",
    "end": "2121420"
  },
  {
    "text": "initially we were thinking Hmm should we have multiple repos one for service one",
    "start": "2121420",
    "end": "2127420"
  },
  {
    "text": "per environment uh one for production pre-production um",
    "start": "2127420",
    "end": "2133660"
  },
  {
    "text": "what we went for was one massive repo we call this the we actually called like the Kraken repo",
    "start": "2133660",
    "end": "2138940"
  },
  {
    "text": "and um it houses all our different um get UPS files from different",
    "start": "2138940",
    "end": "2144520"
  },
  {
    "text": "environments but the good thing about it is all those commits we see there we say okay someone deployed something here",
    "start": "2144520",
    "end": "2150520"
  },
  {
    "text": "someone did this someone did that it's all in one place it's just an overhead of what are you willing to do to manage",
    "start": "2150520",
    "end": "2157060"
  },
  {
    "text": "your how do you manager get get repos and how do you manage access to that because",
    "start": "2157060",
    "end": "2162700"
  },
  {
    "text": "what we consider now is a deployment now is a commit to that git repo so we have to lock that down because whoever has",
    "start": "2162700",
    "end": "2168940"
  },
  {
    "text": "access to the git repo essentially has can change our clusters so for now it's it's Central and we've",
    "start": "2168940",
    "end": "2176440"
  },
  {
    "text": "locked it down just at the CI process um and those specific authorized users",
    "start": "2176440",
    "end": "2181599"
  },
  {
    "text": "can change it um yeah it's a monorepo and there are specific settings",
    "start": "2181599",
    "end": "2188079"
  },
  {
    "text": "in Argo CD if you have a monorepole okay thank you thank you",
    "start": "2188079",
    "end": "2193619"
  }
]