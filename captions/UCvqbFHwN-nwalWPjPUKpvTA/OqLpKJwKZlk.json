[
  {
    "text": "well uh welcome everyone uh today we are going to talk about uh operating global",
    "start": "160",
    "end": "7200"
  },
  {
    "text": "scale Prometheus deployments on Kubernetes uh I'm Orchin uh principal",
    "start": "7200",
    "end": "12800"
  },
  {
    "text": "engineer at AWS uh focusing on open source observability solutions and I'm",
    "start": "12800",
    "end": "19279"
  },
  {
    "text": "Alan uh I'm also software engineer at AWS and a cortex maintainer",
    "start": "19279",
    "end": "26439"
  },
  {
    "text": "all right we have uh a lot of challenges operating at global scale uh and on the",
    "start": "26960",
    "end": "34160"
  },
  {
    "text": "bottom right side uh you can see a postit that I prepared when I joined the team two and a half years ago about the",
    "start": "34160",
    "end": "41200"
  },
  {
    "text": "challenges we had back then and a priority order we are not going to talk through every single one of the problems",
    "start": "41200",
    "end": "47360"
  },
  {
    "text": "and challenges but a subset of the challenges are going to act as our rough",
    "start": "47360",
    "end": "52559"
  },
  {
    "text": "agenda today we're going to touch base on availability multi-tenency uh blast radius reduction operating at global",
    "start": "52559",
    "end": "59920"
  },
  {
    "text": "scale and Kubernetes complexity before we dive in let's",
    "start": "59920",
    "end": "65439"
  },
  {
    "text": "rewind a bit uh in about let's uh talk through uh talk about the ecosystem and",
    "start": "65439",
    "end": "71600"
  },
  {
    "text": "how the ecosystem has evolved uh so in 2012 uh Prometheus was uh started uh and",
    "start": "71600",
    "end": "79600"
  },
  {
    "text": "then open sourced in uh 2015 uh Cortex project a Prometheus compatible uh",
    "start": "79600",
    "end": "86080"
  },
  {
    "text": "scalable uh open source project released shortly after and we released uh manage",
    "start": "86080",
    "end": "92439"
  },
  {
    "text": "Prometheus uh by Amazon um at 2020 we",
    "start": "92439",
    "end": "98000"
  },
  {
    "text": "wanted to take away the operational uh burden of uh handling the infrastructure",
    "start": "98000",
    "end": "104960"
  },
  {
    "text": "from the users uh we have an open-source first uh tenant uh so any improvement",
    "start": "104960",
    "end": "112640"
  },
  {
    "text": "that we do uh we contribute back to open source community and you can see that uh",
    "start": "112640",
    "end": "118479"
  },
  {
    "text": "from our commits uh over the past few years in the cortex",
    "start": "118479",
    "end": "124159"
  },
  {
    "text": "repository so before we launch the service let's go through uh what we",
    "start": "124439",
    "end": "129759"
  },
  {
    "text": "wanted to launch back in 2020 uh we wanted to provide a fully managed",
    "start": "129759",
    "end": "135760"
  },
  {
    "text": "serverless Prometheus compatible monitoring service by AWS that is",
    "start": "135760",
    "end": "141360"
  },
  {
    "text": "scalable highly available integrated with AWS services secure supports IM",
    "start": "141360",
    "end": "147760"
  },
  {
    "text": "authorization as well as data encryption and it can support data retention up to",
    "start": "147760",
    "end": "154080"
  },
  {
    "text": "multiple years as I mentioned earlier we uh use",
    "start": "154080",
    "end": "160400"
  },
  {
    "text": "Cortex under the hood uh but uh we can talk let's talk a little bit about uh",
    "start": "160400",
    "end": "166120"
  },
  {
    "text": "Prometheus so Prometheus is an open-source purpose-built uh time series",
    "start": "166120",
    "end": "171400"
  },
  {
    "text": "database that comes with a very powerful query language called PromQL uh it",
    "start": "171400",
    "end": "177599"
  },
  {
    "text": "enables analytical query capabilities on your time series data but uh also",
    "start": "177599",
    "end": "183680"
  },
  {
    "text": "integrates really well with Kubernetes and it's widely adopted format and",
    "start": "183680",
    "end": "189280"
  },
  {
    "text": "supports wide range of exporters uh in the community however uh it inherently",
    "start": "189280",
    "end": "195599"
  },
  {
    "text": "comes with uh limitations the primary one uh that is our concern is that it's",
    "start": "195599",
    "end": "202560"
  },
  {
    "text": "a single machine uh and it's deployed in a single machine it's not a distributed",
    "start": "202560",
    "end": "207720"
  },
  {
    "text": "system it uh is a monolithic service so",
    "start": "207720",
    "end": "213040"
  },
  {
    "text": "uh depending on your use case your uh your use cases will be competing against",
    "start": "213040",
    "end": "218400"
  },
  {
    "text": "the same resources in the host uh meaning uh if you have an ingest heavy",
    "start": "218400",
    "end": "224640"
  },
  {
    "text": "rightheavy uh workload uh then your uh queries are going to take a hit or your",
    "start": "224640",
    "end": "231680"
  },
  {
    "text": "alarming uh alerting and rule engine will take a hit so uh there's no a",
    "start": "231680",
    "end": "237920"
  },
  {
    "text": "redundancy whatsoever and uh there are retention limits because uh your",
    "start": "237920",
    "end": "243360"
  },
  {
    "text": "retention limit is basically going to be limited by the host's uh disks that you",
    "start": "243360",
    "end": "248720"
  },
  {
    "text": "can attach to a host",
    "start": "248720",
    "end": "252400"
  },
  {
    "text": "so okay uh as Orchin was saying uh Prometheus uh even though we really",
    "start": "253920",
    "end": "259120"
  },
  {
    "text": "wanted to use Prometheus uh it didn't fit uh our use case so we kept looking",
    "start": "259120",
    "end": "265199"
  },
  {
    "text": "around uh exploring different uh solutions out there and we end up uh",
    "start": "265199",
    "end": "271280"
  },
  {
    "text": "picking Cortex what's Cortex cortex is a CNCF project uh the the main proposal of",
    "start": "271280",
    "end": "278000"
  },
  {
    "text": "cortex was breaking down this uh single process uh solution that's prometheus in",
    "start": "278000",
    "end": "284560"
  },
  {
    "text": "several micros service each one of those microservices would have a very specific",
    "start": "284560",
    "end": "289759"
  },
  {
    "text": "role uh in a cortex cluster and uh what's good as well is that cortex use",
    "start": "289759",
    "end": "297600"
  },
  {
    "text": "prometheus code bas under the hood so cortex import prometheus as a library",
    "start": "297600",
    "end": "303199"
  },
  {
    "text": "and this makes uh makes easier for for Cortex to be fully backward fully",
    "start": "303199",
    "end": "309360"
  },
  {
    "text": "compatible with Prometheus and also is easier for us to uh backport new",
    "start": "309360",
    "end": "314560"
  },
  {
    "text": "features that Prometheus is uh is developing such as like native histogram",
    "start": "314560",
    "end": "321720"
  },
  {
    "text": "lately so this is a very high level view of a cortex architecture uh and how the",
    "start": "321720",
    "end": "326880"
  },
  {
    "text": "components interact with each other we are not going into details of what each component does uh we had uh talks about",
    "start": "326880",
    "end": "335280"
  },
  {
    "text": "that in the past already but the main thing here is that uh we have basically",
    "start": "335280",
    "end": "340320"
  },
  {
    "text": "three paths we have the right path in red which includes distributors and uh",
    "start": "340320",
    "end": "346680"
  },
  {
    "text": "ingesters we have the read path in blue uh that in includes store gateways",
    "start": "346680",
    "end": "351919"
  },
  {
    "text": "queries and query front end and the alerts and ruler path in green that",
    "start": "351919",
    "end": "356960"
  },
  {
    "text": "includes the alert manager and the rulers uh service okay cortex not just break break",
    "start": "356960",
    "end": "364560"
  },
  {
    "text": "down Prometheus into different microser but also allow us to scale those microser uh horizontally this means that",
    "start": "364560",
    "end": "372639"
  },
  {
    "text": "in a Prometheus setup we are not bounded by the size of a single host and more I",
    "start": "372639",
    "end": "378400"
  },
  {
    "text": "can scale my fleet uh to handle more traffic uh as the traffic increase also",
    "start": "378400",
    "end": "384400"
  },
  {
    "text": "having those different microservices gives us lots of flexibility in how we scale our cortex cluster so let's say",
    "start": "384400",
    "end": "390880"
  },
  {
    "text": "that if you have a read read have have uh workload we can just scale the read",
    "start": "390880",
    "end": "398479"
  },
  {
    "text": "path microser and those those microservices will not interfere with each other uh so there is some points of",
    "start": "398479",
    "end": "406560"
  },
  {
    "text": "contact between the the two the paths but usually uh they are like the read",
    "start": "406560",
    "end": "412960"
  },
  {
    "text": "path the right path and the alert rules and rules path are very segregated also Cortex uh offer uh",
    "start": "412960",
    "end": "422039"
  },
  {
    "text": "autobalance uh in response to any scaling activity what that means is that",
    "start": "422039",
    "end": "427280"
  },
  {
    "text": "like uh we can uh scale our fleet and cortex will uh take care of rebalancing",
    "start": "427280",
    "end": "433919"
  },
  {
    "text": "the load between nodes and recharding the data between nodes uh and it use",
    "start": "433919",
    "end": "439840"
  },
  {
    "text": "under the hood like consistent hashing so like we we make sure that uh we have",
    "start": "439840",
    "end": "444880"
  },
  {
    "text": "min uh minimal redistribution of load uh when that happens so scaling up and down",
    "start": "444880",
    "end": "451360"
  },
  {
    "text": "cortex clusters is quite easy you can just basically go to a kubernetes deployment change the replica set cortex",
    "start": "451360",
    "end": "458160"
  },
  {
    "text": "will see the new nodes the nodes will register uh and it will start receiving",
    "start": "458160",
    "end": "463599"
  },
  {
    "text": "new nodes uh start receiving loads you can use simple things like uh kubernetes",
    "start": "463599",
    "end": "470160"
  },
  {
    "text": "hpa to scale up and down those microser also uh cortex implement uh",
    "start": "470160",
    "end": "478800"
  },
  {
    "text": "it's built in on cortex the concept of high high availability so cortex",
    "start": "478800",
    "end": "484000"
  },
  {
    "text": "implements high availability by replicating the data across nodes and disase uh and this together with a corum",
    "start": "484000",
    "end": "490720"
  },
  {
    "text": "read a corum operation means that a cortex cluster doesn't have any single",
    "start": "490720",
    "end": "495759"
  },
  {
    "text": "point of failure uh and can tolerates like uh hardware failures and a",
    "start": "495759",
    "end": "502680"
  },
  {
    "text": "outage also which was very important to to us is that cortex supports mult",
    "start": "502680",
    "end": "507759"
  },
  {
    "text": "tenants tendency out of the box uh cortex keeps uh data from different tenants in isolation so uh we don't",
    "start": "507759",
    "end": "515279"
  },
  {
    "text": "share any data on disk uh between tenants uh and also implements lots of",
    "start": "515279",
    "end": "521200"
  },
  {
    "text": "noise neighborhoods mitigations such as like uh cortex will give you limits and quotas uh by tenant to make sure that",
    "start": "521200",
    "end": "529279"
  },
  {
    "text": "one tenant cannot use all the uh costly resources and also strategies like",
    "start": "529279",
    "end": "534320"
  },
  {
    "text": "shuffle sharding that is a strategy that we try to minimize the amount of resource shared between tenants so in",
    "start": "534320",
    "end": "541920"
  },
  {
    "text": "this particular case we can see that for instance we have a bad tenant that's causing three hosts to overload but the",
    "start": "541920",
    "end": "548160"
  },
  {
    "text": "other tenants are happy because uh they don't share uh resources and the",
    "start": "548160",
    "end": "554000"
  },
  {
    "text": "one that they share like because of the redundancy and the high availability uh",
    "start": "554000",
    "end": "559040"
  },
  {
    "text": "the tenant doesn't have any impact whatsoever also uh finally cortex support a",
    "start": "559040",
    "end": "566080"
  },
  {
    "text": "long-term storage uh cortex uh only keep hot data on disk uh and ship this data",
    "start": "566080",
    "end": "574399"
  },
  {
    "text": "every few hours to a object storage like S3 or uh or Google uh GCS it means that",
    "start": "574399",
    "end": "583200"
  },
  {
    "text": "like we are not longer bounded by a local disk of a host uh and this allow",
    "start": "583200",
    "end": "588320"
  },
  {
    "text": "us to increase our intention period from days to years which was something that our customers were were was",
    "start": "588320",
    "end": "595560"
  },
  {
    "text": "demanding okay uh everything seems fine like cortex is great is a good fit for",
    "start": "595560",
    "end": "601120"
  },
  {
    "text": "us let's go ahead but we still have problems to solve uh we don't want to have a we we don't want to have a single",
    "start": "601120",
    "end": "607839"
  },
  {
    "text": "cortex cluster that will grow indefinitely uh we knew from from previous experience that like especially",
    "start": "607839",
    "end": "614720"
  },
  {
    "text": "in big distributed systems uh we can have like a very uh hidden scaling",
    "start": "614720",
    "end": "620720"
  },
  {
    "text": "cliffs that only show ups uh when the cluster uh gets to a become too big uh",
    "start": "620720",
    "end": "628240"
  },
  {
    "text": "those hidden contention points can show up in different ways it can be some communication uh and coordination",
    "start": "628240",
    "end": "634519"
  },
  {
    "text": "overload and those things can cause also noncale nonlinear scaling which means",
    "start": "634519",
    "end": "640560"
  },
  {
    "text": "that you can end up uh when you get to a certain level certain size you end up",
    "start": "640560",
    "end": "646000"
  },
  {
    "text": "adding more hosts to hoping to uh be able to handle more load but the",
    "start": "646000",
    "end": "652160"
  },
  {
    "text": "overhead is so big that adding more uh more host is actually uh make things",
    "start": "652160",
    "end": "657279"
  },
  {
    "text": "worse like the host is making you use more resources and this can become as global effect so we knew that we",
    "start": "657279",
    "end": "663920"
  },
  {
    "text": "couldn't have like a single cluster for everything so then that's when we",
    "start": "663920",
    "end": "669040"
  },
  {
    "text": "started to think about cellular architecture so in a cellular architecture instead of having one",
    "start": "669040",
    "end": "674560"
  },
  {
    "text": "single cortex cluster we have multiple cortex clusters per region",
    "start": "674560",
    "end": "679880"
  },
  {
    "text": "uh each cortex cluster is now what we call cells uh so we instead of having",
    "start": "679880",
    "end": "686959"
  },
  {
    "text": "one we have a multiple uh deployments of cortex and each cell should be self-contained that means that it's not",
    "start": "686959",
    "end": "693440"
  },
  {
    "text": "only cortex that's deployed in the cell but everything else that needs to run cortex needs to run so like uh each cell",
    "start": "693440",
    "end": "700160"
  },
  {
    "text": "has its own S3 bucket uh each cell has like the whole authorization and the authentication st everything is",
    "start": "700160",
    "end": "707360"
  },
  {
    "text": "contained in one cell and finally we assign tenants or",
    "start": "707360",
    "end": "712720"
  },
  {
    "text": "customers to sales so uh we have every time that you create a resource in our",
    "start": "712720",
    "end": "718720"
  },
  {
    "text": "system we assign this resource to a cell and go from there this gives us lots of",
    "start": "718720",
    "end": "724480"
  },
  {
    "text": "uh lots of uh positives uh this means that we have",
    "start": "724480",
    "end": "729760"
  },
  {
    "text": "more scalability like we can always create more cells to handle more more loads to handle more clients it improves",
    "start": "729760",
    "end": "737680"
  },
  {
    "text": "our deployment safety because our deployment is now done by cell so if for",
    "start": "737680",
    "end": "743760"
  },
  {
    "text": "some reason we were shipping uh a bad code instead of having that bad code run",
    "start": "743760",
    "end": "749200"
  },
  {
    "text": "in my whole region I will first run in a cell uh if that thing has some problem",
    "start": "749200",
    "end": "755839"
  },
  {
    "text": "uh only the customers assigned it to that cell will have a problem similarly we have like a blast radius",
    "start": "755839",
    "end": "762079"
  },
  {
    "text": "reduction uh if for some reason uh cortex could not protect himself and the",
    "start": "762079",
    "end": "767279"
  },
  {
    "text": "tenant caused a cortex cluster to get unhealthy i have I I even more like",
    "start": "767279",
    "end": "772639"
  },
  {
    "text": "isolation between tenants because now this tenant can only affect one cluster or anything else like it can be like",
    "start": "772639",
    "end": "778720"
  },
  {
    "text": "some any type of disaster that causes the cortex cluster to get unhealthy also it decreases our mean",
    "start": "778720",
    "end": "786720"
  },
  {
    "text": "time to recover because like when we have a a incident the operator now can",
    "start": "786720",
    "end": "792320"
  },
  {
    "text": "look to a subset of your fleet with a subset of your customers instead of having to dive and like figure out uh",
    "start": "792320",
    "end": "800959"
  },
  {
    "text": "the your whole uh infrastructure okay so uh a little bit",
    "start": "800959",
    "end": "806639"
  },
  {
    "text": "about a cell architecture like those are the main components that we have uh we have a very thin layer called cell",
    "start": "806639",
    "end": "813399"
  },
  {
    "text": "router this layer is like the thinnest possible layer uh that only the only",
    "start": "813399",
    "end": "818720"
  },
  {
    "text": "responsibility is take the request from the outside world and route to a cell uh like this architecture is hidden from",
    "start": "818720",
    "end": "825200"
  },
  {
    "text": "the customer so the customer has a single point of entry in our system that goes to the cell router and the cell",
    "start": "825200",
    "end": "831200"
  },
  {
    "text": "router will uh will forward the request to the right cell we have the cell itself which is a",
    "start": "831200",
    "end": "838000"
  },
  {
    "text": "cortex deployment uh together with everything else uh as I said that needs",
    "start": "838000",
    "end": "843120"
  },
  {
    "text": "to be uh inside of the cell so the S3 bucket uh the authorization uh the the",
    "start": "843120",
    "end": "850240"
  },
  {
    "text": "Kubernetes cluster everything is inside of the cell uh and we have the control plane the control plane is basically",
    "start": "850240",
    "end": "856720"
  },
  {
    "text": "responsible for administration tasks it will like assign customer to sales it will will also provision new cells or do",
    "start": "856720",
    "end": "864240"
  },
  {
    "text": "uh or do or deprovision cells if it's needed okay a little bit about cell uh",
    "start": "864240",
    "end": "871440"
  },
  {
    "text": "capacity management inside of the same cell uh we keep the cells within a limit",
    "start": "871440",
    "end": "876959"
  },
  {
    "text": "that we deem safe uh and this limit we go we get to this limit uh doing lots of",
    "start": "876959",
    "end": "882240"
  },
  {
    "text": "different uh load test and types of test and this is the limit that we we feel",
    "start": "882240",
    "end": "887920"
  },
  {
    "text": "comfortable of running a cortex cluster so every time that we will roll out a",
    "start": "887920",
    "end": "893199"
  },
  {
    "text": "change this change needs to be tested with a cluster a pre-product cluster that is uh that's maxed out that is like",
    "start": "893199",
    "end": "900240"
  },
  {
    "text": "the size that uh we feel like that the max size that we can allow the cell to grow",
    "start": "900240",
    "end": "906560"
  },
  {
    "text": "so then we we inside of the cell we define two thresholds one is called like scaling threshold uh so which is the",
    "start": "906560",
    "end": "913680"
  },
  {
    "text": "threshold that we uh that we set uh to stop assigning new tenants to a cell and",
    "start": "913680",
    "end": "919760"
  },
  {
    "text": "we have a hard limit which is a limit that we don't want the cell to uh go above uh so when you have like uh so",
    "start": "919760",
    "end": "928959"
  },
  {
    "text": "when all all your cells are closer or above your scaling threshold we scale",
    "start": "928959",
    "end": "935040"
  },
  {
    "text": "out instead of scaling up a cell what that means that means that like we stop assigning tenants to that cells and we",
    "start": "935040",
    "end": "943040"
  },
  {
    "text": "scale out uh we scale out the we create a new cell to start assigning tenants to",
    "start": "943040",
    "end": "948320"
  },
  {
    "text": "the new cell uh okay but like what about the hard limit how can we prevent",
    "start": "948320",
    "end": "953759"
  },
  {
    "text": "tenants to grow inside of the cell and cross the hard limit well for that we",
    "start": "953759",
    "end": "958800"
  },
  {
    "text": "have some automations inside of the cell that allocated limits uh and quotas for customers so basically how that works is",
    "start": "958800",
    "end": "966240"
  },
  {
    "text": "we have like controllers that is like constantly looking at usage uh customer",
    "start": "966240",
    "end": "971320"
  },
  {
    "text": "usage uh and it will auto grant limit increases or grant give more limits to a",
    "start": "971320",
    "end": "977519"
  },
  {
    "text": "customer kind of outscaling the limit of the customer when is deemed safe so uh",
    "start": "977519",
    "end": "983199"
  },
  {
    "text": "the controller see like how much how much the limit was granted in this cell and if it's below the hard limit it says",
    "start": "983199",
    "end": "989680"
  },
  {
    "text": "it's it's safe you grant this controller also reclaims limits that's not used so",
    "start": "989680",
    "end": "995839"
  },
  {
    "text": "if the customer was using lots of limits yesterday uh and stopping using so we",
    "start": "995839",
    "end": "1000959"
  },
  {
    "text": "start to reclaim those limits to free up capacity to other customers and finally this controller",
    "start": "1000959",
    "end": "1007440"
  },
  {
    "text": "also pre-provision nodes uh when we create a new cell we don't create the",
    "start": "1007440",
    "end": "1012639"
  },
  {
    "text": "cell with like uh with uh a scaled to handle the full capacity so what we do",
    "start": "1012639",
    "end": "1020160"
  },
  {
    "text": "is like we create a cell with a minimal footprint and we let the cell grow organically uh and when the cell is",
    "start": "1020160",
    "end": "1027438"
  },
  {
    "text": "growing the the same controller before granting limit because a customer need it will pre-provision nodes uh when",
    "start": "1027439",
    "end": "1034720"
  },
  {
    "text": "those nodes are ready it will uh give the capacity to the customer okay uh that works great but",
    "start": "1034720",
    "end": "1042880"
  },
  {
    "text": "what happens if a customer inside of existing c inside of existing cell needs",
    "start": "1042880",
    "end": "1048319"
  },
  {
    "text": "more capacity uh well in that case we trigger a operation that we call cell",
    "start": "1048319",
    "end": "1054400"
  },
  {
    "text": "migration uh we create a new cell or we pick a",
    "start": "1054400",
    "end": "1060000"
  },
  {
    "text": "cell that is has free capacity as target of the migration we mirror this traffic",
    "start": "1060000",
    "end": "1066400"
  },
  {
    "text": "between this we for some time we mirror the traffic between the two cells uh and",
    "start": "1066400",
    "end": "1072160"
  },
  {
    "text": "this is to have same data in both cells uh then we start migrating",
    "start": "1072160",
    "end": "1078080"
  },
  {
    "text": "configurations and historical data from one cell to another remember that uh even like the historical data is is on a",
    "start": "1078080",
    "end": "1085600"
  },
  {
    "text": "S3 bucket that belongs to a cell because we don't want to end share between cells so we need to migrate historical data",
    "start": "1085600",
    "end": "1092559"
  },
  {
    "text": "and then uh when everything's done at the end we stop mirroring the traffic i",
    "start": "1092559",
    "end": "1098000"
  },
  {
    "text": "have all the data in the new cell and I start to route uh also the read operations to the new cell uh the",
    "start": "1098000",
    "end": "1105039"
  },
  {
    "text": "customer doesn't note the oper this this operation the only thing that uh the",
    "start": "1105039",
    "end": "1110559"
  },
  {
    "text": "customer may note and we are working to uh to fix is that in the end of this",
    "start": "1110559",
    "end": "1115919"
  },
  {
    "text": "operation the customer can uh receive some duplicated alerts uh but other than",
    "start": "1115919",
    "end": "1121840"
  },
  {
    "text": "that should be totally seamless for the",
    "start": "1121840",
    "end": "1125520"
  },
  {
    "text": "customer all right uh so you might be wondering how do you release all of this",
    "start": "1129480",
    "end": "1134559"
  },
  {
    "text": "complexity to over tens of regions hundreds of cells and clusters across",
    "start": "1134559",
    "end": "1141520"
  },
  {
    "text": "tens of thousands of hosts so what we do is what we call deployment waves in our",
    "start": "1141520",
    "end": "1148960"
  },
  {
    "text": "deployment process we want to release every change safely but we want to uh",
    "start": "1148960",
    "end": "1155919"
  },
  {
    "text": "respect the cellular boundaries as well as regional boundaries",
    "start": "1155919",
    "end": "1160960"
  },
  {
    "text": "uh in our pre-production steps we run our uh unit tests and uh make sure that",
    "start": "1160960",
    "end": "1168000"
  },
  {
    "text": "every everything is passing and then we deploy the new change to our what we",
    "start": "1168000",
    "end": "1173280"
  },
  {
    "text": "call beta environment and in our beta environment we run our integration tests",
    "start": "1173280",
    "end": "1179039"
  },
  {
    "text": "etc and if everything looks good we start uh what we call uh production",
    "start": "1179039",
    "end": "1185520"
  },
  {
    "text": "waves each wave uh consists of multiple region deployments uh as the big waves",
    "start": "1185520",
    "end": "1192799"
  },
  {
    "text": "you can see up top uh we pick a rather small region to begin with our",
    "start": "1192799",
    "end": "1198640"
  },
  {
    "text": "deployment process uh and each wave consists of subwaves and the subwaves we",
    "start": "1198640",
    "end": "1206400"
  },
  {
    "text": "start deploying our change to our gamma environment first in our GMA environment",
    "start": "1206400",
    "end": "1211919"
  },
  {
    "text": "you will realize that we deploy our changes to two cells to be able to test",
    "start": "1211919",
    "end": "1218400"
  },
  {
    "text": "uh our uh changes in cortex and also cellular",
    "start": "1218400",
    "end": "1224200"
  },
  {
    "text": "router and in our approval steps uh as I mentioned before we go through our",
    "start": "1224200",
    "end": "1230000"
  },
  {
    "text": "integration tests we uh have canaries running 24/7 in our gamma and prod",
    "start": "1230000",
    "end": "1236640"
  },
  {
    "text": "environments uh we wait for a few hours to get the signals from our environment and see if",
    "start": "1236640",
    "end": "1244000"
  },
  {
    "text": "there's anything breaking uh with our changes if there is uh we look at what",
    "start": "1244000",
    "end": "1249840"
  },
  {
    "text": "the issues are and then uh decide uh to roll back the change or uh we roll",
    "start": "1249840",
    "end": "1256600"
  },
  {
    "text": "forward uh in our first production wave we pick a cell from a small region we",
    "start": "1256600",
    "end": "1263600"
  },
  {
    "text": "deploy our changes we run our approval steps again and uh in this phase if we",
    "start": "1263600",
    "end": "1270960"
  },
  {
    "text": "notice that if uh any alarms are firing uh the system auto rolls back the",
    "start": "1270960",
    "end": "1277559"
  },
  {
    "text": "changes so this is addressing one of the fundamental problems with the blast",
    "start": "1277559",
    "end": "1283200"
  },
  {
    "text": "radius uh so when we uh deploy a faulty change we do not want to impact",
    "start": "1283200",
    "end": "1289840"
  },
  {
    "text": "worldwide we do not want to impact the whole region only a subset of customers",
    "start": "1289840",
    "end": "1295760"
  },
  {
    "text": "and users are impacted by this change but uh majority of the time things are",
    "start": "1295760",
    "end": "1301840"
  },
  {
    "text": "looking bright and then we move on to our next phase uh in the next way we uh",
    "start": "1301840",
    "end": "1307200"
  },
  {
    "text": "deploy our change to the remaining cells in that region uh we go through our",
    "start": "1307200",
    "end": "1312559"
  },
  {
    "text": "approval process again and then uh in the next big wave now we pick two",
    "start": "1312559",
    "end": "1319480"
  },
  {
    "text": "regions uh now that we have gained confidence in our first wave uh we",
    "start": "1319480",
    "end": "1327120"
  },
  {
    "text": "slowly uh expand the scope of our release uh we pick the cells in those",
    "start": "1327120",
    "end": "1335280"
  },
  {
    "text": "regions in gamma environment we deploy our changes we go through our approval processes again and then uh notice that",
    "start": "1335280",
    "end": "1342799"
  },
  {
    "text": "we pick a single cell in all these regions in our first prod first prodwave",
    "start": "1342799",
    "end": "1350400"
  },
  {
    "text": "uh we wait we uh see if everything is looking great uh and then we move on to",
    "start": "1350400",
    "end": "1356960"
  },
  {
    "text": "the next uh cells in that region notice that the subwaves and number of subways",
    "start": "1356960",
    "end": "1364080"
  },
  {
    "text": "uh can increase depending on how many cells we have in the targeted regions um",
    "start": "1364080",
    "end": "1370799"
  },
  {
    "text": "the process rinse and repeats to uh next wave of regions we exponentially",
    "start": "1370799",
    "end": "1377360"
  },
  {
    "text": "increase the number of regions that we deploy our changes to and the overall",
    "start": "1377360",
    "end": "1382720"
  },
  {
    "text": "deployment for a single change uh takes uh in the order of days",
    "start": "1382720",
    "end": "1390159"
  },
  {
    "text": "so a quick word about testing as I mentioned we do our unit test acceptance test and like canary tests as I'm sure",
    "start": "1390159",
    "end": "1396960"
  },
  {
    "text": "most of you all do uh but we also do fuz fuzziness and correctness test what that",
    "start": "1396960",
    "end": "1403039"
  },
  {
    "text": "means is especially for our query capabilities given from QL supports wide",
    "start": "1403039",
    "end": "1409520"
  },
  {
    "text": "uh a range of queries if we are making any changes on our query path we want to",
    "start": "1409520",
    "end": "1414559"
  },
  {
    "text": "make sure that we are not introducing regressions on the query logic in order",
    "start": "1414559",
    "end": "1420400"
  },
  {
    "text": "to test that what we do is uh we generate bunch of random queries and",
    "start": "1420400",
    "end": "1428320"
  },
  {
    "text": "then we when we deploy our changes to our GMA environment we uh look at the",
    "start": "1428320",
    "end": "1434400"
  },
  {
    "text": "results from Cortex and then we also have a vanilla Prometheus instance running in our GMA environment that we",
    "start": "1434400",
    "end": "1440880"
  },
  {
    "text": "compare the results to if things are looking fine uh we uh wait a little bit",
    "start": "1440880",
    "end": "1447120"
  },
  {
    "text": "uh to gain our uh gain confidence and release our changes we also do fault",
    "start": "1447120",
    "end": "1452320"
  },
  {
    "text": "injection between um weight production waves uh what that",
    "start": "1452320",
    "end": "1458480"
  },
  {
    "text": "means is uh we basically test the resiliency mechanisms that we have in our system to test such as like uh",
    "start": "1458480",
    "end": "1465840"
  },
  {
    "text": "against a impairment so what happens inside a cell's",
    "start": "1465840",
    "end": "1473120"
  },
  {
    "text": "deployment uh that's when we deploy our cortex uh images to our Kubernetes",
    "start": "1473120",
    "end": "1479039"
  },
  {
    "text": "cluster so uh quick word about our uh architecture is that even though uh as",
    "start": "1479039",
    "end": "1486880"
  },
  {
    "text": "Alan mentioned uh we can tolerate disruption in a single a system incurs a",
    "start": "1486880",
    "end": "1494559"
  },
  {
    "text": "small uh downtime when uh two inesttors are down from uh multiple a so how is",
    "start": "1494559",
    "end": "1503520"
  },
  {
    "text": "this relevant to our deployment process so we have a stateful set when uh we are",
    "start": "1503520",
    "end": "1510320"
  },
  {
    "text": "uh for our injusters when we are deploying cortex so pod disruption",
    "start": "1510320",
    "end": "1515360"
  },
  {
    "text": "budget doesn't necessarily take zone into account when it's replacing pods so",
    "start": "1515360",
    "end": "1522640"
  },
  {
    "text": "if we are taking two pod if we are replacing two pods it's highly likely that we are going uh we are going to be",
    "start": "1522640",
    "end": "1530880"
  },
  {
    "text": "uh taking two pods from different as we don't want to cause any uh impact to our",
    "start": "1530880",
    "end": "1536840"
  },
  {
    "text": "customers so we introduced zone aware controllers sonware controllers are open",
    "start": "1536840",
    "end": "1542480"
  },
  {
    "text": "sourced and you can use that um in your projects if you have stateful sets and",
    "start": "1542480",
    "end": "1548960"
  },
  {
    "text": "relying on u multiple ACS uh how that works is that as you can see from the uh",
    "start": "1548960",
    "end": "1555679"
  },
  {
    "text": "top left um graph uh we start taking uh replacing",
    "start": "1555679",
    "end": "1562159"
  },
  {
    "text": "not uh nodes uh in uh one a we start small uh we take one and then we take",
    "start": "1562159",
    "end": "1569279"
  },
  {
    "text": "two and then we take four we exponentially grow and then once that a's deployment finishes we move on to",
    "start": "1569279",
    "end": "1576400"
  },
  {
    "text": "the next a so this process uh speeds speeds up",
    "start": "1576400",
    "end": "1582000"
  },
  {
    "text": "our deployment uh by a huge",
    "start": "1582000",
    "end": "1586400"
  },
  {
    "text": "margin so what happens after we deploy all of this complexity uh the great",
    "start": "1587080",
    "end": "1592400"
  },
  {
    "text": "failures can happen if you are operating your service uh that relies on tens of",
    "start": "1592400",
    "end": "1597520"
  },
  {
    "text": "thousands of hosts uh any number of those hosts uh can uh go down at any",
    "start": "1597520",
    "end": "1603440"
  },
  {
    "text": "time we do our health checks but there are gray failures uh that uh cannot be",
    "start": "1603440",
    "end": "1610480"
  },
  {
    "text": "necessarily detected through uh health checks so we do host health monitoring",
    "start": "1610480",
    "end": "1616799"
  },
  {
    "text": "to be able to detect gray failures such as network and disk failures uh we uh we",
    "start": "1616799",
    "end": "1623200"
  },
  {
    "text": "use multiple signals to determine if a host needs replacement such as 5xx count",
    "start": "1623200",
    "end": "1629600"
  },
  {
    "text": "or latency we replace the node even if you get a false negative signal to not",
    "start": "1629600",
    "end": "1636240"
  },
  {
    "text": "disrupt the service at all you can use this mechanism to pay",
    "start": "1636240",
    "end": "1642159"
  },
  {
    "text": "away uh from an a in case of a failures if you'd",
    "start": "1642159",
    "end": "1647919"
  },
  {
    "text": "like as we are uh looking at our deployments we also",
    "start": "1648120",
    "end": "1654159"
  },
  {
    "text": "noticed that uh Carpenter project in for Kubernetes is taking off and we are in",
    "start": "1654159",
    "end": "1659679"
  },
  {
    "text": "the process of moving to Carpenter uh Carpenter provides us a better transparency and debugability",
    "start": "1659679",
    "end": "1666799"
  },
  {
    "text": "during deployments which helps uh speeding up the deployments on our observations we also noticed that uh",
    "start": "1666799",
    "end": "1675360"
  },
  {
    "text": "Carpenter provides faster scaling characteristics and in operating uh",
    "start": "1675360",
    "end": "1680640"
  },
  {
    "text": "global scale uh you can have capacity constraints on the hosts that you are",
    "start": "1680640",
    "end": "1687279"
  },
  {
    "text": "running your service uh host types specifically so with carpenter we can uh",
    "start": "1687279",
    "end": "1694480"
  },
  {
    "text": "pick multiple instance types to run our service and then uh we do not get any uh",
    "start": "1694480",
    "end": "1700880"
  },
  {
    "text": "capacity constraints so what's next for cortex uh",
    "start": "1700880",
    "end": "1706720"
  },
  {
    "text": "we are uh uh as I mentioned before we are tolerant across sing uh single a",
    "start": "1706720",
    "end": "1713399"
  },
  {
    "text": "outages but however due to how we shard our data we incur a short small impact",
    "start": "1713399",
    "end": "1720080"
  },
  {
    "text": "when two injusters are down uh from two different ACS because uh of how the series are",
    "start": "1720080",
    "end": "1728480"
  },
  {
    "text": "assigned to injesttors what we are thinking of is introducing a new concept called uh",
    "start": "1728480",
    "end": "1736760"
  },
  {
    "text": "partitions instead of uh directly assigning series into inesttors we are going to assign series to partitions and",
    "start": "1736760",
    "end": "1746159"
  },
  {
    "text": "the partitions will be assigned to inesttors so how does this work in",
    "start": "1746159",
    "end": "1753559"
  },
  {
    "text": "theory um with the introduction of partitions we are more resilient towards",
    "start": "1753559",
    "end": "1760720"
  },
  {
    "text": "multiple injesttors going down across multiple ass notice that the customer or",
    "start": "1760720",
    "end": "1767200"
  },
  {
    "text": "any time series is not going to have any impact even if more than three ingesters",
    "start": "1767200",
    "end": "1774559"
  },
  {
    "text": "are down from three different aes the only downside is we can only",
    "start": "1774559",
    "end": "1780880"
  },
  {
    "text": "have impact if two inesttors uh are down from the same partition but which is",
    "start": "1780880",
    "end": "1788799"
  },
  {
    "text": "less likely to happen compared to our previous state that being said one of the next things",
    "start": "1788799",
    "end": "1795919"
  },
  {
    "text": "we are working on is we are exploring parket format for our long-term storage",
    "start": "1795919",
    "end": "1802279"
  },
  {
    "text": "um working with the upstream community as well as tonos maintainers this will",
    "start": "1802279",
    "end": "1807840"
  },
  {
    "text": "keep the same query performance but with further optimizations we can improve query performance but the main focus of",
    "start": "1807840",
    "end": "1816240"
  },
  {
    "text": "this uh effort is to be able to reduce the operational toil uh introduced by",
    "start": "1816240",
    "end": "1824320"
  },
  {
    "text": "store gateways and reduce the infrastructure needed to run",
    "start": "1824320",
    "end": "1830520"
  },
  {
    "text": "Cortex so uh that's it for today if you're interested in learning more about",
    "start": "1830520",
    "end": "1835840"
  },
  {
    "text": "our improvements and how to run uh Cortex and what's next please feel free",
    "start": "1835840",
    "end": "1842720"
  },
  {
    "text": "to attend our uh the talks uh that our maintainers are going to give uh on",
    "start": "1842720",
    "end": "1847919"
  },
  {
    "text": "Friday you can also find us in uh Cortex and Open Search booths as well as reach",
    "start": "1847919",
    "end": "1854000"
  },
  {
    "text": "out to us uh in CNCF uh channels thank you",
    "start": "1854000",
    "end": "1859600"
  }
]