[
  {
    "text": "hello",
    "start": "400",
    "end": "1240"
  },
  {
    "text": "everyone welcome to our session and and",
    "start": "1240",
    "end": "4799"
  },
  {
    "text": "this session is about unlocking how to",
    "start": "4799",
    "end": "7120"
  },
  {
    "text": "efficiently flexibility and manage the",
    "start": "7120",
    "end": "10559"
  },
  {
    "text": "seven AI chips in Kubernetes it is a",
    "start": "10559",
    "end": "13280"
  },
  {
    "text": "rather abstract title but uh in one in",
    "start": "13280",
    "end": "16400"
  },
  {
    "text": "one sentence it can be sound like how to",
    "start": "16400",
    "end": "19600"
  },
  {
    "text": "improve your GPU utilization in",
    "start": "19600",
    "end": "21800"
  },
  {
    "text": "Kubernetes yes this session is brought",
    "start": "21800",
    "end": "25279"
  },
  {
    "text": "is brought to you by two software",
    "start": "25279",
    "end": "27199"
  },
  {
    "text": "engineers my name is Lemon this is my",
    "start": "27199",
    "end": "29439"
  },
  {
    "text": "colleague Junga we are both from a new",
    "start": "29439",
    "end": "31279"
  },
  {
    "text": "founded company called Dynamia Point",
    "start": "31279",
    "end": "34520"
  },
  {
    "text": "AI and here are the Let me let me first",
    "start": "34520",
    "end": "38559"
  },
  {
    "text": "introduce to you the background the",
    "start": "38559",
    "end": "40800"
  },
  {
    "text": "first background is the burst",
    "start": "40800",
    "end": "42320"
  },
  {
    "text": "requirement for computing power the",
    "start": "42320",
    "end": "44559"
  },
  {
    "text": "global GPU marketing growth is over",
    "start": "44559",
    "end": "48120"
  },
  {
    "text": "60% than last year the mo majority is",
    "start": "48120",
    "end": "52320"
  },
  {
    "text": "majority growth is Nvidia and the",
    "start": "52320",
    "end": "54879"
  },
  {
    "text": "heterogeneous is over 20% as you can see",
    "start": "54879",
    "end": "58239"
  },
  {
    "text": "in in this figure it has been rather",
    "start": "58239",
    "end": "62399"
  },
  {
    "text": "boost after the emergence of large",
    "start": "62399",
    "end": "64720"
  },
  {
    "text": "language",
    "start": "64720",
    "end": "66200"
  },
  {
    "text": "models and uh you may guess we are from",
    "start": "66200",
    "end": "69439"
  },
  {
    "text": "the mainland China so so uh in our",
    "start": "69439",
    "end": "73280"
  },
  {
    "text": "country the highspec Nvidia can't can't",
    "start": "73280",
    "end": "76720"
  },
  {
    "text": "be imported very easily so we have to",
    "start": "76720",
    "end": "80159"
  },
  {
    "text": "use uh alternative cards like these",
    "start": "80159",
    "end": "83759"
  },
  {
    "text": "device vendors they are they are all",
    "start": "83759",
    "end": "86040"
  },
  {
    "text": "some alternative plan for Nvidia cards",
    "start": "86040",
    "end": "89520"
  },
  {
    "text": "and of course they are not as highspec",
    "start": "89520",
    "end": "92320"
  },
  {
    "text": "as Nvidia and the user pre user",
    "start": "92320",
    "end": "94720"
  },
  {
    "text": "experiences may not be as good as Nvidia",
    "start": "94720",
    "end": "97439"
  },
  {
    "text": "but",
    "start": "97439",
    "end": "98840"
  },
  {
    "text": "they but it is very cheap so it so we",
    "start": "98840",
    "end": "102720"
  },
  {
    "text": "can use them in the production level as",
    "start": "102720",
    "end": "104400"
  },
  {
    "text": "well and it has a decent",
    "start": "104400",
    "end": "107399"
  },
  {
    "text": "performance yes but we here we meet a",
    "start": "107399",
    "end": "110000"
  },
  {
    "text": "challenge is that the GPU can't be",
    "start": "110000",
    "end": "112159"
  },
  {
    "text": "shared in a traditional Kubernetes and",
    "start": "112159",
    "end": "115360"
  },
  {
    "text": "suppose you have five GPUs each with the",
    "start": "115360",
    "end": "117680"
  },
  {
    "text": "capacity of 40 40G device memory and it",
    "start": "117680",
    "end": "122079"
  },
  {
    "text": "is all running uh 2G little mode small",
    "start": "122079",
    "end": "125520"
  },
  {
    "text": "model and it will cause the all the",
    "start": "125520",
    "end": "129200"
  },
  {
    "text": "device be not not be able to fit more",
    "start": "129200",
    "end": "134480"
  },
  {
    "text": "ports other than this one so the other",
    "start": "134480",
    "end": "137840"
  },
  {
    "text": "part which uses CPU are in a pending",
    "start": "137840",
    "end": "140480"
  },
  {
    "text": "state which cause the utilization of GPU",
    "start": "140480",
    "end": "143840"
  },
  {
    "text": "in the cluster is very",
    "start": "143840",
    "end": "145879"
  },
  {
    "text": "low and another challenge is the",
    "start": "145879",
    "end": "148560"
  },
  {
    "text": "management of heterogeneous clusters as",
    "start": "148560",
    "end": "151040"
  },
  {
    "text": "you can see there are multiple device",
    "start": "151040",
    "end": "152560"
  },
  {
    "text": "vendors in China and many of them imple",
    "start": "152560",
    "end": "155440"
  },
  {
    "text": "implement their own scheduleuler",
    "start": "155440",
    "end": "157920"
  },
  {
    "text": "extender and it hijacked the filter",
    "start": "157920",
    "end": "160560"
  },
  {
    "text": "hijack the score process and you have",
    "start": "160560",
    "end": "162879"
  },
  {
    "text": "and you have a and if you have a cluster",
    "start": "162879",
    "end": "165680"
  },
  {
    "text": "composed of multiore AI cards then you",
    "start": "165680",
    "end": "168400"
  },
  {
    "text": "have to install their schedule extenders",
    "start": "168400",
    "end": "171200"
  },
  {
    "text": "and you have a more sc scheduling",
    "start": "171200",
    "end": "173599"
  },
  {
    "text": "pipeline and if you enter the filter you",
    "start": "173599",
    "end": "176560"
  },
  {
    "text": "have first go through the multiple",
    "start": "176560",
    "end": "178720"
  },
  {
    "text": "extenders and go back to the filter",
    "start": "178720",
    "end": "180480"
  },
  {
    "text": "computer and then into the score process",
    "start": "180480",
    "end": "183120"
  },
  {
    "text": "you you still need to go through all the",
    "start": "183120",
    "end": "185120"
  },
  {
    "text": "extenders and return to the score this",
    "start": "185120",
    "end": "187840"
  },
  {
    "text": "will cause the performance of the",
    "start": "187840",
    "end": "190560"
  },
  {
    "text": "Kubernetes scheduling is very poor",
    "start": "190560",
    "end": "193519"
  },
  {
    "text": "that's that's a problem needs to solve",
    "start": "193519",
    "end": "196080"
  },
  {
    "text": "and one kind of solution is DIA this is",
    "start": "196080",
    "end": "199280"
  },
  {
    "text": "called dynamic resource allocation it is",
    "start": "199280",
    "end": "201519"
  },
  {
    "text": "an API for requesting and sharing",
    "start": "201519",
    "end": "203280"
  },
  {
    "text": "resources between ports and consenters",
    "start": "203280",
    "end": "205040"
  },
  {
    "text": "inside a pod it is stabled in 1.32",
    "start": "205040",
    "end": "209440"
  },
  {
    "text": "version in Kubernetes and it is um and",
    "start": "209440",
    "end": "213280"
  },
  {
    "text": "you need to set a resource claim and the",
    "start": "213280",
    "end": "216400"
  },
  {
    "text": "resource class and every device vendor",
    "start": "216400",
    "end": "218799"
  },
  {
    "text": "need to implement their own resource DRA",
    "start": "218799",
    "end": "221360"
  },
  {
    "text": "driver and it communicate with Kublet to",
    "start": "221360",
    "end": "224000"
  },
  {
    "text": "do the uh device sharing and uh uh",
    "start": "224000",
    "end": "227200"
  },
  {
    "text": "scheduling and",
    "start": "227200",
    "end": "228599"
  },
  {
    "text": "allocating but it has many restrictions",
    "start": "228599",
    "end": "231599"
  },
  {
    "text": "the first is it has it it requires the",
    "start": "231599",
    "end": "234080"
  },
  {
    "text": "Kubernetes version must be the latest it",
    "start": "234080",
    "end": "236640"
  },
  {
    "text": "means 132i uh Kubernetes and it has to",
    "start": "236640",
    "end": "241920"
  },
  {
    "text": "and it has happens to me that not many",
    "start": "241920",
    "end": "245040"
  },
  {
    "text": "device vendors implements DIA driver for",
    "start": "245040",
    "end": "247840"
  },
  {
    "text": "now the Nvidia DIA driver is under",
    "start": "247840",
    "end": "250159"
  },
  {
    "text": "construction and and it's not production",
    "start": "250159",
    "end": "252799"
  },
  {
    "text": "level ready so you have to maybe use use",
    "start": "252799",
    "end": "256239"
  },
  {
    "text": "this feature in the uh in the future for",
    "start": "256239",
    "end": "259199"
  },
  {
    "text": "it to enter the production",
    "start": "259199",
    "end": "261400"
  },
  {
    "text": "environment and also it needs to create",
    "start": "261400",
    "end": "264080"
  },
  {
    "text": "resource claim and resource class if you",
    "start": "264080",
    "end": "266160"
  },
  {
    "text": "you if want to if you want to share the",
    "start": "266160",
    "end": "268320"
  },
  {
    "text": "GPUs inside the Kubernetes you have to",
    "start": "268320",
    "end": "270560"
  },
  {
    "text": "configure this hole you need to use the",
    "start": "270560",
    "end": "273840"
  },
  {
    "text": "resource claims here which is defined",
    "start": "273840",
    "end": "275680"
  },
  {
    "text": "here and the corresponding resource",
    "start": "275680",
    "end": "278080"
  },
  {
    "text": "class defined here it all it has it has",
    "start": "278080",
    "end": "281280"
  },
  {
    "text": "all be need to be applied in Kubernetes",
    "start": "281280",
    "end": "285199"
  },
  {
    "text": "yes and still this this feature is not",
    "start": "285199",
    "end": "287680"
  },
  {
    "text": "enabled automatically you need to en",
    "start": "287680",
    "end": "290160"
  },
  {
    "text": "enable",
    "start": "290160",
    "end": "292120"
  },
  {
    "text": "explicitly and another solution is uh",
    "start": "292120",
    "end": "295360"
  },
  {
    "text": "what we brought here it's called Hami",
    "start": "295360",
    "end": "297520"
  },
  {
    "text": "hami is a hogenous AI computing virtual",
    "start": "297520",
    "end": "301120"
  },
  {
    "text": "virtualization middleware it is uh it is",
    "start": "301120",
    "end": "305040"
  },
  {
    "text": "used to provide CPU sharing and manage",
    "start": "305040",
    "end": "308000"
  },
  {
    "text": "multi multiple uh multi heterogeneous AI",
    "start": "308000",
    "end": "312080"
  },
  {
    "text": "computing devices from multiple device",
    "start": "312080",
    "end": "314400"
  },
  {
    "text": "vendors it composed of a mutating web",
    "start": "314400",
    "end": "316960"
  },
  {
    "text": "hook a scheduling extender and the",
    "start": "316960",
    "end": "320759"
  },
  {
    "text": "corresponding corresponding device",
    "start": "320759",
    "end": "322639"
  },
  {
    "text": "plugins from for each of the device",
    "start": "322639",
    "end": "324800"
  },
  {
    "text": "vendors and uh we have an additionally",
    "start": "324800",
    "end": "328720"
  },
  {
    "text": "incontainer resource control for each of",
    "start": "328720",
    "end": "331440"
  },
  {
    "text": "these device yes and it is very it is a",
    "start": "331440",
    "end": "334560"
  },
  {
    "text": "plugable non-intrusive standard and",
    "start": "334560",
    "end": "337120"
  },
  {
    "text": "lightweight which means you can helm",
    "start": "337120",
    "end": "339039"
  },
  {
    "text": "install and helm uninstall very easily",
    "start": "339039",
    "end": "341199"
  },
  {
    "text": "and it is a CNCF sandbox project",
    "start": "341199",
    "end": "344800"
  },
  {
    "text": "and the key feature of HAMI is the",
    "start": "344800",
    "end": "347120"
  },
  {
    "text": "device share advanced scheduling and",
    "start": "347120",
    "end": "349440"
  },
  {
    "text": "unified monitoring device share is our",
    "start": "349440",
    "end": "352080"
  },
  {
    "text": "key feature let's show you here if you",
    "start": "352080",
    "end": "354400"
  },
  {
    "text": "have a node composed of four GPUs and",
    "start": "354400",
    "end": "357520"
  },
  {
    "text": "you have two users each of them submit a",
    "start": "357520",
    "end": "360560"
  },
  {
    "text": "task which uses two GPUs this without hi",
    "start": "360560",
    "end": "364240"
  },
  {
    "text": "you need to allocate you you have to use",
    "start": "364240",
    "end": "366240"
  },
  {
    "text": "four of them all and the overall",
    "start": "366240",
    "end": "368319"
  },
  {
    "text": "utilization is less than 50% and with",
    "start": "368319",
    "end": "371280"
  },
  {
    "text": "ham it can be observed that these two",
    "start": "371280",
    "end": "373919"
  },
  {
    "text": "task can be shared on two GPUs and leave",
    "start": "373919",
    "end": "376960"
  },
  {
    "text": "the rest to for other task to use so it",
    "start": "376960",
    "end": "379919"
  },
  {
    "text": "can improve the GPU utilization to",
    "start": "379919",
    "end": "382720"
  },
  {
    "text": "nearly 100 yes and it is very and it is",
    "start": "382720",
    "end": "386639"
  },
  {
    "text": "transparent to task you don't need to",
    "start": "386639",
    "end": "388800"
  },
  {
    "text": "modify the task you don't need to uh",
    "start": "388800",
    "end": "391120"
  },
  {
    "text": "modify the image or the source code or",
    "start": "391120",
    "end": "393759"
  },
  {
    "text": "anything just specify the device memory",
    "start": "393759",
    "end": "396880"
  },
  {
    "text": "you need to use is okay and the this is",
    "start": "396880",
    "end": "400319"
  },
  {
    "text": "the device share inside this is how we",
    "start": "400319",
    "end": "402720"
  },
  {
    "text": "control the device limit inside the",
    "start": "402720",
    "end": "404639"
  },
  {
    "text": "container we inject a library called",
    "start": "404639",
    "end": "406960"
  },
  {
    "text": "humor inside this invocation line it",
    "start": "406960",
    "end": "410000"
  },
  {
    "text": "hijacks calls from the CUDA runtime to",
    "start": "410000",
    "end": "412240"
  },
  {
    "text": "CUDA driver um so we so we can do the",
    "start": "412240",
    "end": "415120"
  },
  {
    "text": "counting here we know how exactly are",
    "start": "415120",
    "end": "417120"
  },
  {
    "text": "the uh device memory allocation inside",
    "start": "417120",
    "end": "419919"
  },
  {
    "text": "each of the container if it pass the",
    "start": "419919",
    "end": "422080"
  },
  {
    "text": "limitation you set in this your inside",
    "start": "422080",
    "end": "424880"
  },
  {
    "text": "this task it will return an OM error yes",
    "start": "424880",
    "end": "428319"
  },
  {
    "text": "it applies to wider range of Avidia and",
    "start": "428319",
    "end": "431680"
  },
  {
    "text": "uh Kubernetes the only the only",
    "start": "431680",
    "end": "434639"
  },
  {
    "text": "requirement is your CUDA version is",
    "start": "434639",
    "end": "436720"
  },
  {
    "text": "greater than 10.2 and the the Avidia",
    "start": "436720",
    "end": "440000"
  },
  {
    "text": "driver version is greater than 440",
    "start": "440000",
    "end": "443319"
  },
  {
    "text": "yes and this is how we use Hammy you",
    "start": "443319",
    "end": "446319"
  },
  {
    "text": "simply need to specify the number of",
    "start": "446319",
    "end": "447840"
  },
  {
    "text": "GPUs you wish to see in this container",
    "start": "447840",
    "end": "450800"
  },
  {
    "text": "and and along with the GPU memory you",
    "start": "450800",
    "end": "453280"
  },
  {
    "text": "wish to cut for this container uh in",
    "start": "453280",
    "end": "456160"
  },
  {
    "text": "this example you uh you said this this",
    "start": "456160",
    "end": "459840"
  },
  {
    "text": "task needs two GPUs and each CPU use 10",
    "start": "459840",
    "end": "463039"
  },
  {
    "text": "GB device memory and the scheduleuler",
    "start": "463039",
    "end": "465440"
  },
  {
    "text": "knows sees that and it will cut the 10 G",
    "start": "465440",
    "end": "468160"
  },
  {
    "text": "device memory for this task and leave",
    "start": "468160",
    "end": "470240"
  },
  {
    "text": "the uh the other 22 for other for other",
    "start": "470240",
    "end": "473599"
  },
  {
    "text": "task to share and we provide the",
    "start": "473599",
    "end": "475919"
  },
  {
    "text": "incontainer resource control you can see",
    "start": "475919",
    "end": "478319"
  },
  {
    "text": "the Nvidia SMI inside the container the",
    "start": "478319",
    "end": "480800"
  },
  {
    "text": "upper limit of device memory is limited",
    "start": "480800",
    "end": "482879"
  },
  {
    "text": "to 10G device 10 yes you can't use",
    "start": "482879",
    "end": "486479"
  },
  {
    "text": "beyond that it is according to this yes",
    "start": "486479",
    "end": "488960"
  },
  {
    "text": "we we guarantee that the upper limit is",
    "start": "488960",
    "end": "492720"
  },
  {
    "text": "contained in this container yes and",
    "start": "492720",
    "end": "496160"
  },
  {
    "text": "other features include that we can",
    "start": "496160",
    "end": "498160"
  },
  {
    "text": "support the device specify you can",
    "start": "498160",
    "end": "500639"
  },
  {
    "text": "specify the type of GPU you wish to use",
    "start": "500639",
    "end": "503039"
  },
  {
    "text": "if if you want only want to use A100",
    "start": "503039",
    "end": "505680"
  },
  {
    "text": "then you can set the annotation of task",
    "start": "505680",
    "end": "508240"
  },
  {
    "text": "here with the use GPU type annotation",
    "start": "508240",
    "end": "511680"
  },
  {
    "text": "which",
    "start": "511680",
    "end": "513240"
  },
  {
    "text": "means if it assigned to A100 then you",
    "start": "513240",
    "end": "516240"
  },
  {
    "text": "can only be applied to A100 cards or you",
    "start": "516240",
    "end": "519680"
  },
  {
    "text": "can avoid be apply applied to A100 by",
    "start": "519680",
    "end": "522719"
  },
  {
    "text": "set this blacklist no use your CPU type",
    "start": "522719",
    "end": "526200"
  },
  {
    "text": "here and we have another feature called",
    "start": "526200",
    "end": "528959"
  },
  {
    "text": "task priority and the the method To use",
    "start": "528959",
    "end": "533040"
  },
  {
    "text": "it it's very easy you simply uh specify",
    "start": "533040",
    "end": "536000"
  },
  {
    "text": "an environment variable here called CUDA",
    "start": "536000",
    "end": "538000"
  },
  {
    "text": "task priority uh for now we support two",
    "start": "538000",
    "end": "540880"
  },
  {
    "text": "types of priority zero is the high",
    "start": "540880",
    "end": "543040"
  },
  {
    "text": "priority and one is the low priority the",
    "start": "543040",
    "end": "546000"
  },
  {
    "text": "difference is that as long as the high",
    "start": "546000",
    "end": "547839"
  },
  {
    "text": "priority pod is submitting kernel to the",
    "start": "547839",
    "end": "550240"
  },
  {
    "text": "certain GPU the low priority pod will be",
    "start": "550240",
    "end": "553440"
  },
  {
    "text": "will be temporarily suspended and wait",
    "start": "553440",
    "end": "556240"
  },
  {
    "text": "for this long uh high priority pod to",
    "start": "556240",
    "end": "559600"
  },
  {
    "text": "stop submitting new kernels to GPU and",
    "start": "559600",
    "end": "562399"
  },
  {
    "text": "the low priority pod will be resumed",
    "start": "562399",
    "end": "564880"
  },
  {
    "text": "running it is all transparent to the",
    "start": "564880",
    "end": "567600"
  },
  {
    "text": "task and it is very automatically",
    "start": "567600",
    "end": "570680"
  },
  {
    "text": "yes and we also support dynamic MIG",
    "start": "570680",
    "end": "573760"
  },
  {
    "text": "feature uh you can use it like the uh",
    "start": "573760",
    "end": "577279"
  },
  {
    "text": "examples uh in this page you simply need",
    "start": "577279",
    "end": "581519"
  },
  {
    "text": "to specify the number of GPU you wish to",
    "start": "581519",
    "end": "583519"
  },
  {
    "text": "use along along with the uh device",
    "start": "583519",
    "end": "586160"
  },
  {
    "text": "memory you wish to allocate you you set",
    "start": "586160",
    "end": "588800"
  },
  {
    "text": "the uh number and memory here and we",
    "start": "588800",
    "end": "592160"
  },
  {
    "text": "will search according to the templates",
    "start": "592160",
    "end": "594000"
  },
  {
    "text": "here and we will find the most f fitting",
    "start": "594000",
    "end": "596720"
  },
  {
    "text": "make instance for you for you to use and",
    "start": "596720",
    "end": "599839"
  },
  {
    "text": "we use make via make parted to",
    "start": "599839",
    "end": "602320"
  },
  {
    "text": "dynamically generate the mega instance",
    "start": "602320",
    "end": "604560"
  },
  {
    "text": "for certain card so the users doesn't",
    "start": "604560",
    "end": "607279"
  },
  {
    "text": "need to really know the make instance",
    "start": "607279",
    "end": "610240"
  },
  {
    "text": "name like 1g 10b because it is different",
    "start": "610240",
    "end": "612959"
  },
  {
    "text": "for different different types of Nvidia",
    "start": "612959",
    "end": "615040"
  },
  {
    "text": "card the user only need to concern how",
    "start": "615040",
    "end": "617839"
  },
  {
    "text": "many GPUs it wishes to use inside the",
    "start": "617839",
    "end": "620000"
  },
  {
    "text": "container along with the device memory",
    "start": "620000",
    "end": "622000"
  },
  {
    "text": "it wishes to use and leave the rest for",
    "start": "622000",
    "end": "624399"
  },
  {
    "text": "us yes and we can apply this device",
    "start": "624399",
    "end": "628320"
  },
  {
    "text": "memory control to other um devices other",
    "start": "628320",
    "end": "631519"
  },
  {
    "text": "than Nvidia like Accent yes this is a",
    "start": "631519",
    "end": "634720"
  },
  {
    "text": "Huawei uh manufacturer uh AI chips yes",
    "start": "634720",
    "end": "639200"
  },
  {
    "text": "if it it is 64 GB in total and in this",
    "start": "639200",
    "end": "643120"
  },
  {
    "text": "example we limited to 16 G and it you",
    "start": "643120",
    "end": "647120"
  },
  {
    "text": "can see here it has been limited and it",
    "start": "647120",
    "end": "649519"
  },
  {
    "text": "it also apply to it and camreen devices",
    "start": "649519",
    "end": "652800"
  },
  {
    "text": "just like Huawei and Nvidia yes and this",
    "start": "652800",
    "end": "656240"
  },
  {
    "text": "is other scheduling features we uh we",
    "start": "656240",
    "end": "659600"
  },
  {
    "text": "ham introduces like the new and topology",
    "start": "659600",
    "end": "662959"
  },
  {
    "text": "aware if you want to allocate more than",
    "start": "662959",
    "end": "666079"
  },
  {
    "text": "one GPUs like you want to deploy an AI",
    "start": "666079",
    "end": "668800"
  },
  {
    "text": "training job across multiple nodes and",
    "start": "668800",
    "end": "670640"
  },
  {
    "text": "multiple GPUs they wish to they probably",
    "start": "670640",
    "end": "673519"
  },
  {
    "text": "wish to minimize the communication",
    "start": "673519",
    "end": "675440"
  },
  {
    "text": "communication cost between multiple GPUs",
    "start": "675440",
    "end": "678079"
  },
  {
    "text": "and for that reason we can observe the",
    "start": "678079",
    "end": "681200"
  },
  {
    "text": "topology inside between each GPUs along",
    "start": "681200",
    "end": "684800"
  },
  {
    "text": "with the uh network topology so by doing",
    "start": "684800",
    "end": "688959"
  },
  {
    "text": "that we can uh allocate the uh nearest",
    "start": "688959",
    "end": "692880"
  },
  {
    "text": "GPU for for this AI training job to",
    "start": "692880",
    "end": "695120"
  },
  {
    "text": "minimize the communication cost yes it",
    "start": "695120",
    "end": "697279"
  },
  {
    "text": "has been applied to Nvidia accent and",
    "start": "697279",
    "end": "700160"
  },
  {
    "text": "this",
    "start": "700160",
    "end": "701000"
  },
  {
    "text": "metex device yes and we have another",
    "start": "701000",
    "end": "705360"
  },
  {
    "text": "beam pack and spread uh schedule policy",
    "start": "705360",
    "end": "709440"
  },
  {
    "text": "it is it is for each task each tasker",
    "start": "709440",
    "end": "712480"
  },
  {
    "text": "can specify their own schedule policy",
    "start": "712480",
    "end": "715040"
  },
  {
    "text": "and the beam",
    "start": "715040",
    "end": "716680"
  },
  {
    "text": "means it it wishes to allocate to a GPU",
    "start": "716680",
    "end": "720959"
  },
  {
    "text": "which has already have the task running",
    "start": "720959",
    "end": "724160"
  },
  {
    "text": "on that GPU to minimize fragmentation",
    "start": "724160",
    "end": "726320"
  },
  {
    "text": "caused by GPU share and the spread is on",
    "start": "726320",
    "end": "730079"
  },
  {
    "text": "the contrary wish it wishes to allocate",
    "start": "730079",
    "end": "732399"
  },
  {
    "text": "to a GPU where no port is running on",
    "start": "732399",
    "end": "735279"
  },
  {
    "text": "that CPU to maximize the performance so",
    "start": "735279",
    "end": "738000"
  },
  {
    "text": "each port may have their own request to",
    "start": "738000",
    "end": "740880"
  },
  {
    "text": "it may have their own schedule policy to",
    "start": "740880",
    "end": "743760"
  },
  {
    "text": "best meet their fits yes we we provide",
    "start": "743760",
    "end": "746720"
  },
  {
    "text": "the beam pack and spread scatter policy",
    "start": "746720",
    "end": "750000"
  },
  {
    "text": "for GPU GPU level and node level yes and",
    "start": "750000",
    "end": "755760"
  },
  {
    "text": "we uh after we introduced the GPU",
    "start": "755760",
    "end": "758480"
  },
  {
    "text": "sharing there are many things you need",
    "start": "758480",
    "end": "761040"
  },
  {
    "text": "to monitor other than the DCGM exporter",
    "start": "761040",
    "end": "764399"
  },
  {
    "text": "matrix used by used to you by the DCGM",
    "start": "764399",
    "end": "768160"
  },
  {
    "text": "exporter like how many device memory has",
    "start": "768160",
    "end": "771680"
  },
  {
    "text": "been allocated for a certain GPU how",
    "start": "771680",
    "end": "773839"
  },
  {
    "text": "many device memory are still free still",
    "start": "773839",
    "end": "776000"
  },
  {
    "text": "available for other task to use and how",
    "start": "776000",
    "end": "779120"
  },
  {
    "text": "many workloads are running on that CPU",
    "start": "779120",
    "end": "780959"
  },
  {
    "text": "and how many workloads and their",
    "start": "780959",
    "end": "783279"
  },
  {
    "text": "corresponding PI",
    "start": "783279",
    "end": "784720"
  },
  {
    "text": "P name container name and etc they are",
    "start": "784720",
    "end": "787680"
  },
  {
    "text": "all contained in this form of matrix",
    "start": "787680",
    "end": "790480"
  },
  {
    "text": "porter which can be easily integrated",
    "start": "790480",
    "end": "792480"
  },
  {
    "text": "into the premises and later be",
    "start": "792480",
    "end": "794720"
  },
  {
    "text": "demonstrated by the graphana dashboard",
    "start": "794720",
    "end": "797720"
  },
  {
    "text": "yes and the volcano vgpo is supported by",
    "start": "797720",
    "end": "801440"
  },
  {
    "text": "hami if you will if you use volcano if",
    "start": "801440",
    "end": "804160"
  },
  {
    "text": "you go go to their PLA project and their",
    "start": "804160",
    "end": "807279"
  },
  {
    "text": "their slice has a has a pages about the",
    "start": "807279",
    "end": "810320"
  },
  {
    "text": "volcano VGP which is contributed by us",
    "start": "810320",
    "end": "813519"
  },
  {
    "text": "and the hammy community is responsible",
    "start": "813519",
    "end": "816160"
  },
  {
    "text": "for the incontainer resource control and",
    "start": "816160",
    "end": "818079"
  },
  {
    "text": "leave the rest scheduling process for",
    "start": "818079",
    "end": "819839"
  },
  {
    "text": "the volcano you can easily find there uh",
    "start": "819839",
    "end": "823920"
  },
  {
    "text": "the volcano vpu document on the volcano",
    "start": "823920",
    "end": "826320"
  },
  {
    "text": "vpu project on the volcano project and",
    "start": "826320",
    "end": "829920"
  },
  {
    "text": "it's the same about the coordinator we",
    "start": "829920",
    "end": "832320"
  },
  {
    "text": "also integrated GPU sharing GPU sharing",
    "start": "832320",
    "end": "835120"
  },
  {
    "text": "mechanism into the coordinator you can",
    "start": "835120",
    "end": "837600"
  },
  {
    "text": "find their um document in on their",
    "start": "837600",
    "end": "841760"
  },
  {
    "text": "website",
    "start": "841760",
    "end": "843000"
  },
  {
    "text": "yes okay uh allow me to pass my phone to",
    "start": "843000",
    "end": "847680"
  },
  {
    "text": "the to my colleague to introduce the",
    "start": "847680",
    "end": "849600"
  },
  {
    "text": "adopters the road maps and the other the",
    "start": "849600",
    "end": "852160"
  },
  {
    "text": "summary yes",
    "start": "852160",
    "end": "855560"
  },
  {
    "text": "hello hello",
    "start": "856079",
    "end": "859040"
  },
  {
    "text": "okay uh thank you Min to introduce the",
    "start": "859040",
    "end": "862560"
  },
  {
    "text": "Hami architecture and the GPU sharing uh",
    "start": "862560",
    "end": "867519"
  },
  {
    "text": "and the uh advanced schedule let me",
    "start": "867519",
    "end": "871440"
  },
  {
    "text": "introduce Hami ecosystem and the",
    "start": "871440",
    "end": "874279"
  },
  {
    "text": "adopters uh from now on uh HA support in",
    "start": "874279",
    "end": "880000"
  },
  {
    "text": "additional to newia we also support such",
    "start": "880000",
    "end": "883440"
  },
  {
    "text": "as the Isula and",
    "start": "883440",
    "end": "887440"
  },
  {
    "text": "compreen met the other sex AI chips and",
    "start": "887639",
    "end": "893839"
  },
  {
    "text": "also we want to uh support more AI chips",
    "start": "893839",
    "end": "898320"
  },
  {
    "text": "and from the the AI chips where the the",
    "start": "898320",
    "end": "903920"
  },
  {
    "text": "killing OS",
    "start": "903920",
    "end": "905760"
  },
  {
    "text": "SE operator system also supported the",
    "start": "905760",
    "end": "908560"
  },
  {
    "text": "hammy to building in the AI uh system",
    "start": "908560",
    "end": "912320"
  },
  {
    "text": "and uh in China and around the world",
    "start": "912320",
    "end": "915560"
  },
  {
    "text": "many vendors also building hammy to",
    "start": "915560",
    "end": "920160"
  },
  {
    "text": "their product such as the docloud and",
    "start": "920160",
    "end": "923880"
  },
  {
    "text": "the silicon cloud and the Ucloud the",
    "start": "923880",
    "end": "927519"
  },
  {
    "text": "Ucloud is the China biggest the natural",
    "start": "927519",
    "end": "931440"
  },
  {
    "text": "cloud",
    "start": "931440",
    "end": "932600"
  },
  {
    "text": "provider and some virtual users also use",
    "start": "932600",
    "end": "938160"
  },
  {
    "text": "Hammy to solve the GPU utilization their",
    "start": "938160",
    "end": "943519"
  },
  {
    "text": "situation is the GPU utilization very",
    "start": "943519",
    "end": "946680"
  },
  {
    "text": "small such as the the slow the south",
    "start": "946680",
    "end": "950399"
  },
  {
    "text": "carol uh company and use ham to uh",
    "start": "950399",
    "end": "955680"
  },
  {
    "text": "combine train and influence in their",
    "start": "955680",
    "end": "958800"
  },
  {
    "text": "production situation and uh such as",
    "start": "958800",
    "end": "964480"
  },
  {
    "text": "the the the travel company and uh some",
    "start": "964519",
    "end": "969199"
  },
  {
    "text": "key users such as the PN security and",
    "start": "969199",
    "end": "972600"
  },
  {
    "text": "the SEBC also some the bank",
    "start": "972600",
    "end": "976600"
  },
  {
    "text": "and business company also use hammy to",
    "start": "976600",
    "end": "980480"
  },
  {
    "text": "in to maximize the GPU utilization and",
    "start": "980480",
    "end": "985000"
  },
  {
    "text": "unified management the heterogeneous air",
    "start": "985000",
    "end": "988639"
  },
  {
    "text": "chips so uh from now uh we have nearly",
    "start": "988639",
    "end": "994560"
  },
  {
    "text": "100 uh and user from the around the",
    "start": "994560",
    "end": "999320"
  },
  {
    "text": "world okay uh so uh this this year uh",
    "start": "999320",
    "end": "1004519"
  },
  {
    "text": "ham also becomes the same safe sandbox",
    "start": "1004519",
    "end": "1007839"
  },
  {
    "text": "but we also have a clean road map in the",
    "start": "1007839",
    "end": "1011639"
  },
  {
    "text": "2025 uh firstly we will support more the",
    "start": "1011639",
    "end": "1017160"
  },
  {
    "text": "heterogeneous AI chips such as the quon",
    "start": "1017160",
    "end": "1020720"
  },
  {
    "text": "and we also want to support the MD inter",
    "start": "1020720",
    "end": "1024640"
  },
  {
    "text": "or the AWS so any any other any any",
    "start": "1024640",
    "end": "1029520"
  },
  {
    "text": "helps we we will",
    "start": "1029520",
    "end": "1033000"
  },
  {
    "text": "welcome and furthermore we also want to",
    "start": "1033000",
    "end": "1036000"
  },
  {
    "text": "support the DR but it's it's more",
    "start": "1036000",
    "end": "1039280"
  },
  {
    "text": "challenges how to have a compatible way",
    "start": "1039280",
    "end": "1042880"
  },
  {
    "text": "to in integrated with Dr and the Hammy",
    "start": "1042880",
    "end": "1047600"
  },
  {
    "text": "because um many of the users use ham to",
    "start": "1047600",
    "end": "1051320"
  },
  {
    "text": "their production environment but um but",
    "start": "1051320",
    "end": "1055919"
  },
  {
    "text": "from on and more many AI chips company",
    "start": "1055919",
    "end": "1060880"
  },
  {
    "text": "have",
    "start": "1060880",
    "end": "1062200"
  },
  {
    "text": "don't implement so also we we will uh",
    "start": "1062200",
    "end": "1067360"
  },
  {
    "text": "create a hammy web UI for easy to use",
    "start": "1067360",
    "end": "1071520"
  },
  {
    "text": "and uh maybe in the end of the year we",
    "start": "1071520",
    "end": "1075520"
  },
  {
    "text": "will propose it to the incubating pro",
    "start": "1075520",
    "end": "1078760"
  },
  {
    "text": "project so yeah and the dynamic MPS for",
    "start": "1078760",
    "end": "1083280"
  },
  {
    "text": "Nvidia",
    "start": "1083280",
    "end": "1085840"
  },
  {
    "text": "uh and if if you want to join us we will",
    "start": "1085840",
    "end": "1090000"
  },
  {
    "text": "very very welcome yeah this is our slack",
    "start": "1090000",
    "end": "1094799"
  },
  {
    "text": "and the GitHub GitHub repo yeah uh we",
    "start": "1094799",
    "end": "1101120"
  },
  {
    "text": "all finished our talk uh any questions",
    "start": "1101120",
    "end": "1104320"
  },
  {
    "text": "is we're welcome",
    "start": "1104320",
    "end": "1108519"
  },
  {
    "text": "okay any questions",
    "start": "1116640",
    "end": "1120280"
  },
  {
    "text": "okay",
    "start": "1125679",
    "end": "1128679"
  },
  {
    "text": "what are the challenges",
    "start": "1133039",
    "end": "1136519"
  },
  {
    "text": "um the biggest challenges have is that",
    "start": "1138640",
    "end": "1141679"
  },
  {
    "text": "the communication we can't reach it",
    "start": "1141679",
    "end": "1143520"
  },
  {
    "text": "through the AMD yes that's the biggest",
    "start": "1143520",
    "end": "1146960"
  },
  {
    "text": "if we have the if we can reach AMD I",
    "start": "1146960",
    "end": "1150799"
  },
  {
    "text": "think this can be easily implemented",
    "start": "1150799",
    "end": "1155039"
  },
  {
    "text": "yes we want to keep in touch with the",
    "start": "1155320",
    "end": "1158320"
  },
  {
    "text": "AMD open source strategy or the software",
    "start": "1158320",
    "end": "1162000"
  },
  {
    "text": "development but it's difficult to",
    "start": "1162000",
    "end": "1165400"
  },
  {
    "text": "ask yeah",
    "start": "1165400",
    "end": "1168720"
  },
  {
    "text": "could you could you say a bit more about",
    "start": "1168720",
    "end": "1170480"
  },
  {
    "text": "how you do the scheduling maybe so how",
    "start": "1170480",
    "end": "1173120"
  },
  {
    "text": "things like pod migration or you know",
    "start": "1173120",
    "end": "1175440"
  },
  {
    "text": "what happens if a node goes down that a",
    "start": "1175440",
    "end": "1177440"
  },
  {
    "text": "pod is running on these kind of uh",
    "start": "1177440",
    "end": "1180400"
  },
  {
    "text": "questions okay the scheduling part is we",
    "start": "1180400",
    "end": "1184720"
  },
  {
    "text": "implemented",
    "start": "1184720",
    "end": "1187440"
  },
  {
    "text": "our we implementment our",
    "start": "1190280",
    "end": "1193480"
  },
  {
    "text": "own scheduleuler extender here and the",
    "start": "1193480",
    "end": "1196480"
  },
  {
    "text": "hammock is composed of a mutating web",
    "start": "1196480",
    "end": "1198400"
  },
  {
    "text": "hook schedule extender and we do the",
    "start": "1198400",
    "end": "1200559"
  },
  {
    "text": "scheduling here we uh we implement the",
    "start": "1200559",
    "end": "1205360"
  },
  {
    "text": "the filter",
    "start": "1205360",
    "end": "1208320"
  },
  {
    "text": "and the filter and score process yes we",
    "start": "1208600",
    "end": "1212640"
  },
  {
    "text": "and we do the uh additional u GPU",
    "start": "1212640",
    "end": "1216720"
  },
  {
    "text": "filtering and GPU node scoring here in",
    "start": "1216720",
    "end": "1219280"
  },
  {
    "text": "the schedule extender yeah so is it a",
    "start": "1219280",
    "end": "1222240"
  },
  {
    "text": "regular scheduleuler plug-in as as in",
    "start": "1222240",
    "end": "1224799"
  },
  {
    "text": "the plug-in framework or is it a",
    "start": "1224799",
    "end": "1227280"
  },
  {
    "text": "completely separate scheduleuler uh it",
    "start": "1227280",
    "end": "1229760"
  },
  {
    "text": "is a scheduleuler extender is not a",
    "start": "1229760",
    "end": "1232240"
  },
  {
    "text": "scheduleuler framework because if you if",
    "start": "1232240",
    "end": "1234400"
  },
  {
    "text": "we adopt the architect of the",
    "start": "1234400",
    "end": "1236080"
  },
  {
    "text": "scheduleuler framework you have to",
    "start": "1236080",
    "end": "1237760"
  },
  {
    "text": "compare every uh every kubernetes",
    "start": "1237760",
    "end": "1241360"
  },
  {
    "text": "schedule version from one from 116 to",
    "start": "1241360",
    "end": "1244799"
  },
  {
    "text": "132 it is difficult for open source",
    "start": "1244799",
    "end": "1247360"
  },
  {
    "text": "project like us so we use the schedule",
    "start": "1247360",
    "end": "1250720"
  },
  {
    "text": "extender to to for for using that uh it",
    "start": "1250720",
    "end": "1254159"
  },
  {
    "text": "can be easily inserted in into every uh",
    "start": "1254159",
    "end": "1257760"
  },
  {
    "text": "scheduleuler version from 116 to 132",
    "start": "1257760",
    "end": "1263120"
  },
  {
    "text": "yes okay any any other",
    "start": "1265880",
    "end": "1270159"
  },
  {
    "text": "questions have a last day bye-bye have a",
    "start": "1271480",
    "end": "1274320"
  },
  {
    "text": "nice day",
    "start": "1274320",
    "end": "1277559"
  }
]