[
  {
    "text": "hi my name is Susan woo I'm in Cloud networking I'm a product manager and so here we're talking about making",
    "start": "359",
    "end": "6319"
  },
  {
    "text": "kubernetes simpler for Accelerated workloads I have this steam panel I'm",
    "start": "6319",
    "end": "11599"
  },
  {
    "text": "going to let them introduce themselves go ahead first testing hey Cool Works um so I'm",
    "start": "11599",
    "end": "18720"
  },
  {
    "text": "ad I work as a product manager at cruser cruser Cloud I guess um what we do is",
    "start": "18720",
    "end": "24640"
  },
  {
    "text": "wear any iCloud but we collocate our data centers next day stranded wasted or sustainable energy sources so we do some",
    "start": "24640",
    "end": "31519"
  },
  {
    "text": "interesting things with energy in the way we power our Cloud um yeah I manage sort of our platform services and",
    "start": "31519",
    "end": "37320"
  },
  {
    "text": "orchestration there and before this I spent a bunch of time with ews building out to the selfless computer Lambda and",
    "start": "37320",
    "end": "42640"
  },
  {
    "text": "then couple other companies before that hi I'm Lucy I'm a senior software",
    "start": "42640",
    "end": "47719"
  },
  {
    "text": "engineer at Uber who I think most of you familiar with if you took an Uber here thanks for keeping me in a job uh I run",
    "start": "47719",
    "end": "53920"
  },
  {
    "text": "uh and work on a stateless compute platform called up which we have a 3 million CPU Calles behind and that runs",
    "start": "53920",
    "end": "59519"
  },
  {
    "text": "everywhere from uh the most the uh trip serving apps for web applications all the way all the way through to uh a Ai",
    "start": "59519",
    "end": "67200"
  },
  {
    "text": "workloads and accelerated workloads everything that is effectively stateless uh and yeah hi I'm Rebecca Weekley I run",
    "start": "67200",
    "end": "74479"
  },
  {
    "text": "infrastructure engineering at Geico and you might wonder what that means at Geico we have six data centers we have",
    "start": "74479",
    "end": "80680"
  },
  {
    "text": "eight different clouds uh so it is actually the hybrid Cloud footprint across all of our infrastructure needs",
    "start": "80680",
    "end": "87040"
  },
  {
    "text": "to support our business hey I'm Mitch McKenzie I'm a site reliability engineer at weave",
    "start": "87040",
    "end": "93280"
  },
  {
    "text": "Communications and uh we uh we're a software company we sell a small uh software to small to mediumsized",
    "start": "93280",
    "end": "99759"
  },
  {
    "text": "businesses to uh help them run their offices so primarily focused on you know dental offices optometrist uh you know",
    "start": "99759",
    "end": "106000"
  },
  {
    "text": "vet type offices and um yeah we're we're sort of like in the early stages of building out our ml platform and and uh",
    "start": "106000",
    "end": "113040"
  },
  {
    "text": "running workloads on accelerators and and Kates so we uh Mitch had to travel in",
    "start": "113040",
    "end": "119119"
  },
  {
    "text": "but his company is headquartered where Lehi 45 minutes",
    "start": "119119",
    "end": "124920"
  },
  {
    "text": "south so okay let's get Dive Right In so you know this session is all about",
    "start": "124920",
    "end": "129959"
  },
  {
    "text": "platform engineers and so for the platform Engineers that we have in this very panel do you generally offer",
    "start": "129959",
    "end": "137480"
  },
  {
    "text": "kubernetes for conventional software development and then add AI or do you have dedicated teams for AI and uh I'm",
    "start": "137480",
    "end": "145440"
  },
  {
    "text": "going to start with Lucy on that one y sure um so uh I work on our stateless",
    "start": "145440",
    "end": "150560"
  },
  {
    "text": "computer platform and we offer kubernetes and effectively service service deployment to all teams across",
    "start": "150560",
    "end": "156760"
  },
  {
    "text": "Uber who want to do any sort of State stateless workloads so that can be anything from um web applications such",
    "start": "156760",
    "end": "163040"
  },
  {
    "text": "as the Uber website uh through through to uh internal report generators and",
    "start": "163040",
    "end": "168239"
  },
  {
    "text": "including in that as well uh AI workloads as accelerated workloads on top so that could be anything from",
    "start": "168239",
    "end": "174120"
  },
  {
    "text": "inference uh workloads that are running in production and that are critical to production working uh over to training",
    "start": "174120",
    "end": "180760"
  },
  {
    "text": "where people are training models for all sorts of uh internal use cases inside of uber and then uh we set we segment we",
    "start": "180760",
    "end": "187920"
  },
  {
    "text": "segment out that uh what is it we have a self- Ser application and applic and the service owners come to us and they uh",
    "start": "187920",
    "end": "194239"
  },
  {
    "text": "say oh I need this specific Hardware that may be unconventional and then our system autonomously then places them",
    "start": "194239",
    "end": "201040"
  },
  {
    "text": "onto that hardware for most folks then it kind of hides the complexity because they just don't request the resource and",
    "start": "201040",
    "end": "206400"
  },
  {
    "text": "they don't have to care but it means that we can keep this all in one platform and not have to rebuil the wheel just because it's machine",
    "start": "206400",
    "end": "212560"
  },
  {
    "text": "learning others can chime in as well how about cuso do they have",
    "start": "212560",
    "end": "220000"
  },
  {
    "text": "dedicated teams or I'm thinking you do because it's ml yeah we don't know Cloud so I think we have a dedicated platform",
    "start": "220000",
    "end": "226319"
  },
  {
    "text": "team I think the interesting bit at least the I think the observation that we have had we obviously work with a bunch of customers and they kind of vary",
    "start": "226319",
    "end": "232799"
  },
  {
    "text": "like they're a lot of sort of AI native companies that you know their whole sort of business is sort of training and",
    "start": "232799",
    "end": "238519"
  },
  {
    "text": "serving model and so that their layout is to different but when we sort of worked with Enterprises and work with",
    "start": "238519",
    "end": "244519"
  },
  {
    "text": "companies that are kind of figuring out that journey I think it's an interesting transition where you have you know",
    "start": "244519",
    "end": "249799"
  },
  {
    "text": "typically you know a lot of the sort of data science in ml workloads have sort of been handled by the research",
    "start": "249799",
    "end": "255760"
  },
  {
    "text": "divisions the research teams that are sort of pushing the boundaries and what you can do right so you have companies that really focus on that but then with",
    "start": "255760",
    "end": "263080"
  },
  {
    "text": "a just becoming more prevalent as as a workload for a lot of different companies I think that that responsibility is Shifting to just sort",
    "start": "263080",
    "end": "269320"
  },
  {
    "text": "of General platform teams General infrastructure teams that are required to learn how to sort of host manage and",
    "start": "269320",
    "end": "274360"
  },
  {
    "text": "deploy these models in production and so I think there's a I know we're going to talk about it more I think there's a lot",
    "start": "274360",
    "end": "280199"
  },
  {
    "text": "of challenges that those teams have to sort of figured out as they learn about these workers learn about sort of optimizing the infrastructure they turn",
    "start": "280199",
    "end": "286440"
  },
  {
    "text": "on preventing sort of cluster sprawl all of those interesting things right what about Geico what happens",
    "start": "286440",
    "end": "292680"
  },
  {
    "text": "there sure so we have a platform team that focuses like I said from an infrastructure perspective on kubernetes",
    "start": "292680",
    "end": "299960"
  },
  {
    "text": "sort of vended compute and storage below and we try very hard to make sure they don't need to think about which of those",
    "start": "299960",
    "end": "306520"
  },
  {
    "text": "many clouds is how or where things are scheduled they just get to focus on the application development ML and AI are",
    "start": "306520",
    "end": "313680"
  },
  {
    "text": "the fun children in all of these problems right just like our data teams before um or the research teams before",
    "start": "313680",
    "end": "320960"
  },
  {
    "text": "that they always want a very specific kind of configuration and of course kubernetes tends to lend itself really",
    "start": "320960",
    "end": "327199"
  },
  {
    "text": "well to cattle and not particularly well to pets um that being said there's a lot",
    "start": "327199",
    "end": "332560"
  },
  {
    "text": "of great opportunities that are coming out to really ensure that kubernetes is",
    "start": "332560",
    "end": "337840"
  },
  {
    "text": "not just being used as a container orchestration platform but truly as an infrastructure management layer and",
    "start": "337840",
    "end": "343520"
  },
  {
    "text": "that's where we've seen all the opportunities whether you're using Cube flow or you know something that is more vended and managed for you like a data",
    "start": "343520",
    "end": "350160"
  },
  {
    "text": "robot those are going to give you a very consistent data pipeline integration with a lot of the different models for",
    "start": "350160",
    "end": "356199"
  },
  {
    "text": "the experimentation folks but they also give you the ability to have much better open Telemetry exporters much more",
    "start": "356199",
    "end": "363360"
  },
  {
    "text": "consistency across clouds so that you can start to optimize once they are done with their tuning and playing in",
    "start": "363360",
    "end": "369319"
  },
  {
    "text": "whatever version of cloud they want to work in you can start to optimize internally and say okay it looks like we",
    "start": "369319",
    "end": "375479"
  },
  {
    "text": "have deterministic load this is how we're going to do this particular workload over time and we can start to",
    "start": "375479",
    "end": "380520"
  },
  {
    "text": "look at opportunities to optimize it I think we covered that question pretty well U Mitch let me ask you we have this",
    "start": "380520",
    "end": "387319"
  },
  {
    "text": "eye chart but uh what kind of customization have you made to kubernetes are there any adjacent open",
    "start": "387319",
    "end": "392360"
  },
  {
    "text": "source projects that you had to implement for this workload yeah so uh we so we adopted kubernetes like I don't",
    "start": "392360",
    "end": "398960"
  },
  {
    "text": "know two years after it came out so we've spent a lot of time investing in our platform that we've built on top of",
    "start": "398960",
    "end": "404639"
  },
  {
    "text": "it uh mostly because we want to protect developers from the complexities of kubernetes not to throw shade on",
    "start": "404639",
    "end": "410440"
  },
  {
    "text": "kubernetes at a kubernetes conference but um yeah that was like the major major motivator so um yeah like our",
    "start": "410440",
    "end": "417120"
  },
  {
    "text": "approach uh to running accelerated work GR is just to keep it really simple and lean into all of the tooling that we",
    "start": "417120",
    "end": "423360"
  },
  {
    "text": "already have built out that our developers know how to use and um that really gives them that that really",
    "start": "423360",
    "end": "429080"
  },
  {
    "text": "allows them to be um to to move quickly right and deliver value to our customers",
    "start": "429080",
    "end": "434120"
  },
  {
    "text": "faster and so um yeah our answer to running accelerators is just you know use core Kates we're using the the core",
    "start": "434120",
    "end": "440919"
  },
  {
    "text": "Cube scheduler um we're just using node Affinity you know uh node taints and and",
    "start": "440919",
    "end": "446039"
  },
  {
    "text": "and pod tolerations to get your your pods placed in the right place um some open source things that we're",
    "start": "446039",
    "end": "452160"
  },
  {
    "text": "using kada for scaling cuz you know most likely you won't be able to scale you know on the traditional things like CPU",
    "start": "452160",
    "end": "458440"
  },
  {
    "text": "or even accelerator usage uh so you'll most likely need some sort of like custom metrics that you want to scale on",
    "start": "458440",
    "end": "463599"
  },
  {
    "text": "so kada is definitely one that we brought in uh to replace uh HPA in our old um Prometheus adapter that we were",
    "start": "463599",
    "end": "470319"
  },
  {
    "text": "using to pull custom metrics with before um and then uh I like metrics so dcgm",
    "start": "470319",
    "end": "476680"
  },
  {
    "text": "exporter is good for knowing how your accelerators are doing so um that's deployed as a Damon set and uh yeah",
    "start": "476680",
    "end": "483360"
  },
  {
    "text": "that's kind of the open source uh pieces that we're using yeah it seems like you touched on a lot of them you know D",
    "start": "483360",
    "end": "489560"
  },
  {
    "text": "you're a PM so as you're building out the platform I'm sure you've added a lot of the adjacent open source projects",
    "start": "489560",
    "end": "494879"
  },
  {
    "text": "into your platform yeah I mean it's I guess we're sort of what a lot of our customers do when",
    "start": "494879",
    "end": "501800"
  },
  {
    "text": "they sort of come on our Cloud the first thing they obviously do is deploy kubernetes because that's tends to be the standard way you tend to interact",
    "start": "501800",
    "end": "506960"
  },
  {
    "text": "with infrastructure especially when you're operating a Ross multiple clouds like a lot of our customers do and then",
    "start": "506960",
    "end": "512440"
  },
  {
    "text": "I think it's I think it very much depends on you know how you want to sort of submit jobs and there are obviously",
    "start": "512440",
    "end": "518320"
  },
  {
    "text": "standard pieces so things like sort of monitoring you typically go through dcgm xor you set up your sort of cube",
    "start": "518320",
    "end": "523919"
  },
  {
    "text": "Prometheus and grafana and like however you choose to visualize your metrics right um as I think the biggest piece of",
    "start": "523919",
    "end": "529600"
  },
  {
    "text": "customization that a lot of our customers do is around the scheduler and the scheduling logic um so you have some",
    "start": "529600",
    "end": "534720"
  },
  {
    "text": "customers who use something like Q um Ray is really popular I think especially as you",
    "start": "534720",
    "end": "540120"
  },
  {
    "text": "especially for a lot of our customers tend to operate in this pattern where they you know you have clusters across",
    "start": "540120",
    "end": "545360"
  },
  {
    "text": "multiple clouds and then you have typically a parent cluster that's like orchestrating jobs across multiple child",
    "start": "545360",
    "end": "550800"
  },
  {
    "text": "clusters that are sitting in Pro like providers like us or in like other sort of GPU clouds or in like gcp or AWS pick",
    "start": "550800",
    "end": "557480"
  },
  {
    "text": "pick your Cloud right um and so sort of setting up that logic I think there's been a bunch of interesting talks on",
    "start": "557480",
    "end": "562600"
  },
  {
    "text": "doing that with q and doing that with like I think Sky day was a talk that happened so yeah that that pattern of",
    "start": "562600",
    "end": "568279"
  },
  {
    "text": "deploying that and customizing your cluster so they sort of receive and sort of handle of AI jobs well I think it's",
    "start": "568279",
    "end": "574920"
  },
  {
    "text": "primarily what we see a lot of customers do Uber I think there's some things you can talk to but without getting there is",
    "start": "574920",
    "end": "582760"
  },
  {
    "text": "disclose what you can yeah no it's fine it's fine um so um what to say uh one of",
    "start": "582760",
    "end": "589040"
  },
  {
    "text": "the things that I think is quite interesting in space with customizing on K8 and what K8 has been weak at until recently is dealing with the fact that",
    "start": "589040",
    "end": "596880"
  },
  {
    "text": "certain workloads want certain Hardware and that that and that that Hardware may be slightly specialized in some ways",
    "start": "596880",
    "end": "602880"
  },
  {
    "text": "back when kubernetes was initially built uh I think a met met quite a few years ago I think some of you folks were",
    "start": "602880",
    "end": "609320"
  },
  {
    "text": "probably around then uh a lot more of our Hardware was quite commoditized we all used either a x86 or amd64 for",
    "start": "609320",
    "end": "616320"
  },
  {
    "text": "example looking at nothing we all just use like a GPU no one really well some people did but not many people really",
    "start": "616320",
    "end": "622480"
  },
  {
    "text": "cared about what Hardware they're on but now you look at machine learning and it's kind of the tip of the iceberg on this and that with machine learning in",
    "start": "622480",
    "end": "628560"
  },
  {
    "text": "particular of course course it's like you use a different uh use a different GPU you get much better training results",
    "start": "628560",
    "end": "634000"
  },
  {
    "text": "uh but also these gpus are really expensive but this applies also to loads of other things I see I have folks with",
    "start": "634000",
    "end": "639480"
  },
  {
    "text": "gpus coming to me saying you know I need to be on this h100 or my model is going to suck but I also even have people like",
    "start": "639480",
    "end": "645519"
  },
  {
    "text": "saying I want to I want my uh program to be running on arm because it's cheaper and it's more efficient and I think it's",
    "start": "645519",
    "end": "652600"
  },
  {
    "text": "like that the whole industry seems to be swinging back this way um that before and this is part of why Incan it's like",
    "start": "652600",
    "end": "658560"
  },
  {
    "text": "we just before Dr we just have memory and CPU and like what CPU a CPU",
    "start": "658560",
    "end": "663680"
  },
  {
    "text": "somewhere who knows what Arch it is who knows what it is and now and now you actually you care a lot about what that",
    "start": "663680",
    "end": "669040"
  },
  {
    "text": "one is and luckily kubernetes is getting better at supporting that um so for example with Dr and things that I think",
    "start": "669040",
    "end": "674959"
  },
  {
    "text": "people should be using very liberally uh the other issue we have which didn't used to be much of a thing particularly",
    "start": "674959",
    "end": "680480"
  },
  {
    "text": "back when Hardware was much more I guess commoditized and more generic was that people really care about squeezing every",
    "start": "680480",
    "end": "686720"
  },
  {
    "text": "last scent out of out of expensive Hardware you back back in previous times it was like oh I could run my cluster at",
    "start": "686720",
    "end": "692200"
  },
  {
    "text": "like 80% capacity and it's like oh cool whatever you know good enough now it's like I get Engineers come to me with",
    "start": "692200",
    "end": "698440"
  },
  {
    "text": "h100 and say like I want to use every single one of these all the time I don't want a single h100 to be idle it is just",
    "start": "698440",
    "end": "705680"
  },
  {
    "text": "a massive money sync if I do that and so for that in particular we start we are using uh different uh schedulers so for",
    "start": "705680",
    "end": "712519"
  },
  {
    "text": "example we're doing a lot of Bin packing to avoid resource fragmentation so you know if you've got a node with like if",
    "start": "712519",
    "end": "717800"
  },
  {
    "text": "you've got 100 nodes each with h g eight gpus all of them used using seven gpus and then you need to schedule a new",
    "start": "717800",
    "end": "723560"
  },
  {
    "text": "workload that needs two gpus can't do it so we found much we've actually found it much more successful for uh using the",
    "start": "723560",
    "end": "731040"
  },
  {
    "text": "maximum resources on our clusters when we have been bin packing we found it much better than the traditional",
    "start": "731040",
    "end": "736240"
  },
  {
    "text": "scheduler not to froche than the traditional Schuler it's great for most cases and most generic cases but",
    "start": "736240",
    "end": "741399"
  },
  {
    "text": "particularly when you have expensive Hardware bin packing I think is a really easy thing you can do to squeeze a bit",
    "start": "741399",
    "end": "747600"
  },
  {
    "text": "more out of them uh show of hands how many folks are using the gpus uh",
    "start": "747600",
    "end": "754160"
  },
  {
    "text": "and a few out there anyone using the tpus not so far oh there's one out there",
    "start": "754160",
    "end": "761680"
  },
  {
    "text": "okay anyone using the arm that the processors that the okay okay got a good",
    "start": "761680",
    "end": "766880"
  },
  {
    "text": "mix of everything okay um did anyone else want to weigh in I I also have a follow",
    "start": "766880",
    "end": "773199"
  },
  {
    "text": "question on you know like how did you choose those specialized compute actually like is there a criteria that",
    "start": "773199",
    "end": "778680"
  },
  {
    "text": "you look for for is it a region is it availability what what do you use to uh",
    "start": "778680",
    "end": "784040"
  },
  {
    "text": "choose those make those choices yeah so uh initially we don't",
    "start": "784040",
    "end": "790440"
  },
  {
    "text": "have strong uh preferences for what gpus so it's uh so like I I touched on node",
    "start": "790440",
    "end": "795519"
  },
  {
    "text": "Affinity so we're using uh we're waiting the node Affinity towards the cheaper gpus so like the t4s um and then the l4s",
    "start": "795519",
    "end": "802839"
  },
  {
    "text": "and then the v100s are the three that we're we're targeting and so um yeah you know finops cost savings is a is a big",
    "start": "802839",
    "end": "809560"
  },
  {
    "text": "part of uh you know kind of the the platform team that we've um that we like to to keep our eye on and make sure that",
    "start": "809560",
    "end": "815199"
  },
  {
    "text": "we keep our cost low so um that's a big criteria so",
    "start": "815199",
    "end": "821680"
  },
  {
    "text": "availability ability to run the model is probably the number one and two concerns",
    "start": "821680",
    "end": "828000"
  },
  {
    "text": "that have happened from an Enterprise perspective over the last year and a half as everyone's thought about and",
    "start": "828000",
    "end": "833320"
  },
  {
    "text": "gotten excited about generative AI um so that actually I think has led to a state",
    "start": "833320",
    "end": "838639"
  },
  {
    "text": "where we are thinking about this from the data perspective first even before the hardware I love Hardware but even",
    "start": "838639",
    "end": "845759"
  },
  {
    "text": "before the hardware you got to start with where is my data that I want to train upon can I actually do or leverage",
    "start": "845759",
    "end": "852639"
  },
  {
    "text": "this model that model do we know is it deterministic right so how do we create the right experimentation opportunities to unlock",
    "start": "852639",
    "end": "860480"
  },
  {
    "text": "business value and then think through okay great we need to be doing this at",
    "start": "860480",
    "end": "867120"
  },
  {
    "text": "the right cost structures to actually deliver value to our business so separating the experimentation flows",
    "start": "867120",
    "end": "873720"
  },
  {
    "text": "into the actual infrastructure design choices and then using methodologies",
    "start": "873720",
    "end": "879320"
  },
  {
    "text": "like kubernetes running on top so that you have the ability to move things much more easily because AI we keep talking",
    "start": "879320",
    "end": "886600"
  },
  {
    "text": "about it it's just one phase of a pipeline we can't we need all the data there to be able to do the work",
    "start": "886600",
    "end": "892680"
  },
  {
    "text": "effectively and that's why you want it to be as close to those end States as possible a d do you have a mixed",
    "start": "892680",
    "end": "899920"
  },
  {
    "text": "environment like current generation GPU and then future yeah talk about that cuz I was in the queue and they were saying",
    "start": "899920",
    "end": "906000"
  },
  {
    "text": "you could put resources and label the GPU Generations yeah I mean we obviously",
    "start": "906000",
    "end": "911519"
  },
  {
    "text": "offer a mix of gpus you know and and I think we're at an interesting point with",
    "start": "911519",
    "end": "916759"
  },
  {
    "text": "the kind of Hardware treadmill that's going on right now so you obviously have obviously dominated by Nvidia and you have the h20s coming out and you have",
    "start": "916759",
    "end": "923360"
  },
  {
    "text": "the sort of black wall generation that sort of starting to peek in right and so interesting point where you know",
    "start": "923360",
    "end": "929959"
  },
  {
    "text": "traditionally you you sort of gpus are primarily different by like availability right it's just where can I find gpus",
    "start": "929959",
    "end": "936120"
  },
  {
    "text": "and I'm going to use them right now I think especially with a lot of the older Generations um you know A1 100s and even",
    "start": "936120",
    "end": "943160"
  },
  {
    "text": "maybe know l4s is untraditionally old but like I think you're starting to see availability sort of pop up there as well I think so there opportunities for",
    "start": "943160",
    "end": "949399"
  },
  {
    "text": "people to sort of especially with inance I'll say we spoke a lot about training but with inference I think there's you",
    "start": "949399",
    "end": "955560"
  },
  {
    "text": "know once you've trained your model there are lots of optimizations that you can do I think there's specifically for",
    "start": "955560",
    "end": "962160"
  },
  {
    "text": "comes down to your use case right for your use case you're kind of trading off against throughput and sort of responsiveness right so how latency",
    "start": "962160",
    "end": "969279"
  },
  {
    "text": "sensitive do you want your model to be can I sort of you know batch sort of batch the you know the data that I'm of",
    "start": "969279",
    "end": "975959"
  },
  {
    "text": "feeding into the model to sort of optimize its utilization um can I optimize the model through quantization and other",
    "start": "975959",
    "end": "981880"
  },
  {
    "text": "techniques right to get them to fit on specific sort of chips and then take advantage of those to improve your price performance right that's what we see a",
    "start": "981880",
    "end": "987680"
  },
  {
    "text": "lot of our customers doing successfully right um especially when taking advantage of chips like even even if you",
    "start": "987680",
    "end": "993279"
  },
  {
    "text": "look at you know AMD for example with their Mi 300 XS right a lot of where they're trying to go after the market is",
    "start": "993279",
    "end": "999079"
  },
  {
    "text": "around inference right with a sort of larger larger sort of vram on the gpus to fit these models and accommodate more",
    "start": "999079",
    "end": "1004560"
  },
  {
    "text": "models in the single GPU so you're able to sort of squeeze more utilization out of it so I think that's what a lot of the opportunity is right um as far as",
    "start": "1004560",
    "end": "1010800"
  },
  {
    "text": "taking advantage of older Generations is specific Hardware let me ask a follow-up questions where do you run your gpus is",
    "start": "1010800",
    "end": "1016040"
  },
  {
    "text": "it in the cloud is it on Prem Rebecca did do you have a Viewpoint both uh for that reason right",
    "start": "1016040",
    "end": "1023839"
  },
  {
    "text": "because if you want people to experiment you want them to experiment in a cloud that's generally a better spot I don't",
    "start": "1023839",
    "end": "1030199"
  },
  {
    "text": "have any optimization I mean I do but it's more in the zone of VM dimming and",
    "start": "1030199",
    "end": "1036480"
  },
  {
    "text": "things that are easier to implement without creating you know churn for my",
    "start": "1036480",
    "end": "1041760"
  },
  {
    "text": "users when it comes to unlocking you know AI ml is core to the business right",
    "start": "1041760",
    "end": "1047839"
  },
  {
    "text": "insurance is a to risk model it is a Data Business that is what we do however",
    "start": "1047839",
    "end": "1052919"
  },
  {
    "text": "well we price risk we make money if we don't price it well we lose money that's",
    "start": "1052919",
    "end": "1058120"
  },
  {
    "text": "it that's the business and so we've been doing Ai and ML and experimenting in this domain for a very long time but",
    "start": "1058120",
    "end": "1065480"
  },
  {
    "text": "maybe not with the generative AI mentality that has become so exciting",
    "start": "1065480",
    "end": "1071080"
  },
  {
    "text": "when you're a regulated industry you have to be able to describe to everyone why you came to that assessment of risk",
    "start": "1071080",
    "end": "1078600"
  },
  {
    "text": "for that individual in that zone at that time with that vehicle so you have to have something that humans and Auditors",
    "start": "1078600",
    "end": "1085120"
  },
  {
    "text": "can rationalize about so it doesn't really lend itself to hallucination so",
    "start": "1085120",
    "end": "1090799"
  },
  {
    "text": "we don't really want to play that game right we want to be very deterministic when it comes to how we run our Core",
    "start": "1090799",
    "end": "1096000"
  },
  {
    "text": "Business when it comes to generative AI That's about productivity of our people and how do we make sure we're actually",
    "start": "1096000",
    "end": "1102960"
  },
  {
    "text": "en enabling our developers to move more quickly how are we enabling our agents to work across systems",
    "start": "1102960",
    "end": "1109480"
  },
  {
    "text": "understand compliance rules in the region that they're in work across state lines so that they don't have to be an expert in that region only all of those",
    "start": "1109480",
    "end": "1116600"
  },
  {
    "text": "factors that can make a human better and that's where if it's not perfectly accurate there's still time to fix it",
    "start": "1116600",
    "end": "1122600"
  },
  {
    "text": "and there's a human interpreting the results to ensure that they are accurate and effective and so it's the time to",
    "start": "1122600",
    "end": "1127840"
  },
  {
    "text": "efficacy for an agent it's the time to be able to get back and be excellent at our jobs and excellent at giving service",
    "start": "1127840",
    "end": "1134320"
  },
  {
    "text": "to our end users so those are very different parts of the stack right one's",
    "start": "1134320",
    "end": "1139640"
  },
  {
    "text": "my core business and you know a set of data that we're going to extract and analyze 10 to the 14 parameters around",
    "start": "1139640",
    "end": "1146760"
  },
  {
    "text": "rate to risk that is running on Prem that is well understood that is constantly being optimized we're running",
    "start": "1146760",
    "end": "1152240"
  },
  {
    "text": "new models against it and tuning on new data as we get new data and it is 60%",
    "start": "1152240",
    "end": "1157760"
  },
  {
    "text": "plus cheaper to do it there than anywhere else if I go and look at ways in which we're playing with generative",
    "start": "1157760",
    "end": "1163600"
  },
  {
    "text": "Ai and trying to look at what might be interesting for an agent assist platform",
    "start": "1163600",
    "end": "1168960"
  },
  {
    "text": "yeah we're going to do that in the cloud we're going to play over there we're going to figure out what's right and then we'll decide this is a part of the",
    "start": "1168960",
    "end": "1174880"
  },
  {
    "text": "Core Business it needs to be it's mostly going to be an inference we'll stay that part in the cloud and then what we need",
    "start": "1174880",
    "end": "1180080"
  },
  {
    "text": "to do if we really need to train it on specific data that is regulated we would keep that from a data like security",
    "start": "1180080",
    "end": "1186080"
  },
  {
    "text": "perspective on Prem so it really comes down to the same factors but just thinking through our security model what",
    "start": "1186080",
    "end": "1192240"
  },
  {
    "text": "is core to the business what do we want to have that is differentiated and then where are the users and the eyeballs and the low latency scenarios or the",
    "start": "1192240",
    "end": "1199200"
  },
  {
    "text": "experimentation that would make us want to be more in the cloud yeah that makes sense trust and safety are like one of",
    "start": "1199200",
    "end": "1205480"
  },
  {
    "text": "the key criterias for selection okay now it's time for folks to get on the get on",
    "start": "1205480",
    "end": "1211039"
  },
  {
    "text": "the microphone and ask questions like I think uh about six months ago I did this",
    "start": "1211039",
    "end": "1216840"
  },
  {
    "text": "panel and a lot of folks were just getting into this and now this this uh at this conference they're like tons of",
    "start": "1216840",
    "end": "1223600"
  },
  {
    "text": "sessions on AI so I'm I'm thinking some folks are already out in the audience already deploying kubernetes clusters",
    "start": "1223600",
    "end": "1231440"
  },
  {
    "text": "building abstractions for you know data scientists research scientists but are there folks that have questions on how",
    "start": "1231440",
    "end": "1237240"
  },
  {
    "text": "to get into this uh feel free to go to the microphone uh you know and and ask",
    "start": "1237240",
    "end": "1243120"
  },
  {
    "text": "your questions because I I think it's um uh I've been meeting a lot of platform",
    "start": "1243120",
    "end": "1248440"
  },
  {
    "text": "Engineers they sort of gradually get into it uh they were providing",
    "start": "1248440",
    "end": "1253799"
  },
  {
    "text": "conventional kubernetes for applications and then suddenly they got drafted",
    "start": "1253799",
    "end": "1259200"
  },
  {
    "text": "so thank you sir go go ahead yeah hi thanks for sharing your uh perspectives I asked this question with few other uh",
    "start": "1259200",
    "end": "1265720"
  },
  {
    "text": "attendees as well but I would like to hear your perspective on this large language models large in size we would",
    "start": "1265720",
    "end": "1271720"
  },
  {
    "text": "like to cash it in Blob storage but then when we want to serve it we have to create a copy of it in your clusters",
    "start": "1271720",
    "end": "1278120"
  },
  {
    "text": "locally and then we would also like to Autos scale it across multiple nodes or multiple gpus so how do you make sure",
    "start": "1278120",
    "end": "1284520"
  },
  {
    "text": "that uh you know you bring this right performance and be able to ser serve uh",
    "start": "1284520",
    "end": "1290279"
  },
  {
    "text": "at an optimal Pace do you serve it from The Blob storage do you download it from The Blob storage every time a new part",
    "start": "1290279",
    "end": "1296840"
  },
  {
    "text": "is created what has been your experience yeah we we cheated a little",
    "start": "1296840",
    "end": "1303159"
  },
  {
    "text": "bit um in that regard because uh so we're I don't know if you're running in in gke uh you do oh a okay you could be",
    "start": "1303159",
    "end": "1313080"
  },
  {
    "text": "any kuet it doesn't matter yeah so anyways like we uh we're dealing with really large container sizes right if",
    "start": "1313080",
    "end": "1319640"
  },
  {
    "text": "you depending on how you build your containers but um this is a Sol problem so you'll need some sort of like pre-etch mechanism um and so in GK it's",
    "start": "1319640",
    "end": "1327240"
  },
  {
    "text": "a solved problem that they solved about three years ago they have container image streaming and so when we ran into",
    "start": "1327240",
    "end": "1332480"
  },
  {
    "text": "like slow pod startup times uh a lot of that was downloading the container image",
    "start": "1332480",
    "end": "1337880"
  },
  {
    "text": "so that took like you know in some cases like 10 minutes and that makes customers unhappy um if we can't scale very quick",
    "start": "1337880",
    "end": "1345000"
  },
  {
    "text": "so um yeah we turned on container image streaming that took it down to like 4 to 7 seconds so um hopefully that that",
    "start": "1345000",
    "end": "1352039"
  },
  {
    "text": "helps yeah but what if the model size is like 70 gig or over",
    "start": "1352039",
    "end": "1357320"
  },
  {
    "text": "100 so I can't build a coner image or the regist wouldn't support it so yeah I",
    "start": "1357320",
    "end": "1363320"
  },
  {
    "text": "I guess I didn't do any testing in in on on that size were're more like the 10 7 to 10 gig range but yeah I think that um",
    "start": "1363320",
    "end": "1371080"
  },
  {
    "text": "the startup time of an application like this is actually a critical part though of whether it should be in the critical path so at least at Uber we do have",
    "start": "1371080",
    "end": "1377640"
  },
  {
    "text": "machine learning models that are the inference models that are part of the critical flow and kind of like GK we we",
    "start": "1377640",
    "end": "1383240"
  },
  {
    "text": "built our own image container prefecture uh so that we could pull those but yes when you get to 70 gabes it does",
    "start": "1383240",
    "end": "1389679"
  },
  {
    "text": "actually um what is it it becomes a question at least for us right now of um",
    "start": "1389679",
    "end": "1395400"
  },
  {
    "text": "well do we want to serve this in the core trip flow what if we need to spr spring it up spring up more containers",
    "start": "1395400",
    "end": "1400520"
  },
  {
    "text": "and we can't is the core trip flow going to die is that okay so while we do have",
    "start": "1400520",
    "end": "1405600"
  },
  {
    "text": "so we do actually have models that are like 7 GB Etc but but we're only using them right now as part of uh internal",
    "start": "1405600",
    "end": "1411880"
  },
  {
    "text": "uses so a great example is uh we have an llm and it's connected to all of Google",
    "start": "1411880",
    "end": "1417279"
  },
  {
    "text": "docs all py everything and internal Engineers go in there and they might ask it what you know what is with this",
    "start": "1417279",
    "end": "1422320"
  },
  {
    "text": "project or Etc um but we don't use that for the critical paring that because this is a tough thing to solve and the",
    "start": "1422320",
    "end": "1428679"
  },
  {
    "text": "even tougher part of it to solve is my my Engineers want um gpus to be used at",
    "start": "1428679",
    "end": "1433960"
  },
  {
    "text": "100% all the time including during inference because every GPU that's Idol is basically just burning cash but at",
    "start": "1433960",
    "end": "1440240"
  },
  {
    "text": "our scale we can't just go to a cloud provider and order more gpus we have like a lead time of like over a month if I want more h100s and so this also kind",
    "start": "1440240",
    "end": "1448039"
  },
  {
    "text": "of ties into this issue with like scaling up and down so for us really for models that big they would be scaled up",
    "start": "1448039",
    "end": "1455200"
  },
  {
    "text": "to the point of like Peak load at all times just because if I want to scale I can't I can't get the extra gpus to",
    "start": "1455200",
    "end": "1462159"
  },
  {
    "text": "scale it further if I need it so I have to assume the worst but yeah I mean the",
    "start": "1462159",
    "end": "1467640"
  },
  {
    "text": "the reason why I was ask asking this is if you're building a platform that could serve multiple teams or an organization",
    "start": "1467640",
    "end": "1473679"
  },
  {
    "text": "at Large Scale then there may be demands that come in where you know hey we we would like to have this model and then",
    "start": "1473679",
    "end": "1480000"
  },
  {
    "text": "you would be a you would want to be able to you know uh build some kind of aut Skilling in it and I know it is not",
    "start": "1480000",
    "end": "1485880"
  },
  {
    "text": "ideal to have a 70 gig model right uh being able to Auto scale but there are challenges so how do you tackle it in",
    "start": "1485880",
    "end": "1492120"
  },
  {
    "text": "production and stuff that was my area but I I think this area is evolving uh but yeah if if you have experience good",
    "start": "1492120",
    "end": "1498159"
  },
  {
    "text": "thank I mean I would really question the model and the use case right so few",
    "start": "1498159",
    "end": "1503880"
  },
  {
    "text": "models require free training like in an Enterprise use case again I I don't know",
    "start": "1503880",
    "end": "1509080"
  },
  {
    "text": "who you're supporting and why and how but if somebody came to me with that I'd say h let's go see what we actually are",
    "start": "1509080",
    "end": "1515960"
  },
  {
    "text": "using can we optimize the model down can we use the 13 billion perimeter version versus the you know because often when",
    "start": "1515960",
    "end": "1523080"
  },
  {
    "text": "we first start playing they're not doing an analysis against Precision yet",
    "start": "1523080",
    "end": "1528640"
  },
  {
    "text": "they're sort of taking in Auto whatever whatever the default settings were and they haven't necessarily done a lot of",
    "start": "1528640",
    "end": "1534600"
  },
  {
    "text": "pruning done a lot of optimization on the model to get to the best configuration for running within the",
    "start": "1534600",
    "end": "1541080"
  },
  {
    "text": "infrastructure that's why I came back to availability first like the best solution is the one you can render to",
    "start": "1541080",
    "end": "1547640"
  },
  {
    "text": "your users so if we can come to a place where we understand and do good testing",
    "start": "1547640",
    "end": "1552919"
  },
  {
    "text": "on the Precision and the accuracy to prune down the model and into the most optimal fashion and that gets them a",
    "start": "1552919",
    "end": "1559080"
  },
  {
    "text": "much lower latency and yeah maybe we have a little less accuracy but it's acceptable that is always going to be",
    "start": "1559080",
    "end": "1565159"
  },
  {
    "text": "easier for us to vend to an end user from rather than trying to make the",
    "start": "1565159",
    "end": "1570399"
  },
  {
    "text": "infrastructure match all the different pet use cases okay just following upon that right I know there are many others",
    "start": "1570399",
    "end": "1576919"
  },
  {
    "text": "so the idea is that you know we are not going to have this particular model serve all the request but this model",
    "start": "1576919",
    "end": "1582120"
  },
  {
    "text": "could be the interface for for anyone to come and say Hey Okay I want to do this from there it gets routed to the",
    "start": "1582120",
    "end": "1587679"
  },
  {
    "text": "respective model models or different use cases that we are building so in such cases you would still have you would",
    "start": "1587679",
    "end": "1593440"
  },
  {
    "text": "want to have a situation where you want have a larger model that has much uh bigger context or knowledge and from",
    "start": "1593440",
    "end": "1599399"
  },
  {
    "text": "there you'll be able to divert to a specific model that is serving a specific domain use case and stuff which",
    "start": "1599399",
    "end": "1604640"
  },
  {
    "text": "is the idea that we are going into but the challenge that large models bring is how do we scale it across multiple nodes",
    "start": "1604640",
    "end": "1609799"
  },
  {
    "text": "and stuff but yeah thanks for your input hi I'm Beth Paris I'm a reporter",
    "start": "1609799",
    "end": "1616840"
  },
  {
    "text": "from Tech Target um um couple of you mentioned things like Dr and Q um what",
    "start": "1616840",
    "end": "1622000"
  },
  {
    "text": "is on your wish list for the projects that you're using to make it easier to support AI apps on kubernetes any other",
    "start": "1622000",
    "end": "1630080"
  },
  {
    "text": "features that you still want to see or that are still being",
    "start": "1630080",
    "end": "1635039"
  },
  {
    "text": "developed shall I go okay um I think that right now one of",
    "start": "1636960",
    "end": "1642440"
  },
  {
    "text": "the toughest things for us is uh resource scheduling across clusters uh why I at the S multi cluster uh talk",
    "start": "1642440",
    "end": "1648440"
  },
  {
    "text": "earlier I think that um one of the tough things that at least we've seen especially when we get to very high",
    "start": "1648440",
    "end": "1654480"
  },
  {
    "text": "resource utilization which is something that actually people want as I've said many times now with gpus it it can very",
    "start": "1654480",
    "end": "1661399"
  },
  {
    "text": "much end up being oh this cluster is 100% usage but this one is not and we don't really have a great story about",
    "start": "1661399",
    "end": "1668000"
  },
  {
    "text": "moving the load between them we kind of have a we have a story about this now at least in Uber about like oh we can",
    "start": "1668000",
    "end": "1673320"
  },
  {
    "text": "rebalance work from this cluster to this cluster but the tough part about that and the part that kind of makes me sad",
    "start": "1673320",
    "end": "1678640"
  },
  {
    "text": "is that I think everyone has kind of built um some sort of multicluster rebalancer at least every Enterprise of",
    "start": "1678640",
    "end": "1684200"
  },
  {
    "text": "a large size and we've all just built our own ones and it's just such a waste of everyone's time that we've had to all go and do this because we could have",
    "start": "1684200",
    "end": "1689840"
  },
  {
    "text": "just built One open source one like like effectively what happened with kuet is instead of having you know probably what",
    "start": "1689840",
    "end": "1695000"
  },
  {
    "text": "10,000 people in like 100 ORS each who have effectively reinvented the wheel 100 times but yeah I think for me like",
    "start": "1695000",
    "end": "1702279"
  },
  {
    "text": "being able to especially now that I'm being asked like a 100% max out GPS all the time being able to do this in a much",
    "start": "1702279",
    "end": "1708720"
  },
  {
    "text": "more smart way through an open source project is something that I would really really yeah",
    "start": "1708720",
    "end": "1715480"
  },
  {
    "text": "want yeah so definitely plus one that um and this I I guess this kind of",
    "start": "1715559",
    "end": "1722440"
  },
  {
    "text": "transcends the scheduling there potentially but like a theme that you would maybe notice in a lot of the talks",
    "start": "1722440",
    "end": "1728120"
  },
  {
    "text": "is sort of fall tolerance and failure to mediation right especially with GPU workloads um and especially with",
    "start": "1728120",
    "end": "1734320"
  },
  {
    "text": "training right so there's a lot of effort going into the space around just how can we make sort of recovering from",
    "start": "1734320",
    "end": "1741159"
  },
  {
    "text": "failures and to what everybody has said here gpus are expensive you want to maximize utilization you want to drive",
    "start": "1741159",
    "end": "1747039"
  },
  {
    "text": "mfu as high as possible so you know right now it's a lot of cobbling",
    "start": "1747039",
    "end": "1752600"
  },
  {
    "text": "together you know dcgm export or trying to get the right metrics out trying to build the logic to sort of handle",
    "start": "1752600",
    "end": "1757720"
  },
  {
    "text": "failures right trying to you know trying to sort of restart from a checkpoint trying to get your workload scheduled as",
    "start": "1757720",
    "end": "1763559"
  },
  {
    "text": "quickly as possible and so I think there's a lot of opportunity in this space I I think that's where I think",
    "start": "1763559",
    "end": "1770039"
  },
  {
    "text": "making GPU training reliable is I think one of the big things for us to solve in 2025 or 2024 I guess and this kind of",
    "start": "1770039",
    "end": "1777480"
  },
  {
    "text": "ties into uh another issue that we have more widely which is um uh res res",
    "start": "1777480",
    "end": "1783320"
  },
  {
    "text": "rescheduling is effectively uh a disruptive event I I went to a talk at reject which is the side conference",
    "start": "1783320",
    "end": "1789360"
  },
  {
    "text": "before this uh before cucon where they talked about how they were using some hack to use firecracker so that they",
    "start": "1789360",
    "end": "1794840"
  },
  {
    "text": "could move a task to a to a new node without without it necessarily realizing that it had just been stopped and",
    "start": "1794840",
    "end": "1800960"
  },
  {
    "text": "restarted that's kind of hacky right now I think in the next two years that will probably get into a much more production ready State I think that in particular",
    "start": "1800960",
    "end": "1807519"
  },
  {
    "text": "would be particularly useful for training because yes even today I have a lot of Engineers who'll come to me and be like oh you're ho I was scheduled on",
    "start": "1807519",
    "end": "1814399"
  },
  {
    "text": "a node and it was a bad host and the host died you've just lost me two days and yeah we we need a better solution to",
    "start": "1814399",
    "end": "1820679"
  },
  {
    "text": "that but I think this is going to be something where I see what I see people doing work on this especially things",
    "start": "1820679",
    "end": "1826399"
  },
  {
    "text": "like firecrack and things that is going to be used ful in the next few years I just don't think it's ready yet but in a",
    "start": "1826399",
    "end": "1831919"
  },
  {
    "text": "year or so watch that space next next year c one i' also go down a layer and",
    "start": "1831919",
    "end": "1838000"
  },
  {
    "text": "say that you know it is incredibly hard to work with most of these GPU vendors",
    "start": "1838000",
    "end": "1843720"
  },
  {
    "text": "they don't all support the same versions of Linux they don't all have the same drivers some want to force you towards",
    "start": "1843720",
    "end": "1848880"
  },
  {
    "text": "Docker some want it it is incredibly obnoxious and I would love to",
    "start": "1848880",
    "end": "1854840"
  },
  {
    "text": "see the GPU vendors put tenth of the effort into open source as I have seen",
    "start": "1854840",
    "end": "1862480"
  },
  {
    "text": "from the CPU vendors and that's why everything felt so nice with a commodity world so it's not just the exporters and",
    "start": "1862480",
    "end": "1868679"
  },
  {
    "text": "the and checks up and trying to be able to rebuild when a node goes down which absolutely has to happen even just",
    "start": "1868679",
    "end": "1874639"
  },
  {
    "text": "bringing it up where I've actually felt like I have to come back in my security posture because to get my kubernetes",
    "start": "1874639",
    "end": "1881240"
  },
  {
    "text": "cluster up with gpus in it I have to roll back to older bad versions of Linux that have known cves I would never never",
    "start": "1881240",
    "end": "1888120"
  },
  {
    "text": "do that but that is the kind of thing you sometimes have to do when working with some very large purple companies",
    "start": "1888120",
    "end": "1894720"
  },
  {
    "text": "and I find that obnoxious I just we have an obligation as an industry to work",
    "start": "1894720",
    "end": "1900279"
  },
  {
    "text": "together and just because you build your own firmware and you do your own systems and you like to sell your own thing",
    "start": "1900279",
    "end": "1906880"
  },
  {
    "text": "there is an ecosystem of Open Source people we want to work together and we expect you to have good Linux",
    "start": "1906880",
    "end": "1913120"
  },
  {
    "text": "availability we expect you to play nicely in kubernetes we expect you to actually show up I think we can take two",
    "start": "1913120",
    "end": "1918840"
  },
  {
    "text": "more questions uh go ahead and somebody else can also go to the microphones hi this is Chen so I have a quick question",
    "start": "1918840",
    "end": "1925600"
  },
  {
    "text": "about if anyone has run any true multi-tenant applications on GPU in",
    "start": "1925600",
    "end": "1931880"
  },
  {
    "text": "kubernetes before the reason why asking this is because say like you have two",
    "start": "1931880",
    "end": "1937880"
  },
  {
    "text": "customers two end users in the same kubernetes cluster both can run like",
    "start": "1937880",
    "end": "1943080"
  },
  {
    "text": "pre-training which is on trusted the code right one is able to out break out of the container and then temper Gres or",
    "start": "1943080",
    "end": "1949679"
  },
  {
    "text": "temper on the nodes and the other aspect is that you could do bean packing right",
    "start": "1949679",
    "end": "1954799"
  },
  {
    "text": "if you do bean packing on the same note currently cloud provider doesn't give you like the big machine P5 Mega in gcp",
    "start": "1954799",
    "end": "1962519"
  },
  {
    "text": "and Azure um you can't slice and dice the big machine you have to do like some",
    "start": "1962519",
    "end": "1968799"
  },
  {
    "text": "sort of gvisor or firecracker or cata whatever you have to make sure that it",
    "start": "1968799",
    "end": "1974600"
  },
  {
    "text": "is a secure multi-end environment and I know I it would be great Cloud providers like gcp could",
    "start": "1974600",
    "end": "1981279"
  },
  {
    "text": "slice and dice a bit more but I just wonder like multitenant multi tenac is",
    "start": "1981279",
    "end": "1986559"
  },
  {
    "text": "always hard problem in kubernetes I think cuso might have in oh sorry M's",
    "start": "1986559",
    "end": "1994240"
  },
  {
    "text": "not yeah um yeah I think it's still like an unsolved problem I think",
    "start": "1994799",
    "end": "2000279"
  },
  {
    "text": "the I think this is potentially going to be very interesting in like the serverless GPU space I think",
    "start": "2000279",
    "end": "2006279"
  },
  {
    "text": "that's I worked in Lambda before and there was sort of firecracker and that and that level of multitenancy and",
    "start": "2006279",
    "end": "2011320"
  },
  {
    "text": "secure multitenancy they were able to do it CPUs and one of the biggest things that we struggled with when sort of thinking about gpus as as a hardware",
    "start": "2011320",
    "end": "2018399"
  },
  {
    "text": "type was really how do we do secure multi-tenancy on it right um and you know Solutions like M um like multi",
    "start": "2018399",
    "end": "2025200"
  },
  {
    "text": "instance gpus from Nvidia weren't really didn't really meet our security bar then so think topic of open discussion some",
    "start": "2025200",
    "end": "2031880"
  },
  {
    "text": "interesting things that I've seen like um like R I don't know some folks from R",
    "start": "2031880",
    "end": "2037240"
  },
  {
    "text": "in the in the and time slicing with the gpus as a way to sort of give you fractional GPU resources I think a lot",
    "start": "2037240",
    "end": "2042360"
  },
  {
    "text": "of people are investing time and effort in that and allowing you to bin pack gpus at least within an Enterprise or",
    "start": "2042360",
    "end": "2048158"
  },
  {
    "text": "like within a given organization and doing that well I think I've seen progress there um through sort of multi1",
    "start": "2048159",
    "end": "2054118"
  },
  {
    "text": "in GP utilization at least I think still an unknown to get that into production I don't know if anybody's actually doing",
    "start": "2054119",
    "end": "2060200"
  },
  {
    "text": "that um I don't know if anybody else has a take on it there is C previous answer there are very proprietary options that",
    "start": "2060200",
    "end": "2067158"
  },
  {
    "text": "exist to do multi-tenancy on a specific GPU and that is absolutely not something",
    "start": "2067159",
    "end": "2073158"
  },
  {
    "text": "that a lot of the cloud providers want to pay for to that particular Hardware vendor to then expose to you as an end",
    "start": "2073159",
    "end": "2079520"
  },
  {
    "text": "user so time slicing is what I would have recommended it's the only sort of easy way to go across the previous older",
    "start": "2079520",
    "end": "2085800"
  },
  {
    "text": "gpus you could use grid Technologies and that was exposed in a more open fashion um but there's a very different business",
    "start": "2085800",
    "end": "2092560"
  },
  {
    "text": "model going on today we'll take one more question there's Challenge on like time",
    "start": "2092560",
    "end": "2097760"
  },
  {
    "text": "oh anyways maybe we can talk about this offline but I'm actually working on like 10y on you know gpus so this is very",
    "start": "2097760",
    "end": "2105880"
  },
  {
    "text": "specific like question from me we we'll take one more question before they pull us out of here go ahead",
    "start": "2105880",
    "end": "2113400"
  },
  {
    "text": "really really quickly uh everyone talks about the you know scarcity of Hardware scarcity of money all that sort of stuff but what about power uh I manage on Prem",
    "start": "2113400",
    "end": "2120359"
  },
  {
    "text": "data centers and that's my biggest problem yeah we uh I I know that we do have challenges",
    "start": "2120359",
    "end": "2127160"
  },
  {
    "text": "here and there is a lot of work on this however luckily for me it is kept far away from me and I don't have to worry",
    "start": "2127160",
    "end": "2132920"
  },
  {
    "text": "about it I don't know if you folks on Prem structured human yeah I mean again I would so the",
    "start": "2132920",
    "end": "2140640"
  },
  {
    "text": "power of an electron is always going to be a value to the world I don't know you",
    "start": "2140640",
    "end": "2145800"
  },
  {
    "text": "know I I won't get poetic um so unfortunately I think what you're seeing",
    "start": "2145800",
    "end": "2151200"
  },
  {
    "text": "maybe from an infrastructure and overall finops lens for the way we're managing this is we're looking for significant",
    "start": "2151200",
    "end": "2158480"
  },
  {
    "text": "increases in efficiency where we can and recognizing that there's this large space heater that we can't get rid of",
    "start": "2158480",
    "end": "2165480"
  },
  {
    "text": "that is very critical to the business and so we work to increase the utilization of it we work to make sure",
    "start": "2165480",
    "end": "2171640"
  },
  {
    "text": "we have excellent methodologies across the sort of feature training solution that we're looking for monitoring of",
    "start": "2171640",
    "end": "2178240"
  },
  {
    "text": "drift across and only retraining when we have to like we're trying to optimize from the software lens and also from",
    "start": "2178240",
    "end": "2186119"
  },
  {
    "text": "every other part of the back both to free up the capital dollars to invest in",
    "start": "2186119",
    "end": "2191560"
  },
  {
    "text": "this domain but also to free up the electrons to invest in this domain I don't want to go out and you know",
    "start": "2191560",
    "end": "2197200"
  },
  {
    "text": "license another 15 megawatts of data centers that's not my goal or 25 or you know I mean you can't have a very large",
    "start": "2197200",
    "end": "2204040"
  },
  {
    "text": "array with that so you really have to think it through and look at how you can balance I think the other thing is",
    "start": "2204040",
    "end": "2210160"
  },
  {
    "text": "people look at the space heers that are training systems and think everyone's",
    "start": "2210160",
    "end": "2215200"
  },
  {
    "text": "going to train and free train I don't know who you are I love you all I'd love to have a conversation we are looking at",
    "start": "2215200",
    "end": "2222880"
  },
  {
    "text": "leveraging models but training them on our own data and that is a much smaller compute problem and so as people are",
    "start": "2222880",
    "end": "2230480"
  },
  {
    "text": "looking at sort of the worst case scenarios I would also just urge folks to Think Through there's usually a much",
    "start": "2230480",
    "end": "2238040"
  },
  {
    "text": "better way of doing it it's just you do actually have to call your infrastructure person like you have to work across and we need to be working",
    "start": "2238040",
    "end": "2244280"
  },
  {
    "text": "from the software and the bottom up",
    "start": "2244280",
    "end": "2248720"
  }
]