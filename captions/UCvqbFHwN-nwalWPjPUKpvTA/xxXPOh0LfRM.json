[
  {
    "text": "all right welcome everyone um and thanks",
    "start": "240",
    "end": "2120"
  },
  {
    "text": "for coming to my talk um this is a",
    "start": "2120",
    "end": "4400"
  },
  {
    "text": "lightning talk I originally presented or",
    "start": "4400",
    "end": "6520"
  },
  {
    "text": "submitted it as a full length talk so I",
    "start": "6520",
    "end": "8639"
  },
  {
    "text": "try to shrink it down um not sure how",
    "start": "8639",
    "end": "11080"
  },
  {
    "text": "successful so let me start with a",
    "start": "11080",
    "end": "13000"
  },
  {
    "text": "clarification I call it this but it",
    "start": "13000",
    "end": "15280"
  },
  {
    "text": "should really be called this because um",
    "start": "15280",
    "end": "18119"
  },
  {
    "text": "at the time we submitted it generative",
    "start": "18119",
    "end": "20160"
  },
  {
    "text": "AI really wasn't there wasn't much video",
    "start": "20160",
    "end": "22240"
  },
  {
    "text": "generation and you know picture",
    "start": "22240",
    "end": "24199"
  },
  {
    "text": "generation yet it was all about you know",
    "start": "24199",
    "end": "26000"
  },
  {
    "text": "text and text completion and and chat",
    "start": "26000",
    "end": "28160"
  },
  {
    "text": "Bots and things like that so I apologize",
    "start": "28160",
    "end": "30199"
  },
  {
    "text": "for any kind of false hopes you might",
    "start": "30199",
    "end": "32599"
  },
  {
    "text": "have had so but let's talk about um what",
    "start": "32599",
    "end": "35559"
  },
  {
    "text": "we can do um with large language models",
    "start": "35559",
    "end": "37840"
  },
  {
    "text": "and how to serve them um at scale so my",
    "start": "37840",
    "end": "41520"
  },
  {
    "text": "name is Christian cner I work for IBM um",
    "start": "41520",
    "end": "44120"
  },
  {
    "text": "I've been a software developer in the",
    "start": "44120",
    "end": "46360"
  },
  {
    "text": "field of um uh machine learning um",
    "start": "46360",
    "end": "49399"
  },
  {
    "text": "working on open source projects um in",
    "start": "49399",
    "end": "51800"
  },
  {
    "text": "Cube flow specifically um K Cube",
    "start": "51800",
    "end": "54600"
  },
  {
    "text": "pipelines on tecton um and",
    "start": "54600",
    "end": "57280"
  },
  {
    "text": "kerf and most recently I've worked on on",
    "start": "57280",
    "end": "60440"
  },
  {
    "text": "projects like kit and Tes which I plan",
    "start": "60440",
    "end": "62920"
  },
  {
    "text": "to introduce in the next few",
    "start": "62920",
    "end": "65040"
  },
  {
    "text": "minutes let's start with some of the",
    "start": "65040",
    "end": "66960"
  },
  {
    "text": "challenges um that that come with",
    "start": "66960",
    "end": "68960"
  },
  {
    "text": "serving large language models um that",
    "start": "68960",
    "end": "71520"
  },
  {
    "text": "are quite unique compared to the",
    "start": "71520",
    "end": "73240"
  },
  {
    "text": "previous generation of models that that",
    "start": "73240",
    "end": "75360"
  },
  {
    "text": "were serving with queso for example so",
    "start": "75360",
    "end": "77240"
  },
  {
    "text": "large language models as the name",
    "start": "77240",
    "end": "78560"
  },
  {
    "text": "applies are huge um and usually they",
    "start": "78560",
    "end": "81479"
  },
  {
    "text": "require gpus um and often times these",
    "start": "81479",
    "end": "84799"
  },
  {
    "text": "models are so big that they can't fit on",
    "start": "84799",
    "end": "86479"
  },
  {
    "text": "a single GPU um specifically exceeding",
    "start": "86479",
    "end": "88439"
  },
  {
    "text": "the GPU memory and and then the very",
    "start": "88439",
    "end": "90640"
  },
  {
    "text": "nature of them as the the the iterative",
    "start": "90640",
    "end": "93479"
  },
  {
    "text": "autoagressive decoding mechanism um",
    "start": "93479",
    "end": "95680"
  },
  {
    "text": "really doesn't work well with",
    "start": "95680",
    "end": "97119"
  },
  {
    "text": "parallelization you know which is the",
    "start": "97119",
    "end": "99000"
  },
  {
    "text": "main benefit of using gpos then of",
    "start": "99000",
    "end": "101000"
  },
  {
    "text": "course you have variable length input",
    "start": "101000",
    "end": "102600"
  },
  {
    "text": "and then you have to manage the whole",
    "start": "102600",
    "end": "103680"
  },
  {
    "text": "state of the the context and I'll link a",
    "start": "103680",
    "end": "105759"
  },
  {
    "text": "very good paper on some of those",
    "start": "105759",
    "end": "107320"
  },
  {
    "text": "challenges at the bottom",
    "start": "107320",
    "end": "109399"
  },
  {
    "text": "here okay let's quickly give an overview",
    "start": "109399",
    "end": "112280"
  },
  {
    "text": "over um kerf kerve started as cube flow",
    "start": "112280",
    "end": "114759"
  },
  {
    "text": "serving and it's really the the",
    "start": "114759",
    "end": "117399"
  },
  {
    "text": "predominant serving solution um for AI",
    "start": "117399",
    "end": "119640"
  },
  {
    "text": "model on kubernetes and it used to be",
    "start": "119640",
    "end": "121880"
  },
  {
    "text": "part of the um Cube flow um um package",
    "start": "121880",
    "end": "126439"
  },
  {
    "text": "the bundle when you downloaded cflow um",
    "start": "126439",
    "end": "129039"
  },
  {
    "text": "KF server in ker was part of it has a",
    "start": "129039",
    "end": "131680"
  },
  {
    "text": "very active um and very diverse um open",
    "start": "131680",
    "end": "134480"
  },
  {
    "text": "source Community with contributions from",
    "start": "134480",
    "end": "136800"
  },
  {
    "text": "you know big companies like Bloomberg",
    "start": "136800",
    "end": "138480"
  },
  {
    "text": "Google IBM Nvidia Seldon and many more",
    "start": "138480",
    "end": "141280"
  },
  {
    "text": "and um it writes itself as being",
    "start": "141280",
    "end": "143319"
  },
  {
    "text": "scalable in performance and it uses um",
    "start": "143319",
    "end": "145360"
  },
  {
    "text": "conative and ISO um to do many many of",
    "start": "145360",
    "end": "148040"
  },
  {
    "text": "the things it can do and then of course",
    "start": "148040",
    "end": "149680"
  },
  {
    "text": "you have you know things you would",
    "start": "149680",
    "end": "150800"
  },
  {
    "text": "expect like pre-processing",
    "start": "150800",
    "end": "152400"
  },
  {
    "text": "postprocessing um Monitor and",
    "start": "152400",
    "end": "154239"
  },
  {
    "text": "expendability or caner roll outs for",
    "start": "154239",
    "end": "157319"
  },
  {
    "text": "example um ker supports most of the most",
    "start": "157319",
    "end": "161120"
  },
  {
    "text": "popular um machine learning Frameworks",
    "start": "161120",
    "end": "163640"
  },
  {
    "text": "and it does that by providing um serving",
    "start": "163640",
    "end": "167200"
  },
  {
    "text": "runtimes that are kind of like",
    "start": "167200",
    "end": "169159"
  },
  {
    "text": "preconfigured that you can use to serve",
    "start": "169159",
    "end": "171239"
  },
  {
    "text": "your models and all you need is you know",
    "start": "171239",
    "end": "172800"
  },
  {
    "text": "a few lines of yaml code or a few lines",
    "start": "172800",
    "end": "175120"
  },
  {
    "text": "of python code if you use the kerve",
    "start": "175120",
    "end": "178120"
  },
  {
    "text": "SDK now most recent recently L um new",
    "start": "178120",
    "end": "181200"
  },
  {
    "text": "features that came into kerf for llm",
    "start": "181200",
    "end": "184200"
  },
  {
    "text": "support um are the new torch torch serve",
    "start": "184200",
    "end": "188280"
  },
  {
    "text": "um v08 runtime um that supports you know",
    "start": "188280",
    "end": "191400"
  },
  {
    "text": "large language models that can fit on a",
    "start": "191400",
    "end": "193159"
  },
  {
    "text": "single GPU using hugging face accelerate",
    "start": "193159",
    "end": "195920"
  },
  {
    "text": "um and deep speed to split your model",
    "start": "195920",
    "end": "197959"
  },
  {
    "text": "over several gpus called charting and",
    "start": "197959",
    "end": "200440"
  },
  {
    "text": "then there's a dedicated um BLM runtime",
    "start": "200440",
    "end": "203440"
  },
  {
    "text": "um and I'll go into more details about",
    "start": "203440",
    "end": "205480"
  },
  {
    "text": "what the Nifty features are there but",
    "start": "205480",
    "end": "207280"
  },
  {
    "text": "they they really have a very high frut",
    "start": "207280",
    "end": "209519"
  },
  {
    "text": "using that and in the little diagram",
    "start": "209519",
    "end": "212080"
  },
  {
    "text": "there you can see that uh the throughput",
    "start": "212080",
    "end": "214640"
  },
  {
    "text": "using continuous badging page attention",
    "start": "214640",
    "end": "216799"
  },
  {
    "text": "is really",
    "start": "216799",
    "end": "219040"
  },
  {
    "text": "exceptional all right but let me go back",
    "start": "219040",
    "end": "221560"
  },
  {
    "text": "a step and and talk about the text",
    "start": "221560",
    "end": "223959"
  },
  {
    "text": "generation inference server this is a um",
    "start": "223959",
    "end": "226439"
  },
  {
    "text": "a project that was released by hugging",
    "start": "226439",
    "end": "228400"
  },
  {
    "text": "face and hugging face is using that um",
    "start": "228400",
    "end": "230640"
  },
  {
    "text": "to serve their large language models um",
    "start": "230640",
    "end": "233280"
  },
  {
    "text": "and I think most recently um they're",
    "start": "233280",
    "end": "235000"
  },
  {
    "text": "using that to for their hugging chat and",
    "start": "235000",
    "end": "238640"
  },
  {
    "text": "so there are several Innovative features",
    "start": "238640",
    "end": "240840"
  },
  {
    "text": "that that came into this here um",
    "start": "240840",
    "end": "243280"
  },
  {
    "text": "including flash detention uh flashh",
    "start": "243280",
    "end": "245239"
  },
  {
    "text": "detention intens",
    "start": "245239",
    "end": "246840"
  },
  {
    "text": "parallelism and and Page detention um",
    "start": "246840",
    "end": "250159"
  },
  {
    "text": "I'm running low on time so I'm going to",
    "start": "250159",
    "end": "251480"
  },
  {
    "text": "skip through the next one um the problem",
    "start": "251480",
    "end": "254840"
  },
  {
    "text": "with um TGI was is that um IBM last year",
    "start": "254840",
    "end": "257919"
  },
  {
    "text": "had a partnership with hugging face and",
    "start": "257919",
    "end": "259519"
  },
  {
    "text": "then a few months later um they flipped",
    "start": "259519",
    "end": "261479"
  },
  {
    "text": "the license on us and so the work that",
    "start": "261479",
    "end": "263240"
  },
  {
    "text": "we've done to contribute to TG and the",
    "start": "263240",
    "end": "265479"
  },
  {
    "text": "the work we needed to do to consume it",
    "start": "265479",
    "end": "267280"
  },
  {
    "text": "internally now you know was in an empty",
    "start": "267280",
    "end": "269720"
  },
  {
    "text": "footing and so we were able to continue",
    "start": "269720",
    "end": "272400"
  },
  {
    "text": "the work on our fork and used the old",
    "start": "272400",
    "end": "275039"
  },
  {
    "text": "Apache 2.0 license that was available",
    "start": "275039",
    "end": "277680"
  },
  {
    "text": "until",
    "start": "277680",
    "end": "279520"
  },
  {
    "text": "whoops until you know before version 01",
    "start": "279520",
    "end": "282919"
  },
  {
    "text": "and then we added a few optimizations",
    "start": "282919",
    "end": "284960"
  },
  {
    "text": "that that you know work for us in",
    "start": "284960",
    "end": "287479"
  },
  {
    "text": "production and but it's basically uh you",
    "start": "287479",
    "end": "291039"
  },
  {
    "text": "know very similar adapation there's a",
    "start": "291039",
    "end": "292440"
  },
  {
    "text": "great talk by Nick Hill uh who is our",
    "start": "292440",
    "end": "295039"
  },
  {
    "text": "technical Elite on the project and that",
    "start": "295039",
    "end": "297960"
  },
  {
    "text": "talk really explains all of the optim Iz",
    "start": "297960",
    "end": "299919"
  },
  {
    "text": "ations and why what was done what what",
    "start": "299919",
    "end": "302120"
  },
  {
    "text": "the page attention uh algorithm does and",
    "start": "302120",
    "end": "304759"
  },
  {
    "text": "and the continues badging It's really",
    "start": "304759",
    "end": "306759"
  },
  {
    "text": "informative highly",
    "start": "306759",
    "end": "309199"
  },
  {
    "text": "recommended um now kerve and and uh TGI",
    "start": "309199",
    "end": "313680"
  },
  {
    "text": "or VM they are great for serving models",
    "start": "313680",
    "end": "315479"
  },
  {
    "text": "but as a developer um it's still hard to",
    "start": "315479",
    "end": "318000"
  },
  {
    "text": "really you know integrate um those large",
    "start": "318000",
    "end": "321319"
  },
  {
    "text": "language models in your application",
    "start": "321319",
    "end": "323000"
  },
  {
    "text": "because most developers don't have the",
    "start": "323000",
    "end": "324800"
  },
  {
    "text": "background um and they don't want to",
    "start": "324800",
    "end": "326520"
  },
  {
    "text": "work with tensors as input and outputs",
    "start": "326520",
    "end": "328759"
  },
  {
    "text": "and so we created that project um kkit",
    "start": "328759",
    "end": "331680"
  },
  {
    "text": "which allows gives a layer of",
    "start": "331680",
    "end": "333840"
  },
  {
    "text": "abstraction so developers can focus on",
    "start": "333840",
    "end": "336039"
  },
  {
    "text": "the actual the model inputs and outputs",
    "start": "336039",
    "end": "338319"
  },
  {
    "text": "they want to work with which for large",
    "start": "338319",
    "end": "339840"
  },
  {
    "text": "language models is text um and then uh",
    "start": "339840",
    "end": "343919"
  },
  {
    "text": "the lay of extraction also allows to",
    "start": "343919",
    "end": "345880"
  },
  {
    "text": "swap out models and and then you know to",
    "start": "345880",
    "end": "348720"
  },
  {
    "text": "develop your application independently",
    "start": "348720",
    "end": "350360"
  },
  {
    "text": "of different versions of the models um",
    "start": "350360",
    "end": "353440"
  },
  {
    "text": "and hunging phase has a few Concepts",
    "start": "353440",
    "end": "355120"
  },
  {
    "text": "that make that even easier U with",
    "start": "355120",
    "end": "357280"
  },
  {
    "text": "modules and and data models and so that",
    "start": "357280",
    "end": "360160"
  },
  {
    "text": "you can you know switch out your",
    "start": "360160",
    "end": "362479"
  },
  {
    "text": "runtimes for training you have different",
    "start": "362479",
    "end": "364319"
  },
  {
    "text": "runtimes for inferencing like you see",
    "start": "364319",
    "end": "366120"
  },
  {
    "text": "here teaches is one inference runtime",
    "start": "366120",
    "end": "368319"
  },
  {
    "text": "and then the actual data model focuses",
    "start": "368319",
    "end": "370599"
  },
  {
    "text": "on a task you want to achieve like you",
    "start": "370599",
    "end": "372440"
  },
  {
    "text": "know some kind of natural language",
    "start": "372440",
    "end": "374440"
  },
  {
    "text": "processing image generation whatever",
    "start": "374440",
    "end": "376199"
  },
  {
    "text": "your task is you can then swap out you",
    "start": "376199",
    "end": "378440"
  },
  {
    "text": "know similar or or equivalent um",
    "start": "378440",
    "end": "381360"
  },
  {
    "text": "functionality underneath without",
    "start": "381360",
    "end": "382639"
  },
  {
    "text": "changing your application",
    "start": "382639",
    "end": "384240"
  },
  {
    "text": "code um one of the creators or two of",
    "start": "384240",
    "end": "386960"
  },
  {
    "text": "the creators um of K are actually here",
    "start": "386960",
    "end": "388720"
  },
  {
    "text": "in the room so if you have questions you",
    "start": "388720",
    "end": "390319"
  },
  {
    "text": "come and find us and they have a great",
    "start": "390319",
    "end": "392160"
  },
  {
    "text": "demo um I call it Kai cat demo because",
    "start": "392160",
    "end": "394440"
  },
  {
    "text": "it's always fun to work with cat",
    "start": "394440",
    "end": "395759"
  },
  {
    "text": "pictures um and one of the demos is is",
    "start": "395759",
    "end": "398560"
  },
  {
    "text": "um analyzing contents you contents of a",
    "start": "398560",
    "end": "400919"
  },
  {
    "text": "picture and it works really great so go",
    "start": "400919",
    "end": "402319"
  },
  {
    "text": "check that out all right so how do we",
    "start": "402319",
    "end": "404639"
  },
  {
    "text": "put this all together um so with redhead",
    "start": "404639",
    "end": "406880"
  },
  {
    "text": "open shift AI U which is used by IBM",
    "start": "406880",
    "end": "409800"
  },
  {
    "text": "Watson X EI um all of those Technologies",
    "start": "409800",
    "end": "412720"
  },
  {
    "text": "are you know playing their part we're",
    "start": "412720",
    "end": "414319"
  },
  {
    "text": "using kerf um to handle the actual model",
    "start": "414319",
    "end": "417240"
  },
  {
    "text": "deployment uh tgis is our serving",
    "start": "417240",
    "end": "420000"
  },
  {
    "text": "backend an inference engine engine and",
    "start": "420000",
    "end": "422479"
  },
  {
    "text": "kkit is the toolkit um that allows us to",
    "start": "422479",
    "end": "424879"
  },
  {
    "text": "to handle the life cycle of the TT",
    "start": "424879",
    "end": "427360"
  },
  {
    "text": "process and gives us the inference end",
    "start": "427360",
    "end": "429560"
  },
  {
    "text": "point and it's all built on top of ketes",
    "start": "429560",
    "end": "431800"
  },
  {
    "text": "uh in redhead open shift so we get all",
    "start": "431800",
    "end": "434080"
  },
  {
    "text": "the scalability and performance that",
    "start": "434080",
    "end": "435599"
  },
  {
    "text": "come with uh with conative and ISO and",
    "start": "435599",
    "end": "438440"
  },
  {
    "text": "and the other Technologies",
    "start": "438440",
    "end": "440400"
  },
  {
    "text": "bundled and there's a great block post",
    "start": "440400",
    "end": "442800"
  },
  {
    "text": "here um that are linked from rad that",
    "start": "442800",
    "end": "445160"
  },
  {
    "text": "explain how does how all of these",
    "start": "445160",
    "end": "446520"
  },
  {
    "text": "Technologies work together and create a",
    "start": "446520",
    "end": "447879"
  },
  {
    "text": "seamless user experience",
    "start": "447879",
    "end": "450520"
  },
  {
    "text": "all right I'm way over time um so I",
    "start": "450520",
    "end": "452960"
  },
  {
    "text": "thank you for attending this talk and if",
    "start": "452960",
    "end": "454680"
  },
  {
    "text": "you have any questions go come find us",
    "start": "454680",
    "end": "456759"
  },
  {
    "text": "there's a bunch of us developers sitting",
    "start": "456759",
    "end": "458160"
  },
  {
    "text": "in the second row here happy to answer",
    "start": "458160",
    "end": "460080"
  },
  {
    "text": "questions thank",
    "start": "460080",
    "end": "463240"
  },
  {
    "text": "you",
    "start": "463479",
    "end": "466479"
  }
]