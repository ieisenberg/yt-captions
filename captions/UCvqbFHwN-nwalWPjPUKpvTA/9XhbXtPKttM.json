[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "hello everyone welcome to virtual amsterdam and is there a place for distributed",
    "start": "80",
    "end": "5920"
  },
  {
    "text": "storage in aiml on kubernetes i'm diane fedema i'm a principal software engineer at red",
    "start": "5920",
    "end": "12559"
  },
  {
    "text": "hat and today i'm speaking with kyle bader who is a principle software architect for cloud",
    "start": "12559",
    "end": "18240"
  },
  {
    "text": "storage and data services at red hat so next slide please",
    "start": "18240",
    "end": "25199"
  },
  {
    "text": "we want to know is there a place for distributed storage for aiml workloads on kubernetes",
    "start": "27279",
    "end": "34800"
  },
  {
    "text": "what do you think i would say yes absolutely um and why",
    "start": "34800",
    "end": "42160"
  },
  {
    "start": "35000",
    "end": "445000"
  },
  {
    "text": "why why were we interested in in doing this um well there was you know kind of kind of",
    "start": "42160",
    "end": "47680"
  },
  {
    "text": "stemmed from a number of conversations that that we had internally as we were you",
    "start": "47680",
    "end": "53039"
  },
  {
    "text": "know thinking about uh distributed storage and uh the implications of",
    "start": "53039",
    "end": "58640"
  },
  {
    "text": "machine learning and um kind of boils down to um you know many of you probably have an",
    "start": "58640",
    "end": "65840"
  },
  {
    "text": "environment uh well if you're lucky have an environment it looks maybe something kind of like this right",
    "start": "65840",
    "end": "71119"
  },
  {
    "text": "and you know kubernetes is awesome so you've chosen to to use kubernetes to to manage this environment and um",
    "start": "71119",
    "end": "78960"
  },
  {
    "text": "uh you know most the times there will be you know some some number of general general compute",
    "start": "78960",
    "end": "85439"
  },
  {
    "text": "general purpose compute nodes that are you know running your applications in your storage and so on and so forth",
    "start": "85439",
    "end": "91360"
  },
  {
    "text": "and then the the lucky part is if you are are able to have some special purpose",
    "start": "91360",
    "end": "96880"
  },
  {
    "text": "compute with uh additional gpus in order to kind of",
    "start": "96880",
    "end": "101920"
  },
  {
    "text": "accelerate your machine learning workloads so that you can um you know finish you know both training",
    "start": "101920",
    "end": "109200"
  },
  {
    "text": "jobs and inference jobs a little bit more expeditiously and collectively um",
    "start": "109200",
    "end": "114399"
  },
  {
    "text": "you know this this kubernetes cluster that we're looking at here uh represents a fairly significant capital investment",
    "start": "114399",
    "end": "120079"
  },
  {
    "text": "so um i want to make sure that that that it's used uh as as efficiently as possible right",
    "start": "120079",
    "end": "126159"
  },
  {
    "text": "we want to maximize our utility from this from this investment so kubernetes to",
    "start": "126159",
    "end": "131440"
  },
  {
    "text": "the rescue right um you know at a very simple level right kubernetes is a",
    "start": "131440",
    "end": "136959"
  },
  {
    "text": "scheduler it does a lot of other things in addition to scheduling but um one of the things that it's it's here",
    "start": "136959",
    "end": "142160"
  },
  {
    "text": "to help you with is is packing bins um you can think of of nodes as these bins and then",
    "start": "142160",
    "end": "148080"
  },
  {
    "text": "instead of having you know dimensions like length and height and depth those nodes have",
    "start": "148080",
    "end": "154080"
  },
  {
    "text": "dimensions like amount of cpu amount of memory how much storage space do they have on their",
    "start": "154080",
    "end": "159120"
  },
  {
    "text": "local storage devices how much i o can those storage devices handle",
    "start": "159120",
    "end": "164640"
  },
  {
    "text": "and we want to we want to effectively fill fill these bins with pause and we want to fill them",
    "start": "164640",
    "end": "171519"
  },
  {
    "text": "as full as possible without them kind of impacting each other in order to to you know maximize the",
    "start": "171519",
    "end": "178560"
  },
  {
    "text": "utility of that cluster so we schedule a pod and that pod is",
    "start": "178560",
    "end": "184319"
  },
  {
    "text": "going to consume some amount of cpus some memory uh you know it will generate some amount of",
    "start": "184319",
    "end": "189519"
  },
  {
    "text": "storage i o and consume some store some amount of storage space and",
    "start": "189519",
    "end": "195360"
  },
  {
    "text": "you know if you if you if you draw this up on onto this kind of you know multi-dimensional object here",
    "start": "195360",
    "end": "201519"
  },
  {
    "text": "you get a shape and so each pod kind of has its own shape and um you know with all these different",
    "start": "201519",
    "end": "207920"
  },
  {
    "text": "shapes of pods you want to try to you know fill up fill up the uh each of these",
    "start": "207920",
    "end": "213040"
  },
  {
    "text": "dimensions as full as possible so as you add another pod right represented here by this",
    "start": "213040",
    "end": "218400"
  },
  {
    "text": "kind of dotted purple line um we want to um we see that it's it's taking up you",
    "start": "218400",
    "end": "224720"
  },
  {
    "text": "know some an additional amount of cpu uh uh additional amount of memory you know it's going to consume some more",
    "start": "224720",
    "end": "230480"
  },
  {
    "text": "storage i o and take up some more storage space now some of you might have noticed a problem here um",
    "start": "230480",
    "end": "236879"
  },
  {
    "text": "and and i'll kind of point it out um is that you know we we've kind of exhausted the amount of",
    "start": "236879",
    "end": "242480"
  },
  {
    "text": "cpu resources here and what does that mean why is that why is that bad right we want to maximize",
    "start": "242480",
    "end": "248239"
  },
  {
    "text": "usage of it and and so we want to use the ball to cpu don't we well yes we do but we don't want to trap",
    "start": "248239",
    "end": "255439"
  },
  {
    "text": "we don't want to trap the the the other resources um basically we don't have it want to have this like stranded capacity in",
    "start": "255439",
    "end": "261759"
  },
  {
    "text": "terms of memory and unused storage space and unused storage io we want to use those we want",
    "start": "261759",
    "end": "267840"
  },
  {
    "text": "to use those efficiently as possible too so one of the problems that uh you know you want to solve",
    "start": "267840",
    "end": "273520"
  },
  {
    "text": "with the scheduler is to avoid trapped capacity one way you can help avoid trap capacity",
    "start": "273520",
    "end": "280720"
  },
  {
    "text": "is by having fewer dimensions that are constraints um and so this is this is where you know",
    "start": "280720",
    "end": "286400"
  },
  {
    "text": "distributed storage gets kind of interesting because you can kind of abstract away storage",
    "start": "286400",
    "end": "291600"
  },
  {
    "text": "and and then there are uh because it's being remotely accessed",
    "start": "291600",
    "end": "296720"
  },
  {
    "text": "um the the the pods have uh ability to be scheduled in more",
    "start": "296720",
    "end": "301919"
  },
  {
    "text": "places and there's less constraints on scheduling them um such that um it's easier to to to fill your bins",
    "start": "301919",
    "end": "309520"
  },
  {
    "text": "maximally and uh in turn maximize the utility of that that significant capital investment",
    "start": "309520",
    "end": "317520"
  },
  {
    "text": "another you know time when distributed storage comes into the picture is when you need more storage space uh for a",
    "start": "318080",
    "end": "324960"
  },
  {
    "text": "particular application then then then you have on any given node in your cluster right so if you",
    "start": "324960",
    "end": "330000"
  },
  {
    "text": "if you if you need to uh have or if you have data that's larger than a node then then you're going to have to spread",
    "start": "330000",
    "end": "336320"
  },
  {
    "text": "that data over multiple nodes and uh guess what distributed storage helps helps you do that right so by",
    "start": "336320",
    "end": "342400"
  },
  {
    "text": "kind of virtualizing the storage um uh kind of collecting a bunch of disks together and then using various",
    "start": "342400",
    "end": "348560"
  },
  {
    "text": "you know software-defined storage uh methodologies um you know it can it can kind of create",
    "start": "348560",
    "end": "354560"
  },
  {
    "text": "this uh either virtualized block or file system storage or object storage and um then you can have you know larger",
    "start": "354560",
    "end": "362560"
  },
  {
    "text": "data sets that pause interacting with um than than what you could get from any individual node",
    "start": "362560",
    "end": "369919"
  },
  {
    "text": "uh the other problem that might present itself is is okay you you have available compute right and and you uh want to",
    "start": "370960",
    "end": "378880"
  },
  {
    "text": "want to use it up but the data already exists on another pod right because you're using local storage somewhere",
    "start": "378880",
    "end": "385840"
  },
  {
    "text": "and your data is over here so now you're kind of you know sure you could have something that's going to you know make that that data",
    "start": "385840",
    "end": "393120"
  },
  {
    "text": "over over on the the right host be remotely accessible right maybe ice goes to your nfs or",
    "start": "393120",
    "end": "399600"
  },
  {
    "text": "something to the to the node or on the left but you know that running that that nfs or is gateway",
    "start": "399600",
    "end": "404880"
  },
  {
    "text": "is going to consume some amount right um and so you can't kind of ad hoc provision it",
    "start": "404880",
    "end": "410160"
  },
  {
    "text": "um and it would really just be better if if we just kind of had a distributed storage system that was in place and and and gave you more flexibility in",
    "start": "410160",
    "end": "417919"
  },
  {
    "text": "term or gave kubernetes more flexibility in terms of scheduling um so this is uh and this is not you",
    "start": "417919",
    "end": "424479"
  },
  {
    "text": "know none of this is unique to kubernetes or uh machine learning workloads",
    "start": "424479",
    "end": "431120"
  },
  {
    "text": "but but it does apply uh equally to machine work learning workloads um and so",
    "start": "431199",
    "end": "436400"
  },
  {
    "text": "it was kind of in in this thinking this this context that we we started to ask the questions about",
    "start": "436400",
    "end": "443280"
  },
  {
    "text": "distributed storage and machine learning so very simplistic",
    "start": "443280",
    "end": "448560"
  },
  {
    "start": "445000",
    "end": "780000"
  },
  {
    "text": "right uh kind of truncated version of the uh machine learning life cycle right now",
    "start": "448560",
    "end": "454560"
  },
  {
    "text": "of course there's gonna be something on the left that's that's you know the applications um or sensors or whatever that's that's",
    "start": "454560",
    "end": "460720"
  },
  {
    "text": "generating the data and then there's there's more on the right like is you know training is not the end all be all",
    "start": "460720",
    "end": "466639"
  },
  {
    "text": "right you're gonna have to take take that model and you know wire it into an intelligent application and do",
    "start": "466639",
    "end": "472160"
  },
  {
    "text": "something useful with it uh you know shopping cart uh suggested products or",
    "start": "472160",
    "end": "477759"
  },
  {
    "text": "you know detecting fraud or so on and so forth but for the purposes of of this discussion",
    "start": "477759",
    "end": "483440"
  },
  {
    "text": "here we've kind of simplified it to okay so you know there's there's to the the time to value",
    "start": "483440",
    "end": "489680"
  },
  {
    "text": "is is something that we want to shrink and um some of the the things that need",
    "start": "489680",
    "end": "495599"
  },
  {
    "text": "to be performed um before we can realize value uh from from that data is you know we're",
    "start": "495599",
    "end": "502080"
  },
  {
    "text": "probably gonna have to prepare it wrangle it together uh in some sort of way and then without distributed storage",
    "start": "502080",
    "end": "508240"
  },
  {
    "text": "we're gonna have to move the data so if we you know if it's going to take a really long time to run it on the",
    "start": "508240",
    "end": "513760"
  },
  {
    "text": "general purpose compute and it was processed on the general purpose compute then we're gonna have to move it to the special purpose compute in order to",
    "start": "513760",
    "end": "520719"
  },
  {
    "text": "you know finish it in any sort of reasonable time frame and then finally once the data is there",
    "start": "520719",
    "end": "525839"
  },
  {
    "text": "then we can actually start training um uh the model against that data",
    "start": "525839",
    "end": "531279"
  },
  {
    "text": "and then once it's once it's completed or we're kind of on to the next next phases of of the life cycle so",
    "start": "531279",
    "end": "539360"
  },
  {
    "text": "um with distributed storage can we can we compress this can we can we compress and have uh get to that value faster can",
    "start": "539360",
    "end": "546560"
  },
  {
    "text": "we reduce the time that it takes to get to that value and if you start if you implement kind",
    "start": "546560",
    "end": "554080"
  },
  {
    "text": "of some sort of distributed storage uh you could potentially kind of compress the the data moving",
    "start": "554080",
    "end": "560320"
  },
  {
    "text": "and model training into uh into the same kind of into the same",
    "start": "560320",
    "end": "565360"
  },
  {
    "text": "space and and and and perhaps perhaps that does mean that it that it inflates the time that the actual model training takes",
    "start": "565360",
    "end": "572160"
  },
  {
    "text": "but you you know you you potentially save yourself that data moving st uh stage now uh of course",
    "start": "572160",
    "end": "579360"
  },
  {
    "text": "an astute reader might go oh okay um well if the data is not remote and the",
    "start": "579360",
    "end": "584800"
  },
  {
    "text": "gpu node is accessing over to the network isn't it kind of like the data is moving and the answer is yes right so",
    "start": "584800",
    "end": "590640"
  },
  {
    "text": "it's going to be kind of over the course of the model training it's kind of incrementally or either uh",
    "start": "590640",
    "end": "596640"
  },
  {
    "text": "either incrementally over the course of the whole training it's grabbing some of the data it needs to keep the keep the gpus busy or it kind of bulk",
    "start": "596640",
    "end": "602959"
  },
  {
    "text": "loads it in the beginning and then um and then and then processes it from there",
    "start": "602959",
    "end": "608399"
  },
  {
    "text": "right so um so yeah this is this is this is what we want right we want to we want to see if we can compress that",
    "start": "608399",
    "end": "614480"
  },
  {
    "text": "time to value by kind of collapsing these two stages by using remote storage and accessing the data in",
    "start": "614480",
    "end": "620240"
  },
  {
    "text": "situ right in some sort of distributed file system or you know object store or something like that",
    "start": "620240",
    "end": "627600"
  },
  {
    "text": "so the other uh interesting thing that that comes about with uh distributed storage is it helps a lot",
    "start": "629680",
    "end": "636560"
  },
  {
    "text": "with uh with with parallelism right and you know just kind of simple simple you know explanation of",
    "start": "636560",
    "end": "643279"
  },
  {
    "text": "parallel parallelism much many of you already know all this but you know if i'm digging a ditch and",
    "start": "643279",
    "end": "648640"
  },
  {
    "text": "i'm alone and i break it up into you know six work units um you know it's gonna take me longer",
    "start": "648640",
    "end": "653920"
  },
  {
    "text": "right um if i want to dig that ditch faster i can recruit two of my best friends that are",
    "start": "653920",
    "end": "659040"
  },
  {
    "text": "that are equally hard workers as me and we'll each compute com compute",
    "start": "659040",
    "end": "665279"
  },
  {
    "text": "complete uh two units of work and that ditch will be dug faster right and that's great and the reduction",
    "start": "665279",
    "end": "672399"
  },
  {
    "text": "in time that it takes to dig that ditch is speed up right and that's the it's kind of the",
    "start": "672399",
    "end": "678079"
  },
  {
    "text": "computer sciencey uh what they call that is is speed up so can we additionally instead in",
    "start": "678079",
    "end": "685440"
  },
  {
    "text": "addition to kind of collapsing those two the compute stage and uh or the the training stage",
    "start": "685440",
    "end": "690880"
  },
  {
    "text": "and the data moving stage but we also affect speed up to uh further reduce the the",
    "start": "690880",
    "end": "697600"
  },
  {
    "text": "training time potentially compensating for or uh going to to further than we would be",
    "start": "697600",
    "end": "704399"
  },
  {
    "text": "able to otherwise so if we have this you know model training and and collapse model training and data moving",
    "start": "704399",
    "end": "710720"
  },
  {
    "text": "step can we can we parallelize can we add additional gpu hosts um",
    "start": "710720",
    "end": "715760"
  },
  {
    "text": "of course this would be after we've we've stuffed as many gpus as is nvidia has engine",
    "start": "715760",
    "end": "722959"
  },
  {
    "text": "engineered to be able to fit into a single system but you know once you once you've exhausted the ability to scale up",
    "start": "722959",
    "end": "729519"
  },
  {
    "text": "you want to need you're only left with the ability to scale out so if we scale out can we can we further reduce the the amount of",
    "start": "729519",
    "end": "736160"
  },
  {
    "text": "time it takes to do this kind of collapsed data moving model training step",
    "start": "736160",
    "end": "741600"
  },
  {
    "text": "um so it's in the in the in like with this in mind like this this is the",
    "start": "741600",
    "end": "746720"
  },
  {
    "text": "overall kind of uh like theory um theory level thinking um behind you know the question about",
    "start": "746720",
    "end": "753839"
  },
  {
    "text": "you know is is there a place for distributed storage um for ai ml in in kubernetes",
    "start": "753839",
    "end": "761200"
  },
  {
    "text": "right so next we needed to we kind of needed to test this hypothesis um",
    "start": "761200",
    "end": "767519"
  },
  {
    "text": "and so without further ado i'll let diane talk a little bit about a",
    "start": "767519",
    "end": "773200"
  },
  {
    "text": "benchmark um that that that we use for some testing",
    "start": "773200",
    "end": "778320"
  },
  {
    "text": "called mlperf kyle next slide please",
    "start": "778320",
    "end": "787839"
  },
  {
    "start": "780000",
    "end": "850000"
  },
  {
    "text": "okay so as kyle said we're going to run an experiment and we need models to train for this",
    "start": "788800",
    "end": "795600"
  },
  {
    "text": "experiment and we're going to train them with data that's sitting on distributed storage",
    "start": "795600",
    "end": "801040"
  },
  {
    "text": "and then compare that to data that's sitting on local storage and see how each of them do and so we decided to choose the",
    "start": "801040",
    "end": "809279"
  },
  {
    "text": "ml per bench training benchmarks uh to run and ml perth is like an emerging",
    "start": "809279",
    "end": "816639"
  },
  {
    "text": "industry standard for um",
    "start": "816639",
    "end": "821440"
  },
  {
    "text": "a standard benchmark that is designed to be used as a level playing field",
    "start": "822000",
    "end": "827600"
  },
  {
    "text": "for comparing machine learning infrastructure there are all these companies that are shown here that are contributors to",
    "start": "827600",
    "end": "835199"
  },
  {
    "text": "mlperf and university researchers as well so we thought this would be a great choice",
    "start": "835199",
    "end": "841839"
  },
  {
    "text": "of a real world uh benchmark that the entire test the entire system",
    "start": "841839",
    "end": "847760"
  },
  {
    "text": "um to run in our our distributed storage experiment next slide please",
    "start": "847760",
    "end": "857600"
  },
  {
    "text": "the mlperf benchmarks uh solve real world real world problems as i said such as uh computer vision problems like",
    "start": "857600",
    "end": "865040"
  },
  {
    "text": "identifying images identifying objects and images",
    "start": "865040",
    "end": "871040"
  },
  {
    "text": "and translating one language to another so we are going to run the ssd",
    "start": "871040",
    "end": "878959"
  },
  {
    "text": "uh benchmark which consists of a model",
    "start": "878959",
    "end": "884160"
  },
  {
    "text": "this ssd resonate with our ssd with resnet34 and uses the cocodataset which is",
    "start": "884160",
    "end": "892160"
  },
  {
    "text": "in a publicly available dataset so then we also are going to run",
    "start": "892160",
    "end": "898959"
  },
  {
    "text": "the transformer model and that uses the wmt public data set so i'll talk a little",
    "start": "898959",
    "end": "907040"
  },
  {
    "text": "bit a little bit about each data set and all the code that we used here",
    "start": "907040",
    "end": "912240"
  },
  {
    "text": "is open source you can go to mlperf.org and see the code that we ran uh and also",
    "start": "912240",
    "end": "918880"
  },
  {
    "text": "we're running lpr version 0.6 training in our experiments next slide",
    "start": "918880",
    "end": "924839"
  },
  {
    "start": "921000",
    "end": "965000"
  },
  {
    "text": "please so this is an example of object",
    "start": "924839",
    "end": "930560"
  },
  {
    "text": "detection and segmentation and you can see that the objects have",
    "start": "930560",
    "end": "936399"
  },
  {
    "text": "been detected and they've been labeled and then the segmentation has happened also",
    "start": "936399",
    "end": "941920"
  },
  {
    "text": "on uh the guy in the bicycle so uh these sorts of applications",
    "start": "941920",
    "end": "948959"
  },
  {
    "text": "uh use things like the coco dataset which we're using",
    "start": "948959",
    "end": "954240"
  },
  {
    "text": "and uh each year there are competitions uh to improve the state of the art",
    "start": "954240",
    "end": "961759"
  },
  {
    "text": "in this object detection and segmentation so we just ran one of these benchmarks",
    "start": "961759",
    "end": "967600"
  },
  {
    "start": "965000",
    "end": "1015000"
  },
  {
    "text": "next slide please",
    "start": "967600",
    "end": "970480"
  },
  {
    "text": "a little bit about the coco data set which that we used it's the common object in context data set",
    "start": "973360",
    "end": "981040"
  },
  {
    "text": "uh and it has over 300 000 images",
    "start": "981040",
    "end": "986639"
  },
  {
    "text": "it uh allows researchers and people are doing",
    "start": "986639",
    "end": "992240"
  },
  {
    "text": "benchmarks like we are to take real-world problems of finding these objects in these scenes",
    "start": "992240",
    "end": "999120"
  },
  {
    "text": "and you know outlining them and labeling them and seeing how your model does at that accuracy accuracy level that you achieve",
    "start": "999120",
    "end": "1007040"
  },
  {
    "text": "so the data set was created by cornell in a joint project with microsoft originally",
    "start": "1007040",
    "end": "1012720"
  },
  {
    "text": "and if you'd like to check out the data set online next slide please",
    "start": "1012720",
    "end": "1020160"
  },
  {
    "start": "1015000",
    "end": "1051000"
  },
  {
    "text": "you can download it and you can also peruse around the data set and search for things like",
    "start": "1020160",
    "end": "1027360"
  },
  {
    "text": "in this case you can submit queries at on the cocoa explorer where you want to",
    "start": "1027360",
    "end": "1033600"
  },
  {
    "text": "see all the scenes with horses and bicycles and cars all the images or",
    "start": "1033600",
    "end": "1038959"
  },
  {
    "text": "whatever combination of the objects it recognizes you can choose them here and just",
    "start": "1038959",
    "end": "1046160"
  },
  {
    "text": "check the data set or if it's got the sort of objects that you are looking to um to recognize in your images",
    "start": "1046160",
    "end": "1054320"
  },
  {
    "start": "1051000",
    "end": "1147000"
  },
  {
    "text": "next slide please so kyle talked a bit about why we want",
    "start": "1054320",
    "end": "1061679"
  },
  {
    "text": "gpus we want to get that speed up that powell was talking about and in these",
    "start": "1061679",
    "end": "1067200"
  },
  {
    "text": "benchmarks we use neural networks that rely heavily on linear algebra things like matrix",
    "start": "1067200",
    "end": "1073520"
  },
  {
    "text": "multiply dot product things like that and gpus are extremely efficient at doing that matrix",
    "start": "1073520",
    "end": "1081440"
  },
  {
    "text": "math they are great number crunchers for matrix math and so you could get you can",
    "start": "1081440",
    "end": "1086720"
  },
  {
    "text": "get thousands of times speed up um it's not unusual to say get a 3 000 times speed",
    "start": "1086720",
    "end": "1092640"
  },
  {
    "text": "up if you run like a matrix large matrix multiply uh on a gpu instead of a cpu",
    "start": "1092640",
    "end": "1100799"
  },
  {
    "text": "so uh what we're trying to do here is use these gpus to reduce the training",
    "start": "1100799",
    "end": "1106160"
  },
  {
    "text": "time make those matrix matrix operations go faster and uh we also have to feed those gpus",
    "start": "1106160",
    "end": "1114400"
  },
  {
    "text": "with data and that's what we're testing you know are we getting the data from this distributed file system there",
    "start": "1114400",
    "end": "1120880"
  },
  {
    "text": "fast enough to keep the gps busy so uh training times can take",
    "start": "1120880",
    "end": "1128880"
  },
  {
    "text": "days or weeks and if you made a slight error in your code and then you figure that out a week",
    "start": "1128880",
    "end": "1134480"
  },
  {
    "text": "later then your turnaround time the work time for the data scientists or the researcher",
    "start": "1134480",
    "end": "1139679"
  },
  {
    "text": "is not reasonable so we want to shrink down that time to train so that you can iterate",
    "start": "1139679",
    "end": "1146080"
  },
  {
    "text": "more make more changes improve your model quicker next slide please",
    "start": "1146080",
    "end": "1153279"
  },
  {
    "start": "1147000",
    "end": "1219000"
  },
  {
    "text": "in mlperf uh we will be using python there are many languages that you can use lists",
    "start": "1155840",
    "end": "1162960"
  },
  {
    "text": "our java you could choose other languages to write a neural network application but",
    "start": "1162960",
    "end": "1170640"
  },
  {
    "text": "the benchmarks that we are going to be running use python and there's a good reason why",
    "start": "1170640",
    "end": "1176400"
  },
  {
    "text": "many data scientists prefer python and it's because you have access to libraries and i'm just listing four here",
    "start": "1176400",
    "end": "1183039"
  },
  {
    "text": "that are very useful when writing neural networks so uh there's theano",
    "start": "1183039",
    "end": "1188320"
  },
  {
    "text": "tensorflow keras and pytorch and today in our example we'll be using pi torch",
    "start": "1188320",
    "end": "1194240"
  },
  {
    "text": "another really nice thing about using python is that in your python code you not only have",
    "start": "1194240",
    "end": "1200080"
  },
  {
    "text": "access in our case to the pi torch library but you also have access to numpy and",
    "start": "1200080",
    "end": "1207039"
  },
  {
    "text": "you can move these multi-dimensional arrays back and forth between um between numpy and pi torch very",
    "start": "1207039",
    "end": "1215280"
  },
  {
    "text": "easily the api is very easy to work with and i think that's why researchers and programmers",
    "start": "1215280",
    "end": "1220799"
  },
  {
    "start": "1219000",
    "end": "1328000"
  },
  {
    "text": "like it so much next slide please",
    "start": "1220799",
    "end": "1224799"
  },
  {
    "text": "so just a little bit more about pytorch it's open source uh it was created by facebook it was",
    "start": "1226640",
    "end": "1233120"
  },
  {
    "text": "initially intended as a replacement for numpy uh that",
    "start": "1233120",
    "end": "1239520"
  },
  {
    "text": "used gpus numpy doesn't vampire runs on cpus um high torch enzyme gpus",
    "start": "1239520",
    "end": "1247200"
  },
  {
    "text": "with just minor modifications to your code you can make your multi-dimensional uh",
    "start": "1247200",
    "end": "1254559"
  },
  {
    "text": "arrays in numpy run on gpus when you're using itorch",
    "start": "1254559",
    "end": "1261760"
  },
  {
    "text": "so um my church was created to so that it was easier to uh",
    "start": "1261760",
    "end": "1268960"
  },
  {
    "text": "build neural networks it uses something called tensors which are really just multi-dimensional arrays",
    "start": "1268960",
    "end": "1275360"
  },
  {
    "text": "imperative multi-dimensional arrays that run on gpus and by imperative i mean that when you execute",
    "start": "1275360",
    "end": "1283840"
  },
  {
    "text": "as you go down through your code and you execute a line that",
    "start": "1283840",
    "end": "1288880"
  },
  {
    "text": "uses a tensor bit the computation is performed immediately when you hit that",
    "start": "1288880",
    "end": "1294080"
  },
  {
    "text": "line of code and um not all implementations are like that the first release of tensorflow",
    "start": "1294080",
    "end": "1300960"
  },
  {
    "text": "was not imperative they have now switched to the imperative model because it's so popular and in tensorflow to 2.0 they also",
    "start": "1300960",
    "end": "1308799"
  },
  {
    "text": "uh use the imperative computation approach so uh in pytorch you can run on cpus",
    "start": "1308799",
    "end": "1316240"
  },
  {
    "text": "rgqs you can switch back and forth easily with some minor changes in your code",
    "start": "1316240",
    "end": "1321760"
  },
  {
    "text": "and you get the benefit of flight changes to your code allow you to",
    "start": "1321760",
    "end": "1326799"
  },
  {
    "text": "get huge speedups by using gpus next slide please",
    "start": "1326799",
    "end": "1335200"
  },
  {
    "start": "1328000",
    "end": "1386000"
  },
  {
    "text": "so when we were preparing our benchmarks to run uh on the cluster that we created uh the",
    "start": "1336960",
    "end": "1343440"
  },
  {
    "text": "kubernetes cluster that we created in aws we first containerized",
    "start": "1343440",
    "end": "1349280"
  },
  {
    "text": "uh the ml per benchmark and we created a docker file for it",
    "start": "1349280",
    "end": "1355440"
  },
  {
    "text": "and in our docker file we used nvidia's version of pi torch we",
    "start": "1355440",
    "end": "1361280"
  },
  {
    "text": "built it from source then we added the scripts that read the mlport benchmark and we",
    "start": "1361280",
    "end": "1369120"
  },
  {
    "text": "created that container image pushed it to quayio just has a",
    "start": "1369120",
    "end": "1375679"
  },
  {
    "text": "repository online repository and then we pulled it um",
    "start": "1375679",
    "end": "1380960"
  },
  {
    "text": "into aws when we actually ran this job on our cluster and i'll show you the the yaml next that",
    "start": "1380960",
    "end": "1388159"
  },
  {
    "text": "we use to uh pull that image next slide please",
    "start": "1388159",
    "end": "1393679"
  },
  {
    "text": "so this is actually a different image that we're pulling but not in this case we were pulling a cuda",
    "start": "1393679",
    "end": "1399440"
  },
  {
    "text": "vector add but in our case we pull the image for the ml perk benchmark",
    "start": "1399440",
    "end": "1406159"
  },
  {
    "text": "but what i'm showing here is that you have some yaml that",
    "start": "1406159",
    "end": "1412240"
  },
  {
    "text": "kubernetes interprets and schedules you onto the correct worker node based on",
    "start": "1412240",
    "end": "1417280"
  },
  {
    "text": "the resources you're asking for so here i'm asking for four gpus which",
    "start": "1417280",
    "end": "1422320"
  },
  {
    "text": "is what we had in our cluster in aws and kubernetes will and openshift",
    "start": "1422320",
    "end": "1428840"
  },
  {
    "text": "will schedule that pod onto a nose that has the gp",
    "start": "1428840",
    "end": "1436159"
  },
  {
    "text": "so um what do we see what happened when when the rubber hit",
    "start": "1437120",
    "end": "1442799"
  },
  {
    "text": "the pavement how did this turn out so um as a as a before before we up before i",
    "start": "1442799",
    "end": "1451279"
  },
  {
    "text": "ever um talk about you know what we see we like to give some just splash up some",
    "start": "1451279",
    "end": "1457760"
  },
  {
    "text": "details about the environment so that if people want to recreate this sort of work they can they can do so",
    "start": "1457760",
    "end": "1463679"
  },
  {
    "text": "um we have kind of the the laundry list of versioning on on the left there we used uh",
    "start": "1463679",
    "end": "1469679"
  },
  {
    "text": "openshift container platform uh 4.5 which you know maps to kube uh 1.18 and then we used openshift",
    "start": "1469679",
    "end": "1476799"
  },
  {
    "text": "container storage 4.4 which gave us you know ceph nautilus and and rook and csi all kind of neatly",
    "start": "1476799",
    "end": "1483360"
  },
  {
    "text": "packaged together and then the the cluster itself was uh you know just a few few modest",
    "start": "1483360",
    "end": "1491440"
  },
  {
    "text": "work uh masternodes and then we had some uh three pretty modestly sized uh storage uh",
    "start": "1491440",
    "end": "1499039"
  },
  {
    "text": "dedicated storage workers uh that we you know used taints and tolerations to ensure that only the uh",
    "start": "1499039",
    "end": "1505600"
  },
  {
    "text": "the the storage uh storage workloads were running on them um they had a couple ssds each and then",
    "start": "1505600",
    "end": "1512559"
  },
  {
    "text": "the workers are are relatively uh not not particularly relevant here",
    "start": "1512559",
    "end": "1518240"
  },
  {
    "text": "because all of the workload that we were running or the one workload that we were running was uh basically the the different pi torch uh",
    "start": "1518240",
    "end": "1526320"
  },
  {
    "text": "ml perf tests and those had those uh gpu constraints um and so that",
    "start": "1526320",
    "end": "1533520"
  },
  {
    "text": "made sure that they were scheduled to uh the p3 8xl that we had provisioned which has uh",
    "start": "1533520",
    "end": "1539760"
  },
  {
    "text": "four of four of the nvidia v100s they each have 16 gigs of memory each um we use ec2 uh for to run our our",
    "start": "1539760",
    "end": "1548720"
  },
  {
    "text": "cluster uh us west ii region and then we spread uh spread both the compute and the the",
    "start": "1548720",
    "end": "1556320"
  },
  {
    "text": "masters storage and the worker nodes across uh different availability zones",
    "start": "1556320",
    "end": "1561360"
  },
  {
    "text": "for fault tolerance and but of course because we only had one gpu worker it was it wasn't a single",
    "start": "1561360",
    "end": "1566799"
  },
  {
    "start": "1565000",
    "end": "1661000"
  },
  {
    "text": "availability zone",
    "start": "1566799",
    "end": "1571840"
  },
  {
    "text": "so uh the first benchmark that we ran is an object detection benchmark called",
    "start": "1572880",
    "end": "1578559"
  },
  {
    "text": "ssd and you can see here that uh",
    "start": "1578559",
    "end": "1584000"
  },
  {
    "text": "you can see the software stack that we ran we you know just like uh we just described a minute",
    "start": "1584000",
    "end": "1590799"
  },
  {
    "text": "ago um and so we're using python and pytorch and cuda",
    "start": "1590799",
    "end": "1598559"
  },
  {
    "text": "and on top of that we're running openshift and um so and of course red hat",
    "start": "1598559",
    "end": "1606640"
  },
  {
    "text": "core os is the operating system that we're running and you can see that uh we have a training time where the",
    "start": "1606640",
    "end": "1613760"
  },
  {
    "text": "data was sitting on local ssd so when we say local ssd we mean on the same worker node where the p100s",
    "start": "1613760",
    "end": "1622080"
  },
  {
    "text": "p100 gpus are and um so we had a time of 45 minutes 92",
    "start": "1622080",
    "end": "1628159"
  },
  {
    "text": "seconds uh well it's 0.92 uh minutes actually",
    "start": "1628159",
    "end": "1633360"
  },
  {
    "text": "and um then we had uh training time using ceph of 45.78",
    "start": "1633360",
    "end": "1641440"
  },
  {
    "text": "minutes so you can see there's essentially no difference here that the difference isn't definitely noise at this point uh",
    "start": "1641440",
    "end": "1649200"
  },
  {
    "text": "we could have run it again and it could have been faster with stuff so that was an excellent outcome and it",
    "start": "1649200",
    "end": "1654480"
  },
  {
    "text": "shows that yes it's reasonable to use distributed storage for these training benchmarks next slide please",
    "start": "1654480",
    "end": "1663120"
  },
  {
    "start": "1661000",
    "end": "1756000"
  },
  {
    "text": "that's good news right we're not we're not completely crazy that's great",
    "start": "1663120",
    "end": "1669840"
  },
  {
    "text": "what was the other one that we ran it was um so we ran um the translation benchmark",
    "start": "1670240",
    "end": "1676240"
  },
  {
    "text": "which translates from uh german to english and english to german and so this is a natural language",
    "start": "1676240",
    "end": "1682640"
  },
  {
    "text": "processing benchmark and it was only about a four percent difference here in our timings",
    "start": "1682640",
    "end": "1688080"
  },
  {
    "text": "slightly over 4.01 i think um so again we you know",
    "start": "1688080",
    "end": "1694720"
  },
  {
    "text": "put the training data on the local node and tested it and uh got a timing of",
    "start": "1694720",
    "end": "1702360"
  },
  {
    "text": "62.91 minutes and then to compare side by side we put the",
    "start": "1702360",
    "end": "1709039"
  },
  {
    "text": "trading data in seth and got a timing of 65.43 minutes",
    "start": "1709039",
    "end": "1714880"
  },
  {
    "text": "so again this was a great experiment and we got a good result",
    "start": "1714880",
    "end": "1720799"
  },
  {
    "text": "so we're happy about that yeah it's interesting i mean they're they're they're so close right",
    "start": "1720799",
    "end": "1725919"
  },
  {
    "text": "when you're well you said four percent variance or something it's like uh if we ran you know if we ran each of these you",
    "start": "1725919",
    "end": "1732080"
  },
  {
    "text": "know 20 times would it just be like in the error bars right it wouldn't even it's effectively",
    "start": "1732080",
    "end": "1738480"
  },
  {
    "text": "you know indifferent and so we kind of go okay well this is this was interesting why why why",
    "start": "1738480",
    "end": "1745200"
  },
  {
    "text": "why did it why did it come out this way so we wanted to dig in a little bit um to uh prometheus right uh",
    "start": "1745200",
    "end": "1752000"
  },
  {
    "text": "look at some of the telemetry from the test to to really try to understand um what what's going on here why do we",
    "start": "1752000",
    "end": "1759279"
  },
  {
    "text": "why do we why don't we see what we saw and so uh you know",
    "start": "1759279",
    "end": "1766159"
  },
  {
    "text": "it's very handy and uh useful to use prometheus and grafana together",
    "start": "1766159",
    "end": "1772399"
  },
  {
    "text": "in kubernetes and openshift and right here we're looking at our transformer run",
    "start": "1772399",
    "end": "1778960"
  },
  {
    "text": "and you can see that along the top there the top row graph is gpu utilization",
    "start": "1778960",
    "end": "1787760"
  },
  {
    "text": "and it goes from zero to nearly 100 percent uh when the job",
    "start": "1787760",
    "end": "1793840"
  },
  {
    "text": "starts at on 9 55 you can see the time",
    "start": "1793840",
    "end": "1799760"
  },
  {
    "text": "along the x-axis there and so we are just keeping these gpus very busy",
    "start": "1799760",
    "end": "1806840"
  },
  {
    "text": "um for the duration of that run and you can see that in seth there are",
    "start": "1806840",
    "end": "1813679"
  },
  {
    "text": "you know and also actually next row down is the graph for the memory utilization and we're at about 15",
    "start": "1813679",
    "end": "1820000"
  },
  {
    "text": "gigabytes of gpu memory used for the duration of the job",
    "start": "1820000",
    "end": "1825919"
  },
  {
    "text": "so and then the the set i o ops are kind of distributed equally across the entire run and are",
    "start": "1825919",
    "end": "1833120"
  },
  {
    "text": "are pretty low a lot of not really getting the storage that hard",
    "start": "1833120",
    "end": "1839200"
  },
  {
    "text": "so uh kyle would you like to say any more about this yeah it was it was kind of",
    "start": "1839200",
    "end": "1844240"
  },
  {
    "text": "like uh you know we were i would love we would launch these but we launched this you know this particular uh workload",
    "start": "1844240",
    "end": "1851919"
  },
  {
    "text": "and then and i was sitting there looking at the storage classroom but it was like is it working i was like",
    "start": "1851919",
    "end": "1859440"
  },
  {
    "text": "you know seth is bored right like we're not doing anything um and and i think uh you know the the i o",
    "start": "1859440",
    "end": "1866640"
  },
  {
    "text": "that we're mapping here is is the the the data operations right um so there might be there might be some",
    "start": "1866640",
    "end": "1873919"
  },
  {
    "text": "additional like uh like file system uh like you know stats",
    "start": "1873919",
    "end": "1881039"
  },
  {
    "text": "and listings and stuff that are going on here they're that are increasing the i o uh a little bit but for the most part",
    "start": "1881039",
    "end": "1886399"
  },
  {
    "text": "these are these are relatively you know it's reading reading like a sentence",
    "start": "1886399",
    "end": "1891600"
  },
  {
    "text": "right where like well those those io sizes was really small like if you divide the number of ios by the amount of data",
    "start": "1891600",
    "end": "1897760"
  },
  {
    "text": "that's coming out or the the particular periods so um and then we're like well you know it's probably",
    "start": "1897760",
    "end": "1903440"
  },
  {
    "text": "just reading like a sentence and then it's converting the sentence and then you know doing a comparison type thing so",
    "start": "1903440",
    "end": "1909440"
  },
  {
    "text": "it's it's a relatively you know at least for this workload it's a not a particularly storage intensive uh",
    "start": "1909440",
    "end": "1916159"
  },
  {
    "text": "um machine learning workflow right and it it trains on sentence pairs so um",
    "start": "1916159",
    "end": "1923120"
  },
  {
    "text": "those like the english sentence and the german sentence are sent together",
    "start": "1923120",
    "end": "1928159"
  },
  {
    "text": "and then you train that's how the model is trying to translate from one to the other and so yeah it's not particularly io",
    "start": "1928159",
    "end": "1937120"
  },
  {
    "text": "intensive and the i o is uh human language sentence pairs",
    "start": "1937120",
    "end": "1943279"
  },
  {
    "text": "together so look at the i was more interesting i",
    "start": "1943279",
    "end": "1950000"
  },
  {
    "text": "thought because it was it was totally different right than um than what we saw with the uh",
    "start": "1950000",
    "end": "1957440"
  },
  {
    "text": "uh with the transformer right this so this was the uh the single what is it single single",
    "start": "1957440",
    "end": "1962640"
  },
  {
    "text": "shield detector detector",
    "start": "1962640",
    "end": "1967039"
  },
  {
    "text": "so uh my favorite because it's the ssd right and being a storage guy we love we love things that we love ssds right",
    "start": "1967840",
    "end": "1974240"
  },
  {
    "text": "so the ssd workload [Laughter]",
    "start": "1974240",
    "end": "1979630"
  },
  {
    "text": "overloaded terminology but it was it was interesting from a storage perspective because it it basically just bulk loaded",
    "start": "1980559",
    "end": "1987440"
  },
  {
    "text": "everything at the very beginning and then you know once once it was once it was over to the gpus",
    "start": "1987440",
    "end": "1993120"
  },
  {
    "text": "then it uh you know staff was just kind of relaxing and while the uh while we",
    "start": "1993120",
    "end": "1999279"
  },
  {
    "text": "you know burned up a bunch of uh energy crunching and doing this uh",
    "start": "1999279",
    "end": "2006320"
  },
  {
    "text": "this image detection so that was kind of interesting we saw a lot more io but it was concentrated in like a brief",
    "start": "2006320",
    "end": "2012080"
  },
  {
    "text": "you know almost two two minute window",
    "start": "2012080",
    "end": "2018240"
  }
]