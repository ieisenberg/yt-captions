[
  {
    "text": "hello everyone this talk is titled 10 more ways to blow up your kubernetes by airbnb and this talk is titled 10",
    "start": "799",
    "end": "8800"
  },
  {
    "text": "more ways because we have found more ways to blow up since last year's talk",
    "start": "8800",
    "end": "14880"
  },
  {
    "text": "that was presented by our colleagues melanie and bruce so uh who are we my name is jin",
    "start": "14880",
    "end": "23039"
  },
  {
    "text": "i'm a software engineer at airbnb on the compute infrastructure team this is a team that manages our",
    "start": "23039",
    "end": "28840"
  },
  {
    "text": "kubernetes clusters and manages the abstraction we have over it to make developing services easier",
    "start": "28840",
    "end": "37680"
  },
  {
    "text": "hi my name is joseph and i'm on the compute infra team with gin i've been on this team and at airbnb for",
    "start": "37680",
    "end": "44079"
  },
  {
    "text": "about eight months now so the outline of this talk will be one we're gonna give a brief intro of kubernetes at",
    "start": "44079",
    "end": "50640"
  },
  {
    "text": "airbnb uh two we're gonna dive into ten cases of how we actually uh",
    "start": "50640",
    "end": "56719"
  },
  {
    "text": "mess things up and three uh we're gonna go over and do a recap",
    "start": "56719",
    "end": "63680"
  },
  {
    "text": "all right kubernetes and containers at airbnb so airbnb started our migration to",
    "start": "63840",
    "end": "70240"
  },
  {
    "text": "kubernetes from the beginning of 2018 and since then it has really taken off uh kubernetes is actually running",
    "start": "70240",
    "end": "78000"
  },
  {
    "text": "in production and we rely on it is a critical part of our of running airbnb",
    "start": "78000",
    "end": "85360"
  },
  {
    "text": "our environment is mostly amazon linux 2 for the actual host we use ubuntu images",
    "start": "85360",
    "end": "92640"
  },
  {
    "text": "we use canal for our cni plug-in we do use node port services and smart stack for",
    "start": "92640",
    "end": "98960"
  },
  {
    "text": "service discovery and we support many languages such as ruby java python and go",
    "start": "98960",
    "end": "104399"
  },
  {
    "text": "services now let's dive into the real fun stuff the first case we're",
    "start": "104399",
    "end": "110079"
  },
  {
    "text": "going to talk about is called replicating replica sets so for context we have a job that",
    "start": "110079",
    "end": "116399"
  },
  {
    "text": "regularly scrapes our clusters and it runs a command something like cubes detail get pods all name spaces this job starts",
    "start": "116399",
    "end": "124479"
  },
  {
    "text": "getting out of memory errors in one specific cluster so let's check it out uh and what do we see",
    "start": "124479",
    "end": "131120"
  },
  {
    "text": "we see that in most of our normal clusters we have maybe on an order of a thousand",
    "start": "131120",
    "end": "137920"
  },
  {
    "text": "to two thousand replica sets any guesses to how many this problematic cluster had it was 56",
    "start": "137920",
    "end": "145840"
  },
  {
    "text": "000. so let's see what else is up with this cluster when we dive into all these replica sets",
    "start": "145840",
    "end": "151519"
  },
  {
    "text": "we notice that all of these are actually under one deployment one name space and when we dived deeper",
    "start": "151519",
    "end": "158000"
  },
  {
    "text": "into it we also noticed that the deployment object when we checked its collision count it",
    "start": "158000",
    "end": "163440"
  },
  {
    "text": "was up to a hundred thousand and so right now our theory is likely that the deployment object is creating",
    "start": "163440",
    "end": "169599"
  },
  {
    "text": "replica sets that aren't getting adopted by it so it keeps on creating more as we dive more into this incident uh we",
    "start": "169599",
    "end": "176720"
  },
  {
    "text": "realized that something different about the deployment of this job on this specific cluster that",
    "start": "176720",
    "end": "182959"
  },
  {
    "text": "was different from its deployment of other clusters was this one we were actually testing this new feature called topology spread",
    "start": "182959",
    "end": "190480"
  },
  {
    "text": "constraints and what we noticed it were that these specs were actually not getting picked",
    "start": "190480",
    "end": "196239"
  },
  {
    "text": "up by the replica sets and the theory here is because",
    "start": "196239",
    "end": "201519"
  },
  {
    "text": "the replica sets being created by the deployment didn't have these specs uh the deployment didn't know that it",
    "start": "201519",
    "end": "207200"
  },
  {
    "text": "had already created these replicas that's it didn't know that these replicas belong to it and so it would",
    "start": "207200",
    "end": "212319"
  },
  {
    "text": "just keep on creating more and more replica sets and good thing we have so many we had so many test",
    "start": "212319",
    "end": "218159"
  },
  {
    "text": "clusters so we could actually like diff the specific deployments to see what was going on and in this specific incident the actual",
    "start": "218159",
    "end": "224480"
  },
  {
    "text": "fix was simply just deleting the whole namespace which is fine because",
    "start": "224480",
    "end": "229840"
  },
  {
    "text": "this was just a test cluster and good thing we were just testing this feature out in our test cluster",
    "start": "229840",
    "end": "236239"
  },
  {
    "text": "and so the takeaway here is do you test new features and test clusters because for us that's what they're for we were testing",
    "start": "236239",
    "end": "242000"
  },
  {
    "text": "this new feature and what we noticed was it kind of blew up our cluster and just kept creating",
    "start": "242000",
    "end": "247200"
  },
  {
    "text": "infinite replica sets but not a problem uh we know to investigate more before we roll this out",
    "start": "247200",
    "end": "252720"
  },
  {
    "text": "to production our second incident is called mutating",
    "start": "252720",
    "end": "257919"
  },
  {
    "text": "time bomb and what we see here is we have the service experiencing high error rate and",
    "start": "257919",
    "end": "265040"
  },
  {
    "text": "the service cannot deploy and we get this cryptic kubernetes error saying uh unable to access invalid index",
    "start": "265040",
    "end": "272000"
  },
  {
    "text": "five so we dive deeper into it and check the status of it and what do we see we see that two two pods are running at",
    "start": "272000",
    "end": "278960"
  },
  {
    "text": "what hpa thinks there should be nine replicas running and the service is panicking because",
    "start": "278960",
    "end": "284080"
  },
  {
    "text": "well we only have two pods left digging more into it we notice um there's only one deployment object",
    "start": "284080",
    "end": "291360"
  },
  {
    "text": "but three replica sets which is really weird and the replica sets that actually have",
    "start": "291360",
    "end": "296560"
  },
  {
    "text": "pods are 16 days old as we dug more into it we noticed uh",
    "start": "296560",
    "end": "303199"
  },
  {
    "text": "yeah there's something really wrong going on with the hp here and the ruffle cassettes",
    "start": "303199",
    "end": "308400"
  },
  {
    "text": "and one hour later what have we tried well we tried deleting the hpa and manually scaling it up",
    "start": "308400",
    "end": "314000"
  },
  {
    "text": "because the theory was perhaps there was something wrong with the hpa uh that did not work we tried deleting",
    "start": "314000",
    "end": "320720"
  },
  {
    "text": "those old bad replica sets thinking perhaps it was those old replica sets that were blocking new deploys from happening",
    "start": "320720",
    "end": "327440"
  },
  {
    "text": "but that was not the problem we also made sure not to delete the replica set that had our last",
    "start": "327440",
    "end": "333280"
  },
  {
    "text": "two pods running we also tried creating a new deployment object just to update it just to kick it",
    "start": "333280",
    "end": "339280"
  },
  {
    "text": "and see what happens that did not solve our problems and we thought about perhaps just recreating",
    "start": "339280",
    "end": "344639"
  },
  {
    "text": "the namespace but that would definitely delete the last two pods running uh which is no good because this service is",
    "start": "344639",
    "end": "352240"
  },
  {
    "text": "production so what do we do now we page a teammate we call a friend one hour later",
    "start": "352240",
    "end": "360479"
  },
  {
    "text": "and so when we started digging more into it we come back to the first page where someone had called",
    "start": "360479",
    "end": "366400"
  },
  {
    "text": "out this cryptic error message and we should have done this earlier where i should have dug",
    "start": "366400",
    "end": "372000"
  },
  {
    "text": "more into it because we realized that this was actually the exact problem when we googled for the problem we",
    "start": "372000",
    "end": "378080"
  },
  {
    "text": "noticed it was actually a problem with json patching and related to our mutating emission",
    "start": "378080",
    "end": "383120"
  },
  {
    "text": "controller which was recently rolled out in the last month",
    "start": "383120",
    "end": "388800"
  },
  {
    "text": "uh specifically the bug was this with our mutating emission controller imagine",
    "start": "388800",
    "end": "394880"
  },
  {
    "text": "that on the top left is our input and this is our spec and our desired output is the spec um at",
    "start": "394880",
    "end": "402560"
  },
  {
    "text": "the bottom left uh here we see we are deleting two items from an",
    "start": "402560",
    "end": "407680"
  },
  {
    "text": "array what the bug was the generator output is this uh we",
    "start": "407680",
    "end": "413919"
  },
  {
    "text": "it generates two operations uh it removes index two and removes index three",
    "start": "413919",
    "end": "419440"
  },
  {
    "text": "when our first class glance this patch looks reasonable but the problem is after the first operation the array",
    "start": "419440",
    "end": "426319"
  },
  {
    "text": "doesn't have an index 3 anymore so the correct patch is you actually remove index 3 then index",
    "start": "426319",
    "end": "432720"
  },
  {
    "text": "2. and luckily this issue was already well known once we googled for it and we can",
    "start": "432720",
    "end": "437759"
  },
  {
    "text": "actually um apply it and so once we realized it was a mutating emission controller",
    "start": "437759",
    "end": "443120"
  },
  {
    "text": "uh to stop the bleed we did actually deleted the mutant emission controller from the",
    "start": "443120",
    "end": "448400"
  },
  {
    "text": "specific cluster which immediately resulted in successful pods coming up and so our immediate fire is resolved so",
    "start": "448400",
    "end": "455520"
  },
  {
    "text": "we're done right no as we dug into it more we realized our",
    "start": "455520",
    "end": "460880"
  },
  {
    "text": "change had actually happened seven days ago we realized that for some deployments over the last seven days",
    "start": "460880",
    "end": "466479"
  },
  {
    "text": "new code actually wasn't even being deployed services were being kept alive by their own replica sets",
    "start": "466479",
    "end": "472160"
  },
  {
    "text": "but over time as pods of the overhead because slowly died off uh services were slowly",
    "start": "472160",
    "end": "477840"
  },
  {
    "text": "getting degraded yeah so this problem was way more insidious uh than we had realized",
    "start": "477840",
    "end": "485360"
  },
  {
    "text": "and the takeaway here is uh be aware of the existence of mutating a mission controller",
    "start": "485360",
    "end": "490639"
  },
  {
    "text": "this was something we had recently wrote out and so for many of us uh when we see an incident it doesn't",
    "start": "490639",
    "end": "496000"
  },
  {
    "text": "immediately come to mind to think hey it could be the mutating emission controller",
    "start": "496000",
    "end": "501039"
  },
  {
    "text": "be willing to ask for help because uh paging our additional team member to help us uh definitely got us moving do google the",
    "start": "501039",
    "end": "508639"
  },
  {
    "text": "error messages and uh do great alerts when we dug more into it later during the post warning we",
    "start": "508639",
    "end": "514560"
  },
  {
    "text": "realized we actually do have metrics that could have detected this problem earlier",
    "start": "514560",
    "end": "520080"
  },
  {
    "text": "all right our next incident is called one to auto scaling so we're gonna begin with this",
    "start": "520080",
    "end": "526080"
  },
  {
    "text": "kubernetes issue this is still an open issue the gist of it is let's say you have the situation",
    "start": "526080",
    "end": "532160"
  },
  {
    "text": "where you have a deployment with three replica later you set up an hpa or horizontal pod auto scaler",
    "start": "532160",
    "end": "539200"
  },
  {
    "text": "with a much larger number you then update your deployment and apply it but",
    "start": "539200",
    "end": "544880"
  },
  {
    "text": "this is where the problem happens the moment you apply that or update that deployment the number of",
    "start": "544880",
    "end": "550640"
  },
  {
    "text": "replica resets down to three and so the suggested solution outlined",
    "start": "550640",
    "end": "556720"
  },
  {
    "text": "by comment was remove replicas from the last deployment and remove replicas from the current and",
    "start": "556720",
    "end": "562800"
  },
  {
    "text": "future deployment to let the hp do its thing and that sounded good to us and so we",
    "start": "562800",
    "end": "569279"
  },
  {
    "text": "ship it great so our fix right now is uh if there's a deployment at hpa we",
    "start": "569279",
    "end": "576160"
  },
  {
    "text": "follow the advice of the comment by editing the last employment and deleting",
    "start": "576160",
    "end": "581600"
  },
  {
    "text": "the replica account from it removing the replicas from our current deployment and applying the deployment",
    "start": "581600",
    "end": "588480"
  },
  {
    "text": "but that was not the end of our auto scaling modes so this next case is called zero to auto scaling",
    "start": "588480",
    "end": "595440"
  },
  {
    "text": "so if you actually look at the hpa algorithm this is the algorithm where",
    "start": "595440",
    "end": "601440"
  },
  {
    "text": "desired replicas is a function of your current replicas",
    "start": "601440",
    "end": "607120"
  },
  {
    "text": "some of you might already see the problem if current replicas is zero desired replicas will always be zero",
    "start": "607120",
    "end": "613200"
  },
  {
    "text": "so this is a sounds like a quick and easy fix and we ship it right and so now our fix",
    "start": "613200",
    "end": "619040"
  },
  {
    "text": "looks like if if deployment and hpa and if replica is not zero uh then we",
    "start": "619040",
    "end": "624959"
  },
  {
    "text": "apply that fix otherwise if replica was zero we can actually just leave it alone and the hp will do the correct thing zero to",
    "start": "624959",
    "end": "632399"
  },
  {
    "text": "auto scaling again but that that was that was not our final problem with the hpa",
    "start": "632399",
    "end": "638399"
  },
  {
    "text": "in this incident someone alerted us when they when they did a deploy their number of",
    "start": "638399",
    "end": "644079"
  },
  {
    "text": "replicas actually dropped to zero so let's see what happened here someone followed up and realized uh",
    "start": "644079",
    "end": "651120"
  },
  {
    "text": "yes this is actually a known issue which we had already fixed but we were actually migrating our",
    "start": "651120",
    "end": "657360"
  },
  {
    "text": "deployment services to spinnaker and spinnaker did not have this standard workaround implemented yet and",
    "start": "657360",
    "end": "664640"
  },
  {
    "text": "so the lesson here is migrations are hard and so now the fix is oh and remember to copy this logic to",
    "start": "664640",
    "end": "671680"
  },
  {
    "text": "spinnaker and once more we have more auto scaling",
    "start": "671680",
    "end": "676959"
  },
  {
    "text": "issues so what do you think happens in this scenario you have a service that just migrated to kubernetes and still has old",
    "start": "676959",
    "end": "684320"
  },
  {
    "text": "ec2 boxes running we deployed this to service to kubernetes replicas equals zero",
    "start": "684320",
    "end": "690880"
  },
  {
    "text": "we then scale the service in kubernetes using cube cube cuddle scale deployment and nice",
    "start": "690880",
    "end": "697600"
  },
  {
    "text": "we realize that kubernetes is now taking traffic successfully and so we delete all of our old ec2 boxes and so now the service has fully",
    "start": "697600",
    "end": "704720"
  },
  {
    "text": "migrated to kubernetes and now we deploy again with hpa",
    "start": "704720",
    "end": "710480"
  },
  {
    "text": "that's right an incident happens and the actual bug here is we had",
    "start": "710880",
    "end": "717440"
  },
  {
    "text": "manually scale this deployment with cube cuddle scale deployment",
    "start": "717440",
    "end": "722800"
  },
  {
    "text": "but this means the last the most recent proper deployment was actually still",
    "start": "722800",
    "end": "728480"
  },
  {
    "text": "zero replica we didn't go through a proper deployment",
    "start": "728480",
    "end": "733839"
  },
  {
    "text": "or what what our infrastructure teams consider a proper deployment because they had used the mat the cube",
    "start": "733839",
    "end": "740320"
  },
  {
    "text": "cuddle tool directly to scale up the replicas and so now our fix is uh add a warning",
    "start": "740320",
    "end": "746720"
  },
  {
    "text": "to a cube cutter scale deployment tool to warn people that this is like a temporary tool but they should still",
    "start": "746720",
    "end": "752880"
  },
  {
    "text": "go through a normal deploy process and to actually check what the current replica account is",
    "start": "752880",
    "end": "758880"
  },
  {
    "text": "and our takeaway here is a turning on horizontal plot autoscaler for the first time is not trivial",
    "start": "758880",
    "end": "764560"
  },
  {
    "text": "uh you should test your edge cases such as zero one etc and remember the non-page paths",
    "start": "764560",
    "end": "771279"
  },
  {
    "text": "because even though we had fixed this issue in our normal deploy tool spinnaker was a new a new path that was",
    "start": "771279",
    "end": "777680"
  },
  {
    "text": "still being paved and our next case is called uh did we really delete all all our master nodes",
    "start": "777680",
    "end": "784639"
  },
  {
    "text": "yes yes we do and what happens when you delete um all the master nodes on a cluster is",
    "start": "784639",
    "end": "790639"
  },
  {
    "text": "it means there's no api server uh cube cuddle commands do not work there's no new deploys and the cluster",
    "start": "790639",
    "end": "796399"
  },
  {
    "text": "is just broken but luckily for us this was just a test",
    "start": "796399",
    "end": "801519"
  },
  {
    "text": "cluster um and so as we try to figure out how can we bring this cluster",
    "start": "801519",
    "end": "807279"
  },
  {
    "text": "back up uh our options where one completely delete the cluster and and",
    "start": "807279",
    "end": "813440"
  },
  {
    "text": "bring it up all over again or to actually try to fix it in place and so these are pretty bad situations",
    "start": "813440",
    "end": "820000"
  },
  {
    "text": "to be in but luckily this was still a test this was just a test cluster and one of",
    "start": "820000",
    "end": "825199"
  },
  {
    "text": "our engineers realized actually uh the situation was already documented in our run book",
    "start": "825199",
    "end": "830959"
  },
  {
    "text": "where the suggestion was one delete our mutation controller as an immediate action",
    "start": "830959",
    "end": "836079"
  },
  {
    "text": "via the cube cuddle command and to terminate the cube dns pod to get it rescheduled",
    "start": "836079",
    "end": "841360"
  },
  {
    "text": "the problem with trying to bring up your masters once they have all gone down is the",
    "start": "841360",
    "end": "846800"
  },
  {
    "text": "order in terms of how you bring things back up really matters in this specific case uh",
    "start": "846800",
    "end": "852000"
  },
  {
    "text": "following the run book allowed the necessary services to actually start up again and get unblocked for the",
    "start": "852000",
    "end": "857519"
  },
  {
    "text": "masters to actually come up successfully and the takeaway here is don't drain",
    "start": "857519",
    "end": "864000"
  },
  {
    "text": "your masternodes if you do do check your run books our next scenario our next case is",
    "start": "864000",
    "end": "870800"
  },
  {
    "text": "called masters out of memory so for this specific incident um we",
    "start": "870800",
    "end": "876240"
  },
  {
    "text": "noticed our master's cpu usage going up",
    "start": "876240",
    "end": "881680"
  },
  {
    "text": "we noticed that the cube api server is going wild and using a block of cpu and memory our xcd",
    "start": "881680",
    "end": "888959"
  },
  {
    "text": "resource counts are shooting up and we're getting uh other memory errors on our api server",
    "start": "888959",
    "end": "896959"
  },
  {
    "text": "and our alerts are firing um more alerts are firing and more alerts",
    "start": "896959",
    "end": "902240"
  },
  {
    "text": "are firing so this isn't this is a real incident we also realize at least as we dug more into it is that",
    "start": "902240",
    "end": "908880"
  },
  {
    "text": "uh we actually had no assets either when we tried to ssh into debug we noticed uh we noticed that the nodes",
    "start": "908880",
    "end": "916800"
  },
  {
    "text": "were having real issues and we actually couldn't get in so what is happening here so our api",
    "start": "916800",
    "end": "924240"
  },
  {
    "text": "server is uh ooming and crashing our control plane nodes are severely degraded we don't have ssh access to them we",
    "start": "924240",
    "end": "932320"
  },
  {
    "text": "wonder is this a stp problem is std having issue because uh save is having issues things would be really bad we noticed",
    "start": "932320",
    "end": "939199"
  },
  {
    "text": "that it was having elevated objects being created but it's actually still healthy and fine and existing workloads are",
    "start": "939199",
    "end": "946320"
  },
  {
    "text": "are also still fine so our response here was to spin up a",
    "start": "946320",
    "end": "952000"
  },
  {
    "text": "bigger instances for the control plane for the masters and and this was done just in time because our old instances actually just",
    "start": "952000",
    "end": "958320"
  },
  {
    "text": "completely died and we noticed that memory was uh still getting eaten up super fast but good",
    "start": "958320",
    "end": "963360"
  },
  {
    "text": "thing these new instances are much bigger and so for postmortem we realized um this was actually one of our",
    "start": "963360",
    "end": "970800"
  },
  {
    "text": "largest clusters that was happening that was having this issue so we were wondering is this um a new scaling",
    "start": "970800",
    "end": "977759"
  },
  {
    "text": "limitation that we've just hit we also realized the incident lined up with a deploy that had dramatically",
    "start": "977759",
    "end": "983920"
  },
  {
    "text": "increased their max surge value and that deployed was having crash loops uh and the theory here is uh that",
    "start": "983920",
    "end": "991040"
  },
  {
    "text": "deployed with a high max surge value while it was crash looking it was actually overwhelming our api server",
    "start": "991040",
    "end": "999040"
  },
  {
    "text": "and the takeaway here is uh uh restricting mac surge would help protect the cluster and it's",
    "start": "999040",
    "end": "1005279"
  },
  {
    "text": "a good idea to be ready to scale vertically we were lucky that we were able to spin",
    "start": "1005279",
    "end": "1010639"
  },
  {
    "text": "up bigger instances for our control plane to make sure that they scaled",
    "start": "1010639",
    "end": "1015839"
  },
  {
    "text": "but for this incident we actually still have some ongoing unknowns such as what exactly is a breaking point",
    "start": "1015839",
    "end": "1021440"
  },
  {
    "text": "in terms of a pods or mac surge we've also had larger clusters in the past before and",
    "start": "1021440",
    "end": "1028640"
  },
  {
    "text": "but we didn't run into this issue so the question is what is different now",
    "start": "1028640",
    "end": "1033760"
  },
  {
    "text": "and three uh why did memory usage actually jump suddenly um if you remember from",
    "start": "1033760",
    "end": "1040319"
  },
  {
    "text": "our graphs the api servers were actually using up to like 50 90 gigs of memory",
    "start": "1040319",
    "end": "1045600"
  },
  {
    "text": "uh which is odd because xtv you know can only have can only use up eight gigs of memory so",
    "start": "1045600",
    "end": "1052400"
  },
  {
    "text": "what was actually using it all up authentication in the world of",
    "start": "1052400",
    "end": "1057840"
  },
  {
    "text": "containerization at airbnb we rely on aws in roles as a primary way",
    "start": "1057840",
    "end": "1063919"
  },
  {
    "text": "to authenticate our services this is simple enough in a traditional non-containerized world",
    "start": "1063919",
    "end": "1069360"
  },
  {
    "text": "where each ec2 instance can have their own i am role and they can assume that role and they",
    "start": "1069360",
    "end": "1075919"
  },
  {
    "text": "can request the ec2 metadata api to receive temporary credentials but what happens when you have multiple",
    "start": "1075919",
    "end": "1082240"
  },
  {
    "text": "pods and services running within a single instance we could let each pod assume any of the other pods",
    "start": "1082240",
    "end": "1088160"
  },
  {
    "text": "rolls or make one big iam roll to encapsulate all the pods i'll be",
    "start": "1088160",
    "end": "1094640"
  },
  {
    "text": "running on it but we don't want to give each pod more authentication or permissions than it",
    "start": "1094640",
    "end": "1099919"
  },
  {
    "text": "needs to address this issue we use an open source project called cube2im",
    "start": "1099919",
    "end": "1106480"
  },
  {
    "text": "q2im im credentials to containers running inside a kubernetes cluster based on annotations but what does this",
    "start": "1106480",
    "end": "1112960"
  },
  {
    "text": "really mean well cube2im works by running as a daemon set within the cluster",
    "start": "1112960",
    "end": "1118799"
  },
  {
    "text": "when the pod makes a request to the ec2 api cube2im intercepts the requests then",
    "start": "1118799",
    "end": "1125919"
  },
  {
    "text": "using a special item role of its own it assumes the pods i am role",
    "start": "1125919",
    "end": "1131280"
  },
  {
    "text": "then it makes a request the ec2 api itself receives a temporary credentials for the pod and",
    "start": "1131280",
    "end": "1136799"
  },
  {
    "text": "then forwards it back now q2im didn't wasn't perfect it came",
    "start": "1136799",
    "end": "1143840"
  },
  {
    "text": "with some race conditions so the expected course of events are as follows with cube2im first the node",
    "start": "1143840",
    "end": "1150559"
  },
  {
    "text": "should start up then the cube2impodstartup then we add an iptable rule to forward",
    "start": "1150559",
    "end": "1157679"
  },
  {
    "text": "request to the ec2 api's ip address to the cube2impod then",
    "start": "1157679",
    "end": "1163200"
  },
  {
    "text": "q2im starts watching for new pods after which an applications pod can",
    "start": "1163200",
    "end": "1169120"
  },
  {
    "text": "start up then q2i notices a new pod caches the pod's ip address to",
    "start": "1169120",
    "end": "1175520"
  },
  {
    "text": "its aws im roll mapping then finally the container in the pod can make a",
    "start": "1175520",
    "end": "1181280"
  },
  {
    "text": "request of the ec2 api however what we were seeing was that containers",
    "start": "1181280",
    "end": "1186960"
  },
  {
    "text": "would make requests to the ec2 api before cube2im had even noticed the pod",
    "start": "1186960",
    "end": "1193120"
  },
  {
    "text": "come up and because so when cube2im receives",
    "start": "1193120",
    "end": "1198240"
  },
  {
    "text": "requests it would not be able to find the pod's ip address within its own cache and it would just",
    "start": "1198240",
    "end": "1205280"
  },
  {
    "text": "reject the requests this was especially problematic with init containers",
    "start": "1205280",
    "end": "1210880"
  },
  {
    "text": "which would run before all of the other containers start solution another init container",
    "start": "1210880",
    "end": "1219120"
  },
  {
    "text": "that's right we made a new init container called cube2im weight that would basically just keep pinging",
    "start": "1219120",
    "end": "1224640"
  },
  {
    "text": "the ec2 api until it received a successful response back",
    "start": "1224640",
    "end": "1229840"
  },
  {
    "text": "this works because we had already set the iptable rule before to forward request to",
    "start": "1229840",
    "end": "1236240"
  },
  {
    "text": "cube2im so if you got a successful response back that means cube2im responded",
    "start": "1236240",
    "end": "1241919"
  },
  {
    "text": "successfully meaning it had already noticed the pod come up now this is only a temporary solution",
    "start": "1241919",
    "end": "1249280"
  },
  {
    "text": "we're currently working on a new authentication scheme by which we have the api server",
    "start": "1249280",
    "end": "1255840"
  },
  {
    "text": "use its own private key to sign a token and inject it into a pod on startup then when a pod needs to get",
    "start": "1255840",
    "end": "1262000"
  },
  {
    "text": "authenticated it uses a different api to get credentials for the pod's imrole",
    "start": "1262000",
    "end": "1268000"
  },
  {
    "text": "using the sign token then the sts verifies the token signature is valid with",
    "start": "1268000",
    "end": "1273280"
  },
  {
    "text": "the public keys after which after the validation is complete sts",
    "start": "1273280",
    "end": "1278880"
  },
  {
    "text": "returns the temporary credentials back to the pod our main takeaways here is that ending",
    "start": "1278880",
    "end": "1285600"
  },
  {
    "text": "containers are very versatile despite their simplicity and they're good solutions even if they're only temporary",
    "start": "1285600",
    "end": "1292480"
  },
  {
    "text": "cpu limit equals number of cpu cores in the node still has throttling",
    "start": "1294559",
    "end": "1300879"
  },
  {
    "text": "so one day we got a message from an engineer asking us can you explain why we'd still see",
    "start": "1300960",
    "end": "1306400"
  },
  {
    "text": "throttling when limit is set to 36 with only 36 cores how can we exceed 36",
    "start": "1306400",
    "end": "1312720"
  },
  {
    "text": "so to kind of give context on this let's first talk about how cpu limits work on kubernetes",
    "start": "1312720",
    "end": "1319440"
  },
  {
    "text": "kubernetes limits pods cpu usage using linux's c groups and the cfs the completely fair",
    "start": "1319440",
    "end": "1325120"
  },
  {
    "text": "scheduler so let's take an example of a 36 core machine with a default scheduling period",
    "start": "1325120",
    "end": "1330960"
  },
  {
    "text": "of 100 milliseconds and let's say we have in our kubernetes manifest limits set to cpu of 1024 millicourse",
    "start": "1330960",
    "end": "1338799"
  },
  {
    "text": "that's equivalent to one cpu core this gets translated into linux's",
    "start": "1338799",
    "end": "1344039"
  },
  {
    "text": "cpu.cfs underscore quota of 100 because we have the 100 millisecond default scheduling period",
    "start": "1344039",
    "end": "1350320"
  },
  {
    "text": "times 1 core which gives us 100. now let's take a few different scenarios",
    "start": "1350320",
    "end": "1356320"
  },
  {
    "text": "um here we have a bunch of different scheduling periods and in our first scenario we have one",
    "start": "1356320",
    "end": "1362960"
  },
  {
    "text": "container running one thread and it can run the 400 milliseconds without any throttling",
    "start": "1362960",
    "end": "1369520"
  },
  {
    "text": "now let's say the container actually has two threads then thread one and 2 can run simultaneously",
    "start": "1369520",
    "end": "1375280"
  },
  {
    "text": "for 50 milliseconds after which it will get throttled because the combined usage within that container",
    "start": "1375280",
    "end": "1381200"
  },
  {
    "text": "would have reached 100 milliseconds now in our final scenario we have thread",
    "start": "1381200",
    "end": "1386400"
  },
  {
    "text": "1 2 and three running simultaneously and then thread one and two either",
    "start": "1386400",
    "end": "1391600"
  },
  {
    "text": "terminates or gets descheduled after 25 milliseconds then thread 3 can run for an additional",
    "start": "1391600",
    "end": "1397760"
  },
  {
    "text": "25 milliseconds until it would have reached the 400 milliseconds of this quota after which the threads from the",
    "start": "1397760",
    "end": "1404240"
  },
  {
    "text": "container will get throttled so in our previous scenario",
    "start": "1404240",
    "end": "1411360"
  },
  {
    "text": "we had in our limits a cpu limit of 36 times 1024 millicourse",
    "start": "1411360",
    "end": "1419120"
  },
  {
    "text": "which gets translated into a cfs quota of 3600 so in that case we should be able to run",
    "start": "1419120",
    "end": "1426240"
  },
  {
    "text": "um we should be able to run our threads on all 36 cores for a full",
    "start": "1426240",
    "end": "1433440"
  },
  {
    "text": "100 milliseconds within that period or within any cfs period and not get throttled at all",
    "start": "1433440",
    "end": "1441120"
  },
  {
    "text": "so to figure out what was going on we ran a bunch of tests we made one app with a limit of 36 cpu",
    "start": "1441120",
    "end": "1448799"
  },
  {
    "text": "and we made another app with no limits at all and we ran them on different nodes swap nodes and we found that on average",
    "start": "1448799",
    "end": "1456799"
  },
  {
    "text": "they all ran pretty much with the same performance",
    "start": "1456799",
    "end": "1462080"
  },
  {
    "text": "but what was surprising was that despite the similar performance we actually did see throttling on the",
    "start": "1462080",
    "end": "1468320"
  },
  {
    "text": "app with cpu limits set to 36. so our main takeaways here were that",
    "start": "1468320",
    "end": "1475760"
  },
  {
    "text": "some throttling is inherent and is okay and that we should be be more careful of",
    "start": "1475760",
    "end": "1481279"
  },
  {
    "text": "our metrics docker built-in cpu usage information may not actually be granular enough to",
    "start": "1481279",
    "end": "1486559"
  },
  {
    "text": "capture fully what's going on we also learned that percentage of throttle periods is a",
    "start": "1486559",
    "end": "1491600"
  },
  {
    "text": "better metric than absolute value of periods since the latter is very heavily affected by the number of threads running",
    "start": "1491600",
    "end": "1497679"
  },
  {
    "text": "within the container cpu limits cause out of memory kills in kubernetes",
    "start": "1497679",
    "end": "1505600"
  },
  {
    "text": "we have memory limits and cpu limits and intuitively memory limits cause out of memory kills if they are",
    "start": "1505600",
    "end": "1511760"
  },
  {
    "text": "exceeded and cpu limits would cause throttling and they do but we were also",
    "start": "1511760",
    "end": "1517039"
  },
  {
    "text": "seeing that cpu limits were causing out-of-memory kills so here's an example",
    "start": "1517039",
    "end": "1522559"
  },
  {
    "text": "we have in the top left our pod usage spike a little bit after 12. after which it",
    "start": "1522559",
    "end": "1528640"
  },
  {
    "text": "was immediately followed by a huge spike in memory usage and eventually an out-of-memory kill",
    "start": "1528640",
    "end": "1536000"
  },
  {
    "text": "so what's going on here well the first reason was that these were",
    "start": "1536000",
    "end": "1542559"
  },
  {
    "text": "java processes and the jvm garbage collector is intensive and spawns a lot of threads",
    "start": "1542559",
    "end": "1547679"
  },
  {
    "text": "and the more threads there are running concurrently the faster each container can get",
    "start": "1547679",
    "end": "1553279"
  },
  {
    "text": "throttled there's two main flags to be concerned with here parallel gc threads and",
    "start": "1553279",
    "end": "1558880"
  },
  {
    "text": "concurrent gc threads um within java and",
    "start": "1558880",
    "end": "1564159"
  },
  {
    "text": "these two parameters are used for different stages of the garbage collection but the number of threads used for",
    "start": "1564159",
    "end": "1571039"
  },
  {
    "text": "garbage collection for either of these flags can easily exceed the number of threads in the main application",
    "start": "1571039",
    "end": "1577919"
  },
  {
    "text": "and when both the main application and the garbage collector is being",
    "start": "1577919",
    "end": "1582960"
  },
  {
    "text": "throttled we can have a situation where data isn't being processed and any data",
    "start": "1582960",
    "end": "1590159"
  },
  {
    "text": "that is processed is not able to be freed",
    "start": "1590159",
    "end": "1595919"
  },
  {
    "text": "our second and third problem was that only certain containers get throttled and the back pressure we had in place",
    "start": "1596720",
    "end": "1604080"
  },
  {
    "text": "to kind of mitigate this wasn't working properly so since kubernetes relies on linux c",
    "start": "1604080",
    "end": "1610720"
  },
  {
    "text": "groups to implement throttling this meant that certain containers in the pod can get",
    "start": "1610720",
    "end": "1616640"
  },
  {
    "text": "throttled while some other containers aren't in our case we had the main container processing the data",
    "start": "1616640",
    "end": "1623120"
  },
  {
    "text": "get throttled while another container envoy accepting requests was not",
    "start": "1623120",
    "end": "1630000"
  },
  {
    "text": "this meant that we kept this meant that we were continuously accepting requests while we",
    "start": "1630000",
    "end": "1636559"
  },
  {
    "text": "were unable to process them and we were unable to again because the garbage collector for the main container",
    "start": "1636559",
    "end": "1641679"
  },
  {
    "text": "was also in throttle we couldn't free the memory properly",
    "start": "1641679",
    "end": "1647120"
  },
  {
    "text": "either we had back pressure in place to mitigate this",
    "start": "1647120",
    "end": "1653360"
  },
  {
    "text": "to tell the clients to stop sending requests in an event something like this happened",
    "start": "1653360",
    "end": "1659360"
  },
  {
    "text": "but it wasn't working properly at the time so our main takeaways here be wary of",
    "start": "1659360",
    "end": "1665200"
  },
  {
    "text": "what other processes may be running on the pod and adjust limits accordingly tune jvm garbage collector threads if necessary",
    "start": "1665200",
    "end": "1672240"
  },
  {
    "text": "and to make sure to have proper back pressure to avoid overloading our services",
    "start": "1672240",
    "end": "1677919"
  },
  {
    "text": "rogue ecr cleaner at airbnb we make a lot of docker images",
    "start": "1679360",
    "end": "1685600"
  },
  {
    "text": "this data is from 2019 and at the time we were making over 2",
    "start": "1685600",
    "end": "1690720"
  },
  {
    "text": "million docker images per day and obviously we can't keep them all but we also can't do",
    "start": "1690720",
    "end": "1696480"
  },
  {
    "text": "something naive like deleting all the old images because we don't know which images might be in use so we built a service",
    "start": "1696480",
    "end": "1703440"
  },
  {
    "text": "called the cr cleaner that runs as a cron job every hour and basically all it does is it finds",
    "start": "1703440",
    "end": "1710720"
  },
  {
    "text": "all the images in use then it loops through each of our ecr repos and deletes all the old images",
    "start": "1710720",
    "end": "1715919"
  },
  {
    "text": "except for the ones in use fairly simple and it was working great until it wasn't",
    "start": "1715919",
    "end": "1723120"
  },
  {
    "text": "ecr cleaner started deleting images in production and to figure out what's going on let's",
    "start": "1723120",
    "end": "1730240"
  },
  {
    "text": "go back and look at what it was doing and specifically let's look at the find",
    "start": "1730240",
    "end": "1735760"
  },
  {
    "text": "all images and use function basically it would find all the clusters",
    "start": "1735760",
    "end": "1742159"
  },
  {
    "text": "then keep a list of active images then loop through each cluster figure out what images are being used by",
    "start": "1742159",
    "end": "1748080"
  },
  {
    "text": "all the pods add to that list and return the list of active images",
    "start": "1748080",
    "end": "1753279"
  },
  {
    "text": "well in a recent change there was an error case and on error it would just return an",
    "start": "1753279",
    "end": "1758640"
  },
  {
    "text": "empty list and you can guess what happens afterwards since our list of clusters is empty it",
    "start": "1758640",
    "end": "1764080"
  },
  {
    "text": "finds no images and returns an empty list of images then coming back up it would loop through each repository",
    "start": "1764080",
    "end": "1774159"
  },
  {
    "text": "and delete all the old images except for nothing so it would just start",
    "start": "1774159",
    "end": "1779679"
  },
  {
    "text": "arbitrarily deleting all the old images",
    "start": "1779679",
    "end": "1784000"
  },
  {
    "text": "and our second mistake was a lack of proper alerting ecr cleaner was failing",
    "start": "1784720",
    "end": "1791760"
  },
  {
    "text": "but we weren't getting alerts for that and we also weren't getting alerts for image poll back off errors in production",
    "start": "1791760",
    "end": "1799039"
  },
  {
    "text": "across multiple pods but once we figured out that this was",
    "start": "1799039",
    "end": "1804159"
  },
  {
    "text": "happening our fix was to first shut down ecr cleaner to prevent it from deleting more",
    "start": "1804159",
    "end": "1809679"
  },
  {
    "text": "images then look for crash-looking containers look through ecer cleaner logs um",
    "start": "1809679",
    "end": "1816159"
  },
  {
    "text": "and kind of cross-reference them to figure out what images are missing and manually rebuild them all on the",
    "start": "1816159",
    "end": "1822159"
  },
  {
    "text": "right you can see a teammate rebuilding all these images and keeping track",
    "start": "1822159",
    "end": "1828240"
  },
  {
    "text": "and this is only a short snippet of all the images that we had to rebuild and it was very tedious so thank you",
    "start": "1828240",
    "end": "1833840"
  },
  {
    "text": "kind teammate and our takeaways here were that the more critical the service is the more",
    "start": "1833840",
    "end": "1840080"
  },
  {
    "text": "crucial and thorough testing and review should be and we should have proper error handling",
    "start": "1840080",
    "end": "1845120"
  },
  {
    "text": "and alerts for infrastructural issues and we should also try to make sure that our fixes aren't causing problems",
    "start": "1845120",
    "end": "1851440"
  },
  {
    "text": "elsewhere in this case because we were rebuilding so many images we were causing other people's builds to",
    "start": "1851440",
    "end": "1858799"
  },
  {
    "text": "slow down a lot um and we should have seen foreseen that",
    "start": "1858799",
    "end": "1866559"
  },
  {
    "text": "be careful what you break as part of our on-call readiness goals",
    "start": "1869039",
    "end": "1874480"
  },
  {
    "text": "we started doing fire drills so this is what they look like someone would set something up",
    "start": "1874480",
    "end": "1880240"
  },
  {
    "text": "and send a message in our slack channel so for example fire drill ci is broken and we cannot do a build for ecr cleaner we need to ship",
    "start": "1880240",
    "end": "1886960"
  },
  {
    "text": "a new version to production that displays a new logline on startup without relying on ci evan who is on call disclaimer ci is not",
    "start": "1886960",
    "end": "1893760"
  },
  {
    "text": "really broken do not panic this is a drill this is only a drill",
    "start": "1893760",
    "end": "1898799"
  },
  {
    "text": "and we did a lot of these and we found a lot of bugs a lot of places to improve our code and our documentation and it was great but there was one",
    "start": "1898799",
    "end": "1906559"
  },
  {
    "text": "particular fire drill that was pretty interesting but also pretty insightful and it had to do with the admission",
    "start": "1906559",
    "end": "1912960"
  },
  {
    "text": "controller this was actually one of the first fire drills that i did as a new member in the team",
    "start": "1912960",
    "end": "1918640"
  },
  {
    "text": "so it was meant to be simple basically a change was made in the emission controller to block any",
    "start": "1918640",
    "end": "1925039"
  },
  {
    "text": "creations of a replica set and it looks something like this it's a new rule that said if the",
    "start": "1925039",
    "end": "1931039"
  },
  {
    "text": "resource being applied is a replica set return an error and the fix was supposed",
    "start": "1931039",
    "end": "1937120"
  },
  {
    "text": "to be simple just find an old working commit and deploy that back onto the test cluster that we were using",
    "start": "1937120",
    "end": "1944320"
  },
  {
    "text": "and that's exactly what i did and that was that and it was all going fine until a few days later the same person who had made",
    "start": "1944320",
    "end": "1950799"
  },
  {
    "text": "the fire drill sent this in our team chat i deployed on to the test cluster but it's as if the",
    "start": "1950799",
    "end": "1956559"
  },
  {
    "text": "mission controller didn't get deployed even though the deployment went through has anyone seen that before",
    "start": "1956559",
    "end": "1962320"
  },
  {
    "text": "well i didn't know the time but as it turned out i had seen it before when i went back to deploy an old commit",
    "start": "1962320",
    "end": "1970640"
  },
  {
    "text": "um i thought the deploy had gone through but a mission controller didn't actually get deployed",
    "start": "1970640",
    "end": "1976880"
  },
  {
    "text": "what happened was that there was a bug in our deploy process reporting success for failed deploys specifically",
    "start": "1976880",
    "end": "1983919"
  },
  {
    "text": "to admission controller in this case but why was a mission controller failing in",
    "start": "1983919",
    "end": "1989360"
  },
  {
    "text": "the first place well it was because the mission controller was not",
    "start": "1989360",
    "end": "1994720"
  },
  {
    "text": "whitelisted by the mission controller this meant that since the emission controller was blocking all replica sets it was",
    "start": "1994720",
    "end": "2002640"
  },
  {
    "text": "also blocking itself from being deployed so we were locked out so our solution just delete the",
    "start": "2002640",
    "end": "2009440"
  },
  {
    "text": "validating web of configuration for the emission controller and that way we should be able to bypass",
    "start": "2009440",
    "end": "2015840"
  },
  {
    "text": "the admission controller step but this didn't work either",
    "start": "2015840",
    "end": "2021279"
  },
  {
    "text": "and that's because when the request is sent to the api server it goes through a bunch of steps eventually lands on the validating",
    "start": "2021279",
    "end": "2027519"
  },
  {
    "text": "admission step at which point it takes a look at the validating web configuration resource which is why we have deleted to figure",
    "start": "2027519",
    "end": "2034080"
  },
  {
    "text": "out what webhooks it should get confirmation from but as it turns out on our mission",
    "start": "2034080",
    "end": "2041600"
  },
  {
    "text": "controller deploys the validating web configuration is applied before the web of the",
    "start": "2041600",
    "end": "2047840"
  },
  {
    "text": "web hook this meant that by the time it got for us to update our web hook which is",
    "start": "2047840",
    "end": "2053118"
  },
  {
    "text": "where the bad rule was the validating web of configuration has had already been replaced so the actual",
    "start": "2053119",
    "end": "2060480"
  },
  {
    "text": "solution just nuke the whole thing delete the mutating emission controller webhook",
    "start": "2060480",
    "end": "2065679"
  },
  {
    "text": "delete the deployment and then redeploy the entire thing from scratch so our takeaways here",
    "start": "2065679",
    "end": "2072398"
  },
  {
    "text": "double check that your deploys actually went through make sure that the thing that controls your deploys can",
    "start": "2072399",
    "end": "2077599"
  },
  {
    "text": "actually fix itself with another deploy be aware of kubernetes apply ordering",
    "start": "2077599",
    "end": "2082800"
  },
  {
    "text": "and that even the simple fire drill can reveal a lot of insights and bugs and tldr that will never write a rule",
    "start": "2082800",
    "end": "2089520"
  },
  {
    "text": "that blocks all replica sets all right to recap some of our top 10",
    "start": "2089520",
    "end": "2095919"
  },
  {
    "text": "takeaways are one do test new features in test clusters two uh be aware of the existence of",
    "start": "2095919",
    "end": "2103280"
  },
  {
    "text": "utility mission controller three remember the non-paid paths or uh",
    "start": "2103280",
    "end": "2108640"
  },
  {
    "text": "whatever new pads you might be paving for uh don't drain your master nodes",
    "start": "2108640",
    "end": "2115440"
  },
  {
    "text": "five be ready to scale vertically and eight containers are versatile despite their simplicity and are good",
    "start": "2115440",
    "end": "2121280"
  },
  {
    "text": "solutions even if they're only temporary some throttling is inherent and is okay be wary of what other processes may be",
    "start": "2121280",
    "end": "2127920"
  },
  {
    "text": "running on the pod and adjust limits accordingly have proper error handling and alerts for infrastructural issues",
    "start": "2127920",
    "end": "2134000"
  },
  {
    "text": "and be aware of kubernetes apply ordering all right thank you everyone this is the",
    "start": "2134000",
    "end": "2140560"
  },
  {
    "text": "end of our presentation if you would like to learn more check out our engineering blog um airbnb is also still hiring",
    "start": "2140560",
    "end": "2148720"
  },
  {
    "text": "and a lot of the work we shared today uh were not done by just me or me or",
    "start": "2148720",
    "end": "2154560"
  },
  {
    "text": "joseph uh it was all done by our wonderful team so if you like if you would like to work",
    "start": "2154560",
    "end": "2160560"
  },
  {
    "text": "with us uh reach out apply and if you have any more questions feel free to contact us to follow up",
    "start": "2160560",
    "end": "2169200"
  },
  {
    "text": "thank you",
    "start": "2169200",
    "end": "2173440"
  }
]