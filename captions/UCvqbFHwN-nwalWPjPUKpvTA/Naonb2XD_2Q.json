[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "text": "I'm Erik Anderson I'm on the GRC Java team I'm the tech lead for that I work",
    "start": "30",
    "end": "6960"
  },
  {
    "text": "at Google and I'll talk some about using gypsy for long lived and streaming RBC's",
    "start": "6960",
    "end": "13280"
  },
  {
    "start": "13000",
    "end": "13000"
  },
  {
    "text": "so a little bit of sort of who I hope is coming so that you get the most out of it the talk I hope you have used G RPC",
    "start": "13280",
    "end": "21210"
  },
  {
    "text": "before you don't necessarily have to have used Allah but maybe made a service you made a couple clients you sort of",
    "start": "21210",
    "end": "27240"
  },
  {
    "text": "played with it and then you're sort of interested in more advanced use cases so",
    "start": "27240",
    "end": "33840"
  },
  {
    "start": "33000",
    "end": "33000"
  },
  {
    "text": "a little bit of an overview we'll start talking about just like long lived are pcs and sort of some of the things",
    "start": "33840",
    "end": "39300"
  },
  {
    "text": "regarding those and then I'll talk a go more into detail about streaming our pcs and then streaming Plus long lived our",
    "start": "39300",
    "end": "46530"
  },
  {
    "text": "pcs so here for long lived our pcs that",
    "start": "46530",
    "end": "51870"
  },
  {
    "start": "49000",
    "end": "49000"
  },
  {
    "text": "can probably mean little different things to different people some people think one second is a really really long RPC I'm not too particular",
    "start": "51870",
    "end": "59760"
  },
  {
    "text": "of myself but generally basically if it's lasting minutes or hours or days that would be long something that's",
    "start": "59760",
    "end": "66689"
  },
  {
    "text": "order of magnitudes more than you normally talk about and where it really starts crossing the lines starts",
    "start": "66689",
    "end": "72570"
  },
  {
    "text": "mattering for some of the issues we'll get into like load balancing and things like that because of how it sort of impacts others parts of the system sort",
    "start": "72570",
    "end": "81060"
  },
  {
    "start": "79000",
    "end": "79000"
  },
  {
    "text": "of some of the use cases that the main one is something sort of resembling a hanging get if if you've used up other",
    "start": "81060",
    "end": "89490"
  },
  {
    "text": "systems but basically you can start an RPC and have it just sit there waiting",
    "start": "89490",
    "end": "95549"
  },
  {
    "text": "for some event to happen if you weren't you and so it may last quite a while",
    "start": "95549",
    "end": "101189"
  },
  {
    "text": "before that event actually happens the benefit of this is like the alternative is you do an RPC hey it hasn't happened",
    "start": "101189",
    "end": "106829"
  },
  {
    "text": "yet and it's like no and then you wait and you sleep awhile and then you try again so going ahead and doing long live",
    "start": "106829",
    "end": "114299"
  },
  {
    "text": "RBC reduces the total number of RPC as you do well also as soon as the event",
    "start": "114299",
    "end": "119369"
  },
  {
    "text": "happens you already have RPC outstanding you can go ahead and be notified that it",
    "start": "119369",
    "end": "124409"
  },
  {
    "text": "happened so watches and notifications is sort of what I'm talking about here whenever you start mixing it in with streaming and stuff there's more more",
    "start": "124409",
    "end": "131940"
  },
  {
    "text": "things you can do with it that's at least enough to give you an idea of why you might but we've not done",
    "start": "131940",
    "end": "138900"
  },
  {
    "start": "136000",
    "end": "136000"
  },
  {
    "text": "very much here it's very simple we've just extent expense extended the length of time that an RPC lasts and we do",
    "start": "138900",
    "end": "144840"
  },
  {
    "text": "notice a couple of issues that you should at least be aware of the first one is load balancing load balancing",
    "start": "144840",
    "end": "151650"
  },
  {
    "text": "decisions are made on a per RPC basis so that means that if you have an RPC at",
    "start": "151650",
    "end": "157020"
  },
  {
    "text": "the moment that's created that you you get some load balancing decisions made but those are going to last for the rest",
    "start": "157020",
    "end": "162870"
  },
  {
    "text": "of the lifetime of that RPC and so if you bring up new backends and you just",
    "start": "162870",
    "end": "168240"
  },
  {
    "text": "have a lot of long-lived rpcs you're like why aren't hitting my backends getting any new traffic and",
    "start": "168240",
    "end": "173820"
  },
  {
    "text": "that is because all of your back ins are all the clients are connected to old backends and they just haven't issued a",
    "start": "173820",
    "end": "180180"
  },
  {
    "text": "new RPC yet the other thing some people will use Mac's connection age it's",
    "start": "180180",
    "end": "185670"
  },
  {
    "text": "pretty useful for elf for proxies and Mac's connection age will start shutting",
    "start": "185670",
    "end": "193260"
  },
  {
    "text": "down a connection whenever it reaches a certain age but it by itself does not kill the connection or killing me at the",
    "start": "193260",
    "end": "199530"
  },
  {
    "text": "our pcs on that so it's a waits for those our pcs to finish well it's the time they take a couple seconds max and",
    "start": "199530",
    "end": "205320"
  },
  {
    "text": "so everything will sort of clean up but whenever you start introducing long-lived our pcs these connections",
    "start": "205320",
    "end": "211010"
  },
  {
    "text": "might last for a while so sort of the pathological case there is each of these",
    "start": "211010",
    "end": "216120"
  },
  {
    "text": "old our pcs has their own TCP connection which if you don't have too many that's not a big deal",
    "start": "216120",
    "end": "221400"
  },
  {
    "text": "but some people if you're doing like lots and lots of these you might start noticing that the other thing is network",
    "start": "221400",
    "end": "228450"
  },
  {
    "text": "failures happen like it's network it happens just because your PC sports",
    "start": "228450",
    "end": "234420"
  },
  {
    "text": "long-lived our pcs does not mean you're guaranteed to get two months of time for",
    "start": "234420",
    "end": "240450"
  },
  {
    "text": "this one RPC this is your cherished IVC you never wanted to end so just whenever",
    "start": "240450",
    "end": "245910"
  },
  {
    "text": "you're thinking about long-lived RPC as well yes they can't have you can get quite long time for some of them just be",
    "start": "245910",
    "end": "253830"
  },
  {
    "text": "aware that you can't you shouldn't assume that you'll always get that amount of time things go things go wrong maybe servers are coming up and down so",
    "start": "253830",
    "end": "261269"
  },
  {
    "text": "you should still expect at some points you'll get less than the whole time that you would hope for",
    "start": "261270",
    "end": "267110"
  },
  {
    "text": "and whenever the failures happen TCP will will get TCP connects and those",
    "start": "267110",
    "end": "272390"
  },
  {
    "text": "will kill the calls is we don't migrate from one TCP connection to another or anything like that the other sort of",
    "start": "272390",
    "end": "278990"
  },
  {
    "text": "more interesting thing is network failures whenever they happen you don't know instantly in fact if there's no TCP",
    "start": "278990",
    "end": "285500"
  },
  {
    "text": "writes you may not even know because there's exponential back-off sand play",
    "start": "285500",
    "end": "291910"
  },
  {
    "text": "the network itself is unreliable so it's it's not just some golden Oracle that tells you so that that you need to be",
    "start": "291910",
    "end": "299120"
  },
  {
    "text": "careful of and because these are long-lived RBC's deadlines by a sort of",
    "start": "299120",
    "end": "304280"
  },
  {
    "text": "very nature if you're using that a lot for unary calls stops being quite as useful for long-lived RBC's you can't",
    "start": "304280",
    "end": "312080"
  },
  {
    "text": "still specify it but it'll be much larger than you would normally have and it serves less of a purpose than you may",
    "start": "312080",
    "end": "318050"
  },
  {
    "text": "be sort of familiar with so there's a couple different things my pull asterisks on the ones that basically are",
    "start": "318050",
    "end": "324770"
  },
  {
    "text": "the issues that we get to live with the other things we can maybe do something to combat it but like network failures",
    "start": "324770",
    "end": "330650"
  },
  {
    "text": "happen yes I'm sorry I do not have any solution stuff but some of the other things we can maybe improve so for the",
    "start": "330650",
    "end": "337940"
  },
  {
    "start": "336000",
    "end": "336000"
  },
  {
    "text": "load balancing easiest thing I would suggest is have the server occasionally close the RPC if you need your if you",
    "start": "337940",
    "end": "345530"
  },
  {
    "text": "really wish that over 30 minute period all your clients were rebalanced well go",
    "start": "345530",
    "end": "350690"
  },
  {
    "text": "ahead and close the our pcs after 30 minutes and then that will over the period get them all reconnected and let",
    "start": "350690",
    "end": "356900"
  },
  {
    "text": "the load balancer make new decisions I'm also I mentioned max connection age before well sort of sibling",
    "start": "356900",
    "end": "364990"
  },
  {
    "text": "configuration is max connection age grace which is a grace period for how",
    "start": "364990",
    "end": "370970"
  },
  {
    "text": "long you let these are pcs last and so that's just a duration like if you say uh five minutes then after the max",
    "start": "370970",
    "end": "378410"
  },
  {
    "text": "connection age is triggered you get our pcs gets another five minutes to complete and then they'll go ahead and",
    "start": "378410",
    "end": "383960"
  },
  {
    "text": "get cancelled so this even if you don't use it all the time this setting might",
    "start": "383960",
    "end": "390080"
  },
  {
    "text": "still be usable as a backup like you still are going to have other mechanisms to close our pcs but let's say you forgot this can can",
    "start": "390080",
    "end": "397370"
  },
  {
    "text": "those and kill them anyway if you're as far as the network failures I highly",
    "start": "397370",
    "end": "402889"
  },
  {
    "text": "suggest people to turn on client-side keep alive this is just a setting what will end up happening is gypsy wound up",
    "start": "402889",
    "end": "408800"
  },
  {
    "text": "sending ping frames and and cause some network connection and sorry some network activity and if we don't get any",
    "start": "408800",
    "end": "415490"
  },
  {
    "text": "responses or things like that we'll go ahead and kill the connection and you you would be notified as the the RPC",
    "start": "415490",
    "end": "422120"
  },
  {
    "text": "would be cancelled that that's that's really really really helpful and then",
    "start": "422120",
    "end": "427130"
  },
  {
    "text": "just while I'm sort of saying things this doesn't really address anything from the previous slide but you may find",
    "start": "427130",
    "end": "432979"
  },
  {
    "text": "wait for ready to be useful so wait for ready I'll say normal are PCs whenever",
    "start": "432979",
    "end": "440419"
  },
  {
    "text": "you issue them and let's say there's no connection because someone tripped over the wire obviously you're not it's not",
    "start": "440419",
    "end": "446300"
  },
  {
    "text": "gonna work you're not gonna get connected to the other side and so those are pcs fail immediately gr casinos that",
    "start": "446300",
    "end": "451699"
  },
  {
    "text": "are gonna fail you issue the RBC NGO bruselas he's like yes it failed however with wait for ready whenever gr",
    "start": "451699",
    "end": "459050"
  },
  {
    "text": "PC knows the RPC would fail immediately because there's no connectivity like we",
    "start": "459050",
    "end": "464240"
  },
  {
    "text": "we've been unable to make a connection the port is closed or those sorts of things will just not send the RPC and",
    "start": "464240",
    "end": "470780"
  },
  {
    "text": "we'll go ahead and wait we'll continue retrying to connect and things like that and then once connectivity is",
    "start": "470780",
    "end": "476270"
  },
  {
    "text": "re-established the the RBC will go out and so that can sort of reduce the",
    "start": "476270",
    "end": "482030"
  },
  {
    "text": "amount of exponential back-off you do manually or the moment the connection is",
    "start": "482030",
    "end": "487400"
  },
  {
    "text": "alive again the RBC will go out and so you get actually a little bit reduced latency and though those it's not that",
    "start": "487400",
    "end": "492740"
  },
  {
    "text": "big of a deal but I thought it would just at least give a little plug for it so that's that's basically long-lived",
    "start": "492740",
    "end": "498680"
  },
  {
    "text": "our pcs in many ways they're sort of simple in some ways they still impact things but sort of moving on to",
    "start": "498680",
    "end": "503900"
  },
  {
    "text": "streaming our pcs streams by their sort of nature are just zero too many",
    "start": "503900",
    "end": "509000"
  },
  {
    "text": "messages normally whenever we talk about normal our pcs we call those unary just",
    "start": "509000",
    "end": "514130"
  },
  {
    "text": "they have one in streaming case it'll be zero too many each of those messages is",
    "start": "514130",
    "end": "519469"
  },
  {
    "text": "ordered so if you receive a later or message you already received the previous ones and streaming is",
    "start": "519469",
    "end": "525290"
  },
  {
    "text": "independent in each direction so that the ordering is independent whether or not you have straight stream",
    "start": "525290",
    "end": "531329"
  },
  {
    "text": "itself is independent so sort of how that looks is so we've got",
    "start": "531329",
    "end": "537420"
  },
  {
    "start": "534000",
    "end": "534000"
  },
  {
    "text": "client-to-server the the client would send some method plus some headers and some things like",
    "start": "537420",
    "end": "542970"
  },
  {
    "text": "that it'll send a message and then half post and so this is just normal unary this is sort of the normal stuff people",
    "start": "542970",
    "end": "548579"
  },
  {
    "text": "used to half-closed doesn't even normally matter because what's the point there's only one message and then the",
    "start": "548579",
    "end": "556499"
  },
  {
    "text": "server server responds back with some some headers of its own it sends back a message and their sins back to status",
    "start": "556499",
    "end": "562439"
  },
  {
    "text": "code which was whether it was okay or not hopefully we you know generally we hope it's okay so this is just a normal unary",
    "start": "562439",
    "end": "568199"
  },
  {
    "text": "we're going to go really advanced and streaming is just adding more of this messages not too much there the big",
    "start": "568199",
    "end": "575129"
  },
  {
    "start": "570000",
    "end": "570000"
  },
  {
    "text": "thing to notice is the the headers and the trailers that's where metadata goes so metadata is only at the beginning at",
    "start": "575129",
    "end": "581279"
  },
  {
    "text": "the end of the RPC you're not going to get those in between the messages but other than that is it's probably what",
    "start": "581279",
    "end": "587009"
  },
  {
    "text": "you would expect also half-closed becomes more important here because that's an actual signal to the server that is not going to send any",
    "start": "587009",
    "end": "593339"
  },
  {
    "text": "more messages and you can use that and so how this ends up looking in the normal sort of Preda buff IDL so I said",
    "start": "593339",
    "end": "601139"
  },
  {
    "text": "that streaming is independent the normal unary RPC is just the normal RPC you get",
    "start": "601139",
    "end": "606959"
  },
  {
    "text": "a single request you send a single response you know then you can end up doing clients streaming client streaming",
    "start": "606959",
    "end": "613559"
  },
  {
    "text": "the client sends multiple messages and then after that the server does a response server streaming there's a",
    "start": "613559",
    "end": "619319"
  },
  {
    "text": "single request initially and then the server can do as many responses at once and then a bi-directional streaming is",
    "start": "619319",
    "end": "626639"
  },
  {
    "text": "the last one where it's streaming in both directions bi-directional is a little interesting",
    "start": "626639",
    "end": "632279"
  },
  {
    "text": "because it has some emergent properties as everything but by I M is what we call",
    "start": "632279",
    "end": "638939"
  },
  {
    "text": "half duplex and that there's only one sender at a time it's and even by de I",
    "start": "638939",
    "end": "644819"
  },
  {
    "text": "can be this way but basically you've got the client send and then the server responds and it's sort of always that",
    "start": "644819",
    "end": "650819"
  },
  {
    "text": "that sort of mix with by de you've got two streams going in the same going in",
    "start": "650819",
    "end": "657029"
  },
  {
    "text": "opposite directions they can happen at the same time and so that opens up full duplex and that's basically gets you",
    "start": "657029",
    "end": "664829"
  },
  {
    "text": "something Kenda TCP but instead of it doing raw bytes back and forth you're gonna go ahead and do messages which can be quite",
    "start": "664829",
    "end": "672330"
  },
  {
    "text": "convenient the if you're really familiar with TCP I'll note that my the TCP",
    "start": "672330",
    "end": "678030"
  },
  {
    "text": "semantics are a little bit off here because close is different between the two but a lot of people don't know the",
    "start": "678030",
    "end": "683280"
  },
  {
    "text": "full TCP closed semantics so most people are probably not that bothered by it because this isn't something even you",
    "start": "683280",
    "end": "692670"
  },
  {
    "text": "recognize necessarily but because there's no longer this the client sends",
    "start": "692670",
    "end": "698010"
  },
  {
    "text": "the sort of the server responds you may not realize that G RPC does not provide any implicit acknowledgments to to",
    "start": "698010",
    "end": "705630"
  },
  {
    "text": "messages that are sent the only thing that happens is is is later messages can",
    "start": "705630",
    "end": "711260"
  },
  {
    "text": "implicitly acknowledge the the messages that came before them so the normal way",
    "start": "711260",
    "end": "716610"
  },
  {
    "text": "this happens is in this re B nary a normal RPC case is there's a request and",
    "start": "716610",
    "end": "721860"
  },
  {
    "text": "then there's the response and the response implies that the server received the request because otherwise",
    "start": "721860",
    "end": "727140"
  },
  {
    "text": "how was it generated and so whenever you start looking at full duplex",
    "start": "727140",
    "end": "732150"
  },
  {
    "text": "bi-directional streaming you need to be aware that you just don't assume that because you sent something meant it was",
    "start": "732150",
    "end": "738600"
  },
  {
    "text": "received and you sort of look at how does each side know that a certain",
    "start": "738600",
    "end": "743730"
  },
  {
    "text": "message was received and so sort of",
    "start": "743730",
    "end": "750000"
  },
  {
    "text": "going into the half duplex and here I'm talking about client streaming server streaming and a little bit of the bye",
    "start": "750000",
    "end": "755070"
  },
  {
    "text": "die I will note though that really whenever we're talking about by day a lot of people get excited about it",
    "start": "755070",
    "end": "761070"
  },
  {
    "text": "because it provides full duplex you can sort of in some ways just assume that whenever someone's talking about",
    "start": "761070",
    "end": "767990"
  },
  {
    "text": "bi-directional streaming it's full duplex but I mean technically it's not understand required so this is a lot of",
    "start": "767990",
    "end": "775410"
  },
  {
    "start": "773000",
    "end": "773000"
  },
  {
    "text": "client streaming and service streaming stuff so is streaming itself has lots of",
    "start": "775410",
    "end": "780600"
  },
  {
    "text": "use cases you can use it just for latency or memory reduction so an",
    "start": "780600",
    "end": "786600"
  },
  {
    "text": "example that sort of provides both is sort of like speech to text API so you need you've got a one-minute clip so",
    "start": "786600",
    "end": "793140"
  },
  {
    "text": "you're recording someone's issuing stuff right now and at the end you like a transcript of what and you can either wait for the full",
    "start": "793140",
    "end": "799990"
  },
  {
    "text": "minutes to be recorded and then send it and then the server will process it and then the response comes but you could",
    "start": "799990",
    "end": "806110"
  },
  {
    "text": "even chunk it up into smaller pieces let's say five seconds or 10 seconds and you send those as as it happens in real",
    "start": "806110",
    "end": "812770"
  },
  {
    "text": "time the server could be processing those as if they happen in approximately real time and then at the last 10 second",
    "start": "812770",
    "end": "820660"
  },
  {
    "text": "chunk the server receives that processed that and Sall ready done most of the processing and so then it goes ahead and",
    "start": "820660",
    "end": "825790"
  },
  {
    "text": "sends back the response with the nice pretty text that knew exactly what you're saying and so that that actually",
    "start": "825790",
    "end": "833740"
  },
  {
    "text": "reduced latency because the the server didn't need to wait before receiving everything before it was able to do some",
    "start": "833740",
    "end": "839230"
  },
  {
    "text": "processing and it also reduced over the overall amount of memory that both the client and the server needed because it",
    "start": "839230",
    "end": "846160"
  },
  {
    "text": "neither side actually needed the full one minute of recording in memory at one",
    "start": "846160",
    "end": "853630"
  },
  {
    "text": "particular time you can also do some you get some little benefits because there's",
    "start": "853630",
    "end": "859330"
  },
  {
    "text": "a separation now between the response and the end of the call normally with a unary response the server responds and",
    "start": "859330",
    "end": "866440"
  },
  {
    "text": "now it's given up like it not given up but it has no more opportunity to say anything more about that RPC with",
    "start": "866440",
    "end": "873730"
  },
  {
    "text": "streaming now you can respond and then if you want you can go ahead and respond again and respond again I'm gonna bring",
    "start": "873730",
    "end": "880270"
  },
  {
    "text": "it up again again but watches are a good easy use case where you can see how this could be useful you can say I'm",
    "start": "880270",
    "end": "886570"
  },
  {
    "text": "interested in some topic and it can tell you oh hey here's what's happened on",
    "start": "886570",
    "end": "892810"
  },
  {
    "text": "that topic and then if something else happens it can go ahead and send that to you immediately you don't need a wait",
    "start": "892810",
    "end": "898000"
  },
  {
    "text": "for any RP RBC to request more information and that can reduce latency and just reduce the amount of churn of",
    "start": "898000",
    "end": "905680"
  },
  {
    "text": "the our pcs another nice benefit of streaming is you get flow control or pushback this is where the if a cinder",
    "start": "905680",
    "end": "914380"
  },
  {
    "text": "is sending faster than the receiver can consume it it'll go ahead and slow down the sender to whatever the rates the",
    "start": "914380",
    "end": "921130"
  },
  {
    "text": "receiver can process it and this can be really useful for like bulk uploads so",
    "start": "921130",
    "end": "927340"
  },
  {
    "text": "normally whenever you upload you like me to do chunks and so you upload a chunk you wait for the response okay that",
    "start": "927340",
    "end": "934089"
  },
  {
    "text": "one's good you then go ahead and send another chunk and you wait for the response that one's good each time is those that you wait that's",
    "start": "934089",
    "end": "940660"
  },
  {
    "text": "latency that was lost bandwidth that you could have had instead you can actually send one after the other in the stream",
    "start": "940660",
    "end": "946990"
  },
  {
    "text": "if you're sending too fast you'll be slowed down to whatever the network can handle and so you don't end up having to",
    "start": "946990",
    "end": "955870"
  },
  {
    "text": "incur those delays where you end up waiting at the end and so that can save",
    "start": "955870",
    "end": "961800"
  },
  {
    "text": "that means you don't need to tune the chunk size as much you don't want one bytes but you also don't want you know",
    "start": "961889",
    "end": "967839"
  },
  {
    "text": "one gig but anything sort of in a medium range will do pretty reasonably well whereas before with the sort of chunked",
    "start": "967839",
    "end": "975819"
  },
  {
    "text": "and then you wait you're sort of encouraged to have really large chunks sizes so that those wait times if you're",
    "start": "975819",
    "end": "982449"
  },
  {
    "text": "on a high latency link or things like that they don't occurs much they don't they're not as big of a penalty and",
    "start": "982449",
    "end": "990819"
  },
  {
    "text": "basically some of this is just pipelining if you've heard about that in other situations streaming just gives",
    "start": "990819",
    "end": "996009"
  },
  {
    "text": "you pipelining so streaming because",
    "start": "996009",
    "end": "1004560"
  },
  {
    "text": "you've got all these messages they're ordered they've got implicit state you can associate state between these different",
    "start": "1004560",
    "end": "1009990"
  },
  {
    "text": "messages so some simple things so that that is all these are piece messages are",
    "start": "1009990",
    "end": "1015990"
  },
  {
    "text": "going to the same back end a stream is tied to the back engine so you know the same back end is receiving everything it",
    "start": "1015990",
    "end": "1021870"
  },
  {
    "text": "knows about the things that came before and you can you make that you can use that to your advantage also the the call",
    "start": "1021870",
    "end": "1029699"
  },
  {
    "text": "lifetime itself has been expanded before is only the request response now you can do a lot more things within the call so",
    "start": "1029699",
    "end": "1036120"
  },
  {
    "text": "for example if you want to have a transaction where you end up sending multiple requests which or each query",
    "start": "1036120",
    "end": "1042000"
  },
  {
    "text": "and or update or things like that the the you can either use the call lifetime",
    "start": "1042000",
    "end": "1048209"
  },
  {
    "text": "itself for managing the transaction or think so if the call gets killed the transaction gets killed but you now have",
    "start": "1048209",
    "end": "1056100"
  },
  {
    "text": "this this new larger scoped thing of this this lifetime of a thing that you",
    "start": "1056100",
    "end": "1061350"
  },
  {
    "text": "didn't have before which is the the RPC lifetime itself and not just an",
    "start": "1061350",
    "end": "1066610"
  },
  {
    "text": "individual message you can also reduce the the per message setup costs so like",
    "start": "1066610",
    "end": "1072519"
  },
  {
    "text": "the watches I was talking about before you don't have to tear everything down and so you can just you know the server",
    "start": "1072519",
    "end": "1080200"
  },
  {
    "text": "already knows what you're wanting you don't have to just artificially create new or pcs just because that's how the",
    "start": "1080200",
    "end": "1085659"
  },
  {
    "text": "system has to work and then one sort of interesting thing is is because all the",
    "start": "1085659",
    "end": "1091629"
  },
  {
    "text": "messages are ordered you can actually like provide a full state at the very beginning and then any like just deltas of changes that are happening after that",
    "start": "1091629",
    "end": "1098919"
  },
  {
    "text": "point and that could be really useful watches again it pretty easy but the",
    "start": "1098919",
    "end": "1104830"
  },
  {
    "text": "sort of deltas and things like that you end up having to do quite a lot of infrastructure if you want to do that in",
    "start": "1104830",
    "end": "1110230"
  },
  {
    "text": "normal our pcs in Ian's area our pcs you're gonna have to have some tokens or recovery tokens so what was the previous",
    "start": "1110230",
    "end": "1116049"
  },
  {
    "text": "state you saw but in this case you can just the server can just say here's the current state and then as changes happen",
    "start": "1116049",
    "end": "1122139"
  },
  {
    "text": "you send it out and so there's much less synchronization between the client and the server and protocol is sort of",
    "start": "1122139",
    "end": "1128320"
  },
  {
    "start": "1128000",
    "end": "1128000"
  },
  {
    "text": "simpler but there's complexities there's issues with with streaming and a lot of",
    "start": "1128320",
    "end": "1135940"
  },
  {
    "text": "this it's it it's not for free there's complexities involved so it",
    "start": "1135940",
    "end": "1141039"
  },
  {
    "text": "makes sure you're getting something out of the streaming or the long-lived our pcs don't do it just because Wells gypsy",
    "start": "1141039",
    "end": "1147460"
  },
  {
    "text": "provides streaming obviously I want to use it because it's the greatest feature in the world unary our PCs I'm not",
    "start": "1147460",
    "end": "1153399"
  },
  {
    "text": "trying to burn them at all they're actually really great most our pcs should be unary our pcs but as we talked",
    "start": "1153399",
    "end": "1160629"
  },
  {
    "text": "about through all these issues keep that in mind that you know make sure that what you're getting out of these",
    "start": "1160629",
    "end": "1165789"
  },
  {
    "text": "advanced features are actually benefitting you more than some of the issues you now get to deal with one",
    "start": "1165789",
    "end": "1172690"
  },
  {
    "text": "thing to be aware of is that gypsy has flow control mention that for the streams but that has very large buffers",
    "start": "1172690",
    "end": "1178539"
  },
  {
    "text": "relative to what some people may consider large and certain circumstances so we see some streaming our pcs that",
    "start": "1178539",
    "end": "1185559"
  },
  {
    "text": "have messages that are 5 bytes large well 64 K seems really really large",
    "start": "1185559",
    "end": "1191259"
  },
  {
    "text": "whenever you're talking about 5 byte messages and the the flow control is",
    "start": "1191259",
    "end": "1196779"
  },
  {
    "text": "point-to-point so if you add new nodes into the middle with new so you've got a new proxy that",
    "start": "1196779",
    "end": "1203290"
  },
  {
    "text": "that's gonna be an l7 proxy in the middle it can add its own buffers and so you can you can very easily be looking",
    "start": "1203290",
    "end": "1209380"
  },
  {
    "text": "at megabytes of buffers that all vary depending on your network and some of actual concrete implementations but just",
    "start": "1209380",
    "end": "1216550"
  },
  {
    "text": "be aware that it could be large and I already mentioned some of the API",
    "start": "1216550",
    "end": "1221710"
  },
  {
    "text": "complexity it's both the complexity of the system itself it can do more that means you need to be aware of more Korth",
    "start": "1221710",
    "end": "1227380"
  },
  {
    "text": "more cases before is real simple but also the API is themselves in this in",
    "start": "1227380",
    "end": "1233500"
  },
  {
    "text": "the languages we we have optimized or we do think that most our pcs are unary",
    "start": "1233500",
    "end": "1239920"
  },
  {
    "text": "those are the ones that we try to make as easy as possible streaming is a more advanced use case and so it's intended",
    "start": "1239920",
    "end": "1247720"
  },
  {
    "text": "for you to be able to get some of that power but it also means that you incur some some complexity and that'll vary a",
    "start": "1247720",
    "end": "1253690"
  },
  {
    "text": "little bit per language that you're you're sort of interacting with but be aware whenever you're doing streaming",
    "start": "1253690",
    "end": "1263730"
  },
  {
    "text": "very often your normal retries like if you've got sort of a framework level retry it stops making as much sense so",
    "start": "1263730",
    "end": "1271990"
  },
  {
    "text": "you may end up especially with server streaming you may end up having trouble",
    "start": "1271990",
    "end": "1277680"
  },
  {
    "text": "and needing to implement reach right at inside the application itself as opposed",
    "start": "1277680",
    "end": "1283660"
  },
  {
    "text": "to any sort of framework level something that's automatically doing it and that's because you're gonna need to have some",
    "start": "1283660",
    "end": "1289270"
  },
  {
    "text": "specialized logic in order to retrieve just the new things that you wanted and not have to reissue the full RVC so just",
    "start": "1289270",
    "end": "1295960"
  },
  {
    "text": "be aware that because it can be a little saddening whenever you realize that and tracing stat tracing and stats systems",
    "start": "1295960",
    "end": "1304270"
  },
  {
    "text": "are not necessarily all universally expecting streaming and so it'll it can",
    "start": "1304270",
    "end": "1310420"
  },
  {
    "text": "mean maybe get muddled a little bit or might be missing that'll vary per system and I don't know all of the systems out",
    "start": "1310420",
    "end": "1317590"
  },
  {
    "text": "there on how well they do here but be aware that it may be a little harder to",
    "start": "1317590",
    "end": "1323280"
  },
  {
    "start": "1321000",
    "end": "1321000"
  },
  {
    "text": "investigate what's happening in the system so the things that we can",
    "start": "1323280",
    "end": "1328929"
  },
  {
    "text": "actually control there is if there's flow control problems I mentioned the large buffer sizes if that's a problem for you then I suggest",
    "start": "1328929",
    "end": "1335080"
  },
  {
    "text": "you actually go full by dye full diaper by dye full duplex and what that allows",
    "start": "1335080",
    "end": "1341320"
  },
  {
    "text": "is you can end up doing application level flow control and what that is is you end up having little messages that",
    "start": "1341320",
    "end": "1346720"
  },
  {
    "text": "you send on as part of your streams that say go ahead and give me two more messages or go ahead and give me two",
    "start": "1346720",
    "end": "1351999"
  },
  {
    "text": "more events or go ahead and give me two more jobs you so that you request that in a normal message and then the the",
    "start": "1351999",
    "end": "1358419"
  },
  {
    "text": "other side like the server can go ahead and give you two more jobs and that way it's not it's not queuing up at all",
    "start": "1358419",
    "end": "1364210"
  },
  {
    "text": "within G RPC you're doing you're sort of preventing the the remote side from sending it in the first place and the",
    "start": "1364210",
    "end": "1371049"
  },
  {
    "text": "benefit of that is that's also in the end you do there's some some complexities there in that at times you",
    "start": "1371049",
    "end": "1378669"
  },
  {
    "text": "have to decide how many do you want to be outstanding at a time you get to tune that but you know that it's it's the",
    "start": "1378669",
    "end": "1386049"
  },
  {
    "text": "this to get that sort of features those are the things you need to do",
    "start": "1386049",
    "end": "1391119"
  },
  {
    "text": "tracing stats for some of the for especially half-duplex streaming a lot",
    "start": "1391119",
    "end": "1396460"
  },
  {
    "text": "of times if you sort of treat it just like unary you're like okay there were five messages but let's just claim they were for one really big message it'll",
    "start": "1396460",
    "end": "1404049"
  },
  {
    "text": "mostly work out depending on what you're doing like some of the watches and stuff it may not work out as well but in",
    "start": "1404049",
    "end": "1410200"
  },
  {
    "text": "plenty of cases it will be a pretty close approximation and so you may not",
    "start": "1410200",
    "end": "1416049"
  },
  {
    "text": "be that bad off but now we're getting into full duplex streaming this is the",
    "start": "1416049",
    "end": "1422830"
  },
  {
    "text": "really advanced stuff this was like it's suppose to be an intermediate talk a lot of the streaming you can do intermediate",
    "start": "1422830",
    "end": "1428559"
  },
  {
    "text": "pretty well at the end of doing a full bi-directional protocol I would consider you advanced that is that the advanced",
    "start": "1428559",
    "end": "1434830"
  },
  {
    "text": "stuff we're doing with GRDC and that's because you basically have tcp with",
    "start": "1434830",
    "end": "1440740"
  },
  {
    "start": "1437000",
    "end": "1437000"
  },
  {
    "text": "messages you can sort of do whatever you want so whenever you can do whatever you want you can do whatever you want and",
    "start": "1440740",
    "end": "1447309"
  },
  {
    "text": "that's you then the time potentially be your own worst enemy but you can make any custom protocols you have a full",
    "start": "1447309",
    "end": "1454409"
  },
  {
    "text": "escape hatch to do whatever you need and as I mentioned before you've got application level flow control we can",
    "start": "1454409",
    "end": "1460990"
  },
  {
    "text": "explicitly request which things you're going to want - not necessarily rely on the GRP flow control transactions can work pretty",
    "start": "1460990",
    "end": "1468219"
  },
  {
    "text": "nicely here because you can actually be issuing requests or queries and then",
    "start": "1468219",
    "end": "1473739"
  },
  {
    "text": "receive the response for the queries and that all happens in conjunction on these streams and then if let's say the client",
    "start": "1473739",
    "end": "1480070"
  },
  {
    "text": "dies it crashes the RPC itself will get canceled on the server and so the server knows to cancel the RV's cancel the",
    "start": "1480070",
    "end": "1486729"
  },
  {
    "text": "transaction you can sort of do a couple sort of fancy things around those you can also do live reconfiguration since",
    "start": "1486729",
    "end": "1494489"
  },
  {
    "text": "things are all you've got the the organized state one messages Falls the",
    "start": "1494489",
    "end": "1499779"
  },
  {
    "text": "other you can say oh I'm interested in five different topics or my watch and then later you're like oh actually a",
    "start": "1499779",
    "end": "1505659"
  },
  {
    "text": "sixth one would be great - and a little bit later like yeah get rid of that third one and you can change it on the",
    "start": "1505659",
    "end": "1511539"
  },
  {
    "text": "fly and you're not needing to tear down the old one bring up a new one and figure out what do you do in the",
    "start": "1511539",
    "end": "1518619"
  },
  {
    "text": "middle point because there's a race or anything like that and you can also do",
    "start": "1518619",
    "end": "1523869"
  },
  {
    "text": "bulk uploads so before I mentioned both downloads for just a half-duplex case the reason uploads are different is",
    "start": "1523869",
    "end": "1531099"
  },
  {
    "text": "because you really need resumption typically with with large bulk uploads",
    "start": "1531099",
    "end": "1536469"
  },
  {
    "text": "and that means as you're uploading you need resumption keys to come back to you and so in that case the client would",
    "start": "1536469",
    "end": "1543579"
  },
  {
    "text": "remember the last resumption key that it received if something goes wrong it will go ahead and create a new RPC and new",
    "start": "1543579",
    "end": "1550570"
  },
  {
    "text": "and just resume based on that key this still has some benefits though because",
    "start": "1550570",
    "end": "1557469"
  },
  {
    "text": "you're not having to use that key continually so normally if you're doing bulk uploads you're using the keys",
    "start": "1557469",
    "end": "1562479"
  },
  {
    "text": "constantly it's a very normal common case in this case with bulk uploads",
    "start": "1562479",
    "end": "1567669"
  },
  {
    "text": "streaming you sort of delegated it - it happens but it's not happening constantly it's doesn't need to be quite",
    "start": "1567669",
    "end": "1574719"
  },
  {
    "text": "as optimized as normal and then sort of off to the side there's the half clothes",
    "start": "1574719",
    "end": "1580659"
  },
  {
    "text": "that I mentioned was sort of came into the picture before and you can go ahead and use that to sort of cleanly hang up",
    "start": "1580659",
    "end": "1587440"
  },
  {
    "text": "until the server you're not going to send anymore and then I can sort of let things close gracefully",
    "start": "1587440",
    "end": "1594119"
  },
  {
    "start": "1594000",
    "end": "1594000"
  },
  {
    "text": "so full duplex we get a couple more issues steps tracing systems may be",
    "start": "1595260",
    "end": "1600399"
  },
  {
    "text": "overly simplistic that looks a little bit similar it becomes a bigger problem with folding Plex we have API protocol",
    "start": "1600399",
    "end": "1609760"
  },
  {
    "text": "complexity you are dealing with two streams that infinite events are",
    "start": "1609760",
    "end": "1614980"
  },
  {
    "text": "happening on that's complex like whenever it like we talk about threading",
    "start": "1614980",
    "end": "1621549"
  },
  {
    "text": "and things like that your code itself will become more complex because you're doing something more complex and I would",
    "start": "1621549",
    "end": "1629110"
  },
  {
    "text": "not underestimate how much impacts that can have at this point you're almost",
    "start": "1629110",
    "end": "1636340"
  },
  {
    "text": "guaranteed to have application level retry it's really really hard to avoid who know like any retry system doesn't",
    "start": "1636340",
    "end": "1642970"
  },
  {
    "text": "really know what's what's going on and so you're gonna have to do it yourself",
    "start": "1642970",
    "end": "1648510"
  },
  {
    "text": "there's flow control there's pushback there's all these things that means you",
    "start": "1649049",
    "end": "1654159"
  },
  {
    "text": "actually open yourself up to deadlocking yes your RBC's can deadlock beware",
    "start": "1654159",
    "end": "1661110"
  },
  {
    "text": "that's not something you you might think about so just beware be a conscious",
    "start": "1661110",
    "end": "1667019"
  },
  {
    "text": "because you you you can go and do all these other our pcs for so long and not have to think about that and if you do",
    "start": "1667019",
    "end": "1673990"
  },
  {
    "text": "full by die then our full duplex then basically breast conversion is is thrown",
    "start": "1673990",
    "end": "1679990"
  },
  {
    "text": "out there's really no hope for that given the the rest ecosystem so a lot of",
    "start": "1679990",
    "end": "1685990"
  },
  {
    "start": "1684000",
    "end": "1684000"
  },
  {
    "text": "these those we couldn't do very much about but as far as the the deadlocks the best way to handle that is make sure",
    "start": "1685990",
    "end": "1692260"
  },
  {
    "text": "that any point in time at least one side is receiving if you do that then everything sort of can flow through the",
    "start": "1692260",
    "end": "1698529"
  },
  {
    "text": "system and it'll be okay but if you ever get to a point where both sides are sending in the nobody's receiving that is a potential for",
    "start": "1698529",
    "end": "1705279"
  },
  {
    "text": "deadlock also know just because you tested it once and it didn't that lock doesn't mean anything I mean there were",
    "start": "1705279",
    "end": "1711100"
  },
  {
    "text": "buffers in the system those can change based on live network behavior and so maybe you had one",
    "start": "1711100",
    "end": "1716649"
  },
  {
    "text": "megabyte buffer whenever you tested and it's been like that for a while and then the buffer sort of diminished and it",
    "start": "1716649",
    "end": "1721990"
  },
  {
    "text": "became 64k and that would be enough for a lot of people to maybe know",
    "start": "1721990",
    "end": "1727929"
  },
  {
    "text": "the deadlock in that case but it didn't happen before and so now let's go ahead",
    "start": "1727929",
    "end": "1733059"
  },
  {
    "text": "and plug the two together long live plus streaming so we have load balancing",
    "start": "1733059",
    "end": "1739360"
  },
  {
    "start": "1735000",
    "end": "1735000"
  },
  {
    "text": "before before it was mainly CPU was the issue for load balancing that's because",
    "start": "1739360",
    "end": "1745029"
  },
  {
    "text": "they wasn't doing very much it was a single request single response it's just",
    "start": "1745029",
    "end": "1750369"
  },
  {
    "text": "sitting there waiting for something that happened most likely so it was sorry",
    "start": "1750369",
    "end": "1756759"
  },
  {
    "text": "sorry it wasn't CPU before it is memory before it's just sort of sitting there consuming some resources but probably not too active on the server",
    "start": "1756759",
    "end": "1762669"
  },
  {
    "text": "now we're talking about streaming there's lots of stuff happening all the time but we're talking about the transaction case for instance all these",
    "start": "1762669",
    "end": "1770679"
  },
  {
    "text": "messages are implying quite a lot of load and so you have both memory and CPU now so at the moment you're doing",
    "start": "1770679",
    "end": "1777340"
  },
  {
    "text": "long-lived streaming our pcs definitely definitely watch out for load balancing before with just the long-lived our PCs",
    "start": "1777340",
    "end": "1783519"
  },
  {
    "text": "load balancing comes into play but it's not as dire and then again chasing and stat systems maybe overly simplistic",
    "start": "1783519",
    "end": "1791399"
  },
  {
    "text": "before I said that if you're doing streaming it's mostly fine because you",
    "start": "1791399",
    "end": "1796990"
  },
  {
    "text": "can just sort of assume all the messages are one big message here we're spreading out the messages over time it's really",
    "start": "1796990",
    "end": "1803649"
  },
  {
    "text": "really spread out and so that that sort of extra delays become very noticeable",
    "start": "1803649",
    "end": "1809649"
  },
  {
    "text": "and make it much harder to debug if you don't see those in your step systems but",
    "start": "1809649",
    "end": "1816340"
  },
  {
    "start": "1813000",
    "end": "1813000"
  },
  {
    "text": "really as far as solving the load balancing issue it's gonna be a lot like what I had said before with the long-lived RPC it's nothing really",
    "start": "1816340",
    "end": "1822279"
  },
  {
    "text": "changes occasionally you should go ahead and clean up our pcs let them go ahead",
    "start": "1822279",
    "end": "1827740"
  },
  {
    "text": "and finish let them go ahead and make another load balancer but that's gonna be basically yeah and so then that's",
    "start": "1827740",
    "end": "1833169"
  },
  {
    "text": "what we've got I'm very eager for QA if",
    "start": "1833169",
    "end": "1838419"
  },
  {
    "text": "I've got my email and jitter up there sorry github I'm on Gator as well if you",
    "start": "1838419",
    "end": "1845529"
  },
  {
    "text": "send me an e mails please send a link to GRP cio as well that's the mailing list because there's lots of people who can",
    "start": "1845529",
    "end": "1852369"
  },
  {
    "text": "answer stuff and then shameless plug that erp at google is hiring you can",
    "start": "1852369",
    "end": "1858850"
  },
  {
    "text": "come talk to me and I'll get you some contacting for me any questions um you didn't talk about",
    "start": "1858850",
    "end": "1867350"
  },
  {
    "text": "there is an option of max concurrent streams which is defined as if I",
    "start": "1867350",
    "end": "1872640"
  },
  {
    "text": "remember by default to 100 which is quite low do you have any recommendation regarding that and how high we could go",
    "start": "1872640",
    "end": "1880320"
  },
  {
    "text": "of of multiple streams between a lot of clients and one server application yes I",
    "start": "1880320",
    "end": "1887100"
  },
  {
    "text": "should have mentioned that although honestly I can apply it in many different cases not just I guess that",
    "start": "1887100",
    "end": "1892560"
  },
  {
    "text": "mainly impacts long-lived are pcs and so there is a configuration setting max concurrent streams that comes from the",
    "start": "1892560",
    "end": "1899850"
  },
  {
    "text": "HTTP 2 spec GRC has it as infinite right now for all all of our main implementations",
    "start": "1899850",
    "end": "1905760"
  },
  {
    "text": "but lots of proxies will go ahead and choose something less than that and so",
    "start": "1905760",
    "end": "1910830"
  },
  {
    "text": "like if you're using some services with some Google API services you might see",
    "start": "1910830",
    "end": "1916110"
  },
  {
    "text": "100 100 is pretty common as its specifies the the it would be how does",
    "start": "1916110",
    "end": "1921630"
  },
  {
    "text": "the spec put it basically it should not be something less than 100 or something sort of wishy-washy like that and so a",
    "start": "1921630",
    "end": "1928590"
  },
  {
    "text": "lot of a lot of proxies want something really small and so they chose 100 yes",
    "start": "1928590",
    "end": "1933780"
  },
  {
    "text": "anytime you're doing lots of concurrent RBC's and especially long-lived if",
    "start": "1933780",
    "end": "1939210"
  },
  {
    "text": "you're doing something on the order of hundreds or thousands of these you could run into that problem and then the the",
    "start": "1939210",
    "end": "1945170"
  },
  {
    "text": "requirements that you open up multiple channels or get your PC to open up multiple connections",
    "start": "1945170",
    "end": "1951470"
  },
  {
    "text": "so the question is if the flow control is back pressure handling is it something automatic or or is it do you",
    "start": "1963350",
    "end": "1971100"
  },
  {
    "text": "have to manage it that'll vary per in language it's sort of an API feature of",
    "start": "1971100",
    "end": "1976740"
  },
  {
    "text": "how does that get exposed so if it's a blocking API it's gonna totally just be",
    "start": "1976740",
    "end": "1982830"
  },
  {
    "text": "just going to slow you down by blocking some of the async api's have a you can",
    "start": "1982830",
    "end": "1988770"
  },
  {
    "text": "only have one outstanding message so until it tells you it is processed that message you can send another one that is",
    "start": "1988770",
    "end": "1994910"
  },
  {
    "text": "another form of it in Java specifically it's its advisory so the idea was that",
    "start": "1994910",
    "end": "2004480"
  },
  {
    "text": "if you already have a message might as well tlg or if you see about it so we're not going to get too upset but if you're",
    "start": "2004480",
    "end": "2011179"
  },
  {
    "text": "not watching the flow control and if you're not looking at say if you don't ask your PC hey are you ready for more",
    "start": "2011179",
    "end": "2016750"
  },
  {
    "text": "then you'll just end up buffering a lot locally but it'll it'll bubble up",
    "start": "2016750",
    "end": "2022400"
  },
  {
    "text": "differently in each language so that does you you do need to look into that but it'll vary yeah what tools do you",
    "start": "2022400",
    "end": "2031160"
  },
  {
    "text": "use to debug on the wire when you're trying to find these issues which are PC",
    "start": "2031160",
    "end": "2036610"
  },
  {
    "text": "could the question was about how do you debug on the wire yeah what I can't use fiddler anymore it's like what sort of",
    "start": "2036610",
    "end": "2042740"
  },
  {
    "text": "things do you use to debug in general or or streaming or long lives specifically",
    "start": "2042740",
    "end": "2048919"
  },
  {
    "text": "I would say long live streaming complex cases I think in general I I wouldn't do",
    "start": "2048919",
    "end": "2057138"
  },
  {
    "text": "it as much on the wire level I would look more at the events that are coming higher in the which messages are being",
    "start": "2057139",
    "end": "2064820"
  },
  {
    "text": "received a lot of stuff can happen on the wire and you don't necessarily need",
    "start": "2064820",
    "end": "2069950"
  },
  {
    "text": "to be an HTTP 2 expert to follow along if things if you get that deadlock issue",
    "start": "2069950",
    "end": "2075648"
  },
  {
    "text": "then that is something you could look at and like oh the window size is different in this case versus ask case there are",
    "start": "2075649",
    "end": "2083120"
  },
  {
    "text": "some debug flags you can put in to see all the HTTP 2 frames unfortunately",
    "start": "2083120",
    "end": "2090320"
  },
  {
    "text": "we've got all very per language so at times you can do generally I try to avoid needing to",
    "start": "2090320",
    "end": "2097730"
  },
  {
    "text": "suggest someone to do that because that's quite involved but I think Oh",
    "start": "2097730",
    "end": "2102799"
  },
  {
    "text": "commonly just at the G RPC layer what the api's provide if you just logged out yourself or I turn on the frame logging",
    "start": "2102799",
    "end": "2111049"
  },
  {
    "text": "and you sort of ignore a lot of the HTTP to stuff you can get pretty far but",
    "start": "2111049",
    "end": "2116329"
  },
  {
    "text": "tracing and Status are also great or tracing there's also pretty good for that yes hi",
    "start": "2116329",
    "end": "2124640"
  },
  {
    "text": "my questions related to if you have a service somewhere and you can't invoke a",
    "start": "2124640",
    "end": "2130279"
  },
  {
    "text": "normal unary RPC to it like let's say it's behind a NAT and you can't punch a",
    "start": "2130279",
    "end": "2135319"
  },
  {
    "text": "hole through that net is using a streaming RPC an acceptable pattern that",
    "start": "2135319",
    "end": "2141079"
  },
  {
    "text": "you've seen used for getting around that sort of thing so you can listen for requests in the opposite direction so",
    "start": "2141079",
    "end": "2147940"
  },
  {
    "text": "it's this is really the you mentioned listening to request in the opposite direction that's I can't use case that",
    "start": "2147940",
    "end": "2154339"
  },
  {
    "text": "comes up pretty frequently it's not everyone having to deal with it but whenever you're needing to deal with it",
    "start": "2154339",
    "end": "2160009"
  },
  {
    "text": "really needing to deal with it and so this is basically the core problem is the client and server are reversed who's",
    "start": "2160009",
    "end": "2166009"
  },
  {
    "text": "requesting who to do what the client can access the the the real client can't",
    "start": "2166009",
    "end": "2172849"
  },
  {
    "text": "access the real server and so you you sort of have to do some hole punching yes you can totally you streaming for",
    "start": "2172849",
    "end": "2178069"
  },
  {
    "text": "that that turns out that turns out to be typically by Die full-duplex streaming which is the more complex case I was",
    "start": "2178069",
    "end": "2183829"
  },
  {
    "text": "mentioning but yes that is totally something that that is totally a use case to do full bi-directional streaming",
    "start": "2183829",
    "end": "2189950"
  },
  {
    "text": "do you foresee any type of updates to G RPC in the future where that sort of use",
    "start": "2189950",
    "end": "2195529"
  },
  {
    "text": "case is abstracted away so you don't have to actually deal with streaming RPC and you can define it as unary RPC not",
    "start": "2195529",
    "end": "2204549"
  },
  {
    "text": "so I I have a proof of concept up here for Java I know it's easy to do in and",
    "start": "2204549",
    "end": "2210019"
  },
  {
    "text": "go as well I worked on it a little bit there was this idea of basically making a reverse tunnel we would have one RPC",
    "start": "2210019",
    "end": "2217430"
  },
  {
    "text": "that would be full duplex PI die and then we end up just using that as a carrier for a bunch of bytes and then it",
    "start": "2217430",
    "end": "2224930"
  },
  {
    "text": "ends up layering HTTP to engineer if you see on top of that and that gets you a pretty good",
    "start": "2224930",
    "end": "2230400"
  },
  {
    "text": "inversion that I I ran out of some time as with that installed a little bit but",
    "start": "2230400",
    "end": "2235470"
  },
  {
    "text": "I think there's been a quiet there's a issue open for that that it receives a reasonable amount of pings because I",
    "start": "2235470",
    "end": "2240930"
  },
  {
    "text": "requested people to ping it so there has been some look at that it's totally",
    "start": "2240930",
    "end": "2246420"
  },
  {
    "text": "feasible and I've got a proof of concept that technique that we get totally works the question is more just protocol",
    "start": "2246420",
    "end": "2253320"
  },
  {
    "text": "details of hoping to do there there's a couple decisions to make and that's something that a cup would be helpful to",
    "start": "2253320",
    "end": "2259860"
  },
  {
    "text": "have a little bit of community involvement with because you basically need the client to register with a",
    "start": "2259860",
    "end": "2266370"
  },
  {
    "text": "server and exactly what information it provides how fancy or simple is that",
    "start": "2266370",
    "end": "2271500"
  },
  {
    "text": "there's that's most of where the therm aining design questions are thank you",
    "start": "2271500",
    "end": "2277850"
  },
  {
    "text": "only one more question hi is the streaming and long-lived operations",
    "start": "2277850",
    "end": "2284310"
  },
  {
    "text": "supported in all the language bindings so yes the the streaming and long-lived",
    "start": "2284310",
    "end": "2291290"
  },
  {
    "text": "are PCs are supported in all the clients it is something though that whenever you start using share PC web those sorts of",
    "start": "2291290",
    "end": "2298110"
  },
  {
    "text": "things that you may not get so for example by die you don't you don't get full by die full duplex streaming and",
    "start": "2298110",
    "end": "2304380"
  },
  {
    "text": "your PC web but all the other I'll say native implementations so the the main",
    "start": "2304380",
    "end": "2311520"
  },
  {
    "text": "nine they support all this thanks",
    "start": "2311520",
    "end": "2317390"
  }
]