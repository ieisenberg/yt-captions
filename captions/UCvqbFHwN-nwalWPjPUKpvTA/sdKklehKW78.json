[
  {
    "start": "0",
    "end": "41000"
  },
  {
    "text": "hi everyone thanks for joining our chat",
    "start": "1360",
    "end": "3840"
  },
  {
    "text": "today about xts relay",
    "start": "3840",
    "end": "5600"
  },
  {
    "text": "it's a new project in the onboarding",
    "start": "5600",
    "end": "7279"
  },
  {
    "text": "system and we are very excited about",
    "start": "7279",
    "end": "9200"
  },
  {
    "text": "introducing it",
    "start": "9200",
    "end": "10639"
  },
  {
    "text": "we are hoping to tell a story about how",
    "start": "10639",
    "end": "12559"
  },
  {
    "text": "it started the problems it's trying to",
    "start": "12559",
    "end": "14880"
  },
  {
    "text": "solve",
    "start": "14880",
    "end": "15519"
  },
  {
    "text": "and we're hoping to answer some of the",
    "start": "15519",
    "end": "17440"
  },
  {
    "text": "open questions",
    "start": "17440",
    "end": "20240"
  },
  {
    "text": "i'm jyoti and i'm here with just today",
    "start": "21279",
    "end": "24480"
  },
  {
    "text": "we both work in the networking team at",
    "start": "24480",
    "end": "26400"
  },
  {
    "text": "lyft we are the maintainers of the xts",
    "start": "26400",
    "end": "29039"
  },
  {
    "text": "relay project",
    "start": "29039",
    "end": "30160"
  },
  {
    "text": "as well as the new maintenance on the go",
    "start": "30160",
    "end": "32078"
  },
  {
    "text": "control plane",
    "start": "32079",
    "end": "33680"
  },
  {
    "text": "our contact information is mentioned on",
    "start": "33680",
    "end": "36320"
  },
  {
    "text": "the slide",
    "start": "36320",
    "end": "37040"
  },
  {
    "text": "and would be very happy to connect and",
    "start": "37040",
    "end": "39040"
  },
  {
    "text": "engage further after the talk",
    "start": "39040",
    "end": "42480"
  },
  {
    "text": "everything has a history and we are not",
    "start": "42640",
    "end": "44399"
  },
  {
    "text": "different the networking team at left",
    "start": "44399",
    "end": "46800"
  },
  {
    "text": "has been maintaining the lift edge",
    "start": "46800",
    "end": "48399"
  },
  {
    "text": "and service mesh over numerous years the",
    "start": "48399",
    "end": "51199"
  },
  {
    "text": "lift compute architecture has evolved",
    "start": "51199",
    "end": "52800"
  },
  {
    "text": "from vm based",
    "start": "52800",
    "end": "54000"
  },
  {
    "text": "asgs to kubernetes initial adapters of",
    "start": "54000",
    "end": "57600"
  },
  {
    "text": "kubernetes slowly tripled into the match",
    "start": "57600",
    "end": "60000"
  },
  {
    "text": "as the platform became more robust the",
    "start": "60000",
    "end": "62320"
  },
  {
    "text": "adoption became more widespread",
    "start": "62320",
    "end": "64320"
  },
  {
    "text": "in order to maintain one way of doing",
    "start": "64320",
    "end": "66159"
  },
  {
    "text": "things the leadership also",
    "start": "66159",
    "end": "67600"
  },
  {
    "text": "made a mandate to move all services to",
    "start": "67600",
    "end": "69680"
  },
  {
    "text": "communities by the end of williamson",
    "start": "69680",
    "end": "72400"
  },
  {
    "text": "lift service discovery based on the open",
    "start": "72400",
    "end": "74400"
  },
  {
    "text": "source discovery service",
    "start": "74400",
    "end": "76400"
  },
  {
    "text": "was accompanied by a kubernetes spot",
    "start": "76400",
    "end": "78640"
  },
  {
    "text": "informal model",
    "start": "78640",
    "end": "80159"
  },
  {
    "text": "the envoy control plane slowly evolved",
    "start": "80159",
    "end": "83040"
  },
  {
    "text": "to",
    "start": "83040",
    "end": "83600"
  },
  {
    "text": "accommodate both mechanisms there were",
    "start": "83600",
    "end": "86159"
  },
  {
    "text": "talks at",
    "start": "86159",
    "end": "86960"
  },
  {
    "text": "con san diego 2019 and amsterdam 2720",
    "start": "86960",
    "end": "90560"
  },
  {
    "text": "which described more about the service",
    "start": "90560",
    "end": "92400"
  },
  {
    "text": "discovery architecture",
    "start": "92400",
    "end": "94479"
  },
  {
    "text": "so much so that by the end of 2019",
    "start": "94479",
    "end": "97680"
  },
  {
    "text": "25 of the services were migrated to",
    "start": "97680",
    "end": "99920"
  },
  {
    "text": "qualities",
    "start": "99920",
    "end": "100720"
  },
  {
    "text": "and were running production workloads",
    "start": "100720",
    "end": "103040"
  },
  {
    "text": "this was enough scale",
    "start": "103040",
    "end": "104320"
  },
  {
    "text": "for ugly incidents to occur and at the",
    "start": "104320",
    "end": "106960"
  },
  {
    "text": "center of it all",
    "start": "106960",
    "end": "108079"
  },
  {
    "text": "was lyft's homegrown controlling by 2019",
    "start": "108079",
    "end": "111759"
  },
  {
    "text": "december",
    "start": "111759",
    "end": "112560"
  },
  {
    "text": "it was clear that the current",
    "start": "112560",
    "end": "113759"
  },
  {
    "text": "architecture won't scale if more",
    "start": "113759",
    "end": "115439"
  },
  {
    "text": "services",
    "start": "115439",
    "end": "116159"
  },
  {
    "text": "kept moving to communities he made point",
    "start": "116159",
    "end": "118640"
  },
  {
    "text": "in time sixes",
    "start": "118640",
    "end": "119759"
  },
  {
    "text": "to keep the systems running and at the",
    "start": "119759",
    "end": "121680"
  },
  {
    "text": "same time came up with the novel",
    "start": "121680",
    "end": "123360"
  },
  {
    "text": "approach",
    "start": "123360",
    "end": "123840"
  },
  {
    "text": "of managing service with discovery it's",
    "start": "123840",
    "end": "126479"
  },
  {
    "text": "called next instrument",
    "start": "126479",
    "end": "129840"
  },
  {
    "text": "briefly describe a bit if about the",
    "start": "130399",
    "end": "132720"
  },
  {
    "text": "before and after model",
    "start": "132720",
    "end": "134000"
  },
  {
    "text": "of the of how service discovery works",
    "start": "134000",
    "end": "136640"
  },
  {
    "text": "left",
    "start": "136640",
    "end": "137520"
  },
  {
    "text": "we have a homegrown control plane based",
    "start": "137520",
    "end": "139520"
  },
  {
    "text": "on the door control plane",
    "start": "139520",
    "end": "141520"
  },
  {
    "text": "subscribes to endpoint updates from vm",
    "start": "141520",
    "end": "143680"
  },
  {
    "text": "based discovery service",
    "start": "143680",
    "end": "145360"
  },
  {
    "text": "and kubernetes api server it also",
    "start": "145360",
    "end": "147840"
  },
  {
    "text": "subscribes to cluster updates",
    "start": "147840",
    "end": "149680"
  },
  {
    "text": "from a relatively low flux s3 files",
    "start": "149680",
    "end": "153680"
  },
  {
    "text": "the control plane is connected to the",
    "start": "153680",
    "end": "155440"
  },
  {
    "text": "on-voice height cards via grpc",
    "start": "155440",
    "end": "158560"
  },
  {
    "text": "these sidecars could be on literacy vms",
    "start": "158560",
    "end": "161519"
  },
  {
    "text": "or containers on the phones",
    "start": "161519",
    "end": "165040"
  },
  {
    "text": "so far so good one striking difference",
    "start": "166480",
    "end": "169200"
  },
  {
    "text": "between the legacy and the kubernetes",
    "start": "169200",
    "end": "170800"
  },
  {
    "text": "stack",
    "start": "170800",
    "end": "171360"
  },
  {
    "text": "was that vms take minutes to spin up",
    "start": "171360",
    "end": "173920"
  },
  {
    "text": "while the pods",
    "start": "173920",
    "end": "174800"
  },
  {
    "text": "take a few seconds to be ready this",
    "start": "174800",
    "end": "177360"
  },
  {
    "text": "gives the services an advantage",
    "start": "177360",
    "end": "179120"
  },
  {
    "text": "to keep the instance counts low and",
    "start": "179120",
    "end": "181120"
  },
  {
    "text": "aggressively scale up when traffic",
    "start": "181120",
    "end": "182959"
  },
  {
    "text": "spikes",
    "start": "182959",
    "end": "184239"
  },
  {
    "text": "this pattern causes more services to",
    "start": "184239",
    "end": "186480"
  },
  {
    "text": "scale up at each morning",
    "start": "186480",
    "end": "188239"
  },
  {
    "text": "and evening commute hours and then",
    "start": "188239",
    "end": "190640"
  },
  {
    "text": "quickly scale down too",
    "start": "190640",
    "end": "193760"
  },
  {
    "text": "we like most control planes use on voice",
    "start": "194480",
    "end": "198720"
  },
  {
    "text": "state of the world xps",
    "start": "198720",
    "end": "200319"
  },
  {
    "text": "this means if there's a service a having",
    "start": "200319",
    "end": "202560"
  },
  {
    "text": "an end point",
    "start": "202560",
    "end": "203680"
  },
  {
    "text": "and a dependent uh service b with m n",
    "start": "203680",
    "end": "206720"
  },
  {
    "text": "points",
    "start": "206720",
    "end": "207920"
  },
  {
    "text": "a new part coming up in service b will",
    "start": "207920",
    "end": "210319"
  },
  {
    "text": "cause at least",
    "start": "210319",
    "end": "211280"
  },
  {
    "text": "m membership updates on service a based",
    "start": "211280",
    "end": "213920"
  },
  {
    "text": "on how much each service scales",
    "start": "213920",
    "end": "215840"
  },
  {
    "text": "this quickly becomes an m crossing",
    "start": "215840",
    "end": "217599"
  },
  {
    "text": "problem and causes too many updates",
    "start": "217599",
    "end": "219680"
  },
  {
    "text": "across the hundreds of services in the",
    "start": "219680",
    "end": "221519"
  },
  {
    "text": "mesh",
    "start": "221519",
    "end": "222640"
  },
  {
    "text": "these operations come at a cost of",
    "start": "222640",
    "end": "224159"
  },
  {
    "text": "elevated cpu they frequently encountered",
    "start": "224159",
    "end": "227040"
  },
  {
    "text": "long periods of elevated cpu on the",
    "start": "227040",
    "end": "228879"
  },
  {
    "text": "control plane instances",
    "start": "228879",
    "end": "230400"
  },
  {
    "text": "and this caused some bad output outages",
    "start": "230400",
    "end": "234560"
  },
  {
    "text": "next such a huge influx of new parts",
    "start": "235920",
    "end": "238239"
  },
  {
    "text": "causes all parts to create new",
    "start": "238239",
    "end": "239840"
  },
  {
    "text": "connections to the control plane",
    "start": "239840",
    "end": "241920"
  },
  {
    "text": "at scale connection management becomes a",
    "start": "241920",
    "end": "244159"
  },
  {
    "text": "bottleneck",
    "start": "244159",
    "end": "245040"
  },
  {
    "text": "it has a cost which adds up quickly and",
    "start": "245040",
    "end": "247519"
  },
  {
    "text": "causes cpu pressure",
    "start": "247519",
    "end": "249200"
  },
  {
    "text": "since the control plane was vm based it",
    "start": "249200",
    "end": "251360"
  },
  {
    "text": "could not scale proportionally",
    "start": "251360",
    "end": "255840"
  },
  {
    "text": "the interesting part starts next it is a",
    "start": "256000",
    "end": "258400"
  },
  {
    "text": "control plane's responsibility to",
    "start": "258400",
    "end": "260079"
  },
  {
    "text": "understand the update",
    "start": "260079",
    "end": "261680"
  },
  {
    "text": "pack the information in the grpc object",
    "start": "261680",
    "end": "264080"
  },
  {
    "text": "and send that",
    "start": "264080",
    "end": "264800"
  },
  {
    "text": "across the dependent sidecars this",
    "start": "264800",
    "end": "267040"
  },
  {
    "text": "process needs a grpc payload to be",
    "start": "267040",
    "end": "268800"
  },
  {
    "text": "serialized into network bytes and sent",
    "start": "268800",
    "end": "271600"
  },
  {
    "text": "serialization is a cpu intensive",
    "start": "271600",
    "end": "273280"
  },
  {
    "text": "operation and we noticed that as more",
    "start": "273280",
    "end": "275360"
  },
  {
    "text": "and more services adopted communities",
    "start": "275360",
    "end": "277600"
  },
  {
    "text": "the control plane cpu was through the",
    "start": "277600",
    "end": "279199"
  },
  {
    "text": "roof when everything scaled up and down",
    "start": "279199",
    "end": "281440"
  },
  {
    "text": "the scaled down characteristics is very",
    "start": "281440",
    "end": "283360"
  },
  {
    "text": "similar to scalar and suffers from the",
    "start": "283360",
    "end": "285120"
  },
  {
    "text": "same symptoms",
    "start": "285120",
    "end": "287759"
  },
  {
    "text": "the next question is why is high cpu bad",
    "start": "287759",
    "end": "291680"
  },
  {
    "text": "high cpu usage causes throttling and",
    "start": "291680",
    "end": "294560"
  },
  {
    "text": "slows down the system",
    "start": "294560",
    "end": "296320"
  },
  {
    "text": "the payload cannot be serialized and",
    "start": "296320",
    "end": "298400"
  },
  {
    "text": "sent fast enough to the sidecars",
    "start": "298400",
    "end": "300800"
  },
  {
    "text": "this happens while there are more",
    "start": "300800",
    "end": "302880"
  },
  {
    "text": "payloads still getting queued",
    "start": "302880",
    "end": "305680"
  },
  {
    "text": "the endpoint discovery at left does not",
    "start": "305680",
    "end": "307600"
  },
  {
    "text": "have a durable storage",
    "start": "307600",
    "end": "309120"
  },
  {
    "text": "and any missed updates would mean",
    "start": "309120",
    "end": "311280"
  },
  {
    "text": "membership information",
    "start": "311280",
    "end": "312560"
  },
  {
    "text": "to diverge and become stale too",
    "start": "312560",
    "end": "315840"
  },
  {
    "text": "a spare membership can cause incorrect",
    "start": "315840",
    "end": "318080"
  },
  {
    "text": "routing and panic loading in your body",
    "start": "318080",
    "end": "325199"
  },
  {
    "text": "while all these problems were happening",
    "start": "325199",
    "end": "326880"
  },
  {
    "text": "we spun into action",
    "start": "326880",
    "end": "330000"
  },
  {
    "text": "one of the first approach was to move",
    "start": "330400",
    "end": "331919"
  },
  {
    "text": "the control plane infrastructure took",
    "start": "331919",
    "end": "333759"
  },
  {
    "text": "abilities",
    "start": "333759",
    "end": "334800"
  },
  {
    "text": "so that it could scale quickly and",
    "start": "334800",
    "end": "337280"
  },
  {
    "text": "proportionally with other services",
    "start": "337280",
    "end": "339840"
  },
  {
    "text": "we could also scale up or pre-scale the",
    "start": "339840",
    "end": "342800"
  },
  {
    "text": "existing legacy control plane",
    "start": "342800",
    "end": "344639"
  },
  {
    "text": "before known events control plane parts",
    "start": "344639",
    "end": "347680"
  },
  {
    "text": "were deployed in the two-way disk",
    "start": "347680",
    "end": "349039"
  },
  {
    "text": "clusters so they could scale",
    "start": "349039",
    "end": "350840"
  },
  {
    "text": "proportionally",
    "start": "350840",
    "end": "353840"
  },
  {
    "text": "we changed the bokeh channels in the go",
    "start": "354000",
    "end": "356880"
  },
  {
    "text": "controls name",
    "start": "356880",
    "end": "358000"
  },
  {
    "text": "from unbuffered to buffer and buffer",
    "start": "358000",
    "end": "361280"
  },
  {
    "text": "channels",
    "start": "361280",
    "end": "361759"
  },
  {
    "text": "of length one for the state of the world",
    "start": "361759",
    "end": "364319"
  },
  {
    "text": "eds",
    "start": "364319",
    "end": "364960"
  },
  {
    "text": "the last update wins if one of the",
    "start": "364960",
    "end": "367280"
  },
  {
    "text": "sidecars network was getting",
    "start": "367280",
    "end": "368960"
  },
  {
    "text": "flow control and the pro control plane's",
    "start": "368960",
    "end": "371199"
  },
  {
    "text": "channel was blocked from making progress",
    "start": "371199",
    "end": "373440"
  },
  {
    "text": "we could keep the keep overriding the",
    "start": "373440",
    "end": "375120"
  },
  {
    "text": "latest update in the buffer channel",
    "start": "375120",
    "end": "377520"
  },
  {
    "text": "to maintain correctness",
    "start": "377520",
    "end": "381840"
  },
  {
    "text": "we performed flame graph analysis and",
    "start": "382160",
    "end": "384400"
  },
  {
    "text": "fixed a few wasteful relationships",
    "start": "384400",
    "end": "386639"
  },
  {
    "text": "both in go control plane library and in",
    "start": "386639",
    "end": "388880"
  },
  {
    "text": "our own private control plane",
    "start": "388880",
    "end": "392560"
  },
  {
    "text": "instantaneous in point update was cost",
    "start": "393199",
    "end": "395520"
  },
  {
    "text": "for everything",
    "start": "395520",
    "end": "396319"
  },
  {
    "text": "so we make sure that the rate limit and",
    "start": "396319",
    "end": "398160"
  },
  {
    "text": "batch kubernetes end point updates to a",
    "start": "398160",
    "end": "400400"
  },
  {
    "text": "configurable interval",
    "start": "400400",
    "end": "401680"
  },
  {
    "text": "say tens of seconds envoy is eventually",
    "start": "401680",
    "end": "404639"
  },
  {
    "text": "consistent",
    "start": "404639",
    "end": "405440"
  },
  {
    "text": "so this worked out fine for now although",
    "start": "405440",
    "end": "408000"
  },
  {
    "text": "we slowed down the membership",
    "start": "408000",
    "end": "409360"
  },
  {
    "text": "convergence",
    "start": "409360",
    "end": "410160"
  },
  {
    "text": "and made the control plan response times",
    "start": "410160",
    "end": "412080"
  },
  {
    "text": "flow",
    "start": "412080",
    "end": "413759"
  },
  {
    "text": "all of this has led us to think about a",
    "start": "413759",
    "end": "416479"
  },
  {
    "text": "different approach or service discovery",
    "start": "416479",
    "end": "418479"
  },
  {
    "text": "and we are calling it the xps relay i'll",
    "start": "418479",
    "end": "421120"
  },
  {
    "start": "419000",
    "end": "692000"
  },
  {
    "text": "take i'll",
    "start": "421120",
    "end": "422560"
  },
  {
    "text": "just talk about it",
    "start": "422560",
    "end": "425759"
  },
  {
    "text": "thanks jody so excess really is a",
    "start": "429440",
    "end": "431919"
  },
  {
    "text": "project that we started early in the",
    "start": "431919",
    "end": "433680"
  },
  {
    "text": "year",
    "start": "433680",
    "end": "434080"
  },
  {
    "text": "to address some of the stock gaps that",
    "start": "434080",
    "end": "436479"
  },
  {
    "text": "we mentioned",
    "start": "436479",
    "end": "438960"
  },
  {
    "text": "from its conception we had a few goals",
    "start": "441039",
    "end": "442880"
  },
  {
    "text": "in mind",
    "start": "442880",
    "end": "444240"
  },
  {
    "text": "first we wanted to be about entirely in",
    "start": "444240",
    "end": "446400"
  },
  {
    "text": "the open",
    "start": "446400",
    "end": "447520"
  },
  {
    "text": "so we spoken with a few companies in the",
    "start": "447520",
    "end": "450080"
  },
  {
    "text": "past months",
    "start": "450080",
    "end": "450800"
  },
  {
    "text": "operating at a similar or larger scale",
    "start": "450800",
    "end": "452720"
  },
  {
    "text": "than lyft and there's always this",
    "start": "452720",
    "end": "454720"
  },
  {
    "text": "reoccurring theme",
    "start": "454720",
    "end": "455680"
  },
  {
    "text": "and questions of what is the standard",
    "start": "455680",
    "end": "458080"
  },
  {
    "text": "for control plane implementation",
    "start": "458080",
    "end": "460560"
  },
  {
    "text": "will lift be open sourcing our control",
    "start": "460560",
    "end": "462840"
  },
  {
    "text": "plane",
    "start": "462840",
    "end": "464560"
  },
  {
    "text": "and control plane development is",
    "start": "464560",
    "end": "466400"
  },
  {
    "text": "difficult and there's",
    "start": "466400",
    "end": "467759"
  },
  {
    "text": "a lot of intricacies to get it operating",
    "start": "467759",
    "end": "469919"
  },
  {
    "text": "at a level that",
    "start": "469919",
    "end": "471120"
  },
  {
    "text": "behaves correctly for a particular",
    "start": "471120",
    "end": "472720"
  },
  {
    "text": "company's infrastructure",
    "start": "472720",
    "end": "475280"
  },
  {
    "text": "one of the goals with xcs relay is to be",
    "start": "475280",
    "end": "477199"
  },
  {
    "text": "able to abstract away the layers of",
    "start": "477199",
    "end": "479360"
  },
  {
    "text": "list control plane that we do believe is",
    "start": "479360",
    "end": "481840"
  },
  {
    "text": "shareable",
    "start": "481840",
    "end": "483520"
  },
  {
    "text": "and secondly we want this to be an out",
    "start": "483520",
    "end": "485599"
  },
  {
    "text": "of the box solution",
    "start": "485599",
    "end": "486879"
  },
  {
    "text": "that users can run and operate with",
    "start": "486879",
    "end": "489520"
  },
  {
    "text": "minimal knobs xcs really also will",
    "start": "489520",
    "end": "493039"
  },
  {
    "text": "implement a popular and well supported",
    "start": "493039",
    "end": "496000"
  },
  {
    "text": "open source go control plane library",
    "start": "496000",
    "end": "499759"
  },
  {
    "text": "so we see xcs relay as a cdn for xds",
    "start": "502000",
    "end": "506639"
  },
  {
    "text": "initially it's an aggregation and",
    "start": "506639",
    "end": "508240"
  },
  {
    "text": "caching layer that's meant to reside",
    "start": "508240",
    "end": "509759"
  },
  {
    "text": "physically close to",
    "start": "509759",
    "end": "511199"
  },
  {
    "text": "xcs clients so those running on the same",
    "start": "511199",
    "end": "513680"
  },
  {
    "text": "region",
    "start": "513680",
    "end": "514399"
  },
  {
    "text": "data center etc",
    "start": "514399",
    "end": "517760"
  },
  {
    "text": "similar to general benefits of using a",
    "start": "517760",
    "end": "519360"
  },
  {
    "text": "cdn users of xcs relay can",
    "start": "519360",
    "end": "521518"
  },
  {
    "text": "benefit from faster delivery of xcs",
    "start": "521519",
    "end": "524159"
  },
  {
    "text": "responses",
    "start": "524159",
    "end": "524959"
  },
  {
    "text": "faster service uptime and reduce",
    "start": "524959",
    "end": "527279"
  },
  {
    "text": "bandwidth costs from caching",
    "start": "527279",
    "end": "529120"
  },
  {
    "text": "and other optimizations that we have for",
    "start": "529120",
    "end": "530800"
  },
  {
    "text": "this project",
    "start": "530800",
    "end": "533440"
  },
  {
    "text": "exes really will be configurable with",
    "start": "533760",
    "end": "535600"
  },
  {
    "text": "rule-based definitions in order to",
    "start": "535600",
    "end": "537279"
  },
  {
    "text": "specify a group of exes requests that",
    "start": "537279",
    "end": "539839"
  },
  {
    "text": "should get aggregated and cached to the",
    "start": "539839",
    "end": "542000"
  },
  {
    "text": "same key",
    "start": "542000",
    "end": "543600"
  },
  {
    "text": "xds really will maintain a grpc stream",
    "start": "543600",
    "end": "545760"
  },
  {
    "text": "to the control plane server for each of",
    "start": "545760",
    "end": "547519"
  },
  {
    "text": "the unique keys",
    "start": "547519",
    "end": "550240"
  },
  {
    "text": "so lyft's current control plane manages",
    "start": "553120",
    "end": "555360"
  },
  {
    "text": "multiple facets",
    "start": "555360",
    "end": "556720"
  },
  {
    "text": "pre-processing of multiple sources of",
    "start": "556720",
    "end": "558959"
  },
  {
    "text": "lift service metadata",
    "start": "558959",
    "end": "560640"
  },
  {
    "text": "in order to generate the xcs responses",
    "start": "560640",
    "end": "563760"
  },
  {
    "text": "pulling from legacy discovery mechanisms",
    "start": "563760",
    "end": "565760"
  },
  {
    "text": "in the kubernetes api server",
    "start": "565760",
    "end": "567360"
  },
  {
    "text": "in order to get endpoint information and",
    "start": "567360",
    "end": "570240"
  },
  {
    "text": "last but not least caching",
    "start": "570240",
    "end": "571920"
  },
  {
    "text": "and fanning out of discovery responses",
    "start": "571920",
    "end": "576000"
  },
  {
    "text": "excess really will pull out the",
    "start": "576000",
    "end": "578080"
  },
  {
    "text": "connection management and",
    "start": "578080",
    "end": "579440"
  },
  {
    "text": "caching aspect allowing the control",
    "start": "579440",
    "end": "582080"
  },
  {
    "text": "plane server to scale independently of",
    "start": "582080",
    "end": "583839"
  },
  {
    "text": "xcs clients",
    "start": "583839",
    "end": "586800"
  },
  {
    "text": "we're also making a lot of optimizations",
    "start": "587040",
    "end": "588800"
  },
  {
    "text": "into the transport layer",
    "start": "588800",
    "end": "590480"
  },
  {
    "text": "in order to make the connections low",
    "start": "590480",
    "end": "592000"
  },
  {
    "text": "latency and low throughput",
    "start": "592000",
    "end": "595600"
  },
  {
    "text": "excess relay will also have built-in",
    "start": "597600",
    "end": "599600"
  },
  {
    "text": "common control plane observability",
    "start": "599600",
    "end": "601360"
  },
  {
    "text": "mechanisms",
    "start": "601360",
    "end": "602720"
  },
  {
    "text": "including stats supporting multiple",
    "start": "602720",
    "end": "604720"
  },
  {
    "text": "syncs",
    "start": "604720",
    "end": "606000"
  },
  {
    "text": "error warning and debug level logs as",
    "start": "606000",
    "end": "608320"
  },
  {
    "text": "well as admin endpoints for",
    "start": "608320",
    "end": "610000"
  },
  {
    "text": "viewing the cache and other common",
    "start": "610000",
    "end": "612079"
  },
  {
    "text": "control plane",
    "start": "612079",
    "end": "613680"
  },
  {
    "text": "endpoint usability tools",
    "start": "613680",
    "end": "618720"
  },
  {
    "text": "so alongside upstream server retries xcs",
    "start": "618720",
    "end": "621200"
  },
  {
    "text": "relay",
    "start": "621200",
    "end": "621839"
  },
  {
    "text": "will implement mechanisms to stop a",
    "start": "621839",
    "end": "623839"
  },
  {
    "text": "thundering herd of requests from xcs",
    "start": "623839",
    "end": "625680"
  },
  {
    "text": "clients",
    "start": "625680",
    "end": "626480"
  },
  {
    "text": "through a queuing and rate limiting",
    "start": "626480",
    "end": "628079"
  },
  {
    "text": "mechanism",
    "start": "628079",
    "end": "630639"
  },
  {
    "text": "lastly we want to build xcs relay in a",
    "start": "630880",
    "end": "633279"
  },
  {
    "text": "way that",
    "start": "633279",
    "end": "634240"
  },
  {
    "text": "the components operate in a",
    "start": "634240",
    "end": "635519"
  },
  {
    "text": "plug-and-play manner",
    "start": "635519",
    "end": "638320"
  },
  {
    "text": "we'll get into a lot of ambitious goals",
    "start": "638320",
    "end": "640000"
  },
  {
    "text": "we have surrounding a general",
    "start": "640000",
    "end": "641519"
  },
  {
    "text": "relaying type component later but we",
    "start": "641519",
    "end": "644240"
  },
  {
    "text": "understand that a feature",
    "start": "644240",
    "end": "645839"
  },
  {
    "text": "rich set can be bloated for a company",
    "start": "645839",
    "end": "648079"
  },
  {
    "text": "that wants to",
    "start": "648079",
    "end": "649279"
  },
  {
    "text": "just run lightweight components for that",
    "start": "649279",
    "end": "651519"
  },
  {
    "text": "reason we're",
    "start": "651519",
    "end": "653040"
  },
  {
    "text": "conscious to make axios relay as",
    "start": "653040",
    "end": "655279"
  },
  {
    "text": "extensible as possible",
    "start": "655279",
    "end": "660480"
  },
  {
    "text": "so at the heart of xcs relay is",
    "start": "660480",
    "end": "663040"
  },
  {
    "text": "rule-based definitions for",
    "start": "663040",
    "end": "664720"
  },
  {
    "text": "request aggregation and caching and",
    "start": "664720",
    "end": "667519"
  },
  {
    "text": "here's an example where we've decided to",
    "start": "667519",
    "end": "669519"
  },
  {
    "text": "cache on",
    "start": "669519",
    "end": "670720"
  },
  {
    "text": "service and request type pairings",
    "start": "670720",
    "end": "675519"
  },
  {
    "text": "we use yaml structured match and result",
    "start": "675519",
    "end": "678560"
  },
  {
    "text": "compilers in order to create the unique",
    "start": "678560",
    "end": "681200"
  },
  {
    "text": "aggregated keys",
    "start": "681200",
    "end": "683600"
  },
  {
    "text": "we won't dive into the specifics here",
    "start": "683600",
    "end": "685440"
  },
  {
    "text": "because we'll be doing a demo",
    "start": "685440",
    "end": "686839"
  },
  {
    "text": "shortly",
    "start": "686839",
    "end": "689839"
  },
  {
    "start": "692000",
    "end": "935000"
  },
  {
    "text": "we thought that it would be easiest to",
    "start": "693200",
    "end": "695120"
  },
  {
    "text": "understand the architecture of",
    "start": "695120",
    "end": "696640"
  },
  {
    "text": "xcs relay by going through a workflow",
    "start": "696640",
    "end": "699120"
  },
  {
    "text": "diagram",
    "start": "699120",
    "end": "701519"
  },
  {
    "text": "so on the slide you can see the",
    "start": "701519",
    "end": "702959"
  },
  {
    "text": "workflows numbered one",
    "start": "702959",
    "end": "704880"
  },
  {
    "text": "all the way to six when discovery",
    "start": "704880",
    "end": "708399"
  },
  {
    "text": "requests first",
    "start": "708399",
    "end": "709360"
  },
  {
    "text": "make their way into xcs relay they go",
    "start": "709360",
    "end": "712480"
  },
  {
    "text": "through an",
    "start": "712480",
    "end": "713279"
  },
  {
    "text": "aggregator component this is the",
    "start": "713279",
    "end": "715760"
  },
  {
    "text": "component that takes the rules we",
    "start": "715760",
    "end": "717279"
  },
  {
    "text": "mentioned in the previous slide",
    "start": "717279",
    "end": "719440"
  },
  {
    "text": "and translates these requests into",
    "start": "719440",
    "end": "721680"
  },
  {
    "text": "unique discovery keys",
    "start": "721680",
    "end": "724880"
  },
  {
    "text": "so in this example we've chosen to map",
    "start": "725120",
    "end": "727200"
  },
  {
    "text": "both requests",
    "start": "727200",
    "end": "728639"
  },
  {
    "text": "to the same key using a combination of",
    "start": "728639",
    "end": "731519"
  },
  {
    "text": "the node id",
    "start": "731519",
    "end": "733040"
  },
  {
    "text": "the cluster and the request type",
    "start": "733040",
    "end": "737199"
  },
  {
    "text": "so once the aggregated key is generated",
    "start": "738959",
    "end": "741120"
  },
  {
    "text": "for a request",
    "start": "741120",
    "end": "742000"
  },
  {
    "text": "the request gets added into an in-memory",
    "start": "742000",
    "end": "744560"
  },
  {
    "text": "cache",
    "start": "744560",
    "end": "745839"
  },
  {
    "text": "with configurable ttl and size limits",
    "start": "745839",
    "end": "749920"
  },
  {
    "text": "if there is already a response in the",
    "start": "749920",
    "end": "751760"
  },
  {
    "text": "cache and the versioning is different",
    "start": "751760",
    "end": "754320"
  },
  {
    "text": "xds relay will immediately return the",
    "start": "754320",
    "end": "756959"
  },
  {
    "text": "discovery response",
    "start": "756959",
    "end": "758480"
  },
  {
    "text": "to the sender however",
    "start": "758480",
    "end": "761760"
  },
  {
    "text": "if there's not a response available or",
    "start": "761760",
    "end": "763839"
  },
  {
    "text": "the versioning is the same",
    "start": "763839",
    "end": "765680"
  },
  {
    "text": "xcs relay will maintain an open watch",
    "start": "765680",
    "end": "768079"
  },
  {
    "text": "for this request",
    "start": "768079",
    "end": "770720"
  },
  {
    "text": "by storing it in the cache and it will",
    "start": "770720",
    "end": "773279"
  },
  {
    "text": "wait for a new response from the",
    "start": "773279",
    "end": "774880"
  },
  {
    "text": "upstream management server",
    "start": "774880",
    "end": "778240"
  },
  {
    "text": "so excess really only ever maintains a",
    "start": "778320",
    "end": "780720"
  },
  {
    "text": "single grpc stream",
    "start": "780720",
    "end": "782880"
  },
  {
    "text": "per cache key and this coincides with",
    "start": "782880",
    "end": "785519"
  },
  {
    "text": "the first unique request",
    "start": "785519",
    "end": "787760"
  },
  {
    "text": "to log a cache entry for the key",
    "start": "787760",
    "end": "790800"
  },
  {
    "text": "the first request is propagated to the",
    "start": "790800",
    "end": "792800"
  },
  {
    "text": "upstream management server",
    "start": "792800",
    "end": "794560"
  },
  {
    "text": "through our upstream client and go",
    "start": "794560",
    "end": "796959"
  },
  {
    "text": "routines and channels",
    "start": "796959",
    "end": "798320"
  },
  {
    "text": "are used in order to",
    "start": "798320",
    "end": "801680"
  },
  {
    "text": "await discover responses from the",
    "start": "801680",
    "end": "803279"
  },
  {
    "text": "management server",
    "start": "803279",
    "end": "806079"
  },
  {
    "text": "upon a response from the server xcs",
    "start": "806079",
    "end": "808720"
  },
  {
    "text": "relay will then",
    "start": "808720",
    "end": "810000"
  },
  {
    "text": "fan out the response to all of the xcs",
    "start": "810000",
    "end": "812000"
  },
  {
    "text": "clients with an open watch in the cache",
    "start": "812000",
    "end": "814320"
  },
  {
    "text": "for the specified aggregated key",
    "start": "814320",
    "end": "817440"
  },
  {
    "text": "and then remove the watches from the",
    "start": "817440",
    "end": "818959"
  },
  {
    "text": "cache and all of these components are",
    "start": "818959",
    "end": "822399"
  },
  {
    "text": "orchestrated by what we internally call",
    "start": "822399",
    "end": "824560"
  },
  {
    "text": "the orchestrator",
    "start": "824560",
    "end": "829839"
  },
  {
    "text": "so at lyft we run a group of control",
    "start": "830720",
    "end": "832800"
  },
  {
    "text": "plane servers on each kubernetes cluster",
    "start": "832800",
    "end": "835519"
  },
  {
    "text": "with services distributed across each of",
    "start": "835519",
    "end": "838240"
  },
  {
    "text": "the clusters",
    "start": "838240",
    "end": "839519"
  },
  {
    "text": "and our control plane cluster was scaled",
    "start": "839519",
    "end": "841519"
  },
  {
    "text": "up to run on",
    "start": "841519",
    "end": "842560"
  },
  {
    "text": "multiple nodes in order to support the",
    "start": "842560",
    "end": "844399"
  },
  {
    "text": "number of online clients we had at lyft",
    "start": "844399",
    "end": "848079"
  },
  {
    "text": "with this new architecture we're running",
    "start": "848079",
    "end": "849760"
  },
  {
    "text": "a group of excess relays on each",
    "start": "849760",
    "end": "851519"
  },
  {
    "text": "kubernetes cluster",
    "start": "851519",
    "end": "853040"
  },
  {
    "text": "allowing us to scale down our control",
    "start": "853040",
    "end": "855680"
  },
  {
    "text": "plane server",
    "start": "855680",
    "end": "856800"
  },
  {
    "text": "and refocus the control plane core logic",
    "start": "856800",
    "end": "858720"
  },
  {
    "text": "on pre-processing and generating new xcs",
    "start": "858720",
    "end": "860959"
  },
  {
    "text": "responses",
    "start": "860959",
    "end": "862959"
  },
  {
    "text": "we're now also able to scale the",
    "start": "862959",
    "end": "865839"
  },
  {
    "text": "response generation",
    "start": "865839",
    "end": "867040"
  },
  {
    "text": "logic independently from the connection",
    "start": "867040",
    "end": "868959"
  },
  {
    "text": "management portion",
    "start": "868959",
    "end": "871839"
  },
  {
    "text": "in this specific example we've",
    "start": "871839",
    "end": "873519"
  },
  {
    "text": "aggregated the request for the location",
    "start": "873519",
    "end": "875279"
  },
  {
    "text": "service",
    "start": "875279",
    "end": "876000"
  },
  {
    "text": "and the request for the user service two",
    "start": "876000",
    "end": "878639"
  },
  {
    "text": "unique cache keys",
    "start": "878639",
    "end": "881040"
  },
  {
    "text": "access relay is responsible for caching",
    "start": "881040",
    "end": "883440"
  },
  {
    "text": "and optimize",
    "start": "883440",
    "end": "884399"
  },
  {
    "text": "response fan out so",
    "start": "884399",
    "end": "887519"
  },
  {
    "text": "this implies that the connections to the",
    "start": "887519",
    "end": "889120"
  },
  {
    "text": "upstream server is also a lot smaller",
    "start": "889120",
    "end": "891519"
  },
  {
    "text": "than",
    "start": "891519",
    "end": "892240"
  },
  {
    "text": "they would be without the relaying type",
    "start": "892240",
    "end": "896240"
  },
  {
    "text": "component",
    "start": "896839",
    "end": "899839"
  },
  {
    "text": "so lyft runs all of its infrastructure",
    "start": "900079",
    "end": "902320"
  },
  {
    "text": "on a giant vpc",
    "start": "902320",
    "end": "904480"
  },
  {
    "text": "but we envision other interesting",
    "start": "904480",
    "end": "906320"
  },
  {
    "text": "topologies",
    "start": "906320",
    "end": "907519"
  },
  {
    "text": "for running axis relay on hybrid clouds",
    "start": "907519",
    "end": "910240"
  },
  {
    "text": "and",
    "start": "910240",
    "end": "910839"
  },
  {
    "text": "multiple vpc infrastructure setups",
    "start": "910839",
    "end": "914320"
  },
  {
    "text": "so for example here's a topology where",
    "start": "914320",
    "end": "917600"
  },
  {
    "text": "we run a cluster of xts relay on-prem so",
    "start": "917600",
    "end": "921040"
  },
  {
    "text": "on this data center in another data",
    "start": "921040",
    "end": "922639"
  },
  {
    "text": "center physically close to",
    "start": "922639",
    "end": "924880"
  },
  {
    "text": "the onboard services while hosting the",
    "start": "924880",
    "end": "927279"
  },
  {
    "text": "control plane server",
    "start": "927279",
    "end": "928320"
  },
  {
    "text": "on a centralized vpc for faster services",
    "start": "928320",
    "end": "931519"
  },
  {
    "text": "discovery",
    "start": "931519",
    "end": "934079"
  },
  {
    "start": "935000",
    "end": "1347000"
  },
  {
    "text": "now we're going to get into a demo",
    "start": "935279",
    "end": "940880"
  },
  {
    "text": "all right in this demo we'll be running",
    "start": "940880",
    "end": "942639"
  },
  {
    "text": "a very simple setup with a management",
    "start": "942639",
    "end": "944560"
  },
  {
    "text": "server that just sends snapshots every",
    "start": "944560",
    "end": "946320"
  },
  {
    "text": "10 seconds",
    "start": "946320",
    "end": "947519"
  },
  {
    "text": "a xcs relay server and two onward",
    "start": "947519",
    "end": "950480"
  },
  {
    "text": "clients",
    "start": "950480",
    "end": "951759"
  },
  {
    "text": "so we'll begin by starting up our",
    "start": "951759",
    "end": "954240"
  },
  {
    "text": "management server",
    "start": "954240",
    "end": "957120"
  },
  {
    "text": "there we have it snapshots are now being",
    "start": "961199",
    "end": "962880"
  },
  {
    "text": "generated every 10 seconds",
    "start": "962880",
    "end": "965199"
  },
  {
    "text": "while that's running let's take a look",
    "start": "965199",
    "end": "966560"
  },
  {
    "text": "at some of our configuration files that",
    "start": "966560",
    "end": "968320"
  },
  {
    "text": "we'll be using for our onward clients",
    "start": "968320",
    "end": "969920"
  },
  {
    "text": "and exeus relay server",
    "start": "969920",
    "end": "973120"
  },
  {
    "text": "so we have two bootstrap files here for",
    "start": "977519",
    "end": "979199"
  },
  {
    "text": "the onward clients",
    "start": "979199",
    "end": "982399"
  },
  {
    "text": "and they're quite simple one of them has",
    "start": "982399",
    "end": "985040"
  },
  {
    "text": "the node id on the client one",
    "start": "985040",
    "end": "987279"
  },
  {
    "text": "the other one has the node id over",
    "start": "987279",
    "end": "988800"
  },
  {
    "text": "client two they both run on slightly",
    "start": "988800",
    "end": "991120"
  },
  {
    "text": "different ports",
    "start": "991120",
    "end": "992000"
  },
  {
    "text": "one runs on port nineteen thousand the",
    "start": "992000",
    "end": "994320"
  },
  {
    "text": "other one runs on port 9001",
    "start": "994320",
    "end": "997279"
  },
  {
    "text": "but everything else is the same most",
    "start": "997279",
    "end": "999680"
  },
  {
    "text": "notably is the",
    "start": "999680",
    "end": "1000959"
  },
  {
    "text": "cluster you notice that they both",
    "start": "1000959",
    "end": "1004079"
  },
  {
    "text": "share the same staging cluster which is",
    "start": "1004079",
    "end": "1006399"
  },
  {
    "text": "what we'll be using to define our xcs",
    "start": "1006399",
    "end": "1008560"
  },
  {
    "text": "relay aggregation rules later on",
    "start": "1008560",
    "end": "1011199"
  },
  {
    "text": "and they both designate the same control",
    "start": "1011199",
    "end": "1012639"
  },
  {
    "text": "plane server to point to xcs relay",
    "start": "1012639",
    "end": "1014959"
  },
  {
    "text": "running on port",
    "start": "1014959",
    "end": "1016360"
  },
  {
    "text": "9991",
    "start": "1016360",
    "end": "1019360"
  },
  {
    "text": "let's just quickly confirm the bootstrap",
    "start": "1019759",
    "end": "1021440"
  },
  {
    "text": "file of our second onward client",
    "start": "1021440",
    "end": "1023440"
  },
  {
    "text": "as you can see it's identical except for",
    "start": "1023440",
    "end": "1026079"
  },
  {
    "text": "the",
    "start": "1026079",
    "end": "1026480"
  },
  {
    "text": "node id and the",
    "start": "1026480",
    "end": "1030240"
  },
  {
    "text": "the port which it's running on",
    "start": "1030400",
    "end": "1033679"
  },
  {
    "text": "so now let's take a look at some of our",
    "start": "1034559",
    "end": "1036000"
  },
  {
    "text": "xcs relay configuration file",
    "start": "1036000",
    "end": "1039839"
  },
  {
    "text": "so as i mentioned xcs relay is going to",
    "start": "1040400",
    "end": "1042480"
  },
  {
    "text": "be running on port 9991",
    "start": "1042480",
    "end": "1045520"
  },
  {
    "text": "it's going to be pointing to our simple",
    "start": "1045520",
    "end": "1047438"
  },
  {
    "text": "control plane server",
    "start": "1047439",
    "end": "1048720"
  },
  {
    "text": "that's running on port 18000 and there's",
    "start": "1048720",
    "end": "1051600"
  },
  {
    "text": "some other miscellaneous server metadata",
    "start": "1051600",
    "end": "1053440"
  },
  {
    "text": "including the log level",
    "start": "1053440",
    "end": "1054720"
  },
  {
    "text": "cache sizing the admin endpoint etc",
    "start": "1054720",
    "end": "1059679"
  },
  {
    "text": "finally let's take a look at the xcs",
    "start": "1061120",
    "end": "1063440"
  },
  {
    "text": "relay aggregation rules",
    "start": "1063440",
    "end": "1066240"
  },
  {
    "text": "and these rules might look quite",
    "start": "1066240",
    "end": "1067840"
  },
  {
    "text": "intimidating at first but it's actually",
    "start": "1067840",
    "end": "1069600"
  },
  {
    "text": "pretty easy to understand",
    "start": "1069600",
    "end": "1072720"
  },
  {
    "text": "so it's going to end up generating keys",
    "start": "1072720",
    "end": "1076000"
  },
  {
    "text": "that look like staging",
    "start": "1076000",
    "end": "1077039"
  },
  {
    "text": "underscore eds or staging underscore cds",
    "start": "1077039",
    "end": "1080559"
  },
  {
    "text": "and how that works is via these",
    "start": "1080559",
    "end": "1083280"
  },
  {
    "text": "fragments so",
    "start": "1083280",
    "end": "1084400"
  },
  {
    "text": "for the first fragments we look at these",
    "start": "1084400",
    "end": "1086480"
  },
  {
    "text": "request types that fall under",
    "start": "1086480",
    "end": "1088400"
  },
  {
    "text": "lds cds eds or rds",
    "start": "1088400",
    "end": "1091600"
  },
  {
    "text": "and we apply this regex match and",
    "start": "1091600",
    "end": "1094320"
  },
  {
    "text": "replace operation",
    "start": "1094320",
    "end": "1095440"
  },
  {
    "text": "on the node cluster",
    "start": "1095440",
    "end": "1098720"
  },
  {
    "text": "so because in our armor bootstrap files",
    "start": "1098720",
    "end": "1101280"
  },
  {
    "text": "we have the node cluster set to staging",
    "start": "1101280",
    "end": "1103840"
  },
  {
    "text": "this will actually always result in",
    "start": "1103840",
    "end": "1105600"
  },
  {
    "text": "staging as the first",
    "start": "1105600",
    "end": "1107200"
  },
  {
    "text": "cache key fragment",
    "start": "1107200",
    "end": "1110159"
  },
  {
    "text": "for the second fragment it's just a",
    "start": "1110640",
    "end": "1112080"
  },
  {
    "text": "static constant",
    "start": "1112080",
    "end": "1114000"
  },
  {
    "text": "so if it's of type listener then",
    "start": "1114000",
    "end": "1117039"
  },
  {
    "text": "we append lds as the second",
    "start": "1117039",
    "end": "1120480"
  },
  {
    "text": "string fragment with cds",
    "start": "1120480",
    "end": "1123679"
  },
  {
    "text": "we append this static constant cds",
    "start": "1123679",
    "end": "1127600"
  },
  {
    "text": "so forth and for the very last fragment",
    "start": "1127600",
    "end": "1129760"
  },
  {
    "text": "if this",
    "start": "1129760",
    "end": "1130799"
  },
  {
    "text": "request type is of type welt",
    "start": "1130799",
    "end": "1132320"
  },
  {
    "text": "configuration we're going to append an",
    "start": "1132320",
    "end": "1134640"
  },
  {
    "text": "additional fragment that",
    "start": "1134640",
    "end": "1136559"
  },
  {
    "text": "has the resource name",
    "start": "1136559",
    "end": "1140159"
  },
  {
    "text": "so again all that is saying is that",
    "start": "1142960",
    "end": "1144960"
  },
  {
    "text": "we're going to end up with these",
    "start": "1144960",
    "end": "1146160"
  },
  {
    "text": "aggregated cache keys in fcs relay that",
    "start": "1146160",
    "end": "1149039"
  },
  {
    "text": "they end up looking like",
    "start": "1149039",
    "end": "1150080"
  },
  {
    "text": "staging cds or staging eds",
    "start": "1150080",
    "end": "1153760"
  },
  {
    "text": "etc",
    "start": "1153760",
    "end": "1156320"
  },
  {
    "text": "all right so now let's start our xcs",
    "start": "1156960",
    "end": "1160000"
  },
  {
    "text": "relay server",
    "start": "1160000",
    "end": "1162080"
  },
  {
    "text": "using those bootstrap configuration and",
    "start": "1162080",
    "end": "1164000"
  },
  {
    "text": "aggregation rules that we just talked",
    "start": "1164000",
    "end": "1165679"
  },
  {
    "text": "about",
    "start": "1165679",
    "end": "1167919"
  },
  {
    "text": "okay so",
    "start": "1169039",
    "end": "1173519"
  },
  {
    "text": "if we were to curl the xcs relay",
    "start": "1174240",
    "end": "1177919"
  },
  {
    "text": "endpoint right now",
    "start": "1177919",
    "end": "1179120"
  },
  {
    "text": "we're going to notice that the cache is",
    "start": "1179120",
    "end": "1181919"
  },
  {
    "text": "going to be empty",
    "start": "1181919",
    "end": "1184720"
  },
  {
    "text": "and the reason for this is that we",
    "start": "1189760",
    "end": "1192400"
  },
  {
    "text": "haven't had any on the clients",
    "start": "1192400",
    "end": "1194640"
  },
  {
    "text": "hitting the cache yet so there is no",
    "start": "1194640",
    "end": "1198400"
  },
  {
    "text": "response being cached or any of the",
    "start": "1198400",
    "end": "1200559"
  },
  {
    "text": "requests being cached",
    "start": "1200559",
    "end": "1202880"
  },
  {
    "text": "so let's start our two onboard clients",
    "start": "1202880",
    "end": "1207840"
  },
  {
    "text": "[Applause]",
    "start": "1213620",
    "end": "1216730"
  },
  {
    "text": "this is the first client being started",
    "start": "1220240",
    "end": "1221600"
  },
  {
    "text": "and you can see a bunch of activity",
    "start": "1221600",
    "end": "1222799"
  },
  {
    "text": "happening in our xcs relay server since",
    "start": "1222799",
    "end": "1224799"
  },
  {
    "text": "we have",
    "start": "1224799",
    "end": "1225679"
  },
  {
    "text": "debug level logs turned on",
    "start": "1225679",
    "end": "1228960"
  },
  {
    "text": "let's start the second client",
    "start": "1228960",
    "end": "1234480"
  },
  {
    "text": "again some more activity and now if we",
    "start": "1234480",
    "end": "1236880"
  },
  {
    "text": "were to run",
    "start": "1236880",
    "end": "1237600"
  },
  {
    "text": "the same curl on our cache endpoint",
    "start": "1237600",
    "end": "1240960"
  },
  {
    "text": "we'll see something different",
    "start": "1240960",
    "end": "1244000"
  },
  {
    "text": "also note here is that we're using jq to",
    "start": "1247520",
    "end": "1249520"
  },
  {
    "text": "make the output more concise for this",
    "start": "1249520",
    "end": "1251200"
  },
  {
    "text": "demo",
    "start": "1251200",
    "end": "1253519"
  },
  {
    "text": "so as you can see",
    "start": "1255919",
    "end": "1260799"
  },
  {
    "text": "the response contains the version of the",
    "start": "1260799",
    "end": "1263600"
  },
  {
    "text": "latest snapshot that was generated by",
    "start": "1263600",
    "end": "1265360"
  },
  {
    "text": "the",
    "start": "1265360",
    "end": "1266000"
  },
  {
    "text": "server in this case it's 95 197",
    "start": "1266000",
    "end": "1270720"
  },
  {
    "text": "more seconds have passed since i've ran",
    "start": "1270720",
    "end": "1272400"
  },
  {
    "text": "this query so that's why we're not",
    "start": "1272400",
    "end": "1274320"
  },
  {
    "text": "seeing the latest but we can also see",
    "start": "1274320",
    "end": "1276720"
  },
  {
    "text": "that",
    "start": "1276720",
    "end": "1277840"
  },
  {
    "text": "both onward clients are cash in the",
    "start": "1277840",
    "end": "1279679"
  },
  {
    "text": "requests",
    "start": "1279679",
    "end": "1282000"
  },
  {
    "text": "all right are all my client two and all",
    "start": "1282000",
    "end": "1283520"
  },
  {
    "text": "the client one",
    "start": "1283520",
    "end": "1286159"
  },
  {
    "text": "so since these clients fall under the",
    "start": "1288480",
    "end": "1291360"
  },
  {
    "text": "same aggregation rule",
    "start": "1291360",
    "end": "1292640"
  },
  {
    "text": "if we were to observe the stats we'd",
    "start": "1292640",
    "end": "1294640"
  },
  {
    "text": "also know that there's only one grpc",
    "start": "1294640",
    "end": "1296880"
  },
  {
    "text": "stream being made to our upstream server",
    "start": "1296880",
    "end": "1298880"
  },
  {
    "text": "despite there being two onward clients",
    "start": "1298880",
    "end": "1302640"
  },
  {
    "text": "we can also validate that the online",
    "start": "1302640",
    "end": "1304159"
  },
  {
    "text": "clients have received valid cds",
    "start": "1304159",
    "end": "1305919"
  },
  {
    "text": "information",
    "start": "1305919",
    "end": "1306799"
  },
  {
    "text": "by querying on voice admin endpoint",
    "start": "1306799",
    "end": "1310960"
  },
  {
    "text": "so this is querying our second onward",
    "start": "1314080",
    "end": "1315840"
  },
  {
    "text": "client and indeed you can see the latest",
    "start": "1315840",
    "end": "1318400"
  },
  {
    "text": "cluster information if we were to clear",
    "start": "1318400",
    "end": "1320080"
  },
  {
    "text": "the other onward client we should",
    "start": "1320080",
    "end": "1322240"
  },
  {
    "text": "receive the same information",
    "start": "1322240",
    "end": "1325280"
  },
  {
    "text": "you can see it very slightly because",
    "start": "1325280",
    "end": "1326480"
  },
  {
    "text": "we've generated a new snapshot since but",
    "start": "1326480",
    "end": "1329440"
  },
  {
    "text": "if we",
    "start": "1329440",
    "end": "1330559"
  },
  {
    "text": "quickly run both in sequence you notice",
    "start": "1330559",
    "end": "1333600"
  },
  {
    "text": "that",
    "start": "1333600",
    "end": "1334159"
  },
  {
    "text": "their cluster information is the same",
    "start": "1334159",
    "end": "1339120"
  },
  {
    "text": "and this example is available on our",
    "start": "1339120",
    "end": "1340720"
  },
  {
    "text": "github if you would like to try it out",
    "start": "1340720",
    "end": "1342320"
  },
  {
    "text": "for yourself",
    "start": "1342320",
    "end": "1344159"
  },
  {
    "text": "please let us know your thoughts thanks",
    "start": "1344159",
    "end": "1347840"
  },
  {
    "start": "1347000",
    "end": "1757000"
  },
  {
    "text": "so we're excited to announce that in the",
    "start": "1353039",
    "end": "1354640"
  },
  {
    "text": "next few weeks here we're going to be",
    "start": "1354640",
    "end": "1356159"
  },
  {
    "text": "releasing version 1 of xts relay",
    "start": "1356159",
    "end": "1359120"
  },
  {
    "text": "and this covers the mvp mechanisms that",
    "start": "1359120",
    "end": "1361200"
  },
  {
    "text": "we showed in the demo",
    "start": "1361200",
    "end": "1362559"
  },
  {
    "text": "and earlier so beyond that we're looking",
    "start": "1362559",
    "end": "1366320"
  },
  {
    "text": "to create",
    "start": "1366320",
    "end": "1366880"
  },
  {
    "text": "extensions including a few here that",
    "start": "1366880",
    "end": "1368960"
  },
  {
    "text": "we're pretty excited about",
    "start": "1368960",
    "end": "1371200"
  },
  {
    "text": "one is the state of the world to delta",
    "start": "1371200",
    "end": "1373600"
  },
  {
    "text": "xcs transformation",
    "start": "1373600",
    "end": "1375840"
  },
  {
    "text": "very few control planes in the wild have",
    "start": "1375840",
    "end": "1377679"
  },
  {
    "text": "implemented support for incremental",
    "start": "1377679",
    "end": "1380480"
  },
  {
    "text": "despite big performance gains so rather",
    "start": "1380480",
    "end": "1383600"
  },
  {
    "text": "than having all control plane servers",
    "start": "1383600",
    "end": "1385440"
  },
  {
    "text": "make this migration xcs relay can",
    "start": "1385440",
    "end": "1388240"
  },
  {
    "text": "implicitly make the conversion",
    "start": "1388240",
    "end": "1389919"
  },
  {
    "text": "and cache the response delta",
    "start": "1389919",
    "end": "1393759"
  },
  {
    "text": "another is api driven configuration",
    "start": "1394159",
    "end": "1397200"
  },
  {
    "text": "rather than having xds relay maintain",
    "start": "1397200",
    "end": "1399039"
  },
  {
    "text": "connections to the upstream server",
    "start": "1399039",
    "end": "1401360"
  },
  {
    "text": "we want to make it possible for",
    "start": "1401360",
    "end": "1402880"
  },
  {
    "text": "operators to directly write to the xds",
    "start": "1402880",
    "end": "1405280"
  },
  {
    "text": "relay cache",
    "start": "1405280",
    "end": "1406400"
  },
  {
    "text": "in a push model when there's updated",
    "start": "1406400",
    "end": "1408080"
  },
  {
    "text": "configuration information",
    "start": "1408080",
    "end": "1411279"
  },
  {
    "text": "another one that we're particularly",
    "start": "1412240",
    "end": "1413520"
  },
  {
    "text": "excited about is endpoint subsetting",
    "start": "1413520",
    "end": "1416559"
  },
  {
    "text": "so in a topology that we mentioned",
    "start": "1416559",
    "end": "1418320"
  },
  {
    "text": "earlier it might not be a deal for",
    "start": "1418320",
    "end": "1420720"
  },
  {
    "text": "xcs clients to be aware of clients",
    "start": "1420720",
    "end": "1422960"
  },
  {
    "text": "running in a different vpc",
    "start": "1422960",
    "end": "1424320"
  },
  {
    "text": "or data center similar to the",
    "start": "1424320",
    "end": "1427279"
  },
  {
    "text": "aggregation rules",
    "start": "1427279",
    "end": "1428400"
  },
  {
    "text": "we're looking at creating rule based",
    "start": "1428400",
    "end": "1429679"
  },
  {
    "text": "configuration where operators can",
    "start": "1429679",
    "end": "1431919"
  },
  {
    "text": "use xcs relay to send back a subset",
    "start": "1431919",
    "end": "1434960"
  },
  {
    "text": "of the eds information rather than all",
    "start": "1434960",
    "end": "1437840"
  },
  {
    "text": "endpoints that exist from the control",
    "start": "1437840",
    "end": "1439279"
  },
  {
    "text": "plane response",
    "start": "1439279",
    "end": "1442080"
  },
  {
    "text": "another interesting use case is blue",
    "start": "1442559",
    "end": "1444240"
  },
  {
    "text": "green control plane deploys",
    "start": "1444240",
    "end": "1446880"
  },
  {
    "text": "so being able to use xcs relay to",
    "start": "1446880",
    "end": "1448559"
  },
  {
    "text": "determine a percentage of traffic",
    "start": "1448559",
    "end": "1450559"
  },
  {
    "text": "that should roll out to the new control",
    "start": "1450559",
    "end": "1452240"
  },
  {
    "text": "plane server",
    "start": "1452240",
    "end": "1454960"
  },
  {
    "text": "and this list goes on in the interest of",
    "start": "1456480",
    "end": "1458559"
  },
  {
    "text": "time we won't be able to cover",
    "start": "1458559",
    "end": "1459760"
  },
  {
    "text": "them all today but as you can see",
    "start": "1459760",
    "end": "1462320"
  },
  {
    "text": "there's plenty of directions that we",
    "start": "1462320",
    "end": "1463760"
  },
  {
    "text": "could and want to take this project",
    "start": "1463760",
    "end": "1465760"
  },
  {
    "text": "so we're always looking for contributors",
    "start": "1465760",
    "end": "1467360"
  },
  {
    "text": "and love talking to people curious about",
    "start": "1467360",
    "end": "1468960"
  },
  {
    "text": "control plan implementations",
    "start": "1468960",
    "end": "1470559"
  },
  {
    "text": "and their use cases please reach out to",
    "start": "1470559",
    "end": "1473520"
  },
  {
    "text": "one of us directly or",
    "start": "1473520",
    "end": "1475200"
  },
  {
    "text": "visit the envoy slack here",
    "start": "1475200",
    "end": "1478799"
  },
  {
    "text": "if you'd like to talk with us more",
    "start": "1478960",
    "end": "1482240"
  },
  {
    "text": "lyft is also looking for engineers that",
    "start": "1482400",
    "end": "1484320"
  },
  {
    "text": "have onboard experience so if that's",
    "start": "1484320",
    "end": "1486159"
  },
  {
    "text": "something that you're interested in",
    "start": "1486159",
    "end": "1487520"
  },
  {
    "text": "please reach out to me or jody directly",
    "start": "1487520",
    "end": "1490400"
  },
  {
    "text": "thanks",
    "start": "1490400",
    "end": "1492640"
  },
  {
    "text": "awesome hi everyone i think we're both",
    "start": "1492640",
    "end": "1496799"
  },
  {
    "text": "live",
    "start": "1498840",
    "end": "1501039"
  },
  {
    "text": "in the context of endpoint subsetting",
    "start": "1501039",
    "end": "1502880"
  },
  {
    "text": "where do you draw the line between logic",
    "start": "1502880",
    "end": "1504559"
  },
  {
    "text": "that belongs in xcs relay and logic that",
    "start": "1504559",
    "end": "1506400"
  },
  {
    "text": "should be in the control plane",
    "start": "1506400",
    "end": "1509679"
  },
  {
    "text": "personally i think it's going to vary by",
    "start": "1510640",
    "end": "1512080"
  },
  {
    "text": "a lot of the use cases that",
    "start": "1512080",
    "end": "1514240"
  },
  {
    "text": "your company is implementing i think",
    "start": "1514240",
    "end": "1517679"
  },
  {
    "text": "the part that we want to implement in",
    "start": "1517679",
    "end": "1519279"
  },
  {
    "text": "exclusive relay is if",
    "start": "1519279",
    "end": "1520880"
  },
  {
    "text": "it can be defined based on rules that",
    "start": "1520880",
    "end": "1523520"
  },
  {
    "text": "anyone might have to live in xts relay",
    "start": "1523520",
    "end": "1525200"
  },
  {
    "text": "but if there is",
    "start": "1525200",
    "end": "1526559"
  },
  {
    "text": "too much custom logic that should be",
    "start": "1526559",
    "end": "1528799"
  },
  {
    "text": "generated from",
    "start": "1528799",
    "end": "1530159"
  },
  {
    "text": "service metadata based on your company's",
    "start": "1530159",
    "end": "1532400"
  },
  {
    "text": "logic then it should fall in the control",
    "start": "1532400",
    "end": "1533919"
  },
  {
    "text": "plane",
    "start": "1533919",
    "end": "1536158"
  },
  {
    "text": "the plug-in model also allows us to",
    "start": "1537120",
    "end": "1540000"
  },
  {
    "text": "define custom",
    "start": "1540000",
    "end": "1541120"
  },
  {
    "text": "ways of splitting the subsetting line",
    "start": "1541120",
    "end": "1543919"
  },
  {
    "text": "points",
    "start": "1543919",
    "end": "1544960"
  },
  {
    "text": "um so yeah that should help too",
    "start": "1544960",
    "end": "1549200"
  },
  {
    "text": "do they maybe you can take this next one",
    "start": "1552159",
    "end": "1553600"
  },
  {
    "text": "just work a lot on the",
    "start": "1553600",
    "end": "1555440"
  },
  {
    "text": "yeah in case the control plane is not",
    "start": "1555440",
    "end": "1557120"
  },
  {
    "text": "present momentarily we'll always",
    "start": "1557120",
    "end": "1558720"
  },
  {
    "text": "continue to receive configuration",
    "start": "1558720",
    "end": "1560880"
  },
  {
    "text": "um the htc delay all this depends on the",
    "start": "1560880",
    "end": "1563919"
  },
  {
    "text": "upstream but if the upstream is not",
    "start": "1563919",
    "end": "1565600"
  },
  {
    "text": "present we have pre-tries and back off",
    "start": "1565600",
    "end": "1568159"
  },
  {
    "text": "mechanisms built in so that we'll keep",
    "start": "1568159",
    "end": "1570480"
  },
  {
    "text": "trying until the controller comes back",
    "start": "1570480",
    "end": "1572799"
  },
  {
    "text": "but it will keep serving contribution",
    "start": "1572799",
    "end": "1575039"
  },
  {
    "text": "from the cash",
    "start": "1575039",
    "end": "1576240"
  },
  {
    "text": "uh to the sidecar so the fleet will",
    "start": "1576240",
    "end": "1578799"
  },
  {
    "text": "continue operating",
    "start": "1578799",
    "end": "1580559"
  },
  {
    "text": "um and there will be no disruption until",
    "start": "1580559",
    "end": "1583360"
  },
  {
    "text": "the back in the total comes back",
    "start": "1583360",
    "end": "1599840"
  },
  {
    "text": "[Music]",
    "start": "1604940",
    "end": "1608199"
  },
  {
    "text": "how widely has this been used in a while",
    "start": "1610559",
    "end": "1613600"
  },
  {
    "text": "because as we haven't issued an initial",
    "start": "1613600",
    "end": "1616000"
  },
  {
    "text": "release yet",
    "start": "1616000",
    "end": "1617360"
  },
  {
    "text": "i would say there's no customers using",
    "start": "1617360",
    "end": "1619520"
  },
  {
    "text": "in the wild we have a few",
    "start": "1619520",
    "end": "1620799"
  },
  {
    "text": "people from other companies that are",
    "start": "1620799",
    "end": "1622720"
  },
  {
    "text": "testing it for us and we're",
    "start": "1622720",
    "end": "1624320"
  },
  {
    "text": "internally mixing it within lyft but",
    "start": "1624320",
    "end": "1627600"
  },
  {
    "text": "so far it's sort of in a early alpha",
    "start": "1627600",
    "end": "1630720"
  },
  {
    "text": "stage",
    "start": "1630720",
    "end": "1632960"
  },
  {
    "text": "we are uh serving our staging traffic on",
    "start": "1632960",
    "end": "1635440"
  },
  {
    "text": "this",
    "start": "1635440",
    "end": "1636080"
  },
  {
    "text": "at this point and once again production",
    "start": "1636080",
    "end": "1638720"
  },
  {
    "text": "will probably do a mvp",
    "start": "1638720",
    "end": "1640399"
  },
  {
    "text": "release",
    "start": "1640399",
    "end": "1642799"
  },
  {
    "text": "in case the back end control plane comes",
    "start": "1652880",
    "end": "1654720"
  },
  {
    "text": "back will it invaded the caching delay",
    "start": "1654720",
    "end": "1656640"
  },
  {
    "text": "so",
    "start": "1656640",
    "end": "1657279"
  },
  {
    "text": "yes uh when the back-end control thing",
    "start": "1657279",
    "end": "1659600"
  },
  {
    "text": "comes back it will be a new stream from",
    "start": "1659600",
    "end": "1661520"
  },
  {
    "text": "exchange delay to the",
    "start": "1661520",
    "end": "1662799"
  },
  {
    "text": "back-end control plane as if everything",
    "start": "1662799",
    "end": "1665279"
  },
  {
    "text": "is fresh",
    "start": "1665279",
    "end": "1666080"
  },
  {
    "text": "so whatever the version updated version",
    "start": "1666080",
    "end": "1668320"
  },
  {
    "text": "there is it will reflect in the cache",
    "start": "1668320",
    "end": "1683840"
  },
  {
    "text": "i guess we're at time almost uh unless",
    "start": "1695440",
    "end": "1699200"
  },
  {
    "text": "there's any other questions",
    "start": "1699200",
    "end": "1701290"
  },
  {
    "text": "[Music]",
    "start": "1701290",
    "end": "1704369"
  },
  {
    "text": "how many points do you have in your",
    "start": "1705120",
    "end": "1706799"
  },
  {
    "text": "clusters",
    "start": "1706799",
    "end": "1708480"
  },
  {
    "text": "uh it varies uh",
    "start": "1708480",
    "end": "1711760"
  },
  {
    "text": "we had times when we are like 50k",
    "start": "1711760",
    "end": "1715360"
  },
  {
    "text": "or something but yeah i have no exact",
    "start": "1715360",
    "end": "1717520"
  },
  {
    "text": "numbers to",
    "start": "1717520",
    "end": "1718559"
  },
  {
    "text": "share at this point",
    "start": "1718559",
    "end": "1725840"
  },
  {
    "text": "yeah probably we can take this offline",
    "start": "1733679",
    "end": "1735919"
  },
  {
    "text": "you could engage with us on the xds",
    "start": "1735919",
    "end": "1737679"
  },
  {
    "text": "relay",
    "start": "1737679",
    "end": "1738159"
  },
  {
    "text": "slack or on github uh i am afraid of the",
    "start": "1738159",
    "end": "1742240"
  },
  {
    "text": "cut off at 130 so it was great",
    "start": "1742240",
    "end": "1745919"
  },
  {
    "text": "uh sharing our information with you",
    "start": "1745919",
    "end": "1749039"
  },
  {
    "text": "today",
    "start": "1749039",
    "end": "1749600"
  },
  {
    "text": "and thank you yeah thanks everyone",
    "start": "1749600",
    "end": "1755039"
  },
  {
    "text": "later",
    "start": "1756440",
    "end": "1759440"
  }
]