[
  {
    "text": "okay hello everyone um thank you for taking your time and staying for one of",
    "start": "160",
    "end": "6160"
  },
  {
    "text": "the last sessions in the cucon hope everyone had a great time and maybe and",
    "start": "6160",
    "end": "13599"
  },
  {
    "text": "I'm hopeful to see you maybe on the next one and we can discuss more but today we have one of the last sessions uh let's",
    "start": "13599",
    "end": "21240"
  },
  {
    "text": "finish it with some secrets of running CD uh I'm Mar shovi",
    "start": "21240",
    "end": "28000"
  },
  {
    "text": "I'm uh I've been maintainer of etcd for last two years uh with recent creation",
    "start": "28000",
    "end": "35879"
  },
  {
    "text": "of Sig ET CDs the special interest group in kubernetes I'm the TL of the uh of",
    "start": "35879",
    "end": "43120"
  },
  {
    "text": "the Sig and I'm also the person uh working at gke and making sure that if",
    "start": "43120",
    "end": "50320"
  },
  {
    "text": "you run in in in Google your class your kubernetes classers are running and the",
    "start": "50320",
    "end": "56039"
  },
  {
    "text": "atcd that we are running is the best that we can bring you so today I wanted",
    "start": "56039",
    "end": "62280"
  },
  {
    "text": "to share some of the experiences and my view on reliability of etcd that I have",
    "start": "62280",
    "end": "68960"
  },
  {
    "text": "seen personally in production um so uh agenda for today is",
    "start": "68960",
    "end": "78080"
  },
  {
    "text": "look at some simple cases of failures in distributed system uh I would want to",
    "start": "78080",
    "end": "85119"
  },
  {
    "text": "focus on especially on cluster scope failures which is in my EXP experience",
    "start": "85119",
    "end": "90400"
  },
  {
    "text": "biggest reason of failures that I have seen uh I will propose couple of",
    "start": "90400",
    "end": "97000"
  },
  {
    "text": "mitigations how you can avoid those problems and we'll finish maybe the",
    "start": "97000",
    "end": "103119"
  },
  {
    "text": "secret maybe some people already know it uh and yeah uh so failures in",
    "start": "103119",
    "end": "110680"
  },
  {
    "text": "distribut systems uh kuber or",
    "start": "110680",
    "end": "115920"
  },
  {
    "text": "etcd is really great at uh handling",
    "start": "115920",
    "end": "122280"
  },
  {
    "text": "failures of single members uh this is because it uses uh rough consensus",
    "start": "122280",
    "end": "128920"
  },
  {
    "text": "algorithm and it allows it to survive failures or of single maybe two nodes",
    "start": "128920",
    "end": "136480"
  },
  {
    "text": "depending of your cluster size so it as long as the Quorum so the majority of",
    "start": "136480",
    "end": "142280"
  },
  {
    "text": "the members are alive we can we can the cluster can proceed and this is great",
    "start": "142280",
    "end": "147720"
  },
  {
    "text": "for like failures of this Network some disconnections so short",
    "start": "147720",
    "end": "154000"
  },
  {
    "text": "temporary uh issues that you don't want to like think like about uh but uh so",
    "start": "154000",
    "end": "164000"
  },
  {
    "text": "this works because of raft uh raft is an algorithm that can take a um any number",
    "start": "164000",
    "end": "172879"
  },
  {
    "text": "of concurrent request and provide us a singular organized and ordered stream of",
    "start": "172879",
    "end": "181239"
  },
  {
    "text": "uh of requests and properly distributed along all members this allows us to be",
    "start": "181239",
    "end": "188120"
  },
  {
    "text": "sure that every member has the same data and every member uh can end up uh in the",
    "start": "188120",
    "end": "196120"
  },
  {
    "text": "same state but unfortunately it also means that uh every time there is a",
    "start": "196120",
    "end": "202239"
  },
  {
    "text": "developer mistake application issue or just a corrupt even a corruption in the",
    "start": "202239",
    "end": "207840"
  },
  {
    "text": "data it's as easily to replicate correct Behavior it's as or it's as easily to",
    "start": "207840",
    "end": "215040"
  },
  {
    "text": "inject failure to all the members so today I would want to give you a couple of examples of Errors like this where",
    "start": "215040",
    "end": "222080"
  },
  {
    "text": "the whole cluster can suffer because of some issue that was either U hard to",
    "start": "222080",
    "end": "229560"
  },
  {
    "text": "predict in etcd or kubernetes has some problems of using uh at CD in a way that",
    "start": "229560",
    "end": "236799"
  },
  {
    "text": "would uh make it more reliable or even issues that we could not predict",
    "start": "236799",
    "end": "244280"
  },
  {
    "text": "at all and are in the Golan language itself yeah so I would want to start",
    "start": "244280",
    "end": "251920"
  },
  {
    "text": "with pretty uh simple case I think everyone that runs kubernetes uh heard",
    "start": "251920",
    "end": "258600"
  },
  {
    "text": "about events and use the events but as stable as kubernetes is it still has",
    "start": "258600",
    "end": "265240"
  },
  {
    "text": "even or there's a lot of people that still have issues with uh storring and",
    "start": "265240",
    "end": "272080"
  },
  {
    "text": "persisting events uh reliably and here I would want to",
    "start": "272080",
    "end": "278120"
  },
  {
    "text": "discuss directly a case that you can easily or how kubernetes uses events can",
    "start": "278120",
    "end": "284320"
  },
  {
    "text": "easily lead to to production uh downtime if you don't handle them properly so",
    "start": "284320",
    "end": "291199"
  },
  {
    "text": "what is a kubernetes event uh kubernetes Stars two types of",
    "start": "291199",
    "end": "298800"
  },
  {
    "text": "distinguished uh resources into the uh etcd one is",
    "start": "298800",
    "end": "306039"
  },
  {
    "text": "objects that represent the state the the the intent and the and the status of of",
    "start": "306039",
    "end": "312960"
  },
  {
    "text": "what is happening in your cluster uh they repres they are of critical importance so we cannot even we need to",
    "start": "312960",
    "end": "320120"
  },
  {
    "text": "persist them and we need to guarantee of their their delivery they are never deleted until we",
    "start": "320120",
    "end": "328080"
  },
  {
    "text": "want them like something we want them deleted so either intent of the uh",
    "start": "328080",
    "end": "333680"
  },
  {
    "text": "developer changes or administrator or they are garbage collected because they are no longer",
    "start": "333680",
    "end": "340240"
  },
  {
    "text": "needed and if you run of clusters of certain size we can easily predict",
    "start": "340240",
    "end": "346639"
  },
  {
    "text": "number of uh of objects that are running in um cluster because you can multiply",
    "start": "346639",
    "end": "352840"
  },
  {
    "text": "number of your notes by some number of PODS you run per note and maybe add some",
    "start": "352840",
    "end": "358639"
  },
  {
    "text": "deployments and there is some multiplication that you can give a high limit on the other hand is events and",
    "start": "358639",
    "end": "366720"
  },
  {
    "text": "events is a a a way for developers that",
    "start": "366720",
    "end": "372919"
  },
  {
    "text": "Deploy on kubernetes to get access to uh to debac information that usually only",
    "start": "372919",
    "end": "379880"
  },
  {
    "text": "administrator can can uh can see so when there is a decision by scheduler uh of",
    "start": "379880",
    "end": "388400"
  },
  {
    "text": "and it failed because there is no note in the cluster that matches I know po",
    "start": "388400",
    "end": "393960"
  },
  {
    "text": "selector the this is what selector was chosen is the decision of developer so",
    "start": "393960",
    "end": "400759"
  },
  {
    "text": "kubernetes makes it really easy for developer to see and run cctl describe",
    "start": "400759",
    "end": "406280"
  },
  {
    "text": "to to see why why the OT was not scheduled and uh having those Lo locks",
    "start": "406280",
    "end": "412680"
  },
  {
    "text": "is very very useful for debugging but uh some of the properties of those locks",
    "start": "412680",
    "end": "418520"
  },
  {
    "text": "are different than the state so they're not critical important their best effort",
    "start": "418520",
    "end": "424360"
  },
  {
    "text": "so kubernetes will even intentionally drop and avoid sending them if it it's",
    "start": "424360",
    "end": "429680"
  },
  {
    "text": "run out of resources or there there is some Network issue so like FYI you should not use events because they're",
    "start": "429680",
    "end": "436280"
  },
  {
    "text": "not guaranteed delivery so they're not critical uh and you cannot make you",
    "start": "436280",
    "end": "442400"
  },
  {
    "text": "cannot always depend that they're uh available uh usually I think they're um",
    "start": "442400",
    "end": "448199"
  },
  {
    "text": "configured to be deleted after 2 hours this can change depending on your release or uh distribution of kuus you",
    "start": "448199",
    "end": "456280"
  },
  {
    "text": "run and uh they have tendency to really",
    "start": "456280",
    "end": "461520"
  },
  {
    "text": "explode uh there uh when there is some failure because if there is an issue",
    "start": "461520",
    "end": "467520"
  },
  {
    "text": "with couple of nodes they get disconnected you get a lot of information or events not only about the",
    "start": "467520",
    "end": "474599"
  },
  {
    "text": "notes you get about the pods that they cannot schedule and then you get about the deployments that they cannot get",
    "start": "474599",
    "end": "481520"
  },
  {
    "text": "number of PODS that they requested so there is a a big bump of of",
    "start": "481520",
    "end": "488520"
  },
  {
    "text": "or they aggregate mostly during failures and one failure can easily lead to",
    "start": "488520",
    "end": "494360"
  },
  {
    "text": "another one and they can really uh grow in",
    "start": "494360",
    "end": "499319"
  },
  {
    "text": "size um and the problem with uh Le or",
    "start": "501720",
    "end": "508080"
  },
  {
    "text": "kubernetes events is that they are using ETD Lees and they are using it in",
    "start": "508080",
    "end": "514120"
  },
  {
    "text": "somewhat uh incorrect way uh and this is",
    "start": "514120",
    "end": "519880"
  },
  {
    "text": "because uh Lees were designed as a short uh",
    "start": "519880",
    "end": "526880"
  },
  {
    "text": "short um short uh for short time to",
    "start": "527080",
    "end": "532560"
  },
  {
    "text": "allow at CD to provide distributed uh Primitives like leader election so for",
    "start": "532560",
    "end": "539200"
  },
  {
    "text": "times like 5 10 seconds and it doesn't and this requirement um mean uh mean or",
    "start": "539200",
    "end": "548360"
  },
  {
    "text": "means that etcd doesn't really persist their um their status so when the lease",
    "start": "548360",
    "end": "556680"
  },
  {
    "text": "is created we save its TTL but uh we don't do any updates or any check points",
    "start": "556680",
    "end": "564040"
  },
  {
    "text": "uh uh for throughout the time of the lease and during this time ETD members",
    "start": "564040",
    "end": "570959"
  },
  {
    "text": "uh can go down up and leader can change and because ETD leases are only counted",
    "start": "570959",
    "end": "578399"
  },
  {
    "text": "down uh by the leader every time that your leader gets disconnected from the",
    "start": "578399",
    "end": "583480"
  },
  {
    "text": "cluster it's a easily case for the time uh the leaste time can to be reset and",
    "start": "583480",
    "end": "589560"
  },
  {
    "text": "kubernetes directly uses leases to provide the TTL for events so if Lees uh",
    "start": "589560",
    "end": "597560"
  },
  {
    "text": "if the leader changes and Le time out is reset your events that should be deleted",
    "start": "597560",
    "end": "603000"
  },
  {
    "text": "like half an hour ago can still leave for another",
    "start": "603000",
    "end": "608839"
  },
  {
    "text": "hour um yeah so two hours make it really we",
    "start": "610000",
    "end": "615320"
  },
  {
    "text": "really unfortunate and every time that there is a leader",
    "start": "615320",
    "end": "622440"
  },
  {
    "text": "election or leader change number of leases can grow",
    "start": "622440",
    "end": "627480"
  },
  {
    "text": "exponentially and Unfortunate Events can cause an full",
    "start": "627480",
    "end": "633480"
  },
  {
    "text": "explosion and domino effect around the size of",
    "start": "633480",
    "end": "639800"
  },
  {
    "text": "etcd and this uh this like cannot be really uh avoided",
    "start": "639800",
    "end": "649120"
  },
  {
    "text": "on the default uh kubernetes clusters or uh so you need to configure and make direct",
    "start": "649120",
    "end": "657959"
  },
  {
    "text": "changes to to or to your uh distrib or how do you architect kubernetes or how",
    "start": "657959",
    "end": "663440"
  },
  {
    "text": "do you run your control plane and how you configure your etcd to to to handle",
    "start": "663440",
    "end": "670440"
  },
  {
    "text": "it so because events are best effort and not",
    "start": "672360",
    "end": "679639"
  },
  {
    "text": "really uh a durable critical thing for your cluster you should really think",
    "start": "679639",
    "end": "685240"
  },
  {
    "text": "about separating the SD instance that sour them making it much easy to to be",
    "start": "685240",
    "end": "693160"
  },
  {
    "text": "not persistent uh this allows your cluster if it even goes down uh because of",
    "start": "693160",
    "end": "699760"
  },
  {
    "text": "events to be then rebooted uh with without the blood of Lees causing the uh",
    "start": "699760",
    "end": "706760"
  },
  {
    "text": "the consistent downtime",
    "start": "706760",
    "end": "712079"
  },
  {
    "text": "um the way if you're making or if you still want to persist information that",
    "start": "712079",
    "end": "718440"
  },
  {
    "text": "is a available events you can easily watch or their ready solution to export",
    "start": "718440",
    "end": "723480"
  },
  {
    "text": "events to your logging solution like elastic search or your preferent Cloud",
    "start": "723480",
    "end": "729200"
  },
  {
    "text": "uh you you can reduce the TL to like 5",
    "start": "729200",
    "end": "734360"
  },
  {
    "text": "minutes which makes it much more resilient to failures and then have a",
    "start": "734360",
    "end": "742680"
  },
  {
    "text": "separate process that watches them and exports them to your uh logging solution",
    "start": "742680",
    "end": "747839"
  },
  {
    "text": "so developers can still read them and look through historical uh down um",
    "start": "747839",
    "end": "753000"
  },
  {
    "text": "historical the back historical issues and um a new thing that was",
    "start": "753000",
    "end": "759720"
  },
  {
    "text": "introduced around a year ago is the list checkpointing it's a direct fix in atcd",
    "start": "759720",
    "end": "766600"
  },
  {
    "text": "to make kubernetes use case reliable and it's uh it has two",
    "start": "766600",
    "end": "774160"
  },
  {
    "text": "iterations uh that uh in first prevents you prevents lead their election from",
    "start": "774160",
    "end": "780240"
  },
  {
    "text": "causing uh uh TTL to be reset by having",
    "start": "780240",
    "end": "785760"
  },
  {
    "text": "every 5 minutes uh leader sending a update and checkpoint to other members",
    "start": "785760",
    "end": "791160"
  },
  {
    "text": "that hey like we we I counted down in five minutes please remember this and",
    "start": "791160",
    "end": "796320"
  },
  {
    "text": "reduce your uh TTL and second was it's persisting so if",
    "start": "796320",
    "end": "801720"
  },
  {
    "text": "your whole atcd cluster goes down perit will will make sure that this uh this",
    "start": "801720",
    "end": "808120"
  },
  {
    "text": "checkpoint is not only in memory in the members but it's also on the disk so even if you shut down your full cluster",
    "start": "808120",
    "end": "814800"
  },
  {
    "text": "the time will be persisted on the",
    "start": "814800",
    "end": "818680"
  },
  {
    "text": "disc so second issue that I would want to",
    "start": "820519",
    "end": "827120"
  },
  {
    "text": "discuss is about a kubernetes qu deadlock I uh if you ever looked",
    "start": "827399",
    "end": "837160"
  },
  {
    "text": "into h if you ever looked into how ETS distur data on the disk and ever f f or",
    "start": "837160",
    "end": "845040"
  },
  {
    "text": "encountered an issue that atcd cannot or kubernetes cannot progress because atcd",
    "start": "845040",
    "end": "852199"
  },
  {
    "text": "um complains about out of qu this this is the mechanism that is",
    "start": "852199",
    "end": "858920"
  },
  {
    "text": "underlying so the ET CD stores both the latest State and the history of uh",
    "start": "858920",
    "end": "866399"
  },
  {
    "text": "history of of all changes that happened so there is two Dimensions to to to",
    "start": "866399",
    "end": "872600"
  },
  {
    "text": "every um to to size of the database",
    "start": "872600",
    "end": "879480"
  },
  {
    "text": "and this is uh a mechanism to prevent uh regression in performance on ATD uh and",
    "start": "879480",
    "end": "887360"
  },
  {
    "text": "if you've ever either increase or if either state of your etcd or grew too",
    "start": "887360",
    "end": "895120"
  },
  {
    "text": "much or number you haven't or number of changes has increased you might run into",
    "start": "895120",
    "end": "902600"
  },
  {
    "text": "issue that etcd will hit the database size limit and in that situation ETD",
    "start": "902600",
    "end": "908279"
  },
  {
    "text": "will raise an alarm and require someone to remove the unnecessary information",
    "start": "908279",
    "end": "916000"
  },
  {
    "text": "and uh release the uh the qu alarm um let's now look at two mechanism",
    "start": "916000",
    "end": "925360"
  },
  {
    "text": "driving the size of ETD the first one is compaction uh it's a mechanism that uh",
    "start": "925360",
    "end": "932920"
  },
  {
    "text": "is responsible for cutting and removing uh long tail of changes in etsc that are",
    "start": "932920",
    "end": "940240"
  },
  {
    "text": "no longer uh access accessed or useful to be accessed it just uh it takes a",
    "start": "940240",
    "end": "949399"
  },
  {
    "text": "full history of all resource version in that in kubernetes that might be used",
    "start": "949399",
    "end": "955560"
  },
  {
    "text": "and it Marks One revision as unavailable and from that and this",
    "start": "955560",
    "end": "961319"
  },
  {
    "text": "allows at cd21 clean up the uh old revisions and ruce the data space and",
    "start": "961319",
    "end": "968800"
  },
  {
    "text": "second mechanism that drives uh the size of database is the defragmentation uh unfortunately uh etcd",
    "start": "968800",
    "end": "977319"
  },
  {
    "text": "algorithm for selecting page selecting and using disk space is still around",
    "start": "977319",
    "end": "983480"
  },
  {
    "text": "windows uh 95 which means we uh from time to time",
    "start": "983480",
    "end": "990480"
  },
  {
    "text": "you need to defract it's less about the performance to like based on my experience it's more about the size of",
    "start": "990480",
    "end": "998920"
  },
  {
    "text": "the surge never decreases so for your like sanity and you making sure that the",
    "start": "998920",
    "end": "1006759"
  },
  {
    "text": "uh qu doesn't increase and you're not feel pressure that you're getting close to it you should defract your at CD uh",
    "start": "1006759",
    "end": "1014360"
  },
  {
    "text": "to prevent uh unoptimized uh uh page",
    "start": "1014360",
    "end": "1021360"
  },
  {
    "text": "layout so the two mechanism uh work together to reduce size of etcd first uh",
    "start": "1021519",
    "end": "1029240"
  },
  {
    "text": "usually you would have your uh um most of your um storage pretty utilized and",
    "start": "1029240",
    "end": "1036360"
  },
  {
    "text": "with only couple of empty spaces and all the revisions uh used if you then",
    "start": "1036360",
    "end": "1041918"
  },
  {
    "text": "compact it you can remove uh the revisions below so here the if we compact on revision 10",
    "start": "1041919",
    "end": "1048960"
  },
  {
    "text": "uh we would remove the pages that have the data for those uh for those",
    "start": "1048960",
    "end": "1054240"
  },
  {
    "text": "revisions leading to even more empty spaces and then you we need to U run a",
    "start": "1054240",
    "end": "1060200"
  },
  {
    "text": "defrag for at CD to rewrite this the page layout and and out at the minimal",
    "start": "1060200",
    "end": "1066559"
  },
  {
    "text": "size of database that is required so going into the problem uh",
    "start": "1066559",
    "end": "1075799"
  },
  {
    "text": "kubernetes compaction is somewhat unaware of uh some prop U or some ETD",
    "start": "1075799",
    "end": "1085000"
  },
  {
    "text": "behaviors and uh can cause a lot of problem that the algorithm that",
    "start": "1085000",
    "end": "1090720"
  },
  {
    "text": "kubernetes uses is uh is assuming that there are multiple API servers talking",
    "start": "1090720",
    "end": "1096919"
  },
  {
    "text": "to atcd and those multii uh uh multiple API servers are racing uh",
    "start": "1096919",
    "end": "1105240"
  },
  {
    "text": "to to do the compaction and to prevent unnecessary compaction or",
    "start": "1105240",
    "end": "1112760"
  },
  {
    "text": "uh those and those two API servers or um interfering with each other they are",
    "start": "1112760",
    "end": "1119640"
  },
  {
    "text": "first trying to race for a change on to the key so they make a",
    "start": "1119640",
    "end": "1126120"
  },
  {
    "text": "right and the one that is first will wins and can do the compaction this has",
    "start": "1126120",
    "end": "1132679"
  },
  {
    "text": "an obvious issue of what happens if you run out of qu we cannot make a right",
    "start": "1132679",
    "end": "1138559"
  },
  {
    "text": "kubernetes just stops it cannot",
    "start": "1138559",
    "end": "1144200"
  },
  {
    "text": "proceed so what you should do to prevent",
    "start": "1144360",
    "end": "1149960"
  },
  {
    "text": "this uh kubernetes algorithm is not perfect but there's there is for long",
    "start": "1149960",
    "end": "1156360"
  },
  {
    "text": "time a solution in atcd that not only avoids the problem but reduces the",
    "start": "1156360",
    "end": "1162600"
  },
  {
    "text": "overall overhead of of your atcd um by default",
    "start": "1162600",
    "end": "1169120"
  },
  {
    "text": "kubernetes algorithms stores data between 5 and 10 minutes uh of all",
    "start": "1169120",
    "end": "1175679"
  },
  {
    "text": "historical changes if uh and this if you only set the compaction period for 5",
    "start": "1175679",
    "end": "1182520"
  },
  {
    "text": "minutes so you can double the size uh by default you can the size of ETD can be",
    "start": "1182520",
    "end": "1187760"
  },
  {
    "text": "double the data necessary by kubernetes and if you use the SD",
    "start": "1187760",
    "end": "1194400"
  },
  {
    "text": "mechanism you will only have around 10% overhead so we can double how you how",
    "start": "1194400",
    "end": "1200960"
  },
  {
    "text": "much you get space from atcd by just setting a flag on atcd and disabling it",
    "start": "1200960",
    "end": "1206240"
  },
  {
    "text": "in kubernetes uh second recommendation I would",
    "start": "1206240",
    "end": "1211840"
  },
  {
    "text": "do is on defrag uh defrag is very",
    "start": "1211840",
    "end": "1218840"
  },
  {
    "text": "expens uh expensive and you should uh avoid it if possible I uh maybe",
    "start": "1218840",
    "end": "1227039"
  },
  {
    "text": "depending on your experience seen some performance Improvement but in my experience they're not big enough",
    "start": "1227039",
    "end": "1234240"
  },
  {
    "text": "to to motivate running defrag too frequently you should",
    "start": "1234240",
    "end": "1239360"
  },
  {
    "text": "defrag uh you should defrag when you or you should execute always the defrag as",
    "start": "1239360",
    "end": "1247960"
  },
  {
    "text": "uh as only when it's uh you should execute",
    "start": "1247960",
    "end": "1253640"
  },
  {
    "text": "defrag when it's appropriate and check this pretty frequently but you should",
    "start": "1253640",
    "end": "1259799"
  },
  {
    "text": "avoid locking uh making the database unavailable because defrag itself requires a full lock on database and",
    "start": "1259799",
    "end": "1266280"
  },
  {
    "text": "rewriting the storage so we can avoid this cost by by just adding some uh",
    "start": "1266280",
    "end": "1274080"
  },
  {
    "text": "simple checks before running it that we that verify that there is at least some",
    "start": "1274080",
    "end": "1280760"
  },
  {
    "text": "space to be freed before you uh and uh before you execute the defract so by",
    "start": "1280760",
    "end": "1287200"
  },
  {
    "text": "running it's pretty frequently you can with the check you can avoid the downside of locking the",
    "start": "1287200",
    "end": "1294919"
  },
  {
    "text": "database too much but still ensure that you don't run out of qu too too uh too",
    "start": "1294919",
    "end": "1301840"
  },
  {
    "text": "fast um one thing to remember you should at the end also uh dis disarm the alarm",
    "start": "1301840",
    "end": "1309880"
  },
  {
    "text": "because from time to time maybe it you will get into situation that you reach the quot and you want to be sure that",
    "start": "1309880",
    "end": "1315960"
  },
  {
    "text": "the alarm after defrag is uh disarmed and you can do autom safely automate that if you're running",
    "start": "1315960",
    "end": "1323840"
  },
  {
    "text": "defrag yeah uh the third issue that I wanted to",
    "start": "1325799",
    "end": "1331880"
  },
  {
    "text": "discuss is the most like recent critical issue in that was in ETD project",
    "start": "1331880",
    "end": "1339720"
  },
  {
    "text": "uh it's about ETD what",
    "start": "1339720",
    "end": "1344000"
  },
  {
    "text": "starvation so to understand how the what starvation can happen especially in",
    "start": "1345760",
    "end": "1351520"
  },
  {
    "text": "kubernetes we need to look a little bit into",
    "start": "1351520",
    "end": "1357159"
  },
  {
    "text": "um uh into a kubernetes so there is what kubernetes does is there is a single ETD",
    "start": "1357159",
    "end": "1364919"
  },
  {
    "text": "client per resource sorry for the mistake uh so if you have multiple",
    "start": "1364919",
    "end": "1371039"
  },
  {
    "text": "controllers like scheduler AP controller manager they will all talk to kubernetes",
    "start": "1371039",
    "end": "1376360"
  },
  {
    "text": "API but at the end and this will be grouped by each resource and uh um which",
    "start": "1376360",
    "end": "1383000"
  },
  {
    "text": "has its own separate storage uh structure goang structure and those will uh and each",
    "start": "1383000",
    "end": "1390799"
  },
  {
    "text": "client will run independent uh grpc connection to to",
    "start": "1390799",
    "end": "1397799"
  },
  {
    "text": "atcd uh the important part is uh that that if there is a a lot of traffic on",
    "start": "1397799",
    "end": "1404880"
  },
  {
    "text": "single resource they are all sent for one connection",
    "start": "1404880",
    "end": "1409880"
  },
  {
    "text": "um so the issue that I encountered myself was uh a simple change uh of",
    "start": "1410120",
    "end": "1418159"
  },
  {
    "text": "enabling TL TLS that caused a what starvation um it was a pretty hard to",
    "start": "1418159",
    "end": "1426960"
  },
  {
    "text": "discover issue because uh we've been rolling out the the change for very long time and only one cluster out",
    "start": "1426960",
    "end": "1435279"
  },
  {
    "text": "of B only one cluster after a year and counter it to Total surprise to of",
    "start": "1435279",
    "end": "1443640"
  },
  {
    "text": "everyone um what was unique about this cluster what was that there was a lot of",
    "start": "1443640",
    "end": "1449360"
  },
  {
    "text": "uh demon set controllers uh that or demon of controllers running as demon",
    "start": "1449360",
    "end": "1455120"
  },
  {
    "text": "set because if you have want to have some logging setup and you want to add",
    "start": "1455120",
    "end": "1460559"
  },
  {
    "text": "some metadata about pots you will uh have your I know fluent D or fluent bit",
    "start": "1460559",
    "end": "1466480"
  },
  {
    "text": "talk to API server and if there is a a lot of of if there",
    "start": "1466480",
    "end": "1472320"
  },
  {
    "text": "is a short downtime those uh no of those nodes they can they will want to",
    "start": "1472320",
    "end": "1479039"
  },
  {
    "text": "reconnect to API server and they will start it from making a list request and",
    "start": "1479039",
    "end": "1484159"
  },
  {
    "text": "this can if there is a pretty big cluster this can cause a lot of traffic on the uh on the wire and if all of",
    "start": "1484159",
    "end": "1492120"
  },
  {
    "text": "those requests are about pots the traffic on the SD client for pots will be will be will will be overwhelming the",
    "start": "1492120",
    "end": "1501919"
  },
  {
    "text": "grpc connection and this happened because",
    "start": "1501919",
    "end": "1507679"
  },
  {
    "text": "etcd uh is serving uh the serving stack",
    "start": "1507679",
    "end": "1513880"
  },
  {
    "text": "has a separate paths for both TLS and non TLS and the underlying issue is that",
    "start": "1513880",
    "end": "1519760"
  },
  {
    "text": "http2 standard doesn't allow multiplexing of DLS connection you",
    "start": "1519760",
    "end": "1525600"
  },
  {
    "text": "cannot easily dis you cannot even distinguish it at all between uh during",
    "start": "1525600",
    "end": "1530640"
  },
  {
    "text": "the protocol negotiation whether it is a TL uh HTTP request or grpc request and",
    "start": "1530640",
    "end": "1537640"
  },
  {
    "text": "whether U so it requires you to to proxy",
    "start": "1537640",
    "end": "1542919"
  },
  {
    "text": "through someone so for if there is a so normally for non TLS case we can just uh",
    "start": "1542919",
    "end": "1550559"
  },
  {
    "text": "check uh we can just check on connection level and send HTTP request to http",
    "start": "1550559",
    "end": "1556320"
  },
  {
    "text": "server and grpc request to drpc server that run as separate go",
    "start": "1556320",
    "end": "1561600"
  },
  {
    "text": "routines but in TLS case uh we because we cannot distinguish them we need to",
    "start": "1561600",
    "end": "1567159"
  },
  {
    "text": "send the the TLs request to http server and it will uh pass if there is a",
    "start": "1567159",
    "end": "1575600"
  },
  {
    "text": "grpc um protocol header it will pass it to grpc Handler and there is a big",
    "start": "1575600",
    "end": "1582159"
  },
  {
    "text": "difference between before uh between those two uh solution the",
    "start": "1582159",
    "end": "1589399"
  },
  {
    "text": "HTTP maybe the or the protocol http2 is the same but the imple implementation is",
    "start": "1589399",
    "end": "1595799"
  },
  {
    "text": "totally different uh uh even in grpc uh documentation it",
    "start": "1595799",
    "end": "1605480"
  },
  {
    "text": "stands it states that performance and features between those two uh Solutions",
    "start": "1605480",
    "end": "1611919"
  },
  {
    "text": "may can really vary and what happens is",
    "start": "1611919",
    "end": "1617279"
  },
  {
    "text": "or what happened in our case was that because http2 supports multiple streams",
    "start": "1617279",
    "end": "1623559"
  },
  {
    "text": "per writer uh it it also needs to pick a",
    "start": "1623559",
    "end": "1630600"
  },
  {
    "text": "algorithm to decide which stream to respond uh which streams to respond first so if",
    "start": "1630600",
    "end": "1638880"
  },
  {
    "text": "there is a two list requests at the same time there's algorithm that will pick",
    "start": "1638880",
    "end": "1644799"
  },
  {
    "text": "whether to respond to the first list or the second list and this uh the same",
    "start": "1644799",
    "end": "1649840"
  },
  {
    "text": "happens uh with the watch so if there is a lot of list requests uh grp or the",
    "start": "1649840",
    "end": "1656559"
  },
  {
    "text": "HTTP server needs to have some way to decide who to respond uh who is in which",
    "start": "1656559",
    "end": "1662760"
  },
  {
    "text": "order it should respond and unfortunately the the main difference between grpc and HTTP server was that",
    "start": "1662760",
    "end": "1670320"
  },
  {
    "text": "HTTP server didn't uh not only didn't prioritize the watch which is much",
    "start": "1670320",
    "end": "1677240"
  },
  {
    "text": "smaller smaller requests that are less frequent but it also uh even worked",
    "start": "1677240",
    "end": "1683519"
  },
  {
    "text": "against it and made it uh it resulted in this uh in watch",
    "start": "1683519",
    "end": "1689120"
  },
  {
    "text": "being always at the end of the queue causing a full starvation that can like that could take minutes to clog out so",
    "start": "1689120",
    "end": "1697640"
  },
  {
    "text": "if there was multiple um if there were multiple uh list requests concurrently",
    "start": "1697640",
    "end": "1704440"
  },
  {
    "text": "and one Watch the watch could never get uh uh could be hanging there waiting for",
    "start": "1704440",
    "end": "1709960"
  },
  {
    "text": "a rent and can be uh there could be minutes passing and it will not get any update and because of how kubernetes",
    "start": "1709960",
    "end": "1716919"
  },
  {
    "text": "reconcilation Loop works this causes total chaos because you deploy a pot you",
    "start": "1716919",
    "end": "1723039"
  },
  {
    "text": "create it and nothing happens and all controllers don't don't observe",
    "start": "1723039",
    "end": "1729320"
  },
  {
    "text": "this um so uh the fix unfortunately was pretty involved required range",
    "start": "1730360",
    "end": "1737880"
  },
  {
    "text": "implementation and collaboration between both etcd and kubernetes scalability and",
    "start": "1737880",
    "end": "1747559"
  },
  {
    "text": "Golan teams to improve the uh to make the algorithm handle the watch",
    "start": "1747559",
    "end": "1754080"
  },
  {
    "text": "properly uh at the end we only recently have update provided an update to the um",
    "start": "1754080",
    "end": "1761600"
  },
  {
    "text": "to the at CD that include this fix um but because it's it took us long",
    "start": "1761600",
    "end": "1768559"
  },
  {
    "text": "time and we knew that it's not a easy fix we needed to have an immediate mitigation so if you're running older",
    "start": "1768559",
    "end": "1777720"
  },
  {
    "text": "um older version of etcd or uh yeah if you're running older version",
    "start": "1777720",
    "end": "1785000"
  },
  {
    "text": "of atcd maybe you can use the client mitigate or mitigation mitigation by",
    "start": "1785000",
    "end": "1790279"
  },
  {
    "text": "separating grpc and HTTP uh server and that's allowing uh the",
    "start": "1790279",
    "end": "1797760"
  },
  {
    "text": "multiplex or does skipping the issue with uh grpc request going through HTTP",
    "start": "1797760",
    "end": "1804360"
  },
  {
    "text": "server to detect if you are interested if you can have this issue or if you can",
    "start": "1804360",
    "end": "1810519"
  },
  {
    "text": "hit this issue in new clusters I really recommend monitoring uh monitoring those",
    "start": "1810519",
    "end": "1816559"
  },
  {
    "text": "two uh metrics so we discussed a couple of",
    "start": "1816559",
    "end": "1823320"
  },
  {
    "text": "failures and you can see the pattern that all all of them are cluster lever",
    "start": "1823320",
    "end": "1828480"
  },
  {
    "text": "there was no thing that XD could done without of help of either operators or",
    "start": "1828480",
    "end": "1835159"
  },
  {
    "text": "or proper Sr met mitigation and for those issues at CD it's only up",
    "start": "1835159",
    "end": "1844960"
  },
  {
    "text": "to you to set up uh your cluster properly to and to mitigate uh you",
    "start": "1844960",
    "end": "1851640"
  },
  {
    "text": "should not think that or you should not assume that kubernetes gives you 100",
    "start": "1851640",
    "end": "1857399"
  },
  {
    "text": "100% reliability and 100% um protection about uh",
    "start": "1857399",
    "end": "1865279"
  },
  {
    "text": "from from developer failures so any change to your cluster can lead to uh",
    "start": "1865279",
    "end": "1872279"
  },
  {
    "text": "lead to a problem that you may be not discovered so putting all your eggs in",
    "start": "1872279",
    "end": "1877320"
  },
  {
    "text": "one basket can end up pretty pretty badly so my suggestion would be to",
    "start": "1877320",
    "end": "1883039"
  },
  {
    "text": "separate your cluster and run run them as or minimize your blast blast radius so",
    "start": "1883039",
    "end": "1891080"
  },
  {
    "text": "if you have a issue with one cluster that you've never um never experienced",
    "start": "1891080",
    "end": "1896679"
  },
  {
    "text": "you should think about having either a a second that can re take over your",
    "start": "1896679",
    "end": "1902279"
  },
  {
    "text": "traffic or have a easy way and documented way to mitigate and recreate",
    "start": "1902279",
    "end": "1908200"
  },
  {
    "text": "a new cluster in its place uh because it's better to uh to",
    "start": "1908200",
    "end": "1916440"
  },
  {
    "text": "have partial down time then be totally uh and maybe some performance uh or",
    "start": "1916440",
    "end": "1921720"
  },
  {
    "text": "latency increase than being totally uh",
    "start": "1921720",
    "end": "1926799"
  },
  {
    "text": "down uh second way to to handle such issues is to do a canary rollout so the",
    "start": "1927399",
    "end": "1934320"
  },
  {
    "text": "same like blue green deployment you can uh you should treat every change as a",
    "start": "1934320",
    "end": "1940120"
  },
  {
    "text": "potential disruption to your cluster any like etcd upgrade any any kubernetes",
    "start": "1940120",
    "end": "1947960"
  },
  {
    "text": "upgrade any of your application that upgrade that maybe has some new traffic",
    "start": "1947960",
    "end": "1955720"
  },
  {
    "text": "pattern that is abusive to atcd uh should be to if you treat all",
    "start": "1955720",
    "end": "1962559"
  },
  {
    "text": "those changes as a blackbox and roll out them independently it should allow you",
    "start": "1962559",
    "end": "1967760"
  },
  {
    "text": "to uh be able to discover the issues uh early and and contain",
    "start": "1967760",
    "end": "1974440"
  },
  {
    "text": "them and if uh uh the the third iteration of the idea would be to to",
    "start": "1974440",
    "end": "1981720"
  },
  {
    "text": "think about changes as uh yeah think all changes as a",
    "start": "1981720",
    "end": "1988720"
  },
  {
    "text": "disruption and uh qualifying and soaking all of your changes that are coming to",
    "start": "1988720",
    "end": "1996760"
  },
  {
    "text": "to your cluster and you can validate if or you can minimize the risk by",
    "start": "1996760",
    "end": "2003799"
  },
  {
    "text": "separating your clusters into group of different criticality and assigning them",
    "start": "2003799",
    "end": "2011200"
  },
  {
    "text": "u a multiphase roll out uh and",
    "start": "2011200",
    "end": "2016480"
  },
  {
    "text": "having uh a direct qualification targets for each each um part of your Fleet and",
    "start": "2016480",
    "end": "2025440"
  },
  {
    "text": "this is especially what GK does by separating uh their Fleet in into",
    "start": "2025440",
    "end": "2031760"
  },
  {
    "text": "channels and allowing customers to pick it which channel or what kind of",
    "start": "2031760",
    "end": "2038039"
  },
  {
    "text": "disturbance in your service you can tolerate so you can run your test",
    "start": "2038039",
    "end": "2043559"
  },
  {
    "text": "clusters in the first phase and qualif we can qualify the um the at CD in the",
    "start": "2043559",
    "end": "2049599"
  },
  {
    "text": "first phase on the test cluster and not cause any cause on minimal issues uh and",
    "start": "2049599",
    "end": "2056118"
  },
  {
    "text": "avoid production impact so all of those Solutions are a standard",
    "start": "2056119",
    "end": "2066000"
  },
  {
    "text": "application um mitigation to to failures and there",
    "start": "2066000",
    "end": "2073200"
  },
  {
    "text": "is nothing new that that I proposed",
    "start": "2073200",
    "end": "2079520"
  },
  {
    "text": "and uh so the main maybe problem with them not",
    "start": "2079800",
    "end": "2085240"
  },
  {
    "text": "everyone can run multiple clusters not everyone can can take the cost of running them so what's",
    "start": "2085240",
    "end": "2092358"
  },
  {
    "text": "the alternative approach what is the secret and for me uh all of the like my",
    "start": "2092359",
    "end": "2099200"
  },
  {
    "text": "understanding is all of those issues were discussed and available publicly you could read them you can verify them",
    "start": "2099200",
    "end": "2105480"
  },
  {
    "text": "and there ET see like for example kubernetes events issue was",
    "start": "2105480",
    "end": "2110720"
  },
  {
    "text": "their uh was there for years as a default separating the HD events uh was",
    "start": "2110720",
    "end": "2118880"
  },
  {
    "text": "a solution available for years and I still see people coming to",
    "start": "2118880",
    "end": "2123920"
  },
  {
    "text": "me and asking about how to handle uh events properly so uh for me there is",
    "start": "2123920",
    "end": "2130119"
  },
  {
    "text": "like one uh observation that people treat open source as a ready U solution",
    "start": "2130119",
    "end": "2138119"
  },
  {
    "text": "that you can just skip your skip your uh a lot of experience or skip Gathering",
    "start": "2138119",
    "end": "2145440"
  },
  {
    "text": "experience and take a a free beer out of the shelf and treat it as a uh um treat",
    "start": "2145440",
    "end": "2154200"
  },
  {
    "text": "it as a solution uh to for for follow your reliability and",
    "start": "2154200",
    "end": "2160480"
  },
  {
    "text": "unfortunately this is not true open source is mostly about the freedom to exchange ideas share them and learn",
    "start": "2160480",
    "end": "2168240"
  },
  {
    "text": "together so when you think about running at CD uh you should",
    "start": "2168240",
    "end": "2175280"
  },
  {
    "text": "either uh you should invest your your time into uh knowing how open source",
    "start": "2175280",
    "end": "2184000"
  },
  {
    "text": "Community runs it and make making sure that you're avoiding common pitfalls by",
    "start": "2184000",
    "end": "2190839"
  },
  {
    "text": "going outside of the uh common paths that at CD has been tested for a long",
    "start": "2190839",
    "end": "2196640"
  },
  {
    "text": "long time uh so etcd is a uh production great",
    "start": "2196640",
    "end": "2203400"
  },
  {
    "text": "key value store but only you can make it production ready and",
    "start": "2203400",
    "end": "2211680"
  },
  {
    "text": "uh most of the issues that I discussed their Solutions cannot be backed into",
    "start": "2211680",
    "end": "2217880"
  },
  {
    "text": "open source because they require full re re redesign or re architecture or your",
    "start": "2217880",
    "end": "2225200"
  },
  {
    "text": "kubernetes cluster and you should take it account and think about how do you uh",
    "start": "2225200",
    "end": "2231640"
  },
  {
    "text": "how do you make sure that um that you can use or that how you can make sure",
    "start": "2231640",
    "end": "2239280"
  },
  {
    "text": "that you're uh you know all the common pit fils so uh for me the best way for that",
    "start": "2239280",
    "end": "2247599"
  },
  {
    "text": "was uh to tap the collective uh Community experience uh most",
    "start": "2247599",
    "end": "2256800"
  },
  {
    "text": "or most people I talk to still run ATD",
    "start": "2257160",
    "end": "2262480"
  },
  {
    "text": "versions that are have been known to have uh Corruptions and been multi",
    "start": "2262480",
    "end": "2269920"
  },
  {
    "text": "time have been multiple emails announcing problems with them and not",
    "start": "2269920",
    "end": "2276359"
  },
  {
    "text": "and there are still uh not follow or there are still many people that don't follow it so my recommendation would be",
    "start": "2276359",
    "end": "2283920"
  },
  {
    "text": "you will save you yourself a lot of time to just know what is community doing and",
    "start": "2283920",
    "end": "2289079"
  },
  {
    "text": "what is the discussion and we do a full announcement about the issues so you should follow the ETS mail link",
    "start": "2289079",
    "end": "2297000"
  },
  {
    "text": "list uh if you have any questions or any",
    "start": "2297000",
    "end": "2302480"
  },
  {
    "text": "problems uh or you go a out of the tested verified dimensions of atcd or",
    "start": "2302480",
    "end": "2309599"
  },
  {
    "text": "kubernetes you should first or you should ask questions and make sure that people uh that you double check with the",
    "start": "2309599",
    "end": "2318040"
  },
  {
    "text": "um you double check what is opinion and what people have already done because",
    "start": "2318040",
    "end": "2325440"
  },
  {
    "text": "you're not you're not unique in what you're doing there are definitely people that",
    "start": "2325440",
    "end": "2330680"
  },
  {
    "text": "have done it before if you just ask them they will be happy to share what they did and be sure to share your experience",
    "start": "2330680",
    "end": "2339680"
  },
  {
    "text": "share uh experience yourself and file an issue and talk about it so we can",
    "start": "2339680",
    "end": "2346000"
  },
  {
    "text": "collaboratively debug and help you get uh what you",
    "start": "2346000",
    "end": "2351160"
  },
  {
    "text": "want so in summary uh cluster scope failures are a thing so you should plan",
    "start": "2351160",
    "end": "2357720"
  },
  {
    "text": "for them you should understand what can happen badly if you",
    "start": "2357720",
    "end": "2363079"
  },
  {
    "text": "don't uh if uh or you should assume that things can go",
    "start": "2363079",
    "end": "2369800"
  },
  {
    "text": "wrong and you should have a plan how how is will be your reaction and you should",
    "start": "2369800",
    "end": "2375040"
  },
  {
    "text": "also test it there is a lot of known issues and uh",
    "start": "2375040",
    "end": "2381200"
  },
  {
    "text": "most of them require unfortunately require a mitigation because of backward in incompatibility or require you to",
    "start": "2381200",
    "end": "2388480"
  },
  {
    "text": "change your uh architecture of or how you run your cluster and my main suggestion would be",
    "start": "2388480",
    "end": "2396560"
  },
  {
    "text": "don't try to figure out your everything yourself T the community Collective Community experience and uh talk to us",
    "start": "2396560",
    "end": "2405240"
  },
  {
    "text": "so we can uh so we can learn together uh that's all from me sorry for",
    "start": "2405240",
    "end": "2411319"
  },
  {
    "text": "running out of time if you have any feedback here's the QR code um there's",
    "start": "2411319",
    "end": "2417680"
  },
  {
    "text": "still some stickers at the front seat if you want there are also some chocolates",
    "start": "2417680",
    "end": "2423640"
  },
  {
    "text": "from the speaker [Music] [Applause]",
    "start": "2423640",
    "end": "2434989"
  }
]