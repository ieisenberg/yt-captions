[
  {
    "text": "hello everybody my name is Jacob and I",
    "start": "640",
    "end": "2639"
  },
  {
    "text": "work at Haroxy Technologies you might",
    "start": "2639",
    "end": "5279"
  },
  {
    "text": "know of us as the company behind Harroxy",
    "start": "5279",
    "end": "8320"
  },
  {
    "text": "the legendary opensource software load",
    "start": "8320",
    "end": "10719"
  },
  {
    "text": "balancer that a lot of you are probably",
    "start": "10719",
    "end": "12840"
  },
  {
    "text": "using today I'm going to be piggybacking",
    "start": "12840",
    "end": "15440"
  },
  {
    "text": "on the previous keynote about",
    "start": "15440",
    "end": "17400"
  },
  {
    "text": "performance and security and LLMs",
    "start": "17400",
    "end": "21279"
  },
  {
    "text": "because I think there are a lot of",
    "start": "21279",
    "end": "22640"
  },
  {
    "text": "topics we should talk about i talked to",
    "start": "22640",
    "end": "25199"
  },
  {
    "text": "a lot of our customers and they all say",
    "start": "25199",
    "end": "27279"
  },
  {
    "text": "we're all in on AI but I think the risks",
    "start": "27279",
    "end": "30480"
  },
  {
    "text": "of that are yet to be understood i think",
    "start": "30480",
    "end": "32960"
  },
  {
    "text": "we're kind of in the 2003 of OASP top 10",
    "start": "32960",
    "end": "36880"
  },
  {
    "text": "for web security right now in terms of",
    "start": "36880",
    "end": "39200"
  },
  {
    "text": "LLM security so when they talk about I'm",
    "start": "39200",
    "end": "43040"
  },
  {
    "text": "being allin on AI security or an AI the",
    "start": "43040",
    "end": "47760"
  },
  {
    "text": "first step they usually think about is",
    "start": "47760",
    "end": "49520"
  },
  {
    "text": "well we're going to build an AI gateway",
    "start": "49520",
    "end": "51760"
  },
  {
    "text": "and so what's an AI gateway well it's an",
    "start": "51760",
    "end": "53840"
  },
  {
    "text": "API gateway right we've all built one at",
    "start": "53840",
    "end": "56879"
  },
  {
    "text": "this point or most of the projects in",
    "start": "56879",
    "end": "59840"
  },
  {
    "text": "load balancers have built one API",
    "start": "59840",
    "end": "62079"
  },
  {
    "text": "gateway right now an AI gateway so we're",
    "start": "62079",
    "end": "64878"
  },
  {
    "text": "going to do some authentication we're",
    "start": "64879",
    "end": "66320"
  },
  {
    "text": "going to do raid limiting we're going to",
    "start": "66320",
    "end": "67840"
  },
  {
    "text": "do PII detection or maybe PII extraction",
    "start": "67840",
    "end": "70960"
  },
  {
    "text": "and prompt routing before we send the",
    "start": "70960",
    "end": "73360"
  },
  {
    "text": "data to an inference engine which is",
    "start": "73360",
    "end": "75439"
  },
  {
    "text": "ultimately an HTTP API but I think one",
    "start": "75439",
    "end": "78159"
  },
  {
    "text": "of the things that's missing there or",
    "start": "78159",
    "end": "80080"
  },
  {
    "text": "possibly missing is prompt security and",
    "start": "80080",
    "end": "83200"
  },
  {
    "text": "that's really you know the ignore all",
    "start": "83200",
    "end": "85600"
  },
  {
    "text": "previous instructions that doesn't work",
    "start": "85600",
    "end": "87840"
  },
  {
    "text": "anymore or a time bandit attack that",
    "start": "87840",
    "end": "90560"
  },
  {
    "text": "used to work with open AI until recently",
    "start": "90560",
    "end": "93600"
  },
  {
    "text": "so there are obviously solutions to that",
    "start": "93600",
    "end": "96159"
  },
  {
    "text": "for example there's a prompt guard model",
    "start": "96159",
    "end": "98400"
  },
  {
    "text": "and a llama guard model from meta",
    "start": "98400",
    "end": "100320"
  },
  {
    "text": "there's a shield gemma from Google and",
    "start": "100320",
    "end": "102560"
  },
  {
    "text": "in the end many of these are built on",
    "start": "102560",
    "end": "104960"
  },
  {
    "text": "some kind of a variation of dberta",
    "start": "104960",
    "end": "106840"
  },
  {
    "text": "classification it's a set of models so",
    "start": "106840",
    "end": "109759"
  },
  {
    "text": "they are actually large language models",
    "start": "109759",
    "end": "112240"
  },
  {
    "text": "that are classifying you ask is this",
    "start": "112240",
    "end": "114799"
  },
  {
    "text": "prompt safe yes or no and they answer",
    "start": "114799",
    "end": "116960"
  },
  {
    "text": "yes or no so it's ultimately",
    "start": "116960",
    "end": "118799"
  },
  {
    "text": "classification problem so we're going to",
    "start": "118799",
    "end": "121680"
  },
  {
    "text": "add prompt security to the AI gateway",
    "start": "121680",
    "end": "124079"
  },
  {
    "text": "and basically run the model itself in",
    "start": "124079",
    "end": "126640"
  },
  {
    "text": "the AI gateway um so I did that and I",
    "start": "126640",
    "end": "130080"
  },
  {
    "text": "ran these models inside an AI gateway",
    "start": "130080",
    "end": "132720"
  },
  {
    "text": "inside a load balancer and ultimately",
    "start": "132720",
    "end": "134879"
  },
  {
    "text": "what I found and this is what I want to",
    "start": "134879",
    "end": "136959"
  },
  {
    "text": "talk about is it's pretty slow so if you",
    "start": "136959",
    "end": "140720"
  },
  {
    "text": "run a model like this on a G6X large AWS",
    "start": "140720",
    "end": "144080"
  },
  {
    "text": "instance which is ultimately a pretty",
    "start": "144080",
    "end": "146000"
  },
  {
    "text": "big instance at this point if you start",
    "start": "146000",
    "end": "148560"
  },
  {
    "text": "approaching 500 tokens it takes",
    "start": "148560",
    "end": "150800"
  },
  {
    "text": "somewhere between 150 to 200",
    "start": "150800",
    "end": "152959"
  },
  {
    "text": "milliseconds to process the prompt and",
    "start": "152959",
    "end": "155680"
  },
  {
    "text": "most prompts are bigger and most of",
    "start": "155680",
    "end": "158480"
  },
  {
    "text": "these models have a context window of",
    "start": "158480",
    "end": "160319"
  },
  {
    "text": "about 500 tokens so if your prompt is",
    "start": "160319",
    "end": "163200"
  },
  {
    "text": "2,000 tokens you have to do this four",
    "start": "163200",
    "end": "165040"
  },
  {
    "text": "times and that's a second of time you're",
    "start": "165040",
    "end": "168000"
  },
  {
    "text": "processing that's that's lifetimes in a",
    "start": "168000",
    "end": "171040"
  },
  {
    "text": "load balancer world and if you start",
    "start": "171040",
    "end": "173840"
  },
  {
    "text": "doing this in parallel like a proper AI",
    "start": "173840",
    "end": "176959"
  },
  {
    "text": "gateway it gets worse so here's if you",
    "start": "176959",
    "end": "180480"
  },
  {
    "text": "can see I ran a nonoptimized model so",
    "start": "180480",
    "end": "183360"
  },
  {
    "text": "basic transformers and then an optimized",
    "start": "183360",
    "end": "185440"
  },
  {
    "text": "model with an inference engine and on",
    "start": "185440",
    "end": "188080"
  },
  {
    "text": "this instance you cannot almost ever",
    "start": "188080",
    "end": "190000"
  },
  {
    "text": "reach more than 60 requests a second and",
    "start": "190000",
    "end": "192560"
  },
  {
    "text": "once you start getting to a concurrency",
    "start": "192560",
    "end": "194239"
  },
  {
    "text": "of let's say eight concurrent requests",
    "start": "194239",
    "end": "197519"
  },
  {
    "text": "you will never reach over 40 basically",
    "start": "197519",
    "end": "200640"
  },
  {
    "text": "so there's a lot of work to be done in",
    "start": "200640",
    "end": "203040"
  },
  {
    "text": "making this faster there are",
    "start": "203040",
    "end": "205200"
  },
  {
    "text": "optimization strategies we can run an",
    "start": "205200",
    "end": "207840"
  },
  {
    "text": "optimized inference engines i did that",
    "start": "207840",
    "end": "210000"
  },
  {
    "text": "and you can get about 30% you can enable",
    "start": "210000",
    "end": "212959"
  },
  {
    "text": "token caching but ultimately token",
    "start": "212959",
    "end": "215360"
  },
  {
    "text": "caching as we know it right now is meant",
    "start": "215360",
    "end": "217840"
  },
  {
    "text": "for generative AI and we're not doing",
    "start": "217840",
    "end": "220400"
  },
  {
    "text": "generative AI we're doing classification",
    "start": "220400",
    "end": "222959"
  },
  {
    "text": "we're classifying the model so it's out",
    "start": "222959",
    "end": "226159"
  },
  {
    "text": "there to know if this is useful long",
    "start": "226159",
    "end": "228080"
  },
  {
    "text": "term there are some advanced techniques",
    "start": "228080",
    "end": "230480"
  },
  {
    "text": "i've tried these as well and it works",
    "start": "230480",
    "end": "232560"
  },
  {
    "text": "like you can filter for some of the",
    "start": "232560",
    "end": "234239"
  },
  {
    "text": "basic prompts and bad words with text",
    "start": "234239",
    "end": "236560"
  },
  {
    "text": "filtering the only problem is if you",
    "start": "236560",
    "end": "239599"
  },
  {
    "text": "make a typo in the filter or in your",
    "start": "239599",
    "end": "241920"
  },
  {
    "text": "text the text filter will no longer work",
    "start": "241920",
    "end": "244560"
  },
  {
    "text": "while the LLM will interpret it",
    "start": "244560",
    "end": "246080"
  },
  {
    "text": "correctly so all you have to do is",
    "start": "246080",
    "end": "247760"
  },
  {
    "text": "mangle a few words and it's just going",
    "start": "247760",
    "end": "249680"
  },
  {
    "text": "to work so I think there are a few",
    "start": "249680",
    "end": "251280"
  },
  {
    "text": "lessons learned we need to innovate new",
    "start": "251280",
    "end": "253519"
  },
  {
    "text": "techniques using the tools we have and",
    "start": "253519",
    "end": "255360"
  },
  {
    "text": "we have a few tools two using AI itself",
    "start": "255360",
    "end": "259519"
  },
  {
    "text": "to secure AI in the load balancer works",
    "start": "259519",
    "end": "262160"
  },
  {
    "text": "but it's still out there if it's viable",
    "start": "262160",
    "end": "264240"
  },
  {
    "text": "it's something that we are researching",
    "start": "264240",
    "end": "265840"
  },
  {
    "text": "we might need to research altogether",
    "start": "265840",
    "end": "267919"
  },
  {
    "text": "some smaller models that can run on a",
    "start": "267919",
    "end": "270400"
  },
  {
    "text": "load balancer and can run much quicker",
    "start": "270400",
    "end": "273520"
  },
  {
    "text": "and three I think that the AI gateways",
    "start": "273520",
    "end": "275919"
  },
  {
    "text": "are necessary but the security is is",
    "start": "275919",
    "end": "278320"
  },
  {
    "text": "evolving in the end as I said I think",
    "start": "278320",
    "end": "280960"
  },
  {
    "text": "we're in the 2003 of OAS top 10 right",
    "start": "280960",
    "end": "284160"
  },
  {
    "text": "now in AI security everybody's trying",
    "start": "284160",
    "end": "286560"
  },
  {
    "text": "something everybody's doing it but we",
    "start": "286560",
    "end": "289199"
  },
  {
    "text": "don't know what's going to work a few",
    "start": "289199",
    "end": "290560"
  },
  {
    "text": "years from now because it's really hard",
    "start": "290560",
    "end": "292560"
  },
  {
    "text": "to keep up so thank you so much if you",
    "start": "292560",
    "end": "294720"
  },
  {
    "text": "have any questions please come to the",
    "start": "294720",
    "end": "296560"
  },
  {
    "text": "booth i would be happy to talk about",
    "start": "296560",
    "end": "297919"
  },
  {
    "text": "this a little bit more",
    "start": "297919",
    "end": "299620"
  },
  {
    "text": "[Applause]",
    "start": "299620",
    "end": "305130"
  }
]