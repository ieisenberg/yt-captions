[
  {
    "text": "hello everyone today we are going to talk about how to handle no shout out in kubernetes my",
    "start": "359",
    "end": "6420"
  },
  {
    "text": "name is Xin Yang I work at VMware in the cloud native storage team I'm also a",
    "start": "6420",
    "end": "11760"
  },
  {
    "text": "co-chair of kubernetes seek storage hi everyone I'm ashitos I work at VMware",
    "start": "11760",
    "end": "19020"
  },
  {
    "text": "on cluster life cycle team and I'm here to talk about notes are down in communities",
    "start": "19020",
    "end": "24840"
  },
  {
    "text": "or what using you can get started here's our agenda today",
    "start": "24840",
    "end": "31199"
  },
  {
    "text": "we're going to talk about what is a graceful no Shadow what is non-gracefulness we will talk about how",
    "start": "31199",
    "end": "37800"
  },
  {
    "text": "to handle them we'll give a demo and we will talk about next steps",
    "start": "37800",
    "end": "44120"
  },
  {
    "text": "in a kubernetes cluster it is possible for node to shut down this could happen",
    "start": "45120",
    "end": "51300"
  },
  {
    "text": "either in a planned way or it could happen unexpectedly",
    "start": "51300",
    "end": "57059"
  },
  {
    "text": "but no shutdown could happen for many reasons it could be that you need to apply a",
    "start": "57239",
    "end": "63059"
  },
  {
    "text": "security patch you need to do your kernel upgrade you need to reboot a node or it could be due to the preemption of",
    "start": "63059",
    "end": "70020"
  },
  {
    "text": "the VM instances or it could be that there's a harbor failure or some software problem that",
    "start": "70020",
    "end": "76740"
  },
  {
    "text": "causes that to happen you can trigger no shutdown by around a",
    "start": "76740",
    "end": "81960"
  },
  {
    "text": "shutdown or a part of comment or just physically push a button to shut down",
    "start": "81960",
    "end": "87780"
  },
  {
    "text": "the the machine if you do that without joining the notes",
    "start": "87780",
    "end": "94439"
  },
  {
    "text": "then that could cause a workload failures anoshadang could either be graceful or",
    "start": "94439",
    "end": "101460"
  },
  {
    "text": "non-graceful let me talk about Greece for no Sharon first",
    "start": "101460",
    "end": "107240"
  },
  {
    "text": "graceful no Shadow feature was introduced in kubernetes 1.20 release",
    "start": "107759",
    "end": "113759"
  },
  {
    "text": "and it moved to Beta in 1.21 this allows cubelet to detect a no",
    "start": "113759",
    "end": "121259"
  },
  {
    "text": "shutdown and properly terminate the parts make sure all the resources are",
    "start": "121259",
    "end": "126600"
  },
  {
    "text": "released before the real shutdown and parts are terminated in two phases",
    "start": "126600",
    "end": "133080"
  },
  {
    "text": "first the regular pots and then the critical parts to make sure that the",
    "start": "133080",
    "end": "138540"
  },
  {
    "text": "critical function of the application can work as long as possible",
    "start": "138540",
    "end": "144739"
  },
  {
    "text": "without this feature user have to make sure they manually join the notes before",
    "start": "146180",
    "end": "153440"
  },
  {
    "text": "shutting down the node however no shutdown Could Happen unexpectedly",
    "start": "153440",
    "end": "160379"
  },
  {
    "text": "in that case the parts could be evicted unsafely if the node is not drained and",
    "start": "160379",
    "end": "167760"
  },
  {
    "text": "your application would see errors and your workloads may not function properly",
    "start": "167760",
    "end": "175340"
  },
  {
    "text": "so let's talk about how the grace for no shutdown works for this feature cubelet relies on the",
    "start": "176819",
    "end": "185819"
  },
  {
    "text": "system D's inhibitor lock mechanism and when cubely starts it requires this",
    "start": "185819",
    "end": "192599"
  },
  {
    "text": "delay type initiator lock and it watches for the shutdown events",
    "start": "192599",
    "end": "198599"
  },
  {
    "text": "when it detects a shutdown it delays the shutdown and terminate the",
    "start": "198599",
    "end": "204599"
  },
  {
    "text": "parts make sure everything is released before the real Sharon",
    "start": "204599",
    "end": "211220"
  },
  {
    "text": "there are two config parameters in couplet that you need to set for this",
    "start": "212040",
    "end": "217680"
  },
  {
    "text": "feature to work the first one is the Sharon grace period That's the total duration for both the regular and the",
    "start": "217680",
    "end": "224220"
  },
  {
    "text": "critical parts to be terminated and the second parameter is the shutdown grace",
    "start": "224220",
    "end": "230040"
  },
  {
    "text": "period critical parts that's the time needed for the critical parts to be",
    "start": "230040",
    "end": "235620"
  },
  {
    "text": "terminated and that's always smaller than the total duration",
    "start": "235620",
    "end": "242400"
  },
  {
    "text": "and the graceful notion feature it works in two phases the notes the parts are",
    "start": "242400",
    "end": "250140"
  },
  {
    "text": "terminating two faces first the regulars and then the critical Parts but if you want to have more granular control then",
    "start": "250140",
    "end": "257579"
  },
  {
    "text": "there is another feature called a pod priority based grease for no shutdown that's an alpha feature introduced in",
    "start": "257579",
    "end": "264780"
  },
  {
    "text": "1.23 release so you can configure your Port priority",
    "start": "264780",
    "end": "270660"
  },
  {
    "text": "classes into a multiple classes types and then the parts will be shut down in",
    "start": "270660",
    "end": "278280"
  },
  {
    "text": "multiple phases depending on how you define your priority classes",
    "start": "278280",
    "end": "284220"
  },
  {
    "text": "so that's how the greasework shutdown works now I'm going to hand it over to",
    "start": "284220",
    "end": "289500"
  },
  {
    "text": "Ashley to talk about non-creation thank you",
    "start": "289500",
    "end": "295740"
  },
  {
    "text": "let's get started on non-graceful sit down and this is a feature that was",
    "start": "295740",
    "end": "302040"
  },
  {
    "text": "introduced in 1.24 and we are targeting beta in 1.26",
    "start": "302040",
    "end": "308220"
  },
  {
    "text": "Zing just talked about graceful shutdown and there are scenarios",
    "start": "308220",
    "end": "313940"
  },
  {
    "text": "when the shutdown Is Not So Graceful it could be due to some configuration error",
    "start": "313940",
    "end": "320520"
  },
  {
    "text": "or maybe due to a hardware failure or if a shutdown is not detected by",
    "start": "320520",
    "end": "326039"
  },
  {
    "text": "cubelet and this kind of shutdown can be termed as known graceful one",
    "start": "326039",
    "end": "331380"
  },
  {
    "text": "and it can be problem for stateful workloads let us see",
    "start": "331380",
    "end": "338660"
  },
  {
    "text": "I've already talked about system D inhibitor log if the shutdown does not",
    "start": "340560",
    "end": "346560"
  },
  {
    "text": "trigger inhibitor inhibitor lock then it will not be ejected by cubelet",
    "start": "346560",
    "end": "352880"
  },
  {
    "text": "or if you set shut down grace period or shut down grace period critical paths",
    "start": "352880",
    "end": "359160"
  },
  {
    "text": "incorrectly this can also let lead to non-graceful Saturn",
    "start": "359160",
    "end": "365100"
  },
  {
    "text": "and as a side effect of this what happens is the prod moves to a",
    "start": "365100",
    "end": "370620"
  },
  {
    "text": "terminating State especially the parts that are using volumes",
    "start": "370620",
    "end": "376440"
  },
  {
    "text": "but if the node that went into an own graceful shutdown comes back online",
    "start": "376440",
    "end": "382560"
  },
  {
    "text": "everything just works fine and let us see why the part gets stuck",
    "start": "382560",
    "end": "388440"
  },
  {
    "text": "in terminating State and what happens next so here is the experiment that I did",
    "start": "388440",
    "end": "394139"
  },
  {
    "text": "like some time back I used vsphere UI to test that so I deployed this stateful set",
    "start": "394139",
    "end": "401220"
  },
  {
    "text": "and I triggered a set down from the vsphere UI button",
    "start": "401220",
    "end": "406620"
  },
  {
    "text": "and that as a matter of fact is a non-graceful set down",
    "start": "406620",
    "end": "411660"
  },
  {
    "text": "and what I observed is the Pod went into terminating state",
    "start": "411660",
    "end": "417539"
  },
  {
    "text": "after five minutes and also what I observed is",
    "start": "417539",
    "end": "424740"
  },
  {
    "text": "that part is stuck in terminating State even after six minutes I code six",
    "start": "424740",
    "end": "430440"
  },
  {
    "text": "minutes here it is an important number uh I'll come to that later",
    "start": "430440",
    "end": "436759"
  },
  {
    "text": "I did another experiment I again created a stateful set",
    "start": "439020",
    "end": "444419"
  },
  {
    "text": "I did the same set down and observe that the Pod went into terminating state",
    "start": "444419",
    "end": "452699"
  },
  {
    "text": "after almost five minutes and then I deleted the Pod forcefully",
    "start": "452699",
    "end": "457740"
  },
  {
    "text": "with a grace period of zero and what I observed is",
    "start": "457740",
    "end": "463199"
  },
  {
    "text": "the Pod got scheduled to a different node and it was in container creating state",
    "start": "463199",
    "end": "471960"
  },
  {
    "text": "for six minutes like but after six minutes the Pod came back online because",
    "start": "471960",
    "end": "477840"
  },
  {
    "text": "six minutes is that Default Time timeout before the volume get detached from the",
    "start": "477840",
    "end": "485160"
  },
  {
    "text": "older stealth pod this may not work for stateful sets",
    "start": "485160",
    "end": "490199"
  },
  {
    "text": "because if the pod is in terminating state state full",
    "start": "490199",
    "end": "495360"
  },
  {
    "text": "set does not allow you to create in your in your pod it is a deployment so you use a deployment",
    "start": "495360",
    "end": "502139"
  },
  {
    "text": "part you know I mean it's a stateful part",
    "start": "502139",
    "end": "506720"
  },
  {
    "text": "uh I wanted to talk about the goals here of this feature",
    "start": "508020",
    "end": "513719"
  },
  {
    "text": "the goal is to actually help increase availability for stateful workloads because as you saw it can take like",
    "start": "513719",
    "end": "521159"
  },
  {
    "text": "approximately 11 minutes for your container of the pot to come back online",
    "start": "521159",
    "end": "527459"
  },
  {
    "text": "and we want to handle such non-recoverable cases when Hardware goes down or the OS is broken",
    "start": "527459",
    "end": "535800"
  },
  {
    "text": "there has also been talks about node and control plane partitioning in case node",
    "start": "535800",
    "end": "541019"
  },
  {
    "text": "shutdowns but it does not include in Gold because when",
    "start": "541019",
    "end": "547320"
  },
  {
    "text": "a node is not ready it could be due to variety of reasons one could be due to a split of the network",
    "start": "547320",
    "end": "553620"
  },
  {
    "text": "and we really don't know if the node is not ready because the network is",
    "start": "553620",
    "end": "559620"
  },
  {
    "text": "and Rachel or the node actually went into a non-graceful set down",
    "start": "559620",
    "end": "565500"
  },
  {
    "text": "there is no goals to also have some in cluster logic to handle such",
    "start": "565500",
    "end": "570600"
  },
  {
    "text": "partitioning",
    "start": "570600",
    "end": "573259"
  },
  {
    "text": "let us take a look at how the gracefuls are down non-gracefuls are Done Works",
    "start": "575760",
    "end": "581100"
  },
  {
    "text": "so we use the native taint apis of kubernetes in this case",
    "start": "581100",
    "end": "586140"
  },
  {
    "text": "and as of now to utilize this feature one will have to",
    "start": "586140",
    "end": "591600"
  },
  {
    "text": "do manual steps and what happens is once you figure out that a node has went into a shutdown",
    "start": "591600",
    "end": "599339"
  },
  {
    "text": "non-gracefully you apply this well-known taint that is",
    "start": "599339",
    "end": "604640"
  },
  {
    "text": "node.cubernetes dot IO out of service and once you do that",
    "start": "604640",
    "end": "612680"
  },
  {
    "text": "there is two steps that is happening behind the scenes what happens is",
    "start": "612680",
    "end": "618300"
  },
  {
    "text": "the Pod GC controller delete support that do not have a",
    "start": "618300",
    "end": "623459"
  },
  {
    "text": "matching generation and it deletes it forcefully and after that attached t-test",
    "start": "623459",
    "end": "630540"
  },
  {
    "text": "controller is going to quickly do a force volume detach operation",
    "start": "630540",
    "end": "636240"
  },
  {
    "text": "so that the newer part that might get spawned on in your node the volume gets attached successfully",
    "start": "636240",
    "end": "644180"
  },
  {
    "text": "if you want to use this feature you'll have to enable the feature gate",
    "start": "647100",
    "end": "652260"
  },
  {
    "text": "because this is still in Alpha and the feature is the feature gate name is node out of service volume detach",
    "start": "652260",
    "end": "660360"
  },
  {
    "text": "you have to set this flag true in Cube controller manager",
    "start": "660360",
    "end": "666060"
  },
  {
    "text": "and once you do that you just have to utilize the taint and",
    "start": "666060",
    "end": "671100"
  },
  {
    "text": "if you if you know that a node has went into shutdown non-gracefully you can just use the taint command that you can",
    "start": "671100",
    "end": "677880"
  },
  {
    "text": "see on the screen it is a well-known taint node.cubernetes dot IO out of service state",
    "start": "677880",
    "end": "685579"
  },
  {
    "text": "there can be cases in non-graceful shutdown when the node comes back online",
    "start": "686399",
    "end": "694680"
  },
  {
    "text": "and there are no side effects to that but you just are required to manually remove",
    "start": "694680",
    "end": "700800"
  },
  {
    "text": "the taint once once the node comes back online",
    "start": "700800",
    "end": "705959"
  },
  {
    "text": "after all the parts have moved to a new node if you are not doing so",
    "start": "705959",
    "end": "711300"
  },
  {
    "text": "the only side effect will be that the newer parts that are going to be created on the cluster will not get scheduled",
    "start": "711300",
    "end": "717720"
  },
  {
    "text": "onto this node",
    "start": "717720",
    "end": "720920"
  },
  {
    "text": "now this is one more experiment that I did with this feature",
    "start": "723660",
    "end": "729060"
  },
  {
    "text": "um I enabled the non-graceful set down feature I took a kubernetes version that",
    "start": "729060",
    "end": "734640"
  },
  {
    "text": "has this feature this time I created a stateful set did the same set down via the vsphere UI",
    "start": "734640",
    "end": "742079"
  },
  {
    "text": "and observe that after five minutes the part change to terminating state",
    "start": "742079",
    "end": "748680"
  },
  {
    "text": "and I used the cube CTL 10 command to obtain the node because now I know",
    "start": "748680",
    "end": "754980"
  },
  {
    "text": "this is the pod which went into a non-graceful shutdown and the Pod immediately failed over to a",
    "start": "754980",
    "end": "762720"
  },
  {
    "text": "different healthy node and it didn't hit had to wait like six minutes for the path to come to running",
    "start": "762720",
    "end": "770339"
  },
  {
    "text": "state and just take caution here whenever you are trying to utilize this feature",
    "start": "770339",
    "end": "777779"
  },
  {
    "text": "just make sure that your node has really went into a non-graceful set down and",
    "start": "777779",
    "end": "784560"
  },
  {
    "text": "you are tainting that particular note we can see a demo here of this",
    "start": "784560",
    "end": "792720"
  },
  {
    "text": "particular thing",
    "start": "792720",
    "end": "795740"
  },
  {
    "text": "I've recorded it let us get started for a demo on non-graceful sit-down feature",
    "start": "801360",
    "end": "807899"
  },
  {
    "text": "I have access to a three node kubernetes cluster which is on gke and it has all",
    "start": "807899",
    "end": "814920"
  },
  {
    "text": "the alpha features enabled so that we can test non-graceful set down feature",
    "start": "814920",
    "end": "822360"
  },
  {
    "text": "first of all I'm going to create a stateful set this is the stateful set piano it has",
    "start": "822360",
    "end": "829860"
  },
  {
    "text": "three duplicates and it is using this volume claim template",
    "start": "829860",
    "end": "835079"
  },
  {
    "text": "which is STS pod PVC",
    "start": "835079",
    "end": "839660"
  },
  {
    "text": "let me just apply",
    "start": "840959",
    "end": "845600"
  },
  {
    "text": "let us see the parts came to running state or not we'll wait for a while",
    "start": "851700",
    "end": "859160"
  },
  {
    "text": "let us check once more okay now all the parts are in running",
    "start": "861180",
    "end": "867180"
  },
  {
    "text": "State let us see which notes these parts are scheduled on",
    "start": "867180",
    "end": "873420"
  },
  {
    "text": "foreign",
    "start": "873420",
    "end": "876079"
  },
  {
    "text": "what I will try to do now is I'll try to do a non-graceful set down and to mimic",
    "start": "878459",
    "end": "886380"
  },
  {
    "text": "that what I will do is I'll SSH onto this nodes so I'm going to do a non-graceful",
    "start": "886380",
    "end": "893160"
  },
  {
    "text": "shutdown for this node and for that I'm going to SSH into this node and",
    "start": "893160",
    "end": "900180"
  },
  {
    "text": "stop the accumulate there once I do that this node should go into",
    "start": "900180",
    "end": "906060"
  },
  {
    "text": "not ready state and the parts running on this node should go into terminating state",
    "start": "906060",
    "end": "913500"
  },
  {
    "text": "so basically steer Spot 2 and St spot 0 should go into terminating state",
    "start": "913500",
    "end": "919620"
  },
  {
    "text": "that's the expectation let's see what happens I'm going to copy this",
    "start": "919620",
    "end": "926339"
  },
  {
    "text": "and I'm going to SSH to this node",
    "start": "926339",
    "end": "933260"
  },
  {
    "text": "let us see the status of cubelet it is running fine",
    "start": "936180",
    "end": "943380"
  },
  {
    "text": "and now I'm going to set down the cubelet",
    "start": "943380",
    "end": "948500"
  },
  {
    "text": "it's verified okay the cubelet has now stopped",
    "start": "956880",
    "end": "963360"
  },
  {
    "text": "let us go back to the terminal and I'm going to check the status of the",
    "start": "963360",
    "end": "968760"
  },
  {
    "text": "node it can take little while for the status",
    "start": "968760",
    "end": "975180"
  },
  {
    "text": "to update here",
    "start": "975180",
    "end": "979040"
  },
  {
    "text": "let us check the status once more okay now this node has a not ready",
    "start": "981540",
    "end": "988800"
  },
  {
    "text": "status and let us check what is the state of the paths",
    "start": "988800",
    "end": "995899"
  },
  {
    "text": "the pots are still in running State and it can take like approximately five",
    "start": "996420",
    "end": "1001820"
  },
  {
    "text": "minutes for this pots to get into terminating state so let us wait for that",
    "start": "1001820",
    "end": "1009639"
  },
  {
    "text": "let us check the status of parts once more",
    "start": "1011060",
    "end": "1016540"
  },
  {
    "text": "and now we can see that approximately after five minutes or so",
    "start": "1016579",
    "end": "1024140"
  },
  {
    "text": "the spots are into terminating State and from here if we do nothing about this",
    "start": "1024140",
    "end": "1030678"
  },
  {
    "text": "then the spots are going to be stuck into this state so let us do one more experiment now",
    "start": "1030679",
    "end": "1038900"
  },
  {
    "text": "what I'm going to do now is I'm going to forcefully terminate these two parts",
    "start": "1038900",
    "end": "1044298"
  },
  {
    "text": "and see what happens Cube CTL delete bot",
    "start": "1044299",
    "end": "1049520"
  },
  {
    "text": "iPhone iPhone 4S steel spawn zero",
    "start": "1049520",
    "end": "1054679"
  },
  {
    "text": "steel spot two",
    "start": "1054679",
    "end": "1058880"
  },
  {
    "text": "I have forcefully deleted these two parts",
    "start": "1063260",
    "end": "1068140"
  },
  {
    "text": "and we can see that STS part 0 has been scheduled onto a different node",
    "start": "1069440",
    "end": "1078740"
  },
  {
    "text": "and STS part 2 cannot come up until this STS point zero comes online and that's",
    "start": "1078740",
    "end": "1085580"
  },
  {
    "text": "how stateful sets work so let us describe this part",
    "start": "1085580",
    "end": "1091779"
  },
  {
    "text": "and we can see an error is reported from a task controller that is multi-attacher",
    "start": "1097880",
    "end": "1106059"
  },
  {
    "text": "and let's do right",
    "start": "1106059",
    "end": "1111620"
  },
  {
    "text": "so this is the problem that we are into now so what I'll do now is",
    "start": "1111620",
    "end": "1118460"
  },
  {
    "text": "I'll wait for like six minutes and if we do so",
    "start": "1118460",
    "end": "1124640"
  },
  {
    "text": "this part should come to running state because this is the default timeout",
    "start": "1124640",
    "end": "1131900"
  },
  {
    "text": "period for this volume detached to happen and once that happens this font",
    "start": "1131900",
    "end": "1137000"
  },
  {
    "text": "will be able to successfully attach the volume",
    "start": "1137000",
    "end": "1142899"
  },
  {
    "text": "let us see what is the state of parts now",
    "start": "1147080",
    "end": "1152200"
  },
  {
    "text": "okay now we can see that this is T S Pawn 0 has came to running State and",
    "start": "1152960",
    "end": "1160280"
  },
  {
    "text": "because this came to running State as TS part 2 was created and it it also went into running state",
    "start": "1160280",
    "end": "1166700"
  },
  {
    "text": "so but we had to wait like for a considerable amount of time",
    "start": "1166700",
    "end": "1173059"
  },
  {
    "text": "and this is unfavorable now I'm going to repeat the same experiment but this time I'm going to",
    "start": "1173059",
    "end": "1179780"
  },
  {
    "text": "utilize the non-graceful set on feature and realize how it helps here",
    "start": "1179780",
    "end": "1185840"
  },
  {
    "text": "so but before that let me just start the",
    "start": "1185840",
    "end": "1191299"
  },
  {
    "text": "cubelet back onto that note",
    "start": "1191299",
    "end": "1194919"
  },
  {
    "text": "the cubelet is started and let us see what's the",
    "start": "1205780",
    "end": "1213260"
  },
  {
    "text": "status of the notes now okay now we have all the notes in ready",
    "start": "1213260",
    "end": "1219380"
  },
  {
    "text": "state I'm going to repeat this experiment",
    "start": "1219380",
    "end": "1226600"
  },
  {
    "text": "and let us say this time I'm going to do a non-graceful set down for this",
    "start": "1228919",
    "end": "1236539"
  },
  {
    "text": "particular node let me SSH into this node",
    "start": "1236539",
    "end": "1243640"
  },
  {
    "text": "check the status of cubelet",
    "start": "1264500",
    "end": "1269900"
  },
  {
    "text": "sorry it's running fine",
    "start": "1270860",
    "end": "1275919"
  },
  {
    "text": "now I'm going to stop the cubelet",
    "start": "1275919",
    "end": "1280899"
  },
  {
    "text": "the cube let it stopped now",
    "start": "1289100",
    "end": "1293500"
  },
  {
    "text": "Cube CTL get node",
    "start": "1295520",
    "end": "1299919"
  },
  {
    "text": "again it can take a while before this node comes to not ready state",
    "start": "1300620",
    "end": "1309100"
  },
  {
    "text": "let us check the status okay now this node is not ready so now",
    "start": "1311000",
    "end": "1320059"
  },
  {
    "text": "I know that this node went into a non-visible set down",
    "start": "1320059",
    "end": "1325460"
  },
  {
    "text": "because I just simulated it by doing it's a down of tubelet",
    "start": "1325460",
    "end": "1331400"
  },
  {
    "text": "what I can do now is I can use non-graceful set down feature and to do so I'm going to taint this node using",
    "start": "1331400",
    "end": "1339919"
  },
  {
    "text": "Cube CTL tent let us do that",
    "start": "1339919",
    "end": "1345100"
  },
  {
    "text": "so that will be Cube CTL 10 node",
    "start": "1345200",
    "end": "1351519"
  },
  {
    "text": "name and we'll use the well-known taint",
    "start": "1355280",
    "end": "1363200"
  },
  {
    "text": "let me just pull that",
    "start": "1363200",
    "end": "1366580"
  },
  {
    "text": "right but before tainting this node actually let us look at the pots once more",
    "start": "1371299",
    "end": "1380080"
  },
  {
    "text": "and we can see that all the pods are in any state but this particular part will",
    "start": "1382159",
    "end": "1387679"
  },
  {
    "text": "go into terminating State and it can take little while so now let us just change this node I'll",
    "start": "1387679",
    "end": "1394820"
  },
  {
    "text": "just copy this command till here",
    "start": "1394820",
    "end": "1398860"
  },
  {
    "text": "this node is Tainted now let us check the part",
    "start": "1403220",
    "end": "1409240"
  },
  {
    "text": "this part now got into terminating State immediately because of the tent policies",
    "start": "1412720",
    "end": "1421780"
  },
  {
    "text": "and a new part is got scheduled onto a different node that is the node ending",
    "start": "1428360",
    "end": "1435140"
  },
  {
    "text": "with 6 KPS and it is in container creating State now",
    "start": "1435140",
    "end": "1441620"
  },
  {
    "text": "let's check once more awesome now this part came to running state so we can see that the Pod came",
    "start": "1441620",
    "end": "1450799"
  },
  {
    "text": "back online fairly quickly and we didn't have to wait like much longer so",
    "start": "1450799",
    "end": "1457159"
  },
  {
    "text": "that is it I wanted to show in this demo thank you",
    "start": "1457159",
    "end": "1462320"
  },
  {
    "text": "thanks uh I would like to invite same zinc to continue further",
    "start": "1462320",
    "end": "1468200"
  },
  {
    "text": "really",
    "start": "1468200",
    "end": "1470799"
  },
  {
    "text": "so we talked about how graceful and non-graceful Sharon",
    "start": "1480140",
    "end": "1485539"
  },
  {
    "text": "works now let's talk about our next steps so right now we are targeting beta in",
    "start": "1485539",
    "end": "1492679"
  },
  {
    "text": "1.26 for this non-graceful no shutdown feature and then depending on feedback",
    "start": "1492679",
    "end": "1498320"
  },
  {
    "text": "we're planning to move this to GA in the future so right now this approach",
    "start": "1498320",
    "end": "1505480"
  },
  {
    "text": "involves a menu step for the user to apply a out of service tint on the shutter node we are looking into how to",
    "start": "1505480",
    "end": "1512720"
  },
  {
    "text": "automate this process and see if we can detect the shutdown and apply the taint",
    "start": "1512720",
    "end": "1517760"
  },
  {
    "text": "automatically and reboot if needed so while working on this feature we did",
    "start": "1517760",
    "end": "1523700"
  },
  {
    "text": "look into several alternative approaches an over earlier version of this cap we",
    "start": "1523700",
    "end": "1531080"
  },
  {
    "text": "have this safe detached approach where we are trying to introduce a safe",
    "start": "1531080",
    "end": "1536179"
  },
  {
    "text": "detached Boolean in the CSI driver spec so CSF driver can opt into this feature",
    "start": "1536179",
    "end": "1543220"
  },
  {
    "text": "then when the voting attachment is deleted see if the driver will get caught try to detach the volume since",
    "start": "1543220",
    "end": "1551659"
  },
  {
    "text": "the driver needs to make sure it is safe to detach and only detach if it is fine",
    "start": "1551659",
    "end": "1558020"
  },
  {
    "text": "to detach but the problem of course is the Cs driver needs to have this knowledge we",
    "start": "1558020",
    "end": "1565279"
  },
  {
    "text": "are not sure if that's possible it's possible for every C-section driver but we will be looking into this",
    "start": "1565279",
    "end": "1572900"
  },
  {
    "text": "approach again in our next step and the second alternative that we",
    "start": "1572900",
    "end": "1579700"
  },
  {
    "text": "evaluated is note fencing in this approach there will be a controller that",
    "start": "1579700",
    "end": "1585260"
  },
  {
    "text": "monitors the status of the notes and if the note Went to went into not",
    "start": "1585260",
    "end": "1591919"
  },
  {
    "text": "not ready status it's going to create this the note fan crd and work on node",
    "start": "1591919",
    "end": "1597080"
  },
  {
    "text": "fencing this meth this requires a node fancy method to be defined and user",
    "start": "1597080",
    "end": "1605179"
  },
  {
    "text": "need to specify the reboot command so we look at this approach you thought it's a intrusive for a reboot comment to",
    "start": "1605179",
    "end": "1614360"
  },
  {
    "text": "be required in in kubernetes so we didn't go with this approach but again",
    "start": "1614360",
    "end": "1619820"
  },
  {
    "text": "we will look into this again in our next steps there are a few other approaches",
    "start": "1619820",
    "end": "1627919"
  },
  {
    "text": "there is a CSI Force detached proposal apparently proposed some new",
    "start": "1627919",
    "end": "1635299"
  },
  {
    "text": "capabilities in CSI controller and the node uh and and also there is this uh Portman",
    "start": "1635299",
    "end": "1643039"
  },
  {
    "text": "project it also uh watches the status of the notes and if it is not ready or it's",
    "start": "1643039",
    "end": "1650240"
  },
  {
    "text": "going to check from the storage side to see if Universal IO and if there's no IO then it's going to forcefully delete the",
    "start": "1650240",
    "end": "1657620"
  },
  {
    "text": "detach the volumes and clean up the parts so we are going to look at all these",
    "start": "1657620",
    "end": "1663620"
  },
  {
    "text": "Alternatives and they're trying to decide what is the best alternative",
    "start": "1663620",
    "end": "1668659"
  },
  {
    "text": "what's the best way for us to move forward",
    "start": "1668659",
    "end": "1672700"
  },
  {
    "text": "won't you give a big shout out to everyone who is involved in this project of course there are a lot more people",
    "start": "1674659",
    "end": "1681440"
  },
  {
    "text": "who have contributed than what we have shown here so we've included some blogs linked here",
    "start": "1681440",
    "end": "1690500"
  },
  {
    "text": "there's a graceful no shout out beta block and there's a non-graceful no sharan Alpha blog here",
    "start": "1690500",
    "end": "1696740"
  },
  {
    "text": "and also if you are interested in this project please join us and get involved",
    "start": "1696740",
    "end": "1702260"
  },
  {
    "text": "in six storage and sick node let's work on this together",
    "start": "1702260",
    "end": "1707620"
  },
  {
    "text": "here's the QR code please scan it and provide feedback that's the end of the",
    "start": "1708860",
    "end": "1714860"
  },
  {
    "text": "session thank you",
    "start": "1714860",
    "end": "1718299"
  },
  {
    "text": "thanks Jinx thanks if you have a question please raise your hand",
    "start": "1721480",
    "end": "1727419"
  },
  {
    "text": "so if I wanted to test using the taint right now is that available in a version",
    "start": "1742820",
    "end": "1748400"
  },
  {
    "text": "of kubernetes or",
    "start": "1748400",
    "end": "1751720"
  },
  {
    "text": "but what what version of kubernetes is that available in",
    "start": "1753980",
    "end": "1759158"
  },
  {
    "text": "okay so I think you said well I'm sorry let me use this",
    "start": "1759980",
    "end": "1766360"
  },
  {
    "text": "yeah yeah so so you're asking the feature it's in 1.24 is Ava feature but",
    "start": "1766640",
    "end": "1773720"
  },
  {
    "text": "we are trying to move it to Beta in 1.26 okay so you just need to enable the future gate and then you can yes great",
    "start": "1773720",
    "end": "1780860"
  },
  {
    "text": "and question I'm very aware of this this situation in my experience with rwo",
    "start": "1780860",
    "end": "1788179"
  },
  {
    "text": "volumes it never gets out of terminate I mean how I know you said after six",
    "start": "1788179",
    "end": "1793940"
  },
  {
    "text": "minutes well this six minutes another six minutes is there a condition which it would",
    "start": "1793940",
    "end": "1799820"
  },
  {
    "text": "never get out of terminate for rwo volumes well so you need to uh well so if you",
    "start": "1799820",
    "end": "1806539"
  },
  {
    "text": "want to you want to use this feature right are you going to if you're not playing like if you don't that's a feature without this feature it's going",
    "start": "1806539",
    "end": "1812840"
  },
  {
    "text": "to be in this terminating State forever if the no if the shutdown node does not come back okay because I thought you",
    "start": "1812840",
    "end": "1819860"
  },
  {
    "text": "showed that after like six minutes yeah uh that is far if the Pod is",
    "start": "1819860",
    "end": "1827000"
  },
  {
    "text": "managed by deployment controller because in case of deployment you know you can",
    "start": "1827000",
    "end": "1833120"
  },
  {
    "text": "have the part come up again depending on Lake policies and your deployment but in case",
    "start": "1833120",
    "end": "1839960"
  },
  {
    "text": "of stateful set if your part is into terminating state",
    "start": "1839960",
    "end": "1845779"
  },
  {
    "text": "you know the New York pod won't come up until it like terminates so yeah",
    "start": "1845779",
    "end": "1852860"
  },
  {
    "text": "yeah oh you have to you know forcefully delete it and delete the volume attachments yeah",
    "start": "1852860",
    "end": "1858740"
  },
  {
    "text": "thanks",
    "start": "1858740",
    "end": "1861220"
  },
  {
    "text": "so if a node is not healthy stage what if I just delete to group control delete",
    "start": "1864940",
    "end": "1870559"
  },
  {
    "text": "node from the cluster with that South Asia oh",
    "start": "1870559",
    "end": "1877100"
  },
  {
    "text": "um if you just do a cube control delete of the node I don't think it will",
    "start": "1877100",
    "end": "1885100"
  },
  {
    "text": "what was the question sorry",
    "start": "1886760",
    "end": "1892399"
  },
  {
    "text": "the question is uh if I don't uh so with the current kubernetes version and then",
    "start": "1892399",
    "end": "1898520"
  },
  {
    "text": "if we detect one node is not healthy and then I first delete a note from the cluster with that so the issue of the",
    "start": "1898520",
    "end": "1906340"
  },
  {
    "text": "volume get stuck in you know not reattach do you understand that I don't",
    "start": "1906340",
    "end": "1912260"
  },
  {
    "text": "understand the question yeah I don't think that's going to solve this problem um because you still need to be able to",
    "start": "1912260",
    "end": "1919940"
  },
  {
    "text": "um kind of delete the original pod but if the origin node is not there anymore you",
    "start": "1919940",
    "end": "1926240"
  },
  {
    "text": "can't even delete that part the cubelet is not there to delete that right so you still need to have a way to delete your",
    "start": "1926240",
    "end": "1932000"
  },
  {
    "text": "pod this for the staple set you can only have like one part",
    "start": "1932000",
    "end": "1937460"
  },
  {
    "text": "like the the name has to be just like you can't have like one unique name",
    "start": "1937460",
    "end": "1942559"
  },
  {
    "text": "right so you can't really like have the part created on uh like two node at the same time",
    "start": "1942559",
    "end": "1950380"
  },
  {
    "text": "foreign I have another question",
    "start": "1963620",
    "end": "1970820"
  },
  {
    "text": "so during the node rebalancing one of the common thing happening is that cloud",
    "start": "1970820",
    "end": "1976940"
  },
  {
    "text": "provider take few notes out from the rotation will this",
    "start": "1976940",
    "end": "1982340"
  },
  {
    "text": "feature help there or not if if you have it's a Down",
    "start": "1982340",
    "end": "1989899"
  },
  {
    "text": "if during if during rebalancing you have to figure out what kind of shutdown it is and",
    "start": "1989899",
    "end": "1996260"
  },
  {
    "text": "uh if it does not trigger if it trigger if it is not a graceful shutdown it will",
    "start": "1996260",
    "end": "2002440"
  },
  {
    "text": "help but right now you have to manually taint the notes and you have to actually know what are those notes that are going",
    "start": "2002440",
    "end": "2009940"
  },
  {
    "text": "out of the cluster and then you can utilize this feature don't we apply across the node this",
    "start": "2009940",
    "end": "2016960"
  },
  {
    "text": "feature or only for right now you have two taint nodes individually yeah and",
    "start": "2016960",
    "end": "2023200"
  },
  {
    "text": "maybe you know going further anything but you know we can you know",
    "start": "2023200",
    "end": "2029740"
  },
  {
    "text": "it's an alpha and going to Beta and we can't think of automating it and you know maybe spanning over a set of nodes",
    "start": "2029740",
    "end": "2036640"
  },
  {
    "text": "so yeah everything you know",
    "start": "2036640",
    "end": "2040380"
  },
  {
    "text": "hi thank you um have you tested graceful shutdown on",
    "start": "2043360",
    "end": "2048700"
  },
  {
    "text": "um say pods that have pod disruption budgets like say you can't have less than one of",
    "start": "2048700",
    "end": "2054280"
  },
  {
    "text": "this pod and you try to initiate a graceful shutdown on a node do you know what happens",
    "start": "2054280",
    "end": "2061139"
  },
  {
    "text": "can you try that I have not like personally tried graceful sat down gotcha but yeah we can talk more about",
    "start": "2062500",
    "end": "2069398"
  },
  {
    "text": "that you know okay offline do you have do you have a guess as to what would happen uh in the case of a non-graceful",
    "start": "2069399",
    "end": "2075820"
  },
  {
    "text": "shutdown either or is that something we can I can ask sure thank you yeah",
    "start": "2075820",
    "end": "2081658"
  },
  {
    "text": "shutdown does not respect so it doesn't respect it's not a retrospective because I was going down",
    "start": "2081659",
    "end": "2087520"
  },
  {
    "text": "regardless I can't really you know right someone else",
    "start": "2087520",
    "end": "2093419"
  },
  {
    "text": "yeah I had a similar question like when you drain a node right uh like um",
    "start": "2095339",
    "end": "2102820"
  },
  {
    "text": "do we have to follow it up with a non-graceful shutdown if there are part",
    "start": "2102820",
    "end": "2109119"
  },
  {
    "text": "disruption budget or any other things alone not allowing a part to come down right",
    "start": "2109119",
    "end": "2115839"
  },
  {
    "text": "um I don't think so because you do this steps when you know that you",
    "start": "2115839",
    "end": "2121359"
  },
  {
    "text": "have a non-graceful shutdown and usually those kind of shutdowns will not be under your",
    "start": "2121359",
    "end": "2127540"
  },
  {
    "text": "control like at most times so you know once you know that this node is",
    "start": "2127540",
    "end": "2133839"
  },
  {
    "text": "into a sat down then you follow these steps and",
    "start": "2133839",
    "end": "2140079"
  },
  {
    "text": "the grace if you are talking about draining what I was mentioning on the slide is",
    "start": "2140079",
    "end": "2146800"
  },
  {
    "text": "you know when even if it was a graceful shutdown you know there there are some shutdown you know for which we can take",
    "start": "2146800",
    "end": "2153640"
  },
  {
    "text": "precautionary steps but we couldn't because the graceful sit down feature was not there and you had to manually go",
    "start": "2153640",
    "end": "2160780"
  },
  {
    "text": "and drain the nodes and do all this stuff but now because we have this graceful sit down feature you really do",
    "start": "2160780",
    "end": "2167740"
  },
  {
    "text": "not require to you know do all this stuff and yeah",
    "start": "2167740",
    "end": "2175320"
  },
  {
    "text": "uh yeah I",
    "start": "2185560",
    "end": "2190920"
  },
  {
    "text": "I I'm not sure about how it will count the part disruption policies but yeah I",
    "start": "2191500",
    "end": "2198220"
  },
  {
    "text": "need to check yeah but in general the idea is uh",
    "start": "2198220",
    "end": "2203320"
  },
  {
    "text": "you'll have Parts marked critical parts or regular pots according to the policies and you basically configure",
    "start": "2203320",
    "end": "2211839"
  },
  {
    "text": "parameters on cubelet for example you have 30 seconds for the base will sit down you can take offline oh yeah thank",
    "start": "2211839",
    "end": "2218140"
  },
  {
    "text": "you we can talk more on that yeah sir thanks thank you everyone [Applause]",
    "start": "2218140",
    "end": "2224869"
  }
]