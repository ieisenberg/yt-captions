[
  {
    "text": "you are in the right place if you came to listen about sign it's a introduction deep dive i'm",
    "start": "1280",
    "end": "8240"
  },
  {
    "text": "Sergey Kangel i work for Google hello everyone i'm Franchesco Romani i work for Red Hat hello my name is Peter",
    "start": "8240",
    "end": "15920"
  },
  {
    "text": "Hunt and I also work for Red Hat okay so in this talk we start with uh",
    "start": "15920",
    "end": "22880"
  },
  {
    "text": "deep with short intro we only have 30 minutes then we'll go into deep dive",
    "start": "22880",
    "end": "28320"
  },
  {
    "text": "then we go into deeper deep dive and then we'll lighten it up and uh finish with all we're doing recently and then",
    "start": "28320",
    "end": "35120"
  },
  {
    "text": "what we did um so yeah um please expect it going lower and lower into details",
    "start": "35120",
    "end": "41840"
  },
  {
    "text": "and then it's pumping up and uh you you'll know about all the awesome things we're doing so s node if you think about",
    "start": "41840",
    "end": "51640"
  },
  {
    "text": "kubernetes everybody in this room knows that kubernetes is 50% API server 50%",
    "start": "51640",
    "end": "57760"
  },
  {
    "text": "sign node and everything else I know if you go to sig node",
    "start": "57760",
    "end": "62960"
  },
  {
    "text": "meeting uh they will say it's 80% networking and 20% everything else but we are in sign node meeting so uh bear",
    "start": "62960",
    "end": "69840"
  },
  {
    "text": "with me uh so sig node uh you see the beautiful ship carrying containers um I",
    "start": "69840",
    "end": "76560"
  },
  {
    "text": "mean yeah we nautical theme here right so we carrying containers there is a runtime that helps",
    "start": "76560",
    "end": "83439"
  },
  {
    "text": "pushing it forward making uh making sure it's running and then there is u",
    "start": "83439",
    "end": "88799"
  },
  {
    "text": "watchdog npd that is thing looking at uh ship and like see that there is no leaks",
    "start": "88799",
    "end": "94079"
  },
  {
    "text": "and anything and there is a bunch of plugins for different resources uh need",
    "start": "94079",
    "end": "99680"
  },
  {
    "text": "for container I'm out of uh uh my depths how to make it nautical theme because I",
    "start": "99680",
    "end": "106159"
  },
  {
    "text": "don't know much about ships uh so you have plugins that uh allocate resources for specific containers and then there",
    "start": "106159",
    "end": "113280"
  },
  {
    "text": "are other components that wasn't drawn here in fact in the past we didn't even draw plugins we only did uh uh kublet",
    "start": "113280",
    "end": "121439"
  },
  {
    "text": "and npd and like contain runtime now plugins making a playing a very good role and we will talk a lot about",
    "start": "121439",
    "end": "127759"
  },
  {
    "text": "plugins in this talk anyway so this is a beautiful ship and then uh out of those",
    "start": "127759",
    "end": "133040"
  },
  {
    "text": "beautiful ships with many many components once you put all the components in place you need to make",
    "start": "133040",
    "end": "138879"
  },
  {
    "text": "sure that this ship is actually running that node is healthy uh that you have enough resources for managing your",
    "start": "138879",
    "end": "145319"
  },
  {
    "text": "containers and then containers for containers themselves you workload like",
    "start": "145319",
    "end": "151599"
  },
  {
    "text": "uh user can decide how much they want to run it reliably and how much they want to run it uh efficiently so there is",
    "start": "151599",
    "end": "158080"
  },
  {
    "text": "always trade-off and you can make this trade-off you can make kublet how much of trade-off you want to make uh between",
    "start": "158080",
    "end": "165840"
  },
  {
    "text": "efficiently uh running and utilizing resources over provisioning and how much you want to run it reliably so it will",
    "start": "165840",
    "end": "172319"
  },
  {
    "text": "never crash that kind of a uh priization that you can make and make can tell",
    "start": "172319",
    "end": "178080"
  },
  {
    "text": "kublet to make and kublet will manage it out and like make sure that it's all running reliably uh again as I said it's",
    "start": "178080",
    "end": "185120"
  },
  {
    "text": "all running reliably only if Kublet allocated some uh resources for itself and it has enough uh um capacity to",
    "start": "185120",
    "end": "193200"
  },
  {
    "text": "actually run the ship so with all that said we will do a deep dive in resource management today uh it will be as I said",
    "start": "193200",
    "end": "201040"
  },
  {
    "text": "slight deep dive and then even deeper deep dive so what resources Kublet manages today we have a small list here",
    "start": "201040",
    "end": "208800"
  },
  {
    "text": "i mean yeah it's not very extensive um we do some standard resources like CPU",
    "start": "208800",
    "end": "215120"
  },
  {
    "text": "memory uh out of CPU we also look about look at numa nodes uh memory is also",
    "start": "215120",
    "end": "221599"
  },
  {
    "text": "different types of memory uh we do some speeds uh limiting and uh uh we also do",
    "start": "221599",
    "end": "226640"
  },
  {
    "text": "devices devices are big anyway it's not a huge list but if you think about all",
    "start": "226640",
    "end": "232159"
  },
  {
    "text": "the things we need to do with those resources um you you will be I mean you",
    "start": "232159",
    "end": "238400"
  },
  {
    "text": "you understand why we're doing it and by the end of like first deep dive I want you to",
    "start": "238400",
    "end": "244000"
  },
  {
    "text": "understand how important it is to make decisions on every step of this way so",
    "start": "244000",
    "end": "249439"
  },
  {
    "text": "let's say advertising resources how do you model your resources how do you make sure that scheduleuler autoscaler knows",
    "start": "249439",
    "end": "256079"
  },
  {
    "text": "about your resources then depend on this decision uh you'll have a next decision how do you tell which port has which",
    "start": "256079",
    "end": "262919"
  },
  {
    "text": "resources then how to apply some quotas how to manage those resources across organization how to allocate and admit",
    "start": "262919",
    "end": "270400"
  },
  {
    "text": "port on a node and then um how to allocate devices and like actually make",
    "start": "270400",
    "end": "275680"
  },
  {
    "text": "them available on a node when port is already scheduled and then how to deallocate them and how to deallocate",
    "start": "275680",
    "end": "281840"
  },
  {
    "text": "them fast if in certain cases when you need to uh evict everything really quickly and then all this question about",
    "start": "281840",
    "end": "287919"
  },
  {
    "text": "overprovisioning and managing uh resources efficiently and monitor them and how to evict uh like some resource",
    "start": "287919",
    "end": "295440"
  },
  {
    "text": "that are not perform like some pose that not performing very well and consuming too much all these questions needs to be",
    "start": "295440",
    "end": "301040"
  },
  {
    "text": "answered and actually every day of maintainer in kub kublet in kubernetes",
    "start": "301040",
    "end": "307360"
  },
  {
    "text": "and sign node is answering many many questions about all the resources and in",
    "start": "307360",
    "end": "312960"
  },
  {
    "text": "fact in some resources in some uh questions we going back and forth what we want to achieve how we want to",
    "start": "312960",
    "end": "318479"
  },
  {
    "text": "achieve it and what kind of uh compromises we want to make so let's say um advertising",
    "start": "318479",
    "end": "325560"
  },
  {
    "text": "resources I mean even for CPU and memory what kubernnees started with is uh how",
    "start": "325560",
    "end": "331600"
  },
  {
    "text": "like just proportion of CPU it didn't uh account for CPU being different like you",
    "start": "331600",
    "end": "336960"
  },
  {
    "text": "have one VM that is uh uh has one type of CPU another have different type of CPU you still saying like I just need a",
    "start": "336960",
    "end": "343120"
  },
  {
    "text": "half and this half may maybe like 10 times faster on another machine because like CPU is just more powerful but",
    "start": "343120",
    "end": "349120"
  },
  {
    "text": "Kubernetes didn't care enough to make it available for you and advertise it proper way so because we just believe",
    "start": "349120",
    "end": "356320"
  },
  {
    "text": "it's it's okay it's enough modeling things that we can make and enough for you to tell what you what you need and",
    "start": "356320",
    "end": "362960"
  },
  {
    "text": "then um good example of compromise here on advertising resources is what we did",
    "start": "362960",
    "end": "368319"
  },
  {
    "text": "with uh uh classic DRA versus uh structured DRA so if you don't know this",
    "start": "368319",
    "end": "373520"
  },
  {
    "text": "terms classic DRA structured DRA you can see some older talks from Kevin and from other people so classic DRA we started",
    "start": "373520",
    "end": "380479"
  },
  {
    "text": "with device allocation and we said that we will have a plugin and you will magically know about resources and if",
    "start": "380479",
    "end": "386560"
  },
  {
    "text": "you ask about it it will tell you something about it that you need to know and it didn't work very well because",
    "start": "386560",
    "end": "392000"
  },
  {
    "text": "cluster at scaler has no idea what these devices are and then we um just uh kill",
    "start": "392000",
    "end": "398160"
  },
  {
    "text": "this uh whole direction of classic DA we said like structure data is a way we wanted we simplified devices into",
    "start": "398160",
    "end": "404400"
  },
  {
    "text": "resource slices and that this is what we want to tell Kubernetes about devices and this is a very primitive and simple",
    "start": "404400",
    "end": "411440"
  },
  {
    "text": "model of devices but it will work good enough for majority of use cases then allocating port to resources",
    "start": "411440",
    "end": "419120"
  },
  {
    "text": "um another story uh for allocating post to resources is uh our long-standing cap of uh memory swap support so for memory",
    "start": "419120",
    "end": "427280"
  },
  {
    "text": "swap it's countable resource you can theoretically expose it and advertise it",
    "start": "427280",
    "end": "432479"
  },
  {
    "text": "as a countable resource and then try to understand do you want to share swap memory do you want to uh exclusive",
    "start": "432479",
    "end": "438639"
  },
  {
    "text": "exclusively allocated to specific ports so in the first iteration of memory swap",
    "start": "438639",
    "end": "443759"
  },
  {
    "text": "support we decided we don't want to even advertise the swap memory we just want it to be magically available to some",
    "start": "443759",
    "end": "450160"
  },
  {
    "text": "ports and you don't even know what that's available um and we get all the way to alpha with that for memory swap",
    "start": "450160",
    "end": "457599"
  },
  {
    "text": "but now we're questioning ourself is it good enough is it uh what people will want and if you enable that support for",
    "start": "457599",
    "end": "464240"
  },
  {
    "text": "memory swap will it be uh putting us in a situation when we cannot evaluate",
    "start": "464240",
    "end": "469639"
  },
  {
    "text": "and can it um work to forward and uh start counting swap memory and start",
    "start": "469639",
    "end": "475840"
  },
  {
    "text": "over provisioning and like what who know what scenarios want to support on that",
    "start": "475840",
    "end": "480960"
  },
  {
    "text": "anyway and uh another good uh example is uh uh for as changing ports to resources",
    "start": "480960",
    "end": "486800"
  },
  {
    "text": "let's say you have some uh QS resources another long-standing uh uh cap that we",
    "start": "486800",
    "end": "492080"
  },
  {
    "text": "have and quality of service resources telling you how good of a quality you have like let's say you have networking",
    "start": "492080",
    "end": "498400"
  },
  {
    "text": "uh switch and you want to say like I want super fast network and then how many ports of super fast network access",
    "start": "498400",
    "end": "504639"
  },
  {
    "text": "you can have on a node do you want to count them and then do you want to apply",
    "start": "504639",
    "end": "509680"
  },
  {
    "text": "limits to them and it goes to the next stage like how do you apply limits how do you count how you quarter those and",
    "start": "509680",
    "end": "515599"
  },
  {
    "text": "do you need to spread them across all the devices evenly or you need to put all of them like first uh node that fits",
    "start": "515599",
    "end": "522159"
  },
  {
    "text": "this uh QS class all these questions needs to be answered and uh this is what",
    "start": "522159",
    "end": "527680"
  },
  {
    "text": "we ask as maintainers uh what we want to do for support for specific uh classes",
    "start": "527680",
    "end": "533600"
  },
  {
    "text": "of resources and uh types of resources next uh when you have an admission on a",
    "start": "533600",
    "end": "539839"
  },
  {
    "text": "port admission it relatively easy topic but uh if you consider um device plug-in",
    "start": "539839",
    "end": "547600"
  },
  {
    "text": "and DRA like the big difference in device plug-in and DRA that we get to when we designed one and another for",
    "start": "547600",
    "end": "554399"
  },
  {
    "text": "device plug-in we said that all devices needs to be pre-allocated pre-shared pre-sliced and uh they pre-advertised so",
    "start": "554399",
    "end": "562640"
  },
  {
    "text": "when port comes to the node devices are already available you need you just need to associate device to the node or to",
    "start": "562640",
    "end": "569200"
  },
  {
    "text": "the port however however with DRA it may take a while to allocate the device and",
    "start": "569200",
    "end": "575279"
  },
  {
    "text": "sometimes you get into weird situation of timeouts so with DRA we said that there is no port admission you cannot",
    "start": "575279",
    "end": "581519"
  },
  {
    "text": "fail admission on the port if you um have a device associated with your port",
    "start": "581519",
    "end": "587760"
  },
  {
    "text": "what you you will end up with if device is not available you will end up with crash loop back off that will keep",
    "start": "587760",
    "end": "594240"
  },
  {
    "text": "trying to create your port create your containers and you'll loop through allocation of the devices and if it",
    "start": "594240",
    "end": "599680"
  },
  {
    "text": "fails it will try again if it fails it try again then um allocation resources as I",
    "start": "599680",
    "end": "605600"
  },
  {
    "text": "said uh u it's a interesting topic and uh uh this allocation is",
    "start": "605600",
    "end": "612760"
  },
  {
    "text": "uh DA is a very good example like how how do you allocate them how quickly you allocate them and what kind of",
    "start": "612760",
    "end": "619040"
  },
  {
    "text": "parameters you can u pro provision to this uh provide to allocate proper devices",
    "start": "619040",
    "end": "626160"
  },
  {
    "text": "let's say uh with the device plug-in everything is pre-allocated with a uh everything pre-provisioned with a set",
    "start": "626160",
    "end": "633600"
  },
  {
    "text": "properties with DRA you able to pass some extra parameters how you want to",
    "start": "633600",
    "end": "638959"
  },
  {
    "text": "allocate this device and modeling that was a quite a challenge because uh uh to",
    "start": "638959",
    "end": "644320"
  },
  {
    "text": "model that we needed to have resource slices uh resource claims and like map them all together and kub will have",
    "start": "644320",
    "end": "650240"
  },
  {
    "text": "extra properties that it will need to pass to allocation of devices and finally A couple more things uh support",
    "start": "650240",
    "end": "657760"
  },
  {
    "text": "overprovisioning is hot topic we don't support it on devices yet uh we support",
    "start": "657760",
    "end": "663040"
  },
  {
    "text": "it a lot on CPUs and uh memory and it gives gives us pain um and we for every",
    "start": "663040",
    "end": "669519"
  },
  {
    "text": "device for every resource that we want to overprovision for we need to be really careful we need to have",
    "start": "669519",
    "end": "675200"
  },
  {
    "text": "monitoring of usage we need to know what's happening uh and for memory swap for instance we don't only need to know",
    "start": "675200",
    "end": "681839"
  },
  {
    "text": "how much memory swap you use but also how often you swap do you swap like every",
    "start": "681839",
    "end": "687800"
  },
  {
    "text": "like do you swap just once in the beginning or you swap all the time and you are affecting the like IO operations",
    "start": "687800",
    "end": "695279"
  },
  {
    "text": "so monitoring is is not like very simple topic and then uh once you know about uh",
    "start": "695279",
    "end": "700640"
  },
  {
    "text": "once you monitor your resource you need to understand how you evict them that's why we don't don't overprovision devices",
    "start": "700640",
    "end": "705760"
  },
  {
    "text": "we don't do this of a device because we don't know how to monitor them and how to evict them so this is a very interesting topics and is all the",
    "start": "705760",
    "end": "712560"
  },
  {
    "text": "questions that we constantly asking for every single resource be it resource that we already support CPU and memory",
    "start": "712560",
    "end": "718160"
  },
  {
    "text": "or it's some new resource and uh Francesca will talk a little bit about uh standard",
    "start": "718160",
    "end": "723720"
  },
  {
    "text": "resources hello thank you okay so uh lot of questions we have",
    "start": "723720",
    "end": "732560"
  },
  {
    "text": "to answer and account for like Sergey mentioned so let's let's look a bit about the answer we currently have and",
    "start": "732560",
    "end": "740639"
  },
  {
    "text": "if and how they are sufficient and how we could move forward to address the current open points and we are not going",
    "start": "740639",
    "end": "748560"
  },
  {
    "text": "to talk about the array for a few slides but we will talk again about the array soon enough so don't worry okay uh first",
    "start": "748560",
    "end": "755839"
  },
  {
    "text": "of all how do we express resource requirement currently well it's let's",
    "start": "755839",
    "end": "761279"
  },
  {
    "text": "consider the extremely simplest example so one pod one container you ask for",
    "start": "761279",
    "end": "767440"
  },
  {
    "text": "resources and so you set a request and this is the minimum the system will grant you and the translation is very",
    "start": "767440",
    "end": "774120"
  },
  {
    "text": "straightforward and that part this croup setting basically was in the cublet since basically forever but we seek that",
    "start": "774120",
    "end": "782480"
  },
  {
    "text": "uh if we start adding things like limits again limits per say are fine the pods",
    "start": "782480",
    "end": "788240"
  },
  {
    "text": "will not use more the container will use not more than that but we already begun",
    "start": "788240",
    "end": "793360"
  },
  {
    "text": "to have an implicit meaning on an overloading of terms so for example the QS is computed out of the limits again",
    "start": "793360",
    "end": "800320"
  },
  {
    "text": "this is the extremely simplest example so if the the you have request recall",
    "start": "800320",
    "end": "807040"
  },
  {
    "text": "limits for all the resources is requested by a pod the pod ends up to be guaranteed quality of service which has",
    "start": "807040",
    "end": "812880"
  },
  {
    "text": "a lot of good properties like for example for shadowing and eviction guarantees but this is again an overloaded of terms so you you don't you",
    "start": "812880",
    "end": "819760"
  },
  {
    "text": "cannot express yet I want my pod to be quality of service guaranteed it has to be inferred so it is more context to",
    "start": "819760",
    "end": "826240"
  },
  {
    "text": "bring you along while you define your research requests if we need something extra and",
    "start": "826240",
    "end": "833360"
  },
  {
    "text": "uh basic basically uh croup enforcement to resource request we need to uh take",
    "start": "833360",
    "end": "839519"
  },
  {
    "text": "in account the so-called resource manager for example the very first and simplest probably addition was the",
    "start": "839519",
    "end": "845120"
  },
  {
    "text": "exclusive allocation of resources instead of saying I want eight core worthy of ch of CPU time we can say hey",
    "start": "845120",
    "end": "853040"
  },
  {
    "text": "I want exactly eight cores and they want exclusive access to them and for that we need the CPU manager and for other uh",
    "start": "853040",
    "end": "860079"
  },
  {
    "text": "special would say allocation we more resource managers these are called also called herdor managers built-in managers",
    "start": "860079",
    "end": "867680"
  },
  {
    "text": "but over time and this is a theme the resource management requirements kept",
    "start": "867680",
    "end": "872880"
  },
  {
    "text": "have a steady flow of request this is not a solve the problems even if we take the array out of the picture and to",
    "start": "872880",
    "end": "879199"
  },
  {
    "text": "demonstrate that we recently well in the last 10 releases or so Kubernetes we added the policy options which are way",
    "start": "879199",
    "end": "886320"
  },
  {
    "text": "options to fine-tune those extra allocation and the fact that we have for TP manager alone six policy option means",
    "start": "886320",
    "end": "893680"
  },
  {
    "text": "this is something people still ask for and care",
    "start": "893680",
    "end": "898760"
  },
  {
    "text": "about let's uh have a deep dive into one of those option this is if I'm not",
    "start": "898760",
    "end": "905680"
  },
  {
    "text": "mistaken the latest way that which is about LLC awareness so in a overly",
    "start": "905680",
    "end": "911440"
  },
  {
    "text": "simplifying modern CPUs don't necessarily have uniform charact physical",
    "start": "911440",
    "end": "918320"
  },
  {
    "text": "characteristics some of them may be actually built like a cluster of resources replicated on the larger um",
    "start": "918320",
    "end": "926480"
  },
  {
    "text": "silicon well physical piece which is lot in the motherboard and to grant best",
    "start": "926480",
    "end": "933440"
  },
  {
    "text": "resource and best uh uh performance we need to uh allocate if we can CPUs uh",
    "start": "933440",
    "end": "939839"
  },
  {
    "text": "according to those boundaries those internal boundaries of the CPUs otherwise what will happen that when we",
    "start": "939839",
    "end": "944959"
  },
  {
    "text": "cross those boundaries we can hit a per uh per we can take a performance price so with this new option the CPU",
    "start": "944959",
    "end": "952959"
  },
  {
    "text": "manager takes into account those internal boundaries when it does the alignment and instead of doing the simplest thing it tries to allocate",
    "start": "952959",
    "end": "959920"
  },
  {
    "text": "respecting those boundaries so this is important because we have now another alignment boundary what's an alignment",
    "start": "959920",
    "end": "965199"
  },
  {
    "text": "boundary is an and it's a structure to take into account when allocate resources to make uh uh best to grant",
    "start": "965199",
    "end": "972160"
  },
  {
    "text": "best performances still another um another recent addition",
    "start": "972160",
    "end": "979279"
  },
  {
    "text": "was the conversation about in place uh update of resource of uh pod",
    "start": "979279",
    "end": "985120"
  },
  {
    "text": "resources the VPA which is a very interesting and complex topic and one of",
    "start": "985120",
    "end": "991040"
  },
  {
    "text": "the goals is to preserve the exclusive allocation so when we scale up a pod in",
    "start": "991040",
    "end": "997680"
  },
  {
    "text": "place don't just take it uh so many new CPUs take into take CPUs such as the",
    "start": "997680",
    "end": "1004399"
  },
  {
    "text": "alignment guarantees are preserved and this is again pose issues it's an hard problem to solve because it basically uh",
    "start": "1004399",
    "end": "1012079"
  },
  {
    "text": "forc us to review a design uh uh an initial design we took with manager with",
    "start": "1012079",
    "end": "1017519"
  },
  {
    "text": "the source allocation which is okay now during the pod lifetime we need to change this allocation so we need to",
    "start": "1017519",
    "end": "1023440"
  },
  {
    "text": "take into account the fragmentation avoid it fragmentation resources and how to avoid it not uh not not a trivial",
    "start": "1023440",
    "end": "1031438"
  },
  {
    "text": "task we I I mentioned previously that implicit characteristics what does it",
    "start": "1032600",
    "end": "1037918"
  },
  {
    "text": "mean it means that there are constraints or desire or things that a workload will",
    "start": "1037919",
    "end": "1043438"
  },
  {
    "text": "like that are not immediately obvious from the podspec okay we can grant for",
    "start": "1043439",
    "end": "1048720"
  },
  {
    "text": "example exclusive allocation but you cannot just say hey I want exclusive CPU you need context to learn that for",
    "start": "1048720",
    "end": "1055200"
  },
  {
    "text": "example this uh resource request I I'm showing up means different things",
    "start": "1055200",
    "end": "1060480"
  },
  {
    "text": "depending on which node it lands which is okay we we grant the the what the workload actually requires but you need",
    "start": "1060480",
    "end": "1066480"
  },
  {
    "text": "to take account the node configuration and means different things depending on different nodes so once again is this an",
    "start": "1066480",
    "end": "1072640"
  },
  {
    "text": "expressive an this is probably an expressiveness problem because we will we will we could benefit from a clear",
    "start": "1072640",
    "end": "1080240"
  },
  {
    "text": "representation of those requirements if okay we can do that but giving the",
    "start": "1080240",
    "end": "1086640"
  },
  {
    "text": "workload the option to explicitly mention if needs that or prefers that allow us to make better decisions",
    "start": "1086640",
    "end": "1092640"
  },
  {
    "text": "because if a workload could tolerate lack of those guarantees which are otherwise always try to be enforced we",
    "start": "1092640",
    "end": "1099840"
  },
  {
    "text": "can make more informed decision and actually for example enable to us to reserve resources for the workloads",
    "start": "1099840",
    "end": "1105919"
  },
  {
    "text": "which actually require them so there are some emerging themes",
    "start": "1105919",
    "end": "1112400"
  },
  {
    "text": "hopefully from the examples I try to to make and in general over the conversation over the months and signal",
    "start": "1112400",
    "end": "1118480"
  },
  {
    "text": "and first of all those are among them the one I have I think there are worth",
    "start": "1118480",
    "end": "1124720"
  },
  {
    "text": "talking about which is f first and foremost the how do we allow the",
    "start": "1124720",
    "end": "1129760"
  },
  {
    "text": "workload uh owners to express in a more explicit way their requirement instead",
    "start": "1129760",
    "end": "1135919"
  },
  {
    "text": "of having to take more context extract for example the cublet configuration to learn about them and which hardware",
    "start": "1135919",
    "end": "1143760"
  },
  {
    "text": "model is cublet considering because for example because nowadays it is using basically the C advisor hardware model",
    "start": "1143760",
    "end": "1150799"
  },
  {
    "text": "so the data structure they represent hardware there are the ones from C advisor which is a very simplistic model",
    "start": "1150799",
    "end": "1157280"
  },
  {
    "text": "we are very close to the breaking point maybe someone could say we are past the breaking point but it is what it is and",
    "start": "1157280",
    "end": "1165039"
  },
  {
    "text": "we need to rethink about it if we want to move forward and unlock more POS possibilities and the fact that we have",
    "start": "1165039",
    "end": "1172160"
  },
  {
    "text": "a steady um flow of requests to review the resource allocation model also",
    "start": "1172160",
    "end": "1178960"
  },
  {
    "text": "brings the topic do we should we make it more modular than it is because nowadays",
    "start": "1178960",
    "end": "1184160"
  },
  {
    "text": "we need to change the cublet to enable those changes there is a a constant demand or chatter about make let's make",
    "start": "1184160",
    "end": "1190880"
  },
  {
    "text": "them pluggable let's make them modular because this way people can experiment with their own uh uh allocation",
    "start": "1190880",
    "end": "1198080"
  },
  {
    "text": "requirements and do their thingy on their own cluster without without changing things that",
    "start": "1198080",
    "end": "1205440"
  },
  {
    "text": "doesn't need to be changed and there there was a cap back in time about enabling those plugins this effort is is",
    "start": "1205440",
    "end": "1211039"
  },
  {
    "text": "not really proceeding but this is again uh a strong indication that such desire exists and still is is there in the",
    "start": "1211039",
    "end": "1220440"
  },
  {
    "text": "background which are the technologies which we can take into account and build upon to uh satisfy those needs i won't",
    "start": "1220440",
    "end": "1227840"
  },
  {
    "text": "just mention the two main ones which is the node resources interface which is basically a plug-in architecture for the",
    "start": "1227840",
    "end": "1233520"
  },
  {
    "text": "runtime interface and of course I promised DRA which is now f fully",
    "start": "1233520",
    "end": "1238960"
  },
  {
    "text": "focused on device but there are already conversation well we started to have conversation about how to expand it to",
    "start": "1238960",
    "end": "1245760"
  },
  {
    "text": "um to use for core resources namely CPU and",
    "start": "1245760",
    "end": "1250840"
  },
  {
    "text": "memory and like I hinted there is no clear direction about all those",
    "start": "1250840",
    "end": "1257440"
  },
  {
    "text": "desired ideas or requirements yet we have uh so many open questions and we have so many challenges to face and",
    "start": "1257440",
    "end": "1264480"
  },
  {
    "text": "again I'm just highlighting some of them which are emergent from the chatter from the conversation we have in signaled",
    "start": "1264480",
    "end": "1269840"
  },
  {
    "text": "which is about open question is first and foremost how much should we delegate outside the cublet if we delegate this",
    "start": "1269840",
    "end": "1277360"
  },
  {
    "text": "question depending on the how do we answer to this question we we we have more questions for example if we",
    "start": "1277360",
    "end": "1283520"
  },
  {
    "text": "delegate fully the the the the allocation and the ownership of the resources like croups manipulation for",
    "start": "1283520",
    "end": "1289200"
  },
  {
    "text": "example how we do how do we do bootstrap how do we ensure the relability again if",
    "start": "1289200",
    "end": "1294400"
  },
  {
    "text": "we delegate or if we do partial delegation because we are move more moving parts how do we ensure a constant",
    "start": "1294400",
    "end": "1300320"
  },
  {
    "text": "UIX because Kubernetes is already very rich ecosystem with many odd don'tons and then we are adding more and speaking",
    "start": "1300320",
    "end": "1307200"
  },
  {
    "text": "about the challenges um how do we keep up with the other requirement because hardware is getting more complex and",
    "start": "1307200",
    "end": "1313919"
  },
  {
    "text": "again some we could probably say that it has outpaced us in some area about the",
    "start": "1313919",
    "end": "1319600"
  },
  {
    "text": "other representation we need we have uh a constant stream of requests to adapt but",
    "start": "1319600",
    "end": "1326559"
  },
  {
    "text": "we also have this constant conversation about our redesigning if we keep we",
    "start": "1326559",
    "end": "1331760"
  },
  {
    "text": "should keep iterating because with the current architecture because there are actual demands but this makes the",
    "start": "1331760",
    "end": "1337120"
  },
  {
    "text": "current cubelet a moving target what we should say hey we we do in the next design versus what we do in the current",
    "start": "1337120",
    "end": "1343919"
  },
  {
    "text": "architecture if we do in the current architecture are the limitations as constraining our solution space so the",
    "start": "1343919",
    "end": "1350080"
  },
  {
    "text": "the solution which to implement is not good enough again open question",
    "start": "1350080",
    "end": "1355600"
  },
  {
    "text": "uh open questions and we we we we have to have to have those conversation and we need we need to iterate over there",
    "start": "1355600",
    "end": "1361520"
  },
  {
    "text": "while we do everything else uh we keep iterating over the over the big features",
    "start": "1361520",
    "end": "1368159"
  },
  {
    "text": "like VPN array and all the things we are currently doing which Peter will explain",
    "start": "1368159",
    "end": "1373440"
  },
  {
    "text": "to us all right thank you Franchesco so we've just done uh depth first and",
    "start": "1373440",
    "end": "1382600"
  },
  {
    "text": "so that was great we learned a lot and now we're going to run through a whole bunch of stuff that Sign Node has done",
    "start": "1382600",
    "end": "1388320"
  },
  {
    "text": "and so it's going to be very much breadth and it's going to be kind of a marathon and um I'm probably going to talk fast because I can't help it so",
    "start": "1388320",
    "end": "1396080"
  },
  {
    "text": "what has Sign Node been up to lately uh it turns out a whole lot um in the 133",
    "start": "1396080",
    "end": "1403679"
  },
  {
    "text": "cycle if you look at this long list I don't know if you can see the text because there's so many things we made",
    "start": "1403679",
    "end": "1409200"
  },
  {
    "text": "progress on 24 caps which I have not seen happen anywhere before so this is",
    "start": "1409200",
    "end": "1415640"
  },
  {
    "text": "potentially a SIG record if not even maybe project record i don't know could",
    "start": "1415640",
    "end": "1420960"
  },
  {
    "text": "be um which is very exciting and we feel um you know very proud of all the work that we've been doing and it's been",
    "start": "1420960",
    "end": "1426960"
  },
  {
    "text": "we've had a lot of help along the way for that i've sort of sorted these that's not five things that's six i",
    "start": "1426960",
    "end": "1433120"
  },
  {
    "text": "should have updated that um into different buckets um generally uh and so I'm going to kind of run through uh some",
    "start": "1433120",
    "end": "1439679"
  },
  {
    "text": "of the different buckets that have been uh you know can sort them in but this is not even an exhaustive um list of them",
    "start": "1439679",
    "end": "1446400"
  },
  {
    "text": "so um starting off one of the big things that we're very proud of is uh we've moved forward with um we've finally uh",
    "start": "1446400",
    "end": "1454000"
  },
  {
    "text": "gone to beta with in place pod resize so what we're scrolling through here is the entire issue uh of in place pod resize",
    "start": "1454000",
    "end": "1460960"
  },
  {
    "text": "and all of the work that has gone into that it's actually not even the whole thing because yes you",
    "start": "1460960",
    "end": "1467320"
  },
  {
    "text": "please it's amazing actually in this video we hide 500 comments so I'm not",
    "start": "1467320",
    "end": "1472720"
  },
  {
    "text": "even going to show all of the things that have happened on this issue but um it's been a really long time coming and",
    "start": "1472720",
    "end": "1477919"
  },
  {
    "text": "um it's taken a lot of work to get there and we're very proud of that um so finally uh you know we've moved forward",
    "start": "1477919",
    "end": "1484159"
  },
  {
    "text": "and um you know looking forward to the next uh changes that'll happen moving towards GA the next thing that we've",
    "start": "1484159",
    "end": "1491039"
  },
  {
    "text": "done um in 133 is we have uh we have come on now there we go ah we've g uh",
    "start": "1491039",
    "end": "1498240"
  },
  {
    "text": "sidec cars um which is also very exciting so this has been um little less long in the making but still it's been a",
    "start": "1498240",
    "end": "1504080"
  },
  {
    "text": "good effort um and it took an entire work group and um multiple caps and you know a lot of energy to uh move forward",
    "start": "1504080",
    "end": "1510960"
  },
  {
    "text": "on sidec cars um so we're very happy to be able to extend the pod life cycle to be able to have you know this new type",
    "start": "1510960",
    "end": "1517039"
  },
  {
    "text": "of container which is quite a tricky thing to do um next up we've got um some smaller",
    "start": "1517039",
    "end": "1522960"
  },
  {
    "text": "DRRA updates we talked a little bit about the DRA updates i mean small in the sense that like you know we're not uh Sign Node is not the entity that's",
    "start": "1522960",
    "end": "1529520"
  },
  {
    "text": "driving a lot of the updates now like things are going to be moving into theuler and a little bit into networking as well but we have gone beta with",
    "start": "1529520",
    "end": "1535360"
  },
  {
    "text": "structured parameters which is very exciting so that'll um we'll be able to be using that um in a lot um more",
    "start": "1535360",
    "end": "1541279"
  },
  {
    "text": "environments now and then al some other um edits that are also really important and we're going to continue moving",
    "start": "1541279",
    "end": "1546640"
  },
  {
    "text": "forward on it and continue investing in the DRRA space um here we have a list of things",
    "start": "1546640",
    "end": "1552880"
  },
  {
    "text": "that are loosely uh related they're all you know node things but they're kind of not related but they are all extensions",
    "start": "1552880",
    "end": "1559360"
  },
  {
    "text": "to the podspec we have you know uh can we do this yeah so host users true this",
    "start": "1559360",
    "end": "1566240"
  },
  {
    "text": "is for the username space feature um which has gone uh on by default beta along with the proc mount type um that's",
    "start": "1566240",
    "end": "1573679"
  },
  {
    "text": "actually a typo it should be procmount um sorry uh so these two are on by",
    "start": "1573679",
    "end": "1578880"
  },
  {
    "text": "default now um which is very exciting so you can have access to uh username spaces in pods um next up we have the",
    "start": "1578880",
    "end": "1586320"
  },
  {
    "text": "container stop signal um which is a way to encode the way that you want to stop",
    "start": "1586320",
    "end": "1591679"
  },
  {
    "text": "the container um outside of the container image which is currently the way or with the runtime default um we",
    "start": "1591679",
    "end": "1597200"
  },
  {
    "text": "have these uh extensions to the pod life cycle through pre-top sleep action um",
    "start": "1597200",
    "end": "1602320"
  },
  {
    "text": "which is both um the 0C option and just just generally the option both of those went to beta we have OCI volume mounts",
    "start": "1602320",
    "end": "1609360"
  },
  {
    "text": "which went to uh beta off by default beta um and allow us to mount in an OCI",
    "start": "1609360",
    "end": "1616400"
  },
  {
    "text": "um image and even potentially an artifact if the runtime supports it into a pod and then we've got a supplemental",
    "start": "1616400",
    "end": "1623840"
  },
  {
    "text": "group policy which allows you to um say what uh how you want to configure the",
    "start": "1623840",
    "end": "1630000"
  },
  {
    "text": "groups inside of a container to have it be a little bit more strict and thus a little bit more secure",
    "start": "1630000",
    "end": "1636080"
  },
  {
    "text": "next up we've got some cubit configuration option changes um ensure secret pulled images um which was driven",
    "start": "1636080",
    "end": "1642880"
  },
  {
    "text": "largely by sigoth by the end there um stand is here thank you for that um it's for but it is uh you know a lot of code",
    "start": "1642880",
    "end": "1650559"
  },
  {
    "text": "in the cublet to make it so that when you pull a uh you have an image which is",
    "start": "1650559",
    "end": "1656240"
  },
  {
    "text": "um pull policy if not present you still go through the authentication for that image so that if you're in a",
    "start": "1656240",
    "end": "1661520"
  },
  {
    "text": "multi-tenant environment you make sure that every pod that tries to use an image is actually authorized to do so but without wasting the pull um extra",
    "start": "1661520",
    "end": "1668400"
  },
  {
    "text": "times we've got some extensions to crash loop back off so now you can sort of better tune crash loop back off and",
    "start": "1668400",
    "end": "1674159"
  },
  {
    "text": "start it off at a different time so you can you know sort sort of uh better tune to the behavior of um the containers on",
    "start": "1674159",
    "end": "1680399"
  },
  {
    "text": "the node and then we have um projected service account tokens for image credentials which is also which is a way",
    "start": "1680399",
    "end": "1687120"
  },
  {
    "text": "to um allow a a image uh plugin uh image credential",
    "start": "1687120",
    "end": "1695039"
  },
  {
    "text": "plugin to use service account tokens instead of um just you hard coding them and that can allow you to customize",
    "start": "1695039",
    "end": "1702159"
  },
  {
    "text": "based on name spaces and stuff like that um finally just a couple of miscellaneous things that I was excited",
    "start": "1702159",
    "end": "1707440"
  },
  {
    "text": "about but couldn't really find a bucket or there's the PSI metrics which we'll be able to be uh reporting in alpha so",
    "start": "1707440",
    "end": "1713760"
  },
  {
    "text": "now we can potentially take uh use those that information and take action on um",
    "start": "1713760",
    "end": "1718960"
  },
  {
    "text": "information reported by the kernel about how long certain pods and certain croups are waiting for certain resources like",
    "start": "1718960",
    "end": "1724320"
  },
  {
    "text": "IO CPU and memory and then we've got some CPU manager policies franchesco",
    "start": "1724320",
    "end": "1729360"
  },
  {
    "text": "mentioned the um split L3 the uncore cache piece of it and then we've also got these other two things so bunch of",
    "start": "1729360",
    "end": "1735760"
  },
  {
    "text": "work this was not even everything um and so we're really excited we've been doing a lot and uh you may be wondering how",
    "start": "1735760",
    "end": "1742480"
  },
  {
    "text": "can I help with all of this exciting work and I'm glad that you asked and here's a non-exhausted list of the way that you could help if you want to join",
    "start": "1742480",
    "end": "1748480"
  },
  {
    "text": "and help out in the SIG this is roughly in order of maybe some of uh from top to bottom of things that like you know we",
    "start": "1748480",
    "end": "1754240"
  },
  {
    "text": "really need we've introduced a new uh role in the SIG which not officially but probably going to do that soon um cap",
    "start": "1754240",
    "end": "1761039"
  },
  {
    "text": "wrangler which is someone who helps out in the cap process you're not actually authoring cups but you're just like helping wrangle them along to make sure",
    "start": "1761039",
    "end": "1766960"
  },
  {
    "text": "that the authors stay along with deadlines which are constantly happening um so this is a really helpful um thing",
    "start": "1766960",
    "end": "1772399"
  },
  {
    "text": "that we found and definitely contributed to our um record number that we hit in 133 we also have a CI subgroup which",
    "start": "1772399",
    "end": "1779120"
  },
  {
    "text": "meets um weekly and is uh you know goes through uh any issues that are happening",
    "start": "1779120",
    "end": "1785039"
  },
  {
    "text": "in CI and you know triages issues and then also goes through open bugs um and",
    "start": "1785039",
    "end": "1790159"
  },
  {
    "text": "helps assign them so if you want to get started on fixing bugs which is another thing you can do joining the CI subgroup",
    "start": "1790159",
    "end": "1795679"
  },
  {
    "text": "is a good way to get introduced to that then we've got just general PR review and feedback we always we have so much",
    "start": "1795679",
    "end": "1801120"
  },
  {
    "text": "stuff going on we can love um feedback um documentation help always can use",
    "start": "1801120",
    "end": "1806240"
  },
  {
    "text": "this um trying out new features that we're pushing in the SIG is a great way especially if you're an end user and let",
    "start": "1806240",
    "end": "1812159"
  },
  {
    "text": "us know what you think of them attending the SIG node meetings um we always talk about a bunch of fun stuff and um and",
    "start": "1812159",
    "end": "1818720"
  },
  {
    "text": "then finally if you really want you could be running features though the barrier for that you know we have a lot",
    "start": "1818720",
    "end": "1823760"
  },
  {
    "text": "of things going on and the approvers have a lot to look at so uh we can't make any guarantees that we can get things in but we love when people um are",
    "start": "1823760",
    "end": "1831039"
  },
  {
    "text": "energetic about the new work and that is everything thank you so much for joining um please we have the feedback form here",
    "start": "1831039",
    "end": "1837600"
  },
  {
    "text": "and you have to be nice to us because we look so nice smiling here so thank",
    "start": "1837600",
    "end": "1842720"
  },
  {
    "text": "you i think we have time for questions do we have time for questions",
    "start": "1843000",
    "end": "1848880"
  },
  {
    "text": "yeah come to the mic please",
    "start": "1848880",
    "end": "1852760"
  },
  {
    "text": "um hey um so I saw the slide about the CPU resource policy and I wanted to know",
    "start": "1855760",
    "end": "1863039"
  },
  {
    "text": "if you ever u made a benchmark on how it really impact the the containers because",
    "start": "1863039",
    "end": "1870080"
  },
  {
    "text": "there is a significant overhead of doing it and understanding what we're doing so",
    "start": "1870080",
    "end": "1876480"
  },
  {
    "text": "what is really the benefits here",
    "start": "1876480",
    "end": "1881240"
  },
  {
    "text": "so yeah um every feature is driven by a cap pro possibly so in that cap uh in",
    "start": "1881520",
    "end": "1888000"
  },
  {
    "text": "order to graduate there is a a benchmark required okay what's the benefit this",
    "start": "1888000",
    "end": "1893039"
  },
  {
    "text": "change is granting for the very example I provided the the benchmarks are in the",
    "start": "1893039",
    "end": "1898720"
  },
  {
    "text": "ballroom from 20 to 30% with certain workloads for that specific feature on on the selected CPU not any random CPU",
    "start": "1898720",
    "end": "1906080"
  },
  {
    "text": "but so yeah but in general I want to stress that yes when a new feature is proposed except especially for a",
    "start": "1906080",
    "end": "1912720"
  },
  {
    "text": "performance-oriented feature people has to demonstrate the benefits and the condition which on which those benefits",
    "start": "1912720",
    "end": "1917840"
  },
  {
    "text": "they manifest themselves thank you thank you a question without a microphone let's",
    "start": "1917840",
    "end": "1924399"
  },
  {
    "text": "assume that I'm using a vertical port scaler that means that these",
    "start": "1924399",
    "end": "1929799"
  },
  {
    "text": "days the VPA won't restart the container um the question was about in place",
    "start": "1929799",
    "end": "1936080"
  },
  {
    "text": "vertical pod autoscaling and it was whether the VPA will be able to use this um that is intended to happen I think",
    "start": "1936080",
    "end": "1943600"
  },
  {
    "text": "yeah yeah so we made a change in um 133 comparing to previous versions so in",
    "start": "1943600",
    "end": "1950480"
  },
  {
    "text": "previous versions what you can express is I want to resize and you don't know",
    "start": "1950480",
    "end": "1955919"
  },
  {
    "text": "whether couplets will be able will be able to resize without restart so you can uh disrupt um resize can disrupt the",
    "start": "1955919",
    "end": "1964080"
  },
  {
    "text": "uh workload so now you can uh like we change the semantic of API so now if you",
    "start": "1964080",
    "end": "1969760"
  },
  {
    "text": "resizeing and restart is required it will rejected so there is options for that so that's why like we intentionally",
    "start": "1969760",
    "end": "1976320"
  },
  {
    "text": "wanted to make a VPA work and we had like cluster scaler uh consultants and",
    "start": "1976320",
    "end": "1981519"
  },
  {
    "text": "like we um we decided to change the semantic last minute and like I mean we almost get to bait it with previous",
    "start": "1981519",
    "end": "1988080"
  },
  {
    "text": "semantic in 132 but uh we got this strong feedback that it's not what um",
    "start": "1988080",
    "end": "1993120"
  },
  {
    "text": "VPA needs that's why we changed it yeah thank you so much",
    "start": "1993120",
    "end": "1999000"
  },
  {
    "text": "i have just two questions about are there any updates about confidential containers and rootless notes",
    "start": "2008240",
    "end": "2017200"
  },
  {
    "text": "no yeah there there was there is one kept",
    "start": "2017200",
    "end": "2023120"
  },
  {
    "text": "that is for push uh going to help out with confidential containers that we keep sort of um not having the cycles to",
    "start": "2023120",
    "end": "2029600"
  },
  {
    "text": "look at but we it is on the list of things that we might be looking at in the future um rootless is also in a",
    "start": "2029600",
    "end": "2036159"
  },
  {
    "text": "similar state um yeah so we haven't I actually don't know if there's been any",
    "start": "2036159",
    "end": "2041279"
  },
  {
    "text": "push for uh more work in that space but um you mentioned a secret verification",
    "start": "2041279",
    "end": "2047039"
  },
  {
    "text": "of images yeah this um the image verification piece I described uh really",
    "start": "2047039",
    "end": "2052398"
  },
  {
    "text": "quickly doesn't necessarily help oh jeez um didn't necessarily help with",
    "start": "2052399",
    "end": "2057480"
  },
  {
    "text": "um root list but does help like is secure a cluster um a little bit better",
    "start": "2057480",
    "end": "2063040"
  },
  {
    "text": "in a multi-tenant environment but um the focus of the sig is really more rootful cublet um because that's just you know",
    "start": "2063040",
    "end": "2069760"
  },
  {
    "text": "we kind of expect the cublet to have a lot of power um and to be able to do all this fun fancy stuff",
    "start": "2069760",
    "end": "2076320"
  },
  {
    "text": "so you have if you have specific needs and like um can increase urgency of this request by coming to meeting and like",
    "start": "2077079",
    "end": "2084000"
  },
  {
    "text": "telling us uh because there's bunch of priorities and uh we're doing our best",
    "start": "2084000",
    "end": "2089040"
  },
  {
    "text": "uh navigating priority and disruptive changes comparing to reliability of the group",
    "start": "2089040",
    "end": "2096839"
  },
  {
    "text": "hey again uh um so in about the pod resize um does it take into",
    "start": "2098079",
    "end": "2104400"
  },
  {
    "text": "consideration the system reserved and the cube reserved i mean it can cause some sort of uh node pressure so how it",
    "start": "2104400",
    "end": "2113119"
  },
  {
    "text": "behaves in this situation it does account for allocatable so",
    "start": "2113119",
    "end": "2118320"
  },
  {
    "text": "before resize will happen we will admit this resize and admission of this resize will account for all those all the other",
    "start": "2118320",
    "end": "2124880"
  },
  {
    "text": "ports running on this node all the coup reserved of the system reserved so it shouldn't affect uh uh so it also might",
    "start": "2124880",
    "end": "2131920"
  },
  {
    "text": "block the resize if there is not enough place thank you",
    "start": "2131920",
    "end": "2138599"
  },
  {
    "text": "okay let's do last question and uh oh last two questions it's fine",
    "start": "2143119",
    "end": "2148960"
  },
  {
    "text": "hello thanks for this call and uh my question was about more storage things uh for example when",
    "start": "2149200",
    "end": "2156640"
  },
  {
    "text": "we want to mount a volume like a fuse driver or something like that we always",
    "start": "2156640",
    "end": "2162240"
  },
  {
    "text": "need to mount to have capabilities admin to mount volume is there some evolutions",
    "start": "2162240",
    "end": "2168400"
  },
  {
    "text": "to fix that to prevent a container being gapsis admin so basically admin of",
    "start": "2168400",
    "end": "2173880"
  },
  {
    "text": "everything to be able to mount a driver fuse",
    "start": "2173880",
    "end": "2180040"
  },
  {
    "text": "i would not expect a container to need well so capsis admin on that the cublet",
    "start": "2180720",
    "end": "2186960"
  },
  {
    "text": "has or that the container process has on the container mounting the fuse driver I",
    "start": "2186960",
    "end": "2192160"
  },
  {
    "text": "would expect so in in a typical case with the volume I would expect the Cuba to be the one responsible for actually",
    "start": "2192160",
    "end": "2198160"
  },
  {
    "text": "doing the M or the OCI runtime but the Cuba asks the OCI runtime transitively through the Z runtime um for a for a um",
    "start": "2198160",
    "end": "2206000"
  },
  {
    "text": "C for when a pod wants to do mounting inside of itself um you actually that's",
    "start": "2206000",
    "end": "2212240"
  },
  {
    "text": "something username spaces could help out with because you can uh we can relax the validation on capsis admin into a",
    "start": "2212240",
    "end": "2218160"
  },
  {
    "text": "container um because if a pod is in a username space it's not a fully um unprivileged thing to do but I think we",
    "start": "2218160",
    "end": "2224320"
  },
  {
    "text": "have it in the baseline um pod security admission policy where if you have a username space pod then it can have the",
    "start": "2224320",
    "end": "2231359"
  },
  {
    "text": "capability like more capabilities um because it's username space so like that may help in your situation but I we're",
    "start": "2231359",
    "end": "2237839"
  },
  {
    "text": "never going to get to a place where we have containers um just having capsis admin and I don't think the kernel is",
    "start": "2237839",
    "end": "2243040"
  },
  {
    "text": "going to have a case where it's not going to give like not going to make it specifically capsis admin or some other capability so there's not much we can do",
    "start": "2243040",
    "end": "2250079"
  },
  {
    "text": "there okay and will name spaces help to run",
    "start": "2250079",
    "end": "2255839"
  },
  {
    "text": "containers inside of containers indeed that is in fact a driving force that I'm trying and proc uh procmount type also",
    "start": "2255839",
    "end": "2262240"
  },
  {
    "text": "will help with that um but yes both of those specifically I'm trying to get that working for so yeah",
    "start": "2262240",
    "end": "2270240"
  },
  {
    "text": "great thank you everyone",
    "start": "2270240",
    "end": "2273880"
  }
]