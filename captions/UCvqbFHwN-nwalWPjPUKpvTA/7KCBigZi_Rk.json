[
  {
    "text": "welcome everybody uh hope you are having a good time here at CubeCon and um today",
    "start": "80",
    "end": "6480"
  },
  {
    "text": "we are going to share how we built a AI HPC cluster on the top of Kubernetes in",
    "start": "6480",
    "end": "12480"
  },
  {
    "text": "public cloud uh so quick introduction my name is",
    "start": "12480",
    "end": "18800"
  },
  {
    "text": "Chandraan Abdut I'm a production engineer at Meta and my co-presenter Kalen Saledi uh he's a software engineer",
    "start": "18800",
    "end": "25760"
  },
  {
    "text": "uh he couldn't be here in person today uh but for his part of the presentation we have a recording that we are going to",
    "start": "25760",
    "end": "32480"
  },
  {
    "text": "play uh we both work on uh different teams but our team collaborate together",
    "start": "32480",
    "end": "38960"
  },
  {
    "text": "to uh build and uh operate uh AHPC the",
    "start": "38960",
    "end": "44000"
  },
  {
    "text": "cluster for research in public",
    "start": "44000",
    "end": "48480"
  },
  {
    "text": "cloud so today we'll go over uh like overview of a IML training how",
    "start": "49079",
    "end": "55039"
  },
  {
    "text": "it looks like what are the uh in infrastructure requirements for running uh AIPC workloads uh running slum on",
    "start": "55039",
    "end": "63199"
  },
  {
    "text": "communities and uh node node life cycle management and uh some of the storage",
    "start": "63199",
    "end": "69119"
  },
  {
    "text": "aspects of the cluster so with that I'm going to uh play a",
    "start": "69119",
    "end": "74240"
  },
  {
    "text": "recording uh so first part of the session is called by Kalyan so I'm just going to uh start the uh video",
    "start": "74240",
    "end": "83240"
  },
  {
    "text": "recording thank you Chandan uh hello everyone my name is Kalyan i'm a software engineer in research",
    "start": "83240",
    "end": "88880"
  },
  {
    "text": "infrastructure team at MA sorry I couldn't be there in person at CubeCon this year but I'm uh I partnered with",
    "start": "88880",
    "end": "95520"
  },
  {
    "text": "Chandan to help you understand what ML training and research space looks like and what does it take to build a",
    "start": "95520",
    "end": "101200"
  },
  {
    "text": "performant and uh uh highly productive research cluster in the cloud using Kubernetes so let's move on and try to",
    "start": "101200",
    "end": "108079"
  },
  {
    "text": "understand what ML training is at a high level this is a oversimplified picture but you",
    "start": "108079",
    "end": "114240"
  },
  {
    "text": "can imagine a training loop typically in PyTorch um deployed on a bunch of uh GPUs right",
    "start": "114240",
    "end": "121840"
  },
  {
    "text": "each of these GPUs is operating on data um training data samples and iteratively",
    "start": "121840",
    "end": "128560"
  },
  {
    "text": "computing and refining the weights this goes on for a while eventually resulting in a set of weights that that would be",
    "start": "128560",
    "end": "135520"
  },
  {
    "text": "called a model right the model is going to be released for production or for open sourcing right uh to give you um uh",
    "start": "135520",
    "end": "142560"
  },
  {
    "text": "what are the examples of models like llama mistral GPT photo you all might be familiar with right um before we",
    "start": "142560",
    "end": "151440"
  },
  {
    "text": "understand what does it take to build uh ML research cluster in the cloud it's",
    "start": "151440",
    "end": "156640"
  },
  {
    "text": "very important to understand what are how does it differ from traditional workloads right uh I'll focus on three",
    "start": "156640",
    "end": "163440"
  },
  {
    "text": "dimensions here number one runtime or lifetime of the job as opposed to web",
    "start": "163440",
    "end": "168800"
  },
  {
    "text": "services where the requests take a few milliseconds or hundreds of milliseconds right here the jobs run for any anywhere",
    "start": "168800",
    "end": "176000"
  },
  {
    "text": "from hours to weeks to months in some cases that means you the time span that",
    "start": "176000",
    "end": "182080"
  },
  {
    "text": "you're looking at is really different the second dimension is job size and semantics uh the semantics are really",
    "start": "182080",
    "end": "189519"
  },
  {
    "text": "important here um you can you have this all or nothing semantics say you have a",
    "start": "189519",
    "end": "195440"
  },
  {
    "text": "um 16 GPU job if any one of the GPUs is unhealthy or is suffering from some sort",
    "start": "195440",
    "end": "202239"
  },
  {
    "text": "of infra failure the entire job is impacted even if the other 15 are healthy you'll have you're effectively",
    "start": "202239",
    "end": "208239"
  },
  {
    "text": "restarting the job right this ch this is quite different from the uh traditional",
    "start": "208239",
    "end": "213280"
  },
  {
    "text": "web services where each node is independent of the other and if a if a particular node is unhealthy you can",
    "start": "213280",
    "end": "220159"
  },
  {
    "text": "retry the request on a different node and still satisfy the request right as you can see given the",
    "start": "220159",
    "end": "227920"
  },
  {
    "text": "lifetime of the job and the job size semantics reliability becomes that much",
    "start": "227920",
    "end": "233280"
  },
  {
    "text": "more important in the context of training uh workload infrastructure right um yeah so let's we we have an",
    "start": "233280",
    "end": "242319"
  },
  {
    "text": "very high level understanding of ML training right with that let's figure out what is an ML research life cycle",
    "start": "242319",
    "end": "248480"
  },
  {
    "text": "which is actually quite early uh in in the if you in the entire ML life cycle if you see these guys are um these",
    "start": "248480",
    "end": "256160"
  },
  {
    "text": "researchers are creating are innovating in the model architecture or the model",
    "start": "256160",
    "end": "261880"
  },
  {
    "text": "capabilities right let's go through a typical research life cycle",
    "start": "261880",
    "end": "267680"
  },
  {
    "text": "researchers are um reading papers understanding the latest model",
    "start": "267759",
    "end": "273000"
  },
  {
    "text": "innovations they're reproducing the results from open source or they are",
    "start": "273000",
    "end": "278560"
  },
  {
    "text": "starting with a state-of-the-art model that they have and trying to identify future directions they eventually come",
    "start": "278560",
    "end": "285199"
  },
  {
    "text": "up with some proposals right um based on the directions identified the proposals result in a in in they move on to",
    "start": "285199",
    "end": "293040"
  },
  {
    "text": "experimentation phase which may involve ablations sweeps hyperparameter sweeps",
    "start": "293040",
    "end": "299199"
  },
  {
    "text": "and u new model architecture innovations where you need to modify the code and",
    "start": "299199",
    "end": "304479"
  },
  {
    "text": "and prove that the architecture works eventually when everything settles down you go into your scale run mode meaning",
    "start": "304479",
    "end": "312240"
  },
  {
    "text": "you're now ready to launch your workload on on on hundreds or thousands of GPUs",
    "start": "312240",
    "end": "317759"
  },
  {
    "text": "based on your based on the models require compute requirements right once this process finishes the final result",
    "start": "317759",
    "end": "325039"
  },
  {
    "text": "is going to be like published model or productionized model or released to open",
    "start": "325039",
    "end": "330320"
  },
  {
    "text": "source community the code and the model right that's what we're looking at um if",
    "start": "330320",
    "end": "335600"
  },
  {
    "text": "you followed the space models have been rapidly evolving getting bigger and more",
    "start": "335600",
    "end": "341080"
  },
  {
    "text": "efficient of late and uh and if you look if you look at this curve the model",
    "start": "341080",
    "end": "347360"
  },
  {
    "text": "complexity in terms of u G-flops expressed is is is a really uh dramatic",
    "start": "347360",
    "end": "354400"
  },
  {
    "text": "illustration of the complexity growth right we we can kind of safely assume",
    "start": "354400",
    "end": "359440"
  },
  {
    "text": "that the curve is not going to meaningfully shift downwards and be prepared to handle the scaling needs of",
    "start": "359440",
    "end": "366479"
  },
  {
    "text": "ML training in the future okay um in before we figure out",
    "start": "366479",
    "end": "373520"
  },
  {
    "text": "how before we figure out how to build a research cluster we need to understand",
    "start": "373520",
    "end": "378639"
  },
  {
    "text": "what why researcher experience is paramount to our design right um as we",
    "start": "378639",
    "end": "385600"
  },
  {
    "text": "talked about it researcher experience uh in in the researcher experience slide um",
    "start": "385600",
    "end": "391520"
  },
  {
    "text": "these uh there is a lot of iteration involved right effectively iteration",
    "start": "391520",
    "end": "396720"
  },
  {
    "text": "speed is what is unlocking innovation if you're able to experiment more and prove or disprove your hypothesis you're able",
    "start": "396720",
    "end": "404000"
  },
  {
    "text": "to move farther along in in in the direction that you want to um that you",
    "start": "404000",
    "end": "409520"
  },
  {
    "text": "have identified right um researchers simply stated researcher taking an idea",
    "start": "409520",
    "end": "415600"
  },
  {
    "text": "to experiment should be uh as fast as and as simple as possible the more the barriers the slower the innovation right",
    "start": "415600",
    "end": "423360"
  },
  {
    "text": "the second part of researcher experience I would like to call out is um uh since",
    "start": "423360",
    "end": "428880"
  },
  {
    "text": "this is research and experimentation there is bound to be failures",
    "start": "428880",
    "end": "434400"
  },
  {
    "text": "researcher is going to face a ton of failures and the key aspect here is can",
    "start": "434400",
    "end": "439599"
  },
  {
    "text": "we separate the research failure versus the infraure right and that's one aspect",
    "start": "439599",
    "end": "446160"
  },
  {
    "text": "the second part is uh can we give first class monitoring and node metric node",
    "start": "446160",
    "end": "453680"
  },
  {
    "text": "resource metrics so that the researcher is able to track how is my model doing",
    "start": "453680",
    "end": "458960"
  },
  {
    "text": "and how am I using the compute and the resources underneath",
    "start": "458960",
    "end": "464160"
  },
  {
    "text": "Right um the third aspect of researcher experience uh is training reliability",
    "start": "464160",
    "end": "470080"
  },
  {
    "text": "the more the interruptions the the more the researcher is wasting time debugging",
    "start": "470080",
    "end": "476160"
  },
  {
    "text": "those failures and figuring out is it a code bug or is it an infra bug so we we",
    "start": "476160",
    "end": "481440"
  },
  {
    "text": "need to we have a goal to keep interruptions to a minimum right and support longunning jobs at massive scale",
    "start": "481440",
    "end": "491240"
  },
  {
    "text": "okay we talked about a uh we talked about reliability already but let's really understand why distributed",
    "start": "491440",
    "end": "498000"
  },
  {
    "text": "training reliability is is such a challenge in this picture you're seeing",
    "start": "498000",
    "end": "503360"
  },
  {
    "text": "four servers with two GPUs each running a workload running a ML um training",
    "start": "503360",
    "end": "509360"
  },
  {
    "text": "workload right slum is dispatching the tasks to the workers uh to the servers",
    "start": "509360",
    "end": "515200"
  },
  {
    "text": "right and the workers are pulling training data from the data store",
    "start": "515200",
    "end": "522039"
  },
  {
    "text": "right there are different training aspects but uh one thing one key thing import one key thing to understand here",
    "start": "522039",
    "end": "528560"
  },
  {
    "text": "is that all workers need to be in a consistent and consistent in terms of",
    "start": "528560",
    "end": "534880"
  },
  {
    "text": "configuration if you have diverges divergences among these nodes you don't",
    "start": "534880",
    "end": "540240"
  },
  {
    "text": "have control over the outcomes right and the nodes need to be healthy while in",
    "start": "540240",
    "end": "545440"
  },
  {
    "text": "order for the job to progress in order for the training to make forward",
    "start": "545440",
    "end": "550600"
  },
  {
    "text": "progress um so this is again goes back to the reliability and why reliability",
    "start": "550600",
    "end": "555760"
  },
  {
    "text": "is so important the outer circle that you see is the workers communicating",
    "start": "555760",
    "end": "560880"
  },
  {
    "text": "among themselves which means you need to have reliable compute storage and the",
    "start": "560880",
    "end": "566640"
  },
  {
    "text": "network if any of these components are weak or unreliable your your job is",
    "start": "566640",
    "end": "574640"
  },
  {
    "text": "getting less productive time training time on the cluster and that's really key here",
    "start": "574640",
    "end": "581519"
  },
  {
    "text": "uh one um this might be you know somewhat obvious to most people but the other nonobvious aspect is the sort of",
    "start": "581519",
    "end": "591440"
  },
  {
    "text": "lockstep training approach that is currently in vogue is is is going to result in load spikes on central",
    "start": "591440",
    "end": "599360"
  },
  {
    "text": "components and services in the component let me explain what I mean say all these",
    "start": "599360",
    "end": "604640"
  },
  {
    "text": "uh let's take a multi,000 GPU job right when we when the job decides to take a",
    "start": "604640",
    "end": "611240"
  },
  {
    "text": "checkpoint there's going to be massive load on the storage servers when the",
    "start": "611240",
    "end": "617120"
  },
  {
    "text": "checkpoint is written right and then right after the checkpoint is written",
    "start": "617120",
    "end": "622959"
  },
  {
    "text": "you will have near zero activity on on the file system and then back again when",
    "start": "622959",
    "end": "629279"
  },
  {
    "text": "it comes to the next checkpoint there are multiple examples of this load spike nature so it's very important to design",
    "start": "629279",
    "end": "635360"
  },
  {
    "text": "the cluster in a way that can sustain that can handle these load",
    "start": "635360",
    "end": "640920"
  },
  {
    "text": "spikes um one other aspect of reliability is which we also alluded to",
    "start": "640920",
    "end": "646720"
  },
  {
    "text": "in the researcher experience part is the attribution of failures is critical and",
    "start": "646720",
    "end": "653120"
  },
  {
    "text": "is also a challenge right imagine a multi-week job that's running on",
    "start": "653120",
    "end": "658480"
  },
  {
    "text": "thousand GPUs and if your same infra component is failing the job but we are",
    "start": "658480",
    "end": "665440"
  },
  {
    "text": "unable to root cause it is going to be really expensive right you need to be able to have good observability in a to",
    "start": "665440",
    "end": "674000"
  },
  {
    "text": "enable you to pinpoint what component is producing job symptoms and how you can",
    "start": "674000",
    "end": "679600"
  },
  {
    "text": "uh mitigate or uh improve close the gaps",
    "start": "679600",
    "end": "685040"
  },
  {
    "text": "All right so we we have a sort of a picture we knew we we we talked about",
    "start": "685600",
    "end": "691680"
  },
  {
    "text": "what ML training is what researcher experience what researcher experience should be now let's talk about what",
    "start": "691680",
    "end": "699440"
  },
  {
    "text": "where did we start before going to the cloud right we this is the picture that",
    "start": "699440",
    "end": "704720"
  },
  {
    "text": "you're seeing is is a real cluster that was customuilt and launched",
    "start": "704720",
    "end": "710399"
  },
  {
    "text": "in 202122 and we had thousands of uh DGX servers",
    "start": "710399",
    "end": "715600"
  },
  {
    "text": "with GPUs uh with CentOS running on them and uh orchestrated by slum slurm is the",
    "start": "715600",
    "end": "722399"
  },
  {
    "text": "scheduleuler that we used it had the cluster had directly managed backend network fabric multiple pabytes of flash",
    "start": "722399",
    "end": "730040"
  },
  {
    "text": "storage and purpose-built storage to accelerate training a purpose-built storage service to accelerate training",
    "start": "730040",
    "end": "736720"
  },
  {
    "text": "right now this is awesome this is direct control that we had but can we uh take",
    "start": "736720",
    "end": "743040"
  },
  {
    "text": "this and the challenge is can we take this and replicate it on the cloud maybe even do better than physical",
    "start": "743040",
    "end": "751959"
  },
  {
    "text": "cluster um to to understand what we are talking about this is what a typical uh",
    "start": "751959",
    "end": "757680"
  },
  {
    "text": "slum research cluster looks like from a researcher's point of view they don't see the rest of the complexity",
    "start": "757680",
    "end": "764240"
  },
  {
    "text": "the researcher is logging on to a a bunch of nodes one of these nodes and",
    "start": "764240",
    "end": "769519"
  },
  {
    "text": "they have access to the control plane which lets them submit jobs to the cluster and those jobs are running on",
    "start": "769519",
    "end": "775920"
  },
  {
    "text": "the worker nodes right and let's take our requirements and and",
    "start": "775920",
    "end": "784079"
  },
  {
    "text": "uh let's project it on on onto the cloud right what does an ideal ML research",
    "start": "784079",
    "end": "789680"
  },
  {
    "text": "cluster in the cloud let's come up with that problem statement we want the same experience of researchers able to log in",
    "start": "789680",
    "end": "799760"
  },
  {
    "text": "get access to control plane submit their job batch jobs to the queue and slum",
    "start": "799760",
    "end": "805040"
  },
  {
    "text": "being uh uh the dispatching agent which distributes the workload onto the",
    "start": "805040",
    "end": "810639"
  },
  {
    "text": "compute nodes of course you have the storage which helps you do the data uh",
    "start": "810639",
    "end": "817040"
  },
  {
    "text": "load the data training samples and uh support checkpointing right this is what",
    "start": "817040",
    "end": "822480"
  },
  {
    "text": "we want but on the characteristics side we want near bare metal performance we",
    "start": "822480",
    "end": "828959"
  },
  {
    "text": "don't want to compromise the researcher experience in and we don't want to slow",
    "start": "828959",
    "end": "834399"
  },
  {
    "text": "down the research velocity right we want to be able to support longunning jobs",
    "start": "834399",
    "end": "839760"
  },
  {
    "text": "with heavy data exchange among the workers right and demanding data loading",
    "start": "839760",
    "end": "845320"
  },
  {
    "text": "requirements with terabytes of data if not pabytes of data being pulled by the",
    "start": "845320",
    "end": "850360"
  },
  {
    "text": "GPUs right all this has to be accomplished while keeping things stable and performant one of fundamental",
    "start": "850360",
    "end": "858160"
  },
  {
    "text": "requirements that we lay down for any cluster that we uh consume that we",
    "start": "858160",
    "end": "863760"
  },
  {
    "text": "operate on is there is no perfect reliability when it comes to GPUs or these nodes right uh",
    "start": "863760",
    "end": "872480"
  },
  {
    "text": "our job becomes can we find the bad nodes fast and isolate them and shield the workload from the infra failures and",
    "start": "872480",
    "end": "880720"
  },
  {
    "text": "and this is a goal that we want to accomplish even on the cloud and you will see later in the presentation as to",
    "start": "880720",
    "end": "887600"
  },
  {
    "text": "how we accomplish that okay now that I have laid down the um case and and uh we",
    "start": "887600",
    "end": "894639"
  },
  {
    "text": "have a problem statement in front of us I'm going to hand it over to Chandan to see how we solved this problems and",
    "start": "894639",
    "end": "902320"
  },
  {
    "text": "built multiple research clusters in the cloud leveraging Kubernetes over to you",
    "start": "902320",
    "end": "909720"
  },
  {
    "text": "Chandan okay so so we just saw like how we wanted to",
    "start": "909720",
    "end": "916800"
  },
  {
    "text": "have like reserve built in the cloud so in the public cloud so when we started we we did not like started using",
    "start": "916800",
    "end": "923920"
  },
  {
    "text": "Kubernetes at the uh from the start so we we use something like this so uh we",
    "start": "923920",
    "end": "930399"
  },
  {
    "text": "have a meta instance u so in meta we have uh something called meta instance which is just a machine image uh that we",
    "start": "930399",
    "end": "937519"
  },
  {
    "text": "use across the public cloud providers uh so we take the uh cloud provider based",
    "start": "937519",
    "end": "942959"
  },
  {
    "text": "images uh host images and then we uh put our own layer on top of it so here in",
    "start": "942959",
    "end": "948160"
  },
  {
    "text": "the hello you can see the containerized uh meta instance layer so pretty much everything runs as a container uh",
    "start": "948160",
    "end": "954320"
  },
  {
    "text": "everything that we manage but on the top of that uh our the different teams at",
    "start": "954320",
    "end": "960720"
  },
  {
    "text": "meta can put their own applications and run it on the public cloud so in this case uh on the CPU nodes typically we",
    "start": "960720",
    "end": "967519"
  },
  {
    "text": "run the control plane components those are easy to containerize and run it so we already have those containerized uh",
    "start": "967519",
    "end": "974480"
  },
  {
    "text": "but uh the slumd component that runs on the GPU node and runs actual uh workloads that was not containerized",
    "start": "974480",
    "end": "981279"
  },
  {
    "text": "just because it was it handles a lot of like resource allocation uh cgroup manipulation and also it needs uh bare",
    "start": "981279",
    "end": "988399"
  },
  {
    "text": "metal access uh or direct access to the host to uh control the resources and",
    "start": "988399",
    "end": "993519"
  },
  {
    "text": "allocated to uh to the jobs so as we so this was our first implementation at the",
    "start": "993519",
    "end": "999120"
  },
  {
    "text": "time the footprint was not that big in the public cloud but slowly we started growing there were like multiple",
    "start": "999120",
    "end": "1004160"
  },
  {
    "text": "clusters uh we also started growing across multiple cloud providers as well uh so this was clearly not the solution",
    "start": "1004160",
    "end": "1010800"
  },
  {
    "text": "that we uh would that will scale uh for the for the future requirements so we",
    "start": "1010800",
    "end": "1016720"
  },
  {
    "text": "knew like we needed a better um container orchestration orchestration mechanism and Kubernetes was obvious",
    "start": "1016720",
    "end": "1023920"
  },
  {
    "text": "choice because of being industry standard and was a across multiple cloud",
    "start": "1023920",
    "end": "1030438"
  },
  {
    "text": "providers so we thought like we if we change the architecture we",
    "start": "1030439",
    "end": "1036640"
  },
  {
    "text": "will do something like this so wherein we'll run everything as a community spot all the control plane components all the",
    "start": "1036640",
    "end": "1042880"
  },
  {
    "text": "infrastructure components required for logging and metrics etc and then we can just leave slumd as it is uh running on",
    "start": "1042880",
    "end": "1050000"
  },
  {
    "text": "the host so that we don't have to deal with the complexity of resource allocation at the sum level and get involved into uh process of uh",
    "start": "1050000",
    "end": "1057200"
  },
  {
    "text": "researcher submitting a job and then that job getting allocated on the kubernetes on the on the GPU node uh but",
    "start": "1057200",
    "end": "1063600"
  },
  {
    "text": "we ended up implementing something like this uh so now cluster is divided into",
    "start": "1063600",
    "end": "1070000"
  },
  {
    "text": "two different parts on the right hand side uh typically uh we run u things like",
    "start": "1070000",
    "end": "1077360"
  },
  {
    "text": "ingress and ingress gateway on top of communities there are a few other infrastructure components that run there that are common that are uh reusable uh",
    "start": "1077360",
    "end": "1085760"
  },
  {
    "text": "across various clusters uh and then there is a left hand side where we actually run the GPU cluster and all the",
    "start": "1085760",
    "end": "1092320"
  },
  {
    "text": "like all the all the components that are required to run the workflow including the login environment control plane",
    "start": "1092320",
    "end": "1098720"
  },
  {
    "text": "components and all the all the components that we need for uh uh need to get the telemetry out of the out of",
    "start": "1098720",
    "end": "1105200"
  },
  {
    "text": "the cluster so reason for doing this split was the right hand side pretty much like stays the same we can reuse it",
    "start": "1105200",
    "end": "1111679"
  },
  {
    "text": "across multiple cloud providers we don't have to customize it at all on the left",
    "start": "1111679",
    "end": "1116880"
  },
  {
    "text": "hand side because we are running on Kubernetes now uh we can take that and run it across multiple cloud providers",
    "start": "1116880",
    "end": "1122799"
  },
  {
    "text": "but we still have to do some customization based on the like crowd provider or specific needs of the cluster uh specific hardware",
    "start": "1122799",
    "end": "1131320"
  },
  {
    "text": "etc so uh only thing we need in between is the L3 connectivity so that like they",
    "start": "1131320",
    "end": "1136559"
  },
  {
    "text": "are uh connected over the network and uh that's about it",
    "start": "1136559",
    "end": "1142799"
  },
  {
    "text": "so as we moved on to Kubernetes and um like it it kind of like made our life",
    "start": "1142799",
    "end": "1149520"
  },
  {
    "text": "easier as infrastructure team to manage infrastructure across various cloud providers um we did not want uh we",
    "start": "1149520",
    "end": "1156960"
  },
  {
    "text": "wanted to make sure the researchers either we provide a better experience for the uh researchers or at least we",
    "start": "1156960",
    "end": "1163360"
  },
  {
    "text": "match it to what was it before we moved to communities uh so typically when researchers log in",
    "start": "1163360",
    "end": "1170320"
  },
  {
    "text": "uh they just run slums uh commands to uh run their workflows and they so we",
    "start": "1170320",
    "end": "1178080"
  },
  {
    "text": "wanted to keep that same so because on the Kubernetes now um you have to run",
    "start": "1178080",
    "end": "1184080"
  },
  {
    "text": "like Helm cube cube kettle uh things like that to interact with the uh Kubernetes cluster uh we do not wanted",
    "start": "1184080",
    "end": "1191280"
  },
  {
    "text": "to have researchers learn all that or change their workflows across various environments uh uh so we wanted to",
    "start": "1191280",
    "end": "1197520"
  },
  {
    "text": "completely avoid that so uh to do that what we did is",
    "start": "1197520",
    "end": "1202799"
  },
  {
    "text": "like we came up with a custom CLI that researchers use to login into the cluster uh so that's custom CLI uh in",
    "start": "1202799",
    "end": "1209679"
  },
  {
    "text": "the back end it just creates a small uh helm release uh custom resource",
    "start": "1209679",
    "end": "1214720"
  },
  {
    "text": "definition and then we have a flux hem controller that is running in the inside the cluster so whenever researchers log",
    "start": "1214720",
    "end": "1221520"
  },
  {
    "text": "in that he release object is created and then dynamically a login pod is created for them and um when that pod is up and",
    "start": "1221520",
    "end": "1230000"
  },
  {
    "text": "running uh SSH session is launched and uh researchers uh get into the login",
    "start": "1230000",
    "end": "1236080"
  },
  {
    "text": "environment and they can uh from there on they can just run their normal workflow they don't even realize they're inside the cluster they don't have to",
    "start": "1236080",
    "end": "1242640"
  },
  {
    "text": "run anything uh they don't have to learn anything about Kubernetes so uh there are so we also run uh Kyo",
    "start": "1242640",
    "end": "1253039"
  },
  {
    "text": "admission controller so that is the uh web hook that you can that you can use",
    "start": "1253039",
    "end": "1258320"
  },
  {
    "text": "uh that's so the admission controller make sure uh the Henry is uh is created",
    "start": "1258320",
    "end": "1265200"
  },
  {
    "text": "as per as we expected it to be so for example if I go and start like customizing it to get elevated",
    "start": "1265200",
    "end": "1272640"
  },
  {
    "text": "privileges inside the cluster uh it won't allow that and also I it won't allow me to create a login pod for with",
    "start": "1272640",
    "end": "1279520"
  },
  {
    "text": "some other user's name things like that so uh if you if you if it validates the",
    "start": "1279520",
    "end": "1284880"
  },
  {
    "text": "policy it will reject the uh that uh object and it will not let you login into the environment so with this now uh",
    "start": "1284880",
    "end": "1293600"
  },
  {
    "text": "let's look at let's compare it to like how it was before so before u we had a",
    "start": "1293600",
    "end": "1300159"
  },
  {
    "text": "virtual machines that uh were shared between the various uh users and um now",
    "start": "1300159",
    "end": "1306320"
  },
  {
    "text": "they get dedicated pod with the dedicated resource allocation so now we have better security posture and also it",
    "start": "1306320",
    "end": "1312559"
  },
  {
    "text": "avoids problems like noisy neighbor so at the end at the end like we we ended up giving them a better experience",
    "start": "1312559",
    "end": "1321400"
  },
  {
    "text": "uh infrastructure component management also uh changed a lot it become a lot easier so now you can see here we",
    "start": "1321919",
    "end": "1328320"
  },
  {
    "text": "already talked about the slum uh so we run few more things on the cluster as a",
    "start": "1328320",
    "end": "1333360"
  },
  {
    "text": "community demonstrate our deployments um as I mentioned earlier we typically just",
    "start": "1333360",
    "end": "1338559"
  },
  {
    "text": "use the cloud provider uh provided host images and we don't like to maintain any customization or have to deal with any",
    "start": "1338559",
    "end": "1345919"
  },
  {
    "text": "customization at the host level but we couldn't really avoid it entirely so we",
    "start": "1345919",
    "end": "1351280"
  },
  {
    "text": "had to do certain things for example uh in the yellow layer there you can see cisco demon set that runs across the",
    "start": "1351280",
    "end": "1357200"
  },
  {
    "text": "cluster configures the kernel parameters that are required to run our uh our",
    "start": "1357200",
    "end": "1362360"
  },
  {
    "text": "workloads uh it also shows the automount demon set um that mounts the NFS volumes",
    "start": "1362360",
    "end": "1369120"
  },
  {
    "text": "across the host providing the consistent access to the file systems we'll see a little bit more about the automount",
    "start": "1369120",
    "end": "1374640"
  },
  {
    "text": "demon set later in the presentation uh we also run various telemetry components combination of open source vendor",
    "start": "1374640",
    "end": "1380799"
  },
  {
    "text": "provided and custom build so as Ken mentioned um it was important to have a",
    "start": "1380799",
    "end": "1387039"
  },
  {
    "text": "like very strong observability story so that if something fails uh we can easily",
    "start": "1387039",
    "end": "1392640"
  },
  {
    "text": "uh triage why it failed uh whether it is a application problem or in problem and if it is impro problem uh how how",
    "start": "1392640",
    "end": "1400159"
  },
  {
    "text": "quickly we can we can solve it so we have some customuilt components",
    "start": "1400159",
    "end": "1405280"
  },
  {
    "text": "such as some job matrix u core dumper uh which uh takes a core dump of u uh",
    "start": "1405280",
    "end": "1412159"
  },
  {
    "text": "whenever process is terminate uh unexpectedly uh we also have few",
    "start": "1412159",
    "end": "1418000"
  },
  {
    "text": "components that are like custom kubernetes controller so in this example here I have added user identity and",
    "start": "1418000",
    "end": "1424320"
  },
  {
    "text": "service identity controller and there is also search manager search manager is a open source tool for managing search on",
    "start": "1424320",
    "end": "1430159"
  },
  {
    "text": "kubernetes uh in our case we use X509 based uh searchs which are signed by",
    "start": "1430159",
    "end": "1436080"
  },
  {
    "text": "private CA and uh to be able to do that and then we have our own policies that apply when uh some like when the user",
    "start": "1436080",
    "end": "1443360"
  },
  {
    "text": "requests a search uh so with the custom uh controllers now we can in uh we can",
    "start": "1443360",
    "end": "1450000"
  },
  {
    "text": "integrate with our own tooling uh so utilizing both uh so utilizing open source tooling with some custom layer on",
    "start": "1450000",
    "end": "1458000"
  },
  {
    "text": "top of that to be able to integrate with our own uh our own uh infrastructure components there are a bunch of other",
    "start": "1458000",
    "end": "1464799"
  },
  {
    "text": "things that are run that run as infrastructure components and now earlier pretty much like our team used",
    "start": "1464799",
    "end": "1470960"
  },
  {
    "text": "to manage that uh working with other teams now it become little bit more uh it's become more independent they can",
    "start": "1470960",
    "end": "1477600"
  },
  {
    "text": "just we can just give them the name space they can run their stuff and uh we don't have to get in between they can",
    "start": "1477600",
    "end": "1482960"
  },
  {
    "text": "automate u uh using our helm automation tooling uh deployment of u deployment of",
    "start": "1482960",
    "end": "1489440"
  },
  {
    "text": "whatever components that need to be run on the cluster uh so as Kalan mentioned earlier",
    "start": "1489440",
    "end": "1497600"
  },
  {
    "text": "it was important for us to make sure uh nodes are consistent um so let's say if",
    "start": "1497600",
    "end": "1503760"
  },
  {
    "text": "researcher submitted a job across like uh 400 nodes all those nodes should have",
    "start": "1503760",
    "end": "1508880"
  },
  {
    "text": "the consistent uh configuration and cons consistent image versions etc so to be",
    "start": "1508880",
    "end": "1514720"
  },
  {
    "text": "able to do that what uh what we have is our everything is handled by our helm",
    "start": "1514720",
    "end": "1520559"
  },
  {
    "text": "deployment automation so whenever there is a change uh in the helm chart or image that gets applied on kubernetes",
    "start": "1520559",
    "end": "1526640"
  },
  {
    "text": "and the custom controller running on the kubernetes um first thing it does it it starts a rolling update on the slumd",
    "start": "1526640",
    "end": "1532720"
  },
  {
    "text": "nodes and um it updates uh it drains all the nodes in the",
    "start": "1532720",
    "end": "1538799"
  },
  {
    "text": "cluster so at that point uh slum makes sure uh those nodes don't get any new",
    "start": "1538799",
    "end": "1544799"
  },
  {
    "text": "jobs allocated and it lets it finish existing jobs that are running on the on the node so once uh the node finishes",
    "start": "1544799",
    "end": "1551520"
  },
  {
    "text": "the uh existing job it it it goes into the drain state and then uh the",
    "start": "1551520",
    "end": "1558400"
  },
  {
    "text": "kubernetes controller then uh replaces the slum pod with the new version and",
    "start": "1558400",
    "end": "1564400"
  },
  {
    "text": "then um undrains them so now it become that node becomes available again back",
    "start": "1564400",
    "end": "1569440"
  },
  {
    "text": "in the cluster for uh to pick up the next workload we run uh a few telemetry",
    "start": "1569440",
    "end": "1576400"
  },
  {
    "text": "uh health checks that run uh in a Kubernetes control plane as well as uh",
    "start": "1576400",
    "end": "1581440"
  },
  {
    "text": "on the slum to be able to detect faulty nodes and then those nodes are kind of like tracked and then our support team",
    "start": "1581440",
    "end": "1589840"
  },
  {
    "text": "uh triages it and in some cases we have to take it back to the cloud provider uh to get it fixed or replaced",
    "start": "1589840",
    "end": "1599159"
  },
  {
    "text": "so for the storage um because um AI HPC",
    "start": "1600799",
    "end": "1606320"
  },
  {
    "text": "workloads are like data uh data intensive we needed to make sure we provide many of storage options starting",
    "start": "1606320",
    "end": "1612320"
  },
  {
    "text": "from simple posic storage to um AI accelerated uh object storage",
    "start": "1612320",
    "end": "1619279"
  },
  {
    "text": "so for non-posic storage such as object store we did not have to do any changes because we moved to kubernetes because",
    "start": "1619279",
    "end": "1625440"
  },
  {
    "text": "pretty much uh it just depends on the client and um uh they just make the same",
    "start": "1625440",
    "end": "1631120"
  },
  {
    "text": "API call they used to make before but for NFS because uh if we look at the",
    "start": "1631120",
    "end": "1636960"
  },
  {
    "text": "Kubernetes and how Kubernetes handles like volume mounts uh using CI drivers posted volume claims and volume mounts",
    "start": "1636960",
    "end": "1643360"
  },
  {
    "text": "etc uh anytime you want to new mount a new volume you need a part restart so uh",
    "start": "1643360",
    "end": "1649279"
  },
  {
    "text": "that that is not possible uh in our case because we don't want to disrupt AI workloads that are running there we",
    "start": "1649279",
    "end": "1655279"
  },
  {
    "text": "wanted to have a way where we can uh without any disruptions uh introduce new",
    "start": "1655279",
    "end": "1661120"
  },
  {
    "text": "file systems or take away existing file systems when they are not needed so we ended up implementing something like",
    "start": "1661120",
    "end": "1666880"
  },
  {
    "text": "this so uh so as I talked about automount demonstrate a little bit earlier so automount demon set runs",
    "start": "1666880",
    "end": "1672799"
  },
  {
    "text": "across the cluster on every single node and it makes sure uh it mounts",
    "start": "1672799",
    "end": "1678320"
  },
  {
    "text": "Nest volumes so sorry Nest file systems so those file systems are categorized",
    "start": "1678320",
    "end": "1683360"
  },
  {
    "text": "into various categories such as checkpoints data set infra uh and those",
    "start": "1683360",
    "end": "1688480"
  },
  {
    "text": "categories are parent directories uh on the host and inside that parent directories those gets mounted and then",
    "start": "1688480",
    "end": "1694720"
  },
  {
    "text": "those parent directories are bound bind mounted inside the slumd and login pod so that whenever new file systems are",
    "start": "1694720",
    "end": "1700799"
  },
  {
    "text": "added nothing changes for uh slumd and login part but those file systems become available for the uh for the",
    "start": "1700799",
    "end": "1709759"
  },
  {
    "text": "[Music] workloads so to to conclude so from",
    "start": "1710080",
    "end": "1715520"
  },
  {
    "text": "onremises to public cloud across uh across multiple cloud providers was made by possible by moving to Kubernetes uh",
    "start": "1715520",
    "end": "1723279"
  },
  {
    "text": "as um as we kind of like iterated over uh the iteration speed so now by because",
    "start": "1723279",
    "end": "1730399"
  },
  {
    "text": "we are on Kubernetes um various teams are can handle their components independently",
    "start": "1730399",
    "end": "1736880"
  },
  {
    "text": "uh we are able to like fast we are able to deliver things faster in the cloud",
    "start": "1736880",
    "end": "1742320"
  },
  {
    "text": "making more services new features available faster uh unlocking the innov",
    "start": "1742320",
    "end": "1748080"
  },
  {
    "text": "uh innovation and uh we are also did not have to sacrifice on research experience",
    "start": "1748080",
    "end": "1754640"
  },
  {
    "text": "that was uh very very important for us uh from performance Once reliability sides uh we we did not had to so we kind",
    "start": "1754640",
    "end": "1762320"
  },
  {
    "text": "of like met the requirement that was set forth uh even though we moved on Kubernetes or in some cases we have",
    "start": "1762320",
    "end": "1769039"
  },
  {
    "text": "better observative story because we are able to utilize um tooling from cloud",
    "start": "1769039",
    "end": "1775679"
  },
  {
    "text": "providers as well as open source as well as custom build uh running HPC on",
    "start": "1775679",
    "end": "1780720"
  },
  {
    "text": "Kubernetes has challenges at scale uh but most of those we found very addressable but sometime but still there",
    "start": "1780720",
    "end": "1788000"
  },
  {
    "text": "there are it's not like painfree environment so there are still some scaling issues that you will you will",
    "start": "1788000",
    "end": "1794080"
  },
  {
    "text": "run into uh Kubernetes ecosystem uh allows us to like move fast uh across",
    "start": "1794080",
    "end": "1801679"
  },
  {
    "text": "multiple pro cloud providers is just obvious like it just uh vastly supported",
    "start": "1801679",
    "end": "1808320"
  },
  {
    "text": "uh industry standard so it's much easier for uh maj easier to adopt and reiterate",
    "start": "1808320",
    "end": "1814880"
  },
  {
    "text": "on",
    "start": "1814880",
    "end": "1817880"
  },
  {
    "text": "okay sorry okay so that that brings us to the uh end of the present presentation uh I just wanted to also",
    "start": "1820799",
    "end": "1828000"
  },
  {
    "text": "thank uh the open source community for making all these various components available uh that made us possible to",
    "start": "1828000",
    "end": "1835520"
  },
  {
    "text": "move to Kubernetes and uh in in very short period of time",
    "start": "1835520",
    "end": "1841100"
  },
  {
    "text": "[Applause] with that um that concludes the session today so with that we I can take some",
    "start": "1841100",
    "end": "1846720"
  },
  {
    "text": "questions i'll be also available here or in the lobby later if you have uh if you have questions as well",
    "start": "1846720",
    "end": "1852730"
  },
  {
    "text": "[Applause] thank you very much for an amazing talk",
    "start": "1852730",
    "end": "1859200"
  },
  {
    "text": "i have a couple questions about reliability for the cloud uh public cloud solution you presented uh first",
    "start": "1859200",
    "end": "1866320"
  },
  {
    "text": "one is about how do you distinguish between uh workload level issues which always happen at scale and the infra",
    "start": "1866320",
    "end": "1873120"
  },
  {
    "text": "level issues and the second is when you exclude certain nodes from the cluster because of the failures do you",
    "start": "1873120",
    "end": "1880240"
  },
  {
    "text": "overprovision on your sides to account for them or do you expect your cloud provider to compensate for this cost",
    "start": "1880240",
    "end": "1886480"
  },
  {
    "text": "somehow yeah so uh regarding the first question so yeah it can be tricky uh and",
    "start": "1886480",
    "end": "1893360"
  },
  {
    "text": "we might not always get it right right away uh but we have obserity story built",
    "start": "1893360",
    "end": "1899440"
  },
  {
    "text": "in to kind of like to help and we are still working on like making it better so for one instance like I can maybe",
    "start": "1899440",
    "end": "1906480"
  },
  {
    "text": "give you an example so in in some cases what happens is in the public cloud uh",
    "start": "1906480",
    "end": "1911840"
  },
  {
    "text": "we found a problem with the node we send it back to the cloud provider it comes back they don't find any issues with it",
    "start": "1911840",
    "end": "1918320"
  },
  {
    "text": "it comes back to the cluster again that node kind of like keeps failing keeps failing so we kind of like track",
    "start": "1918320",
    "end": "1924320"
  },
  {
    "text": "failures on the particular node across uh across various jobs and kind of like",
    "start": "1924320",
    "end": "1929919"
  },
  {
    "text": "try and find okay so this is a like bad a bad node that caused uh x number of",
    "start": "1929919",
    "end": "1937279"
  },
  {
    "text": "failures in last 30 days so kind of like take like keeping the bad actors away uh",
    "start": "1937279",
    "end": "1942880"
  },
  {
    "text": "is kind of like approach uh we have implementing one more one more tricky thing in the public cloud is uh the same",
    "start": "1942880",
    "end": "1950640"
  },
  {
    "text": "hardware can come back as a as a different host so we are also currently",
    "start": "1950640",
    "end": "1957039"
  },
  {
    "text": "like looking into better ways to track that using GPU IDs and uh serial serial ids and so that like okay we don't",
    "start": "1957039",
    "end": "1963519"
  },
  {
    "text": "depend on the actual physical location on the host or actual uh name of the host but uh we can really track down by",
    "start": "1963519",
    "end": "1970399"
  },
  {
    "text": "the uh uh by the by the GPU IDs and uh and things like that",
    "start": "1970399",
    "end": "1978600"
  },
  {
    "text": "i have a question about your W2 or W3 I don't remember about your slam D how how",
    "start": "1978720",
    "end": "1984320"
  },
  {
    "text": "do you deploy that one you you deploy it in the bare metal environment it seems",
    "start": "1984320",
    "end": "1989679"
  },
  {
    "text": "to me uh yeah so it was not clear like entirely a bare metal environment so it",
    "start": "1989679",
    "end": "1995919"
  },
  {
    "text": "was more on like something like AWS EC2 so it has a thin uh virtualization layer",
    "start": "1995919",
    "end": "2001200"
  },
  {
    "text": "on top of it so but we just run but you other components is actually part of the",
    "start": "2001200",
    "end": "2007360"
  },
  {
    "text": "container part of the Kubernetes right yeah yeah but this one is separate yeah",
    "start": "2007360",
    "end": "2012799"
  },
  {
    "text": "so they it just runs on the OS like a system you need uh so but when they",
    "start": "2012799",
    "end": "2018399"
  },
  {
    "text": "launch job what kind of a like when we finally launch the job to the working",
    "start": "2018399",
    "end": "2024159"
  },
  {
    "text": "node what does it look like yeah so in the slum pretty much uh in the in the",
    "start": "2024159",
    "end": "2031279"
  },
  {
    "text": "both the environments when somebody submits a job they log into the slum and they submit the job and then the slumd",
    "start": "2031279",
    "end": "2038480"
  },
  {
    "text": "kind of like spins up the uh task process on the worker node uh and that",
    "start": "2038480",
    "end": "2043760"
  },
  {
    "text": "executes the actual workload so no matter you are running inside the pod or you are running as a process on the host",
    "start": "2043760",
    "end": "2050878"
  },
  {
    "text": "that process doesn't doesn't change much uh and then also about the deployment uh",
    "start": "2050879",
    "end": "2056638"
  },
  {
    "text": "in that way still use SAF and separator deployment on this node only uh yeah so",
    "start": "2056639",
    "end": "2063599"
  },
  {
    "text": "in new model we don't use chef we just run everything as a Kubernetes demonstrate because we do only small",
    "start": "2063599",
    "end": "2069200"
  },
  {
    "text": "things on the host level uh we don't really need a config management uh and have to deal with uh like differences",
    "start": "2069200",
    "end": "2076720"
  },
  {
    "text": "between various cloud providers different versions of kernel and things like that okay thank you",
    "start": "2076720",
    "end": "2084878"
  },
  {
    "text": "uh hi I have just one questions uh have you considered some open source",
    "start": "2084879",
    "end": "2090878"
  },
  {
    "text": "kubernetes operator which managed slarm inside of kubernetes for instance",
    "start": "2090879",
    "end": "2096000"
  },
  {
    "text": "operator or something like that yes we are reeuting that as well u so yeah in",
    "start": "2096000",
    "end": "2102880"
  },
  {
    "text": "near future uh we will pe out on that I think uh but yeah that's that's",
    "start": "2102880",
    "end": "2109520"
  },
  {
    "text": "something we have been like discussing internally okay",
    "start": "2109520",
    "end": "2116680"
  },
  {
    "text": "hi uh great talk so I mean like when you have these long running jobs and you",
    "start": "2117839",
    "end": "2123280"
  },
  {
    "text": "have to do system level updates right um there might be uh a period where you",
    "start": "2123280",
    "end": "2129920"
  },
  {
    "text": "have to wait for that job to finish before you get the feedback uh how do",
    "start": "2129920",
    "end": "2135040"
  },
  {
    "text": "you handle that uh portion of it yeah yes so uh on the Kubernetes because",
    "start": "2135040",
    "end": "2143320"
  },
  {
    "text": "um now slumd runs as a job on the as a pod so let's say if you want to do a",
    "start": "2143320",
    "end": "2150400"
  },
  {
    "text": "security patch on the host uh that doesn't need a restart so you can still do it without affecting the slum because",
    "start": "2150400",
    "end": "2156960"
  },
  {
    "text": "slumd runs in its own name space uh uh and uh uh it doesn't affect it so that's",
    "start": "2156960",
    "end": "2163200"
  },
  {
    "text": "why like everything running uh alongside of like slumd pods on that particular",
    "start": "2163200",
    "end": "2169680"
  },
  {
    "text": "host they can continue to update things but uh workload running inside the slumd",
    "start": "2169680",
    "end": "2175760"
  },
  {
    "text": "pod doesn't see those changes so it still see the whatever there whatever is there there on the slamd some things can",
    "start": "2175760",
    "end": "2182640"
  },
  {
    "text": "be tricky so we have to avoid uh for example like upgrading uh or changing",
    "start": "2182640",
    "end": "2188720"
  },
  {
    "text": "kernel uh parameters or stuff like that or applying a patch",
    "start": "2188720",
    "end": "2194160"
  },
  {
    "text": "So but overall like the things we do normally such as for example hey we run",
    "start": "2194520",
    "end": "2201440"
  },
  {
    "text": "a let's say a metric agent that needs update we can update stuff like that",
    "start": "2201440",
    "end": "2207440"
  },
  {
    "text": "without without having to worry about affecting the workload that runs inside the slum",
    "start": "2207440",
    "end": "2212520"
  },
  {
    "text": "pod thanks hey thanks for the talk it was really",
    "start": "2212520",
    "end": "2218720"
  },
  {
    "text": "nice um and yeah I just had a few questions so number one I think you said",
    "start": "2218720",
    "end": "2224160"
  },
  {
    "text": "focusing on the researcher experience was paramount and I noticed you said you create like a fake CLI in a sense to",
    "start": "2224160",
    "end": "2231280"
  },
  {
    "text": "help do that so why did you insist on uh continuing to use slurm shall we say uh if you were kind of uh able to interject",
    "start": "2231280",
    "end": "2239040"
  },
  {
    "text": "in that point and create cla yeah who",
    "start": "2239040",
    "end": "2244480"
  },
  {
    "text": "uh so question is like why we insisted to use slum and why not kubernetes yeah",
    "start": "2244480",
    "end": "2249520"
  },
  {
    "text": "so for example today I've uh well in this week I've seen sorts of things about uh training pipelines you know",
    "start": "2249520",
    "end": "2255599"
  },
  {
    "text": "cubeflow qflow pipelines and other things like that so you know why not go that approach and create things in the",
    "start": "2255599",
    "end": "2261520"
  },
  {
    "text": "CLI to yes so I think slum is",
    "start": "2261520",
    "end": "2267880"
  },
  {
    "text": "like like very widely uh used tool tool for doing AI research ai is basically",
    "start": "2267880",
    "end": "2274960"
  },
  {
    "text": "kind of like research um and um at least in the near term we did not had uh plans",
    "start": "2274960",
    "end": "2282240"
  },
  {
    "text": "to like change that and then pretty much like lot of people working in acade acade uh academic research they",
    "start": "2282240",
    "end": "2289040"
  },
  {
    "text": "typically have a lot of experience working with slum and uh it also provides uh way better uh controls for",
    "start": "2289040",
    "end": "2296800"
  },
  {
    "text": "running longunning bad jobs so that is something that um uh we did not want it",
    "start": "2296800",
    "end": "2302640"
  },
  {
    "text": "to change but what you're saying is also like we have been like considering so we",
    "start": "2302640",
    "end": "2308720"
  },
  {
    "text": "might end up like having another uh controller that runs on kubernetes but",
    "start": "2308720",
    "end": "2314079"
  },
  {
    "text": "we can keep slum for people who are comfortable with using slum and then but there are there are there are certain",
    "start": "2314079",
    "end": "2320560"
  },
  {
    "text": "users they want to kind of like like use uh kubernetes or some other uh",
    "start": "2320560",
    "end": "2327800"
  },
  {
    "text": "kubernetes way of doing things basically so in future we might end up doing both",
    "start": "2327800",
    "end": "2333359"
  },
  {
    "text": "nice yeah I can totally understand the feeling of sticking to slurm to you know kind of get that researchers buy in the",
    "start": "2333359",
    "end": "2340720"
  },
  {
    "text": "second question is actually very related to what you were saying at the end would you be able to make this work for uh a",
    "start": "2340720",
    "end": "2348000"
  },
  {
    "text": "combination of on-prem clusters and public cloud and I assume that you know helps the companies stay quite agile as",
    "start": "2348000",
    "end": "2354560"
  },
  {
    "text": "well yeah so for internal like we have very mature",
    "start": "2354560",
    "end": "2360480"
  },
  {
    "text": "tooling for internal like uh handling internal like whatever runs in our data centers we already have like automation",
    "start": "2360480",
    "end": "2368240"
  },
  {
    "text": "we have like uh so we don't need to like uh do much customization there but in",
    "start": "2368240",
    "end": "2374480"
  },
  {
    "text": "the public cloud what what happens is because uh we are kind of like limited",
    "start": "2374480",
    "end": "2379599"
  },
  {
    "text": "by what public cloud has to provide and then also kind of like implement something hybrid that works with our",
    "start": "2379599",
    "end": "2385599"
  },
  {
    "text": "internal tooling so that's why like Kubernetes helps there but in on the for",
    "start": "2385599",
    "end": "2391599"
  },
  {
    "text": "the like private clusters we already have uh a good tooling that integrates",
    "start": "2391599",
    "end": "2397760"
  },
  {
    "text": "well with our own infra so it's not necessarily we have to customize that yeah fair enough uh so what I mean is uh",
    "start": "2397760",
    "end": "2404400"
  },
  {
    "text": "having a common interface that would be the same for the private cloud or the",
    "start": "2404400",
    "end": "2410160"
  },
  {
    "text": "public cloud has that ever been considered not really is it no not really yeah cool uh because like there",
    "start": "2410160",
    "end": "2416880"
  },
  {
    "text": "are various groups various like you know it's little bit complex uh yeah but uh",
    "start": "2416880",
    "end": "2423040"
  },
  {
    "text": "because like yeah the training workflows are different there are different requirements and stuff like that that's",
    "start": "2423040",
    "end": "2428880"
  },
  {
    "text": "perfectly fine thank you so much pleasure thank you so thank you everyone for joining",
    "start": "2428880",
    "end": "2436720"
  }
]