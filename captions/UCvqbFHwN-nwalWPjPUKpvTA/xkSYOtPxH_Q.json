[
  {
    "text": "so hello my name is Jack Foy I work for hiya see if i can get my navigation",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "going here and",
    "start": "6330",
    "end": "9139"
  },
  {
    "text": "I'm not an Operations person i'm a developer I came into this because I",
    "start": "13280",
    "end": "19619"
  },
  {
    "text": "wanted to be able to ship code and I got interested in infrastructure for that",
    "start": "19619",
    "end": "25230"
  },
  {
    "text": "reason so I've been interested in working on these kinds of problems off and on for about a decade now and",
    "start": "25230",
    "end": "34100"
  },
  {
    "text": "so I don't have some of the domain expertise that some people are pulling",
    "start": "34100",
    "end": "39719"
  },
  {
    "text": "in here what I've learned I've often learned either from a software design perspective or I've learned on the job",
    "start": "39719",
    "end": "44940"
  },
  {
    "text": "using resources we needed to go to production",
    "start": "44940",
    "end": "51320"
  },
  {
    "text": "so we got interested in Coober Nettie's about two years ago and in",
    "start": "51530",
    "end": "59629"
  },
  {
    "text": "June of 2015 we actually went live and we've been continuously live since then",
    "start": "59690",
    "end": "67580"
  },
  {
    "text": "that's not to work and by that November there was a survey",
    "start": "70600",
    "end": "76899"
  },
  {
    "text": "that was done at the first cube con in san francisco and much to our surprise",
    "start": "76899",
    "end": "82240"
  },
  {
    "text": "we found out we were the largest production site so what I'd like to talk about today is",
    "start": "82240",
    "end": "90240"
  },
  {
    "text": "why we two undertook this project and this might end up being more of an",
    "start": "90240",
    "end": "96880"
  },
  {
    "text": "organizational talk than a technical one there is going to be some technical content but it might not be as meaty as",
    "start": "96880",
    "end": "102130"
  },
  {
    "text": "and so if you would like so be forewarned i'm not going to be offended if you if you're here for a technical",
    "start": "102130",
    "end": "107709"
  },
  {
    "text": "talk and and you and you decide this isn't for you and i'd like to in particular cover some",
    "start": "107709",
    "end": "115659"
  },
  {
    "text": "of the lessons that that we learned as we went through this",
    "start": "115659",
    "end": "121229"
  },
  {
    "text": "so this started with a crying in turn we have a very active internship program",
    "start": "121590",
    "end": "129899"
  },
  {
    "text": "at the time at whitepages now at white pages and hiya and",
    "start": "129899",
    "end": "137340"
  },
  {
    "text": "we pride ourselves on the people that we recruit for this and",
    "start": "137459",
    "end": "143220"
  },
  {
    "text": "it was really striking when somebody got so",
    "start": "143220",
    "end": "148420"
  },
  {
    "text": "frustrated that they were trying to get their system into production and",
    "start": "148420",
    "end": "156239"
  },
  {
    "text": "over and over hitting the various gotchas that they found through the process and so",
    "start": "156419",
    "end": "165239"
  },
  {
    "text": "how did we get there we had made a conscious decision to move towards a",
    "start": "166109",
    "end": "172690"
  },
  {
    "text": "DevOps model a couple of years earlier and we defined that as developers having",
    "start": "172690",
    "end": "179680"
  },
  {
    "text": "responsibility for their system all the way to production and debs loved this they really really liked",
    "start": "179680",
    "end": "187870"
  },
  {
    "text": "the ability to decide when to ship their own software the fact that they could do",
    "start": "187870",
    "end": "193359"
  },
  {
    "text": "it more or less on a team independent basis the fact that we didn't have to do",
    "start": "193359",
    "end": "198389"
  },
  {
    "text": "enormous heavyweight integrated test cycles across the entire stack before",
    "start": "198389",
    "end": "204070"
  },
  {
    "text": "they could ship something to production and they even embrace being on call as a",
    "start": "204070",
    "end": "211629"
  },
  {
    "text": "consequence of that but they really hated the state that our",
    "start": "211629",
    "end": "217180"
  },
  {
    "text": "tools had arrived at in order to facilitate being able to do that",
    "start": "217180",
    "end": "222989"
  },
  {
    "text": "so we first undertook when we this is again kicked off by by an intern having",
    "start": "222989",
    "end": "231040"
  },
  {
    "text": "an unhappy experience at in autumn 2014 and so we started doing a bigger survey",
    "start": "231040",
    "end": "237549"
  },
  {
    "text": "just to capture what we knew or what we could learn out of the organization and",
    "start": "237549",
    "end": "243730"
  },
  {
    "text": "the first was that we had gone from a mono repo with a monolithic release",
    "start": "243730",
    "end": "249220"
  },
  {
    "text": "process to extensive use of chef across the",
    "start": "249220",
    "end": "254379"
  },
  {
    "text": "organization but that had two consequences first in some places it was",
    "start": "254379",
    "end": "259780"
  },
  {
    "text": "just wrapping the legacy monolithic system within Chef resources so it",
    "start": "259780",
    "end": "265810"
  },
  {
    "text": "hadn't really become easier to ship it had just become more isolated to ship",
    "start": "265810",
    "end": "270900"
  },
  {
    "text": "and second we had we had been using chef",
    "start": "270900",
    "end": "276190"
  },
  {
    "text": "as a production site there for some years prior and we had some",
    "start": "276190",
    "end": "283229"
  },
  {
    "text": "cookbook customizations that made changing the overall process hard",
    "start": "283229",
    "end": "289870"
  },
  {
    "text": "to do and in particular I don't have my speaker notes here",
    "start": "289870",
    "end": "296280"
  },
  {
    "text": "there we go and in particular we couldn't adopt any of the more the more",
    "start": "301030",
    "end": "306700"
  },
  {
    "text": "modern cookbook software development methodologies that would allow people to define in environment and hierarchically",
    "start": "306700",
    "end": "313480"
  },
  {
    "text": "composed their their release plans out of out of reusable components if you're",
    "start": "313480",
    "end": "321190"
  },
  {
    "text": "familiar with the Ruby ecosystem chef which is implemented in Ruby move",
    "start": "321190",
    "end": "327280"
  },
  {
    "text": "towards a very gem like model where you can have reusable libraries that you could then use to ship components across",
    "start": "327280",
    "end": "333580"
  },
  {
    "text": "multiple multiple systems and the problem was that we couldn't really",
    "start": "333580",
    "end": "340810"
  },
  {
    "text": "use those we were still in kind of a mono repo with respect to our chef cookbooks because we were tied down by",
    "start": "340810",
    "end": "347200"
  },
  {
    "text": "these customizations and we weren't able to engineer our way out of them",
    "start": "347200",
    "end": "352530"
  },
  {
    "text": "we also had the problem that we couldn't kill off services we would get to the",
    "start": "352710",
    "end": "358570"
  },
  {
    "text": "point where we couldn't ship things easily using modern technologies because we had to go back and do the tech uplift",
    "start": "358570",
    "end": "365200"
  },
  {
    "text": "on them but we couldn't justify the tech couplet because the systems were no longer receiving engineering love but",
    "start": "365200",
    "end": "372820"
  },
  {
    "text": "the business couldn't afford to shut them off this is a common thing I see people nodding in the audience here that",
    "start": "372820",
    "end": "378400"
  },
  {
    "text": "this is something that other organizations have encountered too and so the cost of that is the explosion",
    "start": "378400",
    "end": "385420"
  },
  {
    "text": "of complexity it becomes very hard to do things in the simple way that you're trying to get everyone onto if you can't",
    "start": "385420",
    "end": "390970"
  },
  {
    "text": "pull the tail forward and I can't claim that we've completely solved this problem but at least we've",
    "start": "390970",
    "end": "397420"
  },
  {
    "text": "been able to call attention to it and I'll get to how we've addressed some of it a little bit later in the talk",
    "start": "397420",
    "end": "405090"
  },
  {
    "text": "one of the consequences of doing a deploy time",
    "start": "407130",
    "end": "414210"
  },
  {
    "text": "evaluation and configuration is that multiple environments I don't integrate well you can't test all",
    "start": "414210",
    "end": "423130"
  },
  {
    "text": "of your variability ahead of time and under and what the consequences of introducing a change are going to be and",
    "start": "423130",
    "end": "429740"
  },
  {
    "text": "so we did occasionally have cases where where an unintentional change was made",
    "start": "429740",
    "end": "434960"
  },
  {
    "text": "but more commonly much more commonly we had cases where a change was intentionally made without full",
    "start": "434960",
    "end": "441320"
  },
  {
    "text": "understanding of what the consequence of that would be and that meant that we had to be more conservative about releasing",
    "start": "441320",
    "end": "446480"
  },
  {
    "text": "changes because we didn't know what the effects were going to be and on top of that there were points of articulation",
    "start": "446480",
    "end": "452270"
  },
  {
    "text": "within the system that couldn't be that couldn't be cached or at least not",
    "start": "452270",
    "end": "458750"
  },
  {
    "text": "readily cached and so if one of the upstream services that we were pulling artifacts from went down we could stall",
    "start": "458750",
    "end": "464630"
  },
  {
    "text": "the entire release pipeline and this was considered to be normal it's just the way releases worked",
    "start": "464630",
    "end": "471610"
  },
  {
    "text": "and so I I talked about being able to make changes with confidence",
    "start": "473140",
    "end": "479890"
  },
  {
    "text": "some of that boils down to ownership if you don't have a single ownership owner",
    "start": "479890",
    "end": "485780"
  },
  {
    "text": "for a given component than nobody else if everybody owns it then nobody really owns it and nobody is willing to put in",
    "start": "485780",
    "end": "492560"
  },
  {
    "text": "the effort to comprehend what it given change is going to mean and we had",
    "start": "492560",
    "end": "498320"
  },
  {
    "text": "disagreement about how we should even be apportioning out these responsibilities and where a deaf student",
    "start": "498320",
    "end": "506470"
  },
  {
    "text": "really have a platform we had BMS that represented our application",
    "start": "507310",
    "end": "513760"
  },
  {
    "text": "components but those were considered to be owned by ops at least as far as the",
    "start": "513760",
    "end": "519890"
  },
  {
    "text": "operating system and the security and the network and all of those components on them",
    "start": "519890",
    "end": "525460"
  },
  {
    "text": "but the application was not something that that the operations team understood",
    "start": "525460",
    "end": "532130"
  },
  {
    "text": "it was something where they would say Deb's need to provide us with a set of tests so that we can prove that that the",
    "start": "532130",
    "end": "538400"
  },
  {
    "text": "app is up and running and so we had Deb's who wanted to be",
    "start": "538400",
    "end": "543470"
  },
  {
    "text": "able to ship without having to involve off so we had ops who wanted to be able to introduce system changes that they",
    "start": "543470",
    "end": "548780"
  },
  {
    "text": "asserted were benign or were required without having to involve dev and",
    "start": "548780",
    "end": "557870"
  },
  {
    "text": "so we had we had an impedance mismatch we didn't have a well-defined API boundary that would let each of these",
    "start": "557870",
    "end": "564320"
  },
  {
    "text": "people have a clear area of responsibility and so again it became hard to",
    "start": "564320",
    "end": "569510"
  },
  {
    "text": "characterize changes before we introduce them",
    "start": "569510",
    "end": "573339"
  },
  {
    "text": "and so another consequence of this was that we had shared components that were shared by historical accident in",
    "start": "578490",
    "end": "585959"
  },
  {
    "text": "particularly the example is if you have a load balancer and you have a single instance of your of your separate load",
    "start": "585959",
    "end": "591089"
  },
  {
    "text": "balancer but that's fronting multiple services now each development team that",
    "start": "591089",
    "end": "596970"
  },
  {
    "text": "has a service that is registered on that load balancer has some place where they might need to make changes as part of a",
    "start": "596970",
    "end": "602520"
  },
  {
    "text": "normal release because they're introducing a change to the load balancing logic or any of these other",
    "start": "602520",
    "end": "607580"
  },
  {
    "text": "reasons but it's something that has to be coordinated across the entire",
    "start": "607580",
    "end": "612779"
  },
  {
    "text": "organization and intersects pretty closely with how operations wants to be running that area",
    "start": "612779",
    "end": "619110"
  },
  {
    "text": "of the network so this just becomes it becomes hard to characterize what an application is",
    "start": "619110",
    "end": "626209"
  },
  {
    "text": "so on top of all of this we had actually done an earlier experiment with docker we had identified this as a promising",
    "start": "626209",
    "end": "633180"
  },
  {
    "text": "technology we had said containers look like they're really interesting and we were able to deliver a service to",
    "start": "633180",
    "end": "639510"
  },
  {
    "text": "production using it but that served enough to be able to tell us that we",
    "start": "639510",
    "end": "645720"
  },
  {
    "text": "didn't have all of the infrastructure there in just being able to ship an application in a container that we would",
    "start": "645720",
    "end": "652649"
  },
  {
    "text": "actually need in order to solve these broader system integration problems and so we looked at what was available at",
    "start": "652649",
    "end": "660839"
  },
  {
    "text": "the time decided there isn't really anything here and started looking at what it would",
    "start": "660839",
    "end": "667860"
  },
  {
    "text": "take to be able to roll that up and we never really got a got good momentum on on being able to",
    "start": "667860",
    "end": "675000"
  },
  {
    "text": "carry that forward beyond that one spike because again it revealed the costs how much it was going to take for us to",
    "start": "675000",
    "end": "680910"
  },
  {
    "text": "actually be able to do this transition so I actually already written this slide",
    "start": "680910",
    "end": "688140"
  },
  {
    "text": "when Kelsey tweeted this about a week ago but he's outlining exactly the same the",
    "start": "688140",
    "end": "696870"
  },
  {
    "text": "same problem that we had identified it in early 2014 around that spike",
    "start": "696870",
    "end": "704240"
  },
  {
    "text": "so we were commissioned at the very end of 2014 with solving",
    "start": "704240",
    "end": "710100"
  },
  {
    "text": "this problem by the time insurance came around again",
    "start": "710100",
    "end": "713810"
  },
  {
    "text": "so I'm going to talk a little bit about how we solve this as an organizational problem we decided that in order to make this",
    "start": "715519",
    "end": "722880"
  },
  {
    "text": "happen we needed to enable dev teams to run and admin their own applications",
    "start": "722880",
    "end": "728459"
  },
  {
    "text": "this is something that's come sometimes called app ops or app admin these days",
    "start": "728459",
    "end": "734390"
  },
  {
    "text": "again I'm seeing people nodding and laughing in the audience so I guess people people have dealt with this model and",
    "start": "734390",
    "end": "740450"
  },
  {
    "text": "in order to provide that we needed a better picture of the infrastructure",
    "start": "740450",
    "end": "747209"
  },
  {
    "text": "than we currently had we at the time we're running on on fairly manually",
    "start": "747209",
    "end": "752910"
  },
  {
    "text": "maintained VMware vSphere configuration and we were looking at",
    "start": "752910",
    "end": "759360"
  },
  {
    "text": "doing a move over to to some kind of cloud system and so we tasked the open",
    "start": "759360",
    "end": "766050"
  },
  {
    "text": "we tasked our ops team with becoming an infrastructure development team and providing that kind of an interface for",
    "start": "766050",
    "end": "773220"
  },
  {
    "text": "us to be able to build on but to join the two we needed some kind of a platform and we said you know it's it's",
    "start": "773220",
    "end": "781560"
  },
  {
    "text": "great to provide the level of what an ec2 or an OpenStack does but that's the",
    "start": "781560",
    "end": "787019"
  },
  {
    "text": "wrong set of abstractions to enable app ops we need some kind of a platform that",
    "start": "787019",
    "end": "793589"
  },
  {
    "text": "that provides the API is that join those two",
    "start": "793589",
    "end": "798230"
  },
  {
    "text": "so we looked at what the problem is going to allow us to solve and we decided that since most of our new",
    "start": "799070",
    "end": "805260"
  },
  {
    "text": "back-end services were stateless and Scala based most of our new front-end services were at stateless in flask and",
    "start": "805260",
    "end": "812370"
  },
  {
    "text": "Python and we had existing front-end rails apps and legacy back-end Ruby processes all of which were essentially",
    "start": "812370",
    "end": "818699"
  },
  {
    "text": "stateless in front of the database or in front of a database we decided that this was a ripe target",
    "start": "818699",
    "end": "826020"
  },
  {
    "text": "and in particular we would target Scala applications because that was where most of the new back-end development that was",
    "start": "826020",
    "end": "832050"
  },
  {
    "text": "driving this was happening how many of you have heard this motto",
    "start": "832050",
    "end": "838050"
  },
  {
    "text": "before test what we ship and ship what we test ok",
    "start": "838050",
    "end": "843680"
  },
  {
    "text": "we decided again that containerisation looked like the right model for us to be able to encapsulate behaviors that we",
    "start": "843680",
    "end": "851250"
  },
  {
    "text": "could develop and test and then bite for bite move into production and we then",
    "start": "851250",
    "end": "859320"
  },
  {
    "text": "said great that means that we need to be able to define our environments in terms of how we manage these immutable objects",
    "start": "859320",
    "end": "866000"
  },
  {
    "text": "there's a general principle on this which is that you want to push variability to the left meaning that you",
    "start": "866000",
    "end": "872640"
  },
  {
    "text": "want to be able to to encapsulate your variances in your components as",
    "start": "872640",
    "end": "878610"
  },
  {
    "text": "early in the build process as you can and then compose those in in immutable ways as you move through",
    "start": "878610",
    "end": "885780"
  },
  {
    "text": "your development cycle we were using a lot of chef and a lot of",
    "start": "885780",
    "end": "893400"
  },
  {
    "text": "us really liked chef and a lot of us really did not like chef and we were moving away from Ruby as an organization",
    "start": "893400",
    "end": "902000"
  },
  {
    "text": "so there's a general principle that can come out of this which is that if you're",
    "start": "902960",
    "end": "909210"
  },
  {
    "text": "doing a technology change and you want to be doing a cultural change as part of this you can join the two and say as I",
    "start": "909210",
    "end": "916080"
  },
  {
    "text": "moved into as I move into a new cultural realm I do",
    "start": "916080",
    "end": "922080"
  },
  {
    "text": "that by moving on to a new platform",
    "start": "922080",
    "end": "925760"
  },
  {
    "text": "so that left is the question of how do we in fact configure our hosts and our clusters for use and we evaluated for OS",
    "start": "929139",
    "end": "937790"
  },
  {
    "text": "and decided that this was a pretty good model for where we wanted to go we had been in a in a boon to shop prior to",
    "start": "937790",
    "end": "944899"
  },
  {
    "text": "that we are still in a boon to shop with in many of our container development environments but as far as the actual",
    "start": "944899",
    "end": "951920"
  },
  {
    "text": "hosts that we are running we decided that our new platform would be four or less",
    "start": "951920",
    "end": "957550"
  },
  {
    "text": "we did briefly evaluate a new configuration management systems we",
    "start": "957550",
    "end": "962600"
  },
  {
    "text": "looked at ansible we looked at salt in particular because of Coober Nettie's use of salt and we didn't really adopt",
    "start": "962600",
    "end": "969139"
  },
  {
    "text": "any of them we instead moved towards something with more static configuration based on based on user data within VMs",
    "start": "969139",
    "end": "979180"
  },
  {
    "text": "and we knew that we needed more than docker in order to make this succeed we needed some kind of environment",
    "start": "981520",
    "end": "987810"
  },
  {
    "text": "and there wasn't really an obvious community consensus around this at the end of 2014",
    "start": "987810",
    "end": "996480"
  },
  {
    "text": "Cooper Nettie's had been announced as a project and we were trying to get our heads around the abstractions that they",
    "start": "996480",
    "end": "1001920"
  },
  {
    "text": "were describing at the same time there was mesas and mesosphere and we really liked some of the things that they were",
    "start": "1001920",
    "end": "1007650"
  },
  {
    "text": "doing as an organization we were really impressed by what giant was doing with",
    "start": "1007650",
    "end": "1013290"
  },
  {
    "text": "with their applications and in particular with the the linings personality that they were able to",
    "start": "1013290",
    "end": "1020220"
  },
  {
    "text": "expose under under their bsd jail mode i'm sorry",
    "start": "1020220",
    "end": "1025920"
  },
  {
    "text": "they're under there solaris jl mode and fleet was available in because we were",
    "start": "1025920",
    "end": "1032699"
  },
  {
    "text": "looking strongly at core OS that was one of the options that we could use but fleet doesn't really provide the",
    "start": "1032700",
    "end": "1037770"
  },
  {
    "text": "abstractions that we wanted to be able to manage an application it it's more targeted at managing infrastructure",
    "start": "1037770",
    "end": "1043470"
  },
  {
    "text": "across the fleet and we had a strong contingent within the team that that thought that we could solve this problem",
    "start": "1043470",
    "end": "1049770"
  },
  {
    "text": "better than anyone else had and we should just bite off that that project and we ended with a strongly divided",
    "start": "1049770",
    "end": "1057810"
  },
  {
    "text": "team over this and we ended up having to pull into CTO in order to give us a command decision on it and he just said",
    "start": "1057810",
    "end": "1065040"
  },
  {
    "text": "management wise this is what we're going to do we're going to use community standards pick one but use that",
    "start": "1065040",
    "end": "1071060"
  },
  {
    "text": "don't roll our own and this is a this is a hard line to walk as as development",
    "start": "1071060",
    "end": "1078450"
  },
  {
    "text": "organizations you know we're employed because we have to solve business problems and our business problems",
    "start": "1078450",
    "end": "1084180"
  },
  {
    "text": "really are all different because all of our organizations are different but",
    "start": "1084180",
    "end": "1089510"
  },
  {
    "text": "we use open source because they're not all that different so you have this line",
    "start": "1089510",
    "end": "1095100"
  },
  {
    "text": "you have to walk between being able to use standardized components and be able to track being able to track an upstream as",
    "start": "1095100",
    "end": "1102900"
  },
  {
    "text": "changes come out versus having something that more",
    "start": "1102900",
    "end": "1108690"
  },
  {
    "text": "tightly fits your use of an application I don't always know where the line is on",
    "start": "1108690",
    "end": "1114520"
  },
  {
    "text": "this it i think it's an engineer it's part of the art of engineering",
    "start": "1114520",
    "end": "1119610"
  },
  {
    "text": "so then we face some implementation decisions we had our own data center and that was where we were deploying",
    "start": "1122100",
    "end": "1127870"
  },
  {
    "text": "OpenStack it's much more cost effective to run things in your own data center if you",
    "start": "1127870",
    "end": "1133360"
  },
  {
    "text": "already have one you've already sunk the capital cost you've already sunk the operational costs of running that infrastructure and so your cost per",
    "start": "1133360",
    "end": "1140470"
  },
  {
    "text": "cycle and cost per megabyte a grantor gigabyte of RAM is much much lower than using somebody elses platform on the",
    "start": "1140470",
    "end": "1147910"
  },
  {
    "text": "other hand it's much much more convenient to use somebody else's platform especially since they can invest in better user tooling than",
    "start": "1147910",
    "end": "1156220"
  },
  {
    "text": "typically you can I don't know if any of you have used both OpenStack and ec2 but",
    "start": "1156220",
    "end": "1161290"
  },
  {
    "text": "ec2 I can't believe I'm about to say this is a much more polished experience and that's really saying something about",
    "start": "1161290",
    "end": "1167290"
  },
  {
    "text": "you see too so we knew that we needed to have a",
    "start": "1167290",
    "end": "1174750"
  },
  {
    "text": "something that would look common between the two but they're really very different they",
    "start": "1174750",
    "end": "1181870"
  },
  {
    "text": "have similar abstractions but they're not the same and they behave very differently and",
    "start": "1181870",
    "end": "1187830"
  },
  {
    "text": "the the problem was that as our role in the platform team it was our job to then",
    "start": "1187830",
    "end": "1194140"
  },
  {
    "text": "provide that common platform that our developers could use and just the only",
    "start": "1194140",
    "end": "1199420"
  },
  {
    "text": "thing they would need to know is basically the different latency or cost characteristics of where they were going to be running they wouldn't have to know",
    "start": "1199420",
    "end": "1205120"
  },
  {
    "text": "the differences about how they were going to be running applications",
    "start": "1205120",
    "end": "1209820"
  },
  {
    "text": "so we were already on core OS we reached out to core OS they introduced us to",
    "start": "1214580",
    "end": "1220049"
  },
  {
    "text": "tecktonik we really liked it but we needed it to do more than AWS and",
    "start": "1220049",
    "end": "1226830"
  },
  {
    "text": "on top of that we were getting into January February at this point and their release cycle was not going to be",
    "start": "1226830",
    "end": "1233130"
  },
  {
    "text": "complete early enough for us to be able to meet our deadline of not having interns use the old system by the time",
    "start": "1233130",
    "end": "1239460"
  },
  {
    "text": "they came around that following summer it was four months off at that point and and we expected they weren't they told",
    "start": "1239460",
    "end": "1246059"
  },
  {
    "text": "us that that they couldn't commit to being able to make it was going to be end of summer at least before we would have something that was production ready",
    "start": "1246059",
    "end": "1253669"
  },
  {
    "text": "we took a look at fabricate I don't know if you've seen fabricate demos that it's an amazing piece of technology where you",
    "start": "1253940",
    "end": "1261419"
  },
  {
    "text": "can deploy an application it's very very close to open shift and you can drop in",
    "start": "1261419",
    "end": "1267149"
  },
  {
    "text": "these pre-built applications and they distribute automatically into enter your cluster but it was very tightly coupled and we",
    "start": "1267149",
    "end": "1276059"
  },
  {
    "text": "started picking it apart and finding places where the model again it wasn't going to work for what we were trying to do and it wasn't clear that we could",
    "start": "1276059",
    "end": "1283440"
  },
  {
    "text": "really easily flex it to do what we did need it to do so we took a step back away from that more integrated",
    "start": "1283440",
    "end": "1291169"
  },
  {
    "text": "solution we also took a close look at gke this came online pretty late in our",
    "start": "1291169",
    "end": "1296610"
  },
  {
    "text": "process and we really really liked a lot of the capabilities and we really really were",
    "start": "1296610",
    "end": "1302909"
  },
  {
    "text": "attracted to the idea of not having to run our own cluster but it would mean introducing a new cloud and we already",
    "start": "1302909",
    "end": "1308370"
  },
  {
    "text": "had other investments in AWS and other investments in the data center and so we said we can't we can't really capitalize",
    "start": "1308370",
    "end": "1314730"
  },
  {
    "text": "on that right now so then we started looking at doing our",
    "start": "1314730",
    "end": "1320279"
  },
  {
    "text": "own self management of these clusters and naturally we looked at using cube up because that's where all of the all the",
    "start": "1320279",
    "end": "1327960"
  },
  {
    "text": "documentation points you and you want to be able to start your own cluster and bring it up but even if you do that",
    "start": "1327960",
    "end": "1334110"
  },
  {
    "text": "first the result still isn't isn't production ready and second it's very opinionated about how you're going to be",
    "start": "1334110",
    "end": "1341100"
  },
  {
    "text": "structuring your AWS environment or your OpenStack environment and in addition the OpenStack integration was was",
    "start": "1341100",
    "end": "1346740"
  },
  {
    "text": "basically non-existent so we'd have to go in and invent a whole bunch of it anyway if we were going to make that",
    "start": "1346740",
    "end": "1351770"
  },
  {
    "text": "happen and on top of that it was very cintas centric and we were not a simple shop at",
    "start": "1351770",
    "end": "1357800"
  },
  {
    "text": "all you know we had done some of we were heavily abun to we had started moving into core OS and we just looked at that",
    "start": "1357800",
    "end": "1363350"
  },
  {
    "text": "and said that we don't really want to pull in another platform we'd have to support they did have some core OS support but",
    "start": "1363350",
    "end": "1370130"
  },
  {
    "text": "it was very Rackspace specific so we we took all of that is as input",
    "start": "1370130",
    "end": "1376700"
  },
  {
    "text": "and and produced our own set of fleet configurations and because you're running a bunch of fleets we called it our model and",
    "start": "1376700",
    "end": "1383290"
  },
  {
    "text": "it took the core OS images and the latest group",
    "start": "1383290",
    "end": "1389420"
  },
  {
    "text": "earnings available at the time which is 0-17 and deployed it effectively on to OpenStack",
    "start": "1389420",
    "end": "1396320"
  },
  {
    "text": "first and then AWS and this worked pretty well for us it allowed us to do exactly what we needed the downside is",
    "start": "1396320",
    "end": "1403070"
  },
  {
    "text": "you're maintaining yourself and you don't get to capitalize on the new changes that are coming down from core OS and as it turns out the core OS",
    "start": "1403070",
    "end": "1409490"
  },
  {
    "text": "development cycle doesn't always distinguish which parts of new functionality are getting",
    "start": "1409490",
    "end": "1415490"
  },
  {
    "text": "initialized via the core API is at least at the time this is true and via the",
    "start": "1415490",
    "end": "1420650"
  },
  {
    "text": "cluster scripts and so as as new functionality gets introduced as kind of hacks into the into the cluster scripts",
    "start": "1420650",
    "end": "1426350"
  },
  {
    "text": "you have to then reverse engineer where those are coming in and make sure that your cluster maintenance stuff is",
    "start": "1426350",
    "end": "1432470"
  },
  {
    "text": "keeping up with it it it's a bigger engineering challenge than it appears to be at the beginning and again we get",
    "start": "1432470",
    "end": "1438680"
  },
  {
    "text": "into this question of customization and how much are you willing to carry forward and how much are you willing to",
    "start": "1438680",
    "end": "1444850"
  },
  {
    "text": "to just drop in as upstream comes in and",
    "start": "1444850",
    "end": "1450220"
  },
  {
    "text": "it was entirely manual as far as size and clusters and so on we could add new capacity to clusters as we went but it",
    "start": "1450220",
    "end": "1456890"
  },
  {
    "text": "was manual we didn't tie into any auto scaling we also produce their own developer",
    "start": "1456890",
    "end": "1463370"
  },
  {
    "text": "dashboard which was called Archimedes and that worked really well and the intent was",
    "start": "1463370",
    "end": "1470180"
  },
  {
    "text": "that we were going to open source it but we never did and that was because it",
    "start": "1470180",
    "end": "1476150"
  },
  {
    "text": "was tightly coupled to our own processes it was tightly coupled to a bunch of our own internal interface libraries and CSS",
    "start": "1476150",
    "end": "1483740"
  },
  {
    "text": "objects that we couldn't open source and we delayed long enough that basically",
    "start": "1483740",
    "end": "1490430"
  },
  {
    "text": "Kuber Nettie's dashboard has largely caught up with where we would be if we open sourced it today so today we're",
    "start": "1490430",
    "end": "1496820"
  },
  {
    "text": "largely just running kuba Nettie's dashboard internally and we have a backlog of things that we want to extend",
    "start": "1496820",
    "end": "1502790"
  },
  {
    "text": "it and open source on to that so if you're going to do this today i would suggest just get behind cube dashboard",
    "start": "1502790",
    "end": "1509000"
  },
  {
    "text": "and push don't don't write your own but that didn't exist at the time there was no such thing as cube dash",
    "start": "1509000",
    "end": "1516070"
  },
  {
    "text": "containers are leaking kuber Nettie's excuse me on Linux",
    "start": "1523180",
    "end": "1529300"
  },
  {
    "text": "containers are not perfectly namespace they're not perfectly isolated there are many many shared structures within the",
    "start": "1529300",
    "end": "1535990"
  },
  {
    "text": "colonel and so they're not really secure it's not airtight you know this is one of the main arguments that giant makes",
    "start": "1535990",
    "end": "1542200"
  },
  {
    "text": "about why you want to use an open solaris zone approach rather than using containers that are layered into the",
    "start": "1542200",
    "end": "1548200"
  },
  {
    "text": "kernel after the fact and so we decided to isolate are",
    "start": "1548200",
    "end": "1553620"
  },
  {
    "text": "directly internet facing traffic from things that were not internet-facing and",
    "start": "1553620",
    "end": "1560530"
  },
  {
    "text": "and be able to at least sandbox things in that way we eventually abandoned this it it's",
    "start": "1560530",
    "end": "1567850"
  },
  {
    "text": "probably still a good idea architectural II but the operational pain associated",
    "start": "1567850",
    "end": "1573160"
  },
  {
    "text": "with maintaining these things and being able to do your work distribution",
    "start": "1573160",
    "end": "1578890"
  },
  {
    "text": "across two of them and again we had nothing like Federation made this made this eventually a",
    "start": "1578890",
    "end": "1584590"
  },
  {
    "text": "non-starter you could still do something like this today if you were to do things like node",
    "start": "1584590",
    "end": "1590470"
  },
  {
    "text": "selectors if you are going to constrain your external versus internal traffic within a single cluster to two nodes",
    "start": "1590470",
    "end": "1598830"
  },
  {
    "text": "where I think we're almost there I think you would need a node exclusion policy in order to keep internal traffic off of",
    "start": "1598830",
    "end": "1605590"
  },
  {
    "text": "external nodes if you're going to do that so you could get most of this effect today",
    "start": "1605590",
    "end": "1612060"
  },
  {
    "text": "for networking the infrastructure group provided us with a tenant network we were the first",
    "start": "1614260",
    "end": "1620530"
  },
  {
    "text": "group to be using tenant networks on this open OpenStack cluster or within",
    "start": "1620530",
    "end": "1626500"
  },
  {
    "text": "this this group's maintenance and so we got to discover lots of the ways in which tenant networks were not fully",
    "start": "1626500",
    "end": "1632710"
  },
  {
    "text": "baked those are implemented as a VX LAN overlay on top of a physical network as",
    "start": "1632710",
    "end": "1639490"
  },
  {
    "text": "it happened we were also using a VX land overlay within the cluster for a pod",
    "start": "1639490",
    "end": "1645550"
  },
  {
    "text": "network so we end up with VX land over VX land latency gets very high it's a very janky kind of setup",
    "start": "1645550",
    "end": "1653520"
  },
  {
    "text": "but again for v1 it worked the major problems i'll get i'll get to",
    "start": "1654030",
    "end": "1660790"
  },
  {
    "text": "in a minute about where we had to diagnose issues on that we were using cube proxy v1 sending all traffic",
    "start": "1660790",
    "end": "1667450"
  },
  {
    "text": "through user space that was really the major origin of our latency issues but this VX land on VX land",
    "start": "1667450",
    "end": "1676890"
  },
  {
    "text": "emergent property ended up being making errors very hard to diagnose",
    "start": "1676890",
    "end": "1683130"
  },
  {
    "text": "container logging we had had an existing agent that we were using for log gathering from our existing applications",
    "start": "1684630",
    "end": "1691150"
  },
  {
    "text": "we had a plume agent that we ran throughout and that would send log streams to a kafka a bus that we would",
    "start": "1691150",
    "end": "1698140"
  },
  {
    "text": "then forward on to things like log indexers or archival services or that sort of thing and that worked again",
    "start": "1698140",
    "end": "1704440"
  },
  {
    "text": "pretty well we had a UDP connection from the application to the flume agent and",
    "start": "1704440",
    "end": "1709630"
  },
  {
    "text": "so it was inherently non-blocking but that also meant that we were dealing with inherent size limitations like 64 K",
    "start": "1709630",
    "end": "1715770"
  },
  {
    "text": "max size on on messages and that gets even smaller depending on what your",
    "start": "1715770",
    "end": "1721180"
  },
  {
    "text": "network stack is doing for for MTU locally within your container and so",
    "start": "1721180",
    "end": "1726520"
  },
  {
    "text": "that ended up having some nasty nasty limitations that that emerged over time but again it you know for the for the",
    "start": "1726520",
    "end": "1734110"
  },
  {
    "text": "core mission it was working pretty well we built containers via an svt plugin",
    "start": "1734110",
    "end": "1742020"
  },
  {
    "text": "SBT is the scala a common build tool and",
    "start": "1742020",
    "end": "1747559"
  },
  {
    "text": "we put we i don't think the docker file plugins were",
    "start": "1747559",
    "end": "1754009"
  },
  {
    "text": "available at the time so i believe we just had something that was templating a very simple dockerfile innovating it and then building that through jenkins and",
    "start": "1754009",
    "end": "1761179"
  },
  {
    "text": "publishing to our existing artifactory repo which we then licensed in order to enable the the docker image repo",
    "start": "1761179",
    "end": "1769720"
  },
  {
    "text": "so this is how we went live we're at kuru Nettie's 0-17 we are on core OS we",
    "start": "1769720",
    "end": "1776929"
  },
  {
    "text": "used fleet to deploy that we're on OpenStack ice house which itself was a pretty old release by that point and we",
    "start": "1776929",
    "end": "1784700"
  },
  {
    "text": "were on our own Hardware our first customers were async off",
    "start": "1784700",
    "end": "1791240"
  },
  {
    "text": "co-workers which were supporting some of our mobile apps followed by some of the internet facing",
    "start": "1791240",
    "end": "1797509"
  },
  {
    "text": "helper services we had things like named auto-completion that are firing law facing HTTP requests and",
    "start": "1797509",
    "end": "1804769"
  },
  {
    "text": "so we were fielding those out of poober Nettie's we hadn't yet moved the main app over the main rails app over to",
    "start": "1804769",
    "end": "1810080"
  },
  {
    "text": "Coober Nettie's but this was a great way to start getting some operational experience with running things in production and we had a new flagship",
    "start": "1810080",
    "end": "1819649"
  },
  {
    "text": "application that we were preparing to take massive traffic on that was also",
    "start": "1819649",
    "end": "1824809"
  },
  {
    "text": "one of the major major first users on this and",
    "start": "1824809",
    "end": "1831669"
  },
  {
    "text": "by and large with the exception of the networking things that I'm going to get into in a",
    "start": "1831669",
    "end": "1836809"
  },
  {
    "text": "minute our users were very happy so this is an example of a quote that somebody",
    "start": "1836809",
    "end": "1842529"
  },
  {
    "text": "came and excitedly told me about a week or two after we've gone live that you",
    "start": "1842529",
    "end": "1847759"
  },
  {
    "text": "know he had he had hit go on a release into a and it had to go back and you know scrambled it to fix a bug that",
    "start": "1847759",
    "end": "1855230"
  },
  {
    "text": "he just realized and sent it and he was able to finish the second deployment cycle",
    "start": "1855230",
    "end": "1860618"
  },
  {
    "text": "fixing the bug in production before the alert for the for introducing the bug",
    "start": "1860830",
    "end": "1866509"
  },
  {
    "text": "had hit his phone and so that was considered just an enormous success",
    "start": "1866509",
    "end": "1871639"
  },
  {
    "text": "compared to the close to an hour release cycle that users previously had to go",
    "start": "1871639",
    "end": "1876740"
  },
  {
    "text": "through in order to converge application state and",
    "start": "1876740",
    "end": "1881909"
  },
  {
    "text": "the internet came back",
    "start": "1881909",
    "end": "1885509"
  },
  {
    "text": "so as some follow ons and I'm getting short on time here so I'm going to breeze through a bunch of this we",
    "start": "1887909",
    "end": "1893559"
  },
  {
    "text": "upgraded to cube 11 pretty quickly we built a custom couplet in order to",
    "start": "1893559",
    "end": "1899320"
  },
  {
    "text": "improve the names facing behavior some of our monitoring stuff really wanted to be able to use the hostname in a",
    "start": "1899320",
    "end": "1906309"
  },
  {
    "text": "distinguishable way and that meant we had to bring pod information through into the into the container in such a",
    "start": "1906309",
    "end": "1912700"
  },
  {
    "text": "way that a hostname would would answer with it and this eventually became the default container behavior in 12 but we",
    "start": "1912700",
    "end": "1919539"
  },
  {
    "text": "were running a custom couplet for a while in order to get this behavior we expanded to include auto scaling we",
    "start": "1919539",
    "end": "1927279"
  },
  {
    "text": "never did end up using heat auto scaling we've been OpenStack adjusted the heat implementation within our version just",
    "start": "1927279",
    "end": "1932950"
  },
  {
    "text": "never really got there but we expanded on to AWS and used auto scaling there we",
    "start": "1932950",
    "end": "1939489"
  },
  {
    "text": "eventually did salt TLS but I'll get to that a little bit later I've talked about networking we had very",
    "start": "1939489",
    "end": "1945279"
  },
  {
    "text": "high baseline latency which we later discovered is probably the result of",
    "start": "1945279",
    "end": "1950289"
  },
  {
    "text": "running cute proxy in user space when we upgraded to cube 11 and",
    "start": "1950289",
    "end": "1956850"
  },
  {
    "text": "brought in its version of cube proxy we enabled iptables saw it it worked like charm and blatant seized dropped way",
    "start": "1956850",
    "end": "1963609"
  },
  {
    "text": "down so if you are using Q proxy and you're not enabling iptables you should",
    "start": "1963609",
    "end": "1969100"
  },
  {
    "text": "really take a look it's a massive improvement but we still have this problem of opaque",
    "start": "1969100",
    "end": "1975730"
  },
  {
    "text": "network failures we're all of a sudden we would start throwing connection errors in connection resets within",
    "start": "1975730",
    "end": "1982840"
  },
  {
    "text": "interfaces on the overlay Network and typically isolated to a single node but bouncing the node sometimes but not",
    "start": "1982840",
    "end": "1989289"
  },
  {
    "text": "always fixed it and we eventually discovered that that was because that",
    "start": "1989289",
    "end": "1995529"
  },
  {
    "text": "hypervisor configuration for a contract max which is the maximum number of",
    "start": "1995529",
    "end": "2000690"
  },
  {
    "text": "active connections that you can have across the entire hypervisor was set incorrectly on a few nodes within the",
    "start": "2000690",
    "end": "2006869"
  },
  {
    "text": "cluster so the the catch on this was it wasn't our bug but it was hard for us to",
    "start": "2006869",
    "end": "2013450"
  },
  {
    "text": "demonstrate that it wasn't our bug because of the opacity of the implementation that we put on top of it and the other lesson on this is that all",
    "start": "2013450",
    "end": "2020530"
  },
  {
    "text": "abstraction is like no matter if we're running kuber Nettie's or running VMS or whichever at",
    "start": "2020530",
    "end": "2027940"
  },
  {
    "text": "some point eventually the underlying layers can get you and so you have to be able to reason past your current level",
    "start": "2027940",
    "end": "2038850"
  },
  {
    "text": "dr. D is the bane of my existence it turns out that the log options change",
    "start": "2039510",
    "end": "2047770"
  },
  {
    "text": "a bit across some of the major versions and the doctor demon didn't fail to start if it didn't recognize them it",
    "start": "2047770",
    "end": "2053679"
  },
  {
    "text": "would just ignore the option so we had end up with cases where log rotation would break and we'd end up filling up disks and we had no sign of until the",
    "start": "2053679",
    "end": "2060220"
  },
  {
    "text": "disks filled up again external load balancers we didn't have",
    "start": "2060220",
    "end": "2066850"
  },
  {
    "text": "the OpenStack H a proxy integration in place and so we used our",
    "start": "2066850",
    "end": "2073230"
  },
  {
    "text": "traditional external load balancers which had worked pretty well but we discovered that there's a bug in",
    "start": "2073230",
    "end": "2080260"
  },
  {
    "text": "the DNS pool discovery on that that we were using in order to advertise new",
    "start": "2080260",
    "end": "2085300"
  },
  {
    "text": "nodes to this pool and suddenly the pool would empty and we'd lose production traffic not where you want to be",
    "start": "2085300",
    "end": "2092370"
  },
  {
    "text": "so that was all within whitepages which was which was kind of the parent company where all this",
    "start": "2093000",
    "end": "2099040"
  },
  {
    "text": "happened earlier this year we spun out as a start-up into a new organization",
    "start": "2099040",
    "end": "2104800"
  },
  {
    "text": "called hiya we we are tasked with fixing spam and scam calls if you get robo",
    "start": "2104800",
    "end": "2111460"
  },
  {
    "text": "calls we're trying to stop that and we took the crew benetti's clusters with",
    "start": "2111460",
    "end": "2118420"
  },
  {
    "text": "us when we went the entire platform team wholesale got moved over to the startup and we determined that that was a good",
    "start": "2118420",
    "end": "2126490"
  },
  {
    "text": "interface for us to draw then if we're not not going to have the infrastructure team with our organization anymore we're",
    "start": "2126490",
    "end": "2131620"
  },
  {
    "text": "going to replace them with AWS so we moved wholesale over into the cloud and",
    "start": "2131620",
    "end": "2136710"
  },
  {
    "text": "we ported our existing stuff over to cloud formation we use that to configure our stuff automatically now it works",
    "start": "2136710",
    "end": "2143530"
  },
  {
    "text": "pretty well but we still have problems we move to using the host gateway back in / plan'll which is marvelous it just",
    "start": "2143530",
    "end": "2150880"
  },
  {
    "text": "works its high-performance we've had no issues with it",
    "start": "2150880",
    "end": "2156030"
  },
  {
    "text": "with auto scaling we took the time to do a full TLS root key ceremony you know",
    "start": "2158670",
    "end": "2164230"
  },
  {
    "text": "with an offline machine and everything and a copy of the key kept off-site",
    "start": "2164230",
    "end": "2169950"
  },
  {
    "text": "and that has allowed us to take a few more risks with doing things like putting a drive key up in an automatic",
    "start": "2169950",
    "end": "2178170"
  },
  {
    "text": "sorta thority and so now we have a TLS handle automatically as we auto scale",
    "start": "2178170",
    "end": "2183660"
  },
  {
    "text": "we're using CF SSL to do that it's a great component out of cloud player and",
    "start": "2183660",
    "end": "2188980"
  },
  {
    "text": "so if you're doing auto scaling you should really take a look at that product",
    "start": "2188980",
    "end": "2195570"
  },
  {
    "text": "most devs actually once they got used to it decided they didn't need the UI and they just started using the config files",
    "start": "2195960",
    "end": "2203619"
  },
  {
    "text": "directly and mostly they haven't minded we might choose to work with some higher",
    "start": "2203619",
    "end": "2211359"
  },
  {
    "text": "level abstractions eventually but but for the most part they're saying I can just write a deployment and I'm off and",
    "start": "2211359",
    "end": "2216520"
  },
  {
    "text": "running I don't I don't need to worry about it we're using hosted at a factory now and",
    "start": "2216520",
    "end": "2224619"
  },
  {
    "text": "this is great for the people who are in North America but we also have European engineers and they really don't like this because the latency is even worse",
    "start": "2224619",
    "end": "2231100"
  },
  {
    "text": "than it was before so we're looking at moving our image hosting directly into s3",
    "start": "2231100",
    "end": "2237030"
  },
  {
    "text": "don't ever use cloud formation to add an ALB to an auto scaling group I see",
    "start": "2237030",
    "end": "2242740"
  },
  {
    "text": "people laughing here it sounds like other people have seen this to cloud formation will delete your auto scaling group if you do this you lose your",
    "start": "2242740",
    "end": "2249730"
  },
  {
    "text": "entire production cluster it did not make us happy if we move to criminate es 1 dot for",
    "start": "2249730",
    "end": "2256660"
  },
  {
    "text": "which we haven't yet we believe that load balancer services will give us the automatic handling on this that will",
    "start": "2256660",
    "end": "2262540"
  },
  {
    "text": "make us not have to do elb management directly at all anymore and so that will",
    "start": "2262540",
    "end": "2268920"
  },
  {
    "text": "have the benefit of solving this problem and move it back into the developers hands which is where it should be",
    "start": "2268920",
    "end": "2275210"
  },
  {
    "text": "container logging is still not working well I would love to musing journal tea",
    "start": "2276049",
    "end": "2282119"
  },
  {
    "text": "for this but we've done some extensive profiling and in spite of the core OS driven latency solutions that they have",
    "start": "2282119",
    "end": "2289559"
  },
  {
    "text": "put into place and they've had a couple of blog posts on that throughput is still not where it needs to be in fact",
    "start": "2289559",
    "end": "2295619"
  },
  {
    "text": "it's it's surprisingly low you know we're seeing things max out under a thousand a second and so this",
    "start": "2295619",
    "end": "2305579"
  },
  {
    "text": "wouldn't be a problem except that rocket is is proposing to use journal as its",
    "start": "2305579",
    "end": "2310799"
  },
  {
    "text": "container logging system of record and so if you have solved any any problem like",
    "start": "2310799",
    "end": "2318539"
  },
  {
    "text": "this of handling high-throughput container logging I'd love to talk to you",
    "start": "2318539",
    "end": "2324410"
  },
  {
    "text": "I really want to get dr. out of the picture this is not a universally loved loved opinion but I'd rather we stop",
    "start": "2324410",
    "end": "2331859"
  },
  {
    "text": "using docker and move directly to rocket so this is where we are today a year ago",
    "start": "2331859",
    "end": "2339660"
  },
  {
    "text": "we had 35 nodes in one site now we have about 80 nodes in nine sites around the",
    "start": "2339660",
    "end": "2345900"
  },
  {
    "text": "world and we know for sure that we are not the",
    "start": "2345900",
    "end": "2351480"
  },
  {
    "text": "largest production site anymore but I believe we're still the oldest continuously operating one",
    "start": "2351480",
    "end": "2356750"
  },
  {
    "text": "so we're pretty short on time but I'd love to take at least a couple of questions before we go do we have time",
    "start": "2356750",
    "end": "2363420"
  },
  {
    "text": "for that folks in the back yeah I'm getting nod yes are there any questions yep",
    "start": "2363420",
    "end": "2369750"
  },
  {
    "text": "what were the features in Coober Nettie's that made this a success",
    "start": "2373290",
    "end": "2378300"
  },
  {
    "text": "not having to have individual devs map their production use for their app",
    "start": "2378300",
    "end": "2386770"
  },
  {
    "text": "directly onto the infrastructure I think is you know that's the core problem in Coober Nettie's is solving and I think that really is one of the core things",
    "start": "2386770",
    "end": "2393070"
  },
  {
    "text": "that let us say you can just ship stuff into the cluster and it will find the",
    "start": "2393070",
    "end": "2398350"
  },
  {
    "text": "resources for you and and use them automatically and you can then get visibility into how your app is running",
    "start": "2398350",
    "end": "2404400"
  },
  {
    "text": "but you don't have to tell it how to do that and that's in contrast to you know where we were before where we had",
    "start": "2404400",
    "end": "2410620"
  },
  {
    "text": "essentially custom assigned VMs where an app user would say I want to build I want to build front end 5 and you'd end",
    "start": "2410620",
    "end": "2419050"
  },
  {
    "text": "up with a you know a new copy of the front-end vm and the operations team would have to get involved with",
    "start": "2419050",
    "end": "2424780"
  },
  {
    "text": "scheduling that anything else yeah",
    "start": "2424780",
    "end": "2432300"
  },
  {
    "text": "do we do any security scanning on our containers or on our registry nothing",
    "start": "2434550",
    "end": "2440560"
  },
  {
    "text": "automatic and it's on the backlog right now we basically just keep an eye out on the",
    "start": "2440560",
    "end": "2446950"
  },
  {
    "text": "CBS and and keep our base images patched and but we don't yet have anything that",
    "start": "2446950",
    "end": "2454420"
  },
  {
    "text": "can for example automatically pull out of date base images from use and that's",
    "start": "2454420",
    "end": "2459640"
  },
  {
    "text": "that's obviously desirable yeah",
    "start": "2459640",
    "end": "2464170"
  },
  {
    "text": "how have you been upgrading kuber denny's or any other components that would affect infrastructure downtime we've been doing close to replacement",
    "start": "2469809",
    "end": "2476289"
  },
  {
    "text": "and load migration so far and I would love to get away from that I",
    "start": "2476289",
    "end": "2483279"
  },
  {
    "text": "think that where I think we're approaching a state where we can do that now that we're going to be able to do a",
    "start": "2483279",
    "end": "2489910"
  },
  {
    "text": "che upgrades of the core components and then and then the cube load alongside it but we haven't proven that out yet so",
    "start": "2489910",
    "end": "2497339"
  },
  {
    "text": "right now it's a fairly heavy weight process we've spent up a new environment move load to it and then spend down the",
    "start": "2497339",
    "end": "2502390"
  },
  {
    "text": "old one anything else",
    "start": "2502390",
    "end": "2507689"
  },
  {
    "text": "all right thank you very much",
    "start": "2507689",
    "end": "2511439"
  }
]