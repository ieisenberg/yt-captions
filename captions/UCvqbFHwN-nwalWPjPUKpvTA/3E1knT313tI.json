[
  {
    "text": "uh I'm Peter salanki VP of engineering at core uh who is core we are a specialized",
    "start": "240",
    "end": "5960"
  },
  {
    "text": "cloud provider um we focus only on high performance workload what just means",
    "start": "5960",
    "end": "11639"
  },
  {
    "text": "anyone who needs uh a lot of compute power uh predominantly machine learning training or inference but also molecular",
    "start": "11639",
    "end": "18199"
  },
  {
    "text": "Dynamics and similar use cases uh we're a bit younger Cloud than your traditional hypos scalers we started",
    "start": "18199",
    "end": "24240"
  },
  {
    "text": "building our stack in 2018 um and unlike when when Amazon un",
    "start": "24240",
    "end": "29759"
  },
  {
    "text": "like started building their Cloud we had some new very interesting Technologies available to us uh namely containers and",
    "start": "29759",
    "end": "36360"
  },
  {
    "text": "kues we also realized that all of the major supercomputers in the world uh",
    "start": "36360",
    "end": "42360"
  },
  {
    "text": "were not running VMS they're running bar metal so the choice for us to start building with kubernetes on bar metal um",
    "start": "42360",
    "end": "50000"
  },
  {
    "text": "was a pretty easy one right it made made a lot of sense um mush as overhead and",
    "start": "50000",
    "end": "55760"
  },
  {
    "text": "really great patterns to to manage and deploy software the the road to run in",
    "start": "55760",
    "end": "62320"
  },
  {
    "text": "kubernetes environmental is not necessarily as easy as it seems and we'll get into",
    "start": "62320",
    "end": "67640"
  },
  {
    "text": "that right now um in this in this presentation we're going to focus on a",
    "start": "67640",
    "end": "73680"
  },
  {
    "text": "specific um cluster that we built as part of our our Cloud Fleet uh it's the",
    "start": "73680",
    "end": "79119"
  },
  {
    "text": "one that we used in May June to um set the ml per record on the llm training",
    "start": "79119",
    "end": "86520"
  },
  {
    "text": "Benchmark uh the the project started in in erest in January when we started",
    "start": "86520",
    "end": "92399"
  },
  {
    "text": "building one of the absolutely first Nvidia h100 training clusters um",
    "start": "92399",
    "end": "98439"
  },
  {
    "text": "together with our customer inflection AI uh the ml per Benchmark itself uh was",
    "start": "98439",
    "end": "107240"
  },
  {
    "text": "run over 3,500 gpus uh and finish the benchmarks The Benchmark in 11 minutes",
    "start": "107240",
    "end": "113399"
  },
  {
    "text": "the ml perf LM Benchmark is a GPT free uh model structure uh and um and model",
    "start": "113399",
    "end": "119240"
  },
  {
    "text": "size same five billion parameters it's not trained to convergence obviously it takes a little bit longer than 11",
    "start": "119240",
    "end": "124479"
  },
  {
    "text": "minutes uh at least on this cluster maybe not the next one uh but it gives you an approximation for a true llm",
    "start": "124479",
    "end": "131400"
  },
  {
    "text": "training workload uh the cluster itself at the time of training had 3,500 Nvidia h100",
    "start": "131400",
    "end": "138000"
  },
  {
    "text": "gpus uh these are all interconnected into superc computer using infin band uh",
    "start": "138000",
    "end": "143200"
  },
  {
    "text": "Network Technology uh there's 400 miles of fiber uh inside this um supercomputer",
    "start": "143200",
    "end": "149480"
  },
  {
    "text": "which is all housed inside one sector of a data center so it's a lot of fiber in a small area uh and there are 40,000",
    "start": "149480",
    "end": "157280"
  },
  {
    "text": "connections on um between the systems so you have a Fiber goes into a switch goes",
    "start": "157280",
    "end": "162560"
  },
  {
    "text": "into another switch each of these connectors connect on Optics they all need to be cleaned before you put them in and if any of these fail you have",
    "start": "162560",
    "end": "170879"
  },
  {
    "text": "performance degradation in your cluster uh and the results that we got to 30 times faster than the next next",
    "start": "170879",
    "end": "177760"
  },
  {
    "text": "nearest competitor um so let's dig in how this cluster is built what are",
    "start": "177760",
    "end": "183159"
  },
  {
    "text": "components and what will fail and how we handle it um the servers",
    "start": "183159",
    "end": "190440"
  },
  {
    "text": "themselves it's an 4 Ru or 6 Ru or 8 Ru server H they contain eight Nvidia h100",
    "start": "190440",
    "end": "197040"
  },
  {
    "text": "HDX gpus uh and then they have eight infin band Nicks with eight uplinks and two",
    "start": "197040",
    "end": "204360"
  },
  {
    "text": "EET uplinks so there's a total of 10 fibers coming out of a system all 10 can fail and each individual failure can be",
    "start": "204360",
    "end": "211599"
  },
  {
    "text": "catastrophic to a job these also configured in what you call a rail optimized configuration but",
    "start": "211599",
    "end": "217480"
  },
  {
    "text": "not not dig too much into that uh so this is how it looks in a data center so",
    "start": "217480",
    "end": "222959"
  },
  {
    "text": "you'll see you have rows of core switching racks this is where all the infinite band cables go back this is one",
    "start": "222959",
    "end": "229120"
  },
  {
    "text": "of our core groups uh there's actually eight of these in one of the builds um you have 400 miles of infin",
    "start": "229120",
    "end": "237040"
  },
  {
    "text": "Van um fiber standard mul mod fiber and then you have the servers uh to your",
    "start": "237040",
    "end": "242280"
  },
  {
    "text": "upper right there where you see eight eight cables going in for infinite van in the back front or GPU servers any",
    "start": "242280",
    "end": "248599"
  },
  {
    "text": "failure of any of these components uh will tear down the job U because all these gpus are working",
    "start": "248599",
    "end": "254400"
  },
  {
    "text": "together all the free, 500 plus gpus are working together in one single job and a single failure will cause a failure for",
    "start": "254400",
    "end": "261040"
  },
  {
    "text": "a job it has a restart from a last checkpoint and you lose a lot of your trading time so ensuring that your nodes",
    "start": "261040",
    "end": "267759"
  },
  {
    "text": "are are healthy and your entire fabric is healthy H is is critical to not lose performance on these very expensive AI",
    "start": "267759",
    "end": "275199"
  },
  {
    "text": "training machines so to actually build this and",
    "start": "275199",
    "end": "282039"
  },
  {
    "text": "get it running and get it running faster than pretty much anyone else out there um we build a couple of of core",
    "start": "282039",
    "end": "288759"
  },
  {
    "text": "components uh we choose kubernetes to be very flexible in how we deploy our software uh we run them on bare metal to",
    "start": "288759",
    "end": "295759"
  },
  {
    "text": "skip the virtualization overhead and we boot everything stateless",
    "start": "295759",
    "end": "301560"
  },
  {
    "text": "after the notes are booted there's a full Suite of validations that we run uh both during Burnin and uh continuously",
    "start": "301759",
    "end": "309880"
  },
  {
    "text": "and gather all the metrics and act on them immediately and once the notes are up",
    "start": "309880",
    "end": "315080"
  },
  {
    "text": "and healthy we need to actually run the workloads uh we can either run them in kubernetes using many of the schedulers",
    "start": "315080",
    "end": "321360"
  },
  {
    "text": "or proprietary schedules discussed earlier this week uh or we can run them use some traditional HBC schedu slur uh",
    "start": "321360",
    "end": "327880"
  },
  {
    "text": "we're going to cover all these free steps in next couple of slides starting with how we",
    "start": "327880",
    "end": "335120"
  },
  {
    "text": "boot uh the paramal nodes the system the systems are delivered without any OS on",
    "start": "335120",
    "end": "340440"
  },
  {
    "text": "them we don't want them to come with any Os from the vendor because things change constantly right we have new diers to deploy new Cals these are new CPUs can't",
    "start": "340440",
    "end": "348360"
  },
  {
    "text": "really expect anything that pre-loaded in the factory to work for us inside each server we have an Nvidia",
    "start": "348360",
    "end": "354960"
  },
  {
    "text": "Bluefield 2 or Bluefield 3 uh dpu data processing unit",
    "start": "354960",
    "end": "360199"
  },
  {
    "text": "uh what what these do in the stack is they help us provide our VPC isolation since all of our we Cloud everything is",
    "start": "360199",
    "end": "366240"
  },
  {
    "text": "multitenant they provide isolation between customers and they also act to serve the booth image for the node I'm",
    "start": "366240",
    "end": "373400"
  },
  {
    "text": "not going to go too much into um the dpus sa itself in this presentation",
    "start": "373400",
    "end": "378639"
  },
  {
    "text": "that's a whole different talk uh but what's important to note is that dpus is basically a smartnick with a Raspberry",
    "start": "378639",
    "end": "384360"
  },
  {
    "text": "Pi on them um they also run kubernetes and they they connect to um dpu",
    "start": "384360",
    "end": "390080"
  },
  {
    "text": "management cluster uh where there's a crd running that tells them that this specific node node number a uh should",
    "start": "390080",
    "end": "397000"
  },
  {
    "text": "boot this image and join this kubernetes customer then when a node boots it pulls",
    "start": "397000",
    "end": "402280"
  },
  {
    "text": "a stateless image it's an Ubuntu image um very tiny but has GPU drivers some",
    "start": "402280",
    "end": "407800"
  },
  {
    "text": "Infinity band drivers and a kuet h it gets a join token from the dpu Via cloud",
    "start": "407800",
    "end": "413120"
  },
  {
    "text": "in it and joins a kubernetes cluster whenever a no reboots there's no",
    "start": "413120",
    "end": "418759"
  },
  {
    "text": "persistent state on a node does the same process again pulls an image like it's the first time it booten and joins a",
    "start": "418759",
    "end": "424360"
  },
  {
    "text": "kubernetes cluster This way everything is stateless there's no manual provisioning a node there's no State drift um it's fully ephemeral which",
    "start": "424360",
    "end": "431199"
  },
  {
    "text": "means we can plug in new nodes and get them up and running joining kubernetes cluster",
    "start": "431199",
    "end": "437080"
  },
  {
    "text": "immediately uh moving on to the next phase node life cycle so nodes come online we need to make sure they're",
    "start": "440000",
    "end": "445599"
  },
  {
    "text": "healthy and we need to to um continuously um keep them that way uh we",
    "start": "445599",
    "end": "454800"
  },
  {
    "text": "picked kubernetes as our source of Truth this means that we don't have any other databases uh storing information about",
    "start": "454800",
    "end": "461240"
  },
  {
    "text": "you know what is aown is it up uh who does it belong to uh everything is in kubernetes uh I like to say that",
    "start": "461240",
    "end": "467240"
  },
  {
    "text": "kubernetes is database and it kind of is right it's etcd it's API server and CDs is a a",
    "start": "467240",
    "end": "475800"
  },
  {
    "text": "schema for objects and then you can do nice watches and there's great patterns for writing reconcilers and controllers",
    "start": "475800",
    "end": "481360"
  },
  {
    "text": "so building an ecosystem around kubernetes makes it very easy for us to to plug in new",
    "start": "481360",
    "end": "486720"
  },
  {
    "text": "things uh and uh get metrics out into different metric store without having to build a bunch of glue in between",
    "start": "486720",
    "end": "493560"
  },
  {
    "text": "proprietary systems and and kubernetes itself um we flow all data from",
    "start": "493560",
    "end": "500879"
  },
  {
    "text": "kubernetes from from kubernetes States uh up to promethus uh for observability",
    "start": "500879",
    "end": "506319"
  },
  {
    "text": "and for alerting and then once data is in promethus we can take actions on this",
    "start": "506319",
    "end": "512240"
  },
  {
    "text": "both automatically or manually from an an operations uh point of view uh and you will see the theme of",
    "start": "512240",
    "end": "519760"
  },
  {
    "text": "how everything is how kubernetes is our database um as we go",
    "start": "519760",
    "end": "525279"
  },
  {
    "text": "along we use a bunch of Open Source tools and a couple of proprietary controllers uh in our stack uh no",
    "start": "525519",
    "end": "532760"
  },
  {
    "text": "problem detector uh that many of you probably use is is a very core building block it doesn't come with a lot of",
    "start": "532760",
    "end": "539160"
  },
  {
    "text": "protections from outdoor block box but it allows us to easily plug in um",
    "start": "539160",
    "end": "544279"
  },
  {
    "text": "different Triggers on say kernel messages and colel logs",
    "start": "544279",
    "end": "550120"
  },
  {
    "text": "um kuus itself kind of expects you to have happy notes right like when you run",
    "start": "550120",
    "end": "555279"
  },
  {
    "text": "kubernetes on on a vers a VM from a cloud provider it's usually happy in",
    "start": "555279",
    "end": "561200"
  },
  {
    "text": "itself kubernetes doesn't have a lot of health checks for nodes going bad it doesn't handle nodes being in half bad",
    "start": "561200",
    "end": "566640"
  },
  {
    "text": "States well at all uh so we need to build a lot of tooling around it um we",
    "start": "566640",
    "end": "572320"
  },
  {
    "text": "use the standard node exporter dcgm exporter to get node and GPU metrics promethus for our metric store luki for",
    "start": "572320",
    "end": "579240"
  },
  {
    "text": "log aggregation querying h grafana grafana on call for for visualizing to our operations",
    "start": "579240",
    "end": "585360"
  },
  {
    "text": "teams uh we have proprietary red fish exporter and red fish controller to talk",
    "start": "585360",
    "end": "590720"
  },
  {
    "text": "to the AO band management of nodes this is used to get autoband metrics firmware inventory and do things like reboot the",
    "start": "590720",
    "end": "598160"
  },
  {
    "text": "node when you don't don't want to reboot it from the OS uh we have our proprietary life cycle",
    "start": "598160",
    "end": "603880"
  },
  {
    "text": "controller uh and Argo workflows which is used to to execute he checks talk more",
    "start": "603880",
    "end": "610560"
  },
  {
    "text": "about digging into life cycle controller and how to handle life baral node life cycle in in",
    "start": "614399",
    "end": "620399"
  },
  {
    "text": "kubernetes uh as I said earlier kubernetes expects happy nodes he has plug in a node boot you can probably",
    "start": "620399",
    "end": "626120"
  },
  {
    "text": "schedule a pod on it um does it networking hopefully if the C if the",
    "start": "626120",
    "end": "631760"
  },
  {
    "text": "kuet things is ready you know pause will schedule on it doesn't really mean that your gpus are healthy doesn't mean that",
    "start": "631760",
    "end": "637320"
  },
  {
    "text": "your networking is actually working uh that you can access persistent storage uh and so on uh this is what what why we",
    "start": "637320",
    "end": "644279"
  },
  {
    "text": "develop what we call the node life cycle controller uh node life cycle controller acts on States and states can either be",
    "start": "644279",
    "end": "651959"
  },
  {
    "text": "a kuber label annotation or a node condition it will then take the the node",
    "start": "651959",
    "end": "657440"
  },
  {
    "text": "fruits Journey uh up until it's in production and even when it's in production to take it out of production",
    "start": "657440",
    "end": "663519"
  },
  {
    "text": "uh on some kind of fault or condition um it starts by verifying that the node is",
    "start": "663519",
    "end": "670480"
  },
  {
    "text": "physically correct it has all the discs it should have it has all the infin band connections it should have it's connected to the right uh infinity band",
    "start": "670480",
    "end": "677839"
  },
  {
    "text": "switch and ethernet switch and so on it then takes us for an up firmware upgrade process where we want to upgrade the",
    "start": "677839",
    "end": "684320"
  },
  {
    "text": "firmers of all the gpus the BIOS The BMC of the node especially with these new",
    "start": "684320",
    "end": "689600"
  },
  {
    "text": "platforms there's a lot of changes in the in the kind of microcontroller software running on them um so you need",
    "start": "689600",
    "end": "695880"
  },
  {
    "text": "to be prepared and have an automated way to upgrade all these components um at a as a very frequent",
    "start": "695880",
    "end": "703519"
  },
  {
    "text": "Cadence uh once the node is up verified and uh upgraded uh we run a battery of",
    "start": "703519",
    "end": "710720"
  },
  {
    "text": "self tests uh they run for 24 hours and this includes everything from testing gpus so on um talk more about that in a",
    "start": "710720",
    "end": "718160"
  },
  {
    "text": "little bit uh once the tests have passed the notes are put into production this",
    "start": "718160",
    "end": "723560"
  },
  {
    "text": "means the customers can schedule on them and we can run things like our big ml per training job uh when a noce in",
    "start": "723560",
    "end": "730000"
  },
  {
    "text": "production the cor node controller takes over um this controller is responsible",
    "start": "730000",
    "end": "735079"
  },
  {
    "text": "for labeling nodes with metadata again communties is our database so metadata about you know no's location its",
    "start": "735079",
    "end": "742000"
  },
  {
    "text": "placement in the fabric and so on is all written on the Node object uh the node",
    "start": "742000",
    "end": "747600"
  },
  {
    "text": "controller also handles life events like a reboot which is requested either by a customer or an admin of ours reaches out",
    "start": "747600",
    "end": "754040"
  },
  {
    "text": "via redfish to do so uh a very detailed State diagram of",
    "start": "754040",
    "end": "759760"
  },
  {
    "text": "the the life cycle controller uh not going to go into to everything that's",
    "start": "759760",
    "end": "765320"
  },
  {
    "text": "going on here uh but there's some a couple of of important themes here as you see the cube API server is Central",
    "start": "765320",
    "end": "772079"
  },
  {
    "text": "every action flows for kubernetes there's no path that doesn't go for kubernetes if an admin uh here on the",
    "start": "772079",
    "end": "777600"
  },
  {
    "text": "left requests a Rebo of a node he does so by setting a condition on the Node and then the node controller we will go",
    "start": "777600",
    "end": "783600"
  },
  {
    "text": "out reach out through red fish uh to that node BMC out of band management",
    "start": "783600",
    "end": "788720"
  },
  {
    "text": "interface and Trigger that reboot why we force everything to go where kubernetes",
    "start": "788720",
    "end": "794279"
  },
  {
    "text": "it's because we get built-in flow for event logging we events we it's clear on",
    "start": "794279",
    "end": "800120"
  },
  {
    "text": "the N you can describe it what was the last condition taken and we can use metrics exporting through um say Cube",
    "start": "800120",
    "end": "805959"
  },
  {
    "text": "State metrics and Cube node labels to get all of this in prome automatically um by centralizing the entire management",
    "start": "805959",
    "end": "812639"
  },
  {
    "text": "flow on kubernetes we can of get a lot of stuff for free and we also get a programming model when different teams",
    "start": "812639",
    "end": "817839"
  },
  {
    "text": "need to interact with this that they know because a lot of people know how to program against kubernetes um you also see on the top",
    "start": "817839",
    "end": "825920"
  },
  {
    "text": "there's a red fish exporter goes promethus that goes to something called epeus which is basically a converter",
    "start": "825920",
    "end": "831519"
  },
  {
    "text": "from promethus alerts to conditions back to the to the API server but this means",
    "start": "831519",
    "end": "836639"
  },
  {
    "text": "that if there's some kind of alert on a note um say a bad GPU um we can ultimately",
    "start": "836639",
    "end": "843959"
  },
  {
    "text": "feed that back as a condition uh on the Node and then since the node controller or node life cycle controller acts on",
    "start": "843959",
    "end": "851480"
  },
  {
    "text": "conditions we can take an action on it so this full flow that's driven by uh",
    "start": "851480",
    "end": "856759"
  },
  {
    "text": "labels and conditions that are set on",
    "start": "856759",
    "end": "860920"
  },
  {
    "text": "nodes and an excript from a node described shows how we use use uh commun",
    "start": "863240",
    "end": "869360"
  },
  {
    "text": "to stick all of our metadata uh the first two rows first two annotation shows an output from our automated",
    "start": "869360",
    "end": "874920"
  },
  {
    "text": "Health tests in this case the GPU has has a failure uh and we even have from",
    "start": "874920",
    "end": "880399"
  },
  {
    "text": "the health test that it should be submitted for RMA so our Ops Team knows what to what to do with a node um you see some topology",
    "start": "880399",
    "end": "888120"
  },
  {
    "text": "information in in HPC clusters uh topology and location is important when",
    "start": "888120",
    "end": "893800"
  },
  {
    "text": "you schedule your ml training workloads you want them to be close to each other on the network so annotate every node",
    "start": "893800",
    "end": "900160"
  },
  {
    "text": "with where they are in the infin band fabric uh which switches they're connected to so we can verify connect",
    "start": "900160",
    "end": "905959"
  },
  {
    "text": "correctness um we annotate the node with what we call a firmware bundle which",
    "start": "905959",
    "end": "911079"
  },
  {
    "text": "says this is the firmwares that this node should have and then the node life cycle controller knows to to reconcile",
    "start": "911079",
    "end": "916839"
  },
  {
    "text": "it to those firmwares if that differs uh you also have the state down here which is a very unsuspecting label but but a",
    "start": "916839",
    "end": "923720"
  },
  {
    "text": "core in in entire um uh entire process uh which is a able the drives the node",
    "start": "923720",
    "end": "929680"
  },
  {
    "text": "controller say what state is in this node in right now it's in broken it was automatically put in broken based on the",
    "start": "929680",
    "end": "935440"
  },
  {
    "text": "test failure um and when it's in broken it won't be available to customers to run workloads on the Node will be",
    "start": "935440",
    "end": "942319"
  },
  {
    "text": "cordone off and so on looking at another describe of node conditions you can see a slw of node",
    "start": "942319",
    "end": "949440"
  },
  {
    "text": "conditions here some of these are set by the node problem detector then the no controller can act act on that um I the",
    "start": "949440",
    "end": "956639"
  },
  {
    "text": "GPU fault condition which is based on um kernel log uh some of these are set from alerts uh like the network alert fault",
    "start": "956639",
    "end": "964240"
  },
  {
    "text": "uh where an alert is acted as triggered on the Node and we set a condition on it so the node controller can act on it and",
    "start": "964240",
    "end": "969880"
  },
  {
    "text": "some are set by admins there's the admin maintenance mode in the bottom um that one of our operations Engineers can set",
    "start": "969880",
    "end": "976920"
  },
  {
    "text": "when he wants to take out the node for maintenance for some reason and the node controller will then coordinate off uh from the customer make sure there's no",
    "start": "976920",
    "end": "983120"
  },
  {
    "text": "workload running on it before he lets admin do any work on it and since all these States um are",
    "start": "983120",
    "end": "991240"
  },
  {
    "text": "labels or conditions they're already exported for us for free uh to pruse um",
    "start": "991240",
    "end": "997759"
  },
  {
    "text": "via Cube State metric Cube node labels so it make it very easy for us to visualize what's going on in the cluster",
    "start": "997759",
    "end": "1003560"
  },
  {
    "text": "so this is a visualization of the node controller uh node controller life cycle we see that node starts kind of in",
    "start": "1003560",
    "end": "1009399"
  },
  {
    "text": "onboarding goes through a different test phasing zap is when we upgrade firmwares um test is the 24-hour test",
    "start": "1009399",
    "end": "1016120"
  },
  {
    "text": "window we do when noes boot the first time and then a bunch of D in production",
    "start": "1016120",
    "end": "1021480"
  },
  {
    "text": "and then you have post production phases with uh debugging or broken RMA nodes uh",
    "start": "1021480",
    "end": "1028520"
  },
  {
    "text": "since the test times aren't annotation on a node we can very nicely graph how long time nodes have left in their their",
    "start": "1028520",
    "end": "1034839"
  },
  {
    "text": "self test phase uh talking a bit about our burn in",
    "start": "1034839",
    "end": "1042640"
  },
  {
    "text": "testing uh when a node starts out for the first time we put it through a 24-hour burn in test cycle uh these",
    "start": "1042640",
    "end": "1050799"
  },
  {
    "text": "tests are run U via Aro workflows um they they run it's a continuously over",
    "start": "1050799",
    "end": "1057320"
  },
  {
    "text": "the course of 24 hours they're about six hours each C duration so we run four of them uh and they test everything from",
    "start": "1057320",
    "end": "1065080"
  },
  {
    "text": "PCI performance uh EnV link NV switch performance for NVIDIA gpus uh infinity",
    "start": "1065080",
    "end": "1070520"
  },
  {
    "text": "band performance all across the fabric CPU e foret looks for any error counters",
    "start": "1070520",
    "end": "1075799"
  },
  {
    "text": "if any of these fails uh an alert is raised and then no controller can",
    "start": "1075799",
    "end": "1080880"
  },
  {
    "text": "automatically move it from the testing state to the testing fail state for manual intervention uh after a node is",
    "start": "1080880",
    "end": "1088200"
  },
  {
    "text": "in production we keep running these tests so the Argo workflow as is written checks are there available is there any",
    "start": "1088200",
    "end": "1095440"
  },
  {
    "text": "pods running on the Node consuming gpus uh assuming the node is Idle we can run",
    "start": "1095440",
    "end": "1100480"
  },
  {
    "text": "a test so we continuously test nodes that are idle the tests are run at a lower priority class than normal jobs",
    "start": "1100480",
    "end": "1106520"
  },
  {
    "text": "which means that customer jobs will PR M these tests as soon as they start running um but since we continuously",
    "start": "1106520",
    "end": "1111960"
  },
  {
    "text": "test the nodes we don't have to wait for a customer to tell us oh this node is broken because we usually find that out",
    "start": "1111960",
    "end": "1118480"
  },
  {
    "text": "uh hopefully before they do and since everything is in kubernetes we can",
    "start": "1118480",
    "end": "1123520"
  },
  {
    "text": "easily graph these results and get nice State timelines for different nodes like I'm showing here uh that shows you set",
    "start": "1123520",
    "end": "1129919"
  },
  {
    "text": "of four nodes with different issues um and one of them one of those issues started here at the end of the end of",
    "start": "1129919",
    "end": "1135840"
  },
  {
    "text": "the timeline where the test run all the logs from all of these are easily piped up with luki and we can uh",
    "start": "1135840",
    "end": "1144440"
  },
  {
    "text": "can parse the log results from different tests to graph these over time as well here's a test results for for high",
    "start": "1144440",
    "end": "1150000"
  },
  {
    "text": "performance limac over a part of our Fleet U and we're looking really for Trends here say the new driver version",
    "start": "1150000",
    "end": "1156559"
  },
  {
    "text": "is released we want to make sure that that um performance isn't degrading or a new BMC version or even environmental",
    "start": "1156559",
    "end": "1163240"
  },
  {
    "text": "changes in in the data center part of the data center",
    "start": "1163240",
    "end": "1168919"
  },
  {
    "text": "uh we develop dashboards for our Ops technicians to use when it in troubleshoot nodes this is a very like",
    "start": "1168919",
    "end": "1175720"
  },
  {
    "text": "oneth of this dashboard uh but having all this data available to us in",
    "start": "1175720",
    "end": "1181240"
  },
  {
    "text": "promethus and luki makes it so easy to bring it all together in one place uh and give a really clear view for anyone",
    "start": "1181240",
    "end": "1187880"
  },
  {
    "text": "who needs to to dig in troubleshoot",
    "start": "1187880",
    "end": "1191720"
  },
  {
    "text": "noes um we also use the cfan stack for alerting we use it together with pager",
    "start": "1193799",
    "end": "1199000"
  },
  {
    "text": "Duty uh but grafana on call uh provides really beautiful visualization of alerts",
    "start": "1199000",
    "end": "1204919"
  },
  {
    "text": "in slack so it makes it very easy for for operations technicians uh to go and see okay this note alerted I can get a",
    "start": "1204919",
    "end": "1212000"
  },
  {
    "text": "nice timeline when did it have any alerts previously if it did so I can immediately know what what might I have",
    "start": "1212000",
    "end": "1218320"
  },
  {
    "text": "to do with this node if there's nothing that can be done with it automatically the node controller also",
    "start": "1218320",
    "end": "1225880"
  },
  {
    "text": "listens to alerts uh and can also take take action on alerts if it's a well-known issue like a a GPU has",
    "start": "1225880",
    "end": "1232600"
  },
  {
    "text": "crashed uh we know that okay this node needs to be moved out from the customers environment because probably needs an RMA so we do that automatically as",
    "start": "1232600",
    "end": "1241440"
  },
  {
    "text": "well okay so now we have a bunch of healthy nodes uh they're up and running we",
    "start": "1244440",
    "end": "1249880"
  },
  {
    "text": "vetted out the broken ones and we got this continuous cycle of of node Health now how do we run training jobs on on uh",
    "start": "1249880",
    "end": "1257799"
  },
  {
    "text": "on our cluster uh we started out building this very Cloud native um pods for everything um and",
    "start": "1257799",
    "end": "1266200"
  },
  {
    "text": "there's a lot of work being done in this space with running uh large training jobs and kubernetes there there are schedulers with there's a co- schedulers",
    "start": "1266200",
    "end": "1272919"
  },
  {
    "text": "there's the MPI operators uh there companies like run AI KU flow uh",
    "start": "1272919",
    "end": "1278200"
  },
  {
    "text": "building Frameworks for these um and some of them are great and some of them are getting better better every day uh",
    "start": "1278200",
    "end": "1286120"
  },
  {
    "text": "however in the traditional HPC Comm Community people like slurm slurm is a",
    "start": "1286120",
    "end": "1291679"
  },
  {
    "text": "HBC worklow schedular it is couple years old um and um it's built for for really",
    "start": "1291679",
    "end": "1299000"
  },
  {
    "text": "a supercomputer world where you know you spend two years building a supercomputer",
    "start": "1299000",
    "end": "1304240"
  },
  {
    "text": "and then you test it and then all your you know x, of CPUs gpus are up and it's",
    "start": "1304240",
    "end": "1309799"
  },
  {
    "text": "very staticy that doesn't really work in today's age of Rapid AI Evolution right",
    "start": "1309799",
    "end": "1315320"
  },
  {
    "text": "where we build this uh we build these custers and as soon as there is one node or eight gpus online a customer wants to",
    "start": "1315320",
    "end": "1322080"
  },
  {
    "text": "start start running on them uh so we need something that is agile and we can",
    "start": "1322080",
    "end": "1327880"
  },
  {
    "text": "upgrade it quickly we don't want to take down the entire cluster from maintenance for a week to upgrade all the components",
    "start": "1327880",
    "end": "1333240"
  },
  {
    "text": "uh like you could do with a you know traditional HBC University environment but people coming from research coming",
    "start": "1333240",
    "end": "1339559"
  },
  {
    "text": "from these HBC environments are used to slurm and want to use slurm uh so how",
    "start": "1339559",
    "end": "1344679"
  },
  {
    "text": "how do we solve that uh so we built something called slurm on kubernetes uh code name sunk has I mean",
    "start": "1344679",
    "end": "1353120"
  },
  {
    "text": "when the name if you look at the the kubernetes theme maybe it means that it's sinking I don't know um no comment",
    "start": "1353120",
    "end": "1358240"
  },
  {
    "text": "on that but um um but we tried to integrate slurm uh",
    "start": "1358240",
    "end": "1365080"
  },
  {
    "text": "with its very non-cloud native architecture uh with kubernetes uh very detailed diagram how",
    "start": "1365080",
    "end": "1372520"
  },
  {
    "text": "slur kubernetes Works uh but important pieces are everything is containerized all the controller pieces or",
    "start": "1372520",
    "end": "1379200"
  },
  {
    "text": "containerized the slurm CLD slurm D login nodes which are traditionally a",
    "start": "1379200",
    "end": "1384919"
  },
  {
    "text": "bare metal server that you know hundreds of researchers SSH into and do work and",
    "start": "1384919",
    "end": "1390200"
  },
  {
    "text": "launch jobs from multitenant are also a container and we can launch multiple replicas or individual login noes for",
    "start": "1390200",
    "end": "1396640"
  },
  {
    "text": "individual researchers instead of having needing to have a VM or a paramal machine that 100 people are multiplexed",
    "start": "1396640",
    "end": "1402559"
  },
  {
    "text": "over uh the slurm demon that actually runs the training jobs is also a container",
    "start": "1402559",
    "end": "1408440"
  },
  {
    "text": "uh slurm in itself does a bunch of c groups and so on if you're familiar with that and all of that will inherit from",
    "start": "1408440",
    "end": "1414520"
  },
  {
    "text": "the the Pod cgroup tree uh so the most if not all slurm functionality uh Works",
    "start": "1414520",
    "end": "1421600"
  },
  {
    "text": "kind of out to the Box in this um this pattern with slight modifications that we had made then there are a couple of",
    "start": "1421600",
    "end": "1429080"
  },
  {
    "text": "interesting controllers uh we have what we call a nod set controller which basically acts as a Damon set so what",
    "start": "1429080",
    "end": "1435799"
  },
  {
    "text": "this means is that it will run one of these slurm Damons on every node in your cluster that should be running",
    "start": "1435799",
    "end": "1441520"
  },
  {
    "text": "slurm and it runs them without consuming all the resources on the Node uh so what",
    "start": "1441520",
    "end": "1447640"
  },
  {
    "text": "that means is that slurm is running you can launch jobs in slurm uh but you can also launch jobs in",
    "start": "1447640",
    "end": "1454559"
  },
  {
    "text": "kubernetes so we have the the slurm kubernetes schedular integration uh",
    "start": "1454559",
    "end": "1460279"
  },
  {
    "text": "where the slurm schedular acts as a schedu plugin to kubernetes uh when is this useful one",
    "start": "1460279",
    "end": "1466880"
  },
  {
    "text": "example is if if you have a cluster that's used by researchers uh to train your foundational models right you're",
    "start": "1466880",
    "end": "1472679"
  },
  {
    "text": "training a huge new llm um but you also have production inference running on it and production",
    "start": "1472679",
    "end": "1479120"
  },
  {
    "text": "inference you really want to run that in kubs right uh it's okay slurm you know we got this kind of old pattern of",
    "start": "1479120",
    "end": "1484799"
  },
  {
    "text": "working it's works great for batch jobs but it's really not a good fit for long long running critical services so we",
    "start": "1484799",
    "end": "1490960"
  },
  {
    "text": "want to run inference in kubernetes pods but training in slur and inference",
    "start": "1490960",
    "end": "1497440"
  },
  {
    "text": "traffic you know you have people wild from the internet coming to your app you know chat DPT style um traffic and",
    "start": "1497440",
    "end": "1504919"
  },
  {
    "text": "fluctuate so we want our autos scalers and kubernetes to be able to manage our kubernetes pods and when there's a burst",
    "start": "1504919",
    "end": "1511159"
  },
  {
    "text": "in your inference traffic we want to be able to preempt unimportant jobs in slurm say that you have you know uh one",
    "start": "1511159",
    "end": "1519279"
  },
  {
    "text": "big pre-training job that you don't want to preempt runs at the highest priority class uh but you have some research jobs",
    "start": "1519279",
    "end": "1524720"
  },
  {
    "text": "that you will easily want to preempt for for more inference capacity uh so the slurm kuber schedule",
    "start": "1524720",
    "end": "1531000"
  },
  {
    "text": "integration allows the slurm scheduler with it's very Advanced preemption gang scheduling um Concepts to manage the",
    "start": "1531000",
    "end": "1539000"
  },
  {
    "text": "scheduling of these kubernetes pots uh so instead of using something like you know volcano or whatever in kubernetes",
    "start": "1539000",
    "end": "1545159"
  },
  {
    "text": "we actually leverage the slurms scheduler even for kubernetes and kubernetes pods show up inside slurm and",
    "start": "1545159",
    "end": "1551880"
  },
  {
    "text": "we can use slurm accounting and all those uh slurm features for our kubernetes PS without really",
    "start": "1551880",
    "end": "1557720"
  },
  {
    "text": "compromising on those things running in proper containers in proper container isolation from",
    "start": "1557720",
    "end": "1564480"
  },
  {
    "text": "kubernetes um and we also export all of our metrics from slurm of course into",
    "start": "1567760",
    "end": "1573200"
  },
  {
    "text": "promethus um and since we have all of our other metrics there uh we can create these real nice informative dashboards",
    "start": "1573200",
    "end": "1580120"
  },
  {
    "text": "taking metrics from the job running itself so here we're running the ml perf um uh record breaking job uh we can see",
    "start": "1580120",
    "end": "1587799"
  },
  {
    "text": "there was a Interruption there something crashed you can see that the the flops counter goes to zero uh and by by",
    "start": "1587799",
    "end": "1594760"
  },
  {
    "text": "overlaying the lerts as annotations on top we can see very quickly see that okay this job stopped because a GPU fell",
    "start": "1594760",
    "end": "1601279"
  },
  {
    "text": "off the bus fell off the PCI bus um so this doesn't only help ourselves to",
    "start": "1601279",
    "end": "1606600"
  },
  {
    "text": "diagnose like when there are issues with cluster we can also expose this to customers so they have full insight into",
    "start": "1606600",
    "end": "1612039"
  },
  {
    "text": "what's what's happening with our big training cluster it's not one opaque Black Box where things work and things don't work",
    "start": "1612039",
    "end": "1619600"
  },
  {
    "text": "uh slur on kubernetes uh will be open sourced uh in beginning of next year um",
    "start": "1620919",
    "end": "1626520"
  },
  {
    "text": "we believe this is important to to marry traditional um HBC work with with",
    "start": "1626520",
    "end": "1631760"
  },
  {
    "text": "kubernetes and how containers should be",
    "start": "1631760",
    "end": "1636159"
  },
  {
    "text": "run more data center picture and moving on to",
    "start": "1638799",
    "end": "1645760"
  },
  {
    "text": "questions",
    "start": "1646520",
    "end": "1649520"
  },
  {
    "text": "go for what does kubernetes bring to the HPC what advantage does kubernetes bring",
    "start": "1654279",
    "end": "1659679"
  },
  {
    "text": "to HPC Community um it it brings so why",
    "start": "1659679",
    "end": "1665200"
  },
  {
    "text": "why we like it right is you can roll out software more peac Mill and better isolated we can build containers and",
    "start": "1665200",
    "end": "1671320"
  },
  {
    "text": "it's really all about containers being able to package package up your dependencies uh or your different",
    "start": "1671320",
    "end": "1677159"
  },
  {
    "text": "components onon um as containers roll them out quickly upgrade peace meal and then you have a a whole you know whole",
    "start": "1677159",
    "end": "1684279"
  },
  {
    "text": "ecosystem of tooling that is really built around kubernetes we talked about prome luki like yeah you can use all of",
    "start": "1684279",
    "end": "1689919"
  },
  {
    "text": "these outside of kubernetes but by running in kubernetes you get to take advantage of everything that all these",
    "start": "1689919",
    "end": "1695519"
  },
  {
    "text": "great open source developers that are probably all around here today have built and retrofitting them in on a more",
    "start": "1695519",
    "end": "1701279"
  },
  {
    "text": "you know traditional model with like Chef anible you can do it but by in",
    "start": "1701279",
    "end": "1707720"
  },
  {
    "text": "gives you a lot of stuff uh for free uh then we also have you know like isolation like slurm distal HBC",
    "start": "1707720",
    "end": "1714440"
  },
  {
    "text": "environments not really that that tight on security in my mind it's easy to have noise and",
    "start": "1714440",
    "end": "1720559"
  },
  {
    "text": "neighbor problems uh to get visibility into each other's jobs they're not really um you know come traditionally",
    "start": "1720559",
    "end": "1727120"
  },
  {
    "text": "build from like a university perspective where okay we want you know we want like people keep their data apart but if I",
    "start": "1727120",
    "end": "1733440"
  },
  {
    "text": "know what my neighbor is working on it's not really a big deal and this is changing when we're building you know",
    "start": "1733440",
    "end": "1738880"
  },
  {
    "text": "these AI models and so on that are either very expensive to build because they take thousands of expensive GPS to",
    "start": "1738880",
    "end": "1744559"
  },
  {
    "text": "train or uh they're they're critical because you know they're they're um the",
    "start": "1744559",
    "end": "1750440"
  },
  {
    "text": "level they operate at we're very worried from leaking out so using the kubernetes to for our back and for isolation is",
    "start": "1750440",
    "end": "1757279"
  },
  {
    "text": "just better than anything has been in traditional HBC great talk yeah you can you hear me",
    "start": "1757279",
    "end": "1764360"
  },
  {
    "text": "sorry yeah you turn from Apple uh first question and the clarification question",
    "start": "1764360",
    "end": "1770440"
  },
  {
    "text": "about the sunk so as for SN running on kubernetes did you mention you Rong SN",
    "start": "1770440",
    "end": "1777480"
  },
  {
    "text": "as a pluging of the Native scheduler or it's a standard known the separate",
    "start": "1777480",
    "end": "1782519"
  },
  {
    "text": "schedu uh no so the the so it it RS yeah so the sunk Sinker I'm going to pull up",
    "start": "1782519",
    "end": "1788559"
  },
  {
    "text": "this diagram again maybe this this diagram is best um so the there yeah so it doesn't actually use the",
    "start": "1788559",
    "end": "1794440"
  },
  {
    "text": "native so the it's a different you set on your pod it's a different schedular name uh and the the sunk Sinker is a",
    "start": "1794440",
    "end": "1802200"
  },
  {
    "text": "schedular implementation so it talks the Schuler I mean it access the scheduler and then uh talks to the community",
    "start": "1802200",
    "end": "1808880"
  },
  {
    "text": "scheduler to actually schedule the Pod so it it's not a plug in to the native scheduler like the native schedule is not involved in scheduling the Pods at",
    "start": "1808880",
    "end": "1815320"
  },
  {
    "text": "all yeah then I think previous slides you mention you can run both s job and",
    "start": "1815320",
    "end": "1820919"
  },
  {
    "text": "kuet job on the same node so the kubernetes job and are scheduled by the native and the kubernetes",
    "start": "1820919",
    "end": "1827840"
  },
  {
    "text": "then how to you solve those risk condition the conflicts exactly so it's you can run both slurm jobs and cumus",
    "start": "1827840",
    "end": "1833799"
  },
  {
    "text": "pods but both of them are scheduled by the slurm scheduler so you're replacing you're replacing the community scheduler",
    "start": "1833799",
    "end": "1840360"
  },
  {
    "text": "uh with the slurm scheduler uh so this means that there's some things that slur the community scheduler can do that you",
    "start": "1840360",
    "end": "1845840"
  },
  {
    "text": "won't get uh but for a lot of functionality especially when it comes to these type of workloads uh the slurm",
    "start": "1845840",
    "end": "1851840"
  },
  {
    "text": "schedule is a super set of the community scheduler you know we can do very Advanced pre preemption gang scheduler",
    "start": "1851840",
    "end": "1858000"
  },
  {
    "text": "um and Bin packing is to PA aare so you would use you would use the slurm",
    "start": "1858000",
    "end": "1864000"
  },
  {
    "text": "scheduler for all your pods you short Damon sets and so on that can run in the background you don't use it for but for",
    "start": "1864000",
    "end": "1869399"
  },
  {
    "text": "anything that like uses up resources you would use the slurm schedular otherwise you would have very weird conflicts and",
    "start": "1869399",
    "end": "1874720"
  },
  {
    "text": "wouldn't be good at all okay got this second question about your load provision in load",
    "start": "1874720",
    "end": "1880279"
  },
  {
    "text": "registration have you Ed and class API if not and any reason why not use class",
    "start": "1880279",
    "end": "1886720"
  },
  {
    "text": "API the cluster API yes so I didn't actually talk about how we instantiate these clusters so we run multiple Comm",
    "start": "1886720",
    "end": "1894320"
  },
  {
    "text": "clusters obviously we have multiple uh customers and they own there's a couple different models how we provision them",
    "start": "1894320",
    "end": "1900360"
  },
  {
    "text": "there was a talk on that yesterday um by my colleague Brandon um but we we have",
    "start": "1900360",
    "end": "1905399"
  },
  {
    "text": "our own cluster operator which exposes our crd and that's how we uh how we instantiate clusters it's not actually",
    "start": "1905399",
    "end": "1911240"
  },
  {
    "text": "the cluster API right now it's our own crd because some philosophical differences but principle it's the same",
    "start": "1911240",
    "end": "1916880"
  },
  {
    "text": "so we instantiate new clusters uh with the crd and that really happens before",
    "start": "1916880",
    "end": "1922639"
  },
  {
    "text": "uh anything in presented today so the C the cluster API service and so on are created um before any no is booted they",
    "start": "1922639",
    "end": "1929720"
  },
  {
    "text": "run in a separate management cluster much like GK or AKs and then nodes are",
    "start": "1929720",
    "end": "1935120"
  },
  {
    "text": "booted into this cluster and that's where all the node life cycle happens great thanks again yeah great talk thank",
    "start": "1935120",
    "end": "1941159"
  },
  {
    "text": "you for your question hi um you mentioned that noes",
    "start": "1941159",
    "end": "1947480"
  },
  {
    "text": "are idle you've got uh you you're running testing have you is there a",
    "start": "1947480",
    "end": "1952679"
  },
  {
    "text": "balance between how exhaustively you test them and like the cost of power to do that test and because obviously an",
    "start": "1952679",
    "end": "1959639"
  },
  {
    "text": "idle node doesn't draw Power the same way as a lit node so have you done analysis on sort of that cost and how",
    "start": "1959639",
    "end": "1966039"
  },
  {
    "text": "much you youat them up yeah there is and that's a constant debate between me and our facilities team who doesn't like me",
    "start": "1966039",
    "end": "1972559"
  },
  {
    "text": "to run them at 100% load all the time um and so it it kind of the the compromise",
    "start": "1972559",
    "end": "1978600"
  },
  {
    "text": "we have to right now is they run the ongoing test runs once every hour they take about 30 minutes and then there's",
    "start": "1978600",
    "end": "1984840"
  },
  {
    "text": "30 minutes of downtime so that's you know 50/50 um and probably could run it",
    "start": "1984840",
    "end": "1990559"
  },
  {
    "text": "less to be honest uh could run it run it um less frequent uh to and it was still",
    "start": "1990559",
    "end": "1997320"
  },
  {
    "text": "cash issues uh we're pretty early in the in in the life cycle of these component especially focusing on the h100 right um",
    "start": "1997320",
    "end": "2004120"
  },
  {
    "text": "as the cluster matures like there's a huge curve where you know most of your issues happen in first two months and it",
    "start": "2004120",
    "end": "2009240"
  },
  {
    "text": "goes down from there so as the Clusters mature we can probably decrease that even further uh but right now you know",
    "start": "2009240",
    "end": "2015600"
  },
  {
    "text": "at the time where we are where gpus are so scarce and it's so important for everyone to have healthy gpus like yes",
    "start": "2015600",
    "end": "2022080"
  },
  {
    "text": "we're going to spend some extra money on Power and put some extra load on the data centers to make sure that we have",
    "start": "2022080",
    "end": "2027639"
  },
  {
    "text": "you know as many Healy gpus as we can for our customers yeah I mean I guess rerunning jobs that break halfway",
    "start": "2027639",
    "end": "2033279"
  },
  {
    "text": "through is yes and and it's you know since these the job Tak such a long time you have to load from checkpoint startup",
    "start": "2033279",
    "end": "2040120"
  },
  {
    "text": "Megatron whatever framework you're using like can easily take you you know half an hour to restart the job and it's it's",
    "start": "2040120",
    "end": "2046000"
  },
  {
    "text": "very painful thank you I'm wondering U how much power are",
    "start": "2046000",
    "end": "2053679"
  },
  {
    "text": "you running to the racks and how are you handling cooling okay this is a great question very much outside of node life",
    "start": "2053679",
    "end": "2058960"
  },
  {
    "text": "cycle so it depends on a data center we have um like 15 different data centers",
    "start": "2058960",
    "end": "2064040"
  },
  {
    "text": "plus around the us right now and it's going to be like 40 in a year um some",
    "start": "2064040",
    "end": "2071280"
  },
  {
    "text": "our average is 17 K kws per rack uh which is um it's just easier uh you know",
    "start": "2071280",
    "end": "2077800"
  },
  {
    "text": "since these things are so dense to begin with like space is usually another problem because most buildings are built",
    "start": "2077800",
    "end": "2083240"
  },
  {
    "text": "to like 8 kilow per R uh the one this specific cluster uh is uh we're running",
    "start": "2083240",
    "end": "2089679"
  },
  {
    "text": "at 18 kilow per rack uh no sorry we're running a 32 kilow per rack but most of",
    "start": "2089679",
    "end": "2095760"
  },
  {
    "text": "the builds are are uh are less than that to to let's give more um to give more SP",
    "start": "2095760",
    "end": "2102480"
  },
  {
    "text": "space all of it is air cooled currently uh next year I think we'll see a lot of",
    "start": "2102480",
    "end": "2107560"
  },
  {
    "text": "things switch to directed chip liquid cool as there are newer generations of gpus coming out they're going to get",
    "start": "2107560",
    "end": "2112640"
  },
  {
    "text": "consume so much power that is infeasible to to air cool them one more question",
    "start": "2112640",
    "end": "2118280"
  },
  {
    "text": "what are you using for the base system that you're plugging the h100s into and how many do you get per rack um yes so",
    "start": "2118280",
    "end": "2124920"
  },
  {
    "text": "the the each each h100 h h100 system consumes about 8 kilow of power so if you have a 16 kilowatt rack you get kind",
    "start": "2124920",
    "end": "2131359"
  },
  {
    "text": "of two and we refer to 2 kilowatt racks we get four uh the base system we work",
    "start": "2131359",
    "end": "2136400"
  },
  {
    "text": "with a couple of different oems uh one of them is super micro as an example",
    "start": "2136400",
    "end": "2142359"
  },
  {
    "text": "it's all all h100 HDX system great I've seen that one thank",
    "start": "2142359",
    "end": "2148079"
  },
  {
    "text": "you sorry I think you just I think you just answered my question those are super micro chassis in in the in the",
    "start": "2148119",
    "end": "2153760"
  },
  {
    "text": "picture yes they weren't dgx H they're not the H okay okay cool",
    "start": "2153760",
    "end": "2161160"
  },
  {
    "text": "thank hello I have two question related to the network so the first one is that",
    "start": "2161160",
    "end": "2166599"
  },
  {
    "text": "when you saw the dpu there so you're using dpu to uh provision the hardware",
    "start": "2166599",
    "end": "2172359"
  },
  {
    "text": "first like download image and create note so uh how would you switch like",
    "start": "2172359",
    "end": "2177960"
  },
  {
    "text": "from that installment to the Real kubernetes Environment yeah so that dpu",
    "start": "2177960",
    "end": "2184960"
  },
  {
    "text": "itself uh you know it's a nick it has a CPU on it it has its own OS um so the",
    "start": "2184960",
    "end": "2190240"
  },
  {
    "text": "dpu is always always booted it runs its own ubun to arm CPU um okay I'm supposed",
    "start": "2190240",
    "end": "2195800"
  },
  {
    "text": "to stop now I'm going to talk for one one 30 more second has its own arm CPU so dpu is always running uh and then",
    "start": "2195800",
    "end": "2202720"
  },
  {
    "text": "when the node boots uh the node has nothing on it and it sends a pxe it's",
    "start": "2202720",
    "end": "2207800"
  },
  {
    "text": "pxe boots from the dpu so there's a pxe server on the dpu which serves the image to the node and when a node boots is",
    "start": "2207800",
    "end": "2213400"
  },
  {
    "text": "there's no installation really the node boots stateless an image ser from the dpu over EET so you know talks from the",
    "start": "2213400",
    "end": "2219720"
  },
  {
    "text": "server up to the dpu over EET and then a node boots an ubun image in Ram so the",
    "start": "2219720",
    "end": "2225240"
  },
  {
    "text": "dpu acts to serve the image to the node did that answer your question uh",
    "start": "2225240",
    "end": "2230800"
  },
  {
    "text": "yeah that that's the uh yeah I know that but what you transfer to the C the network that will be used by the",
    "start": "2230800",
    "end": "2238079"
  },
  {
    "text": "customer so that then how that's kind of management Network right um so the the",
    "start": "2238079",
    "end": "2244599"
  },
  {
    "text": "dpu itself has an EET port the connects to the management Network and then the the dpu serves the customer towards a",
    "start": "2244599",
    "end": "2251319"
  },
  {
    "text": "node it exposes the customer's V VPC which is an evpn VPC so the customer's",
    "start": "2251319",
    "end": "2256440"
  },
  {
    "text": "node when his boots is always inside the customer's VPC and that's why that's why the dpu is the one who serves the boot",
    "start": "2256440",
    "end": "2262640"
  },
  {
    "text": "image because inside a customer's VPC there's it can't reach anywhere where the boot image is so the dpu does all",
    "start": "2262640",
    "end": "2268240"
  },
  {
    "text": "the network magic to make the node think you know it's talking to this private Network um but it can still load in",
    "start": "2268240",
    "end": "2274720"
  },
  {
    "text": "stuff like images and so on so I have to stop now so we can do questions like around here",
    "start": "2274720",
    "end": "2282200"
  }
]