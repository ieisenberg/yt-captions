[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "good evening everyone uh my name is Vijay Samuel uh I am a principal architect at eBay and uh I help build",
    "start": "160",
    "end": "8000"
  },
  {
    "text": "the observability platform um you might notice that my co-speaker is missing uh",
    "start": "8000",
    "end": "14120"
  },
  {
    "text": "aishwaria just had her baby boy and uh she's back in the US right now but uh",
    "start": "14120",
    "end": "20420"
  },
  {
    "text": "[Applause] yes congratulations Aishwarya uh we did",
    "start": "20420",
    "end": "26599"
  },
  {
    "text": "manage to get a recording of her portion of the talk uh which play when when those slides come uh we've been uh using",
    "start": "26599",
    "end": "34760"
  },
  {
    "text": "open Telemetry collector and in the past we have talked about uh metrics in previous ccon uh this time we're going",
    "start": "34760",
    "end": "41200"
  },
  {
    "text": "to talk about tracing um how we have been playing around with various uh configurations of the open telary",
    "start": "41200",
    "end": "47960"
  },
  {
    "text": "collector um and uh where we are at right now uh so various configurations",
    "start": "47960",
    "end": "54079"
  },
  {
    "text": "and the conclusions that we were able to make while fine-tuning open Telemetry collector for tracing that said uh uh",
    "start": "54079",
    "end": "61120"
  },
  {
    "text": "we'll do an introduction of what tracing is uh how uh it looks like inside of eBay uh what's the problem we faced uh",
    "start": "61120",
    "end": "68799"
  },
  {
    "text": "how we soled some of the lessons that we learned along the way and if time permits let's do some",
    "start": "68799",
    "end": "76880"
  },
  {
    "start": "78000",
    "end": "121000"
  },
  {
    "text": "questions so what is what is tracing uh open Telemetry the the website defines",
    "start": "78560",
    "end": "84520"
  },
  {
    "text": "tracing as uh traces give us the big picture of what happens when a request is made to an application whether your",
    "start": "84520",
    "end": "91520"
  },
  {
    "text": "application is a monolith with a single database or a sophisticated mesh of services traces are essential to",
    "start": "91520",
    "end": "98159"
  },
  {
    "text": "understanding the full path request takes in your application um so why is it important inside of eBay we have um",
    "start": "98159",
    "end": "105920"
  },
  {
    "text": "call chains that can have like tens of databases uh several microservices to",
    "start": "105920",
    "end": "111840"
  },
  {
    "text": "the point where knowing what exactly the customer saw when uh a request was being",
    "start": "111840",
    "end": "117399"
  },
  {
    "text": "served uh tracing is very critical for that what is our scale we have a 14-day",
    "start": "117399",
    "end": "125079"
  },
  {
    "start": "121000",
    "end": "142000"
  },
  {
    "text": "retention um with uh 190 billion spans that we process every day uh which",
    "start": "125079",
    "end": "130840"
  },
  {
    "text": "roughly translates to 2.2 million per second uh which is served by 3.6k",
    "start": "130840",
    "end": "136120"
  },
  {
    "text": "provision computes and uh 150 terabyt in U uh",
    "start": "136120",
    "end": "141400"
  },
  {
    "text": "storage and from an architecture perspective uh every kubernetes cluster",
    "start": "141400",
    "end": "146720"
  },
  {
    "start": "142000",
    "end": "228000"
  },
  {
    "text": "has three kinds of applications that run uh some of them are what we like to call generic applications um so these are",
    "start": "146720",
    "end": "154319"
  },
  {
    "text": "typically I wrote a program in an arbitrary language uh it it doesn't I just deploy",
    "start": "154319",
    "end": "162159"
  },
  {
    "text": "it on kubernetes um uh it's basically free flow uh on the other hand we have",
    "start": "162159",
    "end": "167760"
  },
  {
    "text": "managed uh applications that are there uh managed meaning they have full support in terms of uh developer life",
    "start": "167760",
    "end": "174200"
  },
  {
    "text": "cycle and whatnot uh and they have uh sdks into which we bundle the open",
    "start": "174200",
    "end": "179959"
  },
  {
    "text": "Elementary SDK so uh generic applications would need to bundle it themselves they need",
    "start": "179959",
    "end": "185120"
  },
  {
    "text": "to handroll instrumentation whereas managed uh the instrumentation comes for free they write their um spans into open",
    "start": "185120",
    "end": "192280"
  },
  {
    "text": "Telemetry collectors and from there we have an ingest uh layer which receives",
    "start": "192280",
    "end": "197879"
  },
  {
    "text": "in uh OTP uh writes it into our tray store um and whatever metric that we are",
    "start": "197879",
    "end": "204280"
  },
  {
    "text": "generating out of the spans it goes into our metric platform um and we have the",
    "start": "204280",
    "end": "211599"
  },
  {
    "text": "shocko console shocko is what we call our absor platform inside of eBay um",
    "start": "211599",
    "end": "217159"
  },
  {
    "text": "there is a trace quer uh which uses the AER protocol uh and a metrix sarer which",
    "start": "217159",
    "end": "223000"
  },
  {
    "text": "uses the um um the prom semantics what is the problem that we",
    "start": "223000",
    "end": "229920"
  },
  {
    "start": "228000",
    "end": "308000"
  },
  {
    "text": "faced let's be honest tracing is uh is not is not easy it's it's quite hard um",
    "start": "229920",
    "end": "236360"
  },
  {
    "text": "tracing can become very expensive depending on the the amount of HTTP requests that you're serving on a",
    "start": "236360",
    "end": "242159"
  },
  {
    "text": "day-to-day basis organic growth means that you're going to spend a lot more uh every span has um an arbitrary number of",
    "start": "242159",
    "end": "250159"
  },
  {
    "text": "uh span metadata attributes that are going to be there there are span events people can abuse it like a logger uh",
    "start": "250159",
    "end": "257320"
  },
  {
    "text": "people want 100% sampling um like to give a to give a analogy we have a an",
    "start": "257320",
    "end": "264240"
  },
  {
    "text": "internal um logging platform which is very similar to tracing uh which generates 15 p by a day uh with 100%",
    "start": "264240",
    "end": "272000"
  },
  {
    "text": "samples and this is without any of the span metadata uh type overhead that uh",
    "start": "272000",
    "end": "277520"
  },
  {
    "text": "we have with the more modern version of tracing that we are using there's a high barrier of entry for end users uh every",
    "start": "277520",
    "end": "284240"
  },
  {
    "text": "app needs to be uh uh instrumented in the call chain and context propagation",
    "start": "284240",
    "end": "289400"
  },
  {
    "text": "needs to be done right um if not you're not going to see the complete structure",
    "start": "289400",
    "end": "294440"
  },
  {
    "text": "which means that if there was one collector that dropped one span it can it can uh it can lead you uh through a",
    "start": "294440",
    "end": "302520"
  },
  {
    "text": "wild goose chase if you land on that exact span or that exact",
    "start": "302520",
    "end": "308000"
  },
  {
    "start": "308000",
    "end": "379000"
  },
  {
    "text": "race open Telemetry is not that easy either uh especially with the collector",
    "start": "308000",
    "end": "313960"
  },
  {
    "text": "there's a very famous Lego analogy that's there there are many building blocks to the collector in terms of",
    "start": "313960",
    "end": "320039"
  },
  {
    "text": "receivers processors exporters you can put them together in any order it",
    "start": "320039",
    "end": "325560"
  },
  {
    "text": "doesn't really uh limit you uh so you can either create something that's really beautiful or you can create a",
    "start": "325560",
    "end": "331759"
  },
  {
    "text": "monstrosity so and you have the ability to have one tier of collectors two tier",
    "start": "331759",
    "end": "336840"
  },
  {
    "text": "end tier where should the kubernetes enrichment uh in which state should it be there are so many things that um are",
    "start": "336840",
    "end": "344560"
  },
  {
    "text": "left to the uh implementer and uh",
    "start": "344560",
    "end": "351080"
  },
  {
    "text": "the the there are challenges adopting as well in the sense that uh tracing is",
    "start": "351080",
    "end": "356440"
  },
  {
    "text": "relatively new there are many people who have talked about their reference",
    "start": "356440",
    "end": "361639"
  },
  {
    "text": "[Music]",
    "start": "364980",
    "end": "368209"
  },
  {
    "start": "379000",
    "end": "493000"
  },
  {
    "text": "we saw with the tail sampler is that all the spans for a given Trace ID U always",
    "start": "419680",
    "end": "425319"
  },
  {
    "text": "need to land on the same uh same open elmary collector for",
    "start": "425319",
    "end": "430680"
  },
  {
    "text": "it to be able to hold it in memory make the decision to sample or not sample and then flush it out so that being said we",
    "start": "430680",
    "end": "438280"
  },
  {
    "text": "have applications that uh right to the first year of open Elementary collector where we were doing uh span metrics and",
    "start": "438280",
    "end": "444800"
  },
  {
    "text": "uh communties enrichment span metrics go into the metric store uh and uh the raw Trace goes",
    "start": "444800",
    "end": "452400"
  },
  {
    "text": "directly in into the tray store but we also write a copy of that into the second tier where you do the tail",
    "start": "452400",
    "end": "460560"
  },
  {
    "text": "sampling and then you ship it into the tray store finally finally we also",
    "start": "460560",
    "end": "466960"
  },
  {
    "text": "tried another where we moved the span metric processor also into its own tier",
    "start": "466960",
    "end": "472360"
  },
  {
    "text": "so on the first tier we just have the cun enrichment that's there um we on the",
    "start": "472360",
    "end": "478360"
  },
  {
    "text": "second year there the there's a there's a path dedicated for metrics that are",
    "start": "478360",
    "end": "483759"
  },
  {
    "text": "coming from the span metric processor there's a dedicated path for tail sampling for the traces and the raw",
    "start": "483759",
    "end": "489280"
  },
  {
    "text": "traces get written into the trace store as well so how did we go about solving",
    "start": "489280",
    "end": "495879"
  },
  {
    "start": "493000",
    "end": "519000"
  },
  {
    "text": "this uh there are multiple things that we did the first one being we solved for Mass",
    "start": "495879",
    "end": "501199"
  },
  {
    "text": "adoption we did uh pipeline Performance Tuning um we did some scaling uh related",
    "start": "501199",
    "end": "508159"
  },
  {
    "text": "uh tweaks um we optimized on storage and then we went for sampling um so the next",
    "start": "508159",
    "end": "514839"
  },
  {
    "text": "few slides we're going to talk about each one of them Mass adoption the F the first thing",
    "start": "514839",
    "end": "521360"
  },
  {
    "start": "519000",
    "end": "634000"
  },
  {
    "text": "we did was we wanted to go for the highest common denominator U during the architecture I described our manage",
    "start": "521360",
    "end": "527959"
  },
  {
    "text": "framework 90% of all the applications that are deployed inside of eBay use uh",
    "start": "527959",
    "end": "533360"
  },
  {
    "text": "the manage framework uh which means that we can uh ship instrumentation through",
    "start": "533360",
    "end": "538560"
  },
  {
    "text": "the manage framework um and immediately you get 90% adoption of tracing inside the company and we",
    "start": "538560",
    "end": "545040"
  },
  {
    "text": "have this concept of a monthly mandatory site R upgrade where every application that's on the manage framework",
    "start": "545040",
    "end": "550920"
  },
  {
    "text": "automatically gets upgraded to the most recent versions of their dependencies",
    "start": "550920",
    "end": "556079"
  },
  {
    "text": "which means that any instrumentation updates that we want to do um there is a one month uh turnaround time to get it",
    "start": "556079",
    "end": "562200"
  },
  {
    "text": "into all the applications and this is around 8,000 applications that we that we are talking about um we we are",
    "start": "562200",
    "end": "569800"
  },
  {
    "text": "working on getting uh uh tracing support on our service mesh we deploy sto internally",
    "start": "569800",
    "end": "575360"
  },
  {
    "text": "which means that if the mesh also uh emits pans any application that does not",
    "start": "575360",
    "end": "580959"
  },
  {
    "text": "have uh tracing uh instrumentation on it the bare minimum you would at least be",
    "start": "580959",
    "end": "586880"
  },
  {
    "text": "able to get the client and server uh s spans through the onvo side",
    "start": "586880",
    "end": "592760"
  },
  {
    "text": "car uh and finally we wanted to make tracing instrumentation optional um so",
    "start": "592760",
    "end": "599240"
  },
  {
    "text": "and end of the day if no developer knew how to instrument spans inside the company um at least I would consider",
    "start": "599240",
    "end": "605360"
  },
  {
    "text": "that to be a win U developers what they would need to do is just use in case of java just use the slf 4J do logger.log",
    "start": "605360",
    "end": "612399"
  },
  {
    "text": "under the hood the open tary SDK would make sure that uh Trace IDs and span IDs",
    "start": "612399",
    "end": "618800"
  },
  {
    "text": "are being tagged to each of these uh log lines and they should just be able to",
    "start": "618800",
    "end": "624160"
  },
  {
    "text": "use uh those to get the get the logs they don't necessarily need to instrument spans",
    "start": "624160",
    "end": "630000"
  },
  {
    "text": "um uh because the framework is already taking care of that so within the managed framework um",
    "start": "630000",
    "end": "637519"
  },
  {
    "start": "634000",
    "end": "703000"
  },
  {
    "text": "what we uh did is that we we shipped in the open uh Telemetry SDK and we made",
    "start": "637519",
    "end": "642839"
  },
  {
    "text": "sure that there is uh instrumentation for all the client calls all the",
    "start": "642839",
    "end": "648959"
  },
  {
    "text": "uh server apis that are there for database calls like uh depending on the",
    "start": "648959",
    "end": "654519"
  },
  {
    "text": "database some of them have instrumentation already we are working with the database providers internally to make sure that the remainder is",
    "start": "654519",
    "end": "660399"
  },
  {
    "text": "covered and we have instrumentation for IOS as well Android coming coming in the future um we head sample at 2% by",
    "start": "660399",
    "end": "667800"
  },
  {
    "text": "default um and basically what this allows us is that um we can do what is",
    "start": "667800",
    "end": "673920"
  },
  {
    "text": "the three click RCA ASX on the uh uh three click because it's a little more",
    "start": "673920",
    "end": "679720"
  },
  {
    "text": "but the idea is for a person to be able to do an alert to metric to trace uh and",
    "start": "679720",
    "end": "687639"
  },
  {
    "text": "then decide if they want to go to log or profile uh later and exemplars play a big role on in all of this uh and for",
    "start": "687639",
    "end": "695639"
  },
  {
    "text": "all of this we make sure that the red metrics are computed for for uh uh every kind of span that's",
    "start": "695639",
    "end": "703320"
  },
  {
    "start": "703000",
    "end": "729000"
  },
  {
    "text": "there the the next part is pipeline Performance Tuning um uh with",
    "start": "703320",
    "end": "708880"
  },
  {
    "text": "performance numbers are are everything uh we broke down the pipeline into uh",
    "start": "708880",
    "end": "714720"
  },
  {
    "text": "independent chunks uh we evaluated how the numbers look like Tred different combinations rinse and repeat to the",
    "start": "714720",
    "end": "721320"
  },
  {
    "text": "point where we can uh figure out how to place these Legos in a way that that",
    "start": "721320",
    "end": "726600"
  },
  {
    "text": "that works for us so that being said the first thing that we did uh OTP in OTP out um memory",
    "start": "726600",
    "end": "734720"
  },
  {
    "start": "729000",
    "end": "953000"
  },
  {
    "text": "limiter and batch uh which we require anyways identify how much uh uh",
    "start": "734720",
    "end": "740199"
  },
  {
    "text": "throughput we were able to get out of that the memory limiter and batch processors are very low over Asis um",
    "start": "740199",
    "end": "746639"
  },
  {
    "text": "require some memory uh but not not too much much uh and they more or less don't",
    "start": "746639",
    "end": "752240"
  },
  {
    "text": "impact your your throughput the next thing we tried adding the span metric processor uh to",
    "start": "752240",
    "end": "759279"
  },
  {
    "text": "see uh how the throughput looks like um it's it's it's it's fine for a single",
    "start": "759279",
    "end": "765720"
  },
  {
    "text": "instance um but for uh multiple instances we'll we'll get to it in a couple of slides the the next one is",
    "start": "765720",
    "end": "773000"
  },
  {
    "text": "basically the the kubernetes U uh metadata enrichment there's an interesting one on on a single instance",
    "start": "773000",
    "end": "779600"
  },
  {
    "text": "we saw that pulling the entire clusters worth of part metadata along with some namespace metadata uh had a had a 5gb",
    "start": "779600",
    "end": "787560"
  },
  {
    "text": "overhead uh but when we add multiple in instances even though uh we were",
    "start": "787560",
    "end": "794320"
  },
  {
    "text": "shedding the uh the number of spans evenly across various instances the",
    "start": "794320",
    "end": "799360"
  },
  {
    "text": "overhead of each kuet is U uh uh enrichment processor Remains the Same",
    "start": "799360",
    "end": "804880"
  },
  {
    "text": "because every instance has to anticipate every pods uh data that's span spans",
    "start": "804880",
    "end": "811040"
  },
  {
    "text": "that are coming in so it has to hold all the uh span pod metadata in memory so",
    "start": "811040",
    "end": "817360"
  },
  {
    "text": "across each one of them you basically see a standard memory overhead that's there uh so the final configuration that",
    "start": "817360",
    "end": "824720"
  },
  {
    "text": "we that we ended up with um the first stage only does the uh kubernetes",
    "start": "824720",
    "end": "831000"
  },
  {
    "text": "enrichment uh it does an OTP exporter into the second tier where we only do uh",
    "start": "831000",
    "end": "838279"
  },
  {
    "text": "uh spanm span metric processing we completely dropped the idea of doing the",
    "start": "838279",
    "end": "843600"
  },
  {
    "text": "tail sampling because the way we see traffic inside the company it doesn't really make sense to do tail sampling on",
    "start": "843600",
    "end": "849880"
  },
  {
    "text": "the uh open Elementary collector uh and the simple reason being that there is always going to be the case where a",
    "start": "849880",
    "end": "856560"
  },
  {
    "text": "request flows from one region to another uh which would mean that we would have to load balance all span traffic across",
    "start": "856560",
    "end": "863600"
  },
  {
    "text": "regions to make sure that the entire requests spans uh are being processed by a single uh open Telemetry collector and",
    "start": "863600",
    "end": "870680"
  },
  {
    "text": "this was um simply not feasible for us and depending on the kind of application",
    "start": "870680",
    "end": "876639"
  },
  {
    "text": "you would have to hold the SP spans for a longer window uh which means that you",
    "start": "876639",
    "end": "882519"
  },
  {
    "text": "need more memory so we went for an entirely different uh tail sampling uh",
    "start": "882519",
    "end": "888120"
  },
  {
    "text": "methodology which aishwaria will will talk about uh and on even on the load balancing exporter aishwara helped add a",
    "start": "888120",
    "end": "895480"
  },
  {
    "text": "feature uh to load balance based on U service name instead of Trace ID because",
    "start": "895480",
    "end": "900720"
  },
  {
    "text": "if you were doing the uh service name uh sorry the trace ID based load balancing",
    "start": "900720",
    "end": "906639"
  },
  {
    "text": "what's B what's going to happen is that every pod in the second TI will need to",
    "start": "906639",
    "end": "911800"
  },
  {
    "text": "uh see every kind of uh span metad data combination but uh but on the other hand",
    "start": "911800",
    "end": "917079"
  },
  {
    "text": "if you do just by service name all met metadata combinations for a single",
    "start": "917079",
    "end": "923160"
  },
  {
    "text": "service is always going to be collocated on a single collector which means that the memory requirements are",
    "start": "923160",
    "end": "929079"
  },
  {
    "text": "substantially reduced it's like everyone needs to know everything versus a collector needs to know about only a",
    "start": "929079",
    "end": "935000"
  },
  {
    "text": "given service so um that change on the open Telemetry collector also made things quite efficient so if you are",
    "start": "935000",
    "end": "941560"
  },
  {
    "text": "doing a two-tier approach where the second tier is going to do span metrics it's better to do the service name based",
    "start": "941560",
    "end": "947600"
  },
  {
    "text": "uh uh load balancing rather than the trace ID based uh load",
    "start": "947600",
    "end": "953000"
  },
  {
    "start": "953000",
    "end": "1020000"
  },
  {
    "text": "balancing on on scaling we did uh two things uh the first thing is uh we",
    "start": "953000",
    "end": "960000"
  },
  {
    "text": "started leveraging kada uh because uh like I said logging and tracing have very seasonal through throughput um um",
    "start": "960000",
    "end": "970160"
  },
  {
    "text": "it's a so at least for us during the holiday period or during weekends when there is more shopping that's happening",
    "start": "970160",
    "end": "976440"
  },
  {
    "text": "you're going to see more volume which means that uh doing uh Auto scaling can can greatly uh improve our ability to",
    "start": "976440",
    "end": "983399"
  },
  {
    "text": "handle higher traffic without having sustained uh uh deployments of large size installations all the time",
    "start": "983399",
    "end": "989720"
  },
  {
    "text": "and the second second one uh second thing that we did is to make sure that we are not using the kubernetes service",
    "start": "989720",
    "end": "996240"
  },
  {
    "text": "object to do the load balancing because the weighted round robin approach that it uses um um for grpc requests it is",
    "start": "996240",
    "end": "1005279"
  },
  {
    "text": "not very efficient and ends up uh sending the open Elementary collector pods out of memory um so uh we basically",
    "start": "1005279",
    "end": "1012720"
  },
  {
    "text": "use the mesh uh which can handle grpc load balancing a lot better for that",
    "start": "1012720",
    "end": "1020440"
  },
  {
    "start": "1020000",
    "end": "1086000"
  },
  {
    "text": "with regards to storage um from a consumption perspective we uh implement the Jager apis um uh because that's more",
    "start": "1020440",
    "end": "1027798"
  },
  {
    "text": "or less a standard right now and we use uh click house for uh for storage uh",
    "start": "1027799",
    "end": "1033400"
  },
  {
    "text": "click house is cheap it's fast uh it really it works really well for us but the one improvisation that we do is that",
    "start": "1033400",
    "end": "1040280"
  },
  {
    "text": "we don't have a single large click house cluster but we have smaller click house clusters that are backed by a ring based",
    "start": "1040280",
    "end": "1047558"
  },
  {
    "text": "Discovery there's a there there's a routing table that we maintain saying that this service name needs to be routed into this particular kubernetes",
    "start": "1047559",
    "end": "1055400"
  },
  {
    "text": "cluster and uh our in just apis they can basically use that to make decisions on",
    "start": "1055400",
    "end": "1060720"
  },
  {
    "text": "where to write the data into and we also make sure that you don't need to do a",
    "start": "1060720",
    "end": "1067200"
  },
  {
    "text": "global scatter gather on all the queries by implementing a global index so every",
    "start": "1067200",
    "end": "1073440"
  },
  {
    "text": "Trace ID um um we know exactly which are the shards on on each of the Clusters uh",
    "start": "1073440",
    "end": "1081320"
  },
  {
    "text": "to basically query to get the data back rather than trying to query all of them this is how our storage architect",
    "start": "1081320",
    "end": "1089440"
  },
  {
    "start": "1086000",
    "end": "1162000"
  },
  {
    "text": "architecture looks like um we have a traces table which has the standard uh",
    "start": "1089440",
    "end": "1097000"
  },
  {
    "text": "um uh parameters that a given um uh span uh requires we supplement that with a",
    "start": "1097000",
    "end": "1103400"
  },
  {
    "text": "bunch of materialized views uh one for service operation uh which has the service name and operation name we have",
    "start": "1103400",
    "end": "1110240"
  },
  {
    "text": "the trace ID and service name in another materialized view and we have an index saying that this is the index ID for",
    "start": "1110240",
    "end": "1117880"
  },
  {
    "text": "this service name plus uh resource attribute combination and this is basically what uh a combination of all",
    "start": "1117880",
    "end": "1124960"
  },
  {
    "text": "three is what we basically use uh to make routing decisions and we also have a sampled uh um uh uh a trace sample",
    "start": "1124960",
    "end": "1135600"
  },
  {
    "text": "table where all the sample traces which are retained longer are stored both these tables and materialized views",
    "start": "1135600",
    "end": "1142080"
  },
  {
    "text": "have different retention we we retain the rot races for 2 hours uh because we believe that um a usual site triaging",
    "start": "1142080",
    "end": "1150400"
  },
  {
    "text": "event should not require more than 2 hours worth of uh uh spans and the",
    "start": "1150400",
    "end": "1155919"
  },
  {
    "text": "sample table is what we retain for uh 14 days after after the 2 hours has",
    "start": "1155919",
    "end": "1162480"
  },
  {
    "start": "1162000",
    "end": "1533000"
  },
  {
    "text": "expired um let me move to awaria",
    "start": "1162480",
    "end": "1170760"
  },
  {
    "text": "hello everyone uh this is Aishwarya I'm working as a senior software engineer for absorbability platform at eBay and",
    "start": "1173919",
    "end": "1181400"
  },
  {
    "text": "I've been working on the tracing project now coming to the sampling part of tracing we have a lot of applications at",
    "start": "1181400",
    "end": "1188039"
  },
  {
    "text": "eBay that send traces and there is a huge amount of data to process and store on the tracing platform so with that we",
    "start": "1188039",
    "end": "1195600"
  },
  {
    "text": "are raised with two fundamental questions these every Trace we really useful if not do we really have a room",
    "start": "1195600",
    "end": "1201799"
  },
  {
    "text": "to be more efficient while processing and storing data on the tracing platform",
    "start": "1201799",
    "end": "1206919"
  },
  {
    "text": "now moving on to the next slide um if you see this diagram uh this shows the high level representation of how tracing",
    "start": "1206919",
    "end": "1213919"
  },
  {
    "text": "data looks like so we have a lot of traces that ended up without any issues",
    "start": "1213919",
    "end": "1218960"
  },
  {
    "text": "and we have a small set of traces that ended up with high latency and also a small set of traces that ended up with",
    "start": "1218960",
    "end": "1225400"
  },
  {
    "text": "errors so we don't really need to store all this data on the platform so to be",
    "start": "1225400",
    "end": "1230440"
  },
  {
    "text": "more efficient and effective so um the ideal representative sample would be to",
    "start": "1230440",
    "end": "1236000"
  },
  {
    "text": "sample a small percentage of traces that ended up without any issues and we can",
    "start": "1236000",
    "end": "1241200"
  },
  {
    "text": "always sample 100% of uh we can always samp sample 100% of traces that ended up",
    "start": "1241200",
    "end": "1247679"
  },
  {
    "text": "with high latency and also um and also with errors if we are really interested",
    "start": "1247679",
    "end": "1253960"
  },
  {
    "text": "then we can also take samples of traces with specific attributes that we are really interested Ed in so uh with that",
    "start": "1253960",
    "end": "1261159"
  },
  {
    "text": "we can be more efficient on this on on on the platform side now moving on to",
    "start": "1261159",
    "end": "1266360"
  },
  {
    "text": "the next slide um yeah we have uh uh we have we we have adapted two types of",
    "start": "1266360",
    "end": "1272000"
  },
  {
    "text": "sampling mechanisms so the first one is SDK based parent uh head sampling uh so",
    "start": "1272000",
    "end": "1278080"
  },
  {
    "text": "we have um uh so we so when applications generate traces so even before sending",
    "start": "1278080",
    "end": "1284400"
  },
  {
    "text": "those traces to the platform they're sampled at 2% uh by default by the",
    "start": "1284400",
    "end": "1289440"
  },
  {
    "text": "framework so the type of sampling that we are using here is parent Tracer based so that means when a root level",
    "start": "1289440",
    "end": "1295880"
  },
  {
    "text": "generates a trace ID uh root roote service decides whether to sample a",
    "start": "1295880",
    "end": "1301159"
  },
  {
    "text": "trace ID or not so when the decision is made the decision is passed on to all the services in the call chain and all",
    "start": "1301159",
    "end": "1308520"
  },
  {
    "text": "those Services obey the decision sent by the parent whether to sample a trace ID or not so um uh with that we also",
    "start": "1308520",
    "end": "1316600"
  },
  {
    "text": "generate exemplars so exemplars generate Trace IDs that are only sampled so",
    "start": "1316600",
    "end": "1321679"
  },
  {
    "text": "basically uh we attach these exemplars to a latency metric that is present on",
    "start": "1321679",
    "end": "1327039"
  },
  {
    "text": "all the applications so this latency metric um has a dimension uh uh has a",
    "start": "1327039",
    "end": "1333960"
  },
  {
    "text": "latency Dimension bucket and we add exemplars to each Dimension bucket so",
    "start": "1333960",
    "end": "1339240"
  },
  {
    "text": "when there is any issue it's real it's uh it's easy to debug because we have at least one examplar present and we can",
    "start": "1339240",
    "end": "1346159"
  },
  {
    "text": "easily jump from Matrix to to see the end to end distribu to see",
    "start": "1346159",
    "end": "1351360"
  },
  {
    "text": "end to end call chain and see where where the issue really exists moving on to the next slide so the next sampling",
    "start": "1351360",
    "end": "1358320"
  },
  {
    "text": "technique that we adapted is Exemplar based tail sampling so when we receive traces sent by uh sent by applications",
    "start": "1358320",
    "end": "1365440"
  },
  {
    "text": "we store all the raw traces for 2 hours and then we apply Exemplar based tail",
    "start": "1365440",
    "end": "1370600"
  },
  {
    "text": "sampling uh so basically as I already told we record examplars for uh metrics",
    "start": "1370600",
    "end": "1375880"
  },
  {
    "text": "on the framework uh when when when a side is recorded as an Exemplar that means it's it's really important and has",
    "start": "1375880",
    "end": "1383159"
  },
  {
    "text": "to be preserved for longer duration of time so all those uh TR side is recorded",
    "start": "1383159",
    "end": "1388320"
  },
  {
    "text": "as exemplars are received um and then they they they sampled and stored for",
    "start": "1388320",
    "end": "1393480"
  },
  {
    "text": "longer duration of time so not only examplars exposed by uh framework we",
    "start": "1393480",
    "end": "1399640"
  },
  {
    "text": "also have we also have span metric processor running and all the traces that are received on um um received on",
    "start": "1399640",
    "end": "1408039"
  },
  {
    "text": "the platform form um use span metric connector to compute red metric meaning",
    "start": "1408039",
    "end": "1413600"
  },
  {
    "text": "request error and duration and we also have examplers attached to each and every red",
    "start": "1413600",
    "end": "1419400"
  },
  {
    "text": "metric so uh recently we also added a new metric called events total um which",
    "start": "1419400",
    "end": "1426120"
  },
  {
    "text": "which record um which which record metrics for error labels that are emitted as part of spans uh and since we",
    "start": "1426120",
    "end": "1433640"
  },
  {
    "text": "have examplars recorded for all these metrics uh so we collect all the tra side is from from from these examplars",
    "start": "1433640",
    "end": "1441520"
  },
  {
    "text": "and they are also stored for longer duration of time now apart from these two um these two um uh sampling",
    "start": "1441520",
    "end": "1449799"
  },
  {
    "text": "techniques we also have 1% of random sampling across all the traces that are",
    "start": "1449799",
    "end": "1454960"
  },
  {
    "text": "received on the platform now moving on to the next slide so coming to the tail",
    "start": "1454960",
    "end": "1460120"
  },
  {
    "text": "sampling architecture that we follow so we have Trace tracing Hotel collectors",
    "start": "1460120",
    "end": "1465279"
  },
  {
    "text": "and framework sends all the pans to the tracing Hotel collector and we also have span metric processor",
    "start": "1465279",
    "end": "1471159"
  },
  {
    "text": "running that generates red metrics and these metrics are sent to our metrics in just",
    "start": "1471159",
    "end": "1476919"
  },
  {
    "text": "Pipeline and this metric in just pipeline also send these metrics to metric aggregator so metric aggregator",
    "start": "1476919",
    "end": "1483799"
  },
  {
    "text": "during rollup it also preserves exemplars and they are not dropped um are not dropped in the pipeline so once",
    "start": "1483799",
    "end": "1490520"
  },
  {
    "text": "aggregator receives U all the metrics and aggregations are done so these metrics are again sent back to the tail",
    "start": "1490520",
    "end": "1497399"
  },
  {
    "text": "sampler in just that we have so this this tail sampler interest collects all the examplars and it it records all the",
    "start": "1497399",
    "end": "1504960"
  },
  {
    "text": "trace IDs and stores in in our click house table so we also have a tail",
    "start": "1504960",
    "end": "1510159"
  },
  {
    "text": "sampler job that is running so tail sampler jobs pick uh tail sampler job",
    "start": "1510159",
    "end": "1515279"
  },
  {
    "text": "picks the trace IDs that are needed to be sampled and it gets all the trace IDs",
    "start": "1515279",
    "end": "1520799"
  },
  {
    "text": "and it gets the tracing data from raw tables and and and and samples all the traces that are being exposed as an",
    "start": "1520799",
    "end": "1527720"
  },
  {
    "text": "examplar",
    "start": "1527720",
    "end": "1530360"
  },
  {
    "start": "1533000",
    "end": "1632000"
  },
  {
    "text": "um so uh couple of more things on that uh on the section that Aishwarya uh",
    "start": "1534360",
    "end": "1539520"
  },
  {
    "text": "talked about so exemplars are really important to us in the sense that um",
    "start": "1539520",
    "end": "1545799"
  },
  {
    "text": "think of latency metrics that are there you basically decide if you're not using",
    "start": "1545799",
    "end": "1551360"
  },
  {
    "text": "the new native hisob histograms you basically decide what are the buckets that U you're defining for your latency",
    "start": "1551360",
    "end": "1558559"
  },
  {
    "text": "and each one of them is going to sample um uh exemplars and then emit it into",
    "start": "1558559",
    "end": "1564480"
  },
  {
    "text": "your back end which basically means that if you take those examplars and you keep",
    "start": "1564480",
    "end": "1570000"
  },
  {
    "text": "uh sampling all of those Trace IDs uh to retain longer what you're essentially",
    "start": "1570000",
    "end": "1575559"
  },
  {
    "text": "going to get is that for every uh bucket that you defined you get sample traces",
    "start": "1575559",
    "end": "1583080"
  },
  {
    "text": "for every host that's emitting uh these metrics which means that when you sample",
    "start": "1583080",
    "end": "1588600"
  },
  {
    "text": "it's true it's almost a a representative sample for your entire uh uh ecosystem",
    "start": "1588600",
    "end": "1595120"
  },
  {
    "text": "and what isia did also is that she worked with the uh open elimary Comm",
    "start": "1595120",
    "end": "1600760"
  },
  {
    "text": "Community to make sure that U uh we are emitting uh events total as a metric",
    "start": "1600760",
    "end": "1607440"
  },
  {
    "text": "where if you record every event that has a unique exception type uh that also",
    "start": "1607440",
    "end": "1613000"
  },
  {
    "text": "means that okay every error scenario also you are making sure for every host every error type you have at least one",
    "start": "1613000",
    "end": "1620200"
  },
  {
    "text": "sample that you're retaining uh much longer this greatly improves uh our our",
    "start": "1620200",
    "end": "1626279"
  },
  {
    "text": "efficiency while at the same time not impairing the customer experience uh too much so what did we get out of all of",
    "start": "1626279",
    "end": "1633880"
  },
  {
    "start": "1632000",
    "end": "1679000"
  },
  {
    "text": "all of this one we got a cheaper and easier",
    "start": "1633880",
    "end": "1639080"
  },
  {
    "text": "platform to operate not having to retain all these pans for a greater window not",
    "start": "1639080",
    "end": "1645200"
  },
  {
    "text": "having to do complex uh sampling algorithms uh on the open Telemetry collector or",
    "start": "1645200",
    "end": "1651039"
  },
  {
    "text": "even after the fact when it has landed on storage uh now that we have",
    "start": "1651039",
    "end": "1658039"
  },
  {
    "text": "distributed tracing without any of the developers having to lift a finger we can create uh uh dependency graphs at",
    "start": "1658039",
    "end": "1665880"
  },
  {
    "text": "both per request level or at the uh service to service level and we have an",
    "start": "1665880",
    "end": "1671120"
  },
  {
    "text": "industry standard uh in terms of open Telemetry but at the same time we are gearing it uh heavily towards what eBay",
    "start": "1671120",
    "end": "1677519"
  },
  {
    "text": "actually needs what are the lessons that we learned the first thing is observability",
    "start": "1677519",
    "end": "1684559"
  },
  {
    "start": "1679000",
    "end": "1775000"
  },
  {
    "text": "is a team sport uh and we should use all the pillars effectively and not try to",
    "start": "1684559",
    "end": "1689600"
  },
  {
    "text": "say that okay use this one pillar and try to derive everything out of that um in our case you saw how closely knit the",
    "start": "1689600",
    "end": "1697399"
  },
  {
    "text": "metrix platform and the trace platform are uh um are being knit together because uh the metrics now provide the",
    "start": "1697399",
    "end": "1704720"
  },
  {
    "text": "signal on what needs to be sampled and what does not um and at the same time all our P1 absorbability or P1 detection",
    "start": "1704720",
    "end": "1712799"
  },
  {
    "text": "functions are are strictly uh done just by metrics we don't try to use the other",
    "start": "1712799",
    "end": "1718640"
  },
  {
    "text": "ones uh exemplars could offer a representative sample uh we just talked",
    "start": "1718640",
    "end": "1723679"
  },
  {
    "text": "about that um numbers really matter uh fine tuning took a lot of uh benchmarking",
    "start": "1723679",
    "end": "1732279"
  },
  {
    "text": "profiling uh and we finally were able to land uh at a pattern that was good uh",
    "start": "1732279",
    "end": "1738279"
  },
  {
    "text": "sometimes the community also offers some prescribed patterns it's better to just stick to them rather than trying to",
    "start": "1738279",
    "end": "1744720"
  },
  {
    "text": "improvise and finally no one should need uh 100% uh",
    "start": "1744720",
    "end": "1750360"
  },
  {
    "text": "sampling to do effective observability at our scale uh we do uh 2% sampling which is plenty enough for us to be able",
    "start": "1750360",
    "end": "1757360"
  },
  {
    "text": "to uh look at all kinds of issues uh because end of the day one in 50",
    "start": "1757360",
    "end": "1762679"
  },
  {
    "text": "requests need to have the issue that uh the customer is facing um and and with",
    "start": "1762679",
    "end": "1768840"
  },
  {
    "text": "the kind of call volumes that we have in a day uh 1 in 50 is fairly easy to",
    "start": "1768840",
    "end": "1774919"
  },
  {
    "text": "achieve um with that um I'll take",
    "start": "1774919",
    "end": "1781639"
  },
  {
    "start": "1775000",
    "end": "2218000"
  },
  {
    "text": "questions thank",
    "start": "1781679",
    "end": "1784880"
  },
  {
    "text": "[Applause]",
    "start": "1787620",
    "end": "1792200"
  },
  {
    "text": "you thank you very great talk um very familiar themes and very familiar",
    "start": "1794559",
    "end": "1801240"
  },
  {
    "text": "journey in in that maybe for other people I'll be interested um I've got some questions what first um the your",
    "start": "1801240",
    "end": "1808080"
  },
  {
    "text": "deployment model for The Collector it seem it sounds like you didn't uh specify but it sounds like you have a",
    "start": "1808080",
    "end": "1814080"
  },
  {
    "text": "central pool of replicas for your correct your gateway and you haven't got like side cars or agent per host or not",
    "start": "1814080",
    "end": "1824399"
  },
  {
    "text": "correct correct yeah and the reason that we do that is that um the there is a there is a severe",
    "start": "1824399",
    "end": "1831880"
  },
  {
    "text": "problem of resource fragmentation when we go down the route of either a demon set or a side car side car you also have",
    "start": "1831880",
    "end": "1838519"
  },
  {
    "text": "the problem that if we if we need to do upgrades uh then we'll have to restart",
    "start": "1838519",
    "end": "1844000"
  },
  {
    "text": "the parent container or we'll have to have the manage platform do a roll out across the site uh which is more than",
    "start": "1844000",
    "end": "1850240"
  },
  {
    "text": "8,000 applications like I mentioned so the the the cluster local gives us the",
    "start": "1850240",
    "end": "1855679"
  },
  {
    "text": "flexibility to do independently roll out we can have bigger pods uh for handling",
    "start": "1855679",
    "end": "1861080"
  },
  {
    "text": "more uh uh uh more throughput and we can scale it uh at will so that's why we do",
    "start": "1861080",
    "end": "1867519"
  },
  {
    "text": "that okay um also the you you didn't uh mention application metrics so you",
    "start": "1867519",
    "end": "1873559"
  },
  {
    "text": "mentioned span metrics but you're not collecting metrics with your collectors or logs either uh so logs we do the file",
    "start": "1873559",
    "end": "1882720"
  },
  {
    "text": "based uh approach it still uses the demon set pattern um we are slowly but surely",
    "start": "1882720",
    "end": "1888559"
  },
  {
    "text": "uh now that the logs API is stable uh trying to get uh people to adopt uh the",
    "start": "1888559",
    "end": "1894840"
  },
  {
    "text": "slf 4J based bridge for open telary sometime this year that should uh happen",
    "start": "1894840",
    "end": "1900000"
  },
  {
    "text": "for metrics um there is some free instrumentation similar to tracing where",
    "start": "1900000",
    "end": "1905519"
  },
  {
    "text": "the four golden signals are instrumented on a dedicated Prometheus endpoint and we scrape it for free for all the",
    "start": "1905519",
    "end": "1911200"
  },
  {
    "text": "applications that use manage stack and there also we emit exemplars for the um",
    "start": "1911200",
    "end": "1918240"
  },
  {
    "text": "uh latency metric as well um there are actually since we the manage framework",
    "start": "1918240",
    "end": "1924039"
  },
  {
    "text": "has a lot of framework providers as well like the database team or the messaging team and whatnot and each one of them",
    "start": "1924039",
    "end": "1930039"
  },
  {
    "text": "have their independent uh Prometheus endpoint which we scrape and then we ship it into into our metrix platform uh",
    "start": "1930039",
    "end": "1937080"
  },
  {
    "text": "and a lot of uh free observability is provided to them in terms of say curated dashboards curated alerts being able to",
    "start": "1937080",
    "end": "1944360"
  },
  {
    "text": "do healthware roll outs and things like that okay one more very quick one on your inest do you have any buffering or",
    "start": "1944360",
    "end": "1951840"
  },
  {
    "text": "asynchronous um read right buffering and storage before stor uh we only do the",
    "start": "1951840",
    "end": "1957840"
  },
  {
    "text": "batch uh batch processor nothing nothing more than that so everything is pushed straight through the pipeline to your storage oh yeah if if the question is",
    "start": "1957840",
    "end": "1964760"
  },
  {
    "text": "are we using something like Kafka uh no we we do not uh we just U uh receive it on the Gateway we pick a pick a click",
    "start": "1964760",
    "end": "1973080"
  },
  {
    "text": "house Shard to right and that Shard will also have a buffer table we push into that and then it periodically flushes",
    "start": "1973080",
    "end": "1979559"
  },
  {
    "text": "into storage thank you very",
    "start": "1979559",
    "end": "1983880"
  },
  {
    "text": "much hey thanks V it was a great talk thank you um I think my question is on",
    "start": "1985000",
    "end": "1991440"
  },
  {
    "text": "the application Level sampling that you did uh can you talk a bit about that",
    "start": "1991440",
    "end": "1996679"
  },
  {
    "text": "Miss uh how did you decide what to uh filter out because my my I'm coming from",
    "start": "1996679",
    "end": "2001960"
  },
  {
    "text": "point where you might be filtering out something which is a high latency Trace",
    "start": "2001960",
    "end": "2009039"
  },
  {
    "text": "uh so on the application as in the SDK based sampling yes we we just do head",
    "start": "2009039",
    "end": "2015399"
  },
  {
    "text": "sampling uh like probabilistic sampling at 2% um so basically it's as good as",
    "start": "2015399",
    "end": "2021240"
  },
  {
    "text": "flipping a coin and deciding if something needs to be sampled or not and we are okay to do that because when we",
    "start": "2021240",
    "end": "2027679"
  },
  {
    "text": "look at all our latency patterns or the number of errors that are being seen at on a given",
    "start": "2027679",
    "end": "2034519"
  },
  {
    "text": "application it's well over 50 per per per host which which generally means",
    "start": "2034519",
    "end": "2039840"
  },
  {
    "text": "that like a 2% sample um the lowest volumes should still see at least one uh",
    "start": "2039840",
    "end": "2046760"
  },
  {
    "text": "request that's coming coming into the platform so that being said like uh we just go go the go down that route uh one",
    "start": "2046760",
    "end": "2054320"
  },
  {
    "text": "followup last from my site so um how did you uh what made you decide to have a",
    "start": "2054320",
    "end": "2060358"
  },
  {
    "text": "trace store as click house uh versus directly using a load balancer and then",
    "start": "2060359",
    "end": "2065480"
  },
  {
    "text": "doing the tail sampling passing on to collector uh the the logic was the one that I",
    "start": "2065480",
    "end": "2072960"
  },
  {
    "text": "mentioned earlier uh in the sense that we have cross region traffic uh which",
    "start": "2072960",
    "end": "2078358"
  },
  {
    "text": "which necessarily means that like if you want to do a proper tail sampling on The Collector all the traffic that was seen",
    "start": "2078359",
    "end": "2085480"
  },
  {
    "text": "across regions needs to come into the same Trace collector and we try to make sure that we are not passing um log or",
    "start": "2085480",
    "end": "2093599"
  },
  {
    "text": "Trace information across regions unless we really have to uh and that being uh",
    "start": "2093599",
    "end": "2098680"
  },
  {
    "text": "and the other reason being that doing a true tail sampling you can always have a",
    "start": "2098680",
    "end": "2104079"
  },
  {
    "text": "5 minute or 10 minute offset or even a 1hour offset for something that's really",
    "start": "2104079",
    "end": "2109119"
  },
  {
    "text": "long for whatever reason to come in and then do the tail sampling decision but",
    "start": "2109119",
    "end": "2114200"
  },
  {
    "text": "we uh in the case of uh the tail sampling processor you at best can do",
    "start": "2114200",
    "end": "2119400"
  },
  {
    "text": "maybe 20 seconds 40 seconds and if your throughput is really high then you need a lot more memory to make sure that",
    "start": "2119400",
    "end": "2125800"
  },
  {
    "text": "you're really doing a good job in the tail",
    "start": "2125800",
    "end": "2129720"
  },
  {
    "text": "sampling uh thank you great talk um two questions about tail sampling the first",
    "start": "2131359",
    "end": "2137400"
  },
  {
    "text": "one is it based on open Telemetry or is it uh something that you implemented yourself second uh you mentioned it it's",
    "start": "2137400",
    "end": "2146079"
  },
  {
    "text": "a tell samping job do you mean that it's a crown job that how how frequently does",
    "start": "2146079",
    "end": "2151720"
  },
  {
    "text": "it run and and could you touch a little bit on how it is implemented uh so so the Exemplar based",
    "start": "2151720",
    "end": "2159079"
  },
  {
    "text": "tail sampling is something that we built inh house but uh it is something that is reasonably easy to uh Implement on your",
    "start": "2159079",
    "end": "2166480"
  },
  {
    "text": "own um all you would need to do is you have a processor that can B basically look at all the um the metric signals",
    "start": "2166480",
    "end": "2175440"
  },
  {
    "text": "and basically write them into uh something like a database table uh so that whenever there is an Exemplar it's",
    "start": "2175440",
    "end": "2182480"
  },
  {
    "text": "it's making note of that and the tail sampler like you mentioned is uh uh is just a Cron job that basically selects",
    "start": "2182480",
    "end": "2189240"
  },
  {
    "text": "that table that has the trace IDs and then uses that as the subquery to basically pull all the Ross panss into a",
    "start": "2189240",
    "end": "2196040"
  },
  {
    "text": "different uh table uh and that window can be configured so we do two parameters one is the offset after how",
    "start": "2196040",
    "end": "2202480"
  },
  {
    "text": "long it needs to kick off and what is the interval so typically we do 5 + 5 so",
    "start": "2202480",
    "end": "2208040"
  },
  {
    "text": "at the fifth minute we make sure that the 10th minute is being um uh sampled",
    "start": "2208040",
    "end": "2213200"
  },
  {
    "text": "so the same query that I talked about it just keeps getting applied for that time window",
    "start": "2213200",
    "end": "2219640"
  }
]