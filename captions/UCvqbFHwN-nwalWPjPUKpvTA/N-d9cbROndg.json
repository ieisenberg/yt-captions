[
  {
    "text": "hello everyone I'm abishek and with me is Aaron and today we are presenting AI store as a fasti storage solution to",
    "start": "640",
    "end": "8120"
  },
  {
    "text": "enhance your P skill deep learning jobs across remote Cloud backends uh so um let's look at in in",
    "start": "8120",
    "end": "16320"
  },
  {
    "text": "this presentation we're going to look at what are the current challenges that we are facing with loading data from the cloud introduce AI store and how AI",
    "start": "16320",
    "end": "23279"
  },
  {
    "text": "store addresses these challenges um uh share some production numbers and workloads with you and",
    "start": "23279",
    "end": "30400"
  },
  {
    "text": "finally dwell into the ongoing research so let's look at the challenges",
    "start": "30400",
    "end": "35879"
  },
  {
    "text": "um so at Nvidia our gpus are becoming insanely powerful but here's the kicker",
    "start": "35879",
    "end": "42239"
  },
  {
    "text": "the io performance isn't keeping up this mismatch means that the gpus are often",
    "start": "42239",
    "end": "47760"
  },
  {
    "text": "underutilized because they can process data at a faster rate than the data can be",
    "start": "47760",
    "end": "54199"
  },
  {
    "text": "delivered let's talk about data in the cloud storing a petabyte up there isn't just expensive it's incredibly slow and",
    "start": "54239",
    "end": "61800"
  },
  {
    "text": "if your gpus are in another region that's another cross layer and in such situations latency can be a real",
    "start": "61800",
    "end": "68360"
  },
  {
    "text": "bottleneck as well um GPU nodes usually don't have enough local storage so if you're",
    "start": "68360",
    "end": "74439"
  },
  {
    "text": "training for multiple epocs you might end up redownloading the same data again and",
    "start": "74439",
    "end": "80360"
  },
  {
    "text": "again if you're really unlucky all your data might be scattered some might be on",
    "start": "80360",
    "end": "85479"
  },
  {
    "text": "Prem some might be on Azure and some might be on AWS and trying to get a consist consistent throughput and",
    "start": "85479",
    "end": "91000"
  },
  {
    "text": "latency from this hod Podge is like juggling lives so training a training on a paby",
    "start": "91000",
    "end": "98280"
  },
  {
    "text": "of data is isn't just a walk in the park you need the right right tools libraries",
    "start": "98280",
    "end": "104399"
  },
  {
    "text": "plugins and operations to handle data at this massive scale without them you are just left wrestling with data instead of",
    "start": "104399",
    "end": "111320"
  },
  {
    "text": "training your models so from what we have observed in",
    "start": "111320",
    "end": "116520"
  },
  {
    "text": "our workloads is that um usually machine Lear learning workloads make random",
    "start": "116520",
    "end": "121960"
  },
  {
    "text": "reads to a vast amount of unstructured data and reading reading a paby of data randomly",
    "start": "121960",
    "end": "128959"
  },
  {
    "text": "without having a petabyte of Ram or any similar fast G cachier it's really difficult and slow and hence storage",
    "start": "128959",
    "end": "136760"
  },
  {
    "text": "becomes the primary bottleneck in contemporary machine learning so with all these challenges like storage",
    "start": "136760",
    "end": "142519"
  },
  {
    "text": "bottlenecks latencies and Cloud how do we keep our gpus fed and avoid costly",
    "start": "142519",
    "end": "149360"
  },
  {
    "text": "downtime let me introduce you to AI store so AI",
    "start": "149360",
    "end": "154959"
  },
  {
    "text": "store is a fully open source lightweight software storage uh object storage",
    "start": "154959",
    "end": "160360"
  },
  {
    "text": "system that is typically tailored for AI related workloads uh AI store offers linear",
    "start": "160360",
    "end": "166840"
  },
  {
    "text": "scalability with each added node or drive so if you're adding more storage nodes to your cluster you'll increase",
    "start": "166840",
    "end": "174239"
  },
  {
    "text": "your IO by that proportion in AI store you can easy",
    "start": "174239",
    "end": "179840"
  },
  {
    "text": "easily scale up and scale down your cluster at any time there is literally no",
    "start": "179840",
    "end": "184920"
  },
  {
    "text": "limitation uh you just need a machine with a dis uh AI store is Deployable",
    "start": "184920",
    "end": "190120"
  },
  {
    "text": "anywhere you can deploy it on a teeny tiny VM to multiat kubernetes",
    "start": "190120",
    "end": "196159"
  },
  {
    "text": "Cluster AI store is in development for over 7 years and we are also in",
    "start": "196159",
    "end": "201400"
  },
  {
    "text": "production for certain workloads at Nvidia so let's take at a take a look at",
    "start": "201400",
    "end": "208080"
  },
  {
    "text": "the overview of AI store at the Crux of the system we have two main components",
    "start": "208080",
    "end": "213239"
  },
  {
    "text": "AIS proxies and AIS targets AIS proxies are lightweight endpoints which the clients reach out to AIS targets are the",
    "start": "213239",
    "end": "221480"
  },
  {
    "text": "storage nodes which house the disk and where the data is stored",
    "start": "221480",
    "end": "227080"
  },
  {
    "text": "so AI store features an elastic cluster which can grow and Shrink at any time there is no limitation on the number of",
    "start": "227080",
    "end": "233599"
  },
  {
    "text": "proxies the number of targets or dis per Target all the targets use a generic",
    "start": "233599",
    "end": "240879"
  },
  {
    "text": "backend interface to connect to multiple different Cloud providers or to multiple different AI store",
    "start": "240879",
    "end": "246439"
  },
  {
    "text": "clusters you can connect to any S3 compatible Object Store as well and AI",
    "start": "246439",
    "end": "252560"
  },
  {
    "text": "store in itself is an S3 compatible Object Store you can also have multiple S3 profiles so that you can load data",
    "start": "252560",
    "end": "259680"
  },
  {
    "text": "from different S3 profiles in different on different S3 compatible object",
    "start": "259680",
    "end": "265440"
  },
  {
    "text": "stores to connect to AI store we have a lot of front-end apis we have our Native",
    "start": "265440",
    "end": "271240"
  },
  {
    "text": "Native uh https rest support on top of that we have built a go and python based",
    "start": "271240",
    "end": "277000"
  },
  {
    "text": "SDK which developers can easily integrate into their existing applications and workflows we also have",
    "start": "277000",
    "end": "283400"
  },
  {
    "text": "an easy to operate and set up AI CLI that you can use to Monitor and um",
    "start": "283400",
    "end": "289680"
  },
  {
    "text": "manage your cluster with and again AI store itself is an S3 compatible client",
    "start": "289680",
    "end": "295199"
  },
  {
    "text": "S3 compatible object store so you can use your any existing S3 compatible clients like boto 3 S3 CMD or AWS CLI to",
    "start": "295199",
    "end": "303520"
  },
  {
    "text": "connect to AI store without any learning curve this architecture was designed",
    "start": "303520",
    "end": "309440"
  },
  {
    "text": "full of flexibility and scalability in mind where you can easily add more nodes and scale up the infrastructure",
    "start": "309440",
    "end": "317319"
  },
  {
    "text": "easily let's take a look at some of the key features that make a special uh so",
    "start": "317960",
    "end": "324360"
  },
  {
    "text": "AI store is highly available and it has robust data protection we achiev this is",
    "start": "324360",
    "end": "329800"
  },
  {
    "text": "using nway mirroring Erasure coding selfhealing and life cycle",
    "start": "329800",
    "end": "335680"
  },
  {
    "text": "management another uh exciting feature of AI store is its capabilities to run",
    "start": "335680",
    "end": "341080"
  },
  {
    "text": "ETL jobs so you can run custom ETL data transformation jobs inline like when",
    "start": "341080",
    "end": "347360"
  },
  {
    "text": "you're making request to the server you can get transform data and also offline where you can transform the entire data",
    "start": "347360",
    "end": "353000"
  },
  {
    "text": "sets and store it on AI store for subsequent computation every storage node in AI",
    "start": "353000",
    "end": "358720"
  },
  {
    "text": "store is responsible for processing the data that resides on that node and hence if you add more storage nodes you are",
    "start": "358720",
    "end": "364880"
  },
  {
    "text": "increasing with that factor your entire data",
    "start": "364880",
    "end": "370199"
  },
  {
    "text": "processing AI store shows read after WR consistency and write through caching so",
    "start": "370240",
    "end": "375960"
  },
  {
    "text": "if your workload is writing to AI store other workflows can pick it up and have",
    "start": "375960",
    "end": "383160"
  },
  {
    "text": "the same data consistently uh we have seamless integration with kubernetes we have an",
    "start": "383160",
    "end": "389080"
  },
  {
    "text": "open source AIS operator that uh does essential tasks such as bootstrapping the a store cluster deploying it",
    "start": "389080",
    "end": "396440"
  },
  {
    "text": "managing it scaling up or even gracefully shutting down the as",
    "start": "396440",
    "end": "402160"
  },
  {
    "text": "cluster um a key problem with large scale model training and big data is",
    "start": "402560",
    "end": "408440"
  },
  {
    "text": "that small files so AI store handles small files using archives and we",
    "start": "408440",
    "end": "415080"
  },
  {
    "text": "support a lot of archive formats as you can see on the screen uh uh so we um we",
    "start": "415080",
    "end": "421639"
  },
  {
    "text": "like we do sharding of original data sets you can add data to the existing",
    "start": "421639",
    "end": "426960"
  },
  {
    "text": "sharts you can also rehard your data and you can without extracting the shards you can read the data which I think is a",
    "start": "426960",
    "end": "434800"
  },
  {
    "text": "very developer productive feature AI store has its own",
    "start": "434800",
    "end": "440720"
  },
  {
    "text": "authentication server which is O2 compliant where you can have role based Access Control you can customize",
    "start": "440720",
    "end": "446840"
  },
  {
    "text": "everything from clusters to uh Ro roles to users and assign users those",
    "start": "446840",
    "end": "452639"
  },
  {
    "text": "roles um so you can even register multiple different AI store clusters to one auen server and you can easily",
    "start": "452639",
    "end": "461319"
  },
  {
    "text": "secure and manage your data with auen so when you're dealing with large",
    "start": "461319",
    "end": "466440"
  },
  {
    "text": "amounts of data it becomes very difficult to like process data so for that we have batch jobs which can you",
    "start": "466440",
    "end": "473680"
  },
  {
    "text": "can easily start stop or monitor we perform a lot of batch operations such as prefetch copy download and transform",
    "start": "473680",
    "end": "481560"
  },
  {
    "text": "they run they run on like each storage node is responsible for playing their",
    "start": "481560",
    "end": "486639"
  },
  {
    "text": "part so it this operation is massively parallel and it scales up as you add",
    "start": "486639",
    "end": "492199"
  },
  {
    "text": "nodes so let me show you how you can get an object and how AI store intelligently",
    "start": "492199",
    "end": "497960"
  },
  {
    "text": "stores it on the target so imagine you have a cluster and you have clients that clients can be anything it can be your",
    "start": "497960",
    "end": "504599"
  },
  {
    "text": "machine learning workloads so if you reach out to a proxy asking for a",
    "start": "504599",
    "end": "509840"
  },
  {
    "text": "certain object on a store the proxies are intelligent systems they know exactly on which Target the data is",
    "start": "509840",
    "end": "516120"
  },
  {
    "text": "stored on or which Target is responsible for pulling the data from the cloud so in turn they send a redirect request to",
    "start": "516120",
    "end": "522518"
  },
  {
    "text": "the client telling the client where the actual data resides the client then Reach Out reaches out to the Target",
    "start": "522519",
    "end": "529000"
  },
  {
    "text": "requesting the data or the object if the object is not found locally as in on",
    "start": "529000",
    "end": "534200"
  },
  {
    "text": "that storage node the target will make a call to the remote backend and and store",
    "start": "534200",
    "end": "540000"
  },
  {
    "text": "the data on the disk and then return the same object back to the user or the client so how",
    "start": "540000",
    "end": "547399"
  },
  {
    "text": "here is where the efficiency of AI stor shines so all subsequent gets to the",
    "start": "547399",
    "end": "552800"
  },
  {
    "text": "same object from any client will be directly returned from the target",
    "start": "552800",
    "end": "558440"
  },
  {
    "text": "instead of making the remote call to the back end so this not only saves egest cost but also saves like increase like",
    "start": "558440",
    "end": "566360"
  },
  {
    "text": "this AI store cluster is usually in the same region as your GP nodes and that's why you get a better throughput and",
    "start": "566360",
    "end": "572320"
  },
  {
    "text": "latency for your subsequent reads in this image we're showing how",
    "start": "572320",
    "end": "580399"
  },
  {
    "text": "like AI store is deployed and and overall architecture so you have your compute nodes which might be a lot of",
    "start": "580399",
    "end": "586720"
  },
  {
    "text": "GPU nodes and you have your slow tier which is your remote backends like uh remote clouds on Prem object stores",
    "start": "586720",
    "end": "593959"
  },
  {
    "text": "something like that so AI store sits in the middle uh between the GPU nodes and",
    "start": "593959",
    "end": "599440"
  },
  {
    "text": "the slow uh slow supported backends and E store intelligently caches the data in",
    "start": "599440",
    "end": "605360"
  },
  {
    "text": "the first epoke so that subsequent oces can directly be returned from the fast",
    "start": "605360",
    "end": "610760"
  },
  {
    "text": "year so this multicluster multi Cloud environment helps you to unify all your",
    "start": "610760",
    "end": "617560"
  },
  {
    "text": "storage resources under one single cohesive",
    "start": "617560",
    "end": "622040"
  },
  {
    "text": "system finally let's take a look at how AI store is deployed onto kubernetes so",
    "start": "623040",
    "end": "628360"
  },
  {
    "text": "imagine you have a kubernetes is cluster with n nodes and you want to deploy an a cluster the very first thing that you",
    "start": "628360",
    "end": "634360"
  },
  {
    "text": "can do is deploy or set up the AIS operator so AIS operator is responsible",
    "start": "634360",
    "end": "640079"
  },
  {
    "text": "for a lot of essential things like boot straing the cluster uh scaling up the",
    "start": "640079",
    "end": "645519"
  },
  {
    "text": "cluster managing the entire life cycle of the cluster AI store operator in turn uses native kubernetes apis to manage",
    "start": "645519",
    "end": "652440"
  },
  {
    "text": "the entire life cycle of AI store with AIS operator we also deploy",
    "start": "652440",
    "end": "659639"
  },
  {
    "text": "an AI store custom resource definition which has all the logic to bring up the proxies and the targets so if once the",
    "start": "659639",
    "end": "667440"
  },
  {
    "text": "AI store operator is completely deployed and running you can now apply a specification of AI store crd which will",
    "start": "667440",
    "end": "674519"
  },
  {
    "text": "mention the number of proxies and the targets that you want So based on that uh the very first",
    "start": "674519",
    "end": "681240"
  },
  {
    "text": "thing that comes up in the cluster is AIS proxies so the proxies uh start up",
    "start": "681240",
    "end": "686399"
  },
  {
    "text": "and they are registered with an AIS proxy service once the proxies are up and running then",
    "start": "686399",
    "end": "692040"
  },
  {
    "text": "we bring up the targets so the targets register to the cluster through proxies so they register to the proxy through",
    "start": "692040",
    "end": "698279"
  },
  {
    "text": "the proxy service and the whole cluster comes up so let's take a deeper look into how the targets are actually set up",
    "start": "698279",
    "end": "705880"
  },
  {
    "text": "so each target has multiple different discs each dis we have so we have a PV",
    "start": "705880",
    "end": "711279"
  },
  {
    "text": "over the entire disk and these PVS are mounted to the Pod using persistent volume claims so in this uh deployment",
    "start": "711279",
    "end": "719320"
  },
  {
    "text": "which we going to show we have all the discs of that node mounted onto the Pod",
    "start": "719320",
    "end": "725399"
  },
  {
    "text": "using PVS and PVCs so this helps us to manage the AI store cluster efficiently and store data for easier",
    "start": "725399",
    "end": "734319"
  },
  {
    "text": "retrieval thanks AB so now that we covered as St as a fast tier uh let's go",
    "start": "734600",
    "end": "739720"
  },
  {
    "text": "into some benchmarks to see what you can expect from a real world uh",
    "start": "739720",
    "end": "744800"
  },
  {
    "text": "deployment so we'll take a look at one of our production clusters which we have",
    "start": "744800",
    "end": "750360"
  },
  {
    "text": "running in Oracle Cloud infrastructure we have a 16 node cluster running on",
    "start": "750360",
    "end": "755560"
  },
  {
    "text": "what Oracle calls BM Denio nodes uh it's a bare metal config uh with most",
    "start": "755560",
    "end": "761800"
  },
  {
    "text": "importantly some really fast high performance drives and 100 gigabit nxs on every",
    "start": "761800",
    "end": "767040"
  },
  {
    "text": "node so we've got about 1.2 pedabytes of storage across the 16 node cluster over",
    "start": "767040",
    "end": "772120"
  },
  {
    "text": "the 192 drives and so for this Benchmark",
    "start": "772120",
    "end": "777399"
  },
  {
    "text": "uh we're going to be benchmarking that with our own tool called as loader so as loader is a load generator that we wrote",
    "start": "777399",
    "end": "784519"
  },
  {
    "text": "to Benchmark as store and S3 compatible back ends uh it basically really",
    "start": "784519",
    "end": "789720"
  },
  {
    "text": "efficiently multi-threaded goes makes request to the cluster and discards the",
    "start": "789720",
    "end": "794800"
  },
  {
    "text": "data so it's the goal is to just completely saturate this asor cluster to see what it can do so as you see there's",
    "start": "794800",
    "end": "802199"
  },
  {
    "text": "over a thousand client threads as it's scaled across our cluster of as AIS loader nodes and every one of of these",
    "start": "802199",
    "end": "809360"
  },
  {
    "text": "as loaders is running on the same uh spec as the actual cluster so that we have plenty of",
    "start": "809360",
    "end": "815600"
  },
  {
    "text": "bandwidth so we are in three different types of benchmarks um to test this cluster um so",
    "start": "815600",
    "end": "821720"
  },
  {
    "text": "first of all we have a direct git so that's just going straight to the S3 back end no AI store involvement at all",
    "start": "821720",
    "end": "828440"
  },
  {
    "text": "then we have our cold git so this is the initial retriever retrieval of an object",
    "start": "828440",
    "end": "834000"
  },
  {
    "text": "through AI store where AI store will go out to that back end it will get that",
    "start": "834000",
    "end": "839320"
  },
  {
    "text": "data it will write it to disk and it will return it to the user then finally we have warm git so for warm git those",
    "start": "839320",
    "end": "847160"
  },
  {
    "text": "objects already exist on the cluster AI store goes in gets those objects and can quickly retrieve",
    "start": "847160",
    "end": "853680"
  },
  {
    "text": "them so one note on our tests is we did use a swift stack Object Store um just",
    "start": "853680",
    "end": "859079"
  },
  {
    "text": "because it's uh closer to our a cluster uh better connectivity and that matters",
    "start": "859079",
    "end": "864399"
  },
  {
    "text": "a lot as we'll see so this is just some of the",
    "start": "864399",
    "end": "869600"
  },
  {
    "text": "relative uh performance numbers we got as you can see there's a direct git",
    "start": "869600",
    "end": "875839"
  },
  {
    "text": "which we used as our Baseline of just one and then cold git there's a slight performance penalty as we might expect",
    "start": "875839",
    "end": "883000"
  },
  {
    "text": "because we are writing to discs and then warm git in this case we saw over a 10x",
    "start": "883000",
    "end": "888240"
  },
  {
    "text": "performance Improvement so one thing to keep in mind though is that 10x performance",
    "start": "888240",
    "end": "893560"
  },
  {
    "text": "Improvement is very much a function of your cluster configuration so as we'll",
    "start": "893560",
    "end": "898800"
  },
  {
    "text": "see with a scaling it doesn't necessarily tell the whole",
    "start": "898800",
    "end": "903839"
  },
  {
    "text": "story so this is just kind of a quick look at what it looks like if you have a lot of clients pulling the same data",
    "start": "904600",
    "end": "911160"
  },
  {
    "text": "from a cluster so you can see at the start here we have 16 as loaders that are running",
    "start": "911160",
    "end": "918120"
  },
  {
    "text": "against this cluster and every one of those requests at the very beginning is going out to the remote back end so",
    "start": "918120",
    "end": "924279"
  },
  {
    "text": "we're only seeing about 15 gigabyte per gigabytes per second of performance but",
    "start": "924279",
    "end": "929319"
  },
  {
    "text": "since since all 16 as loaders are loading the same data set eventually those objects start to be retrieved from",
    "start": "929319",
    "end": "935399"
  },
  {
    "text": "the cluster itself and so you can see that exponential curve as more and more of",
    "start": "935399",
    "end": "941040"
  },
  {
    "text": "the requests are actually resolved from AIS store",
    "start": "941040",
    "end": "945839"
  },
  {
    "text": "diss so one of the most important features of AI store is the scaling and",
    "start": "947639",
    "end": "953199"
  },
  {
    "text": "so when we ran these warm gits one of the things we were able to observe was all 16 of the AI store nodes providing",
    "start": "953199",
    "end": "961000"
  },
  {
    "text": "an equal throughput across the cluster all 16 are providing 11.1 gigabytes per",
    "start": "961000",
    "end": "966560"
  },
  {
    "text": "second of storage and we that's really just the",
    "start": "966560",
    "end": "971959"
  },
  {
    "text": "most important thing with those Benchmark numbers we saw before right so we said 10x performance but if you add",
    "start": "971959",
    "end": "979639"
  },
  {
    "text": "10 more or 16 more nodes you're going to see that go up to 20 times performance because it's all",
    "start": "979639",
    "end": "986240"
  },
  {
    "text": "scalable oh and one more note about about this you'll also notice really stable uh performance numbers throughout",
    "start": "986959",
    "end": "993199"
  },
  {
    "text": "the whole run this was over a couple of hours and you don't see any big spikes or drops because all of these clusters",
    "start": "993199",
    "end": "999240"
  },
  {
    "text": "are able to continue to provide that throughput one other important feature",
    "start": "999240",
    "end": "1005040"
  },
  {
    "text": "right is the individual targets so that they can scale with the discs you add so",
    "start": "1005040",
    "end": "1011480"
  },
  {
    "text": "here we saw I believe it was 12 12 drives per node and you see that because",
    "start": "1011480",
    "end": "1018399"
  },
  {
    "text": "all the object objects are stored across all of the different nodes you see that they're all about 95% utilization",
    "start": "1018399",
    "end": "1025240"
  },
  {
    "text": "because they're all serving different",
    "start": "1025240",
    "end": "1028640"
  },
  {
    "text": "objects so one of the things we wanted to look at when we were looking at this cluster and running these benchmarks is",
    "start": "1030559",
    "end": "1037438"
  },
  {
    "text": "what exactly our bottleneck would be like what what's holding us back because our goal is to scale up with the hardware that we're given and one of the",
    "start": "1037439",
    "end": "1044360"
  },
  {
    "text": "things we noticed actually was that our Network utilization was definitely our",
    "start": "1044360",
    "end": "1049559"
  },
  {
    "text": "limiting factor so what we have here is the graph of total Network bandwidth and",
    "start": "1049559",
    "end": "1057120"
  },
  {
    "text": "if you can see that number it's around 15 29 gigabits per second so",
    "start": "1057120",
    "end": "1063760"
  },
  {
    "text": "theoretically these Nicks can give us a maximum advertised throughput of 1600 gigabits per second so we're coming up",
    "start": "1063760",
    "end": "1069919"
  },
  {
    "text": "really close to that limit and so there's really nothing else that these nvme drives can do right",
    "start": "1069919",
    "end": "1076039"
  },
  {
    "text": "they've completely saturated our Network bandwidth",
    "start": "1076039",
    "end": "1080520"
  },
  {
    "text": "so this is one of the things too that you may not expect to see depending on",
    "start": "1085720",
    "end": "1090960"
  },
  {
    "text": "your workload um because with faster mvme drives maybe you want that latency",
    "start": "1090960",
    "end": "1097559"
  },
  {
    "text": "maybe you need to use those just to get the objects back faster",
    "start": "1097559",
    "end": "1103559"
  },
  {
    "text": "but what we saw here is that it's just entirely the network that's holding us back",
    "start": "1103559",
    "end": "1110200"
  },
  {
    "text": "so we were starting to wonder right what should we be able to expect from these discs and so we ran the industry",
    "start": "1110200",
    "end": "1116440"
  },
  {
    "text": "standard fio Benchmark to kind of get an idea of what we should be seeing right",
    "start": "1116440",
    "end": "1121880"
  },
  {
    "text": "so with these discs we'd expect to see around 3 gigabytes per second per drive for this use case of 10 megabyte object",
    "start": "1121880",
    "end": "1129200"
  },
  {
    "text": "reads so for 192 Drive cluster we can extrapolate that all the way",
    "start": "1129200",
    "end": "1135480"
  },
  {
    "text": "up but that's not what we saw so even with with that we also had this discrete",
    "start": "1135600",
    "end": "1143000"
  },
  {
    "text": "reduction right because all of these objects a certain percentage of them are going to be stored in page cache so in",
    "start": "1143000",
    "end": "1150640"
  },
  {
    "text": "the end we only saw the drives doing about 26% of the theoretical bandwidth",
    "start": "1150640",
    "end": "1155919"
  },
  {
    "text": "that they could provide and so again this kind of brings up the idea of specking your cluster",
    "start": "1155919",
    "end": "1163240"
  },
  {
    "text": "configuration to the workloads you're going to be doing because we were limited in this case by the options from",
    "start": "1163240",
    "end": "1169880"
  },
  {
    "text": "our cloud provider um we picked these E5 dense iio nodes because of their High",
    "start": "1169880",
    "end": "1175799"
  },
  {
    "text": "Network bandwidth but ideally we have more discs maybe slower discs and",
    "start": "1175799",
    "end": "1181760"
  },
  {
    "text": "especially more Network so in a real work workload you",
    "start": "1181760",
    "end": "1188799"
  },
  {
    "text": "might not see this play out because you probably won't have that level of sustained network uh load that our as",
    "start": "1188799",
    "end": "1196919"
  },
  {
    "text": "loaders are doing so so maybe this is something that would work for a good production cluster but in this",
    "start": "1196919",
    "end": "1205039"
  },
  {
    "text": "case it's very much Pol by network but if this is the case you can still add more performance by simply adding more",
    "start": "1205039",
    "end": "1213600"
  },
  {
    "text": "nodes so I wanted to take a quick look at a snapshot of an actual production",
    "start": "1214080",
    "end": "1219400"
  },
  {
    "text": "workload running against this cluster so in this case our clients had",
    "start": "1219400",
    "end": "1224440"
  },
  {
    "text": "16 worker nodes and over a th worker thread pulling data from separate",
    "start": "1224440",
    "end": "1230679"
  },
  {
    "text": "shards um you'll notice here we have some initial spikes and then it lowers",
    "start": "1230679",
    "end": "1236440"
  },
  {
    "text": "down as the clients go back to actually training the model and that throughput",
    "start": "1236440",
    "end": "1241679"
  },
  {
    "text": "drops way down so you might notice that we've got over a thousand threads here",
    "start": "1241679",
    "end": "1247200"
  },
  {
    "text": "there's 16 separate nodes this is pretty similar to our Benchmark so why are we only seeing half the total throughput so",
    "start": "1247200",
    "end": "1254919"
  },
  {
    "text": "we looked into that we're only seeing about 84 GB per second there and we looked into that and we actually",
    "start": "1254919",
    "end": "1260880"
  },
  {
    "text": "realized that because of our Prometheus scraping we actually are only getting an average over that interval and while",
    "start": "1260880",
    "end": "1267000"
  },
  {
    "text": "this doesn't look great from a statistics perspective it's actually a really good thing because what we've done here is these workloads which",
    "start": "1267000",
    "end": "1273640"
  },
  {
    "text": "previously would have had to wait for the data are able to get that data quickly get back to work and they're not",
    "start": "1273640",
    "end": "1279559"
  },
  {
    "text": "waiting on data so they're able to pull their entire buffer of data before our stats interval even captures that",
    "start": "1279559",
    "end": "1286120"
  },
  {
    "text": "they're loading the cluster also wanted to point out that this is two jobs R over the course of three days",
    "start": "1286120",
    "end": "1293320"
  },
  {
    "text": "so that's a pretty long sustained amount of throughput right there as",
    "start": "1293320",
    "end": "1299399"
  },
  {
    "text": "well so now that we've covered the basics uh some of the use cases and performance numbers um I just wanted to",
    "start": "1302080",
    "end": "1309000"
  },
  {
    "text": "do a quick walkr of actually getting started with AI store in",
    "start": "1309000",
    "end": "1314360"
  },
  {
    "text": "kubernetes so let me roll this here",
    "start": "1314360",
    "end": "1320120"
  },
  {
    "text": "here so the prerequisites for running aist door in kubernetes are basically",
    "start": "1320120",
    "end": "1326000"
  },
  {
    "text": "just that you have a kubernetes cluster you have Cube cuddle configured with access to that cluster and that's about",
    "start": "1326000",
    "end": "1332039"
  },
  {
    "text": "it so here we've got a three node cluster and I've got the nodes labeled",
    "start": "1332039",
    "end": "1337600"
  },
  {
    "text": "with as proxy and as Target labels that's because we use node Affinity to handle our",
    "start": "1337600",
    "end": "1344640"
  },
  {
    "text": "scheduling so the next thing to do is to log into the cluster and look at what our disc config is going to be like and",
    "start": "1344640",
    "end": "1351240"
  },
  {
    "text": "in this case we have 16 drives each with about 5.8 terab of storage xfs",
    "start": "1351240",
    "end": "1357720"
  },
  {
    "text": "formatting and mounted on/ AIS so we want to make a note of this",
    "start": "1357720",
    "end": "1362760"
  },
  {
    "text": "for uh configuration of the cluster later and also we just don't have any",
    "start": "1362760",
    "end": "1367919"
  },
  {
    "text": "other pods running outside the namespace in this case right everything's in Cube system we don't have any other name",
    "start": "1367919",
    "end": "1372960"
  },
  {
    "text": "spaces it's a pretty clean cluster so like we said before all the",
    "start": "1372960",
    "end": "1378600"
  },
  {
    "text": "AIS deployment is actually managed by our own AIS operator so this will take a AIS custom",
    "start": "1378600",
    "end": "1385679"
  },
  {
    "text": "resource and it will create everything the cluster needs to bring up pods and",
    "start": "1385679",
    "end": "1390760"
  },
  {
    "text": "get started with the cluster we do offer a Helm chart to install this uh but in",
    "start": "1390760",
    "end": "1396039"
  },
  {
    "text": "this case I'm going to use our included home file um to install CT manager as",
    "start": "1396039",
    "end": "1402279"
  },
  {
    "text": "well and once that's up and running we'll be able to",
    "start": "1402279",
    "end": "1408880"
  },
  {
    "text": "take any as store custom resources and build the cluster so our as charts Helm charts",
    "start": "1408880",
    "end": "1416000"
  },
  {
    "text": "themselves are actually just stored in our kubernetes repo for now um so we'll",
    "start": "1416000",
    "end": "1421080"
  },
  {
    "text": "just use the helm file here to help us configure a deployment uh Helm file is",
    "start": "1421080",
    "end": "1426200"
  },
  {
    "text": "pretty useful because it lets us do separate environments for some of our different deployments um and have it all",
    "start": "1426200",
    "end": "1431520"
  },
  {
    "text": "in in code so here I created a new environment and I've uh enabled TLS",
    "start": "1431520",
    "end": "1438679"
  },
  {
    "text": "and for each new environment you also need to set uh new config files for each",
    "start": "1438679",
    "end": "1443880"
  },
  {
    "text": "of your Sub sub charts here I'm setting up some TLS charts um just to set up a",
    "start": "1443880",
    "end": "1449440"
  },
  {
    "text": "self-signed certificate and you know usually you",
    "start": "1449440",
    "end": "1455039"
  },
  {
    "text": "would probably use a trusted CA but this will just let me get a quick",
    "start": "1455039",
    "end": "1460200"
  },
  {
    "text": "demo so the next thing to do is set up some Cloud credentials um I'll create a new namespace and add some credentials",
    "start": "1460200",
    "end": "1467360"
  },
  {
    "text": "based on my local and this is also optional but this will give us access to my specific Cloud",
    "start": "1467360",
    "end": "1474399"
  },
  {
    "text": "buckets so now I can finally move on to the actual Helm chart values for AI store so these first this first section",
    "start": "1474399",
    "end": "1481360"
  },
  {
    "text": "is just general cluster config information I'm setting nodes I'm setting the mount paths to what we saw",
    "start": "1481360",
    "end": "1488520"
  },
  {
    "text": "before and then moving down I can set the size of the cluster whether to use",
    "start": "1488520",
    "end": "1493919"
  },
  {
    "text": "TLS and then eventually we'll get down to the images we're going to use and",
    "start": "1493919",
    "end": "1500399"
  },
  {
    "text": "these are just our latest",
    "start": "1500399",
    "end": "1503240"
  },
  {
    "text": "releases so finally we can run a Helm file",
    "start": "1511559",
    "end": "1516720"
  },
  {
    "text": "sync and then that will go ahead and create the custom resource on the cluster and it'll bring up the name",
    "start": "1517720",
    "end": "1524080"
  },
  {
    "text": "spaces the persistent volumes all their claims uh the stateful sets all the mounts the pods everything we",
    "start": "1524080",
    "end": "1533000"
  },
  {
    "text": "need so here you can watch the pods actually come up as the operator creates [Music]",
    "start": "1533640",
    "end": "1540959"
  },
  {
    "text": "them and once those are ready we can actually move on to using the cluster so",
    "start": "1541960",
    "end": "1547559"
  },
  {
    "text": "to use the aor cluster we actually need access to all of the target nodes and so",
    "start": "1547559",
    "end": "1553000"
  },
  {
    "text": "the easiest way for this case without a load balancer we can just SSH into one of the nodes and I'll just set up some",
    "start": "1553000",
    "end": "1559080"
  },
  {
    "text": "quick access to the cluster I'll add the certificate that we need to trust and",
    "start": "1559080",
    "end": "1564159"
  },
  {
    "text": "I'll add the endpoint and I already have the as CLI installed on this node so",
    "start": "1564159",
    "end": "1569440"
  },
  {
    "text": "that's all we need as door is running we've set the proxy as our endpoint and we can actually look at the cluster so",
    "start": "1569440",
    "end": "1575559"
  },
  {
    "text": "here you can see that we've got three proxy nodes and three Target nodes all running and we can see our total disc",
    "start": "1575559",
    "end": "1581919"
  },
  {
    "text": "capacity we can view any of the configs there's a lot of configs you can change within the cluster lots of features and",
    "start": "1581919",
    "end": "1588840"
  },
  {
    "text": "then we can even go ahead and start creating buckets putting files in buckets listing buckets and getting",
    "start": "1588840",
    "end": "1597320"
  },
  {
    "text": "objects so we can also because I added those AWS configurations we can access my remote",
    "start": "1600080",
    "end": "1606760"
  },
  {
    "text": "S3 bucket and list objects we can see that this value is not cached and for",
    "start": "1606760",
    "end": "1612120"
  },
  {
    "text": "just a simple example we can get that object get the content and any future",
    "start": "1612120",
    "end": "1617159"
  },
  {
    "text": "calls that object is is cached so all future all future calls will be using the asor disc",
    "start": "1617159",
    "end": "1625158"
  },
  {
    "text": "themselves so that's just a really quick demo of setting up AAR in kubernetes um",
    "start": "1625720",
    "end": "1630960"
  },
  {
    "text": "there's a lot more features available obviously uh but that's just all it takes to get started with a production",
    "start": "1630960",
    "end": "1636200"
  },
  {
    "text": "level deployment of AI store in kubernetes so obviously a lot of that can change um but the main thing is",
    "start": "1636200",
    "end": "1642880"
  },
  {
    "text": "kubernetes and Helm make the deployment simple so",
    "start": "1642880",
    "end": "1650158"
  },
  {
    "text": "so I want to talk a little bit about some of the current projects we're working on and what we're planning for the future so ETL our feature for uh",
    "start": "1651679",
    "end": "1660159"
  },
  {
    "text": "batch jobs doing the transformation in the cluster is a pretty important feature",
    "start": "1660159",
    "end": "1667399"
  },
  {
    "text": "for uh AI training loads um there's a few things we know we can do to improve usability of course uh improve",
    "start": "1667399",
    "end": "1674399"
  },
  {
    "text": "performance and scale up better um for auen our uh authentication",
    "start": "1674399",
    "end": "1680320"
  },
  {
    "text": "and authorization feature um we want to extend some of those extra features um build a more highly available deployment",
    "start": "1680320",
    "end": "1687880"
  },
  {
    "text": "and integrate with some of our existing IIM and then Cloud credential management is one",
    "start": "1687880",
    "end": "1694840"
  },
  {
    "text": "thing we need to address as well um aore is mostly designed for a single tenant",
    "start": "1694840",
    "end": "1701120"
  },
  {
    "text": "use case and one of the requests we've had is we've add some multi-tenant clusters is an ability to have better",
    "start": "1701120",
    "end": "1708320"
  },
  {
    "text": "management of different Cloud credentials for different backends and finally we want to",
    "start": "1708320",
    "end": "1713519"
  },
  {
    "text": "experiment with some new cluster variations uh aor scalability obviously means we want to try to scale up as much",
    "start": "1713519",
    "end": "1720519"
  },
  {
    "text": "as we can and so we want to try some really large scale clusters we've done 16 node we've experimented some with",
    "start": "1720519",
    "end": "1727200"
  },
  {
    "text": "larger but we want to go into the hundreds and see exactly what we can do hyper converge clusters are another",
    "start": "1727200",
    "end": "1733320"
  },
  {
    "text": "interesting point that we want to explore we have the idea where a store",
    "start": "1733320",
    "end": "1738960"
  },
  {
    "text": "can actually run on the same nodes as GPU nodes and so if you can do that then",
    "start": "1738960",
    "end": "1744200"
  },
  {
    "text": "you just remove a lot of the problems with network and then finally we want to look at multi-tier Solutions so AI store",
    "start": "1744200",
    "end": "1752120"
  },
  {
    "text": "clusters can support a separate AI store back end so you can have some AI store clusters that are faster maybe another",
    "start": "1752120",
    "end": "1757679"
  },
  {
    "text": "AI store cluster that's slower cheaper um and there's a lot we can do",
    "start": "1757679",
    "end": "1764200"
  },
  {
    "text": "there so that's just a quick overview of AI store uh we've talked about how to get started and just a few of the long",
    "start": "1764320",
    "end": "1770120"
  },
  {
    "text": "list of features um if you're interested in taking advantage of as do performance",
    "start": "1770120",
    "end": "1775360"
  },
  {
    "text": "um check out our blog take out our GitHub um make commits open PRS and uh",
    "start": "1775360",
    "end": "1781640"
  },
  {
    "text": "come talk to us at the Nvidia Booth uh with that I'd like to open up for any",
    "start": "1781640",
    "end": "1788398"
  },
  {
    "text": "questions sure U can you ask on the mic",
    "start": "1788600",
    "end": "1795919"
  },
  {
    "text": "try again uh can we use uh Triton inferencing server to use this AIS store",
    "start": "1801320",
    "end": "1807760"
  },
  {
    "text": "sorry can you uh raise the volume on that a little so can we use uh the Triton inferencing server to use the AIS",
    "start": "1807760",
    "end": "1815159"
  },
  {
    "text": "store as the object storage for loading the models sorry I was that I'm not able to",
    "start": "1815159",
    "end": "1821360"
  },
  {
    "text": "hear you is it audible yeah this is Bel okay so can we use uh the an inferencing",
    "start": "1821360",
    "end": "1829000"
  },
  {
    "text": "server to use the AIS store as the object storage to load the models so we",
    "start": "1829000",
    "end": "1835279"
  },
  {
    "text": "have use cases where we use minio or S3 to store our models and uh load that in",
    "start": "1835279",
    "end": "1842519"
  },
  {
    "text": "a Titan inferencing server from the Nvidia so if you don't know answer that's fine but just curious to see if",
    "start": "1842519",
    "end": "1849399"
  },
  {
    "text": "uh we can start using the AI store instead of the other object storage so",
    "start": "1849399",
    "end": "1854640"
  },
  {
    "text": "you're talking about loading the models themselves out of an object store rather than just the data yeah basically like",
    "start": "1854640",
    "end": "1860120"
  },
  {
    "text": "currently we are using some other object storages to store our models B use that",
    "start": "1860120",
    "end": "1866399"
  },
  {
    "text": "in the Nvidia inferencing servers to build and catch them yeah we don't have",
    "start": "1866399",
    "end": "1871480"
  },
  {
    "text": "any like specific behavior built in for to handle inferencing like that you can definitely store any anything you want",
    "start": "1871480",
    "end": "1876679"
  },
  {
    "text": "as an object but we don't have a specific inference load okay I I think my question was like more on does uh the",
    "start": "1876679",
    "end": "1885880"
  },
  {
    "text": "Triton server is compatible with the AIS storage so maybe we can take this",
    "start": "1885880",
    "end": "1892080"
  },
  {
    "text": "offline but definitely curious we haven't experimented with Triton right now but you can store anything in Nvidia",
    "start": "1892080",
    "end": "1898399"
  },
  {
    "text": "like we have also stored check like we are trying out to store checkpoints in one region on one AI store cluster and",
    "start": "1898399",
    "end": "1904480"
  },
  {
    "text": "retrieve it in another region so it's like if your models are objects you can store them in AI store okay okay sounds",
    "start": "1904480",
    "end": "1914360"
  },
  {
    "text": "good hi I have a question about the viability of of AI store as a general",
    "start": "1914360",
    "end": "1919440"
  },
  {
    "text": "purpose object Stores um you only show the results for gets you didn't show",
    "start": "1919440",
    "end": "1925240"
  },
  {
    "text": "anything for posts I would imagine posts are much slower because you have to write them in two places local drives as",
    "start": "1925240",
    "end": "1932559"
  },
  {
    "text": "well as backend object storage so any comments on how slow posts are and I",
    "start": "1932559",
    "end": "1938519"
  },
  {
    "text": "have a second question yeah so we've we've done some benchmarks on puts as well we focus on",
    "start": "1938519",
    "end": "1943799"
  },
  {
    "text": "gets pretty heavily because AI store um you know obviously focus on a AI",
    "start": "1943799",
    "end": "1948840"
  },
  {
    "text": "workloads which do a lot more reading thanp putting and especially in our clients um for our put usage I think we",
    "start": "1948840",
    "end": "1955639"
  },
  {
    "text": "saw pretty pretty comparable numbers to actual S3 backends we have to if you want if we want to come by later I think",
    "start": "1955639",
    "end": "1961240"
  },
  {
    "text": "we have some actual numbers we can share on that um but I don't I don't remember there being much of an actual",
    "start": "1961240",
    "end": "1967320"
  },
  {
    "text": "performance hit for puts so I have a benchmark which compares the object sizes and the throughput we which we are",
    "start": "1967320",
    "end": "1973080"
  },
  {
    "text": "receiving for puts so maybe I can show that Benchmark with you it's on my personal laptop right now which is not",
    "start": "1973080",
    "end": "1978440"
  },
  {
    "text": "connected but uh we have the benchmarks which we can show you sounds good thank you and my second question is um how do",
    "start": "1978440",
    "end": "1985799"
  },
  {
    "text": "proxy nodes map request to specific storage nodes is like the metadata um",
    "start": "1985799",
    "end": "1992120"
  },
  {
    "text": "based on consistent hashing is it stored somewhere else is it stor in persistance storage is it stor in memory like yeah",
    "start": "1992120",
    "end": "1998639"
  },
  {
    "text": "it's a hashing function that pulls basically based on some metadata stored by each each node thank",
    "start": "1998639",
    "end": "2005760"
  },
  {
    "text": "you hey guys good presentation so do you find the need to anybody ask for snapshots in your AI store like any",
    "start": "2005760",
    "end": "2013080"
  },
  {
    "text": "of your kind of use use cases can can you repeat I couldn't hear snapshots SN like storage like object",
    "start": "2013080",
    "end": "2020559"
  },
  {
    "text": "level snapshots or things like that anybody cares nobody's asked that question before so what do you mean by",
    "start": "2020559",
    "end": "2026039"
  },
  {
    "text": "snapshots snapshots okay generally storage systems have snapshots like profiles or objects or volumes what can",
    "start": "2026039",
    "end": "2031799"
  },
  {
    "text": "you can you speak closer to the mic generally storage systems have like these snapshot schemes okay for a given",
    "start": "2031799",
    "end": "2037799"
  },
  {
    "text": "file volume whatever you can take Snapchats and keep multiple copies of versions of it yeah we use version",
    "start": "2037799",
    "end": "2042919"
  },
  {
    "text": "there's a version in feature uh with check sums uh we typically use believe it's xash yeah there are multiple things",
    "start": "2042919",
    "end": "2050000"
  },
  {
    "text": "that you can use yeah but it supports the same ones as AWS as well there's a Blog on out of band wres and reads in UI",
    "start": "2050000",
    "end": "2056599"
  },
  {
    "text": "store so in case your object is changing by some other client we can also pull",
    "start": "2056599",
    "end": "2062720"
  },
  {
    "text": "that object so there's a completely different blog on that maybe you can visit the booth and we can talk more on",
    "start": "2062720",
    "end": "2068320"
  },
  {
    "text": "yeah thanks they're telling us we're out of time for questions so uh come talk to us",
    "start": "2068320",
    "end": "2074118"
  },
  {
    "text": "I had the same question it's fine awesome thank you all thank you",
    "start": "2074119",
    "end": "2080839"
  }
]