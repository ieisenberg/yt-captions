[
  {
    "text": "um so we'll be talking about the gateway",
    "start": "320",
    "end": "2480"
  },
  {
    "text": "API or the gateway API inference um",
    "start": "2480",
    "end": "4880"
  },
  {
    "text": "extension we just realized there was a",
    "start": "4880",
    "end": "6319"
  },
  {
    "text": "discrepancy between the title on the",
    "start": "6319",
    "end": "8160"
  },
  {
    "text": "schedule and on the slides but basically",
    "start": "8160",
    "end": "10240"
  },
  {
    "text": "this they mean the same thing the",
    "start": "10240",
    "end": "11679"
  },
  {
    "text": "instance inference gateway is the same",
    "start": "11679",
    "end": "13440"
  },
  {
    "text": "thing my name is Abdel i work at Google",
    "start": "13440",
    "end": "16160"
  },
  {
    "text": "and I'm uh Dian Hansen i'm a principal",
    "start": "16160",
    "end": "18960"
  },
  {
    "text": "software engineer at solo.io",
    "start": "18960",
    "end": "21920"
  },
  {
    "text": "so before we get going we'll start with",
    "start": "21920",
    "end": "23359"
  },
  {
    "text": "a quick primer on the gateway API you",
    "start": "23359",
    "end": "25439"
  },
  {
    "text": "probably have seen these slides multiple",
    "start": "25439",
    "end": "26960"
  },
  {
    "text": "times in different formats um the",
    "start": "26960",
    "end": "29359"
  },
  {
    "text": "gateway API is the nextG ingress API it",
    "start": "29359",
    "end": "31840"
  },
  {
    "text": "was introduced back in 2019 as a",
    "start": "31840",
    "end": "34079"
  },
  {
    "text": "separated project part of the cloud eco",
    "start": "34079",
    "end": "36160"
  },
  {
    "text": "uh native ecosystem and um it's builds",
    "start": "36160",
    "end": "39680"
  },
  {
    "text": "on learnings from the ingress API which",
    "start": "39680",
    "end": "42079"
  },
  {
    "text": "you probably have all used um the whole",
    "start": "42079",
    "end": "44160"
  },
  {
    "text": "idea of the gateway API is to try to",
    "start": "44160",
    "end": "46640"
  },
  {
    "text": "solve some of the limitations that the",
    "start": "46640",
    "end": "48879"
  },
  {
    "text": "ingress API had some of them is the",
    "start": "48879",
    "end": "51120"
  },
  {
    "text": "separation of duty being able to define",
    "start": "51120",
    "end": "53920"
  },
  {
    "text": "resources in different name spaces and",
    "start": "53920",
    "end": "55520"
  },
  {
    "text": "make them point to each other another",
    "start": "55520",
    "end": "57199"
  },
  {
    "text": "one is the permission model with the",
    "start": "57199",
    "end": "58800"
  },
  {
    "text": "ingress uh API you didn't really have a",
    "start": "58800",
    "end": "60719"
  },
  {
    "text": "permission model everything had to be",
    "start": "60719",
    "end": "62879"
  },
  {
    "text": "defined by the team exposing the",
    "start": "62879",
    "end": "64559"
  },
  {
    "text": "application with the gateway API we have",
    "start": "64559",
    "end": "66720"
  },
  {
    "text": "this various objects that allows us to",
    "start": "66720",
    "end": "69200"
  },
  {
    "text": "define the load balancer itself using a",
    "start": "69200",
    "end": "71200"
  },
  {
    "text": "gateway separated from how the routes",
    "start": "71200",
    "end": "73760"
  },
  {
    "text": "point the load balancer to the different",
    "start": "73760",
    "end": "75760"
  },
  {
    "text": "backends using something called HTTP",
    "start": "75760",
    "end": "78080"
  },
  {
    "text": "routes there are also TCP routes UDP",
    "start": "78080",
    "end": "80080"
  },
  {
    "text": "routes and other types of routes but the",
    "start": "80080",
    "end": "82799"
  },
  {
    "text": "thing with the gateway API similar to",
    "start": "82799",
    "end": "84320"
  },
  {
    "text": "the English API is that it was mostly",
    "start": "84320",
    "end": "86640"
  },
  {
    "text": "designed to do web kind of traffic right",
    "start": "86640",
    "end": "89200"
  },
  {
    "text": "um suddenly we live in 2025 where LLMs",
    "start": "89200",
    "end": "92079"
  },
  {
    "text": "are around us and whether we want it or",
    "start": "92079",
    "end": "93600"
  },
  {
    "text": "not they are a big deal and the thing",
    "start": "93600",
    "end": "95520"
  },
  {
    "text": "with LLMs is that they are not a typical",
    "start": "95520",
    "end": "97280"
  },
  {
    "text": "web application uh LLM traffic is not",
    "start": "97280",
    "end": "100320"
  },
  {
    "text": "your typical web traffic there are",
    "start": "100320",
    "end": "102479"
  },
  {
    "text": "multiple reasons why um one of them is",
    "start": "102479",
    "end": "105439"
  },
  {
    "text": "the size of the response and requests",
    "start": "105439",
    "end": "107280"
  },
  {
    "text": "usually web traffic have very short very",
    "start": "107280",
    "end": "109920"
  },
  {
    "text": "frequent response in requests while LM",
    "start": "109920",
    "end": "112399"
  },
  {
    "text": "traffic can the size of the request can",
    "start": "112399",
    "end": "114720"
  },
  {
    "text": "be big depends on the prompt and it also",
    "start": "114720",
    "end": "116720"
  },
  {
    "text": "can be big because some LLM support",
    "start": "116720",
    "end": "118479"
  },
  {
    "text": "multimodel they support video audio so",
    "start": "118479",
    "end": "120399"
  },
  {
    "text": "the size of the inputs or the output",
    "start": "120399",
    "end": "122320"
  },
  {
    "text": "could be big um an LLM can actually take",
    "start": "122320",
    "end": "125360"
  },
  {
    "text": "more time to process a request a lot of",
    "start": "125360",
    "end": "126960"
  },
  {
    "text": "LLM support streaming where you send the",
    "start": "126960",
    "end": "129280"
  },
  {
    "text": "prompt and then it streams the responses",
    "start": "129280",
    "end": "130879"
  },
  {
    "text": "back to you as the LM is thinking and",
    "start": "130879",
    "end": "133520"
  },
  {
    "text": "generating the",
    "start": "133520",
    "end": "134599"
  },
  {
    "text": "outputs which means that the processing",
    "start": "134599",
    "end": "136720"
  },
  {
    "text": "time could be can be very long and it's",
    "start": "136720",
    "end": "138959"
  },
  {
    "text": "not like web web requests where the the",
    "start": "138959",
    "end": "141280"
  },
  {
    "text": "requests are very short um um the the",
    "start": "141280",
    "end": "144400"
  },
  {
    "text": "the the processing time can take time as",
    "start": "144400",
    "end": "146560"
  },
  {
    "text": "I said and um the the the requests in a",
    "start": "146560",
    "end": "150720"
  },
  {
    "text": "web traffic sometimes the requests could",
    "start": "150720",
    "end": "152879"
  },
  {
    "text": "be similar or the responses sorry from",
    "start": "152879",
    "end": "154400"
  },
  {
    "text": "the from the application could be",
    "start": "154400",
    "end": "155440"
  },
  {
    "text": "similar which could be cached but in the",
    "start": "155440",
    "end": "157599"
  },
  {
    "text": "LLM world a lot of times the responses",
    "start": "157599",
    "end": "159599"
  },
  {
    "text": "are the same and we actually use a lot",
    "start": "159599",
    "end": "161360"
  },
  {
    "text": "of caching techniques to make LLMs more",
    "start": "161360",
    "end": "163200"
  },
  {
    "text": "efficient um because you would ask the",
    "start": "163200",
    "end": "165200"
  },
  {
    "text": "same question get back an answer and",
    "start": "165200",
    "end": "166640"
  },
  {
    "text": "then ask the question again or ask a",
    "start": "166640",
    "end": "168560"
  },
  {
    "text": "follow-up question and then you will get",
    "start": "168560",
    "end": "169760"
  },
  {
    "text": "back the answer which probably contains",
    "start": "169760",
    "end": "171360"
  },
  {
    "text": "the same prompts",
    "start": "171360",
    "end": "172879"
  },
  {
    "text": "so these are just some of the",
    "start": "172879",
    "end": "174440"
  },
  {
    "text": "differences um so what is the inference",
    "start": "174440",
    "end": "176959"
  },
  {
    "text": "extension it's essentially an extension",
    "start": "176959",
    "end": "178800"
  },
  {
    "text": "of the gateway API to support inference",
    "start": "178800",
    "end": "181040"
  },
  {
    "text": "traffic and before we move any f forward",
    "start": "181040",
    "end": "183519"
  },
  {
    "text": "we actually thought it would be useful",
    "start": "183519",
    "end": "185120"
  },
  {
    "text": "to explain what inference is because the",
    "start": "185120",
    "end": "186879"
  },
  {
    "text": "word is used um interchangeably many",
    "start": "186879",
    "end": "189280"
  },
  {
    "text": "times i have a very stupid way of",
    "start": "189280",
    "end": "190879"
  },
  {
    "text": "explaining inference um you can think",
    "start": "190879",
    "end": "193360"
  },
  {
    "text": "about if you think of of a web",
    "start": "193360",
    "end": "195120"
  },
  {
    "text": "application as a combination of HTML CSS",
    "start": "195120",
    "end": "197280"
  },
  {
    "text": "and JavaScript those three things are",
    "start": "197280",
    "end": "199200"
  },
  {
    "text": "useless if you don't have a web server",
    "start": "199200",
    "end": "200879"
  },
  {
    "text": "you need a web server to be able to talk",
    "start": "200879",
    "end": "202879"
  },
  {
    "text": "to a website the same way for an LLM you",
    "start": "202879",
    "end": "205360"
  },
  {
    "text": "need an inference server to be able to",
    "start": "205360",
    "end": "206959"
  },
  {
    "text": "talk to an LLM llms are created",
    "start": "206959",
    "end": "209120"
  },
  {
    "text": "differently they have different",
    "start": "209120",
    "end": "210080"
  },
  {
    "text": "architectures and they are for the most",
    "start": "210080",
    "end": "212159"
  },
  {
    "text": "a very big file you need something that",
    "start": "212159",
    "end": "214480"
  },
  {
    "text": "knows how to talk to the LLM prompt it",
    "start": "214480",
    "end": "216799"
  },
  {
    "text": "get back the responses and return it",
    "start": "216799",
    "end": "218879"
  },
  {
    "text": "back to you whether you are a user or an",
    "start": "218879",
    "end": "220560"
  },
  {
    "text": "application in a standard format so a",
    "start": "220560",
    "end": "222640"
  },
  {
    "text": "rest RE rest response or a gRPC response",
    "start": "222640",
    "end": "225440"
  },
  {
    "text": "so that's what an inference is and so in",
    "start": "225440",
    "end": "227840"
  },
  {
    "text": "the inference space there are multiple",
    "start": "227840",
    "end": "229120"
  },
  {
    "text": "open source projects VLM TGI Triton uh",
    "start": "229120",
    "end": "232319"
  },
  {
    "text": "Olama etc etc so think about inference",
    "start": "232319",
    "end": "234879"
  },
  {
    "text": "as the web server I'm putting codes here",
    "start": "234879",
    "end": "237120"
  },
  {
    "text": "codes the web server for a web",
    "start": "237120",
    "end": "239879"
  },
  {
    "text": "application so how are we doing this",
    "start": "239879",
    "end": "242480"
  },
  {
    "text": "well we are we are providing some APIs",
    "start": "242480",
    "end": "244480"
  },
  {
    "text": "for doing uh routing decisions that are",
    "start": "244480",
    "end": "246959"
  },
  {
    "text": "specific to inference uh workloads um",
    "start": "246959",
    "end": "249120"
  },
  {
    "text": "and we're going to talk about it later",
    "start": "249120",
    "end": "250560"
  },
  {
    "text": "um um uh in details we're also providing",
    "start": "250560",
    "end": "253120"
  },
  {
    "text": "a bunch of specs and some reference",
    "start": "253120",
    "end": "254879"
  },
  {
    "text": "implementations for those specs um we'll",
    "start": "254879",
    "end": "257680"
  },
  {
    "text": "we'll talk in details but some of the",
    "start": "257680",
    "end": "259120"
  },
  {
    "text": "specs are being able to grab the model",
    "start": "259120",
    "end": "261199"
  },
  {
    "text": "name from the body of the requests so we",
    "start": "261199",
    "end": "263199"
  },
  {
    "text": "know where to route the traffic exactly",
    "start": "263199",
    "end": "265120"
  },
  {
    "text": "and then also providing some",
    "start": "265120",
    "end": "266639"
  },
  {
    "text": "integrations for various APIs that are",
    "start": "266639",
    "end": "268960"
  },
  {
    "text": "new to the inference extension",
    "start": "268960",
    "end": "273560"
  },
  {
    "text": "so let's talk a little bit about uh the",
    "start": "273759",
    "end": "276000"
  },
  {
    "text": "project you uh use cases um that were",
    "start": "276000",
    "end": "280639"
  },
  {
    "text": "used to really form uh the the the",
    "start": "280639",
    "end": "283440"
  },
  {
    "text": "project uh just a few short months ago",
    "start": "283440",
    "end": "286639"
  },
  {
    "text": "i've come a long way since starting the",
    "start": "286639",
    "end": "288400"
  },
  {
    "text": "project uh at the end of 2024 uh but the",
    "start": "288400",
    "end": "292560"
  },
  {
    "text": "first uh most obvious use case is model",
    "start": "292560",
    "end": "295120"
  },
  {
    "text": "aware routing right so Adele talked",
    "start": "295120",
    "end": "297680"
  },
  {
    "text": "about uh Gen AI traffic is very",
    "start": "297680",
    "end": "301040"
  },
  {
    "text": "different than uh web traffic right and",
    "start": "301040",
    "end": "304880"
  },
  {
    "text": "a big part of that is that in um in",
    "start": "304880",
    "end": "308320"
  },
  {
    "text": "inference traffic uh we've got the the",
    "start": "308320",
    "end": "311280"
  },
  {
    "text": "model name actually in the in the body",
    "start": "311280",
    "end": "313600"
  },
  {
    "text": "of the request and so being able to use",
    "start": "313600",
    "end": "317840"
  },
  {
    "text": "that model name to perform routing is",
    "start": "317840",
    "end": "321120"
  },
  {
    "text": "one of the key use cases of the project",
    "start": "321120",
    "end": "323759"
  },
  {
    "text": "and we'll talk a little bit more why",
    "start": "323759",
    "end": "325120"
  },
  {
    "text": "that is important but also serving",
    "start": "325120",
    "end": "327280"
  },
  {
    "text": "priority right you may have different",
    "start": "327280",
    "end": "328960"
  },
  {
    "text": "models some that have higher criticality",
    "start": "328960",
    "end": "331759"
  },
  {
    "text": "than others uh good example is a",
    "start": "331759",
    "end": "334919"
  },
  {
    "text": "summarization model is not as",
    "start": "334919",
    "end": "337880"
  },
  {
    "text": "time-sensitive as uh let's say your chat",
    "start": "337880",
    "end": "341039"
  },
  {
    "text": "model if you have a",
    "start": "341039",
    "end": "342680"
  },
  {
    "text": "chatbot uh and along with another use",
    "start": "342680",
    "end": "345759"
  },
  {
    "text": "case is model rollouts right so if we're",
    "start": "345759",
    "end": "348800"
  },
  {
    "text": "familiar with being able to uh to manage",
    "start": "348800",
    "end": "352160"
  },
  {
    "text": "web applications we know that we have to",
    "start": "352160",
    "end": "353919"
  },
  {
    "text": "roll out newer versions of that web",
    "start": "353919",
    "end": "355680"
  },
  {
    "text": "application over time we take that same",
    "start": "355680",
    "end": "358240"
  },
  {
    "text": "thought pattern and bring it into uh",
    "start": "358240",
    "end": "361240"
  },
  {
    "text": "inference and uh uh Abdell talked a",
    "start": "361240",
    "end": "364960"
  },
  {
    "text": "little bit about gateway API and the",
    "start": "364960",
    "end": "368479"
  },
  {
    "text": "resources associated with gateway API",
    "start": "368479",
    "end": "370880"
  },
  {
    "text": "and and one of the the the big successes",
    "start": "370880",
    "end": "373840"
  },
  {
    "text": "for gateway API is that the APIs were",
    "start": "373840",
    "end": "377120"
  },
  {
    "text": "were defined based on personas right we",
    "start": "377120",
    "end": "380960"
  },
  {
    "text": "learned through ingress that a single",
    "start": "380960",
    "end": "383360"
  },
  {
    "text": "resource does not work uh because",
    "start": "383360",
    "end": "387280"
  },
  {
    "text": "there's different personas that manage",
    "start": "387280",
    "end": "390080"
  },
  {
    "text": "how ingress works within a cluster",
    "start": "390080",
    "end": "392880"
  },
  {
    "text": "uh and gateway API inference extension",
    "start": "392880",
    "end": "396880"
  },
  {
    "text": "extended that that model and those",
    "start": "396880",
    "end": "399160"
  },
  {
    "text": "personas with an inference platform",
    "start": "399160",
    "end": "402400"
  },
  {
    "text": "owner and a inference workload owner and",
    "start": "402400",
    "end": "405680"
  },
  {
    "text": "creating an inference pool that is",
    "start": "405680",
    "end": "408240"
  },
  {
    "text": "managed by the inf uh inference platform",
    "start": "408240",
    "end": "410800"
  },
  {
    "text": "admin and then an inference model uh",
    "start": "410800",
    "end": "414080"
  },
  {
    "text": "that's managed by uh the workload owners",
    "start": "414080",
    "end": "416400"
  },
  {
    "text": "we'll talk a little bit more about uh",
    "start": "416400",
    "end": "418479"
  },
  {
    "text": "those two resources as well but uh",
    "start": "418479",
    "end": "422000"
  },
  {
    "text": "wanted to kind of do a day in the life",
    "start": "422000",
    "end": "424479"
  },
  {
    "text": "of of an inference request that that",
    "start": "424479",
    "end": "427599"
  },
  {
    "text": "comes into the system right and so a",
    "start": "427599",
    "end": "430960"
  },
  {
    "text": "user or some system makes a a request in",
    "start": "430960",
    "end": "434560"
  },
  {
    "text": "this example it's to your v1 chat",
    "start": "434560",
    "end": "436800"
  },
  {
    "text": "completions endpoint uh that gets routed",
    "start": "436800",
    "end": "439599"
  },
  {
    "text": "to a",
    "start": "439599",
    "end": "441000"
  },
  {
    "text": "gateway and typically that gateway uh in",
    "start": "441000",
    "end": "444400"
  },
  {
    "text": "a web application would perform some",
    "start": "444400",
    "end": "447120"
  },
  {
    "text": "kind of layer 7 analysis right uh based",
    "start": "447120",
    "end": "450880"
  },
  {
    "text": "on maybe the path of the request or",
    "start": "450880",
    "end": "453919"
  },
  {
    "text": "certain headers and then route that",
    "start": "453919",
    "end": "456080"
  },
  {
    "text": "request to a certain backend right um",
    "start": "456080",
    "end": "460560"
  },
  {
    "text": "for inference and and with the inference",
    "start": "460560",
    "end": "462319"
  },
  {
    "text": "extension that gateway now as that",
    "start": "462319",
    "end": "464319"
  },
  {
    "text": "request comes in um it's able to go",
    "start": "464319",
    "end": "468240"
  },
  {
    "text": "ahead and say wait a second this is an",
    "start": "468240",
    "end": "470240"
  },
  {
    "text": "inference request uh let me forward it",
    "start": "470240",
    "end": "473039"
  },
  {
    "text": "over to this extension in this example",
    "start": "473039",
    "end": "475520"
  },
  {
    "text": "the uh endpoint selection extension",
    "start": "475520",
    "end": "478160"
  },
  {
    "text": "which you may also hear it referred to",
    "start": "478160",
    "end": "480080"
  },
  {
    "text": "as the endpoint picker uh the gateway uh",
    "start": "480080",
    "end": "483680"
  },
  {
    "text": "forwards that request over to this",
    "start": "483680",
    "end": "486000"
  },
  {
    "text": "extension and uh typically that",
    "start": "486000",
    "end": "488639"
  },
  {
    "text": "extension is is running as a separate",
    "start": "488639",
    "end": "491120"
  },
  {
    "text": "pod and service um and so we'll talk",
    "start": "491120",
    "end": "494400"
  },
  {
    "text": "about how the gateway discovers that but",
    "start": "494400",
    "end": "496720"
  },
  {
    "text": "the gateway forwards that request over",
    "start": "496720",
    "end": "498800"
  },
  {
    "text": "to uh the endpoint selection extension",
    "start": "498800",
    "end": "502560"
  },
  {
    "text": "uh to make a determination on what",
    "start": "502560",
    "end": "505960"
  },
  {
    "text": "endpoint that the traffic should be",
    "start": "505960",
    "end": "508639"
  },
  {
    "text": "forwarded to and how does that endpoint",
    "start": "508639",
    "end": "511039"
  },
  {
    "text": "selection extension make that",
    "start": "511039",
    "end": "512839"
  },
  {
    "text": "determination well on the back end it's",
    "start": "512839",
    "end": "516240"
  },
  {
    "text": "getting these live reports from all of",
    "start": "516240",
    "end": "519039"
  },
  {
    "text": "the different uh model server pods that",
    "start": "519039",
    "end": "523200"
  },
  {
    "text": "make up an inference pool right so an",
    "start": "523200",
    "end": "526160"
  },
  {
    "text": "inference pool we think of that as an",
    "start": "526160",
    "end": "528480"
  },
  {
    "text": "extra as an abstraction uh to abstract",
    "start": "528480",
    "end": "532200"
  },
  {
    "text": "away the hardware the shared hardware um",
    "start": "532200",
    "end": "536320"
  },
  {
    "text": "like GPU resources that are used to run",
    "start": "536320",
    "end": "539120"
  },
  {
    "text": "your model servers",
    "start": "539120",
    "end": "542000"
  },
  {
    "text": "and based on that live",
    "start": "542000",
    "end": "544440"
  },
  {
    "text": "reporting things like KV cache",
    "start": "544440",
    "end": "547040"
  },
  {
    "text": "utilization uh which model servers have",
    "start": "547040",
    "end": "550640"
  },
  {
    "text": "which fine-tuned adapters loaded and so",
    "start": "550640",
    "end": "553440"
  },
  {
    "text": "forth it gets to make this very",
    "start": "553440",
    "end": "555360"
  },
  {
    "text": "intelligent decision on where to route",
    "start": "555360",
    "end": "556959"
  },
  {
    "text": "that traffic to and when it makes that",
    "start": "556959",
    "end": "559200"
  },
  {
    "text": "decision it forwards it back to the",
    "start": "559200",
    "end": "561200"
  },
  {
    "text": "gateway and says \"Hey gateway use this",
    "start": "561200",
    "end": "563600"
  },
  {
    "text": "endpoint this IP address to forward that",
    "start": "563600",
    "end": "566080"
  },
  {
    "text": "traffic off to.\"",
    "start": "566080",
    "end": "569640"
  },
  {
    "text": "So now with with that understanding",
    "start": "570640",
    "end": "572480"
  },
  {
    "text": "let's take a little bit uh deeper look",
    "start": "572480",
    "end": "574560"
  },
  {
    "text": "into the inference pool resource again",
    "start": "574560",
    "end": "577440"
  },
  {
    "text": "this is a resource that's going to be",
    "start": "577440",
    "end": "578880"
  },
  {
    "text": "managed by uh the uh the platform",
    "start": "578880",
    "end": "582839"
  },
  {
    "text": "admin and this is an abstraction as I",
    "start": "582839",
    "end": "585760"
  },
  {
    "text": "mentioned just a few minutes ago as a",
    "start": "585760",
    "end": "588959"
  },
  {
    "text": "way to abstract away u all the the",
    "start": "588959",
    "end": "593800"
  },
  {
    "text": "resources used to um to um support",
    "start": "593800",
    "end": "598880"
  },
  {
    "text": "inference for your model servers right",
    "start": "598880",
    "end": "601519"
  },
  {
    "text": "and so um as we look at the",
    "start": "601519",
    "end": "604720"
  },
  {
    "text": "specification here we see that uh we use",
    "start": "604720",
    "end": "607200"
  },
  {
    "text": "standard u uh selector mechanism to uh",
    "start": "607200",
    "end": "611600"
  },
  {
    "text": "to use um labels on all of your",
    "start": "611600",
    "end": "616000"
  },
  {
    "text": "different model server pods and to group",
    "start": "616000",
    "end": "618959"
  },
  {
    "text": "those pods together using uh this",
    "start": "618959",
    "end": "621920"
  },
  {
    "text": "selector",
    "start": "621920",
    "end": "623399"
  },
  {
    "text": "mechanism and then there's a target port",
    "start": "623399",
    "end": "625760"
  },
  {
    "text": "number right so as a request comes in uh",
    "start": "625760",
    "end": "629200"
  },
  {
    "text": "we want to know which port should uh be",
    "start": "629200",
    "end": "631920"
  },
  {
    "text": "used as we forward the traffic to the",
    "start": "631920",
    "end": "634720"
  },
  {
    "text": "final destination of those pods and then",
    "start": "634720",
    "end": "637680"
  },
  {
    "text": "you see an extension reference right so",
    "start": "637680",
    "end": "640079"
  },
  {
    "text": "I mentioned that the endpoint picker is",
    "start": "640079",
    "end": "642480"
  },
  {
    "text": "is a pod that has that runs separately",
    "start": "642480",
    "end": "645279"
  },
  {
    "text": "from your gateway typically has a",
    "start": "645279",
    "end": "647600"
  },
  {
    "text": "service to uh to abstract away the pod",
    "start": "647600",
    "end": "651360"
  },
  {
    "text": "where you may have multiple instances of",
    "start": "651360",
    "end": "653440"
  },
  {
    "text": "that pod but this is used by your",
    "start": "653440",
    "end": "656720"
  },
  {
    "text": "gateway to discover where this endpoint",
    "start": "656720",
    "end": "659360"
  },
  {
    "text": "picker is running and how to communicate",
    "start": "659360",
    "end": "661440"
  },
  {
    "text": "to it right because if that gateway is",
    "start": "661440",
    "end": "663279"
  },
  {
    "text": "going to go ahead and say \"Oh I need to",
    "start": "663279",
    "end": "664560"
  },
  {
    "text": "forward this traffic to the endpoint",
    "start": "664560",
    "end": "666480"
  },
  {
    "text": "picker to decide which endpoint that I",
    "start": "666480",
    "end": "668880"
  },
  {
    "text": "should route to i need to somehow tell",
    "start": "668880",
    "end": "671760"
  },
  {
    "text": "the gateway this is where this endpoint",
    "start": "671760",
    "end": "673839"
  },
  {
    "text": "picker resides.\" And then the inference",
    "start": "673839",
    "end": "676800"
  },
  {
    "text": "model is uh again the persona assigned",
    "start": "676800",
    "end": "680240"
  },
  {
    "text": "uh to the inference model is going to be",
    "start": "680240",
    "end": "682160"
  },
  {
    "text": "your workload owner and the the",
    "start": "682160",
    "end": "684560"
  },
  {
    "text": "inference workload owner is able to say",
    "start": "684560",
    "end": "687519"
  },
  {
    "text": "here's my target model so I can go ahead",
    "start": "687519",
    "end": "689920"
  },
  {
    "text": "and take that that model name from the",
    "start": "689920",
    "end": "692880"
  },
  {
    "text": "body and then change it to the target",
    "start": "692880",
    "end": "696399"
  },
  {
    "text": "model as it goes and hits your inference",
    "start": "696399",
    "end": "699040"
  },
  {
    "text": "uh",
    "start": "699040",
    "end": "699720"
  },
  {
    "text": "server why is that important because",
    "start": "699720",
    "end": "702399"
  },
  {
    "text": "again I may want to do traffic splitting",
    "start": "702399",
    "end": "704800"
  },
  {
    "text": "i may want to do canary rollouts uh",
    "start": "704800",
    "end": "708480"
  },
  {
    "text": "based on weights uh you see here um and",
    "start": "708480",
    "end": "712320"
  },
  {
    "text": "so this is a quick overview of the",
    "start": "712320",
    "end": "714480"
  },
  {
    "text": "inference model resource as",
    "start": "714480",
    "end": "717000"
  },
  {
    "text": "well okay um so we did have a slide here",
    "start": "717000",
    "end": "720720"
  },
  {
    "text": "on the benchmarks we did some benchmark",
    "start": "720720",
    "end": "722240"
  },
  {
    "text": "testing uh you're going to see uh a",
    "start": "722240",
    "end": "724480"
  },
  {
    "text": "Kubernetes uh blog post that myself and",
    "start": "724480",
    "end": "727600"
  },
  {
    "text": "some of the other um maintainers created",
    "start": "727600",
    "end": "730560"
  },
  {
    "text": "that's going to be coming out that uh",
    "start": "730560",
    "end": "732240"
  },
  {
    "text": "details the benchmark testing that we've",
    "start": "732240",
    "end": "734560"
  },
  {
    "text": "did that that we've accomplished uh so",
    "start": "734560",
    "end": "737200"
  },
  {
    "text": "take a look at that or or you could even",
    "start": "737200",
    "end": "738800"
  },
  {
    "text": "go to uh the website here uh so if uh if",
    "start": "738800",
    "end": "742639"
  },
  {
    "text": "you'd like to learn more about the",
    "start": "742639",
    "end": "744639"
  },
  {
    "text": "project",
    "start": "744639",
    "end": "746160"
  },
  {
    "text": "yeah scan the QR code and then get",
    "start": "746160",
    "end": "748160"
  },
  {
    "text": "involved open an issue if you have any",
    "start": "748160",
    "end": "751480"
  },
  {
    "text": "ideas we'll leave that up for a few",
    "start": "751480",
    "end": "753680"
  },
  {
    "text": "seconds and that's all for us thank you",
    "start": "753680",
    "end": "755760"
  },
  {
    "text": "very much thank you thank you",
    "start": "755760",
    "end": "760079"
  },
  {
    "text": "i don't think we have All right so thank",
    "start": "760079",
    "end": "762000"
  },
  {
    "text": "you very much this was a nice uh nice",
    "start": "762000",
    "end": "764240"
  },
  {
    "text": "overview of what's coming uh there is",
    "start": "764240",
    "end": "766880"
  },
  {
    "text": "time for one question if someone wants a",
    "start": "766880",
    "end": "769279"
  },
  {
    "text": "quick one yeah there is one there",
    "start": "769279",
    "end": "772880"
  },
  {
    "text": "hello yeah thank you for the talk and",
    "start": "772880",
    "end": "775360"
  },
  {
    "text": "introduction so my question is um uh you",
    "start": "775360",
    "end": "778959"
  },
  {
    "text": "know like uh in the previous talk this",
    "start": "778959",
    "end": "781279"
  },
  {
    "text": "like uh KV cache locality is is",
    "start": "781279",
    "end": "784320"
  },
  {
    "text": "important for the whole system so in",
    "start": "784320",
    "end": "788800"
  },
  {
    "text": "your Thank you uh so in your uh gateway",
    "start": "789560",
    "end": "793360"
  },
  {
    "text": "design where should we implement that",
    "start": "793360",
    "end": "796720"
  },
  {
    "text": "like uh prefix awareness in routing",
    "start": "796720",
    "end": "801680"
  },
  {
    "text": "where should you plug the KV cache",
    "start": "801680",
    "end": "803200"
  },
  {
    "text": "essentially in the G API",
    "start": "803200",
    "end": "805920"
  },
  {
    "text": "where should you plug in the KV cache um",
    "start": "805920",
    "end": "808800"
  },
  {
    "text": "for example if we have some system to",
    "start": "808800",
    "end": "811040"
  },
  {
    "text": "reduce KV cache",
    "start": "811040",
    "end": "813920"
  },
  {
    "text": "uh if you have a system to reduce KV",
    "start": "813920",
    "end": "816240"
  },
  {
    "text": "cache where should you plug that into",
    "start": "816240",
    "end": "817839"
  },
  {
    "text": "the system that that might even be um an",
    "start": "817839",
    "end": "820959"
  },
  {
    "text": "you know a an that might be an op an",
    "start": "820959",
    "end": "825200"
  },
  {
    "text": "opportunity to actually create an",
    "start": "825200",
    "end": "827279"
  },
  {
    "text": "extension um based on that KV cache uh",
    "start": "827279",
    "end": "830800"
  },
  {
    "text": "system or what we're also looking at",
    "start": "830800",
    "end": "832959"
  },
  {
    "text": "doing is the endpoint picker or the",
    "start": "832959",
    "end": "835120"
  },
  {
    "text": "endpoint selection extension uh we're",
    "start": "835120",
    "end": "838320"
  },
  {
    "text": "looking at making that pluggable and so",
    "start": "838320",
    "end": "841680"
  },
  {
    "text": "right now the endpoint selection",
    "start": "841680",
    "end": "843440"
  },
  {
    "text": "extension based on the API semantics",
    "start": "843440",
    "end": "846320"
  },
  {
    "text": "that I shared with you it knows about",
    "start": "846320",
    "end": "848399"
  },
  {
    "text": "all the pods running in a particular",
    "start": "848399",
    "end": "850560"
  },
  {
    "text": "pool and um and is directly",
    "start": "850560",
    "end": "854720"
  },
  {
    "text": "communicating with those pods to scrape",
    "start": "854720",
    "end": "857600"
  },
  {
    "text": "metrics from those pods of KV cache",
    "start": "857600",
    "end": "859839"
  },
  {
    "text": "utilization and and so forth we",
    "start": "859839",
    "end": "861600"
  },
  {
    "text": "basically have a spec design uh for all",
    "start": "861600",
    "end": "864240"
  },
  {
    "text": "the different uh model server frameworks",
    "start": "864240",
    "end": "866480"
  },
  {
    "text": "to follow so it doesn't matter if it's",
    "start": "866480",
    "end": "868959"
  },
  {
    "text": "Triton or VLM we can go ahead and have a",
    "start": "868959",
    "end": "872160"
  },
  {
    "text": "consistent set of metrics that are used",
    "start": "872160",
    "end": "874800"
  },
  {
    "text": "to formulate the scheduling decision",
    "start": "874800",
    "end": "877040"
  },
  {
    "text": "right and so one of two things could ex",
    "start": "877040",
    "end": "880240"
  },
  {
    "text": "exist is either being able to uh like I",
    "start": "880240",
    "end": "883040"
  },
  {
    "text": "said make the scheduler a little more",
    "start": "883040",
    "end": "885600"
  },
  {
    "text": "flexible and instead of pulling that",
    "start": "885600",
    "end": "887519"
  },
  {
    "text": "from the pods we pull it from this",
    "start": "887519",
    "end": "889920"
  },
  {
    "text": "system that you're talking about that",
    "start": "889920",
    "end": "891279"
  },
  {
    "text": "has this KV cache awareness right um and",
    "start": "891279",
    "end": "895760"
  },
  {
    "text": "will the extension API right now being",
    "start": "895760",
    "end": "898160"
  },
  {
    "text": "able to pass the for example the input",
    "start": "898160",
    "end": "901279"
  },
  {
    "text": "content prompt or it's just getting the",
    "start": "901279",
    "end": "905199"
  },
  {
    "text": "matrix",
    "start": "905199",
    "end": "906959"
  },
  {
    "text": "yes right now the the theuler for the",
    "start": "906959",
    "end": "910000"
  },
  {
    "text": "endpoint picker is using the the metrics",
    "start": "910000",
    "end": "913760"
  },
  {
    "text": "that it's pulling from the",
    "start": "913760",
    "end": "916920"
  },
  {
    "text": "endpoints gotcha so so it can't really",
    "start": "916920",
    "end": "919920"
  },
  {
    "text": "pass the content of the data in the",
    "start": "919920",
    "end": "923279"
  },
  {
    "text": "request right so because KV cache",
    "start": "923279",
    "end": "925600"
  },
  {
    "text": "awareness you need to understand the",
    "start": "925600",
    "end": "927680"
  },
  {
    "text": "input context and the prompt",
    "start": "927680",
    "end": "930399"
  },
  {
    "text": "so the endpoint picker does not see the",
    "start": "930399",
    "end": "932399"
  },
  {
    "text": "content of the prompt the prompt gets",
    "start": "932399",
    "end": "934880"
  },
  {
    "text": "into the gateway and then the the the",
    "start": "934880",
    "end": "936639"
  },
  {
    "text": "the gateway extension actually passed",
    "start": "936639",
    "end": "939199"
  },
  {
    "text": "that right all right right so so so the",
    "start": "939199",
    "end": "942519"
  },
  {
    "text": "request the entire request the header",
    "start": "942519",
    "end": "944880"
  },
  {
    "text": "the body the trailer does get sent to",
    "start": "944880",
    "end": "947839"
  },
  {
    "text": "the endpoint picker but the uh the",
    "start": "947839",
    "end": "950399"
  },
  {
    "text": "endpoint picker from a a scheduling",
    "start": "950399",
    "end": "952399"
  },
  {
    "text": "standpoint what it's looking at in the",
    "start": "952399",
    "end": "953920"
  },
  {
    "text": "body is really just the the model name",
    "start": "953920",
    "end": "956800"
  },
  {
    "text": "um and again as use cases uh evolve we",
    "start": "956800",
    "end": "962320"
  },
  {
    "text": "could look at either extending the",
    "start": "962320",
    "end": "963839"
  },
  {
    "text": "endpoint picker the scheduler",
    "start": "963839",
    "end": "966639"
  },
  {
    "text": "introducing a new extension so um what I",
    "start": "966639",
    "end": "969440"
  },
  {
    "text": "would urge you to do is use that uh QR",
    "start": "969440",
    "end": "972959"
  },
  {
    "text": "code go to the the site and maybe even",
    "start": "972959",
    "end": "975680"
  },
  {
    "text": "open open up an issue or conversation",
    "start": "975680",
    "end": "977519"
  },
  {
    "text": "about the use case that you're sharing",
    "start": "977519",
    "end": "979920"
  },
  {
    "text": "with us today okay friendship",
    "start": "979920",
    "end": "984480"
  }
]