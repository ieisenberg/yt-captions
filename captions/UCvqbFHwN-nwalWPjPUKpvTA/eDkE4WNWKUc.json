[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "hello everyone my name is Abdullah gorobei I'm from Google and I'm the",
    "start": "2010",
    "end": "8040"
  },
  {
    "text": "co-chair of six scheduling today I'm gonna talk about what we've been up to",
    "start": "8040",
    "end": "15330"
  },
  {
    "text": "in say scheduling for the Pat in the past two Childress's so this is the agenda for for today's",
    "start": "15330",
    "end": "24060"
  },
  {
    "text": "talk so I'm going to quickly go through a quick introduction of what the",
    "start": "24060",
    "end": "29460"
  },
  {
    "text": "schedule is and then an assorted list of recent developments and planned features",
    "start": "29460",
    "end": "37040"
  },
  {
    "text": "my plan is to go through this quickly like maybe 20 25 minutes 25 minutes and",
    "start": "37040",
    "end": "43410"
  },
  {
    "text": "then leave the rest for for questions so what is the schedule so like everything",
    "start": "43410",
    "end": "50070"
  },
  {
    "start": "46000",
    "end": "283000"
  },
  {
    "text": "else in in kubernetes it's just another controller this controller usually runs",
    "start": "50070",
    "end": "56250"
  },
  {
    "text": "on the master pre watches for pods and",
    "start": "56250",
    "end": "61559"
  },
  {
    "text": "nodes that are created on a CD and what it does basically it assigns pods to",
    "start": "61559",
    "end": "67560"
  },
  {
    "text": "nodes has nothing to do with the pod life cycle at all it doesn't actually create and run the pod in the node it",
    "start": "67560",
    "end": "73740"
  },
  {
    "text": "only thing it does is it creates a sub resource in the pod object that says pod",
    "start": "73740",
    "end": "79409"
  },
  {
    "text": "X should run on nor do I once this happens the assumption is that the",
    "start": "79409",
    "end": "85470"
  },
  {
    "text": "cubelet that runs on the node that was assigned to the pod should pick up that",
    "start": "85470",
    "end": "90780"
  },
  {
    "text": "pod create the pod and started up at that point this kid or his hand off what has",
    "start": "90780",
    "end": "96540"
  },
  {
    "text": "nothing to do with the pod so as I mentioned the scheduler watches for pods",
    "start": "96540",
    "end": "105240"
  },
  {
    "text": "and and nodes those pods that are added there are that the new pods are get",
    "start": "105240",
    "end": "112439"
  },
  {
    "text": "updated they are added into a scheduling queue and this scheduling queue is",
    "start": "112439",
    "end": "120420"
  },
  {
    "text": "sorted by pod priority so it's basically a priority queue and the scalar",
    "start": "120420",
    "end": "126210"
  },
  {
    "text": "basically picks one part at a time runs it through so far has been two phases",
    "start": "126210",
    "end": "132239"
  },
  {
    "text": "the first is what we call like filtering fees so we run the part through a bunch of",
    "start": "132239",
    "end": "138960"
  },
  {
    "text": "filters that decides where what are the feasible nodes that the pod can run on",
    "start": "138960",
    "end": "145460"
  },
  {
    "text": "example filters are as you would expect is whether the node has enough resources",
    "start": "145460",
    "end": "151430"
  },
  {
    "text": "to run the power based on the pod resource requests other filters could be",
    "start": "151430",
    "end": "158820"
  },
  {
    "text": "for example Prada affinity like required port affinity if the we need only to",
    "start": "158820",
    "end": "164340"
  },
  {
    "text": "look at the nose that has the pause that this pod has affinity with and and so on",
    "start": "164340",
    "end": "170880"
  },
  {
    "text": "and so forth so after this phase we filter out the nodes that are unfeasible and we end up with a set of feasible",
    "start": "170880",
    "end": "177900"
  },
  {
    "text": "knows those feasible nodes goes through what we call scoring phase basically we",
    "start": "177900",
    "end": "184230"
  },
  {
    "text": "have a bunch of scoring functions that rank the feasible nodes and we pick the highest scoring node scoring functions",
    "start": "184230",
    "end": "193050"
  },
  {
    "text": "can be for example preferred pod affinity rather than like required so if",
    "start": "193050",
    "end": "198120"
  },
  {
    "text": "you decide that I want like I prefer that pod XP run with pod Y so if the",
    "start": "198120",
    "end": "205500"
  },
  {
    "text": "noise that has pod Y the norther has would have higher ranking than other nodes other priority functions for",
    "start": "205500",
    "end": "211980"
  },
  {
    "text": "example is one that ranks nodes with that has already the image that the",
    "start": "211980",
    "end": "218370"
  },
  {
    "text": "Parden needs it has like for example a higher ranking and so on and so forth so",
    "start": "218370",
    "end": "224850"
  },
  {
    "text": "after we again after we score the nodes we pick the highest scoring node and then we just do the binding that I",
    "start": "224850",
    "end": "231150"
  },
  {
    "text": "mentioned earlier and then the cycle goes on and on and on if for some reason",
    "start": "231150",
    "end": "236910"
  },
  {
    "text": "the pod ended up after the schedule exciter will zero feasible nodes we",
    "start": "236910",
    "end": "242610"
  },
  {
    "text": "place the bot back and what we call a back of queue so basically like the back",
    "start": "242610",
    "end": "248550"
  },
  {
    "text": "of queue for each part has a back of timer we add it back to the active queue",
    "start": "248550",
    "end": "255060"
  },
  {
    "text": "so that we can try to schedule the part again so what I just described before this is what we call a scheduling",
    "start": "255060",
    "end": "261209"
  },
  {
    "text": "attempt and a pod could we could attempt to scheduler called multiple times and",
    "start": "261210",
    "end": "267220"
  },
  {
    "text": "schedule and this is goes until infinity so until the project schedule they will always be moving between the back of",
    "start": "267220",
    "end": "273670"
  },
  {
    "text": "queue and and and the active queue we do have something called unscheduled will",
    "start": "273670",
    "end": "279100"
  },
  {
    "text": "queue but I don't want to get into too much detail on that right now okay so",
    "start": "279100",
    "end": "284290"
  },
  {
    "start": "283000",
    "end": "658000"
  },
  {
    "text": "after this quick introduction I want to talk about what we have been up to recently so we have I have like five",
    "start": "284290",
    "end": "293310"
  },
  {
    "text": "major developments that I want to talk about the first one which i think is the",
    "start": "293310",
    "end": "299170"
  },
  {
    "text": "most important is scheduling framework I believe in the passing pass cube cause",
    "start": "299170",
    "end": "304450"
  },
  {
    "text": "have talked about our plans to restructure refactor the core of the",
    "start": "304450",
    "end": "309940"
  },
  {
    "text": "scheduler into what we call a scheduling framework and basically what we wanted to do is we wanted to turn it into a",
    "start": "309940",
    "end": "315940"
  },
  {
    "text": "pluggable component like the previous talk that was here core DNS or if you know envoy as well we wanted to turn the",
    "start": "315940",
    "end": "325690"
  },
  {
    "text": "default schedule into an engine that executes plugins and you and and the",
    "start": "325690",
    "end": "332169"
  },
  {
    "text": "idea is that we wanted to define a number of extension points that we've",
    "start": "332169",
    "end": "337570"
  },
  {
    "text": "identified from our past experiences with their scheduler and those extension",
    "start": "337570",
    "end": "342610"
  },
  {
    "text": "points you can register callback functions and a collection of callback",
    "start": "342610",
    "end": "349330"
  },
  {
    "text": "functions that define a specific behavior we're going to call it a plugin and I'm gonna give a number of examples",
    "start": "349330",
    "end": "355360"
  },
  {
    "text": "about this but what we've identified is the following the following session",
    "start": "355360",
    "end": "361750"
  },
  {
    "text": "point so from the top left the sort so if you what I'm describing here this",
    "start": "361750",
    "end": "367630"
  },
  {
    "text": "this line is the scheduling a scheduling attempt of a plug-in it goes through these phases while we are trying to",
    "start": "367630",
    "end": "373419"
  },
  {
    "text": "schedule a specific part the sort extension point is a little bit special",
    "start": "373419",
    "end": "379479"
  },
  {
    "text": "in that you can only define a single plug-in that defines how we sort the",
    "start": "379479",
    "end": "386200"
  },
  {
    "text": "scheduling queue the default scheduling behavior is that as I mentioned earlier we sorted by pod priority but maybe you",
    "start": "386200",
    "end": "393130"
  },
  {
    "text": "don't care about priority you want to schedule based on requests or resource requests or whatever",
    "start": "393130",
    "end": "399660"
  },
  {
    "text": "custom sorting behavior that you want so you can't customize that now the second",
    "start": "399660",
    "end": "406890"
  },
  {
    "text": "phase is again like if you look at it is there is pre filter filter and post filter this is what I talked about",
    "start": "406890",
    "end": "412440"
  },
  {
    "text": "earlier it's the phase where we define the feasible nodes but we found that we",
    "start": "412440",
    "end": "419760"
  },
  {
    "text": "needed another two phases another two extension points before and after the filter that allows us to run like to",
    "start": "419760",
    "end": "427770"
  },
  {
    "text": "optimize that behavior sometimes features like what affinity for example it runs much faster if we do some sort",
    "start": "427770",
    "end": "434940"
  },
  {
    "text": "of pre calculation and that's what we would usually do in a pre-filter phase so a pre-filter phase we look at at the",
    "start": "434940",
    "end": "443850"
  },
  {
    "text": "pod for example state or even the cluster state we summarize it and then",
    "start": "443850",
    "end": "448920"
  },
  {
    "text": "for each node in the filtering phase we execute the filters using the exact same",
    "start": "448920",
    "end": "455930"
  },
  {
    "text": "metadata let's call it or pre calculation that we did in the previous in the previous extension point the post",
    "start": "455930",
    "end": "464010"
  },
  {
    "text": "filter we felt that we didn't really need the pre scoring so we call just post filter so you either want to",
    "start": "464010",
    "end": "469110"
  },
  {
    "text": "release some resources that you are located in the and the fill and the filter or preferred the phase you can",
    "start": "469110",
    "end": "474450"
  },
  {
    "text": "also do some sort of pre scoring you can also do some sort of pre calculations or pre computations before you go into for",
    "start": "474450",
    "end": "482370"
  },
  {
    "text": "we trying to score scoring the nodes and then there's a phase called normalized",
    "start": "482370",
    "end": "488580"
  },
  {
    "text": "scoring so even usually the way that we we do things that we execute those",
    "start": "488580",
    "end": "495180"
  },
  {
    "text": "extension points per node but then there there are some scoring functions that",
    "start": "495180",
    "end": "500970"
  },
  {
    "text": "you want to normalize like after you have scored each node you want to do some sort of a normalization and this is",
    "start": "500970",
    "end": "506370"
  },
  {
    "text": "what this this extension point allows you to do and then there's a reserve",
    "start": "506370",
    "end": "512340"
  },
  {
    "text": "where you actually allocate the resources you decide that okay this node is going to take up these resources from",
    "start": "512340",
    "end": "520080"
  },
  {
    "text": "this in that node from this node for example and this is where we're going to update the cache that we have in the",
    "start": "520080",
    "end": "526500"
  },
  {
    "text": "scheduler to determine that this this part is going to run on this on this",
    "start": "526500",
    "end": "531630"
  },
  {
    "text": "node and this is the first phase we call it a scheduling cycle the next phase we",
    "start": "531630",
    "end": "538890"
  },
  {
    "text": "called the binding cycle runs in a separate girl routine so now the scheduler in the main go routine will",
    "start": "538890",
    "end": "544830"
  },
  {
    "text": "pick up another schedule another part and will go through the green phase we",
    "start": "544830",
    "end": "550260"
  },
  {
    "text": "have here and in parallel the binding phase will run for previous parts that are already been associated with",
    "start": "550260",
    "end": "557190"
  },
  {
    "text": "specific node in the binding phase as I mentioned before we try to bind the part",
    "start": "557190",
    "end": "563460"
  },
  {
    "text": "with with the node the reason we do it in a separate go routine is because it requires calling into the API server",
    "start": "563460",
    "end": "570240"
  },
  {
    "text": "because we want to write and update the part object with whichever node the pod",
    "start": "570240",
    "end": "575580"
  },
  {
    "text": "will run on and because as you might know we we do throttle components that",
    "start": "575580",
    "end": "582030"
  },
  {
    "text": "talk to the API server because we don't want to dust the API sever and for that",
    "start": "582030",
    "end": "587250"
  },
  {
    "text": "reason the binding might not like succeed quickly and so we needed a",
    "start": "587250",
    "end": "593130"
  },
  {
    "text": "separate go routine to keep trying until the binding actually succeeds now we we",
    "start": "593130",
    "end": "599100"
  },
  {
    "text": "added an interesting accession point before that which is what we call the pyramid plugin and in pyramid what we",
    "start": "599100",
    "end": "605610"
  },
  {
    "text": "will allow plugins to do is to hold on to binding the pod until a specific",
    "start": "605610",
    "end": "611550"
  },
  {
    "text": "condition is satisfied and one use case",
    "start": "611550",
    "end": "617310"
  },
  {
    "text": "that we had in mind is what we call gank scheduling if you want to not actually start the pod until all the pods that",
    "start": "617310",
    "end": "625080"
  },
  {
    "text": "belong to whatever like for example we can define them using a label selector",
    "start": "625080",
    "end": "630620"
  },
  {
    "text": "schedule label so they have been reserved and like some resource in specific node only at that point you",
    "start": "631460",
    "end": "637980"
  },
  {
    "text": "want to basically say like release okay bind all those parts together and then",
    "start": "637980",
    "end": "643110"
  },
  {
    "text": "we will schedule together so that's one interesting use case we had as trying to make a schedule sort of friendlier for",
    "start": "643110",
    "end": "649860"
  },
  {
    "text": "batch workload I don't think we are there yet but I would picture but we are trying to make it as such so there are a",
    "start": "649860",
    "end": "659220"
  },
  {
    "start": "658000",
    "end": "716000"
  },
  {
    "text": "number of benefits for a schedule framework again as I mentioned it makes",
    "start": "659220",
    "end": "664530"
  },
  {
    "text": "it kind of easier to extend and isolate features so before it was like the specific feature was",
    "start": "664530",
    "end": "672750"
  },
  {
    "text": "spread across different files and every time you want to add a new feature you know like these ugly patches was harder",
    "start": "672750",
    "end": "678990"
  },
  {
    "text": "for developers to actually fork the code and add new features so we wanted to be more pluggable and basically isolate the",
    "start": "678990",
    "end": "686040"
  },
  {
    "text": "features into specific all the feature will be in a specific file that implements all the callbacks that gets",
    "start": "686040",
    "end": "691260"
  },
  {
    "text": "executed in these extension points and last but not least not least previously",
    "start": "691260",
    "end": "699060"
  },
  {
    "text": "we only had predicates and priorities which is the one I discussed earlier scoring and and filtering now I wanted",
    "start": "699060",
    "end": "705600"
  },
  {
    "text": "to add more more behavior and more complex behavior as I mentioned the",
    "start": "705600",
    "end": "711540"
  },
  {
    "text": "permit extension point and Aang gang scheduling so what is the saddle",
    "start": "711540",
    "end": "718230"
  },
  {
    "start": "716000",
    "end": "872000"
  },
  {
    "text": "scheduling framework like I'm we're really happy to announce that the framework itself is fully implemented",
    "start": "718230",
    "end": "724740"
  },
  {
    "text": "it's an alpha right now but then I think the next big like the other biggest",
    "start": "724740",
    "end": "731400"
  },
  {
    "text": "thing that we did in 1.17 and I want to thank the six scheduling community for a",
    "start": "731400",
    "end": "736770"
  },
  {
    "text": "lot for allowing us to do that is we wrapped all existing predicates and priorities into plugins so we had now we",
    "start": "736770",
    "end": "745320"
  },
  {
    "text": "had a chance to actually really make sure the scheduling framework its",
    "start": "745320",
    "end": "751140"
  },
  {
    "text": "implementation actually works covers mostly at least the cases that we already have for the features that the",
    "start": "751140",
    "end": "757080"
  },
  {
    "text": "scheduler implements it requires us I think over 100 PRS in the past few",
    "start": "757080",
    "end": "764760"
  },
  {
    "text": "months so it was a massive massive effort during that though we don't want",
    "start": "764760",
    "end": "771120"
  },
  {
    "text": "to break existing functionalities we don't want to break the existing API that allowed users to define predicates",
    "start": "771120",
    "end": "777990"
  },
  {
    "text": "and priorities through what we call policy policy files and so we added this",
    "start": "777990",
    "end": "783450"
  },
  {
    "text": "translation layer that translates legacy policy file definition of predicates",
    "start": "783450",
    "end": "792060"
  },
  {
    "text": "priorities into what we have now as as plugins so those plugins that trap",
    "start": "792060",
    "end": "797990"
  },
  {
    "text": "the predicates and priorities they don't actually have substantial amount of code on them the only thing they have is they",
    "start": "797990",
    "end": "803779"
  },
  {
    "text": "actually falling back into the older predicates priority functions and so so",
    "start": "803779",
    "end": "808910"
  },
  {
    "text": "far we have two execution paths is one is active one is not in the course scheduler hopefully now in milestone two",
    "start": "808910",
    "end": "815180"
  },
  {
    "text": "or 1.18 we after we make sure that people like in 1.17 we have no major",
    "start": "815180",
    "end": "821509"
  },
  {
    "text": "regressions or or bugs we're going to completely duplicate all the predicates",
    "start": "821509",
    "end": "828230"
  },
  {
    "text": "and priorities and move their coding to native plugins basically run as native plugins and that would allow us to",
    "start": "828230",
    "end": "836209"
  },
  {
    "text": "duplicate the old policy API and hopefully we will have the component",
    "start": "836209",
    "end": "842689"
  },
  {
    "text": "config in GA by that time and the replace is going to be the component",
    "start": "842689",
    "end": "847999"
  },
  {
    "text": "conflict plug-in API for it and milestone 3 and 1.19 we're going to",
    "start": "847999",
    "end": "854600"
  },
  {
    "text": "actually duplicate the policy API completely and remove it and implement more features that we have in the scalar",
    "start": "854600",
    "end": "861709"
  },
  {
    "text": "as plugins like preemption and the sorting logic and the binding logic actually moved them as as plugins and",
    "start": "861709",
    "end": "869379"
  },
  {
    "text": "hopefully framework could be in GA so in the second improve the second",
    "start": "869379",
    "end": "874879"
  },
  {
    "start": "872000",
    "end": "923000"
  },
  {
    "text": "development is performance improvements in 1.17 during this major refactoring we",
    "start": "874879",
    "end": "880189"
  },
  {
    "text": "identify the number of opportunities to improve the scheduler behavior so we managed to improve the scheduler",
    "start": "880189",
    "end": "886389"
  },
  {
    "text": "scheduling latency by 2x but that's only for large clusters like 5k clusters and",
    "start": "886389",
    "end": "893120"
  },
  {
    "text": "the other thing what we managed to do is also improve preferred pod affinity that",
    "start": "893120",
    "end": "899600"
  },
  {
    "text": "was an algorithmic improvement we managed to change the complexity of the algorithm from and and P where n is the",
    "start": "899600",
    "end": "907009"
  },
  {
    "text": "number of nodes and P is the number of pods to to P which is basically depends",
    "start": "907009",
    "end": "912259"
  },
  {
    "text": "on how many poles you already have in the cluster because affinity depends now you need to look at whatever parts they have running and then based on that you",
    "start": "912259",
    "end": "918709"
  },
  {
    "text": "calculate where I'm going to place the pod another thing that we felt was a",
    "start": "918709",
    "end": "926449"
  },
  {
    "start": "923000",
    "end": "1081000"
  },
  {
    "text": "little bit lacking is improved observability we had already a bunch of metrics that the scheduler produced but we felt we",
    "start": "926449",
    "end": "934480"
  },
  {
    "text": "can do better so we we try to to improve things across",
    "start": "934480",
    "end": "939610"
  },
  {
    "text": "three different categories the first one is latency so tip until recently we had",
    "start": "939610",
    "end": "946050"
  },
  {
    "text": "scheduling latency of the attempt so you don't know how long a pod was actually",
    "start": "946050",
    "end": "951699"
  },
  {
    "text": "it took it took us to schedule a pod but the only metric we had was the scheduling attempt as you know a pod can",
    "start": "951699",
    "end": "958899"
  },
  {
    "text": "be attempted multiple time so with it so we add a new metric to track how long it actually took from the time we picked up",
    "start": "958899",
    "end": "966040"
  },
  {
    "text": "the pod from the informal cache until the pod was bound to a node and the",
    "start": "966040",
    "end": "973540"
  },
  {
    "text": "other thing we we added is latency breakdown of of each scheduling step or",
    "start": "973540",
    "end": "979540"
  },
  {
    "text": "or plugin so we have better tracing and ability to break down by if you want to",
    "start": "979540",
    "end": "985149"
  },
  {
    "text": "see okay why is my partner getting scale oh my my schedule is slow you can now break down",
    "start": "985149",
    "end": "991000"
  },
  {
    "text": "by plugins basically kinds you can see if pod affinity is the one that is what",
    "start": "991000",
    "end": "997149"
  },
  {
    "text": "affinity plugin is the one that is you know slowing things down etc the other",
    "start": "997149",
    "end": "1004649"
  },
  {
    "text": "thing is a catalyst traffic we added metrics to track number of incoming pods per second so you can see how fast is",
    "start": "1004649",
    "end": "1012689"
  },
  {
    "text": "scheduled and like draining that that that queue and again like scheduling",
    "start": "1012689",
    "end": "1017730"
  },
  {
    "text": "attempts per second all those we felt are lacking in trying to understand whether the schedule itself is",
    "start": "1017730",
    "end": "1022970"
  },
  {
    "text": "misbehaving or is slow or like for whatever reason we just want more more insights into the scheduler saturation",
    "start": "1022970",
    "end": "1030750"
  },
  {
    "text": "again is related to how much resources the scheduler is consuming in terms of number of binding goal routines that I",
    "start": "1030750",
    "end": "1036600"
  },
  {
    "text": "talked about before and the cache size so the the scaler has a hazard has its",
    "start": "1036600",
    "end": "1045360"
  },
  {
    "text": "cache like after even on top of the informal cache because we want to",
    "start": "1045360",
    "end": "1050549"
  },
  {
    "text": "annotate the nodes and the path with extra information so we have our on our own cache that we actually snapshot for",
    "start": "1050549",
    "end": "1056970"
  },
  {
    "text": "each scheduling cycle so that when we schedule apart we want to have distant state while executing all these",
    "start": "1056970",
    "end": "1063679"
  },
  {
    "text": "extension points so we were concerned about this overhead so tracking the cache size which going to be",
    "start": "1063679",
    "end": "1069470"
  },
  {
    "text": "proportional to how big is your cluster that probably should help you also with like if you look at the history how you",
    "start": "1069470",
    "end": "1076280"
  },
  {
    "text": "provisioned the resources for the for the scheduler itself another thing that",
    "start": "1076280",
    "end": "1083690"
  },
  {
    "start": "1081000",
    "end": "1129000"
  },
  {
    "text": "we've graduated that we implemented and we promised in previous few cons is pod",
    "start": "1083690",
    "end": "1089150"
  },
  {
    "text": "quality spreading I think it was one on one of the keynotes it was it was mentioned before that we had a pod ante",
    "start": "1089150",
    "end": "1097850"
  },
  {
    "text": "affinity but partly wasn't really good enough because if you it will only allow",
    "start": "1097850",
    "end": "1103460"
  },
  {
    "text": "you to place one pod in different topology domains with pod spreading now you can define a skew so you can say",
    "start": "1103460",
    "end": "1111110"
  },
  {
    "text": "okay I want the pods that belong to a specific group to be placed in two",
    "start": "1111110",
    "end": "1116210"
  },
  {
    "text": "different topologies with a skew of maybe at most one so you can always basically try to load balanced properly",
    "start": "1116210",
    "end": "1122300"
  },
  {
    "text": "across the party domains I'm running a little quick because I want to leave time for for questions we",
    "start": "1122300",
    "end": "1130730"
  },
  {
    "start": "1129000",
    "end": "1219000"
  },
  {
    "text": "in 1.17 we graduated to GA a couple of interesting features the first one is",
    "start": "1130730",
    "end": "1135800"
  },
  {
    "text": "scheduling demon seed pods before that demon seed pods were actually scheduled by the demon seed controller so the",
    "start": "1135800",
    "end": "1143030"
  },
  {
    "text": "demon sect controller was actually doing the binding not the scheduler so what we did in 1.17 is the demon seed controller",
    "start": "1143030",
    "end": "1152440"
  },
  {
    "text": "just sets not affinity on the pod and then leave it for the scheduler to",
    "start": "1152440",
    "end": "1158510"
  },
  {
    "text": "actually schedule the ball and that is better because it gives a more consistent view of the schedule of what",
    "start": "1158510",
    "end": "1163550"
  },
  {
    "text": "pods are running on which nodes and how much resources are being allocated the",
    "start": "1163550",
    "end": "1168620"
  },
  {
    "text": "other one is think notes by condition there are some resources that don't get",
    "start": "1168620",
    "end": "1173630"
  },
  {
    "text": "allocated like disk pressure or pids this can't be ID so and and sometimes the node gets into disk pressure or PID",
    "start": "1173630",
    "end": "1181160"
  },
  {
    "text": "pressure and even when you schedule the part in the node even if you have enough resources not get get scheduled because",
    "start": "1181160",
    "end": "1187820"
  },
  {
    "text": "of that so the node controller what it does is it it monitors the north disk",
    "start": "1187820",
    "end": "1194300"
  },
  {
    "text": "pressure for example a depression and taints those nodes with saying for example this",
    "start": "1194300",
    "end": "1200440"
  },
  {
    "text": "node is unscheduled because of there's some some resource pressure on it before that we were looking at what we call",
    "start": "1200440",
    "end": "1207010"
  },
  {
    "text": "node condition now we're looking at its which is the more canonical way of how to play sports on segregating pods and",
    "start": "1207010",
    "end": "1215350"
  },
  {
    "text": "nodes is using teens and generations there are a couple of plant features",
    "start": "1215350",
    "end": "1221680"
  },
  {
    "start": "1219000",
    "end": "1325000"
  },
  {
    "text": "they're not completely scheduler related but it's it's basically cross-leg the first one is pod overhead so with pot",
    "start": "1221680",
    "end": "1231100"
  },
  {
    "text": "overhead the problem is that when pods are starting on a node they have none zero overhead think about the post",
    "start": "1231100",
    "end": "1239800"
  },
  {
    "text": "control post container that that is associated with every with every part also the overhead of the components",
    "start": "1239800",
    "end": "1247930"
  },
  {
    "text": "running on the node that runs the pod for example cubelet has some state associated with the pause that running",
    "start": "1247930",
    "end": "1253240"
  },
  {
    "text": "so typically what the way that we deal with this overhead is that we we have a",
    "start": "1253240",
    "end": "1260170"
  },
  {
    "text": "predefined amount of resources on the node for system components but the",
    "start": "1260170",
    "end": "1266320"
  },
  {
    "text": "problems that we ignore per pod overhead like the partner as I mentioned before and now this problem becomes even more",
    "start": "1266320",
    "end": "1271990"
  },
  {
    "text": "complex because we have sandbox pods like think about divisor cut a container",
    "start": "1271990",
    "end": "1277480"
  },
  {
    "text": "they have their own agents running beside the pod and those consume resources but those are not accounted",
    "start": "1277480",
    "end": "1283000"
  },
  {
    "text": "for and see he comes overhead so with pot overhead you can see at the runtime",
    "start": "1283000",
    "end": "1289210"
  },
  {
    "text": "class level this is how much resource is that for this one-time class should be allocated per pod and it gets propagated",
    "start": "1289210",
    "end": "1296170"
  },
  {
    "text": "and what we've done in 1.17 is that we implemented a scheduler part of that so",
    "start": "1296170",
    "end": "1302680"
  },
  {
    "text": "now the schedule is aware of pod overhead and when it's schedule spots it adds that overhead to the resources",
    "start": "1302680",
    "end": "1309280"
  },
  {
    "text": "requested by by the pod when it makes a decision where to place the node so the",
    "start": "1309280",
    "end": "1318310"
  },
  {
    "text": "schedule part is implemented the cubelet part is not and that's why it's it's plant feature it's under development the",
    "start": "1318310",
    "end": "1326890"
  },
  {
    "start": "1325000",
    "end": "1398000"
  },
  {
    "text": "other one that's I I really like which is in place update of pod resources that's that's really interesting until recently the resource",
    "start": "1326890",
    "end": "1335800"
  },
  {
    "text": "requests part of the pod spec was immutable we can't change it so",
    "start": "1335800",
    "end": "1340990"
  },
  {
    "text": "basically if we change the API several would inject the change and so the only way to actually increase the resource of a pod is to actually delete the pod and",
    "start": "1340990",
    "end": "1347530"
  },
  {
    "text": "create a new one it's basically restarting the pod now with features like vertical pod scalar",
    "start": "1347530",
    "end": "1353200"
  },
  {
    "text": "we want to actually try to change the pod mark producers association with the pod why not starting the pod and so now",
    "start": "1353200",
    "end": "1361930"
  },
  {
    "text": "we we have the cap already merged we agreed on the design and hopefully in",
    "start": "1361930",
    "end": "1370450"
  },
  {
    "text": "1.18 will start developing there are two parts of it the scheduler part and the",
    "start": "1370450",
    "end": "1376060"
  },
  {
    "text": "cubed part as well as similar to pod pod overhead also there are many nonces he related to okay how do i do the memory",
    "start": "1376060",
    "end": "1383110"
  },
  {
    "text": "etc scaling because you know for example java based applications they they take",
    "start": "1383110",
    "end": "1390040"
  },
  {
    "text": "all the memory feeds it will be hard to scale it down but I mean this is this is",
    "start": "1390040",
    "end": "1395950"
  },
  {
    "text": "a long discussion and that's the end of my talk",
    "start": "1395950",
    "end": "1402870"
  },
  {
    "start": "1398000",
    "end": "1545000"
  },
  {
    "text": "[Applause]",
    "start": "1403590",
    "end": "1409750"
  },
  {
    "text": "that's a very yeah yeah right so like",
    "start": "1436289",
    "end": "1445690"
  },
  {
    "text": "right now you could configure can have two schedulers running and did I mention",
    "start": "1445690",
    "end": "1451029"
  },
  {
    "text": "you can set the scheduler name and you can configure them separately what I would like to get to is I don't I don't",
    "start": "1451029",
    "end": "1457119"
  },
  {
    "text": "like the idea of having two schedulers like and what I what I would hope for is",
    "start": "1457119",
    "end": "1462849"
  },
  {
    "text": "that the at the limit is to have what we call like scheduling profiles so you",
    "start": "1462849",
    "end": "1469539"
  },
  {
    "text": "write similar to runtime classes or storage classes and so every group of parts you can associate them with a",
    "start": "1469539",
    "end": "1476950"
  },
  {
    "text": "specific scheduling behavior and right now it's something that we're discussing in scheduling is how we I did how we",
    "start": "1476950",
    "end": "1484359"
  },
  {
    "text": "define that API that you express a scheduling behavior so that you can run",
    "start": "1484359",
    "end": "1490090"
  },
  {
    "text": "multiple like workloads with different behavior on the same in the same cluster because I mean heterogeneity is is",
    "start": "1490090",
    "end": "1497139"
  },
  {
    "text": "useful right like it allows you to bend back better it allows you to to have better utilization but in the short term",
    "start": "1497139",
    "end": "1505659"
  },
  {
    "text": "your only solution is to run multiple schedulers or multiple clusters",
    "start": "1505659",
    "end": "1512129"
  },
  {
    "text": "weighted round robin",
    "start": "1520919",
    "end": "1524309"
  },
  {
    "text": "are you talking about like how we gonna pick the nodes from the queue sorry pick the pods from the queue",
    "start": "1532360",
    "end": "1539460"
  },
  {
    "text": "yeah so that's exactly like I if you so",
    "start": "1539460",
    "end": "1545980"
  },
  {
    "start": "1545000",
    "end": "1782000"
  },
  {
    "text": "we have this sort plug-in here this is gonna change the way that you order the",
    "start": "1545980",
    "end": "1554020"
  },
  {
    "text": "pods in the queue so the problem is that this is this is fixed so if we can't",
    "start": "1554020",
    "end": "1561010"
  },
  {
    "text": "have multiple sorting criterias so you can have only one yeah exactly so you",
    "start": "1561010",
    "end": "1577390"
  },
  {
    "text": "would you need to write your own your own plug-in and and compile it in the difference I think you would like it's",
    "start": "1577390",
    "end": "1593860"
  },
  {
    "text": "right now it's so you would implement the you know the list function that's",
    "start": "1593860",
    "end": "1598960"
  },
  {
    "text": "what you do so it's much simpler yeah",
    "start": "1598960",
    "end": "1603510"
  },
  {
    "text": "so at the high level the permit allows",
    "start": "1618739",
    "end": "1624029"
  },
  {
    "text": "you to wait on a part from a specific time like we have a timer and if that",
    "start": "1624029",
    "end": "1632129"
  },
  {
    "text": "timer expires the pod gets basically gets rejected now the idea is that like",
    "start": "1632129",
    "end": "1639149"
  },
  {
    "text": "for example you have poured one two and three so pod one will when your plugin",
    "start": "1639149",
    "end": "1644279"
  },
  {
    "text": "will return for example wait for five minutes but two will come and then you say wait for five minutes then pod three",
    "start": "1644279",
    "end": "1650879"
  },
  {
    "text": "will come and then you can what you can do like what the framework will allow",
    "start": "1650879",
    "end": "1656759"
  },
  {
    "text": "you to do is you can check which parts are waiting right now and so you're",
    "start": "1656759",
    "end": "1662669"
  },
  {
    "text": "gonna have to define your own criteria of when to say okay those parts that are",
    "start": "1662669",
    "end": "1667950"
  },
  {
    "text": "waiting accept them and accept myself and move on so the state is there the",
    "start": "1667950",
    "end": "1675720"
  },
  {
    "text": "pods that are currently waiting you have access to them and so it depends on the",
    "start": "1675720",
    "end": "1681960"
  },
  {
    "text": "plugin implementation of when to decide how to how to accept all the pods are",
    "start": "1681960",
    "end": "1688139"
  },
  {
    "text": "currently waiting state",
    "start": "1688139",
    "end": "1691009"
  },
  {
    "text": "yeah yeah",
    "start": "1698910",
    "end": "1703230"
  },
  {
    "text": "yeah yeah basically getting more insight into",
    "start": "1707700",
    "end": "1715970"
  },
  {
    "text": "the scheduling behavior so basically what we wanted to do is to get more metrics into what the scheduler is doing",
    "start": "1715970",
    "end": "1723020"
  },
  {
    "text": "for example how many pods is scheduling per second what is the length of the scheduling queue how long it takes to",
    "start": "1723020",
    "end": "1730400"
  },
  {
    "text": "schedule a pod so those metrics in collection we call them like it's it's under observability",
    "start": "1730400",
    "end": "1737600"
  },
  {
    "text": "we wanted more insight into what the scheduler is doing right I think we have",
    "start": "1737600",
    "end": "1757340"
  },
  {
    "text": "Vivek here right you have talked about it",
    "start": "1757340",
    "end": "1763360"
  },
  {
    "start": "1782000",
    "end": "1859000"
  },
  {
    "text": "so the question was how does a cholesterol task a little interact with a jeweler basically we have the",
    "start": "1787110",
    "end": "1795820"
  },
  {
    "text": "scheduler code inside a cluster or a scalar so the cluster order scalar needs",
    "start": "1795820",
    "end": "1800950"
  },
  {
    "text": "to understand how many nodes to create for a certain amount of pending parts to be scheduled and for that we need to",
    "start": "1800950",
    "end": "1806680"
  },
  {
    "text": "understand how many parts we require so we basically have to run some of the",
    "start": "1806680",
    "end": "1813040"
  },
  {
    "text": "filters to figure out if we if these many nodes are enough to run these many",
    "start": "1813040",
    "end": "1818290"
  },
  {
    "text": "parts so there is basically a one-to-one mapping between the amount of between",
    "start": "1818290",
    "end": "1824350"
  },
  {
    "text": "the you know cluster or the scalar version and the cluster and the",
    "start": "1824350",
    "end": "1830020"
  },
  {
    "text": "scheduler version so if you're running let's say kubernetes 116 you would require a cluster or a scalar the exact",
    "start": "1830020",
    "end": "1837010"
  },
  {
    "text": "same version because the cluster order scalar would use the exact share you live code for that particular version",
    "start": "1837010",
    "end": "1844170"
  },
  {
    "text": "yes you can choose thanks",
    "start": "1846690",
    "end": "1851940"
  },
  {
    "text": "how does that work with the schedule or framework do you have just configure the autoscaler with the same plugins and",
    "start": "1859090",
    "end": "1865610"
  },
  {
    "text": "stuff exactly yeah yeah my hope is that if we",
    "start": "1865610",
    "end": "1870860"
  },
  {
    "text": "get to the point where when you find this API that you can define those different scheduling behaviors is the",
    "start": "1870860",
    "end": "1877370"
  },
  {
    "text": "cluster or scale as well looks at that API for example imagine that it is again",
    "start": "1877370",
    "end": "1882580"
  },
  {
    "text": "CR th is like now it's the hot thing right do you have that state this states",
    "start": "1882580",
    "end": "1889370"
  },
  {
    "text": "basically defines what are the plug is that I want to run which waits and then",
    "start": "1889370",
    "end": "1895580"
  },
  {
    "text": "the the cluster autoscaler basically has the part knows which exact you know a",
    "start": "1895580",
    "end": "1903559"
  },
  {
    "text": "qualification has to configure when it's trying to evaluate that part for auto scaling hey good talk lot of",
    "start": "1903559",
    "end": "1913880"
  },
  {
    "text": "improvements so one question about the gang scheduling oh one question about",
    "start": "1913880",
    "end": "1919760"
  },
  {
    "text": "the gang scheduling which you just mentioned how can you make the make it consistent without the quota management",
    "start": "1919760",
    "end": "1925970"
  },
  {
    "text": "on top of it so I'll take an example you have like three parts you are waiting",
    "start": "1925970",
    "end": "1931760"
  },
  {
    "text": "for two parts and the third part is not arrived yet and you are waiting for it but if you",
    "start": "1931760",
    "end": "1938780"
  },
  {
    "text": "are not doing a quota management at the admission control level what is going to happen is you don't have enough",
    "start": "1938780",
    "end": "1944840"
  },
  {
    "text": "resources and the modes other parts gonna come and take the resources and the first two parts is gonna time out",
    "start": "1944840",
    "end": "1951710"
  },
  {
    "text": "then it will go into work again into the scheduling cycles right and then this churn will keep on happening yeah so I",
    "start": "1951710",
    "end": "1958490"
  },
  {
    "text": "need to have quota management on top of it before we do that I have to admit",
    "start": "1958490",
    "end": "1963590"
  },
  {
    "text": "like we did not really make any like progress on that yet so those are very",
    "start": "1963590",
    "end": "1970490"
  },
  {
    "text": "legitimate concerns and I think it's one of the plan features for us is to start",
    "start": "1970490",
    "end": "1976370"
  },
  {
    "text": "thinking more and more about batch workloads and how we how we integrate them with the with the",
    "start": "1976370",
    "end": "1982240"
  },
  {
    "text": "with the current scheduler we have peloton which we can talk about it yes does that yeah so regarding the",
    "start": "1982240",
    "end": "1993790"
  },
  {
    "text": "framework structure do you plan to add any observability plugging something that produces metrics at some points of",
    "start": "1993790",
    "end": "2000450"
  },
  {
    "text": "the pipeline that tells you about the quality of the scheduling you're performing like if you want imagine you",
    "start": "2000450",
    "end": "2005550"
  },
  {
    "text": "implement one of the modules apps like other or like the sorting because you want to enforce a certain you want",
    "start": "2005550",
    "end": "2011250"
  },
  {
    "text": "certain jobs to be a schedule first but you want to measure if those jobs actually caught sort at first booty",
    "start": "2011250",
    "end": "2018060"
  },
  {
    "text": "mixers not will it make sense to have a plugin at the end at the pine stage that you will produce metrics that you can",
    "start": "2018060",
    "end": "2023880"
  },
  {
    "text": "program well your own yeah I mean like some of these actually extension points are like we call them like information",
    "start": "2023880",
    "end": "2031640"
  },
  {
    "text": "and our vision is that some of them are going to be go to Ethernet logs or in",
    "start": "2031640",
    "end": "2039900"
  },
  {
    "text": "met metrics based on previous like the",
    "start": "2039900",
    "end": "2045600"
  },
  {
    "text": "behavior or in the previous accession points did I get your question yeah but",
    "start": "2045600",
    "end": "2050820"
  },
  {
    "text": "if I want in real-time like for example what we do for example is we we produce metrics saying okay we",
    "start": "2050820",
    "end": "2057570"
  },
  {
    "text": "place one job for this user and that goes to Prometheus and we in real time we can see what is the rate of jobs per",
    "start": "2057570",
    "end": "2063030"
  },
  {
    "text": "user that we and that we are facing right and if I will have this framework I would like to have to make a wrapper",
    "start": "2063030",
    "end": "2069389"
  },
  {
    "text": "for the bin thing to add the metric that popular you know that propagation is somewhere else you create if if I could",
    "start": "2069390",
    "end": "2074850"
  },
  {
    "text": "monitor all the jobs I get then without modifying anything just by making a plugin that observes whatever goes into",
    "start": "2074850",
    "end": "2081300"
  },
  {
    "text": "pain and then I extract my custom metrics that have to do with my way of doing scheduling and I you know I just",
    "start": "2081300",
    "end": "2086639"
  },
  {
    "text": "publish them but I don't want to taint any snails in a structure that's what I mean yeah I don't have any thoughts on",
    "start": "2086640",
    "end": "2093990"
  },
  {
    "text": "that I don't know if it'll work it'll work the problem is that metrics in",
    "start": "2093990",
    "end": "2099330"
  },
  {
    "text": "kubernetes in general is like it's I think it's strict some sort like you",
    "start": "2099330",
    "end": "2105900"
  },
  {
    "text": "have to pretty find them and",
    "start": "2105900",
    "end": "2109490"
  },
  {
    "text": "do you have any updates on the D scheduler whether that's going to be rolled into the schedule or at all or is that just a side project still it's",
    "start": "2119260",
    "end": "2126220"
  },
  {
    "text": "still a side project but it's Ravi's here can you talk about it",
    "start": "2126220",
    "end": "2136830"
  },
  {
    "text": "so the D scheduler is just recently updated to 116 so it has been kind of",
    "start": "2138930",
    "end": "2147850"
  },
  {
    "start": "2139000",
    "end": "2496000"
  },
  {
    "text": "dead for a little while Oh Robbie is here so yeah it's I don't",
    "start": "2147850",
    "end": "2154750"
  },
  {
    "text": "know if there's any plans to move that soon but it is a plan to move it to GA",
    "start": "2154750",
    "end": "2161380"
  },
  {
    "text": "eventually and we're working on getting it up to pace and you know a spot where",
    "start": "2161380",
    "end": "2167859"
  },
  {
    "text": "everyone can use it just to add to what",
    "start": "2167859",
    "end": "2172990"
  },
  {
    "text": "my tol so we haven't thought about it but if you think that there are some use cases where T scheduler would actually",
    "start": "2172990",
    "end": "2180040"
  },
  {
    "text": "benefit if it's part of course scheduler we can make it as part of a plug-in at some so it depends on the use cases like",
    "start": "2180040",
    "end": "2187180"
  },
  {
    "text": "if you have something please let us know we'll be more than happy to look at it",
    "start": "2187180",
    "end": "2192210"
  },
  {
    "text": "what city scheduler did you attend the",
    "start": "2204120",
    "end": "2211390"
  },
  {
    "text": "intro yesterday I'm sorry miss kidding so this scheduler at high level is an a",
    "start": "2211390",
    "end": "2217570"
  },
  {
    "text": "Victor whenever the scheduling decisions are not met any more for example once",
    "start": "2217570",
    "end": "2223540"
  },
  {
    "text": "the part comes into running state the default scheduler would not even care about the parts anymore so at that stage",
    "start": "2223540",
    "end": "2229350"
  },
  {
    "text": "if the scheduler sees that there are some parts that are in running state and they are not conforming to the",
    "start": "2229350",
    "end": "2236050"
  },
  {
    "text": "scheduling constraints anymore it would evict those type of parts again while everything we have some constraints in",
    "start": "2236050",
    "end": "2241450"
  },
  {
    "text": "place like we will not have with every part but this is what is the job of a scheduler there are some other use cases",
    "start": "2241450",
    "end": "2247720"
  },
  {
    "text": "for it too like for example if you want the cluster to be defragmented say there is lot of fragmentation we can we can",
    "start": "2247720",
    "end": "2254260"
  },
  {
    "text": "use these scheduler to as a high-level defragmented defragment 802",
    "start": "2254260",
    "end": "2262170"
  },
  {
    "text": "so I have a question for the sorting function because that's assumes you",
    "start": "2269230",
    "end": "2274480"
  },
  {
    "text": "already have all the ports but imagine making a batch case you have like a stream of ports can we make that",
    "start": "2274480",
    "end": "2281710"
  },
  {
    "text": "interface like more like kind of like so what we have is actually a priority queue and what you're gonna give is the",
    "start": "2281710",
    "end": "2288670"
  },
  {
    "text": "compare function so that's what it is it pushes like many times some functions",
    "start": "2288670",
    "end": "2293950"
  },
  {
    "text": "could be more confident and just compare function yeah like a like a Yankee writes okay so you have to paste all the current entitlement of that",
    "start": "2293950",
    "end": "2301869"
  },
  {
    "text": "queue to decide who's who should move to next right if ya if you if you have like a specific",
    "start": "2301869",
    "end": "2309070"
  },
  {
    "text": "like really you know like use case for it be more than happy like if you can",
    "start": "2309070",
    "end": "2314859"
  },
  {
    "text": "find an issue that the framework right now is again it's still in alpha we are interested to improve the extension",
    "start": "2314859",
    "end": "2322210"
  },
  {
    "text": "points that we have to cover more use cases yeah yeah which is awesome but now",
    "start": "2322210",
    "end": "2335380"
  },
  {
    "text": "my question is we also have a lot of use case which we kind of need in place",
    "start": "2335380",
    "end": "2341369"
  },
  {
    "text": "contain update for example like you know for some machine meaning use case",
    "start": "2341369",
    "end": "2346390"
  },
  {
    "text": "inference we have some influence container and then we'll also want treat",
    "start": "2346390",
    "end": "2351910"
  },
  {
    "text": "each model as a container so they are in the same pod but we want to upgrade those model containers without print out",
    "start": "2351910",
    "end": "2358569"
  },
  {
    "text": "the inference container I think it's fair container like the resources are actually different per container so when",
    "start": "2358569",
    "end": "2366190"
  },
  {
    "text": "I user subscribes like the name is misleading it's actually a container like you when you",
    "start": "2366190",
    "end": "2372490"
  },
  {
    "text": "when you when you modify the resources your mocha you mean like a little modified resource of Canada but like if",
    "start": "2372490",
    "end": "2379240"
  },
  {
    "text": "we wanna kind of replace the image of the Canaries they're gonna be supportive oh okay yeah we gotta bring down the",
    "start": "2379240",
    "end": "2386170"
  },
  {
    "text": "port of course can anywhere be restarted but like the other can let's say you have one part with two containers",
    "start": "2386170",
    "end": "2391599"
  },
  {
    "text": "you know if I'll only replace the image of one container then I don't want to bring down here right so you just want",
    "start": "2391599",
    "end": "2397270"
  },
  {
    "text": "to restart one container yes yeah yeah that's a good signal question",
    "start": "2397270",
    "end": "2403560"
  },
  {
    "text": "you know yeah nobody that that's a like yeah I didn't arresting valid use kids",
    "start": "2403560",
    "end": "2412290"
  },
  {
    "text": "so it sounds like for the scheduling framework your plugins have to be compiled for environments like a gke",
    "start": "2414270",
    "end": "2422140"
  },
  {
    "text": "it's not really available right is there a demand for making like webhook plugins",
    "start": "2422140",
    "end": "2428500"
  },
  {
    "text": "it seemed like I had heard about performance concerns but it seems similar to like a mutating webhook",
    "start": "2428500",
    "end": "2433990"
  },
  {
    "text": "admission controller would have the same sort of performance impacts not likely",
    "start": "2433990",
    "end": "2439930"
  },
  {
    "text": "that's the problem as I mentioned like it's it's performance once you start making these calls to various you know",
    "start": "2439930",
    "end": "2445630"
  },
  {
    "text": "yeah my point is it seems like the amount of performance impact would be similar are you talking at the",
    "start": "2445630",
    "end": "2455890"
  },
  {
    "text": "scheduling stage or the pod admission stage if the admission sorry if the web",
    "start": "2455890",
    "end": "2461710"
  },
  {
    "text": "hook needs to be called during the scheduler or after or at some stage of scheduler it is already available we",
    "start": "2461710",
    "end": "2466780"
  },
  {
    "text": "have a HTTP extender like a predicate stage or priority stage or bind stage",
    "start": "2466780",
    "end": "2472480"
  },
  {
    "text": "you can make an HTTP call if that's what you are asking for and that still exists and that will exist continue to exist as",
    "start": "2472480",
    "end": "2478930"
  },
  {
    "text": "well but it's it exists in thank you",
    "start": "2478930",
    "end": "2492480"
  },
  {
    "text": "[Applause]",
    "start": "2493940",
    "end": "2497380"
  }
]