[
  {
    "text": "uh thanks everyone for joining the session today uh let me start uh with the",
    "start": "280",
    "end": "5720"
  },
  {
    "text": "introduction so uh I'm Sumit Mur group engineering manager at intute uh I",
    "start": "5720",
    "end": "10759"
  },
  {
    "text": "manages the traffic team and with me I have shashant he's a senior staff engineer in intute uh yeah so uh intute",
    "start": "10759",
    "end": "20680"
  },
  {
    "text": "is a a VTech company based out of us and uh we have uh products in uh accounting",
    "start": "20680",
    "end": "26199"
  },
  {
    "text": "and tax we are on the mission of powering the prosperity around the world and we believe in uh open source",
    "start": "26199",
    "end": "33120"
  },
  {
    "text": "Technologies and some of the contribution to open source includes Argo Numa Admiral and uh we are the",
    "start": "33120",
    "end": "40800"
  },
  {
    "text": "consumer of the open source Technologies which includes ketes esto Prometheus",
    "start": "40800",
    "end": "46960"
  },
  {
    "text": "Etc yeah we are the part of a traffic platform teams which provides the",
    "start": "46960",
    "end": "52399"
  },
  {
    "text": "platform capabilities to rest of the intu developers so our customers are the intu internal developers they use our",
    "start": "52399",
    "end": "58760"
  },
  {
    "text": "platform to develop the business Logics and all the products that intute has uh the main goal is basically uh to",
    "start": "58760",
    "end": "67400"
  },
  {
    "text": "abstract the network platform from them so that they can really focus on the development of the product and this is",
    "start": "67400",
    "end": "72640"
  },
  {
    "text": "the scale at which we operate we have more than 300 plus kubernetes clusters with 16,000 name spaces uh more than uh",
    "start": "72640",
    "end": "79720"
  },
  {
    "text": "10,000 developers are using these platforms with more than 2,000 Services uh on these",
    "start": "79720",
    "end": "85520"
  },
  {
    "text": "platforms yeah some of the contribution from the traffic team includes uh Admiral and navic we are going to talk",
    "start": "85520",
    "end": "92680"
  },
  {
    "text": "about both of these in the later part of the session today yeah uh this is our agenda uh we",
    "start": "92680",
    "end": "99280"
  },
  {
    "text": "are going through uh touch base some of the basics of traffic and then uh how does the in traffic platform looks like",
    "start": "99280",
    "end": "106040"
  },
  {
    "text": "and our journey while building this platform in the last 10 years uh which includes like uh the different",
    "start": "106040",
    "end": "111960"
  },
  {
    "text": "capabilities that we have built in this platform including authorization rate limiting and config distributions yeah",
    "start": "111960",
    "end": "118799"
  },
  {
    "text": "so uh let's start uh with a 2 minutes intro like what is an API Gateway so",
    "start": "118799",
    "end": "125759"
  },
  {
    "text": "let's consider a developer uh in your company who is developing the apis and uh once the apis are ready uh they get",
    "start": "125759",
    "end": "132560"
  },
  {
    "text": "their first client who started using it they share the credentials and the things really works well but as the uh",
    "start": "132560",
    "end": "139000"
  },
  {
    "text": "business Grows Right uh you get multiple sets of clients like desktop clients mobile clients uh the browser clients or",
    "start": "139000",
    "end": "145959"
  },
  {
    "text": "you may have a services within your own Enterprise boundary which want to consume these apis or you may have a",
    "start": "145959",
    "end": "151000"
  },
  {
    "text": "third party apps which want to leverage your apis to uh build the products right so now uh the focus of the developer who",
    "start": "151000",
    "end": "156840"
  },
  {
    "text": "was developing the business logic apis will change to how do I protect my apis right how do authorize like who can",
    "start": "156840",
    "end": "162720"
  },
  {
    "text": "access what and at the same time how do I um uh make sure like one client does",
    "start": "162720",
    "end": "168560"
  },
  {
    "text": "not uh consume all the capacity of my services right so that's where uh API Gateway comes in picture all these",
    "start": "168560",
    "end": "174400"
  },
  {
    "text": "Services uh interact with the API Gateway API Gateway is the front door once it goes through the API gate",
    "start": "174400",
    "end": "179840"
  },
  {
    "text": "gateway then it makes a call to the backend services so this is at high level what's an API",
    "start": "179840",
    "end": "185159"
  },
  {
    "text": "Gateway uh now let's see what a service mesh so consider a architecture where a",
    "start": "185159",
    "end": "191760"
  },
  {
    "text": "service is deployed in a kubernetes cluster so consider like service S1 which is deployed in a kubernetes",
    "start": "191760",
    "end": "197080"
  },
  {
    "text": "clusters in a multi cluster scenarios where it is communicating to service 2 and service 3 now to get a similar",
    "start": "197080",
    "end": "204360"
  },
  {
    "text": "capability like API Gateway what you need is something like this right where servic is communicating to uh another",
    "start": "204360",
    "end": "211239"
  },
  {
    "text": "service through an API Gateway uh in our case in intute this API Gateway is again deployed in the kubernetes Clusters",
    "start": "211239",
    "end": "217760"
  },
  {
    "text": "hosted its own uh hosted in its own VPC so that means every time the request need to go to a different networking",
    "start": "217760",
    "end": "223560"
  },
  {
    "text": "Zone and that incur the latency and at the same time uh like it's not a really",
    "start": "223560",
    "end": "228959"
  },
  {
    "text": "U uh cost effective way right so what we really want is uh each of the services because we are in a cometes where we",
    "start": "228959",
    "end": "235319"
  },
  {
    "text": "have a concepts of containers and all that what we want is a mini uh Gateway",
    "start": "235319",
    "end": "240519"
  },
  {
    "text": "which is running alongside of your own uh service right so in kubernetes uh uh we have a",
    "start": "240519",
    "end": "248640"
  },
  {
    "text": "project called sto which uh really provide us the service mesh kind of capability it helps us in service",
    "start": "248640",
    "end": "253959"
  },
  {
    "text": "management traffic management right so that's where um is comes in picture so now uh knowing like what is Gateway what",
    "start": "253959",
    "end": "261639"
  },
  {
    "text": "is service mesh let's look into uh the in traffic platform what we have done basically uh so in traffic platform",
    "start": "261639",
    "end": "268440"
  },
  {
    "text": "basically includes like it come combines both API Gateway and service mesh together and we call it as a traffic",
    "start": "268440",
    "end": "273960"
  },
  {
    "text": "platform so all the browser traffic or the internet traffic comes through a",
    "start": "273960",
    "end": "279039"
  },
  {
    "text": "Gateway and we call it as a North South traffic and anything within the intute Enterprise boundary it goes through a",
    "start": "279039",
    "end": "285680"
  },
  {
    "text": "service to service communication uh through a service smash right so this is at a high level uh Network diagram some",
    "start": "285680",
    "end": "292320"
  },
  {
    "text": "of the features of this uh platform is like front door for all the services all the request and service to service",
    "start": "292320",
    "end": "298560"
  },
  {
    "text": "communication other things are routing so we provide both static and dynamic routing security uh authentication",
    "start": "298560",
    "end": "306160"
  },
  {
    "text": "authorization uh metrics so it's a source of golden signal metrics for all the ined services uh access logs rate",
    "start": "306160",
    "end": "312960"
  },
  {
    "text": "limiting and traffic dying so some of these some of these are the core capabilities that we have built uh Within These traffic platform and uh",
    "start": "312960",
    "end": "319520"
  },
  {
    "text": "that's what we are going to talk about and this is the stats like uh on a daily basis uh we get close to 30 billion",
    "start": "319520",
    "end": "326280"
  },
  {
    "text": "requests uh 1.5 million transactions per second uh the availability is 59 uh",
    "start": "326280",
    "end": "332319"
  },
  {
    "text": "golden signal I just talked about and then uh it's a very uh like you know performant uh platform with P99 is less",
    "start": "332319",
    "end": "338680"
  },
  {
    "text": "than 30 milliseconds and self-service yeah so I think uh the entire traffic platform uh is built on the principle to",
    "start": "338680",
    "end": "345800"
  },
  {
    "text": "provide a self-service capability so that means the users really don't need to understand the Integrity of the",
    "start": "345800",
    "end": "352120"
  },
  {
    "text": "platform we provide them the self-service which is more an abcentric way users should come and do the",
    "start": "352120",
    "end": "358160"
  },
  {
    "text": "abcentric way and then we take take care of translating those to the platform capabilities and this accelerate the",
    "start": "358160",
    "end": "364840"
  },
  {
    "text": "velocity now let's go through the architecture so some of the design principles while building this platform",
    "start": "364840",
    "end": "370120"
  },
  {
    "text": "in last 10 years are like partitioning so we want to make sure like partitioning is the first class citizen",
    "start": "370120",
    "end": "376319"
  },
  {
    "text": "for us so the entire platform is sharded with multiple swim lanes and these swim",
    "start": "376319",
    "end": "381360"
  },
  {
    "text": "lanes are designed on the basis of the products that we have like the business units or on the security like some rpci",
    "start": "381360",
    "end": "387639"
  },
  {
    "text": "compliance zo Lanes or on the basis of like you know timeouts some Services required a high timeout or some Services",
    "start": "387639",
    "end": "393680"
  },
  {
    "text": "required a high payload right so that's where the entire platform is shed across multiple swim Lanes it's a straight less and",
    "start": "393680",
    "end": "400039"
  },
  {
    "text": "horizontally scalable it's highly reliable so the entire platform is resilient to entire",
    "start": "400039",
    "end": "406000"
  },
  {
    "text": "region as well as one a failure so we can still serve the same uh 30 billion",
    "start": "406000",
    "end": "411039"
  },
  {
    "text": "request with entire one region down and one a failure it's highly cachable so uh it's",
    "start": "411039",
    "end": "417280"
  },
  {
    "text": "and that's the reason it to scale out very fast we don't uh at a run time depend on any database uh cell service I just talked",
    "start": "417280",
    "end": "424120"
  },
  {
    "text": "about it yeah let's uh yeah so this is the first U design uh when we started",
    "start": "424120",
    "end": "429759"
  },
  {
    "text": "our journey so in API Gateway is a multitenant uh API Gateway uh what does it mean basically so this is uh one of",
    "start": "429759",
    "end": "437440"
  },
  {
    "text": "the deployment uh name space where you have a pods the API Gateway pods are running with its own load balancer let's",
    "start": "437440",
    "end": "443759"
  },
  {
    "text": "consider a scenario a service one is onboarded on API Gateway with its own",
    "start": "443759",
    "end": "449039"
  },
  {
    "text": "domain name service one.com now what happens is whenever the request comes for service one it goes to",
    "start": "449039",
    "end": "456319"
  },
  {
    "text": "the load balancer it goes to the Pod and it goes to the back end it's straightforward right and if anyone do",
    "start": "456319",
    "end": "461919"
  },
  {
    "text": "the NS lookup of service one.com it get the IP of load balancer now what is what is a",
    "start": "461919",
    "end": "467720"
  },
  {
    "text": "multi-tenant here is what we have done is multiple intute services are deployed",
    "start": "467720",
    "end": "472879"
  },
  {
    "text": "on the same uh cluster using the same load balancer so what does it mean is uh",
    "start": "472879",
    "end": "478759"
  },
  {
    "text": "all these 2 service 3 they maps to the same load balancer the request comes to",
    "start": "478759",
    "end": "484159"
  },
  {
    "text": "the load balancer then it can go to any pod and then the Pod has a routing logic right it understands okay if this is the",
    "start": "484159",
    "end": "490120"
  },
  {
    "text": "domain name then I can I need to Route the request to service 2 or service 3 so that's what we call as a multi-tenant",
    "start": "490120",
    "end": "495560"
  },
  {
    "text": "architecture and this really helped us in reducing the cost uh cost drastically uh as I mentioned intute uh",
    "start": "495560",
    "end": "502800"
  },
  {
    "text": "has more than 2,000 Services right so the number of domain names that we have across all the environment is more than",
    "start": "502800",
    "end": "508159"
  },
  {
    "text": "20,000 plus so that means if I have to uh map a given domain name with a one",
    "start": "508159",
    "end": "514000"
  },
  {
    "text": "load balance it's a huge cost right so with this architecture we actually drastically reduced the cost by mapping",
    "start": "514000",
    "end": "519919"
  },
  {
    "text": "multiple services to the same load balancer and having a a good routing",
    "start": "519919",
    "end": "524920"
  },
  {
    "text": "logic within the API Gateway to cender traffic to the back end yeah so this is uh one uh design",
    "start": "524920",
    "end": "532360"
  },
  {
    "text": "second thing is multi swiming so as I mentioned intu API Gateway is multi swiming where uh we are hosting uh like",
    "start": "532360",
    "end": "539480"
  },
  {
    "text": "into AP Gateway is spread across the multiple clusters and each clusters can host multiple s lanes and this really",
    "start": "539480",
    "end": "544880"
  },
  {
    "text": "helped us in doing a chargeback model so we really want our customers uh to be very cautious about the when I say",
    "start": "544880",
    "end": "550920"
  },
  {
    "text": "customers it's int internal developers who are using the platform to be very careful in using this platform so they know like the cost that it is going to",
    "start": "550920",
    "end": "557440"
  },
  {
    "text": "incur and the so this is another um snapshot where a single clusters can",
    "start": "557440",
    "end": "563240"
  },
  {
    "text": "have a multiple swim Lane and this is another thing like where the swim lanes are spread across the multiple um",
    "start": "563240",
    "end": "569040"
  },
  {
    "text": "cluster now uh knowing the basics like uh the",
    "start": "569040",
    "end": "574160"
  },
  {
    "text": "multi- tenant and the multi swiming concept now let's go one level down to understand the entire ecosystem of the",
    "start": "574160",
    "end": "581120"
  },
  {
    "text": "traffic so the API Gateway as I mentioned is spread across the multiple swim lanes and similarly the mesh",
    "start": "581120",
    "end": "587040"
  },
  {
    "text": "service mesh is spread across all the int kubernetes clusters and uh and there are certain services and int uh which",
    "start": "587040",
    "end": "593839"
  },
  {
    "text": "are still not on service mesh so what happens is for those cases when the request comes through a load balancer to",
    "start": "593839",
    "end": "601040"
  },
  {
    "text": "uh API Gateway those requests get routed to the back end through a net Gateway if the services is on the service smash",
    "start": "601040",
    "end": "607920"
  },
  {
    "text": "then it goes through Transit Gateway now uh to run these uh clusters like uh uh",
    "start": "607920",
    "end": "613640"
  },
  {
    "text": "all these 300 kubernetes clusters for the service smash we have a admin",
    "start": "613640",
    "end": "618839"
  },
  {
    "text": "cluster uh which basically host all the admin capabilities or uh all the admin",
    "start": "618839",
    "end": "624519"
  },
  {
    "text": "capabilities that we need to run these clusters so first capability is service smesh sorry uh my bad it's service",
    "start": "624519",
    "end": "630800"
  },
  {
    "text": "registry so service registry stores all the configurations that API Gateway needs to boot a strap so whenever the",
    "start": "630800",
    "end": "637519"
  },
  {
    "text": "API gway boot straps it gets all the data from the service registry and then it keep on running and we are going to",
    "start": "637519",
    "end": "643480"
  },
  {
    "text": "uh learn more about uh it in the later part of the session like how it refreshes that configurations as",
    "start": "643480",
    "end": "649160"
  },
  {
    "text": "well yeah uh we have another uh config service which provides the data uh to",
    "start": "649160",
    "end": "655000"
  },
  {
    "text": "install the sto on all these 300 kuus clusters then we have a component called navic",
    "start": "655000",
    "end": "661360"
  },
  {
    "text": "which helps in distributing the service configuration so let's uh understand more about it so we all know like uh in",
    "start": "661360",
    "end": "668639"
  },
  {
    "text": "STO um if I have to configure capabilities like traffic dialing rate limiting or timeouts uh we need to",
    "start": "668639",
    "end": "675480"
  },
  {
    "text": "understand how to configure the St custom objects right we have to write uh the virtual services or NY filters right",
    "start": "675480",
    "end": "680959"
  },
  {
    "text": "so now consider where you have a more than 10,000 developers and we need to ensure like all of them to understand",
    "start": "680959",
    "end": "686279"
  },
  {
    "text": "the service mesh Concepts write those filters and then deploy those filters either on the client or on the service",
    "start": "686279",
    "end": "691760"
  },
  {
    "text": "sites right depending on the capability that we are configuring that's a huge uh like you know developer productivity uh",
    "start": "691760",
    "end": "698680"
  },
  {
    "text": "impact so what we want is to abstract those uh configurations so we have a developer portal users comes in the",
    "start": "698680",
    "end": "705440"
  },
  {
    "text": "developer portal they provide the configurations more in the abcentric manner like timeout is this much or rate",
    "start": "705440",
    "end": "711000"
  },
  {
    "text": "limiting is this much and then navic reads those configuration generates all the St custom objects and then",
    "start": "711000",
    "end": "716600"
  },
  {
    "text": "distribute it across the Clusters either on the client or on the on the destination side depending on the uh configurations yeah third is",
    "start": "716600",
    "end": "724760"
  },
  {
    "text": "Admiral this is another open source that we did uh so this helps in the service discoverability in the multicluster",
    "start": "724760",
    "end": "730519"
  },
  {
    "text": "scenario so let's understand this so consider like uh you have a service with uh multiple uh clients onboarded on it",
    "start": "730519",
    "end": "737560"
  },
  {
    "text": "right now the service can be in a multiple clusters as well so how do I how does the client know okay where to",
    "start": "737560",
    "end": "743720"
  },
  {
    "text": "send the request so that's where Admiral comes so Admiral generates all those virtual Services which helps in routing",
    "start": "743720",
    "end": "749440"
  },
  {
    "text": "and it helps in discovery of those Services uh both of these projects like Admiral and NAIC as I mentioned earlier",
    "start": "749440",
    "end": "755839"
  },
  {
    "text": "are open source you can read more about these uh in the GitHub the similar architecture is",
    "start": "755839",
    "end": "762079"
  },
  {
    "text": "replicated in the second region to provide a high availability to all the services that we have in",
    "start": "762079",
    "end": "767279"
  },
  {
    "text": "in uh and the thing is now because all the services are like highly available",
    "start": "767279",
    "end": "772600"
  },
  {
    "text": "we really need to have some capability to fail over so the services in the int can be active active or active passive",
    "start": "772600",
    "end": "778720"
  },
  {
    "text": "depending on Service uh capability and for that we have provided a capability called DNS",
    "start": "778720",
    "end": "784279"
  },
  {
    "text": "manager and it's a single point of contact for the services to fail over both front end that is at the API",
    "start": "784279",
    "end": "790480"
  },
  {
    "text": "Gateway level as well at the service MH level and to know more about like how do we uh fail over the services at scale do",
    "start": "790480",
    "end": "797880"
  },
  {
    "text": "attend a talk in the same Hall at 2:50 p.m. uh my team is going to present about the uh like you know more deeper",
    "start": "797880",
    "end": "803639"
  },
  {
    "text": "of DNS manager how we do the failover at a scale",
    "start": "803639",
    "end": "809240"
  },
  {
    "text": "yeah so now uh understanding like the different components of the traffic let's start our uh learnings like what",
    "start": "809240",
    "end": "815199"
  },
  {
    "text": "are the different learnings that we have uh while solving the problems of building this highly scalable",
    "start": "815199",
    "end": "821399"
  },
  {
    "text": "architecture and uh this is not the only way I think there can be multiple ways so we are going to talk about only the",
    "start": "821399",
    "end": "826760"
  },
  {
    "text": "journey that we have gone through so let's start our authorization Journey uh how we uh buil an in like you know",
    "start": "826760",
    "end": "834480"
  },
  {
    "text": "solution and then how we externalize it to Opa so authorization at a high level is",
    "start": "834480",
    "end": "841440"
  },
  {
    "text": "uh protecting your apis right so let's say a service can have a multiple apis one is your admin API second can be the",
    "start": "841440",
    "end": "848360"
  },
  {
    "text": "apis which can be used by the other services so what you really want is there should be some rules to protect",
    "start": "848360",
    "end": "853880"
  },
  {
    "text": "the apis who can access what right so that's simply authorization means and um",
    "start": "853880",
    "end": "859480"
  },
  {
    "text": "in intute we have a developer portal where users can come and write those rules so this is how one of the rules",
    "start": "859480",
    "end": "866040"
  },
  {
    "text": "looks like yeah let's start with the design one like how we build uh this",
    "start": "866040",
    "end": "872079"
  },
  {
    "text": "authorization without Opa so intute API Gateway is uh built on um JT container",
    "start": "872079",
    "end": "878160"
  },
  {
    "text": "which is a web server or container so we have a request Handler",
    "start": "878160",
    "end": "883600"
  },
  {
    "text": "uh with it with its own thread pool so whenever the request reaches an uh API",
    "start": "883600",
    "end": "888880"
  },
  {
    "text": "Gateway request Handler takes that request one of the threads take that request the request passes through a different policies in the request path",
    "start": "888880",
    "end": "895759"
  },
  {
    "text": "like authentication authorization rate limiting and whatnot and then eventually it goes to the back end so on the last",
    "start": "895759",
    "end": "903000"
  },
  {
    "text": "policy that we have is a outbound policy with its own thread pool called outbound thread pool and um we really don't wait",
    "start": "903000",
    "end": "909160"
  },
  {
    "text": "for the request to come back uh and whenever the response is available another thread picks up so this also",
    "start": "909160",
    "end": "914639"
  },
  {
    "text": "helps us in attaining that multi-tenant architecture I talked about right where you are just uh separating both uh",
    "start": "914639",
    "end": "920639"
  },
  {
    "text": "inbound and outbound so this was really working well for us uh the only uh issue that we had",
    "start": "920639",
    "end": "927519"
  },
  {
    "text": "at that time was the logic of the authorization right the rules are outside but the logic is still within",
    "start": "927519",
    "end": "932839"
  },
  {
    "text": "the Gateway code right within the jvm memory of the Gateway so what we did to uh solve that",
    "start": "932839",
    "end": "939199"
  },
  {
    "text": "thing is we moved to Opa so we took a decision let's use Opa uh and uh what we",
    "start": "939199",
    "end": "944800"
  },
  {
    "text": "did is along with the Gateway container we introduced another container called Opa container as a side car now when the",
    "start": "944800",
    "end": "951199"
  },
  {
    "text": "request comes uh at a ji container or the Gateway container we introduced another component called uh SDK we",
    "start": "951199",
    "end": "957759"
  },
  {
    "text": "created another SDK called odzi sgk using JY async with its own thread",
    "start": "957759",
    "end": "963440"
  },
  {
    "text": "pool so whenever the request reaches the odzi policy it makes a call to OD SDK",
    "start": "963440",
    "end": "969639"
  },
  {
    "text": "and then one of the threat picks at that request makes a call to Opa and whenever the response is available another threat",
    "start": "969639",
    "end": "975959"
  },
  {
    "text": "picks it up and then the uh the request flow continues so this really well",
    "start": "975959",
    "end": "981440"
  },
  {
    "text": "worked well for us uh to achieve a high DPS and things were really working well until a certain point right so so",
    "start": "981440",
    "end": "989279"
  },
  {
    "text": "let's look at what went wrong like with us with this design so if you really",
    "start": "989279",
    "end": "994600"
  },
  {
    "text": "look at it we have something called rate limiting here which makes a call to remote cache we are going to discuss",
    "start": "994600",
    "end": "1000040"
  },
  {
    "text": "more in detail later like why we have that concept so but if you see the thread whenever uh this remote cache",
    "start": "1000040",
    "end": "1007519"
  },
  {
    "text": "becomes slow lot of thre threads start blocking there so we have a thread blockage there and if it really slow",
    "start": "1007519",
    "end": "1015079"
  },
  {
    "text": "what happens is this oddy thread pull is star you don't have enough threads uh to make a call to Opa and that impacted the",
    "start": "1015079",
    "end": "1021800"
  },
  {
    "text": "entire request flow for us so to solve this what we did is we thought okay let's introduce another thread pool so",
    "start": "1021800",
    "end": "1028520"
  },
  {
    "text": "with between rate limiting to remote cach we introduce another thread pool to solve this problem but what we found is",
    "start": "1028520",
    "end": "1035400"
  },
  {
    "text": "we have another more policies in the request chain uh especially like Dynamic routing which helps in resolving the",
    "start": "1035400",
    "end": "1040918"
  },
  {
    "text": "outbound at a run time the request uh again we Face a similar problem so we",
    "start": "1040919",
    "end": "1046079"
  },
  {
    "text": "thought uh we look back again our design like what's going wrong here and how we can solve this problem rather than",
    "start": "1046079",
    "end": "1052240"
  },
  {
    "text": "introducing this multiple thread pools so when we uh really uh yeah so when we uh really look at the",
    "start": "1052240",
    "end": "1061360"
  },
  {
    "text": "data what type of these requests are we found right like many times uh most of the requests are repetitive right so",
    "start": "1061360",
    "end": "1067520"
  },
  {
    "text": "what we did is to solve this is again it's the same side car but instead of using a sync jti what",
    "start": "1067520",
    "end": "1075200"
  },
  {
    "text": "we did is we used a sync jety with a cache so in so what it really helps us is instead of making a call every time",
    "start": "1075200",
    "end": "1080880"
  },
  {
    "text": "to site car we can uh cat the response in the SDK and we can serve it from",
    "start": "1080880",
    "end": "1086520"
  },
  {
    "text": "there so instead of introducing another thread pool it's a same thread pool and it gets the response from the uh SDK and",
    "start": "1086520",
    "end": "1092679"
  },
  {
    "text": "it really worked well for us so we were able to achieve the same high TPS uh with a similar resource",
    "start": "1092679",
    "end": "1098600"
  },
  {
    "text": "utilization so yeah let's look at the learnings from this uh introducing a a",
    "start": "1098600",
    "end": "1104400"
  },
  {
    "text": "syn threat pull it impacted the entire request flow at high TPS right now now if I have to make uh solve this problem",
    "start": "1104400",
    "end": "1110960"
  },
  {
    "text": "I have to make intermediate steps also a sync right so that means all the steps need to be a sync now this introduce",
    "start": "1110960",
    "end": "1117360"
  },
  {
    "text": "another problem is if I have to make this then fine-tuning of this multiple threadpool is a tedious process right",
    "start": "1117360",
    "end": "1123000"
  },
  {
    "text": "you have to keep on fine tuning it and what we obser is it often breaks as you fine tune as your TPS skills or you get",
    "start": "1123000",
    "end": "1129000"
  },
  {
    "text": "a different sets of services it often break so these are our learnings uh with",
    "start": "1129000",
    "end": "1134480"
  },
  {
    "text": "odzi when we onboarded to Opa now let me call shushan to talk about a rate",
    "start": "1134480",
    "end": "1139799"
  },
  {
    "text": "limiting Journey thanks smitth all right folks uh",
    "start": "1139799",
    "end": "1146120"
  },
  {
    "text": "let's have some fun let's talk about uh how things work rate limiting and config distribution at scale so these are the",
    "start": "1146120",
    "end": "1152280"
  },
  {
    "text": "two things that we are going to talk about let's have a quick show off hands how many of you know what is rate",
    "start": "1152280",
    "end": "1157840"
  },
  {
    "text": "limiting ah almost everyone nice so let's uh keep it simple there's",
    "start": "1157840",
    "end": "1164760"
  },
  {
    "text": "a service which says say I am only going to take three requests per second right so let's see what happens uh currently",
    "start": "1164760",
    "end": "1172880"
  },
  {
    "text": "all the users of this service access it using a new domain called service one.com let's see what happens to the",
    "start": "1172880",
    "end": "1179000"
  },
  {
    "text": "request the first request lands on the load balancer hits a pod the first pod the Pod allows the request the second",
    "start": "1179000",
    "end": "1186159"
  },
  {
    "text": "request again same thing and uh yeah the third request also",
    "start": "1186159",
    "end": "1192320"
  },
  {
    "text": "anded landed on the third part and it was allowed now what happens to the fourth request",
    "start": "1192320",
    "end": "1200320"
  },
  {
    "text": "yeah this got rejected so what really happened here so we use the simplest rate limiting algorithm here so we have",
    "start": "1200360",
    "end": "1207480"
  },
  {
    "text": "three requests is the threshold and we have three PS and we said hey if every p",
    "start": "1207480",
    "end": "1213480"
  },
  {
    "text": "is going to allow only one request every 1 second we are good we have rate limiting right so this works when we",
    "start": "1213480",
    "end": "1220280"
  },
  {
    "text": "have one service in the entire swim Lane so yeah we we were happy about this basic rate limiting",
    "start": "1220280",
    "end": "1226960"
  },
  {
    "text": "Works what happened then suit spoke about multi architecture right so we brought that into the mix that means on",
    "start": "1226960",
    "end": "1233320"
  },
  {
    "text": "the same swim Lane now we are going to host a bunch of swim Lan a bunch of services and the requests are going to",
    "start": "1233320",
    "end": "1240240"
  },
  {
    "text": "get distributed across all the pods now what happens to our rate limiting what's the side effect of this let's see what happens let's say",
    "start": "1240240",
    "end": "1247120"
  },
  {
    "text": "there are two requests for the same service land on the load balancer there's a good chance that they'll land",
    "start": "1247120",
    "end": "1252559"
  },
  {
    "text": "up on the same pod we just said load balance is a round robing of request right that that still is is true but",
    "start": "1252559",
    "end": "1259200"
  },
  {
    "text": "that is doing it across the three domains service one service 2 service 3.com but not at a particular domain",
    "start": "1259200",
    "end": "1265120"
  },
  {
    "text": "service one right so now what happens we allow the first request and the second request yeah oops",
    "start": "1265120",
    "end": "1273799"
  },
  {
    "text": "right so instead of three you get rate Limited at the first request itself the second request is rate limited so this",
    "start": "1273799",
    "end": "1280559"
  },
  {
    "text": "is yeah we're not really happy about this let's see what can be done so what's the problem here essentially we",
    "start": "1280559",
    "end": "1286120"
  },
  {
    "text": "are not able to uh have a proper round draw distribution across a single domain on if you are a multi-end uh system so",
    "start": "1286120",
    "end": "1294240"
  },
  {
    "text": "it was only one service we could do this but since we added two more services yeah things stopped working so yeah we",
    "start": "1294240",
    "end": "1302159"
  },
  {
    "text": "can do one thing hey you say why do we want to keep the count on every pod let's yeah use a cash um and store the",
    "start": "1302159",
    "end": "1309279"
  },
  {
    "text": "counter there let's see what happens in this case first request comes up now the part checks with the cash hey I'm going",
    "start": "1309279",
    "end": "1315600"
  },
  {
    "text": "to increment the request by one um and is it good enough is the total number of",
    "start": "1315600",
    "end": "1321440"
  },
  {
    "text": "request that is allowed less than or greater than threshold based on this I'm going to um allow the request to my",
    "start": "1321440",
    "end": "1327799"
  },
  {
    "text": "service so since it's the first request we good what about the second request we still good third request yes the",
    "start": "1327799",
    "end": "1334640"
  },
  {
    "text": "threshold is less right we're resuming everything's on the same part here uh what about the fourth request now we see",
    "start": "1334640",
    "end": "1340200"
  },
  {
    "text": "in the cache that hey what happened to what about the total request on um",
    "start": "1340200",
    "end": "1346000"
  },
  {
    "text": "across all the parts because all the all the three parts are updating the same key on the cachee and we saying yeah the",
    "start": "1346000",
    "end": "1352440"
  },
  {
    "text": "fourth request shouldn't be allowed simple right uh now you may say suan",
    "start": "1352440",
    "end": "1357960"
  },
  {
    "text": "what happens if the request lands on the third pod it's still good the cash is sh shared across all the three pods so uh",
    "start": "1357960",
    "end": "1364919"
  },
  {
    "text": "now all the parts are collectively allowing only three requests at any given point of time provided you have a",
    "start": "1364919",
    "end": "1370760"
  },
  {
    "text": "distributed uh a proper locking mechanism on on the cach so we are really good here and yeah rate limiting",
    "start": "1370760",
    "end": "1377080"
  },
  {
    "text": "works yeah what can go wrong so we updating",
    "start": "1377080",
    "end": "1382480"
  },
  {
    "text": "the cash for every single request so cash scalability is a problem so what we",
    "start": "1382480",
    "end": "1388799"
  },
  {
    "text": "noticed is that with the cash that we were using we were able to only do th000 rights per second um per key so we had",
    "start": "1388799",
    "end": "1397320"
  },
  {
    "text": "millions of keys so if it was just one key that we were updating we were good we could update as many as we want but",
    "start": "1397320",
    "end": "1403039"
  },
  {
    "text": "since there are millions of keys the cash system that we used had a problem that we couldn't do more than th000",
    "start": "1403039",
    "end": "1408919"
  },
  {
    "text": "rights at any given time per key and what's the second problem there's",
    "start": "1408919",
    "end": "1413960"
  },
  {
    "text": "obviously there is latency right so every single request we are adding couple of milliseconds uh of latency",
    "start": "1413960",
    "end": "1419679"
  },
  {
    "text": "going to going to the cache which for a latency sensitive system doesn't really work right so yeah we're not really",
    "start": "1419679",
    "end": "1426799"
  },
  {
    "text": "happy about this let's go to the next iteration and also you may say hey what your service you spoke about 1.5 million",
    "start": "1426799",
    "end": "1432919"
  },
  {
    "text": "TPS now you're not able to do have a solution for th000 so what if I want 2,000 2,000 500 3,000 TPS what do I",
    "start": "1432919",
    "end": "1440960"
  },
  {
    "text": "really do if I'm not able to write uh thousand more than thousand times to a particular key what are we",
    "start": "1440960",
    "end": "1447000"
  },
  {
    "text": "going to uh how are we able to scale this rate limiting solution yeah let's see what did we do we used a fixed leaky",
    "start": "1447000",
    "end": "1453360"
  },
  {
    "text": "bucket counter now let's see what's a fixed leaky bucket counter so essentially every pod gets a local",
    "start": "1453360",
    "end": "1459919"
  },
  {
    "text": "bucket that's an in-memory uh bucket let's see what do we do with this the request comes in we just update the",
    "start": "1459919",
    "end": "1466480"
  },
  {
    "text": "local bucket we did not update the cash we did not increment the cash and we allow the request to go through let's",
    "start": "1466480",
    "end": "1472520"
  },
  {
    "text": "say the same thing happened for the second request now for the third",
    "start": "1472520",
    "end": "1478039"
  },
  {
    "text": "request we check with the cash saying Hey what if what is the total number of request what is the current count on the",
    "start": "1478200",
    "end": "1484919"
  },
  {
    "text": "cash that is updated by all the different Services all the different pods on that particular key we reconcile",
    "start": "1484919",
    "end": "1491120"
  },
  {
    "text": "that value and now based on this uh the total count in the cash and my current bucket count I'm going to take a",
    "start": "1491120",
    "end": "1497320"
  },
  {
    "text": "decision that hey should I allow the request or not so this was fine this in this case it was less than the threshold",
    "start": "1497320",
    "end": "1503760"
  },
  {
    "text": "but yeah finally we are able to do rate limiting in this case uh but yeah",
    "start": "1503760",
    "end": "1509080"
  },
  {
    "text": "something is missing right what now scalability right so if you really",
    "start": "1509080",
    "end": "1514159"
  },
  {
    "text": "look at it uh our traffic is uh traffic varies in the morning in the off PE cars",
    "start": "1514159",
    "end": "1519279"
  },
  {
    "text": "it's going to vary a lot and we have we use kubernetes so we have a very dynamically uh scalable um environment",
    "start": "1519279",
    "end": "1526720"
  },
  {
    "text": "during Peak traffic is going to scale out 500,000 pods and all those things but during off pickar it may Shing down",
    "start": "1526720",
    "end": "1533279"
  },
  {
    "text": "to 10 15 so what's the problem with all this and also you may say hey you are a",
    "start": "1533279",
    "end": "1538399"
  },
  {
    "text": "multitenant uh uh system there are multiple service not all of them have the same um traffic requirements some of",
    "start": "1538399",
    "end": "1545720"
  },
  {
    "text": "them may be small uh some of them may be extremely big Services now let's see what happens when we put all the things",
    "start": "1545720",
    "end": "1551679"
  },
  {
    "text": "into multitenant setup let's say somebody like to configure 1,54 as the",
    "start": "1551679",
    "end": "1558000"
  },
  {
    "text": "thresh old let's see uh now this service gets 3,000 requests so since the",
    "start": "1558000",
    "end": "1564480"
  },
  {
    "text": "threshold is 1,54 you are expecting to block half the request simple right 3,000 minus 1,54 almost 1,500 requests",
    "start": "1564480",
    "end": "1572440"
  },
  {
    "text": "have to be blocked and let's say worst case there are no request on the other parts no other other services service",
    "start": "1572440",
    "end": "1579640"
  },
  {
    "text": "one service three doesn't have any request what happens in this case for",
    "start": "1579640",
    "end": "1584840"
  },
  {
    "text": "the first let's say we split this into two sets set 1,500 request set to another 1,500 request the first set of",
    "start": "1584840",
    "end": "1591000"
  },
  {
    "text": "1,500 request since we have five 500 Parts gets properly Round Rob and distributed every single part gets three",
    "start": "1591000",
    "end": "1598960"
  },
  {
    "text": "requests right and we just described our around uh our leaky bucket algorithm so",
    "start": "1598960",
    "end": "1604200"
  },
  {
    "text": "we allow all the parts are going to allow two requests and on the third request we are going to update the cache",
    "start": "1604200",
    "end": "1610480"
  },
  {
    "text": "right so what just happened so yeah we allowed 1,500 requests and since we are reconciling the cash value",
    "start": "1610480",
    "end": "1617080"
  },
  {
    "text": "the cash value can be from 3 to 1,500 let's say we got unlucky there as well",
    "start": "1617080",
    "end": "1622120"
  },
  {
    "text": "we got 1,500 as uh uh the value that's there in the cash we reconciled it now",
    "start": "1622120",
    "end": "1627200"
  },
  {
    "text": "the second set of 1,500 requests land in now what happens all the parts they're going to allow two more",
    "start": "1627200",
    "end": "1633760"
  },
  {
    "text": "requests essentially yeah we instead of 1,500 we allow 3,000 requests pretty",
    "start": "1633760",
    "end": "1639880"
  },
  {
    "text": "sure we are going to have an incident right we sent 2x the number of requests that are allowed by that was requested",
    "start": "1639880",
    "end": "1645919"
  },
  {
    "text": "by this particular service the PO service now is saturated and we have an incident yeah I'm not really happy about",
    "start": "1645919",
    "end": "1652399"
  },
  {
    "text": "this as well now you must say suan you said what about smaller Services what if somebody has capacity of only four",
    "start": "1652399",
    "end": "1659279"
  },
  {
    "text": "requests right I only have four request capacity what do I do again same gets",
    "start": "1659279",
    "end": "1664679"
  },
  {
    "text": "3,000 requests with the Leaky bucket algorithm that we spoke about this thing is absolutely saturated it gets, 500",
    "start": "1664679",
    "end": "1672600"
  },
  {
    "text": "requests instead of four so yeah not U not a good situation to in and also what",
    "start": "1672600",
    "end": "1679120"
  },
  {
    "text": "about extremely big Services 25,000 TPS is the threshold the service is getting",
    "start": "1679120",
    "end": "1684360"
  },
  {
    "text": "50k requests you are expected to block half of them yeah we're saying th000 we",
    "start": "1684360",
    "end": "1690399"
  },
  {
    "text": "said 1,000 TPS is my cash right we are not going to be able to support more than 3,000 yeah this doesn't work either",
    "start": "1690399",
    "end": "1696720"
  },
  {
    "text": "so our rate limiting doesn't really work right yeah so let's see what can we",
    "start": "1696720",
    "end": "1702200"
  },
  {
    "text": "change uh to solve all this right so essentially one size doesn't fit all for",
    "start": "1702200",
    "end": "1707279"
  },
  {
    "text": "all these different types of services that we have let's see what can we do about this so he said the bucket doesn't",
    "start": "1707279",
    "end": "1713720"
  },
  {
    "text": "have to be uh a static bucket so if you're getting 50,000 requests we are expected to block 25,000 instead of",
    "start": "1713720",
    "end": "1720519"
  },
  {
    "text": "starting with the bucket size of three you pick a bigger number right you pick a bigger number let's say 45 so now 45",
    "start": "1720519",
    "end": "1728200"
  },
  {
    "text": "if the bucket size is 45 the first 22,500 requests are allowed right that's when all the pods they have cleaned up",
    "start": "1728200",
    "end": "1735640"
  },
  {
    "text": "their buckets and uh they have updated the values in the cash and they all have the latest values uh that are available",
    "start": "1735640",
    "end": "1742120"
  },
  {
    "text": "and now for the remaining 2,500 request we shring the bucket size right from 45",
    "start": "1742120",
    "end": "1748159"
  },
  {
    "text": "we bring it down to five so what happens essentially we can if things go very well we can rate limit at 25,000 but",
    "start": "1748159",
    "end": "1755279"
  },
  {
    "text": "even if it doesn't uh there can be what we notice is we get an error of up to 10% 5% 10% uh so that that is what we",
    "start": "1755279",
    "end": "1762880"
  },
  {
    "text": "have noticed with this implementation so yeah we happy with this but what about the small the",
    "start": "1762880",
    "end": "1768240"
  },
  {
    "text": "thresholds you said what what happens to four requests this is fine right because for if you have four requests then I'm",
    "start": "1768240",
    "end": "1774919"
  },
  {
    "text": "going to have a bucket count of one for every single request I'm going to update the cash and we are able to do thousand",
    "start": "1774919",
    "end": "1781120"
  },
  {
    "text": "rights for the cash so this is just four rights that are going to happen and after that all the pods know that hey",
    "start": "1781120",
    "end": "1786880"
  },
  {
    "text": "this is the quota is done I'm not going to allow more requests into this so smaller thresholds also work so let's",
    "start": "1786880",
    "end": "1793640"
  },
  {
    "text": "look at the learnings of our learnings from our rate limiting Journey right so one is services with smaller threshold",
    "start": "1793640",
    "end": "1800799"
  },
  {
    "text": "they cannot tolerate another 10 another 30 another 50 requests so we said hey we'll give them",
    "start": "1800799",
    "end": "1807440"
  },
  {
    "text": "extremely high Precision so we'll go update the cash for every single request so that works for them and services with",
    "start": "1807440",
    "end": "1814840"
  },
  {
    "text": "a bit higher TPS you know like maybe 10,000 they have a capacity of 10,000 even if they get another 200 300 it's",
    "start": "1814840",
    "end": "1821200"
  },
  {
    "text": "not going to saturate it right it's okay so if we are in a multi setup we are",
    "start": "1821200",
    "end": "1827640"
  },
  {
    "text": "able to to serve both ends of the spectrum the smaller smaller services and as well as the bigger services and",
    "start": "1827640",
    "end": "1833559"
  },
  {
    "text": "also if you say hey 1 and a half million TPS is what we spoke about uh so there",
    "start": "1833559",
    "end": "1838600"
  },
  {
    "text": "could be bigger Services as well extremely big Services Beyond 50,000 TPS",
    "start": "1838600",
    "end": "1843679"
  },
  {
    "text": "um then what we do is we use the first algorithm that we spoke about the per pod uh quota and we hosted them in",
    "start": "1843679",
    "end": "1850480"
  },
  {
    "text": "individual swim Lanes right so so essentially they have good blast radius as well because 50,000 is sufficient",
    "start": "1850480",
    "end": "1857000"
  },
  {
    "text": "Capac uh sufficent I TPS to have a swim Lan of its own so this is U yeah about",
    "start": "1857000",
    "end": "1862440"
  },
  {
    "text": "our rate limiting let's talk about self service right so everybody loves",
    "start": "1862440",
    "end": "1867600"
  },
  {
    "text": "self-service config U distribution at scale across API Gateway and uh uh",
    "start": "1867600",
    "end": "1873399"
  },
  {
    "text": "service mesh so let's since we discussing rate limiting let's take this uh idea forward the same concept forward",
    "start": "1873399",
    "end": "1881039"
  },
  {
    "text": "uh let's say we have to uh one service in two beautiful regions region one and region two um and it's also getting",
    "start": "1881039",
    "end": "1888200"
  },
  {
    "text": "traffic from uh API Gateway right so it's going to get north south request",
    "start": "1888200",
    "end": "1893480"
  },
  {
    "text": "from API Gateway and east west traffic through service mesh right so",
    "start": "1893480",
    "end": "1898880"
  },
  {
    "text": "everybody's with me this right so north south API Gateway East West um over",
    "start": "1898880",
    "end": "1903919"
  },
  {
    "text": "service mesh and let's say you want to configure rate limit again you want to change uh the rate limit or set the rate",
    "start": "1903919",
    "end": "1910000"
  },
  {
    "text": "limit at 50 requests per second for this particular service okay so at 50 request uh uh uh",
    "start": "1910000",
    "end": "1918200"
  },
  {
    "text": "50 requests per second so let's see what happens so we'll see hey you have to configure it in two different places we",
    "start": "1918200",
    "end": "1924000"
  },
  {
    "text": "have north south and east west um so let we know API Gateway it's a central place",
    "start": "1924000",
    "end": "1930000"
  },
  {
    "text": "between uh you have all the pods uh you can it's a centralized configuration can work here uh so this is not a problem",
    "start": "1930000",
    "end": "1937519"
  },
  {
    "text": "but what about uh service mesh yeah we have to uh if it's rate limiting you have to do it on the service cluster so",
    "start": "1937519",
    "end": "1944159"
  },
  {
    "text": "yeah you do it here and yeah this is it's okay right so if you're using sto you configure uh an ano filter or",
    "start": "1944159",
    "end": "1951159"
  },
  {
    "text": "whatever is required to uh configure your rate limit in the service mesh this is really ideal right so now",
    "start": "1951159",
    "end": "1958480"
  },
  {
    "text": "you have two different places to configure uh your API Gateway and service it's the same set of Parts finally that that's getting traffic from",
    "start": "1958480",
    "end": "1965080"
  },
  {
    "text": "API Gateway or service mesh do you really want to keep these two things uh if you want to change the number you",
    "start": "1965080",
    "end": "1970559"
  },
  {
    "text": "have to keep uh updating both AP Gateway and service SM and make sure they have not drifted out right so yeah not a",
    "start": "1970559",
    "end": "1977320"
  },
  {
    "text": "really good situation to be in and also it's service mesh you notice there are some configurations that go to a client",
    "start": "1977320",
    "end": "1983600"
  },
  {
    "text": "side let's say we talking about a feature like traffic dialing so if you have to decide if the if the client",
    "start": "1983600",
    "end": "1990279"
  },
  {
    "text": "hosted in this beautiful client cluster one has to decide that I'm going to make a call either to region one or region",
    "start": "1990279",
    "end": "1995840"
  },
  {
    "text": "two this decision have to happen on the client cluster so the configuration resides in client cluster one so yeah",
    "start": "1995840",
    "end": "2003240"
  },
  {
    "text": "now the problem is there's some configuration that has to be done on the service cluster and another set of",
    "start": "2003240",
    "end": "2008600"
  },
  {
    "text": "configurations on the client cluster right so now how do I educate uh you know thousands of my developers uh at",
    "start": "2008600",
    "end": "2015960"
  },
  {
    "text": "into it then hey some configuration you have to do it at service cluster some of them in the client cluster and uh yeah",
    "start": "2015960",
    "end": "2023039"
  },
  {
    "text": "if you are extremely uh big service extremely critical service you may have",
    "start": "2023039",
    "end": "2028120"
  },
  {
    "text": "clients across hundreds of clusters right that means if you want to change something you have to work with hundreds of uh um clients of yours you're not",
    "start": "2028120",
    "end": "2035440"
  },
  {
    "text": "really happy about this distributed way of computing and uh yeah you are thinking about all your decisions in",
    "start": "2035440",
    "end": "2040880"
  },
  {
    "text": "life about uh yeah adopting service mesh and uh all those things you're not really happy about uh yeah doing this so",
    "start": "2040880",
    "end": "2048560"
  },
  {
    "text": "what we want to improve developer velocity is we wanted to have unified config management that means hey give me",
    "start": "2048560",
    "end": "2055040"
  },
  {
    "text": "one place one UI one simple place where I can go make all my changes and now",
    "start": "2055040",
    "end": "2060358"
  },
  {
    "text": "this gets reconciled across both AP Gateway and service mesh my developers",
    "start": "2060359",
    "end": "2065480"
  },
  {
    "text": "at my company don't have to know about what how a particular thing has to be configured don't have to make any",
    "start": "2065480",
    "end": "2071240"
  },
  {
    "text": "syntactical uh errors right if all of this can be abstracted that would give us extremely good developer velocity",
    "start": "2071240",
    "end": "2078240"
  },
  {
    "text": "right I want to change my rate limit from 50 to 75 just give me one button update one place it gets updated to both",
    "start": "2078240",
    "end": "2084118"
  },
  {
    "text": "AP Gateway and service mesh that's what we want let's see how did we achieve this so we have a beautiful developer",
    "start": "2084119",
    "end": "2090000"
  },
  {
    "text": "portal at interbit where most of the sell service configurations for all the platforms that um we own our um um uh",
    "start": "2090000",
    "end": "2097760"
  },
  {
    "text": "built into this is essentially used by all the developers at init for configuring any of their uh uh needs so",
    "start": "2097760",
    "end": "2105000"
  },
  {
    "text": "we have a beautiful traffic page this is where all the configurations are um updated so if you want to change the",
    "start": "2105000",
    "end": "2110599"
  },
  {
    "text": "rate limit again this is a page where you go now this configuration gets stored in uh in Services registry and we",
    "start": "2110599",
    "end": "2116960"
  },
  {
    "text": "use messaging service why messaging service we have to notify these two platforms the service mesh and API",
    "start": "2116960",
    "end": "2123599"
  },
  {
    "text": "Gateway that something changed right it's an event so yeah we use an inventing service in this case which is",
    "start": "2123599",
    "end": "2129320"
  },
  {
    "text": "messaging service let's say what happens so introduce the service mesh part of the world now we get go to the same",
    "start": "2129320",
    "end": "2135680"
  },
  {
    "text": "thing we have two uh clusters where this configuration has to be updated about",
    "start": "2135680",
    "end": "2140760"
  },
  {
    "text": "the rate limiting configuration so how did we achieve this right so we use this component called navic uh so this navic",
    "start": "2140760",
    "end": "2148440"
  },
  {
    "text": "provides this abstraction it knows where is the service one hosted it is hosted in cluster one cluster two or whether",
    "start": "2148440",
    "end": "2155480"
  },
  {
    "text": "the configuration has to be done on your client cluster or on in the service clusters this is all uh provided by this",
    "start": "2155480",
    "end": "2163319"
  },
  {
    "text": "navic and uh we if you want to understand more in details of how this",
    "start": "2163319",
    "end": "2168760"
  },
  {
    "text": "actually works we presented this at cucon Paris and this is uploaded in the cncf uh uh YouTube channel um so yeah",
    "start": "2168760",
    "end": "2177160"
  },
  {
    "text": "but today's talk we're going to talk about the second part of it which we did not cover again last time we're going to talk about the API Gateway config",
    "start": "2177160",
    "end": "2183359"
  },
  {
    "text": "distribution so somebody made a config change 50 to 75 TPS this gets notified",
    "start": "2183359",
    "end": "2188480"
  },
  {
    "text": "to Gateway and now the Gateway has to get this value from some place which is service registry sounds extremely easy",
    "start": "2188480",
    "end": "2195400"
  },
  {
    "text": "right why are we discussing all this it's not so simple let's see what happens at scale right so we introduce",
    "start": "2195400",
    "end": "2202079"
  },
  {
    "text": "this uh the Box on the right side which shows the all the parts that are involved in Gateway U and service",
    "start": "2202079",
    "end": "2208160"
  },
  {
    "text": "registry and developer portal I'm going to abstract it out and show it as service registry let's say again the",
    "start": "2208160",
    "end": "2213599"
  },
  {
    "text": "user comes in and yeah we introduce a messaging service as well uh the user user comes in and makes a config change",
    "start": "2213599",
    "end": "2219280"
  },
  {
    "text": "from 50 to 75 TPS we use a messaging service the messaging service notifies all the parts in the swim Lane saying",
    "start": "2219280",
    "end": "2226400"
  },
  {
    "text": "hey something Chang you have to go talk to the registry which is where the config is is available let me go read",
    "start": "2226400",
    "end": "2232880"
  },
  {
    "text": "the configuration and yeah we're happy right so Sant it's extremely simple what",
    "start": "2232880",
    "end": "2238000"
  },
  {
    "text": "are you talking about scale right what happens if you have 200 Parts instead of",
    "start": "2238000",
    "end": "2243280"
  },
  {
    "text": "uh um you know those three parts what happens in this case now we notified 200 ports at the same time 200 ports are",
    "start": "2243280",
    "end": "2250160"
  },
  {
    "text": "going to make a call to yeah my service registry ports they're going to hit the database and you now start having",
    "start": "2250160",
    "end": "2256599"
  },
  {
    "text": "database U you know issues it's going to burn your database if all 100ed uh uh",
    "start": "2256599",
    "end": "2263119"
  },
  {
    "text": "come at the same time you can say Hey you can throw more resources at it you can add more read replicas all of that can be done but we didn't want to do",
    "start": "2263119",
    "end": "2268880"
  },
  {
    "text": "that it's an engineering problem we want it to solve uh it without throwing resources at it let's see what can be done yeah we are not happy about this",
    "start": "2268880",
    "end": "2275280"
  },
  {
    "text": "whole situation and the other problem that is also U second problem here is what if",
    "start": "2275280",
    "end": "2281760"
  },
  {
    "text": "the configuration that is inhaled the configuration that is done is actually something that you don't have a",
    "start": "2281760",
    "end": "2287920"
  },
  {
    "text": "validation on in your service registry it broke something on your API Gateway then you have all 200 Parts have",
    "start": "2287920",
    "end": "2295079"
  },
  {
    "text": "consumed this configuration and uh yeah you have an incident yeah so what can we",
    "start": "2295079",
    "end": "2300680"
  },
  {
    "text": "do about this we introduce scary release pattern right so what is scary release pattern so saying hey instead of",
    "start": "2300680",
    "end": "2306839"
  },
  {
    "text": "notifying all 200 parts we are only going to notify one part so now this one pod of API Gateway is going to make a",
    "start": "2306839",
    "end": "2314119"
  },
  {
    "text": "call to service registry read the configuration from database we store it in a cache and now",
    "start": "2314119",
    "end": "2320760"
  },
  {
    "text": "if this uh the first part has inhal this configuration is going to run a smoke test it says hey this configuration",
    "start": "2320760",
    "end": "2327200"
  },
  {
    "text": "looks good now if it thinks the if the smoke test is successful I'm going to use a messaging service to notify the",
    "start": "2327200",
    "end": "2333760"
  },
  {
    "text": "remaining 199 parts that hey boss this is configuration is good now you can go get that configuration from service",
    "start": "2333760",
    "end": "2340400"
  },
  {
    "text": "registry and now the the the configs not read from the cash it's read from uh",
    "start": "2340400",
    "end": "2346599"
  },
  {
    "text": "sorry it's not read from the database it's read from the cache so we have solved both the problems right so we have uh solved the the bad configuration",
    "start": "2346599",
    "end": "2355079"
  },
  {
    "text": "uh problem as well as our um you know database uh uh we we not reading uh 200",
    "start": "2355079",
    "end": "2361000"
  },
  {
    "text": "calls at the time saying hey this is fine so things are working right um then",
    "start": "2361000",
    "end": "2366280"
  },
  {
    "text": "what happened is we have 30 40 30 33 Old swim Lanes some of the swim lanes are",
    "start": "2366280",
    "end": "2371520"
  },
  {
    "text": "normal size yeah 300 parts 400 Parts but some of them are extremely big and for this big swim Lanes which have extremely",
    "start": "2371520",
    "end": "2378880"
  },
  {
    "text": "big number of Parts 300,000 Parts sometimes if you make a config change uh",
    "start": "2378880",
    "end": "2384160"
  },
  {
    "text": "yeah to keep my service registry scaled out right so and this doesn't happen so very frequently um so now just for those",
    "start": "2384160",
    "end": "2391599"
  },
  {
    "text": "one or two events that may happen once a week or twice a week we don't know when this happens um yeah we have to keep our",
    "start": "2391599",
    "end": "2398079"
  },
  {
    "text": "uh service registry scaled out right so this is not an extremely good situation to be in we have to pay for all this",
    "start": "2398079",
    "end": "2404440"
  },
  {
    "text": "right we'll say hey let's move away from uh scaling out registry let's store it in S3 right hey why not so we use uh",
    "start": "2404440",
    "end": "2412240"
  },
  {
    "text": "same thing Canada release pattern one pod gets notified the one pod makes a call to uh service registry reads the",
    "start": "2412240",
    "end": "2418400"
  },
  {
    "text": "data from database and now instead of involving",
    "start": "2418400",
    "end": "2423480"
  },
  {
    "text": "service registry we just write it to S3 and um if uh this configuration looks good this part",
    "start": "2423480",
    "end": "2430119"
  },
  {
    "text": "is going to notify everybody that the configuration looks good you are good to get the configuration from S3 and yeah",
    "start": "2430119",
    "end": "2437240"
  },
  {
    "text": "we save some money uh in doing this yeah so let's quickly summarize whatever we",
    "start": "2437240",
    "end": "2442960"
  },
  {
    "text": "spoke um so suth initially started us uh with API Gateway service mesh ecosystem",
    "start": "2442960",
    "end": "2449720"
  },
  {
    "text": "how is all of this built uh on kubernets spoke about uh um Opa integration learnings from resing integration r",
    "start": "2449720",
    "end": "2457319"
  },
  {
    "text": "meeting uh is a multi in a multitenant setup with up to 10% error rate and config distribution at scale across AP",
    "start": "2457319",
    "end": "2464440"
  },
  {
    "text": "Gateway and service mesh so if you really look at it there are many different ways to solve all these problems is one of the ways um and uh",
    "start": "2464440",
    "end": "2471720"
  },
  {
    "text": "yeah we here to share uh um learnings uh about all this so these are contact details uh Sumit and U uh me sushant and",
    "start": "2471720",
    "end": "2480520"
  },
  {
    "text": "we have a booth today uh and tomorrow as well uh where we're going to talk about Argo and um uh Numa so if you want to",
    "start": "2480520",
    "end": "2487040"
  },
  {
    "text": "learn more about all this or if you have questions uh about traffic whatever we spoke about as well you can uh reach out",
    "start": "2487040",
    "end": "2493720"
  },
  {
    "text": "uh on the in the uh in our booth uh scan this uh code if you want to follow us on",
    "start": "2493720",
    "end": "2499280"
  },
  {
    "text": "um about our open source work that we are doing um thanks everyone uh you've been an um extremely nice crowd and uh",
    "start": "2499280",
    "end": "2506720"
  },
  {
    "text": "hope you all enjoyed uh as much as I was enjoying this presenting yeah thanks",
    "start": "2506720",
    "end": "2512119"
  },
  {
    "text": "everyone and yeah thanks to the organizers as well thank you uh question",
    "start": "2512119",
    "end": "2518240"
  },
  {
    "text": "uh any questions oh yeah if you have any questions yeah we are around yeah yep",
    "start": "2518240",
    "end": "2525200"
  }
]