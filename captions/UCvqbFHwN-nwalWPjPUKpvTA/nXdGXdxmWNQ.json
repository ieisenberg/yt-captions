[
  {
    "text": "hi everybody Welcome to this talk about memory noisy neighbor I hope you're all",
    "start": "80",
    "end": "6080"
  },
  {
    "text": "caffeinated and ready to go I want to start with a few snapshots",
    "start": "6080",
    "end": "12160"
  },
  {
    "text": "from this YouTube video called positive affirmations for site reliability engineers I highly recommend you watch",
    "start": "12160",
    "end": "19359"
  },
  {
    "text": "it It starts at night A lone SRE is sitting at his",
    "start": "19359",
    "end": "25160"
  },
  {
    "text": "desk looking at exceptions in production and the rising cost of the",
    "start": "25160",
    "end": "31719"
  },
  {
    "text": "deployment He's feeling so frustrated and haggarded He",
    "start": "31719",
    "end": "37800"
  },
  {
    "text": "meditates A calm female voice tells him \"Your pipeline is",
    "start": "37800",
    "end": "44360"
  },
  {
    "text": "green Your tests are well written and stable",
    "start": "44360",
    "end": "50320"
  },
  {
    "text": "your friends and family understand what you do You were born to deploy Kubernetes",
    "start": "50320",
    "end": "59239"
  },
  {
    "text": "clusters And so this got me thinking why do we take on this heroic task of",
    "start": "59239",
    "end": "65760"
  },
  {
    "text": "maintaining these large deployments and for me the ultimate affirmation is this",
    "start": "65760",
    "end": "71920"
  },
  {
    "text": "Your deployment is highly available and costefficient Users delight in the",
    "start": "71920",
    "end": "77360"
  },
  {
    "text": "product and its performance And this is the holy grail what we all strive",
    "start": "77360",
    "end": "83479"
  },
  {
    "text": "towards and performance that uh part of our holy grail really matters and both",
    "start": "83479",
    "end": "90880"
  },
  {
    "text": "companies understand uh the importance both to users the user experience and",
    "start": "90880",
    "end": "95920"
  },
  {
    "text": "their bottom line Uh Amazon released a case study uh saying that for every 100",
    "start": "95920",
    "end": "102079"
  },
  {
    "text": "millisecond increase in uh user response times they lose 1% of revenue And there",
    "start": "102079",
    "end": "108320"
  },
  {
    "text": "have been dozens of case studies over the years uh reinforcing those results I",
    "start": "108320",
    "end": "115119"
  },
  {
    "text": "only were was able to bring just a few of the highc caliber names I was I found",
    "start": "115119",
    "end": "120880"
  },
  {
    "text": "Uh one of the one of the uh case studies I really liked is this one by Raku 1024",
    "start": "120880",
    "end": "128000"
  },
  {
    "text": "which is a Japanese online grocery where they ran an AB test and they ran two",
    "start": "128000",
    "end": "134640"
  },
  {
    "text": "applications with exactly the same functionality but one of them was optimized and ran 400 milliseconds",
    "start": "134640",
    "end": "140800"
  },
  {
    "text": "faster and they had startling results from the faster application It increased",
    "start": "140800",
    "end": "146400"
  },
  {
    "text": "revenue per user by 53% and reduced bounce rates by",
    "start": "146400",
    "end": "152360"
  },
  {
    "text": "35% So performance really matters But with that users also expect",
    "start": "152360",
    "end": "159280"
  },
  {
    "text": "better functionality And your systems might not be as complex as these extreme",
    "start": "159280",
    "end": "165360"
  },
  {
    "text": "examples of service maps published by big companies but still uh your systems",
    "start": "165360",
    "end": "171360"
  },
  {
    "text": "are probably quite complex users expect uh better functionality right they recom",
    "start": "171360",
    "end": "177680"
  },
  {
    "text": "better better recommendations search engines maybe AI and in general just a more featureful application And as our",
    "start": "177680",
    "end": "184879"
  },
  {
    "text": "systems become more and more complex it becomes harder to reason about performance And it's easy when",
    "start": "184879",
    "end": "191519"
  },
  {
    "text": "performance degrade to blame the spaghetti monster of complexity or the",
    "start": "191519",
    "end": "196680"
  },
  {
    "text": "network But really a lot of that uh degradation in performance comes from",
    "start": "196680",
    "end": "204080"
  },
  {
    "text": "simple resource congestion on the server that we just have no visibility into And",
    "start": "204080",
    "end": "210480"
  },
  {
    "text": "that's the topic of this talk Now what if I told you that a small cabal of",
    "start": "210480",
    "end": "217040"
  },
  {
    "text": "engineers got together and created this resource allocation capability that allowed them to run 50% more",
    "start": "217040",
    "end": "224239"
  },
  {
    "text": "transactions on the same size of servers and reduced their tail latency their P95",
    "start": "224239",
    "end": "229280"
  },
  {
    "text": "and P99 by factors of five 14 Like that would be fantastic right how can we not",
    "start": "229280",
    "end": "235519"
  },
  {
    "text": "know about this how can they not tell us well it turns out that this capability",
    "start": "235519",
    "end": "240560"
  },
  {
    "text": "actually exists but it was not developed in secret Over more than a decade there have been more than a dozen papers by",
    "start": "240560",
    "end": "248080"
  },
  {
    "text": "well-known research universities and our hyperscalers that explore this",
    "start": "248080",
    "end": "253120"
  },
  {
    "text": "capability and uh and give us detail on how to mitigate these noisy neighbors",
    "start": "253120",
    "end": "259840"
  },
  {
    "text": "And today I I'm giving this talk because I think it's time for the Kubernetes community uh to get a a solution based",
    "start": "259840",
    "end": "267759"
  },
  {
    "text": "on this research uh that we can all enjoy Uh so in this talk I want to talk",
    "start": "267759",
    "end": "274320"
  },
  {
    "text": "about what is memory noisy neighbor should we care and what are the benefits we're going to get from solving it Uh",
    "start": "274320",
    "end": "281360"
  },
  {
    "text": "talk about some of the hardware and software mechanisms that we have in order to approach the problem and what",
    "start": "281360",
    "end": "286960"
  },
  {
    "text": "we're doing in open source right now Uh I did my PhD in noisy neighbor",
    "start": "286960",
    "end": "294320"
  },
  {
    "text": "mitigation specifically network noisy neighbor mitigation I started a network observability company which we sold to",
    "start": "294320",
    "end": "300400"
  },
  {
    "text": "Splunk and I'm a maintainer on the open telemetry network collector which we contributed and I'm now working on noisy",
    "start": "300400",
    "end": "307520"
  },
  {
    "text": "neighbor mitigation So let's start with what is memory noisy",
    "start": "307520",
    "end": "314759"
  },
  {
    "text": "neighbor Our cloudnative applications ultimately run on physical servers and",
    "start": "314759",
    "end": "322080"
  },
  {
    "text": "the applications have to share the finite resources that these servers",
    "start": "322080",
    "end": "327400"
  },
  {
    "text": "have when uh in one in noisy neighbor one application takes more than its fair",
    "start": "327400",
    "end": "333840"
  },
  {
    "text": "share of resources and so other applications cannot get that resource and their performance degrade In this",
    "start": "333840",
    "end": "341039"
  },
  {
    "text": "talk I'm going to cover I'm going to talk about the last level uh caches",
    "start": "341039",
    "end": "347280"
  },
  {
    "text": "which I'm just going to call caches and the memory",
    "start": "347280",
    "end": "352919"
  },
  {
    "text": "bandwidth So how does cache uh noisy neighbor look like uh usually your",
    "start": "352919",
    "end": "359680"
  },
  {
    "text": "application fits into the different levels of hardware caches that are on the CPU You have the blazingly hot",
    "start": "359680",
    "end": "366639"
  },
  {
    "text": "working set in L1 cache and then the hot warm and cool working sets in L2 L3 and",
    "start": "366639",
    "end": "373160"
  },
  {
    "text": "DRAM When a noisy neighbor comes in it uses the shared L3 cache very",
    "start": "373160",
    "end": "381440"
  },
  {
    "text": "extensively and evicts your application out of that cache And so when the application wants to access its warm uh",
    "start": "381440",
    "end": "389039"
  },
  {
    "text": "data set it has to go to DRAM instead of L3 In the more extreme example where the",
    "start": "389039",
    "end": "395919"
  },
  {
    "text": "noisy neighbor is running at the hyperthread next to your application it also evicts your application from L2",
    "start": "395919",
    "end": "402080"
  },
  {
    "text": "caches And so now your application has to go to DRAM where it would have gone",
    "start": "402080",
    "end": "407199"
  },
  {
    "text": "to L2 which is a lot faster right it's like it could be up to 50 times uh difference So that could degrade",
    "start": "407199",
    "end": "413960"
  },
  {
    "text": "applications quite a lot and in practice it does uh Google uh",
    "start": "413960",
    "end": "421759"
  },
  {
    "text": "published results from experimenting with different synthetic noise generators on their production services",
    "start": "421759",
    "end": "428160"
  },
  {
    "text": "Uh so here you can see three production services Uh you have web search which is the Google search node ML cluster is an",
    "start": "428160",
    "end": "436560"
  },
  {
    "text": "online machine learning text classification system that works in real time and memal is an in-memory key value",
    "start": "436560",
    "end": "444960"
  },
  {
    "text": "store like memcache and they found that the slowdowns across the different loads",
    "start": "444960",
    "end": "450479"
  },
  {
    "text": "on the system was uh five times for web search five times for ML cluster and up",
    "start": "450479",
    "end": "458160"
  },
  {
    "text": "to 14 times for meal So performance degradation on the",
    "start": "458160",
    "end": "465440"
  },
  {
    "text": "P95 P99 is there and",
    "start": "465440",
    "end": "471319"
  },
  {
    "text": "dramatic So I'd like to take a short survey Uh can you please raise your hand",
    "start": "471319",
    "end": "476800"
  },
  {
    "text": "if you know what type of VMs are used in production is it bare metal is it like",
    "start": "476800",
    "end": "482720"
  },
  {
    "text": "the four core 8 core 16 core roughly speaking now Okay Leave your hands up",
    "start": "482720",
    "end": "489440"
  },
  {
    "text": "please and leave your hands up if you never use fractions of a CPU You only use bare metal or you only use full",
    "start": "489440",
    "end": "496520"
  },
  {
    "text": "CPUs I think um 75% of hands uh were",
    "start": "496520",
    "end": "501919"
  },
  {
    "text": "retracted Thank you Um and so best practices today is to separate our big",
    "start": "501919",
    "end": "509440"
  },
  {
    "text": "data analytics from our production userfacing latency critical applications",
    "start": "509440",
    "end": "514719"
  },
  {
    "text": "because we don't want somebody to run a big data analytics job and uh destroy",
    "start": "514719",
    "end": "520320"
  },
  {
    "text": "performance for our latency critical workloads But if you're working with VMs",
    "start": "520320",
    "end": "526240"
  },
  {
    "text": "that are smaller than a full physical machine then what ends up happening actually looks like this right like next",
    "start": "526240",
    "end": "533120"
  },
  {
    "text": "to your VM with your uh interactive workloads are other tenants of you know your cloud provider or infrastructure",
    "start": "533120",
    "end": "539839"
  },
  {
    "text": "provider that might be running batch analytics right and some random dude on the internet is running uh these batch",
    "start": "539839",
    "end": "545760"
  },
  {
    "text": "workloads next to your workloads um and it's fine",
    "start": "545760",
    "end": "551839"
  },
  {
    "text": "um the question is does the qu cloud provider protect you from these noisy neighbor running alongside potentially",
    "start": "551839",
    "end": "559040"
  },
  {
    "text": "noisy neighbors and I've been able I have not been able to find evidence that",
    "start": "559040",
    "end": "564080"
  },
  {
    "text": "they do and I I believe that there is very little protection from at least memory noisy",
    "start": "564080",
    "end": "570440"
  },
  {
    "text": "neighbors So imagine this It's you know our engineers can spend months and years",
    "start": "570440",
    "end": "576959"
  },
  {
    "text": "of their lives optimizing deployments They could be adding indices to the database or uh changing data schema so",
    "start": "576959",
    "end": "584560"
  },
  {
    "text": "queries run faster They could be taking services and breaking them down to multiple services so that queries run in",
    "start": "584560",
    "end": "590720"
  },
  {
    "text": "parallel and they could just do profiling and alleviate bottlenecks All so that the performance uh for users",
    "start": "590720",
    "end": "597800"
  },
  {
    "text": "improves and all of that work can be obliterated destroyed by a noisy run",
    "start": "597800",
    "end": "605360"
  },
  {
    "text": "neighbor running alongside the workload on the compute infrastructure that is just degrading the compute",
    "start": "605360",
    "end": "611920"
  },
  {
    "text": "infrastructures uh performance",
    "start": "611920",
    "end": "616519"
  },
  {
    "text": "And so one takeaway is if you're running let's call it a thousand cores or more",
    "start": "617279",
    "end": "623120"
  },
  {
    "text": "then you should prefer running on fewer 100 core instances the the larger bare",
    "start": "623120",
    "end": "628399"
  },
  {
    "text": "metal instances rather than a lot of the four core 8 core machines Uh because then you at least know who your noisy",
    "start": "628399",
    "end": "635040"
  },
  {
    "text": "neighbors are They're going to be latency sensitive workloads that are much better behaved than batch analytics",
    "start": "635040",
    "end": "640800"
  },
  {
    "text": "workloads or some random workload some other folks could be running next to you",
    "start": "640800",
    "end": "646720"
  },
  {
    "text": "Now do you have to have batch analytics run next to your workloads in order to experience noisy neighbor the answer is",
    "start": "647279",
    "end": "653920"
  },
  {
    "text": "no Um researchers from MIT ran memcache alongside a garbage collected workload",
    "start": "653920",
    "end": "661440"
  },
  {
    "text": "You can see on the top graph here the memory bandwidth It is relatively low and stable throughout the experiment",
    "start": "661440",
    "end": "667440"
  },
  {
    "text": "until the mark phase of the garbage collector which is very memory intensive and saturates memory bandwidth Then if",
    "start": "667440",
    "end": "675040"
  },
  {
    "text": "you look at latency it is pretty stable at 50 micros 99.9th percentile latency",
    "start": "675040",
    "end": "682800"
  },
  {
    "text": "but then as the mark phase starts it jumps up increases three orders of",
    "start": "682800",
    "end": "688240"
  },
  {
    "text": "magnitude a thousand times So you don't have to have big data",
    "start": "688240",
    "end": "694880"
  },
  {
    "text": "analytics neighbors in order to experience noisy neighbors There are many workloads that we run today that",
    "start": "694880",
    "end": "700480"
  },
  {
    "text": "experience them and for example the garbage collection Uh if you have a system that has heavy transactions and",
    "start": "700480",
    "end": "706160"
  },
  {
    "text": "once every 10,000 transaction it just has to run through a lot of rows in your database then that could be noisy",
    "start": "706160",
    "end": "712959"
  },
  {
    "text": "neighbor as well So should we care does it matter",
    "start": "712959",
    "end": "719360"
  },
  {
    "text": "and what can we get from solving this uh you might have seen one of these uh",
    "start": "719360",
    "end": "725600"
  },
  {
    "text": "published uh surveys This one was published by data dog showing that a lot of the containers that we run today use",
    "start": "725600",
    "end": "732480"
  },
  {
    "text": "a lot less of the requested CPU And usually I run surveys and I ask people",
    "start": "732480",
    "end": "737519"
  },
  {
    "text": "kind of how much is your CPU utilization and I get answers between 10% and 40% for most people And that's uh in good",
    "start": "737519",
    "end": "744560"
  },
  {
    "text": "company because some of the world's largest companies have published their",
    "start": "744560",
    "end": "749839"
  },
  {
    "text": "average cluster utilization Uh and these are companies that certainly have resources to",
    "start": "749839",
    "end": "755680"
  },
  {
    "text": "optimize their deployments There's a lot to gain from optimizing them Um but what",
    "start": "755680",
    "end": "761200"
  },
  {
    "text": "happened over the last uh few years is that some companies have been able to",
    "start": "761200",
    "end": "766480"
  },
  {
    "text": "get a lot better efficiencies Uh Google was uh published",
    "start": "766480",
    "end": "772079"
  },
  {
    "text": "around uh 35% in 2011 and increased to 50%",
    "start": "772079",
    "end": "778000"
  },
  {
    "text": "uh in 2019 So that's one and a half times more efficient They run a lot more",
    "start": "778000",
    "end": "783120"
  },
  {
    "text": "on the same infrastructure So what happened uh to increase efficiency so much in these",
    "start": "783120",
    "end": "789519"
  },
  {
    "text": "organizations I was able to find two major contributors There might be more I've I was able to find these major ones",
    "start": "789519",
    "end": "796800"
  },
  {
    "text": "One is advancements in vertical autoscaling So uh making sure that the",
    "start": "796800",
    "end": "802079"
  },
  {
    "text": "memory and CPU requests fit the workload So you don't have waste and you can binack better And another one is",
    "start": "802079",
    "end": "808880"
  },
  {
    "text": "handling noisy neighbor which decreases P95 P99 latency and enables uh to run a",
    "start": "808880",
    "end": "815760"
  },
  {
    "text": "lot more workload on you know more dense on the same amount of systems And to just to illustrate uh why",
    "start": "815760",
    "end": "825040"
  },
  {
    "text": "we are not able to run at high density today we simulated two systems One runs",
    "start": "825040",
    "end": "830079"
  },
  {
    "text": "at high load You can see it on the left And so it has a lot of these memory saturation events or you know garbage",
    "start": "830079",
    "end": "836160"
  },
  {
    "text": "collections for you know just a as a way to think about it And a system with low load that has fewer of these events",
    "start": "836160",
    "end": "842079"
  },
  {
    "text": "because it has low load needs less garbage collection And if you look at",
    "start": "842079",
    "end": "847240"
  },
  {
    "text": "these you can intuitively say that you know the low system would have better",
    "start": "847240",
    "end": "853199"
  },
  {
    "text": "P95 P99 right that has a lot less of these interference events happening slowing down our applications And in I",
    "start": "853199",
    "end": "860639"
  },
  {
    "text": "think a lot of us intuitively know that when we experience high P95 P99 we scale",
    "start": "860639",
    "end": "867440"
  },
  {
    "text": "out right we want to reach this low load So let let me say it uh another way",
    "start": "867440",
    "end": "873279"
  },
  {
    "text": "right like we have high load on our systems which generates memory contention decreases CPU efficiency we",
    "start": "873279",
    "end": "880480"
  },
  {
    "text": "saw that you know our noisy neighbor are kicking the applications out of caches so they're slower and that causes high",
    "start": "880480",
    "end": "886880"
  },
  {
    "text": "response times and us breaching our SLO targets and what we've been doing today",
    "start": "886880",
    "end": "892480"
  },
  {
    "text": "is scale out our systems so that we run at lower load and I think this is an",
    "start": "892480",
    "end": "897760"
  },
  {
    "text": "anti-attern we should stop doing it it's wasteful and inefficient Instead we should make sure",
    "start": "897760",
    "end": "905680"
  },
  {
    "text": "that the memory contention these noisy neighbor events do not decrease performance on our systems by mitigating",
    "start": "905680",
    "end": "912720"
  },
  {
    "text": "noisy neighbor behavior And so if we're able to do it and run at higher load then first we can",
    "start": "912720",
    "end": "922399"
  },
  {
    "text": "scale down our systems and we'll still keep our SLOs's uh and just run at",
    "start": "922399",
    "end": "928079"
  },
  {
    "text": "higher CPU utilization We can allow product teams to build more features They don't have to go and",
    "start": "928079",
    "end": "934320"
  },
  {
    "text": "optimize the system and reduce the SLOs's reduce the P95 P99 before they ship more features They can just go",
    "start": "934320",
    "end": "940440"
  },
  {
    "text": "ahead and we can have users enjoy improved uh SLOs's And in practice we",
    "start": "940440",
    "end": "947519"
  },
  {
    "text": "get a mix We get all of these and uh if we mitigate noisy neighbor and so why",
    "start": "947519",
    "end": "952959"
  },
  {
    "text": "would only the hypers scales be able to enjoy it we should all be able to enjoy it And I think this gets us closer to",
    "start": "952959",
    "end": "960079"
  },
  {
    "text": "our holy grail right it touches on three of the important uh uh goals that we",
    "start": "960079",
    "end": "965759"
  },
  {
    "text": "have both cost you know the cost efficiency product and uh sorry the product features and product",
    "start": "965759",
    "end": "974480"
  },
  {
    "text": "performance So what mechanisms do we have in order to measure and mitigate",
    "start": "975959",
    "end": "981440"
  },
  {
    "text": "noisy neighbors modern CPUs allow the operating system",
    "start": "981440",
    "end": "987839"
  },
  {
    "text": "to control how much of the cache and memory bandwidth each application can",
    "start": "987839",
    "end": "993279"
  },
  {
    "text": "use But even if you don't have support for these techniques in your CPU you can",
    "start": "993279",
    "end": "1000160"
  },
  {
    "text": "still restrict the ability of noisy neighbors to create damage and noise in the system by pinning those noisy",
    "start": "1000160",
    "end": "1006959"
  },
  {
    "text": "neighbors to a small number of cores and perhaps reducing the frequency of these cores so that they just can't do uh much",
    "start": "1006959",
    "end": "1016120"
  },
  {
    "text": "damage I get this question a lot You know containers should do this right like we expect containers to isolate",
    "start": "1016120",
    "end": "1022399"
  },
  {
    "text": "workloads u but they don't today this is not built into our container",
    "start": "1022399",
    "end": "1027760"
  },
  {
    "text": "infrastructures and in fact containers are not very good at isolating performance more security uh but not all",
    "start": "1027760",
    "end": "1034000"
  },
  {
    "text": "is lost there is access to these mechanisms but through a different subsystem called resource control it's",
    "start": "1034000",
    "end": "1040160"
  },
  {
    "text": "configurable through CISFS okay so we want to tackle memory",
    "start": "1040160",
    "end": "1047839"
  },
  {
    "text": "uh contention and memory noisy neighbor we have these mechanisms that allow us to uh to uh control and limit noisy",
    "start": "1047839",
    "end": "1056039"
  },
  {
    "text": "neighbors So what do we measure in order to uh uh close this loop uh we care",
    "start": "1056039",
    "end": "1062480"
  },
  {
    "text": "about application service time So why don't we measure P95 P99 and then adjust",
    "start": "1062480",
    "end": "1067600"
  },
  {
    "text": "resource allocation based on which application has a bad P95",
    "start": "1067600",
    "end": "1073080"
  },
  {
    "text": "P99 Uh this turn the the these measurements turn out to be noisy I mean first we have to measure P95 or P99 So",
    "start": "1073080",
    "end": "1080720"
  },
  {
    "text": "you know just to begin with we have to measure at least a few hundred transactions in order to get a good signal for P95 or P99 Uh it turns out",
    "start": "1080720",
    "end": "1088000"
  },
  {
    "text": "this is noisy so it reacts very slowly Uh another thing we could do is",
    "start": "1088000",
    "end": "1093360"
  },
  {
    "text": "we could measure CPU efficiency So CPU wants to execute some number of instructions If there is a lot of memory",
    "start": "1093360",
    "end": "1099919"
  },
  {
    "text": "contention and the CPU has to wait for memory the number of cycles that it has to uh take to run those instructions is",
    "start": "1099919",
    "end": "1107280"
  },
  {
    "text": "large If there isn't a lot of memory contention then the number of cycles is small So this ratio of cycles per",
    "start": "1107280",
    "end": "1113600"
  },
  {
    "text": "instructions could be a good metric Uh the problem is that this is also",
    "start": "1113600",
    "end": "1120400"
  },
  {
    "text": "noisy because it measures a bunch of other things happening on the system and it is also uh these systems end up being",
    "start": "1120400",
    "end": "1127280"
  },
  {
    "text": "complex because we don't know what a good cycles per instruction value is right like is three good is it terrible",
    "start": "1127280",
    "end": "1133440"
  },
  {
    "text": "we need to profile the application beforehand so these systems end up relatively complex uh but it turns out",
    "start": "1133440",
    "end": "1140080"
  },
  {
    "text": "that Google uh has deployed this type of system on all of their shared clusters",
    "start": "1140080",
    "end": "1146919"
  },
  {
    "text": "um as of 2013 So it's been it's been deployed for a while",
    "start": "1146919",
    "end": "1153320"
  },
  {
    "text": "now Um another thing we can do is we can measure the memory contention itself We",
    "start": "1153320",
    "end": "1158960"
  },
  {
    "text": "can measure the utilization of memory bandwidth and caches in our applications directly and then if we find somebody",
    "start": "1158960",
    "end": "1164880"
  },
  {
    "text": "using more than their fair share we limit them And this is actually a good measurement and this is what our",
    "start": "1164880",
    "end": "1170520"
  },
  {
    "text": "open-source collector is doing right now And in fact Alibaba published that they",
    "start": "1170520",
    "end": "1176400"
  },
  {
    "text": "have a system based on direct collection of uh of memory contention events uh in",
    "start": "1176400",
    "end": "1183600"
  },
  {
    "text": "production for over two years on order of a million cores as of",
    "start": "1183600",
    "end": "1189600"
  },
  {
    "text": "2020 The problem is if we want to solve the memory noisy neighbor problem we",
    "start": "1191480",
    "end": "1198320"
  },
  {
    "text": "need very frequent measurements Um on the top here is a simulation of uh you've seen this before right like the",
    "start": "1198320",
    "end": "1204960"
  },
  {
    "text": "uh the memory uh contention events and on the bottom is a a sampling of the",
    "start": "1204960",
    "end": "1211840"
  },
  {
    "text": "memory metrics every 1 second And you can see that you know you lose all of the signal if you don't measure",
    "start": "1211840",
    "end": "1217200"
  },
  {
    "text": "frequently enough And so with this collector we set out to measure at 1",
    "start": "1217200",
    "end": "1222640"
  },
  {
    "text": "millisecond granularity Um and we started this as an Apache 2 project uh",
    "start": "1222640",
    "end": "1228159"
  },
  {
    "text": "called the unvariance collector right like the the idea is to reduce the variance of response",
    "start": "1228159",
    "end": "1235039"
  },
  {
    "text": "times Okay So uh with that uh I want to talk a little bit about what we're doing in order to build this",
    "start": "1235799",
    "end": "1243320"
  },
  {
    "text": "collector Well I I told you we want to take these 1 millisecond measurements So we want to have uh measurements across",
    "start": "1243320",
    "end": "1250320"
  },
  {
    "text": "all of the cores that we're running every millisecond So we expect it to be you know very you know very regular and",
    "start": "1250320",
    "end": "1256640"
  },
  {
    "text": "want the measurements to be really good In practice what you get is this jitter Some cores might take longer to to",
    "start": "1256640",
    "end": "1263200"
  },
  {
    "text": "respond to these timers and to do the measurement And so we wanted to know how much jitter we're going to have Jitter",
    "start": "1263200",
    "end": "1270080"
  },
  {
    "text": "is bad because if each core is measuring a different time interval then how can you take all of these measurements",
    "start": "1270080",
    "end": "1276240"
  },
  {
    "text": "together and say okay this application used this much memory bandwidth in this time interval or that much uh bandwidth",
    "start": "1276240",
    "end": "1282480"
  },
  {
    "text": "right like we have fuzzy intervals And so to measure jitter we",
    "start": "1282480",
    "end": "1288480"
  },
  {
    "text": "set up Linux high resolution timers at 1 millisecond intervals And these graphs show how much",
    "start": "1288480",
    "end": "1296640"
  },
  {
    "text": "uh uh each core the time that it responds to a timer differs from the the",
    "start": "1296640",
    "end": "1304799"
  },
  {
    "text": "time that we wanted the timer to fire right and you have these vertical lines between the fastest core and the slowest",
    "start": "1304799",
    "end": "1311520"
  },
  {
    "text": "core So the top of the graphs are the slowest cores that took to respond And so um I'm I I want to ask you what do",
    "start": "1311520",
    "end": "1319760"
  },
  {
    "text": "you think is the noisy graph on the bottom right like the noisy graph is 300 microsconds to respond to a timer",
    "start": "1319760",
    "end": "1324960"
  },
  {
    "text": "interrupt That's 30% of our measurement interval That's quite a lot of jitter Um you know it could be the metal because",
    "start": "1324960",
    "end": "1331919"
  },
  {
    "text": "there are 96 cores It's like the same instance but we take either the four core slice or the 96 core slice So it",
    "start": "1331919",
    "end": "1338400"
  },
  {
    "text": "could be the the metal instances because you know 96 cores one of them is going to be slow or the four cores because you",
    "start": "1338400",
    "end": "1345360"
  },
  {
    "text": "know the hypervisor is adding noise and there's noisy neighbor So",
    "start": "1345360",
    "end": "1351520"
  },
  {
    "text": "metal extra large okay so I kind of gave it away Yes Thank you Uh yeah um the extra large is",
    "start": "1352760",
    "end": "1360720"
  },
  {
    "text": "a lot more noisy So the there's I think there's an artifact with the hypervisor These graphs actually show it much worse",
    "start": "1360720",
    "end": "1366480"
  },
  {
    "text": "than it is It just shows the extremes because the scale is seconds and we have a thousand measurements per second So it",
    "start": "1366480",
    "end": "1372159"
  },
  {
    "text": "kind of hides the good measurements but so statistics lies and all that But uh",
    "start": "1372159",
    "end": "1377280"
  },
  {
    "text": "these are both workable but there's a lot more uh noise on the smaller",
    "start": "1377280",
    "end": "1383440"
  },
  {
    "text": "systems Um so this influenced how we built the collector and its architecture",
    "start": "1383640",
    "end": "1389520"
  },
  {
    "text": "Uh so I'll go through it very quickly Um whenever there's a fork in the system",
    "start": "1389520",
    "end": "1396480"
  },
  {
    "text": "so a new thread or task comes in the collector assigns a resource monitoring",
    "start": "1396480",
    "end": "1402000"
  },
  {
    "text": "ID RMID to it This is something that the hardware needs On every context switch between",
    "start": "1402000",
    "end": "1408120"
  },
  {
    "text": "applications the collector tells the CPU what RMID is active now And you can",
    "start": "1408120",
    "end": "1413679"
  },
  {
    "text": "think about it as coloring the traffic You say okay these are all going to be blue axises These are going all to be",
    "start": "1413679",
    "end": "1419760"
  },
  {
    "text": "red axises And now we can ask later every 1 millisecond how many blue",
    "start": "1419760",
    "end": "1425440"
  },
  {
    "text": "accesses did you have how many red accesses did you have and how much of the cache is blue or red or the other",
    "start": "1425440",
    "end": "1430880"
  },
  {
    "text": "colors and there are hundreds of colors you can use There's enough for you know hundreds of containers on the system So",
    "start": "1430880",
    "end": "1436320"
  },
  {
    "text": "this all of the telemetry at 1 millisecond granularity and all of the allocations of",
    "start": "1436320",
    "end": "1441720"
  },
  {
    "text": "uh containers to RM ids flow to a shared memory buffer that the user space",
    "start": "1441720",
    "end": "1448880"
  },
  {
    "text": "component can then come and analyze either output or action on what we're doing today The goal we",
    "start": "1448880",
    "end": "1456880"
  },
  {
    "text": "are outputting the raw telemetry into parquet files and the goal is to develop",
    "start": "1456880",
    "end": "1461919"
  },
  {
    "text": "good detection algorithms and the idea is like let's let's back back test using the raw data that we have um in in these",
    "start": "1461919",
    "end": "1470559"
  },
  {
    "text": "files to make decisions which who is a noisy neighbor at any given time And so",
    "start": "1470559",
    "end": "1475919"
  },
  {
    "text": "we are looking right now for production data We're building synthetic workloads and we're looking for production data If",
    "start": "1475919",
    "end": "1481600"
  },
  {
    "text": "you have a system that you can run the collector on and share the data maybe a test cluster or a um a staging cluster",
    "start": "1481600",
    "end": "1489840"
  },
  {
    "text": "uh that has real production traffic we would love to see it so that we can build better detectors Uh the idea",
    "start": "1489840",
    "end": "1495520"
  },
  {
    "text": "eventually is to output for observability statistics on detection So",
    "start": "1495520",
    "end": "1500799"
  },
  {
    "text": "they would say okay this pod was noisy neighbor 1% of the time and that pod was noisy neighbor 1.5% of the time And uh",
    "start": "1500799",
    "end": "1509760"
  },
  {
    "text": "with with these detections on each 1 millisecond slice hopefully we'd be able",
    "start": "1509760",
    "end": "1514799"
  },
  {
    "text": "to also use those to mitigate noisy neighbor by limiting by configuring resource control So that's the plan Uh I",
    "start": "1514799",
    "end": "1522000"
  },
  {
    "text": "want to uh to add one uh note on overhead We're designing the system",
    "start": "1522000",
    "end": "1527520"
  },
  {
    "text": "for.1% uh overhead in line with the traffic and 1% of the analysis in user space that is",
    "start": "1527520",
    "end": "1534000"
  },
  {
    "text": "not in line does not increase transaction uh latency Uh but even if the overhead was high it almost doesn't",
    "start": "1534000",
    "end": "1541679"
  },
  {
    "text": "matter if you're able to mitigate noisy neighbor So the on the bottom here is an example So imagine if the average",
    "start": "1541679",
    "end": "1549120"
  },
  {
    "text": "service time increased in your service from 40 milliseconds to 42 milliseconds That's huge 5% We're not aiming for that",
    "start": "1549120",
    "end": "1555279"
  },
  {
    "text": "We're aiming for.1% but 5% 42 milliseconds But then we're able to reduce the P95 latency from 250",
    "start": "1555279",
    "end": "1562400"
  },
  {
    "text": "milliseconds to 75 milliseconds I think almost everybody I talked to would say yes let's do this right like this is a",
    "start": "1562400",
    "end": "1569200"
  },
  {
    "text": "great trade-off to have Of course we're not going to have that much uh added overhead but it almost doesn't",
    "start": "1569200",
    "end": "1577400"
  },
  {
    "text": "matter Uh so I'd like to invite everybody uh to contribute I want to thank our existing contributors Uh and",
    "start": "1577400",
    "end": "1584799"
  },
  {
    "text": "if you want to join the project please do And and come My details are here I'm",
    "start": "1584799",
    "end": "1590240"
  },
  {
    "text": "available on CNCF Slack Um and if you have these clusters that you can",
    "start": "1590240",
    "end": "1595520"
  },
  {
    "text": "contribute data um we'd really appreciate it It's a very powerful way to help the project right now Um and so",
    "start": "1595520",
    "end": "1602080"
  },
  {
    "text": "I hope this will help us get closer to our holy grail of better performance",
    "start": "1602080",
    "end": "1607520"
  },
  {
    "text": "cost efficiency and better product to our users Uh I'm thank you for being",
    "start": "1607520",
    "end": "1612799"
  },
  {
    "text": "here and I'm happy to take some questions [Applause]",
    "start": "1612799",
    "end": "1622559"
  },
  {
    "text": "Microphones over there",
    "start": "1622559",
    "end": "1626360"
  },
  {
    "text": "Hi Uh quick question So you mentioned um that can you hear me yeah Okay Um you",
    "start": "1634320",
    "end": "1640880"
  },
  {
    "text": "mentioned that there are two basically causes that are not identified is like the shared cache uh levels and the",
    "start": "1640880",
    "end": "1649440"
  },
  {
    "text": "memory bandwidth and memory um IO's let's say um if you are running like a",
    "start": "1649440",
    "end": "1656320"
  },
  {
    "text": "static CPUs incubator which is uh fairly easy to to set up uh do we know which",
    "start": "1656320",
    "end": "1662559"
  },
  {
    "text": "which part is in your measurements or the data that you've collected which part is due to the cash poisoning let's",
    "start": "1662559",
    "end": "1670240"
  },
  {
    "text": "and which part is due to the memory bandwidth or latency",
    "start": "1670240",
    "end": "1675520"
  },
  {
    "text": "So the question is what percentage of degradation can you attribute to memory",
    "start": "1675520",
    "end": "1681200"
  },
  {
    "text": "bandwidth or to caches when you run on fixed CPU so when you pin workloads to different uh to different CPUs Yes Yes",
    "start": "1681200",
    "end": "1689520"
  },
  {
    "text": "Okay Um so uh usually memory memory bandwidth itself when you saturate it um",
    "start": "1689520",
    "end": "1696320"
  },
  {
    "text": "the latency can increase from 75 nconds to 250 nonds It's on a that benchmark",
    "start": "1696320",
    "end": "1702320"
  },
  {
    "text": "was done on a specific hardware but that's kind of what you can expect So um what is it three four times uh with",
    "start": "1702320",
    "end": "1709360"
  },
  {
    "text": "cache you can expect 10 times uh difference but they compound frequently when you have memory bandwidth noisy",
    "start": "1709360",
    "end": "1715840"
  },
  {
    "text": "neighbor you also have the cache noisy neighbor If you're running on these chiplets there's been advancements in",
    "start": "1715840",
    "end": "1721120"
  },
  {
    "text": "Kubernetes where where you can run some workloads and reserve a chiplet you know a set of CPUs to them and they only they",
    "start": "1721120",
    "end": "1727840"
  },
  {
    "text": "have exclusive access to the cache Then only those workloads are noisy neighbor for themselves on the caches and you",
    "start": "1727840",
    "end": "1734320"
  },
  {
    "text": "only experience maybe memory noisy neighbor but it's very hard to manage You know these are very specific use",
    "start": "1734320",
    "end": "1739840"
  },
  {
    "text": "cases maybe in telco that need the really low latency and it's makes it really hard to um pack more workloads uh",
    "start": "1739840",
    "end": "1747840"
  },
  {
    "text": "when you run on these chiplets uh architectures usually I hope that answers the question Yeah thank you",
    "start": "1747840",
    "end": "1756679"
  },
  {
    "text": "Thanks for the talks Important work Um if we're sampling every 1 milliseconds what are the storage requirements can't",
    "start": "1756799",
    "end": "1762240"
  },
  {
    "text": "hear you Sorry Sorry If we're sampling every 1 millisecond what are the storage requirements to to put on disk i'm",
    "start": "1762240",
    "end": "1770320"
  },
  {
    "text": "really sorry Really can't hear you",
    "start": "1770320",
    "end": "1774759"
  },
  {
    "text": "Can you hear me now yeah Okay Sorry I have to put it in my mouth Um if we're sampling every 1 milliseconds what's the",
    "start": "1776480",
    "end": "1783279"
  },
  {
    "text": "storage requirement for that right So for the raw data it's goes into parquet",
    "start": "1783279",
    "end": "1788880"
  },
  {
    "text": "format which is a columner store and it's compressed and uh it is relatively heavy weight It's one I think we",
    "start": "1788880",
    "end": "1796000"
  },
  {
    "text": "computed up to one megabyte per second Uh but you wouldn't you wouldn't output",
    "start": "1796000",
    "end": "1801679"
  },
  {
    "text": "the raw telemetry What you would do is you would analyze it You would do detection for noisy neighbor and then you would output statistics on who is",
    "start": "1801679",
    "end": "1807760"
  },
  {
    "text": "the noisy neighbor And then for maybe every 60 seconds right today we measure at 5 seconds or 15 seconds or 60 seconds",
    "start": "1807760",
    "end": "1813840"
  },
  {
    "text": "you would output per pod how what fraction how many how many 1 millisecond",
    "start": "1813840",
    "end": "1818960"
  },
  {
    "text": "slots out of the full 60 seconds For example it was noisy neighbor So that is a lot less cardality and a lot less",
    "start": "1818960",
    "end": "1825600"
  },
  {
    "text": "volume and that's something that we're capable of Okay Thank you",
    "start": "1825600",
    "end": "1832520"
  },
  {
    "text": "Thank you Great talk Exciting project Um does it have to run uh can you hear me",
    "start": "1832640",
    "end": "1839600"
  },
  {
    "text": "does it have to run always on you know or it's more like you know I'm profiling the if the applications are safe it can",
    "start": "1839600",
    "end": "1847840"
  },
  {
    "text": "be done on you know periodically than closed you know profile identify the noise numbers I don't think it does any",
    "start": "1847840",
    "end": "1853600"
  },
  {
    "text": "you know you you need to analyze do some adjustments and you know keep on monitoring It's more like that right",
    "start": "1853600",
    "end": "1861360"
  },
  {
    "text": "so the question is do you need to keep it on the entire time or no or not so if you do automatic mitigation for example",
    "start": "1861360",
    "end": "1868080"
  },
  {
    "text": "if you have garbage collection events those are really hard to optimize what you want to do is to maybe either",
    "start": "1868080",
    "end": "1875440"
  },
  {
    "text": "automatically label those as noisy neighbor and decrease the amount of resources that they can use uh or um I",
    "start": "1875440",
    "end": "1883440"
  },
  {
    "text": "guess I mean that's what you can do So you need this automatic mitigation for some types of events Uh there you could",
    "start": "1883440",
    "end": "1889840"
  },
  {
    "text": "use the data to find applications that are consistently clashing on resources",
    "start": "1889840",
    "end": "1895200"
  },
  {
    "text": "and create an anti-affffinity rule So that is possible and it's a use case Um",
    "start": "1895200",
    "end": "1900399"
  },
  {
    "text": "and we want to support that too But I think the the next level of value that",
    "start": "1900399",
    "end": "1906000"
  },
  {
    "text": "you get out of the system is automatic mitigation And I believe a lot of the incidents uh that you have of memory",
    "start": "1906000",
    "end": "1912399"
  },
  {
    "text": "saturation can be mitigated by uh by uh automatic mitigation without need of for",
    "start": "1912399",
    "end": "1919600"
  },
  {
    "text": "profiling and changing of the applications Thank you I think last question Thank you Uh so I",
    "start": "1919600",
    "end": "1926559"
  },
  {
    "text": "have very uh specific question So we ingest like pabyte of data daily from our customers and usually what happens",
    "start": "1926559",
    "end": "1932720"
  },
  {
    "text": "when we see performance degradation We update the instance type or we update the disk type Uh and and this is",
    "start": "1932720",
    "end": "1939760"
  },
  {
    "text": "basically our solution today But as we scale this is not a viable solution Um plus so the question is how do we know u",
    "start": "1939760",
    "end": "1948240"
  },
  {
    "text": "that if this is happening due to noisy neighbor Uh and the second point is like",
    "start": "1948240",
    "end": "1953760"
  },
  {
    "text": "how do we enforce to our platform team to solve this problem uh because um like",
    "start": "1953760",
    "end": "1960640"
  },
  {
    "text": "basically what would be the cost implication if the problem is solved via like implementing a solution for noisy",
    "start": "1960640",
    "end": "1966000"
  },
  {
    "text": "neighbor as compared to just doing uh instance upgrade for different deployments",
    "start": "1966000",
    "end": "1973440"
  },
  {
    "text": "So I think if I understand the question is how do you know that you have noisy neighbor how do you know that you've",
    "start": "1973440",
    "end": "1978880"
  },
  {
    "text": "solved it uh and would vertical scaling help yeah I think so What would be the",
    "start": "1978880",
    "end": "1985200"
  },
  {
    "text": "cost implication as compared to just upgrading the instance type uh yeah so",
    "start": "1985200",
    "end": "1990320"
  },
  {
    "text": "uh I think you should run on large instance types fewer of them So that that should be a good recommendation You",
    "start": "1990320",
    "end": "1998080"
  },
  {
    "text": "would know that you have noisy neighbor by using a collector like we're building now uh and you know that you've solved",
    "start": "1998080",
    "end": "2003919"
  },
  {
    "text": "it by seeing that you have the high incidence of memory contention but no",
    "start": "2003919",
    "end": "2009519"
  },
  {
    "text": "degradation in performance Um as to kind of the different resource types I think you mentioned you have disks and and",
    "start": "2009519",
    "end": "2016159"
  },
  {
    "text": "others uh IO is another noisy neighbor that we'd want to solve but we're starting with this So I I hope I",
    "start": "2016159",
    "end": "2022240"
  },
  {
    "text": "answered the question Yeah All right Thank you everybody for coming Enjoy the conference",
    "start": "2022240",
    "end": "2029960"
  }
]