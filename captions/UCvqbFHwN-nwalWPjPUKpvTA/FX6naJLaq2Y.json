[
  {
    "start": "0",
    "end": "433000"
  },
  {
    "text": "hi everyone how are you",
    "start": "399",
    "end": "2560"
  },
  {
    "text": "my name is alexa griffith and i like you",
    "start": "2560",
    "end": "5359"
  },
  {
    "text": "said i'm on the data science platform",
    "start": "5359",
    "end": "7040"
  },
  {
    "text": "team at bloomberg i've been there now",
    "start": "7040",
    "end": "8960"
  },
  {
    "text": "for a few months i started in january",
    "start": "8960",
    "end": "10880"
  },
  {
    "text": "and what the data science platform team",
    "start": "10880",
    "end": "12719"
  },
  {
    "text": "does is we support multiple machine",
    "start": "12719",
    "end": "14880"
  },
  {
    "text": "learning features from turning from",
    "start": "14880",
    "end": "17199"
  },
  {
    "text": "training to serving models",
    "start": "17199",
    "end": "19359"
  },
  {
    "text": "i work specifically on the",
    "start": "19359",
    "end": "20560"
  },
  {
    "text": "infrastructure team that helps with",
    "start": "20560",
    "end": "22560"
  },
  {
    "text": "infant services and i work with dan sun",
    "start": "22560",
    "end": "25199"
  },
  {
    "text": "i'm lucky enough to work with them who",
    "start": "25199",
    "end": "26880"
  },
  {
    "text": "is a co-founder of k-serve",
    "start": "26880",
    "end": "29199"
  },
  {
    "text": "i've been a software engineer now for a",
    "start": "29199",
    "end": "31119"
  },
  {
    "text": "little over two and a half years almost",
    "start": "31119",
    "end": "32480"
  },
  {
    "text": "three years now so a lot of things are",
    "start": "32480",
    "end": "34960"
  },
  {
    "text": "new to me which is great and i find that",
    "start": "34960",
    "end": "36880"
  },
  {
    "text": "i enjoy learning new things by you know",
    "start": "36880",
    "end": "39120"
  },
  {
    "text": "writing or discussing those topics and",
    "start": "39120",
    "end": "42719"
  },
  {
    "text": "like blogs or podcasts and sometimes",
    "start": "42719",
    "end": "44879"
  },
  {
    "text": "being a beginner can hopefully bring",
    "start": "44879",
    "end": "47200"
  },
  {
    "text": "some new insight or new perspective into",
    "start": "47200",
    "end": "49520"
  },
  {
    "text": "a topic",
    "start": "49520",
    "end": "50640"
  },
  {
    "text": "and i like to have fun with it so",
    "start": "50640",
    "end": "52079"
  },
  {
    "text": "they're going to be like some silly sick",
    "start": "52079",
    "end": "53840"
  },
  {
    "text": "figure drawings in the mix but yeah so",
    "start": "53840",
    "end": "56320"
  },
  {
    "text": "that's pretty much what i'm doing uh",
    "start": "56320",
    "end": "57520"
  },
  {
    "text": "when i joined bloomberg i was very",
    "start": "57520",
    "end": "59199"
  },
  {
    "text": "unfamiliar with k-serve so i find that",
    "start": "59199",
    "end": "61359"
  },
  {
    "text": "sometimes abstractions on top of",
    "start": "61359",
    "end": "62719"
  },
  {
    "text": "kubernetes can be a little confusing or",
    "start": "62719",
    "end": "64478"
  },
  {
    "text": "a little um",
    "start": "64479",
    "end": "66000"
  },
  {
    "text": "a little interesting to talk about so in",
    "start": "66000",
    "end": "68000"
  },
  {
    "text": "this talk i'll attempt to present uh",
    "start": "68000",
    "end": "70560"
  },
  {
    "text": "both the basic and advanced features of",
    "start": "70560",
    "end": "72799"
  },
  {
    "text": "k-serve from that perspective",
    "start": "72799",
    "end": "76320"
  },
  {
    "text": "so as you can tell this is my first pr",
    "start": "76320",
    "end": "78400"
  },
  {
    "text": "that was just merged for k-serve i'm a",
    "start": "78400",
    "end": "79920"
  },
  {
    "text": "really big contributor so i'm obviously",
    "start": "79920",
    "end": "81600"
  },
  {
    "text": "very qualified to give this talk but no",
    "start": "81600",
    "end": "84000"
  },
  {
    "text": "this is a joke but this is my first of",
    "start": "84000",
    "end": "86000"
  },
  {
    "text": "uh hopefully many contributions",
    "start": "86000",
    "end": "89200"
  },
  {
    "text": "yeah so let's explore",
    "start": "89200",
    "end": "91200"
  },
  {
    "text": "k-serve so does anyone know who lives on",
    "start": "91200",
    "end": "93680"
  },
  {
    "text": "planet k-serve",
    "start": "93680",
    "end": "96079"
  },
  {
    "text": "who",
    "start": "96079",
    "end": "97920"
  },
  {
    "text": "oh you don't know okay well it's the k",
    "start": "97920",
    "end": "100000"
  },
  {
    "text": "natives",
    "start": "100000",
    "end": "102000"
  },
  {
    "text": "yeah",
    "start": "102000",
    "end": "102880"
  },
  {
    "text": "apparently there's also a best joke a",
    "start": "102880",
    "end": "105119"
  },
  {
    "text": "competition today that i did not know",
    "start": "105119",
    "end": "106399"
  },
  {
    "text": "about but i'm not saying it could win",
    "start": "106399",
    "end": "107920"
  },
  {
    "text": "but it might um if you vote for it",
    "start": "107920",
    "end": "111360"
  },
  {
    "text": "anyways okay so given the environment",
    "start": "111360",
    "end": "113200"
  },
  {
    "text": "that this is kubecon i'm not really",
    "start": "113200",
    "end": "114560"
  },
  {
    "text": "going to go over exactly what a pod or",
    "start": "114560",
    "end": "116799"
  },
  {
    "text": "um what a container is but i do want to",
    "start": "116799",
    "end": "119360"
  },
  {
    "text": "just focus on the basic kubernetes",
    "start": "119360",
    "end": "121439"
  },
  {
    "text": "controller pattern because i think",
    "start": "121439",
    "end": "122640"
  },
  {
    "text": "that's relevant to my talk today so",
    "start": "122640",
    "end": "124799"
  },
  {
    "text": "kubernetes implements a controller",
    "start": "124799",
    "end": "126479"
  },
  {
    "text": "pattern",
    "start": "126479",
    "end": "127680"
  },
  {
    "text": "and it's the control plane is in the",
    "start": "127680",
    "end": "129119"
  },
  {
    "text": "dotted line each one has some components",
    "start": "129119",
    "end": "130959"
  },
  {
    "text": "and each component is in charge of",
    "start": "130959",
    "end": "132800"
  },
  {
    "text": "handling different parts of the system",
    "start": "132800",
    "end": "134560"
  },
  {
    "text": "so the api server is like the front end",
    "start": "134560",
    "end": "136160"
  },
  {
    "text": "handling some requests the scheduler",
    "start": "136160",
    "end": "138160"
  },
  {
    "text": "manages the scheduling and eviction of",
    "start": "138160",
    "end": "140640"
  },
  {
    "text": "the nodes etc so similarly there are",
    "start": "140640",
    "end": "142879"
  },
  {
    "text": "components on the nodes themselves",
    "start": "142879",
    "end": "145200"
  },
  {
    "text": "that make sure everything's running",
    "start": "145200",
    "end": "146239"
  },
  {
    "text": "smoothly so like the network proxy that",
    "start": "146239",
    "end": "148000"
  },
  {
    "text": "allows communication between nodes for",
    "start": "148000",
    "end": "150319"
  },
  {
    "text": "traffic etc and there are other add-ons",
    "start": "150319",
    "end": "152239"
  },
  {
    "text": "that you can plug in like dns so",
    "start": "152239",
    "end": "154080"
  },
  {
    "text": "kubernetes allows you to create these",
    "start": "154080",
    "end": "156160"
  },
  {
    "text": "things called custom resource",
    "start": "156160",
    "end": "157280"
  },
  {
    "text": "definitions i'm sure most of you are",
    "start": "157280",
    "end": "159360"
  },
  {
    "text": "familiar with this and custom",
    "start": "159360",
    "end": "161040"
  },
  {
    "text": "controllers so when you combine these",
    "start": "161040",
    "end": "163040"
  },
  {
    "text": "two you get a declarative api or desired",
    "start": "163040",
    "end": "165440"
  },
  {
    "text": "state system so this is what k-serve",
    "start": "165440",
    "end": "167519"
  },
  {
    "text": "does it extends the kubernetes api",
    "start": "167519",
    "end": "170800"
  },
  {
    "text": "so i'm not a machine learning engineer",
    "start": "170800",
    "end": "172319"
  },
  {
    "text": "but i work pretty closely with them i",
    "start": "172319",
    "end": "173680"
  },
  {
    "text": "know they're like a few parts of the",
    "start": "173680",
    "end": "174879"
  },
  {
    "text": "machine learning life cycle or workflow",
    "start": "174879",
    "end": "177360"
  },
  {
    "text": "of course you have like the pipeline",
    "start": "177360",
    "end": "178640"
  },
  {
    "text": "development the data pipeline you can",
    "start": "178640",
    "end": "180720"
  },
  {
    "text": "develop it and there's also a training",
    "start": "180720",
    "end": "183440"
  },
  {
    "text": "component where you need a certain data",
    "start": "183440",
    "end": "185360"
  },
  {
    "text": "set to train the model and then once you",
    "start": "185360",
    "end": "187040"
  },
  {
    "text": "train the model there's also an",
    "start": "187040",
    "end": "188319"
  },
  {
    "text": "inference component so you use the model",
    "start": "188319",
    "end": "191120"
  },
  {
    "text": "in some",
    "start": "191120",
    "end": "192159"
  },
  {
    "text": "live machine learning algorithm to give",
    "start": "192159",
    "end": "193920"
  },
  {
    "text": "some output so today i'm going to focus",
    "start": "193920",
    "end": "196400"
  },
  {
    "text": "on the inference component that's what",
    "start": "196400",
    "end": "197680"
  },
  {
    "text": "caser focuses on",
    "start": "197680",
    "end": "199519"
  },
  {
    "text": "to be even more specific what caser",
    "start": "199519",
    "end": "201680"
  },
  {
    "text": "focuses on what i'm going to call core",
    "start": "201680",
    "end": "203440"
  },
  {
    "text": "inference and it focuses on the",
    "start": "203440",
    "end": "205200"
  },
  {
    "text": "deploying and monitoring of the core",
    "start": "205200",
    "end": "207440"
  },
  {
    "text": "inference part of an inference service",
    "start": "207440",
    "end": "209440"
  },
  {
    "text": "so the goal of case serve is to make",
    "start": "209440",
    "end": "211360"
  },
  {
    "text": "both the monitoring and deploying for",
    "start": "211360",
    "end": "212879"
  },
  {
    "text": "inference services super easy",
    "start": "212879",
    "end": "214959"
  },
  {
    "text": "so let's discuss the components that uh",
    "start": "214959",
    "end": "217200"
  },
  {
    "text": "make up part of the machine learning",
    "start": "217200",
    "end": "218720"
  },
  {
    "text": "workflow and some commonalities between",
    "start": "218720",
    "end": "221040"
  },
  {
    "text": "them",
    "start": "221040",
    "end": "221920"
  },
  {
    "text": "so just so we know what we're all",
    "start": "221920",
    "end": "223440"
  },
  {
    "text": "talking about before we get into it into",
    "start": "223440",
    "end": "225440"
  },
  {
    "text": "what k-ser really does",
    "start": "225440",
    "end": "227200"
  },
  {
    "text": "a lot of inference workflows have a",
    "start": "227200",
    "end": "230000"
  },
  {
    "text": "predictor in common so what exactly is a",
    "start": "230000",
    "end": "232480"
  },
  {
    "text": "predictor a predictor consists of a",
    "start": "232480",
    "end": "234159"
  },
  {
    "text": "model so maybe it's something like a",
    "start": "234159",
    "end": "236480"
  },
  {
    "text": "tensorflow run some regression does some",
    "start": "236480",
    "end": "238480"
  },
  {
    "text": "calculations and it runs on a serving",
    "start": "238480",
    "end": "240879"
  },
  {
    "text": "runtime called like torch serve for",
    "start": "240879",
    "end": "242720"
  },
  {
    "text": "example so all it's serving a model",
    "start": "242720",
    "end": "244640"
  },
  {
    "text": "server is is it wraps the model instead",
    "start": "244640",
    "end": "246799"
  },
  {
    "text": "of rest api so they can talk to the",
    "start": "246799",
    "end": "248319"
  },
  {
    "text": "model",
    "start": "248319",
    "end": "249599"
  },
  {
    "text": "but this is a very simple use case uh",
    "start": "249599",
    "end": "251760"
  },
  {
    "text": "usually the interference services also",
    "start": "251760",
    "end": "253360"
  },
  {
    "text": "have some more components like let's say",
    "start": "253360",
    "end": "255760"
  },
  {
    "text": "what we're going to call a transformer",
    "start": "255760",
    "end": "258239"
  },
  {
    "text": "so a transformer is optional but very",
    "start": "258239",
    "end": "260639"
  },
  {
    "text": "commonly used and it costs the predictor",
    "start": "260639",
    "end": "263680"
  },
  {
    "text": "it handles pre and post-processing of",
    "start": "263680",
    "end": "266240"
  },
  {
    "text": "the data like you have raw data and you",
    "start": "266240",
    "end": "267600"
  },
  {
    "text": "want to turn it into vectors before you",
    "start": "267600",
    "end": "269440"
  },
  {
    "text": "before you hit your model",
    "start": "269440",
    "end": "271120"
  },
  {
    "text": "um so you can also plug in and this is a",
    "start": "271120",
    "end": "273440"
  },
  {
    "text": "very simple view of a workflow as well",
    "start": "273440",
    "end": "275040"
  },
  {
    "text": "you can have a lot of other components",
    "start": "275040",
    "end": "276240"
  },
  {
    "text": "so you plug in like something we call an",
    "start": "276240",
    "end": "277919"
  },
  {
    "text": "explainer or a detector to detect things",
    "start": "277919",
    "end": "280240"
  },
  {
    "text": "that how the model is doing and we're",
    "start": "280240",
    "end": "282240"
  },
  {
    "text": "going to talk about those a little later",
    "start": "282240",
    "end": "283680"
  },
  {
    "text": "but basically this is a transformer and",
    "start": "283680",
    "end": "286479"
  },
  {
    "text": "you can't just make this and go to",
    "start": "286479",
    "end": "288479"
  },
  {
    "text": "production right there are a few things",
    "start": "288479",
    "end": "289919"
  },
  {
    "text": "you still need to do to have a",
    "start": "289919",
    "end": "291600"
  },
  {
    "text": "production ready and front service",
    "start": "291600",
    "end": "293759"
  },
  {
    "text": "for example you might not be able to use",
    "start": "293759",
    "end": "295840"
  },
  {
    "text": "any of the um out of the box model",
    "start": "295840",
    "end": "298320"
  },
  {
    "text": "servers and then you might need to",
    "start": "298320",
    "end": "299759"
  },
  {
    "text": "create your own so that takes some work",
    "start": "299759",
    "end": "301360"
  },
  {
    "text": "some code",
    "start": "301360",
    "end": "302479"
  },
  {
    "text": "you're going to need an ingress probably",
    "start": "302479",
    "end": "304080"
  },
  {
    "text": "to send requests from an external",
    "start": "304080",
    "end": "305680"
  },
  {
    "text": "service you need metrics you need to set",
    "start": "305680",
    "end": "307759"
  },
  {
    "text": "that up yourself and monitoring you need",
    "start": "307759",
    "end": "309520"
  },
  {
    "text": "load balancing distribute tracing uh",
    "start": "309520",
    "end": "312639"
  },
  {
    "text": "scaling both gpus and or cpus and it you",
    "start": "312639",
    "end": "316400"
  },
  {
    "text": "might need a team that has extensive",
    "start": "316400",
    "end": "317759"
  },
  {
    "text": "knowledge about the tools that allow you",
    "start": "317759",
    "end": "319680"
  },
  {
    "text": "to do this like",
    "start": "319680",
    "end": "321039"
  },
  {
    "text": "tools like k-native and seo that you",
    "start": "321039",
    "end": "322560"
  },
  {
    "text": "might want to use",
    "start": "322560",
    "end": "324800"
  },
  {
    "text": "so the question is like how can this be",
    "start": "324800",
    "end": "326320"
  },
  {
    "text": "made easier",
    "start": "326320",
    "end": "327840"
  },
  {
    "text": "well income's k-serve so k-serve was",
    "start": "327840",
    "end": "330560"
  },
  {
    "text": "formerly known as kf serving it was part",
    "start": "330560",
    "end": "332880"
  },
  {
    "text": "of the kubeflow project and it's taken",
    "start": "332880",
    "end": "334479"
  },
  {
    "text": "out now um into its own thing and it's",
    "start": "334479",
    "end": "336639"
  },
  {
    "text": "called k-serve so this is the first",
    "start": "336639",
    "end": "339280"
  },
  {
    "text": "sentence that uh k-serve has on its",
    "start": "339280",
    "end": "341280"
  },
  {
    "text": "github and i'll just read it real quick",
    "start": "341280",
    "end": "343360"
  },
  {
    "text": "k-state provides a kubernetes custom",
    "start": "343360",
    "end": "345120"
  },
  {
    "text": "resource definition",
    "start": "345120",
    "end": "347039"
  },
  {
    "text": "for serving machine learning models on",
    "start": "347039",
    "end": "349039"
  },
  {
    "text": "arbitrary frameworks it aims to solve",
    "start": "349039",
    "end": "350960"
  },
  {
    "text": "production model serving use cases by",
    "start": "350960",
    "end": "352479"
  },
  {
    "text": "providing performant high abstraction",
    "start": "352479",
    "end": "353759"
  },
  {
    "text": "interfaces for common ml frameworks like",
    "start": "353759",
    "end": "355360"
  },
  {
    "text": "tensorflow xt boost scikit-learn pytorch",
    "start": "355360",
    "end": "358000"
  },
  {
    "text": "and onyx okay so what does that really",
    "start": "358000",
    "end": "359919"
  },
  {
    "text": "mean so basically k-serve is a custom",
    "start": "359919",
    "end": "362319"
  },
  {
    "text": "resource definition it's just built on",
    "start": "362319",
    "end": "364400"
  },
  {
    "text": "top of",
    "start": "364400",
    "end": "365360"
  },
  {
    "text": "it's built on top of and abstracts it's",
    "start": "365360",
    "end": "367520"
  },
  {
    "text": "underlying layers like k-nav and seo for",
    "start": "367520",
    "end": "370080"
  },
  {
    "text": "example so it makes it really easy to",
    "start": "370080",
    "end": "371520"
  },
  {
    "text": "use you have a very simple yaml and",
    "start": "371520",
    "end": "373199"
  },
  {
    "text": "everything is created underneath for you",
    "start": "373199",
    "end": "375520"
  },
  {
    "text": "so k-surf supports many common machine",
    "start": "375520",
    "end": "377759"
  },
  {
    "text": "learning frameworks that data scientists",
    "start": "377759",
    "end": "379680"
  },
  {
    "text": "use it also allows you to build your own",
    "start": "379680",
    "end": "381600"
  },
  {
    "text": "custom one if you need",
    "start": "381600",
    "end": "383360"
  },
  {
    "text": "so k-serve is made for machine learning",
    "start": "383360",
    "end": "385280"
  },
  {
    "text": "workflows and allows for data scientists",
    "start": "385280",
    "end": "386880"
  },
  {
    "text": "to easily plug in their model and deploy",
    "start": "386880",
    "end": "388960"
  },
  {
    "text": "their workflow",
    "start": "388960",
    "end": "390080"
  },
  {
    "text": "workflow without needing to manually",
    "start": "390080",
    "end": "391520"
  },
  {
    "text": "deploy",
    "start": "391520",
    "end": "392880"
  },
  {
    "text": "and set up that infrastructure that",
    "start": "392880",
    "end": "394319"
  },
  {
    "text": "makes that workflow work efficiently",
    "start": "394319",
    "end": "397039"
  },
  {
    "text": "so let's look at the main components",
    "start": "397039",
    "end": "398880"
  },
  {
    "text": "that k-serve is built upon",
    "start": "398880",
    "end": "401440"
  },
  {
    "text": "so of course on the lower layer you have",
    "start": "401440",
    "end": "403280"
  },
  {
    "text": "your compute cluster like your gpu cpu",
    "start": "403280",
    "end": "405520"
  },
  {
    "text": "and then on top of it we have kubernetes",
    "start": "405520",
    "end": "407120"
  },
  {
    "text": "which is what we're running on and then",
    "start": "407120",
    "end": "408639"
  },
  {
    "text": "this optional layer of k-native and",
    "start": "408639",
    "end": "410400"
  },
  {
    "text": "istio k-nate is an open source community",
    "start": "410400",
    "end": "412800"
  },
  {
    "text": "project it adds components for deploying",
    "start": "412800",
    "end": "415360"
  },
  {
    "text": "running and managing services serverless",
    "start": "415360",
    "end": "417680"
  },
  {
    "text": "applications and kubernetes if you're",
    "start": "417680",
    "end": "418960"
  },
  {
    "text": "not familiar with it",
    "start": "418960",
    "end": "420400"
  },
  {
    "text": "so the reason i talk about it a lot",
    "start": "420400",
    "end": "421680"
  },
  {
    "text": "today even though it's optional is",
    "start": "421680",
    "end": "423039"
  },
  {
    "text": "because i feel like it actually makes",
    "start": "423039",
    "end": "424560"
  },
  {
    "text": "the use case 4k serve a lot more",
    "start": "424560",
    "end": "426960"
  },
  {
    "text": "interesting um and things you can do",
    "start": "426960",
    "end": "429039"
  },
  {
    "text": "like certain features it allows for is",
    "start": "429039",
    "end": "431360"
  },
  {
    "text": "like revision revision control traffic",
    "start": "431360",
    "end": "433759"
  },
  {
    "text": "management and rate limiting",
    "start": "433759",
    "end": "435840"
  },
  {
    "text": "so if you see in the future if you're",
    "start": "435840",
    "end": "437360"
  },
  {
    "text": "going to see a little alien pop up just",
    "start": "437360",
    "end": "438880"
  },
  {
    "text": "know that that is a k native component",
    "start": "438880",
    "end": "441280"
  },
  {
    "text": "that i'm introducing so it is optional",
    "start": "441280",
    "end": "443120"
  },
  {
    "text": "but nice to have",
    "start": "443120",
    "end": "444560"
  },
  {
    "text": "um also istio is an open source project",
    "start": "444560",
    "end": "446880"
  },
  {
    "text": "that focuses on",
    "start": "446880",
    "end": "448479"
  },
  {
    "text": "security connection and monitoring of",
    "start": "448479",
    "end": "450479"
  },
  {
    "text": "micro services",
    "start": "450479",
    "end": "452080"
  },
  {
    "text": "and at the top we have case surf so case",
    "start": "452080",
    "end": "453759"
  },
  {
    "text": "serve like i said is integrated with a",
    "start": "453759",
    "end": "455199"
  },
  {
    "text": "lot of common machine learning",
    "start": "455199",
    "end": "456319"
  },
  {
    "text": "frameworks and we'll talk more about",
    "start": "456319",
    "end": "458000"
  },
  {
    "text": "those",
    "start": "458000",
    "end": "458800"
  },
  {
    "text": "later",
    "start": "458800",
    "end": "460400"
  },
  {
    "start": "460000",
    "end": "500000"
  },
  {
    "text": "okay so here are some of the main",
    "start": "460400",
    "end": "462000"
  },
  {
    "text": "features of case serve",
    "start": "462000",
    "end": "464080"
  },
  {
    "text": "it allows you to scale up and down from",
    "start": "464080",
    "end": "465919"
  },
  {
    "text": "zero to and from zero you can have",
    "start": "465919",
    "end": "468240"
  },
  {
    "text": "request space auto scaling based on your",
    "start": "468240",
    "end": "470160"
  },
  {
    "text": "cpu or gpu",
    "start": "470160",
    "end": "472000"
  },
  {
    "text": "um it also offers request batching if",
    "start": "472000",
    "end": "474319"
  },
  {
    "text": "that's what you want and then you can do",
    "start": "474319",
    "end": "476080"
  },
  {
    "text": "requests and response logging as well",
    "start": "476080",
    "end": "478879"
  },
  {
    "text": "it offers security with authentication",
    "start": "478879",
    "end": "480800"
  },
  {
    "text": "and authorization traffic management",
    "start": "480800",
    "end": "484240"
  },
  {
    "text": "distributed tracing and out-of-the-box",
    "start": "484240",
    "end": "486160"
  },
  {
    "text": "metrics so here's a few of them and it",
    "start": "486160",
    "end": "488560"
  },
  {
    "text": "also offers like rollout strategies like",
    "start": "488560",
    "end": "490080"
  },
  {
    "text": "canary rollout which i'll do a little",
    "start": "490080",
    "end": "491680"
  },
  {
    "text": "demo on later but here are just a few of",
    "start": "491680",
    "end": "494000"
  },
  {
    "text": "the main features of k-serve and next",
    "start": "494000",
    "end": "496560"
  },
  {
    "text": "let's talk about how we're actually able",
    "start": "496560",
    "end": "498560"
  },
  {
    "text": "to achieve that",
    "start": "498560",
    "end": "500319"
  },
  {
    "start": "500000",
    "end": "580000"
  },
  {
    "text": "okay so let's talk about the predictor",
    "start": "500319",
    "end": "503599"
  },
  {
    "text": "we talked about a predictor before like",
    "start": "503599",
    "end": "504960"
  },
  {
    "text": "in general terms but let's walk through",
    "start": "504960",
    "end": "506479"
  },
  {
    "text": "how k-serve enhances the single model",
    "start": "506479",
    "end": "508800"
  },
  {
    "text": "use case of a predictor",
    "start": "508800",
    "end": "510879"
  },
  {
    "text": "so first we have something called a",
    "start": "510879",
    "end": "512159"
  },
  {
    "text": "storage initializer which is an init",
    "start": "512159",
    "end": "513760"
  },
  {
    "text": "container so it comes up",
    "start": "513760",
    "end": "515760"
  },
  {
    "text": "it reaches out to your model storage",
    "start": "515760",
    "end": "517279"
  },
  {
    "text": "wherever your model is stored there's",
    "start": "517279",
    "end": "518479"
  },
  {
    "text": "multiple options in case there for that",
    "start": "518479",
    "end": "520880"
  },
  {
    "text": "download your model and then go away",
    "start": "520880",
    "end": "523599"
  },
  {
    "text": "so the nexus",
    "start": "523599",
    "end": "524959"
  },
  {
    "text": "the other two things that are going to",
    "start": "524959",
    "end": "526080"
  },
  {
    "text": "be up running are the q proxy this is a",
    "start": "526080",
    "end": "528160"
  },
  {
    "text": "k native component so it is optional but",
    "start": "528160",
    "end": "531519"
  },
  {
    "text": "it routes the request to a k-serve",
    "start": "531519",
    "end": "533600"
  },
  {
    "text": "container uh with max concurrent",
    "start": "533600",
    "end": "535440"
  },
  {
    "text": "requests configured so it gives you some",
    "start": "535440",
    "end": "537200"
  },
  {
    "text": "rate limiting",
    "start": "537200",
    "end": "538399"
  },
  {
    "text": "um",
    "start": "538399",
    "end": "540000"
  },
  {
    "text": "and the main part of the inference",
    "start": "540000",
    "end": "541519"
  },
  {
    "text": "service is the model and the model",
    "start": "541519",
    "end": "542800"
  },
  {
    "text": "server like we talked about and it",
    "start": "542800",
    "end": "544080"
  },
  {
    "text": "allows you to easily plug in",
    "start": "544080",
    "end": "546000"
  },
  {
    "text": "some things out of the box as well as",
    "start": "546000",
    "end": "547200"
  },
  {
    "text": "create your own custom",
    "start": "547200",
    "end": "548480"
  },
  {
    "text": "model server",
    "start": "548480",
    "end": "550560"
  },
  {
    "text": "so",
    "start": "550560",
    "end": "551440"
  },
  {
    "text": "next let's talk about a transformer",
    "start": "551440",
    "end": "554240"
  },
  {
    "text": "so transformers are commonly used for",
    "start": "554240",
    "end": "556080"
  },
  {
    "text": "pre and post processing as i discussed",
    "start": "556080",
    "end": "557839"
  },
  {
    "text": "earlier",
    "start": "557839",
    "end": "559440"
  },
  {
    "text": "and another great thing about this is",
    "start": "559440",
    "end": "561600"
  },
  {
    "text": "that you can easily plug in certain",
    "start": "561600",
    "end": "564240"
  },
  {
    "text": "things for feature augmentation like",
    "start": "564240",
    "end": "566000"
  },
  {
    "text": "fees feature store in k-serve which is",
    "start": "566000",
    "end": "567839"
  },
  {
    "text": "really nice",
    "start": "567839",
    "end": "569040"
  },
  {
    "text": "and you can also create your own custom",
    "start": "569040",
    "end": "570480"
  },
  {
    "text": "transformer if you like",
    "start": "570480",
    "end": "572000"
  },
  {
    "text": "and uh now that we have a better idea of",
    "start": "572000",
    "end": "574720"
  },
  {
    "text": "how k-serve enhances the core inference",
    "start": "574720",
    "end": "577040"
  },
  {
    "text": "components let's discuss the control",
    "start": "577040",
    "end": "578880"
  },
  {
    "text": "plane",
    "start": "578880",
    "end": "580560"
  },
  {
    "start": "580000",
    "end": "624000"
  },
  {
    "text": "so",
    "start": "580560",
    "end": "581440"
  },
  {
    "text": "if you can remember from the control",
    "start": "581440",
    "end": "583120"
  },
  {
    "text": "plane slide",
    "start": "583120",
    "end": "584399"
  },
  {
    "text": "the control plane handles the state of",
    "start": "584399",
    "end": "586399"
  },
  {
    "text": "of the system so the inference service",
    "start": "586399",
    "end": "588480"
  },
  {
    "text": "defines the state and the k-12 k-serve",
    "start": "588480",
    "end": "591040"
  },
  {
    "text": "controller is in charge of reconciling",
    "start": "591040",
    "end": "593920"
  },
  {
    "text": "that so we make sure we're getting to",
    "start": "593920",
    "end": "595920"
  },
  {
    "text": "the desired state",
    "start": "595920",
    "end": "597519"
  },
  {
    "text": "it creates all the resources for the",
    "start": "597519",
    "end": "599279"
  },
  {
    "text": "infant service needs below it",
    "start": "599279",
    "end": "602640"
  },
  {
    "text": "so next we have something like canadian",
    "start": "602640",
    "end": "604880"
  },
  {
    "text": "services manage revisions network",
    "start": "604880",
    "end": "607120"
  },
  {
    "text": "routing",
    "start": "607120",
    "end": "608079"
  },
  {
    "text": "and um",
    "start": "608079",
    "end": "609920"
  },
  {
    "text": "the canadian provision that is like",
    "start": "609920",
    "end": "611600"
  },
  {
    "text": "handling those revisions directly",
    "start": "611600",
    "end": "614000"
  },
  {
    "text": "and then the deployment so if you don't",
    "start": "614000",
    "end": "616560"
  },
  {
    "text": "have k-native for example you wouldn't",
    "start": "616560",
    "end": "618240"
  },
  {
    "text": "have the two things on the left and you",
    "start": "618240",
    "end": "619680"
  },
  {
    "text": "would just have directly deployment but",
    "start": "619680",
    "end": "621120"
  },
  {
    "text": "you wouldn't get a few of those features",
    "start": "621120",
    "end": "622640"
  },
  {
    "text": "that are added",
    "start": "622640",
    "end": "624720"
  },
  {
    "start": "624000",
    "end": "669000"
  },
  {
    "text": "so moving on from the deployment um",
    "start": "624720",
    "end": "627200"
  },
  {
    "text": "which watches the traffic flow and then",
    "start": "627200",
    "end": "629440"
  },
  {
    "text": "it helps go to the replicas",
    "start": "629440",
    "end": "631440"
  },
  {
    "text": "what we also have is you can plug in the",
    "start": "631440",
    "end": "633760"
  },
  {
    "text": "k native parallel scalar or the",
    "start": "633760",
    "end": "635600"
  },
  {
    "text": "horizontal auto scaler which um",
    "start": "635600",
    "end": "639519"
  },
  {
    "text": "which handles like the",
    "start": "639519",
    "end": "641360"
  },
  {
    "text": "uh the replicas and scaling up and down",
    "start": "641360",
    "end": "644160"
  },
  {
    "text": "and then you'll have something like your",
    "start": "644160",
    "end": "645200"
  },
  {
    "text": "transformer pods and your predictor pods",
    "start": "645200",
    "end": "649440"
  },
  {
    "text": "um so this is just a",
    "start": "650000",
    "end": "651600"
  },
  {
    "text": "higher level overview of what we just",
    "start": "651600",
    "end": "653120"
  },
  {
    "text": "went through um again you'll see the",
    "start": "653120",
    "end": "654959"
  },
  {
    "text": "model storage there like i mentioned",
    "start": "654959",
    "end": "657120"
  },
  {
    "text": "there are multiple of types of model",
    "start": "657120",
    "end": "659360"
  },
  {
    "text": "storage that's supported through k-serve",
    "start": "659360",
    "end": "661040"
  },
  {
    "text": "like s3 uri azure and persistent volume",
    "start": "661040",
    "end": "664399"
  },
  {
    "text": "claims and more being added in this next",
    "start": "664399",
    "end": "666720"
  },
  {
    "text": "version as well so",
    "start": "666720",
    "end": "668880"
  },
  {
    "text": "more options there",
    "start": "668880",
    "end": "670240"
  },
  {
    "start": "669000",
    "end": "706000"
  },
  {
    "text": "so this is like a wider view of what i",
    "start": "670240",
    "end": "672000"
  },
  {
    "text": "just showed and the reason i like this",
    "start": "672000",
    "end": "673519"
  },
  {
    "text": "because i use this uh command line tool",
    "start": "673519",
    "end": "675440"
  },
  {
    "text": "called coop pedal tree and i think it's",
    "start": "675440",
    "end": "677440"
  },
  {
    "text": "really helpful for me to just see i'm",
    "start": "677440",
    "end": "679040"
  },
  {
    "text": "not going to go into each box but uh the",
    "start": "679040",
    "end": "681120"
  },
  {
    "text": "pink is istio the dart blues",
    "start": "681120",
    "end": "683360"
  },
  {
    "text": "the api is k-serve and then the gray the",
    "start": "683360",
    "end": "685920"
  },
  {
    "text": "api is k-native i'm not going to go into",
    "start": "685920",
    "end": "688480"
  },
  {
    "text": "each of these but i think this is just",
    "start": "688480",
    "end": "689680"
  },
  {
    "text": "helpful especially as someone who's",
    "start": "689680",
    "end": "690800"
  },
  {
    "text": "learning about this to get a view of all",
    "start": "690800",
    "end": "693120"
  },
  {
    "text": "the resources that are created and then",
    "start": "693120",
    "end": "694640"
  },
  {
    "text": "you can like",
    "start": "694640",
    "end": "695760"
  },
  {
    "text": "cut or describe or look into each of",
    "start": "695760",
    "end": "698079"
  },
  {
    "text": "those resources and like see see what",
    "start": "698079",
    "end": "700160"
  },
  {
    "text": "their settings are so just for",
    "start": "700160",
    "end": "701600"
  },
  {
    "text": "understanding how k-serve works i",
    "start": "701600",
    "end": "703279"
  },
  {
    "text": "thought this was super helpful",
    "start": "703279",
    "end": "706320"
  },
  {
    "start": "706000",
    "end": "740000"
  },
  {
    "text": "okay so next",
    "start": "707040",
    "end": "709680"
  },
  {
    "text": "now that we talked about the control",
    "start": "709680",
    "end": "710800"
  },
  {
    "text": "plane let's talk a little bit about the",
    "start": "710800",
    "end": "711920"
  },
  {
    "text": "data plane so let's follow a request",
    "start": "711920",
    "end": "713279"
  },
  {
    "text": "path so you have your ai application it",
    "start": "713279",
    "end": "715360"
  },
  {
    "text": "hits a transformer service um then it's",
    "start": "715360",
    "end": "717680"
  },
  {
    "text": "going",
    "start": "717680",
    "end": "718800"
  },
  {
    "text": "sorry then it's gonna hit the",
    "start": "718800",
    "end": "720240"
  },
  {
    "text": "transformer pod okay there we go",
    "start": "720240",
    "end": "722800"
  },
  {
    "text": "it's gonna hit the transformer it's",
    "start": "722800",
    "end": "724000"
  },
  {
    "text": "gonna hit the q proxy and hit the",
    "start": "724000",
    "end": "725600"
  },
  {
    "text": "transformer prod and it's gonna maybe do",
    "start": "725600",
    "end": "727440"
  },
  {
    "text": "some pre-processing and then it's gonna",
    "start": "727440",
    "end": "728880"
  },
  {
    "text": "reach out to the predictor service",
    "start": "728880",
    "end": "731360"
  },
  {
    "text": "and then it's going to",
    "start": "731360",
    "end": "733200"
  },
  {
    "text": "hit hit the",
    "start": "733200",
    "end": "734800"
  },
  {
    "text": "predictor pod and that q proxy and then",
    "start": "734800",
    "end": "737360"
  },
  {
    "text": "it's going to run the model it's going",
    "start": "737360",
    "end": "738560"
  },
  {
    "text": "to run your inference for you",
    "start": "738560",
    "end": "741120"
  },
  {
    "start": "740000",
    "end": "798000"
  },
  {
    "text": "okay so that was the david plane and now",
    "start": "741120",
    "end": "743120"
  },
  {
    "text": "let's talk about something that's",
    "start": "743120",
    "end": "744160"
  },
  {
    "text": "interesting that's just added",
    "start": "744160",
    "end": "745839"
  },
  {
    "text": "the inference graph",
    "start": "745839",
    "end": "747279"
  },
  {
    "text": "so the inference services can be set up",
    "start": "747279",
    "end": "749120"
  },
  {
    "text": "as a chain pipeline with k-serve which",
    "start": "749120",
    "end": "751200"
  },
  {
    "text": "is super nice and useful",
    "start": "751200",
    "end": "753360"
  },
  {
    "text": "and this new feature allows that this",
    "start": "753360",
    "end": "755360"
  },
  {
    "text": "new feature allows for chaining them in",
    "start": "755360",
    "end": "756720"
  },
  {
    "text": "different ways and the case their",
    "start": "756720",
    "end": "758240"
  },
  {
    "text": "contributors have been working very hard",
    "start": "758240",
    "end": "759519"
  },
  {
    "text": "on it so i just think some of the",
    "start": "759519",
    "end": "761440"
  },
  {
    "text": "different modes you can use so you can",
    "start": "761440",
    "end": "763040"
  },
  {
    "text": "use sequence node which allows you to",
    "start": "763040",
    "end": "765360"
  },
  {
    "text": "connect two",
    "start": "765360",
    "end": "766480"
  },
  {
    "text": "inference services in a sequence which",
    "start": "766480",
    "end": "768800"
  },
  {
    "text": "is like",
    "start": "768800",
    "end": "769920"
  },
  {
    "text": "how i have it there you can also use",
    "start": "769920",
    "end": "772720"
  },
  {
    "text": "this mode called switch node which",
    "start": "772720",
    "end": "774320"
  },
  {
    "text": "allows users to select an inference",
    "start": "774320",
    "end": "775839"
  },
  {
    "text": "service to handle requests based on a",
    "start": "775839",
    "end": "777279"
  },
  {
    "text": "condition so kind of like which one do",
    "start": "777279",
    "end": "779360"
  },
  {
    "text": "you want to go to splitter node allows",
    "start": "779360",
    "end": "781440"
  },
  {
    "text": "users to split traffic between multiple",
    "start": "781440",
    "end": "783680"
  },
  {
    "text": "inference services",
    "start": "783680",
    "end": "785120"
  },
  {
    "text": "using a weighted distribution",
    "start": "785120",
    "end": "787600"
  },
  {
    "text": "and you can also use something called",
    "start": "787600",
    "end": "788880"
  },
  {
    "text": "ensemble node which allows users to",
    "start": "788880",
    "end": "790560"
  },
  {
    "text": "route requests to multiple models in",
    "start": "790560",
    "end": "792320"
  },
  {
    "text": "parallel",
    "start": "792320",
    "end": "794000"
  },
  {
    "text": "so here are some of the new features",
    "start": "794000",
    "end": "795279"
  },
  {
    "text": "that we're adding um yeah i think that's",
    "start": "795279",
    "end": "797519"
  },
  {
    "text": "exciting",
    "start": "797519",
    "end": "799200"
  },
  {
    "start": "798000",
    "end": "841000"
  },
  {
    "text": "so another important feature of k-serve",
    "start": "799200",
    "end": "801440"
  },
  {
    "text": "is that it uses a standard standardized",
    "start": "801440",
    "end": "803279"
  },
  {
    "text": "inference protocol across all of the",
    "start": "803279",
    "end": "805440"
  },
  {
    "text": "serving run times so that people can",
    "start": "805440",
    "end": "807200"
  },
  {
    "text": "easily switch from",
    "start": "807200",
    "end": "808800"
  },
  {
    "text": "say tensorflow to pytorch without",
    "start": "808800",
    "end": "810399"
  },
  {
    "text": "needing to",
    "start": "810399",
    "end": "811680"
  },
  {
    "text": "change the way you talk to the inference",
    "start": "811680",
    "end": "813040"
  },
  {
    "text": "service so this also makes it really",
    "start": "813040",
    "end": "814959"
  },
  {
    "text": "easy for testing and benchmarking for",
    "start": "814959",
    "end": "816720"
  },
  {
    "text": "the user so it's just something that's",
    "start": "816720",
    "end": "819040"
  },
  {
    "text": "really",
    "start": "819040",
    "end": "819839"
  },
  {
    "text": "nice for users to be able to use and be",
    "start": "819839",
    "end": "821440"
  },
  {
    "text": "able to switch between models that they",
    "start": "821440",
    "end": "822720"
  },
  {
    "text": "need to so i have a cute little",
    "start": "822720",
    "end": "824320"
  },
  {
    "text": "animation here he's like hi these",
    "start": "824320",
    "end": "826399"
  },
  {
    "text": "suitcases standard inference protocol",
    "start": "826399",
    "end": "828560"
  },
  {
    "text": "and she's like yeah i have health uh i",
    "start": "828560",
    "end": "830720"
  },
  {
    "text": "have standard endpoints for health",
    "start": "830720",
    "end": "832480"
  },
  {
    "text": "server and model metadata and inference",
    "start": "832480",
    "end": "834720"
  },
  {
    "text": "he's like great how can i talk to you",
    "start": "834720",
    "end": "836959"
  },
  {
    "text": "http or trpc because that's what k",
    "start": "836959",
    "end": "839120"
  },
  {
    "text": "server supports",
    "start": "839120",
    "end": "842399"
  },
  {
    "start": "841000",
    "end": "919000"
  },
  {
    "text": "okay so here are some of the data plane",
    "start": "842399",
    "end": "844320"
  },
  {
    "text": "plugins like i said we have this core",
    "start": "844320",
    "end": "846639"
  },
  {
    "text": "inference part which is the transformer",
    "start": "846639",
    "end": "848399"
  },
  {
    "text": "and the predictor but sometimes we need",
    "start": "848399",
    "end": "850480"
  },
  {
    "text": "to add some more components and plug-ins",
    "start": "850480",
    "end": "853120"
  },
  {
    "text": "into it as well",
    "start": "853120",
    "end": "855199"
  },
  {
    "text": "so we can do something called explain so",
    "start": "855199",
    "end": "857120"
  },
  {
    "text": "instead of hitting the prediction point",
    "start": "857120",
    "end": "858800"
  },
  {
    "text": "and this one you would um hit explain",
    "start": "858800",
    "end": "861040"
  },
  {
    "text": "which would then hit predict",
    "start": "861040",
    "end": "862639"
  },
  {
    "text": "and there's multiple",
    "start": "862639",
    "end": "864959"
  },
  {
    "text": "explainers you can use so basically the",
    "start": "864959",
    "end": "866639"
  },
  {
    "text": "way i understand is that explainer says",
    "start": "866639",
    "end": "868720"
  },
  {
    "text": "why did my model make this prediction",
    "start": "868720",
    "end": "870399"
  },
  {
    "text": "for a given instance um so if you have",
    "start": "870399",
    "end": "872720"
  },
  {
    "text": "an image and it defines that image as a",
    "start": "872720",
    "end": "875760"
  },
  {
    "text": "dog then uh",
    "start": "875760",
    "end": "877760"
  },
  {
    "text": "then it will explain what part of the",
    "start": "877760",
    "end": "879440"
  },
  {
    "text": "image is most relevant to making um that",
    "start": "879440",
    "end": "881680"
  },
  {
    "text": "result so that's one thing you can have",
    "start": "881680",
    "end": "884560"
  },
  {
    "text": "a batcher for is a sad a side car that",
    "start": "884560",
    "end": "887440"
  },
  {
    "text": "sits in between the q proxy and the k",
    "start": "887440",
    "end": "889040"
  },
  {
    "text": "surf container",
    "start": "889040",
    "end": "890639"
  },
  {
    "text": "you can set things like max latency and",
    "start": "890639",
    "end": "892639"
  },
  {
    "text": "max bat batch size",
    "start": "892639",
    "end": "895120"
  },
  {
    "text": "and then also there's a monitor and",
    "start": "895120",
    "end": "896880"
  },
  {
    "text": "logger plugin that you can add um",
    "start": "896880",
    "end": "899760"
  },
  {
    "text": "so it has asynchronous payload logging",
    "start": "899760",
    "end": "902240"
  },
  {
    "text": "monitoring the distributed uh the",
    "start": "902240",
    "end": "903839"
  },
  {
    "text": "distribution of incoming requests with",
    "start": "903839",
    "end": "905360"
  },
  {
    "text": "detectors and we'll talk about a few of",
    "start": "905360",
    "end": "906959"
  },
  {
    "text": "those detectors on the next slide",
    "start": "906959",
    "end": "908639"
  },
  {
    "text": "you can also hook up alerting and also i",
    "start": "908639",
    "end": "910480"
  },
  {
    "text": "see people taking pictures which is",
    "start": "910480",
    "end": "911839"
  },
  {
    "text": "totally fine but just let you know um i",
    "start": "911839",
    "end": "913440"
  },
  {
    "text": "do have these uploaded as well if you",
    "start": "913440",
    "end": "915040"
  },
  {
    "text": "want to see them later if you'd like to",
    "start": "915040",
    "end": "916480"
  },
  {
    "text": "reference them i have them on the um",
    "start": "916480",
    "end": "918480"
  },
  {
    "text": "i have them on the site",
    "start": "918480",
    "end": "920320"
  },
  {
    "start": "919000",
    "end": "1013000"
  },
  {
    "text": "um so yeah let's talk a little bit about",
    "start": "920320",
    "end": "923760"
  },
  {
    "text": "how the monitor and logger works so the",
    "start": "923760",
    "end": "925680"
  },
  {
    "text": "broker",
    "start": "925680",
    "end": "927040"
  },
  {
    "text": "routes events to the consumers so you",
    "start": "927040",
    "end": "928959"
  },
  {
    "text": "can have something called a trigger",
    "start": "928959",
    "end": "930560"
  },
  {
    "text": "which forwards events to a message",
    "start": "930560",
    "end": "932160"
  },
  {
    "text": "dumper service and",
    "start": "932160",
    "end": "935040"
  },
  {
    "text": "you can set up asynchronous payload",
    "start": "935040",
    "end": "937759"
  },
  {
    "text": "logging to a dumb mattress based on a",
    "start": "937759",
    "end": "939839"
  },
  {
    "text": "trigger like cloud events or k native",
    "start": "939839",
    "end": "942480"
  },
  {
    "text": "anything like that",
    "start": "942480",
    "end": "943839"
  },
  {
    "text": "and",
    "start": "943839",
    "end": "945440"
  },
  {
    "text": "you can monitor incoming requests with",
    "start": "945440",
    "end": "948480"
  },
  {
    "text": "detectors in order to trust the models",
    "start": "948480",
    "end": "950240"
  },
  {
    "text": "so detectors like drift checks if the",
    "start": "950240",
    "end": "952800"
  },
  {
    "text": "distribution of an incoming request",
    "start": "952800",
    "end": "954399"
  },
  {
    "text": "diverges from",
    "start": "954399",
    "end": "955839"
  },
  {
    "text": "a reference distribution like the",
    "start": "955839",
    "end": "957279"
  },
  {
    "text": "training data for example",
    "start": "957279",
    "end": "959120"
  },
  {
    "text": "the outlier detector flags single",
    "start": "959120",
    "end": "960720"
  },
  {
    "text": "instances outside the distribution and",
    "start": "960720",
    "end": "963279"
  },
  {
    "text": "you can have something like a biased",
    "start": "963279",
    "end": "964320"
  },
  {
    "text": "attack detector adversarial detector",
    "start": "964320",
    "end": "967759"
  },
  {
    "text": "for adversarially modified inputs",
    "start": "967759",
    "end": "970399"
  },
  {
    "text": "and with all of this you can also hook",
    "start": "970399",
    "end": "972480"
  },
  {
    "text": "up alerting",
    "start": "972480",
    "end": "973920"
  },
  {
    "text": "so you can have good observable good",
    "start": "973920",
    "end": "975680"
  },
  {
    "text": "observability into your inference",
    "start": "975680",
    "end": "977199"
  },
  {
    "text": "service",
    "start": "977199",
    "end": "978800"
  },
  {
    "text": "yeah",
    "start": "978800",
    "end": "980000"
  },
  {
    "text": "okay",
    "start": "980000",
    "end": "980959"
  },
  {
    "text": "so we",
    "start": "980959",
    "end": "982399"
  },
  {
    "text": "discussed a little bit about some model",
    "start": "982399",
    "end": "984000"
  },
  {
    "text": "serving run times but there are a few of",
    "start": "984000",
    "end": "986399"
  },
  {
    "text": "them and caser keeps adding support for",
    "start": "986399",
    "end": "988639"
  },
  {
    "text": "more out of the box um for example you",
    "start": "988639",
    "end": "991360"
  },
  {
    "text": "can use the triton inference server tort",
    "start": "991360",
    "end": "993440"
  },
  {
    "text": "serve sklearn sc boost just uh some of",
    "start": "993440",
    "end": "996320"
  },
  {
    "text": "the few ones you can use and you can use",
    "start": "996320",
    "end": "998480"
  },
  {
    "text": "with those tensorflows ports torch",
    "start": "998480",
    "end": "1000639"
  },
  {
    "text": "script pytorch onyx and xgboost so there",
    "start": "1000639",
    "end": "1003519"
  },
  {
    "text": "are a lot of options and they're a lot",
    "start": "1003519",
    "end": "1004720"
  },
  {
    "text": "more coming out of the box which makes",
    "start": "1004720",
    "end": "1006639"
  },
  {
    "text": "it really easy for data science to just",
    "start": "1006639",
    "end": "1008079"
  },
  {
    "text": "plug in their serving runtime and get",
    "start": "1008079",
    "end": "1010240"
  },
  {
    "text": "going with their with their inference",
    "start": "1010240",
    "end": "1011600"
  },
  {
    "text": "service",
    "start": "1011600",
    "end": "1013759"
  },
  {
    "start": "1013000",
    "end": "1055000"
  },
  {
    "text": "so how does this actually look you can",
    "start": "1013759",
    "end": "1015120"
  },
  {
    "text": "define the serving runtime if you want",
    "start": "1015120",
    "end": "1017519"
  },
  {
    "text": "and the or you can divide the model",
    "start": "1017519",
    "end": "1019199"
  },
  {
    "text": "format in the yaml so this is like a",
    "start": "1019199",
    "end": "1020720"
  },
  {
    "text": "very basic inference service this is",
    "start": "1020720",
    "end": "1022320"
  },
  {
    "text": "what the yama would look like and",
    "start": "1022320",
    "end": "1024798"
  },
  {
    "text": "you can define that you're using say sk",
    "start": "1024799",
    "end": "1026959"
  },
  {
    "text": "an sk learn model and from here in the",
    "start": "1026959",
    "end": "1030000"
  },
  {
    "text": "new format and the v2 format",
    "start": "1030000",
    "end": "1032558"
  },
  {
    "text": "k server will be able to just infer",
    "start": "1032559",
    "end": "1034240"
  },
  {
    "text": "which wallet serving runtime you're",
    "start": "1034240",
    "end": "1035438"
  },
  {
    "text": "running you don't even have to define it",
    "start": "1035439",
    "end": "1037038"
  },
  {
    "text": "but if you want to you can",
    "start": "1037039",
    "end": "1039120"
  },
  {
    "text": "so what does this do",
    "start": "1039120",
    "end": "1040880"
  },
  {
    "text": "it creates a custom resource called",
    "start": "1040880",
    "end": "1042400"
  },
  {
    "text": "serving runtime so on the right you'll",
    "start": "1042400",
    "end": "1044240"
  },
  {
    "text": "see the custom resource that it creates",
    "start": "1044240",
    "end": "1046319"
  },
  {
    "text": "and this way you as a as a user don't",
    "start": "1046319",
    "end": "1048880"
  },
  {
    "text": "have to set this up yourself it's just",
    "start": "1048880",
    "end": "1050240"
  },
  {
    "text": "generated for you",
    "start": "1050240",
    "end": "1051679"
  },
  {
    "text": "and you can figure it as you can",
    "start": "1051679",
    "end": "1053039"
  },
  {
    "text": "configure it as you like",
    "start": "1053039",
    "end": "1056000"
  },
  {
    "start": "1055000",
    "end": "1183000"
  },
  {
    "text": "okay so let's talk about what we've",
    "start": "1056000",
    "end": "1058640"
  },
  {
    "text": "what we've discussed so far as a single",
    "start": "1058640",
    "end": "1060640"
  },
  {
    "text": "model",
    "start": "1060640",
    "end": "1061679"
  },
  {
    "text": "serving is a single model serving model",
    "start": "1061679",
    "end": "1064160"
  },
  {
    "text": "so basically what that means is that for",
    "start": "1064160",
    "end": "1065840"
  },
  {
    "text": "each pod we have um one model right so",
    "start": "1065840",
    "end": "1070160"
  },
  {
    "text": "you probably at least have one replica",
    "start": "1070160",
    "end": "1071919"
  },
  {
    "text": "of each pod running um just just to be",
    "start": "1071919",
    "end": "1074720"
  },
  {
    "text": "safe uh what happens if you have i don't",
    "start": "1074720",
    "end": "1077280"
  },
  {
    "text": "know let's say like",
    "start": "1077280",
    "end": "1078640"
  },
  {
    "text": "five models so you at least have tin",
    "start": "1078640",
    "end": "1079840"
  },
  {
    "text": "pots running",
    "start": "1079840",
    "end": "1081200"
  },
  {
    "text": "but you also probably have a transformer",
    "start": "1081200",
    "end": "1083600"
  },
  {
    "text": "and for each replica you probably have",
    "start": "1083600",
    "end": "1084960"
  },
  {
    "text": "another one so you probably at least",
    "start": "1084960",
    "end": "1086960"
  },
  {
    "text": "have for each model four pods running",
    "start": "1086960",
    "end": "1090559"
  },
  {
    "text": "um so as you can tell this is okay for",
    "start": "1090559",
    "end": "1093360"
  },
  {
    "text": "maybe you know five models what happens",
    "start": "1093360",
    "end": "1095360"
  },
  {
    "text": "when you get to like a hundred or a",
    "start": "1095360",
    "end": "1097120"
  },
  {
    "text": "thousand models",
    "start": "1097120",
    "end": "1098480"
  },
  {
    "text": "um obviously this isn't very scalable",
    "start": "1098480",
    "end": "1100799"
  },
  {
    "text": "and it's very costly so",
    "start": "1100799",
    "end": "1103520"
  },
  {
    "text": "kubernetes also has this max pod limit",
    "start": "1103520",
    "end": "1105600"
  },
  {
    "text": "about per cluster so it's like 110 100",
    "start": "1105600",
    "end": "1108080"
  },
  {
    "text": "it's best practice and there's also an",
    "start": "1108080",
    "end": "1110000"
  },
  {
    "text": "ip limit uh i ip address limitation so",
    "start": "1110000",
    "end": "1113919"
  },
  {
    "text": "there's like you can have at most 4096",
    "start": "1113919",
    "end": "1116559"
  },
  {
    "text": "so about",
    "start": "1116559",
    "end": "1117799"
  },
  {
    "text": "1024 models are still running at a time",
    "start": "1117799",
    "end": "1120799"
  },
  {
    "text": "with this use case",
    "start": "1120799",
    "end": "1122640"
  },
  {
    "text": "so there's one way we can do better and",
    "start": "1122640",
    "end": "1124559"
  },
  {
    "text": "that's called multi-model serving",
    "start": "1124559",
    "end": "1126640"
  },
  {
    "text": "so multi-minus serving is designed to",
    "start": "1126640",
    "end": "1128559"
  },
  {
    "text": "address uh three types of limitations",
    "start": "1128559",
    "end": "1130559"
  },
  {
    "text": "that case server run into that we just",
    "start": "1130559",
    "end": "1131919"
  },
  {
    "text": "discussed so the compute resource",
    "start": "1131919",
    "end": "1134080"
  },
  {
    "text": "limitation there's an overhead of",
    "start": "1134080",
    "end": "1135760"
  },
  {
    "text": "running you know just one model per pod",
    "start": "1135760",
    "end": "1138320"
  },
  {
    "text": "the maximum pod limitation and the",
    "start": "1138320",
    "end": "1140240"
  },
  {
    "text": "maximum ip address limitation",
    "start": "1140240",
    "end": "1142720"
  },
  {
    "text": "but there's still a limitation to even",
    "start": "1142720",
    "end": "1144720"
  },
  {
    "text": "this model which allows you to run",
    "start": "1144720",
    "end": "1146320"
  },
  {
    "text": "multiple",
    "start": "1146320",
    "end": "1147679"
  },
  {
    "text": "models on one single serving run time",
    "start": "1147679",
    "end": "1151520"
  },
  {
    "text": "for this one each replicas were high",
    "start": "1151520",
    "end": "1153200"
  },
  {
    "text": "required to hold the same set of models",
    "start": "1153200",
    "end": "1157120"
  },
  {
    "text": "so you see always cba and bcd",
    "start": "1157280",
    "end": "1160320"
  },
  {
    "text": "each pod can only have so many models",
    "start": "1160320",
    "end": "1162320"
  },
  {
    "text": "and those have to be defined so",
    "start": "1162320",
    "end": "1165200"
  },
  {
    "text": "this in this case you'll still run into",
    "start": "1165200",
    "end": "1167360"
  },
  {
    "text": "some limitations if you start running at",
    "start": "1167360",
    "end": "1169039"
  },
  {
    "text": "scale and you have a lot of models",
    "start": "1169039",
    "end": "1170880"
  },
  {
    "text": "um don't worry though because the caser",
    "start": "1170880",
    "end": "1173280"
  },
  {
    "text": "contributors have been working very hard",
    "start": "1173280",
    "end": "1175200"
  },
  {
    "text": "to find an even better solution for a",
    "start": "1175200",
    "end": "1177280"
  },
  {
    "text": "high volume high density use case and we",
    "start": "1177280",
    "end": "1179600"
  },
  {
    "text": "call this multi-model serving with model",
    "start": "1179600",
    "end": "1181679"
  },
  {
    "text": "mesh",
    "start": "1181679",
    "end": "1182799"
  },
  {
    "text": "okay so imagine that you have a large",
    "start": "1182799",
    "end": "1185679"
  },
  {
    "start": "1183000",
    "end": "1294000"
  },
  {
    "text": "number of models deployed which isn't",
    "start": "1185679",
    "end": "1187360"
  },
  {
    "text": "uncommon so an example of this use case",
    "start": "1187360",
    "end": "1189440"
  },
  {
    "text": "would be like a news classification",
    "start": "1189440",
    "end": "1190840"
  },
  {
    "text": "service you may have a model for every",
    "start": "1190840",
    "end": "1193200"
  },
  {
    "text": "type of news category and for privacy",
    "start": "1193200",
    "end": "1195679"
  },
  {
    "text": "and security reasons you might need to",
    "start": "1195679",
    "end": "1197679"
  },
  {
    "text": "train those at the user level so a lot",
    "start": "1197679",
    "end": "1199600"
  },
  {
    "text": "of small running models",
    "start": "1199600",
    "end": "1201679"
  },
  {
    "text": "so using the many models in this use",
    "start": "1201679",
    "end": "1203679"
  },
  {
    "text": "case will have the benefit of a better",
    "start": "1203679",
    "end": "1205520"
  },
  {
    "text": "and more accurate results but you're",
    "start": "1205520",
    "end": "1207520"
  },
  {
    "text": "going to run into a lot of the issues we",
    "start": "1207520",
    "end": "1209120"
  },
  {
    "text": "just discussed with inference services",
    "start": "1209120",
    "end": "1210880"
  },
  {
    "text": "at this scale running on a kubernetes",
    "start": "1210880",
    "end": "1212480"
  },
  {
    "text": "cluster",
    "start": "1212480",
    "end": "1213440"
  },
  {
    "text": "um so another use case say let's let's",
    "start": "1213440",
    "end": "1216159"
  },
  {
    "text": "say like neural network based models are",
    "start": "1216159",
    "end": "1218000"
  },
  {
    "text": "best suited for gpus but gpus are",
    "start": "1218000",
    "end": "1220559"
  },
  {
    "text": "usually expensive and running many of",
    "start": "1220559",
    "end": "1222640"
  },
  {
    "text": "these is costly so using something like",
    "start": "1222640",
    "end": "1224159"
  },
  {
    "text": "what we're going to",
    "start": "1224159",
    "end": "1225360"
  },
  {
    "text": "describe here in model mesh is usually a",
    "start": "1225360",
    "end": "1227120"
  },
  {
    "text": "good option",
    "start": "1227120",
    "end": "1228320"
  },
  {
    "text": "so multi-serving",
    "start": "1228320",
    "end": "1230480"
  },
  {
    "text": "multi-model serving with model mess is a",
    "start": "1230480",
    "end": "1232159"
  },
  {
    "text": "solution this problem and it allows",
    "start": "1232159",
    "end": "1234159"
  },
  {
    "text": "infant services to scale",
    "start": "1234159",
    "end": "1235919"
  },
  {
    "text": "so it decreases the average resource",
    "start": "1235919",
    "end": "1237600"
  },
  {
    "text": "overhead per model um so it becomes more",
    "start": "1237600",
    "end": "1240000"
  },
  {
    "text": "cost efficient and the number of models",
    "start": "1240000",
    "end": "1241760"
  },
  {
    "text": "that can be deployed in a cluster are no",
    "start": "1241760",
    "end": "1243280"
  },
  {
    "text": "longer limited by the maximum power",
    "start": "1243280",
    "end": "1244720"
  },
  {
    "text": "limitation and ip address limitation so",
    "start": "1244720",
    "end": "1247039"
  },
  {
    "text": "what does this do basically you have the",
    "start": "1247039",
    "end": "1249919"
  },
  {
    "text": "mesh which is routing the routing the",
    "start": "1249919",
    "end": "1252080"
  },
  {
    "text": "request you have the puller which is is",
    "start": "1252080",
    "end": "1254480"
  },
  {
    "text": "replaced by or replaces the storage",
    "start": "1254480",
    "end": "1256480"
  },
  {
    "text": "initializer in a way so in storage",
    "start": "1256480",
    "end": "1257919"
  },
  {
    "text": "initializer we had a knit container that",
    "start": "1257919",
    "end": "1259760"
  },
  {
    "text": "came up pulled the model and then left",
    "start": "1259760",
    "end": "1262400"
  },
  {
    "text": "now we have a puller which is a long",
    "start": "1262400",
    "end": "1263760"
  },
  {
    "text": "running side car which is constantly",
    "start": "1263760",
    "end": "1265760"
  },
  {
    "text": "pulling models as needed",
    "start": "1265760",
    "end": "1268960"
  },
  {
    "text": "so that's a very simple use case of",
    "start": "1269280",
    "end": "1270880"
  },
  {
    "text": "course you're probably going to have",
    "start": "1270880",
    "end": "1272080"
  },
  {
    "text": "again like at least one replica and",
    "start": "1272080",
    "end": "1275039"
  },
  {
    "text": "and this just shows that you can have",
    "start": "1275039",
    "end": "1276880"
  },
  {
    "text": "multiple models on different pods",
    "start": "1276880",
    "end": "1279120"
  },
  {
    "text": "running or different replicas of the",
    "start": "1279120",
    "end": "1280720"
  },
  {
    "text": "same serving run time so molar server x",
    "start": "1280720",
    "end": "1283440"
  },
  {
    "text": "and the left plot can have model a b or",
    "start": "1283440",
    "end": "1286159"
  },
  {
    "text": "c and then the right part it can have",
    "start": "1286159",
    "end": "1288320"
  },
  {
    "text": "any other number of models that run on",
    "start": "1288320",
    "end": "1289919"
  },
  {
    "text": "model server x",
    "start": "1289919",
    "end": "1292799"
  },
  {
    "start": "1294000",
    "end": "1414000"
  },
  {
    "text": "okay",
    "start": "1295360",
    "end": "1297120"
  },
  {
    "text": "so now i have",
    "start": "1297120",
    "end": "1300000"
  },
  {
    "text": "a small or a very short demo let's see",
    "start": "1300000",
    "end": "1303039"
  },
  {
    "text": "this is just going to be setting up an",
    "start": "1303039",
    "end": "1304960"
  },
  {
    "text": "inference service this is like this is",
    "start": "1304960",
    "end": "1306640"
  },
  {
    "text": "one of our uh okay this is one of our",
    "start": "1306640",
    "end": "1309039"
  },
  {
    "text": "main",
    "start": "1309039",
    "end": "1311120"
  },
  {
    "text": "examples about how to set up an",
    "start": "1311120",
    "end": "1312400"
  },
  {
    "text": "inference service",
    "start": "1312400",
    "end": "1313679"
  },
  {
    "text": "so yeah i made a i made a namespace",
    "start": "1313679",
    "end": "1315679"
  },
  {
    "text": "called case serve contacts and then i",
    "start": "1315679",
    "end": "1317440"
  },
  {
    "text": "just set up one of the the sk learn",
    "start": "1317440",
    "end": "1320000"
  },
  {
    "text": "yammels for an inference service which",
    "start": "1320000",
    "end": "1321520"
  },
  {
    "text": "is very similar to what i just showed",
    "start": "1321520",
    "end": "1323520"
  },
  {
    "text": "you i already had it applied because i",
    "start": "1323520",
    "end": "1325200"
  },
  {
    "text": "didn't want like any issues or i have to",
    "start": "1325200",
    "end": "1327280"
  },
  {
    "text": "wait for it to come up and i show that",
    "start": "1327280",
    "end": "1328799"
  },
  {
    "text": "it's already running",
    "start": "1328799",
    "end": "1331440"
  },
  {
    "text": "and here i'm just like looking at",
    "start": "1331440",
    "end": "1333039"
  },
  {
    "text": "different resources so this is the",
    "start": "1333039",
    "end": "1334400"
  },
  {
    "text": "inference service resource you can see",
    "start": "1334400",
    "end": "1335919"
  },
  {
    "text": "it's ready true and latest is like 100",
    "start": "1335919",
    "end": "1338080"
  },
  {
    "text": "of the traffic is going there um i'm",
    "start": "1338080",
    "end": "1340320"
  },
  {
    "text": "setting up the ingress gateway because i",
    "start": "1340320",
    "end": "1341679"
  },
  {
    "text": "want to port forward and send",
    "start": "1341679",
    "end": "1343760"
  },
  {
    "text": "send a request to the predictor to get",
    "start": "1343760",
    "end": "1345600"
  },
  {
    "text": "some prediction results",
    "start": "1345600",
    "end": "1347679"
  },
  {
    "text": "okay so i put forward",
    "start": "1347679",
    "end": "1350400"
  },
  {
    "text": "there and i set my ingress port",
    "start": "1350400",
    "end": "1352559"
  },
  {
    "text": "here's my input it's just a very simple",
    "start": "1352559",
    "end": "1354320"
  },
  {
    "text": "input that i'm going to",
    "start": "1354320",
    "end": "1356320"
  },
  {
    "text": "that i'm going to submit into the",
    "start": "1356320",
    "end": "1357919"
  },
  {
    "text": "inference service",
    "start": "1357919",
    "end": "1359520"
  },
  {
    "text": "and now i curl it and you can see under",
    "start": "1359520",
    "end": "1361200"
  },
  {
    "text": "predictions we have an answer",
    "start": "1361200",
    "end": "1363679"
  },
  {
    "text": "okay so what i'm going to show next is",
    "start": "1363679",
    "end": "1366400"
  },
  {
    "text": "a quick example of using a canary",
    "start": "1366400",
    "end": "1368320"
  },
  {
    "text": "rollout which is one of the features of",
    "start": "1368320",
    "end": "1369520"
  },
  {
    "text": "case serve",
    "start": "1369520",
    "end": "1371280"
  },
  {
    "text": "so here i'm going to add a canary",
    "start": "1371280",
    "end": "1373919"
  },
  {
    "text": "traffic percent at 10 that was an",
    "start": "1373919",
    "end": "1376159"
  },
  {
    "text": "annotation onto the yaml and i'm going",
    "start": "1376159",
    "end": "1378159"
  },
  {
    "text": "to apply it",
    "start": "1378159",
    "end": "1380720"
  },
  {
    "text": "so now it's configured so what we should",
    "start": "1380720",
    "end": "1382159"
  },
  {
    "text": "see is that 10 of the traffic is going",
    "start": "1382159",
    "end": "1384000"
  },
  {
    "text": "to route to the new version and 90 will",
    "start": "1384000",
    "end": "1386400"
  },
  {
    "text": "be left in the old version",
    "start": "1386400",
    "end": "1388559"
  },
  {
    "text": "so here i'm checking you can see now",
    "start": "1388559",
    "end": "1391200"
  },
  {
    "text": "it's ready with 90 on the previous",
    "start": "1391200",
    "end": "1393440"
  },
  {
    "text": "version and 10",
    "start": "1393440",
    "end": "1394960"
  },
  {
    "text": "of traffic on the latest version",
    "start": "1394960",
    "end": "1398399"
  },
  {
    "text": "and then if i remove that",
    "start": "1398559",
    "end": "1400720"
  },
  {
    "text": "i can",
    "start": "1400720",
    "end": "1401600"
  },
  {
    "text": "promote this version to okay i can",
    "start": "1401600",
    "end": "1404159"
  },
  {
    "text": "promote this version to 100 traffic",
    "start": "1404159",
    "end": "1408240"
  },
  {
    "text": "and because of time oh yeah okay now",
    "start": "1408240",
    "end": "1410400"
  },
  {
    "text": "it's good yay",
    "start": "1410400",
    "end": "1412400"
  },
  {
    "text": "okay there's a little demo",
    "start": "1412400",
    "end": "1415360"
  },
  {
    "start": "1414000",
    "end": "1458000"
  },
  {
    "text": "and really quickly there are a lot of",
    "start": "1415360",
    "end": "1416799"
  },
  {
    "text": "cool new features coming out i'm just",
    "start": "1416799",
    "end": "1418480"
  },
  {
    "text": "going to leave this up because we don't",
    "start": "1418480",
    "end": "1419440"
  },
  {
    "text": "have a lot of time but be sure to go",
    "start": "1419440",
    "end": "1421120"
  },
  {
    "text": "look at on the website and you can look",
    "start": "1421120",
    "end": "1423200"
  },
  {
    "text": "at all these features and also on case",
    "start": "1423200",
    "end": "1425120"
  },
  {
    "text": "serve they have them listed as well",
    "start": "1425120",
    "end": "1427279"
  },
  {
    "text": "congrats to everyone who's worked on",
    "start": "1427279",
    "end": "1428480"
  },
  {
    "text": "this i know a lot of people have put a",
    "start": "1428480",
    "end": "1429919"
  },
  {
    "text": "lot of hard work into this and thanks to",
    "start": "1429919",
    "end": "1431919"
  },
  {
    "text": "the k-serve contributors um it's a great",
    "start": "1431919",
    "end": "1434320"
  },
  {
    "text": "community to be a part of i'm so glad",
    "start": "1434320",
    "end": "1435760"
  },
  {
    "text": "that i joined it and i'm having a great",
    "start": "1435760",
    "end": "1437840"
  },
  {
    "text": "time so yeah",
    "start": "1437840",
    "end": "1440320"
  },
  {
    "text": "although also here are some other talks",
    "start": "1440320",
    "end": "1442240"
  },
  {
    "text": "that bloomberg is going to be involved",
    "start": "1442240",
    "end": "1444400"
  },
  {
    "text": "in and here's a career page linked if",
    "start": "1444400",
    "end": "1446799"
  },
  {
    "text": "you're interested",
    "start": "1446799",
    "end": "1448559"
  },
  {
    "text": "so thank you for coming on this journey",
    "start": "1448559",
    "end": "1450080"
  },
  {
    "text": "with me to learn about k-serve",
    "start": "1450080",
    "end": "1452750"
  },
  {
    "text": "[Applause]",
    "start": "1452750",
    "end": "1456230"
  },
  {
    "start": "1458000",
    "end": "1687000"
  },
  {
    "text": "now uh do we have any questions for",
    "start": "1458720",
    "end": "1460799"
  },
  {
    "text": "alexa",
    "start": "1460799",
    "end": "1462320"
  },
  {
    "text": "i want over here",
    "start": "1462320",
    "end": "1463760"
  },
  {
    "text": "let's do this take the microphone over",
    "start": "1463760",
    "end": "1468039"
  },
  {
    "text": "yeah",
    "start": "1472799",
    "end": "1474080"
  },
  {
    "text": "what do you use ai for in bloomberg",
    "start": "1474080",
    "end": "1476960"
  },
  {
    "text": "is it for predicting prices of assets",
    "start": "1476960",
    "end": "1479360"
  },
  {
    "text": "analyzing news maybe or something hot",
    "start": "1479360",
    "end": "1482000"
  },
  {
    "text": "like this",
    "start": "1482000",
    "end": "1484480"
  },
  {
    "text": "um yeah so the question was like what do",
    "start": "1485039",
    "end": "1487840"
  },
  {
    "text": "we use uh ai for at bloomberg so",
    "start": "1487840",
    "end": "1490320"
  },
  {
    "text": "bloomberg is uh is a financial services",
    "start": "1490320",
    "end": "1492720"
  },
  {
    "text": "company and we use it for a lot of",
    "start": "1492720",
    "end": "1494320"
  },
  {
    "text": "things like news news is one of uh one",
    "start": "1494320",
    "end": "1496640"
  },
  {
    "text": "use case for it yeah a lot of financial",
    "start": "1496640",
    "end": "1499679"
  },
  {
    "text": "models",
    "start": "1499679",
    "end": "1502080"
  },
  {
    "text": "can you please talk about the",
    "start": "1505440",
    "end": "1506799"
  },
  {
    "text": "differences between seldom core and",
    "start": "1506799",
    "end": "1508640"
  },
  {
    "text": "using kf serving or k-serve in its new",
    "start": "1508640",
    "end": "1511760"
  },
  {
    "text": "name",
    "start": "1511760",
    "end": "1513039"
  },
  {
    "text": "between can you seldom core it uh",
    "start": "1513039",
    "end": "1516240"
  },
  {
    "text": "you had it in the you know salon core",
    "start": "1516240",
    "end": "1518799"
  },
  {
    "text": "it's also a possibility to run",
    "start": "1518799",
    "end": "1520960"
  },
  {
    "text": "deployments and ai models using",
    "start": "1520960",
    "end": "1522720"
  },
  {
    "text": "kubernetes api",
    "start": "1522720",
    "end": "1524480"
  },
  {
    "text": "okay yeah um so the differences are",
    "start": "1524480",
    "end": "1527760"
  },
  {
    "text": "the question was what's the difference",
    "start": "1527760",
    "end": "1529200"
  },
  {
    "text": "in um southern core",
    "start": "1529200",
    "end": "1531039"
  },
  {
    "text": "seldom seldom core okay sorry southern",
    "start": "1531039",
    "end": "1533279"
  },
  {
    "text": "core and k-serve um that's a good",
    "start": "1533279",
    "end": "1535440"
  },
  {
    "text": "question",
    "start": "1535440",
    "end": "1536320"
  },
  {
    "text": "why don't we take it offline so i can",
    "start": "1536320",
    "end": "1538159"
  },
  {
    "text": "make sure i can give you a good answer",
    "start": "1538159",
    "end": "1542039"
  },
  {
    "text": "hi thanks for the talk by the way um the",
    "start": "1542799",
    "end": "1545520"
  },
  {
    "text": "i have a question about multi model uh",
    "start": "1545520",
    "end": "1547840"
  },
  {
    "text": "use case that you talk about the mesh um",
    "start": "1547840",
    "end": "1550559"
  },
  {
    "text": "so",
    "start": "1550559",
    "end": "1551440"
  },
  {
    "text": "um what uh in the in this case whenever",
    "start": "1551440",
    "end": "1553840"
  },
  {
    "text": "a request comes in",
    "start": "1553840",
    "end": "1555200"
  },
  {
    "text": "is it at run time the model is pulled or",
    "start": "1555200",
    "end": "1558080"
  },
  {
    "text": "um like just at the moment when the",
    "start": "1558080",
    "end": "1559760"
  },
  {
    "text": "request comes in",
    "start": "1559760",
    "end": "1561520"
  },
  {
    "text": "or how does it work because i see that",
    "start": "1561520",
    "end": "1563760"
  },
  {
    "text": "okay that",
    "start": "1563760",
    "end": "1565120"
  },
  {
    "text": "there are models are you said that the",
    "start": "1565120",
    "end": "1566799"
  },
  {
    "text": "initializer is replaced with something",
    "start": "1566799",
    "end": "1568720"
  },
  {
    "text": "called puller or something like that",
    "start": "1568720",
    "end": "1570480"
  },
  {
    "text": "does that happen when the port comes up",
    "start": "1570480",
    "end": "1571919"
  },
  {
    "text": "or when the request api request comes in",
    "start": "1571919",
    "end": "1574320"
  },
  {
    "text": "for prediction",
    "start": "1574320",
    "end": "1575679"
  },
  {
    "text": "yeah so in model mesh um the question",
    "start": "1575679",
    "end": "1578240"
  },
  {
    "text": "was about the differences in the storage",
    "start": "1578240",
    "end": "1580240"
  },
  {
    "text": "initializer and the puller so and the",
    "start": "1580240",
    "end": "1582480"
  },
  {
    "text": "model uh and the model mesh the storage",
    "start": "1582480",
    "end": "1584720"
  },
  {
    "text": "initializer is replaced by the puller",
    "start": "1584720",
    "end": "1587440"
  },
  {
    "text": "which is a long running sidecar so it's",
    "start": "1587440",
    "end": "1589600"
  },
  {
    "text": "able to pull models as needed as you're",
    "start": "1589600",
    "end": "1592960"
  },
  {
    "text": "as you're hitting that that model um",
    "start": "1592960",
    "end": "1596240"
  },
  {
    "text": "yeah that's",
    "start": "1596240",
    "end": "1598799"
  },
  {
    "text": "hello so you talked about the",
    "start": "1598799",
    "end": "1600799"
  },
  {
    "text": "transformers and i was wondering uh",
    "start": "1600799",
    "end": "1603120"
  },
  {
    "text": "something i had been recently exploring",
    "start": "1603120",
    "end": "1604799"
  },
  {
    "text": "was adding the pre-processing steps to",
    "start": "1604799",
    "end": "1607200"
  },
  {
    "text": "the graph itself",
    "start": "1607200",
    "end": "1608640"
  },
  {
    "text": "so",
    "start": "1608640",
    "end": "1609440"
  },
  {
    "text": "is that something possible with uh kiso",
    "start": "1609440",
    "end": "1613760"
  },
  {
    "text": "i'm sorry because you're adding the",
    "start": "1613760",
    "end": "1614720"
  },
  {
    "text": "pre-processing pre-processing steps to",
    "start": "1614720",
    "end": "1617039"
  },
  {
    "text": "what adding the pre-processing steps to",
    "start": "1617039",
    "end": "1619360"
  },
  {
    "text": "the model itself something like",
    "start": "1619360",
    "end": "1620720"
  },
  {
    "text": "tensorflow transform",
    "start": "1620720",
    "end": "1622480"
  },
  {
    "text": "uh is that possible with kso",
    "start": "1622480",
    "end": "1624960"
  },
  {
    "text": "could you should i be right on that",
    "start": "1624960",
    "end": "1627520"
  },
  {
    "text": "yeah so k serve allows for you to do pre",
    "start": "1627520",
    "end": "1629760"
  },
  {
    "text": "and post processing in the transformer",
    "start": "1629760",
    "end": "1632080"
  },
  {
    "text": "you can use some of the things that come",
    "start": "1632080",
    "end": "1633440"
  },
  {
    "text": "out of the box or you can create your",
    "start": "1633440",
    "end": "1634720"
  },
  {
    "text": "own custom one so there are some options",
    "start": "1634720",
    "end": "1636640"
  },
  {
    "text": "there as to what you can use for the",
    "start": "1636640",
    "end": "1638240"
  },
  {
    "text": "preprocessor",
    "start": "1638240",
    "end": "1641039"
  },
  {
    "text": "um thank you for the talk i see you",
    "start": "1641039",
    "end": "1642960"
  },
  {
    "text": "talked about scale to zero yeah um do",
    "start": "1642960",
    "end": "1645279"
  },
  {
    "text": "you have to use that like what if you",
    "start": "1645279",
    "end": "1646559"
  },
  {
    "text": "have an application that you know you",
    "start": "1646559",
    "end": "1648640"
  },
  {
    "text": "all want to have you know always up or",
    "start": "1648640",
    "end": "1650240"
  },
  {
    "text": "it's like mission critical like",
    "start": "1650240",
    "end": "1651840"
  },
  {
    "text": "is that configurable or do you have to",
    "start": "1651840",
    "end": "1653520"
  },
  {
    "text": "do that yeah definitely so scale to zero",
    "start": "1653520",
    "end": "1655279"
  },
  {
    "text": "is something you can configure sometimes",
    "start": "1655279",
    "end": "1657600"
  },
  {
    "text": "you want it to save resources but for",
    "start": "1657600",
    "end": "1659520"
  },
  {
    "text": "other mission critical things you may",
    "start": "1659520",
    "end": "1661039"
  },
  {
    "text": "not ever or things that can't handle the",
    "start": "1661039",
    "end": "1663840"
  },
  {
    "text": "latency of a pod starting up when it's",
    "start": "1663840",
    "end": "1666320"
  },
  {
    "text": "been down but for those use cases you",
    "start": "1666320",
    "end": "1668159"
  },
  {
    "text": "wouldn't want to use scala zero or",
    "start": "1668159",
    "end": "1669440"
  },
  {
    "text": "something that um used to be up all the",
    "start": "1669440",
    "end": "1671200"
  },
  {
    "text": "time",
    "start": "1671200",
    "end": "1672080"
  },
  {
    "text": "yeah",
    "start": "1672080",
    "end": "1674320"
  },
  {
    "text": "who did they give the microphone to",
    "start": "1674480",
    "end": "1678080"
  },
  {
    "text": "i'll have that back",
    "start": "1678080",
    "end": "1680080"
  },
  {
    "text": "i don't i'm probably have any more type",
    "start": "1680080",
    "end": "1681440"
  },
  {
    "text": "of questions thank you very much alexa",
    "start": "1681440",
    "end": "1682880"
  },
  {
    "text": "give her a big round of applause",
    "start": "1682880",
    "end": "1685360"
  },
  {
    "text": "thank you for coming my talk",
    "start": "1685360",
    "end": "1688880"
  }
]