[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "all right hello everyone uh welcome to",
    "start": "480",
    "end": "2320"
  },
  {
    "text": "the rook intro and theft deep dive talk",
    "start": "2320",
    "end": "4640"
  },
  {
    "text": "uh for kubecon north america uh",
    "start": "4640",
    "end": "8080"
  },
  {
    "text": "here we are virtual again happy to be",
    "start": "8080",
    "end": "10080"
  },
  {
    "text": "with you i am travis nielsen uh one of",
    "start": "10080",
    "end": "12639"
  },
  {
    "text": "the maintainers",
    "start": "12639",
    "end": "13599"
  },
  {
    "text": "on the rook project since it's beginning",
    "start": "13599",
    "end": "16960"
  },
  {
    "text": "i work for red hat and happy to be here",
    "start": "16960",
    "end": "19680"
  },
  {
    "text": "with sebastian",
    "start": "19680",
    "end": "21760"
  },
  {
    "text": "hello virtual kubecon i'm sebastian i",
    "start": "21760",
    "end": "24480"
  },
  {
    "text": "work with travis as one of the rook",
    "start": "24480",
    "end": "26160"
  },
  {
    "text": "maintainers",
    "start": "26160",
    "end": "27119"
  },
  {
    "text": "and also work for red hat thanks and",
    "start": "27119",
    "end": "30080"
  },
  {
    "text": "also",
    "start": "30080",
    "end": "31039"
  },
  {
    "text": "to answer questions we'll have blaine",
    "start": "31039",
    "end": "32558"
  },
  {
    "text": "gardner and alexander trust",
    "start": "32559",
    "end": "35040"
  },
  {
    "text": "other rook maintainers all right first",
    "start": "35040",
    "end": "37040"
  },
  {
    "start": "37000",
    "end": "37000"
  },
  {
    "text": "big announcement rook is graduated",
    "start": "37040",
    "end": "39200"
  },
  {
    "text": "we are very excited about this",
    "start": "39200",
    "end": "42239"
  },
  {
    "text": "we've been working hard on this for the",
    "start": "42239",
    "end": "43680"
  },
  {
    "text": "last few years",
    "start": "43680",
    "end": "45440"
  },
  {
    "text": "just a little history here so rook did",
    "start": "45440",
    "end": "48480"
  },
  {
    "text": "get initially uh inducted as a sandbox",
    "start": "48480",
    "end": "51920"
  },
  {
    "text": "project",
    "start": "51920",
    "end": "52640"
  },
  {
    "text": "back almost three years ago in january",
    "start": "52640",
    "end": "54399"
  },
  {
    "text": "2018",
    "start": "54399",
    "end": "55920"
  },
  {
    "text": "we progressed to incubation in september",
    "start": "55920",
    "end": "57840"
  },
  {
    "text": "of that same year and then graduated",
    "start": "57840",
    "end": "60399"
  },
  {
    "text": "now in october uh just",
    "start": "60399",
    "end": "64080"
  },
  {
    "text": "just this last month so thank you a big",
    "start": "64080",
    "end": "66240"
  },
  {
    "text": "thank you to the rook community",
    "start": "66240",
    "end": "68000"
  },
  {
    "text": "cncf committee rook maintainers and",
    "start": "68000",
    "end": "70240"
  },
  {
    "text": "everyone involved",
    "start": "70240",
    "end": "71439"
  },
  {
    "text": "it has been a wonderful journey and",
    "start": "71439",
    "end": "73840"
  },
  {
    "text": "we're grateful your support",
    "start": "73840",
    "end": "75119"
  },
  {
    "text": "for your support and look forward to",
    "start": "75119",
    "end": "77520"
  },
  {
    "text": "continued",
    "start": "77520",
    "end": "78560"
  },
  {
    "text": "growth in the project with that support",
    "start": "78560",
    "end": "82240"
  },
  {
    "text": "so what does this mean to get to",
    "start": "82240",
    "end": "84840"
  },
  {
    "text": "graduated",
    "start": "84840",
    "end": "86080"
  },
  {
    "text": "um it really means that we have a lot of",
    "start": "86080",
    "end": "88240"
  },
  {
    "text": "large-scale production",
    "start": "88240",
    "end": "89680"
  },
  {
    "text": "deployments out there we know people are",
    "start": "89680",
    "end": "92560"
  },
  {
    "text": "using it",
    "start": "92560",
    "end": "94079"
  },
  {
    "text": "to store their their critical data in",
    "start": "94079",
    "end": "96479"
  },
  {
    "text": "production environments",
    "start": "96479",
    "end": "98079"
  },
  {
    "text": "and this is that's a great um evidence",
    "start": "98079",
    "end": "101119"
  },
  {
    "text": "of that",
    "start": "101119",
    "end": "102079"
  },
  {
    "text": "we went through a security review by the",
    "start": "102079",
    "end": "103759"
  },
  {
    "text": "third party",
    "start": "103759",
    "end": "105360"
  },
  {
    "text": "that cncf helped us coordinate and",
    "start": "105360",
    "end": "109040"
  },
  {
    "text": "maybe the third point is that really",
    "start": "109040",
    "end": "111680"
  },
  {
    "text": "brooke is an open project",
    "start": "111680",
    "end": "113439"
  },
  {
    "text": "it's it's not only open source but",
    "start": "113439",
    "end": "116719"
  },
  {
    "text": "the governance around the project says",
    "start": "116719",
    "end": "118479"
  },
  {
    "text": "that hey this is this is vendor neutral",
    "start": "118479",
    "end": "120719"
  },
  {
    "text": "we have multiple companies contributing",
    "start": "120719",
    "end": "122719"
  },
  {
    "text": "and expected to contribute so that it",
    "start": "122719",
    "end": "125600"
  },
  {
    "text": "stays",
    "start": "125600",
    "end": "126399"
  },
  {
    "text": "remains a project that is useful to the",
    "start": "126399",
    "end": "129039"
  },
  {
    "text": "whole community",
    "start": "129039",
    "end": "130160"
  },
  {
    "text": "and and not vendor driven so",
    "start": "130160",
    "end": "134319"
  },
  {
    "text": "again thanks to for all your support uh",
    "start": "134319",
    "end": "136560"
  },
  {
    "text": "we've had a great journey to get here",
    "start": "136560",
    "end": "138080"
  },
  {
    "text": "all right let's get started on",
    "start": "138080",
    "end": "139599"
  },
  {
    "text": "you know what what are we talking about",
    "start": "139599",
    "end": "141360"
  },
  {
    "text": "what is rook what does it address",
    "start": "141360",
    "end": "143360"
  },
  {
    "text": "what what are issues with storage and",
    "start": "143360",
    "end": "146319"
  },
  {
    "text": "kubernetes",
    "start": "146319",
    "end": "148879"
  },
  {
    "start": "147000",
    "end": "147000"
  },
  {
    "text": "well kubernetes is traditionally a",
    "start": "148879",
    "end": "150959"
  },
  {
    "text": "platform",
    "start": "150959",
    "end": "151920"
  },
  {
    "text": "of course to manage distributed apps and",
    "start": "151920",
    "end": "155040"
  },
  {
    "text": "those apps",
    "start": "155040",
    "end": "155760"
  },
  {
    "text": "have been stateless for the most part if",
    "start": "155760",
    "end": "158640"
  },
  {
    "text": "you want storage",
    "start": "158640",
    "end": "159680"
  },
  {
    "text": "you rely on x storage external to the",
    "start": "159680",
    "end": "162160"
  },
  {
    "text": "cluster",
    "start": "162160",
    "end": "163440"
  },
  {
    "text": "storage that's not portable you have to",
    "start": "163440",
    "end": "165360"
  },
  {
    "text": "figure out how to deploy it",
    "start": "165360",
    "end": "168319"
  },
  {
    "text": "you have maybe you have a reliance on a",
    "start": "168319",
    "end": "170400"
  },
  {
    "text": "cloud provider to",
    "start": "170400",
    "end": "171760"
  },
  {
    "text": "to give you that managed storage uh",
    "start": "171760",
    "end": "175120"
  },
  {
    "text": "and quickly you get into a vendor",
    "start": "175120",
    "end": "176640"
  },
  {
    "text": "lock-in if you're using",
    "start": "176640",
    "end": "178239"
  },
  {
    "text": "cloud storage if you're managing that",
    "start": "178239",
    "end": "181360"
  },
  {
    "text": "storage",
    "start": "181360",
    "end": "182400"
  },
  {
    "text": "okay you have to you know day two",
    "start": "182400",
    "end": "184560"
  },
  {
    "text": "operations at once you've set it up",
    "start": "184560",
    "end": "186480"
  },
  {
    "text": "you've got to upgrade it you've got to",
    "start": "186480",
    "end": "187840"
  },
  {
    "text": "manage it",
    "start": "187840",
    "end": "189120"
  },
  {
    "text": "make sure it stays reliable so all of",
    "start": "189120",
    "end": "192480"
  },
  {
    "text": "those",
    "start": "192480",
    "end": "193040"
  },
  {
    "text": "are challenges that that everyone has",
    "start": "193040",
    "end": "196239"
  },
  {
    "text": "with any stateful application in",
    "start": "196239",
    "end": "197920"
  },
  {
    "text": "kubernetes so here's where rook comes in",
    "start": "197920",
    "end": "200239"
  },
  {
    "text": "we saw this as a real challenge with",
    "start": "200239",
    "end": "201599"
  },
  {
    "text": "kubernetes and we wanted to",
    "start": "201599",
    "end": "203280"
  },
  {
    "text": "address it and kind of solve this",
    "start": "203280",
    "end": "205440"
  },
  {
    "text": "problem",
    "start": "205440",
    "end": "206400"
  },
  {
    "text": "for the community so rook really does",
    "start": "206400",
    "end": "209440"
  },
  {
    "start": "207000",
    "end": "207000"
  },
  {
    "text": "have the goal of making storage",
    "start": "209440",
    "end": "210879"
  },
  {
    "text": "available inside your kubernetes cluster",
    "start": "210879",
    "end": "214080"
  },
  {
    "text": "it's not something external it's",
    "start": "214080",
    "end": "216000"
  },
  {
    "text": "something that's",
    "start": "216000",
    "end": "217680"
  },
  {
    "text": "deployed as any other application in",
    "start": "217680",
    "end": "220560"
  },
  {
    "text": "your kubernetes cluster",
    "start": "220560",
    "end": "222640"
  },
  {
    "text": "it's managed with kubernetes operators",
    "start": "222640",
    "end": "224640"
  },
  {
    "text": "and crds",
    "start": "224640",
    "end": "226400"
  },
  {
    "text": "just like any other application",
    "start": "226400",
    "end": "229840"
  },
  {
    "text": "and because it's driven with operators",
    "start": "229840",
    "end": "234159"
  },
  {
    "text": "the deployment of of the storage",
    "start": "234159",
    "end": "236720"
  },
  {
    "text": "platform",
    "start": "236720",
    "end": "237280"
  },
  {
    "text": "is automated so deploying it configuring",
    "start": "237280",
    "end": "239840"
  },
  {
    "text": "it",
    "start": "239840",
    "end": "240560"
  },
  {
    "text": "upgrading it you you get to decide how",
    "start": "240560",
    "end": "243200"
  },
  {
    "text": "it's deployed",
    "start": "243200",
    "end": "244239"
  },
  {
    "text": "uh through the crds and other settings",
    "start": "244239",
    "end": "246799"
  },
  {
    "text": "but",
    "start": "246799",
    "end": "247760"
  },
  {
    "text": "ultimately rook manages all of the",
    "start": "247760",
    "end": "249840"
  },
  {
    "text": "headaches of",
    "start": "249840",
    "end": "251519"
  },
  {
    "text": "of that day two management well day one",
    "start": "251519",
    "end": "253599"
  },
  {
    "text": "as well to get it installed",
    "start": "253599",
    "end": "255280"
  },
  {
    "text": "and once you have deployed rook now you",
    "start": "255280",
    "end": "257919"
  },
  {
    "text": "can consume",
    "start": "257919",
    "end": "258880"
  },
  {
    "text": "the storage just like any other",
    "start": "258880",
    "end": "261359"
  },
  {
    "text": "kubernetes",
    "start": "261359",
    "end": "262720"
  },
  {
    "text": "storage with storage classes with",
    "start": "262720",
    "end": "264800"
  },
  {
    "text": "persistent volume claims",
    "start": "264800",
    "end": "266240"
  },
  {
    "text": "so bring the storage platform inside",
    "start": "266240",
    "end": "268960"
  },
  {
    "text": "kubernetes is the point",
    "start": "268960",
    "end": "271280"
  },
  {
    "text": "uh rook is open source with an apache",
    "start": "271280",
    "end": "273199"
  },
  {
    "text": "200 license",
    "start": "273199",
    "end": "275199"
  },
  {
    "text": "so it is so what storage are we talking",
    "start": "275199",
    "end": "277680"
  },
  {
    "start": "277000",
    "end": "277000"
  },
  {
    "text": "about here",
    "start": "277680",
    "end": "278560"
  },
  {
    "text": "uh we have several different storage",
    "start": "278560",
    "end": "280240"
  },
  {
    "text": "providers in rook",
    "start": "280240",
    "end": "281600"
  },
  {
    "text": "at different levels of progression so",
    "start": "281600",
    "end": "284880"
  },
  {
    "text": "seth",
    "start": "284880",
    "end": "285360"
  },
  {
    "text": "is our stable storage provider it's been",
    "start": "285360",
    "end": "288320"
  },
  {
    "text": "in since the",
    "start": "288320",
    "end": "289280"
  },
  {
    "text": "start of the project since then we've",
    "start": "289280",
    "end": "291040"
  },
  {
    "text": "added other",
    "start": "291040",
    "end": "292560"
  },
  {
    "text": "storage providers as well to expand you",
    "start": "292560",
    "end": "295120"
  },
  {
    "text": "know the the storage",
    "start": "295120",
    "end": "296880"
  },
  {
    "text": "platform nfs cassandra uydb cockroachdb",
    "start": "296880",
    "end": "301280"
  },
  {
    "text": "these are still in alpha and uh we're",
    "start": "301280",
    "end": "303520"
  },
  {
    "text": "always looking for community involvement",
    "start": "303520",
    "end": "305039"
  },
  {
    "text": "to help",
    "start": "305039",
    "end": "305840"
  },
  {
    "text": "grow those uh we do have one",
    "start": "305840",
    "end": "309600"
  },
  {
    "text": "storage provider that is deprecated so",
    "start": "309600",
    "end": "310960"
  },
  {
    "text": "edge of s the",
    "start": "310960",
    "end": "313039"
  },
  {
    "text": "the owners of edge of fest are working",
    "start": "313039",
    "end": "314479"
  },
  {
    "text": "on a replacement for it",
    "start": "314479",
    "end": "316880"
  },
  {
    "text": "um the timeline of that i i'm not aware",
    "start": "316880",
    "end": "319440"
  },
  {
    "text": "of but",
    "start": "319440",
    "end": "320240"
  },
  {
    "text": "that's why it's deprecated since",
    "start": "320240",
    "end": "321919"
  },
  {
    "text": "something new will be coming",
    "start": "321919",
    "end": "323280"
  },
  {
    "text": "all right so kind of overall project you",
    "start": "323280",
    "end": "326240"
  },
  {
    "text": "know",
    "start": "326240",
    "end": "326720"
  },
  {
    "text": "how big is rook where is it so version",
    "start": "326720",
    "end": "329440"
  },
  {
    "text": "1.5",
    "start": "329440",
    "end": "330320"
  },
  {
    "text": "is our latest release at least it will",
    "start": "330320",
    "end": "333039"
  },
  {
    "text": "be as of",
    "start": "333039",
    "end": "334240"
  },
  {
    "text": "tubecon november 2020 we will have that",
    "start": "334240",
    "end": "338160"
  },
  {
    "text": "release out",
    "start": "338160",
    "end": "340080"
  },
  {
    "text": "you know as far as how popular project",
    "start": "340080",
    "end": "342240"
  },
  {
    "text": "is you know",
    "start": "342240",
    "end": "343280"
  },
  {
    "text": "all the github stars downloads going on",
    "start": "343280",
    "end": "345840"
  },
  {
    "text": "200 million downloads now",
    "start": "345840",
    "end": "347759"
  },
  {
    "text": "for our containers uh",
    "start": "347759",
    "end": "350800"
  },
  {
    "text": "about 300 contributors now on on the",
    "start": "350800",
    "end": "352880"
  },
  {
    "text": "github project",
    "start": "352880",
    "end": "354720"
  },
  {
    "text": "and as mentioned we are a graduated",
    "start": "354720",
    "end": "356880"
  },
  {
    "text": "project",
    "start": "356880",
    "end": "357919"
  },
  {
    "text": "so thanks again to the the community",
    "start": "357919",
    "end": "360639"
  },
  {
    "text": "these are just a few of the stats that",
    "start": "360639",
    "end": "362080"
  },
  {
    "text": "show",
    "start": "362080",
    "end": "362800"
  },
  {
    "text": "how how much it is growing okay so rook",
    "start": "362800",
    "end": "367039"
  },
  {
    "text": "and sef if we're getting into the ceph",
    "start": "367039",
    "end": "369039"
  },
  {
    "text": "portion of this",
    "start": "369039",
    "end": "370400"
  },
  {
    "text": "what does that mean to bring ceph into",
    "start": "370400",
    "end": "372960"
  },
  {
    "text": "kubernetes",
    "start": "372960",
    "end": "374000"
  },
  {
    "text": "that's what rick will do rook will bring",
    "start": "374000",
    "end": "376080"
  },
  {
    "text": "the ceph storage layer into kubernetes",
    "start": "376080",
    "end": "378479"
  },
  {
    "text": "and make it work well so what is steph",
    "start": "378479",
    "end": "380960"
  },
  {
    "start": "380000",
    "end": "380000"
  },
  {
    "text": "if you're not familiar with it seth",
    "start": "380960",
    "end": "382479"
  },
  {
    "text": "is another open source project but it is",
    "start": "382479",
    "end": "385360"
  },
  {
    "text": "a",
    "start": "385360",
    "end": "386080"
  },
  {
    "text": "software defined storage solution it",
    "start": "386080",
    "end": "388400"
  },
  {
    "text": "provides",
    "start": "388400",
    "end": "389199"
  },
  {
    "text": "block shared file system and object",
    "start": "389199",
    "end": "392560"
  },
  {
    "text": "storage that is s3 compliant so these",
    "start": "392560",
    "end": "395680"
  },
  {
    "text": "are the",
    "start": "395680",
    "end": "396240"
  },
  {
    "text": "really the three building blocks of any",
    "start": "396240",
    "end": "399039"
  },
  {
    "text": "storage system",
    "start": "399039",
    "end": "400080"
  },
  {
    "text": "that you'll find out there and ceph has",
    "start": "400080",
    "end": "401840"
  },
  {
    "text": "them all in one",
    "start": "401840",
    "end": "403600"
  },
  {
    "text": "in one system no need for deploying one",
    "start": "403600",
    "end": "407039"
  },
  {
    "text": "solution for block and another one",
    "start": "407039",
    "end": "408639"
  },
  {
    "text": "for object or file system the ceph",
    "start": "408639",
    "end": "411840"
  },
  {
    "text": "provides all of those",
    "start": "411840",
    "end": "413599"
  },
  {
    "text": "types of storage together so early on",
    "start": "413599",
    "end": "416319"
  },
  {
    "text": "the project we really",
    "start": "416319",
    "end": "417440"
  },
  {
    "text": "saw ceph as being a reliable platform",
    "start": "417440",
    "end": "420080"
  },
  {
    "text": "it's been around for a long time",
    "start": "420080",
    "end": "421680"
  },
  {
    "text": "we just wanted to bring it to kubernetes",
    "start": "421680",
    "end": "423520"
  },
  {
    "text": "so how did we bring it to kubernetes",
    "start": "423520",
    "end": "425120"
  },
  {
    "start": "424000",
    "end": "424000"
  },
  {
    "text": "um think of it as having three different",
    "start": "425120",
    "end": "428319"
  },
  {
    "text": "layers in the system where rook",
    "start": "428319",
    "end": "431360"
  },
  {
    "text": "is really the operator in kubernetes",
    "start": "431360",
    "end": "434800"
  },
  {
    "text": "that owns the management of ceph",
    "start": "434800",
    "end": "436960"
  },
  {
    "text": "so the deployment of it the upgrading of",
    "start": "436960",
    "end": "439199"
  },
  {
    "text": "stuff like rook",
    "start": "439199",
    "end": "440720"
  },
  {
    "text": "manages the configuration of stuff",
    "start": "440720",
    "end": "444160"
  },
  {
    "text": "if you're familiar with staff you know",
    "start": "444160",
    "end": "445599"
  },
  {
    "text": "that it is kind of",
    "start": "445599",
    "end": "447680"
  },
  {
    "text": "a complicated system to deploy",
    "start": "447680",
    "end": "451199"
  },
  {
    "text": "as distributed storage so rook takes a",
    "start": "451199",
    "end": "453840"
  },
  {
    "text": "lot of that",
    "start": "453840",
    "end": "455120"
  },
  {
    "text": "complexity out of it and manages it for",
    "start": "455120",
    "end": "457199"
  },
  {
    "text": "you second layer now we've got",
    "start": "457199",
    "end": "459280"
  },
  {
    "text": "the ceph csi driver so this csi driver",
    "start": "459280",
    "end": "463039"
  },
  {
    "text": "just like any other storage platform",
    "start": "463039",
    "end": "464800"
  },
  {
    "text": "kubernetes will provision",
    "start": "464800",
    "end": "466639"
  },
  {
    "text": "and then mount the storage into your app",
    "start": "466639",
    "end": "469120"
  },
  {
    "text": "pod",
    "start": "469120",
    "end": "470080"
  },
  {
    "text": "so this is how you consume this the ceph",
    "start": "470080",
    "end": "472879"
  },
  {
    "text": "storage",
    "start": "472879",
    "end": "473599"
  },
  {
    "text": "once you have rook installed and then",
    "start": "473599",
    "end": "475360"
  },
  {
    "text": "finally the third layer",
    "start": "475360",
    "end": "477280"
  },
  {
    "text": "is the data layer so when you are",
    "start": "477280",
    "end": "479360"
  },
  {
    "text": "reading and writing data to the cluster",
    "start": "479360",
    "end": "481919"
  },
  {
    "text": "ceph is purely the one that is",
    "start": "481919",
    "end": "485199"
  },
  {
    "text": "responding to that that layer and",
    "start": "485199",
    "end": "488240"
  },
  {
    "text": "there is no rook management code in the",
    "start": "488240",
    "end": "491280"
  },
  {
    "text": "data path",
    "start": "491280",
    "end": "492240"
  },
  {
    "text": "it's just ceph at the data layer so you",
    "start": "492240",
    "end": "494560"
  },
  {
    "text": "really get",
    "start": "494560",
    "end": "496240"
  },
  {
    "text": "performance high performance storage",
    "start": "496240",
    "end": "499440"
  },
  {
    "text": "at that layer all right so let's look at",
    "start": "499440",
    "end": "501120"
  },
  {
    "start": "501000",
    "end": "501000"
  },
  {
    "text": "some pictures of what these layers look",
    "start": "501120",
    "end": "502400"
  },
  {
    "text": "like",
    "start": "502400",
    "end": "502960"
  },
  {
    "text": "so for the first layer with rook so this",
    "start": "502960",
    "end": "505599"
  },
  {
    "text": "this is a picture trying to show that",
    "start": "505599",
    "end": "507440"
  },
  {
    "text": "okay we've got",
    "start": "507440",
    "end": "508240"
  },
  {
    "text": "a single cluster with three nodes so",
    "start": "508240",
    "end": "510400"
  },
  {
    "text": "each of these black boxes",
    "start": "510400",
    "end": "511599"
  },
  {
    "text": "is is a node in your kubernetes cluster",
    "start": "511599",
    "end": "514800"
  },
  {
    "text": "and each of these nodes is running pods",
    "start": "514800",
    "end": "517440"
  },
  {
    "text": "that run different types of demons",
    "start": "517440",
    "end": "519360"
  },
  {
    "text": "so in the center center node here we've",
    "start": "519360",
    "end": "522399"
  },
  {
    "text": "got the rook",
    "start": "522399",
    "end": "523039"
  },
  {
    "text": "operator pod and of course that",
    "start": "523039",
    "end": "526160"
  },
  {
    "text": "that's the management layer that's going",
    "start": "526160",
    "end": "528000"
  },
  {
    "text": "to manage everything else",
    "start": "528000",
    "end": "529760"
  },
  {
    "text": "so this operator pod is is going to",
    "start": "529760",
    "end": "533279"
  },
  {
    "text": "deploy ceph and and the csi driver",
    "start": "533279",
    "end": "537120"
  },
  {
    "text": "depending on what settings and how",
    "start": "537120",
    "end": "540240"
  },
  {
    "text": "how you tell it to be configured okay so",
    "start": "540240",
    "end": "542720"
  },
  {
    "text": "the these blue",
    "start": "542720",
    "end": "544000"
  },
  {
    "text": "pods are really",
    "start": "544000",
    "end": "547279"
  },
  {
    "text": "core rook pods that are providing the",
    "start": "547279",
    "end": "549600"
  },
  {
    "text": "management layer",
    "start": "549600",
    "end": "550560"
  },
  {
    "text": "the discovery pod is discovering what",
    "start": "550560",
    "end": "554240"
  },
  {
    "text": "storage is available on each node so you",
    "start": "554240",
    "end": "557279"
  },
  {
    "text": "um so rook knows how to deploy the",
    "start": "557279",
    "end": "561519"
  },
  {
    "text": "sep demons the green pods are csi",
    "start": "561519",
    "end": "564160"
  },
  {
    "text": "drivers",
    "start": "564160",
    "end": "564720"
  },
  {
    "text": "so on each node you need a csi driver",
    "start": "564720",
    "end": "566880"
  },
  {
    "text": "that will",
    "start": "566880",
    "end": "568080"
  },
  {
    "text": "mount mount the storage and then the red",
    "start": "568080",
    "end": "570880"
  },
  {
    "text": "pods are all the ceph demons so there",
    "start": "570880",
    "end": "572720"
  },
  {
    "text": "are",
    "start": "572720",
    "end": "573279"
  },
  {
    "text": "quite a number of seth demons that",
    "start": "573279",
    "end": "576959"
  },
  {
    "text": "provide that data layer the mons and the",
    "start": "576959",
    "end": "580320"
  },
  {
    "text": "osds",
    "start": "580320",
    "end": "581760"
  },
  {
    "text": "are backed by local storage on the node",
    "start": "581760",
    "end": "585519"
  },
  {
    "text": "so it's very important for sef to have",
    "start": "585519",
    "end": "586959"
  },
  {
    "text": "that local storage anyway rook",
    "start": "586959",
    "end": "588800"
  },
  {
    "text": "deploys all these pods for you and does",
    "start": "588800",
    "end": "592240"
  },
  {
    "text": "that management at the",
    "start": "592240",
    "end": "594000"
  },
  {
    "text": "kubernetes resource layer creating",
    "start": "594000",
    "end": "596880"
  },
  {
    "text": "deployments pod services",
    "start": "596880",
    "end": "598880"
  },
  {
    "text": "okay so now at the next layer we got csi",
    "start": "598880",
    "end": "601279"
  },
  {
    "start": "599000",
    "end": "599000"
  },
  {
    "text": "provisioning",
    "start": "601279",
    "end": "602800"
  },
  {
    "text": "so this is uh this is a picture of how",
    "start": "602800",
    "end": "606560"
  },
  {
    "text": "your application can request storage",
    "start": "606560",
    "end": "610240"
  },
  {
    "text": "from the csi driver so if we start on",
    "start": "610240",
    "end": "612399"
  },
  {
    "text": "the left here we've got an application",
    "start": "612399",
    "end": "614000"
  },
  {
    "text": "that needs block storage",
    "start": "614000",
    "end": "616079"
  },
  {
    "text": "okay so you create a persistent volume",
    "start": "616079",
    "end": "618880"
  },
  {
    "text": "claim",
    "start": "618880",
    "end": "619760"
  },
  {
    "text": "uh usually with uh read write once",
    "start": "619760",
    "end": "623120"
  },
  {
    "text": "access",
    "start": "623120",
    "end": "623920"
  },
  {
    "text": "okay so that when you create that claim",
    "start": "623920",
    "end": "626720"
  },
  {
    "text": "the the request goes to the",
    "start": "626720",
    "end": "628560"
  },
  {
    "text": "storage class which is which uses",
    "start": "628560",
    "end": "630399"
  },
  {
    "text": "cephrbd that rook sets up for you",
    "start": "630399",
    "end": "633120"
  },
  {
    "text": "and then the csi driver will mount that",
    "start": "633120",
    "end": "636959"
  },
  {
    "text": "that's rbd storage into your application",
    "start": "636959",
    "end": "640000"
  },
  {
    "text": "pod",
    "start": "640000",
    "end": "640399"
  },
  {
    "text": "something very similar happens for",
    "start": "640399",
    "end": "641760"
  },
  {
    "text": "shared file system where you've got two",
    "start": "641760",
    "end": "644000"
  },
  {
    "text": "applications who need to share the file",
    "start": "644000",
    "end": "646640"
  },
  {
    "text": "system",
    "start": "646640",
    "end": "647279"
  },
  {
    "text": "so they both have their persistent",
    "start": "647279",
    "end": "649360"
  },
  {
    "text": "volume claims",
    "start": "649360",
    "end": "651120"
  },
  {
    "text": "and the request goes to the storage",
    "start": "651120",
    "end": "652640"
  },
  {
    "text": "class and then this csr driver",
    "start": "652640",
    "end": "655200"
  },
  {
    "text": "for the shared file system mounts that",
    "start": "655200",
    "end": "657040"
  },
  {
    "text": "storage okay now the third type of",
    "start": "657040",
    "end": "658880"
  },
  {
    "text": "storage",
    "start": "658880",
    "end": "659600"
  },
  {
    "text": "we've got with ceph is object with the",
    "start": "659600",
    "end": "662000"
  },
  {
    "text": "s3 input",
    "start": "662000",
    "end": "663279"
  },
  {
    "text": "now this this is different but we",
    "start": "663279",
    "end": "666399"
  },
  {
    "text": "have exposed it in a way that will be",
    "start": "666399",
    "end": "668959"
  },
  {
    "text": "consistent",
    "start": "668959",
    "end": "670160"
  },
  {
    "text": "with as consistent as it can be with",
    "start": "670160",
    "end": "673680"
  },
  {
    "text": "block and file storage",
    "start": "673680",
    "end": "675440"
  },
  {
    "text": "okay so the if you want object storage",
    "start": "675440",
    "end": "679519"
  },
  {
    "text": "really what you want is a bucket so you",
    "start": "679519",
    "end": "681040"
  },
  {
    "text": "can rewrite objects to the bucket",
    "start": "681040",
    "end": "682959"
  },
  {
    "text": "and so we create a bucket claim which",
    "start": "682959",
    "end": "685600"
  },
  {
    "text": "requests",
    "start": "685600",
    "end": "686560"
  },
  {
    "text": "that storage from the storage class",
    "start": "686560",
    "end": "689440"
  },
  {
    "text": "crafted for object storage",
    "start": "689440",
    "end": "691680"
  },
  {
    "text": "and we have a bucket provisioner then",
    "start": "691680",
    "end": "694079"
  },
  {
    "text": "that",
    "start": "694079",
    "end": "694720"
  },
  {
    "text": "actually creates a bucket in enceph the",
    "start": "694720",
    "end": "698240"
  },
  {
    "text": "sept optic storage",
    "start": "698240",
    "end": "699600"
  },
  {
    "text": "and provides that back to the",
    "start": "699600",
    "end": "700720"
  },
  {
    "text": "applications all right so that gives us",
    "start": "700720",
    "end": "702560"
  },
  {
    "text": "all three types of storage",
    "start": "702560",
    "end": "704240"
  },
  {
    "text": "at the provisioning now what does it",
    "start": "704240",
    "end": "706000"
  },
  {
    "start": "706000",
    "end": "706000"
  },
  {
    "text": "look like at the data path",
    "start": "706000",
    "end": "707360"
  },
  {
    "text": "you've installed rook at layer one",
    "start": "707360",
    "end": "708959"
  },
  {
    "text": "you've got the csi driver with your",
    "start": "708959",
    "end": "711040"
  },
  {
    "text": "storage mounted layer two now",
    "start": "711040",
    "end": "714160"
  },
  {
    "text": "your application just needs to write and",
    "start": "714160",
    "end": "716320"
  },
  {
    "text": "read data",
    "start": "716320",
    "end": "717279"
  },
  {
    "text": "to the to the cluster so again your",
    "start": "717279",
    "end": "719519"
  },
  {
    "text": "application",
    "start": "719519",
    "end": "720480"
  },
  {
    "text": "it's already got the volume mounted and",
    "start": "720480",
    "end": "723120"
  },
  {
    "text": "there is",
    "start": "723120",
    "end": "724240"
  },
  {
    "text": "um the sef rbd kernel driver then that",
    "start": "724240",
    "end": "726880"
  },
  {
    "text": "will",
    "start": "726880",
    "end": "727440"
  },
  {
    "text": "take care of writing to the ceph cluster",
    "start": "727440",
    "end": "729360"
  },
  {
    "text": "for you and that driver will",
    "start": "729360",
    "end": "731200"
  },
  {
    "text": "it knows how to connect to the different",
    "start": "731200",
    "end": "732800"
  },
  {
    "text": "ceph demons the mons and osds and",
    "start": "732800",
    "end": "734880"
  },
  {
    "text": "manager",
    "start": "734880",
    "end": "735600"
  },
  {
    "text": "you know to make that that write that",
    "start": "735600",
    "end": "737600"
  },
  {
    "text": "data to the cluster",
    "start": "737600",
    "end": "738720"
  },
  {
    "text": "again at the file system file system",
    "start": "738720",
    "end": "740959"
  },
  {
    "text": "layer connect to the mds daemon in ceph",
    "start": "740959",
    "end": "743760"
  },
  {
    "text": "which manages the you know the locks and",
    "start": "743760",
    "end": "746320"
  },
  {
    "text": "the file system",
    "start": "746320",
    "end": "747680"
  },
  {
    "text": "semantics and the s3 client then",
    "start": "747680",
    "end": "750000"
  },
  {
    "text": "connects to the rdw",
    "start": "750000",
    "end": "752800"
  },
  {
    "text": "endpoint which then",
    "start": "752800",
    "end": "756480"
  },
  {
    "text": "writes the objects into the ceph cluster",
    "start": "756480",
    "end": "758880"
  },
  {
    "text": "hopefully",
    "start": "758880",
    "end": "759519"
  },
  {
    "text": "we're good there after layer three that",
    "start": "759519",
    "end": "762399"
  },
  {
    "text": "you've",
    "start": "762399",
    "end": "763040"
  },
  {
    "text": "set up rook and now you can write to the",
    "start": "763040",
    "end": "765680"
  },
  {
    "text": "cluster",
    "start": "765680",
    "end": "766480"
  },
  {
    "text": "so what does it take to get started this",
    "start": "766480",
    "end": "768880"
  },
  {
    "text": "is you know i'm sure it sounds like",
    "start": "768880",
    "end": "770639"
  },
  {
    "text": "a complex system but really to get",
    "start": "770639",
    "end": "773760"
  },
  {
    "text": "started with rook we've tried to make it",
    "start": "773760",
    "end": "775200"
  },
  {
    "start": "775000",
    "end": "775000"
  },
  {
    "text": "as simple as possible",
    "start": "775200",
    "end": "777120"
  },
  {
    "text": "and installing ceph has never been",
    "start": "777120",
    "end": "780160"
  },
  {
    "text": "this simple really there are three steps",
    "start": "780160",
    "end": "782959"
  },
  {
    "text": "for",
    "start": "782959",
    "end": "783519"
  },
  {
    "text": "for installing sap through okay the",
    "start": "783519",
    "end": "786480"
  },
  {
    "text": "first thing is well you need to",
    "start": "786480",
    "end": "788720"
  },
  {
    "text": "set up authorization or rbac in",
    "start": "788720",
    "end": "790720"
  },
  {
    "text": "kubernetes so you say",
    "start": "790720",
    "end": "792000"
  },
  {
    "text": "cuddle create common.yaml okay next you",
    "start": "792000",
    "end": "794880"
  },
  {
    "text": "need to run the operator",
    "start": "794880",
    "end": "796320"
  },
  {
    "text": "and so you create the operator that gets",
    "start": "796320",
    "end": "798800"
  },
  {
    "text": "the rook operator running but",
    "start": "798800",
    "end": "800480"
  },
  {
    "text": "now the operator you have to tell it how",
    "start": "800480",
    "end": "802880"
  },
  {
    "text": "to deploy ceph",
    "start": "802880",
    "end": "804399"
  },
  {
    "text": "so now we've got what we call the",
    "start": "804399",
    "end": "805760"
  },
  {
    "text": "cluster that cluster cr",
    "start": "805760",
    "end": "808480"
  },
  {
    "text": "and you can see a picture of that on the",
    "start": "808480",
    "end": "809839"
  },
  {
    "text": "right here where you tell rook how to",
    "start": "809839",
    "end": "812240"
  },
  {
    "text": "deploy stuff you tell it well what",
    "start": "812240",
    "end": "813839"
  },
  {
    "text": "version of stuff do you want to deploy",
    "start": "813839",
    "end": "815680"
  },
  {
    "text": "now do you want ceph octopus or do you",
    "start": "815680",
    "end": "818000"
  },
  {
    "text": "want which is v15 or do you want a",
    "start": "818000",
    "end": "820720"
  },
  {
    "text": "an earlier version seth nautilus which",
    "start": "820720",
    "end": "822800"
  },
  {
    "text": "would be v14",
    "start": "822800",
    "end": "824480"
  },
  {
    "text": "we all and you can choose what do you",
    "start": "824480",
    "end": "826800"
  },
  {
    "text": "want the latest version",
    "start": "826800",
    "end": "828079"
  },
  {
    "text": "or a previous version of stuff so you",
    "start": "828079",
    "end": "829920"
  },
  {
    "text": "can choose when to",
    "start": "829920",
    "end": "831440"
  },
  {
    "text": "update the the ceph version",
    "start": "831440",
    "end": "834959"
  },
  {
    "text": "and then some other there are many many",
    "start": "834959",
    "end": "837519"
  },
  {
    "text": "settings here we're just showing a few",
    "start": "837519",
    "end": "838800"
  },
  {
    "text": "of them",
    "start": "838800",
    "end": "839760"
  },
  {
    "text": "how many mods how to deploy the storage",
    "start": "839760",
    "end": "842000"
  },
  {
    "text": "that is the basic cluster",
    "start": "842000",
    "end": "843680"
  },
  {
    "text": "once you've created these three these",
    "start": "843680",
    "end": "845519"
  },
  {
    "text": "three manifests",
    "start": "845519",
    "end": "846720"
  },
  {
    "text": "you'll have a basic ceph cluster up and",
    "start": "846720",
    "end": "849279"
  },
  {
    "text": "running in rook",
    "start": "849279",
    "end": "850480"
  },
  {
    "text": "as a side note there's also a helm chart",
    "start": "850480",
    "end": "852639"
  },
  {
    "text": "we have that will simplify",
    "start": "852639",
    "end": "854639"
  },
  {
    "text": "these first two steps and we're working",
    "start": "854639",
    "end": "856800"
  },
  {
    "text": "on health charts",
    "start": "856800",
    "end": "857760"
  },
  {
    "text": "for uh our other crs going forward",
    "start": "857760",
    "end": "861040"
  },
  {
    "text": "let's assume rook is set up now with sef",
    "start": "861040",
    "end": "863279"
  },
  {
    "start": "862000",
    "end": "862000"
  },
  {
    "text": "what does it take to",
    "start": "863279",
    "end": "864399"
  },
  {
    "text": "consume that storage uh just like any",
    "start": "864399",
    "end": "867440"
  },
  {
    "text": "other",
    "start": "867440",
    "end": "868240"
  },
  {
    "text": "storage platform first of all the admin",
    "start": "868240",
    "end": "870800"
  },
  {
    "text": "has to create the storage class",
    "start": "870800",
    "end": "872399"
  },
  {
    "text": "after they have increased the storage",
    "start": "872399",
    "end": "873760"
  },
  {
    "text": "class the application",
    "start": "873760",
    "end": "875839"
  },
  {
    "text": "request that storage was with the pvc",
    "start": "875839",
    "end": "878160"
  },
  {
    "text": "and then you create your application pod",
    "start": "878160",
    "end": "880079"
  },
  {
    "text": "that",
    "start": "880079",
    "end": "881279"
  },
  {
    "text": "that will mount that that volume so here",
    "start": "881279",
    "end": "883760"
  },
  {
    "text": "on the right we've just got an example",
    "start": "883760",
    "end": "885120"
  },
  {
    "text": "pod",
    "start": "885120",
    "end": "886000"
  },
  {
    "text": "that shows this the pvc",
    "start": "886000",
    "end": "890079"
  },
  {
    "text": "that's going to be mounted into this",
    "start": "890079",
    "end": "892480"
  },
  {
    "text": "this demo",
    "start": "892480",
    "end": "893360"
  },
  {
    "text": "web server and really this if you've",
    "start": "893360",
    "end": "895839"
  },
  {
    "text": "done any storage",
    "start": "895839",
    "end": "896959"
  },
  {
    "text": "and kubernetes before this should be",
    "start": "896959",
    "end": "899519"
  },
  {
    "text": "very familiar",
    "start": "899519",
    "end": "900320"
  },
  {
    "text": "just the same pattern that used to plug",
    "start": "900320",
    "end": "902480"
  },
  {
    "text": "in any other storage provider",
    "start": "902480",
    "end": "904639"
  },
  {
    "text": "outside of rook either okay and now i'll",
    "start": "904639",
    "end": "906639"
  },
  {
    "text": "hand off to sebastian",
    "start": "906639",
    "end": "908240"
  },
  {
    "text": "to talk more about key features and",
    "start": "908240",
    "end": "910560"
  },
  {
    "text": "everything and stuff",
    "start": "910560",
    "end": "911600"
  },
  {
    "text": "okay thanks travis now let's dive into",
    "start": "911600",
    "end": "915120"
  },
  {
    "text": "some of the key features of rook",
    "start": "915120",
    "end": "918720"
  },
  {
    "start": "918000",
    "end": "918000"
  },
  {
    "text": "so first environments ruksef is capable",
    "start": "918720",
    "end": "922079"
  },
  {
    "text": "of deploying",
    "start": "922079",
    "end": "923680"
  },
  {
    "text": "ceph permanent environments so bring",
    "start": "923680",
    "end": "926880"
  },
  {
    "text": "your own hardware",
    "start": "926880",
    "end": "927839"
  },
  {
    "text": "if you have your own infrastructure and",
    "start": "927839",
    "end": "930399"
  },
  {
    "text": "you want to work on premise",
    "start": "930399",
    "end": "931680"
  },
  {
    "text": "then this scenario is for you",
    "start": "931680",
    "end": "935120"
  },
  {
    "text": "we also support deploying rooksef into",
    "start": "935120",
    "end": "938079"
  },
  {
    "text": "the cloud",
    "start": "938079",
    "end": "939120"
  },
  {
    "text": "with various cloud providers so if you",
    "start": "939120",
    "end": "942399"
  },
  {
    "text": "already",
    "start": "942399",
    "end": "943519"
  },
  {
    "text": "if your entire infrastructure relies on",
    "start": "943519",
    "end": "946240"
  },
  {
    "text": "the cloud and",
    "start": "946240",
    "end": "947360"
  },
  {
    "text": "everything is running there already then",
    "start": "947360",
    "end": "950480"
  },
  {
    "text": "you can consume storage through rook",
    "start": "950480",
    "end": "954480"
  },
  {
    "text": "inside inside the cloud environment but",
    "start": "954480",
    "end": "956880"
  },
  {
    "text": "first",
    "start": "956880",
    "end": "957839"
  },
  {
    "start": "957000",
    "end": "957000"
  },
  {
    "text": "you might you might ask yourself why",
    "start": "957839",
    "end": "960079"
  },
  {
    "text": "would you run",
    "start": "960079",
    "end": "961360"
  },
  {
    "text": "ceph in the cloud so basically it's all",
    "start": "961360",
    "end": "963519"
  },
  {
    "text": "about consistency",
    "start": "963519",
    "end": "965600"
  },
  {
    "text": "because kubernetes has been adopted by",
    "start": "965600",
    "end": "967920"
  },
  {
    "text": "all the major cloud providers",
    "start": "967920",
    "end": "969920"
  },
  {
    "text": "today then it is very easy to run any",
    "start": "969920",
    "end": "972800"
  },
  {
    "text": "cloud native application as part of",
    "start": "972800",
    "end": "974959"
  },
  {
    "text": "any cloud providers available out there",
    "start": "974959",
    "end": "977519"
  },
  {
    "text": "and also there are a bunch of",
    "start": "977519",
    "end": "979440"
  },
  {
    "text": "limitations shortcomings coming from the",
    "start": "979440",
    "end": "982639"
  },
  {
    "text": "cloud providers",
    "start": "982639",
    "end": "984639"
  },
  {
    "text": "for instance the limitation of pvs you",
    "start": "984639",
    "end": "986639"
  },
  {
    "text": "can attach to a given node",
    "start": "986639",
    "end": "988720"
  },
  {
    "text": "let's assume that we are in a cloud",
    "start": "988720",
    "end": "990320"
  },
  {
    "text": "environment then obviously we have",
    "start": "990320",
    "end": "992320"
  },
  {
    "text": "hypervisors and",
    "start": "992320",
    "end": "993759"
  },
  {
    "text": "those hypervisors typically have a",
    "start": "993759",
    "end": "995440"
  },
  {
    "text": "limitation in a number of disks",
    "start": "995440",
    "end": "998240"
  },
  {
    "text": "that can be attached to a given virtual",
    "start": "998240",
    "end": "1000880"
  },
  {
    "text": "machine",
    "start": "1000880",
    "end": "1001839"
  },
  {
    "text": "and typically that limit is around 30",
    "start": "1001839",
    "end": "1005040"
  },
  {
    "text": "which means that if you're looking at",
    "start": "1005040",
    "end": "1007440"
  },
  {
    "text": "providing dynamic provisioning",
    "start": "1007440",
    "end": "1009519"
  },
  {
    "text": "to applications then you might be",
    "start": "1009519",
    "end": "1011600"
  },
  {
    "text": "limited by",
    "start": "1011600",
    "end": "1013519"
  },
  {
    "text": "the number of vms times the number of",
    "start": "1013519",
    "end": "1015680"
  },
  {
    "text": "pvs you can attach to it",
    "start": "1015680",
    "end": "1017759"
  },
  {
    "text": "if you do run rook in the cloud though",
    "start": "1017759",
    "end": "1021440"
  },
  {
    "text": "rook will be running inside virtual",
    "start": "1021440",
    "end": "1023120"
  },
  {
    "text": "machines and because we will be",
    "start": "1023120",
    "end": "1025280"
  },
  {
    "text": "attaching",
    "start": "1025280",
    "end": "1026319"
  },
  {
    "text": "devices then we use our own technology",
    "start": "1026319",
    "end": "1029280"
  },
  {
    "text": "to provide dynamic provisioning",
    "start": "1029280",
    "end": "1031520"
  },
  {
    "text": "then we can scale up to thousands and",
    "start": "1031520",
    "end": "1034558"
  },
  {
    "text": "thousands",
    "start": "1034559",
    "end": "1035199"
  },
  {
    "text": "of of pvs instead of that 30",
    "start": "1035199",
    "end": "1038240"
  },
  {
    "text": "limitation by virtual machine and not",
    "start": "1038240",
    "end": "1041360"
  },
  {
    "text": "only we",
    "start": "1041360",
    "end": "1042480"
  },
  {
    "text": "bypass that limitation but also because",
    "start": "1042480",
    "end": "1044640"
  },
  {
    "text": "we are aggregating storage",
    "start": "1044640",
    "end": "1046798"
  },
  {
    "text": "then we're providing much much better",
    "start": "1046799",
    "end": "1049840"
  },
  {
    "text": "performance overall",
    "start": "1049840",
    "end": "1051760"
  },
  {
    "text": "if you were to use a single disk which",
    "start": "1051760",
    "end": "1053919"
  },
  {
    "text": "is typically attached to",
    "start": "1053919",
    "end": "1056320"
  },
  {
    "text": "a storage class which essentially",
    "start": "1056320",
    "end": "1058320"
  },
  {
    "text": "represents the flavor of storage",
    "start": "1058320",
    "end": "1060720"
  },
  {
    "text": "then that flavor would have iops as well",
    "start": "1060720",
    "end": "1064160"
  },
  {
    "text": "as",
    "start": "1064160",
    "end": "1064640"
  },
  {
    "text": "throughput limitations so for a given",
    "start": "1064640",
    "end": "1067280"
  },
  {
    "text": "application you will be limited to",
    "start": "1067280",
    "end": "1069360"
  },
  {
    "text": "what that flavor can can provide or if",
    "start": "1069360",
    "end": "1072640"
  },
  {
    "text": "you use rook self because as i said",
    "start": "1072640",
    "end": "1074559"
  },
  {
    "text": "again we are aggregating many pvs then",
    "start": "1074559",
    "end": "1077039"
  },
  {
    "text": "we're providing much",
    "start": "1077039",
    "end": "1078240"
  },
  {
    "text": "much better performance out of the box",
    "start": "1078240",
    "end": "1080720"
  },
  {
    "text": "we have configurable",
    "start": "1080720",
    "end": "1082000"
  },
  {
    "start": "1081000",
    "end": "1081000"
  },
  {
    "text": "clustered topologies seth is really",
    "start": "1082000",
    "end": "1085280"
  },
  {
    "text": "amazing as",
    "start": "1085280",
    "end": "1086559"
  },
  {
    "text": "being topology award seth",
    "start": "1086559",
    "end": "1090080"
  },
  {
    "text": "knows and because it's being told to",
    "start": "1090080",
    "end": "1093520"
  },
  {
    "text": "where it is running in what kind of",
    "start": "1093520",
    "end": "1095919"
  },
  {
    "text": "topology it is running on",
    "start": "1095919",
    "end": "1098000"
  },
  {
    "text": "rook can support that so it really",
    "start": "1098000",
    "end": "1100880"
  },
  {
    "text": "allows you to",
    "start": "1100880",
    "end": "1102000"
  },
  {
    "text": "deploy a cluster and define your what",
    "start": "1102000",
    "end": "1105760"
  },
  {
    "text": "your topology is",
    "start": "1105760",
    "end": "1107760"
  },
  {
    "text": "so if you run ceph in rook step inside a",
    "start": "1107760",
    "end": "1110640"
  },
  {
    "text": "data center and then you have multiple",
    "start": "1110640",
    "end": "1112400"
  },
  {
    "text": "rooms",
    "start": "1112400",
    "end": "1113679"
  },
  {
    "text": "multiple racks and multiple nodes then",
    "start": "1113679",
    "end": "1115679"
  },
  {
    "text": "we can easily build",
    "start": "1115679",
    "end": "1116880"
  },
  {
    "text": "such topology by assigning labels to",
    "start": "1116880",
    "end": "1119280"
  },
  {
    "text": "nodes",
    "start": "1119280",
    "end": "1120160"
  },
  {
    "text": "and all of that will be reflected as",
    "start": "1120160",
    "end": "1122080"
  },
  {
    "text": "part of step by doing so you're",
    "start": "1122080",
    "end": "1123760"
  },
  {
    "text": "increasing",
    "start": "1123760",
    "end": "1124720"
  },
  {
    "text": "data availability and resiliency because",
    "start": "1124720",
    "end": "1128080"
  },
  {
    "text": "you're spreading it",
    "start": "1128080",
    "end": "1129039"
  },
  {
    "text": "across different field domains and again",
    "start": "1129039",
    "end": "1131360"
  },
  {
    "text": "those domains can be",
    "start": "1131360",
    "end": "1133120"
  },
  {
    "text": "nodes racks rooms and even",
    "start": "1133120",
    "end": "1136400"
  },
  {
    "text": "across zones between different data",
    "start": "1136400",
    "end": "1138720"
  },
  {
    "text": "centers if you want to stretch your",
    "start": "1138720",
    "end": "1139840"
  },
  {
    "text": "cluster",
    "start": "1139840",
    "end": "1140960"
  },
  {
    "text": "safe csi drivers so from the very",
    "start": "1140960",
    "end": "1143360"
  },
  {
    "start": "1141000",
    "end": "1141000"
  },
  {
    "text": "beginning rook",
    "start": "1143360",
    "end": "1145440"
  },
  {
    "text": "once csi was released and the spec was",
    "start": "1145440",
    "end": "1147919"
  },
  {
    "text": "released",
    "start": "1147919",
    "end": "1148720"
  },
  {
    "text": "we started to collaborate the cfcsi team",
    "start": "1148720",
    "end": "1151200"
  },
  {
    "text": "to integrate as",
    "start": "1151200",
    "end": "1152240"
  },
  {
    "text": "much as possible so just like travis",
    "start": "1152240",
    "end": "1154080"
  },
  {
    "text": "mentioned we support a large variety of",
    "start": "1154080",
    "end": "1156559"
  },
  {
    "text": "dynamic provisioning modes",
    "start": "1156559",
    "end": "1158440"
  },
  {
    "text": "rwx and rocks",
    "start": "1158440",
    "end": "1161440"
  },
  {
    "text": "and this for both block and file system",
    "start": "1161440",
    "end": "1164080"
  },
  {
    "text": "interfaces",
    "start": "1164080",
    "end": "1166160"
  },
  {
    "text": "rook the latest release of rook 1 5",
    "start": "1166160",
    "end": "1169360"
  },
  {
    "text": "ships with the latest version of the",
    "start": "1169360",
    "end": "1171840"
  },
  {
    "text": "system csi driver",
    "start": "1171840",
    "end": "1174240"
  },
  {
    "text": "which now supports volume expansion and",
    "start": "1174240",
    "end": "1176960"
  },
  {
    "text": "as a better feature supports",
    "start": "1176960",
    "end": "1178960"
  },
  {
    "text": "snapshots and clones we still do support",
    "start": "1178960",
    "end": "1182880"
  },
  {
    "text": "the flex driver but it has really",
    "start": "1182880",
    "end": "1185360"
  },
  {
    "text": "limited support",
    "start": "1185360",
    "end": "1186320"
  },
  {
    "text": "and we really encourage everybody to",
    "start": "1186320",
    "end": "1188559"
  },
  {
    "text": "switch to",
    "start": "1188559",
    "end": "1189679"
  },
  {
    "text": "using the staff csi spec upgrading is",
    "start": "1189679",
    "end": "1192640"
  },
  {
    "text": "automated",
    "start": "1192640",
    "end": "1194400"
  },
  {
    "start": "1193000",
    "end": "1193000"
  },
  {
    "text": "typically upgrades are some kind of a",
    "start": "1194400",
    "end": "1197840"
  },
  {
    "text": "tedious process",
    "start": "1197840",
    "end": "1200000"
  },
  {
    "text": "and it is really making administrators",
    "start": "1200000",
    "end": "1203200"
  },
  {
    "text": "nervous",
    "start": "1203200",
    "end": "1203840"
  },
  {
    "text": "especially when it comes to storage you",
    "start": "1203840",
    "end": "1205919"
  },
  {
    "text": "always be wondering okay",
    "start": "1205919",
    "end": "1207679"
  },
  {
    "text": "if something goes wrong will i be losing",
    "start": "1207679",
    "end": "1210000"
  },
  {
    "text": "data",
    "start": "1210000",
    "end": "1211360"
  },
  {
    "text": "first this cannot really happen with sev",
    "start": "1211360",
    "end": "1213919"
  },
  {
    "text": "and also",
    "start": "1213919",
    "end": "1215039"
  },
  {
    "text": "one of the good things about surf is",
    "start": "1215039",
    "end": "1217200"
  },
  {
    "text": "from the very beginning it has proven to",
    "start": "1217200",
    "end": "1218960"
  },
  {
    "text": "be super robust at performing upgrades",
    "start": "1218960",
    "end": "1220960"
  },
  {
    "text": "and it's one of the few",
    "start": "1220960",
    "end": "1223360"
  },
  {
    "text": "storage solution not to say software",
    "start": "1223360",
    "end": "1225919"
  },
  {
    "text": "most of the time",
    "start": "1225919",
    "end": "1227200"
  },
  {
    "text": "that can upgrade in a rolling fashion",
    "start": "1227200",
    "end": "1231200"
  },
  {
    "text": "with no downtime and yeah while",
    "start": "1231200",
    "end": "1234720"
  },
  {
    "text": "providing a service",
    "start": "1234720",
    "end": "1236159"
  },
  {
    "text": "so seth is already really good at it and",
    "start": "1236159",
    "end": "1238960"
  },
  {
    "text": "with rook we really took that",
    "start": "1238960",
    "end": "1240960"
  },
  {
    "text": "to the next we really really took that",
    "start": "1240960",
    "end": "1243840"
  },
  {
    "text": "further",
    "start": "1243840",
    "end": "1244559"
  },
  {
    "text": "to the next level by aggregating",
    "start": "1244559",
    "end": "1247280"
  },
  {
    "text": "collecting",
    "start": "1247280",
    "end": "1248080"
  },
  {
    "text": "all the operational knowledge that is",
    "start": "1248080",
    "end": "1249679"
  },
  {
    "text": "needed to do upgrades",
    "start": "1249679",
    "end": "1251679"
  },
  {
    "text": "and we embedded all of that logic into",
    "start": "1251679",
    "end": "1253840"
  },
  {
    "text": "rook",
    "start": "1253840",
    "end": "1254799"
  },
  {
    "text": "so to a network it is really easy as",
    "start": "1254799",
    "end": "1257360"
  },
  {
    "text": "just changing the image in the spec",
    "start": "1257360",
    "end": "1259360"
  },
  {
    "text": "of the deployment file of the operator",
    "start": "1259360",
    "end": "1262400"
  },
  {
    "text": "to update to the step cluster then it's",
    "start": "1262400",
    "end": "1264720"
  },
  {
    "text": "as easy as changing the",
    "start": "1264720",
    "end": "1266880"
  },
  {
    "text": "self image version into the cluster cr",
    "start": "1266880",
    "end": "1269919"
  },
  {
    "text": "and rook will handle all the details",
    "start": "1269919",
    "end": "1272240"
  },
  {
    "text": "when it comes to the upgrade",
    "start": "1272240",
    "end": "1273760"
  },
  {
    "text": "internally going notes by nodes and",
    "start": "1273760",
    "end": "1276000"
  },
  {
    "text": "making sure",
    "start": "1276000",
    "end": "1276799"
  },
  {
    "text": "they're all recovering and being",
    "start": "1276799",
    "end": "1278720"
  },
  {
    "text": "upgraded and backfilling properly",
    "start": "1278720",
    "end": "1281200"
  },
  {
    "text": "and then go to the other one so upgrades",
    "start": "1281200",
    "end": "1284720"
  },
  {
    "text": "made easy basically",
    "start": "1284720",
    "end": "1286320"
  },
  {
    "text": "x no cluster connection so this is a",
    "start": "1286320",
    "end": "1288080"
  },
  {
    "text": "really popular scenario",
    "start": "1288080",
    "end": "1290400"
  },
  {
    "start": "1289000",
    "end": "1289000"
  },
  {
    "text": "and we have introduced it in the 1.3",
    "start": "1290400",
    "end": "1294720"
  },
  {
    "text": "release cycle",
    "start": "1294720",
    "end": "1296240"
  },
  {
    "text": "and the the use case is quite simple",
    "start": "1296240",
    "end": "1298080"
  },
  {
    "text": "because not everything is about",
    "start": "1298080",
    "end": "1300640"
  },
  {
    "text": "green field environments you already",
    "start": "1300640",
    "end": "1302880"
  },
  {
    "text": "have",
    "start": "1302880",
    "end": "1304320"
  },
  {
    "text": "clusters out there up and running",
    "start": "1304320",
    "end": "1307360"
  },
  {
    "text": "which are maybe standalone clusters",
    "start": "1307360",
    "end": "1309440"
  },
  {
    "text": "providing object storage",
    "start": "1309440",
    "end": "1311280"
  },
  {
    "text": "or you might have subclusters providing",
    "start": "1311280",
    "end": "1313280"
  },
  {
    "text": "block for",
    "start": "1313280",
    "end": "1314880"
  },
  {
    "text": "let's say openstack for example you're",
    "start": "1314880",
    "end": "1317039"
  },
  {
    "text": "ready to run applications",
    "start": "1317039",
    "end": "1319120"
  },
  {
    "text": "on kubernetes but you're not you're not",
    "start": "1319120",
    "end": "1321520"
  },
  {
    "text": "really ready to",
    "start": "1321520",
    "end": "1322720"
  },
  {
    "text": "move all of that storage because it is",
    "start": "1322720",
    "end": "1324880"
  },
  {
    "text": "difficult and it's challenging to do",
    "start": "1324880",
    "end": "1327760"
  },
  {
    "text": "to two kubernetes so you want to keep",
    "start": "1327760",
    "end": "1330720"
  },
  {
    "text": "your cluster which was deployed by",
    "start": "1330720",
    "end": "1333280"
  },
  {
    "text": "whatever tool can be can be defensible",
    "start": "1333280",
    "end": "1336320"
  },
  {
    "text": "can be rook as well if you want",
    "start": "1336320",
    "end": "1338080"
  },
  {
    "text": "the print the goal is to",
    "start": "1338080",
    "end": "1341120"
  },
  {
    "text": "from your kubernetes cluster you deploy",
    "start": "1341120",
    "end": "1343039"
  },
  {
    "text": "rook",
    "start": "1343039",
    "end": "1344159"
  },
  {
    "text": "and then with a few details you connect",
    "start": "1344159",
    "end": "1347200"
  },
  {
    "text": "to the external cluster and then once",
    "start": "1347200",
    "end": "1349280"
  },
  {
    "text": "the connection is established we're",
    "start": "1349280",
    "end": "1350799"
  },
  {
    "text": "really into this consumer producer",
    "start": "1350799",
    "end": "1352480"
  },
  {
    "text": "relationship where rook is at this point",
    "start": "1352480",
    "end": "1354480"
  },
  {
    "text": "only consuming the external",
    "start": "1354480",
    "end": "1356159"
  },
  {
    "text": "storage getting all the connection",
    "start": "1356159",
    "end": "1358320"
  },
  {
    "text": "details and passing them to csi",
    "start": "1358320",
    "end": "1360559"
  },
  {
    "text": "so that we can provide persistent",
    "start": "1360559",
    "end": "1361919"
  },
  {
    "text": "storage to containers but that is the",
    "start": "1361919",
    "end": "1363440"
  },
  {
    "text": "only thing we do we don't",
    "start": "1363440",
    "end": "1365120"
  },
  {
    "text": "get into the business of managing nodes",
    "start": "1365120",
    "end": "1368159"
  },
  {
    "text": "and things like this when we are in",
    "start": "1368159",
    "end": "1370240"
  },
  {
    "text": "external mode so we just consume the",
    "start": "1370240",
    "end": "1372159"
  },
  {
    "text": "storage that is",
    "start": "1372159",
    "end": "1373679"
  },
  {
    "text": "available to us object packet",
    "start": "1373679",
    "end": "1375760"
  },
  {
    "text": "provisioning",
    "start": "1375760",
    "end": "1377440"
  },
  {
    "text": "so just like travis mentioned earlier",
    "start": "1377440",
    "end": "1380000"
  },
  {
    "text": "when we were showing the topology of the",
    "start": "1380000",
    "end": "1381840"
  },
  {
    "text": "storage interfaces are supported by",
    "start": "1381840",
    "end": "1383440"
  },
  {
    "text": "rooksef",
    "start": "1383440",
    "end": "1385360"
  },
  {
    "text": "we mentioned the object bucket claim and",
    "start": "1385360",
    "end": "1388320"
  },
  {
    "start": "1388000",
    "end": "1388000"
  },
  {
    "text": "again",
    "start": "1388320",
    "end": "1389200"
  },
  {
    "text": "the this is quite simple as a user",
    "start": "1389200",
    "end": "1392320"
  },
  {
    "text": "the only thing i care about is because",
    "start": "1392320",
    "end": "1394080"
  },
  {
    "text": "i'm developing an s3 compliant",
    "start": "1394080",
    "end": "1396720"
  },
  {
    "text": "application",
    "start": "1396720",
    "end": "1397520"
  },
  {
    "text": "i just want to have a bucket with a few",
    "start": "1397520",
    "end": "1399600"
  },
  {
    "text": "policies on that bucket",
    "start": "1399600",
    "end": "1401600"
  },
  {
    "text": "and i want to be able to consume that",
    "start": "1401600",
    "end": "1402960"
  },
  {
    "text": "bucket storing data and",
    "start": "1402960",
    "end": "1404799"
  },
  {
    "text": "retrieving data and that's all i care",
    "start": "1404799",
    "end": "1407360"
  },
  {
    "text": "about",
    "start": "1407360",
    "end": "1408000"
  },
  {
    "text": "so in a similar fashion of claiming for",
    "start": "1408000",
    "end": "1411280"
  },
  {
    "text": "block or file then i want to claim for a",
    "start": "1411280",
    "end": "1413440"
  },
  {
    "text": "bucket",
    "start": "1413440",
    "end": "1414559"
  },
  {
    "text": "and for that we have embedded for",
    "start": "1414559",
    "end": "1417919"
  },
  {
    "text": "quite a while now the lead bucket",
    "start": "1417919",
    "end": "1419760"
  },
  {
    "text": "provisioner",
    "start": "1419760",
    "end": "1422000"
  },
  {
    "text": "but because we we really wanted to have",
    "start": "1422000",
    "end": "1425200"
  },
  {
    "text": "it in a more",
    "start": "1425200",
    "end": "1426559"
  },
  {
    "text": "native slash native kubernetes way",
    "start": "1426559",
    "end": "1429840"
  },
  {
    "text": "there was enough upstream to",
    "start": "1429840",
    "end": "1432880"
  },
  {
    "text": "implement a cozy interface which is",
    "start": "1432880",
    "end": "1435600"
  },
  {
    "text": "similar to csi",
    "start": "1435600",
    "end": "1437120"
  },
  {
    "text": "but for container object and it is",
    "start": "1437120",
    "end": "1440480"
  },
  {
    "text": "really",
    "start": "1440480",
    "end": "1441120"
  },
  {
    "text": "the csi equivalent of object",
    "start": "1441120",
    "end": "1444720"
  },
  {
    "text": "so the capped kubernetes announcement",
    "start": "1444720",
    "end": "1446799"
  },
  {
    "text": "proposal was merged upstream",
    "start": "1446799",
    "end": "1449200"
  },
  {
    "text": "and now we will be able to",
    "start": "1449200",
    "end": "1452240"
  },
  {
    "text": "vendors will be able to integrate their",
    "start": "1452240",
    "end": "1454080"
  },
  {
    "text": "object solution",
    "start": "1454080",
    "end": "1455360"
  },
  {
    "text": "through a native kubernetes interface",
    "start": "1455360",
    "end": "1458000"
  },
  {
    "text": "which is really great",
    "start": "1458000",
    "end": "1459200"
  },
  {
    "text": "so we just released rook one five",
    "start": "1459200",
    "end": "1462799"
  },
  {
    "text": "and we're super excited because we have",
    "start": "1462799",
    "end": "1465600"
  },
  {
    "text": "a bunch of",
    "start": "1465600",
    "end": "1466480"
  },
  {
    "text": "features and it's probably one of the",
    "start": "1466480",
    "end": "1469600"
  },
  {
    "text": "most feature-rich release we have",
    "start": "1469600",
    "end": "1472640"
  },
  {
    "text": "ever delivered so first is",
    "start": "1472640",
    "end": "1477360"
  },
  {
    "text": "encryption with kms support during the",
    "start": "1477360",
    "end": "1479840"
  },
  {
    "text": "wonderful cycle we",
    "start": "1479840",
    "end": "1481039"
  },
  {
    "text": "introduced encryption for osds on pvcs",
    "start": "1481039",
    "end": "1484799"
  },
  {
    "text": "and we were storing encryption keys into",
    "start": "1484799",
    "end": "1487279"
  },
  {
    "text": "kubernetes secrets so we all know that",
    "start": "1487279",
    "end": "1488880"
  },
  {
    "text": "kubernetes secrets are not so",
    "start": "1488880",
    "end": "1490640"
  },
  {
    "start": "1490000",
    "end": "1490000"
  },
  {
    "text": "secret and secure because they're just",
    "start": "1490640",
    "end": "1492559"
  },
  {
    "text": "merely a",
    "start": "1492559",
    "end": "1494240"
  },
  {
    "text": "a base 64 hash value of of your value",
    "start": "1494240",
    "end": "1497520"
  },
  {
    "text": "to move that to the next level we have",
    "start": "1497520",
    "end": "1500799"
  },
  {
    "text": "introduced support for",
    "start": "1500799",
    "end": "1502880"
  },
  {
    "text": "kms and the first one we have introduced",
    "start": "1502880",
    "end": "1505520"
  },
  {
    "text": "is hashicorp vault",
    "start": "1505520",
    "end": "1507200"
  },
  {
    "text": "which is really popular and now we",
    "start": "1507200",
    "end": "1511520"
  },
  {
    "text": "not only deploy encrypted osds but we",
    "start": "1511520",
    "end": "1514000"
  },
  {
    "text": "store those secret keys",
    "start": "1514000",
    "end": "1515840"
  },
  {
    "text": "inside vault and vault",
    "start": "1515840",
    "end": "1519279"
  },
  {
    "text": "as a key management service will manage",
    "start": "1519279",
    "end": "1522159"
  },
  {
    "text": "all the keys for us",
    "start": "1522159",
    "end": "1523360"
  },
  {
    "text": "and in this really secure fashion in a",
    "start": "1523360",
    "end": "1525279"
  },
  {
    "text": "really secure environment",
    "start": "1525279",
    "end": "1527279"
  },
  {
    "text": "all the connections to vault are tls",
    "start": "1527279",
    "end": "1529919"
  },
  {
    "text": "encrypted",
    "start": "1529919",
    "end": "1530960"
  },
  {
    "text": "and today we only support the token",
    "start": "1530960",
    "end": "1533440"
  },
  {
    "text": "based authentication",
    "start": "1533440",
    "end": "1534799"
  },
  {
    "text": "you know in the future we are planning",
    "start": "1534799",
    "end": "1536400"
  },
  {
    "text": "on supporting the",
    "start": "1536400",
    "end": "1538159"
  },
  {
    "text": "kubernetes native authentication and",
    "start": "1538159",
    "end": "1541039"
  },
  {
    "text": "obviously",
    "start": "1541039",
    "end": "1542400"
  },
  {
    "text": "much more kms as well mirroring of block",
    "start": "1542400",
    "end": "1545360"
  },
  {
    "text": "data",
    "start": "1545360",
    "end": "1546880"
  },
  {
    "text": "this one is really interesting because",
    "start": "1546880",
    "end": "1549760"
  },
  {
    "text": "we have been having",
    "start": "1549760",
    "end": "1551279"
  },
  {
    "text": "support for bootstrapping rbd mirror",
    "start": "1551279",
    "end": "1553360"
  },
  {
    "text": "demons",
    "start": "1553360",
    "end": "1554880"
  },
  {
    "text": "for a while but we never got into the",
    "start": "1554880",
    "end": "1556880"
  },
  {
    "text": "business of configuring mirroring",
    "start": "1556880",
    "end": "1558559"
  },
  {
    "text": "between two sites automatically",
    "start": "1558559",
    "end": "1560799"
  },
  {
    "text": "and now it's possible with rook one five",
    "start": "1560799",
    "end": "1565360"
  },
  {
    "text": "just to get a a step back a little bit",
    "start": "1565360",
    "end": "1568880"
  },
  {
    "text": "seth by design is strongly consistent",
    "start": "1568880",
    "end": "1572799"
  },
  {
    "text": "meaning that whenever a client writes a",
    "start": "1572799",
    "end": "1576080"
  },
  {
    "start": "1576000",
    "end": "1576000"
  },
  {
    "text": "data",
    "start": "1576080",
    "end": "1577039"
  },
  {
    "text": "then it has to wait for",
    "start": "1577039",
    "end": "1580240"
  },
  {
    "text": "all the replicas to be written so that",
    "start": "1580240",
    "end": "1582000"
  },
  {
    "text": "the writing is acknowledged",
    "start": "1582000",
    "end": "1584559"
  },
  {
    "text": "and because to that it makes",
    "start": "1584559",
    "end": "1588159"
  },
  {
    "text": "difficult to stretch stuff between",
    "start": "1588159",
    "end": "1591679"
  },
  {
    "text": "regions for example because of the",
    "start": "1591679",
    "end": "1593360"
  },
  {
    "text": "latencies and that won't be really",
    "start": "1593360",
    "end": "1595279"
  },
  {
    "text": "practical",
    "start": "1595279",
    "end": "1596960"
  },
  {
    "text": "so as as a solution for that",
    "start": "1596960",
    "end": "1600559"
  },
  {
    "text": "we the rook team has built the arbiter",
    "start": "1600559",
    "end": "1604240"
  },
  {
    "text": "the rbd mirror daemon so that we can",
    "start": "1604240",
    "end": "1606400"
  },
  {
    "text": "replicate block devices asynchronously",
    "start": "1606400",
    "end": "1608960"
  },
  {
    "text": "between clusters now you can have",
    "start": "1608960",
    "end": "1612960"
  },
  {
    "text": "two kubernetes environments and",
    "start": "1612960",
    "end": "1616320"
  },
  {
    "text": "connect each other's and all the blocked",
    "start": "1616320",
    "end": "1619279"
  },
  {
    "text": "devices so the pvcs will be replicated",
    "start": "1619279",
    "end": "1622640"
  },
  {
    "text": "obviously this works along with with",
    "start": "1622640",
    "end": "1624720"
  },
  {
    "text": "steps csi",
    "start": "1624720",
    "end": "1627600"
  },
  {
    "text": "and and work is uh is still still moving",
    "start": "1627600",
    "end": "1631120"
  },
  {
    "text": "ahead to to improve that support so now",
    "start": "1631120",
    "end": "1633279"
  },
  {
    "text": "we do support automating",
    "start": "1633279",
    "end": "1634799"
  },
  {
    "text": "automatic configuration of of peers",
    "start": "1634799",
    "end": "1638480"
  },
  {
    "text": "and where pr is basically a site last",
    "start": "1638480",
    "end": "1641360"
  },
  {
    "text": "but not least",
    "start": "1641360",
    "end": "1642399"
  },
  {
    "start": "1642000",
    "end": "1642000"
  },
  {
    "text": "the the stretch cluster accumulates uh",
    "start": "1642399",
    "end": "1645360"
  },
  {
    "text": "the stretch kubernetes cluster",
    "start": "1645360",
    "end": "1647039"
  },
  {
    "text": "which is essentially one cluster that is",
    "start": "1647039",
    "end": "1650159"
  },
  {
    "text": "stretched",
    "start": "1650159",
    "end": "1651120"
  },
  {
    "text": "one kubernetes crystal is stretched",
    "start": "1651120",
    "end": "1652799"
  },
  {
    "text": "across different zones",
    "start": "1652799",
    "end": "1654799"
  },
  {
    "text": "so the use case typically is that i have",
    "start": "1654799",
    "end": "1657360"
  },
  {
    "text": "two data centers but i only have two",
    "start": "1657360",
    "end": "1659600"
  },
  {
    "text": "i don't have three data centers which",
    "start": "1659600",
    "end": "1661279"
  },
  {
    "text": "will be really ideal",
    "start": "1661279",
    "end": "1663760"
  },
  {
    "text": "but i only have two and because ceph is",
    "start": "1663760",
    "end": "1667039"
  },
  {
    "text": "a quorum based election based",
    "start": "1667039",
    "end": "1670240"
  },
  {
    "text": "we need to have some",
    "start": "1670240",
    "end": "1673279"
  },
  {
    "text": "quorum resolution when we do certain",
    "start": "1673279",
    "end": "1675600"
  },
  {
    "text": "operations",
    "start": "1675600",
    "end": "1676559"
  },
  {
    "text": "in order to maintain the stability of",
    "start": "1676559",
    "end": "1678480"
  },
  {
    "text": "the cluster",
    "start": "1678480",
    "end": "1680399"
  },
  {
    "text": "so if we had three sides and it would be",
    "start": "1680399",
    "end": "1682640"
  },
  {
    "text": "really easy we would have",
    "start": "1682640",
    "end": "1684480"
  },
  {
    "text": "each one on one side and done but",
    "start": "1684480",
    "end": "1687200"
  },
  {
    "text": "because i only have",
    "start": "1687200",
    "end": "1688159"
  },
  {
    "text": "two and my first site is really limited",
    "start": "1688159",
    "end": "1691679"
  },
  {
    "text": "it can be a vm running on the cloud or",
    "start": "1691679",
    "end": "1694399"
  },
  {
    "text": "or it can be in the admin",
    "start": "1694399",
    "end": "1698080"
  },
  {
    "text": "desk something like this we",
    "start": "1698080",
    "end": "1701360"
  },
  {
    "text": "have to add a solution for sev to",
    "start": "1701360",
    "end": "1704640"
  },
  {
    "text": "work a bit smoothly",
    "start": "1704640",
    "end": "1708640"
  },
  {
    "text": "with latencies with higher latencies",
    "start": "1708640",
    "end": "1711200"
  },
  {
    "text": "basically",
    "start": "1711200",
    "end": "1712320"
  },
  {
    "text": "and we implemented this new mode uh in",
    "start": "1712320",
    "end": "1715440"
  },
  {
    "text": "rook so now you can",
    "start": "1715440",
    "end": "1717520"
  },
  {
    "text": "have a stretched kubernetes cluster in",
    "start": "1717520",
    "end": "1720399"
  },
  {
    "text": "an orbital zone",
    "start": "1720399",
    "end": "1721520"
  },
  {
    "text": "which will only contain one month to",
    "start": "1721520",
    "end": "1724880"
  },
  {
    "text": "resolve quorum and act as a member of",
    "start": "1724880",
    "end": "1727120"
  },
  {
    "text": "the quran",
    "start": "1727120",
    "end": "1728240"
  },
  {
    "text": "and perform elections and pursue",
    "start": "1728240",
    "end": "1731120"
  },
  {
    "text": "decision making",
    "start": "1731120",
    "end": "1733120"
  },
  {
    "text": "all of the other zones will have storage",
    "start": "1733120",
    "end": "1735520"
  },
  {
    "text": "available",
    "start": "1735520",
    "end": "1736480"
  },
  {
    "text": "and i guess that's it for today so it is",
    "start": "1736480",
    "end": "1739360"
  },
  {
    "text": "really easy to get involved",
    "start": "1739360",
    "end": "1740880"
  },
  {
    "text": "as soon as you reach rio then you will",
    "start": "1740880",
    "end": "1743679"
  },
  {
    "text": "have",
    "start": "1743679",
    "end": "1744640"
  },
  {
    "text": "links to everywhere basically um",
    "start": "1744640",
    "end": "1747679"
  },
  {
    "text": "the doc the slack channel and",
    "start": "1747679",
    "end": "1750960"
  },
  {
    "text": "how to contribute uh obviously on on",
    "start": "1750960",
    "end": "1753120"
  },
  {
    "text": "github thank you for",
    "start": "1753120",
    "end": "1755200"
  },
  {
    "text": "your attention uh thanks for being here",
    "start": "1755200",
    "end": "1757679"
  },
  {
    "text": "today",
    "start": "1757679",
    "end": "1758320"
  },
  {
    "text": "it was a pleasure talking with everybody",
    "start": "1758320",
    "end": "1760480"
  },
  {
    "text": "and uh",
    "start": "1760480",
    "end": "1761520"
  },
  {
    "text": "hopefully we will see everybody in",
    "start": "1761520",
    "end": "1763120"
  },
  {
    "text": "person next time have a good",
    "start": "1763120",
    "end": "1764960"
  },
  {
    "text": "day and stay safe bye-bye yes thank you",
    "start": "1764960",
    "end": "1768159"
  },
  {
    "text": "it's been great to be with you hopefully",
    "start": "1768159",
    "end": "1769760"
  },
  {
    "text": "you've learned a bit a little bit about",
    "start": "1769760",
    "end": "1771200"
  },
  {
    "text": "rook and we hope to",
    "start": "1771200",
    "end": "1772480"
  },
  {
    "text": "to see you in the rook slack or other",
    "start": "1772480",
    "end": "1774720"
  },
  {
    "text": "community",
    "start": "1774720",
    "end": "1776360"
  },
  {
    "text": "thanks",
    "start": "1776360",
    "end": "1779360"
  }
]