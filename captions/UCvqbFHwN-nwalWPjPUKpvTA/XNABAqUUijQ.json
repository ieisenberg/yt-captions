[
  {
    "text": "hello everybody My name is Jacob and I",
    "start": "640",
    "end": "2720"
  },
  {
    "text": "work at Harroxy Technologies You might",
    "start": "2720",
    "end": "5279"
  },
  {
    "text": "know of us as the company behind Haroxy",
    "start": "5279",
    "end": "8320"
  },
  {
    "text": "the legendary opensource software load",
    "start": "8320",
    "end": "10800"
  },
  {
    "text": "balancer that a lot of you are probably",
    "start": "10800",
    "end": "12840"
  },
  {
    "text": "using Today I'm going to be piggybacking",
    "start": "12840",
    "end": "15440"
  },
  {
    "text": "on the previous keynote about",
    "start": "15440",
    "end": "17400"
  },
  {
    "text": "performance and security and LLMs",
    "start": "17400",
    "end": "21279"
  },
  {
    "text": "because I think there are a lot of",
    "start": "21279",
    "end": "22640"
  },
  {
    "text": "topics we should talk about I talked to",
    "start": "22640",
    "end": "25279"
  },
  {
    "text": "a lot of our customers and they all say",
    "start": "25279",
    "end": "27359"
  },
  {
    "text": "we're all in on AI but I think the risks",
    "start": "27359",
    "end": "30480"
  },
  {
    "text": "of that are yet to be understood I think",
    "start": "30480",
    "end": "32960"
  },
  {
    "text": "we're kind of in the 2003 of OASP top 10",
    "start": "32960",
    "end": "36880"
  },
  {
    "text": "for web security right now in terms of",
    "start": "36880",
    "end": "39280"
  },
  {
    "text": "LLM security So when they talk about I'm",
    "start": "39280",
    "end": "43040"
  },
  {
    "text": "being allin on AI security or an AI the",
    "start": "43040",
    "end": "47760"
  },
  {
    "text": "first step they usually think about is",
    "start": "47760",
    "end": "49520"
  },
  {
    "text": "well we're going to build an AI gateway",
    "start": "49520",
    "end": "51840"
  },
  {
    "text": "And so what's an AI gateway well it's an",
    "start": "51840",
    "end": "53920"
  },
  {
    "text": "API gateway right we've all built one at",
    "start": "53920",
    "end": "56960"
  },
  {
    "text": "this point or most of the projects in",
    "start": "56960",
    "end": "59840"
  },
  {
    "text": "load balancers have built one API",
    "start": "59840",
    "end": "62079"
  },
  {
    "text": "gateway right now an AI gateway So we're",
    "start": "62079",
    "end": "64878"
  },
  {
    "text": "going to do some authentication We're",
    "start": "64879",
    "end": "66320"
  },
  {
    "text": "going to do rate limiting We're going to",
    "start": "66320",
    "end": "67920"
  },
  {
    "text": "do PII detection or maybe PII extraction",
    "start": "67920",
    "end": "70960"
  },
  {
    "text": "and prompt routing before we send the",
    "start": "70960",
    "end": "73360"
  },
  {
    "text": "data to an inference engine which is",
    "start": "73360",
    "end": "75439"
  },
  {
    "text": "ultimately an HTTP API But I think one",
    "start": "75439",
    "end": "78240"
  },
  {
    "text": "of the things that's missing there or",
    "start": "78240",
    "end": "80159"
  },
  {
    "text": "possibly missing is prompt security and",
    "start": "80159",
    "end": "83280"
  },
  {
    "text": "that's really you know the ignore all",
    "start": "83280",
    "end": "85600"
  },
  {
    "text": "previous instructions that doesn't work",
    "start": "85600",
    "end": "87920"
  },
  {
    "text": "anymore or a time bandit attack that",
    "start": "87920",
    "end": "90640"
  },
  {
    "text": "used to work with open AI until recently",
    "start": "90640",
    "end": "93600"
  },
  {
    "text": "So there are obviously solutions to that",
    "start": "93600",
    "end": "96240"
  },
  {
    "text": "For example there's a prompt guard model",
    "start": "96240",
    "end": "98479"
  },
  {
    "text": "and a llama guard model from meta",
    "start": "98479",
    "end": "100320"
  },
  {
    "text": "There's a shield gemma from Google And",
    "start": "100320",
    "end": "102640"
  },
  {
    "text": "in the end many of these are built on",
    "start": "102640",
    "end": "104960"
  },
  {
    "text": "some kind of a variation of dberta",
    "start": "104960",
    "end": "106920"
  },
  {
    "text": "classification It's a set of models So",
    "start": "106920",
    "end": "109759"
  },
  {
    "text": "they are actually large language models",
    "start": "109759",
    "end": "112320"
  },
  {
    "text": "that are classifying You ask is this",
    "start": "112320",
    "end": "114880"
  },
  {
    "text": "prompt safe yes or no and they answer",
    "start": "114880",
    "end": "117040"
  },
  {
    "text": "yes or no So it's ultimately",
    "start": "117040",
    "end": "118880"
  },
  {
    "text": "classification problem So we're going to",
    "start": "118880",
    "end": "121680"
  },
  {
    "text": "add prompt security to the AI gateway",
    "start": "121680",
    "end": "124079"
  },
  {
    "text": "and basically run the model itself in",
    "start": "124079",
    "end": "126640"
  },
  {
    "text": "the AI gateway Um so I did that and I",
    "start": "126640",
    "end": "130080"
  },
  {
    "text": "ran these models inside an AI gateway",
    "start": "130080",
    "end": "132720"
  },
  {
    "text": "inside a load balancer and ultimately",
    "start": "132720",
    "end": "134959"
  },
  {
    "text": "what I found and this is what I want to",
    "start": "134959",
    "end": "137040"
  },
  {
    "text": "talk about is it's pretty slow So if you",
    "start": "137040",
    "end": "140800"
  },
  {
    "text": "run a model like this on a G6X large AWS",
    "start": "140800",
    "end": "144080"
  },
  {
    "text": "instance which is ultimately a pretty",
    "start": "144080",
    "end": "146000"
  },
  {
    "text": "big instance at this point if you start",
    "start": "146000",
    "end": "148560"
  },
  {
    "text": "approaching 500 tokens it takes",
    "start": "148560",
    "end": "150800"
  },
  {
    "text": "somewhere between 150 to 200",
    "start": "150800",
    "end": "152959"
  },
  {
    "text": "milliseconds to process the prompt And",
    "start": "152959",
    "end": "155680"
  },
  {
    "text": "most prompts are bigger and most of",
    "start": "155680",
    "end": "158560"
  },
  {
    "text": "these models have a context window of",
    "start": "158560",
    "end": "160319"
  },
  {
    "text": "about 500 tokens So if your prompt is",
    "start": "160319",
    "end": "163280"
  },
  {
    "text": "2,000 tokens you have to do this four",
    "start": "163280",
    "end": "165120"
  },
  {
    "text": "times And that's a second of time you're",
    "start": "165120",
    "end": "168080"
  },
  {
    "text": "processing That's that's lifetimes in a",
    "start": "168080",
    "end": "171120"
  },
  {
    "text": "load balancer world And if you start",
    "start": "171120",
    "end": "173920"
  },
  {
    "text": "doing this in parallel like a proper AI",
    "start": "173920",
    "end": "176959"
  },
  {
    "text": "gateway it gets worse So here's if you",
    "start": "176959",
    "end": "180560"
  },
  {
    "text": "can see I ran a non optimized model So",
    "start": "180560",
    "end": "183360"
  },
  {
    "text": "basic transformers and then an optimized",
    "start": "183360",
    "end": "185519"
  },
  {
    "text": "model with an inference engine And on",
    "start": "185519",
    "end": "188159"
  },
  {
    "text": "this instance you cannot almost ever",
    "start": "188159",
    "end": "190000"
  },
  {
    "text": "reach more than 60 requests a second And",
    "start": "190000",
    "end": "192560"
  },
  {
    "text": "once you start getting to a concurrency",
    "start": "192560",
    "end": "194239"
  },
  {
    "text": "of let's say eight concurrent requests",
    "start": "194239",
    "end": "197599"
  },
  {
    "text": "you will never reach over 40 basically",
    "start": "197599",
    "end": "200640"
  },
  {
    "text": "So there's a lot of work to be done in",
    "start": "200640",
    "end": "203040"
  },
  {
    "text": "making this faster There are",
    "start": "203040",
    "end": "205200"
  },
  {
    "text": "optimization strategies We can run an",
    "start": "205200",
    "end": "207920"
  },
  {
    "text": "optimized inference engines I did that",
    "start": "207920",
    "end": "210000"
  },
  {
    "text": "and you can get about 30% You can enable",
    "start": "210000",
    "end": "212959"
  },
  {
    "text": "token caching but ultimately token",
    "start": "212959",
    "end": "215360"
  },
  {
    "text": "caching as we know it right now is meant",
    "start": "215360",
    "end": "217840"
  },
  {
    "text": "for generative AI And we're not doing",
    "start": "217840",
    "end": "220400"
  },
  {
    "text": "generative AI we're doing classification",
    "start": "220400",
    "end": "222959"
  },
  {
    "text": "We're classifying the model So it's out",
    "start": "222959",
    "end": "226159"
  },
  {
    "text": "there to know if this is useful long",
    "start": "226159",
    "end": "228080"
  },
  {
    "text": "term There are some advanced techniques",
    "start": "228080",
    "end": "230480"
  },
  {
    "text": "I've tried these as well and it works",
    "start": "230480",
    "end": "232560"
  },
  {
    "text": "Like you can filter for some of the",
    "start": "232560",
    "end": "234239"
  },
  {
    "text": "basic prompts and bad words with text",
    "start": "234239",
    "end": "236640"
  },
  {
    "text": "filtering The only problem is if you",
    "start": "236640",
    "end": "239599"
  },
  {
    "text": "make a typo in the filter or in your",
    "start": "239599",
    "end": "241920"
  },
  {
    "text": "text the text filter will no longer work",
    "start": "241920",
    "end": "244640"
  },
  {
    "text": "while the LM will interpret it correctly",
    "start": "244640",
    "end": "246720"
  },
  {
    "text": "So all you have to do is mangle a few",
    "start": "246720",
    "end": "248480"
  },
  {
    "text": "words and it's just going to work So I",
    "start": "248480",
    "end": "250799"
  },
  {
    "text": "think there are a few lessons learned We",
    "start": "250799",
    "end": "252239"
  },
  {
    "text": "need to innovate new techniques using",
    "start": "252239",
    "end": "254319"
  },
  {
    "text": "the tools we have and we have a few",
    "start": "254319",
    "end": "256280"
  },
  {
    "text": "tools Two using AI itself to secure AI",
    "start": "256280",
    "end": "260560"
  },
  {
    "text": "in the load balancer works but it's",
    "start": "260560",
    "end": "262720"
  },
  {
    "text": "still out there if it's viable It's",
    "start": "262720",
    "end": "264560"
  },
  {
    "text": "something that we are researching We",
    "start": "264560",
    "end": "266080"
  },
  {
    "text": "might need to research all together some",
    "start": "266080",
    "end": "268320"
  },
  {
    "text": "smaller models that can run on a load",
    "start": "268320",
    "end": "270639"
  },
  {
    "text": "balancer and can run much quicker And",
    "start": "270639",
    "end": "273759"
  },
  {
    "text": "three I think that the AI gateways are",
    "start": "273759",
    "end": "276160"
  },
  {
    "text": "necessary but the security is is",
    "start": "276160",
    "end": "278400"
  },
  {
    "text": "evolving In the end as I said I think",
    "start": "278400",
    "end": "280960"
  },
  {
    "text": "we're in the 2003 of OS top 10 right now",
    "start": "280960",
    "end": "284320"
  },
  {
    "text": "in AI security Everybody's trying",
    "start": "284320",
    "end": "286560"
  },
  {
    "text": "something Everybody's doing it but we",
    "start": "286560",
    "end": "289199"
  },
  {
    "text": "don't know what's going to work a few",
    "start": "289199",
    "end": "290639"
  },
  {
    "text": "years from now because it's really hard",
    "start": "290639",
    "end": "292560"
  },
  {
    "text": "to keep up So thank you so much If you",
    "start": "292560",
    "end": "294720"
  },
  {
    "text": "have any questions please come to the",
    "start": "294720",
    "end": "296560"
  },
  {
    "text": "booth I would be happy to talk about",
    "start": "296560",
    "end": "297919"
  },
  {
    "text": "this a little bit more",
    "start": "297919",
    "end": "301560"
  },
  {
    "text": "Thank you Jagger",
    "start": "303199",
    "end": "306600"
  }
]