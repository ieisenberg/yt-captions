[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "hey everybody welcome to my talk let me get started I'll be chatting a little bit about it scaling your service on",
    "start": "30",
    "end": "6720"
  },
  {
    "text": "what matters which spoiler I'm going to claim as latency but before we get",
    "start": "6720",
    "end": "12210"
  },
  {
    "text": "started since I'm going to be talking about auto scaling if my slide moves",
    "start": "12210",
    "end": "19080"
  },
  {
    "text": "forward here I have got a application up that I'm hoping everyone is gonna do s",
    "start": "19080",
    "end": "25500"
  },
  {
    "text": "me on I'll just show it here while I'm waiting please vote on what the best",
    "start": "25500",
    "end": "31349"
  },
  {
    "text": "editor in the world is I might have gamed this I'm expecting everybody to vote only for Emacs actually I'm a",
    "start": "31349",
    "end": "37770"
  },
  {
    "text": "little sad that the votes are going the way they are but please this is actually the live app that we're going to be",
    "start": "37770",
    "end": "44340"
  },
  {
    "text": "auto-scaling here I've got it on the",
    "start": "44340",
    "end": "51449"
  },
  {
    "text": "slide just give it a second here ah there we go but first before we get started now that",
    "start": "51449",
    "end": "58440"
  },
  {
    "start": "54000",
    "end": "84000"
  },
  {
    "text": "I've introduced the app let me tell you a little bit about myself my name is Thomas ramble burg I'm a software engineer at buoyant and what that means",
    "start": "58440",
    "end": "65100"
  },
  {
    "text": "is that I'm spoiled rotten because I get to work on open source all day every day and contribute to linker D which is",
    "start": "65100",
    "end": "72360"
  },
  {
    "text": "going to be part of this talk if anyone's remotely interested in hearing grants about pretty much",
    "start": "72360",
    "end": "78030"
  },
  {
    "text": "anything in the world follow me on Twitter my Twitter handles up there and here we go",
    "start": "78030",
    "end": "84590"
  },
  {
    "start": "84000",
    "end": "243000"
  },
  {
    "text": "KCl 5di oh please go get your votes in if anyone wants to try and do s this I'm",
    "start": "84590",
    "end": "91130"
  },
  {
    "text": "very excited so now that we've got that introduced I'd like to talk a little bit",
    "start": "91130",
    "end": "98100"
  },
  {
    "text": "about what auto-scaling is in kubernetes because it's actually a little complex there's three major types of",
    "start": "98100",
    "end": "105329"
  },
  {
    "text": "auto-scaling kubernetes one is the cluster autoscaler that goes and creates new nodes for you and grows and shrinks",
    "start": "105329",
    "end": "112320"
  },
  {
    "text": "the pool based off of the request limits on your pods and containers basically if",
    "start": "112320",
    "end": "118290"
  },
  {
    "text": "you run out of resources kubernetes can't schedule something your cloud provider will go create your new nodes",
    "start": "118290",
    "end": "123439"
  },
  {
    "text": "what this talk is going to be about is the horizontal pod autoscaler that goes",
    "start": "123439",
    "end": "128759"
  },
  {
    "text": "and takes a metric by default the CPU and memory and adjusts the replicas for you to go",
    "start": "128759",
    "end": "136319"
  },
  {
    "text": "and increase the number of replicas on your deployments for example then we have the vertical pod autoscaler which",
    "start": "136319",
    "end": "142770"
  },
  {
    "text": "goes and does that vertically instead of horizontally it'll go monitor the CPU and memory usage and adjust your",
    "start": "142770",
    "end": "148410"
  },
  {
    "text": "resource limits so that your pod fits in it this actually solves an awesome problem of it's really hard to figure out what",
    "start": "148410",
    "end": "156209"
  },
  {
    "text": "the limits and resource requests are supposed to be and this will help you do that but let's talk a little bit about",
    "start": "156209",
    "end": "162150"
  },
  {
    "text": "what horizontal pod autoscaler is and where it gets the metrics what's up oh",
    "start": "162150",
    "end": "173930"
  },
  {
    "text": "the screen is jumping up and down crazy",
    "start": "173930",
    "end": "181040"
  },
  {
    "text": "well we'll just continue on and I apologize okay so three types of metrics",
    "start": "181700",
    "end": "189780"
  },
  {
    "text": "in the kubernetes api number one is the metric server or the metrics api that is",
    "start": "189780",
    "end": "195600"
  },
  {
    "text": "in works off of the metric server which is now integrated into the couplet and that gets you all of your system metrics",
    "start": "195600",
    "end": "202620"
  },
  {
    "text": "that you would expect that CPU and memory free out of the box it's a really great way to start doing monitoring on",
    "start": "202620",
    "end": "208260"
  },
  {
    "text": "your cluster we then have the custom metrics API that's what I'm going to be talking about and that goes and lets you",
    "start": "208260",
    "end": "215100"
  },
  {
    "text": "do it on any metric that you want obviously we're going to be taking a look at latency today and then you've",
    "start": "215100",
    "end": "221430"
  },
  {
    "text": "got external metrics which is something external to a pod or a namespace so this",
    "start": "221430",
    "end": "227160"
  },
  {
    "text": "could be the ingress that you have or since we're doing a talk on auto-scaling imagine a cron job that goes and sets a",
    "start": "227160",
    "end": "234000"
  },
  {
    "text": "bit to auto scale when you have a flash sale about to happen so now you can go and do scheduled auto scaling which is",
    "start": "234000",
    "end": "241440"
  },
  {
    "text": "pretty awesome so let me get started with why CPU is not necessarily what you",
    "start": "241440",
    "end": "248880"
  },
  {
    "start": "243000",
    "end": "350000"
  },
  {
    "text": "want to do the first obvious one is not every workload is CPU bound you really have to figure out what your workload is",
    "start": "248880",
    "end": "255570"
  },
  {
    "text": "sometimes it's i/o bound sometimes it's memory bound you really have to guess the biggest thing for me though is",
    "start": "255570",
    "end": "262580"
  },
  {
    "text": "to auto-scale you have to put the threshold below 100% so you have to say",
    "start": "262580",
    "end": "268430"
  },
  {
    "text": "ok all autoscale but it will only be at 90% which means that you now have 10%",
    "start": "268430",
    "end": "274090"
  },
  {
    "text": "unused capacity in your cluster isn't auto-scaling all about being cheap and getting the best use out of your",
    "start": "274090",
    "end": "280460"
  },
  {
    "text": "resources another good reason CPUs are different I'm sure all of us have experienced it depending on a region",
    "start": "280460",
    "end": "287210"
  },
  {
    "text": "depending on your instance type depending on your cloud provider a CPU is kind of a random guess as to whether",
    "start": "287210",
    "end": "293599"
  },
  {
    "text": "you're going to get good performance or not and then probably the biggest thing for me is that orchestrated environments",
    "start": "293599",
    "end": "300050"
  },
  {
    "text": "are super complex imagine having someone schedule a Big Data job right next to",
    "start": "300050",
    "end": "305120"
  },
  {
    "text": "your service that's going to destroy the disk it's gonna kill the CPU and even though you're using your resources",
    "start": "305120",
    "end": "310819"
  },
  {
    "text": "correctly your response times are going to go out the window and so you really need to have a way to look at it since",
    "start": "310819",
    "end": "318169"
  },
  {
    "text": "I'm a bit of a CPU utilization geek I actually have two notes at the bottom of",
    "start": "318169",
    "end": "323750"
  },
  {
    "text": "this one of them is CPU utilization is wrong it's written by Brendan Gregg and he actually explains CPU utilization",
    "start": "323750",
    "end": "330500"
  },
  {
    "text": "really is an approximation and you need to think about it a little bit more complexly the other one is by Adrienne",
    "start": "330500",
    "end": "337009"
  },
  {
    "text": "Crockford and if these claims utilization is completely useless as a metric so those are two good reasons to",
    "start": "337009",
    "end": "343039"
  },
  {
    "text": "think through why the out-of-the-box CPU is not necessarily the thing that you should be using to auto scale on so then",
    "start": "343039",
    "end": "350930"
  },
  {
    "start": "350000",
    "end": "496000"
  },
  {
    "text": "let's talk a little bit about memory memory is actually great but it's SuperDuper specific about your workload",
    "start": "350930",
    "end": "356870"
  },
  {
    "text": "auto scale on memory for memcache of course Redis of course but your normal",
    "start": "356870",
    "end": "361969"
  },
  {
    "text": "API service it's probably not going to work so I've thrown out the normal",
    "start": "361969",
    "end": "367520"
  },
  {
    "text": "monitoring what can you actually scale on let's talk about what you should be monitoring for your services the Google",
    "start": "367520",
    "end": "375589"
  },
  {
    "text": "sre handbook talks a lot about the golden signals and these four golden signals are what they claim are the only",
    "start": "375589",
    "end": "381650"
  },
  {
    "text": "things that you absolutely need to monitor on your service sure you can implement other ones but these are the",
    "start": "381650",
    "end": "386659"
  },
  {
    "text": "four that they say are the most important we have latency which is the time that a customer requires to get a",
    "start": "386659",
    "end": "393380"
  },
  {
    "text": "response there is traffic the Quest's per second that are going through there are errors this is the",
    "start": "393380",
    "end": "399400"
  },
  {
    "text": "number of 500s for example that you're returning or unknown if you're in the GRP sea world and then there is",
    "start": "399400",
    "end": "405640"
  },
  {
    "text": "saturation at which point the service degrades so heavily just because you've",
    "start": "405640",
    "end": "410680"
  },
  {
    "text": "saturated it you need to track these four things and I've got a link there to",
    "start": "410680",
    "end": "416740"
  },
  {
    "text": "the page actually in the book if anyone is interested so we're talking about",
    "start": "416740",
    "end": "421840"
  },
  {
    "text": "latency I've kind of said obviously you should be measuring latency why don't we use that as a way to do auto scaling the",
    "start": "421840",
    "end": "428770"
  },
  {
    "text": "other thing to think about and and this actually blew my mind when I found it the link at the bottom is to an article",
    "start": "428770",
    "end": "435010"
  },
  {
    "text": "called everything you know about latency is wrong tail latency is super-important",
    "start": "435010",
    "end": "440440"
  },
  {
    "text": "in fact I've got a chart here on the right that is the number of requests for",
    "start": "440440",
    "end": "445540"
  },
  {
    "text": "popular websites there are a hundred ninety requests fired off when you load amazon.com if you're looking at AP 99",
    "start": "445540",
    "end": "453310"
  },
  {
    "text": "which is the 99th percentile latency what that means is we've thrown out the 99 fastest latency measurements and take",
    "start": "453310",
    "end": "460960"
  },
  {
    "text": "the one slowest you have an 85 percent chance of having your customer on a",
    "start": "460960",
    "end": "466780"
  },
  {
    "text": "single load of amazon.com hit your P 99 latency so they're gonna have a bad experience 85% of the time you've got",
    "start": "466780",
    "end": "474610"
  },
  {
    "text": "Google and that's just the basic Google search page you're guaranteed 25 percent",
    "start": "474610",
    "end": "480250"
  },
  {
    "text": "of the time to hit a P 99 latency on at least one of those requests the users",
    "start": "480250",
    "end": "485500"
  },
  {
    "text": "see responses they don't see your CPU so that's what they're going to be judging your service on and latency is not",
    "start": "485500",
    "end": "491530"
  },
  {
    "text": "actually normally distributed it's kind of random you need to go figure it out",
    "start": "491530",
    "end": "496680"
  },
  {
    "start": "496000",
    "end": "549000"
  },
  {
    "text": "so here is a little bit of a background on what I'm going to be doing from a",
    "start": "496680",
    "end": "502570"
  },
  {
    "text": "live demo perspective I'm going to show you measuring the latency of a service we're gonna expose the custom metrics",
    "start": "502570",
    "end": "508510"
  },
  {
    "text": "and then we're going to do some auto scaling to talk about how to measure the latency of a service though I'd like to",
    "start": "508510",
    "end": "514419"
  },
  {
    "text": "talk a little bit about link rudy and the reason for that is how many of you",
    "start": "514419",
    "end": "519760"
  },
  {
    "text": "here actually I'm interested have latency based monitoring on their services today that's pretty awesome is",
    "start": "519760",
    "end": "528250"
  },
  {
    "text": "it for all of the applications in your organization or just at the ingress anyways it's kind of",
    "start": "528250",
    "end": "535670"
  },
  {
    "text": "hard I've done the instrumentation you have to go figure out Prometheus you have to go hook it up it takes a while so I've got the world's simplest Python",
    "start": "535670",
    "end": "542810"
  },
  {
    "text": "application serving up the leaderboard that I hope you're all are still voting for and linker D is going to be used to",
    "start": "542810",
    "end": "549050"
  },
  {
    "start": "549000",
    "end": "835000"
  },
  {
    "text": "instrument it so what is link or D link rudy is a open source service mesh it's a CNC F member project used in",
    "start": "549050",
    "end": "556370"
  },
  {
    "text": "production with quite a few people but that doesn't really explain what it is or why you need it so at its most basic",
    "start": "556370",
    "end": "563740"
  },
  {
    "text": "link Rudy is nothing more than a proxy that gets scheduled alongside your application because of the magic of IP",
    "start": "563740",
    "end": "570560"
  },
  {
    "text": "tables it receives all of the incoming traffic and all of the outgoing traffic and because it's at layer 7 we can go",
    "start": "570560",
    "end": "577820"
  },
  {
    "text": "and instrument all of the requests and responses this isn't TCP traffic this is actually what requests are and how long",
    "start": "577820",
    "end": "584720"
  },
  {
    "text": "responses take but that's only half of the picture to do a full monitoring",
    "start": "584720",
    "end": "590690"
  },
  {
    "text": "setup you actually need a control plane to go along with your data plan so we ship with Gravano dashboards and",
    "start": "590690",
    "end": "597560"
  },
  {
    "text": "Prometheus out of the box so with a single install that I'm about to show you you get the latency and all of the",
    "start": "597560",
    "end": "604310"
  },
  {
    "text": "four golden signals immediately for your services really quite quickly so let's",
    "start": "604310",
    "end": "610459"
  },
  {
    "text": "go do that so I've downloaded the linker de CLI",
    "start": "610459",
    "end": "617540"
  },
  {
    "text": "onto my laptop already but we provide this thing called link ready check which",
    "start": "617540",
    "end": "623449"
  },
  {
    "text": "will pre-flight and post-flight your cluster I have gone and installed so many kubernetes packages on my cluster",
    "start": "623449",
    "end": "629959"
  },
  {
    "text": "and they work half the time sometimes ish I'm not usually sure and so here we",
    "start": "629959",
    "end": "635990"
  },
  {
    "text": "go with the tool that tells you ok you have all the permissions you're ready to go which at which point you can type",
    "start": "635990",
    "end": "641540"
  },
  {
    "text": "link your d install that outputs the gamal that you can then pipe through cuddle the reason why we do this is is",
    "start": "641540",
    "end": "649430"
  },
  {
    "text": "that you now can go validate this before you pass it in to coop cuddle there's nothing magical happening on your",
    "start": "649430",
    "end": "654560"
  },
  {
    "text": "cluster you're not curl bashing something",
    "start": "654560",
    "end": "658660"
  },
  {
    "text": "so we'll put that onto our cluster here",
    "start": "661370",
    "end": "666890"
  },
  {
    "text": "and while the internet gods smile on me",
    "start": "669170",
    "end": "674550"
  },
  {
    "text": "I hope there we go what we'll see is we're just applying",
    "start": "674550",
    "end": "680610"
  },
  {
    "text": "all of the things that I explained for the control plane this is the Prometheus",
    "start": "680610",
    "end": "685710"
  },
  {
    "text": "the Griffin we have a web dashboard that ships and a couple other things and then once this is done we also have a link",
    "start": "685710",
    "end": "693120"
  },
  {
    "text": "ready check command for post flighting and this is actually going to hang up because it hasn't started the containers",
    "start": "693120",
    "end": "699090"
  },
  {
    "text": "yet we'll come back to this but this is a way to validate that your install was successful and is actually working so",
    "start": "699090",
    "end": "704790"
  },
  {
    "text": "you don't need to go debug weird edge cases so now that's the control plane",
    "start": "704790",
    "end": "710460"
  },
  {
    "text": "it's going to be up and running I promise let's talk a little bit about the data plane and getting it into my application what you want to do there is",
    "start": "710460",
    "end": "717360"
  },
  {
    "text": "inject the link or de sidecar proxy and in it container into your application",
    "start": "717360",
    "end": "722850"
  },
  {
    "text": "you could go get your e amyl and do it but I'm really lazy and I'm just going",
    "start": "722850",
    "end": "727950"
  },
  {
    "text": "to get the gamal from the kubernetes api server and pipe that through so here I",
    "start": "727950",
    "end": "737100"
  },
  {
    "text": "am getting the ammo and then I'm just going to inject and here you'll see if I",
    "start": "737100",
    "end": "743160"
  },
  {
    "text": "scroll up a little bit we've got the IP tables init container and we have the",
    "start": "743160",
    "end": "748950"
  },
  {
    "text": "proxy injected we've also done the pre and post-flight checks on the data layer",
    "start": "748950",
    "end": "754410"
  },
  {
    "text": "as well as the control plane so you can validate that your gamal is correctly formed it doesn't have any weird",
    "start": "754410",
    "end": "760680"
  },
  {
    "text": "configuration it's ready to go so if we pipe that on through this is going to do",
    "start": "760680",
    "end": "768930"
  },
  {
    "text": "a rolling deploy of my application I hope people are loading it because I'm excited to see if it stays up and if we",
    "start": "768930",
    "end": "776730"
  },
  {
    "text": "do a link ready check we'll make sure that the control plane is up and running which it is and then if we do a link or",
    "start": "776730",
    "end": "782580"
  },
  {
    "text": "do you check proxy",
    "start": "782580",
    "end": "785900"
  },
  {
    "text": "we'll see that that container is still rolling but it will be up soon",
    "start": "790420",
    "end": "795920"
  },
  {
    "text": "fact it just came up so now let's go take a look at the dashboard and get a",
    "start": "795920",
    "end": "807260"
  },
  {
    "text": "feel for what's actually happening on our cluster so here you'll see that we're actually doing about 500 RPS I",
    "start": "807260",
    "end": "814100"
  },
  {
    "text": "have a load generator that I added that's already doing a thousand RPS and you'll see that our p99 latency is three",
    "start": "814100",
    "end": "821089"
  },
  {
    "text": "milliseconds so let's open up the graph on a dashboard to have it there to be a",
    "start": "821089",
    "end": "826490"
  },
  {
    "text": "reference nice 1300 RPS is where it's at",
    "start": "826490",
    "end": "833450"
  },
  {
    "text": "cool okay so I've showed you how to measure the latency of a service now",
    "start": "833450",
    "end": "839839"
  },
  {
    "start": "835000",
    "end": "1088000"
  },
  {
    "text": "it's time to talk a little bit about what it takes to expose the custom metrics for kubernetes back in the heady",
    "start": "839839",
    "end": "847760"
  },
  {
    "text": "days of 2017 when kubernetes was young and a small project the team spent quite",
    "start": "847760",
    "end": "853370"
  },
  {
    "text": "a bit of time trying to figure out how to extend the API so pre CR DS they introduced this idea of the API server",
    "start": "853370",
    "end": "859850"
  },
  {
    "text": "as an aggregator the API server being the single place that you want to talk to to figure out all of your data and to",
    "start": "859850",
    "end": "865670"
  },
  {
    "text": "do that what they did is introduced a API service object so I've got the resource up here and basically this is",
    "start": "865670",
    "end": "873200"
  },
  {
    "text": "just saying I would like to proxy everything to the custom metrics API back to a Prometheus adapter which is",
    "start": "873200",
    "end": "879560"
  },
  {
    "text": "about what I'm going to show you right now so here's the Prometheus adapter it",
    "start": "879560",
    "end": "885649"
  },
  {
    "text": "takes the custom metrics API and literally just translate it translates",
    "start": "885649",
    "end": "891920"
  },
  {
    "text": "it into Prometheus queries so I've got the API at the top and I have the",
    "start": "891920",
    "end": "896959"
  },
  {
    "text": "Prometheus query that we are translating to in the bottom and so you can almost think of this as just a simple",
    "start": "896959",
    "end": "903230"
  },
  {
    "text": "translator it's going to run one pot on our cluster and be ready to go that way so here is kind of an architecture it's",
    "start": "903230",
    "end": "911269"
  },
  {
    "text": "fun to talk about this and show how all of the kubernetes components actually work together so here we've got a cube",
    "start": "911269",
    "end": "916670"
  },
  {
    "text": "CTL apply calling the API server that gonna be calling the custom metrics API which is going to get proxy back to the",
    "start": "916670",
    "end": "923690"
  },
  {
    "text": "Prometheus adapter which is then going to hit Prometheus for query really pretty simple so why don't I show you",
    "start": "923690",
    "end": "930589"
  },
  {
    "text": "how to do that so if we go back here I'm going to be using helm to do this",
    "start": "930589",
    "end": "937520"
  },
  {
    "text": "there's a stable Prometheus adapter package that's absolutely fantastic and let me show you a little bit of what it",
    "start": "937520",
    "end": "944000"
  },
  {
    "text": "requires for me to actually configure",
    "start": "944000",
    "end": "949400"
  },
  {
    "text": "that so here I have pretty simple I tell it where my Prometheus is and I've",
    "start": "949400",
    "end": "955130"
  },
  {
    "text": "configured three queries I've got a 50th percentile a 95th percentile and a 99th",
    "start": "955130",
    "end": "960380"
  },
  {
    "text": "percentile out of the box they actually have quite a few more metrics I've stripped this down for the demo so that",
    "start": "960380",
    "end": "965450"
  },
  {
    "text": "we don't get overwhelmed with the sheer number of awesome metrics that you can get out of this so to get it installed",
    "start": "965450",
    "end": "972020"
  },
  {
    "text": "I'll do a helm install here and pass in",
    "start": "972020",
    "end": "984500"
  },
  {
    "text": "my configuration that's going to render",
    "start": "984500",
    "end": "990680"
  },
  {
    "text": "some templates get them applied to the cluster if I scroll up here you'll see that there is an API service that's been",
    "start": "990680",
    "end": "999800"
  },
  {
    "text": "added so that's going to be what tells the kubernetes api to proxy it back to the Prometheus adapter and a couple",
    "start": "999800",
    "end": "1006550"
  },
  {
    "text": "other things including a command so before I run that let's go take a look at what the what the application is",
    "start": "1006550",
    "end": "1018580"
  },
  {
    "text": "doing through the linker D CLI and so",
    "start": "1018580",
    "end": "1026740"
  },
  {
    "text": "here we've got looks like we're doing about 1200 RPS right now with an interesting latency of 73 milliseconds",
    "start": "1026740",
    "end": "1032678"
  },
  {
    "text": "this is going to be pretty exciting so let me show you now what the custom metrics API looks like and because I",
    "start": "1032679",
    "end": "1041709"
  },
  {
    "text": "don't like to remember this one here we've got a query you'll see the route",
    "start": "1041709",
    "end": "1048100"
  },
  {
    "text": "right here we go and say tell us all of your metrics and if we look through this we've got",
    "start": "1048100",
    "end": "1054970"
  },
  {
    "text": "the pods 50th percentile latency I promised that the 99th there it is right",
    "start": "1054970",
    "end": "1059980"
  },
  {
    "text": "there and then I've got this other query here that's gonna go and say tell me all of your values and you'll see that it's",
    "start": "1059980",
    "end": "1066910"
  },
  {
    "text": "this insanely large value remember that Prometheus to avoid any kind excuse me remember that kubernetes",
    "start": "1066910",
    "end": "1074650"
  },
  {
    "text": "to avoid any kind of floating point math multiplies everything times a thousand and so this is basically what is that 20",
    "start": "1074650",
    "end": "1082360"
  },
  {
    "text": "milliseconds give or take and that's pretty much it I've now exposed my metrics so we're now measuring the",
    "start": "1082360",
    "end": "1091300"
  },
  {
    "start": "1088000",
    "end": "1277000"
  },
  {
    "text": "latency of our service just by injecting link or D into our data plane no code",
    "start": "1091300",
    "end": "1096340"
  },
  {
    "text": "changes nothing special happened there I'm exposing the custom metrics through",
    "start": "1096340",
    "end": "1101350"
  },
  {
    "text": "the API service through the Prometheus adapter and now let's actually talk about doing the auto scaling and to do",
    "start": "1101350",
    "end": "1108520"
  },
  {
    "text": "that let's talk about what the horizontal pod autoscaler does for real so what it does is every 30 seconds as a",
    "start": "1108520",
    "end": "1115870"
  },
  {
    "text": "controller on your kubernetes cluster it queries the API server that that's it",
    "start": "1115870",
    "end": "1120930"
  },
  {
    "text": "it's so simple it's awesome so it queries the API server every 30 seconds that eventually gets proxy back to our",
    "start": "1120930",
    "end": "1128380"
  },
  {
    "text": "applications that have been fed up into Prometheus and if your stat is out of",
    "start": "1128380",
    "end": "1134110"
  },
  {
    "text": "its range well it'll auto scale it for you the algorithm is pretty simple I recommend anybody go look at the",
    "start": "1134110",
    "end": "1140800"
  },
  {
    "text": "documentation if you're interested the whole thing is just kind of simple it's really nice so let's go do some auto",
    "start": "1140800",
    "end": "1149020"
  },
  {
    "text": "scaling and to do that what I'm gonna do is first show you what the policy looks",
    "start": "1149020",
    "end": "1155080"
  },
  {
    "text": "like so here we have a HPA e resource and I'm saying I want a min replicas of",
    "start": "1155080",
    "end": "1161110"
  },
  {
    "text": "one max of 100 and target 8 milliseconds I'm gonna apply this and we're gonna see",
    "start": "1161110",
    "end": "1168520"
  },
  {
    "text": "some auto scaling happening",
    "start": "1168520",
    "end": "1171660"
  },
  {
    "text": "so while that is doing its thing let's",
    "start": "1178340",
    "end": "1184260"
  },
  {
    "text": "go take a look at the blinker de CLI again take a look at what our Layton",
    "start": "1184260",
    "end": "1189510"
  },
  {
    "text": "sees our whoo oh boy you guys are really killing me this is awesome so we're",
    "start": "1189510",
    "end": "1195660"
  },
  {
    "text": "doing it's pretty obvious this needs to get auto scaled at this point right away there's no question but I bet you that",
    "start": "1195660",
    "end": "1203490"
  },
  {
    "text": "we're actually already doing that so if we go do a coop CTL described on the web",
    "start": "1203490",
    "end": "1212370"
  },
  {
    "text": "HPA excuse me will see that it's already",
    "start": "1212370",
    "end": "1220860"
  },
  {
    "text": "scaled my pods up and the response",
    "start": "1220860",
    "end": "1227940"
  },
  {
    "text": "latency is way out of the range of possibility and so if I go over here and",
    "start": "1227940",
    "end": "1236610"
  },
  {
    "text": "do a watch we'll see that I've got four pods that have spun up it's taking them",
    "start": "1236610",
    "end": "1241919"
  },
  {
    "text": "a little time to get the traffic spread out but here in just a couple seconds",
    "start": "1241919",
    "end": "1247500"
  },
  {
    "text": "thank goodness it didn't make a liar out of me you'll see that we start to automatically load balanced traffic from",
    "start": "1247500",
    "end": "1253730"
  },
  {
    "text": "you guys and my load generator and immediately start to see the latencies",
    "start": "1253730",
    "end": "1259799"
  },
  {
    "text": "go down in fact I'm wondering what's broken on that one pod because everything else is down in the one",
    "start": "1259799",
    "end": "1265110"
  },
  {
    "text": "millisecond range and if we go over here you'll notice the request rate is",
    "start": "1265110",
    "end": "1270150"
  },
  {
    "text": "measured and eventually we're going to have everything normalized there that's",
    "start": "1270150",
    "end": "1278850"
  },
  {
    "start": "1277000",
    "end": "1394000"
  },
  {
    "text": "it but I want to talk a little bit about the cool things that you can do on top",
    "start": "1278850",
    "end": "1283980"
  },
  {
    "text": "of this and to get started I want to talk a little bit about route based",
    "start": "1283980",
    "end": "1289679"
  },
  {
    "text": "scaling so in the latest version of linker D we added the ability to not just get latency on a service which is",
    "start": "1289679",
    "end": "1295919"
  },
  {
    "text": "useful but to get latency on a route by route basis and what that empowers you to do with auto scaling is usually you",
    "start": "1295919",
    "end": "1303570"
  },
  {
    "text": "only have a couple of mission-critical API endpoints just Auto scale on the latency for those don't do it for the",
    "start": "1303570",
    "end": "1309940"
  },
  {
    "text": "whole service this is even more optimized than for example changing at a hundred percent CPU utilization and",
    "start": "1309940",
    "end": "1317009"
  },
  {
    "text": "because I think it's an awesome demo I'm going to show that off just really quick",
    "start": "1317009",
    "end": "1322389"
  },
  {
    "text": "here in fact I have the ability to introspect and get a open API aka",
    "start": "1322389",
    "end": "1331389"
  },
  {
    "text": "swagger out of the leaderboard application and then I'm going to pipe that into the link Rudy CLI that's going",
    "start": "1331389",
    "end": "1337720"
  },
  {
    "text": "to convert it from the open API specification into a CRD that basically",
    "start": "1337720",
    "end": "1344289"
  },
  {
    "text": "is regex is to be mount to be matches and handle that so we pipe this on through to coop CTL",
    "start": "1344289",
    "end": "1352080"
  },
  {
    "text": "and if my cluster hasn't melted down",
    "start": "1359110",
    "end": "1365100"
  },
  {
    "text": "we just started generating statistics for this so it will take a couple seconds but in that time we could go and",
    "start": "1369970",
    "end": "1376330"
  },
  {
    "text": "have crafted a query for Prometheus exported it through the custom metrics API and be able to get all of the",
    "start": "1376330",
    "end": "1383890"
  },
  {
    "text": "metrics for what's going on there in fact you'll see here that we're getting the latency it's slowly shifting over on",
    "start": "1383890",
    "end": "1389440"
  },
  {
    "text": "a per route basis wow that's awesome and",
    "start": "1389440",
    "end": "1396370"
  },
  {
    "start": "1394000",
    "end": "1428000"
  },
  {
    "text": "then finally and this is the one I think maybe I'm actually the most excited about prometheus is basically the most",
    "start": "1396370",
    "end": "1401919"
  },
  {
    "text": "amazing thing on earth as far as I can tell and it comes with the ability to do linear regression so you can actually do",
    "start": "1401919",
    "end": "1407649"
  },
  {
    "text": "predictive auto scaling with this setup on Prometheus remember it takes 30",
    "start": "1407649",
    "end": "1413200"
  },
  {
    "text": "seconds 60 seconds for everything to warm up you can say I know that my latency is going to be past 8",
    "start": "1413200",
    "end": "1420850"
  },
  {
    "text": "milliseconds let's scale up 60 seconds in advance so that everything is running and ready to go right when we get there",
    "start": "1420850",
    "end": "1427799"
  },
  {
    "text": "and that's actually it's really simple to get going with this you don't need to",
    "start": "1427799",
    "end": "1433570"
  },
  {
    "text": "stick with CPU and memory in the box but really quick because I've thrown so much data at everybody I've got this final",
    "start": "1433570",
    "end": "1440620"
  },
  {
    "text": "slide here I've got my slides up on github it's a PDF I'd love for everybody to go take a look I also have the code",
    "start": "1440620",
    "end": "1447370"
  },
  {
    "text": "up to hopefully run this yourself we have a getting started guide for linker D if anyone's remotely interested don't",
    "start": "1447370",
    "end": "1454929"
  },
  {
    "text": "worry you don't need a kubernetes cluster we're actually using Cod ACOTA to be able to get be able to run a",
    "start": "1454929",
    "end": "1460269"
  },
  {
    "text": "kubernetes cluster in your browser run through the whole getting started and finally we've got the Prometheus adapter",
    "start": "1460269",
    "end": "1465909"
  },
  {
    "text": "there for everyone - give it a try and that's it thank you so much",
    "start": "1465909",
    "end": "1472260"
  },
  {
    "text": "so we definitely have some time left I would love questions and I just saw a hand raised",
    "start": "1477159",
    "end": "1483700"
  },
  {
    "text": "so the question was what happens if you have a dependency in fact my a pact has a dependency on Redis and I suspect that",
    "start": "1493610",
    "end": "1499520"
  },
  {
    "text": "the reason why the numbers right now are not what I've tested in the pastor because Redis is going a little bit slow",
    "start": "1499520",
    "end": "1505390"
  },
  {
    "text": "you need to understand auto scaling isn't a silver bullet you have to really think deeply about what your",
    "start": "1505390",
    "end": "1511100"
  },
  {
    "text": "architecture is and how to scale it if you're running a micro services architectural you want auto scaling all",
    "start": "1511100",
    "end": "1517340"
  },
  {
    "text": "of the different layers and then you want to go and think how it interacts while I I made it seem super easy it",
    "start": "1517340",
    "end": "1522860"
  },
  {
    "text": "really is not just a silver bullet you have to think about your workloads and how it all works together for better or worse any other questions yes",
    "start": "1522860",
    "end": "1534040"
  },
  {
    "text": "so you wouldn't necessarily need to use link or D for that I would probably use the custom metrics based off of your cue",
    "start": "1544500",
    "end": "1551280"
  },
  {
    "text": "length and just Auto scale on that but it is it gets to the real power of custom metrics or external metrics again",
    "start": "1551280",
    "end": "1557400"
  },
  {
    "text": "if you've got your kafka cluster for example hosted off kubernetes you can go use the external metrics API to do the",
    "start": "1557400",
    "end": "1563880"
  },
  {
    "text": "auto scaling on that but that gets back to your workload is your workload you",
    "start": "1563880",
    "end": "1569280"
  },
  {
    "text": "need to kind of think through how it works this works pretty well for very specific API services but once you get",
    "start": "1569280",
    "end": "1574770"
  },
  {
    "text": "out of that you need to kind of figure out where you want to go with that any other questions yes",
    "start": "1574770",
    "end": "1582169"
  },
  {
    "text": "so you definitely could do that the problem is that then you have to think out think through what your overhead is",
    "start": "1594560",
    "end": "1600530"
  },
  {
    "text": "and really understand what your saturation is because I don't actually",
    "start": "1600530",
    "end": "1605660"
  },
  {
    "text": "know what my service here saturates at it's really hard to say ok Auto scale but Auto scale at a thousand RPS because",
    "start": "1605660",
    "end": "1613640"
  },
  {
    "text": "who knows it's going to get scheduled to something different and so this does again kind of get back to it's your own",
    "start": "1613640",
    "end": "1619610"
  },
  {
    "text": "workload to figure it out but this is kind of the next level of approximation",
    "start": "1619610",
    "end": "1625040"
  },
  {
    "text": "if you know your saturation layer and you know what CPUs you're scheduled on and you're sure that you're getting all",
    "start": "1625040",
    "end": "1630230"
  },
  {
    "text": "those resources that is actually a better way to do it but the amount of information required to get there is so",
    "start": "1630230",
    "end": "1637730"
  },
  {
    "text": "much that taking a step back I actually feel that this is a better approximation but not the best anybody else yes so you",
    "start": "1637730",
    "end": "1657530"
  },
  {
    "text": "can but it would end up being a external metric and then you need to think through what that means on a pod by pod",
    "start": "1657530",
    "end": "1664160"
  },
  {
    "text": "basis because maybe one pod is going away slow and one pod is going fast so you definitely can do it",
    "start": "1664160",
    "end": "1670370"
  },
  {
    "text": "I mean honestly technically you don't need link Rd for this if you want to go instrument your app for Prometheus you",
    "start": "1670370",
    "end": "1675740"
  },
  {
    "text": "can go stand that up for yourself the power is is that you don't have to it's monitoring and my experience is always",
    "start": "1675740",
    "end": "1682130"
  },
  {
    "text": "the thing that falls to the bottom of the priority list and so you would normally have to make this trade-off are",
    "start": "1682130",
    "end": "1687530"
  },
  {
    "text": "we going to have some better to monitoring so people get page less or is a product manager gonna ask to put more",
    "start": "1687530",
    "end": "1692750"
  },
  {
    "text": "features in and I'm sure you know which way that goes and so you can really think of link or D is just this like",
    "start": "1692750",
    "end": "1698420"
  },
  {
    "text": "easy way to start out okay we've got monitoring it's common let's go from there I saw a hand yes",
    "start": "1698420",
    "end": "1706510"
  },
  {
    "text": "well so this comes to how you're doing your deployments but it's smooth insofar as it's going to be a rolling deployment",
    "start": "1713150",
    "end": "1719220"
  },
  {
    "text": "and so when the new pod comes up it will already have the proxy there and the",
    "start": "1719220",
    "end": "1724620"
  },
  {
    "text": "traffic once it becomes ready the traffic will slowly shift over there so one of the cool things about linker Diaz",
    "start": "1724620",
    "end": "1730950"
  },
  {
    "text": "is that it's really focused on being incremental having a big bang and production scares the crap out of me",
    "start": "1730950",
    "end": "1736590"
  },
  {
    "text": "just do it on one service just do it for one pod get really comfortable with it and then start to expand it out so that",
    "start": "1736590",
    "end": "1742530"
  },
  {
    "text": "you start to get comfortable with the latencies and figure out how all of that works together yes so I'm so glad you",
    "start": "1742530",
    "end": "1756480"
  },
  {
    "text": "asked that one of the coolest things about Lync rudy is is that it's G RPC aware and not just that but it's G RPC",
    "start": "1756480",
    "end": "1762600"
  },
  {
    "text": "aware from a status code perspective because every gr if you see response is a HTTP 200 ok and so not only does it",
    "start": "1762600",
    "end": "1769980"
  },
  {
    "text": "each do HTTP 1 and 2 it also is aware of G RPC it doesn't do things like Kafka or",
    "start": "1769980",
    "end": "1777420"
  },
  {
    "text": "Redis or MySQL some of the other binary protocols it could be added but the instrumentation isn't there yet it",
    "start": "1777420",
    "end": "1783450"
  },
  {
    "text": "really is specific to the family of HTTP protocols and where you go for there today cool",
    "start": "1783450",
    "end": "1790410"
  },
  {
    "text": "anybody else yes",
    "start": "1790410",
    "end": "1794390"
  },
  {
    "text": "ooh that's a good question I don't know the answer to that if you come by the buoyant booth we'll get somebody to actually answer that we do we've just",
    "start": "1801250",
    "end": "1809440"
  },
  {
    "text": "introduced retries and timeouts and so I think that there's a good answer there but I don't know for sure off the top of",
    "start": "1809440",
    "end": "1814539"
  },
  {
    "text": "my head yes",
    "start": "1814539",
    "end": "1817830"
  },
  {
    "text": "so you so that's actually what it does",
    "start": "1831490",
    "end": "1843870"
  },
  {
    "text": "you're teeing me up for a very large rant that I have I'm gonna take a step back but say the latency is actually",
    "start": "1843870",
    "end": "1851620"
  },
  {
    "text": "getting measured depending completely depending on where you look in the system so it is at a pod level it's at a deploy level it's at a service level and",
    "start": "1851620",
    "end": "1859120"
  },
  {
    "text": "so you can slice and dice it all without passing headers on which is kind of the Achilles heel of most of the distributed",
    "start": "1859120",
    "end": "1865450"
  },
  {
    "text": "tracing in my experience most folks want to understand the dag and the",
    "start": "1865450",
    "end": "1870669"
  },
  {
    "text": "connections between services they're not actually interested in tracking spans that are specific to a single request",
    "start": "1870669",
    "end": "1877380"
  },
  {
    "text": "yes yes",
    "start": "1877380",
    "end": "1882450"
  },
  {
    "text": "I I honestly think that the end goal",
    "start": "1902850",
    "end": "1909720"
  },
  {
    "text": "here is to go and create aggregate metrics exactly where you're going where you don't just Auto scale on latency you",
    "start": "1909720",
    "end": "1915150"
  },
  {
    "text": "actually measure a bunch of the golden signals and MIT police CPU and feed that into the horizontal pot autoscaler so",
    "start": "1915150",
    "end": "1922890"
  },
  {
    "text": "it's totally possible to do I just didn't happen to show it off here I think we're out of time I'm gonna hang",
    "start": "1922890",
    "end": "1928500"
  },
  {
    "text": "around here and be back at the buoyant booth for all of your questions I would love to answer them thank you so much",
    "start": "1928500",
    "end": "1934650"
  },
  {
    "text": "for coming [Applause]",
    "start": "1934650",
    "end": "1938358"
  }
]