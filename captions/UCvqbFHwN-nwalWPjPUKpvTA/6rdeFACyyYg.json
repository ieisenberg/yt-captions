[
  {
    "text": "what are we going to talk about about uh optimizing kolam performance uh with",
    "start": "199",
    "end": "5359"
  },
  {
    "text": "open Telemetry uh let us introduce ourselves",
    "start": "5359",
    "end": "10559"
  },
  {
    "text": "hi I'm mashuk Chandra I'm a senior software engineer at Google I work on a inference uh workloads performance",
    "start": "10559",
    "end": "17640"
  },
  {
    "text": "optimization and I'm also leading the benchmarking and Metric standardization effort in cative serving working",
    "start": "17640",
    "end": "24680"
  },
  {
    "text": "group and I'm Luda mova I work on up and Telemetry and uh on Azure client",
    "start": "24680",
    "end": "31320"
  },
  {
    "text": "libraries I'm technical Committee Member and a maintainer of open Telemetry semantic",
    "start": "31320",
    "end": "36960"
  },
  {
    "text": "conventions so we are going to start with a quick intro uh what's special",
    "start": "36960",
    "end": "42760"
  },
  {
    "text": "about uh LM gen you call it uh we will",
    "start": "42760",
    "end": "47879"
  },
  {
    "text": "talk about the observability needs of uh gen applications and workloads uh we",
    "start": "47879",
    "end": "53480"
  },
  {
    "text": "will quickly dive into client side of the Telemetry things and then a sh will",
    "start": "53480",
    "end": "58879"
  },
  {
    "text": "take cover and talk about uh the model server site we will do a quick demo and",
    "start": "58879",
    "end": "66040"
  },
  {
    "text": "uh talk more about a scaling and the server side of things finally we hope we",
    "start": "66040",
    "end": "73280"
  },
  {
    "text": "can influence you to come contribute to some of the things we're working on if you're interested and we are going to",
    "start": "73280",
    "end": "79920"
  },
  {
    "text": "share how you can participate okay so um llms are getting more popular uh",
    "start": "79920",
    "end": "89320"
  },
  {
    "text": "but also self hosting LMS is getting very popular um for example hugging phase has",
    "start": "89320",
    "end": "97040"
  },
  {
    "text": "more than a million models hosted now and a very active community of people um",
    "start": "97040",
    "end": "103280"
  },
  {
    "text": "kubernetes be is becoming a preferred platform to host this workloads and there are new capabilities uh being",
    "start": "103280",
    "end": "110399"
  },
  {
    "text": "introduced for uh specifically for a Amal workloads um okay so in terms of",
    "start": "110399",
    "end": "119399"
  },
  {
    "text": "performance and observability what's special right so first it's it's a new",
    "start": "119399",
    "end": "125680"
  },
  {
    "text": "technology is growing really fast uh there are a lot of new usage patterns uh",
    "start": "125680",
    "end": "131280"
  },
  {
    "text": "they have kind of high complexity right uh the uh responses are not",
    "start": "131280",
    "end": "138080"
  },
  {
    "text": "deterministic we now need to evaluate them we need to record this Telemetry in a certain way um and also on the server",
    "start": "138080",
    "end": "145680"
  },
  {
    "text": "side the operations are kind of different than when you serve the HTTP",
    "start": "145680",
    "end": "152680"
  },
  {
    "text": "apis and we need different insights into this very computer and data intensive uh",
    "start": "152680",
    "end": "159280"
  },
  {
    "text": "workloads and to be fair we just don't know how to use the thing right we as an",
    "start": "159280",
    "end": "164400"
  },
  {
    "text": "industry we are all trying to figure out how to use llms and what does it mean to",
    "start": "164400",
    "end": "169959"
  },
  {
    "text": "for it to be optimal um then how um how to get insights into it um",
    "start": "169959",
    "end": "177920"
  },
  {
    "text": "yeah okay so let's dive into the client side first um so on the client side uh",
    "start": "177920",
    "end": "185840"
  },
  {
    "text": "as a member of open Telemetry I'm going to talk about up Telemetry we are working actively on uh defining semantic",
    "start": "185840",
    "end": "193720"
  },
  {
    "text": "conventions and building instrumentation libraries let me quickly uh share what it means so um we want people to use a",
    "start": "193720",
    "end": "204080"
  },
  {
    "text": "some instrumentation it should it will be high quality it will emit Telemetry according to conventions which is a",
    "start": "204080",
    "end": "211080"
  },
  {
    "text": "contract between uh something that produces Telemetry and some other people",
    "start": "211080",
    "end": "216720"
  },
  {
    "text": "or tools that consume it you can build dashboards you can build alerts you can build queries",
    "start": "216720",
    "end": "223000"
  },
  {
    "text": "visualizations um based on this contract uh and we need to Define what what",
    "start": "223000",
    "end": "228480"
  },
  {
    "text": "Telemetry we're going to meet and what what this contract is so that somebody can um uh build stuff on top of it um",
    "start": "228480",
    "end": "236480"
  },
  {
    "text": "okay so we provide some basic uh the Bas line uh for typical performance analysis",
    "start": "236480",
    "end": "244120"
  },
  {
    "text": "and monitoring debugging but also there is a gen specific context like prompts completions uh parameters specific to",
    "start": "244120",
    "end": "252319"
  },
  {
    "text": "gen um and we're working on evaluations okay um in terms of more",
    "start": "252319",
    "end": "259040"
  },
  {
    "text": "practical things uh let's look at the distributed Trace uh that describes a",
    "start": "259040",
    "end": "264199"
  },
  {
    "text": "specific operation here uh if you're not familiar with it we have a incoming HTTP",
    "start": "264199",
    "end": "271000"
  },
  {
    "text": "call uh there is a first call to the model um which has underlying HTTP call",
    "start": "271000",
    "end": "278639"
  },
  {
    "text": "then the model tells us okay go go call the current weather tool we're calling",
    "start": "278639",
    "end": "283680"
  },
  {
    "text": "the tool and oops uh during the tool call we get some transend in error it's retried several times finally it",
    "start": "283680",
    "end": "290919"
  },
  {
    "text": "succeeds we send a second response to the model and now we see we are being",
    "start": "290919",
    "end": "297400"
  },
  {
    "text": "throttled we get http for 29 so if you look at this there are 20 seconds but",
    "start": "297400",
    "end": "303840"
  },
  {
    "text": "you probably can guess how to optimize this performance right you can look into the transend failures on your weather",
    "start": "303840",
    "end": "310800"
  },
  {
    "text": "API and you might want to do something about thring um so things are good on the",
    "start": "310800",
    "end": "319360"
  },
  {
    "text": "client side if we want to look into gen specific context here it is uh there are",
    "start": "319360",
    "end": "324639"
  },
  {
    "text": "attributes on the things we saw in the previous picture so everything here is a",
    "start": "324639",
    "end": "330280"
  },
  {
    "text": "span uh and here are the attributes of a specific span we can see specific uh",
    "start": "330280",
    "end": "335960"
  },
  {
    "text": "response model we can see usage tokens and other stuff",
    "start": "335960",
    "end": "341479"
  },
  {
    "text": "um okay some people want to capture prompts and completions as somebody",
    "start": "341479",
    "end": "347120"
  },
  {
    "text": "working on Telemetry I still have uh some reservations about it but yeah people want it so why not uh we're doing",
    "start": "347120",
    "end": "355199"
  },
  {
    "text": "it as uptin so because it's sensitive it's extremely verbose but you can up into this uh and get the full prompts",
    "start": "355199",
    "end": "363160"
  },
  {
    "text": "and completions the cool part about it that we use uh it's called events log",
    "start": "363160",
    "end": "369759"
  },
  {
    "text": "based events uh the cool part about them they are structured and they're named so here for example this is Gen user",
    "start": "369759",
    "end": "376720"
  },
  {
    "text": "message and every time you get this event you should you can rely on it",
    "start": "376720",
    "end": "383680"
  },
  {
    "text": "having a certain structure so you can parse it you can visualize it uh you can",
    "start": "383680",
    "end": "389560"
  },
  {
    "text": "depend on the structure still experimental um",
    "start": "389560",
    "end": "396599"
  },
  {
    "text": "okay traces are awesome for individual flows but sometimes you want to",
    "start": "396599",
    "end": "404120"
  },
  {
    "text": "understand the overall State and house right you kind of uh know both how the individual flow goes and how the overall",
    "start": "404120",
    "end": "411080"
  },
  {
    "text": "thing works so we Define a bunch of metrics there's some boring stuff throughput latency histogram V",
    "start": "411080",
    "end": "418759"
  },
  {
    "text": "percentiles uh gener specific stuff like usage rate there are more to come especially for",
    "start": "418759",
    "end": "424280"
  },
  {
    "text": "streaming and stuff like this and there is even more boring stuff like CPU memory HTTP databases whatever you can",
    "start": "424280",
    "end": "431120"
  },
  {
    "text": "get from uh the VIS openen Telemetry from your application that uses Ai and",
    "start": "431120",
    "end": "438240"
  },
  {
    "text": "also something else okay so client Telemetry awesome for the",
    "start": "438240",
    "end": "445400"
  },
  {
    "text": "client things the moment you start self hosting your models you need a different kinds of tools and",
    "start": "445400",
    "end": "454240"
  },
  {
    "text": "observability you need to understand how your deployment goes how you ow the scale and how to manage your resources",
    "start": "454240",
    "end": "461560"
  },
  {
    "text": "and this is where a shock will tell us how to do this yeah thanks for going",
    "start": "461560",
    "end": "467360"
  },
  {
    "text": "over the client side so there is this other side right which is how does our server do uh so we'll talk about model",
    "start": "467360",
    "end": "473680"
  },
  {
    "text": "server Telemetry what does it mean to measure performance and how you can optimize it uh so before we measure",
    "start": "473680",
    "end": "479680"
  },
  {
    "text": "performance we need to Define what performance is from the model server side right um traditionally you would",
    "start": "479680",
    "end": "485840"
  },
  {
    "text": "have throughput and latency in terms of request per second and uh how many milliseconds or seconds it takes per",
    "start": "485840",
    "end": "491960"
  },
  {
    "text": "request uh but with llms all of it is in tokens right because you have these long",
    "start": "491960",
    "end": "497240"
  },
  {
    "text": "context models which can take an age for you to get a response back um sometimes",
    "start": "497240",
    "end": "502919"
  },
  {
    "text": "it can be 30,000 tokens or more um usually it can be like 30 to 60 seconds",
    "start": "502919",
    "end": "508080"
  },
  {
    "text": "so you want to measure in tokens per second uh so with throughput we have output tokens per second which tells you",
    "start": "508080",
    "end": "514080"
  },
  {
    "text": "how many tokens you are actually generating and you have input tokens per second which is your prefill phase which",
    "start": "514080",
    "end": "519399"
  },
  {
    "text": "tells you how many tokens there are in your prompt that your model is ingesting right and with latency we have uh time",
    "start": "519399",
    "end": "526839"
  },
  {
    "text": "to First token uh time to First token is probably the most important latency metric because it tells you how long it",
    "start": "526839",
    "end": "532920"
  },
  {
    "text": "takes from the time the end user sensor request to the time you get the first token back right uh and then you have",
    "start": "532920",
    "end": "539200"
  },
  {
    "text": "time per output token time per output token is the time you spend uh getting each successive token after your first",
    "start": "539200",
    "end": "544880"
  },
  {
    "text": "token itself comes this is important too um and then price buff right all of",
    "start": "544880",
    "end": "549920"
  },
  {
    "text": "these models are running on accelerators which are very expensive so knowing what kind of performance you are getting for",
    "start": "549920",
    "end": "556000"
  },
  {
    "text": "the price you are paying is important this is usually measured in dollar per million output tokens or input tokens if",
    "start": "556000",
    "end": "562120"
  },
  {
    "text": "you have subscribed to like uh Gemini API Chad GPT Vortex AI any of those uh",
    "start": "562120",
    "end": "567279"
  },
  {
    "text": "you would be paying dollars based on the tokens you are consuming right so these are like the basic set of metrics we",
    "start": "567279",
    "end": "572680"
  },
  {
    "text": "want to use to tell how good our model is performing uh now let's look at some of",
    "start": "572680",
    "end": "578560"
  },
  {
    "text": "the metrics that allow us to measure performance on the right you are seeing the VM dashboard on grafana uh this has",
    "start": "578560",
    "end": "586760"
  },
  {
    "text": "a whole bunch of metrics uh you can kind of categorize them into uh the categories that are listed there right",
    "start": "586760",
    "end": "592959"
  },
  {
    "text": "one is how much load is your model server currently facing and two how much",
    "start": "592959",
    "end": "598040"
  },
  {
    "text": "capacity does your model server have uh what capacity is it operating at now what is the maximum capacity it can go",
    "start": "598040",
    "end": "604600"
  },
  {
    "text": "to uh and then latency around how long it takes for your request to be admitted",
    "start": "604600",
    "end": "609760"
  },
  {
    "text": "and how long it takes for inference itself to happen uh so all of these things we can measure",
    "start": "609760",
    "end": "615600"
  },
  {
    "text": "um so it's more than just observability right you have all these cool metrics uh that you can observe but what more can",
    "start": "615600",
    "end": "622079"
  },
  {
    "text": "you do with it uh one you can build performance profiles of your models because anytime you want to run run a",
    "start": "622079",
    "end": "628320"
  },
  {
    "text": "model you want to actually look at whether this accelerator makes sense for me running this model on this",
    "start": "628320",
    "end": "633480"
  },
  {
    "text": "accelerator will it give me the best price POF so by getting these metrics you can actually formulate performance",
    "start": "633480",
    "end": "639560"
  },
  {
    "text": "profiles I will discuss later how you would do that um and in addition to that",
    "start": "639560",
    "end": "644720"
  },
  {
    "text": "you can also uh kind of do intelligent load balancing right load balancing is also different from traditional web",
    "start": "644720",
    "end": "650440"
  },
  {
    "text": "applications uh because of the long running requests you need to know what load a model server is under to be able",
    "start": "650440",
    "end": "656279"
  },
  {
    "text": "to load balance effectively that is something you can do with the metrics as well and uh you can do intelligent Auto",
    "start": "656279",
    "end": "662399"
  },
  {
    "text": "scaling too right some people might be focused on maximizing throughput others might Focus might be focused on ultra",
    "start": "662399",
    "end": "669959"
  },
  {
    "text": "low latency right so Auto scaling for these two different scenarios also changes uh and at last uh because all of",
    "start": "669959",
    "end": "677519"
  },
  {
    "text": "these Model S have a good queuing system to queue the request as they come in you can do different sort of scheduling with",
    "start": "677519",
    "end": "683440"
  },
  {
    "text": "it you can do priority based scheduling you can do fairness based scheduling and all that",
    "start": "683440",
    "end": "689560"
  },
  {
    "text": "uh so far we saw uh the client side and model server side Telemetry that is available now let's put this in action",
    "start": "689560",
    "end": "695600"
  },
  {
    "text": "and look at a demo where we uh identify an issue with our setup and then we",
    "start": "695600",
    "end": "701200"
  },
  {
    "text": "debug it and then we optimize it",
    "start": "701200",
    "end": "706720"
  },
  {
    "text": "okay okay so in this demo we're going to start with this awesome ux I created I",
    "start": "706720",
    "end": "712680"
  },
  {
    "text": "cannot sender def as you can see anyway so we're going to submit a prompt and",
    "start": "712680",
    "end": "718639"
  },
  {
    "text": "get a complete potion and it's nice and fast Everything is Awesome uh so let's let's do it",
    "start": "718639",
    "end": "726839"
  },
  {
    "text": "again um okay we're submitting and you",
    "start": "726839",
    "end": "732800"
  },
  {
    "text": "know this time it takes a while so this time it's going to take about 20 seconds",
    "start": "732800",
    "end": "739839"
  },
  {
    "text": "the response is the same everything is fine but it's it's kind of long so let's look into the traces and see what",
    "start": "739839",
    "end": "746839"
  },
  {
    "text": "happened there so we're going to find the long one uh the one that took 22",
    "start": "746839",
    "end": "753079"
  },
  {
    "text": "seconds remember the previous Trace I showed it also took 20 seconds but there",
    "start": "753079",
    "end": "758199"
  },
  {
    "text": "were a lot of things going on there here there's just one call and the whole HTTP",
    "start": "758199",
    "end": "764199"
  },
  {
    "text": "call to VM server takes around 20",
    "start": "764199",
    "end": "769440"
  },
  {
    "text": "seconds there is not much on the client Telemetry that could help us but we can",
    "start": "769440",
    "end": "774519"
  },
  {
    "text": "at least look and see okay this is the end point and the API we called",
    "start": "774519",
    "end": "779600"
  },
  {
    "text": "everything seems to be fine from the client side but where where where did we spend this 20 seconds okay let's also",
    "start": "779600",
    "end": "786920"
  },
  {
    "text": "take a look at the metric in this application we report uh metric to predus from our client and also from",
    "start": "786920",
    "end": "794360"
  },
  {
    "text": "VM itself um they are published",
    "start": "794360",
    "end": "799760"
  },
  {
    "text": "uh uh using uh the peredia operator and okay let's let's take a look at the",
    "start": "799760",
    "end": "807399"
  },
  {
    "text": "dashboards uh you've seen the previous one about the client there is not much interesting going on except you see that",
    "start": "807399",
    "end": "814600"
  },
  {
    "text": "the duration is actually went very high it went from something around zero uh to",
    "start": "814600",
    "end": "820680"
  },
  {
    "text": "p50 is around 30 seconds now uh so yeah",
    "start": "820680",
    "end": "826680"
  },
  {
    "text": "the metrics just show what we've seen uh in the traces but now we know that it",
    "start": "826680",
    "end": "832440"
  },
  {
    "text": "happens on p50 it's not an outlier it's The New Normal",
    "start": "832440",
    "end": "840240"
  },
  {
    "text": "yeah so now we are looking at the server side so we saw the request was taking too long and we know that the latency is",
    "start": "842199",
    "end": "849199"
  },
  {
    "text": "coming from the server side uh now when I look at the server side uh you can see the end to end request latency here",
    "start": "849199",
    "end": "855360"
  },
  {
    "text": "which was around like 2.48 seconds P99 uh started going really high right uh",
    "start": "855360",
    "end": "861399"
  },
  {
    "text": "the server is facing a lot of load now uh which is resulting in uh increasing request latency uh and you can also see",
    "start": "861399",
    "end": "869199"
  },
  {
    "text": "from this graph on the right the throughput has really gone up obviously sending more load increases the",
    "start": "869199",
    "end": "874480"
  },
  {
    "text": "throughput uh but latency is really suffering for us right um this is the time per output token uh latency chart",
    "start": "874480",
    "end": "882079"
  },
  {
    "text": "uh it tells you how much time it is taking per token this has gone up significantly as well P90 reaching like",
    "start": "882079",
    "end": "889920"
  },
  {
    "text": "uh around 2 seconds and time to force token which was like 1 second or less is",
    "start": "889920",
    "end": "895959"
  },
  {
    "text": "now at close to 10 seconds right this is not a good user experience at all and we need to do something about it uh I'll",
    "start": "895959",
    "end": "903399"
  },
  {
    "text": "show you some more interesting things here uh before we go on to what we do to address it uh the schedu state one is",
    "start": "903399",
    "end": "911000"
  },
  {
    "text": "interesting it tells you how many requests are currently in Flight uh you can see there are 754 requests running",
    "start": "911000",
    "end": "917120"
  },
  {
    "text": "which is a lot um and that's sort of like the cap where you are seeing like there are more requests starting to get",
    "start": "917120",
    "end": "923120"
  },
  {
    "text": "queed right and we are uh reaching up to like 400 requests which are cued and no work is being performed on them uh so",
    "start": "923120",
    "end": "930759"
  },
  {
    "text": "then I look at uh the cash utilization um which is gone up to like 99% which",
    "start": "930759",
    "end": "935800"
  },
  {
    "text": "tells me that the model Ser is operating at limit um What GPU cache usage usually",
    "start": "935800",
    "end": "941319"
  },
  {
    "text": "tells is how much memory are you using for your KV cache uh usually KV cach is",
    "start": "941319",
    "end": "946440"
  },
  {
    "text": "uh needed to store your key and value vectors when performing inference so it's a good indicator of uh how uh",
    "start": "946440",
    "end": "952600"
  },
  {
    "text": "overloaded the models Ser itself is um I have a few more interesting metrics here",
    "start": "952600",
    "end": "958319"
  },
  {
    "text": "that I can look at uh there is the prompt length distribution uh which has gone from a few tokens to like thousands of tokens",
    "start": "958319",
    "end": "965759"
  },
  {
    "text": "and a whole bunch of requests uh sending in like thousands of tokens and like requesting 500 tokens or so um and I can",
    "start": "965759",
    "end": "974319"
  },
  {
    "text": "also see uh the Finish reason uh you can actually finish a request for a couple of reasons one the uh model itself uh",
    "start": "974319",
    "end": "981920"
  },
  {
    "text": "decides uh I'm done at the other is when you hit the maximum token limit right um",
    "start": "981920",
    "end": "988560"
  },
  {
    "text": "so I refreshed this dashboard now uh you can see uh the load has gone up and now it's starting to come down uh with the",
    "start": "988560",
    "end": "995800"
  },
  {
    "text": "KV cach utilization request latency all of it is starting to drop um and you can",
    "start": "995800",
    "end": "1001839"
  },
  {
    "text": "see how bad it was it was uh hitting up to like 1 minute P90 and all that so now",
    "start": "1001839",
    "end": "1007759"
  },
  {
    "text": "let's address it right so I see that a burst came in now I'm going to set up Autos scaling to handle this burst in",
    "start": "1007759",
    "end": "1013800"
  },
  {
    "text": "traffic better uh so here I have my horizontal pod autoscaler uh configuration uh you can see uh it is",
    "start": "1013800",
    "end": "1020759"
  },
  {
    "text": "operating on the VM deployment that we have and I am setting my Min and Max replicas to one and two two is the",
    "start": "1020759",
    "end": "1027360"
  },
  {
    "text": "maximum number of gpus I have on this Mission um and you can see what metric we are using to do our Autos scaling",
    "start": "1027360",
    "end": "1034520"
  },
  {
    "text": "right I have set set up to use this custom metric called VM GPU cache usage percentage so basically we have set up",
    "start": "1034520",
    "end": "1041000"
  },
  {
    "text": "Prometheus to scrape this metric from VM and uh use it in a way when we hit like",
    "start": "1041000",
    "end": "1046558"
  },
  {
    "text": "50% uh cache usage that is when I want to scale up uh so now I'm going to uh",
    "start": "1046559",
    "end": "1052880"
  },
  {
    "text": "create this U HPA config now when I describe it uh I can see that uh the",
    "start": "1052880",
    "end": "1058760"
  },
  {
    "text": "config was successfully created and it shows that uh the VM GPU cache usage",
    "start": "1058760",
    "end": "1064400"
  },
  {
    "text": "percentage uh threshold is set at 50% so anytime now it goes above that uh HPA",
    "start": "1064400",
    "end": "1071160"
  },
  {
    "text": "will trigger a scale up I'll give a couple of seconds and",
    "start": "1071160",
    "end": "1076320"
  },
  {
    "text": "then when I describe the HPI again here uh we can see that the HPA has uh",
    "start": "1076320",
    "end": "1081520"
  },
  {
    "text": "started acting on this um so you can see uh the HPA is now looking at the VM",
    "start": "1081520",
    "end": "1088159"
  },
  {
    "text": "deployment and you can also see uh the current number of PODS is one and desired is one uh because HP actually",
    "start": "1088159",
    "end": "1095640"
  },
  {
    "text": "found the metric it realized that it is within the desired range um and no scaleup is needed at the moment uh next",
    "start": "1095640",
    "end": "1103480"
  },
  {
    "text": "we will look at a new spike in traffic that is coming up this time we are better equipped to hand hand it because",
    "start": "1103480",
    "end": "1109640"
  },
  {
    "text": "we just set up HPA um so we are making one more request from the chat client",
    "start": "1109640",
    "end": "1117120"
  },
  {
    "text": "you can see it is taking some time not as Snappy as the very first time we did it uh but not too bad as well uh so now",
    "start": "1117120",
    "end": "1124200"
  },
  {
    "text": "I go into my dashboard to look at it I can see the request latency start to go up uh it is not reached uh the peak yet",
    "start": "1124200",
    "end": "1133480"
  },
  {
    "text": "um and I can also see the other metrics uh that are here",
    "start": "1133480",
    "end": "1139919"
  },
  {
    "text": "so P90 is it about is at 40 seconds and we can see uh there are like a lot of",
    "start": "1140559",
    "end": "1147480"
  },
  {
    "text": "requests that are starting to add up we are at 183 to like 400 requests now um",
    "start": "1147480",
    "end": "1153760"
  },
  {
    "text": "cash utilization this is what we set our HPA threshold on uh that one has reached",
    "start": "1153760",
    "end": "1158919"
  },
  {
    "text": "around 67% um and time to First token is not terrible yet uh but it is at around 2",
    "start": "1158919",
    "end": "1166760"
  },
  {
    "text": "seconds so you might be wondering ing hey uh did this guy not set it up to Auto scale at 50% what is going on uh so",
    "start": "1166760",
    "end": "1174640"
  },
  {
    "text": "we did set it to Auto scale at 50% uh but there is some lag from when the Autos scaling happens to when the model",
    "start": "1174640",
    "end": "1181799"
  },
  {
    "text": "actually starts Ser servicing request right uh so we will see what happened in the HPA here um so in HPA we can see",
    "start": "1181799",
    "end": "1190320"
  },
  {
    "text": "that uh it is recognizing that the kvc utilization is at 69% now uh compared to",
    "start": "1190320",
    "end": "1196440"
  },
  {
    "text": "50% which is when we should out scale and HP has already triggered Auto scaling now that the current number of",
    "start": "1196440",
    "end": "1201919"
  },
  {
    "text": "replicas is two which is the desired number and you can see that the auto scaling happened like6 seconds ago but",
    "start": "1201919",
    "end": "1208640"
  },
  {
    "text": "because the model had to be loaded into memory uh because of the longer startup time U it is happening late right now",
    "start": "1208640",
    "end": "1215960"
  },
  {
    "text": "you can see this second um lello line that is coming up on the cash utilization so that is our new part",
    "start": "1215960",
    "end": "1222200"
  },
  {
    "text": "starting to uh serve requests you can also see from the number of requests running that um the new part is starting",
    "start": "1222200",
    "end": "1229640"
  },
  {
    "text": "to serve more requests uh now when I run this again uh it takes a couple of seconds uh but it is getting a little",
    "start": "1229640",
    "end": "1236720"
  },
  {
    "text": "faster than before uh and if I go in here we can see that the cash utilization on the first part start to",
    "start": "1236720",
    "end": "1242840"
  },
  {
    "text": "come down as the new one starts to rise up right and after a few more seconds um",
    "start": "1242840",
    "end": "1250400"
  },
  {
    "text": "the traffic has been going on for a while now but you can clearly see the difference from the first time to the uh",
    "start": "1250400",
    "end": "1255960"
  },
  {
    "text": "second time that we did it right um we hit really high end to latency up to like 4 minutes now we are uh being",
    "start": "1255960",
    "end": "1264000"
  },
  {
    "text": "closer to around 50 seconds maximum uh time per output token doesn't change much uh but time to First token from",
    "start": "1264000",
    "end": "1271080"
  },
  {
    "text": "what you can see here is really low right which is one of the main metric that we wanted to minimize uh which we",
    "start": "1271080",
    "end": "1276600"
  },
  {
    "text": "are able to do with auto scaling uh from the cash usage percentage also you can see we were hitting 99% before but now",
    "start": "1276600",
    "end": "1284120"
  },
  {
    "text": "when we were at about like 68% uh the new model S part came out and it kind of saturated at around 30%",
    "start": "1284120",
    "end": "1291760"
  },
  {
    "text": "load on each of the parts right um yeah and ttft as you can see from there was",
    "start": "1291760",
    "end": "1297840"
  },
  {
    "text": "981 millisecond whereas previously it was like 10 seconds so just by setting",
    "start": "1297840",
    "end": "1302880"
  },
  {
    "text": "up this s scaling based on the KV cach usage percentage we are able to really",
    "start": "1302880",
    "end": "1308200"
  },
  {
    "text": "uh bring down the latency that we were experiencing before and thanks to all the metrics",
    "start": "1308200",
    "end": "1314480"
  },
  {
    "text": "that we have uh this makes it easier for us to do",
    "start": "1314480",
    "end": "1319520"
  },
  {
    "text": "so that's our demo let me switch over to the",
    "start": "1319520",
    "end": "1326480"
  },
  {
    "text": "slides okay so we saw how you can set up order scaling now I want to talk about some of the uh challenges with",
    "start": "1329240",
    "end": "1335760"
  },
  {
    "text": "autoscaling Gen or llm workloads right first it is not as simple as web server",
    "start": "1335760",
    "end": "1340799"
  },
  {
    "text": "Autos scaling where you usually set it up to CPU utilization and it just works",
    "start": "1340799",
    "end": "1346000"
  },
  {
    "text": "right then you might have a question why not use GPU utilization because these are running on gpus and that should be a",
    "start": "1346000",
    "end": "1351400"
  },
  {
    "text": "good indicator right uh from our experimentation we found out that is not often the case uh we will discuss more",
    "start": "1351400",
    "end": "1356880"
  },
  {
    "text": "in the next slide um one other challenge we see is the longer part startup time uh we experienced that in the demo as",
    "start": "1356880",
    "end": "1363760"
  },
  {
    "text": "well it took like 2 minutes for the part to load this 7 billion parameter model uh so that is considerable amount of",
    "start": "1363760",
    "end": "1370520"
  },
  {
    "text": "time when you are not reacting yet right so that is something we need to address as well um and the last challenge is",
    "start": "1370520",
    "end": "1377720"
  },
  {
    "text": "there are different use cases right not all gen workloads are same some could be",
    "start": "1377720",
    "end": "1382840"
  },
  {
    "text": "offline inference where you might want to maximize throughput you don't worry too much about latency the other could",
    "start": "1382840",
    "end": "1388600"
  },
  {
    "text": "be latency sensitive where you really like code completion right as you are typing you want to see the suggestions",
    "start": "1388600",
    "end": "1393720"
  },
  {
    "text": "come up so how do you aut scale for these two different workloads that's a challenge as",
    "start": "1393720",
    "end": "1399919"
  },
  {
    "text": "well um so we saw the challenges so what kind of metrics will help us and U how",
    "start": "1399919",
    "end": "1406159"
  },
  {
    "text": "does Hardware metrics like GP utilization compar with model Ser metrics that we saw just now right there",
    "start": "1406159",
    "end": "1412400"
  },
  {
    "text": "are a lot of GP utilization metrics available you have the duty cycle which tells you how much of the time you using",
    "start": "1412400",
    "end": "1418960"
  },
  {
    "text": "your GPU and then you have power usage um you have memory bandwidth usage there",
    "start": "1418960",
    "end": "1424159"
  },
  {
    "text": "are a bunch of others that you would get from dcgm uh as well uh but there is not",
    "start": "1424159",
    "end": "1430559"
  },
  {
    "text": "a single GPU metric that we can reliably said that will help us with auto scaling uh the reason is the nature of the",
    "start": "1430559",
    "end": "1436720"
  },
  {
    "text": "inference workload itself and the different bottlenecks it has so when you are loading a model um into your",
    "start": "1436720",
    "end": "1443200"
  },
  {
    "text": "accelerator the first bottleneck you would encounter is how much memory you have for the model weights and the KV",
    "start": "1443200",
    "end": "1448600"
  },
  {
    "text": "cache right usually this is the first bottleneck you'll hit but if you are able to do that successfully the second",
    "start": "1448600",
    "end": "1454640"
  },
  {
    "text": "bottleneck you might have is the memory bandwidth right how many bytes are you able to transfer into your GPU memory",
    "start": "1454640",
    "end": "1460080"
  },
  {
    "text": "per second uh once you get past that is when you actually become compute B right",
    "start": "1460080",
    "end": "1465600"
  },
  {
    "text": "so based on which model you are using what you're accelerator is and what model architecture you are using uh you",
    "start": "1465600",
    "end": "1472480"
  },
  {
    "text": "can hit any of these bottlenecks so you can't use a single GPU metric to actually do scaling well um so we need",
    "start": "1472480",
    "end": "1479960"
  },
  {
    "text": "to take a workload Centric approach like the one we just saw where we look at the load and capacity of the model server to",
    "start": "1479960",
    "end": "1486559"
  },
  {
    "text": "be able to do Autos scaling successfully uh this will actually uh depict this very well so this is a",
    "start": "1486559",
    "end": "1493080"
  },
  {
    "text": "performance profile of a model uh running on an accelerator and you can see latency on the x-axis and throughput",
    "start": "1493080",
    "end": "1500240"
  },
  {
    "text": "on the y- axis right uh we vary the load and see what happens uh so as you can",
    "start": "1500240",
    "end": "1506080"
  },
  {
    "text": "see as we increase the load all the Blue Points you are seeing um is that latency",
    "start": "1506080",
    "end": "1512240"
  },
  {
    "text": "increases a little bit but throughput increases a lot because we are able to handle more request in parallel and then",
    "start": "1512240",
    "end": "1519120"
  },
  {
    "text": "we hit an inflection point which is the Green Dot you are seeing where you have achieved maximum throughput you possibly",
    "start": "1519120",
    "end": "1524960"
  },
  {
    "text": "can right and then things get really bad where l can see continues to increase a lot but you're not seeing any gain in",
    "start": "1524960",
    "end": "1531799"
  },
  {
    "text": "throughput uh that's the Red Dot there so where would the auto scale here right you want to Auto scale in one of the",
    "start": "1531799",
    "end": "1538840"
  },
  {
    "text": "blue points if you are very latency sensitive right here you can see the Green Point is a little over 100",
    "start": "1538840",
    "end": "1544080"
  },
  {
    "text": "millisec uh if you want to stay to like 50 or 25 millisecond you would Autos scale on one of the BL points there um",
    "start": "1544080",
    "end": "1551320"
  },
  {
    "text": "if you want to Autos scale on throughput you want to catch the Green Point as well as you can right because you're getting the maximum throughput and your",
    "start": "1551320",
    "end": "1558600"
  },
  {
    "text": "is not terrible um you you can also see the bat size and Q size uh that are",
    "start": "1558600",
    "end": "1565240"
  },
  {
    "text": "listed along each of those points you can see bat size uh growing from like five all the way up to 250 when we hit",
    "start": "1565240",
    "end": "1572399"
  },
  {
    "text": "the inflection point uh which is when queuing starts to happen and the Q size starts to increase right uh so what can",
    "start": "1572399",
    "end": "1579240"
  },
  {
    "text": "we recommend uh with this latency profile uh we can recommend two things",
    "start": "1579240",
    "end": "1584679"
  },
  {
    "text": "uh if you want low latency Auto scaling you would set it up to use bat size uh",
    "start": "1584679",
    "end": "1589919"
  },
  {
    "text": "like we saw the other option is to use KV cach usage percentage uh like we did during the demo right both of these",
    "start": "1589919",
    "end": "1596559"
  },
  {
    "text": "approaches work uh one challenge with bath size is that based on your request",
    "start": "1596559",
    "end": "1601880"
  },
  {
    "text": "length uh the context length itself your bat size will vary right if you have like longer context your bat size will",
    "start": "1601880",
    "end": "1608159"
  },
  {
    "text": "be shorter if you have shorter context bat size will be longer so you need to dynamically identify that which might be",
    "start": "1608159",
    "end": "1614000"
  },
  {
    "text": "harder uh which is where KV Cas usage actually helps because this is a percentage you can just set a number",
    "start": "1614000",
    "end": "1620399"
  },
  {
    "text": "like 50% or 70% based on where you want uh uh the point to be in that lenc",
    "start": "1620399",
    "end": "1626440"
  },
  {
    "text": "profile where you want to actually do the Autos scaling uh and for maximum throughput we recommend using Q size uh",
    "start": "1626440",
    "end": "1633200"
  },
  {
    "text": "Q size is a good indicator because when the when we are operating at Max throughput that's when we start to",
    "start": "1633200",
    "end": "1639039"
  },
  {
    "text": "actually cue the request so once you notice you're starting to queue uh that is your Q to Auto scale uh so that you",
    "start": "1639039",
    "end": "1646520"
  },
  {
    "text": "are able to hit maximum utilization and not incur too much latency uh the only",
    "start": "1646520",
    "end": "1651679"
  },
  {
    "text": "drawback is that obviously Q size is a lagging indicator so you use it only when you don't care about latency as",
    "start": "1651679",
    "end": "1659760"
  },
  {
    "text": "much uh to wrap up I want to point to uh some of the ongoing work around this in",
    "start": "1659760",
    "end": "1665000"
  },
  {
    "text": "the community uh so we have a benchmarking work stream where we are trying to Benchmark llm workloads well",
    "start": "1665000",
    "end": "1672480"
  },
  {
    "text": "uh and produce uh cool little graphs like the one you saw before um so this is happening in kubernetes work group",
    "start": "1672480",
    "end": "1678559"
  },
  {
    "text": "serving where we are doing it in a model Ser agnostic way we are analyzing how we can improve Auto scaling load balancing",
    "start": "1678559",
    "end": "1684840"
  },
  {
    "text": "and all of that with this um we also have the instance Gateway project which is trying to automate uh the a scaling",
    "start": "1684840",
    "end": "1692440"
  },
  {
    "text": "and load balancing based on how your uh models are performing um that's a cool",
    "start": "1692440",
    "end": "1697880"
  },
  {
    "text": "project to be part of as well um and call to action um if you",
    "start": "1697880",
    "end": "1703200"
  },
  {
    "text": "found this interesting and you would like to participate uh in this and help make this better uh please U show up to",
    "start": "1703200",
    "end": "1711200"
  },
  {
    "text": "any of the kubernetes serving working group meeting uh you can also participate in the open Telemetry uh gen",
    "start": "1711200",
    "end": "1718039"
  },
  {
    "text": "semantic conventions and instrumentations Sig uh where we are standardizing metrics adding new metrics",
    "start": "1718039",
    "end": "1724159"
  },
  {
    "text": "traces and all of it uh to make this better uh that ends our presentation I",
    "start": "1724159",
    "end": "1730320"
  },
  {
    "text": "will answer any questions thanks",
    "start": "1730320",
    "end": "1734720"
  },
  {
    "text": "hi there um very cool talk um couple of questions the first one is the aut",
    "start": "1739399",
    "end": "1744799"
  },
  {
    "text": "scaling have you have you uh tried these for different workloads like in a long context or video genen and do you see",
    "start": "1744799",
    "end": "1751360"
  },
  {
    "text": "the same kind of recommendations for each of these or do you have different recommendations for long context and you",
    "start": "1751360",
    "end": "1756840"
  },
  {
    "text": "know let's say long context to start with uh second um I think like uh the uh",
    "start": "1756840",
    "end": "1763840"
  },
  {
    "text": "you know the prefill versus the decode disaggregation um do you have any suest question on like how do you autoscale uh",
    "start": "1763840",
    "end": "1771480"
  },
  {
    "text": "you know in the scenario where you're separating these out yeah uh so to answer your first",
    "start": "1771480",
    "end": "1777720"
  },
  {
    "text": "question we have experimented with um longer context and shorter context uh",
    "start": "1777720",
    "end": "1783919"
  },
  {
    "text": "but mainly with text though we haven't done like video or multimodel uh models",
    "start": "1783919",
    "end": "1789120"
  },
  {
    "text": "yet uh but these recommendations should kind of work uh the same with both short",
    "start": "1789120",
    "end": "1794640"
  },
  {
    "text": "and long context obviously like if you're prefill heavy and or decode heavy uh the point where you want to Autos",
    "start": "1794640",
    "end": "1801360"
  },
  {
    "text": "scale might differ but using something like KV Cas utilization kind of uh makes it easier where you don't have to tune",
    "start": "1801360",
    "end": "1808320"
  },
  {
    "text": "it individually uh so yeah at least that's where we are we want to do other gen workloads image generation uh video and",
    "start": "1808320",
    "end": "1815880"
  },
  {
    "text": "all that too in the future uh to answer your second question uh disaggregated",
    "start": "1815880",
    "end": "1820960"
  },
  {
    "text": "serving will definitely change some of the things right obviously we prefill and decode will be handled separately so",
    "start": "1820960",
    "end": "1827960"
  },
  {
    "text": "so we might want a separate Auto scaling mechanism for when we actually scale up the prefill servers versus when we do",
    "start": "1827960",
    "end": "1834080"
  },
  {
    "text": "decode servers uh so there we are just waiting for the community to get to a point where disaggregated serving is uh",
    "start": "1834080",
    "end": "1841200"
  },
  {
    "text": "something that is being used for example BLM has full support for it and all of that um yeah so their recommendations",
    "start": "1841200",
    "end": "1848360"
  },
  {
    "text": "could change a bit uh but yeah we are just keeping track of where we are there",
    "start": "1848360",
    "end": "1854679"
  },
  {
    "text": "than yeah hi uh so my question is for uh Auto scaling the llm pot have you tried",
    "start": "1854679",
    "end": "1860960"
  },
  {
    "text": "the uh vpa as opposed to HPA like will it make it any better or it's the same like 2 minute",
    "start": "1860960",
    "end": "1867200"
  },
  {
    "text": "delay yeah so uh vpa is a lot harder uh compared to HPA mainly because let's say",
    "start": "1867200",
    "end": "1875519"
  },
  {
    "text": "you got like um L4 gpus set up uh now if I want to scale up and go to like uh get",
    "start": "1875519",
    "end": "1882480"
  },
  {
    "text": "like a h100 GPU instead uh usually you might not have capacity uh so it is",
    "start": "1882480",
    "end": "1887639"
  },
  {
    "text": "harder to just U Pick a different machine one thing you could potentially do is like maybe instead of using one",
    "start": "1887639",
    "end": "1893279"
  },
  {
    "text": "gpus you can use more right uh you can increase tensor parallelism and use like four gpus instead uh so there what we",
    "start": "1893279",
    "end": "1900440"
  },
  {
    "text": "find is in a lot of cases uh tensor parallelism adds some overhead where it might be better to actually horizontally",
    "start": "1900440",
    "end": "1906840"
  },
  {
    "text": "Auto scale and use 4 gpus instead of scaling your uh workload vertically to use 4 GPS",
    "start": "1906840",
    "end": "1914600"
  }
]