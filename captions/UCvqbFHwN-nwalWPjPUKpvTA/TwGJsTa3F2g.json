[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "hello welcome to the rook intro nsf deep",
    "start": "80",
    "end": "2399"
  },
  {
    "text": "dive i am travis nielsen one of the rook",
    "start": "2399",
    "end": "4560"
  },
  {
    "text": "maintainers and i work for red hat",
    "start": "4560",
    "end": "7200"
  },
  {
    "text": "let's get going",
    "start": "7200",
    "end": "8480"
  },
  {
    "text": "all right for our agenda today we're",
    "start": "8480",
    "end": "9920"
  },
  {
    "text": "going to start off by talking about what",
    "start": "9920",
    "end": "11519"
  },
  {
    "text": "are the storage challenges that you",
    "start": "11519",
    "end": "12960"
  },
  {
    "text": "might have with kubernetes",
    "start": "12960",
    "end": "14719"
  },
  {
    "text": "then what is rook what is sef just cover",
    "start": "14719",
    "end": "16640"
  },
  {
    "text": "some basics and then we'll get into the",
    "start": "16640",
    "end": "18560"
  },
  {
    "text": "key features of what rook provides what",
    "start": "18560",
    "end": "21039"
  },
  {
    "text": "new features are in our latest 1.9",
    "start": "21039",
    "end": "23439"
  },
  {
    "text": "release that just came out in april",
    "start": "23439",
    "end": "25840"
  },
  {
    "text": "and we'll get into a demo that blaine",
    "start": "25840",
    "end": "27920"
  },
  {
    "text": "will give if you have questions we'll",
    "start": "27920",
    "end": "29679"
  },
  {
    "text": "should have some time at the end to",
    "start": "29679",
    "end": "31279"
  },
  {
    "text": "answer some of those",
    "start": "31279",
    "end": "33360"
  },
  {
    "start": "33000",
    "end": "33000"
  },
  {
    "text": "okay so what are your storage challenges",
    "start": "33360",
    "end": "35040"
  },
  {
    "text": "kubernetes really was a platform built",
    "start": "35040",
    "end": "38079"
  },
  {
    "text": "to manage sort distributed applications",
    "start": "38079",
    "end": "40800"
  },
  {
    "text": "ideally stateless and if they need",
    "start": "40800",
    "end": "43520"
  },
  {
    "text": "storage",
    "start": "43520",
    "end": "44559"
  },
  {
    "text": "well storage is kind of an afterthought",
    "start": "44559",
    "end": "46079"
  },
  {
    "text": "there's a way to plug in storage to",
    "start": "46079",
    "end": "47600"
  },
  {
    "text": "kubernetes but storage is not a native",
    "start": "47600",
    "end": "49840"
  },
  {
    "text": "component of kubernetes",
    "start": "49840",
    "end": "52160"
  },
  {
    "text": "so you if you rely on external storage",
    "start": "52160",
    "end": "54079"
  },
  {
    "text": "it's either not portable maybe it's a",
    "start": "54079",
    "end": "55920"
  },
  {
    "text": "burden to deploy it",
    "start": "55920",
    "end": "57520"
  },
  {
    "text": "if you're in a cloud provider you know",
    "start": "57520",
    "end": "59440"
  },
  {
    "text": "you're locked into that that vendor so",
    "start": "59440",
    "end": "61280"
  },
  {
    "text": "how do you get out of that vendor",
    "start": "61280",
    "end": "62320"
  },
  {
    "text": "locking so some of those questions is",
    "start": "62320",
    "end": "64320"
  },
  {
    "text": "where we started with rook and we wanted",
    "start": "64320",
    "end": "66880"
  },
  {
    "text": "to really bring storage to kubernetes in",
    "start": "66880",
    "end": "69280"
  },
  {
    "text": "a native way so that brings us to the",
    "start": "69280",
    "end": "71280"
  },
  {
    "start": "71000",
    "end": "71000"
  },
  {
    "text": "question what really basically does",
    "start": "71280",
    "end": "73040"
  },
  {
    "text": "provide",
    "start": "73040",
    "end": "74000"
  },
  {
    "text": "so rook brings storage inside your",
    "start": "74000",
    "end": "76400"
  },
  {
    "text": "kubernetes cluster",
    "start": "76400",
    "end": "78000"
  },
  {
    "text": "and",
    "start": "78000",
    "end": "79119"
  },
  {
    "text": "manages it for you",
    "start": "79119",
    "end": "80799"
  },
  {
    "text": "um it makes it so your applications can",
    "start": "80799",
    "end": "82479"
  },
  {
    "text": "consume storage just like any other",
    "start": "82479",
    "end": "84720"
  },
  {
    "text": "kubernetes storage with storage classes",
    "start": "84720",
    "end": "86960"
  },
  {
    "text": "and persistent volume claims the way we",
    "start": "86960",
    "end": "89439"
  },
  {
    "text": "do this is with kubernetes recovering",
    "start": "89439",
    "end": "91680"
  },
  {
    "text": "operator",
    "start": "91680",
    "end": "92720"
  },
  {
    "text": "and crds so you tell",
    "start": "92720",
    "end": "95600"
  },
  {
    "text": "rook how you want to manage steph and",
    "start": "95600",
    "end": "98000"
  },
  {
    "text": "the storage layer and then rook will go",
    "start": "98000",
    "end": "100000"
  },
  {
    "text": "automate that for you the work will",
    "start": "100000",
    "end": "102159"
  },
  {
    "text": "deploy",
    "start": "102159",
    "end": "103280"
  },
  {
    "text": "configure upgrade step",
    "start": "103280",
    "end": "105920"
  },
  {
    "text": "for you so you don't have to worry about",
    "start": "105920",
    "end": "107200"
  },
  {
    "text": "all the details of managing that store",
    "start": "107200",
    "end": "109040"
  },
  {
    "text": "storage layer",
    "start": "109040",
    "end": "110799"
  },
  {
    "text": "rookie is open source apache 2.0 license",
    "start": "110799",
    "end": "114640"
  },
  {
    "text": "and we just try to have an open",
    "start": "114640",
    "end": "116079"
  },
  {
    "text": "community we want to do what's best for",
    "start": "116079",
    "end": "118079"
  },
  {
    "text": "the community",
    "start": "118079",
    "end": "120159"
  },
  {
    "start": "119000",
    "end": "119000"
  },
  {
    "text": "as you're getting started with rook we",
    "start": "120159",
    "end": "121439"
  },
  {
    "text": "want to make sure you know about all the",
    "start": "121439",
    "end": "122719"
  },
  {
    "text": "resources that we have on our website",
    "start": "122719",
    "end": "124479"
  },
  {
    "text": "documentation",
    "start": "124479",
    "end": "125759"
  },
  {
    "text": "join our slack for questions at the",
    "start": "125759",
    "end": "127759"
  },
  {
    "text": "bottom of the slide what we want to",
    "start": "127759",
    "end": "128879"
  },
  {
    "text": "point out is we're just starting to",
    "start": "128879",
    "end": "130640"
  },
  {
    "text": "release some new training videos i'm",
    "start": "130640",
    "end": "132800"
  },
  {
    "text": "getting started with some basic concepts",
    "start": "132800",
    "end": "134400"
  },
  {
    "text": "with brooke",
    "start": "134400",
    "end": "135680"
  },
  {
    "text": "on",
    "start": "135680",
    "end": "136599"
  },
  {
    "text": "cubebuyexample.com so check it out and i",
    "start": "136599",
    "end": "139680"
  },
  {
    "text": "hope it's useful for you",
    "start": "139680",
    "end": "141599"
  },
  {
    "text": "now what is ceph many of you i'm sure",
    "start": "141599",
    "end": "144000"
  },
  {
    "start": "142000",
    "end": "142000"
  },
  {
    "text": "i've heard of it",
    "start": "144000",
    "end": "145440"
  },
  {
    "text": "ceph is an open source fault tolerant",
    "start": "145440",
    "end": "148400"
  },
  {
    "text": "storage service",
    "start": "148400",
    "end": "150879"
  },
  {
    "text": "it provides three types of storage so",
    "start": "150879",
    "end": "153200"
  },
  {
    "text": "block storage a",
    "start": "153200",
    "end": "155519"
  },
  {
    "text": "shared file system where you need to",
    "start": "155519",
    "end": "156959"
  },
  {
    "text": "share storage among pods",
    "start": "156959",
    "end": "158879"
  },
  {
    "text": "and or an object storage that's s3",
    "start": "158879",
    "end": "161360"
  },
  {
    "text": "compliant seth favors consistency",
    "start": "161360",
    "end": "164560"
  },
  {
    "text": "and it was first released in july 2012",
    "start": "164560",
    "end": "167360"
  },
  {
    "text": "so we're coming up on the 10th",
    "start": "167360",
    "end": "168720"
  },
  {
    "text": "anniversary since seth has been running",
    "start": "168720",
    "end": "170640"
  },
  {
    "text": "in production",
    "start": "170640",
    "end": "172640"
  },
  {
    "text": "storage clusters we're really excited",
    "start": "172640",
    "end": "175120"
  },
  {
    "text": "about that anniversary",
    "start": "175120",
    "end": "177040"
  },
  {
    "start": "177000",
    "end": "177000"
  },
  {
    "text": "so what basic architecture layers does",
    "start": "177040",
    "end": "179360"
  },
  {
    "text": "rook have so rook again really deploys",
    "start": "179360",
    "end": "182319"
  },
  {
    "text": "and manages",
    "start": "182319",
    "end": "184159"
  },
  {
    "text": "everything you need for storage with",
    "start": "184159",
    "end": "186319"
  },
  {
    "text": "seth",
    "start": "186319",
    "end": "187360"
  },
  {
    "text": "so",
    "start": "187360",
    "end": "188319"
  },
  {
    "text": "the ceph csi driver now it will",
    "start": "188319",
    "end": "191599"
  },
  {
    "text": "provide that plug-in layer with",
    "start": "191599",
    "end": "192959"
  },
  {
    "text": "kubernetes so you can provision and",
    "start": "192959",
    "end": "194720"
  },
  {
    "text": "mount the stuff storage into your",
    "start": "194720",
    "end": "196000"
  },
  {
    "text": "application pods just like you do with",
    "start": "196000",
    "end": "198080"
  },
  {
    "text": "any other",
    "start": "198080",
    "end": "199360"
  },
  {
    "text": "csi driver and then ceph itself provides",
    "start": "199360",
    "end": "202879"
  },
  {
    "text": "the data layer",
    "start": "202879",
    "end": "204400"
  },
  {
    "text": "really so when you're writing reading",
    "start": "204400",
    "end": "205920"
  },
  {
    "text": "and writing data",
    "start": "205920",
    "end": "207200"
  },
  {
    "text": "it's going to go straight to that data",
    "start": "207200",
    "end": "209120"
  },
  {
    "text": "layer for optimal performance so here's",
    "start": "209120",
    "end": "212000"
  },
  {
    "start": "212000",
    "end": "212000"
  },
  {
    "text": "a view of what",
    "start": "212000",
    "end": "213599"
  },
  {
    "text": "uh management looks like from work's",
    "start": "213599",
    "end": "214879"
  },
  {
    "text": "perspective and kind of looking at what",
    "start": "214879",
    "end": "216799"
  },
  {
    "text": "pods are created so there are a lot of",
    "start": "216799",
    "end": "218239"
  },
  {
    "text": "pods that need to be created but again",
    "start": "218239",
    "end": "220239"
  },
  {
    "text": "you don't have to worry about this",
    "start": "220239",
    "end": "221120"
  },
  {
    "text": "recreates all of them so you start with",
    "start": "221120",
    "end": "222319"
  },
  {
    "text": "the rook operator",
    "start": "222319",
    "end": "223840"
  },
  {
    "text": "and you tell rook i want to deploy seth",
    "start": "223840",
    "end": "226000"
  },
  {
    "text": "and this is how i want it configured so",
    "start": "226000",
    "end": "227760"
  },
  {
    "text": "then rook goes and starts",
    "start": "227760",
    "end": "229680"
  },
  {
    "text": "the",
    "start": "229680",
    "end": "230480"
  },
  {
    "text": "you know sf bond pod on each",
    "start": "230480",
    "end": "233920"
  },
  {
    "text": "of three nodes",
    "start": "233920",
    "end": "235360"
  },
  {
    "text": "then it will go look for all of the",
    "start": "235360",
    "end": "237120"
  },
  {
    "text": "devices so say you have devices on your",
    "start": "237120",
    "end": "240159"
  },
  {
    "text": "your nodes that you just want to",
    "start": "240159",
    "end": "242239"
  },
  {
    "text": "consume as storage in your data center",
    "start": "242239",
    "end": "244720"
  },
  {
    "text": "rook will create osds on each of those",
    "start": "244720",
    "end": "247439"
  },
  {
    "text": "devices osd is the fundamental storage",
    "start": "247439",
    "end": "250319"
  },
  {
    "text": "component of sev",
    "start": "250319",
    "end": "251920"
  },
  {
    "text": "so all these red pods basically are ceph",
    "start": "251920",
    "end": "254879"
  },
  {
    "text": "demons",
    "start": "254879",
    "end": "255920"
  },
  {
    "text": "and",
    "start": "255920",
    "end": "257120"
  },
  {
    "text": "the green pods are really",
    "start": "257120",
    "end": "259759"
  },
  {
    "text": "csi driver pods that help you then",
    "start": "259759",
    "end": "262800"
  },
  {
    "text": "provision and attach the storage",
    "start": "262800",
    "end": "265280"
  },
  {
    "text": "so that's in a nutshell",
    "start": "265280",
    "end": "266880"
  },
  {
    "text": "kind of idea of what pods you're going",
    "start": "266880",
    "end": "268960"
  },
  {
    "text": "to get with rook and so",
    "start": "268960",
    "end": "271199"
  },
  {
    "start": "271000",
    "end": "271000"
  },
  {
    "text": "now when your application's ready to go",
    "start": "271199",
    "end": "273520"
  },
  {
    "text": "consume provision that storage",
    "start": "273520",
    "end": "276720"
  },
  {
    "text": "we've got",
    "start": "276720",
    "end": "278240"
  },
  {
    "text": "this picture here of three different",
    "start": "278240",
    "end": "280080"
  },
  {
    "text": "types of storage so here on the left we",
    "start": "280080",
    "end": "281840"
  },
  {
    "text": "start with if your app needs",
    "start": "281840",
    "end": "283840"
  },
  {
    "text": "block storage you're going to create a",
    "start": "283840",
    "end": "286560"
  },
  {
    "text": "read write once volume claim",
    "start": "286560",
    "end": "288639"
  },
  {
    "text": "you define a storage class which uses",
    "start": "288639",
    "end": "290639"
  },
  {
    "text": "ceph rbd",
    "start": "290639",
    "end": "292080"
  },
  {
    "text": "and then the csi driver for ceph is",
    "start": "292080",
    "end": "294960"
  },
  {
    "text": "going to create one of these ceph rbd",
    "start": "294960",
    "end": "297440"
  },
  {
    "text": "volumes and attach it to",
    "start": "297440",
    "end": "299840"
  },
  {
    "text": "your apple your application pod okay so",
    "start": "299840",
    "end": "302240"
  },
  {
    "text": "that's block storage",
    "start": "302240",
    "end": "304240"
  },
  {
    "text": "now in the middle here we have an",
    "start": "304240",
    "end": "305600"
  },
  {
    "text": "example of file storage where what",
    "start": "305600",
    "end": "307520"
  },
  {
    "text": "you're going to do you've got an",
    "start": "307520",
    "end": "308479"
  },
  {
    "text": "application",
    "start": "308479",
    "end": "310000"
  },
  {
    "text": "two applications or two instances of an",
    "start": "310000",
    "end": "312000"
  },
  {
    "text": "application that need to share",
    "start": "312000",
    "end": "313680"
  },
  {
    "text": "a claim so you create",
    "start": "313680",
    "end": "315680"
  },
  {
    "text": "a claim and read write menu mode",
    "start": "315680",
    "end": "318880"
  },
  {
    "text": "uh it uses the staff ffs storage class",
    "start": "318880",
    "end": "322320"
  },
  {
    "text": "which allows for uh the sharing of the",
    "start": "322320",
    "end": "325120"
  },
  {
    "text": "pod or the volume",
    "start": "325120",
    "end": "327280"
  },
  {
    "text": "and then",
    "start": "327280",
    "end": "328400"
  },
  {
    "text": "it mounts it with the sepsiet seppfest",
    "start": "328400",
    "end": "331039"
  },
  {
    "text": "driver",
    "start": "331039",
    "end": "331919"
  },
  {
    "text": "all right so that's file storage and",
    "start": "331919",
    "end": "333759"
  },
  {
    "text": "then",
    "start": "333759",
    "end": "334720"
  },
  {
    "text": "if you have an application that needs to",
    "start": "334720",
    "end": "336880"
  },
  {
    "text": "use the object",
    "start": "336880",
    "end": "338400"
  },
  {
    "text": "storage rest endpoint with an s3 api",
    "start": "338400",
    "end": "341520"
  },
  {
    "text": "well we have the we kind of follow the",
    "start": "341520",
    "end": "343120"
  },
  {
    "text": "same pattern that you see with",
    "start": "343120",
    "end": "345199"
  },
  {
    "text": "pbc's so you can create a bucket claim",
    "start": "345199",
    "end": "348320"
  },
  {
    "text": "to say i want a bucket",
    "start": "348320",
    "end": "350960"
  },
  {
    "text": "to read and write data to",
    "start": "350960",
    "end": "353199"
  },
  {
    "text": "we define a storage class object",
    "start": "353199",
    "end": "355440"
  },
  {
    "text": "and then we've got a bucket provisioner",
    "start": "355440",
    "end": "357759"
  },
  {
    "text": "then that makes that",
    "start": "357759",
    "end": "359120"
  },
  {
    "text": "bucket available to your pod now this is",
    "start": "359120",
    "end": "361440"
  },
  {
    "text": "kind of a precursor to the new causey",
    "start": "361440",
    "end": "364800"
  },
  {
    "text": "implementation that's coming in",
    "start": "364800",
    "end": "366000"
  },
  {
    "text": "kubernetes soon",
    "start": "366000",
    "end": "367680"
  },
  {
    "text": "so we're looking forward to that",
    "start": "367680",
    "end": "368880"
  },
  {
    "text": "supporting that implementation",
    "start": "368880",
    "end": "370880"
  },
  {
    "text": "as well",
    "start": "370880",
    "end": "372080"
  },
  {
    "start": "372000",
    "end": "372000"
  },
  {
    "text": "now here's a view of what the data path",
    "start": "372080",
    "end": "374080"
  },
  {
    "text": "looks like with ceph so this is assuming",
    "start": "374080",
    "end": "376080"
  },
  {
    "text": "we've already got the storage provision",
    "start": "376080",
    "end": "377759"
  },
  {
    "text": "attached to your pods",
    "start": "377759",
    "end": "379280"
  },
  {
    "text": "now when your application is ready to",
    "start": "379280",
    "end": "381919"
  },
  {
    "text": "write data",
    "start": "381919",
    "end": "383360"
  },
  {
    "text": "you're you're going to write to your",
    "start": "383360",
    "end": "385280"
  },
  {
    "text": "volume inside the pod just like you",
    "start": "385280",
    "end": "387039"
  },
  {
    "text": "write to any other stored local storage",
    "start": "387039",
    "end": "389120"
  },
  {
    "text": "that write or read will go through the",
    "start": "389120",
    "end": "392000"
  },
  {
    "text": "ceph rbd kernel driver which will then",
    "start": "392000",
    "end": "395120"
  },
  {
    "text": "talk to the ceph",
    "start": "395120",
    "end": "397039"
  },
  {
    "text": "daemons that are running in your cluster",
    "start": "397039",
    "end": "399520"
  },
  {
    "text": "and you don't need to know anything",
    "start": "399520",
    "end": "401039"
  },
  {
    "text": "about where they're running how they're",
    "start": "401039",
    "end": "402160"
  },
  {
    "text": "running seth just takes care of that for",
    "start": "402160",
    "end": "404160"
  },
  {
    "text": "you no matter what type of storage",
    "start": "404160",
    "end": "406479"
  },
  {
    "text": "whether it's the block file or",
    "start": "406479",
    "end": "408800"
  },
  {
    "text": "object storage",
    "start": "408800",
    "end": "410240"
  },
  {
    "text": "with the s3 client",
    "start": "410240",
    "end": "411840"
  },
  {
    "text": "those clients know how to connect to",
    "start": "411840",
    "end": "413520"
  },
  {
    "text": "this",
    "start": "413520",
    "end": "414319"
  },
  {
    "text": "storage okay let's get into some of the",
    "start": "414319",
    "end": "416240"
  },
  {
    "text": "basic features that rook brings to the",
    "start": "416240",
    "end": "418319"
  },
  {
    "text": "table with seth so first of all",
    "start": "418319",
    "end": "421199"
  },
  {
    "start": "420000",
    "end": "420000"
  },
  {
    "text": "installing stuff in kubernetes is is",
    "start": "421199",
    "end": "424000"
  },
  {
    "text": "simple we've made it as simple that's",
    "start": "424000",
    "end": "426160"
  },
  {
    "text": "been one of our",
    "start": "426160",
    "end": "428000"
  },
  {
    "text": "goals from the start of the project how",
    "start": "428000",
    "end": "429680"
  },
  {
    "text": "to make ceph deployable",
    "start": "429680",
    "end": "432000"
  },
  {
    "text": "for",
    "start": "432000",
    "end": "433280"
  },
  {
    "text": "the majority of clusters without having",
    "start": "433280",
    "end": "435360"
  },
  {
    "text": "too much management overhead",
    "start": "435360",
    "end": "437199"
  },
  {
    "text": "we've got some sample",
    "start": "437199",
    "end": "438720"
  },
  {
    "text": "yaml files some manifests that you just",
    "start": "438720",
    "end": "441280"
  },
  {
    "text": "say go create all of those and we'll",
    "start": "441280",
    "end": "443520"
  },
  {
    "text": "create and configure ceph",
    "start": "443520",
    "end": "445919"
  },
  {
    "text": "as desired the last one is cluster.yaml",
    "start": "445919",
    "end": "448800"
  },
  {
    "text": "is the one that's pictured here on the",
    "start": "448800",
    "end": "450960"
  },
  {
    "text": "right that's where you tell",
    "start": "450960",
    "end": "453440"
  },
  {
    "text": "rook what settings you want to deploy",
    "start": "453440",
    "end": "455520"
  },
  {
    "text": "with like how many mons and how to",
    "start": "455520",
    "end": "458000"
  },
  {
    "text": "consume the storage that rooks finds on",
    "start": "458000",
    "end": "460240"
  },
  {
    "text": "those nodes",
    "start": "460240",
    "end": "461680"
  },
  {
    "text": "to create that storage cluster okay we",
    "start": "461680",
    "end": "464080"
  },
  {
    "text": "also have helm charts that make this",
    "start": "464080",
    "end": "466560"
  },
  {
    "text": "deployment even",
    "start": "466560",
    "end": "467919"
  },
  {
    "text": "simpler for those who are using helm",
    "start": "467919",
    "end": "470720"
  },
  {
    "text": "all right so the csi driver brings a",
    "start": "470720",
    "end": "472960"
  },
  {
    "start": "471000",
    "end": "471000"
  },
  {
    "text": "number of features",
    "start": "472960",
    "end": "474319"
  },
  {
    "text": "standard csi features we've got dynamic",
    "start": "474319",
    "end": "476160"
  },
  {
    "text": "provisioning for flock and file storage",
    "start": "476160",
    "end": "478080"
  },
  {
    "text": "we've got volume expansion snapshots and",
    "start": "478080",
    "end": "480240"
  },
  {
    "text": "clones",
    "start": "480240",
    "end": "481280"
  },
  {
    "text": "and",
    "start": "481280",
    "end": "482400"
  },
  {
    "text": "there's this enables all sorts of",
    "start": "482400",
    "end": "484319"
  },
  {
    "text": "applications to do",
    "start": "484319",
    "end": "486080"
  },
  {
    "text": "failover",
    "start": "486080",
    "end": "487599"
  },
  {
    "text": "work across data centers work across",
    "start": "487599",
    "end": "489520"
  },
  {
    "text": "clusters",
    "start": "489520",
    "end": "490720"
  },
  {
    "text": "mirroring and a number of scenarios",
    "start": "490720",
    "end": "493919"
  },
  {
    "text": "so what environments can you deploy rook",
    "start": "493919",
    "end": "495919"
  },
  {
    "start": "494000",
    "end": "494000"
  },
  {
    "text": "in",
    "start": "495919",
    "end": "496879"
  },
  {
    "text": "so first of all primarily where we",
    "start": "496879",
    "end": "498720"
  },
  {
    "text": "started was in bare metal you've got",
    "start": "498720",
    "end": "500479"
  },
  {
    "text": "your own hardware",
    "start": "500479",
    "end": "501840"
  },
  {
    "text": "and on bare metal you don't even have",
    "start": "501840",
    "end": "504479"
  },
  {
    "text": "storage options unless you plug in some",
    "start": "504479",
    "end": "506479"
  },
  {
    "text": "external appliance",
    "start": "506479",
    "end": "508080"
  },
  {
    "text": "but if you have your own hardware you",
    "start": "508080",
    "end": "509840"
  },
  {
    "text": "can deploy rook there to provide that",
    "start": "509840",
    "end": "511360"
  },
  {
    "text": "storage platform",
    "start": "511360",
    "end": "512800"
  },
  {
    "text": "also if you're running in cloud",
    "start": "512800",
    "end": "514000"
  },
  {
    "text": "providers when cloud providers have",
    "start": "514000",
    "end": "516800"
  },
  {
    "text": "limitations in their storage",
    "start": "516800",
    "end": "518800"
  },
  {
    "text": "and you can deploy rook there to also",
    "start": "518800",
    "end": "520320"
  },
  {
    "text": "provide",
    "start": "520320",
    "end": "521279"
  },
  {
    "text": "a consistent storage platform",
    "start": "521279",
    "end": "524480"
  },
  {
    "start": "524000",
    "end": "524000"
  },
  {
    "text": "so let's talk a little bit more about",
    "start": "524480",
    "end": "525760"
  },
  {
    "text": "that the rook in the cloud environment",
    "start": "525760",
    "end": "528000"
  },
  {
    "text": "can overcome shortcomings so some of the",
    "start": "528000",
    "end": "530080"
  },
  {
    "text": "shortcomings include",
    "start": "530080",
    "end": "531600"
  },
  {
    "text": "the cloud provider might only limit",
    "start": "531600",
    "end": "533360"
  },
  {
    "text": "storage to inside a single availability",
    "start": "533360",
    "end": "535839"
  },
  {
    "text": "zone well rook can span az's bailover",
    "start": "535839",
    "end": "539680"
  },
  {
    "text": "times can be long in the in cloud",
    "start": "539680",
    "end": "541680"
  },
  {
    "text": "providers so you can fall over in",
    "start": "541680",
    "end": "543440"
  },
  {
    "text": "seconds instead of minutes you can have",
    "start": "543440",
    "end": "545440"
  },
  {
    "text": "more",
    "start": "545440",
    "end": "546320"
  },
  {
    "text": "basically unlimited number of pbs per",
    "start": "546320",
    "end": "549040"
  },
  {
    "text": "node",
    "start": "549040",
    "end": "550000"
  },
  {
    "text": "instead of you know some providers might",
    "start": "550000",
    "end": "552000"
  },
  {
    "text": "only have a limit of 30 per node",
    "start": "552000",
    "end": "554320"
  },
  {
    "text": "but you can also get better performance",
    "start": "554320",
    "end": "555680"
  },
  {
    "text": "to cost ratio if you provision large pvs",
    "start": "555680",
    "end": "558640"
  },
  {
    "text": "from the cloud provider and then you put",
    "start": "558640",
    "end": "560560"
  },
  {
    "text": "step on top of that get better",
    "start": "560560",
    "end": "562959"
  },
  {
    "text": "performance because of those large",
    "start": "562959",
    "end": "564959"
  },
  {
    "text": "underlying pvs",
    "start": "564959",
    "end": "566480"
  },
  {
    "text": "from the storage provider so ultimately",
    "start": "566480",
    "end": "569360"
  },
  {
    "text": "in this environment also seth can use",
    "start": "569360",
    "end": "571279"
  },
  {
    "text": "pvcs as the underlying storage you just",
    "start": "571279",
    "end": "573680"
  },
  {
    "text": "tell which storage class you want to",
    "start": "573680",
    "end": "575120"
  },
  {
    "text": "provision the storage from",
    "start": "575120",
    "end": "577519"
  },
  {
    "start": "577000",
    "end": "577000"
  },
  {
    "text": "right now rook works well",
    "start": "577519",
    "end": "579440"
  },
  {
    "text": "for many cluster topologies you can tell",
    "start": "579440",
    "end": "581839"
  },
  {
    "text": "it",
    "start": "581839",
    "end": "582560"
  },
  {
    "text": "to work across zones across racks or",
    "start": "582560",
    "end": "584880"
  },
  {
    "text": "whatever your",
    "start": "584880",
    "end": "586240"
  },
  {
    "text": "data center configuration is or in the",
    "start": "586240",
    "end": "588160"
  },
  {
    "text": "cloud",
    "start": "588160",
    "end": "589120"
  },
  {
    "text": "you can spread the sap demons across",
    "start": "589120",
    "end": "591279"
  },
  {
    "text": "failure domains to make sure you don't",
    "start": "591279",
    "end": "593200"
  },
  {
    "text": "fail in a single az",
    "start": "593200",
    "end": "595600"
  },
  {
    "text": "so even if one az goes down",
    "start": "595600",
    "end": "597600"
  },
  {
    "text": "your other two az's can keep running the",
    "start": "597600",
    "end": "599920"
  },
  {
    "text": "applications",
    "start": "599920",
    "end": "601200"
  },
  {
    "text": "and you can tell",
    "start": "601200",
    "end": "602399"
  },
  {
    "text": "how to deploy based on node affinity",
    "start": "602399",
    "end": "604640"
  },
  {
    "text": "paints and tolerations it's very",
    "start": "604640",
    "end": "606160"
  },
  {
    "text": "flexible",
    "start": "606160",
    "end": "607360"
  },
  {
    "text": "all right when you're ready to update",
    "start": "607360",
    "end": "609120"
  },
  {
    "start": "608000",
    "end": "608000"
  },
  {
    "text": "rook or your data layer with seth",
    "start": "609120",
    "end": "611920"
  },
  {
    "text": "rook can handle everything rook can",
    "start": "611920",
    "end": "613440"
  },
  {
    "text": "update",
    "start": "613440",
    "end": "614480"
  },
  {
    "text": "and patch all the set demons to the",
    "start": "614480",
    "end": "616399"
  },
  {
    "text": "latest release",
    "start": "616399",
    "end": "617760"
  },
  {
    "text": "even with ceph major upgrades that",
    "start": "617760",
    "end": "620240"
  },
  {
    "text": "update is automatic automatic",
    "start": "620240",
    "end": "623279"
  },
  {
    "text": "our upgrade guide",
    "start": "623279",
    "end": "625200"
  },
  {
    "text": "can look a little scary kind of a long",
    "start": "625200",
    "end": "627040"
  },
  {
    "text": "document but really it's just trying to",
    "start": "627040",
    "end": "629360"
  },
  {
    "text": "be extra careful make sure you feel",
    "start": "629360",
    "end": "631600"
  },
  {
    "text": "confident knowing your cluster is",
    "start": "631600",
    "end": "633360"
  },
  {
    "text": "healthy before during and after you're",
    "start": "633360",
    "end": "635440"
  },
  {
    "text": "upgrading",
    "start": "635440",
    "end": "636480"
  },
  {
    "text": "another key feature of rook is that the",
    "start": "636480",
    "end": "638560"
  },
  {
    "start": "637000",
    "end": "637000"
  },
  {
    "text": "csi driver allows you to connect to ceph",
    "start": "638560",
    "end": "640720"
  },
  {
    "text": "that is running external to your",
    "start": "640720",
    "end": "642240"
  },
  {
    "text": "kubernetes cluster maybe you already",
    "start": "642240",
    "end": "643839"
  },
  {
    "text": "have a step cluster running or maybe you",
    "start": "643839",
    "end": "645760"
  },
  {
    "text": "just",
    "start": "645760",
    "end": "646399"
  },
  {
    "text": "really don't want to run your storage on",
    "start": "646399",
    "end": "648640"
  },
  {
    "text": "the same hardware with kubernetes or in",
    "start": "648640",
    "end": "650800"
  },
  {
    "text": "the same environment so you can run ceph",
    "start": "650800",
    "end": "653200"
  },
  {
    "text": "independently deploy usually with sep",
    "start": "653200",
    "end": "655600"
  },
  {
    "text": "adm and then connect your cluster to",
    "start": "655600",
    "end": "657920"
  },
  {
    "text": "that",
    "start": "657920",
    "end": "659360"
  },
  {
    "start": "659000",
    "end": "659000"
  },
  {
    "text": "i did mention briefly already",
    "start": "659360",
    "end": "661279"
  },
  {
    "text": "using buckets with object storage the",
    "start": "661279",
    "end": "664480"
  },
  {
    "text": "obcs the object bucket claims allow you",
    "start": "664480",
    "end": "667040"
  },
  {
    "text": "to provision buckets easily",
    "start": "667040",
    "end": "669519"
  },
  {
    "text": "and you know we're looking forward to",
    "start": "669519",
    "end": "671600"
  },
  {
    "text": "the quasi",
    "start": "671600",
    "end": "673040"
  },
  {
    "text": "the container object storage interface",
    "start": "673040",
    "end": "674800"
  },
  {
    "text": "that will be coming",
    "start": "674800",
    "end": "676240"
  },
  {
    "text": "with the kubernetes enhancement soon",
    "start": "676240",
    "end": "678880"
  },
  {
    "text": "alright so rook 1.9 was just released in",
    "start": "678880",
    "end": "680959"
  },
  {
    "text": "april let's talk about some of those new",
    "start": "680959",
    "end": "682880"
  },
  {
    "text": "features that are just out",
    "start": "682880",
    "end": "684880"
  },
  {
    "text": "first of all seth quincy is the latest",
    "start": "684880",
    "end": "686959"
  },
  {
    "start": "685000",
    "end": "685000"
  },
  {
    "text": "major release of seth b17 that was just",
    "start": "686959",
    "end": "689680"
  },
  {
    "text": "released",
    "start": "689680",
    "end": "690720"
  },
  {
    "text": "in april as well",
    "start": "690720",
    "end": "692560"
  },
  {
    "text": "uh on their annual release cycle so with",
    "start": "692560",
    "end": "695360"
  },
  {
    "text": "quincy",
    "start": "695360",
    "end": "696560"
  },
  {
    "text": "comes the latest and greatest storage",
    "start": "696560",
    "end": "698800"
  },
  {
    "text": "layer so we don't get into what those",
    "start": "698800",
    "end": "701040"
  },
  {
    "text": "features all are what rook does support",
    "start": "701040",
    "end": "703120"
  },
  {
    "text": "that latest major release the csi driver",
    "start": "703120",
    "end": "705600"
  },
  {
    "start": "705000",
    "end": "705000"
  },
  {
    "text": "has had some good updates in the 3.6",
    "start": "705600",
    "end": "708640"
  },
  {
    "text": "release that rook deploys by default",
    "start": "708640",
    "end": "711360"
  },
  {
    "text": "so you can for example fuse mount",
    "start": "711360",
    "end": "713040"
  },
  {
    "text": "recovery like we can i can detect the",
    "start": "713040",
    "end": "715600"
  },
  {
    "text": "corruption of fuse mounts and remounted",
    "start": "715600",
    "end": "718079"
  },
  {
    "text": "automatically",
    "start": "718079",
    "end": "719680"
  },
  {
    "text": "there's aws kms encryption and many",
    "start": "719680",
    "end": "722800"
  },
  {
    "text": "other fixes and updates",
    "start": "722800",
    "end": "724639"
  },
  {
    "text": "another major feature is nfs",
    "start": "724639",
    "end": "726240"
  },
  {
    "start": "725000",
    "end": "725000"
  },
  {
    "text": "provisioning so",
    "start": "726240",
    "end": "728160"
  },
  {
    "text": "nfs is still useful in some scenarios",
    "start": "728160",
    "end": "730079"
  },
  {
    "text": "where maybe you're migrating a",
    "start": "730079",
    "end": "732320"
  },
  {
    "text": "legacy workload",
    "start": "732320",
    "end": "734320"
  },
  {
    "text": "into kubernetes so you can create nfs",
    "start": "734320",
    "end": "737440"
  },
  {
    "text": "exports via pvcs now with the csi driver",
    "start": "737440",
    "end": "741680"
  },
  {
    "text": "a csi driver will provision them and",
    "start": "741680",
    "end": "743279"
  },
  {
    "text": "then the kubernetes nfs will well mount",
    "start": "743279",
    "end": "746000"
  },
  {
    "text": "them for you",
    "start": "746000",
    "end": "747360"
  },
  {
    "text": "and that community nfs driver",
    "start": "747360",
    "end": "749760"
  },
  {
    "text": "is available today",
    "start": "749760",
    "end": "751680"
  },
  {
    "text": "and our documentation and brooke",
    "start": "751680",
    "end": "753600"
  },
  {
    "text": "explains how to work through that and",
    "start": "753600",
    "end": "755519"
  },
  {
    "text": "get that working",
    "start": "755519",
    "end": "756959"
  },
  {
    "text": "in this release we do have a new crd for",
    "start": "756959",
    "end": "758959"
  },
  {
    "start": "757000",
    "end": "757000"
  },
  {
    "text": "creating rados name spaces so a rados",
    "start": "758959",
    "end": "761760"
  },
  {
    "text": "namespace is a concept concept that",
    "start": "761760",
    "end": "764560"
  },
  {
    "text": "really gives you isolation and",
    "start": "764560",
    "end": "765680"
  },
  {
    "text": "multi-tenancy",
    "start": "765680",
    "end": "767279"
  },
  {
    "text": "so that you don't need to create",
    "start": "767279",
    "end": "768560"
  },
  {
    "text": "separate pools a pool is kind of a",
    "start": "768560",
    "end": "771360"
  },
  {
    "text": "large",
    "start": "771360",
    "end": "772480"
  },
  {
    "text": "entity inside ceph and so this",
    "start": "772480",
    "end": "775440"
  },
  {
    "text": "just gives you isolation within pools",
    "start": "775440",
    "end": "778160"
  },
  {
    "start": "778000",
    "end": "778000"
  },
  {
    "text": "uh some network features so on what",
    "start": "778160",
    "end": "781120"
  },
  {
    "text": "happens on the wire with set",
    "start": "781120",
    "end": "783120"
  },
  {
    "text": "communication so you can have encryption",
    "start": "783120",
    "end": "785040"
  },
  {
    "text": "on the wire now and compression on the",
    "start": "785040",
    "end": "787440"
  },
  {
    "text": "wire as well",
    "start": "787440",
    "end": "788959"
  },
  {
    "text": "they do require a recent kernel 5.11",
    "start": "788959",
    "end": "791760"
  },
  {
    "text": "and then there's much more of course",
    "start": "791760",
    "end": "793200"
  },
  {
    "text": "lots of fixes with each each update to",
    "start": "793200",
    "end": "795360"
  },
  {
    "text": "rook emission controller for example is",
    "start": "795360",
    "end": "797120"
  },
  {
    "text": "enabled by default if we find the cert",
    "start": "797120",
    "end": "798880"
  },
  {
    "text": "manager is available we support multis",
    "start": "798880",
    "end": "801200"
  },
  {
    "text": "networking now",
    "start": "801200",
    "end": "802639"
  },
  {
    "text": "with our latest release and then updated",
    "start": "802639",
    "end": "805279"
  },
  {
    "text": "prometheus alerts",
    "start": "805279",
    "end": "806800"
  },
  {
    "text": "and much more",
    "start": "806800",
    "end": "808560"
  },
  {
    "text": "and those alerts will really help you",
    "start": "808560",
    "end": "809680"
  },
  {
    "text": "make sure staffed on your storage",
    "start": "809680",
    "end": "811120"
  },
  {
    "text": "cluster is healthy all right now we'll",
    "start": "811120",
    "end": "813200"
  },
  {
    "text": "turn the time over to blaine for a demo",
    "start": "813200",
    "end": "817360"
  },
  {
    "text": "thanks travis",
    "start": "817519",
    "end": "819519"
  },
  {
    "text": "we also want to show you what it's like",
    "start": "819519",
    "end": "820959"
  },
  {
    "text": "to run rook",
    "start": "820959",
    "end": "822240"
  },
  {
    "text": "on an everyday basis so we have a demo",
    "start": "822240",
    "end": "824800"
  },
  {
    "text": "prepared for you",
    "start": "824800",
    "end": "826959"
  },
  {
    "text": "the environment that i'm going to be",
    "start": "826959",
    "end": "828079"
  },
  {
    "text": "using for the demo today i'm running on",
    "start": "828079",
    "end": "831199"
  },
  {
    "text": "openshift",
    "start": "831199",
    "end": "832800"
  },
  {
    "text": "which is running kubernetes 1.22",
    "start": "832800",
    "end": "836560"
  },
  {
    "text": "this is on",
    "start": "836560",
    "end": "838320"
  },
  {
    "text": "amazon web services i'm going to have",
    "start": "838320",
    "end": "840320"
  },
  {
    "text": "three control nodes for kubernetes and",
    "start": "840320",
    "end": "842639"
  },
  {
    "text": "three worker nodes",
    "start": "842639",
    "end": "844880"
  },
  {
    "text": "i've chosen to use m5.8x large nodes",
    "start": "844880",
    "end": "847920"
  },
  {
    "text": "here",
    "start": "847920",
    "end": "848880"
  },
  {
    "text": "this will allow us to run",
    "start": "848880",
    "end": "851120"
  },
  {
    "text": "the final size of the stuff cluster",
    "start": "851120",
    "end": "852639"
  },
  {
    "text": "we're going to get to as well as having",
    "start": "852639",
    "end": "855279"
  },
  {
    "text": "about 50 of the node left over for user",
    "start": "855279",
    "end": "857519"
  },
  {
    "text": "applications",
    "start": "857519",
    "end": "858880"
  },
  {
    "text": "which is just kind of a rough estimate",
    "start": "858880",
    "end": "861600"
  },
  {
    "text": "i'm going to be using",
    "start": "861600",
    "end": "863040"
  },
  {
    "text": "slow but pretty",
    "start": "863040",
    "end": "865600"
  },
  {
    "text": "cost effective gp2 volumes",
    "start": "865600",
    "end": "869040"
  },
  {
    "text": "and this is using what is",
    "start": "869040",
    "end": "871519"
  },
  {
    "text": "at the time i'm recording this the",
    "start": "871519",
    "end": "872800"
  },
  {
    "text": "latest version of rook which is 1.9.0",
    "start": "872800",
    "end": "875680"
  },
  {
    "text": "and the pre-release version of seth",
    "start": "875680",
    "end": "878160"
  },
  {
    "text": "quincy which is version 17 of seth's",
    "start": "878160",
    "end": "881279"
  },
  {
    "text": "release",
    "start": "881279",
    "end": "884000"
  },
  {
    "start": "883000",
    "end": "883000"
  },
  {
    "text": "before getting into the demo",
    "start": "884000",
    "end": "885920"
  },
  {
    "text": "i want to briefly talk about the two",
    "start": "885920",
    "end": "888079"
  },
  {
    "text": "basic types of rook ceph clusters that",
    "start": "888079",
    "end": "890160"
  },
  {
    "text": "we",
    "start": "890160",
    "end": "890959"
  },
  {
    "text": "kind of talk about",
    "start": "890959",
    "end": "892720"
  },
  {
    "text": "this is",
    "start": "892720",
    "end": "894399"
  },
  {
    "text": "host-based clusters and pvc-based",
    "start": "894399",
    "end": "896839"
  },
  {
    "text": "clusters so for host-based clusters this",
    "start": "896839",
    "end": "899440"
  },
  {
    "text": "is going to",
    "start": "899440",
    "end": "901120"
  },
  {
    "text": "rook is just going to look at the node",
    "start": "901120",
    "end": "903120"
  },
  {
    "text": "itself and pick up disks in order to use",
    "start": "903120",
    "end": "905600"
  },
  {
    "text": "those for",
    "start": "905600",
    "end": "907199"
  },
  {
    "text": "ceph osds",
    "start": "907199",
    "end": "908959"
  },
  {
    "text": "in a pvc based cluster",
    "start": "908959",
    "end": "911519"
  },
  {
    "text": "we instead instruct to use ppcs",
    "start": "911519",
    "end": "914959"
  },
  {
    "text": "these might be dynamic from like gp2",
    "start": "914959",
    "end": "917519"
  },
  {
    "text": "today or these might be local persistent",
    "start": "917519",
    "end": "920800"
  },
  {
    "text": "volumes that you've created yourself",
    "start": "920800",
    "end": "923199"
  },
  {
    "text": "that represent physical disks on the",
    "start": "923199",
    "end": "925279"
  },
  {
    "text": "hardware but are claimed",
    "start": "925279",
    "end": "928079"
  },
  {
    "text": "via kubernetes native mechanisms",
    "start": "928079",
    "end": "932000"
  },
  {
    "start": "931000",
    "end": "931000"
  },
  {
    "text": "for the host-based cluster this is",
    "start": "932160",
    "end": "934320"
  },
  {
    "text": "suitable for a simple cluster especially",
    "start": "934320",
    "end": "936480"
  },
  {
    "text": "for proof of concept clusters",
    "start": "936480",
    "end": "938560"
  },
  {
    "text": "we can say use all nodes use all devices",
    "start": "938560",
    "end": "941440"
  },
  {
    "text": "and it becomes pretty easy",
    "start": "941440",
    "end": "944240"
  },
  {
    "text": "this starts to get complicated when",
    "start": "944240",
    "end": "946880"
  },
  {
    "text": "we don't use all nodes",
    "start": "946880",
    "end": "948800"
  },
  {
    "text": "or all devices",
    "start": "948800",
    "end": "950560"
  },
  {
    "text": "when we're using heterogeneous hardware",
    "start": "950560",
    "end": "953120"
  },
  {
    "text": "or if we want to for whatever reason",
    "start": "953120",
    "end": "955440"
  },
  {
    "text": "customize the device layout on a per",
    "start": "955440",
    "end": "957279"
  },
  {
    "text": "node basis",
    "start": "957279",
    "end": "959920"
  },
  {
    "text": "the pvc based cluster",
    "start": "959920",
    "end": "962639"
  },
  {
    "text": "on the surface seems a little more",
    "start": "962639",
    "end": "963839"
  },
  {
    "text": "complicated but we don't need to",
    "start": "963839",
    "end": "965360"
  },
  {
    "text": "describe hardware configuration",
    "start": "965360",
    "end": "967519"
  },
  {
    "text": "and it becomes pretty easy to expand we",
    "start": "967519",
    "end": "969839"
  },
  {
    "text": "can increase the count of disks that we",
    "start": "969839",
    "end": "972079"
  },
  {
    "text": "use for stuff osds",
    "start": "972079",
    "end": "974000"
  },
  {
    "text": "or we can increase the size of those",
    "start": "974000",
    "end": "975759"
  },
  {
    "text": "disks",
    "start": "975759",
    "end": "977440"
  },
  {
    "text": "by increasing the resources.size",
    "start": "977440",
    "end": "980160"
  },
  {
    "text": "parameter here in the storage class",
    "start": "980160",
    "end": "982160"
  },
  {
    "text": "device set",
    "start": "982160",
    "end": "984720"
  },
  {
    "start": "984000",
    "end": "984000"
  },
  {
    "text": "jumping straight into the demo i want to",
    "start": "986000",
    "end": "988240"
  },
  {
    "text": "break it down onto what we're going to",
    "start": "988240",
    "end": "990000"
  },
  {
    "text": "see first",
    "start": "990000",
    "end": "991759"
  },
  {
    "text": "so we're going to see creating the rook",
    "start": "991759",
    "end": "993360"
  },
  {
    "text": "operator",
    "start": "993360",
    "end": "994800"
  },
  {
    "text": "from there what it's like to create the",
    "start": "994800",
    "end": "997120"
  },
  {
    "text": "the rook ceph cluster",
    "start": "997120",
    "end": "999600"
  },
  {
    "text": "from there uh we have something",
    "start": "999600",
    "end": "1001440"
  },
  {
    "text": "something we've been working on for a",
    "start": "1001440",
    "end": "1002639"
  },
  {
    "text": "little while and it's a little new for a",
    "start": "1002639",
    "end": "1004480"
  },
  {
    "text": "kubecon we're going to be using a crew",
    "start": "1004480",
    "end": "1006720"
  },
  {
    "text": "plugin that we've created for rook ceph",
    "start": "1006720",
    "end": "1009440"
  },
  {
    "text": "to see some of those cluster details",
    "start": "1009440",
    "end": "1011839"
  },
  {
    "text": "and we're going to use that throughout",
    "start": "1011839",
    "end": "1012800"
  },
  {
    "text": "the rest of the demo when we expand the",
    "start": "1012800",
    "end": "1014720"
  },
  {
    "text": "subcluster's osd size as well as expand",
    "start": "1014720",
    "end": "1017920"
  },
  {
    "text": "the subcluster's osd count",
    "start": "1017920",
    "end": "1021360"
  },
  {
    "text": "and for this demo we're also using",
    "start": "1021360",
    "end": "1023839"
  },
  {
    "text": "recommended configurations for",
    "start": "1023839",
    "end": "1026319"
  },
  {
    "text": "production",
    "start": "1026319",
    "end": "1027678"
  },
  {
    "text": "so",
    "start": "1027679",
    "end": "1028558"
  },
  {
    "text": "we'll have these files be provided to",
    "start": "1028559",
    "end": "1030640"
  },
  {
    "text": "you as well so that you can reference",
    "start": "1030640",
    "end": "1033438"
  },
  {
    "text": "what it's like to run a kind of best",
    "start": "1033439",
    "end": "1035199"
  },
  {
    "text": "practice",
    "start": "1035199",
    "end": "1036558"
  },
  {
    "text": "cluster",
    "start": "1036559",
    "end": "1039199"
  },
  {
    "start": "1038000",
    "end": "1038000"
  },
  {
    "text": "so first off uh we talked about creating",
    "start": "1039199",
    "end": "1041199"
  },
  {
    "text": "the work operator this is really just as",
    "start": "1041199",
    "end": "1043360"
  },
  {
    "text": "simple as creating a deployment a pod",
    "start": "1043360",
    "end": "1045360"
  },
  {
    "text": "that runs on",
    "start": "1045360",
    "end": "1047038"
  },
  {
    "text": "some kubernetes node i've depicted it",
    "start": "1047039",
    "end": "1049200"
  },
  {
    "text": "here on worker one although this really",
    "start": "1049200",
    "end": "1050960"
  },
  {
    "text": "might be any",
    "start": "1050960",
    "end": "1053120"
  },
  {
    "text": "available",
    "start": "1053120",
    "end": "1054400"
  },
  {
    "text": "and",
    "start": "1054400",
    "end": "1055360"
  },
  {
    "text": "schedulable worker on your kubernetes",
    "start": "1055360",
    "end": "1057360"
  },
  {
    "text": "node",
    "start": "1057360",
    "end": "1058880"
  },
  {
    "text": "let's jump over to my terminal here and",
    "start": "1058880",
    "end": "1060640"
  },
  {
    "text": "we'll see what it's like to install the",
    "start": "1060640",
    "end": "1062960"
  },
  {
    "text": "operator",
    "start": "1062960",
    "end": "1064799"
  },
  {
    "text": "the first thing we're going to want to",
    "start": "1064799",
    "end": "1065840"
  },
  {
    "text": "do is install some prerequisites this is",
    "start": "1065840",
    "end": "1068400"
  },
  {
    "text": "going to be crds that rook will use for",
    "start": "1068400",
    "end": "1072640"
  },
  {
    "text": "taking your user configuration about",
    "start": "1072640",
    "end": "1074799"
  },
  {
    "text": "cluster and add-ons and as well",
    "start": "1074799",
    "end": "1077520"
  },
  {
    "text": "role-based authentication which is going",
    "start": "1077520",
    "end": "1079360"
  },
  {
    "text": "to give permissions to create the",
    "start": "1079360",
    "end": "1081200"
  },
  {
    "text": "storage that it needs",
    "start": "1081200",
    "end": "1084640"
  },
  {
    "text": "once that's done we can create the",
    "start": "1084640",
    "end": "1086160"
  },
  {
    "text": "operator itself",
    "start": "1086160",
    "end": "1087840"
  },
  {
    "text": "because i'm running an open shift i'm",
    "start": "1087840",
    "end": "1089600"
  },
  {
    "text": "running the openshift flavor of this",
    "start": "1089600",
    "end": "1091120"
  },
  {
    "text": "operator",
    "start": "1091120",
    "end": "1092720"
  },
  {
    "text": "although if you're running on normal",
    "start": "1092720",
    "end": "1094320"
  },
  {
    "text": "kubernetes just operator.yaml is the one",
    "start": "1094320",
    "end": "1096960"
  },
  {
    "text": "you want",
    "start": "1096960",
    "end": "1099360"
  },
  {
    "text": "and we'll see that it gets scheduled and",
    "start": "1099919",
    "end": "1101919"
  },
  {
    "text": "it starts running pretty soon after here",
    "start": "1101919",
    "end": "1104080"
  },
  {
    "text": "on the left side",
    "start": "1104080",
    "end": "1106720"
  },
  {
    "text": "from here i'm going to go ahead and",
    "start": "1106720",
    "end": "1108000"
  },
  {
    "text": "start cluster installation",
    "start": "1108000",
    "end": "1110320"
  },
  {
    "text": "you can see that",
    "start": "1110320",
    "end": "1111760"
  },
  {
    "text": "that rook",
    "start": "1111760",
    "end": "1112960"
  },
  {
    "text": "starts scheduling some resources already",
    "start": "1112960",
    "end": "1116640"
  },
  {
    "text": "this is going to take about 10 minutes",
    "start": "1116640",
    "end": "1118880"
  },
  {
    "text": "so while this is running we're going to",
    "start": "1118880",
    "end": "1120480"
  },
  {
    "text": "jump back to the presentation",
    "start": "1120480",
    "end": "1122480"
  },
  {
    "text": "these resources are some ancillary",
    "start": "1122480",
    "end": "1124480"
  },
  {
    "text": "resources to start off with including",
    "start": "1124480",
    "end": "1126160"
  },
  {
    "text": "the csi driver",
    "start": "1126160",
    "end": "1128000"
  },
  {
    "start": "1128000",
    "end": "1128000"
  },
  {
    "text": "but what i want to drill down into is",
    "start": "1128000",
    "end": "1129760"
  },
  {
    "text": "really the core stuff components",
    "start": "1129760",
    "end": "1132400"
  },
  {
    "text": "so you can see here spread across our",
    "start": "1132400",
    "end": "1134160"
  },
  {
    "text": "worker nodes three monitors three osds",
    "start": "1134160",
    "end": "1137840"
  },
  {
    "text": "and",
    "start": "1137840",
    "end": "1138640"
  },
  {
    "text": "what's called a manager",
    "start": "1138640",
    "end": "1141200"
  },
  {
    "text": "the monitors are what i like to think of",
    "start": "1141200",
    "end": "1143600"
  },
  {
    "text": "as the brains of the stuff cluster",
    "start": "1143600",
    "end": "1145840"
  },
  {
    "text": "the manager provides",
    "start": "1145840",
    "end": "1148480"
  },
  {
    "text": "cli and api services for the ceph",
    "start": "1148480",
    "end": "1150480"
  },
  {
    "text": "cluster",
    "start": "1150480",
    "end": "1151520"
  },
  {
    "text": "and the osds are really what provides",
    "start": "1151520",
    "end": "1153440"
  },
  {
    "text": "the backing storage",
    "start": "1153440",
    "end": "1155200"
  },
  {
    "text": "uh on the nitty-gritty details",
    "start": "1155200",
    "end": "1159440"
  },
  {
    "text": "the way that this cluster is configured",
    "start": "1160000",
    "end": "1162160"
  },
  {
    "text": "the monitor should be spread across the",
    "start": "1162160",
    "end": "1163919"
  },
  {
    "text": "nodes",
    "start": "1163919",
    "end": "1165039"
  },
  {
    "text": "and the osds should as well",
    "start": "1165039",
    "end": "1169120"
  },
  {
    "text": "all right a little over 10 minutes has",
    "start": "1172880",
    "end": "1175520"
  },
  {
    "text": "passed",
    "start": "1175520",
    "end": "1176640"
  },
  {
    "text": "most of this time was spent setting up",
    "start": "1176640",
    "end": "1178720"
  },
  {
    "text": "the rick monitors",
    "start": "1178720",
    "end": "1180240"
  },
  {
    "text": "this seems to be a result of the",
    "start": "1180240",
    "end": "1182960"
  },
  {
    "text": "pvc driver for gp2 being a little slow",
    "start": "1182960",
    "end": "1186559"
  },
  {
    "text": "so from here let's look at the crew",
    "start": "1186559",
    "end": "1189039"
  },
  {
    "text": "plug-in and how that gets used",
    "start": "1189039",
    "end": "1192799"
  },
  {
    "text": "the first thing being to install the",
    "start": "1192799",
    "end": "1195600"
  },
  {
    "text": "rucksack crew plugin so that's crew",
    "start": "1195600",
    "end": "1197440"
  },
  {
    "text": "install rxf",
    "start": "1197440",
    "end": "1198880"
  },
  {
    "text": "i've already got this installed and from",
    "start": "1198880",
    "end": "1201120"
  },
  {
    "text": "here",
    "start": "1201120",
    "end": "1202159"
  },
  {
    "text": "um let's check out some of the some of",
    "start": "1202159",
    "end": "1204400"
  },
  {
    "text": "the commands that we have going in this",
    "start": "1204400",
    "end": "1206159"
  },
  {
    "text": "version",
    "start": "1206159",
    "end": "1208080"
  },
  {
    "text": "so we have a basic overall rook status",
    "start": "1208080",
    "end": "1210880"
  },
  {
    "text": "this is going to show us the status of",
    "start": "1210880",
    "end": "1212480"
  },
  {
    "text": "this rucksaf cluster",
    "start": "1212480",
    "end": "1215200"
  },
  {
    "text": "we can see that it was created",
    "start": "1215200",
    "end": "1216400"
  },
  {
    "text": "successfully",
    "start": "1216400",
    "end": "1217919"
  },
  {
    "text": "and we can also see that it's in health",
    "start": "1217919",
    "end": "1220000"
  },
  {
    "text": "worn state right now",
    "start": "1220000",
    "end": "1222640"
  },
  {
    "text": "so if i suspect something's wrong i",
    "start": "1222640",
    "end": "1225039"
  },
  {
    "text": "might want to",
    "start": "1225039",
    "end": "1226480"
  },
  {
    "text": "uh set the rook log level uh to debug so",
    "start": "1226480",
    "end": "1229520"
  },
  {
    "text": "i can get more information out of the",
    "start": "1229520",
    "end": "1231360"
  },
  {
    "text": "rook operator logs",
    "start": "1231360",
    "end": "1233280"
  },
  {
    "text": "so we can say",
    "start": "1233280",
    "end": "1234799"
  },
  {
    "text": "coop cuddle or accept operator set rook",
    "start": "1234799",
    "end": "1237200"
  },
  {
    "text": "log level to debug",
    "start": "1237200",
    "end": "1239520"
  },
  {
    "text": "and this will do that for us",
    "start": "1239520",
    "end": "1241840"
  },
  {
    "text": "i don't really want to run through",
    "start": "1241840",
    "end": "1242799"
  },
  {
    "text": "debugging right now",
    "start": "1242799",
    "end": "1244960"
  },
  {
    "text": "but we can also run a sep command so we",
    "start": "1244960",
    "end": "1247679"
  },
  {
    "text": "can just say rook ceph status and this",
    "start": "1247679",
    "end": "1250000"
  },
  {
    "text": "will give us the overall sub status and",
    "start": "1250000",
    "end": "1252400"
  },
  {
    "text": "we now see the cost the health of our",
    "start": "1252400",
    "end": "1254080"
  },
  {
    "text": "cluster is okay",
    "start": "1254080",
    "end": "1256159"
  },
  {
    "text": "it's just taken an extra minute for the",
    "start": "1256159",
    "end": "1258480"
  },
  {
    "text": "cluster to stabilize and become ready",
    "start": "1258480",
    "end": "1260720"
  },
  {
    "text": "and in addition here we see that we have",
    "start": "1260720",
    "end": "1264159"
  },
  {
    "text": "three osds that are up and ready and we",
    "start": "1264159",
    "end": "1266960"
  },
  {
    "text": "have 30 gigabytes of raw capacity",
    "start": "1266960",
    "end": "1270000"
  },
  {
    "text": "each of our osd's is 10 gigabytes so",
    "start": "1270000",
    "end": "1272159"
  },
  {
    "text": "this is really what we expect",
    "start": "1272159",
    "end": "1274720"
  },
  {
    "text": "i also want to talk about a ceph command",
    "start": "1274720",
    "end": "1276400"
  },
  {
    "text": "that's a little bit advanced called set",
    "start": "1276400",
    "end": "1278000"
  },
  {
    "text": "osd tree",
    "start": "1278000",
    "end": "1280000"
  },
  {
    "text": "and this is going to show us the",
    "start": "1280000",
    "end": "1282080"
  },
  {
    "text": "hierarchy of ceph osb says ceph",
    "start": "1282080",
    "end": "1284799"
  },
  {
    "text": "understands it",
    "start": "1284799",
    "end": "1286559"
  },
  {
    "text": "so we can see that we have osds running",
    "start": "1286559",
    "end": "1288960"
  },
  {
    "text": "in one region this is u.s west one",
    "start": "1288960",
    "end": "1291919"
  },
  {
    "text": "and within this region we have two zones",
    "start": "1291919",
    "end": "1294799"
  },
  {
    "text": "being",
    "start": "1294799",
    "end": "1295919"
  },
  {
    "text": "west one b and west one c",
    "start": "1295919",
    "end": "1298880"
  },
  {
    "text": "i have two nodes in one b and one node",
    "start": "1298880",
    "end": "1301280"
  },
  {
    "text": "n1c and so the osds are spread",
    "start": "1301280",
    "end": "1304400"
  },
  {
    "text": "2 and 1b",
    "start": "1304400",
    "end": "1305760"
  },
  {
    "text": "and 1 and 1c here",
    "start": "1305760",
    "end": "1308400"
  },
  {
    "text": "30 gigabytes is not really a lot of",
    "start": "1308400",
    "end": "1310320"
  },
  {
    "text": "capacity this is great for just testing",
    "start": "1310320",
    "end": "1312559"
  },
  {
    "text": "things out but at some point",
    "start": "1312559",
    "end": "1314799"
  },
  {
    "text": "we definitely want to increase this we",
    "start": "1314799",
    "end": "1318400"
  },
  {
    "text": "can do that by editing our cluster",
    "start": "1318400",
    "end": "1321280"
  },
  {
    "text": "manifest",
    "start": "1321280",
    "end": "1322640"
  },
  {
    "text": "and here i'm going to increase the",
    "start": "1322640",
    "end": "1324000"
  },
  {
    "text": "storage size to 100 gigabytes",
    "start": "1324000",
    "end": "1326720"
  },
  {
    "text": "per each of these three claims",
    "start": "1326720",
    "end": "1329760"
  },
  {
    "text": "and i'm going to apply those changes",
    "start": "1329760",
    "end": "1331679"
  },
  {
    "text": "again",
    "start": "1331679",
    "end": "1332960"
  },
  {
    "text": "and we should see that rook begins",
    "start": "1332960",
    "end": "1335600"
  },
  {
    "text": "reconciling this change and in a few",
    "start": "1335600",
    "end": "1337679"
  },
  {
    "text": "minutes we should have",
    "start": "1337679",
    "end": "1339840"
  },
  {
    "text": "300 raw gigabytes of storage",
    "start": "1339840",
    "end": "1342880"
  },
  {
    "text": "so what's happening here visually",
    "start": "1342880",
    "end": "1345520"
  },
  {
    "text": "is just we're increasing the size of",
    "start": "1345520",
    "end": "1348080"
  },
  {
    "text": "these persistent volumes",
    "start": "1348080",
    "end": "1350640"
  },
  {
    "text": "attached to the osds and right now we're",
    "start": "1350640",
    "end": "1353520"
  },
  {
    "text": "increasing them",
    "start": "1353520",
    "end": "1354799"
  },
  {
    "text": "tenfold",
    "start": "1354799",
    "end": "1356159"
  },
  {
    "text": "but that could be whatever amount is",
    "start": "1356159",
    "end": "1357760"
  },
  {
    "text": "right for you in your cluster",
    "start": "1357760",
    "end": "1361280"
  },
  {
    "text": "jumping back to see what our cluster is",
    "start": "1361280",
    "end": "1362799"
  },
  {
    "text": "doing and skipping a few minutes ahead",
    "start": "1362799",
    "end": "1366559"
  },
  {
    "text": "we can now see that our osd's have each",
    "start": "1366559",
    "end": "1369840"
  },
  {
    "text": "re-initialized",
    "start": "1369840",
    "end": "1372799"
  },
  {
    "text": "and jumping back to our crew plugin we",
    "start": "1373520",
    "end": "1377120"
  },
  {
    "text": "can get the stuff status",
    "start": "1377120",
    "end": "1379360"
  },
  {
    "text": "and we should see and indeed do see that",
    "start": "1379360",
    "end": "1382400"
  },
  {
    "text": "we have 300 raw gigabytes of capacity",
    "start": "1382400",
    "end": "1387400"
  },
  {
    "start": "1389000",
    "end": "1389000"
  },
  {
    "text": "at some point we will",
    "start": "1389760",
    "end": "1392159"
  },
  {
    "text": "reach the iops limit of these gp2",
    "start": "1392159",
    "end": "1394559"
  },
  {
    "text": "volumes with scale up",
    "start": "1394559",
    "end": "1396960"
  },
  {
    "text": "increasing them in size more won't",
    "start": "1396960",
    "end": "1399280"
  },
  {
    "text": "really get us more iops",
    "start": "1399280",
    "end": "1402240"
  },
  {
    "text": "so we may want to scale out the cluster",
    "start": "1402240",
    "end": "1405600"
  },
  {
    "text": "instead to create more osds and this",
    "start": "1405600",
    "end": "1408080"
  },
  {
    "text": "will effectively allow us to get",
    "start": "1408080",
    "end": "1410880"
  },
  {
    "text": "better performance for the size we're",
    "start": "1410880",
    "end": "1412960"
  },
  {
    "text": "increasing",
    "start": "1412960",
    "end": "1415360"
  },
  {
    "text": "again we're going to want to edit a",
    "start": "1415360",
    "end": "1417039"
  },
  {
    "text": "cluster manifest",
    "start": "1417039",
    "end": "1419440"
  },
  {
    "text": "and here instead of changing the size",
    "start": "1419440",
    "end": "1421679"
  },
  {
    "text": "we're going to be changing the count so",
    "start": "1421679",
    "end": "1423279"
  },
  {
    "text": "i'm going to change this from 3 to 6",
    "start": "1423279",
    "end": "1425600"
  },
  {
    "text": "which is doubling the number of osds and",
    "start": "1425600",
    "end": "1428080"
  },
  {
    "text": "should double the available capacity",
    "start": "1428080",
    "end": "1430480"
  },
  {
    "text": "to 600 gigabytes",
    "start": "1430480",
    "end": "1432400"
  },
  {
    "text": "once i apply this",
    "start": "1432400",
    "end": "1435360"
  },
  {
    "text": "to look at this visually",
    "start": "1436000",
    "end": "1438559"
  },
  {
    "text": "we are adding three new osd's here in",
    "start": "1438559",
    "end": "1441919"
  },
  {
    "text": "blue",
    "start": "1441919",
    "end": "1442960"
  },
  {
    "text": "and",
    "start": "1442960",
    "end": "1444480"
  },
  {
    "text": "rook is going to create these and try to",
    "start": "1444480",
    "end": "1446720"
  },
  {
    "text": "spread them evenly across the node so we",
    "start": "1446720",
    "end": "1448720"
  },
  {
    "text": "should see",
    "start": "1448720",
    "end": "1450159"
  },
  {
    "text": "one extra osd on each node running",
    "start": "1450159",
    "end": "1454320"
  },
  {
    "text": "skipping forward a few minutes until",
    "start": "1454320",
    "end": "1455760"
  },
  {
    "text": "when that's done",
    "start": "1455760",
    "end": "1459320"
  },
  {
    "text": "we can again go back to our crew plugin",
    "start": "1461360",
    "end": "1463760"
  },
  {
    "text": "and see",
    "start": "1463760",
    "end": "1464720"
  },
  {
    "text": "the cf status",
    "start": "1464720",
    "end": "1467600"
  },
  {
    "text": "and we now have six osds and 600",
    "start": "1469520",
    "end": "1472640"
  },
  {
    "text": "gigabytes available",
    "start": "1472640",
    "end": "1475520"
  },
  {
    "text": "this is also where i'm going to come",
    "start": "1475919",
    "end": "1477520"
  },
  {
    "text": "back to our osd tree command",
    "start": "1477520",
    "end": "1481120"
  },
  {
    "text": "and we can see",
    "start": "1482240",
    "end": "1483840"
  },
  {
    "text": "all of these six osds and now there are",
    "start": "1483840",
    "end": "1488559"
  },
  {
    "text": "four osds in the zone with two nodes and",
    "start": "1488559",
    "end": "1491279"
  },
  {
    "text": "two osds in the zone with one node",
    "start": "1491279",
    "end": "1494799"
  },
  {
    "text": "i do want to make one small note",
    "start": "1494799",
    "end": "1496960"
  },
  {
    "start": "1495000",
    "end": "1495000"
  },
  {
    "text": "with this scale out i talked about",
    "start": "1496960",
    "end": "1498799"
  },
  {
    "text": "potentially doing this for performance",
    "start": "1498799",
    "end": "1500880"
  },
  {
    "text": "reasons",
    "start": "1500880",
    "end": "1502000"
  },
  {
    "text": "another option is to",
    "start": "1502000",
    "end": "1504400"
  },
  {
    "text": "create a new storage glass device set",
    "start": "1504400",
    "end": "1506720"
  },
  {
    "text": "rather than",
    "start": "1506720",
    "end": "1508080"
  },
  {
    "text": "expanding an existing one",
    "start": "1508080",
    "end": "1510159"
  },
  {
    "text": "with a new storage class device set i",
    "start": "1510159",
    "end": "1512080"
  },
  {
    "text": "might use",
    "start": "1512080",
    "end": "1513440"
  },
  {
    "text": "a different and faster storage like io2",
    "start": "1513440",
    "end": "1516799"
  },
  {
    "text": "instead of gp2",
    "start": "1516799",
    "end": "1518720"
  },
  {
    "text": "and i could provide this",
    "start": "1518720",
    "end": "1521039"
  },
  {
    "text": "as",
    "start": "1521039",
    "end": "1521919"
  },
  {
    "text": "a backing pool for faster storage for",
    "start": "1521919",
    "end": "1524559"
  },
  {
    "text": "some user applications if",
    "start": "1524559",
    "end": "1526640"
  },
  {
    "text": "i have different users that have",
    "start": "1526640",
    "end": "1528080"
  },
  {
    "text": "different storage speed needs obviously",
    "start": "1528080",
    "end": "1530559"
  },
  {
    "text": "the faster storage",
    "start": "1530559",
    "end": "1532240"
  },
  {
    "text": "is going to cost me more and so i",
    "start": "1532240",
    "end": "1534240"
  },
  {
    "text": "probably want a little bit less of it",
    "start": "1534240",
    "end": "1537360"
  },
  {
    "text": "thank you so much i'll pass that back to",
    "start": "1537360",
    "end": "1538960"
  },
  {
    "text": "travis to wrap things up",
    "start": "1538960",
    "end": "1540720"
  },
  {
    "text": "all right thanks blaine for that demo",
    "start": "1540720",
    "end": "1542559"
  },
  {
    "start": "1541000",
    "end": "1541000"
  },
  {
    "text": "again we refer you back to all these",
    "start": "1542559",
    "end": "1543679"
  },
  {
    "text": "resources we have for getting started",
    "start": "1543679",
    "end": "1545840"
  },
  {
    "text": "with rook the website the docs please",
    "start": "1545840",
    "end": "1547840"
  },
  {
    "text": "join our slack it's a great place for",
    "start": "1547840",
    "end": "1550559"
  },
  {
    "text": "asking questions",
    "start": "1550559",
    "end": "1552080"
  },
  {
    "text": "or go to our github join our community",
    "start": "1552080",
    "end": "1554240"
  },
  {
    "text": "meeting if you have if you want to talk",
    "start": "1554240",
    "end": "1556000"
  },
  {
    "text": "to us over the call",
    "start": "1556000",
    "end": "1557919"
  },
  {
    "text": "and again check out our training videos",
    "start": "1557919",
    "end": "1559840"
  },
  {
    "text": "to get started",
    "start": "1559840",
    "end": "1561600"
  },
  {
    "text": "thanks for joining",
    "start": "1561600",
    "end": "1564640"
  }
]