[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "hello and welcome to kubecon Europe 2023 this is kubernetes Signet intra and deep",
    "start": "0",
    "end": "6359"
  },
  {
    "text": "dive and uh today uh your speakers are Don Chen from Google they're a car from",
    "start": "6359",
    "end": "12540"
  },
  {
    "text": "Red Hat Sergey can jelly from Google and Bruno from Red Hat we all uh",
    "start": "12540",
    "end": "19020"
  },
  {
    "text": "chairs and TLS of signaled and we're happy to presenting you uh what's happening with Signet and what you're",
    "start": "19020",
    "end": "25500"
  },
  {
    "text": "working on before we begin please remember to be",
    "start": "25500",
    "end": "30840"
  },
  {
    "start": "27000",
    "end": "27000"
  },
  {
    "text": "nice to each other and read a code of conduct if you're interested previously we updated uh",
    "start": "30840",
    "end": "38880"
  },
  {
    "text": "commission signaled achievements and covered everything happened up to 126 of",
    "start": "38880",
    "end": "44579"
  },
  {
    "text": "kubernetes at kubecon North America 2022. you can find recording and slides",
    "start": "44579",
    "end": "51180"
  },
  {
    "text": "everything available online so if you're interested what was happening before please go there we covered interesting",
    "start": "51180",
    "end": "58800"
  },
  {
    "text": "deep Dives uh during that presentation you may be interested in C group V2 and in place memory in Place Port upgrade",
    "start": "58800",
    "end": "68760"
  },
  {
    "text": "today we will cover who are going to seek not either view again uh we will talk about signal",
    "start": "68760",
    "end": "77420"
  },
  {
    "text": "areas of interest and then we can deep dive into Google resource management",
    "start": "77420",
    "end": "82979"
  },
  {
    "text": "after that we will talk about what was happening in 126 and 127",
    "start": "82979",
    "end": "88080"
  },
  {
    "text": "what our plans for 128 and it will go into one of the highlights of recent developments the sidecar containers",
    "start": "88080",
    "end": "95100"
  },
  {
    "text": "finally we will talk about leadership updates and the ways you can be involved into our community with all that let's",
    "start": "95100",
    "end": "102000"
  },
  {
    "text": "go to Signet app review Signet is uh",
    "start": "102000",
    "end": "107240"
  },
  {
    "start": "104000",
    "end": "104000"
  },
  {
    "text": "responsible for many things happening on a kubernetes nodes it consists of multiple areas as kublet",
    "start": "107240",
    "end": "115259"
  },
  {
    "text": "uh it talks about node and Port lifecycle how they managed and what kind",
    "start": "115259",
    "end": "121560"
  },
  {
    "text": "of stages they're going through it also talks about resource management and during the today presentation will",
    "start": "121560",
    "end": "127380"
  },
  {
    "text": "go deep into Resource Management specifically and also communicates with operating system through the container runtimes",
    "start": "127380",
    "end": "134459"
  },
  {
    "start": "134000",
    "end": "134000"
  },
  {
    "text": "our Charter is to be responsible for all the components that control interaction",
    "start": "134459",
    "end": "141180"
  },
  {
    "text": "between Port and host resources signal is vertical seek so we control",
    "start": "141180",
    "end": "146400"
  },
  {
    "text": "specific area of code and opposed to some horizontal six",
    "start": "146400",
    "end": "152900"
  },
  {
    "text": "instrumentation uh vertical Sig means that we own specific components and if",
    "start": "152900",
    "end": "158459"
  },
  {
    "text": "somebody wants to change any feature touching this component we need to be involved",
    "start": "158459",
    "end": "165599"
  },
  {
    "text": "and we have multiple sub projects Beyond one of the horizontal sub-project is CI",
    "start": "165599",
    "end": "172080"
  },
  {
    "text": "sub project that uh doing some uh where we watch for reliability of components",
    "start": "172080",
    "end": "178200"
  },
  {
    "text": "of signals and looking at CI status we also have sub-project by specific",
    "start": "178200",
    "end": "183660"
  },
  {
    "text": "components there's a project for couplet for contain runtime interface",
    "start": "183660",
    "end": "188760"
  },
  {
    "text": "for not problem detector something that will detect the notify you about the",
    "start": "188760",
    "end": "193920"
  },
  {
    "text": "issues with your nodes and many more with all that I want to go past the past",
    "start": "193920",
    "end": "202440"
  },
  {
    "text": "to direct and directly go into Google resource management thanks sir",
    "start": "202440",
    "end": "208430"
  },
  {
    "text": "[Music] so as a surgeon noted in sign note uh one of our primary responsibilities is",
    "start": "208430",
    "end": "214019"
  },
  {
    "text": "to figure out how to give pods access to host resources and how to make sure those resources are fairly shared among",
    "start": "214019",
    "end": "221040"
  },
  {
    "text": "pods or containers on that same note so today cubot supports a number of resources uh CPU memory disk ephemeral",
    "start": "221040",
    "end": "229920"
  },
  {
    "text": "storage that you often see in your pod specs when as an application deployer",
    "start": "229920",
    "end": "234959"
  },
  {
    "text": "you're deploying to kubernetes then there are some resource types that you don't see in the",
    "start": "234959",
    "end": "240900"
  },
  {
    "text": "foreground in your pod spec that the qubit is managing the background for fair sharing one of those things like",
    "start": "240900",
    "end": "246420"
  },
  {
    "text": "pits and we're also supporting Frameworks in the background to allow device plugins for example to advertise",
    "start": "246420",
    "end": "254040"
  },
  {
    "text": "Dynamic resources for you to consume your application basically this is a",
    "start": "254040",
    "end": "259199"
  },
  {
    "text": "rich set of growing and diverse set of resources that we need to manage in the",
    "start": "259199",
    "end": "265320"
  },
  {
    "text": "state next slide so when uh if you're interested in",
    "start": "265320",
    "end": "270540"
  },
  {
    "start": "267000",
    "end": "267000"
  },
  {
    "text": "understanding how we in the Sig approach this problem around advertising a resource oftentimes uh community members",
    "start": "270540",
    "end": "277620"
  },
  {
    "text": "come to the Sig and say there's a new uh entity that we want to make uh known within kubernetes whether that's a new",
    "start": "277620",
    "end": "284400"
  },
  {
    "text": "device a new uh class of resource or a new specialization on one of those",
    "start": "284400",
    "end": "290400"
  },
  {
    "text": "resources one of the first questions that we have to ask is you know how do we want to make that resource known how",
    "start": "290400",
    "end": "297660"
  },
  {
    "text": "does the user Express an intent to desire it and then how do how do we",
    "start": "297660",
    "end": "302940"
  },
  {
    "text": "model it relative to like the concrete representation of that resource a lot of times we don't think as a kubernetes",
    "start": "302940",
    "end": "308759"
  },
  {
    "text": "user what does it mean when I claim uh one or two or three CPUs but we in the",
    "start": "308759",
    "end": "315060"
  },
  {
    "text": "Sig are then responsible for figuring out for example how that maps to fractional shares in a c group",
    "start": "315060",
    "end": "320220"
  },
  {
    "text": "scheduling system many resources can be static and they never change over the life of a node",
    "start": "320220",
    "end": "326699"
  },
  {
    "text": "other resources are Dynamic and how we then reflect that back up to the control plane and the scheduler are important",
    "start": "326699",
    "end": "333000"
  },
  {
    "text": "questions to to reason through and then particularly for some of the items that we're exploring in the future uh we have",
    "start": "333000",
    "end": "339780"
  },
  {
    "text": "to be very careful to avoid node bootstrapping Loops so we have to make sure that the cubic can work absent the",
    "start": "339780",
    "end": "345720"
  },
  {
    "text": "presence of this resource being known and uh it's it's a Perpetual challenge",
    "start": "345720",
    "end": "350940"
  },
  {
    "text": "to have to always reason through these things so next slide so at the end of the day users are wanting to make a",
    "start": "350940",
    "end": "356699"
  },
  {
    "start": "354000",
    "end": "354000"
  },
  {
    "text": "claim on a resource so some resources are uh fixed and uh things like memory",
    "start": "356699",
    "end": "364100"
  },
  {
    "text": "or disk space you can very easily count how much of that resource you want and",
    "start": "364100",
    "end": "369840"
  },
  {
    "text": "you can make sure that then you can hold that container or pod within that allocated budget other resources we",
    "start": "369840",
    "end": "377340"
  },
  {
    "text": "could sometimes describe as non-countable and these might be class-based resources where you're just",
    "start": "377340",
    "end": "383280"
  },
  {
    "text": "trying to give a certain service quality around that resource that iPod can",
    "start": "383280",
    "end": "389340"
  },
  {
    "text": "express so if you have a new resource that you're interested in having to say you understand or articulate it one of",
    "start": "389340",
    "end": "394979"
  },
  {
    "text": "the first questions is is it actually counted or not similar to that not all resources can be",
    "start": "394979",
    "end": "402020"
  },
  {
    "text": "uh over committed in kubernetes you can make a request for a minimal amount of",
    "start": "402020",
    "end": "407280"
  },
  {
    "text": "resource where on the cubelet we guarantee that you will get that resource at minimum you will never get",
    "start": "407280",
    "end": "413280"
  },
  {
    "text": "less than that uh you'll make sure that you always have that reservation of a resource and we we map that to the",
    "start": "413280",
    "end": "419100"
  },
  {
    "text": "request field for resources that can be over committed uh we allow you to give a separate field",
    "start": "419100",
    "end": "424979"
  },
  {
    "text": "which we call a limit and for example you can say a container can get between one and two CPUs at any given time or",
    "start": "424979",
    "end": "433020"
  },
  {
    "text": "between one gig and two gigs of memory at any given time but not all resources",
    "start": "433020",
    "end": "438419"
  },
  {
    "text": "can be over committed a good example of that would be something like huge Pages",
    "start": "438419",
    "end": "444000"
  },
  {
    "text": "where right now for example if you have to have a pod that requests the huge page you you have you cannot over commit",
    "start": "444000",
    "end": "451800"
  },
  {
    "text": "that resource you you basically request certain amount of huge pages and you can't uh",
    "start": "451800",
    "end": "458160"
  },
  {
    "text": "schedule out more of those than exist on the Node and then as we're thinking about these",
    "start": "458160",
    "end": "463259"
  },
  {
    "text": "resource problems mostly we're trying to make sure that the this cluster scheduler can make an informed",
    "start": "463259",
    "end": "468300"
  },
  {
    "text": "scheduling decision but uh we also have to keep in mind that there's a certain amount of overhead uh",
    "start": "468300",
    "end": "474360"
  },
  {
    "text": "to just support running the management components on the Node or the life cycle of the Pod itself and we have to do work",
    "start": "474360",
    "end": "481139"
  },
  {
    "text": "to sometimes figure out ways to make that reservation known so that the cluster in the end is more reliable",
    "start": "481139",
    "end": "487680"
  },
  {
    "text": "if you're interested in scheduling problems to say what's the best node to fit a given budget that's typically the",
    "start": "487680",
    "end": "495900"
  },
  {
    "text": "domain of six scheduling and a lot of these Resource Management problems are worked across these two sigs but if",
    "start": "495900",
    "end": "503220"
  },
  {
    "text": "you're interested in figuring out how to make sure that like the request is actually fulfilled once scheduled that that's where signal really shines next",
    "start": "503220",
    "end": "511199"
  },
  {
    "text": "slide so other things that are typically beyond the cubelet but come up when",
    "start": "511199",
    "end": "516959"
  },
  {
    "text": "thinking about how to support new resources are things like limit ranges",
    "start": "516959",
    "end": "522479"
  },
  {
    "text": "where you can say a policy rule that says a pod and a given namespace",
    "start": "522479",
    "end": "528600"
  },
  {
    "text": "must request between one or two CPUs right or it must request no less than",
    "start": "528600",
    "end": "537060"
  },
  {
    "text": "five Megs of RAM and no more than 100 gigs of RAM you have a way of setting",
    "start": "537060",
    "end": "543240"
  },
  {
    "text": "very subtle uh like policy windows that can Define valid resource requests and",
    "start": "543240",
    "end": "550320"
  },
  {
    "text": "so that's the domain typically of the limit Ranger and if you have a new resource it sometimes is worthwhile asking",
    "start": "550320",
    "end": "556680"
  },
  {
    "text": "do cluster admins benefit from being able to constrain the windows of consumption for that",
    "start": "556680",
    "end": "563820"
  },
  {
    "text": "given resource and then typically even more important than that is uh resource",
    "start": "563820",
    "end": "569880"
  },
  {
    "text": "quotas which basically deals with ensuring the amount of that resource that a given",
    "start": "569880",
    "end": "575940"
  },
  {
    "text": "namespace can claim in your cluster as a whole so a lot of users in the world",
    "start": "575940",
    "end": "581519"
  },
  {
    "text": "will partition their kubernetes clusters into a set of of namespaces and they",
    "start": "581519",
    "end": "586620"
  },
  {
    "text": "want to control one namespace as ability to consume all resources relative to another and if there's a reasonable expectation",
    "start": "586620",
    "end": "592920"
  },
  {
    "text": "that your your the resource you want to introduce in the kubernetes might be viewed as precious or scarce it's often",
    "start": "592920",
    "end": "598500"
  },
  {
    "text": "important to then ask should it be incorporated into quota next slide",
    "start": "598500",
    "end": "604440"
  },
  {
    "start": "603000",
    "end": "603000"
  },
  {
    "text": "so once the schedule once the cubelet knows how to advertise your new resource and once the scheduler uh then wants to",
    "start": "604440",
    "end": "611220"
  },
  {
    "text": "schedule that resource the cubelet sees that the Pod has been scheduled to that",
    "start": "611220",
    "end": "616560"
  },
  {
    "text": "node and it runs its own local admission check a lot of people are familiar in",
    "start": "616560",
    "end": "621959"
  },
  {
    "text": "kubernetes today with admission controllers or web hooks that can extend the kubernetes control plane",
    "start": "621959",
    "end": "628380"
  },
  {
    "text": "and intercept a control plane API requests similar to that complementary",
    "start": "628380",
    "end": "633720"
  },
  {
    "text": "on the Node we make admission decisions that intercept Cod",
    "start": "633720",
    "end": "639720"
  },
  {
    "text": "scheduling acknowledgments on the cubelet and ensure that cubelet could",
    "start": "639720",
    "end": "645240"
  },
  {
    "text": "actually meet the needs expressed by that pod and but we'll do checks to ensure that",
    "start": "645240",
    "end": "651420"
  },
  {
    "text": "the resources are actually available on that node to support that pod and in some cases for more exotic or",
    "start": "651420",
    "end": "659339"
  },
  {
    "text": "specialized node local topology decisions it might make higher order checks for example",
    "start": "659339",
    "end": "666720"
  },
  {
    "text": "it'll say this pod wants to use one CPU and it also wants a GPU and it has",
    "start": "666720",
    "end": "673680"
  },
  {
    "text": "expressed the desire for those two things to be co-located on a common pneuma node",
    "start": "673680",
    "end": "678959"
  },
  {
    "text": "for some of those node local topology decisions a scheduler at the cluster",
    "start": "678959",
    "end": "684300"
  },
  {
    "text": "level doesn't have that total system View and only at the cubelet is that",
    "start": "684300",
    "end": "689459"
  },
  {
    "text": "system view known today for example the cubelet has to ask do we actually have the feasibility",
    "start": "689459",
    "end": "695240"
  },
  {
    "text": "constraints satisfied on that node to make it this is an area where we continue to try to improve or get better",
    "start": "695240",
    "end": "701279"
  },
  {
    "text": "on and as an area where uh folks that they're interested in trying to help",
    "start": "701279",
    "end": "706440"
  },
  {
    "text": "contribute to the Sig uh it's it's an area where we try to avoid any race conditions or false positives or false",
    "start": "706440",
    "end": "712800"
  },
  {
    "text": "negatives uh with accepting that workload because at the end of the day the goal would be once the pot is",
    "start": "712800",
    "end": "718200"
  },
  {
    "text": "scheduled we want to have a good confidence that pod will run next slide once the qubit acknowledges the desired",
    "start": "718200",
    "end": "726660"
  },
  {
    "start": "721000",
    "end": "721000"
  },
  {
    "text": "intent of that pod being run on that node oftentimes then the keyboard has to start the process of allocating or",
    "start": "726660",
    "end": "733740"
  },
  {
    "text": "reserving resources for that node many uh resources that the cubelet",
    "start": "733740",
    "end": "740040"
  },
  {
    "text": "allocates are stateless and oftentimes just controlled by c groups and but for",
    "start": "740040",
    "end": "746700"
  },
  {
    "text": "example if a pod is designing a particular amount of CPU or a memory this allocation step you can",
    "start": "746700",
    "end": "753360"
  },
  {
    "text": "think of as the cubelet just creating the C group structure that constrains the Pod but then there are other",
    "start": "753360",
    "end": "759360"
  },
  {
    "text": "resource types uh which might require the cable to do",
    "start": "759360",
    "end": "764399"
  },
  {
    "text": "something new for example if your pod has a an empty dirt and it wants to",
    "start": "764399",
    "end": "770760"
  },
  {
    "text": "store a state independent of a container backed by memory now the cubelet might",
    "start": "770760",
    "end": "777360"
  },
  {
    "text": "need to go and allocate a a location on temp Fest to fulfill that",
    "start": "777360",
    "end": "783899"
  },
  {
    "text": "request and you can imagine uh for other types of resources",
    "start": "783899",
    "end": "790200"
  },
  {
    "text": "um particularly for new emergent class-based resources we're exploring uh you might want to dynamically attach a",
    "start": "790200",
    "end": "796440"
  },
  {
    "text": "given device to that node to fulfill that request a lot of things that we have to explore in this area and then",
    "start": "796440",
    "end": "802440"
  },
  {
    "text": "typically we need to make sure that when that pod is completed that we can clean up any allocations and we want to know",
    "start": "802440",
    "end": "808620"
  },
  {
    "text": "when that allocation has been uh confidently deallocated as a part of",
    "start": "808620",
    "end": "814260"
  },
  {
    "text": "the Pod life cycle because ultimately the other day a new pod is going to run on that node and expect those resources",
    "start": "814260",
    "end": "819480"
  },
  {
    "text": "available to it next slide as we discussed earlier some resources",
    "start": "819480",
    "end": "826200"
  },
  {
    "start": "821000",
    "end": "821000"
  },
  {
    "text": "are over provisioned and can support over provisioning not all can as we said",
    "start": "826200",
    "end": "832860"
  },
  {
    "text": "earlier if if you are interested in exploring new resources in the",
    "start": "832860",
    "end": "839160"
  },
  {
    "text": "kubernetes community this is one of the first questions we would want to ask a good way of highlighting this is",
    "start": "839160",
    "end": "844639"
  },
  {
    "text": "today a given node may run on a given node May support many pods and in effect",
    "start": "844639",
    "end": "852600"
  },
  {
    "text": "well each part is making a request for a certain amount of CPU that CP the set of",
    "start": "852600",
    "end": "858240"
  },
  {
    "text": "concrete CPUs that are running in that pod are shared on that node",
    "start": "858240",
    "end": "864019"
  },
  {
    "text": "and ultimately they're they're getting kind of a time share on that CPU time other resources you can imagine things",
    "start": "864120",
    "end": "870779"
  },
  {
    "text": "like gpus when you want a GPU in your pod typically that GPU is for your pod and",
    "start": "870779",
    "end": "877380"
  },
  {
    "text": "no others right and that would be an example of a resource that is uh not over provisioned",
    "start": "877380",
    "end": "882600"
  },
  {
    "text": "for a certain set of our resources we have a concept of a quality of service class which says",
    "start": "882600",
    "end": "889560"
  },
  {
    "text": "dependent on how you request and limit particular resources we might say you go",
    "start": "889560",
    "end": "895380"
  },
  {
    "text": "into a guaranteed Quality Service bucket versus like a burst of a bucket and in cases where you're in that",
    "start": "895380",
    "end": "902279"
  },
  {
    "text": "guaranteed quality of service class we might support very higher order node",
    "start": "902279",
    "end": "907680"
  },
  {
    "text": "local optimizations to give you greater performance benefit for your workload for example if you have a if you make a",
    "start": "907680",
    "end": "915959"
  },
  {
    "text": "request for an exclusive CPU and you want a GPU at the same time you probably want those two things close on the",
    "start": "915959",
    "end": "921899"
  },
  {
    "text": "physical host so lots of interesting things to think through on this one next slide at the end of the day uh you're running",
    "start": "921899",
    "end": "929339"
  },
  {
    "start": "925000",
    "end": "925000"
  },
  {
    "text": "your workloads on nodes and you want to know what are you actually properly sizing your workloads",
    "start": "929339",
    "end": "935339"
  },
  {
    "text": "um how much resources are your workloads actually using in production",
    "start": "935339",
    "end": "940760"
  },
  {
    "text": "our goal and signal is to make sure that you can make the most optimal resource optimal usage of your resources as",
    "start": "940860",
    "end": "947940"
  },
  {
    "text": "possible so that you can get the best density and the best utilization in your uh environment",
    "start": "947940",
    "end": "953760"
  },
  {
    "text": "so typically then when a new resource is explored we want to follow up with questions on how do we know it's being",
    "start": "953760",
    "end": "959820"
  },
  {
    "text": "used so if it's counted for example is it being observed in things like C advisor is it being fed back into",
    "start": "959820",
    "end": "967800"
  },
  {
    "text": "metrics Loops for monitoring Stacks to scrape and measure um and then beyond that once some of",
    "start": "967800",
    "end": "974579"
  },
  {
    "text": "these things can be measured the question becomes can we support other adjacency abilities to make better",
    "start": "974579",
    "end": "980360"
  },
  {
    "text": "Dynamic resource resizing requests so a lot of neat things can come out of this",
    "start": "980360",
    "end": "985500"
  },
  {
    "text": "um so next slide sometimes the node particularly for over",
    "start": "985500",
    "end": "990720"
  },
  {
    "text": "committed resources may find that the actual amount of resource being used is",
    "start": "990720",
    "end": "996120"
  },
  {
    "text": "greater than that which is available on the Node and unfortunately when that happens you have to make a a decision",
    "start": "996120",
    "end": "1002060"
  },
  {
    "text": "about what to do and so there's capabilities in the keyboard today that when we think about new resources we",
    "start": "1002060",
    "end": "1008839"
  },
  {
    "text": "have to ask like if this resource is scarce what to do what do we do when scarcity occurs",
    "start": "1008839",
    "end": "1015199"
  },
  {
    "text": "and when thinking about new resources we often then think about how to handle those problems and then how quickly can",
    "start": "1015199",
    "end": "1021440"
  },
  {
    "text": "we handle those problems when they occur so for example if uh the node is running",
    "start": "1021440",
    "end": "1026600"
  },
  {
    "text": "low on available memory because pods are using more than they actually requested",
    "start": "1026600",
    "end": "1032240"
  },
  {
    "text": "uh the cubelet might make a decision to evict a lower priority pod relative to",
    "start": "1032240",
    "end": "1037280"
  },
  {
    "text": "another that's consuming more than what it requested and ultimately it makes",
    "start": "1037280",
    "end": "1042380"
  },
  {
    "text": "those decisions that a workload can be rescheduled to another location and we want to make sure that the workloads",
    "start": "1042380",
    "end": "1048380"
  },
  {
    "text": "remain healthy on the Node that are still there next slide Montgomery hopefully from this brief",
    "start": "1048380",
    "end": "1054380"
  },
  {
    "start": "1050000",
    "end": "1050000"
  },
  {
    "text": "overview you can sense that resource management and touches a lot of topics around API modeling the life cycle of a",
    "start": "1054380",
    "end": "1062480"
  },
  {
    "text": "node the life cycle of a pod the health of a host and it's actually quite a",
    "start": "1062480",
    "end": "1068120"
  },
  {
    "text": "complicated topic that takes a large number of people's Collective brain power to reason through a lot of these",
    "start": "1068120",
    "end": "1073700"
  },
  {
    "text": "resources are very specialized and have subtle Behavior that's hard for any one of us to keep in mind so if you're",
    "start": "1073700",
    "end": "1079340"
  },
  {
    "text": "interested in this space I think we're all happy for uh more Collective brain power to be brought to the problem",
    "start": "1079340",
    "end": "1085120"
  },
  {
    "text": "there's a few areas that we're exploring uh today that are highlighted here I'll just call it a few one of course is we",
    "start": "1085120",
    "end": "1092419"
  },
  {
    "text": "want workloads to be more performant and more optimized on the nodes they run and so better understanding that the the",
    "start": "1092419",
    "end": "1099440"
  },
  {
    "text": "node local topology and the name of space has been uh interest of ours was late as well as being able to expand to",
    "start": "1099440",
    "end": "1106700"
  },
  {
    "text": "a greater set of resources or specialized resources whether that's Network bandwidth or any other class-based entities um exciting work",
    "start": "1106700",
    "end": "1113480"
  },
  {
    "text": "ahead with that uh we'll turn it over to yumino there's been a focus on not",
    "start": "1113480",
    "end": "1119299"
  },
  {
    "text": "keeping betas forever since 120 uh next slide if you've done a bunch of uh work",
    "start": "1119299",
    "end": "1124940"
  },
  {
    "text": "to either deprecate or graduate lingering betas so these are some of the features that were either graduated or",
    "start": "1124940",
    "end": "1132679"
  },
  {
    "text": "deprecated next slide so these are the features that we",
    "start": "1132679",
    "end": "1138679"
  },
  {
    "start": "1134000",
    "end": "1134000"
  },
  {
    "text": "graduated in 126 and 127 so device plugins CPU manager downward API for",
    "start": "1138679",
    "end": "1144799"
  },
  {
    "text": "huge Pages uh cubelet credential providers and topology manager was graduated in 127.",
    "start": "1144799",
    "end": "1151820"
  },
  {
    "text": "next slide so that said we still have a bunch of work to do because we still have some features that are stuck in",
    "start": "1151820",
    "end": "1159020"
  },
  {
    "text": "beta and as a sake we are trying to either graduate them or deprecate them so as we go into 128 planning we'll try",
    "start": "1159020",
    "end": "1165799"
  },
  {
    "text": "to address as many as we can so let's take a look at what we did during 126",
    "start": "1165799",
    "end": "1170960"
  },
  {
    "text": "and 127 these are the three new features that we",
    "start": "1170960",
    "end": "1176179"
  },
  {
    "text": "want to highlight in 126 so first of them this Dynamic resource allocation it's a whole new API to request share",
    "start": "1176179",
    "end": "1183799"
  },
  {
    "text": "initialize and clean up resources it's like a generalized version of how storage is accessed today so I would",
    "start": "1183799",
    "end": "1190160"
  },
  {
    "text": "encourage you to either read a blog or watch another talk about this feature it's an exciting new feature that opens",
    "start": "1190160",
    "end": "1197360"
  },
  {
    "text": "up uh possibilities like splitting your GPU into multiple slices and using them from",
    "start": "1197360",
    "end": "1202820"
  },
  {
    "text": "pods so the next one is event at like Alpha so typically cubelet realists the pods",
    "start": "1202820",
    "end": "1210380"
  },
  {
    "text": "and containers from the container runtime every few seconds and this puts a lot of",
    "start": "1210380",
    "end": "1215960"
  },
  {
    "text": "load on both the cubelet and the runtime so event it like is an effort to make",
    "start": "1215960",
    "end": "1222620"
  },
  {
    "text": "the updates evented rather than frequently listing so the alpha was added in 126.",
    "start": "1222620",
    "end": "1229280"
  },
  {
    "text": "and then we added improved multi-numa alignment options to the topology manager in 126. so 127 we learned a lot",
    "start": "1229280",
    "end": "1237919"
  },
  {
    "text": "of stuff so a couple of gas to call out first the grpc container probe support",
    "start": "1237919",
    "end": "1243980"
  },
  {
    "text": "second is second default it enables better security by default than the invented plague moved to a beta",
    "start": "1243980",
    "end": "1251480"
  },
  {
    "text": "another one to call out is the In-Place pod vertical scaling so this was a big effort it took a lot of time from",
    "start": "1251480",
    "end": "1258740"
  },
  {
    "text": "Community as well as reviewers and approvers to get it right so it basically allows you to resize pod",
    "start": "1258740",
    "end": "1264440"
  },
  {
    "text": "resources without having to restart them by adjusting the c groups and it will be",
    "start": "1264440",
    "end": "1270440"
  },
  {
    "text": "useful in various Auto scaling scenarios when we added some other utility",
    "start": "1270440",
    "end": "1276860"
  },
  {
    "text": "features like cubelet tracing which allows you to hook add open Telemetry",
    "start": "1276860",
    "end": "1281900"
  },
  {
    "text": "tracing and then look at the traces of what's happening through the cubelet as requests are flowing in",
    "start": "1281900",
    "end": "1288380"
  },
  {
    "text": "then we made some improvements to image pulls so we have better control over how",
    "start": "1288380",
    "end": "1293900"
  },
  {
    "text": "many images are being pulled at the same time uh what's coming up next we have sidecar containers that Sergey is going",
    "start": "1293900",
    "end": "1299960"
  },
  {
    "start": "1295000",
    "end": "1295000"
  },
  {
    "text": "to cover in more detail soon then we did some initial work in swap we want to",
    "start": "1299960",
    "end": "1306020"
  },
  {
    "text": "explore the next steps there and see if we can provide more controls over what percentage of swap can be accessed by",
    "start": "1306020",
    "end": "1313760"
  },
  {
    "text": "the workloads also another intersection here is with c groups V2 once we enable swap support it",
    "start": "1313760",
    "end": "1321020"
  },
  {
    "text": "allows us to enable the user space boom killer like UMD so we have a better",
    "start": "1321020",
    "end": "1326780"
  },
  {
    "text": "predictability in which container and pods are um killed compared to what the kernel does today",
    "start": "1326780",
    "end": "1334400"
  },
  {
    "text": "then there'll be further improvements in dra hopefully we'll graduate event based",
    "start": "1334400",
    "end": "1339500"
  },
  {
    "text": "on feedback and we have a couple of enhancements in the area of image pools",
    "start": "1339500",
    "end": "1345559"
  },
  {
    "text": "so there's a lot a lot of other topics also being explored so we encourage you to come and join uh signal planning",
    "start": "1345559",
    "end": "1352400"
  },
  {
    "text": "meetings and uh help us move things forward so next uh Sergey is going to cover sidecar",
    "start": "1352400",
    "end": "1358700"
  },
  {
    "text": "containers thank you for now lots of exciting things happening in kubernetes and",
    "start": "1358700",
    "end": "1365480"
  },
  {
    "text": "signaled this uh I wanna talk in details about sidecar containers",
    "start": "1365480",
    "end": "1373159"
  },
  {
    "start": "1368000",
    "end": "1368000"
  },
  {
    "text": "sidecar continues is a concept was introduced long time ago I don't",
    "start": "1373159",
    "end": "1378559"
  },
  {
    "text": "even have a data but uh it was used for a long time and",
    "start": "1378559",
    "end": "1384740"
  },
  {
    "text": "people were using it for many continuously running uh ports that they're running typically for web",
    "start": "1384740",
    "end": "1390679"
  },
  {
    "text": "services and such uh sidecars were introduced to uh support generic",
    "start": "1390679",
    "end": "1397159"
  },
  {
    "text": "functionality like logging log forwarding call like metrics collection uh stuff like that and uh it worked",
    "start": "1397159",
    "end": "1405320"
  },
  {
    "text": "reasonably fine with all the long running ports and",
    "start": "1405320",
    "end": "1410900"
  },
  {
    "text": "continuously running ports that needs to be uh that never finishes with the",
    "start": "1410900",
    "end": "1416960"
  },
  {
    "text": "kubernetes growing and supporting more workloads we start putting more emphasis on jobs",
    "start": "1416960",
    "end": "1423760"
  },
  {
    "text": "besides other improvements we're doing for jobs and buys like workloads we see",
    "start": "1423760",
    "end": "1429620"
  },
  {
    "text": "that sidecar pattern is not working for jobs uh when you start a job or like any",
    "start": "1429620",
    "end": "1434659"
  },
  {
    "text": "bike workload that has a completion sidecars will prevent ports from being",
    "start": "1434659",
    "end": "1441140"
  },
  {
    "text": "terminated and removed so people were coming up with a very various",
    "start": "1441140",
    "end": "1447200"
  },
  {
    "text": "workarounds to support sidecars for jobs and jobs Indeed needs uh an inside cars",
    "start": "1447200",
    "end": "1453200"
  },
  {
    "text": "a typical example is log forward income metrics connection you want to know what's happening with the metrics on",
    "start": "1453200",
    "end": "1460280"
  },
  {
    "text": "your job you want to understand the status of it it's the state of it and uh having generic Sidecar",
    "start": "1460280",
    "end": "1467799"
  },
  {
    "text": "allowing helping with that is very useful another example uh where sidecars start",
    "start": "1467799",
    "end": "1474980"
  },
  {
    "text": "being useful in service mesh various",
    "start": "1474980",
    "end": "1480159"
  },
  {
    "text": "proxy that people install in a port we can monitor connections from into port",
    "start": "1480159",
    "end": "1487039"
  },
  {
    "text": "and from the port we can look what's happening in this connections uh modify this connection",
    "start": "1487039",
    "end": "1493940"
  },
  {
    "text": "and force some security on us it's all important scenarios before built-in",
    "start": "1493940",
    "end": "1500120"
  },
  {
    "text": "support for sidecar containers serious mesh has worked reasonably well with continuously running tasks so they have",
    "start": "1500120",
    "end": "1506059"
  },
  {
    "text": "the same problem as of Port termination as with jobs but for regular containers",
    "start": "1506059",
    "end": "1511280"
  },
  {
    "text": "they work fine the only problem was that init containers wasn't covered with service mesh if you have a unique",
    "start": "1511280",
    "end": "1517760"
  },
  {
    "text": "container that needs to utilize all the security benefits that service mesh provides you wasn't able to express it",
    "start": "1517760",
    "end": "1523880"
  },
  {
    "text": "uh with typical tooling provided by kubernetes built in so we wanted to",
    "start": "1523880",
    "end": "1530600"
  },
  {
    "text": "address this problem as well scenarios when sidecars are used for continuously updating Secrets or reading",
    "start": "1530600",
    "end": "1538580"
  },
  {
    "text": "some configuration files it's also a genetic task that can be implemented in single container and it",
    "start": "1538580",
    "end": "1545960"
  },
  {
    "text": "will be nice to have it implemented as a sidecar that wouldn't terminate or wouldn't uh",
    "start": "1545960",
    "end": "1551360"
  },
  {
    "text": "prevent Port from termination we worked on in 127 on proposal for",
    "start": "1551360",
    "end": "1557539"
  },
  {
    "start": "1552000",
    "end": "1552000"
  },
  {
    "text": "sidecar containers and the proposal is we wouldn't introduce any sidecar term",
    "start": "1557539",
    "end": "1563000"
  },
  {
    "text": "into uh kubernetes API what we will do is we'll uh allow init containers to run",
    "start": "1563000",
    "end": "1570440"
  },
  {
    "text": "continuously so some new containers will be marked with the restart policy always which will indicate to us that this",
    "start": "1570440",
    "end": "1577220"
  },
  {
    "text": "container is a sidecar and it needs to start the same order as all adding containers",
    "start": "1577220",
    "end": "1583820"
  },
  {
    "text": "but it will other inflators will not wait for its termination so it will on completion it will start running other",
    "start": "1583820",
    "end": "1591980"
  },
  {
    "text": "any contents will wait for sidecar to get into started state so startup props will complete successfully and after",
    "start": "1591980",
    "end": "1599240"
  },
  {
    "text": "that all other new containers will run or regular contains to run but this container will keep running and if it",
    "start": "1599240",
    "end": "1605360"
  },
  {
    "text": "fails even if it fails on pause with Marcus will start policy never it will keep being restarted because you want to",
    "start": "1605360",
    "end": "1611779"
  },
  {
    "text": "keep monitoring or work forwarding to your job even if uh your lot further",
    "start": "1611779",
    "end": "1618679"
  },
  {
    "text": "crashed or was killed for some reason I wanted to talk a little bit about development process for this feature",
    "start": "1618679",
    "end": "1625340"
  },
  {
    "start": "1620000",
    "end": "1620000"
  },
  {
    "text": "um we this uh feature was uh proposed long time ago and we've been working on",
    "start": "1625340",
    "end": "1631760"
  },
  {
    "text": "implementation the difficulty was that API service and",
    "start": "1631760",
    "end": "1637400"
  },
  {
    "text": "any API service was hard to agree on and we needed to make sure that it's a future",
    "start": "1637400",
    "end": "1644299"
  },
  {
    "text": "proof and we went back and forth between very simple implementation very uh complicated implementation we end up",
    "start": "1644299",
    "end": "1651500"
  },
  {
    "text": "creating a working group to uh log on decisions and make it more efficient to",
    "start": "1651500",
    "end": "1656720"
  },
  {
    "text": "go through these conversations and this working group was very successful over time",
    "start": "1656720",
    "end": "1662740"
  },
  {
    "text": "we locked on decisions relatively quickly and then proceeded with implementation uh decryption of",
    "start": "1662740",
    "end": "1669799"
  },
  {
    "text": "implementation steps and in terms of implementation steps we took some lessons from other major caps from the",
    "start": "1669799",
    "end": "1677179"
  },
  {
    "text": "past and we split our work into some refactoring that we do before API change",
    "start": "1677179",
    "end": "1682520"
  },
  {
    "text": "and then some big PR that will go with API change including specific features",
    "start": "1682520",
    "end": "1688640"
  },
  {
    "text": "we want to support out uh as a MVP product and then after API change you",
    "start": "1688640",
    "end": "1693919"
  },
  {
    "text": "want to catch up with additional features and uh this is mostly to make",
    "start": "1693919",
    "end": "1699440"
  },
  {
    "text": "sure that big PR that will land it will not be either relevantly bigger it will",
    "start": "1699440",
    "end": "1704480"
  },
  {
    "text": "not take too much time to maintain and to long for a progress to review",
    "start": "1704480",
    "end": "1709760"
  },
  {
    "text": "and with that I wanted to thank everybody on status of it uh we have a",
    "start": "1709760",
    "end": "1716120"
  },
  {
    "text": "PR work in progress PR in 127 but we didn't merge it uh for multiple reasons and one of that is uh we didn't want to",
    "start": "1716120",
    "end": "1722960"
  },
  {
    "text": "put two major features in one uh release uh 127. so we likely will merged in",
    "start": "1722960",
    "end": "1728179"
  },
  {
    "text": "early 128. this is the plan and we still have some open questions for follow-ups",
    "start": "1728179",
    "end": "1733220"
  },
  {
    "text": "for after Alpha uh it's primarily a termination ordering and how we",
    "start": "1733220",
    "end": "1738620"
  },
  {
    "text": "gracefully terminate sidecars how we allow ordering of termination",
    "start": "1738620",
    "end": "1745700"
  },
  {
    "start": "1743000",
    "end": "1743000"
  },
  {
    "text": "that I wanted to say thank you for people participating in the working group I just took a list from",
    "start": "1745700",
    "end": "1752539"
  },
  {
    "text": "all the nodes and thank you everybody uh who joined and gave your opinion or",
    "start": "1752539",
    "end": "1758000"
  },
  {
    "text": "presented something and I want to have special shout out to people who was implementing this work in",
    "start": "1758000",
    "end": "1764480"
  },
  {
    "text": "progress PR thank you uh we really appreciate everything happening and the",
    "start": "1764480",
    "end": "1770179"
  },
  {
    "text": "sidecar supposed to like promise to be very uh successful and very uh needed",
    "start": "1770179",
    "end": "1775279"
  },
  {
    "text": "feature is that I want to pass the Derek to go through the leadership updates so thanks",
    "start": "1775279",
    "end": "1782299"
  },
  {
    "text": "Sergey so since our last uh meeting uh many of you who track our mailing list might have seen an update to the Sig's",
    "start": "1782299",
    "end": "1789260"
  },
  {
    "text": "leadership structure but it's been a long-term goal of ours to support growth of uh new uh members in the Sig",
    "start": "1789260",
    "end": "1796279"
  },
  {
    "text": "in one way uh that we've done that is to split our chair and Technical lead roles",
    "start": "1796279",
    "end": "1801980"
  },
  {
    "text": "within the Sig in its corresponding governance uh and recognize my co-presenters here as uh I want to thank",
    "start": "1801980",
    "end": "1809059"
  },
  {
    "text": "both sergeymanal is agreeing to to step up and take the chair uh role in the Sig",
    "start": "1809059",
    "end": "1814220"
  },
  {
    "text": "which was relieving both Dawn and myself uh from that role uh and helping make",
    "start": "1814220",
    "end": "1820039"
  },
  {
    "text": "sure that the sake runs smooth and is executing well and meeting the needs of the community so big thank you sergeyman",
    "start": "1820039",
    "end": "1825260"
  },
  {
    "text": "for for taking that on uh and then we've expanded our roster of technical leads",
    "start": "1825260",
    "end": "1830419"
  },
  {
    "text": "in the seg to include murnal to make sure that we can help navigate uh tough",
    "start": "1830419",
    "end": "1837080"
  },
  {
    "text": "decision making or conflicts that might arise in the state ground what to do next and how we look to achieve it so",
    "start": "1837080",
    "end": "1842299"
  },
  {
    "text": "big thank you renal for helping uh grow the team there and uh look forward to",
    "start": "1842299",
    "end": "1847520"
  },
  {
    "text": "the success that that brings us here I wanted to",
    "start": "1847520",
    "end": "1853340"
  },
  {
    "start": "1850000",
    "end": "1850000"
  },
  {
    "text": "um say thank you for uh all the trust that put us to us in the community and with that I wanted to talk about how you",
    "start": "1853340",
    "end": "1860000"
  },
  {
    "text": "can you can contribute to Sig note so first of all whenever you contribute and what we ask is uh to pay attention",
    "start": "1860000",
    "end": "1867980"
  },
  {
    "text": "stability stability always comes first any bug fix that you submit without",
    "start": "1867980",
    "end": "1873520"
  },
  {
    "text": "tests uh raises questions we need to make sure that we're not introducing new",
    "start": "1873520",
    "end": "1879559"
  },
  {
    "text": "problems and that old features and all behaviors are covered with the test very well",
    "start": "1879559",
    "end": "1885020"
  },
  {
    "text": "then we want to make sure that we do optimizations we Google it grows and the",
    "start": "1885020",
    "end": "1892700"
  },
  {
    "text": "workload that we can run is various significantly from where we started with",
    "start": "1892700",
    "end": "1897919"
  },
  {
    "text": "that's why invented plug feature is very desired and we worked on it hard and we",
    "start": "1897919",
    "end": "1904279"
  },
  {
    "text": "want more optimizations more improvements in this space features and the final review also care",
    "start": "1904279",
    "end": "1911539"
  },
  {
    "text": "about user and developer experience so if you can contribute documentation or",
    "start": "1911539",
    "end": "1916580"
  },
  {
    "text": "just give us feedback on PRS and issues please come and contribute",
    "start": "1916580",
    "end": "1922880"
  },
  {
    "text": "you can contribute by attending our Sig meetings we have two meetings one of STI and one",
    "start": "1922880",
    "end": "1930440"
  },
  {
    "text": "for our main meeting uh and you can also participate in dogs",
    "start": "1930440",
    "end": "1936020"
  },
  {
    "text": "rising and doing some triage I don't know you can join the regular meetings",
    "start": "1936020",
    "end": "1941360"
  },
  {
    "text": "on Tuesdays at 10 Pacific and the CIA meeting on Wednesdays at 10 Pacific we are available on the signode slack",
    "start": "1941360",
    "end": "1948500"
  },
  {
    "text": "channel on kubernetes slack and we have a mailing list as well and know how to",
    "start": "1948500",
    "end": "1953899"
  },
  {
    "text": "reach the chairs and TLS well thanks for joining this talk and we hope it was",
    "start": "1953899",
    "end": "1959240"
  },
  {
    "text": "useful for you and we look forward to hearing from you",
    "start": "1959240",
    "end": "1964240"
  }
]