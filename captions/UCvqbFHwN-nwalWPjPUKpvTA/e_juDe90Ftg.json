[
  {
    "text": "so hi everyone Welcome to our talk thanks for attending my name is kareim Lani and",
    "start": "320",
    "end": "6520"
  },
  {
    "text": "today we'll be talking about kubernetes on a budget how to get paper use",
    "start": "6520",
    "end": "12480"
  },
  {
    "text": "right so as I mentioned I'm kareim Lani I'm a senior staff software engineer at int and I'm here with basuki prad who a",
    "start": "12880",
    "end": "20800"
  },
  {
    "text": "staff engineer so our agenda for today we'll",
    "start": "20800",
    "end": "26519"
  },
  {
    "text": "be discussing our context our in API Gateway and our journey into Cloud native",
    "start": "26519",
    "end": "35200"
  },
  {
    "text": "Technologies be discussing our cost overruns as we migrated our workloads and PR pre-production and then our",
    "start": "35200",
    "end": "42440"
  },
  {
    "text": "challenges learnings and solutions and finally we'll close with our",
    "start": "42440",
    "end": "48520"
  },
  {
    "text": "takeaways so into it is a fintech company that operates on a large scale",
    "start": "50120",
    "end": "56359"
  },
  {
    "text": "as you can see we have over 1 million active cores over 28,000 name spaces",
    "start": "56359",
    "end": "62480"
  },
  {
    "text": "over 25,000 services and over 300 kubernetes",
    "start": "62480",
    "end": "68400"
  },
  {
    "text": "clusters and we P specifically work on the intu API",
    "start": "70880",
    "end": "76520"
  },
  {
    "text": "Gateway so what is our in API",
    "start": "77600",
    "end": "81880"
  },
  {
    "text": "Gateway St work it's the front door for all the",
    "start": "83200",
    "end": "89320"
  },
  {
    "text": "requests that come into into it and a lot of servico service",
    "start": "89320",
    "end": "95680"
  },
  {
    "text": "communication also goes through the in API gway so our service mes Journey started about",
    "start": "95680",
    "end": "103320"
  },
  {
    "text": "three years ago but even before that we started our microservices journey and so",
    "start": "103320",
    "end": "109240"
  },
  {
    "text": "the in API gway was used for service to service and it continues to be used for a lot of service to service",
    "start": "109240",
    "end": "116600"
  },
  {
    "text": "communication so what is the a in an AP gway provider it provides routing Security in the form",
    "start": "116920",
    "end": "124840"
  },
  {
    "text": "of authentication and authorization of both the application and the user it",
    "start": "124840",
    "end": "131120"
  },
  {
    "text": "produces metrics that are used to feed our golden signals which provides availability error rates for all the",
    "start": "131120",
    "end": "138280"
  },
  {
    "text": "services behind API gway as well as it provides detailed access logging of all",
    "start": "138280",
    "end": "143480"
  },
  {
    "text": "the requests and responses flowing through our system and finally it provides quality",
    "start": "143480",
    "end": "148920"
  },
  {
    "text": "of service features including rate limiting and traffic",
    "start": "148920",
    "end": "153640"
  },
  {
    "text": "dialing and a few benefits and stats of our Gateway it's highly scalable at Peak",
    "start": "153959",
    "end": "159800"
  },
  {
    "text": "we handle over 30 billion requests per day over 1 million requests per second",
    "start": "159800",
    "end": "165959"
  },
  {
    "text": "and our infrastructure is about 30 plus clusters and 250 plus name",
    "start": "165959",
    "end": "172120"
  },
  {
    "text": "spaces it's highly available with 49es of availability highly reliable it has to",
    "start": "172120",
    "end": "178840"
  },
  {
    "text": "be trusted it has has to be up all the time it has to provide correct metrics",
    "start": "178840",
    "end": "184120"
  },
  {
    "text": "it's low latency we target 30 milliseconds of overhead at the",
    "start": "184120",
    "end": "189640"
  },
  {
    "text": "P99 and it has deep uh self-service capabilities it's integrated into our",
    "start": "189640",
    "end": "196200"
  },
  {
    "text": "developer portal and so onboarding configuration management is all",
    "start": "196200",
    "end": "201599"
  },
  {
    "text": "self-service um we're supporting over 2,000 developers uh on using our API",
    "start": "201599",
    "end": "207360"
  },
  {
    "text": "Gateway add into it so our in AP Gateway was built in house",
    "start": "207360",
    "end": "215239"
  },
  {
    "text": "uh I want to discuss quickly why so it's our journey started about 11 years ago in the into a data center before a lot",
    "start": "215239",
    "end": "222720"
  },
  {
    "text": "of cloud native Technologies even existed and um we wanted to move towards",
    "start": "222720",
    "end": "227959"
  },
  {
    "text": "microservices from a monolithic architecture and so we needed an API",
    "start": "227959",
    "end": "234239"
  },
  {
    "text": "Gateway and we decided to build one um really the value we saw was building",
    "start": "234239",
    "end": "240159"
  },
  {
    "text": "something that we could customize for our needs over the next uh decade and that's what we've",
    "start": "240159",
    "end": "245480"
  },
  {
    "text": "done now our approach is very similar to Netflix Zu 2 and AWS API Gateway it uses",
    "start": "245480",
    "end": "252959"
  },
  {
    "text": "a non-blocking asynchronous um Communications using Jetty it's written",
    "start": "252959",
    "end": "259280"
  },
  {
    "text": "in Java and one of the key features of his partitioning is built in as a first class feature meaning that different API",
    "start": "259280",
    "end": "267800"
  },
  {
    "text": "Gateway workloads serve services and that way we're able to keep workloads separated between QuickBooks",
    "start": "267800",
    "end": "274919"
  },
  {
    "text": "Turbo Tax mint MailChimp and such it also has more relaxed quotas",
    "start": "274919",
    "end": "280960"
  },
  {
    "text": "compared to the AWS API Gateway such as longer timeouts are allowed longer",
    "start": "280960",
    "end": "286320"
  },
  {
    "text": "bigger payloads and so we started in the data",
    "start": "286320",
    "end": "292039"
  },
  {
    "text": "center we did a lift and shift into AWS and then we discussed whether moving to",
    "start": "292039",
    "end": "297240"
  },
  {
    "text": "kubernetes and Cloud native Technologies made sense so why migrate to Cloud native Technologies the first thing was ISO uh",
    "start": "297240",
    "end": "305280"
  },
  {
    "text": "we as I mentioned started service mesh Journey about three years ago and wanted to integrate with that improve our",
    "start": "305280",
    "end": "311240"
  },
  {
    "text": "security reliability and observability by integrating with thisdo and as a project of that we released",
    "start": "311240",
    "end": "318120"
  },
  {
    "text": "Admiral which is our multicluster service mesh solution we wanted to reduce n data",
    "start": "318120",
    "end": "323800"
  },
  {
    "text": "transfer costs and we also wanted to improve the client developer experience",
    "start": "323800",
    "end": "329039"
  },
  {
    "text": "by AB extracting away whether they're going through a service mesh endpoint or going to an API Gateway endpoint with",
    "start": "329039",
    "end": "336160"
  },
  {
    "text": "service mesh with API Gateway coming into the service mesh that could be made",
    "start": "336160",
    "end": "342319"
  },
  {
    "text": "transparent the next thing is observability we had metrics and logging in our old API Gateway but we we had",
    "start": "342319",
    "end": "349319"
  },
  {
    "text": "some gaps it was difficult to add new metrics and we were had custom Solutions",
    "start": "349319",
    "end": "355440"
  },
  {
    "text": "custom Technologies um and caused overhead in our deployment so we saw",
    "start": "355440",
    "end": "361160"
  },
  {
    "text": "opportunities there to improve using Prometheus and other Cloud native Technologies as well as our structure of",
    "start": "361160",
    "end": "368599"
  },
  {
    "text": "our system made cost analytics very challenging so we saw an opportunity there with cluster names space",
    "start": "368599",
    "end": "375599"
  },
  {
    "text": "partitioning to improve our cost analytics as well as using some of the platform tools that were built around",
    "start": "375599",
    "end": "383000"
  },
  {
    "text": "this and so finally we wanted to utilize our intuits modern SAS platform which is",
    "start": "383000",
    "end": "389000"
  },
  {
    "text": "something that over the last few years has been built up to support a lot of new services that into it and a lot of",
    "start": "389000",
    "end": "394479"
  },
  {
    "text": "existing Services we wanted to take advantage of Argo CD Argo rollouts Argo",
    "start": "394479",
    "end": "399800"
  },
  {
    "text": "workflows esto service mesh our in service mesh Prometheus and phase out",
    "start": "399800",
    "end": "405560"
  },
  {
    "text": "these Legacy tools and deployment scripts that were causing us like headaches every",
    "start": "405560",
    "end": "411479"
  },
  {
    "text": "week and add into it we believe deeply in open source and open collaboration",
    "start": "411479",
    "end": "416759"
  },
  {
    "text": "where the uh recipient of the end user award in 2019 and",
    "start": "416759",
    "end": "421800"
  },
  {
    "text": "2022 and we have a few open source projects including Argo numof flow",
    "start": "421800",
    "end": "429160"
  },
  {
    "text": "Admiral and as well we're the end user of many Cloud native Technologies of course kubernetes as well as sto",
    "start": "429160",
    "end": "436400"
  },
  {
    "text": "Envoy Prometheus and here's a link for our open source community in",
    "start": "436400",
    "end": "443120"
  },
  {
    "text": "LinkedIn so after migrating to Cloud native Technologies what did our architecture look like it's starts with",
    "start": "443120",
    "end": "449840"
  },
  {
    "text": "our applications our mobile apps web apps our users they're calling intu apis",
    "start": "449840",
    "end": "456520"
  },
  {
    "text": "through the in API Gateway which is now running in kubernetes with sdo enabled",
    "start": "456520",
    "end": "462039"
  },
  {
    "text": "so one one cluster might be serving 500 different services and then we use Admiral to have",
    "start": "462039",
    "end": "469720"
  },
  {
    "text": "uh data for all the all the cross cluster communication all the different",
    "start": "469720",
    "end": "474759"
  },
  {
    "text": "clusters that API Gateway needs to send requests to so with that configur eration API Gateway is able to forward",
    "start": "474759",
    "end": "481240"
  },
  {
    "text": "the request to the appropriate service which is going to be in a different cluster maybe in a different region and",
    "start": "481240",
    "end": "487520"
  },
  {
    "text": "then those services are able to talk to each other through the service",
    "start": "487520",
    "end": "492159"
  },
  {
    "text": "mesh and a little bit deeper dive into that so we have our control plane which is STO based we have Argo Admiral and a",
    "start": "495120",
    "end": "503280"
  },
  {
    "text": "certificate management system and then we have a data plane the requests come in either through the public balancer or",
    "start": "503280",
    "end": "510280"
  },
  {
    "text": "through the sto Ingress Gateway that's for our Network abstraction they come into our sto proxy",
    "start": "510280",
    "end": "518279"
  },
  {
    "text": "which has a mtls or TLS termination goes to our into API Gateway container which",
    "start": "518279",
    "end": "525240"
  },
  {
    "text": "does all the routing traffic management uh throttling authentication",
    "start": "525240",
    "end": "530839"
  },
  {
    "text": "and such and then through itdo proxy it goes out to the Target service whichever",
    "start": "530839",
    "end": "536040"
  },
  {
    "text": "that is and again we have over 2,000 such services that we send requests to",
    "start": "536040",
    "end": "541560"
  },
  {
    "text": "and then we have this mtls sidecar which is getting updated SS every hour to do",
    "start": "541560",
    "end": "547519"
  },
  {
    "text": "the mtls so with this we improved our security our observability and our",
    "start": "547519",
    "end": "554240"
  },
  {
    "text": "[Music] um so now I want to talk about our what",
    "start": "554240",
    "end": "560040"
  },
  {
    "text": "we're here for our Cloud native cost optimization Journey so as we started migrating to cow native Technologies and",
    "start": "560040",
    "end": "566160"
  },
  {
    "text": "preprod we actually realized that our costs were going to be too X over what we planned it to be so that was a moment",
    "start": "566160",
    "end": "573200"
  },
  {
    "text": "of an emergency and we had to figure out what was going on and so we embarked on an analysis",
    "start": "573200",
    "end": "579720"
  },
  {
    "text": "investigating all the different things that are part of our architecture where we can optimize the cost how we can",
    "start": "579720",
    "end": "585680"
  },
  {
    "text": "bring it down to what we thought it would be and what it was in our AWS",
    "start": "585680",
    "end": "591680"
  },
  {
    "text": "workload and so as a results we actually found several areas of improvement which",
    "start": "591680",
    "end": "597480"
  },
  {
    "text": "basuki will be going into and by implementing these changes we actually reduced our expenditure by 50% bringing",
    "start": "597480",
    "end": "604760"
  },
  {
    "text": "us in line with our expectations now I want to talk about",
    "start": "604760",
    "end": "610240"
  },
  {
    "text": "how we discovered the cost overruns it's very important to have the tools and observability to understand the cost",
    "start": "610240",
    "end": "616480"
  },
  {
    "text": "analyze the costs so it's very important to have month-over-month metrics week",
    "start": "616480",
    "end": "621720"
  },
  {
    "text": "over week metrics and daily cost metrics as well on the cost as well as if you",
    "start": "621720",
    "end": "627360"
  },
  {
    "text": "can have metrics on how different Services contribute to the total cost that's really helpful to understand when",
    "start": "627360",
    "end": "634079"
  },
  {
    "text": "costs go up why is it and how to address it without these you're basically Flying",
    "start": "634079",
    "end": "641240"
  },
  {
    "text": "Blind and then we also have further detailed analysis breaking down at the cluster level name Space level",
    "start": "641519",
    "end": "648120"
  },
  {
    "text": "environment level business unit level and service level so we fortunately had",
    "start": "648120",
    "end": "653639"
  },
  {
    "text": "these tools to be able to understand our cost um and uh so that helped us a lot",
    "start": "653639",
    "end": "658839"
  },
  {
    "text": "to to bring the cost down and then the second part of it is",
    "start": "658839",
    "end": "665279"
  },
  {
    "text": "combining the cost metrics with the usage metrics with our API Gateway we don't control the number of requests",
    "start": "665279",
    "end": "671560"
  },
  {
    "text": "coming through the system they're coming from browsers web apps and so the more requests that come the more it costs and",
    "start": "671560",
    "end": "678639"
  },
  {
    "text": "so just because the cost goes up doesn't mean we're doing a worse job so it's important to also correlate the cost",
    "start": "678639",
    "end": "685240"
  },
  {
    "text": "metrics to the usage metrics to understand that how much does it cost per some number like some for us per",
    "start": "685240",
    "end": "693360"
  },
  {
    "text": "million requests what does that cost so that's the number that we can optimize um and then to understand how",
    "start": "693360",
    "end": "700160"
  },
  {
    "text": "much it costs as well you want to see how many nodes did you have how much pods do you have how many pods are you",
    "start": "700160",
    "end": "706079"
  },
  {
    "text": "running per node um how much idle do you have on the nodes as well as the CPU",
    "start": "706079",
    "end": "712560"
  },
  {
    "text": "memory us usage of each container on the Pod and establishing those baselines for",
    "start": "712560",
    "end": "718040"
  },
  {
    "text": "what's expected so when it goes up you're able to address it right",
    "start": "718040",
    "end": "723600"
  },
  {
    "text": "away so these are the results in our pre-prod account our largest pre-prod workload so starting off our Baseline",
    "start": "724120",
    "end": "731800"
  },
  {
    "text": "cost was around 45,000 which is not shown here because it was in a different account but it was around 45,000 and as",
    "start": "731800",
    "end": "739920"
  },
  {
    "text": "we migrated the cost started going up and then we saw that our total cost in",
    "start": "739920",
    "end": "745399"
  },
  {
    "text": "in that month was 2x over what we expected so it was not not not acceptable and then so we started uh",
    "start": "745399",
    "end": "752360"
  },
  {
    "text": "investigating applying optimizations and then the next month we got it down 50%",
    "start": "752360",
    "end": "757760"
  },
  {
    "text": "and then since then it stayed pretty stable and again the cost fluctuates based on the workload that we're getting",
    "start": "757760",
    "end": "764240"
  },
  {
    "text": "but it's been pretty stable since then so with that I'm going to turn it",
    "start": "764240",
    "end": "769760"
  },
  {
    "text": "over to basuki who's going to be discussing more details on what we found and how we fixed",
    "start": "769760",
    "end": "776480"
  },
  {
    "text": "it thank you Karim um so um while we were in this journey",
    "start": "777839",
    "end": "785040"
  },
  {
    "text": "definitely one of the areas was the cost that we saw that had to be looked in immediately and some of the areas of",
    "start": "785040",
    "end": "790720"
  },
  {
    "text": "opportunities that we found where where where where are we going wrong and how can we actually uh account for it so the",
    "start": "790720",
    "end": "797519"
  },
  {
    "text": "biggest was the compute while I'm saying compute there are many aspects of compute that how we Autos scale how is",
    "start": "797519",
    "end": "803600"
  },
  {
    "text": "your node utilization what is your PO density and we also incured a new uh",
    "start": "803600",
    "end": "809040"
  },
  {
    "text": "cost which was the data transfer cost because which I'll touch upon why that new cost it came to us while we adopted",
    "start": "809040",
    "end": "816399"
  },
  {
    "text": "our latest platform we also increased on our monitoring cost and also the developer infra uh infrastructure cost",
    "start": "816399",
    "end": "823120"
  },
  {
    "text": "that was also additional thing that we had during this whole migration so moving on uh let's first uh",
    "start": "823120",
    "end": "829120"
  },
  {
    "text": "Deep dive onto the compute and we'll touch the autoscaling so what is autoscaling I",
    "start": "829120",
    "end": "835279"
  },
  {
    "text": "mean if there is a demand that is created by your application the parts will get scheduled they are in pending",
    "start": "835279",
    "end": "841959"
  },
  {
    "text": "the autoscaler will take that request it will create that capacity for you the nodes will get scheduled and the PS will",
    "start": "841959",
    "end": "848480"
  },
  {
    "text": "get the the resources that it needs and it will start getting scheduled so this is a life cycle of the cluster we'll get",
    "start": "848480",
    "end": "854920"
  },
  {
    "text": "that auto scaling in place and this is a typical scenario for any cluster in uh you can take but some of the challenges",
    "start": "854920",
    "end": "862839"
  },
  {
    "text": "that we had in terms of autoscaling was since API Gateway to support that large scale of request we had to solve for how",
    "start": "862839",
    "end": "870160"
  },
  {
    "text": "do we account for Rapid scaling or instant surges the simple HPS that we had configured was never working the way",
    "start": "870160",
    "end": "876880"
  },
  {
    "text": "we wanted it to work either we were overscaling or we were to under scaling we always ended up having issues and",
    "start": "876880",
    "end": "882880"
  },
  {
    "text": "always had fcis with that so CPU was too oscillating with simple HPS we had to",
    "start": "882880",
    "end": "888320"
  },
  {
    "text": "run more higher minimums just to accommodate certain surges that we used to get this itself was too much running",
    "start": "888320",
    "end": "894759"
  },
  {
    "text": "higher minimums that means you're adding more cost to your infrastructure so we wanted to make sure that we solve this in a way that we can optimize this whole",
    "start": "894759",
    "end": "903279"
  },
  {
    "text": "usage uh before we touch upon the solutions I would say for any service",
    "start": "903279",
    "end": "908839"
  },
  {
    "text": "first you should know what metric that you need to be scaling or it could be CPU memory request connections or any",
    "start": "908839",
    "end": "914759"
  },
  {
    "text": "custom metric that you want to derive that should be the key indicator for your service before we start scaling in",
    "start": "914759",
    "end": "920279"
  },
  {
    "text": "our case CPU was the one that was the bottleneck and we had to look at uh how do we optimize that so knowing your",
    "start": "920279",
    "end": "927000"
  },
  {
    "text": "target CPU means at what is the range in which you can operate your application",
    "start": "927000",
    "end": "932279"
  },
  {
    "text": "in the most optimal way is what you need to look at uh so knowing your target CPU",
    "start": "932279",
    "end": "937519"
  },
  {
    "text": "is very crucial and how you can Autos scale aggressive scaling you should avoid it because that has a lot of other",
    "start": "937519",
    "end": "943920"
  },
  {
    "text": "cascading effects for your infrastructure which can have uh impact on your monitoring you will increase",
    "start": "943920",
    "end": "949319"
  },
  {
    "text": "your cardinality that leads to additional cost so avoid aggressive scaling so some of the techniques that",
    "start": "949319",
    "end": "955560"
  },
  {
    "text": "we used was step scaling which would which gave us an option to do gradually ramp ups and ramp downs and also we uh",
    "start": "955560",
    "end": "963000"
  },
  {
    "text": "established a pattern called over capacity pattern within our which helped to accommodate surges so I'll touch upon",
    "start": "963000",
    "end": "971160"
  },
  {
    "text": "uh what is the step scaling so if you look at this uh step scaling we we overrode the behavior of our HPA to say",
    "start": "971160",
    "end": "979319"
  },
  {
    "text": "that I want to do it in the most gradual fashion so in our case the most sweet SP spot was between 40 and 60% and if you",
    "start": "979319",
    "end": "986319"
  },
  {
    "text": "look at that particular full block We are Never aggressively scaling we are trying to maintain our state to make",
    "start": "986319",
    "end": "991800"
  },
  {
    "text": "sure that we are under that operating at that Target State and it was a very fine",
    "start": "991800",
    "end": "996959"
  },
  {
    "text": "grain controlled approach we did lot of iteration to get this number so that we don't uh have any uh capacity issues or",
    "start": "996959",
    "end": "1004720"
  },
  {
    "text": "even we are scaling we're not over scaling for any within this range next topic that I want to touch is",
    "start": "1004720",
    "end": "1011279"
  },
  {
    "text": "what is over capacity pattern so just an example that in your cluster say you have an application name space and you",
    "start": "1011279",
    "end": "1017639"
  },
  {
    "text": "have set of shared nodes and you have application running at scale so with over capacity pattern what we do is we",
    "start": "1017639",
    "end": "1024880"
  },
  {
    "text": "create an addition one more name space called over capacity and we deploy a low priority pods in that particular shared",
    "start": "1024880",
    "end": "1031678"
  },
  {
    "text": "notes which is the same shared notes that an application would share if you look at here I have only one pod in that",
    "start": "1031679",
    "end": "1037240"
  },
  {
    "text": "shared node that means you should uh the way we need to schedule that pod is it should take the entire resource of that",
    "start": "1037240",
    "end": "1043839"
  },
  {
    "text": "node so that that only one part is running in that sh node and to set a low priority pod uh you can update the",
    "start": "1043839",
    "end": "1051080"
  },
  {
    "text": "priority class for your pod to the least level so that it has the least priority for the",
    "start": "1051080",
    "end": "1056280"
  },
  {
    "text": "Schuler say for example I have an autoscaling event that happened now application there was a surge I started",
    "start": "1056280",
    "end": "1062559"
  },
  {
    "text": "getting uh in the HPA started uh triggering and I got a lot of pending ports on my application now the",
    "start": "1062559",
    "end": "1068520"
  },
  {
    "text": "scheduler is looking for a resource in an idle scenario if you do not have an or capacity that means it will go to the",
    "start": "1068520",
    "end": "1074160"
  },
  {
    "text": "cluster Autos scaler it will want it will request for more nodes and it has to wait and this weight period is what",
    "start": "1074160",
    "end": "1080400"
  },
  {
    "text": "was always uh uh impacting us because we were oscillating that CPU so with or",
    "start": "1080400",
    "end": "1086600"
  },
  {
    "text": "capacity what happens in this case is the scheduler will evict that low priority P which is already setting on",
    "start": "1086600",
    "end": "1092360"
  },
  {
    "text": "your shared node and it will make space for these pending ports all these pending ports gets that instance",
    "start": "1092360",
    "end": "1097840"
  },
  {
    "text": "capacity that means the amount of time that you took to get to accommodate that surge was instant and we were able to",
    "start": "1097840",
    "end": "1103480"
  },
  {
    "text": "add that capacity on this so if you look at this now that application got the capacity that it needs now those low",
    "start": "1103480",
    "end": "1110039"
  },
  {
    "text": "priority ports which were in the pending States the scheduler will spin up additional shared uh shared nodes for it",
    "start": "1110039",
    "end": "1116520"
  },
  {
    "text": "and they will also get scheduled so that means that pods that over capacity that you're running for your service is",
    "start": "1116520",
    "end": "1122919"
  },
  {
    "text": "always available for you in the entire life cycle of your application so that you're giving that instance capacity for",
    "start": "1122919",
    "end": "1129640"
  },
  {
    "text": "it so this was our before State before we implemented step scaling and over",
    "start": "1129640",
    "end": "1134799"
  },
  {
    "text": "capacity if you look at the first CPU uh where is or HPA metrics the CP is two",
    "start": "1134799",
    "end": "1140280"
  },
  {
    "text": "oscillating the the big block that you're seeing was the cause of not getting that instance capacity and that",
    "start": "1140280",
    "end": "1146400"
  },
  {
    "text": "is the reason our HPA requested more and more uh more and more uh compute on",
    "start": "1146400",
    "end": "1153120"
  },
  {
    "text": "ports so that it was never actually needed so if you see the the second diagraph where it is a desired over the",
    "start": "1153120",
    "end": "1159280"
  },
  {
    "text": "ready ports the amount of time it took to even get to a state that it had to operate that that point of time we",
    "start": "1159280",
    "end": "1164520"
  },
  {
    "text": "started seeing issues so we are not scaling for the demand and if you look at the the third graph we were still in",
    "start": "1164520",
    "end": "1171600"
  },
  {
    "text": "the ramp up stage but if you look at our HPA graph the number of ports started to drop off so that was a clear indication",
    "start": "1171600",
    "end": "1178760"
  },
  {
    "text": "that we were overscaling too much because we were not able to control our Target at the state that we wanted to",
    "start": "1178760",
    "end": "1185280"
  },
  {
    "text": "operate and after we implemented implemented this change so if you look at the first graph where talking about",
    "start": "1185280",
    "end": "1191360"
  },
  {
    "text": "the HPA uh we were M for that initial surge we never over scaled because that",
    "start": "1191360",
    "end": "1197640"
  },
  {
    "text": "over capacity was what it was protecting us our resir and the ready state was maintaining and the the Delta was",
    "start": "1197640",
    "end": "1204159"
  },
  {
    "text": "minimal and we never went ahead our uh went to ahead uh we didn't miss miss the",
    "start": "1204159",
    "end": "1210200"
  },
  {
    "text": "target State and we were staying on the Target and if we look at the whole ramp up of our request we stayed in the",
    "start": "1210200",
    "end": "1215440"
  },
  {
    "text": "steady state while our demand was met so what we really saved by doing this so it was a big saving it was a 25%",
    "start": "1215440",
    "end": "1222919"
  },
  {
    "text": "on our compute saving cost just improving how we were scaling our services this % in the larger scale when",
    "start": "1222919",
    "end": "1229760"
  },
  {
    "text": "we are multi-tenant this 25% is a big amount that we saved as a part of just improving how we scaled and accommodated",
    "start": "1229760",
    "end": "1236480"
  },
  {
    "text": "for the surges that we got so moving on within the same compute area one of the things we had as",
    "start": "1236480",
    "end": "1243960"
  },
  {
    "text": "underutilize nodes I'll touch upon this cold and idle worker nodes so say in",
    "start": "1243960",
    "end": "1251240"
  },
  {
    "text": "your cluster you have an application name space and you are running all your workload on the shared nodes and you",
    "start": "1251240",
    "end": "1257440"
  },
  {
    "text": "have it is running operating at scale good this is fine now there is a scal down activity that happened and you the",
    "start": "1257440",
    "end": "1265320"
  },
  {
    "text": "number of nodes that awards that it had to operate it will start going down now this is the state that we stayed in so",
    "start": "1265320",
    "end": "1272840"
  },
  {
    "text": "if you look at this the target state in reality we really scaled down the pods",
    "start": "1272840",
    "end": "1278080"
  },
  {
    "text": "but the nodes continued to stay we were not taking of the nodes at all they were sitting idle and cold and the amount of",
    "start": "1278080",
    "end": "1286240"
  },
  {
    "text": "time for it to get was it Tak to rebalance was anywhere between 10 to 12 hours or sometimes even even the days so",
    "start": "1286240",
    "end": "1292640"
  },
  {
    "text": "we were just paying for free uh uh just for underutilized nodes because we were",
    "start": "1292640",
    "end": "1297760"
  },
  {
    "text": "not able to rebalance this so some of the challenges were as I",
    "start": "1297760",
    "end": "1303120"
  },
  {
    "text": "mentioned we were slow scale down of nodes leading to inefficient resource utilization and also and what was",
    "start": "1303120",
    "end": "1309840"
  },
  {
    "text": "contributing to it because we had a slow cluster out of scaler which was not taking of the notes on like",
    "start": "1309840",
    "end": "1316559"
  },
  {
    "text": "quickly so Sol solution was simple don't rely on the default configuration what the cluster autoscaler would give uh",
    "start": "1316559",
    "end": "1322640"
  },
  {
    "text": "typically you need to look at all those configurations that are responsible for your cluster autoscaler how it will fit",
    "start": "1322640",
    "end": "1328279"
  },
  {
    "text": "in for your service and so that you would be able to make that decisions that how hot you want to run on your",
    "start": "1328279",
    "end": "1335240"
  },
  {
    "text": "nodes so these are all the settings that are available for the cluster autoscaler which are responsible for uh draining",
    "start": "1335240",
    "end": "1342880"
  },
  {
    "text": "your nodes quickly it it decides when you want to scale down at how long you need to wait to remove the node how hot",
    "start": "1342880",
    "end": "1349120"
  },
  {
    "text": "you want to run on your nodes so the the third parameter which says that scaled down utilization threshold if you keep",
    "start": "1349120",
    "end": "1355840"
  },
  {
    "text": "that higher that means you are using the maximum node utilization so you're running hotter and while and while your",
    "start": "1355840",
    "end": "1361880"
  },
  {
    "text": "pods would stay in the Target state that you want to operate so with this changes that we uh added we got better P to node",
    "start": "1361880",
    "end": "1369520"
  },
  {
    "text": "ratio we were able to Cordon the nodes quickly and more efficiently and the rebalance of PODS and notes was quick",
    "start": "1369520",
    "end": "1375720"
  },
  {
    "text": "and uh efficient by doing this change we had a big saving",
    "start": "1375720",
    "end": "1381159"
  },
  {
    "text": "15% on the compute this was an cumulative save saving on all the once we fixed Auto scaling and also the",
    "start": "1381159",
    "end": "1388400"
  },
  {
    "text": "cluster Auto scaler the default configuration was utilizing uh like making the notes",
    "start": "1388400",
    "end": "1395080"
  },
  {
    "text": "idle so let's touch upon another thing within the same compute area part",
    "start": "1395080",
    "end": "1400960"
  },
  {
    "text": "density this is little interesting uh just take a single node in your cluster",
    "start": "1400960",
    "end": "1407159"
  },
  {
    "text": "uh in this I have like that the whole this is like a node capacity that I can have I have pod one which is scheduled I",
    "start": "1407159",
    "end": "1413440"
  },
  {
    "text": "have a pod two I have I was able to schedule and also have a port three which I I was able to schedule and there",
    "start": "1413440",
    "end": "1418480"
  },
  {
    "text": "is a certain system level resources that needs to get allocated which is for demon sets and other system resources so",
    "start": "1418480",
    "end": "1425360"
  },
  {
    "text": "this top portion of our node which was 20% of our capacity which was not utilized by anything it was just sitting",
    "start": "1425360",
    "end": "1431679"
  },
  {
    "text": "idle so when we see this 20% when you have thousands of notes that you operate",
    "start": "1431679",
    "end": "1437120"
  },
  {
    "text": "this 20% is huge you are looking at wasting 300 course for not utilizing",
    "start": "1437120",
    "end": "1442720"
  },
  {
    "text": "anything or 400 course not utilizing anything and the main cause the challenge was we were we were",
    "start": "1442720",
    "end": "1449440"
  },
  {
    "text": "inefficiently bin packing our pods so that it was not taking the full capacity",
    "start": "1449440",
    "end": "1455080"
  },
  {
    "text": "of the node a lot of the times we would use platform capabilities for side car which will come automatically you you",
    "start": "1455080",
    "end": "1461240"
  },
  {
    "text": "need to even resize that you should never go by default always resize based on what instance type you operate so",
    "start": "1461240",
    "end": "1467080"
  },
  {
    "text": "that you you are able to utilize the full capacity you will have your own side curs that you develop even that",
    "start": "1467080",
    "end": "1472240"
  },
  {
    "text": "needs to be resized and uh based on what instance type you operate solution was simple make sure",
    "start": "1472240",
    "end": "1479120"
  },
  {
    "text": "that for a given instance type fine tune your request and limit and identify how",
    "start": "1479120",
    "end": "1484320"
  },
  {
    "text": "many ports you can fit into that given node and that should be the Target that you need to be operating once you know",
    "start": "1484320",
    "end": "1489559"
  },
  {
    "text": "that you are reaching your desired state that means you're utilizing your full node capacity continuing to the same",
    "start": "1489559",
    "end": "1495360"
  },
  {
    "text": "within the port density it selecting an instance type plays a very big big role here for in our case larger node gave us",
    "start": "1495360",
    "end": "1503399"
  },
  {
    "text": "a better throughput but it increased the blast radius for us in the sense our error appetite was low so we couldn't",
    "start": "1503399",
    "end": "1510080"
  },
  {
    "text": "operate on a bigger bigger instance type so we had to operate on certain instance type so that we are able to our fcis are",
    "start": "1510080",
    "end": "1516039"
  },
  {
    "text": "lower even if we lose an instance uh even in case of a failure so depending",
    "start": "1516039",
    "end": "1521200"
  },
  {
    "text": "on the instance type you need to be always resizing your pod so that you are not wasting any resources within your",
    "start": "1521200",
    "end": "1527080"
  },
  {
    "text": "node so solution I mean it's all about your service in our case availability is the",
    "start": "1527080",
    "end": "1533880"
  },
  {
    "text": "biggest Factor so we we make sure that we size in such a way that we are able to get the desired throughput and",
    "start": "1533880",
    "end": "1539399"
  },
  {
    "text": "operate uh in that fashion and again always look for an opportunities if you",
    "start": "1539399",
    "end": "1544440"
  },
  {
    "text": "can upgrade to a newer instance types which can give you better performance so in our case we were operating at c5n",
    "start": "1544440",
    "end": "1551080"
  },
  {
    "text": "before the analysis we went to C6 and we got a 15% just performance Improvement and it was a improvement in our cost as",
    "start": "1551080",
    "end": "1559880"
  },
  {
    "text": "well so one example how we calculated the pot density so I'm just taking one simple example for this one specific",
    "start": "1559880",
    "end": "1566520"
  },
  {
    "text": "instance type which is a C6 ion for extra large which comes with a 16 course",
    "start": "1566520",
    "end": "1571919"
  },
  {
    "text": "but in reality the usable space is around 13,600 cores because the rest of the CPUs for the systems and Demon sets",
    "start": "1571919",
    "end": "1580520"
  },
  {
    "text": "then we have all the side cars app is Theo spun it can be any side car that in your case so you are saying that I would",
    "start": "1580520",
    "end": "1586360"
  },
  {
    "text": "be operating per pod I would have around 3,300 cores and then I have a pod",
    "start": "1586360",
    "end": "1591720"
  },
  {
    "text": "density so pod density is the calculation that you can do from usable",
    "start": "1591720",
    "end": "1596760"
  },
  {
    "text": "uh compute over the compute byp so that means this is a number so that means you should be having this metric on your",
    "start": "1596760",
    "end": "1602520"
  },
  {
    "text": "dashboard to say that on my my ecosystem I'll be operating four nodes four pods",
    "start": "1602520",
    "end": "1609399"
  },
  {
    "text": "in that given node if the factor is in somewhere in like 4.5 3.5 that means",
    "start": "1609399",
    "end": "1615760"
  },
  {
    "text": "you're doing wrong you need to resize the prod again because you are literally wasting 05 part of the compute and that",
    "start": "1615760",
    "end": "1621679"
  },
  {
    "text": "is what we always went on Computing and we made sure that we have this whole number all the time which we're always",
    "start": "1621679",
    "end": "1628480"
  },
  {
    "text": "targeting so this was a big saving just by resizing the pods in such a way that we utilizing the full uh capacity of the",
    "start": "1628480",
    "end": "1635799"
  },
  {
    "text": "node we saved almost 10% this was all cumulative saving on top of what we were saving um uh with",
    "start": "1635799",
    "end": "1643279"
  },
  {
    "text": "autoscaling so this diagram is an outcome of all the changes that we did in terms of compute so for example we",
    "start": "1643279",
    "end": "1650159"
  },
  {
    "text": "have a ramp up that we had uh to accommodate the surges we got a we were",
    "start": "1650159",
    "end": "1655960"
  },
  {
    "text": "able to scale up the pods quickly because we had that over capacity which was protecting us we got the notes",
    "start": "1655960",
    "end": "1661519"
  },
  {
    "text": "scaled up because the demand was higher we were able to match the demand and say",
    "start": "1661519",
    "end": "1666919"
  },
  {
    "text": "we got a ram down at some point at that point of time the pot density is going to drop for you because we are going to",
    "start": "1666919",
    "end": "1673320"
  },
  {
    "text": "bring down the pods very quickly and nodes will continue to stay the goal was was uh and in this case we have an",
    "start": "1673320",
    "end": "1679960"
  },
  {
    "text": "optimized scale down because we updated our scaling step down scale down and",
    "start": "1679960",
    "end": "1685480"
  },
  {
    "text": "then even nodes are also getting rebalanced the graph for node is very crucial that you should see that nodes",
    "start": "1685480",
    "end": "1691200"
  },
  {
    "text": "are dying quickly while your pods are also going down and the outcome was this quick rebalance I think the amount of",
    "start": "1691200",
    "end": "1697640"
  },
  {
    "text": "time that it used to take previously was anywhere between like a day or even 12 and it never reached the stage that we",
    "start": "1697640",
    "end": "1703679"
  },
  {
    "text": "wanted it to and that particular Delta was the entire saving that we achieved as a part of our autoscaling and within",
    "start": "1703679",
    "end": "1710360"
  },
  {
    "text": "the compute space moving on uh data transfer so as Karim mentioned that",
    "start": "1710360",
    "end": "1717039"
  },
  {
    "text": "during this migration Journey we also onboarded to IO and with IO with with",
    "start": "1717039",
    "end": "1722640"
  },
  {
    "text": "the the the scale that API Gateway operates we incurred a new cost and we were we never we never knew that this is",
    "start": "1722640",
    "end": "1728960"
  },
  {
    "text": "a new cost that we would be incurring so you have a control plane you have an application name space you have all Theo",
    "start": "1728960",
    "end": "1735840"
  },
  {
    "text": "proxies which would make a connection to the isod which are sitting on the same control plane imagine like you have",
    "start": "1735840",
    "end": "1741880"
  },
  {
    "text": "thousands of ports that you operate in a given cluster they would make connection to all this uh is theodes and that",
    "start": "1741880",
    "end": "1748679"
  },
  {
    "text": "connection can go all over the place there will be cross a connections and and that they need to be communicating",
    "start": "1748679",
    "end": "1754720"
  },
  {
    "text": "to each other because they are passing on the configurations so challenge was we",
    "start": "1754720",
    "end": "1760200"
  },
  {
    "text": "encountered the high volume data transferred because of this change and we never have the observability but we",
    "start": "1760200",
    "end": "1766000"
  },
  {
    "text": "establish that observability first who is which name spaces are contributing which ports are doing we did that deep",
    "start": "1766000",
    "end": "1771159"
  },
  {
    "text": "dive analysis the solution was simple we had to just annotate our services to",
    "start": "1771159",
    "end": "1776880"
  },
  {
    "text": "topology our hints so the service object that is sits in front of your isod we",
    "start": "1776880",
    "end": "1782240"
  },
  {
    "text": "need to annotate that so that that will help establish it establishes the a",
    "start": "1782240",
    "end": "1787840"
  },
  {
    "text": "awareness and it all the connections and the data transfer that happens will be within the same AC starting eks 127 they",
    "start": "1787840",
    "end": "1795000"
  },
  {
    "text": "updated The annotation to the to topology mode so while in this journey while we",
    "start": "1795000",
    "end": "1802360"
  },
  {
    "text": "had this uh migration we also had a new monitoring solution we ended up with",
    "start": "1802360",
    "end": "1807559"
  },
  {
    "text": "Prometheus so every cluster that we operate has a different workload and they have this different capacity",
    "start": "1807559",
    "end": "1814159"
  },
  {
    "text": "requirement everything is different so we have Prometheus that we deploy for all our monitoring purposes so with est",
    "start": "1814159",
    "end": "1821760"
  },
  {
    "text": "so we started seeing an increased monitoring cost our metrics cardinality increased because our scaling was too",
    "start": "1821760",
    "end": "1826880"
  },
  {
    "text": "high and we were scra scraping too many irrelevant metrics the first thing we did we did a t-shirt sizing based me",
    "start": "1826880",
    "end": "1833880"
  },
  {
    "text": "approach we t-shirt sized all our Prometheus in such a way that which could match uh to its requirement so",
    "start": "1833880",
    "end": "1840240"
  },
  {
    "text": "that we could operate in certain instance types we were able to control the cardinality based on we're",
    "start": "1840240",
    "end": "1845840"
  },
  {
    "text": "protecting our system with certain uh metrics that we could get so that we know that if there are any offenders",
    "start": "1845840",
    "end": "1851440"
  },
  {
    "text": "they are actually dropped at the source itself we filtered lot of unused metrics lot of the times too many metrics get",
    "start": "1851440",
    "end": "1858760"
  },
  {
    "text": "ingested which are never used we started filtering them this is a simple example",
    "start": "1858760",
    "end": "1863880"
  },
  {
    "text": "from one of the service monitor crd where we can control how you drop the metrics from a monitor and also how you",
    "start": "1863880",
    "end": "1869760"
  },
  {
    "text": "protect the how many scrapes that you can get from a monitor so this kind of Saved um lot of Prometheus capacity that",
    "start": "1869760",
    "end": "1876639"
  },
  {
    "text": "we were just wasting so we saved this was a significant saving while we did just",
    "start": "1876639",
    "end": "1882480"
  },
  {
    "text": "small changes it was a 40% saving on our Prometheus cost because we did the right t-shirt sizing to operate for uh the",
    "start": "1882480",
    "end": "1890519"
  },
  {
    "text": "Gateway the other thing was the developer infrastructure cost now we are in a platform there are a lot of",
    "start": "1890519",
    "end": "1897000"
  },
  {
    "text": "developers contributing to a PL platform they are developing apis controllers",
    "start": "1897000",
    "end": "1902200"
  },
  {
    "text": "operators add-ons and they have their own independence of creating clusters testing and you know certifying it so",
    "start": "1902200",
    "end": "1909600"
  },
  {
    "text": "there were so many clusters that were getting created there was no hygiene process involved with that we had hundreds of Dem name spaces which were",
    "start": "1909600",
    "end": "1916399"
  },
  {
    "text": "again not hygiene and the lot of resources getting uh unused so to solve this first we started we created an",
    "start": "1916399",
    "end": "1923000"
  },
  {
    "text": "observability for every cluster that was created which was not managed so that was the key we wanted to know that we",
    "start": "1923000",
    "end": "1928919"
  },
  {
    "text": "have the metric for everything so that we know that how many clusters are getting created we created an internal",
    "start": "1928919",
    "end": "1935120"
  },
  {
    "text": "tool we never outsourced it we called it as an hangman in reality it actually just pauses it actually brings down all",
    "start": "1935120",
    "end": "1941880"
  },
  {
    "text": "the worker nodes for that cluster keeps the control plane up and running the key here is we didn't want to impact the",
    "start": "1941880",
    "end": "1947799"
  },
  {
    "text": "developer productivity uh so that we are and we are able to do in such a way that it is not too much of operational churn",
    "start": "1947799",
    "end": "1954000"
  },
  {
    "text": "for them and also we established the same pattern as port snoozer and we were able to achieve the same",
    "start": "1954000",
    "end": "1961039"
  },
  {
    "text": "thing we this was a big saving for us just in terms of developer uh infrastructure cost it was a 50 to 60%",
    "start": "1961039",
    "end": "1967919"
  },
  {
    "text": "saving uh for making sure that we do it in a way that it doesn't impact developers it was seamless uh",
    "start": "1967919",
    "end": "1975799"
  },
  {
    "text": "implementation for for all the developers who were contributing to the platform so yeah that was our",
    "start": "1975799",
    "end": "1982320"
  },
  {
    "text": "presentation in terms of how we were able to do and we have we were able to uh save in certain areas we have many",
    "start": "1982320",
    "end": "1989000"
  },
  {
    "text": "other areas that we we saved we had a lot of saving that we did in vaps logging and we are further optimizing",
    "start": "1989000",
    "end": "1995480"
  },
  {
    "text": "and we're looking at more opportunities so some of the takeaways from from this presentation is have cost",
    "start": "1995480",
    "end": "2001840"
  },
  {
    "text": "optimized design F that while during the time you are designing your system that is the key",
    "start": "2001840",
    "end": "2008039"
  },
  {
    "text": "second thing is I picked up this code uh it says don't throw money at the problem throw ideas at the problem you do get",
    "start": "2008039",
    "end": "2014320"
  },
  {
    "text": "short-term gains when you just uh you know you throw money for your situation but try to go back to the drawing board",
    "start": "2014320",
    "end": "2020480"
  },
  {
    "text": "and see how you can fix it and the third one is the most key you need to have an",
    "start": "2020480",
    "end": "2025919"
  },
  {
    "text": "observability don't rely on the default metrics that you get from any monitoring solution you need to derive your own",
    "start": "2025919",
    "end": "2031760"
  },
  {
    "text": "metric and make sure that you always look at it if you are deviating from that particular metric that means there",
    "start": "2031760",
    "end": "2037559"
  },
  {
    "text": "is already a uh already you are starting to see that there is a deviation and you",
    "start": "2037559",
    "end": "2042919"
  },
  {
    "text": "are going to get that cost thank you um so this is our QR code",
    "start": "2042919",
    "end": "2049240"
  },
  {
    "text": "for um the our booth please do visit and please share your feedback uh using this",
    "start": "2049240",
    "end": "2055280"
  },
  {
    "text": "QR code thank you",
    "start": "2055280",
    "end": "2060800"
  },
  {
    "text": "and so we have a minute for any questions yeah what is the process that",
    "start": "2064359",
    "end": "2071398"
  },
  {
    "text": "you use",
    "start": "2071399",
    "end": "2074000"
  },
  {
    "text": "toest so the question was that what's our process for dealing with requests limits our HPA thresholds that's a very",
    "start": "2078159",
    "end": "2085320"
  },
  {
    "text": "good question and it was basically a lot of testing you know it's it's hard to come up with those values right off the",
    "start": "2085320",
    "end": "2091320"
  },
  {
    "text": "bat we knew sort of what we had with our ec2 but it doesn't map exactly so we had",
    "start": "2091320",
    "end": "2096599"
  },
  {
    "text": "to run a lot of testing as well as even in production observing what are how",
    "start": "2096599",
    "end": "2101640"
  },
  {
    "text": "much CP are we using for different workloads and adjusting to see like you",
    "start": "2101640",
    "end": "2106839"
  },
  {
    "text": "know and very important to watch the CPU throttling metric to see if you're actually getting throttled because we",
    "start": "2106839",
    "end": "2113040"
  },
  {
    "text": "didn't have that so thank you for the question I think with that we're out of time so y thank you everyone thank",
    "start": "2113040",
    "end": "2122560"
  },
  {
    "text": "you",
    "start": "2123240",
    "end": "2126240"
  }
]