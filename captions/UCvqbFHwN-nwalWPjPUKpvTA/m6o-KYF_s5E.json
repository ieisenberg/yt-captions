[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "hello everyone today we're talking five ways for the cni I'm stick Telfer I'm",
    "start": "359",
    "end": "5819"
  },
  {
    "text": "CTO of a technical consultancy business called stack HPC we specialize in high performance Cloud infrastructure and",
    "start": "5819",
    "end": "12599"
  },
  {
    "text": "platforms for scientific computing",
    "start": "12599",
    "end": "16280"
  },
  {
    "text": "I'm responsible for cloud networking at Nvidia in this talk we will discuss and",
    "start": "17840",
    "end": "24359"
  },
  {
    "text": "compare some of the most common kubernetes networking configurations for",
    "start": "24359",
    "end": "29519"
  },
  {
    "text": "performance intensive workloads high performance Computing or HPC is a",
    "start": "29519",
    "end": "37920"
  },
  {
    "start": "33000",
    "end": "33000"
  },
  {
    "text": "field of computer science that solves complex problems such as fluid dynamics for aircraft design for example",
    "start": "37920",
    "end": "45500"
  },
  {
    "text": "large-scale weather forecast or drug Discovery through the use of large-scale",
    "start": "45500",
    "end": "51660"
  },
  {
    "text": "compute simulations HPC is one of the most compute Network",
    "start": "51660",
    "end": "57719"
  },
  {
    "text": "and storage intensive workloads Technologies developed for HPC often",
    "start": "57719",
    "end": "63539"
  },
  {
    "text": "make their way to more standout data center applications",
    "start": "63539",
    "end": "69020"
  },
  {
    "start": "69000",
    "end": "69000"
  },
  {
    "text": "another example of compute intensive applications are artificial intelligence or AI",
    "start": "69900",
    "end": "76320"
  },
  {
    "text": "AI is probably the greatest revolution of our time it allows computers to solve",
    "start": "76320",
    "end": "81720"
  },
  {
    "text": "problems that only few years ago seemed impossible AI can create images and text from Human",
    "start": "81720",
    "end": "88500"
  },
  {
    "text": "description translate languages recommend specific items from wealth of",
    "start": "88500",
    "end": "94080"
  },
  {
    "text": "options and even write code AI is very common these days and used in",
    "start": "94080",
    "end": "100140"
  },
  {
    "text": "many web services and other common applications",
    "start": "100140",
    "end": "104720"
  },
  {
    "start": "104000",
    "end": "104000"
  },
  {
    "text": "both Ai and HPC are compute intensive and they typically cannot run on a",
    "start": "106140",
    "end": "112799"
  },
  {
    "text": "single server they require a cluster of servers and run as a distributed application",
    "start": "112799",
    "end": "119700"
  },
  {
    "text": "when we're running application in that fashion networking become a critical element for the proper execution of the",
    "start": "119700",
    "end": "127320"
  },
  {
    "text": "application when we're looking at the natural consideration the first and foremost is throughput or",
    "start": "127320",
    "end": "134879"
  },
  {
    "text": "bandwidth and here we would like to have as much as we can today we're running at",
    "start": "134879",
    "end": "140520"
  },
  {
    "text": "100 Gig and going forward we're already deploying 400 gigabit per second",
    "start": "140520",
    "end": "146580"
  },
  {
    "text": "but latency and the predictability of latency is is as important for those",
    "start": "146580",
    "end": "152160"
  },
  {
    "text": "kind of workloads and obviously we would like to have low and predictable latency across all packet types",
    "start": "152160",
    "end": "159840"
  },
  {
    "text": "when we're running a lot of networking the CPU is uh very busy with the network",
    "start": "159840",
    "end": "166860"
  },
  {
    "text": "itself and consumes quite a lot of core CPU cores um we would like obviously to free up",
    "start": "166860",
    "end": "173760"
  },
  {
    "text": "those CPU for running the application itself and therefore we're looking for CPU off-road capabilities so that the",
    "start": "173760",
    "end": "181019"
  },
  {
    "text": "networking will be handled by the neck and not by the CPU and lastly in many of those application",
    "start": "181019",
    "end": "187319"
  },
  {
    "text": "environments gpus are in play and we would like to make sure that not only the CPU can access the network properly",
    "start": "187319",
    "end": "194040"
  },
  {
    "text": "but also the GPU Technologies like GPU direct become very important",
    "start": "194040",
    "end": "200099"
  },
  {
    "text": "RDMA can address a lot of those requirements and I'll explain RDMA",
    "start": "200099",
    "end": "206780"
  },
  {
    "start": "207000",
    "end": "207000"
  },
  {
    "text": "RDMA or remote direct memory access is a transport service similar is the TCP and",
    "start": "208560",
    "end": "216180"
  },
  {
    "text": "UDP but it is a more modern one in addition to send and receive it also",
    "start": "216180",
    "end": "223799"
  },
  {
    "text": "support memory read and write semantics which allows us to write a very",
    "start": "223799",
    "end": "230099"
  },
  {
    "text": "efficient applications it also supports natively kernel bypass which allows to",
    "start": "230099",
    "end": "236280"
  },
  {
    "text": "reduce significantly the latencies of the application can send and receive without going through the kernel and it",
    "start": "236280",
    "end": "243540"
  },
  {
    "text": "supports natively harder offloads with the right Hardware",
    "start": "243540",
    "end": "248700"
  },
  {
    "text": "like the Nvidia harder for example which allows us to transfer",
    "start": "248700",
    "end": "253939"
  },
  {
    "text": "hundreds of gigabits per second without any CPU intervention RMA was designed initially for",
    "start": "253939",
    "end": "261120"
  },
  {
    "text": "technology called infiniband but for a long time now it's also",
    "start": "261120",
    "end": "266460"
  },
  {
    "text": "available on ethernet under the name of Rocky RDMA over converged ethernet in",
    "start": "266460",
    "end": "272820"
  },
  {
    "text": "this talk we will see how rocky can be used with kubernetes",
    "start": "272820",
    "end": "279500"
  },
  {
    "text": "now let's uh review the various Network configurations that we will use for this stock and we'll start with Calico",
    "start": "280259",
    "end": "288680"
  },
  {
    "text": "is a popular open source networking and network security platform for kubernetes",
    "start": "288680",
    "end": "293940"
  },
  {
    "text": "created by tigera Calico addresses connectivity between pods to pods and",
    "start": "293940",
    "end": "301800"
  },
  {
    "text": "pods to service and service to ponds and I think in it and it includes Ingress",
    "start": "301800",
    "end": "307740"
  },
  {
    "text": "and egress load balancing Calico also provides network security",
    "start": "307740",
    "end": "312960"
  },
  {
    "text": "features which is out of the scope of our talk today Calico has a central controller that",
    "start": "312960",
    "end": "319800"
  },
  {
    "text": "calculate the policies and routes and stored them in an etcd data store",
    "start": "319800",
    "end": "325620"
  },
  {
    "text": "on each node Calico runs three components conf day which monitors the",
    "start": "325620",
    "end": "332160"
  },
  {
    "text": "etcd data store for configuration changes and update bird configuration files",
    "start": "332160",
    "end": "338520"
  },
  {
    "text": "Felix which configures iptables and Linux routes based on the policies and",
    "start": "338520",
    "end": "344039"
  },
  {
    "text": "connectivity and bird which is an open source bgp agent which advertise Felix routes",
    "start": "344039",
    "end": "352740"
  },
  {
    "text": "conical support both bgp and Vixen routing in our talk we will use vx9",
    "start": "352740",
    "end": "360060"
  },
  {
    "text": "overlay in addition Calico have two data path implementation standard Linux which uses",
    "start": "360060",
    "end": "367440"
  },
  {
    "text": "Linux route and iptable as I said before and ebpf which is extended Berkeley",
    "start": "367440",
    "end": "373800"
  },
  {
    "text": "packet filters for our talk we use the standout Linux option",
    "start": "373800",
    "end": "380960"
  },
  {
    "start": "380000",
    "end": "380000"
  },
  {
    "text": "the next cni is ovn kubernetes uh ovm kubernetes is the ovn cni for",
    "start": "381240",
    "end": "390419"
  },
  {
    "text": "kubernetes environment naturally it uses ovn open Virtual networking",
    "start": "390419",
    "end": "396300"
  },
  {
    "text": "which is an open source SDM controller um that uses ovs it's from the same",
    "start": "396300",
    "end": "403080"
  },
  {
    "text": "project ovn uses logical components such as",
    "start": "403080",
    "end": "408199"
  },
  {
    "text": "switches and routers it supports security and access control",
    "start": "408199",
    "end": "415199"
  },
  {
    "text": "list load balancers and many other features ovn programs ovs data is its data plane",
    "start": "415199",
    "end": "424880"
  },
  {
    "text": "for those of you that are not familiar with obvious OBS is a virtual switch that is flow based meaning it can be",
    "start": "424880",
    "end": "432900"
  },
  {
    "text": "programmed to implement most of the data plan elements of switches and routers",
    "start": "432900",
    "end": "441740"
  },
  {
    "start": "442000",
    "end": "442000"
  },
  {
    "text": "is sriov single root IO virtualization or srlv is a PCI technology which allows",
    "start": "443539",
    "end": "452699"
  },
  {
    "text": "to segment the PCI device into virtual devices and assign them to the Pod or VM",
    "start": "452699",
    "end": "459720"
  },
  {
    "text": "through a virtual function which exposes basically the device or sub-device into",
    "start": "459720",
    "end": "466620"
  },
  {
    "text": "the the Pod or the uh the VM um in networking the device is pretty",
    "start": "466620",
    "end": "474300"
  },
  {
    "text": "simple even though it can support Advanced capabilities like RDMA the networking element is pretty simple and",
    "start": "474300",
    "end": "481319"
  },
  {
    "text": "it behaves like a Mac VLAN uh there's no overlay you know sdn capabilities other",
    "start": "481319",
    "end": "488160"
  },
  {
    "text": "than basic switching and quality of service for virtual function",
    "start": "488160",
    "end": "494940"
  },
  {
    "text": "however slav provide us with very very high performance close to Bare Metal as",
    "start": "494940",
    "end": "502440"
  },
  {
    "text": "it bypass the entire virtualization stack to use uh sriv in kubernetes we're using",
    "start": "502440",
    "end": "510360"
  },
  {
    "text": "SDS or vcni and also in addition the SRV",
    "start": "510360",
    "end": "515459"
  },
  {
    "text": "device plugin um slav C90",
    "start": "515459",
    "end": "522800"
  },
  {
    "text": "support the all the network requirements of a pod and therefore it is used",
    "start": "522800",
    "end": "528420"
  },
  {
    "text": "typically as a secondary cni and apparently like Calico or rovian it",
    "start": "528420",
    "end": "535560"
  },
  {
    "text": "together with parameters in cni as Calico ovn through the use of multis",
    "start": "535560",
    "end": "540959"
  },
  {
    "text": "which is a meta Plugin or meta cni",
    "start": "540959",
    "end": "545300"
  },
  {
    "start": "546000",
    "end": "546000"
  },
  {
    "text": "this next configuration is accelerated ovn now before I'll explain the technology let's first understand the",
    "start": "546240",
    "end": "553440"
  },
  {
    "text": "challenges of software-based virtual switches ovs is built of two components",
    "start": "553440",
    "end": "561240"
  },
  {
    "text": "a kernel model and a space demon when a packet comes in it will go to the",
    "start": "561240",
    "end": "568260"
  },
  {
    "text": "kernel module or the fast path and classify the packet and see if there is",
    "start": "568260",
    "end": "573720"
  },
  {
    "text": "a rule what to do with that pocket if there is a rule the packet will be executed meaning will be sent to the Pod",
    "start": "573720",
    "end": "579779"
  },
  {
    "text": "or do something else if there's a miss this packet will go to the user space Daemon for further uh",
    "start": "579779",
    "end": "588300"
  },
  {
    "text": "analysis and once a decision is made what to do with this new flow it will",
    "start": "588300",
    "end": "594360"
  },
  {
    "text": "program the the kernel data plane and from that point on packets will just go",
    "start": "594360",
    "end": "600120"
  },
  {
    "text": "through the fast path now both the kernel the kernel module and the user",
    "start": "600120",
    "end": "605760"
  },
  {
    "text": "space demon are CPU running on the CPU and the general purpose CPU is not very",
    "start": "605760",
    "end": "612660"
  },
  {
    "text": "good in processing High rate of packets and therefore ovs and many of the other",
    "start": "612660",
    "end": "620060"
  },
  {
    "text": "virtual switches are have very low bandwidth and very high latency and high",
    "start": "620060",
    "end": "626880"
  },
  {
    "text": "CP utilization in addition um because the interface to the Pod or",
    "start": "626880",
    "end": "634500"
  },
  {
    "text": "to the VM is eth pair none of the advanced features of the modern X are",
    "start": "634500",
    "end": "642899"
  },
  {
    "text": "supported like RDMA and dpdk and so on",
    "start": "642899",
    "end": "647940"
  },
  {
    "text": "to address these challenges Nvidia added to our NYX an embedded switch",
    "start": "647940",
    "end": "654899"
  },
  {
    "text": "ovs was enhanced through a set of API to program that embedded switch on the neck",
    "start": "654899",
    "end": "662279"
  },
  {
    "text": "and program its data plane the pods are now connected directly to",
    "start": "662279",
    "end": "669660"
  },
  {
    "text": "the neck through advanced sriov mode called e-switch mode every packet that comes from the network",
    "start": "669660",
    "end": "676920"
  },
  {
    "text": "will hit the e-switch first and will classify the packets there if there is a",
    "start": "676920",
    "end": "683160"
  },
  {
    "text": "rule that defines what to do with that the packet will be executed and be sent",
    "start": "683160",
    "end": "688320"
  },
  {
    "text": "to the Pod and vice versa if there is no such rule the packet will go into ovs kernel module and from there",
    "start": "688320",
    "end": "696720"
  },
  {
    "text": "will continue just like before with this approach we actually can have",
    "start": "696720",
    "end": "702420"
  },
  {
    "text": "the cake in 82. um we have one primary cni ovn in this",
    "start": "702420",
    "end": "709320"
  },
  {
    "text": "case that supports all the capabilities needed for kubernetes all the kubernetes services all the Network Services the",
    "start": "709320",
    "end": "716880"
  },
  {
    "text": "quality of service and visibility and so on it support bonding which Legacy srav",
    "start": "716880",
    "end": "722880"
  },
  {
    "text": "does not and it supports security but unlike the virtual switches and",
    "start": "722880",
    "end": "730019"
  },
  {
    "text": "other cnis that are software based it provides bare metal performance of the",
    "start": "730019",
    "end": "736200"
  },
  {
    "text": "server and in addition it provides additional capabilities that the network",
    "start": "736200",
    "end": "742320"
  },
  {
    "text": "adapter may have such as RDMA and dpdk",
    "start": "742320",
    "end": "748920"
  },
  {
    "text": "so those were the configuration that we will run in our lab let me just explain",
    "start": "748920",
    "end": "754399"
  },
  {
    "start": "749000",
    "end": "749000"
  },
  {
    "text": "what lab did we build we actually built two Labs one for kubernetes bare metal",
    "start": "754399",
    "end": "760200"
  },
  {
    "text": "and one for kubernetes over openstack on the kubernetes over bear metal we have",
    "start": "760200",
    "end": "767279"
  },
  {
    "text": "two worker nodes uh the servers are quite high-end for the performance",
    "start": "767279",
    "end": "773459"
  },
  {
    "text": "benchmarking we're running HP DL 380 gen10 with 8380 CPUs uh two pneumas each",
    "start": "773459",
    "end": "784079"
  },
  {
    "text": "one of 40 core so overall 80 cores and half a terabyte of RAM and those devices",
    "start": "784079",
    "end": "792060"
  },
  {
    "text": "are and those servers running in video connecting 6dx with two ports of 100 gigabit each on the OS we're running",
    "start": "792060",
    "end": "799860"
  },
  {
    "text": "Ubuntu 2004 with latest kernel and then video offered for latest uh networking",
    "start": "799860",
    "end": "806820"
  },
  {
    "text": "drivers kubernetes version is 1.23.7 and we deployed with Cube spray",
    "start": "806820",
    "end": "813720"
  },
  {
    "text": "and the port is running Ubuntu again with the offered packages",
    "start": "813720",
    "end": "820399"
  },
  {
    "text": "the network interfaces are bonded and we're running at 9kmtu connecting to a",
    "start": "820399",
    "end": "828540"
  },
  {
    "text": "switch and a SN through 3700 32 volts of",
    "start": "828540",
    "end": "834660"
  },
  {
    "text": "up to 200 gigabit per second so plenty of interfaces running cumulus Linux 4.4",
    "start": "834660",
    "end": "840620"
  },
  {
    "text": "the master node is a lower grade server running Ubuntu 2004 and the same",
    "start": "840620",
    "end": "847139"
  },
  {
    "text": "kubernetes version so this is the bare metal environment",
    "start": "847139",
    "end": "853560"
  },
  {
    "text": "the kubernetes over openstack environment is obviously the same",
    "start": "853560",
    "end": "859560"
  },
  {
    "start": "854000",
    "end": "854000"
  },
  {
    "text": "Hardware two compute nodes but this time on the operating system we did install yoga openstack and uh we created a VM",
    "start": "859560",
    "end": "868620"
  },
  {
    "text": "running Ubuntu 2004 and the latest kernels and and offered packages and",
    "start": "868620",
    "end": "874920"
  },
  {
    "text": "inside the VM we basically deployed the same kubernetes as before and then obviously the pods running in that",
    "start": "874920",
    "end": "880980"
  },
  {
    "text": "kubernetes the switches and all the rest is exactly the same uh of course in the",
    "start": "880980",
    "end": "888300"
  },
  {
    "text": "controller node we did need to install uh openstack yoga controller",
    "start": "888300",
    "end": "894720"
  },
  {
    "text": "now the the networking on the nested environment uh kubernetes of our openstack is a little bit uh tricky",
    "start": "894720",
    "end": "902820"
  },
  {
    "text": "um we don't have the connectix device and inside the holster running ovn with",
    "start": "902820",
    "end": "908339"
  },
  {
    "text": "ovs offload so this ovs is offloaded the data plane into the connectics as I",
    "start": "908339",
    "end": "914459"
  },
  {
    "text": "explained before inside the VM we're running Calico which is connected to the",
    "start": "914459",
    "end": "919560"
  },
  {
    "text": "obvious non-accelerated and that's the primary Network for the Pod and in",
    "start": "919560",
    "end": "924720"
  },
  {
    "text": "addition we have a secondary ovn SRI V Network going to the Pod and so the",
    "start": "924720",
    "end": "932279"
  },
  {
    "text": "application can use RDMA which basically goes to the virtual function and then",
    "start": "932279",
    "end": "937860"
  },
  {
    "text": "we'll go out through the neck with the rules and the sdn layers of the OBS all",
    "start": "937860",
    "end": "943800"
  },
  {
    "text": "it can use is either the TCP from the net device of the virtual function and then it's accelerated or from the Calico",
    "start": "943800",
    "end": "951120"
  },
  {
    "text": "which is non-accelerated and with this I would like to hand it to Stig to discuss",
    "start": "951120",
    "end": "958620"
  },
  {
    "text": "our benchmarks and results we have an awesome lab environment now",
    "start": "958620",
    "end": "966360"
  },
  {
    "start": "963000",
    "end": "963000"
  },
  {
    "text": "we need to Define some representative test cases Eris mentioned those key considerations",
    "start": "966360",
    "end": "972180"
  },
  {
    "text": "for networking high bandwidth low latency low CPU overhead and predictable",
    "start": "972180",
    "end": "978120"
  },
  {
    "text": "fairness under contention we created a suite of test cases that",
    "start": "978120",
    "end": "983699"
  },
  {
    "text": "should be both insightful and representative for HPC and AI applications",
    "start": "983699",
    "end": "989160"
  },
  {
    "text": "this Benchmark Suite is also open source and available on GitHub today under this",
    "start": "989160",
    "end": "994680"
  },
  {
    "text": "link it uses the volcano drop scheduler to run through a set of test cases and",
    "start": "994680",
    "end": "1000860"
  },
  {
    "text": "extract results in an automated and repeatable process it uses kubeflow's MPI operator to",
    "start": "1000860",
    "end": "1007579"
  },
  {
    "text": "orchestrate the execution of those parallel applications that we're benchmarking",
    "start": "1007579",
    "end": "1013540"
  },
  {
    "start": "1014000",
    "end": "1014000"
  },
  {
    "text": "first up we can start on familiar ground by benchmarking TCP performance using",
    "start": "1014959",
    "end": "1020360"
  },
  {
    "text": "the ubiquitous iperf benchmark we use version two of the application here",
    "start": "1020360",
    "end": "1026480"
  },
  {
    "text": "we measure aggregate TCP bandwidth for increasing numbers of concurrent iperf",
    "start": "1026480",
    "end": "1031760"
  },
  {
    "text": "clients in these tests we're using pod to pod networking which is more representative",
    "start": "1031760",
    "end": "1037280"
  },
  {
    "text": "for these scenarios rather than include service IPS which would not be representative for most of",
    "start": "1037280",
    "end": "1043400"
  },
  {
    "text": "our use cases I'm plotting the individual client bandwidth here as a stacked bar this",
    "start": "1043400",
    "end": "1050000"
  },
  {
    "text": "shows us fairness even fairness results in even width stripes and it's fairly",
    "start": "1050000",
    "end": "1055039"
  },
  {
    "text": "easy to see that the aggregate bandwidth is the total height of the bar",
    "start": "1055039",
    "end": "1060620"
  },
  {
    "text": "all the graphs are scaled to the physical limits of our lab networking Hardware 200 gigabits a second per",
    "start": "1060620",
    "end": "1067700"
  },
  {
    "text": "server we set the scene here with the Baseline performance",
    "start": "1067700",
    "end": "1073100"
  },
  {
    "text": "of host networking bypassing the cni altogether",
    "start": "1073100",
    "end": "1078039"
  },
  {
    "text": "now a common choice in high bandwidth networking is to increase the ethernet frame size or MTU from 1500 bytes which",
    "start": "1078860",
    "end": "1086780"
  },
  {
    "text": "is the standard to 9000 which is known as a jumbo frame this has the effect of increasing",
    "start": "1086780",
    "end": "1092960"
  },
  {
    "text": "application bandwidth while reducing the CPU overhead of packet processing",
    "start": "1092960",
    "end": "1099380"
  },
  {
    "text": "on this slide we have two charts measuring the effect of a 9000 byte MTU",
    "start": "1099380",
    "end": "1104419"
  },
  {
    "text": "and its uplift on performance it's actually not such a massive difference in results here",
    "start": "1104419",
    "end": "1109820"
  },
  {
    "text": "due probably to the high-end network cards that we're using but what is not shown is the extra work behind the",
    "start": "1109820",
    "end": "1116480"
  },
  {
    "text": "scenes that the host kernel is doing in packet processing load for standard frame sizes",
    "start": "1116480",
    "end": "1122299"
  },
  {
    "text": "this hidden cost is not shown in the end results but will return to expose and explore this a bit later",
    "start": "1122299",
    "end": "1130360"
  },
  {
    "text": "so we've adopted a 9000 byte MTU for the benchmarks in this presentation as being",
    "start": "1130880",
    "end": "1136880"
  },
  {
    "text": "considered representative of HPC networking use cases before we go on though it's just amazing",
    "start": "1136880",
    "end": "1143660"
  },
  {
    "text": "to see that Modern Hardware a Modern Hardware server can readily saturate 200",
    "start": "1143660",
    "end": "1149419"
  },
  {
    "text": "gigabits a second of networking foreign first up let's start with cni results",
    "start": "1149419",
    "end": "1157160"
  },
  {
    "text": "with a couple of standard choices two of the most popular cnis for kubernetes are calico and ovn",
    "start": "1157160",
    "end": "1165140"
  },
  {
    "text": "in this slide Calico is the chart on the left and ovn is the chart on the right",
    "start": "1165140",
    "end": "1170360"
  },
  {
    "text": "color code is configured with default settings we've done nothing special here it's using vxlan as the transport layer",
    "start": "1170360",
    "end": "1178400"
  },
  {
    "text": "ovn is configured without sdn acceleration at this stage it's also at default settings we'll return to the sdn",
    "start": "1178400",
    "end": "1185179"
  },
  {
    "text": "acceleration shortly the difference in performance here is Stark we can see that Calico costs",
    "start": "1185179",
    "end": "1192740"
  },
  {
    "text": "almost no additional overhead over host networking and is delivering good fairness",
    "start": "1192740",
    "end": "1198200"
  },
  {
    "text": "both cnis get 30 and a half gigabits a second for a single iperb TCP stream",
    "start": "1198200",
    "end": "1204620"
  },
  {
    "text": "but for ovn Beyond a small number of concurrent TCP streams our performance",
    "start": "1204620",
    "end": "1209960"
  },
  {
    "text": "tails off to a fraction of peak bandwidth why is that we can speculate about the different",
    "start": "1209960",
    "end": "1215960"
  },
  {
    "text": "fortunes of these two cnis for ovn offloading performance may be",
    "start": "1215960",
    "end": "1221299"
  },
  {
    "text": "impacted by genetic encapsulation or the complexity of ovn's programmable sdn",
    "start": "1221299",
    "end": "1226340"
  },
  {
    "text": "pipeline routing net security groups and connection tracking are all parts of a",
    "start": "1226340",
    "end": "1231620"
  },
  {
    "text": "flexible solution in OPN but make it harder to apply offloads",
    "start": "1231620",
    "end": "1237020"
  },
  {
    "text": "ovian may also be affected by latent configuration in our lab environment for Hardware acceleration",
    "start": "1237020",
    "end": "1243320"
  },
  {
    "text": "it's present here but not used for this test which may be affecting the capacity of the network receive path it's a",
    "start": "1243320",
    "end": "1250039"
  },
  {
    "text": "theory that we don't have time to test it for Calico we appear to be getting",
    "start": "1250039",
    "end": "1255260"
  },
  {
    "text": "benefit from the stateless offloads of the connect X6 network cards we are using",
    "start": "1255260",
    "end": "1260720"
  },
  {
    "text": "in particular the network card can do TCP segmentation offload to a vxlan vni",
    "start": "1260720",
    "end": "1266720"
  },
  {
    "text": "the kind of networking used by Calico enabling the Kernel's networking stack to process network data in much bigger",
    "start": "1266720",
    "end": "1273799"
  },
  {
    "text": "chunks in a Calico configuration there's a need here for further",
    "start": "1273799",
    "end": "1280340"
  },
  {
    "text": "investigation clearly but for lack of time with ovn there may well be room to",
    "start": "1280340",
    "end": "1286100"
  },
  {
    "text": "improve on these results foreign",
    "start": "1286100",
    "end": "1291158"
  },
  {
    "text": "choice of cni for high performance use cases is to use sriv in which pod",
    "start": "1293380",
    "end": "1299299"
  },
  {
    "text": "networking has direct access to Nic Hardware unfortunately there is no bonding",
    "start": "1299299",
    "end": "1304820"
  },
  {
    "text": "support in Lego legacy sriv and our test configuration has bonding enabled so",
    "start": "1304820",
    "end": "1310220"
  },
  {
    "text": "this restricts us to only one port of our dual Port network interface in the test Hardware",
    "start": "1310220",
    "end": "1316460"
  },
  {
    "text": "and this is clear from the results where the bandwidth quickly hits a cap of 100 gigabits a second",
    "start": "1316460",
    "end": "1322400"
  },
  {
    "text": "the other network interface of course can be used for Calico or another cni providing flexible options alongside the",
    "start": "1322400",
    "end": "1328400"
  },
  {
    "text": "sriv interface the state-of-the-art today is for sdn",
    "start": "1328400",
    "end": "1335419"
  },
  {
    "text": "acceleration offloads as are as described we can transfer ovn's sdn flow rules",
    "start": "1335419",
    "end": "1341480"
  },
  {
    "text": "from the host processor to the network card Hardware directly offloading the",
    "start": "1341480",
    "end": "1346580"
  },
  {
    "text": "packet processing work of openv switch from host kernel to network card",
    "start": "1346580",
    "end": "1352520"
  },
  {
    "text": "this technology combines the flexibility of ovn software-defined networking with the performance of Hardware offloading",
    "start": "1352520",
    "end": "1359659"
  },
  {
    "text": "but does it fulfill this potential well on the left we see the accelerated",
    "start": "1359659",
    "end": "1365419"
  },
  {
    "text": "ovn in a bare metal kubernetes environment and on the right we see accelerated ovn",
    "start": "1365419",
    "end": "1371480"
  },
  {
    "text": "in openstack running kubernetes worker nodes as VMS providing direct access to",
    "start": "1371480",
    "end": "1377120"
  },
  {
    "text": "Hardware using the Mac VLAN cni in both cases our single client TCP",
    "start": "1377120",
    "end": "1384140"
  },
  {
    "text": "performers is over 50 gigabits a second somehow exceeding the performance of the host networking configuration which is",
    "start": "1384140",
    "end": "1391400"
  },
  {
    "text": "hard to explain but maybe due to Evolutions in our lab setup tuning",
    "start": "1391400",
    "end": "1397039"
  },
  {
    "text": "with accelerated ovn we achieve very high performance quickly saturating the",
    "start": "1397039",
    "end": "1402679"
  },
  {
    "text": "200 gigabits a second available I should say the bare metal accelerated over and graph on the left is actually",
    "start": "1402679",
    "end": "1409580"
  },
  {
    "text": "the best result of five runs aggregate performance can be variable in particular for smaller number of streams",
    "start": "1409580",
    "end": "1416059"
  },
  {
    "text": "in a bonded network configuration this is likely due to the static nature of lag distribution functions on the",
    "start": "1416059",
    "end": "1422840"
  },
  {
    "text": "bond creating uneven balance in the flows between the two ports",
    "start": "1422840",
    "end": "1428600"
  },
  {
    "text": "the same effect would apply to the Mac VLAN chart on the right but unfortunately we only gathered one",
    "start": "1428600",
    "end": "1433820"
  },
  {
    "text": "result here and didn't have the time to return to this configuration and repeat the experiment",
    "start": "1433820",
    "end": "1440440"
  },
  {
    "text": "one thing to note is the unfairness apparent in the ovn configuration the",
    "start": "1440600",
    "end": "1447140"
  },
  {
    "text": "different stripe widths of the bare metal accelerated over the end result implies significant unfairness between",
    "start": "1447140",
    "end": "1452480"
  },
  {
    "text": "clients under contention which requires some further investigation to understand a little",
    "start": "1452480",
    "end": "1457940"
  },
  {
    "text": "better one theory is that a reduced number of Nick receive cues used in the ovn",
    "start": "1457940",
    "end": "1464179"
  },
  {
    "text": "virtual functions was leading to uneven performance cutting-edge developments such as these",
    "start": "1464179",
    "end": "1469940"
  },
  {
    "text": "push the capabilities of today's hardware and software to their limits the",
    "start": "1469940",
    "end": "1476320"
  },
  {
    "start": "1476000",
    "end": "1476000"
  },
  {
    "text": "benefit we mentioned earlier of Hardware offloading is clear when you examine the host CPU load during the test execution",
    "start": "1477559",
    "end": "1485720"
  },
  {
    "text": "with the Calico test in the top row here the system CPU light was measured at about 30 percent while the tests were",
    "start": "1485720",
    "end": "1492440"
  },
  {
    "text": "underway with accelerated ovn the CPU load drops to only 15 giving a Delta of 15 of our",
    "start": "1492440",
    "end": "1499700"
  },
  {
    "text": "CPU load between the two cnis now the CPUs we're using in this test",
    "start": "1499700",
    "end": "1505220"
  },
  {
    "text": "are 40 core Platinum ICE Lake xeons that's a part with an 8 600 list price",
    "start": "1505220",
    "end": "1512539"
  },
  {
    "text": "saving 15 of the CPU cores in this system on a dual socket configuration would be 12 cores or two thousand six",
    "start": "1512539",
    "end": "1519320"
  },
  {
    "text": "hundred dollars if we are working these systems hard as we really should be then that's a lot of",
    "start": "1519320",
    "end": "1525500"
  },
  {
    "text": "CPU resource that is not going into our workloads we can also observe the accelerated ovn",
    "start": "1525500",
    "end": "1533480"
  },
  {
    "start": "1530000",
    "end": "1530000"
  },
  {
    "text": "flow offloads at work during the benchmarking in our tests up to 120 sdn",
    "start": "1533480",
    "end": "1539299"
  },
  {
    "text": "flow rules were offloaded from openv switch to the cut from the in the kernel direct to the neck Hardware",
    "start": "1539299",
    "end": "1546440"
  },
  {
    "text": "in fact the connect X6 can support hundreds of thousands or even millions of hard rock loaded flows we're barely",
    "start": "1546440",
    "end": "1553039"
  },
  {
    "text": "scratching the surface of this capability here this can be particularly powerful for advanced networking",
    "start": "1553039",
    "end": "1558440"
  },
  {
    "text": "applications concurrently serving many thousands of connections",
    "start": "1558440",
    "end": "1564278"
  },
  {
    "start": "1564000",
    "end": "1564000"
  },
  {
    "text": "on to the next test for HBC and AI applications we usually Implement",
    "start": "1565520",
    "end": "1571159"
  },
  {
    "text": "parallelism using a library called the message passing interface or MPI this",
    "start": "1571159",
    "end": "1576260"
  },
  {
    "text": "defines how parallel processes can work together by sharing a data set and communicating efficiently with one",
    "start": "1576260",
    "end": "1581539"
  },
  {
    "text": "another MPI embodies a very different Paradigm we don't really see so much of this in",
    "start": "1581539",
    "end": "1587000"
  },
  {
    "text": "cloud computing and it's vitally important for HBC and also large-scale AI",
    "start": "1587000",
    "end": "1593919"
  },
  {
    "text": "mpi's design also fits naturally with the RDMA Network protocols that Ares was",
    "start": "1593919",
    "end": "1599299"
  },
  {
    "text": "describing earlier the network performance of MPI can be measured using a suite of benchmarks a",
    "start": "1599299",
    "end": "1605480"
  },
  {
    "text": "standard one of which is called IMB or the Intel MPI benchmarks one of the simplest of those benchmarks",
    "start": "1605480",
    "end": "1612500"
  },
  {
    "text": "is a pairwise ping pong in which messages of different size are bounced between processes on two servers",
    "start": "1612500",
    "end": "1619820"
  },
  {
    "text": "in this Benchmark we are most interested in the latency for transmitting short messages and the bandwidth for",
    "start": "1619820",
    "end": "1626240"
  },
  {
    "text": "transferring large ones here we can see the performance of our",
    "start": "1626240",
    "end": "1632240"
  },
  {
    "text": "range of cnis for the MPI ping pong benchmark the left chart shows a short message",
    "start": "1632240",
    "end": "1638720"
  },
  {
    "text": "latency lower is better the lowest latency is Legacy sriv at about 1.83",
    "start": "1638720",
    "end": "1645940"
  },
  {
    "text": "microseconds message latency to get a lower latency than this you would need to go to an advanced HPC",
    "start": "1645940",
    "end": "1652220"
  },
  {
    "text": "Network fabric like infiniband accelerated ovn delivers a message",
    "start": "1652220",
    "end": "1657260"
  },
  {
    "text": "latency we measured at about 4.4 microseconds for bare metal and 4.8",
    "start": "1657260",
    "end": "1662539"
  },
  {
    "text": "microseconds for virtualized openstack configurations quantifying the overhead",
    "start": "1662539",
    "end": "1667580"
  },
  {
    "text": "of virtualization as a small fraction of a microsecond in latency the right chart shows large message",
    "start": "1667580",
    "end": "1674779"
  },
  {
    "text": "bandwidth higher message higher is better the highest performance is accelerated ovn in bare metal and openstack",
    "start": "1674779",
    "end": "1681860"
  },
  {
    "text": "configurations accelerated ovn on bare metal reaches over 180 gigabits a second bandwidth for",
    "start": "1681860",
    "end": "1688700"
  },
  {
    "text": "transmitting a 32 megabyte message in both latency and bandwidth we can see",
    "start": "1688700",
    "end": "1695900"
  },
  {
    "text": "that Hardware offloaded configurations perform far better we can also see clear advantages of RDMA",
    "start": "1695900",
    "end": "1703100"
  },
  {
    "text": "over TCP both for lower latency and higher bandwidth on these charts results for RDMA",
    "start": "1703100",
    "end": "1710240"
  },
  {
    "text": "Protocols are marked with pluses and data points for TCP results are marked",
    "start": "1710240",
    "end": "1715460"
  },
  {
    "text": "with crosses the difference is clear",
    "start": "1715460",
    "end": "1720220"
  },
  {
    "start": "1722000",
    "end": "1722000"
  },
  {
    "text": "so the results for synthetic benchmarks show a clear difference but what about real world application codes",
    "start": "1723740",
    "end": "1730940"
  },
  {
    "text": "computational fluid dynamics the simulation of air or water flows for example is a domain that makes extensive",
    "start": "1730940",
    "end": "1738140"
  },
  {
    "text": "use of parallel programming open foam is the leading open source",
    "start": "1738140",
    "end": "1743659"
  },
  {
    "text": "code for this we have a benchmark here where we model the vortices created when a fluid blows",
    "start": "1743659",
    "end": "1750080"
  },
  {
    "text": "over the top of a square box as shown in this animation to simulate this scenario we will use 48",
    "start": "1750080",
    "end": "1757400"
  },
  {
    "text": "parallel processes that work together using MPI",
    "start": "1757400",
    "end": "1763000"
  },
  {
    "text": "we also wanted to explore the effect of pod organization what if we put all the processes in our",
    "start": "1765679",
    "end": "1772159"
  },
  {
    "text": "parallel workload into one pot versus what do we use more pods with fewer",
    "start": "1772159",
    "end": "1777320"
  },
  {
    "text": "processes in each within a pod parallel processes will communicate by using shared memory",
    "start": "1777320",
    "end": "1783919"
  },
  {
    "text": "between pods they should use the local networking but within the host between hosts they will use the ethernet",
    "start": "1783919",
    "end": "1790340"
  },
  {
    "text": "networking and the cni we set up a few test cases to explore",
    "start": "1790340",
    "end": "1795620"
  },
  {
    "text": "the effect of this so how did we get on",
    "start": "1795620",
    "end": "1801200"
  },
  {
    "text": "in this chart we are plotting runtime of our fluid dynamics model for different pod to process geometries",
    "start": "1801200",
    "end": "1808220"
  },
  {
    "text": "in all cases we used 48 parallel processors a low lower run time on this graph means",
    "start": "1808220",
    "end": "1815120"
  },
  {
    "text": "better performance disappointingly there's no obvious effect to the different pod",
    "start": "1815120",
    "end": "1820580"
  },
  {
    "text": "configurations our lines are pretty flat perhaps MPI was smarter than we thought",
    "start": "1820580",
    "end": "1825980"
  },
  {
    "text": "and was using the same shared memory transport both within pods and between pods on the same node",
    "start": "1825980",
    "end": "1833539"
  },
  {
    "text": "unfortunately we did not have time to return to this setup and investigate this further",
    "start": "1833539",
    "end": "1840559"
  },
  {
    "text": "what is clear depending on the cni and protocol used our simulation took",
    "start": "1840559",
    "end": "1845720"
  },
  {
    "text": "between 125 seconds and 407 seconds to complete",
    "start": "1845720",
    "end": "1851000"
  },
  {
    "text": "Network performance is clearly a critical factor for parallel simulations such as this",
    "start": "1851000",
    "end": "1856940"
  },
  {
    "text": "highest performance was with accelerated ovn and Legacy sriov",
    "start": "1856940",
    "end": "1862220"
  },
  {
    "text": "accelerated ovn yielded about 125 seconds of runtime",
    "start": "1862220",
    "end": "1867860"
  },
  {
    "text": "once again RDMA protocol data points are marked with pluses and TCP data points",
    "start": "1867860",
    "end": "1873440"
  },
  {
    "text": "are marked with crosses one interesting thing we can see the",
    "start": "1873440",
    "end": "1878600"
  },
  {
    "text": "hardware accelerated cnis that supported RDMA protocol have a clear advantage over everything else but when the same",
    "start": "1878600",
    "end": "1886700"
  },
  {
    "text": "cnis run with TCP instead of RDMA the application performance drops from 125",
    "start": "1886700",
    "end": "1893120"
  },
  {
    "text": "seconds runtime to 244 seconds that is almost the same runtime as",
    "start": "1893120",
    "end": "1898700"
  },
  {
    "text": "achieved with Calico the implication here is that the full potential of the harder accelerated",
    "start": "1898700",
    "end": "1905659"
  },
  {
    "text": "offloads is only fully realized when using RDMA protocols",
    "start": "1905659",
    "end": "1911600"
  },
  {
    "text": "once again the non-offloaded software only ovn is having a hard time in this",
    "start": "1911600",
    "end": "1917120"
  },
  {
    "text": "configuration",
    "start": "1917120",
    "end": "1919658"
  },
  {
    "start": "1921000",
    "end": "1921000"
  },
  {
    "text": "a very different but equally intensive workload is genome sequencing something",
    "start": "1922700",
    "end": "1928580"
  },
  {
    "text": "that played a vital role of course in the world's response to the covet pandemic",
    "start": "1928580",
    "end": "1933740"
  },
  {
    "text": "we wanted to include a test case for base calling the first stage in the pipeline for processing data for the DNA",
    "start": "1933740",
    "end": "1940640"
  },
  {
    "text": "DNA sequence of technology developed by companies such as Oxford nanopore base calling uses neural networks to",
    "start": "1940640",
    "end": "1948200"
  },
  {
    "text": "extract base sequences from the noises signal data from the sequence of device",
    "start": "1948200",
    "end": "1953299"
  },
  {
    "text": "this application harnesses the machine learning power of a GPU about a terabyte of signal data is",
    "start": "1953299",
    "end": "1960320"
  },
  {
    "text": "processed for each genome sequenced using open source code from Oxford",
    "start": "1960320",
    "end": "1966799"
  },
  {
    "text": "nanoport we created a kubernetes deployment where the base calling process is presented as a service",
    "start": "1966799",
    "end": "1973340"
  },
  {
    "text": "we wanted to see what effect this would have on sequencing performance",
    "start": "1973340",
    "end": "1978620"
  },
  {
    "text": "we measured sequencing performance for host networks and also for cnis",
    "start": "1978620",
    "end": "1985360"
  },
  {
    "text": "unfortunately for lack of time and due to various integration issues we only managed to run the base calling",
    "start": "1985820",
    "end": "1992179"
  },
  {
    "text": "Benchmark on host networking and the Calico cni however what we can see from this single",
    "start": "1992179",
    "end": "1998480"
  },
  {
    "text": "comparison is encouraging there was no obvious impact to Performance for using kubernetes service networking for base",
    "start": "1998480",
    "end": "2005500"
  },
  {
    "text": "calling indeed the base cooling rate was actually about one percent higher when",
    "start": "2005500",
    "end": "2010539"
  },
  {
    "text": "put behind a service IP in Calico which is hard to explain but within the",
    "start": "2010539",
    "end": "2015880"
  },
  {
    "text": "margin of variability of The Benchmark performance",
    "start": "2015880",
    "end": "2020760"
  },
  {
    "text": "in summary we found that kubernetes platforms perform well for HBC and AI",
    "start": "2022360",
    "end": "2027820"
  },
  {
    "text": "application workloads at the small to moderate scale of our test lab kubernetes would make a convenient job",
    "start": "2027820",
    "end": "2034480"
  },
  {
    "text": "execution framework when coupled with interactive software platforms like Jupiter and this doesn't",
    "start": "2034480",
    "end": "2040960"
  },
  {
    "text": "have to be a compromise of convenience for infrastructure without sdn acceleration our experiments showed",
    "start": "2040960",
    "end": "2047440"
  },
  {
    "text": "Calico performed particularly well although it is hard to separate out the uplift of all the different offloads and",
    "start": "2047440",
    "end": "2053618"
  },
  {
    "text": "accelerations a modern high-end network card will perform further work is definitely needed to",
    "start": "2053619",
    "end": "2060040"
  },
  {
    "text": "understand some of the issues we encountered with performance along the way",
    "start": "2060040",
    "end": "2065200"
  },
  {
    "text": "for the most demanding use cases sdn acceleration offloads enable the highest",
    "start": "2065200",
    "end": "2070358"
  },
  {
    "text": "performance supporting the use of RDMA protocols which our results showed make a significant difference",
    "start": "2070359",
    "end": "2078158"
  },
  {
    "text": "our results also showed that with Hardware accelerated networking performance of kubernetes both on bare",
    "start": "2078159",
    "end": "2084580"
  },
  {
    "text": "metal hosts and in openstack VMS can be almost indistinguishable from performance on bare metal",
    "start": "2084580",
    "end": "2091300"
  },
  {
    "text": "with the right networking in place we can harness the advantages of kubernetes either on bare metal or nvms without",
    "start": "2091300",
    "end": "2098740"
  },
  {
    "text": "sacrificing performance sdn offloads are now at a coming-of-age",
    "start": "2098740",
    "end": "2105820"
  },
  {
    "text": "moment where the technology remains complex but delivers ultimate performance for cloud native",
    "start": "2105820",
    "end": "2112000"
  },
  {
    "text": "environments finally I would like to say a huge",
    "start": "2112000",
    "end": "2118540"
  },
  {
    "text": "thanks on behalf of eres and myself to the Nvidia and stack HBC engineering teams who performed This research and",
    "start": "2118540",
    "end": "2125740"
  },
  {
    "text": "made this presentation possible these people are heroes we hope you've enjoyed it and found it",
    "start": "2125740",
    "end": "2132099"
  },
  {
    "text": "useful please do feel free to scan the QR code here and leave us some feedback",
    "start": "2132099",
    "end": "2137380"
  },
  {
    "text": "thanks very much",
    "start": "2137380",
    "end": "2140400"
  }
]