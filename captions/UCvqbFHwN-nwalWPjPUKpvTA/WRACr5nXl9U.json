[
  {
    "text": "okay hello everyone welcome everyone thank you for coming we are excited to be here at",
    "start": "120",
    "end": "7160"
  },
  {
    "text": "kilon this is our first time as a speaker in Paris",
    "start": "7160",
    "end": "12880"
  },
  {
    "text": "so and lastly thank you cncf for hosting shout out to all the staff and",
    "start": "12880",
    "end": "19520"
  },
  {
    "text": "volunteers here and let's just start so today we are going to talk",
    "start": "19520",
    "end": "26720"
  },
  {
    "text": "about the architecting resilience and what we learn from managing over s 7,000 kubernetes",
    "start": "26720",
    "end": "34440"
  },
  {
    "text": "clusters at scale um uh I am Quang Choy people call",
    "start": "34440",
    "end": "43280"
  },
  {
    "text": "me honi and beside me is",
    "start": "43280",
    "end": "47800"
  },
  {
    "text": "c yeah okay um we are Cloud engineer at",
    "start": "49000",
    "end": "55520"
  },
  {
    "text": "Kaka we developed and maintain a private kubernetes clusters",
    "start": "55520",
    "end": "60640"
  },
  {
    "text": "as a service which runs on open stack based",
    "start": "60640",
    "end": "66080"
  },
  {
    "text": "infrastructure yeah hi I'm C I'm a kubernetes as a service team leader in",
    "start": "66400",
    "end": "72159"
  },
  {
    "text": "Kaka uh thank you for attending our session we hope our experience uh can help you thank",
    "start": "72159",
    "end": "79680"
  },
  {
    "text": "you so before we start uh let me introduce our team we Pro provide",
    "start": "83600",
    "end": "90560"
  },
  {
    "text": "kubernetes as a service in the company which is called dkos and dkos stands for data center of",
    "start": "90560",
    "end": "98600"
  },
  {
    "text": "kaka operating system and as a member of the private",
    "start": "98600",
    "end": "103720"
  },
  {
    "text": "kubernetes service team of kaka we manage over uh more than 7,000 clusters",
    "start": "103720",
    "end": "110000"
  },
  {
    "text": "which consists of more than 120,000 nodes and the Clusters are deployed in",
    "start": "110000",
    "end": "117399"
  },
  {
    "text": "different zones at Kaka we consider a single data center as a single",
    "start": "117399",
    "end": "124000"
  },
  {
    "text": "zone and lastly we get a lot of one calls every week uh we hope developers",
    "start": "124000",
    "end": "130560"
  },
  {
    "text": "get kubernetes up and running properly by the way we only have seven",
    "start": "130560",
    "end": "136360"
  },
  {
    "text": "people on the team so every week is very",
    "start": "136360",
    "end": "141840"
  },
  {
    "text": "tough so this is the sad story uh back in in October",
    "start": "144760",
    "end": "151400"
  },
  {
    "text": "2022 there was a data center fire and we had a significant economic and social",
    "start": "151400",
    "end": "160120"
  },
  {
    "text": "impact because of the variety variety of services such as messenger Maps comics",
    "start": "160120",
    "end": "168680"
  },
  {
    "text": "online shopping Banking and taxes the Koreans use every day the",
    "start": "168680",
    "end": "174599"
  },
  {
    "text": "impact was significant since all of the services",
    "start": "174599",
    "end": "180159"
  },
  {
    "text": "run on kaka's multiple kubernetes clusters the failure in a data center",
    "start": "180159",
    "end": "186720"
  },
  {
    "text": "affected these multiple",
    "start": "186720",
    "end": "190200"
  },
  {
    "text": "services so here's what really happened that day when the data center suddenly",
    "start": "192920",
    "end": "198920"
  },
  {
    "text": "lost power all of our kubernetes notes in the data center shut down developers",
    "start": "198920",
    "end": "205480"
  },
  {
    "text": "needed to migrate their kubernetes to a new Zone to restore",
    "start": "205480",
    "end": "210640"
  },
  {
    "text": "Services uh however the developers didn't have time to migrate their",
    "start": "210640",
    "end": "215680"
  },
  {
    "text": "clusters to new zones because it happens so suddenly",
    "start": "215680",
    "end": "221879"
  },
  {
    "text": "and even if there was enough time it take some time to set up new",
    "start": "221879",
    "end": "228519"
  },
  {
    "text": "clusters and deploy new applications set up load balancers Security checks and",
    "start": "228519",
    "end": "235760"
  },
  {
    "text": "proxies and so on so there are a lot of things to take care",
    "start": "235760",
    "end": "242680"
  },
  {
    "text": "of and in addition as the kubernetes notes they were down in the data center",
    "start": "245159",
    "end": "251640"
  },
  {
    "text": "came back up it was difficult to uh determine the impact of bringing the",
    "start": "251640",
    "end": "257560"
  },
  {
    "text": "service back up because we uh distributed the kubernetes nodes across different",
    "start": "257560",
    "end": "266080"
  },
  {
    "text": "hypervisors when the hypervisor powered up the kubernetes VM nodes powered up",
    "start": "266080",
    "end": "274000"
  },
  {
    "text": "randomly and since nodes come up randomly it was hard to predict when",
    "start": "274000",
    "end": "279479"
  },
  {
    "text": "cluster manipulation would be available so some data plane nodes have",
    "start": "279479",
    "end": "285120"
  },
  {
    "text": "started booting up before Contra plane notes and so it start processing service",
    "start": "285120",
    "end": "292360"
  },
  {
    "text": "request before developers could manage application through Contra",
    "start": "292360",
    "end": "298000"
  },
  {
    "text": "plane since as you all know control plane nodes must be available before",
    "start": "298000",
    "end": "303639"
  },
  {
    "text": "adjusting application replicas or Ingress rules this caused confusion for",
    "start": "303639",
    "end": "311960"
  },
  {
    "text": "developers uh to think about why this happened let's first explain the structure of",
    "start": "315479",
    "end": "321360"
  },
  {
    "text": "dkos dkos creates kubernetes in a single Zone by",
    "start": "321360",
    "end": "327160"
  },
  {
    "text": "default because of this most developed didn't think about running kubernetes",
    "start": "327160",
    "end": "332759"
  },
  {
    "text": "clusters in multiple zones for their services and as a result when data",
    "start": "332759",
    "end": "339560"
  },
  {
    "text": "center power power went down Services they were running in or connected to the veil Zone were",
    "start": "339560",
    "end": "347240"
  },
  {
    "text": "affected uh for developers who are unfamiliar with kubernetes it's hard to",
    "start": "349560",
    "end": "355400"
  },
  {
    "text": "think about running kubernetes clusters in multiple zones and most of developer",
    "start": "355400",
    "end": "361479"
  },
  {
    "text": "developers are busy enough uh developing their own services so of course even with the",
    "start": "361479",
    "end": "369479"
  },
  {
    "text": "single zone structure if applications are spread across multiple clusters",
    "start": "369479",
    "end": "375880"
  },
  {
    "text": "which are located in different zones with each load balancer connected to a",
    "start": "375880",
    "end": "381560"
  },
  {
    "text": "gslb it will be more resilient to failures in fact there are many",
    "start": "381560",
    "end": "387319"
  },
  {
    "text": "developers who build clusters with this structure but that means more clusters",
    "start": "387319",
    "end": "394840"
  },
  {
    "text": "to devel uh for developers to manage and more clusters to Monitor and more human",
    "start": "394840",
    "end": "401280"
  },
  {
    "text": "resources to deploy and manage the apps they serving this is why many service teams",
    "start": "401280",
    "end": "407919"
  },
  {
    "text": "have their own separate Cloud Engineers so we will explain in a bit",
    "start": "407919",
    "end": "413280"
  },
  {
    "text": "more detail with examples uh what went wrong in the following slides good tell",
    "start": "413280",
    "end": "418599"
  },
  {
    "text": "take here thank you um let's discuss all the structure",
    "start": "418599",
    "end": "424360"
  },
  {
    "text": "in detail each cluster should use only a single zone for the control plane and",
    "start": "424360",
    "end": "430680"
  },
  {
    "text": "data plane they are in the same Zone in a Zone they are nose and they are",
    "start": "430680",
    "end": "436639"
  },
  {
    "text": "scheduled to run in different hypervisors to explain with just a",
    "start": "436639",
    "end": "442080"
  },
  {
    "text": "single cluster we can take for tolerance for hypervisor or wreck level failures",
    "start": "442080",
    "end": "449840"
  },
  {
    "text": "therefore we had to use multicluster as shown in the figure uh this is because",
    "start": "449840",
    "end": "456240"
  },
  {
    "text": "we need to overcome data center failure where Jone goes",
    "start": "456240",
    "end": "461599"
  },
  {
    "text": "down as you can see in the slide uh there are three crossers so if two Jones",
    "start": "461599",
    "end": "468039"
  },
  {
    "text": "go down at the same time the service will not experience any",
    "start": "468039",
    "end": "474039"
  },
  {
    "text": "problem why single Jone now we have three or more Jones",
    "start": "474039",
    "end": "480039"
  },
  {
    "text": "but 9 years ago in the early days our private crowd started very small uh with",
    "start": "480039",
    "end": "486560"
  },
  {
    "text": "just one Jone after that we have scale out the Jones in scaling out we adopted",
    "start": "486560",
    "end": "493960"
  },
  {
    "text": "the way of Jone duplication because to scale out",
    "start": "493960",
    "end": "499240"
  },
  {
    "text": "quickly as a result as like the slide we have a road balancer and kubernetes",
    "start": "499240",
    "end": "505919"
  },
  {
    "text": "cruster pon",
    "start": "505919",
    "end": "511240"
  },
  {
    "text": "there might be the assumption that there is a failure or mistaken work in this",
    "start": "511240",
    "end": "517399"
  },
  {
    "text": "structure however there is absolutely no issue in this",
    "start": "517399",
    "end": "522680"
  },
  {
    "text": "structure even if Jone a is on available due to your data center failure gslb",
    "start": "522680",
    "end": "529760"
  },
  {
    "text": "will detect Jon A's failure it it can take time equivalent",
    "start": "529760",
    "end": "535160"
  },
  {
    "text": "to the heal interval however gslb immediately exclude J A from the",
    "start": "535160",
    "end": "543120"
  },
  {
    "text": "service for these Services they can still placd operate through kuis cluster",
    "start": "543120",
    "end": "550399"
  },
  {
    "text": "located in J B and C however we could learn the lessons",
    "start": "550399",
    "end": "556640"
  },
  {
    "text": "from data center of fire in theoretical and structural manner uh this",
    "start": "556640",
    "end": "563120"
  },
  {
    "text": "architecture is robust still there can be unexpected issue in the real world",
    "start": "563120",
    "end": "571360"
  },
  {
    "text": "from now I would like to share with you what were the issues before first there",
    "start": "573000",
    "end": "579320"
  },
  {
    "text": "is an omission of mod redundancy our cluster suppers diverse",
    "start": "579320",
    "end": "585120"
  },
  {
    "text": "walk clth ranging from thousands to 100 for example developers promptly execute",
    "start": "585120",
    "end": "592680"
  },
  {
    "text": "a proof of concept of new feature in JN a after all I call proof of concept as",
    "start": "592680",
    "end": "599360"
  },
  {
    "text": "PC so when you look at the left side of the slide you can see red color icon",
    "start": "599360",
    "end": "606720"
  },
  {
    "text": "those are PC here the issue is that develop developers may forget to deploy",
    "start": "606720",
    "end": "613839"
  },
  {
    "text": "in j p because POC module is prompt and immediate execution or RAR working",
    "start": "613839",
    "end": "622160"
  },
  {
    "text": "set we have two cruster in the slide but there is a high possibility of omission",
    "start": "622160",
    "end": "629360"
  },
  {
    "text": "when the increase in the number of cluster tools like Aro CD can help to",
    "start": "629360",
    "end": "636920"
  },
  {
    "text": "reduce Omission but to the end multi cluster will lead to the omission of",
    "start": "636920",
    "end": "642720"
  },
  {
    "text": "modu redundancy that's because we need to take care of more Resource",
    "start": "642720",
    "end": "650480"
  },
  {
    "text": "Management second conflicting jobs when you run btic crusters you have",
    "start": "651000",
    "end": "657560"
  },
  {
    "text": "jobs that shouldn't be learning in parallel on each cluster one example is",
    "start": "657560",
    "end": "663600"
  },
  {
    "text": "a job that performs post processing or loaded data in the figure this job is handled",
    "start": "663600",
    "end": "672040"
  },
  {
    "text": "by a cruster in Jun a uh the Green Card icon typically for",
    "start": "672040",
    "end": "679000"
  },
  {
    "text": "this postprocessing job we can manage the scheduling by our own therefore",
    "start": "679000",
    "end": "685600"
  },
  {
    "text": "importance is considered but concurrency is not considered Ed in addition this",
    "start": "685600",
    "end": "692200"
  },
  {
    "text": "job is deployed in only one single",
    "start": "692200",
    "end": "696600"
  },
  {
    "text": "cluster the operation of job within a single cluster has no issue in general",
    "start": "698279",
    "end": "704720"
  },
  {
    "text": "cases however if there is down in Jone a it will be the",
    "start": "704720",
    "end": "710560"
  },
  {
    "text": "issue the the developer detected failures and migrate the job from Jone a",
    "start": "710560",
    "end": "716480"
  },
  {
    "text": "to Jone B cluster the deployment to J may take time however it's only op y therefore",
    "start": "716480",
    "end": "726040"
  },
  {
    "text": "the delay is not significant the main issues occur when",
    "start": "726040",
    "end": "731079"
  },
  {
    "text": "Jun is back up that is say when it is recovered and operated",
    "start": "731079",
    "end": "738600"
  },
  {
    "text": "again after the cruster in Jun is recovered their will jobs which should",
    "start": "739880",
    "end": "746240"
  },
  {
    "text": "not running this operation new jobs will run in the Crosser",
    "start": "746240",
    "end": "752040"
  },
  {
    "text": "concurrently in this situation job can be stopped by Cub steer",
    "start": "752040",
    "end": "757079"
  },
  {
    "text": "command however it can't be automatically stopped when you can command Cub steel",
    "start": "757079",
    "end": "764800"
  },
  {
    "text": "it means kubernetes API server is recovered after its recovery kubernetes",
    "start": "764800",
    "end": "771320"
  },
  {
    "text": "shop control row will reconcile jobs at this time for a moment there can",
    "start": "771320",
    "end": "778000"
  },
  {
    "text": "be the conflict in in our jobs afterward it will data",
    "start": "778000",
    "end": "784160"
  },
  {
    "text": "conflict likewise we had experience that multicluster structure caus the omission",
    "start": "784160",
    "end": "791160"
  },
  {
    "text": "of modal redundancy and conflicting",
    "start": "791160",
    "end": "795800"
  },
  {
    "text": "jobs the third case is the omission of gsv health check",
    "start": "796680",
    "end": "803320"
  },
  {
    "text": "configuration even if even if you are not under those two prior cases uh",
    "start": "803320",
    "end": "808720"
  },
  {
    "text": "develop must configure gslb he checks in appropriate",
    "start": "808720",
    "end": "814600"
  },
  {
    "text": "manner in this slide we have an active active gslb and we are monitoring",
    "start": "814600",
    "end": "821839"
  },
  {
    "text": "service a and service B and the services consist of subd",
    "start": "821839",
    "end": "829839"
  },
  {
    "text": "directories here we have added a workload for a new service C the green",
    "start": "831560",
    "end": "836959"
  },
  {
    "text": "cires are the workload that added of course the Ingress and service",
    "start": "836959",
    "end": "843120"
  },
  {
    "text": "configuration would have been deployed but we only added the deployment to help",
    "start": "843120",
    "end": "849160"
  },
  {
    "text": "you understand this figure now the services C health check",
    "start": "849160",
    "end": "855519"
  },
  {
    "text": "for this workload is supposed to be configured in the gslb but developers may forget to",
    "start": "855519",
    "end": "863560"
  },
  {
    "text": "configure gslb this is because service C is a sub",
    "start": "863560",
    "end": "869120"
  },
  {
    "text": "directory of example.com therefore DNS queries will work fine",
    "start": "869120",
    "end": "875839"
  },
  {
    "text": "without configuration of the he check in addition if developers cover",
    "start": "875839",
    "end": "881839"
  },
  {
    "text": "more service they might miss more",
    "start": "881839",
    "end": "886240"
  },
  {
    "text": "configuration as you mentioned earlier when Jon a goes down and recovers nose",
    "start": "888240",
    "end": "894199"
  },
  {
    "text": "will randomly recover similar thing happens when the data Center goes",
    "start": "894199",
    "end": "900560"
  },
  {
    "text": "down as same as John a case those randomly go down that's because the",
    "start": "900560",
    "end": "907880"
  },
  {
    "text": "power goes down on a floor by floor or section by section therefore some node with service",
    "start": "907880",
    "end": "916440"
  },
  {
    "text": "a and service B are up but only node with service C are",
    "start": "916440",
    "end": "923240"
  },
  {
    "text": "down especially this possibility will increase when it select certain node for",
    "start": "923240",
    "end": "930000"
  },
  {
    "text": "the part the GSB still sends 50% of each",
    "start": "930000",
    "end": "936560"
  },
  {
    "text": "request to J A because it's service a and service B are",
    "start": "936560",
    "end": "942920"
  },
  {
    "text": "healthy so in j a Subs will",
    "start": "942920",
    "end": "948040"
  },
  {
    "text": "fail we have discovered those three major issues and I turn them Rob to",
    "start": "948040",
    "end": "953880"
  },
  {
    "text": "Quang to discuss its solution",
    "start": "953880",
    "end": "960920"
  },
  {
    "text": "so for our new goal we had a simple single goal for the new design the goal",
    "start": "961000",
    "end": "967600"
  },
  {
    "text": "is to make it easy for developers to deploy multizone",
    "start": "967600",
    "end": "972720"
  },
  {
    "text": "clusters DK also requires users to manage the infrastructure of the contr",
    "start": "972720",
    "end": "978680"
  },
  {
    "text": "plane such as o update or monitoring so",
    "start": "978680",
    "end": "984440"
  },
  {
    "text": "configuring a single multis zone cluster solves the inconvenience of managing and",
    "start": "984440",
    "end": "990040"
  },
  {
    "text": "monitoring the Clusters in use in addition since an application is",
    "start": "990040",
    "end": "997040"
  },
  {
    "text": "located on a single cluster it can be managed with a single deployment or",
    "start": "997040",
    "end": "1002920"
  },
  {
    "text": "state R set enabling automatic failover even if a Zone goes",
    "start": "1002920",
    "end": "1010600"
  },
  {
    "text": "down now this is the recent image of our dkos web it shows a single cluster",
    "start": "1011399",
    "end": "1018120"
  },
  {
    "text": "deployed in multis zone you can see that there are five control plane nodes each",
    "start": "1018120",
    "end": "1023759"
  },
  {
    "text": "evenly distributed across zones a b and c as mentioned before you can see that",
    "start": "1023759",
    "end": "1031120"
  },
  {
    "text": "the contol plane is exposed on the web because the user manages the control",
    "start": "1031120",
    "end": "1036280"
  },
  {
    "text": "plane directly so the final structure looks",
    "start": "1036280",
    "end": "1044120"
  },
  {
    "text": "like this in the new dek structure the cluster is still accessible in the event",
    "start": "1044120",
    "end": "1050280"
  },
  {
    "text": "of a failure and it is easy to identify dead nodes an application is deployed as a",
    "start": "1050280",
    "end": "1058360"
  },
  {
    "text": "single uh workload so if one zone fails new parts will pop up on any other Zone",
    "start": "1058360",
    "end": "1066880"
  },
  {
    "text": "nodes services that should not be restarted upon failover can be controlled in Advan with settings such",
    "start": "1066880",
    "end": "1074720"
  },
  {
    "text": "as node qun or drain uh we therefore expect the new DQ",
    "start": "1074720",
    "end": "1081080"
  },
  {
    "text": "structure to be more resilient in the event of a single zone failure and now developers can safely",
    "start": "1081080",
    "end": "1088240"
  },
  {
    "text": "use kubernetes without worrying about it and in the following sections we will",
    "start": "1088240",
    "end": "1094120"
  },
  {
    "text": "describe the parts we consider for the multis Zone impr",
    "start": "1094120",
    "end": "1099240"
  },
  {
    "text": "provisioning since the dqs cluster is a stacked highly available cluster",
    "start": "1101200",
    "end": "1106799"
  },
  {
    "text": "structure which means as nodes are collocated with control plane nodes sdd",
    "start": "1106799",
    "end": "1113000"
  },
  {
    "text": "needed to be verified the inner Zone Network throughput and performance was",
    "start": "1113000",
    "end": "1118559"
  },
  {
    "text": "met and also as the worker nodes use engine proxy to look at the Contra plane",
    "start": "1118559",
    "end": "1124520"
  },
  {
    "text": "nodes and worker nodes are deployed in multiple zones the cost of using cross",
    "start": "1124520",
    "end": "1130240"
  },
  {
    "text": "Zone traffic between workloads was also an important",
    "start": "1130240",
    "end": "1135880"
  },
  {
    "text": "consideration and finally we'll talk a little bit about the consideration for",
    "start": "1135880",
    "end": "1141360"
  },
  {
    "text": "the gslv controller we will be creating",
    "start": "1141360",
    "end": "1146000"
  },
  {
    "text": "later for SD first since the traffic between inner zones means netor cost we",
    "start": "1148400",
    "end": "1156039"
  },
  {
    "text": "looked at how much aity inner Zone traffic there is the table shows aity traffic between",
    "start": "1156039",
    "end": "1163360"
  },
  {
    "text": "zones this is the result of four randomly sampled clusters with different",
    "start": "1163360",
    "end": "1168840"
  },
  {
    "text": "City storage size we measure the traffic going in and",
    "start": "1168840",
    "end": "1174240"
  },
  {
    "text": "out of ports 2379 and 2380 on the ACD ler control",
    "start": "1174240",
    "end": "1180200"
  },
  {
    "text": "plane node as the size increases so does the network traffic on average however SCD",
    "start": "1180200",
    "end": "1188559"
  },
  {
    "text": "traffic usage was not excessive for multis zones also SD uses a 100",
    "start": "1188559",
    "end": "1195280"
  },
  {
    "text": "millisecond heart bit interal by default but again 100 millisecond interval was",
    "start": "1195280",
    "end": "1201080"
  },
  {
    "text": "not a problem in between kaka's zones however depending on the amount of",
    "start": "1201080",
    "end": "1208080"
  },
  {
    "text": "recast made to the API server the value may vary from cluster to Cluster but we",
    "start": "1208080",
    "end": "1214880"
  },
  {
    "text": "have found that using multis Zone shouldn't be a big problem and in The Following part Q will",
    "start": "1214880",
    "end": "1221559"
  },
  {
    "text": "explain more about Network traffic in Cross",
    "start": "1221559",
    "end": "1226240"
  },
  {
    "text": "Zone if in multi Jone there will be increase in cion network",
    "start": "1227480",
    "end": "1233080"
  },
  {
    "text": "traffic this figure shows the client pass sending packet to a kubernetes",
    "start": "1233080",
    "end": "1239240"
  },
  {
    "text": "service the service is handled by Q proxy thus Q proxy will Road balance the",
    "start": "1239240",
    "end": "1247159"
  },
  {
    "text": "existing part in the cluster if the selected end points are part which are located in different Zone",
    "start": "1247159",
    "end": "1255200"
  },
  {
    "text": "there will be cross Zone traffic of course you can isolate the select",
    "start": "1255200",
    "end": "1262120"
  },
  {
    "text": "application in the service or use topology array hint to avoid cross CH",
    "start": "1262120",
    "end": "1269159"
  },
  {
    "text": "traffic however in this case for developers who concern about increasing",
    "start": "1269159",
    "end": "1274880"
  },
  {
    "text": "resource management and who are not used to this structure they choose not to isolate",
    "start": "1274880",
    "end": "1281200"
  },
  {
    "text": "their end point what will be the most significant",
    "start": "1281200",
    "end": "1287760"
  },
  {
    "text": "issue in Cross traffic that is rency even if the rency of the Cross",
    "start": "1287760",
    "end": "1296120"
  },
  {
    "text": "traffic is just 1 millisecond the rency of the service can be more",
    "start": "1296120",
    "end": "1302120"
  },
  {
    "text": "delayed it's explained that the length of the chain can be longer in the",
    "start": "1302120",
    "end": "1307840"
  },
  {
    "text": "microservice architecture in the worst case let's assume that the length of the chain is",
    "start": "1307840",
    "end": "1315200"
  },
  {
    "text": "10 then it will be delayed by 10 Mill second furthermore it's even harder to",
    "start": "1315200",
    "end": "1323279"
  },
  {
    "text": "estimate it rency if you are calling other department apis within the",
    "start": "1323279",
    "end": "1330360"
  },
  {
    "text": "chain for the specific example like advertisement Services they",
    "start": "1330360",
    "end": "1336919"
  },
  {
    "text": "are particularly latency sensitive since the nature of",
    "start": "1336919",
    "end": "1342600"
  },
  {
    "text": "advertisement is to deliver messages it should be delivered quickly to each users",
    "start": "1342600",
    "end": "1349279"
  },
  {
    "text": "at least 1 millisecond how can we reduce the",
    "start": "1349279",
    "end": "1356120"
  },
  {
    "text": "latency to reduce the latency we may choose to revamp physical infrar rayer",
    "start": "1356120",
    "end": "1362440"
  },
  {
    "text": "and vatal layer in fact it is not the only",
    "start": "1362440",
    "end": "1368720"
  },
  {
    "text": "solution as you can see in this slide we can choose to revamp the software",
    "start": "1368720",
    "end": "1374279"
  },
  {
    "text": "performance in container Netto layer because we mainly manage Kum World",
    "start": "1374279",
    "end": "1383240"
  },
  {
    "text": "layer this is a container Network before the reorganization at the time we use",
    "start": "1384120",
    "end": "1391679"
  },
  {
    "text": "among the cni plugins and we use celum with ebpf and",
    "start": "1391679",
    "end": "1398679"
  },
  {
    "text": "ipvs this figures show how package is folded from norport to the",
    "start": "1398679",
    "end": "1404720"
  },
  {
    "text": "path it comes as is zero and it is translated by",
    "start": "1404720",
    "end": "1410720"
  },
  {
    "text": "ipvs then the address is changed to endpoint part address there is starting",
    "start": "1410720",
    "end": "1416640"
  },
  {
    "text": "with 192 192 as seen in the slide after this it is it is process to",
    "start": "1416640",
    "end": "1425120"
  },
  {
    "text": "routing decision next the result of routing decision is fored to celum host",
    "start": "1425120",
    "end": "1432840"
  },
  {
    "text": "interface then they are folded by Eep to another load or to the local path",
    "start": "1432840",
    "end": "1440000"
  },
  {
    "text": "in the case of another load forward there can be crossr",
    "start": "1440000",
    "end": "1445880"
  },
  {
    "text": "traffic to reduce the latency we have changed the structure as",
    "start": "1446360",
    "end": "1451960"
  },
  {
    "text": "follows as can be seen on the ride we removed the ipbs and made a replacement",
    "start": "1451960",
    "end": "1458120"
  },
  {
    "text": "with ebpf the ebpf code on E zero for the",
    "start": "1458120",
    "end": "1463799"
  },
  {
    "text": "package from the node port to the locer or other loads the simpl structure",
    "start": "1463799",
    "end": "1469799"
  },
  {
    "text": "reduce the number of CPU instruction for CU proxy fuel instruction means better",
    "start": "1469799",
    "end": "1477039"
  },
  {
    "text": "performance which means lower latency as a result now we have the",
    "start": "1477039",
    "end": "1483799"
  },
  {
    "text": "chance to improve the performance of both inone and crossone",
    "start": "1483799",
    "end": "1490120"
  },
  {
    "text": "traffic let's see how much the latency has",
    "start": "1490120",
    "end": "1495158"
  },
  {
    "text": "improved uh digital test environment you you can see those five we use two node",
    "start": "1495559",
    "end": "1501640"
  },
  {
    "text": "cluster with the general performance time in Kaka Cloud each node in a different",
    "start": "1501640",
    "end": "1508559"
  },
  {
    "text": "Zone and we measure the round tra latency of one bite of TCP using",
    "start": "1508559",
    "end": "1516520"
  },
  {
    "text": "Neer with full ebpf we reduce the Q proxy Laten latency by 0.25",
    "start": "1517880",
    "end": "1525360"
  },
  {
    "text": "millisecond to explain that is changed from 0.35 millisecond to 0.1",
    "start": "1525360",
    "end": "1532880"
  },
  {
    "text": "millisecond typically our cral raty is between 0.25 millisecond and 1",
    "start": "1532880",
    "end": "1539679"
  },
  {
    "text": "millisecond the difference occurs by the physical placement of the",
    "start": "1539679",
    "end": "1544840"
  },
  {
    "text": "Zone in the figure we've used the example of a crossone rency 0.5",
    "start": "1544840",
    "end": "1552240"
  },
  {
    "text": "millisecond in this case we reduce the latency by 0.25 millisecond",
    "start": "1552240",
    "end": "1558840"
  },
  {
    "text": "thus it can reduce the cross J latency by",
    "start": "1558840",
    "end": "1564279"
  },
  {
    "text": "50% and finally the gsrb custom resource I mentioned that when you miss",
    "start": "1565000",
    "end": "1572000"
  },
  {
    "text": "GSB house configuration it can be the issue so we're going to provide the",
    "start": "1572000",
    "end": "1579200"
  },
  {
    "text": "custom Resource as showing in the slide after that we'll develop the controller",
    "start": "1579200",
    "end": "1586039"
  },
  {
    "text": "so we can configure gslb easily in addition we remain mandatory has for",
    "start": "1586039",
    "end": "1595200"
  },
  {
    "text": "gslb this is a custom resource real example we've specified our Ro balancer",
    "start": "1596480",
    "end": "1603480"
  },
  {
    "text": "to connect to the J configure hashtag we allow developers to configure",
    "start": "1603480",
    "end": "1610200"
  },
  {
    "text": "J through the kuet apis and they are most familiar",
    "start": "1610200",
    "end": "1617240"
  },
  {
    "text": "with until now we' talked about the challenges and difficulty of single zone",
    "start": "1617240",
    "end": "1624720"
  },
  {
    "text": "multier and our new goals and our consideration thank you for listening",
    "start": "1624720",
    "end": "1630440"
  },
  {
    "text": "and let's open our question and answer thank",
    "start": "1630440",
    "end": "1634600"
  },
  {
    "text": "[Applause] you if you have any question there's a",
    "start": "1636810",
    "end": "1642960"
  },
  {
    "text": "microphone two microphones on the hallway so",
    "start": "1642960",
    "end": "1650159"
  },
  {
    "text": "okay so I would have a question okay yeah uh my question would be in this",
    "start": "1663279",
    "end": "1670279"
  },
  {
    "text": "multicluster setup if you have like quick failovers with applications and stuff uh how did or do you manage",
    "start": "1670279",
    "end": "1677480"
  },
  {
    "text": "stateful applications and and their storage if if applications are running in multiple clusters in",
    "start": "1677480",
    "end": "1683919"
  },
  {
    "text": "parallel uh so we Guide to the developers to use storage class each on",
    "start": "1683919",
    "end": "1690960"
  },
  {
    "text": "different zones so if you want to deploy apps in zone a to use Zone a storage",
    "start": "1690960",
    "end": "1698919"
  },
  {
    "text": "class okay but so the storage is not highly available for this stle applications so it's the application",
    "start": "1700080",
    "end": "1706640"
  },
  {
    "text": "stop to ensure that that they can fil okay in the case you have to use other type of uh volumes like S3 or other",
    "start": "1706640",
    "end": "1715279"
  },
  {
    "text": "types yes okay thanks thanks yeah hello yeah so yeah my first",
    "start": "1715279",
    "end": "1722880"
  },
  {
    "text": "question is actually the same as oh okay uh this guy but the second question is",
    "start": "1722880",
    "end": "1728279"
  },
  {
    "text": "how do you provision all of those clusters what uh Technologies do you use",
    "start": "1728279",
    "end": "1733919"
  },
  {
    "text": "underneath uh I'm sorry could you speak that again yes CL how do you provision",
    "start": "1733919",
    "end": "1740399"
  },
  {
    "text": "those 7,000 clusters what automation do you",
    "start": "1740399",
    "end": "1745960"
  },
  {
    "text": "use or it's a company secret that's also fine uh we it's not a secret we have a",
    "start": "1748320",
    "end": "1754720"
  },
  {
    "text": "block how we provision it we use uh for static data we have a base image and for",
    "start": "1754720",
    "end": "1763559"
  },
  {
    "text": "different Dynamic uh Dynamic type of info we use cloud in it",
    "start": "1763559",
    "end": "1770320"
  },
  {
    "text": "for VM notes so we inject those type of",
    "start": "1770320",
    "end": "1775600"
  },
  {
    "text": "uh infos into the cloud in it when it put up is that enough okay thank you",
    "start": "1775600",
    "end": "1781640"
  },
  {
    "text": "okay okay thank",
    "start": "1781640",
    "end": "1784240"
  },
  {
    "text": "you hi hi I really liked seeing the crd",
    "start": "1790960",
    "end": "1796039"
  },
  {
    "text": "for the gslb but I was wondering why are you not using annotations wouldn't that be easier for the developers instead of",
    "start": "1796039",
    "end": "1802039"
  },
  {
    "text": "learning a complete crd managing the life cycle of the crd just adding annotations to the",
    "start": "1802039",
    "end": "1807480"
  },
  {
    "text": "[Music]",
    "start": "1807480",
    "end": "1810650"
  },
  {
    "text": "Ingress we actually use as what you said but this is our uh",
    "start": "1814399",
    "end": "1821360"
  },
  {
    "text": "option for automatic usage so there there can be two types to use first is",
    "start": "1821360",
    "end": "1828559"
  },
  {
    "text": "as you said Ingress you can add an annotation of uh which zone you want to",
    "start": "1828559",
    "end": "1836320"
  },
  {
    "text": "use and we are trying to make it with the gslb",
    "start": "1836320",
    "end": "1842080"
  },
  {
    "text": "so I should say there are two options okay nice thank you thank",
    "start": "1842080",
    "end": "1848880"
  },
  {
    "text": "you uh hello I just want to ask about the gslb thing you said that g in the",
    "start": "1849159",
    "end": "1854679"
  },
  {
    "text": "gslb you can Define your services and the type of services Etc so from the infrastructure perspective where exactly",
    "start": "1854679",
    "end": "1861960"
  },
  {
    "text": "this gslb crds will be applied do we need a separate cluster for the gslb and",
    "start": "1861960",
    "end": "1868399"
  },
  {
    "text": "the another thing if we really need a separate cluster for the gslb what about the internal Services which are not",
    "start": "1868399",
    "end": "1875480"
  },
  {
    "text": "exposed to the internet or something so how exactly the dslb will connect to the",
    "start": "1875480",
    "end": "1880639"
  },
  {
    "text": "service which is internal of type load",
    "start": "1880639",
    "end": "1884440"
  },
  {
    "text": "balancer",
    "start": "1886919",
    "end": "1889919"
  },
  {
    "text": "one second no",
    "start": "1902799",
    "end": "1906120"
  },
  {
    "text": "problem so for for the first answer uh we do have a multizone gslv cluster okay",
    "start": "1911919",
    "end": "1921320"
  },
  {
    "text": "but we don't manage it we have a a separate gslv team and for the second",
    "start": "1921320",
    "end": "1926559"
  },
  {
    "text": "question Q will",
    "start": "1926559",
    "end": "1930000"
  },
  {
    "text": "answer yes yeah yeah we know that uh problem so uh so we are planning we",
    "start": "1933120",
    "end": "1940360"
  },
  {
    "text": "might get a road bancer yeah uh but by then way we may have a new region and",
    "start": "1940360",
    "end": "1946320"
  },
  {
    "text": "may use gsv for it okay okay so to add it up uh we only use",
    "start": "1946320",
    "end": "1955880"
  },
  {
    "text": "lb in single Jone so this is why we are trying to make a gslb so that we can",
    "start": "1955880",
    "end": "1961799"
  },
  {
    "text": "connect all the lbs but for private uh load balancers uhuh as you",
    "start": "1961799",
    "end": "1968480"
  },
  {
    "text": "can as you said it's hard to use gslb so we are trying to make a load",
    "start": "1968480",
    "end": "1974240"
  },
  {
    "text": "balancers have members in all different zones don't so users don't have to use",
    "start": "1974240",
    "end": "1980919"
  },
  {
    "text": "gsmv in the in the near future yeah okay okay that's thank you so much thank you",
    "start": "1980919",
    "end": "1986240"
  },
  {
    "text": "thank you so much he so if I understood correctly you",
    "start": "1986240",
    "end": "1991960"
  },
  {
    "text": "have one control plane per cluster and there is nodes for example ETD nodes on",
    "start": "1991960",
    "end": "1999200"
  },
  {
    "text": "every Zone when you introduced this change did you have to tune anything in ETD or QB",
    "start": "1999200",
    "end": "2008039"
  },
  {
    "text": "server any other configuration to account for the fact there now more latency between the",
    "start": "2008039",
    "end": "2013159"
  },
  {
    "text": "nodes is there any tweak that you've done as part of this migration uh uh we",
    "start": "2013159",
    "end": "2020960"
  },
  {
    "text": "looked at uh all the options as C have but we didn't uh change any type of",
    "start": "2020960",
    "end": "2028480"
  },
  {
    "text": "options it we worked so wow fantastic yeah fantastic thank",
    "start": "2028480",
    "end": "2035120"
  },
  {
    "text": "you hello so uh how totally avoid interzone",
    "start": "2035279",
    "end": "2041600"
  },
  {
    "text": "traffic some heavy traffic passage without creating any additional services",
    "start": "2041600",
    "end": "2047200"
  },
  {
    "text": "and with any additional",
    "start": "2047200",
    "end": "2051598"
  },
  {
    "text": "labels uh would you please say that again I couldn't get it yeah would you please say that again yes yes",
    "start": "2061359",
    "end": "2069358"
  },
  {
    "text": "so we have multiple zones and we don't want to inter Zone traffic passage like",
    "start": "2069359",
    "end": "2076398"
  },
  {
    "text": "from Zone a uh Ingress controller should not reach to the PS to the Zone",
    "start": "2076399",
    "end": "2083960"
  },
  {
    "text": "B uh so how avoid that case without creating Service uh with",
    "start": "2083960",
    "end": "2092638"
  },
  {
    "text": "different labels for each Zone to avoid like many in",
    "start": "2092639",
    "end": "2100319"
  },
  {
    "text": "overhead yeah uh we recommend uh we recommend using topus or hint then you",
    "start": "2101200",
    "end": "2108800"
  },
  {
    "text": "can separate the end point um by per Zone um so it is possible to avoid avoid",
    "start": "2108800",
    "end": "2117359"
  },
  {
    "text": "cross traffic uh with the to or H we can",
    "start": "2117359",
    "end": "2122760"
  },
  {
    "text": "avoid chain length of the microservice architecture and rate",
    "start": "2122760",
    "end": "2128960"
  },
  {
    "text": "see okay",
    "start": "2128960",
    "end": "2133040"
  },
  {
    "text": "thanks okay thank",
    "start": "2140320",
    "end": "2143720"
  },
  {
    "text": "you",
    "start": "2145520",
    "end": "2148520"
  }
]