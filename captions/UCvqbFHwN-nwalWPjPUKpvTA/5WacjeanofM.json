[
  {
    "start": "0",
    "end": "115000"
  },
  {
    "text": "well hello everyone thank you for swinging by today especially with the sort of like room shuffle the other half",
    "start": "30",
    "end": "6690"
  },
  {
    "text": "of the conference center apparently doesn't have power and we'll see what",
    "start": "6690",
    "end": "11790"
  },
  {
    "text": "happens anyway this is just a little bit of an overview on the CN CF research",
    "start": "11790",
    "end": "17820"
  },
  {
    "text": "user group and will actually sort of do a little bit of like live polling because we want to see where you as",
    "start": "17820",
    "end": "23369"
  },
  {
    "text": "researchers really want to see where the research user group goes and what features we want to drive oh I did",
    "start": "23369",
    "end": "36719"
  },
  {
    "text": "something dumb I need to also introduce the other people involved so the other co-chair here Steve Lynette it's right",
    "start": "36719",
    "end": "42420"
  },
  {
    "text": "there and our tech lead Klaus mah unfortunately could not be here today but he is pretty active within the",
    "start": "42420",
    "end": "48660"
  },
  {
    "text": "community as well so if you could you see that poll link down there please pop",
    "start": "48660",
    "end": "55440"
  },
  {
    "text": "it open let me activate this",
    "start": "55440",
    "end": "62570"
  },
  {
    "text": "yep",
    "start": "67280",
    "end": "69880"
  },
  {
    "text": "we actually won't spend too much time on this I'm going to go through but we will take a look at this later and the link",
    "start": "72649",
    "end": "79039"
  },
  {
    "text": "at the top hole Evie comm / Bob a1 will",
    "start": "79039",
    "end": "84380"
  },
  {
    "text": "be on every slide",
    "start": "84380",
    "end": "87310"
  },
  {
    "text": "okay is active so why why are we actually",
    "start": "92150",
    "end": "104960"
  },
  {
    "text": "here as research users why do we actually need a user group to be honest",
    "start": "104960",
    "end": "111700"
  },
  {
    "text": "research needs are changing I with this",
    "start": "111700",
    "end": "116810"
  },
  {
    "text": "crowd I probably don't need to spend too much time on these points by kinda want to least touch base on it anyway",
    "start": "116810",
    "end": "123020"
  },
  {
    "text": "we are seeing an increased use of containers well everywhere we're seeing more complex workflows and not just for",
    "start": "123020",
    "end": "130429"
  },
  {
    "text": "the sort of research process itself but with more enterprises and all sorts of groups sort of getting involved with you",
    "start": "130430",
    "end": "137540"
  },
  {
    "text": "know more machine learning things like model lifecycle are becoming more important and sort of riding on that",
    "start": "137540",
    "end": "144440"
  },
  {
    "text": "we're also seeing more adoption of things like streaming data and eventing based on that data and it's something",
    "start": "144440",
    "end": "152300"
  },
  {
    "text": "that has been historically not really done that much in our field we're also seeing much greater use of interactive",
    "start": "152300",
    "end": "161209"
  },
  {
    "text": "science gateway so things like Jupiter hub these days like that is sort of becoming the de facto standard",
    "start": "161209",
    "end": "166520"
  },
  {
    "text": "especially in the academic setting where you know everyone's getting notebooks that's their first real experience with",
    "start": "166520",
    "end": "173480"
  },
  {
    "text": "playing with this stuff and backing those sort of you know science gateways",
    "start": "173480",
    "end": "180410"
  },
  {
    "text": "we see a much greater demand on sort of persistent services a persistent",
    "start": "180410",
    "end": "185870"
  },
  {
    "text": "services supporting those things historically you know you'd have",
    "start": "185870",
    "end": "191420"
  },
  {
    "text": "something like your login node in your workload manager and that was pretty",
    "start": "191420",
    "end": "196550"
  },
  {
    "text": "much it now everyone is spitting up a bunch of additional services to just completely support research and all of",
    "start": "196550",
    "end": "204050"
  },
  {
    "text": "these things have actually really lend themselves to kubernetes and sort of the cloud native ecosystem as it perform or",
    "start": "204050",
    "end": "211520"
  },
  {
    "text": "provides a sort of platform agnostic api that sort of gives us very good",
    "start": "211520",
    "end": "217190"
  },
  {
    "text": "primitives in the scheduling of these different workloads oh why do we",
    "start": "217190",
    "end": "225109"
  },
  {
    "text": "actually want a for our user group well like it or not we are a much smaller subset of the users when",
    "start": "225109",
    "end": "232890"
  },
  {
    "text": "compared to something like Enterprise especially where they're primarily focused on services and they're not",
    "start": "232890",
    "end": "238590"
  },
  {
    "text": "gonna be spinning up you know possibly thousands or hundreds of thousands of jobs the enterprise type services also",
    "start": "238590",
    "end": "247709"
  },
  {
    "text": "tend to like you know they don't tend to be as resource intensive and there's things like you know we don't",
    "start": "247709",
    "end": "253350"
  },
  {
    "text": "necessarily want in packing and beyond that we require like more advanced",
    "start": "253350",
    "end": "259709"
  },
  {
    "text": "scheduling to support things like MPI we want you know fair share backfill and whole slew of the stuff that's come from",
    "start": "259709",
    "end": "266430"
  },
  {
    "text": "sort of the classic HPC world that's not a saying like you know this stuff",
    "start": "266430",
    "end": "271950"
  },
  {
    "text": "doesn't have MPI support we do see a little bit of that in you know keep flow with the MPI operator cube batch in the",
    "start": "271950",
    "end": "278340"
  },
  {
    "text": "upstream project and volcano a sort of a more cloud native batch scheduler and",
    "start": "278340",
    "end": "284610"
  },
  {
    "text": "all of which are actually powered by the upstream cube batch and while all these",
    "start": "284610",
    "end": "290790"
  },
  {
    "text": "things haven't necessarily been forgotten by upstream they have very largely been underrepresented so the",
    "start": "290790",
    "end": "300900"
  },
  {
    "start": "300000",
    "end": "380000"
  },
  {
    "text": "mission of the CN CF research group I've been involved with like kubernetes the",
    "start": "300900",
    "end": "306210"
  },
  {
    "text": "scenes you haven't spoken with like many research groups over the past couple years many of them are sort of you know",
    "start": "306210",
    "end": "311669"
  },
  {
    "text": "grappling with the same sort of problems and developing sort of solutions and workarounds in silos and aren't",
    "start": "311669",
    "end": "317700"
  },
  {
    "text": "necessarily sharing this information with each other that well and there also hasn't been like a strong unifying voice",
    "start": "317700",
    "end": "325680"
  },
  {
    "text": "or group to help drive the research computing needs in the cloud native ecosystem in the upstream kubernetes",
    "start": "325680",
    "end": "331890"
  },
  {
    "text": "project there's been sort of some little spin-offs and things like the big data user group the machine learning working",
    "start": "331890",
    "end": "337740"
  },
  {
    "text": "group but for the most part they haven't driven a lot of upstream development so",
    "start": "337740",
    "end": "346820"
  },
  {
    "text": "at Cube Connie you earlier this year and Barcelona a bunch of us actually got",
    "start": "346820",
    "end": "352200"
  },
  {
    "text": "together and so like decided hey and we needed something like this we need a place where we you know researchers sort of coordinate their actions",
    "start": "352200",
    "end": "359419"
  },
  {
    "text": "bring that stuff upstream we weren't quite sure at the time so it was going to be a kubernetes working group CN CF",
    "start": "359419",
    "end": "366740"
  },
  {
    "text": "or you know working group who raised user group or CN CF thing and it turns",
    "start": "366740",
    "end": "373370"
  },
  {
    "text": "out that CN CF research user group was the one that won so we have you haven't",
    "start": "373370",
    "end": "381620"
  },
  {
    "start": "380000",
    "end": "462000"
  },
  {
    "text": "been out like working on this stuff for too long but we've already sort of identified a few common themes we sort",
    "start": "381620",
    "end": "388159"
  },
  {
    "text": "of hold everyone that originally got involved and the first one thing that really counts like we don't really know",
    "start": "388159",
    "end": "394520"
  },
  {
    "text": "who's all doing research on like ratings and research using this stuff and everyone sort of feels like they're you",
    "start": "394520",
    "end": "400909"
  },
  {
    "text": "know alone in the dark sort of you know humming their own thing seconding that",
    "start": "400909",
    "end": "406159"
  },
  {
    "text": "is that there isn't really a good set of best practices for operating large clusters or clusters where you're",
    "start": "406159",
    "end": "412400"
  },
  {
    "text": "churning through a bunch of jobs the sort of defaults of the Box aren't you know the greatest for that sort of thing",
    "start": "412400",
    "end": "419560"
  },
  {
    "text": "the other thing is like urban aids itself you know lacks sort of the more advanced scheduling primitives that you",
    "start": "419949",
    "end": "426319"
  },
  {
    "text": "know we would historically use namely being MPI and Co scheduling and while",
    "start": "426319",
    "end": "432020"
  },
  {
    "text": "there have been the other like big thing is we again mostly coming from classic",
    "start": "432020",
    "end": "437389"
  },
  {
    "text": "sites things like multi cluster and multi user or multi-tenancy are very",
    "start": "437389",
    "end": "443360"
  },
  {
    "text": "important to us and there hasn't been",
    "start": "443360",
    "end": "448430"
  },
  {
    "text": "like a focused effort on getting a lot of that stuff going upstream so what",
    "start": "448430",
    "end": "455240"
  },
  {
    "text": "these come themes we actually started just a couple you know small little missions again this is fairly new so we haven't you know been able to do too",
    "start": "455240",
    "end": "461629"
  },
  {
    "text": "much but we created a Research Institute survey so just sort of asking like you know who's doing what if you're if",
    "start": "461629",
    "end": "469430"
  },
  {
    "start": "462000",
    "end": "585000"
  },
  {
    "text": "you're interested in like you know sharing if you're running contains at a site and you want to talk a little bit about how you're doing it this helps",
    "start": "469430",
    "end": "475339"
  },
  {
    "text": "other research institutions sort of discover what they're doing and you know",
    "start": "475339",
    "end": "481069"
  },
  {
    "text": "they might ping you for a few questions if they're trying to do something similar what we really want to know like how are you deploying it how are they",
    "start": "481069",
    "end": "488089"
  },
  {
    "text": "managing it how are they handling like user manager and things like that and a similar effort was actually done I",
    "start": "488089",
    "end": "494020"
  },
  {
    "text": "believe in the OpenStack community talking about that stuff earlier with with a Steve and then eventually you",
    "start": "494020",
    "end": "502750"
  },
  {
    "text": "know we might get something more concise put together regarding that stuff we",
    "start": "502750",
    "end": "508030"
  },
  {
    "text": "also have a just it's like an awesome list of sort of helpful resources so",
    "start": "508030",
    "end": "513159"
  },
  {
    "text": "some best practices regarding like you know how do you run big clusters some of",
    "start": "513160",
    "end": "518950"
  },
  {
    "text": "the tools out there like volcano Armada a multi cluster scheduler and they're",
    "start": "518950",
    "end": "526780"
  },
  {
    "text": "like a few other like little random things they're sort of helpful to us and getting started we've also been meeting",
    "start": "526780",
    "end": "533740"
  },
  {
    "text": "with some of the groups in the upstream community like the multi-tenancy working group they're working on a hierarchical",
    "start": "533740",
    "end": "540130"
  },
  {
    "text": "namespace controller to sort of Wow subdivide out a namespace into smaller groups and apply policies down we are",
    "start": "540130",
    "end": "547120"
  },
  {
    "text": "also going to be working with the meeting with the multi cluster sig to",
    "start": "547120",
    "end": "552850"
  },
  {
    "text": "help drive some of the more that stuff upstream and with that out of the way",
    "start": "552850",
    "end": "559650"
  },
  {
    "text": "now it's actually sort of like dive in and have this discussion",
    "start": "559650",
    "end": "565350"
  },
  {
    "text": "it should be showing the results",
    "start": "572709",
    "end": "577139"
  },
  {
    "text": "oh there we go okay oh this is a note with us being you know",
    "start": "581060",
    "end": "589270"
  },
  {
    "start": "585000",
    "end": "646000"
  },
  {
    "text": "researchers and data science he type people the results of this will all be PR later into the CN CF research he's a",
    "start": "589270",
    "end": "596110"
  },
  {
    "text": "group repo so it'll be available for anyone to play with and take a look at",
    "start": "596110",
    "end": "601270"
  },
  {
    "text": "if people want I can I think it will will save be like names of people but I",
    "start": "601270",
    "end": "606430"
  },
  {
    "text": "can sanitize that stuff out looking this a lot of us are actually academics say",
    "start": "606430",
    "end": "614740"
  },
  {
    "text": "like not not not too much of a surprise from what I've seen but uh it's",
    "start": "614740",
    "end": "620170"
  },
  {
    "text": "definitely good to know",
    "start": "620170",
    "end": "622920"
  },
  {
    "text": "okay [Music]",
    "start": "632209",
    "end": "637520"
  },
  {
    "text": "yeah I give me give me one sec",
    "start": "637520",
    "end": "642100"
  },
  {
    "start": "646000",
    "end": "724000"
  },
  {
    "text": "I have like a little control panel and it's being really slow for me to try like activate the results sorry about",
    "start": "647000",
    "end": "654470"
  },
  {
    "text": "that okay so this actually surprises me",
    "start": "654470",
    "end": "661520"
  },
  {
    "text": "again about there hasn't been a whole lot of groups",
    "start": "661520",
    "end": "668020"
  },
  {
    "text": "that I've dealt with that are actually running in production okay sorry sorry",
    "start": "668020",
    "end": "677190"
  },
  {
    "text": "that's a very good thing I would say supporting research users and are they",
    "start": "677430",
    "end": "684850"
  },
  {
    "text": "actually you know using it to a capacity that like besides you know like a little",
    "start": "684850",
    "end": "690670"
  },
  {
    "text": "POC that might not be a good definition for production but that that might sort",
    "start": "690670",
    "end": "695950"
  },
  {
    "text": "of satisfy that good enough so out of curiosity if people don't mind",
    "start": "695950",
    "end": "706300"
  },
  {
    "text": "sharing who's actually running in production well oh I know you're running",
    "start": "706300",
    "end": "713110"
  },
  {
    "text": "in production what sort of research workloads are you running on there you have the microphone",
    "start": "713110",
    "end": "721720"
  },
  {
    "text": "hi Adam Tillman from University California San Diego and so we have",
    "start": "721720",
    "end": "727810"
  },
  {
    "start": "724000",
    "end": "1114000"
  },
  {
    "text": "about 100 graduate students you I'm sorry it's all good yeah okay so we have about a hundred graduate students that",
    "start": "727810",
    "end": "734230"
  },
  {
    "text": "are running various either batch or Jupiter interactive workloads data",
    "start": "734230",
    "end": "739720"
  },
  {
    "text": "science machine learning so we've got a GPU CPU cluster on campus we have a similar platform some of you may have",
    "start": "739720",
    "end": "747250"
  },
  {
    "text": "run into a Pacific Research platform or yeah and so we worked very closely with those folks on campus and so they have a",
    "start": "747250",
    "end": "753400"
  },
  {
    "text": "much broader kind of footprint in the kind of production research side of things yeah",
    "start": "753400",
    "end": "760820"
  },
  {
    "text": "yeah Larry Smarr worked very closely with him and Tom Davonte John Graham",
    "start": "760820",
    "end": "767290"
  },
  {
    "text": "yeah okay so I'm Ricardo from CERN we've",
    "start": "769660",
    "end": "776540"
  },
  {
    "text": "been using corners in production for a while now and we do internal services",
    "start": "776540",
    "end": "783050"
  },
  {
    "text": "but one more research-oriented we do offer what we call spark as a service",
    "start": "783050",
    "end": "788660"
  },
  {
    "text": "where people can just click a button and get a spark cluster that is backed by a kubernetes cluster and they submit their",
    "start": "788660",
    "end": "795110"
  },
  {
    "text": "spark jobs there we have some Jupiter and deployments that are backed also by",
    "start": "795110",
    "end": "801350"
  },
  {
    "text": "corners clusters and this is both for running the notebooks hosting notebooks",
    "start": "801350",
    "end": "806420"
  },
  {
    "text": "and also offloading like larger computations to also corners clusters",
    "start": "806420",
    "end": "812990"
  },
  {
    "text": "and we've been migrating part of our HD Condor batch farm to host it on",
    "start": "812990",
    "end": "820790"
  },
  {
    "text": "coronaries so basically deployed Condor on coronaries so I think these are the",
    "start": "820790",
    "end": "826100"
  },
  {
    "text": "most fitting use cases we have for this community yeah so we we offer a",
    "start": "826100",
    "end": "837560"
  },
  {
    "text": "kubernetes as a service so it's very easy to get your own cluster so in the end we get way too many we have like 500",
    "start": "837560",
    "end": "843830"
  },
  {
    "text": "clusters but of those if like there's",
    "start": "843830",
    "end": "849830"
  },
  {
    "text": "clusters that are just test for test purposes there's also clusters that are for",
    "start": "849830",
    "end": "856340"
  },
  {
    "text": "services that are like migrating to corners from traditional VMs and physical machines and then we have some",
    "start": "856340",
    "end": "864640"
  },
  {
    "text": "like 10% of those are production I would say and this can be anything from five",
    "start": "864640",
    "end": "872600"
  },
  {
    "text": "or ten mill clusters to something like a couple of hundred the largest clusters we had or a thousand notes this was as",
    "start": "872600",
    "end": "880400"
  },
  {
    "text": "big as we got did you want to say anything about size",
    "start": "880400",
    "end": "888050"
  },
  {
    "text": "yeah so so we have about a we have a 25 no cluster and about a hundred GPUs so",
    "start": "888410",
    "end": "894780"
  },
  {
    "text": "much smaller scale yeah it isn't yours actually like stretched cause like you",
    "start": "894780",
    "end": "902640"
  },
  {
    "text": "have lots of like one note like one note servers is joined with like one set of grades control plane notes or do you",
    "start": "902640",
    "end": "911970"
  },
  {
    "text": "have like multiple clusters so so we've got the cluster that I personally manage is a kind of monolithic and kind of",
    "start": "911970",
    "end": "921230"
  },
  {
    "text": "located a single site Pacific research platform has nodes scattered all over",
    "start": "921230",
    "end": "927320"
  },
  {
    "text": "the west coast of the US and then I think they're actually running multiple control planes and then shifting nodes",
    "start": "927320",
    "end": "933540"
  },
  {
    "text": "between between those as load warrants",
    "start": "933540",
    "end": "940519"
  },
  {
    "text": "we I I've got a cold is all about ready",
    "start": "940610",
    "end": "948480"
  },
  {
    "text": "voice Jamie from G research we do financial research over in London we've",
    "start": "948480",
    "end": "954089"
  },
  {
    "text": "got quite a lot of kubernetes in production mostly for services at the moment so",
    "start": "954089",
    "end": "959580"
  },
  {
    "text": "something in the order of one hundred hundred and fifty nodes multiple clusters but our big interest is working",
    "start": "959580",
    "end": "966780"
  },
  {
    "text": "out how to migrate our large Condor infrastructure so similar to what Ricardo's go in a way I guess in the",
    "start": "966780",
    "end": "972839"
  },
  {
    "text": "order of thousands tens of thousands of cause trying to work out how to migrate these workloads over to Cuba Nettie's",
    "start": "972839",
    "end": "979610"
  },
  {
    "text": "curiosity is anyone else doing stuff with HP Condor",
    "start": "979610",
    "end": "984950"
  },
  {
    "text": "I know if you get record unless you told",
    "start": "986000",
    "end": "991579"
  },
  {
    "text": "me we're an innkeeper knitties for about a year before that project got canceled",
    "start": "991579",
    "end": "997730"
  },
  {
    "text": "but we have done it successfully sorry are you still running kubernetes then yeah we have about a dozen clusters now",
    "start": "997730",
    "end": "1006660"
  },
  {
    "text": "who's next",
    "start": "1006750",
    "end": "1010050"
  },
  {
    "text": "[Laughter]",
    "start": "1014000",
    "end": "1017810"
  },
  {
    "text": "hey I'm uh I'm Jeremy Rodgers I work at the Oak Ridge National Laboratory so",
    "start": "1023360",
    "end": "1028490"
  },
  {
    "text": "people who were no one's ever heard of us except for people in research but as of yesterday we're still the we still",
    "start": "1028490",
    "end": "1033860"
  },
  {
    "text": "run the fastest supercomputer in the world so that's great I'm on the I mean the group that runs summit but my team",
    "start": "1033860",
    "end": "1040220"
  },
  {
    "text": "specifically runs services around summit so we used to be core infrastructure and",
    "start": "1040220",
    "end": "1046720"
  },
  {
    "text": "what we had is like lots of scientists like to use lots of different tools I'm an Operations guy not a researcher by",
    "start": "1046720",
    "end": "1051890"
  },
  {
    "text": "the way but they come to us and they're like hey we need to run this Maria DB we need to run this whatever so we moved to",
    "start": "1051890",
    "end": "1058940"
  },
  {
    "text": "kubernetes about I want to say about your and a half ago in production we rolled it out we've been running it for",
    "start": "1058940",
    "end": "1064640"
  },
  {
    "text": "about two and a half years now and it's really nice we provide kubernetes as a service and like also assistance with",
    "start": "1064640",
    "end": "1071990"
  },
  {
    "text": "getting things into kubernetes to run all of the ancillary stuff and our big project right now we're working on is trying to get Jupiter hub as a service",
    "start": "1071990",
    "end": "1078770"
  },
  {
    "text": "right so ideally there's like a front end and the scientists researchers can just like click a button they get a",
    "start": "1078770",
    "end": "1085130"
  },
  {
    "text": "Jupiter hub that uses their allocation on all of our really big clusters so that's what we're using in production",
    "start": "1085130",
    "end": "1091370"
  },
  {
    "text": "with so in mind closing the door we're getting come on in boys we have three",
    "start": "1091370",
    "end": "1100130"
  },
  {
    "text": "clusters each one has about 40 nodes on it yeah yeah who's who is interested in",
    "start": "1100130",
    "end": "1110960"
  },
  {
    "text": "doing something like you've heard the production people talk about running like chubarov HD Condor some the other",
    "start": "1110960",
    "end": "1117200"
  },
  {
    "text": "beer clusters interest okay who's next anyone else",
    "start": "1117200",
    "end": "1127450"
  },
  {
    "text": "it's gonna go somewhere okay let's get over to the next one then",
    "start": "1128349",
    "end": "1134859"
  },
  {
    "text": "that's kind of weird spaghetti",
    "start": "1140790",
    "end": "1144260"
  },
  {
    "text": "we see a lot of cute flow get lab",
    "start": "1146420",
    "end": "1154040"
  },
  {
    "text": "openshift",
    "start": "1154040",
    "end": "1156670"
  },
  {
    "text": "huh yeah Deaton Juber oh yeah I've",
    "start": "1159900",
    "end": "1165309"
  },
  {
    "text": "gotten bigger so the interest in Jupiter",
    "start": "1165309",
    "end": "1174789"
  },
  {
    "text": "is probably the one that I've seen the most that's the one that everyone's been trying to spit up and run and most of us",
    "start": "1174789",
    "end": "1180760"
  },
  {
    "text": "actually these days is trying to get to cube flow since cube flow actually packages so much into it I've got a",
    "start": "1180760",
    "end": "1191320"
  },
  {
    "text": "little bit of spark that's our flow this",
    "start": "1191320",
    "end": "1196360"
  },
  {
    "text": "will probably be easier to take a look at once the data is sort of spread out and like a CSV or something",
    "start": "1196360",
    "end": "1203429"
  },
  {
    "text": "yeah so if you're typing like a phrase it's it's it's it's not the best one but",
    "start": "1210020",
    "end": "1216890"
  },
  {
    "text": "it sort of is our group has had a lot of",
    "start": "1216890",
    "end": "1223550"
  },
  {
    "text": "conversation and you guys just behind me mentioned that like slow and other queueing buildings and how that relates",
    "start": "1223550",
    "end": "1229309"
  },
  {
    "start": "1225000",
    "end": "1366000"
  },
  {
    "text": "that hasn't shown up on this so much yet",
    "start": "1229309",
    "end": "1234669"
  },
  {
    "text": "slurm is actually one of our small use cases because like the main batch farm",
    "start": "1236950",
    "end": "1242750"
  },
  {
    "text": "is actually what we call high throughput computing not necessarily HPC but we do",
    "start": "1242750",
    "end": "1248090"
  },
  {
    "text": "have one large HPC cluster where we run slurm and what we are looking actually",
    "start": "1248090",
    "end": "1253520"
  },
  {
    "text": "is what was being described before which is to simplify the life for people submitting jobs to learn but also",
    "start": "1253520",
    "end": "1260059"
  },
  {
    "text": "analyzing the output data we are looking into offering one profile in the Jupiter",
    "start": "1260059",
    "end": "1265490"
  },
  {
    "text": "hub where they will get a notebook that gives them access to the slum cluster and also allows them to use notebooks to",
    "start": "1265490",
    "end": "1271610"
  },
  {
    "text": "analyze the output which is like something that is not trivial to do it's LARM you would have to copy the data",
    "start": "1271610",
    "end": "1277040"
  },
  {
    "text": "somewhere else and do your analysis so having this in a gypped a notebook actually I think it's a pretty good",
    "start": "1277040",
    "end": "1284210"
  },
  {
    "text": "thing I'll kick it over the next one",
    "start": "1284210",
    "end": "1293200"
  },
  {
    "text": "scaling",
    "start": "1311860",
    "end": "1314760"
  },
  {
    "text": "multi-tenancy has definitely been a big one that we've seen especially for those of us coming from the classic HPC side",
    "start": "1326920",
    "end": "1332290"
  },
  {
    "text": "we are where we are used to sharing a large sort of shared POSIX environment and there definitely hasn't been a good",
    "start": "1332290",
    "end": "1338320"
  },
  {
    "text": "sort of translation over to how you go from like a POSIX identity to more of",
    "start": "1338320",
    "end": "1343450"
  },
  {
    "text": "like an API identity where things are running things with services there's",
    "start": "1343450",
    "end": "1348940"
  },
  {
    "text": "definite been a lot of efforts to sort of like bridge that gap a little bit especially in the multi-tenancy working",
    "start": "1348940",
    "end": "1355210"
  },
  {
    "text": "group pointing out and over there is a member of the multi-tenancy I'm",
    "start": "1355210",
    "end": "1365200"
  },
  {
    "text": "interested in the person that's mentioned security can ask who that was",
    "start": "1365200",
    "end": "1370330"
  },
  {
    "start": "1366000",
    "end": "1598000"
  },
  {
    "text": "yeah okay enough I understand their financial financial industry yeah anyone",
    "start": "1370330",
    "end": "1375640"
  },
  {
    "text": "else expecting most sensitive payloads on",
    "start": "1375640",
    "end": "1382179"
  },
  {
    "text": "yeah yeah he's my boss we run a restricted environment yes how",
    "start": "1382179",
    "end": "1394150"
  },
  {
    "text": "about you",
    "start": "1394150",
    "end": "1396480"
  },
  {
    "text": "we're a university that has a health system attached to it and so we get massive amounts of Health System data as",
    "start": "1399540",
    "end": "1405520"
  },
  {
    "text": "well as insurance claims data that I think people really want to manipulate and their data set sizes are now",
    "start": "1405520",
    "end": "1411520"
  },
  {
    "text": "reaching like they'll get Medicare datasets I almost a terabyte in size which SAS doesn't deal well so I'm in",
    "start": "1411520",
    "end": "1421150"
  },
  {
    "text": "the same exact time but do you maintain separate environments or just put a long",
    "start": "1421150",
    "end": "1428530"
  },
  {
    "text": "one and to treat them all as like you know s4 type of data so we actually have",
    "start": "1428530",
    "end": "1435310"
  },
  {
    "text": "separate environments for a restricted data compared so we have an HPC restricted data environment and an HPC not restricted environment and then we",
    "start": "1435310",
    "end": "1441760"
  },
  {
    "text": "have a private cloud for the sent does the same thing basically our university mandates that are restricted data stay",
    "start": "1441760",
    "end": "1447970"
  },
  {
    "text": "separate the ease of use so that you know that you know that's secure because it's over there",
    "start": "1447970",
    "end": "1455309"
  },
  {
    "text": "so both actually we whatever we build on Purim we mirror basically and whatever we build in the cloud we're mirroring so",
    "start": "1464660",
    "end": "1471260"
  },
  {
    "text": "we have we are building in public a restricted data environment then we're",
    "start": "1471260",
    "end": "1476300"
  },
  {
    "text": "starting with AWS and then we'll slowly branch out other things as as people a sport ins as we sort of we don't want to",
    "start": "1476300",
    "end": "1482720"
  },
  {
    "text": "do everything all at once because we don't have the people to do that and so start one place leave even though the researchers were clamoring for Google",
    "start": "1482720",
    "end": "1488890"
  },
  {
    "text": "for some reason leadership said let's do AWS so you know start somewhere I guess",
    "start": "1488890",
    "end": "1495050"
  },
  {
    "text": "you know one of the things I'm seeing also multiplies others like it's easy in",
    "start": "1495050",
    "end": "1501230"
  },
  {
    "text": "the learning curve that is definitely something that we have seen repeatedly over and over again the internal",
    "start": "1501230",
    "end": "1508400"
  },
  {
    "text": "training is definitely a problem we have",
    "start": "1508400",
    "end": "1514250"
  },
  {
    "text": "sort of solved this a little bit like we have a set of tutorials that we have or getting people sort of up to speed on",
    "start": "1514250",
    "end": "1519740"
  },
  {
    "text": "kubernetes and part of that tutorial if it's a research user we we get super hop",
    "start": "1519740",
    "end": "1525650"
  },
  {
    "text": "up and going and show them how to like use you used you put a lab and that sort of thing you know question",
    "start": "1525650",
    "end": "1535809"
  },
  {
    "text": "that documentation for kione's is really focused kind of around the operators",
    "start": "1538650",
    "end": "1544510"
  },
  {
    "text": "using it and there's not a clean separation of like I am providing a cluster for you here's the documentation",
    "start": "1544510",
    "end": "1551980"
  },
  {
    "text": "on how you would use it as a user who else has had this sort of problem",
    "start": "1551980",
    "end": "1557760"
  },
  {
    "text": "everyone decent chunk know that is something that is a I wouldn't call it",
    "start": "1557760",
    "end": "1564430"
  },
  {
    "text": "like low-hanging fruit low-hanging fruits the wrong word that is something that we should be able to solve fairly quickly a lot of us now have experience",
    "start": "1564430",
    "end": "1571720"
  },
  {
    "text": "with running these sort of things so one thing that was identified as is building",
    "start": "1571720",
    "end": "1576730"
  },
  {
    "text": "that common best practices and building out some documentation I think this point is just finding a few people that",
    "start": "1576730",
    "end": "1581770"
  },
  {
    "text": "might be and will sort of get going with this effort and and start you know putting words down and some that we",
    "start": "1581770",
    "end": "1587980"
  },
  {
    "text": "actually have more like internally - it's just not you know publicly available I think this sum relates to your very",
    "start": "1587980",
    "end": "1597400"
  },
  {
    "text": "opening question asking saying everyone this room or researchers but I'm if I",
    "start": "1597400",
    "end": "1602440"
  },
  {
    "start": "1598000",
    "end": "1753000"
  },
  {
    "text": "asked you how many of you are actual academics there wouldn't be probably that many I imagine right there or",
    "start": "1602440",
    "end": "1607840"
  },
  {
    "text": "custodians of research infrastructure that's why I tend to call and then the question is how do we make how do we",
    "start": "1607840",
    "end": "1614320"
  },
  {
    "text": "empower ourselves to make the best experience for our research users",
    "start": "1614320",
    "end": "1621029"
  },
  {
    "text": "so if we call you researchers you know remain cool",
    "start": "1625750",
    "end": "1632669"
  },
  {
    "text": "I'll kick over to the next one bye the most important",
    "start": "1634740",
    "end": "1641330"
  },
  {
    "text": "so where should we as a group you know we've we've talked a little bit about some of the problems we've encountered",
    "start": "1646360",
    "end": "1652690"
  },
  {
    "text": "some of the tools we were looking at where should we as a group focus our",
    "start": "1652690",
    "end": "1659380"
  },
  {
    "text": "efforts what do you think we should prioritize",
    "start": "1659380",
    "end": "1664710"
  },
  {
    "text": "it's sort of like reddit so you can upvote and downvote options",
    "start": "1668660",
    "end": "1673630"
  },
  {
    "text": "multi-tenancy is definitely looking like the big one so far it's less I would say",
    "start": "1676930",
    "end": "1688640"
  },
  {
    "text": "duplicating and more so you know if you are interested in this like start",
    "start": "1688640",
    "end": "1696200"
  },
  {
    "text": "talking to the multi-tenancy working group in the upstream project or you know you know maybe Ryan wouldn't mind",
    "start": "1696200",
    "end": "1703400"
  },
  {
    "text": "coming back again to discuss that stuff with the project more the other thing is",
    "start": "1703400",
    "end": "1709130"
  },
  {
    "text": "just you know getting end-user feedback for some of this stuff a lot of times you know something gets sort of deployed",
    "start": "1709130",
    "end": "1714650"
  },
  {
    "text": "and and there isn't a good response there's no feedback and driving a stuff",
    "start": "1714650",
    "end": "1720980"
  },
  {
    "text": "really really requires that this has been a fairly large problem in the multi",
    "start": "1720980",
    "end": "1727040"
  },
  {
    "text": "cluster sig they've been trying to get more end-user feedback regarding some of",
    "start": "1727040",
    "end": "1732680"
  },
  {
    "text": "the stuff that they're doing and for the most part it's been been pretty quiet they they're like they want to you know",
    "start": "1732680",
    "end": "1740210"
  },
  {
    "text": "accommodate this stuff they just haven't gotten good feedback regarding how they should design it and you know the sort",
    "start": "1740210",
    "end": "1746510"
  },
  {
    "text": "of different needs that we have and I",
    "start": "1746510",
    "end": "1752780"
  },
  {
    "text": "think part of that is rather than was that there's 21 responses rather than",
    "start": "1752780",
    "end": "1758600"
  },
  {
    "start": "1753000",
    "end": "1811000"
  },
  {
    "text": "all of us try and speak to each of these group ourselves you know as one coordinated effort we're gonna have far",
    "start": "1758600",
    "end": "1764900"
  },
  {
    "text": "more impact and we can you know work towards some best practice if we can say you know two dozen research sites are",
    "start": "1764900",
    "end": "1772340"
  },
  {
    "text": "interested in you know pushing for with this feature and especially if they're willing to test it and provide feedback",
    "start": "1772340",
    "end": "1778280"
  },
  {
    "text": "we will get a much better thing out of the entire effort",
    "start": "1778280",
    "end": "1783850"
  },
  {
    "text": "documentation is the next one yeah that one is something honestly it's probably",
    "start": "1785710",
    "end": "1791620"
  },
  {
    "text": "gonna fall into us as users to solve their there really just isn't a lot for",
    "start": "1791620",
    "end": "1797140"
  },
  {
    "text": "people out there that are trying to run no research workloads on communities and the documentation itself is very much",
    "start": "1797140",
    "end": "1803260"
  },
  {
    "text": "geared towards the more enterprising type workloads one other comment about",
    "start": "1803260",
    "end": "1811990"
  },
  {
    "text": "documentation is there's almost too much of it right I like I almost never start with one browser tab on documentation",
    "start": "1811990",
    "end": "1818770"
  },
  {
    "text": "and I almost never end up with one tab because I always end up feeling like it's like the Wikipedia game where you",
    "start": "1818770",
    "end": "1823810"
  },
  {
    "text": "yeah I'll get to this later you know and they never do so so building more",
    "start": "1823810",
    "end": "1829540"
  },
  {
    "text": "concise documentation or having a single place for it yeah okay concise documentation yeah that I think",
    "start": "1829540",
    "end": "1836410"
  },
  {
    "text": "it's something we can definitely work on especially if we we have the CN CF research user group get repo if we want",
    "start": "1836410",
    "end": "1844330"
  },
  {
    "text": "to create a folder in there and start like dumping you know some best practice some of the documentation for stuff",
    "start": "1844330",
    "end": "1849690"
  },
  {
    "text": "that'd be great it's essentially accessible to everyone and you know if something gets out of date it's just a",
    "start": "1849690",
    "end": "1856450"
  },
  {
    "text": "PR away from getting fixed one last",
    "start": "1856450",
    "end": "1862510"
  },
  {
    "text": "thing I want to touch on this the batch by batch there's a couple different efforts going on right now on this for",
    "start": "1862510",
    "end": "1870760"
  },
  {
    "text": "those of they don't there's like queue batch which is sort of an upstream project to sort of provide the base batch capabilities but now we've sort of",
    "start": "1870760",
    "end": "1877300"
  },
  {
    "text": "seen a lot of being developed outside of the kubernetes project itself there is a",
    "start": "1877300",
    "end": "1882370"
  },
  {
    "text": "project called volcano which actually is developed by the the one of the leads of",
    "start": "1882370",
    "end": "1889420"
  },
  {
    "text": "six scheduling who is in a X IBM er and they're adding things like Hugh's fair",
    "start": "1889420",
    "end": "1896200"
  },
  {
    "text": "share backfill the stuff that's you know pretty stereotypical the things that we work with and then recently I haven't",
    "start": "1896200",
    "end": "1902560"
  },
  {
    "text": "had a chance to play with it it was announced at SC but Google just released their sort of K batch or batch on gke",
    "start": "1902560",
    "end": "1911040"
  },
  {
    "text": "that one like it's still in I believe in beta and it's like the only support",
    "start": "1914260",
    "end": "1919940"
  },
  {
    "text": "likes deploying what clothes to single node right now but that is something we could also engage more with it's",
    "start": "1919940",
    "end": "1931400"
  },
  {
    "text": "complicated no no but possibly in the future right now it's not as far as no",
    "start": "1931400",
    "end": "1940490"
  },
  {
    "text": "there's more development going on internally but again like I don't know where it will be in the future",
    "start": "1940490",
    "end": "1948039"
  },
  {
    "text": "my guess would be things like going to like luster or you know GPFS there we go",
    "start": "1954800",
    "end": "1965090"
  },
  {
    "text": "thank you the more classic sure yeah yeah sorry I guess the question was more",
    "start": "1965090",
    "end": "1971180"
  },
  {
    "start": "1971000",
    "end": "2069000"
  },
  {
    "text": "like to to engage with the CSI drivers that will expose those file systems or",
    "start": "1971180",
    "end": "1977420"
  },
  {
    "text": "what we use like surface and to other internal storage and we rely on CSI for",
    "start": "1977420",
    "end": "1982520"
  },
  {
    "text": "all of them is this like something we would follow so I'm from nurse which is",
    "start": "1982520",
    "end": "1992540"
  },
  {
    "text": "another do a super computing lab and our challenge really is identity mapping to",
    "start": "1992540",
    "end": "1999010"
  },
  {
    "text": "pods yep and so it's it's it's synching up the inside world and the outside",
    "start": "1999010",
    "end": "2004480"
  },
  {
    "text": "world around you IDs and I that's very difficult to do without kernel or you",
    "start": "2004480",
    "end": "2010750"
  },
  {
    "text": "know we need help it's not something that we likely can solve on our own yeah I'd actually it's still in POC form",
    "start": "2010750",
    "end": "2016660"
  },
  {
    "text": "it's a project I developed like over a year ago that would you like an LDAP look up and pull the POSIX identity from",
    "start": "2016660",
    "end": "2023710"
  },
  {
    "text": "that and basically mutate the pod to run the pod as that UID and GID",
    "start": "2023710",
    "end": "2029170"
  },
  {
    "text": "but that does have some other problems with like adding that identity to the container itself yes I'm similar we have existing",
    "start": "2029170",
    "end": "2035130"
  },
  {
    "text": "Luster's and NFS shares and things that users would like to go into Jupiter hub",
    "start": "2035130",
    "end": "2040420"
  },
  {
    "text": "you know login through Jupiter Jupiter hub get a Jupiter then access their data",
    "start": "2040420",
    "end": "2045670"
  },
  {
    "text": "from the cluster and Jupiter so we're looking currently looking at like OPA",
    "start": "2045670",
    "end": "2051250"
  },
  {
    "text": "policies to allow them to only use their own you√≠d and then let them use their",
    "start": "2051250",
    "end": "2057669"
  },
  {
    "text": "mounts directly then okay we're actually I think we're now out of time but yeah",
    "start": "2057669",
    "end": "2063669"
  },
  {
    "text": "well okay that's that's exactly how we",
    "start": "2063669",
    "end": "2069879"
  },
  {
    "start": "2069000",
    "end": "2303000"
  },
  {
    "text": "do it right now so on all of our kubernetes hosts we mount GPFS we mount NFS and we mount lustre and we on every",
    "start": "2069880",
    "end": "2078220"
  },
  {
    "text": "namespace we have an annotation that we set that's their specific ID of their",
    "start": "2078220",
    "end": "2083530"
  },
  {
    "text": "project and then we use OPA to force they're enforced that their pods run is that user ID and group ID and then and",
    "start": "2083530",
    "end": "2090090"
  },
  {
    "text": "then host mounted up into the container it's not perfect but I mean that's a possessed workaround that we have you",
    "start": "2090090",
    "end": "2096270"
  },
  {
    "text": "know right now yes refactoring that our",
    "start": "2096270",
    "end": "2108960"
  },
  {
    "text": "scientists have to do and they're inheriting a bunch of containers that expect to run its route right now and so",
    "start": "2108960",
    "end": "2115590"
  },
  {
    "text": "you know we can be the Department of no but you know the telling them no go ahead and here's how you go ahead and",
    "start": "2115590",
    "end": "2121980"
  },
  {
    "text": "you make your pod so it can run as you it's it's not a pleasant answer to deliver over and oh yeah I had I had a",
    "start": "2121980",
    "end": "2128160"
  },
  {
    "text": "workaround for that one but essentially it would in a net container that would add the identity to it and like you",
    "start": "2128160",
    "end": "2135930"
  },
  {
    "text": "still had some file permissions issues depending on how it was deployed but for the most part it worked but we are where",
    "start": "2135930",
    "end": "2142170"
  },
  {
    "text": "I think could now passed our time so I just want to toss this out if this is",
    "start": "2142170",
    "end": "2147780"
  },
  {
    "text": "stuff you are interested in please join the mailing list please join it on some of the calls that we have our times",
    "start": "2147780",
    "end": "2155520"
  },
  {
    "text": "right now are not West Coast friendly it's because most of our users have been either in the EU or a PAC but you know",
    "start": "2155520",
    "end": "2161790"
  },
  {
    "text": "we can revisit that again yeah we don't",
    "start": "2161790",
    "end": "2170010"
  },
  {
    "text": "have a Google Group we have just the mailing list on the the Linux found the the one that the CN CF uses there's a",
    "start": "2170010",
    "end": "2178620"
  },
  {
    "text": "chance for us to catch up tonight not only at the block party but I'll introduce Cheryl and could I go for you",
    "start": "2178620",
    "end": "2188180"
  },
  {
    "text": "oh yeah sure no worries your HDMI hey so I'm Cheryl I'm the director of ecosystem",
    "start": "2191180",
    "end": "2197460"
  },
  {
    "text": "at the CN CF and I've run some of the end user groups of the CN CF perspective",
    "start": "2197460",
    "end": "2204900"
  },
  {
    "text": "don't have any adapter for it nope okay um I don't have a USB see",
    "start": "2204900",
    "end": "2211050"
  },
  {
    "text": "don't go here I can uh is a shared with me",
    "start": "2211050",
    "end": "2216740"
  },
  {
    "text": "or here yeah we have people waiting over",
    "start": "2217230",
    "end": "2224530"
  },
  {
    "text": "there so uh okay you can do this okay so",
    "start": "2224530",
    "end": "2232630"
  },
  {
    "text": "okay so I run the user groups from the CN Seattle perspective so we're super",
    "start": "2232630",
    "end": "2238840"
  },
  {
    "text": "super keen to get as many people as possible to not necessarily contribute",
    "start": "2238840",
    "end": "2244900"
  },
  {
    "text": "code back but at least be active and engaged and make their thoughts known and within the CN CF so tonight we're",
    "start": "2244900",
    "end": "2253720"
  },
  {
    "text": "going to have eight till 10:00 p.m. social kind of happy hour so after",
    "start": "2253720",
    "end": "2261340"
  },
  {
    "text": "they're all attendee party and it's not specific for back up ones up one there",
    "start": "2261340",
    "end": "2268990"
  },
  {
    "text": "we go it's not specific for research but it's for end users generally so it's six",
    "start": "2268990",
    "end": "2275230"
  },
  {
    "text": "minutes walk from the conference center please come along and just go and meet other people there Thanks cool now we",
    "start": "2275230",
    "end": "2288730"
  },
  {
    "text": "should do that outside though cuz there's a line of people out there the Alaska thing if you go to this present a",
    "start": "2288730",
    "end": "2294280"
  },
  {
    "text": "the presentation from earlier the PDF of the research user group there's a bunch of links to the other sessions that are",
    "start": "2294280",
    "end": "2300340"
  },
  {
    "text": "irrelevant as a sort of single source of truth for them",
    "start": "2300340",
    "end": "2304619"
  }
]