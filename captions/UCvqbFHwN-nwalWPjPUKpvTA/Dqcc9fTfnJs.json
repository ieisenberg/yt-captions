[
  {
    "text": "hi everyone yeah so uh good morning uh we're excited to start our sharing today and I hope everyone's settled down and",
    "start": "0",
    "end": "6240"
  },
  {
    "text": "making yourself comfortable so uh we're excited to start sharing today on how we co-locate Hadoop yarn",
    "start": "6240",
    "end": "11280"
  },
  {
    "text": "jobs with kubernetes in order to save massive costs on Big Data my name is Irvin and with menu today is my teammate",
    "start": "11280",
    "end": "17100"
  },
  {
    "text": "hylin yeah yeah and we're both platform Engineers working in the engineering",
    "start": "17100",
    "end": "23160"
  },
  {
    "text": "infrastructure team at shopee so uh before we start I just I just want to give a quick introduction of our",
    "start": "23160",
    "end": "29580"
  },
  {
    "text": "company so we are from shopee an e-commerce platform that operates across multiple markets across the world",
    "start": "29580",
    "end": "36120"
  },
  {
    "text": "today we are the leading e-commerce platform in Southeast Asia Taiwan and Brazil we are also the number one",
    "start": "36120",
    "end": "41700"
  },
  {
    "text": "shopping app in these markets by average monthly active users as well as total time spending app",
    "start": "41700",
    "end": "48180"
  },
  {
    "text": "we continue to grow in scale so we can build on our strong brand recognition across the region",
    "start": "48180",
    "end": "53460"
  },
  {
    "text": "so why did I just share all of this with you guys today so as an e-commerce platform I cannot understand the",
    "start": "53460",
    "end": "59579"
  },
  {
    "text": "importance of robust and scalable infrastructure to support our fast growth and large user base across the",
    "start": "59579",
    "end": "64920"
  },
  {
    "text": "world we have completely embraced kubernetes within our company running hundreds of",
    "start": "64920",
    "end": "70080"
  },
  {
    "text": "thousands of clusters oh sorry hundreds of thousands of ports across tens of thousands of nodes within hundreds of",
    "start": "70080",
    "end": "75720"
  },
  {
    "text": "clusters on more than 10 data centers spanning across the globe as we support the growth of the business",
    "start": "75720",
    "end": "81299"
  },
  {
    "text": "we expect these numbers to keep increasing in the months and years ahead so now that introductions are out of the",
    "start": "81299",
    "end": "88140"
  },
  {
    "text": "way let me dive right into the problem that we want to share with you guys today so one of the largest and most important",
    "start": "88140",
    "end": "94020"
  },
  {
    "text": "days at shopee are what we call campaign days we run and operate several large",
    "start": "94020",
    "end": "99180"
  },
  {
    "text": "campaigns every year including the 99 super shopping day in September as well as the 11 11 big sale in November",
    "start": "99180",
    "end": "107700"
  },
  {
    "text": "as you might expect supporting an e-commerce platform's poses several unique and difficult challenges for us",
    "start": "107700",
    "end": "114479"
  },
  {
    "text": "in order to support millions of active users on our platform we need huge amounts of compute resources",
    "start": "114479",
    "end": "119820"
  },
  {
    "text": "large numbers of users tend to log on to shopee at the same time since many of our larger markets are in the same few",
    "start": "119820",
    "end": "125700"
  },
  {
    "text": "time zones around the region during low volume periods at night we also experience rather deep trials in",
    "start": "125700",
    "end": "132060"
  },
  {
    "text": "which large amounts of our resources remain idle and underutilized",
    "start": "132060",
    "end": "137360"
  },
  {
    "text": "so to make things even worse during large campaigns like mentioned in on the previous slide we also have even higher",
    "start": "137640",
    "end": "143879"
  },
  {
    "text": "Peaks as what we call what is known as campaign spikes",
    "start": "143879",
    "end": "149459"
  },
  {
    "text": "so during campaigns the peak CPU utilization can rise up to 10 times as",
    "start": "149459",
    "end": "154560"
  },
  {
    "text": "compared to non-campaign days this is also because the campaigns in our larger markets are also tend to",
    "start": "154560",
    "end": "159840"
  },
  {
    "text": "start around the same time as such campaign days are one of the most important days of the year for the",
    "start": "159840",
    "end": "165599"
  },
  {
    "text": "business in order to support such a disproportionate increase in user traffic we need to ensure that our clusters are",
    "start": "165599",
    "end": "172379"
  },
  {
    "text": "adequately provisioned with sufficient buffer capacity",
    "start": "172379",
    "end": "176780"
  },
  {
    "text": "to summarize the challenges that we faced we can see that firstly as an e-commerce platform our user traffic",
    "start": "178019",
    "end": "183720"
  },
  {
    "text": "patterns are very busy since our users are all around the same time zones on campaign days the Peaks have sick the",
    "start": "183720",
    "end": "191760"
  },
  {
    "text": "Peaks are significantly larger than normal which makes things even more difficult and lastly our users are extremely",
    "start": "191760",
    "end": "198720"
  },
  {
    "text": "sensitive to latencies especially during the most important campaign days like mansion when put together this makes capacity",
    "start": "198720",
    "end": "205560"
  },
  {
    "text": "planning rather challenging for us as a company and within our engineering infrastructure organization",
    "start": "205560",
    "end": "212480"
  },
  {
    "text": "so here lies the main problem in order to spot traffic that is so bursty we usually have to prepare large",
    "start": "212879",
    "end": "219720"
  },
  {
    "text": "amounts of resources that tend to end up wasted and underutilized most of the time",
    "start": "219720",
    "end": "225180"
  },
  {
    "text": "but at the same time we also found that other departments in our company such as Big Data teams they are also facing",
    "start": "225180",
    "end": "232379"
  },
  {
    "text": "rather tight resource crunch from rapidly increasing demand from their users so is there a way for us to reconcile",
    "start": "232379",
    "end": "239700"
  },
  {
    "text": "these two problems together",
    "start": "239700",
    "end": "242840"
  },
  {
    "text": "it'll be a good idea that during periods of low utilization we could run some low priority workloads to have weak latency",
    "start": "244739",
    "end": "250739"
  },
  {
    "text": "requirements especially if they have pretty predictable usage patterns",
    "start": "250739",
    "end": "256380"
  },
  {
    "text": "this way we can evict them whenever we urgently need resources to handle large spikes in user traffic suddenly",
    "start": "256380",
    "end": "264680"
  },
  {
    "text": "for example we can run some low priority batch jobs Big Data queries coin drops",
    "start": "265860",
    "end": "270960"
  },
  {
    "text": "or even some machine learning training tasks whenever resources are idle so let's take a moment to think how can we",
    "start": "270960",
    "end": "277979"
  },
  {
    "text": "do this with uh what we have in kubernetes currently",
    "start": "277979",
    "end": "282740"
  },
  {
    "text": "so let me just share a potential solution over here so using the horizontal pod Auto scaler",
    "start": "283259",
    "end": "289500"
  },
  {
    "text": "as well as a priority class that you can configure on your pods we might be able to support these requirements and let me",
    "start": "289500",
    "end": "295440"
  },
  {
    "text": "show you how let's assume we managed to schedule some batch jobs represented in yellow on the",
    "start": "295440",
    "end": "300780"
  },
  {
    "text": "right and these have a lower priority class than the prod Services represented in",
    "start": "300780",
    "end": "306000"
  },
  {
    "text": "green on the left so when resources are idle best jobs would be able to exist peacefully with",
    "start": "306000",
    "end": "312540"
  },
  {
    "text": "the broad Services since there should be ample amounts of idle CPU resources available to schedule the best shops",
    "start": "312540",
    "end": "318840"
  },
  {
    "text": "so let's say there's now a sudden increase sudden spike in users opening",
    "start": "318840",
    "end": "323880"
  },
  {
    "text": "the app at the same time and the CPU usage has also increased dramatically using HPA we can automatically scale up",
    "start": "323880",
    "end": "330960"
  },
  {
    "text": "the number of product service instance creating more ports that need to be scheduled",
    "start": "330960",
    "end": "336720"
  },
  {
    "text": "so if there are not enough CPU resources to schedule the product service the kubernetes schedule will start to evict",
    "start": "336720",
    "end": "342300"
  },
  {
    "text": "the lower priority batch job ports at this point the product service ports have now successfully been scheduled and",
    "start": "342300",
    "end": "348660"
  },
  {
    "text": "scaled up and we can now handle the sudden increase in user traffic and let's say it's now night time and",
    "start": "348660",
    "end": "355080"
  },
  {
    "text": "our users are sleeping the HPA will automatically scale down those parts and since then now lots of idle resources",
    "start": "355080",
    "end": "361139"
  },
  {
    "text": "any previously evicted best jobs can now be scheduled again successfully or by the kubernetes scheduler",
    "start": "361139",
    "end": "367560"
  },
  {
    "text": "so do we use this solution in the end as you might guess I wouldn't have",
    "start": "367560",
    "end": "372600"
  },
  {
    "text": "presented our actual solution so early on at the start of our presentation it's actually a lot more complicated than",
    "start": "372600",
    "end": "377699"
  },
  {
    "text": "that so firstly depending on eviction to free up resources is too slow as you might",
    "start": "377699",
    "end": "383460"
  },
  {
    "text": "have seen earlier that user traffic can increase super quickly super quickly especially when a campaign starts",
    "start": "383460",
    "end": "389880"
  },
  {
    "text": "next if we wish to support non-cubernetes workloads like Hadoop and Presto this is currently not very",
    "start": "389880",
    "end": "395580"
  },
  {
    "text": "feasible if we have to reprovision the entire node then it is both slow and risky",
    "start": "395580",
    "end": "401160"
  },
  {
    "text": "since we have to keep draining notes very frequently and lastly frequent eviction of best",
    "start": "401160",
    "end": "407160"
  },
  {
    "text": "jobs may result in the actual uh actually without wasted CPU utilization which does not contribute any real value",
    "start": "407160",
    "end": "412919"
  },
  {
    "text": "to a company as such we want to present our solution today that we have painstakingly worked",
    "start": "412919",
    "end": "419340"
  },
  {
    "text": "on for the past one year we'll explain how we manage to co-locate low priority Hadoop yarn jobs alongside",
    "start": "419340",
    "end": "425759"
  },
  {
    "text": "critical kubernetes ports in a safe but yet scalable manner",
    "start": "425759",
    "end": "431220"
  },
  {
    "text": "we allow Hadoop jobs to reclaim allocated but underutilized resources in real time while ensuring stability and",
    "start": "431220",
    "end": "437880"
  },
  {
    "text": "performance of critical ports to handle rapid spikes in user traffic so how do we do this let me welcome my",
    "start": "437880",
    "end": "444780"
  },
  {
    "text": "teammate Highland who will share more about how we managed to do so thank you",
    "start": "444780",
    "end": "449479"
  },
  {
    "text": "okay so thank you Arvin so let's see how to achieve the co-location so actually",
    "start": "450000",
    "end": "455160"
  },
  {
    "text": "in my opinion you could be divided into two parts the first part is the allocation so literally it defines how",
    "start": "455160",
    "end": "461940"
  },
  {
    "text": "to allocate the pro service at best service in the cluster and what's the",
    "start": "461940",
    "end": "467039"
  },
  {
    "text": "amount of product Services should be scheduled for every node so to schedule",
    "start": "467039",
    "end": "472319"
  },
  {
    "text": "clearly we need to First monitor all nodes and calculate the estimation and resources so after that we are able to",
    "start": "472319",
    "end": "479400"
  },
  {
    "text": "allow to do some scheduling stuff according to the strategies important part is the allocation it's",
    "start": "479400",
    "end": "486180"
  },
  {
    "text": "isolation sorry so to achieve this we have separation at eviction so suppression here means to separate",
    "start": "486180",
    "end": "493319"
  },
  {
    "text": "the back surfaces in real time and in case of any Spike from the plot services so therefore it is used as a protection",
    "start": "493319",
    "end": "501000"
  },
  {
    "text": "strategy to protect the stress to protect the product service in real time",
    "start": "501000",
    "end": "506160"
  },
  {
    "text": "and the eviction you avoid star version Crossing by a long time of suppression",
    "start": "506160",
    "end": "511319"
  },
  {
    "text": "because of the birth in Port service and it enables us the rescheduling to",
    "start": "511319",
    "end": "517080"
  },
  {
    "text": "guarantee the stability of the batch service so consider this is one of the node so",
    "start": "517080",
    "end": "523320"
  },
  {
    "text": "before we have collocation there's a couplet with some of the service port running here so in order to enable",
    "start": "523320",
    "end": "530240"
  },
  {
    "text": "co-location with yarn there are several ways to make it so the easiest way is we",
    "start": "530240",
    "end": "535740"
  },
  {
    "text": "deploy the node manager and make it co-hosted on the Node so assuming that",
    "start": "535740",
    "end": "540779"
  },
  {
    "text": "we have a yeah node manager deployed here so and then we can schedule some backshop here",
    "start": "540779",
    "end": "546440"
  },
  {
    "text": "Service as a young job I will interfere with each other therefore we introduce",
    "start": "546440",
    "end": "552660"
  },
  {
    "text": "our Lumia agent which will run on Evernote to negotiate with a couplet as",
    "start": "552660",
    "end": "558720"
  },
  {
    "text": "a yard node manager so you might be curious about what exactly the agent could do within the negotiation so",
    "start": "558720",
    "end": "565680"
  },
  {
    "text": "here's a brief overview of its responsibilities so basically you take in charge of six areas",
    "start": "565680",
    "end": "572339"
  },
  {
    "text": "firstly the agent will help to create the Matrix from different Source like the C group process wrist control and so",
    "start": "572339",
    "end": "579360"
  },
  {
    "text": "on with this metrics you will analyze the real-time status of all prod Apache",
    "start": "579360",
    "end": "585060"
  },
  {
    "text": "services on their load and also you re it reports a reclaimable resource to make the",
    "start": "585060",
    "end": "591720"
  },
  {
    "text": "scheduler be aware of any changes so the definition of the reclinable resource we are introduced later also it utilized",
    "start": "591720",
    "end": "599160"
  },
  {
    "text": "tools like the same group at risk control to do the isolations and it detects the separation status of the bad",
    "start": "599160",
    "end": "605880"
  },
  {
    "text": "services and then you will trigger the eviction according to the strategies alas you will use the metrics to try to",
    "start": "605880",
    "end": "613140"
  },
  {
    "text": "Auto recover from some failures if possible okay so after the brief introduction on",
    "start": "613140",
    "end": "619500"
  },
  {
    "text": "the two parts of the collocation so let's go into details see how the first part the allocation is achieved so we",
    "start": "619500",
    "end": "626040"
  },
  {
    "text": "will continue to use yarn as an example for the batch service so imagine this is a kubernetes node so",
    "start": "626040",
    "end": "632820"
  },
  {
    "text": "it has 48 CPU in total so you make it simple we assume there is no reservation",
    "start": "632820",
    "end": "638220"
  },
  {
    "text": "so all of the CPU belongs to the allocatable CPU so assuming some of the services coming in the kubernetes",
    "start": "638220",
    "end": "645360"
  },
  {
    "text": "scheduler comes on the CPU request and then schedule those four to this node so now we have a status of 45 CPU scheduled",
    "start": "645360",
    "end": "653399"
  },
  {
    "text": "so however imagine now it's at light time so the user is sleeping the real CPU usage for the home machine is super",
    "start": "653399",
    "end": "659940"
  },
  {
    "text": "low it's around like 7.2 CPU at this moment so to make his safety during estimating",
    "start": "659940",
    "end": "666420"
  },
  {
    "text": "the real usage with smoothly as on Buffer so by some mathematics we",
    "start": "666420",
    "end": "672180"
  },
  {
    "text": "calculate that the estimation of the CPU usage is actually about 7.7 uh it's 12.7",
    "start": "672180",
    "end": "677399"
  },
  {
    "text": "sorry so which means now we have 48 minus 12.7 resulting 35.3 CPU available",
    "start": "677399",
    "end": "683640"
  },
  {
    "text": "to overseal on this machine at the moment so this means that we can reduce this amount of CPU to run some best",
    "start": "683640",
    "end": "690240"
  },
  {
    "text": "shops so let's add one more Dimensions because just now we are talking about one moment",
    "start": "690240",
    "end": "695339"
  },
  {
    "text": "stuff so here is a Time series for this node so you will see the black line at the top is a no allocatable CPU which is",
    "start": "695339",
    "end": "702000"
  },
  {
    "text": "48 in constant and then the originally below is a pro requested CPU so assuming",
    "start": "702000",
    "end": "707519"
  },
  {
    "text": "no allocation happens at this time so it's also a constant line in orange color the red line is the actual Pro CPU",
    "start": "707519",
    "end": "714839"
  },
  {
    "text": "usage so we'll say it's spiking as a turbulating so originally is it's a blue",
    "start": "714839",
    "end": "720660"
  },
  {
    "text": "line actually it's the estimation value using some of the buffer okay so let's",
    "start": "720660",
    "end": "726000"
  },
  {
    "text": "take the area so in this graph so total green area is the available reclaimable",
    "start": "726000",
    "end": "731100"
  },
  {
    "text": "CPU over time so after knowing how the risk claimable resource is calculated so",
    "start": "731100",
    "end": "736560"
  },
  {
    "text": "let's say how the scheduling is impact for both product service so to make it compatible with the",
    "start": "736560",
    "end": "742620"
  },
  {
    "text": "current kubernetes we reduce will reduce the mechanism of the extended resource so the reclammable resource are",
    "start": "742620",
    "end": "749640"
  },
  {
    "text": "introduced as new is 20 resource so in this page they are named batch CPU",
    "start": "749640",
    "end": "754800"
  },
  {
    "text": "and batch memory accordingly let's say at one of the more they see at",
    "start": "754800",
    "end": "759899"
  },
  {
    "text": "one point the CPU and memory of support service start to burst so immediately",
    "start": "759899",
    "end": "765540"
  },
  {
    "text": "the agent will capture the utilization change it will calculate the new reclaimable resource and lastly you will",
    "start": "765540",
    "end": "772860"
  },
  {
    "text": "update to the node status so as a result you will see the batch",
    "start": "772860",
    "end": "777959"
  },
  {
    "text": "CPU at batch memory is reduced to 260 at 64 gigabyte",
    "start": "777959",
    "end": "783420"
  },
  {
    "text": "so for the kubernetes scheduler the update on the note is 20 resource will soon captured by the informers so it",
    "start": "783420",
    "end": "790740"
  },
  {
    "text": "will influence any subsequential scheduling decisions for the product service for Hadoop yarn co-hosted on the same",
    "start": "790740",
    "end": "797220"
  },
  {
    "text": "node it will slightly be more complicated and let's see how so firstly our agent will notify the co-hosted node",
    "start": "797220",
    "end": "804300"
  },
  {
    "text": "manager using some customized API for the resource change so in the example above the agent will tell the node",
    "start": "804300",
    "end": "811980"
  },
  {
    "text": "manager that the CPU is 6 6 to 16 CPUs and then no measure will report the",
    "start": "811980",
    "end": "818700"
  },
  {
    "text": "status to the yarn resource manager at the meantime assuming there is a job study the application method will make a",
    "start": "818700",
    "end": "825300"
  },
  {
    "text": "request to the resource manager so here we assume you requests for eight CPUs so the resource manager check if all of",
    "start": "825300",
    "end": "831839"
  },
  {
    "text": "the fulfill the requirements so luckily we can find the node a even though its",
    "start": "831839",
    "end": "837240"
  },
  {
    "text": "resource is claimed as a reclaimable resource is is still response is the",
    "start": "837240",
    "end": "843300"
  },
  {
    "text": "resource manager will still respond to the application method to say the node a is available so after receiving the",
    "start": "843300",
    "end": "849540"
  },
  {
    "text": "response the application method will be glad to see that a load is available so",
    "start": "849540",
    "end": "854760"
  },
  {
    "text": "you will tell the node manager that the node to run to to that it will tell the",
    "start": "854760",
    "end": "859860"
  },
  {
    "text": "application method that there's a note available and then you will to run some best jobs so cool so still here we",
    "start": "859860",
    "end": "866100"
  },
  {
    "text": "managed to allocate the batch service wait a minute so what just now what we",
    "start": "866100",
    "end": "872339"
  },
  {
    "text": "are talking about is just allocation so in previous slices we already allocate in our service at best service in the",
    "start": "872339",
    "end": "878700"
  },
  {
    "text": "same node by super greedy so what if the product service start bursting after the batch job has been scheduled and the",
    "start": "878700",
    "end": "886680"
  },
  {
    "text": "amount of the reclinable results certainly drops so under normal scenarios the kubernetes scheduler is",
    "start": "886680",
    "end": "894600"
  },
  {
    "text": "very important based on the real utilization so this is really scary so you'll say let's take a look on this",
    "start": "894600",
    "end": "901380"
  },
  {
    "text": "scenario so at the beginning some plot services with 20 requested CPU total is",
    "start": "901380",
    "end": "907139"
  },
  {
    "text": "scheduled to this node and then because the note has 48 CPU in total so by",
    "start": "907139",
    "end": "912180"
  },
  {
    "text": "gradually we allocate 30 cores of the best shop since a real utilization for the product service is just 12.",
    "start": "912180",
    "end": "919680"
  },
  {
    "text": "all of a sudden our campaign starts so a lot of users start to use our app and",
    "start": "919680",
    "end": "924839"
  },
  {
    "text": "then the real CPU usage for the product service jump to 38 cores so now we are",
    "start": "924839",
    "end": "930000"
  },
  {
    "text": "in big trouble because of seriously the poor machine with only 48 cores it",
    "start": "930000",
    "end": "935699"
  },
  {
    "text": "cannot support which is in total City 68 of total CPU usage so both of the",
    "start": "935699",
    "end": "942120"
  },
  {
    "text": "services will be infected will be affected and the user will start to notice that the Sharpie app is not",
    "start": "942120",
    "end": "948300"
  },
  {
    "text": "responsible anymore so intuitively we need some kind of solution to reduce the CPU usage of the",
    "start": "948300",
    "end": "954959"
  },
  {
    "text": "backshop immediately once support service needs a CPU so how do I achieve that okay I will",
    "start": "954959",
    "end": "962160"
  },
  {
    "text": "leave this challenge of expanding this to my teammate Irvin",
    "start": "962160",
    "end": "967040"
  },
  {
    "text": "okay thanks silent so in this section we'll look at how we can isolate noisy neighbors or in other words we'll",
    "start": "969360",
    "end": "975839"
  },
  {
    "text": "explore how we can minimize the effects caused by co-locating batch workloads on the same node as product services to",
    "start": "975839",
    "end": "981899"
  },
  {
    "text": "resolve the problem that Highland has Illustrated earlier so before I proceed any further we first",
    "start": "981899",
    "end": "988560"
  },
  {
    "text": "need to have a system to categorize workloads based on their latency requirements we found that the original kubernetes",
    "start": "988560",
    "end": "995040"
  },
  {
    "text": "Port Qs classes were insufficient to represent these latency requirements and",
    "start": "995040",
    "end": "1000199"
  },
  {
    "text": "so we extended them to introduce a few new more classes of workloads or what we call workload qos",
    "start": "1000199",
    "end": "1007100"
  },
  {
    "text": "in this table we show several different workload qos classes in order of priority basically product workloads",
    "start": "1007100",
    "end": "1013820"
  },
  {
    "text": "will have the highest priority in the system and they can unconditionally suppress all lower priority workload",
    "start": "1013820",
    "end": "1019220"
  },
  {
    "text": "classes so to reiterate in order to address the problem that highly mentioned earlier we",
    "start": "1019220",
    "end": "1026298"
  },
  {
    "text": "make use of several links kernel features so that we can ensure that batch workloads can give up their CPU",
    "start": "1026299",
    "end": "1032660"
  },
  {
    "text": "time to the higher priority product Services whenever they need it",
    "start": "1032660",
    "end": "1038480"
  },
  {
    "text": "so the unimi agent component that runs on every node is responsible for allocating many different kinds of",
    "start": "1038480",
    "end": "1043520"
  },
  {
    "text": "resources in real time to achieve this these include see group features such as CFS quota CPU weight memory limits and",
    "start": "1043520",
    "end": "1051020"
  },
  {
    "text": "so on as well as other rest control features including L3 cache and memory",
    "start": "1051020",
    "end": "1056299"
  },
  {
    "text": "bandwidth uh however in the interest of time we'll only be focusing on a few examples in today's sharing",
    "start": "1056299",
    "end": "1063440"
  },
  {
    "text": "in this example we can see how the C group hierarchy is typically set up so on the left we have a few ports under",
    "start": "1063440",
    "end": "1069799"
  },
  {
    "text": "the Q Port C group folder from the previous example let's say the real usage is only a measly 12 CPU cores and",
    "start": "1069799",
    "end": "1076700"
  },
  {
    "text": "after smoothing and adding a safety buffer we can round this up to 16 cores okay so the this actually leaves us with",
    "start": "1076700",
    "end": "1083120"
  },
  {
    "text": "a total of 32 reclaimable CPU cores that the best jobs can use and as Highland has explained earlier",
    "start": "1083120",
    "end": "1088700"
  },
  {
    "text": "how the best jobs can be allocated to the node we now have a new C group that we call batch at uh just under the",
    "start": "1088700",
    "end": "1094039"
  },
  {
    "text": "secret route and we can set the CFS quota as well the capu set to be 32 CPU cores as computed from the reclaable",
    "start": "1094039",
    "end": "1100400"
  },
  {
    "text": "CPUs earlier so let's say there is now a sudden spike in CPU usage for product services",
    "start": "1100400",
    "end": "1106580"
  },
  {
    "text": "so when the agent detects this it can immediately update the reclaimable resource to 5C blue cores and",
    "start": "1106580",
    "end": "1112220"
  },
  {
    "text": "subsequently update the better C group in real time to reduce the CFS quota and CPU set",
    "start": "1112220",
    "end": "1117380"
  },
  {
    "text": "correspondingly so that product Services can handle the sudden increase in user requests",
    "start": "1117380",
    "end": "1122419"
  },
  {
    "text": "so by changing the batch C group dynamically we are able actually able to achieve real-time suppression of batch",
    "start": "1122419",
    "end": "1128240"
  },
  {
    "text": "workloads in order to ensure that prod is always has a higher priority than batch okay",
    "start": "1128240",
    "end": "1135799"
  },
  {
    "text": "so what happens if there is a high CPU usage on the Node that persists over a long period of time so this might",
    "start": "1135799",
    "end": "1141860"
  },
  {
    "text": "actually cause the Hadoop jobs to not be able to complete if they were suppressed for a really long time",
    "start": "1141860",
    "end": "1146960"
  },
  {
    "text": "as such our agent will also evict batch workloads should their CPU requests exceed the amount of reclaimable",
    "start": "1146960",
    "end": "1152600"
  },
  {
    "text": "resources after an extended period of time so let's now do a deep dive into another",
    "start": "1152600",
    "end": "1158179"
  },
  {
    "text": "case study involving C group V2 writeback in our classes we sometimes observe that",
    "start": "1158179",
    "end": "1163760"
  },
  {
    "text": "some batch jobs May read and write files very quickly causing huge iops and affecting latency sensitive broad",
    "start": "1163760",
    "end": "1170000"
  },
  {
    "text": "services that also depend on the disk therefore we want to find a way that we can control i o limits for batch jobs",
    "start": "1170000",
    "end": "1176720"
  },
  {
    "text": "only in order to prevent them from causing side effects onto product Services whenever they were co-located onto the",
    "start": "1176720",
    "end": "1183200"
  },
  {
    "text": "same node conveniently both C group V1 and V2 provide interfaces for us to specify our",
    "start": "1183200",
    "end": "1190100"
  },
  {
    "text": "i o limits for a c group on C group V2 we can specify the i o max value in order to limit both the iops",
    "start": "1190100",
    "end": "1197240"
  },
  {
    "text": "and the bytes per second or BPS returned to the disk however",
    "start": "1197240",
    "end": "1202520"
  },
  {
    "text": "is it really that simple though as you might expect the answer is no",
    "start": "1202520",
    "end": "1208400"
  },
  {
    "text": "so the main issue is that configuring IO Max will throttle both direct and buffered i o in secret V2",
    "start": "1208400",
    "end": "1214640"
  },
  {
    "text": "this means that this actually controls the rate of right back of 30 pages in for the C group",
    "start": "1214640",
    "end": "1220160"
  },
  {
    "text": "so if batch jobs continuously write files faster than the configured i o Max this actually can result in 30 pages",
    "start": "1220160",
    "end": "1225919"
  },
  {
    "text": "piling up over time and eventually this will reach result in a huge memory pressure for the whole system",
    "start": "1225919",
    "end": "1231679"
  },
  {
    "text": "and as we all know and having not enough free memory is a really bad thing and this can actually result in product",
    "start": "1231679",
    "end": "1237260"
  },
  {
    "text": "containers starts start stalling whenever memory is reclaimed from the product containers instead therefore",
    "start": "1237260",
    "end": "1243440"
  },
  {
    "text": "naively setting IO Max might actually result in an even worse situation than before so what can we do about this",
    "start": "1243440",
    "end": "1250160"
  },
  {
    "text": "oh sorry let me show you first an example of how setting IO Max can go wrong on the top we we set iomax to 100 Mbps",
    "start": "1250160",
    "end": "1258860"
  },
  {
    "text": "for batch C group and at the same time we also have a prod C group reading 1kb",
    "start": "1258860",
    "end": "1264919"
  },
  {
    "text": "so when the batch C group writes a lot of files or writes a lot of data to the disk the prod C group might stall for as",
    "start": "1264919",
    "end": "1271100"
  },
  {
    "text": "long as just as as long as 46 seconds just to write one kilobyte so this you can see it's a actually it's quite a",
    "start": "1271100",
    "end": "1277700"
  },
  {
    "text": "rather serious thing so we need another approach to solve our original problem so rather than letting",
    "start": "1277700",
    "end": "1283580"
  },
  {
    "text": "30 pages generated by batch C Roots power up forever we need to make the batch C group sleep once they hit a",
    "start": "1283580",
    "end": "1288919"
  },
  {
    "text": "maximum dirty limit since I O Max limits how fast 30 pages are flush uh together we can use the the",
    "start": "1288919",
    "end": "1297260"
  },
  {
    "text": "maximum Dot Page size per C group plus IO Max together to resolve the original issue",
    "start": "1297260",
    "end": "1303080"
  },
  {
    "text": "so in order to achieve this we are currently working with our in-house Linux kernel team at shopee to implement",
    "start": "1303080",
    "end": "1308419"
  },
  {
    "text": "the first part to implement patches for 30 page size limits per C group",
    "start": "1308419",
    "end": "1313700"
  },
  {
    "text": "and I'd like to pass on the time to Highland who will share more about how we managed to evaluate the safety and",
    "start": "1313700",
    "end": "1319220"
  },
  {
    "text": "effectiveness of rolling out core location in our company okay so thank you Arvin so after",
    "start": "1319220",
    "end": "1326480"
  },
  {
    "text": "implementing and rolling our collocation in our cluster so even though we reclaim",
    "start": "1326480",
    "end": "1332179"
  },
  {
    "text": "lots of resources to use for best shop we didn't really low wiser it's safe for",
    "start": "1332179",
    "end": "1337520"
  },
  {
    "text": "the building is so as such we want to measure some kind of service level objective or we call slo's so that we",
    "start": "1337520",
    "end": "1345620"
  },
  {
    "text": "can control the risk after introducing collocation so the first step is to collect enough",
    "start": "1345620",
    "end": "1350659"
  },
  {
    "text": "metrics we scrap messages from three three main areas so there are no matches",
    "start": "1350659",
    "end": "1356059"
  },
  {
    "text": "workload mattress and business mattress these three metrics uh cover some",
    "start": "1356059",
    "end": "1361220"
  },
  {
    "text": "machine status container status and also the user program status with this metrics on hand we can start",
    "start": "1361220",
    "end": "1368840"
  },
  {
    "text": "to generate useful alerts when the SLO are violated what else can we do with the metrics",
    "start": "1368840",
    "end": "1375380"
  },
  {
    "text": "that we collected where you introduce another way to fix problematic machines automatically So within our agent we",
    "start": "1375380",
    "end": "1383480"
  },
  {
    "text": "introduce a negative feedback loop which is so-called performance guards so the",
    "start": "1383480",
    "end": "1388520"
  },
  {
    "text": "performance cost controllers were read from the in-memory Matrix stores the controller will check if the SLO is",
    "start": "1388520",
    "end": "1395059"
  },
  {
    "text": "violated and then generate some using record so-called prescriptions which can be applied to save group to do isolation",
    "start": "1395059",
    "end": "1403039"
  },
  {
    "text": "on resources quickly and without need any human interventions so after the overall globalist has been",
    "start": "1403039",
    "end": "1410419"
  },
  {
    "text": "recovered the controller will also detected and gradually relax the isolation over time",
    "start": "1410419",
    "end": "1417020"
  },
  {
    "text": "so behind besides of the self self healing this Matrix can be also",
    "start": "1417020",
    "end": "1423020"
  },
  {
    "text": "introduced contribute to schedulings so metrics collected from different clusters are merged together into a",
    "start": "1423020",
    "end": "1429440"
  },
  {
    "text": "global hives tables we have some spark job running periodically to calculate",
    "start": "1429440",
    "end": "1434780"
  },
  {
    "text": "and analyze the Matrix and then you write to an aggregated data store so the",
    "start": "1434780",
    "end": "1440179"
  },
  {
    "text": "analyze is mainly focus on calculating on the persona for all of the services including both our batch services",
    "start": "1440179",
    "end": "1447860"
  },
  {
    "text": "this personal statistics will be read by some of the scheduler plugins inside the",
    "start": "1447860",
    "end": "1453140"
  },
  {
    "text": "cluster and to generate more comprehensive affinities so for example",
    "start": "1453140",
    "end": "1458179"
  },
  {
    "text": "we can avoid scheduled Services which will burst at the same time to the same machine and also we can avoid IO bug",
    "start": "1458179",
    "end": "1466580"
  },
  {
    "text": "services to be scheduled to the same machine we can also influence the scheduling",
    "start": "1466580",
    "end": "1471980"
  },
  {
    "text": "using other math business and low level metrics yeah but because of the limit of time we won't introduce today",
    "start": "1471980",
    "end": "1479179"
  },
  {
    "text": "so let's go to the result part so it's a moment we have been waiting for so let's",
    "start": "1479179",
    "end": "1484400"
  },
  {
    "text": "see all of the Power we're putting has been paid off so let's work on Irvin",
    "start": "1484400",
    "end": "1491200"
  },
  {
    "text": "thanks Eileen so let me go through some of the results that we have seen as a result of the hard work that we have put",
    "start": "1491200",
    "end": "1496400"
  },
  {
    "text": "in over the past few months actually over the past year so after enabling co-location in our",
    "start": "1496400",
    "end": "1501559"
  },
  {
    "text": "clusters we got some really promising results firstly we were able to reclaim large portions of unused resources that",
    "start": "1501559",
    "end": "1508220"
  },
  {
    "text": "were sitting idle during low periods uh to provide for Hadoop young jobs and other low priority tasks providing the",
    "start": "1508220",
    "end": "1515659"
  },
  {
    "text": "much needed resources to teams that actually needed them the most these are actually amounted to more than",
    "start": "1515659",
    "end": "1520700"
  },
  {
    "text": "70 as you can see represented in green uh more than 70 of CPUs that were",
    "start": "1520700",
    "end": "1526159"
  },
  {
    "text": "reclaimed and could potentially be reused for best jobs every day",
    "start": "1526159",
    "end": "1531500"
  },
  {
    "text": "across the Clusters where co-location is enabled we also track the CPU utilization rate across the Clusters as",
    "start": "1531500",
    "end": "1537440"
  },
  {
    "text": "a means to measure how efficiently resources are allocated so when comparing non-colication classes",
    "start": "1537440",
    "end": "1543919"
  },
  {
    "text": "and collocation clusters we managed to improve the peak CPU utilization by up to 4.3 times as well as the average CPU",
    "start": "1543919",
    "end": "1550220"
  },
  {
    "text": "utilization improving by up to 3.2 times we also want to ensure that such optimizations did not cause any adverse",
    "start": "1550220",
    "end": "1557840"
  },
  {
    "text": "side effects to the product Services when co-located onto the same node we compared the latencies for some",
    "start": "1557840",
    "end": "1563480"
  },
  {
    "text": "critical RPC services that were deployed on both co-location and non-colocation nodes and we observed that the product",
    "start": "1563480",
    "end": "1570200"
  },
  {
    "text": "services co-located with bad jobs had less than five percent impact in the P99",
    "start": "1570200",
    "end": "1575900"
  },
  {
    "text": "tail latencies so this is actually one such example to demonstrate that our resource isolation is effective enough",
    "start": "1575900",
    "end": "1582200"
  },
  {
    "text": "to prevent batch drops from impacting critical product Services whenever they need their resource the most",
    "start": "1582200",
    "end": "1587960"
  },
  {
    "text": "and lastly we also measured the overall job failure rate for Hadoop yarn jobs that ran on our co-located kubernetes",
    "start": "1587960",
    "end": "1594020"
  },
  {
    "text": "clusters and we found that less than one percent of such jobs failed every day which shows that our co-location system is",
    "start": "1594020",
    "end": "1600980"
  },
  {
    "text": "indeed sufficiently stable even though these groups these drops sorry could potentially be evicted or suppressed at",
    "start": "1600980",
    "end": "1607520"
  },
  {
    "text": "any time now how does this uh all of how does all of this translate into cost savings",
    "start": "1607520",
    "end": "1613760"
  },
  {
    "text": "so of course every company's compute costs and they'll be different uh",
    "start": "1613760",
    "end": "1619220"
  },
  {
    "text": "depending on their cloud provider and many many different other factors but let me just make an attempt to make a",
    "start": "1619220",
    "end": "1624380"
  },
  {
    "text": "back of the napkin calculation over here so assuming we managed to reclaim eight cores per machine in a 5000 node cluster",
    "start": "1624380",
    "end": "1631460"
  },
  {
    "text": "we can estimate how much this would have potentially cost whenever if let's say we're using AWS ec2 spot instances",
    "start": "1631460",
    "end": "1638000"
  },
  {
    "text": "instead uh this this translates to Savings of up to 4.2 million USD per year for every 8",
    "start": "1638000",
    "end": "1645500"
  },
  {
    "text": "CPU CPU course reclaim per node and this figure can actually go even higher depending on how underutilized your",
    "start": "1645500",
    "end": "1651980"
  },
  {
    "text": "clusters actually are as you saw earlier we actually reclaim more than 70 of our CPU cores and also depends on like the",
    "start": "1651980",
    "end": "1658760"
  },
  {
    "text": "total number of clusters you have as well as the original cost price of Hadoop machines",
    "start": "1658760",
    "end": "1664100"
  },
  {
    "text": "so before we end today's sharing I'd like to quickly share some takeaways that we learned throughout our journey in designing and rolling out the",
    "start": "1664100",
    "end": "1670340"
  },
  {
    "text": "co-location system in our company for the past year firstly we knew we had to design a",
    "start": "1670340",
    "end": "1675919"
  },
  {
    "text": "system that could scale to multiple clusters across tens of thousands of nodes we chose to adopt a bottom-up",
    "start": "1675919",
    "end": "1681380"
  },
  {
    "text": "design such that most components could act in a decentralized manner for example we implemented an in-memory",
    "start": "1681380",
    "end": "1686900"
  },
  {
    "text": "Lister Watcher implementation that allows us to gain the benefits of using crds in our code without potentially",
    "start": "1686900",
    "end": "1693860"
  },
  {
    "text": "overloading a centralized API server so such a design actually allowed us to reach more than 50 000 write operations",
    "start": "1693860",
    "end": "1700100"
  },
  {
    "text": "per second in a single cluster which would not have been possible with the native API server of the box",
    "start": "1700100",
    "end": "1705980"
  },
  {
    "text": "next we also learned many ways to mitigate and control the risk of rollout system that is complex yet potentially",
    "start": "1705980",
    "end": "1711919"
  },
  {
    "text": "risky to the business as the co-location system so we knew that modifying low level C",
    "start": "1711919",
    "end": "1717740"
  },
  {
    "text": "group and kernel parameters were risky from the very beginning and that these",
    "start": "1717740",
    "end": "1723140"
  },
  {
    "text": "risks can and will be vastly Amplified when deploying to even just a really small change to thousands and thousands",
    "start": "1723140",
    "end": "1729200"
  },
  {
    "text": "of machines as such we have refined and come up with a very comprehensive risk classification",
    "start": "1729200",
    "end": "1734240"
  },
  {
    "text": "system to get coupled together with strict release policies that all developers in the team must adhere to",
    "start": "1734240",
    "end": "1740600"
  },
  {
    "text": "uh we also heavily invested in release automation for our project we have up to",
    "start": "1740600",
    "end": "1745700"
  },
  {
    "text": "four stages in the release process and the higher the risk of the rollout the longer the entire process which could take several weeks per rollout",
    "start": "1745700",
    "end": "1752900"
  },
  {
    "text": "these are all automated with custom controllers and command line tools that are used within our team to minimize",
    "start": "1752900",
    "end": "1758779"
  },
  {
    "text": "human errors and lastly we also want to share some several takeaways in terms of",
    "start": "1758779",
    "end": "1764000"
  },
  {
    "text": "observability we learned to capture as many metrics as possible from low level metrics like",
    "start": "1764000",
    "end": "1769700"
  },
  {
    "text": "using ebpf tracers to capture several kernel functions like direct reclaimed",
    "start": "1769700",
    "end": "1774740"
  },
  {
    "text": "latency all the way to high level metrics that the business actually cares about",
    "start": "1774740",
    "end": "1779899"
  },
  {
    "text": "we also found that a good way to measure the effectiveness of our project was to use something that we call the co-location cost",
    "start": "1779899",
    "end": "1786200"
  },
  {
    "text": "basically by comparing the metrics for the same service on both co-location and non-colocation machines we can actually",
    "start": "1786200",
    "end": "1791960"
  },
  {
    "text": "get a very accurate estimation of the impact that code location can cost to this uh this particular service",
    "start": "1791960",
    "end": "1798860"
  },
  {
    "text": "so this actually greatly helps us to evaluate risks and identify problems uh before causing widespread problems to",
    "start": "1798860",
    "end": "1804980"
  },
  {
    "text": "the whole cluster and to the business so with that I'd like to end today's sharing and you can scan the QR code to",
    "start": "1804980",
    "end": "1811039"
  },
  {
    "text": "leave feedback on today's session if you might have and will open the floor to any questions thank you",
    "start": "1811039",
    "end": "1817880"
  },
  {
    "text": "thank you",
    "start": "1817880",
    "end": "1820000"
  },
  {
    "text": "sorry",
    "start": "1826460",
    "end": "1828940"
  },
  {
    "text": "hi um did you ever test spark on kubernetes",
    "start": "1831980",
    "end": "1838179"
  },
  {
    "text": "with the in your in your presentation you showed",
    "start": "1838179",
    "end": "1846020"
  },
  {
    "text": "us combining online workloads with spark on yarn but did you",
    "start": "1846020",
    "end": "1851720"
  },
  {
    "text": "consider running it with spark on kubernetes the resource manager",
    "start": "1851720",
    "end": "1858039"
  },
  {
    "text": "your question is uh whether we use a spark on kubernetes to do collocation right",
    "start": "1860200",
    "end": "1867100"
  },
  {
    "text": "if you sort of using spargor kubernetes because in your example you showed us",
    "start": "1867500",
    "end": "1875740"
  },
  {
    "text": "oh yeah yeah actually we choose yarn is because uh in our company the DI data",
    "start": "1875860",
    "end": "1881960"
  },
  {
    "text": "infrastructure team they are choose to use yarn that's why we are not able to",
    "start": "1881960",
    "end": "1887179"
  },
  {
    "text": "use the uh I mean spot on kubernetes but actually we indeed run some spot inside",
    "start": "1887179",
    "end": "1893419"
  },
  {
    "text": "the yeah node manager not sure if this answer your questions oh actually I can try to",
    "start": "1893419",
    "end": "1899419"
  },
  {
    "text": "I can try to follow up on the question as well so our system is actually able to support both kubernetes and",
    "start": "1899419",
    "end": "1905059"
  },
  {
    "text": "non-cubernetes workloads natively so in this example we show that we don't actually need to modify our existing uh",
    "start": "1905059",
    "end": "1911960"
  },
  {
    "text": "teams uh Technologies they currently only use yarn so needing them to migrate to using spark kubernetes could involve",
    "start": "1911960",
    "end": "1918440"
  },
  {
    "text": "a lot more costs and a lot more uh time invested into the project so basically",
    "start": "1918440",
    "end": "1923960"
  },
  {
    "text": "we're able to support um if you saw we actually have",
    "start": "1923960",
    "end": "1929179"
  },
  {
    "text": "um oh sorry I",
    "start": "1929179",
    "end": "1932320"
  },
  {
    "text": "so on this slide actually you can see that we are able to support both our kubernetes ports using the extended",
    "start": "1937940",
    "end": "1943220"
  },
  {
    "text": "Resource as well as a non-uh non-kubernetes workloads but we need to make a slight modification in the node",
    "start": "1943220",
    "end": "1949279"
  },
  {
    "text": "one node manager and the whole yarn architecture which is basically just to implement a customized API so I'm not",
    "start": "1949279",
    "end": "1954980"
  },
  {
    "text": "sure if this answers the question okay well it's okay thank you thank you",
    "start": "1954980",
    "end": "1960880"
  },
  {
    "text": "are you supporting also a presto or trino DB in that and also are you then",
    "start": "1967700",
    "end": "1974179"
  },
  {
    "text": "using hdfs mounted on every node of the kubernetes cluster",
    "start": "1974179",
    "end": "1979580"
  },
  {
    "text": "okay so your question is whether we're mounted hdfs on every node inside the kubernetes cluster so actually the",
    "start": "1979580",
    "end": "1985820"
  },
  {
    "text": "answer is no so we use a remote uh remote hdfs to achieve that yeah and",
    "start": "1985820",
    "end": "1992059"
  },
  {
    "text": "then also we are adapting some technique called RSS for remote Shuffle inside our cluster to reduce the io course yeah",
    "start": "1992059",
    "end": "2000039"
  },
  {
    "text": "uh then for the question about the Play Store actually we are integrating with the crystal worker inside our cluster so",
    "start": "2000039",
    "end": "2006820"
  },
  {
    "text": "actually because of the Play Store you have a nature of consuming large amount of memory and because of the latency it",
    "start": "2006820",
    "end": "2013179"
  },
  {
    "text": "cannot release of the because it claims the memory using the nominal claims of",
    "start": "2013179",
    "end": "2018220"
  },
  {
    "text": "memory as anomalous memory so it means means it cannot it's not paid in page cache so means that if memory cannot",
    "start": "2018220",
    "end": "2023860"
  },
  {
    "text": "release quickly so we are using our scheduling mechanism to find a stable",
    "start": "2023860",
    "end": "2028899"
  },
  {
    "text": "memory among the reclaimable resource for to run to a mixture we can collocate",
    "start": "2028899",
    "end": "2035500"
  },
  {
    "text": "the Play Store worker I mean with capabilities yeah",
    "start": "2035500",
    "end": "2040559"
  },
  {
    "text": "guys I really like the metrics and slas that you presented about how you are",
    "start": "2043720",
    "end": "2049780"
  },
  {
    "text": "measuring the efficiency of kubernetes so I wanted to ask if I want to learn",
    "start": "2049780",
    "end": "2055780"
  },
  {
    "text": "more about how the metrics and Essay days are structured and how it's implemented can you make any",
    "start": "2055780",
    "end": "2061720"
  },
  {
    "text": "recommendation how can we learn more about the metrics",
    "start": "2061720",
    "end": "2066720"
  },
  {
    "text": "um okay so thanks for your question so um I think we have several kinds of metrics",
    "start": "2067000",
    "end": "2072460"
  },
  {
    "text": "as a rep shown earlier let me try to",
    "start": "2072460",
    "end": "2076560"
  },
  {
    "text": "yeah so actually we split our metrics into three different kinds so uh firstly we have low level metrics that are on",
    "start": "2078220",
    "end": "2084099"
  },
  {
    "text": "every single machine and we also have workload metrics that uh maybe it's like CPU utilization",
    "start": "2084099",
    "end": "2090460"
  },
  {
    "text": "um LG cache uh utilization per C group or per process as well as some high",
    "start": "2090460",
    "end": "2095500"
  },
  {
    "text": "level business metrics for example like latency server rates that the business actually cares about so we the the point",
    "start": "2095500",
    "end": "2102760"
  },
  {
    "text": "we're trying to make is that we actually collect a whole slew of metrics all the way from low level to high level in order to make a much more comprehensive",
    "start": "2102760",
    "end": "2108520"
  },
  {
    "text": "and effective um evaluation on uh how we should roll",
    "start": "2108520",
    "end": "2114220"
  },
  {
    "text": "out as well as to do certain automated uh recoveries as like Eileen has",
    "start": "2114220",
    "end": "2119859"
  },
  {
    "text": "mentioned earlier yeah I'm not sure if this answers your question",
    "start": "2119859",
    "end": "2124680"
  },
  {
    "text": "any more questions hi",
    "start": "2128079",
    "end": "2135579"
  },
  {
    "text": "um just to kind of clarify is this all like internal tooling that you guys to develop like it's not um out there for",
    "start": "2137200",
    "end": "2144220"
  },
  {
    "text": "General usage and also does it work with like any batch job that might be running",
    "start": "2144220",
    "end": "2150760"
  },
  {
    "text": "uh yeah so actually at the moment this is all uh internal system with our company but uh I think it also depends",
    "start": "2150760",
    "end": "2158500"
  },
  {
    "text": "on uh quite a lot of uh uh the situation in the company for example if you",
    "start": "2158500",
    "end": "2164320"
  },
  {
    "text": "currently use uh the yarn architecture with and we also need to depend on certain like kernel features as",
    "start": "2164320",
    "end": "2170680"
  },
  {
    "text": "mentioned that need to be developed within the company so uh but that's it actually a lot of the code and our agent",
    "start": "2170680",
    "end": "2177579"
  },
  {
    "text": "is actually able to be open source but I'm not sure if that's in our roadmap for this year or in anytime soon and to",
    "start": "2177579",
    "end": "2184000"
  },
  {
    "text": "answer the second part of your question as to whether any batch workloads can be supported theoretically yes but uh",
    "start": "2184000",
    "end": "2190060"
  },
  {
    "text": "depending on the nature of the batch works workloads for example like trino or Presto workloads which actually Reserve high amounts of uh mapped",
    "start": "2190060",
    "end": "2197859"
  },
  {
    "text": "Anonymous memory uh we actually need to mix certain maybe more long-term",
    "start": "2197859",
    "end": "2203380"
  },
  {
    "text": "predictions and to provide more stable memory stable reclaimed memory for the uh for these batch workloads so it",
    "start": "2203380",
    "end": "2210460"
  },
  {
    "text": "really depends on Case by case basis yeah",
    "start": "2210460",
    "end": "2214440"
  },
  {
    "text": "yeah I'm sorry I already asked a question but this question actually made me like expand and what I also wanted to",
    "start": "2220300",
    "end": "2227260"
  },
  {
    "text": "ask you is the implementation and like how are the metrics implemented are you implementing it internally or are you",
    "start": "2227260",
    "end": "2234400"
  },
  {
    "text": "using any publicly available tools like data dog or other tools for measuring",
    "start": "2234400",
    "end": "2239500"
  },
  {
    "text": "the data points oh I see okay so um maybe for some of the low level metrics um these are",
    "start": "2239500",
    "end": "2246400"
  },
  {
    "text": "mostly collected from like kernel uh basically a Linux kernel interfaces for",
    "start": "2246400",
    "end": "2251920"
  },
  {
    "text": "example if we're reading from the proc FS so uh those actually are collected by",
    "start": "2251920",
    "end": "2257760"
  },
  {
    "text": "it's actually the same way that things like node exporter actually collects and uh for SQL metrics these are also",
    "start": "2257760",
    "end": "2264700"
  },
  {
    "text": "collected uh in the same way that c advisor would do it so um for the business metrics these are actually",
    "start": "2264700",
    "end": "2270400"
  },
  {
    "text": "mostly collected by our in-house observability team and um I guess you can say like it's it's more similar to",
    "start": "2270400",
    "end": "2276940"
  },
  {
    "text": "what uh what data offers I guess yeah so I I guess that's how we actually split",
    "start": "2276940",
    "end": "2282099"
  },
  {
    "text": "the different kind of ways of collecting the metrics based on um what kind of metrics they are okay",
    "start": "2282099",
    "end": "2287740"
  },
  {
    "text": "thank you so much thank you",
    "start": "2287740",
    "end": "2292260"
  },
  {
    "text": "very insightful presentation thank you if you could a little bit a little",
    "start": "2293260",
    "end": "2299040"
  },
  {
    "text": "elaborate on uh did you need to do any modifications to the source code of",
    "start": "2299040",
    "end": "2304300"
  },
  {
    "text": "kubernetes or yarn how does your agent you know interface with these two different schedulers",
    "start": "2304300",
    "end": "2310599"
  },
  {
    "text": "okay thanks for the question that's actually a really insightful question so as for modifying the kubernetes source",
    "start": "2310599",
    "end": "2317740"
  },
  {
    "text": "code actually we minimize and actually we don't need any modification to kubernetes so Asian actually in a way we",
    "start": "2317740",
    "end": "2323980"
  },
  {
    "text": "replace our cubelets CPU manager and memory manager we basically extend all",
    "start": "2323980",
    "end": "2329260"
  },
  {
    "text": "the logic and add on our own Logic on top of it and we basically convert the cubelet CPU manager policy to none so",
    "start": "2329260",
    "end": "2335740"
  },
  {
    "text": "that we can actually take over control of the C group for all parts and for modifications to yarn because as",
    "start": "2335740",
    "end": "2342280"
  },
  {
    "text": "mentioned for certain non-kubernetes workloads we need to adapt a certain framework slightly so in our case we",
    "start": "2342280",
    "end": "2349119"
  },
  {
    "text": "modified node manager just by adding a single API to so that we can actually change the amount of available vcores",
    "start": "2349119",
    "end": "2355720"
  },
  {
    "text": "CPUs for the known uh in real time so that's actually just a very minimal change",
    "start": "2355720",
    "end": "2361720"
  },
  {
    "text": "just to add down so for our schedule system we are using a dedicated",
    "start": "2361720",
    "end": "2366960"
  },
  {
    "text": "scheduler plugin which is also didn't modify the scheduler code so inside that",
    "start": "2366960",
    "end": "2372579"
  },
  {
    "text": "scheduled parking we mimic the schedule cache to connect to our data store and",
    "start": "2372579",
    "end": "2377680"
  },
  {
    "text": "then to catch all of the prediction data and then which can be used for yeah all",
    "start": "2377680",
    "end": "2382780"
  },
  {
    "text": "of the following allocations",
    "start": "2382780",
    "end": "2386680"
  },
  {
    "text": "hey don't show agent adding additional latency to the cubelet and to the node",
    "start": "2389619",
    "end": "2396099"
  },
  {
    "text": "manager at all I would say no because actually",
    "start": "2396099",
    "end": "2401859"
  },
  {
    "text": "um we are basically acting we are actually modifying the C Group after the container is actually created and ready",
    "start": "2401859",
    "end": "2408339"
  },
  {
    "text": "so if the C group already exists on the machine after that then we would modify any necessary SQL parameters so actually",
    "start": "2408339",
    "end": "2415599"
  },
  {
    "text": "it's not in the critical path of creating a port or anything like that we actually modify it maybe like less than",
    "start": "2415599",
    "end": "2422260"
  },
  {
    "text": "a second after it's created so it's actually not in the in the way of that for node manager it's uh we also don't",
    "start": "2422260",
    "end": "2429400"
  },
  {
    "text": "add any latency to that because we just do a best effort update of the amount of",
    "start": "2429400",
    "end": "2434980"
  },
  {
    "text": "reclaimable resource to no manager whenever we can if let's say the API fails for whatever reason we actually",
    "start": "2434980",
    "end": "2441579"
  },
  {
    "text": "can fall back onto real-time suppression like using CFS quota as mentioned without causing any adverse effects to",
    "start": "2441579",
    "end": "2447280"
  },
  {
    "text": "the product Services still hey thanks for the wonderful",
    "start": "2447280",
    "end": "2454060"
  },
  {
    "text": "presentation quick question on this Slide the aggregation and feeding the data into the cube scheduler",
    "start": "2454060",
    "end": "2461079"
  },
  {
    "text": "are you thinking in the future that you will be doing in real time",
    "start": "2461079",
    "end": "2466260"
  },
  {
    "text": "I think it won't be used for real time because the prediction actually is used as a long-term petition as in actually",
    "start": "2466300",
    "end": "2473260"
  },
  {
    "text": "we have two kinds of predictions so actually the agent itself has a short-term prediction which will know uh",
    "start": "2473260",
    "end": "2479680"
  },
  {
    "text": "which will introduced in your slides as a smoother for the real utilization uh",
    "start": "2479680",
    "end": "2486220"
  },
  {
    "text": "as in as an uploads the real upload the estimation to as a reasonable resource",
    "start": "2486220",
    "end": "2491740"
  },
  {
    "text": "and then the Predator here which is used for long-term uh prediction I use some uh strategies like the fully transform",
    "start": "2491740",
    "end": "2498700"
  },
  {
    "text": "uh so it will so you will want to find a stable I mean",
    "start": "2498700",
    "end": "2504420"
  },
  {
    "text": "stable memory or stable CPU for a few hours uh so as such which needs a lot of",
    "start": "2504420",
    "end": "2511780"
  },
  {
    "text": "data and then because we have so many services inside the cluster it's not possible for us to do it in a very",
    "start": "2511780",
    "end": "2518500"
  },
  {
    "text": "quickly or say in your real-time way yeah and then because uh as in because in our design of the agent the agent is",
    "start": "2518500",
    "end": "2524980"
  },
  {
    "text": "designed to I mean to will react fastly to do separation and adjusting resources",
    "start": "2524980",
    "end": "2530859"
  },
  {
    "text": "to allocation isolations I mean do isolations quickly so the schedule itself uh don't have to take care of",
    "start": "2530859",
    "end": "2537820"
  },
  {
    "text": "those things okay thank you",
    "start": "2537820",
    "end": "2542460"
  },
  {
    "text": "no more questions thank you",
    "start": "2546760",
    "end": "2551400"
  },
  {
    "text": "[Applause] [Music]",
    "start": "2551990",
    "end": "2557689"
  }
]