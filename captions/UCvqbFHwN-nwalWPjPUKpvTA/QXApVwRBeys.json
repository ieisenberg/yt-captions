[
  {
    "start": "0",
    "end": "198000"
  },
  {
    "text": "all right hello everyone the title of",
    "start": "30",
    "end": "2970"
  },
  {
    "text": "this talk is a dip kubernetes make my",
    "start": "2970",
    "end": "5130"
  },
  {
    "text": "p95 worse quick show of hands how many",
    "start": "5130",
    "end": "8340"
  },
  {
    "text": "of you think the answer is yes all right",
    "start": "8340",
    "end": "11040"
  },
  {
    "text": "cool that's like half of you who are we",
    "start": "11040",
    "end": "16320"
  },
  {
    "text": "hi I'm Jen I'm on a service",
    "start": "16320",
    "end": "19470"
  },
  {
    "text": "orchestration team at Airbnb and we",
    "start": "19470",
    "end": "21510"
  },
  {
    "text": "provide tools and guidance for engineers",
    "start": "21510",
    "end": "23340"
  },
  {
    "text": "to configure and operate services of",
    "start": "23340",
    "end": "25170"
  },
  {
    "text": "scale so here's a quick outline of what",
    "start": "25170",
    "end": "46050"
  },
  {
    "text": "we're gonna go through today so we're",
    "start": "46050",
    "end": "48300"
  },
  {
    "text": "gonna start with a brief introduction",
    "start": "48300",
    "end": "50340"
  },
  {
    "text": "about",
    "start": "50340",
    "end": "52940"
  },
  {
    "text": "hello awesome so we're gonna start with",
    "start": "87330",
    "end": "95740"
  },
  {
    "text": "an introduction of how we migrated",
    "start": "95740",
    "end": "98140"
  },
  {
    "text": "kubernetes onto communities urban bee",
    "start": "98140",
    "end": "100210"
  },
  {
    "text": "then we'll dive into some specific",
    "start": "100210",
    "end": "101620"
  },
  {
    "text": "interesting cases of potential",
    "start": "101620",
    "end": "103630"
  },
  {
    "text": "performance for questions on the",
    "start": "103630",
    "end": "105340"
  },
  {
    "text": "services that we migrated and finally",
    "start": "105340",
    "end": "106660"
  },
  {
    "text": "we'll go through some of the lessons",
    "start": "106660",
    "end": "107670"
  },
  {
    "text": "okay so this is mostly about kubernetes",
    "start": "107670",
    "end": "111790"
  },
  {
    "text": "but also a little bit about containers",
    "start": "111790",
    "end": "113290"
  },
  {
    "text": "that Airbnb so to set some context we",
    "start": "113290",
    "end": "117850"
  },
  {
    "text": "had a lot of services that used to be",
    "start": "117850",
    "end": "120640"
  },
  {
    "text": "running on ec2 on their own individual",
    "start": "120640",
    "end": "124420"
  },
  {
    "text": "instances so when we migrated onto",
    "start": "124420",
    "end": "126640"
  },
  {
    "text": "kubernetes we migrated onto both",
    "start": "126640",
    "end": "128619"
  },
  {
    "text": "containers and kubernetes so that I'll",
    "start": "128619",
    "end": "130479"
  },
  {
    "text": "play out in some of the specific",
    "start": "130479",
    "end": "132340"
  },
  {
    "text": "examples so here's a graph of our",
    "start": "132340",
    "end": "136570"
  },
  {
    "text": "migration so production migration",
    "start": "136570",
    "end": "141430"
  },
  {
    "text": "started in earnest in the start of 2018",
    "start": "141430",
    "end": "144190"
  },
  {
    "text": "and as of September 2019 we're at around",
    "start": "144190",
    "end": "147989"
  },
  {
    "text": "looks like a thousand services here so",
    "start": "147989",
    "end": "151360"
  },
  {
    "text": "we've we've seen a lot of migrations and",
    "start": "151360",
    "end": "154590"
  },
  {
    "text": "got a few interesting cases and here's a",
    "start": "154590",
    "end": "158380"
  },
  {
    "text": "little detail a few details about our",
    "start": "158380",
    "end": "160390"
  },
  {
    "text": "environment that will come out later so",
    "start": "160390",
    "end": "161950"
  },
  {
    "text": "we're running on Amazon the next two on",
    "start": "161950",
    "end": "164560"
  },
  {
    "text": "our minyan instances we're running",
    "start": "164560",
    "end": "167590"
  },
  {
    "text": "Ubuntu images and we're for our CNI",
    "start": "167590",
    "end": "171430"
  },
  {
    "text": "plugin we're using Canal which is a",
    "start": "171430",
    "end": "173019"
  },
  {
    "text": "combination of calico and from that",
    "start": "173019",
    "end": "174250"
  },
  {
    "text": "flannel we're making pretty heavy usage",
    "start": "174250",
    "end": "178480"
  },
  {
    "text": "of new port services as a integration",
    "start": "178480",
    "end": "181420"
  },
  {
    "text": "point with our in-house service",
    "start": "181420",
    "end": "184239"
  },
  {
    "text": "discovery system smart stack and for our",
    "start": "184239",
    "end": "186910"
  },
  {
    "text": "specific application services we've got",
    "start": "186910",
    "end": "190000"
  },
  {
    "text": "many different languages a lot of them",
    "start": "190000",
    "end": "192100"
  },
  {
    "text": "are in Ruby and Java but we also got",
    "start": "192100",
    "end": "194440"
  },
  {
    "text": "Python and go alright I'm gonna start",
    "start": "194440",
    "end": "201340"
  },
  {
    "start": "198000",
    "end": "258000"
  },
  {
    "text": "talking about the problems then so",
    "start": "201340",
    "end": "203769"
  },
  {
    "text": "during this graph right there are both",
    "start": "203769",
    "end": "206830"
  },
  {
    "text": "that lots of new services created in",
    "start": "206830",
    "end": "208989"
  },
  {
    "text": "kubernetes and containers first but",
    "start": "208989",
    "end": "210670"
  },
  {
    "text": "there are also a lot of migrations from",
    "start": "210670",
    "end": "212410"
  },
  {
    "text": "ec2 and those are",
    "start": "212410",
    "end": "214360"
  },
  {
    "text": "more of the interesting problems because",
    "start": "214360",
    "end": "216610"
  },
  {
    "text": "we actually have something for",
    "start": "216610",
    "end": "218140"
  },
  {
    "text": "side-by-side comparison because during",
    "start": "218140",
    "end": "221620"
  },
  {
    "text": "that time we would actually start",
    "start": "221620",
    "end": "223000"
  },
  {
    "text": "getting questions from our service",
    "start": "223000",
    "end": "225940"
  },
  {
    "text": "developers asking me hey why do I have",
    "start": "225940",
    "end": "228910"
  },
  {
    "text": "late and she latency issues with like",
    "start": "228910",
    "end": "231070"
  },
  {
    "text": "one specific pod or why is my latency",
    "start": "231070",
    "end": "233080"
  },
  {
    "text": "issue getting worse after I do a",
    "start": "233080",
    "end": "235510"
  },
  {
    "text": "migration and we get more questions",
    "start": "235510",
    "end": "239320"
  },
  {
    "text": "about latency and why things are slow",
    "start": "239320",
    "end": "240940"
  },
  {
    "text": "and just more latency issues and even",
    "start": "240940",
    "end": "244570"
  },
  {
    "text": "more lessee issues and this makes us sad",
    "start": "244570",
    "end": "247780"
  },
  {
    "text": "because like things are slow and they're",
    "start": "247780",
    "end": "250480"
  },
  {
    "text": "asking in front engine areas like hey",
    "start": "250480",
    "end": "252010"
  },
  {
    "text": "help us what's happening this migration",
    "start": "252010",
    "end": "255040"
  },
  {
    "text": "is like seems like it's not going so",
    "start": "255040",
    "end": "257049"
  },
  {
    "text": "well but let's actually dive into the",
    "start": "257049",
    "end": "260560"
  },
  {
    "start": "258000",
    "end": "402000"
  },
  {
    "text": "actual problems let's talk about case",
    "start": "260560",
    "end": "263320"
  },
  {
    "text": "where actually it seems like Layton sees",
    "start": "263320",
    "end": "265090"
  },
  {
    "text": "might have improved here is a graph",
    "start": "265090",
    "end": "268480"
  },
  {
    "text": "where during the migration Layton sees",
    "start": "268480",
    "end": "271780"
  },
  {
    "text": "were kind of like went down actually",
    "start": "271780",
    "end": "274090"
  },
  {
    "text": "when they were doing a side by side when",
    "start": "274090",
    "end": "276700"
  },
  {
    "text": "they have a side by side deployment and",
    "start": "276700",
    "end": "278050"
  },
  {
    "text": "actually when they fully deployed let",
    "start": "278050",
    "end": "279970"
  },
  {
    "text": "you see he's just like dramatically got",
    "start": "279970",
    "end": "281530"
  },
  {
    "text": "better so what happened here well what",
    "start": "281530",
    "end": "286450"
  },
  {
    "text": "do you know about the service in",
    "start": "286450",
    "end": "287680"
  },
  {
    "text": "Margaret from ec2 to kubernetes plus",
    "start": "287680",
    "end": "289570"
  },
  {
    "text": "containers there were no code changes",
    "start": "289570",
    "end": "291460"
  },
  {
    "text": "roughly the same amount CPU memory was a",
    "start": "291460",
    "end": "294190"
  },
  {
    "text": "Java service latency dramatically",
    "start": "294190",
    "end": "296680"
  },
  {
    "text": "improved then the service was also spun",
    "start": "296680",
    "end": "298630"
  },
  {
    "text": "up in early 2018 for us the biggest clue",
    "start": "298630",
    "end": "302470"
  },
  {
    "text": "was that the fact that this service was",
    "start": "302470",
    "end": "304240"
  },
  {
    "text": "spun up in early 2018",
    "start": "304240",
    "end": "306370"
  },
  {
    "text": "when we actually dug into it this was",
    "start": "306370",
    "end": "309070"
  },
  {
    "text": "actually a very common issue or a common",
    "start": "309070",
    "end": "313419"
  },
  {
    "text": "my surprise the service was actually",
    "start": "313419",
    "end": "316270"
  },
  {
    "text": "running on previous generations hardware",
    "start": "316270",
    "end": "318190"
  },
  {
    "text": "so the migration just so happened to",
    "start": "318190",
    "end": "320140"
  },
  {
    "text": "have also upgraded to services hardware",
    "start": "320140",
    "end": "321880"
  },
  {
    "text": "and this was like one quote where the",
    "start": "321880",
    "end": "323919"
  },
  {
    "text": "guy was like wow it's just a better box",
    "start": "323919",
    "end": "325510"
  },
  {
    "text": "with faster network and it's cheaper",
    "start": "325510",
    "end": "327840"
  },
  {
    "text": "we've also seen cases where things got",
    "start": "327840",
    "end": "331120"
  },
  {
    "text": "slower because they were only super",
    "start": "331120",
    "end": "333729"
  },
  {
    "text": "beefy hardware and now we've just kind",
    "start": "333729",
    "end": "335380"
  },
  {
    "text": "of like moved them into our cluster",
    "start": "335380",
    "end": "336610"
  },
  {
    "text": "where we have like one type of hardware",
    "start": "336610",
    "end": "338940"
  },
  {
    "text": "and for this example the question is",
    "start": "338940",
    "end": "342610"
  },
  {
    "text": "like the kubernetes make my p95 is",
    "start": "342610",
    "end": "344440"
  },
  {
    "text": "better I would say no",
    "start": "344440",
    "end": "348190"
  },
  {
    "text": "but we will tell our customers yes if it",
    "start": "348190",
    "end": "349990"
  },
  {
    "text": "got better and I say this because I",
    "start": "349990",
    "end": "354640"
  },
  {
    "text": "think hardware choices have to be made",
    "start": "354640",
    "end": "355870"
  },
  {
    "text": "anyways and usually instance types",
    "start": "355870",
    "end": "358060"
  },
  {
    "text": "aren't intentionally picked to match the",
    "start": "358060",
    "end": "360250"
  },
  {
    "text": "application usually the hardware is just",
    "start": "360250",
    "end": "363460"
  },
  {
    "text": "like chosen in the beginning and people",
    "start": "363460",
    "end": "365770"
  },
  {
    "text": "just like kind of forget about it later",
    "start": "365770",
    "end": "367300"
  },
  {
    "text": "on the lessons here are like that the",
    "start": "367300",
    "end": "371950"
  },
  {
    "text": "things that can be running on can be",
    "start": "371950",
    "end": "373780"
  },
  {
    "text": "different for better or worse so one",
    "start": "373780",
    "end": "375940"
  },
  {
    "text": "example here is hardware right they",
    "start": "375940",
    "end": "377320"
  },
  {
    "text": "could have older hardware or even better",
    "start": "377320",
    "end": "380620"
  },
  {
    "text": "hardware and now this migration just so",
    "start": "380620",
    "end": "382450"
  },
  {
    "text": "happens to have also change that other",
    "start": "382450",
    "end": "384790"
  },
  {
    "text": "things that might get you are like the",
    "start": "384790",
    "end": "386740"
  },
  {
    "text": "host operating system because when your",
    "start": "386740",
    "end": "389950"
  },
  {
    "text": "service was running on its own box it",
    "start": "389950",
    "end": "392410"
  },
  {
    "text": "might have some OS but now all your",
    "start": "392410",
    "end": "395950"
  },
  {
    "text": "kubernetes host minions are running on",
    "start": "395950",
    "end": "398530"
  },
  {
    "text": "might might be running on a different",
    "start": "398530",
    "end": "399910"
  },
  {
    "text": "operating system all right now I'm gonna",
    "start": "399910",
    "end": "404500"
  },
  {
    "start": "402000",
    "end": "724000"
  },
  {
    "text": "talk about the problem of noisy",
    "start": "404500",
    "end": "406300"
  },
  {
    "text": "neighbors we've seen this case where",
    "start": "406300",
    "end": "409270"
  },
  {
    "text": "sometimes there's only certain pods so",
    "start": "409270",
    "end": "411400"
  },
  {
    "text": "for our service they have like many many",
    "start": "411400",
    "end": "412900"
  },
  {
    "text": "pods and it just so happens like two or",
    "start": "412900",
    "end": "415120"
  },
  {
    "text": "three of these pods have extra high",
    "start": "415120",
    "end": "416800"
  },
  {
    "text": "latency we have also seen this issue",
    "start": "416800",
    "end": "420340"
  },
  {
    "text": "where latency they were running on",
    "start": "420340",
    "end": "423010"
  },
  {
    "text": "kubernetes",
    "start": "423010",
    "end": "423570"
  },
  {
    "text": "fine for multiple days and suddenly",
    "start": "423570",
    "end": "426330"
  },
  {
    "text": "their Layton sees went up and it just",
    "start": "426330",
    "end": "428950"
  },
  {
    "text": "happened that became the new normal and",
    "start": "428950",
    "end": "432300"
  },
  {
    "text": "then sometimes it becomes an incident",
    "start": "432300",
    "end": "434560"
  },
  {
    "text": "where this is a sharp spike and all of",
    "start": "434560",
    "end": "436690"
  },
  {
    "text": "our pods and they all have latency",
    "start": "436690",
    "end": "438520"
  },
  {
    "text": "problems so what happened here the hint",
    "start": "438520",
    "end": "443020"
  },
  {
    "text": "is it's in the title as noisy neighbors",
    "start": "443020",
    "end": "448260"
  },
  {
    "text": "and explanation of noisy neighbors is a",
    "start": "448260",
    "end": "451680"
  },
  {
    "text": "the idea that no you have multiple",
    "start": "451680",
    "end": "453880"
  },
  {
    "text": "containers or apps or processes and",
    "start": "453880",
    "end": "455710"
  },
  {
    "text": "they're all sharing the resources of a",
    "start": "455710",
    "end": "457630"
  },
  {
    "text": "computer and there's only so much CPU to",
    "start": "457630",
    "end": "459880"
  },
  {
    "text": "go around in this little example you",
    "start": "459880",
    "end": "462220"
  },
  {
    "text": "know we have like three services like",
    "start": "462220",
    "end": "464160"
  },
  {
    "text": "green blue and orange one and the green",
    "start": "464160",
    "end": "467140"
  },
  {
    "text": "one is like definitely dominating the",
    "start": "467140",
    "end": "469000"
  },
  {
    "text": "CPU time and we can say like this noisy",
    "start": "469000",
    "end": "472510"
  },
  {
    "text": "service great this green noisy services",
    "start": "472510",
    "end": "474550"
  },
  {
    "text": "it's just like just like taking up all",
    "start": "474550",
    "end": "476770"
  },
  {
    "text": "the CPU time so the blue and orange one",
    "start": "476770",
    "end": "479440"
  },
  {
    "text": "is just not getting scheduled let's call",
    "start": "479440",
    "end": "481720"
  },
  {
    "text": "this green service",
    "start": "481720",
    "end": "482530"
  },
  {
    "text": "service kill so in this case we actually",
    "start": "482530",
    "end": "486250"
  },
  {
    "text": "learn sometimes we're in certain pods it",
    "start": "486250",
    "end": "488140"
  },
  {
    "text": "was specifically pods that were",
    "start": "488140",
    "end": "489310"
  },
  {
    "text": "co-located with a service scale and when",
    "start": "489310",
    "end": "493840"
  },
  {
    "text": "it became constant it was actually the",
    "start": "493840",
    "end": "496060"
  },
  {
    "text": "same service service scale that migrated",
    "start": "496060",
    "end": "497890"
  },
  {
    "text": "to the same cluster just that's also",
    "start": "497890",
    "end": "501010"
  },
  {
    "text": "made issues where people are like well",
    "start": "501010",
    "end": "502600"
  },
  {
    "text": "can I get off this cluster cuz this is",
    "start": "502600",
    "end": "504670"
  },
  {
    "text": "no longer a good cluster we tell them no",
    "start": "504670",
    "end": "509100"
  },
  {
    "text": "and in this case where it became an",
    "start": "509100",
    "end": "513130"
  },
  {
    "text": "incident it actually was not service",
    "start": "513130",
    "end": "514870"
  },
  {
    "text": "killed this was just us accidentally",
    "start": "514870",
    "end": "516430"
  },
  {
    "text": "deploying a staging service to the wrong",
    "start": "516430",
    "end": "519729"
  },
  {
    "text": "cluster and it just like got deployed",
    "start": "519729",
    "end": "521680"
  },
  {
    "text": "through a production cluster and just",
    "start": "521680",
    "end": "523539"
  },
  {
    "text": "took a lot of CPU and now some of you",
    "start": "523539",
    "end": "528580"
  },
  {
    "text": "are thinking how can this be a problem",
    "start": "528580",
    "end": "529870"
  },
  {
    "text": "like where is the isolation well in the",
    "start": "529870",
    "end": "533500"
  },
  {
    "text": "early days of air beam being kubernetes",
    "start": "533500",
    "end": "534910"
  },
  {
    "text": "we actually decided not to set CPU",
    "start": "534910",
    "end": "536590"
  },
  {
    "text": "limits because it seemed to have hurt",
    "start": "536590",
    "end": "538510"
  },
  {
    "text": "performance I would say that's a bad",
    "start": "538510",
    "end": "541480"
  },
  {
    "text": "choice and we don't recommend it out of",
    "start": "541480",
    "end": "545980"
  },
  {
    "text": "Crowell study how many of you do not set",
    "start": "545980",
    "end": "547720"
  },
  {
    "text": "limits just like one of you a few of you",
    "start": "547720",
    "end": "550870"
  },
  {
    "text": "okay I can see there being some use",
    "start": "550870",
    "end": "553510"
  },
  {
    "text": "cases if you don't have a latency or",
    "start": "553510",
    "end": "556180"
  },
  {
    "text": "performance sensitive applications for",
    "start": "556180",
    "end": "558520"
  },
  {
    "text": "any RN cluster but in general it's like",
    "start": "558520",
    "end": "560050"
  },
  {
    "text": "I would not recommend it because now now",
    "start": "560050",
    "end": "562780"
  },
  {
    "text": "it's a problem for us and you think the",
    "start": "562780",
    "end": "566890"
  },
  {
    "text": "fix is easy and simple right like you",
    "start": "566890",
    "end": "568450"
  },
  {
    "text": "just add two CPU limits but that's not",
    "start": "568450",
    "end": "572560"
  },
  {
    "text": "the case so in this example let's say",
    "start": "572560",
    "end": "575080"
  },
  {
    "text": "you have a CPU request a limit of like",
    "start": "575080",
    "end": "578110"
  },
  {
    "text": "ten mill occurs so the idea is like how",
    "start": "578110",
    "end": "581680"
  },
  {
    "text": "do you spend your 10 CPU quota well",
    "start": "581680",
    "end": "583860"
  },
  {
    "text": "there's this thing there's idea like you",
    "start": "583860",
    "end": "586089"
  },
  {
    "text": "have a CPU say FCFS quota of 100",
    "start": "586089",
    "end": "588040"
  },
  {
    "text": "milliseconds and you can use your ten",
    "start": "588040",
    "end": "591450"
  },
  {
    "text": "Miller Coors up within this 100",
    "start": "591450",
    "end": "594640"
  },
  {
    "text": "milliseconds and in the first case you",
    "start": "594640",
    "end": "597430"
  },
  {
    "text": "could use it all up in like the first 20",
    "start": "597430",
    "end": "599320"
  },
  {
    "text": "milliseconds but then you'd be throttled",
    "start": "599320",
    "end": "601660"
  },
  {
    "text": "for the next 80 780 milliseconds and in",
    "start": "601660",
    "end": "605110"
  },
  {
    "text": "second case maybe your CPU is actually",
    "start": "605110",
    "end": "607000"
  },
  {
    "text": "like spread out over the whole 100",
    "start": "607000",
    "end": "609370"
  },
  {
    "text": "milliseconds and your application will",
    "start": "609370",
    "end": "611110"
  },
  {
    "text": "actually be fine and this matter is",
    "start": "611110",
    "end": "614830"
  },
  {
    "text": "because",
    "start": "614830",
    "end": "615940"
  },
  {
    "text": "in multiple use cases our metrics",
    "start": "615940",
    "end": "618880"
  },
  {
    "text": "collector would just show similar CPU or",
    "start": "618880",
    "end": "621490"
  },
  {
    "text": "low CPU utilization but we would have",
    "start": "621490",
    "end": "624850"
  },
  {
    "text": "really fine-grained hot spots where",
    "start": "624850",
    "end": "627420"
  },
  {
    "text": "services would just use up all of their",
    "start": "627420",
    "end": "630130"
  },
  {
    "text": "CPU and then just get throttled so this",
    "start": "630130",
    "end": "635320"
  },
  {
    "text": "means that most merchant metrics",
    "start": "635320",
    "end": "636730"
  },
  {
    "text": "collectors it's really hard to detect",
    "start": "636730",
    "end": "639220"
  },
  {
    "text": "fine-grained hotspots some things we've",
    "start": "639220",
    "end": "641830"
  },
  {
    "text": "tried we're changing my CFS quota it did",
    "start": "641830",
    "end": "644230"
  },
  {
    "text": "I helped in our case we second thing we",
    "start": "644230",
    "end": "647470"
  },
  {
    "text": "tried whereas a more CPU allocation and",
    "start": "647470",
    "end": "649630"
  },
  {
    "text": "using a more finer grained CPU metrics",
    "start": "649630",
    "end": "651700"
  },
  {
    "text": "collector that was really useful because",
    "start": "651700",
    "end": "653590"
  },
  {
    "text": "we were able to actually see what hey",
    "start": "653590",
    "end": "655300"
  },
  {
    "text": "the service would show like really low",
    "start": "655300",
    "end": "657490"
  },
  {
    "text": "utilization of a CPU requests but when",
    "start": "657490",
    "end": "661030"
  },
  {
    "text": "we got to find great metrics we actually",
    "start": "661030",
    "end": "662800"
  },
  {
    "text": "see all the spikes happening and three",
    "start": "662800",
    "end": "666430"
  },
  {
    "text": "uh we actually set CPU limits now other",
    "start": "666430",
    "end": "670870"
  },
  {
    "text": "things we were actually looking into is",
    "start": "670870",
    "end": "672430"
  },
  {
    "text": "like CPU pinning or CPU sets or CPU",
    "start": "672430",
    "end": "675460"
  },
  {
    "text": "managers there but for us uh we found",
    "start": "675460",
    "end": "678340"
  },
  {
    "text": "some open issues that didn't work with",
    "start": "678340",
    "end": "680110"
  },
  {
    "text": "it for us it's funny uh actually most of",
    "start": "680110",
    "end": "683410"
  },
  {
    "text": "these open issues got there there were",
    "start": "683410",
    "end": "685420"
  },
  {
    "text": "recent movements like in the last two",
    "start": "685420",
    "end": "687190"
  },
  {
    "text": "weeks so I think that's like a coupon",
    "start": "687190",
    "end": "688810"
  },
  {
    "text": "effect we were just like finishing all",
    "start": "688810",
    "end": "690430"
  },
  {
    "text": "uh open issues so in this case let's say",
    "start": "690430",
    "end": "697480"
  },
  {
    "text": "the question is like the kubernetes make",
    "start": "697480",
    "end": "699250"
  },
  {
    "text": "my pee 95s worse and I would say yes",
    "start": "699250",
    "end": "701250"
  },
  {
    "text": "because multi-tenancy is awesome but",
    "start": "701250",
    "end": "704200"
  },
  {
    "text": "it's hard to not take some performance",
    "start": "704200",
    "end": "705910"
  },
  {
    "text": "hits from it and before applications",
    "start": "705910",
    "end": "708460"
  },
  {
    "text": "were running on their own dedicated",
    "start": "708460",
    "end": "709840"
  },
  {
    "text": "boxes but now they're kind of sharing",
    "start": "709840",
    "end": "712120"
  },
  {
    "text": "all the resources with other strange",
    "start": "712120",
    "end": "714610"
  },
  {
    "text": "applications the lessons here are",
    "start": "714610",
    "end": "717670"
  },
  {
    "text": "containers should be contained and we",
    "start": "717670",
    "end": "720610"
  },
  {
    "text": "recommend setting resource limits",
    "start": "720610",
    "end": "722470"
  },
  {
    "text": "upfront okay so this is a continuation",
    "start": "722470",
    "end": "727150"
  },
  {
    "text": "of noise and neighbors made slightly",
    "start": "727150",
    "end": "729280"
  },
  {
    "text": "worse like kubernetes so here's a graph",
    "start": "729280",
    "end": "733540"
  },
  {
    "text": "of a somewhat large service that we have",
    "start": "733540",
    "end": "736170"
  },
  {
    "text": "and one day it decided to go from you",
    "start": "736170",
    "end": "740290"
  },
  {
    "text": "know about 600 pods and scaled up to a",
    "start": "740290",
    "end": "743380"
  },
  {
    "text": "little over a thousand right so auto",
    "start": "743380",
    "end": "747850"
  },
  {
    "text": "scaling is working that's great",
    "start": "747850",
    "end": "749589"
  },
  {
    "text": "and then on some of our hosts we saw you",
    "start": "749589",
    "end": "754959"
  },
  {
    "text": "know no idle CPU CPU contention but in",
    "start": "754959",
    "end": "758560"
  },
  {
    "text": "aggregate the load was fine right like",
    "start": "758560",
    "end": "761319"
  },
  {
    "text": "the overall CPU utilization was just",
    "start": "761319",
    "end": "764350"
  },
  {
    "text": "around 50% so again a lot of scaling is",
    "start": "764350",
    "end": "767290"
  },
  {
    "text": "working great",
    "start": "767290",
    "end": "769139"
  },
  {
    "text": "so we dug into it a little deeper we",
    "start": "769139",
    "end": "772209"
  },
  {
    "text": "found 18 identical service pods running",
    "start": "772209",
    "end": "775600"
  },
  {
    "text": "on a single host and for contexts like",
    "start": "775600",
    "end": "777459"
  },
  {
    "text": "this was not a small cluster by any",
    "start": "777459",
    "end": "779920"
  },
  {
    "text": "means so we weren't trying to cram a ton",
    "start": "779920",
    "end": "782259"
  },
  {
    "text": "of pods so this was an unexpected result",
    "start": "782259",
    "end": "785259"
  },
  {
    "text": "for us so why did this happen so we're",
    "start": "785259",
    "end": "791439"
  },
  {
    "text": "gonna go a little bit into the scheduler",
    "start": "791439",
    "end": "793240"
  },
  {
    "text": "and talk about some of those features so",
    "start": "793240",
    "end": "798639"
  },
  {
    "text": "at high-level scheduler is broken down",
    "start": "798639",
    "end": "801279"
  },
  {
    "text": "into kind of two logical sets of",
    "start": "801279",
    "end": "804550"
  },
  {
    "text": "components one is like filters which",
    "start": "804550",
    "end": "806740"
  },
  {
    "text": "explain where your pod can or cannot run",
    "start": "806740",
    "end": "809860"
  },
  {
    "text": "or must or must not run so some examples",
    "start": "809860",
    "end": "812709"
  },
  {
    "text": "of filters are resources so like if your",
    "start": "812709",
    "end": "816670"
  },
  {
    "text": "node has like 10 CPUs it's already been",
    "start": "816670",
    "end": "820899"
  },
  {
    "text": "filled up with like 9 requests it can",
    "start": "820899",
    "end": "823029"
  },
  {
    "text": "only fit on max like one additional CPU",
    "start": "823029",
    "end": "825279"
  },
  {
    "text": "right simple and then there's also",
    "start": "825279",
    "end": "828089"
  },
  {
    "text": "topology so if your service pods have a",
    "start": "828089",
    "end": "836050"
  },
  {
    "text": "like required affinity to a certain like",
    "start": "836050",
    "end": "838809"
  },
  {
    "text": "a Z or a certain node and then other",
    "start": "838809",
    "end": "841930"
  },
  {
    "text": "kinds of like anti pod affinity and then",
    "start": "841930",
    "end": "844779"
  },
  {
    "text": "after a filtering there's scoring of the",
    "start": "844779",
    "end": "847629"
  },
  {
    "text": "remaining eligible nodes right which is",
    "start": "847629",
    "end": "849600"
  },
  {
    "text": "kind of saying where should my pod run",
    "start": "849600",
    "end": "852730"
  },
  {
    "text": "or where should it not run so some",
    "start": "852730",
    "end": "854259"
  },
  {
    "text": "examples of that are preferred affinity",
    "start": "854259",
    "end": "858809"
  },
  {
    "text": "spreading by topologies so like if",
    "start": "858809",
    "end": "861279"
  },
  {
    "text": "you're running a cluster in multiple ICS",
    "start": "861279",
    "end": "862990"
  },
  {
    "text": "which is what we do it will try to",
    "start": "862990",
    "end": "864910"
  },
  {
    "text": "spread pods evenly across multiple ACS",
    "start": "864910",
    "end": "868059"
  },
  {
    "text": "and spread them evenly across multiple",
    "start": "868059",
    "end": "870429"
  },
  {
    "text": "nodes finally there's one scoring",
    "start": "870429",
    "end": "874959"
  },
  {
    "text": "priority that's called image locality",
    "start": "874959",
    "end": "876999"
  },
  {
    "text": "right so that one basically says if",
    "start": "876999",
    "end": "880360"
  },
  {
    "text": "you've already downloaded an image",
    "start": "880360",
    "end": "883150"
  },
  {
    "text": "on two one minion node it's your pods",
    "start": "883150",
    "end": "886030"
  },
  {
    "text": "are more likely to schedule on to that",
    "start": "886030",
    "end": "887890"
  },
  {
    "text": "minion node and idea is usually to have",
    "start": "887890",
    "end": "892000"
  },
  {
    "text": "all these priorities kind of",
    "start": "892000",
    "end": "893580"
  },
  {
    "text": "counterbalance each other and offer like",
    "start": "893580",
    "end": "895600"
  },
  {
    "text": "a reasonable compromise but in our case",
    "start": "895600",
    "end": "897670"
  },
  {
    "text": "that services had one gigantic image so",
    "start": "897670",
    "end": "904480"
  },
  {
    "text": "image locality was really out weighing",
    "start": "904480",
    "end": "906790"
  },
  {
    "text": "all the other priorities so basically",
    "start": "906790",
    "end": "909070"
  },
  {
    "text": "the the tendency was just to pile up on",
    "start": "909070",
    "end": "911080"
  },
  {
    "text": "one single node and to make things a",
    "start": "911080",
    "end": "913930"
  },
  {
    "text": "little bit worse this service was also",
    "start": "913930",
    "end": "917500"
  },
  {
    "text": "not doing its requests and and limits",
    "start": "917500",
    "end": "920680"
  },
  {
    "text": "quite quite correctly it was using more",
    "start": "920680",
    "end": "923320"
  },
  {
    "text": "than its requested CPU and it didn't",
    "start": "923320",
    "end": "925570"
  },
  {
    "text": "have any limits so yeah so back to the",
    "start": "925570",
    "end": "929200"
  },
  {
    "text": "the question did kubernetes make my p95",
    "start": "929200",
    "end": "931210"
  },
  {
    "text": "worse definitely yes in this case so",
    "start": "931210",
    "end": "935800"
  },
  {
    "text": "lessons here are that the scheduler can",
    "start": "935800",
    "end": "939280"
  },
  {
    "text": "work against you in some pathological",
    "start": "939280",
    "end": "942310"
  },
  {
    "text": "cases for example like if you have",
    "start": "942310",
    "end": "945250"
  },
  {
    "text": "really large damages and something that",
    "start": "945250",
    "end": "949900"
  },
  {
    "text": "we're interested in trying is the pod",
    "start": "949900",
    "end": "952120"
  },
  {
    "text": "topology spread constraints to limit",
    "start": "952120",
    "end": "954520"
  },
  {
    "text": "exactly how many pots we can run per",
    "start": "954520",
    "end": "957280"
  },
  {
    "text": "node crazy but we haven't tried it yet",
    "start": "957280",
    "end": "959640"
  },
  {
    "text": "okay some more lessons one thing to",
    "start": "959640",
    "end": "964510"
  },
  {
    "text": "mention here is like we didn't quite get",
    "start": "964510",
    "end": "966880"
  },
  {
    "text": "into is like why was why was CPU and",
    "start": "966880",
    "end": "971200"
  },
  {
    "text": "aggregate so good why was it like evenly",
    "start": "971200",
    "end": "973600"
  },
  {
    "text": "spread at 50% and like some hosts were",
    "start": "973600",
    "end": "976540"
  },
  {
    "text": "totally starved for CPU so we'll get",
    "start": "976540",
    "end": "979270"
  },
  {
    "text": "into this a little more later but",
    "start": "979270",
    "end": "980620"
  },
  {
    "text": "basically kubernetes services can cause",
    "start": "980620",
    "end": "983230"
  },
  {
    "text": "traffic imbalance when you're using like",
    "start": "983230",
    "end": "985270"
  },
  {
    "text": "the iptables proxy ER and also another",
    "start": "985270",
    "end": "988180"
  },
  {
    "text": "lesson is that auto scaling uses the",
    "start": "988180",
    "end": "989920"
  },
  {
    "text": "average CPU across all pods so this can",
    "start": "989920",
    "end": "992590"
  },
  {
    "text": "even cause some pathological behavior",
    "start": "992590",
    "end": "993970"
  },
  {
    "text": "for example if some of your pods are",
    "start": "993970",
    "end": "996970"
  },
  {
    "text": "let's say unreachable then the aggregate",
    "start": "996970",
    "end": "999820"
  },
  {
    "text": "CPU will drop and then autoscaler will",
    "start": "999820",
    "end": "1002250"
  },
  {
    "text": "scale down the deployment further and",
    "start": "1002250",
    "end": "1003840"
  },
  {
    "text": "then maybe more pods will become",
    "start": "1003840",
    "end": "1005940"
  },
  {
    "text": "unavailable but we haven't gotten that",
    "start": "1005940",
    "end": "1008040"
  },
  {
    "text": "one in this presentation",
    "start": "1008040",
    "end": "1012079"
  },
  {
    "start": "1012000",
    "end": "1244000"
  },
  {
    "text": "all right now we're going to talk about",
    "start": "1014150",
    "end": "1015650"
  },
  {
    "text": "use case of write once run anywhere this",
    "start": "1015650",
    "end": "1022250"
  },
  {
    "text": "was a stock or internal Stack Overflow",
    "start": "1022250",
    "end": "1023900"
  },
  {
    "text": "post where it's like hey my lady's got",
    "start": "1023900",
    "end": "1026420"
  },
  {
    "text": "worse but a key points is this is a java",
    "start": "1026420",
    "end": "1028370"
  },
  {
    "text": "application",
    "start": "1028370",
    "end": "1029030"
  },
  {
    "text": "p95 went from like 30 milliseconds to",
    "start": "1029030",
    "end": "1031790"
  },
  {
    "text": "100 milliseconds p99 s well through yet",
    "start": "1031790",
    "end": "1034610"
  },
  {
    "text": "2x and it was specifically only for DB",
    "start": "1034610",
    "end": "1037308"
  },
  {
    "text": "connections so not their servers to",
    "start": "1037309",
    "end": "1040010"
  },
  {
    "text": "service connections here's a close-up",
    "start": "1040010",
    "end": "1043819"
  },
  {
    "text": "graph of like what happened with it",
    "start": "1043819",
    "end": "1045410"
  },
  {
    "text": "latency and this is really mysterious",
    "start": "1045410",
    "end": "1047660"
  },
  {
    "text": "for us because like we're probably a",
    "start": "1047660",
    "end": "1049429"
  },
  {
    "text": "hundred service service my questions",
    "start": "1049429",
    "end": "1050960"
  },
  {
    "text": "deep all other services seem to be",
    "start": "1050960",
    "end": "1054170"
  },
  {
    "text": "running steady and fine and we're",
    "start": "1054170",
    "end": "1056900"
  },
  {
    "text": "thinking alright this is probably just",
    "start": "1056900",
    "end": "1058610"
  },
  {
    "text": "another missing configuration by the",
    "start": "1058610",
    "end": "1060110"
  },
  {
    "text": "application but this is indeed",
    "start": "1060110",
    "end": "1062120"
  },
  {
    "text": "interesting because it's only DB",
    "start": "1062120",
    "end": "1064190"
  },
  {
    "text": "connections is it's not like others it's",
    "start": "1064190",
    "end": "1066230"
  },
  {
    "text": "not service to service connections when",
    "start": "1066230",
    "end": "1070220"
  },
  {
    "text": "we actually dug into it the problem was",
    "start": "1070220",
    "end": "1072320"
  },
  {
    "text": "we learned that for a specific endpoint",
    "start": "1072320",
    "end": "1074960"
  },
  {
    "text": "when connecting to databases we have",
    "start": "1074960",
    "end": "1077150"
  },
  {
    "text": "created a new thread pool per requests",
    "start": "1077150",
    "end": "1079400"
  },
  {
    "text": "and when we actually fix it by reusing a",
    "start": "1079400",
    "end": "1081770"
  },
  {
    "text": "thread pool in a static context it did",
    "start": "1081770",
    "end": "1083750"
  },
  {
    "text": "fix the performance issues but the",
    "start": "1083750",
    "end": "1088309"
  },
  {
    "text": "question is still why did this work",
    "start": "1088309",
    "end": "1089929"
  },
  {
    "text": "before kubernetes and let's go back to",
    "start": "1089929",
    "end": "1095000"
  },
  {
    "text": "you explaining like how Java services",
    "start": "1095000",
    "end": "1096530"
  },
  {
    "text": "work like if you were running a java",
    "start": "1096530",
    "end": "1098750"
  },
  {
    "text": "application in a in a node all by itself",
    "start": "1098750",
    "end": "1101530"
  },
  {
    "text": "the JVM might tell your application hey",
    "start": "1101530",
    "end": "1103970"
  },
  {
    "text": "I have like 36 CPUs like that's great",
    "start": "1103970",
    "end": "1107890"
  },
  {
    "text": "now you like put it in a pod this still",
    "start": "1107890",
    "end": "1110960"
  },
  {
    "text": "works but I saw this slide for my",
    "start": "1110960",
    "end": "1114350"
  },
  {
    "text": "colleague but now if you have like three",
    "start": "1114350",
    "end": "1117980"
  },
  {
    "text": "of these pods all running on the same",
    "start": "1117980",
    "end": "1119570"
  },
  {
    "text": "node each of them think they have 36",
    "start": "1119570",
    "end": "1122450"
  },
  {
    "text": "CPUs and this is a problem because",
    "start": "1122450",
    "end": "1127130"
  },
  {
    "text": "that's how Java kind of Tunes itself so",
    "start": "1127130",
    "end": "1131090"
  },
  {
    "text": "this is actually a was an open bug older",
    "start": "1131090",
    "end": "1133760"
  },
  {
    "text": "versions of Java just were just not",
    "start": "1133760",
    "end": "1135350"
  },
  {
    "text": "container aware and this is important",
    "start": "1135350",
    "end": "1137690"
  },
  {
    "text": "because Java Tunes itself based on how",
    "start": "1137690",
    "end": "1138980"
  },
  {
    "text": "much resources like CPU cores it thinks",
    "start": "1138980",
    "end": "1141110"
  },
  {
    "text": "the system has and it affects things",
    "start": "1141110",
    "end": "1143690"
  },
  {
    "text": "like how thread pools work this is fixed",
    "start": "1143690",
    "end": "1146720"
  },
  {
    "text": "in a Java version",
    "start": "1146720",
    "end": "1148350"
  },
  {
    "text": "and there are lots of posts on this if",
    "start": "1148350",
    "end": "1149910"
  },
  {
    "text": "you just google for it so for us when we",
    "start": "1149910",
    "end": "1154110"
  },
  {
    "text": "actually like move to the new JVM",
    "start": "1154110",
    "end": "1156750"
  },
  {
    "text": "performance did dramatically improve we",
    "start": "1156750",
    "end": "1159120"
  },
  {
    "text": "also tried like this other flag but it",
    "start": "1159120",
    "end": "1162390"
  },
  {
    "text": "didn't have much of an effect so so here",
    "start": "1162390",
    "end": "1166410"
  },
  {
    "text": "the question is like there could mean",
    "start": "1166410",
    "end": "1167610"
  },
  {
    "text": "that I used to make make my performance",
    "start": "1167610",
    "end": "1169050"
  },
  {
    "text": "worse and after I would say yes because",
    "start": "1169050",
    "end": "1171600"
  },
  {
    "text": "the containers promise of build once run",
    "start": "1171600",
    "end": "1174270"
  },
  {
    "text": "anywhere isn't 100% accurate languages",
    "start": "1174270",
    "end": "1177000"
  },
  {
    "text": "and applications can have deeper",
    "start": "1177000",
    "end": "1178380"
  },
  {
    "text": "dependencies on the underlying systems",
    "start": "1178380",
    "end": "1180180"
  },
  {
    "text": "that they run on and some of you might",
    "start": "1180180",
    "end": "1185640"
  },
  {
    "text": "say like well this is more containers",
    "start": "1185640",
    "end": "1187260"
  },
  {
    "text": "not kubernetes but for us we consider it",
    "start": "1187260",
    "end": "1189990"
  },
  {
    "text": "all as like one whole ecosystem during",
    "start": "1189990",
    "end": "1191670"
  },
  {
    "text": "the migration way moving from raw",
    "start": "1191670",
    "end": "1193500"
  },
  {
    "text": "application living for applications on",
    "start": "1193500",
    "end": "1195390"
  },
  {
    "text": "raw host nodes to containers and",
    "start": "1195390",
    "end": "1199320"
  },
  {
    "text": "kubernetes",
    "start": "1199320",
    "end": "1200900"
  },
  {
    "text": "that sort of lessons here are yeah",
    "start": "1200900",
    "end": "1204050"
  },
  {
    "text": "languages and applications have deeper",
    "start": "1204050",
    "end": "1206190"
  },
  {
    "text": "dependencies on the underlying",
    "start": "1206190",
    "end": "1207540"
  },
  {
    "text": "underlying systems you should upgrade",
    "start": "1207540",
    "end": "1209610"
  },
  {
    "text": "your systems to be more container aware",
    "start": "1209610",
    "end": "1211470"
  },
  {
    "text": "and for us I would say we were very",
    "start": "1211470",
    "end": "1213120"
  },
  {
    "text": "lucky that we had a migration because",
    "start": "1213120",
    "end": "1216030"
  },
  {
    "text": "having a baseline can be very",
    "start": "1216030",
    "end": "1217560"
  },
  {
    "text": "enlightening lots of services were",
    "start": "1217560",
    "end": "1219810"
  },
  {
    "text": "actually created in kubernetes first and",
    "start": "1219810",
    "end": "1222900"
  },
  {
    "text": "the and this meant like they were",
    "start": "1222900",
    "end": "1224970"
  },
  {
    "text": "blissfully unaware of the performance",
    "start": "1224970",
    "end": "1226650"
  },
  {
    "text": "gains to be have to be like have because",
    "start": "1226650",
    "end": "1231090"
  },
  {
    "text": "we had this migration we like realize",
    "start": "1231090",
    "end": "1233940"
  },
  {
    "text": "this is a performance problem and then",
    "start": "1233940",
    "end": "1235560"
  },
  {
    "text": "we were able to like tell all of our",
    "start": "1235560",
    "end": "1237480"
  },
  {
    "text": "Java applications hey upgrade it and you",
    "start": "1237480",
    "end": "1239550"
  },
  {
    "text": "might just see a dramatic performance",
    "start": "1239550",
    "end": "1241970"
  },
  {
    "text": "improvement okay so now I'm going to get",
    "start": "1241970",
    "end": "1247080"
  },
  {
    "start": "1244000",
    "end": "1484000"
  },
  {
    "text": "a little bit more into traffic and",
    "start": "1247080",
    "end": "1249660"
  },
  {
    "text": "balance details the horrors of IP tables",
    "start": "1249660",
    "end": "1252620"
  },
  {
    "text": "okay so this is one another question",
    "start": "1252620",
    "end": "1257220"
  },
  {
    "text": "from internal stack overflow it says",
    "start": "1257220",
    "end": "1259970"
  },
  {
    "text": "this service has a lot of pots sometimes",
    "start": "1259970",
    "end": "1263030"
  },
  {
    "text": "some pods have markedly different QPS",
    "start": "1263030",
    "end": "1266310"
  },
  {
    "text": "distribution you can see that on the top",
    "start": "1266310",
    "end": "1268650"
  },
  {
    "text": "graph some pods seem to have double to",
    "start": "1268650",
    "end": "1271740"
  },
  {
    "text": "keep es of other pods and of course that",
    "start": "1271740",
    "end": "1276360"
  },
  {
    "text": "means they have higher latency so let's",
    "start": "1276360",
    "end": "1278850"
  },
  {
    "text": "get a little bit into the details of",
    "start": "1278850",
    "end": "1282360"
  },
  {
    "text": "kubernetes services work so like I said",
    "start": "1282360",
    "end": "1285430"
  },
  {
    "text": "earlier we use node port services as a",
    "start": "1285430",
    "end": "1288910"
  },
  {
    "text": "bridge between our smart stack service",
    "start": "1288910",
    "end": "1290980"
  },
  {
    "text": "discovery system and kubernetes so that",
    "start": "1290980",
    "end": "1293530"
  },
  {
    "text": "means that you can talk from surfaces in",
    "start": "1293530",
    "end": "1296490"
  },
  {
    "text": "ec2 to services running in kubernetes",
    "start": "1296490",
    "end": "1299380"
  },
  {
    "text": "you can talk between different clusters",
    "start": "1299380",
    "end": "1301900"
  },
  {
    "text": "and that usually works great right so a",
    "start": "1301900",
    "end": "1306060"
  },
  {
    "text": "little bit about no port services right",
    "start": "1306060",
    "end": "1308680"
  },
  {
    "text": "like when you your hit the node port on",
    "start": "1308680",
    "end": "1311590"
  },
  {
    "text": "the the instance has IP tables and it",
    "start": "1311590",
    "end": "1315910"
  },
  {
    "text": "will rewrite from the node IP and the",
    "start": "1315910",
    "end": "1317860"
  },
  {
    "text": "node port into the pot IP and the port",
    "start": "1317860",
    "end": "1322000"
  },
  {
    "text": "and the important note here is that pod",
    "start": "1322000",
    "end": "1325720"
  },
  {
    "text": "a in this example is running on an",
    "start": "1325720",
    "end": "1328540"
  },
  {
    "text": "overlay network so it's a different IP",
    "start": "1328540",
    "end": "1330940"
  },
  {
    "text": "space okay so what happens if you have",
    "start": "1330940",
    "end": "1335410"
  },
  {
    "text": "two pods of the same service and the",
    "start": "1335410",
    "end": "1339850"
  },
  {
    "text": "traffic comes in again from the new port",
    "start": "1339850",
    "end": "1342430"
  },
  {
    "text": "IP and the port port so hits IP tables",
    "start": "1342430",
    "end": "1346180"
  },
  {
    "text": "and then every rights dive you import",
    "start": "1346180",
    "end": "1349060"
  },
  {
    "text": "and then has to choose from two of these",
    "start": "1349060",
    "end": "1351700"
  },
  {
    "text": "pods now so how does it choose them",
    "start": "1351700",
    "end": "1354160"
  },
  {
    "text": "well there's it uses essentially random",
    "start": "1354160",
    "end": "1356620"
  },
  {
    "text": "allocation and that's a problem for us",
    "start": "1356620",
    "end": "1361470"
  },
  {
    "text": "because you can't spread load very",
    "start": "1361470",
    "end": "1366790"
  },
  {
    "text": "evenly with random low allocation its IP",
    "start": "1366790",
    "end": "1369970"
  },
  {
    "text": "tables is it's not meant to be a load",
    "start": "1369970",
    "end": "1372760"
  },
  {
    "text": "balancer and also if you have two pods",
    "start": "1372760",
    "end": "1376240"
  },
  {
    "text": "on the same node and one pod on the",
    "start": "1376240",
    "end": "1379240"
  },
  {
    "text": "other node they're likely to have",
    "start": "1379240",
    "end": "1381940"
  },
  {
    "text": "different traffic levels so in the",
    "start": "1381940",
    "end": "1384460"
  },
  {
    "text": "example of the service that I gave",
    "start": "1384460",
    "end": "1385690"
  },
  {
    "text": "earlier some knows there were multiple",
    "start": "1385690",
    "end": "1389320"
  },
  {
    "text": "pods and then others there was a single",
    "start": "1389320",
    "end": "1391750"
  },
  {
    "text": "pod and so this is kind of the pod",
    "start": "1391750",
    "end": "1394510"
  },
  {
    "text": "colocation problem that ends up with",
    "start": "1394510",
    "end": "1397630"
  },
  {
    "text": "with traffic in balance so kubernetes",
    "start": "1397630",
    "end": "1403090"
  },
  {
    "text": "make might be 95s worse maybe so traffic",
    "start": "1403090",
    "end": "1406420"
  },
  {
    "text": "imbalance caused variable load and",
    "start": "1406420",
    "end": "1408220"
  },
  {
    "text": "latency but it certainly made latency is",
    "start": "1408220",
    "end": "1410950"
  },
  {
    "text": "more unpredictable",
    "start": "1410950",
    "end": "1413460"
  },
  {
    "text": "so some of the lessons are that when we",
    "start": "1413880",
    "end": "1416490"
  },
  {
    "text": "first ran kubernetes for started",
    "start": "1416490",
    "end": "1418500"
  },
  {
    "text": "migrating into it",
    "start": "1418500",
    "end": "1419429"
  },
  {
    "text": "we were running an overlay network so",
    "start": "1419429",
    "end": "1421200"
  },
  {
    "text": "that provides a lot of flexibility right",
    "start": "1421200",
    "end": "1422580"
  },
  {
    "text": "you have you can do infinite IPS",
    "start": "1422580",
    "end": "1426240"
  },
  {
    "text": "basically you can have them running in",
    "start": "1426240",
    "end": "1427590"
  },
  {
    "text": "the same cider range you can also add",
    "start": "1427590",
    "end": "1430700"
  },
  {
    "text": "like network policy but that adds a lot",
    "start": "1430700",
    "end": "1434100"
  },
  {
    "text": "of complications and also IP tables load",
    "start": "1434100",
    "end": "1438900"
  },
  {
    "text": "balancing is it's not ideal right you",
    "start": "1438900",
    "end": "1441240"
  },
  {
    "text": "don't have like round-robin you don't",
    "start": "1441240",
    "end": "1443910"
  },
  {
    "text": "have like any other advanced load",
    "start": "1443910",
    "end": "1445500"
  },
  {
    "text": "balancing strategies so some",
    "start": "1445500",
    "end": "1447870"
  },
  {
    "text": "alternatives that you could use are",
    "start": "1447870",
    "end": "1450299"
  },
  {
    "text": "using envoy for for balancing between",
    "start": "1450299",
    "end": "1452400"
  },
  {
    "text": "poppies you can also use a cloud",
    "start": "1452400",
    "end": "1455970"
  },
  {
    "text": "provider native IPS to avoid the overlay",
    "start": "1455970",
    "end": "1460909"
  },
  {
    "text": "also you can use IP vs as your through",
    "start": "1460909",
    "end": "1465659"
  },
  {
    "text": "proxy proxy er if it were if it works",
    "start": "1465659",
    "end": "1468210"
  },
  {
    "text": "for you we found some small open issues",
    "start": "1468210",
    "end": "1471390"
  },
  {
    "text": "that caused us to stick with IP tables",
    "start": "1471390",
    "end": "1474480"
  },
  {
    "text": "for now but the one that were most",
    "start": "1474480",
    "end": "1477090"
  },
  {
    "text": "strongly considering moving towards is",
    "start": "1477090",
    "end": "1479220"
  },
  {
    "text": "using the cloud provider native IPs",
    "start": "1479220",
    "end": "1483380"
  },
  {
    "text": "okay now we'll talk about keep DNS",
    "start": "1483380",
    "end": "1485789"
  },
  {
    "start": "1484000",
    "end": "1766000"
  },
  {
    "text": "slowness so here's another service",
    "start": "1485789",
    "end": "1490039"
  },
  {
    "text": "latency graph so on May 10th everything",
    "start": "1490039",
    "end": "1496380"
  },
  {
    "text": "was fine",
    "start": "1496380",
    "end": "1497039"
  },
  {
    "text": "saw a couple of spikes and latency may",
    "start": "1497039",
    "end": "1499710"
  },
  {
    "text": "be up to like one second and then it",
    "start": "1499710",
    "end": "1501750"
  },
  {
    "text": "went to five seconds and then 20 seconds",
    "start": "1501750",
    "end": "1503549"
  },
  {
    "text": "so and it stayed there for quite a long",
    "start": "1503549",
    "end": "1508620"
  },
  {
    "text": "time and this was not unique to one",
    "start": "1508620",
    "end": "1513059"
  },
  {
    "text": "service right so these are multiple",
    "start": "1513059",
    "end": "1514830"
  },
  {
    "text": "graphs across multiple services right",
    "start": "1514830",
    "end": "1516510"
  },
  {
    "text": "there's high latency elevator right it's",
    "start": "1516510",
    "end": "1520380"
  },
  {
    "text": "like build health checks so what did",
    "start": "1520380",
    "end": "1523890"
  },
  {
    "text": "these services have in common they all",
    "start": "1523890",
    "end": "1525720"
  },
  {
    "text": "relied heavily on DNS and made a lot of",
    "start": "1525720",
    "end": "1528330"
  },
  {
    "text": "external DNS queries so this is once we",
    "start": "1528330",
    "end": "1534299"
  },
  {
    "text": "dug a little deeper we looked at cube",
    "start": "1534299",
    "end": "1537450"
  },
  {
    "text": "DNS load right and we saw that some q p",
    "start": "1537450",
    "end": "1541020"
  },
  {
    "text": "NS pods had pretty high keep yes and",
    "start": "1541020",
    "end": "1544320"
  },
  {
    "text": "some pods had rather local yes so",
    "start": "1544320",
    "end": "1546850"
  },
  {
    "text": "traffic imbalance strikes again so let's",
    "start": "1546850",
    "end": "1549130"
  },
  {
    "text": "talk about how cute DNS works now so",
    "start": "1549130",
    "end": "1551890"
  },
  {
    "text": "let's say you're potty",
    "start": "1551890",
    "end": "1553750"
  },
  {
    "text": "running in your note you make a DNS",
    "start": "1553750",
    "end": "1556060"
  },
  {
    "text": "request again it's going to hit iptables",
    "start": "1556060",
    "end": "1557980"
  },
  {
    "text": "this time it's going to use the cluster",
    "start": "1557980",
    "end": "1559570"
  },
  {
    "text": "IP of cheap DNS and so it's gonna",
    "start": "1559570",
    "end": "1563740"
  },
  {
    "text": "balance across multiple pots and again",
    "start": "1563740",
    "end": "1566710"
  },
  {
    "text": "it's going to do so randomly and so that",
    "start": "1566710",
    "end": "1568810"
  },
  {
    "text": "causes traffic imbalance and then in",
    "start": "1568810",
    "end": "1570430"
  },
  {
    "text": "this case we hit 80s DNS packet per",
    "start": "1570430",
    "end": "1573640"
  },
  {
    "text": "second limit so that caused slow fancies",
    "start": "1573640",
    "end": "1577720"
  },
  {
    "text": "and timeouts for multiple services",
    "start": "1577720",
    "end": "1581550"
  },
  {
    "text": "so did kubernetes make my p95 source yes",
    "start": "1581550",
    "end": "1585510"
  },
  {
    "text": "so the lessons are by default DNS is",
    "start": "1585510",
    "end": "1589660"
  },
  {
    "text": "discovered through cluster IP and you",
    "start": "1589660",
    "end": "1592480"
  },
  {
    "text": "also by default your pods use DNS policy",
    "start": "1592480",
    "end": "1595510"
  },
  {
    "text": "of cluster first which reaches out to",
    "start": "1595510",
    "end": "1597370"
  },
  {
    "text": "keep DNS so even if you're making like",
    "start": "1597370",
    "end": "1599670"
  },
  {
    "text": "external DNS queries that aren't like",
    "start": "1599670",
    "end": "1602980"
  },
  {
    "text": "the like within the service address URLs",
    "start": "1602980",
    "end": "1607090"
  },
  {
    "text": "then you're gonna go through ups first",
    "start": "1607090",
    "end": "1610960"
  },
  {
    "text": "so if you don't need to burn ith DNS",
    "start": "1610960",
    "end": "1613900"
  },
  {
    "text": "resolution you can set the pod DNS",
    "start": "1613900",
    "end": "1615550"
  },
  {
    "text": "policy to default or not if you want to",
    "start": "1615550",
    "end": "1618580"
  },
  {
    "text": "do like further customization by like",
    "start": "1618580",
    "end": "1621120"
  },
  {
    "text": "using more fine-grained parameter tuning",
    "start": "1621120",
    "end": "1625800"
  },
  {
    "text": "alright for the recap so let's tally up",
    "start": "1627480",
    "end": "1631030"
  },
  {
    "text": "the scores or let's ease improved yeah I",
    "start": "1631030",
    "end": "1637000"
  },
  {
    "text": "was like kubernetes did not make it",
    "start": "1637000",
    "end": "1638290"
  },
  {
    "text": "better did not make the performance",
    "start": "1638290",
    "end": "1640600"
  },
  {
    "text": "worse but take the credit if it improved",
    "start": "1640600",
    "end": "1642790"
  },
  {
    "text": "because there's so many underlying",
    "start": "1642790",
    "end": "1645010"
  },
  {
    "text": "systems that affect performance like",
    "start": "1645010",
    "end": "1646420"
  },
  {
    "text": "hardware or hose os cetera and then",
    "start": "1646420",
    "end": "1650770"
  },
  {
    "text": "noisy neighbor cares like yes and some",
    "start": "1650770",
    "end": "1653350"
  },
  {
    "text": "of the lessons are cetera limits and be",
    "start": "1653350",
    "end": "1655180"
  },
  {
    "text": "wary of how CPU is actually counted",
    "start": "1655180",
    "end": "1659430"
  },
  {
    "text": "noisy neighbors made worse by a",
    "start": "1660330",
    "end": "1662320"
  },
  {
    "text": "kubernetes like yes you should try to",
    "start": "1662320",
    "end": "1665050"
  },
  {
    "text": "loop maybe like look into how priorities",
    "start": "1665050",
    "end": "1667510"
  },
  {
    "text": "and predicates are affecting how your",
    "start": "1667510",
    "end": "1669730"
  },
  {
    "text": "posit getting scheduled and for write",
    "start": "1669730",
    "end": "1673870"
  },
  {
    "text": "once run anywhere let's say yes ish like",
    "start": "1673870",
    "end": "1676570"
  },
  {
    "text": "obviously more of this was because of",
    "start": "1676570",
    "end": "1678370"
  },
  {
    "text": "the move to containers",
    "start": "1678370",
    "end": "1680329"
  },
  {
    "text": "they're just like gacho's when you move",
    "start": "1680329",
    "end": "1682459"
  },
  {
    "text": "applications to a containerized",
    "start": "1682459",
    "end": "1684199"
  },
  {
    "text": "environment and ideally you can just",
    "start": "1684199",
    "end": "1686690"
  },
  {
    "text": "like fix it by upgrading your",
    "start": "1686690",
    "end": "1687949"
  },
  {
    "text": "applications and languages to be",
    "start": "1687949",
    "end": "1689419"
  },
  {
    "text": "container aware for traffic and",
    "start": "1689419",
    "end": "1693319"
  },
  {
    "text": "imbalance maybe be wary of IP tables",
    "start": "1693319",
    "end": "1696919"
  },
  {
    "text": "load balancing again it's not a little",
    "start": "1696919",
    "end": "1701059"
  },
  {
    "text": "balancer okay and then cube dns sons",
    "start": "1701059",
    "end": "1704529"
  },
  {
    "text": "kubernetes make my p95 source yes so",
    "start": "1704529",
    "end": "1707269"
  },
  {
    "text": "check your dns policy early and often",
    "start": "1707269",
    "end": "1709179"
  },
  {
    "text": "make sure you're using the right one for",
    "start": "1709179",
    "end": "1711019"
  },
  {
    "text": "use case a few other takeaways",
    "start": "1711019",
    "end": "1714319"
  },
  {
    "text": "so like the overall message right is",
    "start": "1714319",
    "end": "1717739"
  },
  {
    "text": "that performance includes tuning layers",
    "start": "1717739",
    "end": "1719479"
  },
  {
    "text": "have all layers of the stack kubernetes",
    "start": "1719479",
    "end": "1721279"
  },
  {
    "text": "has a lot of layers in the stack there's",
    "start": "1721279",
    "end": "1723349"
  },
  {
    "text": "a cluster like container application",
    "start": "1723349",
    "end": "1725869"
  },
  {
    "text": "language network a few others right and",
    "start": "1725869",
    "end": "1729789"
  },
  {
    "text": "before you do your migration set the",
    "start": "1729789",
    "end": "1732440"
  },
  {
    "text": "expectations that some small performance",
    "start": "1732440",
    "end": "1734299"
  },
  {
    "text": "differences it can't happen",
    "start": "1734299",
    "end": "1736249"
  },
  {
    "text": "as long as you have a baseline right you",
    "start": "1736249",
    "end": "1738199"
  },
  {
    "text": "can measure to see exactly how much",
    "start": "1738199",
    "end": "1740929"
  },
  {
    "text": "worse how much better just keep tracking",
    "start": "1740929",
    "end": "1743690"
  },
  {
    "text": "and run side by side for a little while",
    "start": "1743690",
    "end": "1746089"
  },
  {
    "text": "and then fix any like major regressions",
    "start": "1746089",
    "end": "1748399"
  },
  {
    "text": "that come up we're all so sorry so learn",
    "start": "1748399",
    "end": "1753589"
  },
  {
    "text": "more at our medium Airbnb engineering",
    "start": "1753589",
    "end": "1756739"
  },
  {
    "text": "blog posts or visit airbnb.com slash",
    "start": "1756739",
    "end": "1759769"
  },
  {
    "text": "career thanks",
    "start": "1759769",
    "end": "1762160"
  },
  {
    "text": "[Applause]",
    "start": "1762160",
    "end": "1767959"
  }
]