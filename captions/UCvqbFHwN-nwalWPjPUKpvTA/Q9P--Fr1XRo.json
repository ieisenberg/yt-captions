[
  {
    "text": "welcome to day two of cubec Con",
    "start": "40",
    "end": "2520"
  },
  {
    "text": "Chicago it's great to be here my name is",
    "start": "2520",
    "end": "5759"
  },
  {
    "text": "hant and today we're here to talk to you",
    "start": "5759",
    "end": "9559"
  },
  {
    "text": "about the hardest incident we ever had",
    "start": "9559",
    "end": "11639"
  },
  {
    "text": "to deal with at data dog what is data",
    "start": "11639",
    "end": "14960"
  },
  {
    "text": "dog data dog is a cloud monitoring",
    "start": "14960",
    "end": "17680"
  },
  {
    "text": "security and observability Company and",
    "start": "17680",
    "end": "20600"
  },
  {
    "text": "at data dog we run kubernetes at a large",
    "start": "20600",
    "end": "23359"
  },
  {
    "text": "scale we run hundreds of self-managed",
    "start": "23359",
    "end": "25960"
  },
  {
    "text": "kubernetes clusters which help us manage",
    "start": "25960",
    "end": "28800"
  },
  {
    "text": "tens of thousands of kubernetes nodes",
    "start": "28800",
    "end": "30920"
  },
  {
    "text": "and some of our kubernetes clusters have",
    "start": "30920",
    "end": "33000"
  },
  {
    "text": "4,000 plus nodes in them so what",
    "start": "33000",
    "end": "36239"
  },
  {
    "text": "happened what was this hardest incident",
    "start": "36239",
    "end": "38920"
  },
  {
    "text": "so on March 8th 2023 our platform",
    "start": "38920",
    "end": "41800"
  },
  {
    "text": "experienced a global outage and our",
    "start": "41800",
    "end": "43800"
  },
  {
    "text": "users were unable to access the platform",
    "start": "43800",
    "end": "46440"
  },
  {
    "text": "for around 10 hours and it would take us",
    "start": "46440",
    "end": "49199"
  },
  {
    "text": "another 12 hours to recover the last",
    "start": "49199",
    "end": "51320"
  },
  {
    "text": "major service and this happened because",
    "start": "51320",
    "end": "54399"
  },
  {
    "text": "we lost 60% of our compute capacity",
    "start": "54399",
    "end": "57239"
  },
  {
    "text": "across multiple Cloud providers",
    "start": "57239",
    "end": "59440"
  },
  {
    "text": "Distributing in several availability",
    "start": "59440",
    "end": "61239"
  },
  {
    "text": "zones in a span of 1",
    "start": "61239",
    "end": "63640"
  },
  {
    "text": "hour this is a story of how we lost",
    "start": "63640",
    "end": "67280"
  },
  {
    "text": "almost everything everywhere all at once",
    "start": "67280",
    "end": "70119"
  },
  {
    "text": "at data",
    "start": "70119",
    "end": "72040"
  },
  {
    "text": "dog so during the incident our internal",
    "start": "72040",
    "end": "75000"
  },
  {
    "text": "data dog employees realized that they",
    "start": "75000",
    "end": "77159"
  },
  {
    "text": "were not able to access the kubernetes",
    "start": "77159",
    "end": "78840"
  },
  {
    "text": "control ples and our admin teams were",
    "start": "78840",
    "end": "81799"
  },
  {
    "text": "having trouble trying to even as into",
    "start": "81799",
    "end": "84000"
  },
  {
    "text": "the nodes to try and debug what was",
    "start": "84000",
    "end": "85799"
  },
  {
    "text": "happening so at this point it seemed",
    "start": "85799",
    "end": "88360"
  },
  {
    "text": "like a widespread Network outage",
    "start": "88360",
    "end": "91040"
  },
  {
    "text": "so did we roll out a bad change",
    "start": "91040",
    "end": "93560"
  },
  {
    "text": "fleetwide no we actually have explicit",
    "start": "93560",
    "end": "96960"
  },
  {
    "text": "policies in place to prevent our",
    "start": "96960",
    "end": "98799"
  },
  {
    "text": "Engineers from deploying code too",
    "start": "98799",
    "end": "101479"
  },
  {
    "text": "fast so how did this happen in fact we",
    "start": "101479",
    "end": "105360"
  },
  {
    "text": "run completely independent Stacks with",
    "start": "105360",
    "end": "107640"
  },
  {
    "text": "no explicit dependencies between them so",
    "start": "107640",
    "end": "110079"
  },
  {
    "text": "there's no way we could have deployed",
    "start": "110079",
    "end": "111640"
  },
  {
    "text": "something that can impact all of our",
    "start": "111640",
    "end": "113920"
  },
  {
    "text": "data centers at the same",
    "start": "113920",
    "end": "116159"
  },
  {
    "text": "time luckily for us at data dog we have",
    "start": "116159",
    "end": "119280"
  },
  {
    "text": "a strong culture of incident response so",
    "start": "119280",
    "end": "122799"
  },
  {
    "text": "it was pretty easy for us to get all",
    "start": "122799",
    "end": "124880"
  },
  {
    "text": "hands on deck pretty quickly our",
    "start": "124880",
    "end": "127039"
  },
  {
    "text": "Engineers who build these Services own",
    "start": "127039",
    "end": "129239"
  },
  {
    "text": "them end to end and as you can see from",
    "start": "129239",
    "end": "131160"
  },
  {
    "text": "the screenshot we were having around 400",
    "start": "131160",
    "end": "134480"
  },
  {
    "text": "plus Engineers working in different",
    "start": "134480",
    "end": "136400"
  },
  {
    "text": "breakout rooms trying to figure out what",
    "start": "136400",
    "end": "138480"
  },
  {
    "text": "was",
    "start": "138480",
    "end": "139319"
  },
  {
    "text": "happening soon enough one of our",
    "start": "139319",
    "end": "141640"
  },
  {
    "text": "Engineers figured out that we were able",
    "start": "141640",
    "end": "143920"
  },
  {
    "text": "to recover some of the Lost nodes by",
    "start": "143920",
    "end": "146160"
  },
  {
    "text": "simply restarting them from Google Cloud",
    "start": "146160",
    "end": "148160"
  },
  {
    "text": "console or via the GC API and once we",
    "start": "148160",
    "end": "151280"
  },
  {
    "text": "recovered those nodes we were able to",
    "start": "151280",
    "end": "153040"
  },
  {
    "text": "SSH into them and take a closer look at",
    "start": "153040",
    "end": "155319"
  },
  {
    "text": "the system",
    "start": "155319",
    "end": "156440"
  },
  {
    "text": "logs and that system logs told us that",
    "start": "156440",
    "end": "159560"
  },
  {
    "text": "there was an unattended upgrade on these",
    "start": "159560",
    "end": "161239"
  },
  {
    "text": "nodes and soon after the unattended",
    "start": "161239",
    "end": "163360"
  },
  {
    "text": "upgrade these nodes were losing network",
    "start": "163360",
    "end": "166879"
  },
  {
    "text": "connectivity so what are unattended",
    "start": "166879",
    "end": "168920"
  },
  {
    "text": "upgrades unattended upgrades are our",
    "start": "168920",
    "end": "171120"
  },
  {
    "text": "Legacy way of getting critical security",
    "start": "171120",
    "end": "173519"
  },
  {
    "text": "updates onto our nodes and at data dog",
    "start": "173519",
    "end": "176319"
  },
  {
    "text": "at the same time we've also been working",
    "start": "176319",
    "end": "178480"
  },
  {
    "text": "very hard to build a node life cycle",
    "start": "178480",
    "end": "180280"
  },
  {
    "text": "automation platform that can replace",
    "start": "180280",
    "end": "182480"
  },
  {
    "text": "thousands of nodes every day with",
    "start": "182480",
    "end": "184680"
  },
  {
    "text": "minimal impact to applications and it",
    "start": "184680",
    "end": "187319"
  },
  {
    "text": "does that in a kubernetes native way but",
    "start": "187319",
    "end": "190480"
  },
  {
    "text": "as you can imagine this is a long",
    "start": "190480",
    "end": "192239"
  },
  {
    "text": "migration so at this point in time we",
    "start": "192239",
    "end": "194799"
  },
  {
    "text": "had both of these systems enabled and if",
    "start": "194799",
    "end": "196840"
  },
  {
    "text": "you're interested in learning more about",
    "start": "196840",
    "end": "198319"
  },
  {
    "text": "this node life cycle automation platform",
    "start": "198319",
    "end": "200560"
  },
  {
    "text": "my colleagues are giving a talk later",
    "start": "200560",
    "end": "202519"
  },
  {
    "text": "today do check it",
    "start": "202519",
    "end": "204879"
  },
  {
    "text": "out so using this node life cycle",
    "start": "204879",
    "end": "207480"
  },
  {
    "text": "automation platform we were upgrading",
    "start": "207480",
    "end": "209439"
  },
  {
    "text": "our Fleet from Ubuntu 2004 to Ubuntu",
    "start": "209439",
    "end": "212760"
  },
  {
    "text": "2204 and as you can see from this graph",
    "start": "212760",
    "end": "215439"
  },
  {
    "text": "we started",
    "start": "215439",
    "end": "217959"
  },
  {
    "text": "our hello yeah so we started our",
    "start": "219319",
    "end": "222159"
  },
  {
    "text": "migration in November",
    "start": "222159",
    "end": "223920"
  },
  {
    "text": "2022 and by March 2023 90% of our fleet",
    "start": "223920",
    "end": "228680"
  },
  {
    "text": "was upgraded to bundu",
    "start": "228680",
    "end": "231920"
  },
  {
    "text": "20204 so we wanted to take a closer look",
    "start": "231920",
    "end": "234760"
  },
  {
    "text": "at what exactly was included in these",
    "start": "234760",
    "end": "236640"
  },
  {
    "text": "unattended upgrades turns out there was",
    "start": "236640",
    "end": "239519"
  },
  {
    "text": "a a patch to a vulnerability in system D",
    "start": "239519",
    "end": "242079"
  },
  {
    "text": "that was being addressed in these",
    "start": "242079",
    "end": "243400"
  },
  {
    "text": "unattended upgrades but this patch had",
    "start": "243400",
    "end": "246360"
  },
  {
    "text": "nothing to do with",
    "start": "246360",
    "end": "248159"
  },
  {
    "text": "networking and all we know so far is any",
    "start": "248159",
    "end": "251040"
  },
  {
    "text": "node that receives these patches seems",
    "start": "251040",
    "end": "253400"
  },
  {
    "text": "to be",
    "start": "253400",
    "end": "254319"
  },
  {
    "text": "broken and we know that this had",
    "start": "254319",
    "end": "256359"
  },
  {
    "text": "something to do with system D so we",
    "start": "256359",
    "end": "258600"
  },
  {
    "text": "tried restarting just system D and that",
    "start": "258600",
    "end": "261759"
  },
  {
    "text": "alone was enough to break networking",
    "start": "261759",
    "end": "264080"
  },
  {
    "text": "consistently on one of our recovered",
    "start": "264080",
    "end": "266120"
  },
  {
    "text": "nodes so is there a relationship between",
    "start": "266120",
    "end": "268680"
  },
  {
    "text": "networking and Sy D yes there's actually",
    "start": "268680",
    "end": "271600"
  },
  {
    "text": "a component called Network D systemd",
    "start": "271600",
    "end": "273800"
  },
  {
    "text": "that manages default networking on these",
    "start": "273800",
    "end": "276199"
  },
  {
    "text": "NES so we tried restarting just system D",
    "start": "276199",
    "end": "279400"
  },
  {
    "text": "Network D and that was enough to break",
    "start": "279400",
    "end": "281840"
  },
  {
    "text": "networking on these nodes as well and",
    "start": "281840",
    "end": "284320"
  },
  {
    "text": "after some more investigation we",
    "start": "284320",
    "end": "286680"
  },
  {
    "text": "realized that this was happening only on",
    "start": "286680",
    "end": "288919"
  },
  {
    "text": "ubun 224",
    "start": "288919",
    "end": "290960"
  },
  {
    "text": "nodes so we wanted to look at what",
    "start": "290960",
    "end": "293280"
  },
  {
    "text": "really changed between ubun 2004 and",
    "start": "293280",
    "end": "296120"
  },
  {
    "text": "2204 and we started digging into the",
    "start": "296120",
    "end": "298360"
  },
  {
    "text": "commit history of system system D",
    "start": "298360",
    "end": "299840"
  },
  {
    "text": "Network D and we discovered this very",
    "start": "299840",
    "end": "302160"
  },
  {
    "text": "interesting comit according to this",
    "start": "302160",
    "end": "304639"
  },
  {
    "text": "system D Network D would wipe out any IP",
    "start": "304639",
    "end": "307560"
  },
  {
    "text": "rules that were installed by anyone but",
    "start": "307560",
    "end": "310440"
  },
  {
    "text": "system D Network D and now things",
    "start": "310440",
    "end": "313400"
  },
  {
    "text": "started to make a lot more sense in",
    "start": "313400",
    "end": "315520"
  },
  {
    "text": "order to understand how this would",
    "start": "315520",
    "end": "317000"
  },
  {
    "text": "impact data dog let's take a closer look",
    "start": "317000",
    "end": "319560"
  },
  {
    "text": "at how our kubernetes networking looks",
    "start": "319560",
    "end": "321840"
  },
  {
    "text": "like so at data dog we use for our",
    "start": "321840",
    "end": "324960"
  },
  {
    "text": "container networking and in kubernetes",
    "start": "324960",
    "end": "327560"
  },
  {
    "text": "we know that every part gets it own IP",
    "start": "327560",
    "end": "330080"
  },
  {
    "text": "address and its own network name space",
    "start": "330080",
    "end": "333120"
  },
  {
    "text": "so most cni plugins install a few route",
    "start": "333120",
    "end": "335720"
  },
  {
    "text": "table entries or IP rules to make sure",
    "start": "335720",
    "end": "338400"
  },
  {
    "text": "that the kernel can do the necessary",
    "start": "338400",
    "end": "340240"
  },
  {
    "text": "routing and they can send and receive",
    "start": "340240",
    "end": "342560"
  },
  {
    "text": "packets so on systemd networkd restart",
    "start": "342560",
    "end": "346160"
  },
  {
    "text": "all these IP rules that were installed",
    "start": "346160",
    "end": "348039"
  },
  {
    "text": "by celium were completely wiped out and",
    "start": "348039",
    "end": "350800"
  },
  {
    "text": "that explains why these nodes were",
    "start": "350800",
    "end": "352720"
  },
  {
    "text": "losing network",
    "start": "352720",
    "end": "354759"
  },
  {
    "text": "connectivity so it turns out systemd",
    "start": "354759",
    "end": "357520"
  },
  {
    "text": "Network D never needs to restart in the",
    "start": "357520",
    "end": "359880"
  },
  {
    "text": "happy flow and a patch for a completely",
    "start": "359880",
    "end": "362520"
  },
  {
    "text": "unrelated cve in system D triggered a",
    "start": "362520",
    "end": "365800"
  },
  {
    "text": "restart in system D Network D and that",
    "start": "365800",
    "end": "368440"
  },
  {
    "text": "restart of system D Network D broke",
    "start": "368440",
    "end": "370960"
  },
  {
    "text": "networking on these",
    "start": "370960",
    "end": "372680"
  },
  {
    "text": "NES and on top of that unattended",
    "start": "372680",
    "end": "375319"
  },
  {
    "text": "upgrades run everywhere on our Fleet",
    "start": "375319",
    "end": "377520"
  },
  {
    "text": "between 6:00 a.m. and 7: a.m. and that",
    "start": "377520",
    "end": "380840"
  },
  {
    "text": "explains why we lost so much compute",
    "start": "380840",
    "end": "382960"
  },
  {
    "text": "capacity in such a short span of",
    "start": "382960",
    "end": "385759"
  },
  {
    "text": "time so how did we recover from this in",
    "start": "385759",
    "end": "388919"
  },
  {
    "text": "order to walk us through what happened",
    "start": "388919",
    "end": "390560"
  },
  {
    "text": "next I would like to invite luron",
    "start": "390560",
    "end": "392479"
  },
  {
    "text": "principal engineer at data dog to walk",
    "start": "392479",
    "end": "394120"
  },
  {
    "text": "us through",
    "start": "394120",
    "end": "396479"
  },
  {
    "text": "it so as you can imagine recovering from",
    "start": "403280",
    "end": "407120"
  },
  {
    "text": "this incident was pretty challenging and",
    "start": "407120",
    "end": "409840"
  },
  {
    "text": "it involved multiple steps as hont was",
    "start": "409840",
    "end": "414160"
  },
  {
    "text": "saying uh all our regions were impacted",
    "start": "414160",
    "end": "416919"
  },
  {
    "text": "right but they were impacted differently",
    "start": "416919",
    "end": "419080"
  },
  {
    "text": "from instance on gcp the web pages was",
    "start": "419080",
    "end": "422800"
  },
  {
    "text": "were not even loading and on AWS we had",
    "start": "422800",
    "end": "425240"
  },
  {
    "text": "a web page but with limited content so",
    "start": "425240",
    "end": "428400"
  },
  {
    "text": "we decided to focus first on gcp and the",
    "start": "428400",
    "end": "431080"
  },
  {
    "text": "first step we had to go through was to",
    "start": "431080",
    "end": "433319"
  },
  {
    "text": "get our Comm clusters in a in a healthy",
    "start": "433319",
    "end": "436440"
  },
  {
    "text": "state so as heymont explained we had",
    "start": "436440",
    "end": "439720"
  },
  {
    "text": "discovered pretty early on in the",
    "start": "439720",
    "end": "441319"
  },
  {
    "text": "incident that we could simply fix a gcp",
    "start": "441319",
    "end": "443840"
  },
  {
    "text": "node by repoting it that's simple right",
    "start": "443840",
    "end": "447199"
  },
  {
    "text": "so first well we had to recover our",
    "start": "447199",
    "end": "449599"
  },
  {
    "text": "commun control plane so we could know",
    "start": "449599",
    "end": "451919"
  },
  {
    "text": "what the state of the Clusters was and",
    "start": "451919",
    "end": "454440"
  },
  {
    "text": "this is what we started to do early in",
    "start": "454440",
    "end": "455840"
  },
  {
    "text": "the morning we started to recover the",
    "start": "455840",
    "end": "457199"
  },
  {
    "text": "nodes of the Comm control planes and at",
    "start": "457199",
    "end": "460160"
  },
  {
    "text": "this moment we started to get a full",
    "start": "460160",
    "end": "462280"
  },
  {
    "text": "picture of the impact of the incident",
    "start": "462280",
    "end": "464960"
  },
  {
    "text": "because we discovered that 60% of the",
    "start": "464960",
    "end": "466720"
  },
  {
    "text": "nodes were not ready and",
    "start": "466720",
    "end": "469319"
  },
  {
    "text": "down and at this time of course we had",
    "start": "469319",
    "end": "472360"
  },
  {
    "text": "to recover all these nodes and this",
    "start": "472360",
    "end": "474080"
  },
  {
    "text": "graph is showing all the nodes we had to",
    "start": "474080",
    "end": "475879"
  },
  {
    "text": "restart and each color is a different",
    "start": "475879",
    "end": "477800"
  },
  {
    "text": "cluster and of course you can imagine",
    "start": "477800",
    "end": "480199"
  },
  {
    "text": "that we have thousands of nodes in this",
    "start": "480199",
    "end": "481879"
  },
  {
    "text": "environment and it took us",
    "start": "481879",
    "end": "485039"
  },
  {
    "text": "time I said earlier that the Ed regions",
    "start": "485039",
    "end": "489039"
  },
  {
    "text": "were in a better State because the way",
    "start": "489039",
    "end": "490840"
  },
  {
    "text": "Page was loading so very quickly after",
    "start": "490840",
    "end": "493479"
  },
  {
    "text": "we started working on fixing the gcp",
    "start": "493479",
    "end": "495120"
  },
  {
    "text": "region we started looking at the AWS",
    "start": "495120",
    "end": "497120"
  },
  {
    "text": "ones and we discovered that instances",
    "start": "497120",
    "end": "500440"
  },
  {
    "text": "were running and healthy but they were",
    "start": "500440",
    "end": "502560"
  },
  {
    "text": "very recent I mean they had been up for",
    "start": "502560",
    "end": "504199"
  },
  {
    "text": "only a few hours at Best and by",
    "start": "504199",
    "end": "506960"
  },
  {
    "text": "investigating a little bit more we",
    "start": "506960",
    "end": "508759"
  },
  {
    "text": "actually that the instances had been",
    "start": "508759",
    "end": "511080"
  },
  {
    "text": "replaced and the reason for this is if",
    "start": "511080",
    "end": "513880"
  },
  {
    "text": "you're familiar with gcp and AWS on gcp",
    "start": "513880",
    "end": "516640"
  },
  {
    "text": "when you run an instance in a managed",
    "start": "516640",
    "end": "518080"
  },
  {
    "text": "instance group there's no health checks",
    "start": "518080",
    "end": "520320"
  },
  {
    "text": "right so if the instance is failing the",
    "start": "520320",
    "end": "521599"
  },
  {
    "text": "network is failing its Network it's just",
    "start": "521599",
    "end": "524480"
  },
  {
    "text": "going to remain there without the",
    "start": "524480",
    "end": "525880"
  },
  {
    "text": "network and if you restart it it's going",
    "start": "525880",
    "end": "527600"
  },
  {
    "text": "to be fixed in our case on AWS however",
    "start": "527600",
    "end": "530480"
  },
  {
    "text": "the Autos sking groups will detect the",
    "start": "530480",
    "end": "532240"
  },
  {
    "text": "instances as being in hey and the autoc",
    "start": "532240",
    "end": "534920"
  },
  {
    "text": "scanning group will actively replace",
    "start": "534920",
    "end": "536680"
  },
  {
    "text": "them that's amazing right because we",
    "start": "536680",
    "end": "538800"
  },
  {
    "text": "Auto Fields well except we actually run",
    "start": "538800",
    "end": "542440"
  },
  {
    "text": "a lot of data stores at data dog and",
    "start": "542440",
    "end": "544720"
  },
  {
    "text": "these data stores quite often use local",
    "start": "544720",
    "end": "547200"
  },
  {
    "text": "dis once again if you're familiar with",
    "start": "547200",
    "end": "549800"
  },
  {
    "text": "AWS you will know that when you have",
    "start": "549800",
    "end": "552040"
  },
  {
    "text": "local dis and you replace the instance",
    "start": "552040",
    "end": "554320"
  },
  {
    "text": "you lose the",
    "start": "554320",
    "end": "555480"
  },
  {
    "text": "data and here is an a list of example of",
    "start": "555480",
    "end": "558880"
  },
  {
    "text": "data source we use so we use a lot of",
    "start": "558880",
    "end": "560480"
  },
  {
    "text": "Open Source ones and internal ones of",
    "start": "560480",
    "end": "562839"
  },
  {
    "text": "course all of them are very resilient to",
    "start": "562839",
    "end": "565880"
  },
  {
    "text": "the loss of a single node or of a few",
    "start": "565880",
    "end": "567839"
  },
  {
    "text": "nodes but given the impact of this",
    "start": "567839",
    "end": "570480"
  },
  {
    "text": "incident we had lost 60% of our nodes",
    "start": "570480",
    "end": "572680"
  },
  {
    "text": "you can imagine that in many cases we",
    "start": "572680",
    "end": "574880"
  },
  {
    "text": "had lost Corum and actually lost data we",
    "start": "574880",
    "end": "577880"
  },
  {
    "text": "don't store any source of Truth in the",
    "start": "577880",
    "end": "579920"
  },
  {
    "text": "DAT saw so we were able to rebuild the",
    "start": "579920",
    "end": "581920"
  },
  {
    "text": "data either from backups or for other",
    "start": "581920",
    "end": "584560"
  },
  {
    "text": "source of data but of course it took us",
    "start": "584560",
    "end": "587839"
  },
  {
    "text": "time so at this time our Comm cluster",
    "start": "587839",
    "end": "591279"
  },
  {
    "text": "were healthy and we had a pretty good",
    "start": "591279",
    "end": "593040"
  },
  {
    "text": "idea what the state of the word was",
    "start": "593040",
    "end": "595360"
  },
  {
    "text": "however we had accumulated a lot of",
    "start": "595360",
    "end": "597480"
  },
  {
    "text": "backlog because of course our users at",
    "start": "597480",
    "end": "600160"
  },
  {
    "text": "can selling us data right and so we need",
    "start": "600160",
    "end": "601880"
  },
  {
    "text": "to process all this",
    "start": "601880",
    "end": "603600"
  },
  {
    "text": "backlog and to do this we required",
    "start": "603600",
    "end": "606440"
  },
  {
    "text": "scaling up many of applications to",
    "start": "606440",
    "end": "608120"
  },
  {
    "text": "process all this in a timely",
    "start": "608120",
    "end": "610880"
  },
  {
    "text": "fashion and luckily a data dog with",
    "start": "610880",
    "end": "613160"
  },
  {
    "text": "Cloud first right and cloud is",
    "start": "613160",
    "end": "615160"
  },
  {
    "text": "elastic well the problem is at our scale",
    "start": "615160",
    "end": "619279"
  },
  {
    "text": "elasticity is not always an easy promise",
    "start": "619279",
    "end": "622000"
  },
  {
    "text": "right and a very good example is for",
    "start": "622000",
    "end": "624120"
  },
  {
    "text": "instance that instances don't run in a",
    "start": "624120",
    "end": "626240"
  },
  {
    "text": "vacuum they run in subnets and this",
    "start": "626240",
    "end": "628079"
  },
  {
    "text": "subnet have a",
    "start": "628079",
    "end": "630120"
  },
  {
    "text": "and we're making sure always that we can",
    "start": "630120",
    "end": "632360"
  },
  {
    "text": "absorb the capacity of the loss of an",
    "start": "632360",
    "end": "634440"
  },
  {
    "text": "availability zone right however we",
    "start": "634440",
    "end": "636800"
  },
  {
    "text": "scaled so fast during this incident that",
    "start": "636800",
    "end": "639160"
  },
  {
    "text": "we actually run out run out of ips in",
    "start": "639160",
    "end": "641480"
  },
  {
    "text": "some",
    "start": "641480",
    "end": "643079"
  },
  {
    "text": "subnets another problem we faced is is",
    "start": "643079",
    "end": "645880"
  },
  {
    "text": "kotas right of course we are being very",
    "start": "645880",
    "end": "648480"
  },
  {
    "text": "proactive about the katas for the cloud",
    "start": "648480",
    "end": "650440"
  },
  {
    "text": "providers at data dog and one of the",
    "start": "650440",
    "end": "652800"
  },
  {
    "text": "very typical cut I we careful about is",
    "start": "652800",
    "end": "655040"
  },
  {
    "text": "the maximum number of instances you can",
    "start": "655040",
    "end": "657279"
  },
  {
    "text": "run on a in a project on gcp the red",
    "start": "657279",
    "end": "659920"
  },
  {
    "text": "line on this slide and as you can see",
    "start": "659920",
    "end": "662120"
  },
  {
    "text": "during the incident we always remain",
    "start": "662120",
    "end": "664040"
  },
  {
    "text": "below the line however you can see a",
    "start": "664040",
    "end": "666480"
  },
  {
    "text": "flat blue line in the middle and it",
    "start": "666480",
    "end": "668560"
  },
  {
    "text": "turns out during that incident we",
    "start": "668560",
    "end": "670320"
  },
  {
    "text": "actually discovered a CA we didn't know",
    "start": "670320",
    "end": "672200"
  },
  {
    "text": "about which is the maximum number of",
    "start": "672200",
    "end": "674519"
  },
  {
    "text": "instances you can have in a peering",
    "start": "674519",
    "end": "676440"
  },
  {
    "text": "group when you have multiple vpcs peered",
    "start": "676440",
    "end": "678680"
  },
  {
    "text": "together once we had discovered this I",
    "start": "678680",
    "end": "681040"
  },
  {
    "text": "mean we have a very strong relationship",
    "start": "681040",
    "end": "682480"
  },
  {
    "text": "with gcp and we were able to create a",
    "start": "682480",
    "end": "684639"
  },
  {
    "text": "ticket and very quickly get the Kota",
    "start": "684639",
    "end": "687279"
  },
  {
    "text": "raised but of course it took us",
    "start": "687279",
    "end": "690320"
  },
  {
    "text": "time another example here is U we run Co",
    "start": "690320",
    "end": "694440"
  },
  {
    "text": "this ourselves however we heavily depend",
    "start": "694440",
    "end": "697040"
  },
  {
    "text": "on CL provider apis and of course when",
    "start": "697040",
    "end": "700240"
  },
  {
    "text": "you scale very fast you're going to ask",
    "start": "700240",
    "end": "701920"
  },
  {
    "text": "a lot from these apis and in this",
    "start": "701920",
    "end": "704200"
  },
  {
    "text": "example here we can see an extreme one",
    "start": "704200",
    "end": "706600"
  },
  {
    "text": "where we actually have a controller that",
    "start": "706600",
    "end": "709000"
  },
  {
    "text": "is responsible for allocating network",
    "start": "709000",
    "end": "711480"
  },
  {
    "text": "interfaces to nodes and this controller",
    "start": "711480",
    "end": "714399"
  },
  {
    "text": "was trying to allocate interfaces but of",
    "start": "714399",
    "end": "717360"
  },
  {
    "text": "course we were creating hundreds",
    "start": "717360",
    "end": "719120"
  },
  {
    "text": "thousands of nodes and at some point",
    "start": "719120",
    "end": "720839"
  },
  {
    "text": "this control was starting to get re",
    "start": "720839",
    "end": "722160"
  },
  {
    "text": "limited and because it was actively",
    "start": "722160",
    "end": "724279"
  },
  {
    "text": "retrying we got into a state where about",
    "start": "724279",
    "end": "726760"
  },
  {
    "text": "100% of these requests were being rate",
    "start": "726760",
    "end": "729720"
  },
  {
    "text": "limited to recover from this we had to",
    "start": "729720",
    "end": "732240"
  },
  {
    "text": "decrease the pressure by reducing the",
    "start": "732240",
    "end": "734199"
  },
  {
    "text": "number of nodes requiring additional",
    "start": "734199",
    "end": "735800"
  },
  {
    "text": "network interface and by also asking AWS",
    "start": "735800",
    "end": "738639"
  },
  {
    "text": "to increase the limit which they also",
    "start": "738639",
    "end": "740320"
  },
  {
    "text": "did pretty",
    "start": "740320",
    "end": "742320"
  },
  {
    "text": "fast and this gets us to step three now",
    "start": "742320",
    "end": "745120"
  },
  {
    "text": "that we had healthy clusters that were",
    "start": "745120",
    "end": "747519"
  },
  {
    "text": "able to get additional nodes",
    "start": "747519",
    "end": "749440"
  },
  {
    "text": "we need to recover Dat Dog like all the",
    "start": "749440",
    "end": "751120"
  },
  {
    "text": "applications we have and of course we",
    "start": "751120",
    "end": "753079"
  },
  {
    "text": "wanted to do this as fast as possible so",
    "start": "753079",
    "end": "754959"
  },
  {
    "text": "we wanted to do this in parallel however",
    "start": "754959",
    "end": "757560"
  },
  {
    "text": "as you can imagine we have a very",
    "start": "757560",
    "end": "759360"
  },
  {
    "text": "complex microservice architecture and as",
    "start": "759360",
    "end": "761480"
  },
  {
    "text": "you can see in this example the chain of",
    "start": "761480",
    "end": "763360"
  },
  {
    "text": "dependencies is often pretty complex and",
    "start": "763360",
    "end": "766240"
  },
  {
    "text": "so we did our best to do things in",
    "start": "766240",
    "end": "767639"
  },
  {
    "text": "parallel but of course we had to make",
    "start": "767639",
    "end": "769279"
  },
  {
    "text": "sure that we were recovering things in",
    "start": "769279",
    "end": "770600"
  },
  {
    "text": "order in order to recover data",
    "start": "770600",
    "end": "773720"
  },
  {
    "text": "do so we learned a lot of lessons from",
    "start": "773720",
    "end": "777240"
  },
  {
    "text": "this incident and we're going to share a",
    "start": "777240",
    "end": "780079"
  },
  {
    "text": "few today right the first thing is we're",
    "start": "780079",
    "end": "783160"
  },
  {
    "text": "doing our best to make sure that our",
    "start": "783160",
    "end": "785399"
  },
  {
    "text": "regions are isolated but the problem is",
    "start": "785399",
    "end": "787880"
  },
  {
    "text": "they run the same operating system with",
    "start": "787880",
    "end": "789440"
  },
  {
    "text": "the same configuration and in this case",
    "start": "789440",
    "end": "791680"
  },
  {
    "text": "here this is why we had a Shar face Shar",
    "start": "791680",
    "end": "794040"
  },
  {
    "text": "fate sorry where all the instances",
    "start": "794040",
    "end": "796160"
  },
  {
    "text": "worldwide fail at the same time another",
    "start": "796160",
    "end": "798920"
  },
  {
    "text": "thing we we learned the hardware is very",
    "start": "798920",
    "end": "801040"
  },
  {
    "text": "simple abstractions like instances and",
    "start": "801040",
    "end": "803160"
  },
  {
    "text": "autoscaling can actually leak and have",
    "start": "803160",
    "end": "805480"
  },
  {
    "text": "subtle differences between providers I",
    "start": "805480",
    "end": "807680"
  },
  {
    "text": "mentioned the difference between the a",
    "start": "807680",
    "end": "809760"
  },
  {
    "text": "scaling group and the manage instance",
    "start": "809760",
    "end": "811440"
  },
  {
    "text": "group in gcp which of course is it's a",
    "start": "811440",
    "end": "814240"
  },
  {
    "text": "small difference but in our case it made",
    "start": "814240",
    "end": "816040"
  },
  {
    "text": "debugging and understanding what was",
    "start": "816040",
    "end": "817440"
  },
  {
    "text": "happening a bit",
    "start": "817440",
    "end": "818760"
  },
  {
    "text": "harder I mentioned the complex chain of",
    "start": "818760",
    "end": "821680"
  },
  {
    "text": "dependencies we have between in our",
    "start": "821680",
    "end": "823279"
  },
  {
    "text": "applications and of course during this",
    "start": "823279",
    "end": "825320"
  },
  {
    "text": "incident we had to restart the do from",
    "start": "825320",
    "end": "827480"
  },
  {
    "text": "scratch right and we don't do that often",
    "start": "827480",
    "end": "830000"
  },
  {
    "text": "so we actually discovered dependencies",
    "start": "830000",
    "end": "831639"
  },
  {
    "text": "we didn't know about which of course",
    "start": "831639",
    "end": "834120"
  },
  {
    "text": "also made recovery a bit a bit harder",
    "start": "834120",
    "end": "836600"
  },
  {
    "text": "and finally and this one is kind of",
    "start": "836600",
    "end": "838360"
  },
  {
    "text": "object right like when you run at scale",
    "start": "838360",
    "end": "840839"
  },
  {
    "text": "everything is a challenge because when",
    "start": "840839",
    "end": "842199"
  },
  {
    "text": "you want to do large scale operations",
    "start": "842199",
    "end": "844240"
  },
  {
    "text": "you're going to put so much pressure on",
    "start": "844240",
    "end": "846600"
  },
  {
    "text": "the underlying dependencies that you can",
    "start": "846600",
    "end": "848560"
  },
  {
    "text": "to face very uh interesting",
    "start": "848560",
    "end": "851480"
  },
  {
    "text": "issues so we really want to do better in",
    "start": "851480",
    "end": "854040"
  },
  {
    "text": "the future we want to continue to",
    "start": "854040",
    "end": "855759"
  },
  {
    "text": "decrease the blast radius and and make",
    "start": "855759",
    "end": "857839"
  },
  {
    "text": "sure that uh our regions are very well",
    "start": "857839",
    "end": "860399"
  },
  {
    "text": "isolated that zone that our zones within",
    "start": "860399",
    "end": "863600"
  },
  {
    "text": "a region are very isolated we also want",
    "start": "863600",
    "end": "867160"
  },
  {
    "text": "to degrade more gracefully right because",
    "start": "867160",
    "end": "869519"
  },
  {
    "text": "if we have limited capacity we want to",
    "start": "869519",
    "end": "871759"
  },
  {
    "text": "prioritize the data that matters most",
    "start": "871759",
    "end": "873600"
  },
  {
    "text": "and it's usually live data because it",
    "start": "873600",
    "end": "875720"
  },
  {
    "text": "allows you to see what's happening in",
    "start": "875720",
    "end": "877680"
  },
  {
    "text": "your systems and and to have monitors",
    "start": "877680",
    "end": "880279"
  },
  {
    "text": "and and finally we we already do a lot",
    "start": "880279",
    "end": "882959"
  },
  {
    "text": "of game days and Chaos test but of",
    "start": "882959",
    "end": "885480"
  },
  {
    "text": "course we've never tried something that",
    "start": "885480",
    "end": "886880"
  },
  {
    "text": "big what what when we do this test",
    "start": "886880",
    "end": "888560"
  },
  {
    "text": "usually what we do is we try and fail a",
    "start": "888560",
    "end": "891079"
  },
  {
    "text": "zone right but nothing of that scale and",
    "start": "891079",
    "end": "893279"
  },
  {
    "text": "so we plan to do much larger a much",
    "start": "893279",
    "end": "895560"
  },
  {
    "text": "larger scale C test to make sure that we",
    "start": "895560",
    "end": "897639"
  },
  {
    "text": "exercise our our resiliency but also our",
    "start": "897639",
    "end": "900240"
  },
  {
    "text": "incident response",
    "start": "900240",
    "end": "902519"
  },
  {
    "text": "process something we really wanted to",
    "start": "902519",
    "end": "904680"
  },
  {
    "text": "mention is we wanted we wanted to thank",
    "start": "904680",
    "end": "906839"
  },
  {
    "text": "our partners there so first the cloud",
    "start": "906839",
    "end": "909040"
  },
  {
    "text": "providers who I mentioned earlier",
    "start": "909040",
    "end": "911040"
  },
  {
    "text": "because they were extremely helpful and",
    "start": "911040",
    "end": "912800"
  },
  {
    "text": "they went out of their ways uh to help",
    "start": "912800",
    "end": "915000"
  },
  {
    "text": "us that day we also wanted to thank the",
    "start": "915000",
    "end": "917839"
  },
  {
    "text": "celium community of course because the",
    "start": "917839",
    "end": "919959"
  },
  {
    "text": "celium team very quickly created a patch",
    "start": "919959",
    "end": "923320"
  },
  {
    "text": "to to make sure that um IP roles created",
    "start": "923320",
    "end": "927120"
  },
  {
    "text": "by the CM agent would not be replaced",
    "start": "927120",
    "end": "929519"
  },
  {
    "text": "and deleted uh by System D networkd to",
    "start": "929519",
    "end": "932519"
  },
  {
    "text": "make sure that this incident would not",
    "start": "932519",
    "end": "934160"
  },
  {
    "text": "happen to anyone",
    "start": "934160",
    "end": "936480"
  },
  {
    "text": "again thank you very much it was a bit",
    "start": "936480",
    "end": "938920"
  },
  {
    "text": "challenging to summarize this incident",
    "start": "938920",
    "end": "940399"
  },
  {
    "text": "in 15 minutes so if your curs and if you",
    "start": "940399",
    "end": "942800"
  },
  {
    "text": "want more details uh we wrote very",
    "start": "942800",
    "end": "944800"
  },
  {
    "text": "detailed blog post and also uh we're",
    "start": "944800",
    "end": "947440"
  },
  {
    "text": "going to be around for the conference",
    "start": "947440",
    "end": "949079"
  },
  {
    "text": "hont and I and also many other people",
    "start": "949079",
    "end": "950639"
  },
  {
    "text": "from data dog thank you very much thank",
    "start": "950639",
    "end": "953120"
  },
  {
    "text": "you everyone have a great",
    "start": "953120",
    "end": "954680"
  },
  {
    "text": "youc",
    "start": "954680",
    "end": "957680"
  }
]