[
  {
    "start": "0",
    "end": "53000"
  },
  {
    "text": "so hola buenas tardes hello thank you for coming to our talk enhancing the performance testing",
    "start": "640",
    "end": "6799"
  },
  {
    "text": "process for grpc model inferencing at scale yes that",
    "start": "6799",
    "end": "11920"
  },
  {
    "text": "is a mouthful that's like 20 plus syllables so yes i am paul vanakk we're",
    "start": "11920",
    "end": "17520"
  },
  {
    "text": "and this is my colleague ted chang we're both from ibm silicon valley labs in california",
    "start": "17520",
    "end": "23039"
  },
  {
    "text": "uh still a bit jet lagged very tired but we're here to talk to you about",
    "start": "23039",
    "end": "28400"
  },
  {
    "text": "kind of what we use for production model model deployment uh k-serve and model mesh which there",
    "start": "28400",
    "end": "34800"
  },
  {
    "text": "was a wonderfully artistic talk introduction today earlier by alexa so a",
    "start": "34800",
    "end": "40000"
  },
  {
    "text": "little bit of overlap but it will be a good recap and then some performance testing",
    "start": "40000",
    "end": "45280"
  },
  {
    "text": "and monitoring setup which ted will talk about then we'll kind of go into a brief demo",
    "start": "45280",
    "end": "53840"
  },
  {
    "start": "53000",
    "end": "101000"
  },
  {
    "text": "so yeah first let's just talk about model deployment in general it's of course an integral part of the ml ops",
    "start": "54160",
    "end": "60879"
  },
  {
    "text": "pipeline of the ai lifecycle you do your training that you need to obviously you need to host it somewhere",
    "start": "60879",
    "end": "67600"
  },
  {
    "text": "for your applications to consume the model right so deploying a model as a",
    "start": "67600",
    "end": "74000"
  },
  {
    "text": "microservice is probably the most common model serving strategy",
    "start": "74000",
    "end": "79840"
  },
  {
    "text": "especially in production settings right so in this in this paradigm the",
    "start": "79840",
    "end": "85439"
  },
  {
    "text": "models are exposed via an api endpoint whether it be rest or",
    "start": "85439",
    "end": "90560"
  },
  {
    "text": "grpc then clients make a inference request with their payload to receive inference",
    "start": "90560",
    "end": "97920"
  },
  {
    "text": "response to get what they need right and as we speak about microservices of",
    "start": "97920",
    "end": "103360"
  },
  {
    "start": "101000",
    "end": "150000"
  },
  {
    "text": "course kubernetes is the kind of deployment target of choice right",
    "start": "103360",
    "end": "108960"
  },
  {
    "text": "so kubernetes as you probably all know solves a whole host of problems for you",
    "start": "108960",
    "end": "114000"
  },
  {
    "text": "right from scaling your service up and down you know launching and spinning up new",
    "start": "114000",
    "end": "120320"
  },
  {
    "text": "new model server pods as uh maybe request load goes up and down",
    "start": "120320",
    "end": "125920"
  },
  {
    "text": "then it allows for zero downtime model upgrades you know using rolling deployments",
    "start": "125920",
    "end": "131440"
  },
  {
    "text": "and of course the aspect of resiliency where you know model serving pod is you know",
    "start": "131440",
    "end": "137520"
  },
  {
    "text": "it restarts automatically right if there's a failed pod but you know with all this even just getting",
    "start": "137520",
    "end": "144160"
  },
  {
    "text": "production grade model serving is tough there are some additional complexities you have to be wary of",
    "start": "144160",
    "end": "149599"
  },
  {
    "text": "you know so yeah complexities everywhere there are you know quite many things to consider especially when you're dealing",
    "start": "149599",
    "end": "156239"
  },
  {
    "start": "150000",
    "end": "184000"
  },
  {
    "text": "with a variety of frameworks you have to know how to containerize them for kubernetes um",
    "start": "156239",
    "end": "161840"
  },
  {
    "text": "how you should deploy it how how should the inference or inference request",
    "start": "161840",
    "end": "167280"
  },
  {
    "text": "be formatted and of course making sure inference response times are",
    "start": "167280",
    "end": "172879"
  },
  {
    "text": "acceptable especially when deploying at scale and so this is these are some of the",
    "start": "172879",
    "end": "178560"
  },
  {
    "text": "things that a user would have to navigate when considering production grade model serving on kubernetes",
    "start": "178560",
    "end": "185200"
  },
  {
    "start": "184000",
    "end": "299000"
  },
  {
    "text": "so back to k-serve so for those who were here earlier in the morning alexa did give a",
    "start": "185200",
    "end": "191200"
  },
  {
    "text": "a good introduction to k-serve so i'll just keep it brief here",
    "start": "191200",
    "end": "196840"
  },
  {
    "text": "so it's a k-serve is an incubating project in the lf ai and data foundation",
    "start": "196840",
    "end": "202640"
  },
  {
    "text": "this was recently added originally it was under the kubeflow umbrella",
    "start": "202640",
    "end": "208000"
  },
  {
    "text": "and it allows highly scalable and standard based model inference on kubernetes for trusted ai",
    "start": "208000",
    "end": "214080"
  },
  {
    "text": "so the key features are the serverless inferencing workload um you can serve your models in a serverless manner",
    "start": "214080",
    "end": "220239"
  },
  {
    "text": "using k native um you don't necessarily have to use k native or ndsto with with",
    "start": "220239",
    "end": "225760"
  },
  {
    "text": "uh with k-serve you can also use what we have what we call raw deployment mode um",
    "start": "225760",
    "end": "231680"
  },
  {
    "text": "this just uses the native kubernetes resources like deployments ingress and services",
    "start": "231680",
    "end": "237040"
  },
  {
    "text": "just to kind of deploy your model then one good thing about k-survey provides",
    "start": "237040",
    "end": "243280"
  },
  {
    "text": "the standardized inference protocol what we call the v2 protocol and this is something that's been collaborated and",
    "start": "243280",
    "end": "249920"
  },
  {
    "text": "developed in the community especially with folks from seldon and nvidia",
    "start": "249920",
    "end": "256400"
  },
  {
    "text": "it's a good thing about this protocol is that it'll work your inference request will work across all these different",
    "start": "256400",
    "end": "263280"
  },
  {
    "text": "cloud providers and i guess model serving runtimes like triton or selden's",
    "start": "263280",
    "end": "268479"
  },
  {
    "text": "ml server and i think even pi torch's torch serve supports the v2 protocol",
    "start": "268479",
    "end": "275360"
  },
  {
    "text": "and of course casey provides an easy to use yaml where you just supply a storage uri which is the endpoint to your model",
    "start": "275360",
    "end": "282240"
  },
  {
    "text": "file and then you can actually just in case you will handle the rest and will determine",
    "start": "282240",
    "end": "287360"
  },
  {
    "text": "where that model of what type of container or image that model should be loaded into",
    "start": "287360",
    "end": "293280"
  },
  {
    "text": "so you apply yaml and then ksrib will spin up the kubernetes deployments necessary to serve the provided model for inference",
    "start": "293280",
    "end": "300639"
  },
  {
    "start": "299000",
    "end": "372000"
  },
  {
    "text": "so as mentioned earlier in alexa's talk there are scalability concerns with this",
    "start": "300639",
    "end": "306560"
  },
  {
    "text": "pod per model paradigm right you know each inference service has a",
    "start": "306560",
    "end": "312400"
  },
  {
    "text": "kind of has a resource request and of course how kubernetes is resource allocation you'll you'll quickly hit the",
    "start": "312400",
    "end": "319120"
  },
  {
    "text": "limits of your node then of course kubelet has a maximum i",
    "start": "319120",
    "end": "324240"
  },
  {
    "text": "think the default maximum power limit is 110 and i think kubernetes best practice",
    "start": "324240",
    "end": "329360"
  },
  {
    "text": "kind of dictates or suggests that you shouldn't have more than 100 pods per node",
    "start": "329360",
    "end": "335919"
  },
  {
    "text": "and you know there's also the aspect of the maximum maximum ip address limitations you write",
    "start": "335919",
    "end": "341280"
  },
  {
    "text": "each specific pod and replica of the pods uh each has its own ip address so you might quickly if especially if",
    "start": "341280",
    "end": "347520"
  },
  {
    "text": "you're dealing with thousands of models you might quickly exhaust your ip addresses",
    "start": "347520",
    "end": "353199"
  },
  {
    "text": "so all of this leads us to the notion that need to be able to serve multiple models in a single pod per container",
    "start": "353199",
    "end": "360960"
  },
  {
    "text": "oh sorry you need to serve multiple models in one singular pod right",
    "start": "360960",
    "end": "367280"
  },
  {
    "text": "so we want to reduce the overhead and avoid hitting all these limitations",
    "start": "367280",
    "end": "372639"
  },
  {
    "start": "372000",
    "end": "538000"
  },
  {
    "text": "so this leads us to model mesh this is something that i work very closely with and this is something that",
    "start": "372639",
    "end": "378160"
  },
  {
    "text": "was open sourced last year um as a part of the as a part of the case of organization it's it's intended to serve",
    "start": "378160",
    "end": "384479"
  },
  {
    "text": "as a multi-model serving back-end for k-serve somalo mesh has been used in production",
    "start": "384479",
    "end": "389520"
  },
  {
    "text": "for several years um underpinning a lot of the watson services like watson assistant natural language understanding",
    "start": "389520",
    "end": "396000"
  },
  {
    "text": "and discovery and for those for stuff like that you have maybe hundreds of thousands of",
    "start": "396000",
    "end": "401039"
  },
  {
    "text": "models and not all of these models are being used right some people create the model and",
    "start": "401039",
    "end": "406240"
  },
  {
    "text": "might not perform inference on it for several weeks and so you don't necessarily have to keep it um available",
    "start": "406240",
    "end": "412560"
  },
  {
    "text": "in memory right so model mesh has the capability to page out dormant models right pods not",
    "start": "412560",
    "end": "419039"
  },
  {
    "text": "receiving any or models not receiving any traffic and can load it just in time if it does",
    "start": "419039",
    "end": "424639"
  },
  {
    "text": "receive traffic this kind of falls in line with the k native serverless approach but just",
    "start": "424639",
    "end": "431599"
  },
  {
    "text": "uh using in one singular container",
    "start": "431599",
    "end": "436800"
  },
  {
    "text": "so yeah so model mesh does handles the intelligent loading and unloading right",
    "start": "436800",
    "end": "443759"
  },
  {
    "text": "he's trying to find an intelligent trade-off between responsiveness to users and their computational footprint",
    "start": "443759",
    "end": "449919"
  },
  {
    "text": "and so the overall architecture i'm not sure if yeah you can see that so a user would",
    "start": "449919",
    "end": "456240"
  },
  {
    "text": "yes as multiple predictors or our models are deployed",
    "start": "456240",
    "end": "462000"
  },
  {
    "text": "compatible serving runtime deployments are scaled up to host these models so you might see several run serving",
    "start": "462000",
    "end": "467360"
  },
  {
    "text": "runtime deployments um default it's two deployments or two replicas per serving",
    "start": "467360",
    "end": "473680"
  },
  {
    "text": "runtime [Music] so in each pod you can see that there are three containers the model mesh",
    "start": "473680",
    "end": "479360"
  },
  {
    "text": "container adapter or puller container for retrieving models from the external object store",
    "start": "479360",
    "end": "484960"
  },
  {
    "text": "like s3 or google cloud store and then of course the model server",
    "start": "484960",
    "end": "490080"
  },
  {
    "text": "itself which are third third-party serving runtime are inferent services inference servers like triton or ml",
    "start": "490080",
    "end": "497440"
  },
  {
    "text": "server and all of these support loading multiple models and two into one container",
    "start": "497440",
    "end": "504479"
  },
  {
    "text": "so the model mesh sidecar they're all connected together they help route traffic um which is",
    "start": "504479",
    "end": "511599"
  },
  {
    "text": "performed through the singular kubernetes service at the top um so external inferencing requests are",
    "start": "511599",
    "end": "517518"
  },
  {
    "text": "made via this service and the ingress smaller mesh pod which could be any of these routes them to other pauses needed",
    "start": "517519",
    "end": "523919"
  },
  {
    "text": "in order to yeah and so that that kind of creates kind of like your model mesh",
    "start": "523919",
    "end": "529600"
  },
  {
    "text": "and in order to coordinate operations and persist model and instant states and an xcd",
    "start": "529600",
    "end": "535440"
  },
  {
    "text": "cluster or instance is used yeah so so briefly some of model measures key features are",
    "start": "535440",
    "end": "541920"
  },
  {
    "start": "538000",
    "end": "628000"
  },
  {
    "text": "the cache management so modemesh treats the set of pre-provisioned serving runtime pods on",
    "start": "541920",
    "end": "547839"
  },
  {
    "text": "kubernetes as an lru cache so malamesh decides what models are loaded or unloaded based off",
    "start": "547839",
    "end": "554800"
  },
  {
    "text": "of the usage recency and current request volumes",
    "start": "554800",
    "end": "560160"
  },
  {
    "text": "next model placement will be done in such a way to balance both the cache across the pods as well as the request load",
    "start": "560240",
    "end": "567760"
  },
  {
    "text": "so this just means more commonly used models are placed on pods that perhaps aren't getting as much traffic",
    "start": "567760",
    "end": "574640"
  },
  {
    "text": "in models that may not be getting much traffic or traffic is sporadic will be placed on less utilized pods",
    "start": "574640",
    "end": "580640"
  },
  {
    "text": "then there's resiliency um you know if for some reason object store is",
    "start": "580640",
    "end": "585839"
  },
  {
    "text": "unavailable it'll keep retrying then there's operational simplicity so rolling hand rolling updates are",
    "start": "585839",
    "end": "591760"
  },
  {
    "text": "handled you can traffic will continue to be routed to an older model as you're deploying a newer",
    "start": "591760",
    "end": "597760"
  },
  {
    "text": "model and once the newer model is loaded in the cache and available to use then it'll switch traffic to the newer model",
    "start": "597760",
    "end": "604959"
  },
  {
    "text": "so yeah so model mesh does have quite a lot of moving pieces and you know especially when running something like model",
    "start": "604959",
    "end": "610560"
  },
  {
    "text": "mission production for thousands and thousands of models um and there are several aspects that can",
    "start": "610560",
    "end": "617040"
  },
  {
    "text": "really impact the effect effectiveness of the platform and you know from varying configurations um environments",
    "start": "617040",
    "end": "624079"
  },
  {
    "text": "runtimes and you know the different types of models you might load right and so",
    "start": "624079",
    "end": "629440"
  },
  {
    "start": "628000",
    "end": "714000"
  },
  {
    "text": "this is why performance testing automation and monitoring are critical so as we develop model mesh and as",
    "start": "629440",
    "end": "636560"
  },
  {
    "text": "pieces change um you know performance testing automation becomes very important",
    "start": "636560",
    "end": "641920"
  },
  {
    "text": "so you know this is a platform that serves hundreds of thousands of models so we need to be able to validate performance",
    "start": "641920",
    "end": "647760"
  },
  {
    "text": "with different config and different code bases you know different dependencies models and runtimes",
    "start": "647760",
    "end": "654320"
  },
  {
    "text": "then we also need to ensure that new code doesn't introduce performance regression",
    "start": "654320",
    "end": "659600"
  },
  {
    "text": "and we need to keep doing this periodically throughout the development cycle right we don't want to test at the",
    "start": "659600",
    "end": "665040"
  },
  {
    "text": "very end of the release um and just be caught",
    "start": "665040",
    "end": "670160"
  },
  {
    "text": "caught by surprise by some performance degradation so you know this is all viable and you know by creating",
    "start": "670160",
    "end": "676640"
  },
  {
    "text": "automation for this and adding performance testing as part of our ci cd process and by creating tooling for",
    "start": "676640",
    "end": "682000"
  },
  {
    "text": "effectively monitoring our model mesh performance you know this really helps with finding",
    "start": "682000",
    "end": "688880"
  },
  {
    "text": "you know costly defects sooner and so this saves a bunch of time and money",
    "start": "688880",
    "end": "694240"
  },
  {
    "text": "so here i'm going to pass it off to ted who will kind of show what we do what we our performance testing setup and what",
    "start": "694240",
    "end": "700640"
  },
  {
    "text": "we use for our performance monitoring",
    "start": "700640",
    "end": "705240"
  },
  {
    "start": "714000",
    "end": "796000"
  },
  {
    "text": "so as paul mentioned um there are many critical reasons for",
    "start": "716320",
    "end": "722240"
  },
  {
    "text": "running performance tests right so now i will talk about how we actually run some",
    "start": "722240",
    "end": "727600"
  },
  {
    "text": "of the performance tests so it all boils down to three things at",
    "start": "727600",
    "end": "733680"
  },
  {
    "text": "least for us so the first thing is that the automation framework",
    "start": "733680",
    "end": "740079"
  },
  {
    "text": "should run repeatable tests should be able to run repeat repeatable",
    "start": "740079",
    "end": "745200"
  },
  {
    "text": "tests for every code release and then second is the load driver it",
    "start": "745200",
    "end": "750800"
  },
  {
    "text": "should be very flexible in writing the workload",
    "start": "750800",
    "end": "756720"
  },
  {
    "text": "and the same workload should be able to run tests in both http and grpc",
    "start": "756720",
    "end": "763760"
  },
  {
    "text": "and it should be fast and it should have low overhead and we are currently using k6",
    "start": "763760",
    "end": "771600"
  },
  {
    "text": "we have tried other low driver before and k6 is what we're currently using",
    "start": "771600",
    "end": "778720"
  },
  {
    "text": "metrics monitoring should be able to monitor cpu and memory utilization from worker nodes and",
    "start": "779040",
    "end": "786079"
  },
  {
    "text": "containers inside the cube cluster right now we are using prometheus operator",
    "start": "786079",
    "end": "793120"
  },
  {
    "start": "796000",
    "end": "853000"
  },
  {
    "text": "so this is kind of like our environment the performance test environment so",
    "start": "797600",
    "end": "803040"
  },
  {
    "text": "we have three nodes right so the first node is the automation",
    "start": "803040",
    "end": "808560"
  },
  {
    "text": "in the automation we are we use uh kfp tekton this is the kubeflow",
    "start": "808560",
    "end": "815839"
  },
  {
    "text": "pipeline and then we have k6 inside the keyflow pipeline as",
    "start": "815839",
    "end": "822720"
  },
  {
    "text": "you know part of the task and then on the second note we have our",
    "start": "822720",
    "end": "828880"
  },
  {
    "text": "prometheus operator which has the grafana and prometheus and also influx db",
    "start": "828880",
    "end": "837199"
  },
  {
    "text": "and then the third node is our model mesh this model mesh deploy only one worker now",
    "start": "838399",
    "end": "844560"
  },
  {
    "text": "because this is uh i'm using a very small deployment here",
    "start": "844560",
    "end": "850800"
  },
  {
    "start": "853000",
    "end": "878000"
  },
  {
    "text": "so let's talk about the kefla pipeline so q flow as you may know is an umbrella project",
    "start": "853440",
    "end": "860320"
  },
  {
    "text": "for many other smaller projects a kubeflow pipeline is one of them",
    "start": "860320",
    "end": "867600"
  },
  {
    "text": "this particular q-flow pipeline is running on top of the",
    "start": "867600",
    "end": "873760"
  },
  {
    "text": "tecton back end so next the k6 tools",
    "start": "873760",
    "end": "881199"
  },
  {
    "start": "878000",
    "end": "937000"
  },
  {
    "text": "the k6 load tools is you know an open source load testing tools you support",
    "start": "881199",
    "end": "887760"
  },
  {
    "text": "sending payload in http and grpc protocols and it's written in gold lane so it's",
    "start": "887760",
    "end": "894240"
  },
  {
    "text": "pretty quick and you you can you can define your workload",
    "start": "894240",
    "end": "900000"
  },
  {
    "text": "in javascript uh it also provides a cli which you can run the",
    "start": "900000",
    "end": "907920"
  },
  {
    "text": "script using k6 run and then it also provides some",
    "start": "907920",
    "end": "913279"
  },
  {
    "text": "functionality for you to check whether your test has met any threshold",
    "start": "913279",
    "end": "921600"
  },
  {
    "text": "to pass or either fail and then it also provides output",
    "start": "921680",
    "end": "927839"
  },
  {
    "text": "integrated to sending output to influx db so later on we can use the grafana dashboard to",
    "start": "927839",
    "end": "935040"
  },
  {
    "text": "visualize it now we've been talking about grpc right",
    "start": "935040",
    "end": "941040"
  },
  {
    "start": "937000",
    "end": "969000"
  },
  {
    "text": "so grpc is the main uh api for model mesh",
    "start": "941040",
    "end": "946560"
  },
  {
    "text": "so that's why we keep talking about jpc but the grpc plus the protocol above is",
    "start": "946560",
    "end": "953120"
  },
  {
    "text": "usually faster than the rest plus json interface because uh",
    "start": "953120",
    "end": "960000"
  },
  {
    "text": "the grpc is is built on http 2 and it can send payload in binary",
    "start": "960000",
    "end": "966959"
  },
  {
    "text": "format",
    "start": "966959",
    "end": "969839"
  },
  {
    "start": "969000",
    "end": "1005000"
  },
  {
    "text": "so last thing i want to talk about is a prometheus operator this operator provides",
    "start": "972079",
    "end": "980959"
  },
  {
    "text": "many things you know by default such as prometheus db and also the no exporter",
    "start": "981440",
    "end": "988880"
  },
  {
    "text": "which collects the linux matrix on your cube cluster and also they provide",
    "start": "988880",
    "end": "994320"
  },
  {
    "text": "grafana which can visualize the matrix and you can view it on your browser",
    "start": "994320",
    "end": "1002880"
  },
  {
    "start": "1005000",
    "end": "1043000"
  },
  {
    "text": "so right now i'll probably show you some demo",
    "start": "1005440",
    "end": "1011839"
  },
  {
    "text": "let's switch to",
    "start": "1018079",
    "end": "1021800"
  },
  {
    "text": "all right",
    "start": "1026319",
    "end": "1028720"
  },
  {
    "text": "so",
    "start": "1034160",
    "end": "1037160"
  },
  {
    "text": "so let's see take a look first we have three notes and then all three",
    "start": "1047600",
    "end": "1054000"
  },
  {
    "text": "this is because this is so slow",
    "start": "1054000",
    "end": "1059039"
  },
  {
    "text": "okay",
    "start": "1062960",
    "end": "1065960"
  },
  {
    "text": "so yeah we have three notes so one note for and then this particular note is for",
    "start": "1069760",
    "end": "1075440"
  },
  {
    "text": "the model mesh deployment uh so our model match deploy is running on",
    "start": "1075440",
    "end": "1081600"
  },
  {
    "text": "a relatively small worker now",
    "start": "1081600",
    "end": "1087360"
  },
  {
    "text": "it has",
    "start": "1087360",
    "end": "1089919"
  },
  {
    "text": "two tritone inference pot only tried on two and then the note",
    "start": "1092799",
    "end": "1098320"
  },
  {
    "text": "itself has eight cpus and 32 gig of memories and all the model match component is",
    "start": "1098320",
    "end": "1104799"
  },
  {
    "text": "running on just one worker node",
    "start": "1104799",
    "end": "1109840"
  },
  {
    "text": "and this is the grafana dashboard so this grafana dashboard is set up to monitor",
    "start": "1111919",
    "end": "1117679"
  },
  {
    "text": "the model mesh matrix and as well as the",
    "start": "1117679",
    "end": "1123600"
  },
  {
    "text": "cluster level system metrics such as cpu and container",
    "start": "1123600",
    "end": "1128960"
  },
  {
    "text": "utilization cpu memory utilizations and as well as the k6",
    "start": "1128960",
    "end": "1135360"
  },
  {
    "text": "matrix from the load driver so at this point",
    "start": "1135360",
    "end": "1140480"
  },
  {
    "text": "let's start a test",
    "start": "1140480",
    "end": "1145520"
  },
  {
    "start": "1141000",
    "end": "1218000"
  },
  {
    "text": "and this is the flow queue for pipeline ui",
    "start": "1147280",
    "end": "1154960"
  },
  {
    "text": "so i'm going to start my test using the q flow pipeline ui",
    "start": "1154960",
    "end": "1160880"
  },
  {
    "text": "so this is a pipeline this pipeline has two components so first the pipeline will",
    "start": "1161600",
    "end": "1167360"
  },
  {
    "text": "deploy 20 models and then it'll run",
    "start": "1167360",
    "end": "1173280"
  },
  {
    "text": "some load tests with the k6 driver so let's start tests real quick",
    "start": "1173280",
    "end": "1182280"
  },
  {
    "text": "okay uh so while it's running test let me show you the number of models on my system so i",
    "start": "1193600",
    "end": "1199679"
  },
  {
    "text": "have 10k models already deployed",
    "start": "1199679",
    "end": "1205039"
  },
  {
    "text": "and uh on the system there are",
    "start": "1205039",
    "end": "1210720"
  },
  {
    "text": "5 000 amnest onyx models we can see",
    "start": "1211039",
    "end": "1217280"
  },
  {
    "text": "and then another 5 000",
    "start": "1217280",
    "end": "1221840"
  },
  {
    "start": "1218000",
    "end": "1272000"
  },
  {
    "text": "cfar pie torch models oh these are very small small models",
    "start": "1223600",
    "end": "1229559"
  },
  {
    "text": "and while the test is running you can see the log here so",
    "start": "1235039",
    "end": "1240480"
  },
  {
    "text": "you can actually see 20 more amnes onyx model has already been",
    "start": "1240480",
    "end": "1245520"
  },
  {
    "text": "deployed now it's running the performance test",
    "start": "1245520",
    "end": "1252480"
  },
  {
    "text": "using k6 so the k6 while it's running it's sending",
    "start": "1252480",
    "end": "1257600"
  },
  {
    "text": "a matrix to influx db which we can graph using the grafana dashboard",
    "start": "1257600",
    "end": "1264559"
  },
  {
    "text": "later okay so let's switch back to",
    "start": "1264559",
    "end": "1270240"
  },
  {
    "text": "the dashboard so these are on the dashboard uh these these two",
    "start": "1270240",
    "end": "1277120"
  },
  {
    "start": "1272000",
    "end": "1527000"
  },
  {
    "text": "numbers are the same because it's basically saying that okay i have",
    "start": "1277120",
    "end": "1282480"
  },
  {
    "text": "10k model registered and 10k model also loaded in the cache",
    "start": "1282480",
    "end": "1290960"
  },
  {
    "text": "and these two numbers here are the number of models on each triton",
    "start": "1290960",
    "end": "1297120"
  },
  {
    "text": "runtime pods so there are roughly 5 600 and",
    "start": "1297120",
    "end": "1302960"
  },
  {
    "text": "four 5400 here uh you may be wondering huh this don't add up to 10k right it's because uh",
    "start": "1302960",
    "end": "1309440"
  },
  {
    "text": "some models are replicated across the two parts and right now as the tests running you",
    "start": "1309440",
    "end": "1315840"
  },
  {
    "text": "can see that each triton is taking",
    "start": "1315840",
    "end": "1320960"
  },
  {
    "text": "a 1000 request currently and the request latency is single digit",
    "start": "1320960",
    "end": "1328960"
  },
  {
    "text": "below like 10 milliseconds or so and also here you can see the number of",
    "start": "1328960",
    "end": "1334720"
  },
  {
    "text": "total requests coming in from the model mesh server side",
    "start": "1334720",
    "end": "1340480"
  },
  {
    "text": "and so this is all the server side data and as the test is running you can see",
    "start": "1340480",
    "end": "1346000"
  },
  {
    "text": "the cpu also increased the serving runtime containers",
    "start": "1346000",
    "end": "1351200"
  },
  {
    "text": "also have higher cpu also the tritone containers cpu becomes higher",
    "start": "1351200",
    "end": "1357840"
  },
  {
    "text": "and this is the memory so you can see uh tritons uses the most memory",
    "start": "1357840",
    "end": "1364320"
  },
  {
    "text": "okay so i think our test has finished let me just zoom in here so you can see a little better",
    "start": "1364320",
    "end": "1372559"
  },
  {
    "text": "all right so this is uh this top three charts are from the k6",
    "start": "1372640",
    "end": "1378720"
  },
  {
    "text": "driver um so the test i was running i was running",
    "start": "1378720",
    "end": "1384880"
  },
  {
    "text": "roughly 100 second load test the first 30 seconds are",
    "start": "1384880",
    "end": "1390400"
  },
  {
    "text": "like a ramp up so you'll see gradually ramp up and then",
    "start": "1390400",
    "end": "1396400"
  },
  {
    "text": "run for 60 seconds and then ramp down so on the client side i'm using 40",
    "start": "1396400",
    "end": "1406240"
  },
  {
    "text": "virtual users to drive this test and these 40 users are running",
    "start": "1406240",
    "end": "1412159"
  },
  {
    "text": "sending payloads as fast as they can sometimes some",
    "start": "1412159",
    "end": "1418880"
  },
  {
    "text": "if you like to emulate real users you may want to add some delayed or",
    "start": "1418880",
    "end": "1424480"
  },
  {
    "text": "timers for each user but for this test i'm only",
    "start": "1424480",
    "end": "1430400"
  },
  {
    "text": "curious about what's the maximum capacity of my model mesh",
    "start": "1430960",
    "end": "1436159"
  },
  {
    "text": "deployment so as you can see 2k it can take 2k requests per second",
    "start": "1436159",
    "end": "1443679"
  },
  {
    "text": "for this particular model i'm testing and it's very similar to what the server",
    "start": "1443679",
    "end": "1449120"
  },
  {
    "text": "the model mesh server metrics as well [Music] but the grpc",
    "start": "1449120",
    "end": "1455600"
  },
  {
    "text": "request duration is a bit higher like here it's reading about 15",
    "start": "1455600",
    "end": "1460880"
  },
  {
    "text": "milliseconds 20 milliseconds but like down here the single digit",
    "start": "1460880",
    "end": "1466000"
  },
  {
    "text": "so that's because the client side may have some network delay and maybe",
    "start": "1466000",
    "end": "1471919"
  },
  {
    "text": "spending some cpu time for processing the response data",
    "start": "1471919",
    "end": "1478080"
  },
  {
    "text": "so it's it's always good to look at the metrics from both sides",
    "start": "1478080",
    "end": "1485278"
  },
  {
    "text": "now the arrow we didn't have any arrows or the arrow is empty uh the error rate is nothing",
    "start": "1485679",
    "end": "1492158"
  },
  {
    "text": "so some this chart here shows that when you know initially when we load 20 more models",
    "start": "1493360",
    "end": "1498960"
  },
  {
    "text": "it showed that model loading and unloading activity here and also the",
    "start": "1498960",
    "end": "1504000"
  },
  {
    "text": "loaded model size here we don't have any activity here in the cache miss",
    "start": "1504000",
    "end": "1510320"
  },
  {
    "text": "because basically all the models are already in the cache",
    "start": "1510320",
    "end": "1515360"
  },
  {
    "text": "so to test this i have some tests also made for this",
    "start": "1515360",
    "end": "1522240"
  },
  {
    "text": "corner case and i think we can run it",
    "start": "1522240",
    "end": "1528400"
  },
  {
    "start": "1527000",
    "end": "1631000"
  },
  {
    "text": "just to show some cash miss action",
    "start": "1528400",
    "end": "1536320"
  },
  {
    "text": "okay so i'm going to run uh this pipeline only has one",
    "start": "1540960",
    "end": "1546799"
  },
  {
    "text": "test in it that's running the k6 stress and for this test i'm going to",
    "start": "1546799",
    "end": "1554320"
  },
  {
    "text": "uh wrong for five minutes and i'm going to",
    "start": "1554320",
    "end": "1561360"
  },
  {
    "text": "send 3000",
    "start": "1561360",
    "end": "1568559"
  },
  {
    "text": "send a payload to 3000 tie torch sci-fi python model and 3000 amnes",
    "start": "1568559",
    "end": "1576559"
  },
  {
    "text": "onyx models",
    "start": "1576559",
    "end": "1579440"
  },
  {
    "text": "uh so i'm here doing all i'm doing here is that if i hit enough uh i'm trying to",
    "start": "1582960",
    "end": "1588480"
  },
  {
    "text": "hit some models that have not been used for a long time and then to see if that",
    "start": "1588480",
    "end": "1594559"
  },
  {
    "text": "we can get some cash missus action",
    "start": "1594559",
    "end": "1599240"
  },
  {
    "text": "so i",
    "start": "1601279",
    "end": "1603600"
  },
  {
    "text": "just switched",
    "start": "1607279",
    "end": "1610520"
  },
  {
    "text": "so while this is running i guess we can take some questions because we're almost",
    "start": "1614880",
    "end": "1620320"
  },
  {
    "text": "at the end of our time yeah sure so",
    "start": "1620320",
    "end": "1626880"
  },
  {
    "text": "uh so we had a question on uh slack that was the question was uh do i have should i run model mesh if i",
    "start": "1626880",
    "end": "1634320"
  },
  {
    "start": "1631000",
    "end": "1767000"
  },
  {
    "text": "just have a few models i think that was the question let me just double check yeah should i consider model mesh when i",
    "start": "1634320",
    "end": "1639760"
  },
  {
    "text": "used to have a few models that is before running into cadence limits on pods ips etc what would you",
    "start": "1639760",
    "end": "1646000"
  },
  {
    "text": "recommend yeah oh it's my mic okay so i think",
    "start": "1646000",
    "end": "1651360"
  },
  {
    "text": "if you just have a few months so the main the main i guess use case for model mesh is for the high scale high density",
    "start": "1651520",
    "end": "1656799"
  },
  {
    "text": "right you have thousands and thousands of models if you only have a few models i think just using this case your single",
    "start": "1656799",
    "end": "1661840"
  },
  {
    "text": "model serving is fits the bill well enough you can easily plug in transformers and explainers just using",
    "start": "1661840",
    "end": "1667039"
  },
  {
    "text": "case serve a single model serving but it's when you really need to kind of",
    "start": "1667039",
    "end": "1673039"
  },
  {
    "text": "load in and out of memory multiple models just to make room and",
    "start": "1673039",
    "end": "1678480"
  },
  {
    "text": "well utilize your cluster is when you want to use model mesh okay it's like so if you're not already hitting",
    "start": "1678480",
    "end": "1684559"
  },
  {
    "text": "limitations and i think just using k-serve is fine it's perfectly fits the bill",
    "start": "1684559",
    "end": "1690240"
  },
  {
    "text": "awesome so take down if you want to explain yeah sure",
    "start": "1690240",
    "end": "1695679"
  },
  {
    "text": "so if we don't have any more questions so let me show you so yeah because we are sending tests to so",
    "start": "1695679",
    "end": "1701200"
  },
  {
    "text": "many different models now we are actually getting some cash misses now in real life this may not be a good thing",
    "start": "1701200",
    "end": "1707679"
  },
  {
    "text": "to do to your server because this actually break the lr the l",
    "start": "1707679",
    "end": "1712960"
  },
  {
    "text": "ru cache but for this for the test we want to test this this is how you can test",
    "start": "1712960",
    "end": "1720799"
  },
  {
    "text": "and here because i'm sending so many payloads to two different models so i'm getting",
    "start": "1721279",
    "end": "1727760"
  },
  {
    "text": "two different response time here for two different models here so i can",
    "start": "1727760",
    "end": "1733919"
  },
  {
    "text": "actually toggle different models here both",
    "start": "1733919",
    "end": "1740158"
  },
  {
    "text": "so that's it for our talk so today we have show you how what model mesh is",
    "start": "1745200",
    "end": "1751520"
  },
  {
    "text": "and then we have show you how to run performance test model mesh and",
    "start": "1751520",
    "end": "1757120"
  },
  {
    "text": "we're actually using some of those tests in the public model mesh github ripple for our nightly",
    "start": "1757120",
    "end": "1763600"
  },
  {
    "text": "build process and we do have a few",
    "start": "1763600",
    "end": "1770240"
  },
  {
    "text": "more talks coming up that's related to this collocated event so feel free",
    "start": "1770240",
    "end": "1776559"
  },
  {
    "text": "to participate those talks thank you very much",
    "start": "1776559",
    "end": "1781840"
  },
  {
    "text": "thank you i think we have time for one or two more questions is there anyone that has questions in the audience",
    "start": "1786000",
    "end": "1792799"
  },
  {
    "text": "there's a question in the back",
    "start": "1793279",
    "end": "1796320"
  },
  {
    "text": "thank you very much for the presentation in the beginning of the presentation you mentioned that you",
    "start": "1803039",
    "end": "1808559"
  },
  {
    "text": "could post models or to save to save resources for money so",
    "start": "1808559",
    "end": "1813760"
  },
  {
    "text": "i'm curious how how do you do this how how how do you post models which are not in use and then you recover them",
    "start": "1813760",
    "end": "1821440"
  },
  {
    "text": "all right so so model mesh so one of the main components of model mesh is the third-party um inference",
    "start": "1821440",
    "end": "1828880"
  },
  {
    "text": "services or servers so we have nvidia's triton it could be intel's open",
    "start": "1828880",
    "end": "1833919"
  },
  {
    "text": "vino or selden's ml servers all of these servers support",
    "start": "1833919",
    "end": "1839440"
  },
  {
    "text": "loading have a load and unload api where you can load in models and",
    "start": "1839440",
    "end": "1844799"
  },
  {
    "text": "unload models and so model mesh acts as kind of like a management management layer on top of that to load",
    "start": "1844799",
    "end": "1851200"
  },
  {
    "text": "and unload based off of usage or usage demand and based off of usage recency so",
    "start": "1851200",
    "end": "1857919"
  },
  {
    "text": "pretty much model mesh handles the loading and unloading just it's a it treats all of the serving runtime",
    "start": "1857919",
    "end": "1863760"
  },
  {
    "text": "deployments and these servers as an lru cache right and it'll if there's if it's already used of all",
    "start": "1863760",
    "end": "1869519"
  },
  {
    "text": "of the memory capacity allocated for these models and it'll bump out the least recently used model using the",
    "start": "1869519",
    "end": "1875600"
  },
  {
    "text": "unload api for the model servers then we'll load in or scale up",
    "start": "1875600",
    "end": "1881360"
  },
  {
    "text": "the model being requested and so that's kind of the kind of the notion behind model mesh",
    "start": "1881360",
    "end": "1887519"
  },
  {
    "text": "and kind of making making use of all the cluster capacity as much as possible",
    "start": "1887519",
    "end": "1893519"
  },
  {
    "text": "do more with less",
    "start": "1893519",
    "end": "1896960"
  }
]