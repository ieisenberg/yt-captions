[
  {
    "start": "0",
    "end": "160000"
  },
  {
    "text": "okay now can you yeah that's better right where was I so let's start with a bit of audience participation who here",
    "start": "30",
    "end": "7109"
  },
  {
    "text": "is already using Prometheus oh wow okay I'm gonna yeah I'm gonna have to",
    "start": "7109",
    "end": "13860"
  },
  {
    "text": "make this a bit more interesting then [Music] right then how about mean I guess is",
    "start": "13860",
    "end": "20609"
  },
  {
    "text": "where as we're here we're probably also using kubernetes hands up so more people",
    "start": "20609",
    "end": "25949"
  },
  {
    "text": "using kubernetes and Prometheus and using kubernetes and Prometheus together okay that's it's not as many as I was",
    "start": "25949",
    "end": "32398"
  },
  {
    "text": "expecting actually with the over set the overlap all right then as I said my name's tom started this company called causal",
    "start": "32399",
    "end": "37440"
  },
  {
    "text": "recently we do a hosted version of Prometheus basically I also make my own beer which kind of explains and that's",
    "start": "37440",
    "end": "43350"
  },
  {
    "text": "also the barbecue for lunch used to work for a company called weave works who also here and also do a lot of work on",
    "start": "43350",
    "end": "48480"
  },
  {
    "text": "prometheus before that I was at Google for a few years and before that another startup so we're going to cover four things today",
    "start": "48480",
    "end": "54960"
  },
  {
    "text": "we've kind of done the first already the use method I know that the talk was pitched as the read method but we're",
    "start": "54960",
    "end": "60989"
  },
  {
    "text": "actually gonna talk about the use method first because that's kind of my inspiration at least for the naming scheme the read method which is what we",
    "start": "60989",
    "end": "68610"
  },
  {
    "text": "have mainly here to talk about and then the footballs for golden signals if anyone here has read the the Google SAE",
    "start": "68610",
    "end": "74280"
  },
  {
    "text": "handbook so quickly the introduction why does this matter why do we even need patterns and why should we even care",
    "start": "74280",
    "end": "80159"
  },
  {
    "text": "about instrumenting our applications so I'll start first with why I'm giving",
    "start": "80159",
    "end": "85680"
  },
  {
    "text": "this talk there was a recent recent prometheus conference in Munich and the",
    "start": "85680",
    "end": "90780"
  },
  {
    "text": "read method was alluded to multiple different talks and no one actually explained what it was and more and more",
    "start": "90780",
    "end": "97079"
  },
  {
    "text": "like I'm seeing mentions of it on Twitter and things like this and no one's actually kind of come along and said this is what the read method is so",
    "start": "97079",
    "end": "104040"
  },
  {
    "text": "as I kind of tongue-in-cheek came up with this name a couple years ago I thought there should be an authoritative",
    "start": "104040",
    "end": "109950"
  },
  {
    "text": "talk on what the read method is at least from my point of view so why are these patterns important well if you're a",
    "start": "109950",
    "end": "116040"
  },
  {
    "text": "software engineer you probably don't want to spend your life thinking about the best way to instrument your code you",
    "start": "116040",
    "end": "122340"
  },
  {
    "text": "know probably more important what your code actually does so by having these patterns and by having kind of tried and",
    "start": "122340",
    "end": "127860"
  },
  {
    "text": "tested methods hopefully it can you know reduce a bit of the cognitive load and you just go well this guy I saw a",
    "start": "127860",
    "end": "133660"
  },
  {
    "text": "conference said this is the best way to do this so I'm just going to go and do it that way there's a there's a whole",
    "start": "133660",
    "end": "139750"
  },
  {
    "text": "debate about white box versus black box monitoring if you're already using from easiest it sounds like most of you are then we've already won that debate and",
    "start": "139750",
    "end": "146110"
  },
  {
    "text": "white box monitoring it is and if you want the debate about push versus pull you're in the wrong place where that's a",
    "start": "146110",
    "end": "154450"
  },
  {
    "text": "bit of an in-joke sorry all the examples I'll give will be Prometheus running on kubernetes as well so to start with use method this was",
    "start": "154450",
    "end": "162190"
  },
  {
    "start": "160000",
    "end": "234000"
  },
  {
    "text": "invented by a chap called Brendan Greg or at least coined by him and defines three things for every resource in your",
    "start": "162190",
    "end": "169690"
  },
  {
    "text": "system you're going to monitor the utilization the saturation and the errors okay and Brendan has this",
    "start": "169690",
    "end": "177040"
  },
  {
    "text": "absolutely excellent like de facto webpage that describes this incredible detail and I'm just going to kind of",
    "start": "177040",
    "end": "183459"
  },
  {
    "text": "lightly skim through it a little bit here so utilization how much time like wall clock time was the resource busy",
    "start": "183459",
    "end": "189610"
  },
  {
    "text": "for as a percentage saturation this one kind of doesn't have such a nice definition it's kind of how much work",
    "start": "189610",
    "end": "196540"
  },
  {
    "text": "did this thing have to do and so maybe per resource the units for this are going to be different I like to try and",
    "start": "196540",
    "end": "203470"
  },
  {
    "text": "find a way to normalize it to a percentage because then I don't really have to like interpret the number and",
    "start": "203470",
    "end": "209290"
  },
  {
    "text": "then errors just what is the rate of errors for this thing so resources are",
    "start": "209290",
    "end": "214420"
  },
  {
    "text": "CPUs disks memory network some things people normally forget things like interconnects yeah and so on and this is",
    "start": "214420",
    "end": "223600"
  },
  {
    "text": "very very targeted at infrastructure at real things it'd be quite challenging to",
    "start": "223600",
    "end": "229930"
  },
  {
    "text": "apply this to your micro services which is why we did the read method anyway moving on you end up with this kind of",
    "start": "229930",
    "end": "236650"
  },
  {
    "start": "234000",
    "end": "284000"
  },
  {
    "text": "checklist approach and when you've got a problem I don't know about you but I find my problems kind of bisects quite",
    "start": "236650",
    "end": "243430"
  },
  {
    "text": "nicely and there's half of my problems I like a very obvious after 10 minutes of fiddling and then there's the other half",
    "start": "243430",
    "end": "248980"
  },
  {
    "text": "which I have no clue what the problem is and I really don't know what I don't know so one of the nice things about",
    "start": "248980",
    "end": "254890"
  },
  {
    "text": "having these kind of checklist based approaches they really help in the second instance in the instance when you",
    "start": "254890",
    "end": "260410"
  },
  {
    "text": "don't know what's wrong it's quite reassuring to go look if I just go and check all of these things in order then I'm likely to find a problem",
    "start": "260410",
    "end": "267699"
  },
  {
    "text": "that gives me the clue for the next thing so you end up with this kind of checklist and if you're like me you turn",
    "start": "267699",
    "end": "273970"
  },
  {
    "text": "this into a dashboard yeah you turn you're you're hunting for problems into",
    "start": "273970",
    "end": "279759"
  },
  {
    "text": "something that's a bit more methodical it but a little bit less kind of haphazard so diving into the real stuff",
    "start": "279759",
    "end": "285039"
  },
  {
    "start": "284000",
    "end": "400000"
  },
  {
    "text": "how'd you do this with Prometheus well CPU utilization one of them is that I love from atheists because the queries",
    "start": "285039",
    "end": "290560"
  },
  {
    "text": "fit on a single page so you get nice neat queries like this this is using the",
    "start": "290560",
    "end": "295870"
  },
  {
    "text": "node exporter which is a primitive exporter you run on every machine if you're using cuban Hattie's use a daemon",
    "start": "295870",
    "end": "301090"
  },
  {
    "text": "set and then you go in and you say for each CPU in the machine how much time",
    "start": "301090",
    "end": "307539"
  },
  {
    "text": "was it idle for and you take that as a percentage of the total time and you take one out and take it away from one",
    "start": "307539",
    "end": "313419"
  },
  {
    "text": "so this is super easy right CPU saturation this one's where it gets a little bit more interesting CPU",
    "start": "313419",
    "end": "318819"
  },
  {
    "text": "saturations digitally measured as load average so I picked the one minute load average here load average for those who",
    "start": "318819",
    "end": "324280"
  },
  {
    "text": "aren't aware is the number of processes on the run queue like a sampled average",
    "start": "324280",
    "end": "330729"
  },
  {
    "text": "of that so this is you can quite easily have you know a 10 core box or tenza with them but a 16 core box and have 300",
    "start": "330729",
    "end": "339520"
  },
  {
    "text": "things waiting for CPU time and then your load average would be 300 and so on because I don't like to interpret these",
    "start": "339520",
    "end": "345009"
  },
  {
    "text": "numbers as kind of what does 300 mean I like to take it as a proportion of of",
    "start": "345009",
    "end": "350710"
  },
  {
    "text": "the number of CPUs which is why I've divided it by number of CPUs this one here is a open do that this one here is",
    "start": "350710",
    "end": "358419"
  },
  {
    "text": "a handy little recording wall that I've written that gives me per node the number of CPUs Missha punishing you'll",
    "start": "358419",
    "end": "365979"
  },
  {
    "text": "notice I don't have errors on this slide so this is really the us method so far it's a terrible joke a cpu errors are",
    "start": "365979",
    "end": "374560"
  },
  {
    "text": "kind of really hard to measure it turns out I looked into a bunch of places and the best advice I got was like wrapping",
    "start": "374560",
    "end": "380259"
  },
  {
    "text": "through D message for like exceptions and machine check exceptions and they're kind of machines specific and they're",
    "start": "380259",
    "end": "387370"
  },
  {
    "text": "kind of like that the Intel processors have some nice counters on but they're different on other processors and so on",
    "start": "387370",
    "end": "392409"
  },
  {
    "text": "so I don't really have a way to do that but luckily CPU errors don't seem to be a thing I need to worry about",
    "start": "392409",
    "end": "399330"
  },
  {
    "text": "this will get pretty dull if I was to go through every single query in this level of detail so I'm actually not going to",
    "start": "399419",
    "end": "405490"
  },
  {
    "start": "400000",
    "end": "430000"
  },
  {
    "text": "do that I'm actually just going to jump straight to the dashboards and show you what I've got this is another good",
    "start": "405490",
    "end": "411039"
  },
  {
    "text": "example this memory one of where saturation is in some weird units and the best advice I've seen for how to",
    "start": "411039",
    "end": "417009"
  },
  {
    "text": "measure your memory saturation is actually to look at how much paging your machine is doing because if it's doing a",
    "start": "417009",
    "end": "422740"
  },
  {
    "text": "lot it kind of tells you that your memory is being is oversubscribed and if it's not doing any then saturation is",
    "start": "422740",
    "end": "428650"
  },
  {
    "text": "not a problem the list really does go on you know I've already touched on CPU errors and memory errors are another",
    "start": "428650",
    "end": "434470"
  },
  {
    "text": "thing that are really hard to actually get a counter for I found it really hard to get a counter for hard disk errors",
    "start": "434470",
    "end": "439810"
  },
  {
    "text": "you'd think this would be quite obvious it turns out there is a counter in Sisyphus but Prometheus doesn't expose",
    "start": "439810",
    "end": "447160"
  },
  {
    "text": "it yet so I was hoping to get it done before I gave this talk but I'm going to plum that through node exporter so you",
    "start": "447160",
    "end": "452560"
  },
  {
    "text": "can actually have a counter disk capacity versus disk i/o I don't know about you but disk errors have never",
    "start": "452560",
    "end": "458740"
  },
  {
    "text": "caused an outage for me whereas running out of this capacity is probably the root cause of most of my outages so this",
    "start": "458740",
    "end": "465070"
  },
  {
    "text": "capacity doesn't really seem to fit into this use model very well maybe this is a criticism of a model or my understanding",
    "start": "465070",
    "end": "470199"
  },
  {
    "text": "of it network utilization is a another interesting one because I really want to have this as a percentage of some like",
    "start": "470199",
    "end": "476949"
  },
  {
    "text": "total but I've not found a way to kind of know the actual capacity of my link",
    "start": "476949",
    "end": "482199"
  },
  {
    "text": "like programmatically so currently I just in my dashboard just say look I know that for this machine type on AWS I",
    "start": "482199",
    "end": "488500"
  },
  {
    "text": "have this much network traffic and network bandwidth and then I use that as the denominator and get my percentage",
    "start": "488500",
    "end": "493780"
  },
  {
    "text": "but I'm not very happy with that ya know so for that way I also don't have any",
    "start": "493780",
    "end": "500740"
  },
  {
    "text": "clue how to measure interconnects so let's see if this MiFi works we've done a set of dashboards",
    "start": "500740",
    "end": "510720"
  },
  {
    "text": "okay so if we go to the yes good so I",
    "start": "512649",
    "end": "523899"
  },
  {
    "text": "didn't mean to catch of meteors I mean to go to the fauna if we go to our graph",
    "start": "523899",
    "end": "529119"
  },
  {
    "text": "honor I've published this set of dashboards that I'm calling and I love this name I'm calling clumps because it's the",
    "start": "529119",
    "end": "535959"
  },
  {
    "text": "kubernetes Linux use method with Prometheus",
    "start": "535959",
    "end": "540688"
  },
  {
    "text": "so David my co-founder of my company absolutely hates this name and really",
    "start": "541860",
    "end": "547179"
  },
  {
    "text": "didn't want me to include that joke so if anyone has like a better suggestion for a name then please do like come",
    "start": "547179",
    "end": "552610"
  },
  {
    "text": "forward and let me know because clumps is a bit bit weak anyway",
    "start": "552610",
    "end": "558100"
  },
  {
    "text": "you end up with these use method dashboards and so this is a cluster wide use method dashboard with every single",
    "start": "558100",
    "end": "564819"
  },
  {
    "text": "one of those queries I talked about encoded into it you know you can see my cluster is not particularly heavily",
    "start": "564819",
    "end": "570490"
  },
  {
    "text": "loaded and it's just really straightforward each row is one resource each column is a different bit of the",
    "start": "570490",
    "end": "576369"
  },
  {
    "text": "use method this is all open source if you're interested it's on github in who",
    "start": "576369",
    "end": "582490"
  },
  {
    "text": "else here does ma know repos yeah me too we have a public and a private mono repo",
    "start": "582490",
    "end": "588850"
  },
  {
    "text": "and I'm not sure it's a good idea yet but so all of our open source projects deliver it cause of all public and",
    "start": "588850",
    "end": "594220"
  },
  {
    "text": "Klumps is here so if you want them the lot of the dashboards Agrafena",
    "start": "594220",
    "end": "599290"
  },
  {
    "text": "dashboards they're generated with something called JSON it which is like this really cool conflict management language for like merging JSON",
    "start": "599290",
    "end": "605439"
  },
  {
    "text": "dictionaries and doing all sorts of really cool stuff so you have to like generate them but this means you can parameterize them if you use slightly",
    "start": "605439",
    "end": "611170"
  },
  {
    "text": "different job name for various like node exporters named differently on your cluster then you can just bang that in and generate out the dashboards and that",
    "start": "611170",
    "end": "618819"
  },
  {
    "text": "is pretty much the user for dashboards I want to iterate on these and get them to include kind of the fixes that I've discussed here and get them to include",
    "start": "618819",
    "end": "624910"
  },
  {
    "text": "disk errors and so on and and maybe other people will find them useful and we're also going to add things like drill down so you can see it broken down",
    "start": "624910",
    "end": "631209"
  },
  {
    "text": "by host and by service and so on anyway that is clumps",
    "start": "631209",
    "end": "637529"
  },
  {
    "text": "I swear keynote used to make this",
    "start": "640400",
    "end": "645680"
  },
  {
    "text": "switching between mirrored and not mirror and much easier so yeah if you",
    "start": "645680",
    "end": "651530"
  },
  {
    "text": "want to do some further reading on this the use method website by Brendan Gregg is absolutely fantastic",
    "start": "651530",
    "end": "656750"
  },
  {
    "text": "goes into a lot more detail and has a lot more kind of pros and cons and side effects and so on you should definitely",
    "start": "656750",
    "end": "662150"
  },
  {
    "text": "read that and then we've got the Klumps dashboards that I just mentioned the read method what we're really here to",
    "start": "662150",
    "end": "667940"
  },
  {
    "start": "666000",
    "end": "812000"
  },
  {
    "text": "talk about so the read method was kind of inspired by a question we had at work which was like why aren't we using the",
    "start": "667940",
    "end": "675620"
  },
  {
    "text": "use method and I found it really hard to explain to people that like the use method is for resources and the read",
    "start": "675620",
    "end": "680720"
  },
  {
    "text": "method is for my services and they're kind of two different things that we do use the use method but the use method doesn't tell me if my services are",
    "start": "680720",
    "end": "686600"
  },
  {
    "text": "healthy they tell me if my machines are healthy so the read method is a bunch of stuff I learned at Google that I kind of",
    "start": "686600",
    "end": "693530"
  },
  {
    "text": "miss remembered and so if you're familiar with Google's for golden signals it includes saturation and when I came up with this I forgot about that",
    "start": "693530",
    "end": "699560"
  },
  {
    "text": "one it's also like use has three characters so this needs three characters and I couldn't really fit",
    "start": "699560",
    "end": "705770"
  },
  {
    "text": "away of you know it could be the Reds method but that's a bit bit lame so the red method is for every service in your",
    "start": "705770",
    "end": "710900"
  },
  {
    "text": "architecture monitor the request rate the rate of errors of those requests and",
    "start": "710900",
    "end": "716360"
  },
  {
    "text": "the duration of those requests is some kind of distribution why well this is a",
    "start": "716360",
    "end": "723980"
  },
  {
    "text": "nice way of knowing kind of without knowing what the service does or how it works knowing if the service is healthy ish",
    "start": "723980",
    "end": "730610"
  },
  {
    "text": "right this is really useful for when you've got an on-call team that are on-call for things they didn't write and",
    "start": "730610",
    "end": "736760"
  },
  {
    "text": "and if you want kind of your DevOps to to scale and to be something you do as a company eventually you're going to have",
    "start": "736760",
    "end": "743120"
  },
  {
    "text": "to be on call for stuff you didn't write you know eventually someone else is going to have to be on call for your code and you're gonna have to be on that",
    "start": "743120",
    "end": "748490"
  },
  {
    "text": "call for their code and that's how you build this kind of scalable you know operations team so to do that you'd not",
    "start": "748490",
    "end": "755030"
  },
  {
    "text": "it's nice to have a nice uniform method of like knowing what a service does knowing that if a service is healthy so",
    "start": "755030",
    "end": "761450"
  },
  {
    "text": "the read method it also is nice because this is kind of the building blocks for",
    "start": "761450",
    "end": "766760"
  },
  {
    "text": "being SLA driven you should really only care if your customers are happy you shouldn't care if your computers are happy",
    "start": "766760",
    "end": "772570"
  },
  {
    "text": "so this gets like starts getting towards high-level metrics that tell you if your customers are happy you know if things",
    "start": "772570",
    "end": "778420"
  },
  {
    "text": "are slow if your durations and Layton seas are really high then your customers probably aren't happy if your errors are",
    "start": "778420",
    "end": "784990"
  },
  {
    "text": "high they're definitely not happy if your rates not high that's an interesting one maybe you're just not popular enough so this started this I",
    "start": "784990",
    "end": "793600"
  },
  {
    "text": "think is the first mention I could do I had a different haircut at this time this is the first mention I could find",
    "start": "793600",
    "end": "799360"
  },
  {
    "text": "of the read method and this kind of caused a big storm because the person said an alternative and really it should",
    "start": "799360",
    "end": "805360"
  },
  {
    "text": "be a kind of an addition like a different method for monitoring a different thing anyway how do we do this",
    "start": "805360",
    "end": "810370"
  },
  {
    "text": "with Prometheus so that's what we're here to look at right it turns out it's really easy to do with Prometheus you",
    "start": "810370",
    "end": "815920"
  },
  {
    "start": "812000",
    "end": "853000"
  },
  {
    "text": "have a block of code like this this declares the Prometheus metrics so I have a laser pointer but um this",
    "start": "815920",
    "end": "823540"
  },
  {
    "text": "declares the metric we're going to use a single histogram to represent all three of the attributes of the read method and",
    "start": "823540",
    "end": "830290"
  },
  {
    "text": "we're going to call it by best practices we're going to include the SI unit we're going to record things in seconds not in",
    "start": "830290",
    "end": "835510"
  },
  {
    "text": "milliseconds we're going to include various different labels and this is how",
    "start": "835510",
    "end": "840520"
  },
  {
    "text": "we're going to differ a JIT between successful and failed requests so we've got a status code in there and then",
    "start": "840520",
    "end": "845710"
  },
  {
    "text": "we're going to register that metric so you need this once in your application normally at the top of like main logo",
    "start": "845710",
    "end": "852150"
  },
  {
    "text": "then I prefer the approach of having a piece of middleware that I can wrap around my HTTP handlers that will go and",
    "start": "852150",
    "end": "860470"
  },
  {
    "start": "853000",
    "end": "993000"
  },
  {
    "text": "instrument them instead of kind of manually putting the instrumentation into every single HTTP handler there's a",
    "start": "860470",
    "end": "867880"
  },
  {
    "text": "nice set of generic middleware out there there's a Prometheus one the Weaver works guys have one in their common repo which is",
    "start": "867880",
    "end": "874570"
  },
  {
    "text": "one I use and this is the the one I've come up with here this uses something",
    "start": "874570",
    "end": "880330"
  },
  {
    "text": "called HTTP snoop which is a really cool little go library if you've ever if you've never seen it that gives you this",
    "start": "880330",
    "end": "885490"
  },
  {
    "text": "fake response writer that records a whole bunch of metrics as it's being used like how long it was used for and",
    "start": "885490",
    "end": "891730"
  },
  {
    "text": "what the error code was and how much bytes were sent in so much and then actually recording the metric is just",
    "start": "891730",
    "end": "898000"
  },
  {
    "text": "that request duration with labels method path code so on if anyone can spot the obvious mistake",
    "start": "898000",
    "end": "905230"
  },
  {
    "text": "on this they can win a causal sticker if anyone",
    "start": "905230",
    "end": "913030"
  },
  {
    "text": "can spot the other obvious mistake on this code is there an extra paren yeah",
    "start": "913030",
    "end": "923980"
  },
  {
    "text": "no no that's correct actually that's closing the one after the wig labels with label values yeah attracted no so",
    "start": "923980",
    "end": "931840"
  },
  {
    "text": "the the obvious or they've got different",
    "start": "931840",
    "end": "937120"
  },
  {
    "text": "paths to maps and I think that's so the",
    "start": "937120",
    "end": "942400"
  },
  {
    "text": "obvious mistake here is I've used the URL path as the label and like wool zero of Prometheus rules is don't use high",
    "start": "942400",
    "end": "950020"
  },
  {
    "text": "cardinality values in your labels because this will really overload the time series data base underneath it so",
    "start": "950020",
    "end": "956290"
  },
  {
    "text": "actually what we tend to use instead of using URL paths which will include things like you know whatever you know random user IDs you've got in your path",
    "start": "956290",
    "end": "964050"
  },
  {
    "text": "we tend to erase out the high cardinality things so if you if you defined like if you're using like",
    "start": "964050",
    "end": "970540"
  },
  {
    "text": "gorilla bucks and you define your your pattern matcher for your path we would",
    "start": "970540",
    "end": "975640"
  },
  {
    "text": "use that instead of the actual path and this helps reduce the cardinality help to keep your queries fast basically so I",
    "start": "975640",
    "end": "981910"
  },
  {
    "text": "get to keep my stick anyway and then we use it by we serve the Prometheus metrics so Prometheus can come and on",
    "start": "981910",
    "end": "988060"
  },
  {
    "text": "and scrape them and we serve a function which won't compile because it's three dots so then to get the rate of requests",
    "start": "988060",
    "end": "997660"
  },
  {
    "start": "993000",
    "end": "1239000"
  },
  {
    "text": "is super easy you just look at the request Eurasian seconds count and so this is kind of an internal",
    "start": "997660",
    "end": "1003570"
  },
  {
    "text": "implementation detail of how histograms are implemented in Prometheus is they actually export a whole bunch of time",
    "start": "1003570",
    "end": "1010290"
  },
  {
    "text": "series they don't just export a single one they explore account sum and these buckets and so we use this as a count to",
    "start": "1010290",
    "end": "1018990"
  },
  {
    "text": "see how many requests are going and then we do a count and we filter them by the status code that is not 200 and that",
    "start": "1018990",
    "end": "1025680"
  },
  {
    "text": "tells us the error request and similarly we use a histogram quantile with a very complicated expression I can never write",
    "start": "1025680",
    "end": "1032040"
  },
  {
    "text": "first time and that is missing a quote to tell history so let's have a quick",
    "start": "1032040",
    "end": "1038670"
  },
  {
    "text": "look at how this will work this time I",
    "start": "1038670",
    "end": "1049620"
  },
  {
    "text": "do want Prometheus so actually I would",
    "start": "1049620",
    "end": "1057240"
  },
  {
    "text": "normally type it in here but I find it much easier to use my product of course but this is not a pitch you can do this just in normal Prometheus",
    "start": "1057240",
    "end": "1063240"
  },
  {
    "text": "so first we'll do some rate request",
    "start": "1063240",
    "end": "1069260"
  },
  {
    "text": "duration and the reason I find it easier because it's got this auto company so we",
    "start": "1069260",
    "end": "1075420"
  },
  {
    "text": "can do we're going to need a rate of that and I'm going to filter this so you're only seeing the dev traffic okay",
    "start": "1075420",
    "end": "1084240"
  },
  {
    "text": "there we go yeah but we can start to do more interesting stuff if we go and",
    "start": "1084240",
    "end": "1090660"
  },
  {
    "text": "break this down by by request sorry by status code so here we go we're going to drill down into the status code and now",
    "start": "1090660",
    "end": "1099840"
  },
  {
    "text": "we can see that there is no errors per second and there are 36 QPS to our dev",
    "start": "1099840",
    "end": "1105120"
  },
  {
    "text": "front-end so this is like the first two bits of the read method in a single",
    "start": "1105120",
    "end": "1110190"
  },
  {
    "text": "query and then this is where it gets way more interesting because I have to write and get right first time a histogram",
    "start": "1110190",
    "end": "1117420"
  },
  {
    "text": "want I'll which I almost never get right so we're going to give us some and rate",
    "start": "1117420",
    "end": "1123150"
  },
  {
    "text": "and we're gonna do it request spelling",
    "start": "1123150",
    "end": "1131190"
  },
  {
    "text": "as opposed now this time I use the bucket prefects and this tells us a set of cumulative buckets that the",
    "start": "1131190",
    "end": "1138300"
  },
  {
    "text": "histograms counted over and then it will kind of interpolate between those buckets to tell you whether 99th",
    "start": "1138300",
    "end": "1143610"
  },
  {
    "text": "percentile is again I'm just going to do it for dev because it's much faster than the product and I'm going to the last",
    "start": "1143610",
    "end": "1156030"
  },
  {
    "text": "minute and then the thing I always forget is to break it down by this le and this is really where the leaky abstraction comes in for Prometheus the",
    "start": "1156030",
    "end": "1162960"
  },
  {
    "text": "le is the dimension we use in Prometheus to differentiate in different buckets then I run that and",
    "start": "1162960",
    "end": "1170350"
  },
  {
    "text": "it worked so that's a six milliseconds I think for requests at the 90th",
    "start": "1170350",
    "end": "1176500"
  },
  {
    "text": "percentile you can change this just to give you a taste of what this looks like if I didn't do all of that jazz reviews",
    "start": "1176500",
    "end": "1183700"
  },
  {
    "text": "as a table if I just look at the raw if",
    "start": "1183700",
    "end": "1189760"
  },
  {
    "text": "I just look at the raw set of time series that query returns you can see",
    "start": "1189760",
    "end": "1194890"
  },
  {
    "text": "this le dimension has like of various numbers like all the requests would be",
    "start": "1194890",
    "end": "1199900"
  },
  {
    "text": "in the plus infinity 1 and then only ones that took one second or lower being 12.5 and so on and you can see it broken",
    "start": "1199900",
    "end": "1206020"
  },
  {
    "text": "down my method by namespace you can even see the route who got it right in America doesn't include user names so",
    "start": "1206020",
    "end": "1214840"
  },
  {
    "text": "there we go that's that's how you do the use method with Prometheus just see",
    "start": "1214840",
    "end": "1221470"
  },
  {
    "text": "what's gonna yeah okay and I like this approach because it's just super simple",
    "start": "1221470",
    "end": "1227760"
  },
  {
    "text": "there's not a lot there's one one thing you need to instrument all your apps with most companies probably have a",
    "start": "1227760",
    "end": "1233380"
  },
  {
    "text": "common library that they can put this into and so the developers don't even need to think about this when it gets a",
    "start": "1233380",
    "end": "1239860"
  },
  {
    "start": "1239000",
    "end": "1261000"
  },
  {
    "text": "bit more interesting is your you know your generic micro service architecture usually looks like lots of services that",
    "start": "1239860",
    "end": "1245350"
  },
  {
    "text": "all talk to each other and if you're really lucky there's no loops obviously",
    "start": "1245350",
    "end": "1250600"
  },
  {
    "text": "I just talked about how to instrument a single service but generally you actually want to instrument your whole service you know the collection of Micra",
    "start": "1250600",
    "end": "1256960"
  },
  {
    "text": "services so if you think about this as as a graph of services and you do a kind of traversal of that graph you know we",
    "start": "1256960",
    "end": "1263410"
  },
  {
    "start": "1261000",
    "end": "1304000"
  },
  {
    "text": "do it breadth-first but you can do that first if you like and then you build a graph a dashboard that looks like this where one column is QPS broken down by",
    "start": "1263410",
    "end": "1271300"
  },
  {
    "text": "errors and the other column is latency then you get to kind of you get these nice standardized dashboards that you",
    "start": "1271300",
    "end": "1277450"
  },
  {
    "text": "can understand whether you've seen the service or not and it also allows you to see where the errors are coming in you know if you've got some errors at the",
    "start": "1277450",
    "end": "1283270"
  },
  {
    "text": "top you basically have to scroll down until the errors stop and it's the service above that that's introducing those errors if you if you kind of go",
    "start": "1283270",
    "end": "1290230"
  },
  {
    "text": "what I mean this is kind of me taking that graph and traversing it row by row so I find this a really nice way of",
    "start": "1290230",
    "end": "1297130"
  },
  {
    "text": "writing dashboards and then to the extent where this completely automated now like the system will just generate these",
    "start": "1297130",
    "end": "1302500"
  },
  {
    "text": "for me using JSON the other one this is something I learned the hard way when",
    "start": "1302500",
    "end": "1308410"
  },
  {
    "start": "1304000",
    "end": "1380000"
  },
  {
    "text": "you're plotting your latency graphs you should never plot average right you should never use average this has been",
    "start": "1308410",
    "end": "1313420"
  },
  {
    "text": "drummed into over the multiple years of doing monitoring but except for you'll notice I've got a verage plotted here and there's two reasons I plot average",
    "start": "1313420",
    "end": "1320230"
  },
  {
    "text": "one is because Layton sees done some you know if you've got one service that's",
    "start": "1320230",
    "end": "1325270"
  },
  {
    "text": "got a really high latency and you look at the two services it calls the you know it quite quite often happens that they've got really low latency and it",
    "start": "1325270",
    "end": "1331120"
  },
  {
    "text": "just doesn't help you because latency use do not some like like that averages do some so if you've got one service",
    "start": "1331120",
    "end": "1336790"
  },
  {
    "text": "with a very high average then one of them below is going to have a very high average and that helps you point the",
    "start": "1336790",
    "end": "1343480"
  },
  {
    "text": "second reason is that averages can be used to sanity check whether you've got",
    "start": "1343480",
    "end": "1348790"
  },
  {
    "text": "your buckets right and this is the reason I was talking about Elian buckets for histograms like it's an implementation detail but it's when",
    "start": "1348790",
    "end": "1354130"
  },
  {
    "text": "people get wrong sometimes so sometimes you'll see a nice low latency no problems at all because you've only",
    "start": "1354130",
    "end": "1359830"
  },
  {
    "text": "declared your buckets go up to 100 milliseconds and obviously it can't report more than 100 milliseconds and",
    "start": "1359830",
    "end": "1364960"
  },
  {
    "text": "then you go and plot your average which is taken just by dividing the count by the sum note the sum by the count and",
    "start": "1364960",
    "end": "1372210"
  },
  {
    "text": "and then you see that's it like you know tens of seconds and you've realized you've got your buckets wrong so I like",
    "start": "1372210",
    "end": "1377830"
  },
  {
    "text": "to include average in there if you want to be more about the read method there's",
    "start": "1377830",
    "end": "1383050"
  },
  {
    "start": "1380000",
    "end": "1401000"
  },
  {
    "text": "a bunch of work I did we've works but it's really taken off with other companies now there's Cindy's got a this",
    "start": "1383050",
    "end": "1388330"
  },
  {
    "text": "fantastic blog post about logs and methods and metrics which I'd really go into the slides will be online and these",
    "start": "1388330",
    "end": "1394420"
  },
  {
    "text": "are clickable links have them so there's not much porting taken photo but yeah that's peter has good writings on these",
    "start": "1394420",
    "end": "1400900"
  },
  {
    "text": "as well so finally we're going to talk about the four golden signals so this is",
    "start": "1400900",
    "end": "1405940"
  },
  {
    "start": "1401000",
    "end": "1416000"
  },
  {
    "text": "what I should have done when I did the read method and this is not a sticker that you put on your mobile phone to",
    "start": "1405940",
    "end": "1411160"
  },
  {
    "text": "improve your reception how can that even work it doesn't so you'll notice this is very",
    "start": "1411160",
    "end": "1419560"
  },
  {
    "start": "1416000",
    "end": "1431000"
  },
  {
    "text": "similar to the read method and it just has saturation you know latency is the same as duration traffic is the same as",
    "start": "1419560",
    "end": "1425560"
  },
  {
    "text": "well requests errors is the same but this saturation thing how full your services is like okay",
    "start": "1425560",
    "end": "1432860"
  },
  {
    "start": "1431000",
    "end": "1445000"
  },
  {
    "text": "is that your definition how full your services and one of the reasons I excluded it is because what does that",
    "start": "1432860",
    "end": "1438140"
  },
  {
    "text": "even mean but it turns out if you're using premier if you're using cube Nettie's there's a nice definition of this right so everyone puts ricourt",
    "start": "1438140",
    "end": "1446600"
  },
  {
    "start": "1445000",
    "end": "1758000"
  },
  {
    "text": "resource requests on their pods right I'm not seeing many people nod you",
    "start": "1446600",
    "end": "1452090"
  },
  {
    "text": "should put the resource requests on your pod you should be saying this pod leads one CPU or a tenth of a CPU or so on and",
    "start": "1452090",
    "end": "1457850"
  },
  {
    "text": "this gives you a nice denominator to tell how full your services so this one",
    "start": "1457850",
    "end": "1464450"
  },
  {
    "text": "I am going to cheat with I'm afraid because it's quite a handful to go into",
    "start": "1464450",
    "end": "1469580"
  },
  {
    "text": "cube State metrics and see advisor and build this up from from like the actual primitives but I've defined a bunch of",
    "start": "1469580",
    "end": "1475820"
  },
  {
    "text": "recording laws and it is under I don't",
    "start": "1475820",
    "end": "1483470"
  },
  {
    "text": "know I actually have forgotten namespace yeah that's not good idea let's do pod",
    "start": "1483470",
    "end": "1488920"
  },
  {
    "text": "[Music] here we go I've got it written in my",
    "start": "1488920",
    "end": "1498559"
  },
  {
    "text": "slide notes okay",
    "start": "1498559",
    "end": "1506240"
  },
  {
    "text": "well even more because I built this into a dashboard because I saw I don't have to remember these queries I have these",
    "start": "1506240",
    "end": "1512409"
  },
  {
    "text": "used method - but these cluster resorts dashboards which tell me how full my cluster is how committed my cluster is",
    "start": "1512409",
    "end": "1519529"
  },
  {
    "text": "so how much of the CPU I physically got is in use by it has been committed in",
    "start": "1519529",
    "end": "1525169"
  },
  {
    "text": "requests so this is what these mean and but these will tell me David's fixed him awesome this is this",
    "start": "1525169",
    "end": "1531139"
  },
  {
    "text": "is a fork of graph and we use because with a graph on you can't have these these multi column tables with numbers",
    "start": "1531139",
    "end": "1538700"
  },
  {
    "text": "in the columns so we've got a fork we're merging it all back in of course we're just currently running this fork but",
    "start": "1538700",
    "end": "1544760"
  },
  {
    "text": "let's go and have a look at this so yeah",
    "start": "1544760",
    "end": "1551090"
  },
  {
    "text": "there's the ax so we're going to take this so this is the container resource",
    "start": "1551090",
    "end": "1556519"
  },
  {
    "text": "requests I'll put this in here so this is what",
    "start": "1556519",
    "end": "1563760"
  },
  {
    "text": "this one looks like",
    "start": "1563760",
    "end": "1566330"
  },
  {
    "text": "so not particularly actually easier to see in in causal",
    "start": "1570009",
    "end": "1576398"
  },
  {
    "text": "but slower I blame the MiFi so here we",
    "start": "1580019",
    "end": "1585880"
  },
  {
    "text": "go you can see it's got it broken down by instance actually the incidents the same he's got it broken down by node and",
    "start": "1585880",
    "end": "1591490"
  },
  {
    "text": "namespace and the container and so on and it tells you basically how many CPUs",
    "start": "1591490",
    "end": "1597909"
  },
  {
    "text": "are requested for that pod so this pot needs two CPUs okay it's a pretty straightforward and then this one says",
    "start": "1597909",
    "end": "1604590"
  },
  {
    "text": "container CPU seconds by namespace now if we just take this you should be able",
    "start": "1604590",
    "end": "1615429"
  },
  {
    "text": "to see the same thing and this gives us the ongoing CPU usage of the containers",
    "start": "1615429",
    "end": "1623830"
  },
  {
    "text": "in my cluster and that graph was a bad idea and now we need to divide out the 2 and that will give me some some idea of",
    "start": "1623830",
    "end": "1630309"
  },
  {
    "text": "ego CPU usage of all the pods in mica in my cluster and if we divide that 2 it should give us the idea of how full are",
    "start": "1630309",
    "end": "1637179"
  },
  {
    "text": "my pods and I'm really gutted that I can't find the query for this because",
    "start": "1637179",
    "end": "1642490"
  },
  {
    "text": "it's not as simple as just dividing the two how long have I got timewise five minutes okay",
    "start": "1642490",
    "end": "1649120"
  },
  {
    "text": "I might not try and do it live because it's not the most interesting thing to watch yeah sorry about that you can't",
    "start": "1649120",
    "end": "1658690"
  },
  {
    "text": "you can divide the two but the problem is the pod name is different if you look here this one's using pod and the other",
    "start": "1658690",
    "end": "1666340"
  },
  {
    "text": "ones using pod underscore name and so you have to do a real able don't think",
    "start": "1666340",
    "end": "1672309"
  },
  {
    "text": "that in my recording rules anyway this is all in the in the open source so you can go knock yourself out and and show",
    "start": "1672309",
    "end": "1679899"
  },
  {
    "text": "me how to do this better all right here we go this is the one I need so we take",
    "start": "1679899",
    "end": "1689409"
  },
  {
    "text": "this one which is the recording rule that's tidied it all up and then we take",
    "start": "1689409",
    "end": "1695880"
  },
  {
    "text": "that one CPU cores contain a CPU usage is some rate that's the one and this is",
    "start": "1697280",
    "end": "1703520"
  },
  {
    "text": "the one that has it all tidied up as well and then we can just divide the two and we can go away there's a bug in our",
    "start": "1703520",
    "end": "1715820"
  },
  {
    "text": "dev you eye and you can see how full the services are if I do some label name and",
    "start": "1715820",
    "end": "1732250"
  },
  {
    "text": "some of this byte label name this will give it in a much more meaningful way",
    "start": "1732250",
    "end": "1739870"
  },
  {
    "text": "here we go these are the various services we've got running in the cluster and here you can see how full",
    "start": "1741850",
    "end": "1747080"
  },
  {
    "text": "they are you know 40% is a bit high maybe but it's fine and so this finally you can get an answer for you know what is the",
    "start": "1747080",
    "end": "1753940"
  },
  {
    "text": "saturation of their services in my cluster so if you want to read more about this the Google s3 book is online",
    "start": "1753940",
    "end": "1760010"
  },
  {
    "text": "and has an absolutely fantastic chapter on this and there's another blog that mentions it that I found on Google so in",
    "start": "1760010",
    "end": "1767390"
  },
  {
    "start": "1767000",
    "end": "1794000"
  },
  {
    "text": "summary we've covered the use method using it for monitoring the resource",
    "start": "1767390",
    "end": "1774080"
  },
  {
    "text": "utilization saturation requests for every resource really useful for infrastructure the read method that is",
    "start": "1774080",
    "end": "1780170"
  },
  {
    "text": "request errors and duration for every service in your cluster we've taken a quick look at for golden signals which",
    "start": "1780170",
    "end": "1785630"
  },
  {
    "text": "is red plus saturation and how you might want to do saturation on your cuba Nettie's cluster and then I've kind of",
    "start": "1785630",
    "end": "1793190"
  },
  {
    "text": "given you a couple of demos there's a couple of other methods out there and the TSA method is actually another one",
    "start": "1793190",
    "end": "1798200"
  },
  {
    "start": "1794000",
    "end": "1804000"
  },
  {
    "text": "of Brendan's Greg Brendan Greg's that looks at threading and Method R is I have no idea I need to",
    "start": "1798200",
    "end": "1804320"
  },
  {
    "text": "look at them for the rest of the day so you're already in the read method talk which means you're probably interested in prometheus there's a meet the",
    "start": "1804320",
    "end": "1810020"
  },
  {
    "text": "maintainer x' where you can come in and argue with me about the stuff i've discussed here and we're also going to have frederick and girl from there if",
    "start": "1810020",
    "end": "1816020"
  },
  {
    "text": "you want to ask them some more interesting questions Ilya from weave works is giving a talk practical rights",
    "start": "1816020",
    "end": "1821210"
  },
  {
    "text": "of Prometheus for app developers and the guys from the French FedEx company are giving a talk about how they use",
    "start": "1821210",
    "end": "1826460"
  },
  {
    "text": "Prometheus and then with a p.m. after the booth call we've got some drinks a local hotel sponsored by the fresh",
    "start": "1826460",
    "end": "1834040"
  },
  {
    "text": "tracks guys there's the link here if you want to sign up I take it with probably getting close to capacity so sign up",
    "start": "1834040",
    "end": "1839410"
  },
  {
    "text": "quick the Prometheus salon we did earlier today was so full like standing-room-only",
    "start": "1839410",
    "end": "1844480"
  },
  {
    "text": "that we're going to run it again on Friday at 2 p.m. if anyone didn't manage to get through the door yes we will Pete",
    "start": "1844480",
    "end": "1851919"
  },
  {
    "text": "not this session the salon anyway thank you very much any questions there's a",
    "start": "1851919",
    "end": "1862090"
  },
  {
    "start": "1861000",
    "end": "1873000"
  },
  {
    "text": "there's a microphone behind you if you want to talk into the microphone and then and then everyone will be able to",
    "start": "1862090",
    "end": "1869620"
  },
  {
    "text": "hear form nordley queue at the battery's",
    "start": "1869620",
    "end": "1877840"
  },
  {
    "start": "1873000",
    "end": "1880000"
  },
  {
    "text": "flat alright just shout it I'll repeat the question what do we alert on we",
    "start": "1877840",
    "end": "1884650"
  },
  {
    "start": "1880000",
    "end": "2094000"
  },
  {
    "text": "generally will alert on error rate a key thing to get right when you're alerting",
    "start": "1884650",
    "end": "1889990"
  },
  {
    "text": "on an error rate is not alert on the error rate for the entire service because you could have like you know",
    "start": "1889990",
    "end": "1895179"
  },
  {
    "text": "most services have more than one RPC that they accept especially if it's your front end you might have a hundred different parts in your front end and",
    "start": "1895179",
    "end": "1901809"
  },
  {
    "text": "your one root in your front end might be were returning a hundred percent errors and you'll miss it if that route isn't",
    "start": "1901809",
    "end": "1907450"
  },
  {
    "text": "called as much as say another one you know so in in causal we've got one route the right path that's called like three",
    "start": "1907450",
    "end": "1913299"
  },
  {
    "text": "orders of magnitude more fun than the query path so any errors on the query get completely drowned out if if we just",
    "start": "1913299",
    "end": "1919360"
  },
  {
    "text": "were to alert on the total error rate so we do error rate per service per boot",
    "start": "1919360",
    "end": "1924460"
  },
  {
    "text": "and then latency as well we alert on latency we have some pretty strict targets that we don't always meet so",
    "start": "1924460",
    "end": "1931840"
  },
  {
    "text": "right now they don't actually pages unless they're like really really bad but hopefully as we start to meet our",
    "start": "1931840",
    "end": "1938500"
  },
  {
    "text": "stricter targets will start alerting on breaches of those but that's pretty much it and then you I tend to be of the idea",
    "start": "1938500",
    "end": "1945850"
  },
  {
    "text": "that you shouldn't alert you know it's google standard google practice don't alert on symptoms don't alert on causes",
    "start": "1945850",
    "end": "1951040"
  },
  {
    "text": "a lot on symptoms so you should alert on things that feature SLA and we don't alert on you know random service over",
    "start": "1951040",
    "end": "1957700"
  },
  {
    "text": "here it hasn't been able to elect a master unless some something at the top starts then complaining the that that's causing errors to a class to",
    "start": "1957700",
    "end": "1964820"
  },
  {
    "text": "a user the only exception I make for that is disk capacity because when you run out of disk capacity everything stops so that's the only one where I",
    "start": "1964820",
    "end": "1971749"
  },
  {
    "text": "alert on a on a kind of symptom ball cause I'll give them more way around thank you good question",
    "start": "1971749",
    "end": "1979570"
  },
  {
    "text": "do you instrument outgoing requests with the read method that's a really good question I don't except for when I'm talking to",
    "start": "1983499",
    "end": "1990469"
  },
  {
    "text": "Amazon so nothing wrong with Amazon like it works it's just there's I you know",
    "start": "1990469",
    "end": "1997339"
  },
  {
    "text": "I've been really dissatisfied with the metrics I can get out of cloud watch so what I tend to do for instance when I'm",
    "start": "1997339",
    "end": "2003009"
  },
  {
    "text": "talking to DynamoDB or when I'm talking to you know s3 or any other service that I don't own I instrument on the client",
    "start": "2003009",
    "end": "2009009"
  },
  {
    "text": "side but normally when I'm talking services that I do own I think it's good enough to just instrument on the server",
    "start": "2009009",
    "end": "2014799"
  },
  {
    "text": "side I mean you can do both but then that kind of depth-first traversal of all your services starts become really",
    "start": "2014799",
    "end": "2020649"
  },
  {
    "text": "complicated right and when the network is broken it normally becomes pretty obvious pretty quickly so hopefully that",
    "start": "2020649",
    "end": "2027940"
  },
  {
    "text": "answers your question any other questions anyone from over on stage right middle",
    "start": "2027940",
    "end": "2036809"
  },
  {
    "text": "done",
    "start": "2036809",
    "end": "2039809"
  },
  {
    "text": "so the question was in this dashboard where do I get the the graph of what",
    "start": "2046010",
    "end": "2051240"
  },
  {
    "text": "service talks to what other service now I just know in my head and so that's just you know what I do but there is a",
    "start": "2051240",
    "end": "2057570"
  },
  {
    "text": "cool tool from my previous company which I wrote called weave scope do you use",
    "start": "2057570",
    "end": "2063990"
  },
  {
    "text": "read scope great I I spent two years of my life writing them and then I moved on",
    "start": "2063990",
    "end": "2070470"
  },
  {
    "text": "to prometheus but we've scoped kind of gives you that um nuggets screenshots but that's probably better screenshots",
    "start": "2070470",
    "end": "2077638"
  },
  {
    "text": "yeah gives you that kind of dag of of what talks to what and you could use that for instance if you didn't know but I tend",
    "start": "2077639",
    "end": "2084240"
  },
  {
    "text": "to just know so I use those anymore questions okay thank you very much",
    "start": "2084240",
    "end": "2091120"
  },
  {
    "text": "[Applause]",
    "start": "2091120",
    "end": "2096750"
  }
]